Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=54, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3024-3079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876445
Iteration 2/25 | Loss: 0.00209508
Iteration 3/25 | Loss: 0.00151023
Iteration 4/25 | Loss: 0.00135979
Iteration 5/25 | Loss: 0.00124747
Iteration 6/25 | Loss: 0.00098237
Iteration 7/25 | Loss: 0.00093735
Iteration 8/25 | Loss: 0.00092658
Iteration 9/25 | Loss: 0.00091435
Iteration 10/25 | Loss: 0.00091333
Iteration 11/25 | Loss: 0.00090894
Iteration 12/25 | Loss: 0.00090581
Iteration 13/25 | Loss: 0.00090640
Iteration 14/25 | Loss: 0.00090348
Iteration 15/25 | Loss: 0.00090289
Iteration 16/25 | Loss: 0.00090237
Iteration 17/25 | Loss: 0.00090225
Iteration 18/25 | Loss: 0.00090225
Iteration 19/25 | Loss: 0.00090225
Iteration 20/25 | Loss: 0.00090225
Iteration 21/25 | Loss: 0.00090225
Iteration 22/25 | Loss: 0.00090225
Iteration 23/25 | Loss: 0.00090225
Iteration 24/25 | Loss: 0.00090225
Iteration 25/25 | Loss: 0.00090225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61152637
Iteration 2/25 | Loss: 0.00131842
Iteration 3/25 | Loss: 0.00131840
Iteration 4/25 | Loss: 0.00131840
Iteration 5/25 | Loss: 0.00131840
Iteration 6/25 | Loss: 0.00131840
Iteration 7/25 | Loss: 0.00131840
Iteration 8/25 | Loss: 0.00131840
Iteration 9/25 | Loss: 0.00131840
Iteration 10/25 | Loss: 0.00131840
Iteration 11/25 | Loss: 0.00131840
Iteration 12/25 | Loss: 0.00131840
Iteration 13/25 | Loss: 0.00131840
Iteration 14/25 | Loss: 0.00131840
Iteration 15/25 | Loss: 0.00131840
Iteration 16/25 | Loss: 0.00131840
Iteration 17/25 | Loss: 0.00131840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013183957198634744, 0.0013183957198634744, 0.0013183957198634744, 0.0013183957198634744, 0.0013183957198634744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013183957198634744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131840
Iteration 2/1000 | Loss: 0.00003597
Iteration 3/1000 | Loss: 0.00005618
Iteration 4/1000 | Loss: 0.00002886
Iteration 5/1000 | Loss: 0.00004640
Iteration 6/1000 | Loss: 0.00003750
Iteration 7/1000 | Loss: 0.00002312
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00004039
Iteration 10/1000 | Loss: 0.00003113
Iteration 11/1000 | Loss: 0.00004237
Iteration 12/1000 | Loss: 0.00002732
Iteration 13/1000 | Loss: 0.00002372
Iteration 14/1000 | Loss: 0.00003451
Iteration 15/1000 | Loss: 0.00002238
Iteration 16/1000 | Loss: 0.00002132
Iteration 17/1000 | Loss: 0.00002016
Iteration 18/1000 | Loss: 0.00001989
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001964
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001960
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001952
Iteration 28/1000 | Loss: 0.00001948
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001943
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001937
Iteration 43/1000 | Loss: 0.00001936
Iteration 44/1000 | Loss: 0.00001936
Iteration 45/1000 | Loss: 0.00001936
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001932
Iteration 49/1000 | Loss: 0.00001931
Iteration 50/1000 | Loss: 0.00001928
Iteration 51/1000 | Loss: 0.00001928
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001927
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001924
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001920
Iteration 69/1000 | Loss: 0.00001920
Iteration 70/1000 | Loss: 0.00001920
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001919
Iteration 73/1000 | Loss: 0.00001919
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001918
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001918
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001915
Iteration 89/1000 | Loss: 0.00001915
Iteration 90/1000 | Loss: 0.00001915
Iteration 91/1000 | Loss: 0.00001915
Iteration 92/1000 | Loss: 0.00001915
Iteration 93/1000 | Loss: 0.00001915
Iteration 94/1000 | Loss: 0.00001914
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001913
Iteration 100/1000 | Loss: 0.00001913
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001912
Iteration 117/1000 | Loss: 0.00001912
Iteration 118/1000 | Loss: 0.00001912
Iteration 119/1000 | Loss: 0.00001912
Iteration 120/1000 | Loss: 0.00001912
Iteration 121/1000 | Loss: 0.00001912
Iteration 122/1000 | Loss: 0.00001912
Iteration 123/1000 | Loss: 0.00001911
Iteration 124/1000 | Loss: 0.00001911
Iteration 125/1000 | Loss: 0.00001911
Iteration 126/1000 | Loss: 0.00001911
Iteration 127/1000 | Loss: 0.00001911
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001911
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001911
Iteration 136/1000 | Loss: 0.00001911
Iteration 137/1000 | Loss: 0.00001911
Iteration 138/1000 | Loss: 0.00001911
Iteration 139/1000 | Loss: 0.00001911
Iteration 140/1000 | Loss: 0.00001911
Iteration 141/1000 | Loss: 0.00001911
Iteration 142/1000 | Loss: 0.00001911
Iteration 143/1000 | Loss: 0.00001911
Iteration 144/1000 | Loss: 0.00001911
Iteration 145/1000 | Loss: 0.00001911
Iteration 146/1000 | Loss: 0.00001911
Iteration 147/1000 | Loss: 0.00001911
Iteration 148/1000 | Loss: 0.00001911
Iteration 149/1000 | Loss: 0.00001911
Iteration 150/1000 | Loss: 0.00001911
Iteration 151/1000 | Loss: 0.00001911
Iteration 152/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.9108421838609502e-05, 1.9108421838609502e-05, 1.9108421838609502e-05, 1.9108421838609502e-05, 1.9108421838609502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9108421838609502e-05

Optimization complete. Final v2v error: 3.704439401626587 mm

Highest mean error: 4.710093975067139 mm for frame 197

Lowest mean error: 3.5205328464508057 mm for frame 39

Saving results

Total time: 78.47983312606812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047828
Iteration 2/25 | Loss: 0.01047827
Iteration 3/25 | Loss: 0.01047827
Iteration 4/25 | Loss: 0.01047826
Iteration 5/25 | Loss: 0.01047826
Iteration 6/25 | Loss: 0.01047825
Iteration 7/25 | Loss: 0.00217623
Iteration 8/25 | Loss: 0.00125059
Iteration 9/25 | Loss: 0.00112951
Iteration 10/25 | Loss: 0.00109388
Iteration 11/25 | Loss: 0.00106813
Iteration 12/25 | Loss: 0.00104602
Iteration 13/25 | Loss: 0.00102185
Iteration 14/25 | Loss: 0.00098751
Iteration 15/25 | Loss: 0.00097339
Iteration 16/25 | Loss: 0.00095830
Iteration 17/25 | Loss: 0.00094960
Iteration 18/25 | Loss: 0.00093464
Iteration 19/25 | Loss: 0.00093790
Iteration 20/25 | Loss: 0.00093591
Iteration 21/25 | Loss: 0.00093308
Iteration 22/25 | Loss: 0.00094078
Iteration 23/25 | Loss: 0.00093343
Iteration 24/25 | Loss: 0.00092573
Iteration 25/25 | Loss: 0.00093378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65795088
Iteration 2/25 | Loss: 0.00365447
Iteration 3/25 | Loss: 0.00351742
Iteration 4/25 | Loss: 0.00351741
Iteration 5/25 | Loss: 0.00351741
Iteration 6/25 | Loss: 0.00351741
Iteration 7/25 | Loss: 0.00351741
Iteration 8/25 | Loss: 0.00351741
Iteration 9/25 | Loss: 0.00351741
Iteration 10/25 | Loss: 0.00351741
Iteration 11/25 | Loss: 0.00351741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0035174076911062002, 0.0035174076911062002, 0.0035174076911062002, 0.0035174076911062002, 0.0035174076911062002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035174076911062002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351741
Iteration 2/1000 | Loss: 0.00222481
Iteration 3/1000 | Loss: 0.00446003
Iteration 4/1000 | Loss: 0.00216410
Iteration 5/1000 | Loss: 0.00084219
Iteration 6/1000 | Loss: 0.00051081
Iteration 7/1000 | Loss: 0.00037776
Iteration 8/1000 | Loss: 0.00048468
Iteration 9/1000 | Loss: 0.00057065
Iteration 10/1000 | Loss: 0.00059784
Iteration 11/1000 | Loss: 0.00038201
Iteration 12/1000 | Loss: 0.00071614
Iteration 13/1000 | Loss: 0.00059034
Iteration 14/1000 | Loss: 0.00090247
Iteration 15/1000 | Loss: 0.00060037
Iteration 16/1000 | Loss: 0.00023707
Iteration 17/1000 | Loss: 0.00013783
Iteration 18/1000 | Loss: 0.00006670
Iteration 19/1000 | Loss: 0.00016937
Iteration 20/1000 | Loss: 0.00086418
Iteration 21/1000 | Loss: 0.00101278
Iteration 22/1000 | Loss: 0.00067360
Iteration 23/1000 | Loss: 0.00039456
Iteration 24/1000 | Loss: 0.00043900
Iteration 25/1000 | Loss: 0.00016808
Iteration 26/1000 | Loss: 0.00008750
Iteration 27/1000 | Loss: 0.00057222
Iteration 28/1000 | Loss: 0.00017242
Iteration 29/1000 | Loss: 0.00020530
Iteration 30/1000 | Loss: 0.00006831
Iteration 31/1000 | Loss: 0.00038117
Iteration 32/1000 | Loss: 0.00017997
Iteration 33/1000 | Loss: 0.00054062
Iteration 34/1000 | Loss: 0.00020308
Iteration 35/1000 | Loss: 0.00025291
Iteration 36/1000 | Loss: 0.00016906
Iteration 37/1000 | Loss: 0.00009712
Iteration 38/1000 | Loss: 0.00010864
Iteration 39/1000 | Loss: 0.00026580
Iteration 40/1000 | Loss: 0.00011339
Iteration 41/1000 | Loss: 0.00004507
Iteration 42/1000 | Loss: 0.00019376
Iteration 43/1000 | Loss: 0.00025279
Iteration 44/1000 | Loss: 0.00017113
Iteration 45/1000 | Loss: 0.00004661
Iteration 46/1000 | Loss: 0.00017534
Iteration 47/1000 | Loss: 0.00032739
Iteration 48/1000 | Loss: 0.00023597
Iteration 49/1000 | Loss: 0.00006265
Iteration 50/1000 | Loss: 0.00004827
Iteration 51/1000 | Loss: 0.00003971
Iteration 52/1000 | Loss: 0.00029528
Iteration 53/1000 | Loss: 0.00055464
Iteration 54/1000 | Loss: 0.00035386
Iteration 55/1000 | Loss: 0.00020730
Iteration 56/1000 | Loss: 0.00018053
Iteration 57/1000 | Loss: 0.00018305
Iteration 58/1000 | Loss: 0.00015520
Iteration 59/1000 | Loss: 0.00018450
Iteration 60/1000 | Loss: 0.00014550
Iteration 61/1000 | Loss: 0.00038787
Iteration 62/1000 | Loss: 0.00014657
Iteration 63/1000 | Loss: 0.00020943
Iteration 64/1000 | Loss: 0.00014840
Iteration 65/1000 | Loss: 0.00012850
Iteration 66/1000 | Loss: 0.00005716
Iteration 67/1000 | Loss: 0.00003724
Iteration 68/1000 | Loss: 0.00003100
Iteration 69/1000 | Loss: 0.00003767
Iteration 70/1000 | Loss: 0.00002841
Iteration 71/1000 | Loss: 0.00002659
Iteration 72/1000 | Loss: 0.00014400
Iteration 73/1000 | Loss: 0.00073949
Iteration 74/1000 | Loss: 0.00071725
Iteration 75/1000 | Loss: 0.00026356
Iteration 76/1000 | Loss: 0.00071222
Iteration 77/1000 | Loss: 0.00018130
Iteration 78/1000 | Loss: 0.00010810
Iteration 79/1000 | Loss: 0.00020183
Iteration 80/1000 | Loss: 0.00003030
Iteration 81/1000 | Loss: 0.00016776
Iteration 82/1000 | Loss: 0.00017411
Iteration 83/1000 | Loss: 0.00017711
Iteration 84/1000 | Loss: 0.00015209
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00018568
Iteration 87/1000 | Loss: 0.00002646
Iteration 88/1000 | Loss: 0.00020367
Iteration 89/1000 | Loss: 0.00020838
Iteration 90/1000 | Loss: 0.00019907
Iteration 91/1000 | Loss: 0.00019627
Iteration 92/1000 | Loss: 0.00015681
Iteration 93/1000 | Loss: 0.00018930
Iteration 94/1000 | Loss: 0.00014544
Iteration 95/1000 | Loss: 0.00017999
Iteration 96/1000 | Loss: 0.00002373
Iteration 97/1000 | Loss: 0.00012900
Iteration 98/1000 | Loss: 0.00003275
Iteration 99/1000 | Loss: 0.00002289
Iteration 100/1000 | Loss: 0.00002133
Iteration 101/1000 | Loss: 0.00058286
Iteration 102/1000 | Loss: 0.00014733
Iteration 103/1000 | Loss: 0.00007803
Iteration 104/1000 | Loss: 0.00015003
Iteration 105/1000 | Loss: 0.00003990
Iteration 106/1000 | Loss: 0.00006858
Iteration 107/1000 | Loss: 0.00002352
Iteration 108/1000 | Loss: 0.00002019
Iteration 109/1000 | Loss: 0.00001914
Iteration 110/1000 | Loss: 0.00001849
Iteration 111/1000 | Loss: 0.00001785
Iteration 112/1000 | Loss: 0.00002873
Iteration 113/1000 | Loss: 0.00001713
Iteration 114/1000 | Loss: 0.00002868
Iteration 115/1000 | Loss: 0.00011622
Iteration 116/1000 | Loss: 0.00015207
Iteration 117/1000 | Loss: 0.00006231
Iteration 118/1000 | Loss: 0.00013994
Iteration 119/1000 | Loss: 0.00006134
Iteration 120/1000 | Loss: 0.00009865
Iteration 121/1000 | Loss: 0.00006412
Iteration 122/1000 | Loss: 0.00011407
Iteration 123/1000 | Loss: 0.00008097
Iteration 124/1000 | Loss: 0.00006864
Iteration 125/1000 | Loss: 0.00010021
Iteration 126/1000 | Loss: 0.00004880
Iteration 127/1000 | Loss: 0.00024634
Iteration 128/1000 | Loss: 0.00024694
Iteration 129/1000 | Loss: 0.00014775
Iteration 130/1000 | Loss: 0.00005433
Iteration 131/1000 | Loss: 0.00012048
Iteration 132/1000 | Loss: 0.00009883
Iteration 133/1000 | Loss: 0.00009170
Iteration 134/1000 | Loss: 0.00011085
Iteration 135/1000 | Loss: 0.00008789
Iteration 136/1000 | Loss: 0.00009583
Iteration 137/1000 | Loss: 0.00010444
Iteration 138/1000 | Loss: 0.00015687
Iteration 139/1000 | Loss: 0.00014926
Iteration 140/1000 | Loss: 0.00002189
Iteration 141/1000 | Loss: 0.00004110
Iteration 142/1000 | Loss: 0.00002266
Iteration 143/1000 | Loss: 0.00002125
Iteration 144/1000 | Loss: 0.00001805
Iteration 145/1000 | Loss: 0.00001857
Iteration 146/1000 | Loss: 0.00001992
Iteration 147/1000 | Loss: 0.00001849
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00006460
Iteration 150/1000 | Loss: 0.00001622
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001607
Iteration 153/1000 | Loss: 0.00001607
Iteration 154/1000 | Loss: 0.00002028
Iteration 155/1000 | Loss: 0.00001688
Iteration 156/1000 | Loss: 0.00001724
Iteration 157/1000 | Loss: 0.00002915
Iteration 158/1000 | Loss: 0.00002274
Iteration 159/1000 | Loss: 0.00001583
Iteration 160/1000 | Loss: 0.00001581
Iteration 161/1000 | Loss: 0.00001581
Iteration 162/1000 | Loss: 0.00001580
Iteration 163/1000 | Loss: 0.00001578
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001659
Iteration 167/1000 | Loss: 0.00001606
Iteration 168/1000 | Loss: 0.00001606
Iteration 169/1000 | Loss: 0.00001564
Iteration 170/1000 | Loss: 0.00001562
Iteration 171/1000 | Loss: 0.00001561
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00001559
Iteration 174/1000 | Loss: 0.00001599
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001529
Iteration 178/1000 | Loss: 0.00001509
Iteration 179/1000 | Loss: 0.00001504
Iteration 180/1000 | Loss: 0.00002293
Iteration 181/1000 | Loss: 0.00004151
Iteration 182/1000 | Loss: 0.00002281
Iteration 183/1000 | Loss: 0.00001466
Iteration 184/1000 | Loss: 0.00001465
Iteration 185/1000 | Loss: 0.00001465
Iteration 186/1000 | Loss: 0.00001659
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001458
Iteration 189/1000 | Loss: 0.00001458
Iteration 190/1000 | Loss: 0.00001458
Iteration 191/1000 | Loss: 0.00001458
Iteration 192/1000 | Loss: 0.00001458
Iteration 193/1000 | Loss: 0.00001458
Iteration 194/1000 | Loss: 0.00001458
Iteration 195/1000 | Loss: 0.00001458
Iteration 196/1000 | Loss: 0.00001458
Iteration 197/1000 | Loss: 0.00001458
Iteration 198/1000 | Loss: 0.00001458
Iteration 199/1000 | Loss: 0.00001458
Iteration 200/1000 | Loss: 0.00001458
Iteration 201/1000 | Loss: 0.00001458
Iteration 202/1000 | Loss: 0.00001458
Iteration 203/1000 | Loss: 0.00001458
Iteration 204/1000 | Loss: 0.00001458
Iteration 205/1000 | Loss: 0.00001458
Iteration 206/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4575400200556032e-05, 1.4575400200556032e-05, 1.4575400200556032e-05, 1.4575400200556032e-05, 1.4575400200556032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4575400200556032e-05

Optimization complete. Final v2v error: 3.1134965419769287 mm

Highest mean error: 9.74313735961914 mm for frame 196

Lowest mean error: 2.530970335006714 mm for frame 203

Saving results

Total time: 322.4654905796051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847640
Iteration 2/25 | Loss: 0.00151960
Iteration 3/25 | Loss: 0.00110211
Iteration 4/25 | Loss: 0.00102154
Iteration 5/25 | Loss: 0.00099996
Iteration 6/25 | Loss: 0.00099382
Iteration 7/25 | Loss: 0.00099374
Iteration 8/25 | Loss: 0.00098868
Iteration 9/25 | Loss: 0.00098562
Iteration 10/25 | Loss: 0.00098460
Iteration 11/25 | Loss: 0.00098393
Iteration 12/25 | Loss: 0.00098320
Iteration 13/25 | Loss: 0.00098254
Iteration 14/25 | Loss: 0.00098171
Iteration 15/25 | Loss: 0.00098082
Iteration 16/25 | Loss: 0.00097978
Iteration 17/25 | Loss: 0.00097930
Iteration 18/25 | Loss: 0.00097908
Iteration 19/25 | Loss: 0.00097893
Iteration 20/25 | Loss: 0.00097888
Iteration 21/25 | Loss: 0.00097888
Iteration 22/25 | Loss: 0.00097887
Iteration 23/25 | Loss: 0.00097887
Iteration 24/25 | Loss: 0.00097887
Iteration 25/25 | Loss: 0.00097887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.73443389
Iteration 2/25 | Loss: 0.00342463
Iteration 3/25 | Loss: 0.00342457
Iteration 4/25 | Loss: 0.00342457
Iteration 5/25 | Loss: 0.00342457
Iteration 6/25 | Loss: 0.00342457
Iteration 7/25 | Loss: 0.00342457
Iteration 8/25 | Loss: 0.00342457
Iteration 9/25 | Loss: 0.00342457
Iteration 10/25 | Loss: 0.00342457
Iteration 11/25 | Loss: 0.00342457
Iteration 12/25 | Loss: 0.00342457
Iteration 13/25 | Loss: 0.00342457
Iteration 14/25 | Loss: 0.00342457
Iteration 15/25 | Loss: 0.00342457
Iteration 16/25 | Loss: 0.00342457
Iteration 17/25 | Loss: 0.00342457
Iteration 18/25 | Loss: 0.00342457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003424566239118576, 0.003424566239118576, 0.003424566239118576, 0.003424566239118576, 0.003424566239118576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003424566239118576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342457
Iteration 2/1000 | Loss: 0.00955085
Iteration 3/1000 | Loss: 0.00173760
Iteration 4/1000 | Loss: 0.00027328
Iteration 5/1000 | Loss: 0.00016699
Iteration 6/1000 | Loss: 0.00009504
Iteration 7/1000 | Loss: 0.00006041
Iteration 8/1000 | Loss: 0.00004166
Iteration 9/1000 | Loss: 0.00003373
Iteration 10/1000 | Loss: 0.00002748
Iteration 11/1000 | Loss: 0.00002463
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002143
Iteration 14/1000 | Loss: 0.00029578
Iteration 15/1000 | Loss: 0.00003350
Iteration 16/1000 | Loss: 0.00002692
Iteration 17/1000 | Loss: 0.00049289
Iteration 18/1000 | Loss: 0.00031117
Iteration 19/1000 | Loss: 0.00003218
Iteration 20/1000 | Loss: 0.00002178
Iteration 21/1000 | Loss: 0.00001953
Iteration 22/1000 | Loss: 0.00001840
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001628
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001607
Iteration 34/1000 | Loss: 0.00001606
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001602
Iteration 40/1000 | Loss: 0.00001601
Iteration 41/1000 | Loss: 0.00001601
Iteration 42/1000 | Loss: 0.00001601
Iteration 43/1000 | Loss: 0.00001600
Iteration 44/1000 | Loss: 0.00001600
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001598
Iteration 47/1000 | Loss: 0.00001598
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001597
Iteration 50/1000 | Loss: 0.00001597
Iteration 51/1000 | Loss: 0.00001596
Iteration 52/1000 | Loss: 0.00001596
Iteration 53/1000 | Loss: 0.00001594
Iteration 54/1000 | Loss: 0.00001594
Iteration 55/1000 | Loss: 0.00001594
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001593
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001593
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001593
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001592
Iteration 67/1000 | Loss: 0.00001592
Iteration 68/1000 | Loss: 0.00001591
Iteration 69/1000 | Loss: 0.00001591
Iteration 70/1000 | Loss: 0.00001591
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001585
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001585
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001580
Iteration 126/1000 | Loss: 0.00001580
Iteration 127/1000 | Loss: 0.00001580
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001579
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001578
Iteration 142/1000 | Loss: 0.00001578
Iteration 143/1000 | Loss: 0.00001578
Iteration 144/1000 | Loss: 0.00001578
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001576
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001576
Iteration 154/1000 | Loss: 0.00001576
Iteration 155/1000 | Loss: 0.00001576
Iteration 156/1000 | Loss: 0.00001576
Iteration 157/1000 | Loss: 0.00001576
Iteration 158/1000 | Loss: 0.00001576
Iteration 159/1000 | Loss: 0.00001575
Iteration 160/1000 | Loss: 0.00001575
Iteration 161/1000 | Loss: 0.00001575
Iteration 162/1000 | Loss: 0.00001575
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001574
Iteration 165/1000 | Loss: 0.00001574
Iteration 166/1000 | Loss: 0.00001574
Iteration 167/1000 | Loss: 0.00001574
Iteration 168/1000 | Loss: 0.00001573
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00001573
Iteration 171/1000 | Loss: 0.00001573
Iteration 172/1000 | Loss: 0.00001573
Iteration 173/1000 | Loss: 0.00001573
Iteration 174/1000 | Loss: 0.00001573
Iteration 175/1000 | Loss: 0.00001573
Iteration 176/1000 | Loss: 0.00001573
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001572
Iteration 182/1000 | Loss: 0.00001572
Iteration 183/1000 | Loss: 0.00001572
Iteration 184/1000 | Loss: 0.00001572
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001570
Iteration 189/1000 | Loss: 0.00001570
Iteration 190/1000 | Loss: 0.00001570
Iteration 191/1000 | Loss: 0.00001570
Iteration 192/1000 | Loss: 0.00001570
Iteration 193/1000 | Loss: 0.00001569
Iteration 194/1000 | Loss: 0.00001569
Iteration 195/1000 | Loss: 0.00001569
Iteration 196/1000 | Loss: 0.00001569
Iteration 197/1000 | Loss: 0.00001569
Iteration 198/1000 | Loss: 0.00001569
Iteration 199/1000 | Loss: 0.00001569
Iteration 200/1000 | Loss: 0.00001569
Iteration 201/1000 | Loss: 0.00001569
Iteration 202/1000 | Loss: 0.00001569
Iteration 203/1000 | Loss: 0.00001569
Iteration 204/1000 | Loss: 0.00001569
Iteration 205/1000 | Loss: 0.00001569
Iteration 206/1000 | Loss: 0.00001569
Iteration 207/1000 | Loss: 0.00001568
Iteration 208/1000 | Loss: 0.00001568
Iteration 209/1000 | Loss: 0.00001568
Iteration 210/1000 | Loss: 0.00001568
Iteration 211/1000 | Loss: 0.00001568
Iteration 212/1000 | Loss: 0.00001568
Iteration 213/1000 | Loss: 0.00001568
Iteration 214/1000 | Loss: 0.00001568
Iteration 215/1000 | Loss: 0.00001568
Iteration 216/1000 | Loss: 0.00001568
Iteration 217/1000 | Loss: 0.00001568
Iteration 218/1000 | Loss: 0.00001568
Iteration 219/1000 | Loss: 0.00001568
Iteration 220/1000 | Loss: 0.00001568
Iteration 221/1000 | Loss: 0.00001568
Iteration 222/1000 | Loss: 0.00001568
Iteration 223/1000 | Loss: 0.00001568
Iteration 224/1000 | Loss: 0.00001568
Iteration 225/1000 | Loss: 0.00001568
Iteration 226/1000 | Loss: 0.00001568
Iteration 227/1000 | Loss: 0.00001568
Iteration 228/1000 | Loss: 0.00001568
Iteration 229/1000 | Loss: 0.00001568
Iteration 230/1000 | Loss: 0.00001568
Iteration 231/1000 | Loss: 0.00001568
Iteration 232/1000 | Loss: 0.00001568
Iteration 233/1000 | Loss: 0.00001568
Iteration 234/1000 | Loss: 0.00001568
Iteration 235/1000 | Loss: 0.00001568
Iteration 236/1000 | Loss: 0.00001568
Iteration 237/1000 | Loss: 0.00001568
Iteration 238/1000 | Loss: 0.00001568
Iteration 239/1000 | Loss: 0.00001568
Iteration 240/1000 | Loss: 0.00001568
Iteration 241/1000 | Loss: 0.00001568
Iteration 242/1000 | Loss: 0.00001568
Iteration 243/1000 | Loss: 0.00001568
Iteration 244/1000 | Loss: 0.00001568
Iteration 245/1000 | Loss: 0.00001568
Iteration 246/1000 | Loss: 0.00001568
Iteration 247/1000 | Loss: 0.00001568
Iteration 248/1000 | Loss: 0.00001568
Iteration 249/1000 | Loss: 0.00001568
Iteration 250/1000 | Loss: 0.00001568
Iteration 251/1000 | Loss: 0.00001568
Iteration 252/1000 | Loss: 0.00001568
Iteration 253/1000 | Loss: 0.00001568
Iteration 254/1000 | Loss: 0.00001568
Iteration 255/1000 | Loss: 0.00001568
Iteration 256/1000 | Loss: 0.00001568
Iteration 257/1000 | Loss: 0.00001568
Iteration 258/1000 | Loss: 0.00001568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.5681958757340908e-05, 1.5681958757340908e-05, 1.5681958757340908e-05, 1.5681958757340908e-05, 1.5681958757340908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5681958757340908e-05

Optimization complete. Final v2v error: 3.3533990383148193 mm

Highest mean error: 3.795109510421753 mm for frame 59

Lowest mean error: 3.1402854919433594 mm for frame 2

Saving results

Total time: 102.98881387710571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935452
Iteration 2/25 | Loss: 0.00193397
Iteration 3/25 | Loss: 0.00138070
Iteration 4/25 | Loss: 0.00120123
Iteration 5/25 | Loss: 0.00105118
Iteration 6/25 | Loss: 0.00102694
Iteration 7/25 | Loss: 0.00100370
Iteration 8/25 | Loss: 0.00097179
Iteration 9/25 | Loss: 0.00095383
Iteration 10/25 | Loss: 0.00092716
Iteration 11/25 | Loss: 0.00093194
Iteration 12/25 | Loss: 0.00091288
Iteration 13/25 | Loss: 0.00091280
Iteration 14/25 | Loss: 0.00090351
Iteration 15/25 | Loss: 0.00090000
Iteration 16/25 | Loss: 0.00090490
Iteration 17/25 | Loss: 0.00089769
Iteration 18/25 | Loss: 0.00089549
Iteration 19/25 | Loss: 0.00089506
Iteration 20/25 | Loss: 0.00089485
Iteration 21/25 | Loss: 0.00089382
Iteration 22/25 | Loss: 0.00089250
Iteration 23/25 | Loss: 0.00089881
Iteration 24/25 | Loss: 0.00089727
Iteration 25/25 | Loss: 0.00089097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74482775
Iteration 2/25 | Loss: 0.00159546
Iteration 3/25 | Loss: 0.00159544
Iteration 4/25 | Loss: 0.00159544
Iteration 5/25 | Loss: 0.00159544
Iteration 6/25 | Loss: 0.00159544
Iteration 7/25 | Loss: 0.00159544
Iteration 8/25 | Loss: 0.00159544
Iteration 9/25 | Loss: 0.00159544
Iteration 10/25 | Loss: 0.00159544
Iteration 11/25 | Loss: 0.00159544
Iteration 12/25 | Loss: 0.00159544
Iteration 13/25 | Loss: 0.00159544
Iteration 14/25 | Loss: 0.00159544
Iteration 15/25 | Loss: 0.00159544
Iteration 16/25 | Loss: 0.00159544
Iteration 17/25 | Loss: 0.00159544
Iteration 18/25 | Loss: 0.00159544
Iteration 19/25 | Loss: 0.00159544
Iteration 20/25 | Loss: 0.00159544
Iteration 21/25 | Loss: 0.00159544
Iteration 22/25 | Loss: 0.00159544
Iteration 23/25 | Loss: 0.00159544
Iteration 24/25 | Loss: 0.00159544
Iteration 25/25 | Loss: 0.00159544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159544
Iteration 2/1000 | Loss: 0.00008185
Iteration 3/1000 | Loss: 0.00006170
Iteration 4/1000 | Loss: 0.00005186
Iteration 5/1000 | Loss: 0.00004738
Iteration 6/1000 | Loss: 0.00141254
Iteration 7/1000 | Loss: 0.00087380
Iteration 8/1000 | Loss: 0.00005277
Iteration 9/1000 | Loss: 0.00003571
Iteration 10/1000 | Loss: 0.00002877
Iteration 11/1000 | Loss: 0.00002525
Iteration 12/1000 | Loss: 0.00002288
Iteration 13/1000 | Loss: 0.00002151
Iteration 14/1000 | Loss: 0.00002085
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00001961
Iteration 18/1000 | Loss: 0.00001925
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001880
Iteration 26/1000 | Loss: 0.00001878
Iteration 27/1000 | Loss: 0.00001878
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001876
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001875
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001874
Iteration 43/1000 | Loss: 0.00001874
Iteration 44/1000 | Loss: 0.00001874
Iteration 45/1000 | Loss: 0.00001873
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001873
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001872
Iteration 53/1000 | Loss: 0.00001872
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001871
Iteration 61/1000 | Loss: 0.00001871
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00001871
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001870
Iteration 69/1000 | Loss: 0.00001870
Iteration 70/1000 | Loss: 0.00001870
Iteration 71/1000 | Loss: 0.00001870
Iteration 72/1000 | Loss: 0.00001870
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001869
Iteration 83/1000 | Loss: 0.00001869
Iteration 84/1000 | Loss: 0.00001869
Iteration 85/1000 | Loss: 0.00001869
Iteration 86/1000 | Loss: 0.00001869
Iteration 87/1000 | Loss: 0.00001869
Iteration 88/1000 | Loss: 0.00001869
Iteration 89/1000 | Loss: 0.00001869
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00001868
Iteration 93/1000 | Loss: 0.00001868
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001868
Iteration 96/1000 | Loss: 0.00001868
Iteration 97/1000 | Loss: 0.00001868
Iteration 98/1000 | Loss: 0.00001868
Iteration 99/1000 | Loss: 0.00001868
Iteration 100/1000 | Loss: 0.00001868
Iteration 101/1000 | Loss: 0.00001868
Iteration 102/1000 | Loss: 0.00001868
Iteration 103/1000 | Loss: 0.00001868
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001867
Iteration 107/1000 | Loss: 0.00001867
Iteration 108/1000 | Loss: 0.00001867
Iteration 109/1000 | Loss: 0.00001867
Iteration 110/1000 | Loss: 0.00001867
Iteration 111/1000 | Loss: 0.00001867
Iteration 112/1000 | Loss: 0.00001867
Iteration 113/1000 | Loss: 0.00001867
Iteration 114/1000 | Loss: 0.00001867
Iteration 115/1000 | Loss: 0.00001867
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001867
Iteration 118/1000 | Loss: 0.00001867
Iteration 119/1000 | Loss: 0.00001867
Iteration 120/1000 | Loss: 0.00001867
Iteration 121/1000 | Loss: 0.00001867
Iteration 122/1000 | Loss: 0.00001867
Iteration 123/1000 | Loss: 0.00001867
Iteration 124/1000 | Loss: 0.00001867
Iteration 125/1000 | Loss: 0.00001867
Iteration 126/1000 | Loss: 0.00001867
Iteration 127/1000 | Loss: 0.00001867
Iteration 128/1000 | Loss: 0.00001867
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001867
Iteration 132/1000 | Loss: 0.00001867
Iteration 133/1000 | Loss: 0.00001867
Iteration 134/1000 | Loss: 0.00001867
Iteration 135/1000 | Loss: 0.00001867
Iteration 136/1000 | Loss: 0.00001867
Iteration 137/1000 | Loss: 0.00001867
Iteration 138/1000 | Loss: 0.00001867
Iteration 139/1000 | Loss: 0.00001867
Iteration 140/1000 | Loss: 0.00001867
Iteration 141/1000 | Loss: 0.00001867
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8667109543457627e-05, 1.8667109543457627e-05, 1.8667109543457627e-05, 1.8667109543457627e-05, 1.8667109543457627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8667109543457627e-05

Optimization complete. Final v2v error: 3.6310319900512695 mm

Highest mean error: 4.314205646514893 mm for frame 94

Lowest mean error: 2.8507866859436035 mm for frame 139

Saving results

Total time: 81.05029797554016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385421
Iteration 2/25 | Loss: 0.00093896
Iteration 3/25 | Loss: 0.00077579
Iteration 4/25 | Loss: 0.00075223
Iteration 5/25 | Loss: 0.00074631
Iteration 6/25 | Loss: 0.00074423
Iteration 7/25 | Loss: 0.00074360
Iteration 8/25 | Loss: 0.00074344
Iteration 9/25 | Loss: 0.00074343
Iteration 10/25 | Loss: 0.00074343
Iteration 11/25 | Loss: 0.00074343
Iteration 12/25 | Loss: 0.00074343
Iteration 13/25 | Loss: 0.00074343
Iteration 14/25 | Loss: 0.00074343
Iteration 15/25 | Loss: 0.00074343
Iteration 16/25 | Loss: 0.00074343
Iteration 17/25 | Loss: 0.00074343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007434281869791448, 0.0007434281869791448, 0.0007434281869791448, 0.0007434281869791448, 0.0007434281869791448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007434281869791448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71455371
Iteration 2/25 | Loss: 0.00134941
Iteration 3/25 | Loss: 0.00134941
Iteration 4/25 | Loss: 0.00134941
Iteration 5/25 | Loss: 0.00134941
Iteration 6/25 | Loss: 0.00134941
Iteration 7/25 | Loss: 0.00134941
Iteration 8/25 | Loss: 0.00134941
Iteration 9/25 | Loss: 0.00134941
Iteration 10/25 | Loss: 0.00134941
Iteration 11/25 | Loss: 0.00134941
Iteration 12/25 | Loss: 0.00134941
Iteration 13/25 | Loss: 0.00134941
Iteration 14/25 | Loss: 0.00134941
Iteration 15/25 | Loss: 0.00134941
Iteration 16/25 | Loss: 0.00134941
Iteration 17/25 | Loss: 0.00134941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013494108570739627, 0.0013494108570739627, 0.0013494108570739627, 0.0013494108570739627, 0.0013494108570739627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013494108570739627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134941
Iteration 2/1000 | Loss: 0.00002860
Iteration 3/1000 | Loss: 0.00001774
Iteration 4/1000 | Loss: 0.00001519
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001229
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001210
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001195
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001194
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001192
Iteration 24/1000 | Loss: 0.00001191
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001187
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001185
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001183
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001181
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001164
Iteration 121/1000 | Loss: 0.00001164
Iteration 122/1000 | Loss: 0.00001164
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001164
Iteration 128/1000 | Loss: 0.00001164
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001163
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001163
Iteration 137/1000 | Loss: 0.00001163
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001163
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001163
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001163
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001163
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.1626433661149349e-05, 1.1626433661149349e-05, 1.1626433661149349e-05, 1.1626433661149349e-05, 1.1626433661149349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1626433661149349e-05

Optimization complete. Final v2v error: 2.848212957382202 mm

Highest mean error: 3.8951351642608643 mm for frame 55

Lowest mean error: 2.509932279586792 mm for frame 34

Saving results

Total time: 39.904733657836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068381
Iteration 2/25 | Loss: 0.00185166
Iteration 3/25 | Loss: 0.00118273
Iteration 4/25 | Loss: 0.00144852
Iteration 5/25 | Loss: 0.00097264
Iteration 6/25 | Loss: 0.00098121
Iteration 7/25 | Loss: 0.00103347
Iteration 8/25 | Loss: 0.00100128
Iteration 9/25 | Loss: 0.00098273
Iteration 10/25 | Loss: 0.00091794
Iteration 11/25 | Loss: 0.00085553
Iteration 12/25 | Loss: 0.00084409
Iteration 13/25 | Loss: 0.00083105
Iteration 14/25 | Loss: 0.00083392
Iteration 15/25 | Loss: 0.00083313
Iteration 16/25 | Loss: 0.00082256
Iteration 17/25 | Loss: 0.00082093
Iteration 18/25 | Loss: 0.00081818
Iteration 19/25 | Loss: 0.00081756
Iteration 20/25 | Loss: 0.00081923
Iteration 21/25 | Loss: 0.00081377
Iteration 22/25 | Loss: 0.00081154
Iteration 23/25 | Loss: 0.00081501
Iteration 24/25 | Loss: 0.00082788
Iteration 25/25 | Loss: 0.00081853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65703475
Iteration 2/25 | Loss: 0.00192710
Iteration 3/25 | Loss: 0.00192710
Iteration 4/25 | Loss: 0.00192710
Iteration 5/25 | Loss: 0.00192710
Iteration 6/25 | Loss: 0.00192710
Iteration 7/25 | Loss: 0.00192710
Iteration 8/25 | Loss: 0.00192710
Iteration 9/25 | Loss: 0.00192710
Iteration 10/25 | Loss: 0.00192710
Iteration 11/25 | Loss: 0.00192710
Iteration 12/25 | Loss: 0.00192710
Iteration 13/25 | Loss: 0.00192710
Iteration 14/25 | Loss: 0.00192710
Iteration 15/25 | Loss: 0.00192710
Iteration 16/25 | Loss: 0.00192710
Iteration 17/25 | Loss: 0.00192710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001927101518958807, 0.001927101518958807, 0.001927101518958807, 0.001927101518958807, 0.001927101518958807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001927101518958807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192710
Iteration 2/1000 | Loss: 0.00094870
Iteration 3/1000 | Loss: 0.00041674
Iteration 4/1000 | Loss: 0.00043541
Iteration 5/1000 | Loss: 0.00047627
Iteration 6/1000 | Loss: 0.00060632
Iteration 7/1000 | Loss: 0.00057161
Iteration 8/1000 | Loss: 0.00037770
Iteration 9/1000 | Loss: 0.00032804
Iteration 10/1000 | Loss: 0.00047647
Iteration 11/1000 | Loss: 0.00048915
Iteration 12/1000 | Loss: 0.00032647
Iteration 13/1000 | Loss: 0.00097258
Iteration 14/1000 | Loss: 0.00102280
Iteration 15/1000 | Loss: 0.00138175
Iteration 16/1000 | Loss: 0.00058633
Iteration 17/1000 | Loss: 0.00066219
Iteration 18/1000 | Loss: 0.00064421
Iteration 19/1000 | Loss: 0.00085950
Iteration 20/1000 | Loss: 0.00079684
Iteration 21/1000 | Loss: 0.00047199
Iteration 22/1000 | Loss: 0.00070739
Iteration 23/1000 | Loss: 0.00078238
Iteration 24/1000 | Loss: 0.00059808
Iteration 25/1000 | Loss: 0.00060366
Iteration 26/1000 | Loss: 0.00045067
Iteration 27/1000 | Loss: 0.00037739
Iteration 28/1000 | Loss: 0.00058116
Iteration 29/1000 | Loss: 0.00055450
Iteration 30/1000 | Loss: 0.00037365
Iteration 31/1000 | Loss: 0.00055592
Iteration 32/1000 | Loss: 0.00099586
Iteration 33/1000 | Loss: 0.00064391
Iteration 34/1000 | Loss: 0.00034765
Iteration 35/1000 | Loss: 0.00021060
Iteration 36/1000 | Loss: 0.00020572
Iteration 37/1000 | Loss: 0.00064832
Iteration 38/1000 | Loss: 0.00079346
Iteration 39/1000 | Loss: 0.00077581
Iteration 40/1000 | Loss: 0.00053150
Iteration 41/1000 | Loss: 0.00052611
Iteration 42/1000 | Loss: 0.00090810
Iteration 43/1000 | Loss: 0.00130522
Iteration 44/1000 | Loss: 0.00171421
Iteration 45/1000 | Loss: 0.00080123
Iteration 46/1000 | Loss: 0.00051634
Iteration 47/1000 | Loss: 0.00100323
Iteration 48/1000 | Loss: 0.00054421
Iteration 49/1000 | Loss: 0.00140621
Iteration 50/1000 | Loss: 0.00074519
Iteration 51/1000 | Loss: 0.00164233
Iteration 52/1000 | Loss: 0.00116529
Iteration 53/1000 | Loss: 0.00119545
Iteration 54/1000 | Loss: 0.00140789
Iteration 55/1000 | Loss: 0.00108236
Iteration 56/1000 | Loss: 0.00132368
Iteration 57/1000 | Loss: 0.00069411
Iteration 58/1000 | Loss: 0.00142508
Iteration 59/1000 | Loss: 0.00106340
Iteration 60/1000 | Loss: 0.00039979
Iteration 61/1000 | Loss: 0.00047750
Iteration 62/1000 | Loss: 0.00044551
Iteration 63/1000 | Loss: 0.00041108
Iteration 64/1000 | Loss: 0.00038046
Iteration 65/1000 | Loss: 0.00048051
Iteration 66/1000 | Loss: 0.00042709
Iteration 67/1000 | Loss: 0.00050801
Iteration 68/1000 | Loss: 0.00049054
Iteration 69/1000 | Loss: 0.00045278
Iteration 70/1000 | Loss: 0.00035596
Iteration 71/1000 | Loss: 0.00078419
Iteration 72/1000 | Loss: 0.00037760
Iteration 73/1000 | Loss: 0.00052385
Iteration 74/1000 | Loss: 0.00034928
Iteration 75/1000 | Loss: 0.00042587
Iteration 76/1000 | Loss: 0.00057212
Iteration 77/1000 | Loss: 0.00074555
Iteration 78/1000 | Loss: 0.00115670
Iteration 79/1000 | Loss: 0.00119669
Iteration 80/1000 | Loss: 0.00094828
Iteration 81/1000 | Loss: 0.00057526
Iteration 82/1000 | Loss: 0.00120223
Iteration 83/1000 | Loss: 0.00056253
Iteration 84/1000 | Loss: 0.00125797
Iteration 85/1000 | Loss: 0.00133774
Iteration 86/1000 | Loss: 0.00167497
Iteration 87/1000 | Loss: 0.00035641
Iteration 88/1000 | Loss: 0.00094866
Iteration 89/1000 | Loss: 0.00103315
Iteration 90/1000 | Loss: 0.00096481
Iteration 91/1000 | Loss: 0.00036844
Iteration 92/1000 | Loss: 0.00050306
Iteration 93/1000 | Loss: 0.00023461
Iteration 94/1000 | Loss: 0.00044464
Iteration 95/1000 | Loss: 0.00065926
Iteration 96/1000 | Loss: 0.00067368
Iteration 97/1000 | Loss: 0.00077707
Iteration 98/1000 | Loss: 0.00029340
Iteration 99/1000 | Loss: 0.00100245
Iteration 100/1000 | Loss: 0.00101677
Iteration 101/1000 | Loss: 0.00023231
Iteration 102/1000 | Loss: 0.00017750
Iteration 103/1000 | Loss: 0.00032271
Iteration 104/1000 | Loss: 0.00032069
Iteration 105/1000 | Loss: 0.00025775
Iteration 106/1000 | Loss: 0.00031566
Iteration 107/1000 | Loss: 0.00070782
Iteration 108/1000 | Loss: 0.00142252
Iteration 109/1000 | Loss: 0.00082525
Iteration 110/1000 | Loss: 0.00110782
Iteration 111/1000 | Loss: 0.00069220
Iteration 112/1000 | Loss: 0.00112963
Iteration 113/1000 | Loss: 0.00106851
Iteration 114/1000 | Loss: 0.00070048
Iteration 115/1000 | Loss: 0.00066631
Iteration 116/1000 | Loss: 0.00135831
Iteration 117/1000 | Loss: 0.00030342
Iteration 118/1000 | Loss: 0.00030084
Iteration 119/1000 | Loss: 0.00044330
Iteration 120/1000 | Loss: 0.00017979
Iteration 121/1000 | Loss: 0.00017254
Iteration 122/1000 | Loss: 0.00016422
Iteration 123/1000 | Loss: 0.00041975
Iteration 124/1000 | Loss: 0.00040270
Iteration 125/1000 | Loss: 0.00044280
Iteration 126/1000 | Loss: 0.00043363
Iteration 127/1000 | Loss: 0.00046673
Iteration 128/1000 | Loss: 0.00073721
Iteration 129/1000 | Loss: 0.00027569
Iteration 130/1000 | Loss: 0.00016065
Iteration 131/1000 | Loss: 0.00016553
Iteration 132/1000 | Loss: 0.00014293
Iteration 133/1000 | Loss: 0.00014271
Iteration 134/1000 | Loss: 0.00014432
Iteration 135/1000 | Loss: 0.00026287
Iteration 136/1000 | Loss: 0.00010812
Iteration 137/1000 | Loss: 0.00005307
Iteration 138/1000 | Loss: 0.00034179
Iteration 139/1000 | Loss: 0.00004498
Iteration 140/1000 | Loss: 0.00008374
Iteration 141/1000 | Loss: 0.00012779
Iteration 142/1000 | Loss: 0.00003399
Iteration 143/1000 | Loss: 0.00015049
Iteration 144/1000 | Loss: 0.00002752
Iteration 145/1000 | Loss: 0.00002554
Iteration 146/1000 | Loss: 0.00053161
Iteration 147/1000 | Loss: 0.00021791
Iteration 148/1000 | Loss: 0.00005641
Iteration 149/1000 | Loss: 0.00021245
Iteration 150/1000 | Loss: 0.00008017
Iteration 151/1000 | Loss: 0.00012283
Iteration 152/1000 | Loss: 0.00007554
Iteration 153/1000 | Loss: 0.00017984
Iteration 154/1000 | Loss: 0.00016872
Iteration 155/1000 | Loss: 0.00002318
Iteration 156/1000 | Loss: 0.00027303
Iteration 157/1000 | Loss: 0.00032913
Iteration 158/1000 | Loss: 0.00050105
Iteration 159/1000 | Loss: 0.00022938
Iteration 160/1000 | Loss: 0.00017902
Iteration 161/1000 | Loss: 0.00024070
Iteration 162/1000 | Loss: 0.00013101
Iteration 163/1000 | Loss: 0.00055659
Iteration 164/1000 | Loss: 0.00002848
Iteration 165/1000 | Loss: 0.00013933
Iteration 166/1000 | Loss: 0.00013250
Iteration 167/1000 | Loss: 0.00003741
Iteration 168/1000 | Loss: 0.00002180
Iteration 169/1000 | Loss: 0.00096395
Iteration 170/1000 | Loss: 0.00003310
Iteration 171/1000 | Loss: 0.00011162
Iteration 172/1000 | Loss: 0.00001941
Iteration 173/1000 | Loss: 0.00001894
Iteration 174/1000 | Loss: 0.00001750
Iteration 175/1000 | Loss: 0.00020829
Iteration 176/1000 | Loss: 0.00011084
Iteration 177/1000 | Loss: 0.00091048
Iteration 178/1000 | Loss: 0.00002523
Iteration 179/1000 | Loss: 0.00002247
Iteration 180/1000 | Loss: 0.00002100
Iteration 181/1000 | Loss: 0.00019146
Iteration 182/1000 | Loss: 0.00002452
Iteration 183/1000 | Loss: 0.00003334
Iteration 184/1000 | Loss: 0.00018696
Iteration 185/1000 | Loss: 0.00006214
Iteration 186/1000 | Loss: 0.00009198
Iteration 187/1000 | Loss: 0.00001954
Iteration 188/1000 | Loss: 0.00008599
Iteration 189/1000 | Loss: 0.00010860
Iteration 190/1000 | Loss: 0.00007973
Iteration 191/1000 | Loss: 0.00027683
Iteration 192/1000 | Loss: 0.00004661
Iteration 193/1000 | Loss: 0.00002474
Iteration 194/1000 | Loss: 0.00001863
Iteration 195/1000 | Loss: 0.00001771
Iteration 196/1000 | Loss: 0.00016026
Iteration 197/1000 | Loss: 0.00002286
Iteration 198/1000 | Loss: 0.00001896
Iteration 199/1000 | Loss: 0.00012961
Iteration 200/1000 | Loss: 0.00001530
Iteration 201/1000 | Loss: 0.00001445
Iteration 202/1000 | Loss: 0.00001380
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001305
Iteration 205/1000 | Loss: 0.00001742
Iteration 206/1000 | Loss: 0.00014275
Iteration 207/1000 | Loss: 0.00001327
Iteration 208/1000 | Loss: 0.00001254
Iteration 209/1000 | Loss: 0.00001227
Iteration 210/1000 | Loss: 0.00001212
Iteration 211/1000 | Loss: 0.00001212
Iteration 212/1000 | Loss: 0.00001211
Iteration 213/1000 | Loss: 0.00001199
Iteration 214/1000 | Loss: 0.00001197
Iteration 215/1000 | Loss: 0.00001196
Iteration 216/1000 | Loss: 0.00001196
Iteration 217/1000 | Loss: 0.00001195
Iteration 218/1000 | Loss: 0.00001194
Iteration 219/1000 | Loss: 0.00001194
Iteration 220/1000 | Loss: 0.00001193
Iteration 221/1000 | Loss: 0.00001189
Iteration 222/1000 | Loss: 0.00001186
Iteration 223/1000 | Loss: 0.00001186
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001184
Iteration 226/1000 | Loss: 0.00001184
Iteration 227/1000 | Loss: 0.00001183
Iteration 228/1000 | Loss: 0.00001183
Iteration 229/1000 | Loss: 0.00001183
Iteration 230/1000 | Loss: 0.00001183
Iteration 231/1000 | Loss: 0.00001183
Iteration 232/1000 | Loss: 0.00001183
Iteration 233/1000 | Loss: 0.00001182
Iteration 234/1000 | Loss: 0.00001182
Iteration 235/1000 | Loss: 0.00001182
Iteration 236/1000 | Loss: 0.00001182
Iteration 237/1000 | Loss: 0.00001181
Iteration 238/1000 | Loss: 0.00001181
Iteration 239/1000 | Loss: 0.00001181
Iteration 240/1000 | Loss: 0.00001181
Iteration 241/1000 | Loss: 0.00001181
Iteration 242/1000 | Loss: 0.00001181
Iteration 243/1000 | Loss: 0.00001180
Iteration 244/1000 | Loss: 0.00001180
Iteration 245/1000 | Loss: 0.00001180
Iteration 246/1000 | Loss: 0.00001180
Iteration 247/1000 | Loss: 0.00001180
Iteration 248/1000 | Loss: 0.00001180
Iteration 249/1000 | Loss: 0.00001180
Iteration 250/1000 | Loss: 0.00001180
Iteration 251/1000 | Loss: 0.00001180
Iteration 252/1000 | Loss: 0.00001180
Iteration 253/1000 | Loss: 0.00001180
Iteration 254/1000 | Loss: 0.00001180
Iteration 255/1000 | Loss: 0.00001180
Iteration 256/1000 | Loss: 0.00001180
Iteration 257/1000 | Loss: 0.00001180
Iteration 258/1000 | Loss: 0.00001179
Iteration 259/1000 | Loss: 0.00001179
Iteration 260/1000 | Loss: 0.00001179
Iteration 261/1000 | Loss: 0.00001179
Iteration 262/1000 | Loss: 0.00001179
Iteration 263/1000 | Loss: 0.00001179
Iteration 264/1000 | Loss: 0.00001179
Iteration 265/1000 | Loss: 0.00001179
Iteration 266/1000 | Loss: 0.00001179
Iteration 267/1000 | Loss: 0.00001178
Iteration 268/1000 | Loss: 0.00001178
Iteration 269/1000 | Loss: 0.00001178
Iteration 270/1000 | Loss: 0.00001178
Iteration 271/1000 | Loss: 0.00001178
Iteration 272/1000 | Loss: 0.00001178
Iteration 273/1000 | Loss: 0.00001178
Iteration 274/1000 | Loss: 0.00001178
Iteration 275/1000 | Loss: 0.00001177
Iteration 276/1000 | Loss: 0.00001177
Iteration 277/1000 | Loss: 0.00001177
Iteration 278/1000 | Loss: 0.00001177
Iteration 279/1000 | Loss: 0.00001177
Iteration 280/1000 | Loss: 0.00001177
Iteration 281/1000 | Loss: 0.00001177
Iteration 282/1000 | Loss: 0.00001177
Iteration 283/1000 | Loss: 0.00001177
Iteration 284/1000 | Loss: 0.00001177
Iteration 285/1000 | Loss: 0.00001176
Iteration 286/1000 | Loss: 0.00001176
Iteration 287/1000 | Loss: 0.00001176
Iteration 288/1000 | Loss: 0.00001176
Iteration 289/1000 | Loss: 0.00001176
Iteration 290/1000 | Loss: 0.00001176
Iteration 291/1000 | Loss: 0.00001176
Iteration 292/1000 | Loss: 0.00001176
Iteration 293/1000 | Loss: 0.00001176
Iteration 294/1000 | Loss: 0.00001176
Iteration 295/1000 | Loss: 0.00001176
Iteration 296/1000 | Loss: 0.00001176
Iteration 297/1000 | Loss: 0.00001176
Iteration 298/1000 | Loss: 0.00001176
Iteration 299/1000 | Loss: 0.00001176
Iteration 300/1000 | Loss: 0.00001175
Iteration 301/1000 | Loss: 0.00001175
Iteration 302/1000 | Loss: 0.00001175
Iteration 303/1000 | Loss: 0.00001175
Iteration 304/1000 | Loss: 0.00001175
Iteration 305/1000 | Loss: 0.00001175
Iteration 306/1000 | Loss: 0.00001175
Iteration 307/1000 | Loss: 0.00001175
Iteration 308/1000 | Loss: 0.00001175
Iteration 309/1000 | Loss: 0.00001175
Iteration 310/1000 | Loss: 0.00001175
Iteration 311/1000 | Loss: 0.00001175
Iteration 312/1000 | Loss: 0.00001175
Iteration 313/1000 | Loss: 0.00001175
Iteration 314/1000 | Loss: 0.00001175
Iteration 315/1000 | Loss: 0.00001175
Iteration 316/1000 | Loss: 0.00001175
Iteration 317/1000 | Loss: 0.00001175
Iteration 318/1000 | Loss: 0.00001175
Iteration 319/1000 | Loss: 0.00001175
Iteration 320/1000 | Loss: 0.00001175
Iteration 321/1000 | Loss: 0.00001175
Iteration 322/1000 | Loss: 0.00001175
Iteration 323/1000 | Loss: 0.00001175
Iteration 324/1000 | Loss: 0.00001175
Iteration 325/1000 | Loss: 0.00001175
Iteration 326/1000 | Loss: 0.00001175
Iteration 327/1000 | Loss: 0.00001175
Iteration 328/1000 | Loss: 0.00001175
Iteration 329/1000 | Loss: 0.00001175
Iteration 330/1000 | Loss: 0.00001175
Iteration 331/1000 | Loss: 0.00001175
Iteration 332/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [1.1753705621231347e-05, 1.1753705621231347e-05, 1.1753705621231347e-05, 1.1753705621231347e-05, 1.1753705621231347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1753705621231347e-05

Optimization complete. Final v2v error: 2.9139859676361084 mm

Highest mean error: 4.140487194061279 mm for frame 72

Lowest mean error: 2.585449695587158 mm for frame 10

Saving results

Total time: 345.20701265335083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993892
Iteration 2/25 | Loss: 0.00360381
Iteration 3/25 | Loss: 0.00244257
Iteration 4/25 | Loss: 0.00160616
Iteration 5/25 | Loss: 0.00145507
Iteration 6/25 | Loss: 0.00140547
Iteration 7/25 | Loss: 0.00137762
Iteration 8/25 | Loss: 0.00132966
Iteration 9/25 | Loss: 0.00130175
Iteration 10/25 | Loss: 0.00129097
Iteration 11/25 | Loss: 0.00128521
Iteration 12/25 | Loss: 0.00127783
Iteration 13/25 | Loss: 0.00125931
Iteration 14/25 | Loss: 0.00125189
Iteration 15/25 | Loss: 0.00124836
Iteration 16/25 | Loss: 0.00124682
Iteration 17/25 | Loss: 0.00124302
Iteration 18/25 | Loss: 0.00124503
Iteration 19/25 | Loss: 0.00123477
Iteration 20/25 | Loss: 0.00122923
Iteration 21/25 | Loss: 0.00123092
Iteration 22/25 | Loss: 0.00122961
Iteration 23/25 | Loss: 0.00123820
Iteration 24/25 | Loss: 0.00123829
Iteration 25/25 | Loss: 0.00122283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.57210684
Iteration 2/25 | Loss: 0.00499927
Iteration 3/25 | Loss: 0.00476611
Iteration 4/25 | Loss: 0.00476611
Iteration 5/25 | Loss: 0.00476611
Iteration 6/25 | Loss: 0.00476610
Iteration 7/25 | Loss: 0.00476610
Iteration 8/25 | Loss: 0.00476610
Iteration 9/25 | Loss: 0.00476610
Iteration 10/25 | Loss: 0.00476610
Iteration 11/25 | Loss: 0.00476610
Iteration 12/25 | Loss: 0.00476610
Iteration 13/25 | Loss: 0.00476610
Iteration 14/25 | Loss: 0.00476610
Iteration 15/25 | Loss: 0.00476610
Iteration 16/25 | Loss: 0.00476610
Iteration 17/25 | Loss: 0.00476610
Iteration 18/25 | Loss: 0.00476610
Iteration 19/25 | Loss: 0.00476610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004766102880239487, 0.004766102880239487, 0.004766102880239487, 0.004766102880239487, 0.004766102880239487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004766102880239487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00476610
Iteration 2/1000 | Loss: 0.00257669
Iteration 3/1000 | Loss: 0.00155861
Iteration 4/1000 | Loss: 0.00184782
Iteration 5/1000 | Loss: 0.00122411
Iteration 6/1000 | Loss: 0.00085953
Iteration 7/1000 | Loss: 0.00137772
Iteration 8/1000 | Loss: 0.00116471
Iteration 9/1000 | Loss: 0.00207710
Iteration 10/1000 | Loss: 0.00183841
Iteration 11/1000 | Loss: 0.00091707
Iteration 12/1000 | Loss: 0.00029202
Iteration 13/1000 | Loss: 0.00444784
Iteration 14/1000 | Loss: 0.00196683
Iteration 15/1000 | Loss: 0.00121703
Iteration 16/1000 | Loss: 0.00229895
Iteration 17/1000 | Loss: 0.00362631
Iteration 18/1000 | Loss: 0.00281680
Iteration 19/1000 | Loss: 0.00136669
Iteration 20/1000 | Loss: 0.00096464
Iteration 21/1000 | Loss: 0.00077719
Iteration 22/1000 | Loss: 0.00079432
Iteration 23/1000 | Loss: 0.00051357
Iteration 24/1000 | Loss: 0.00083247
Iteration 25/1000 | Loss: 0.00073043
Iteration 26/1000 | Loss: 0.00062641
Iteration 27/1000 | Loss: 0.00047232
Iteration 28/1000 | Loss: 0.00053990
Iteration 29/1000 | Loss: 0.00112777
Iteration 30/1000 | Loss: 0.00200326
Iteration 31/1000 | Loss: 0.00162770
Iteration 32/1000 | Loss: 0.00062521
Iteration 33/1000 | Loss: 0.00139421
Iteration 34/1000 | Loss: 0.00101069
Iteration 35/1000 | Loss: 0.00073178
Iteration 36/1000 | Loss: 0.00030069
Iteration 37/1000 | Loss: 0.00030406
Iteration 38/1000 | Loss: 0.00028355
Iteration 39/1000 | Loss: 0.00025777
Iteration 40/1000 | Loss: 0.00028252
Iteration 41/1000 | Loss: 0.00028586
Iteration 42/1000 | Loss: 0.00109454
Iteration 43/1000 | Loss: 0.00029117
Iteration 44/1000 | Loss: 0.00023798
Iteration 45/1000 | Loss: 0.00012802
Iteration 46/1000 | Loss: 0.00106662
Iteration 47/1000 | Loss: 0.00287374
Iteration 48/1000 | Loss: 0.00388936
Iteration 49/1000 | Loss: 0.00053482
Iteration 50/1000 | Loss: 0.00054174
Iteration 51/1000 | Loss: 0.00024576
Iteration 52/1000 | Loss: 0.00012495
Iteration 53/1000 | Loss: 0.00199783
Iteration 54/1000 | Loss: 0.00274805
Iteration 55/1000 | Loss: 0.00030640
Iteration 56/1000 | Loss: 0.00038770
Iteration 57/1000 | Loss: 0.00026536
Iteration 58/1000 | Loss: 0.00061829
Iteration 59/1000 | Loss: 0.00155077
Iteration 60/1000 | Loss: 0.00106721
Iteration 61/1000 | Loss: 0.00074618
Iteration 62/1000 | Loss: 0.00041368
Iteration 63/1000 | Loss: 0.00067309
Iteration 64/1000 | Loss: 0.00026380
Iteration 65/1000 | Loss: 0.00034439
Iteration 66/1000 | Loss: 0.00069399
Iteration 67/1000 | Loss: 0.00015604
Iteration 68/1000 | Loss: 0.00009407
Iteration 69/1000 | Loss: 0.00090057
Iteration 70/1000 | Loss: 0.00016140
Iteration 71/1000 | Loss: 0.00142920
Iteration 72/1000 | Loss: 0.00027888
Iteration 73/1000 | Loss: 0.00092546
Iteration 74/1000 | Loss: 0.00096881
Iteration 75/1000 | Loss: 0.00094218
Iteration 76/1000 | Loss: 0.00091528
Iteration 77/1000 | Loss: 0.00026342
Iteration 78/1000 | Loss: 0.00007867
Iteration 79/1000 | Loss: 0.00132607
Iteration 80/1000 | Loss: 0.00145000
Iteration 81/1000 | Loss: 0.00026665
Iteration 82/1000 | Loss: 0.00043515
Iteration 83/1000 | Loss: 0.00010661
Iteration 84/1000 | Loss: 0.00007461
Iteration 85/1000 | Loss: 0.00056879
Iteration 86/1000 | Loss: 0.00006658
Iteration 87/1000 | Loss: 0.00052076
Iteration 88/1000 | Loss: 0.00006226
Iteration 89/1000 | Loss: 0.00052556
Iteration 90/1000 | Loss: 0.00050451
Iteration 91/1000 | Loss: 0.00083096
Iteration 92/1000 | Loss: 0.00138037
Iteration 93/1000 | Loss: 0.00007153
Iteration 94/1000 | Loss: 0.00005393
Iteration 95/1000 | Loss: 0.00004752
Iteration 96/1000 | Loss: 0.00004378
Iteration 97/1000 | Loss: 0.00063407
Iteration 98/1000 | Loss: 0.00019746
Iteration 99/1000 | Loss: 0.00004228
Iteration 100/1000 | Loss: 0.00003964
Iteration 101/1000 | Loss: 0.00003788
Iteration 102/1000 | Loss: 0.00003601
Iteration 103/1000 | Loss: 0.00003510
Iteration 104/1000 | Loss: 0.00003442
Iteration 105/1000 | Loss: 0.00003383
Iteration 106/1000 | Loss: 0.00063340
Iteration 107/1000 | Loss: 0.00004692
Iteration 108/1000 | Loss: 0.00003825
Iteration 109/1000 | Loss: 0.00003563
Iteration 110/1000 | Loss: 0.00003409
Iteration 111/1000 | Loss: 0.00003334
Iteration 112/1000 | Loss: 0.00003248
Iteration 113/1000 | Loss: 0.00027832
Iteration 114/1000 | Loss: 0.00012423
Iteration 115/1000 | Loss: 0.00026587
Iteration 116/1000 | Loss: 0.00012004
Iteration 117/1000 | Loss: 0.00025398
Iteration 118/1000 | Loss: 0.00008601
Iteration 119/1000 | Loss: 0.00006118
Iteration 120/1000 | Loss: 0.00074314
Iteration 121/1000 | Loss: 0.00088470
Iteration 122/1000 | Loss: 0.00005434
Iteration 123/1000 | Loss: 0.00004189
Iteration 124/1000 | Loss: 0.00003593
Iteration 125/1000 | Loss: 0.00004459
Iteration 126/1000 | Loss: 0.00003944
Iteration 127/1000 | Loss: 0.00003612
Iteration 128/1000 | Loss: 0.00004662
Iteration 129/1000 | Loss: 0.00004162
Iteration 130/1000 | Loss: 0.00003781
Iteration 131/1000 | Loss: 0.00004084
Iteration 132/1000 | Loss: 0.00004153
Iteration 133/1000 | Loss: 0.00003010
Iteration 134/1000 | Loss: 0.00002696
Iteration 135/1000 | Loss: 0.00002582
Iteration 136/1000 | Loss: 0.00002497
Iteration 137/1000 | Loss: 0.00002435
Iteration 138/1000 | Loss: 0.00002411
Iteration 139/1000 | Loss: 0.00002397
Iteration 140/1000 | Loss: 0.00002385
Iteration 141/1000 | Loss: 0.00002384
Iteration 142/1000 | Loss: 0.00002381
Iteration 143/1000 | Loss: 0.00002381
Iteration 144/1000 | Loss: 0.00002380
Iteration 145/1000 | Loss: 0.00002380
Iteration 146/1000 | Loss: 0.00002378
Iteration 147/1000 | Loss: 0.00002377
Iteration 148/1000 | Loss: 0.00002377
Iteration 149/1000 | Loss: 0.00002376
Iteration 150/1000 | Loss: 0.00002372
Iteration 151/1000 | Loss: 0.00002372
Iteration 152/1000 | Loss: 0.00002359
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002350
Iteration 157/1000 | Loss: 0.00002350
Iteration 158/1000 | Loss: 0.00002345
Iteration 159/1000 | Loss: 0.00002345
Iteration 160/1000 | Loss: 0.00002343
Iteration 161/1000 | Loss: 0.00002343
Iteration 162/1000 | Loss: 0.00002343
Iteration 163/1000 | Loss: 0.00002342
Iteration 164/1000 | Loss: 0.00002342
Iteration 165/1000 | Loss: 0.00002342
Iteration 166/1000 | Loss: 0.00002342
Iteration 167/1000 | Loss: 0.00002342
Iteration 168/1000 | Loss: 0.00002341
Iteration 169/1000 | Loss: 0.00002341
Iteration 170/1000 | Loss: 0.00002341
Iteration 171/1000 | Loss: 0.00002341
Iteration 172/1000 | Loss: 0.00002341
Iteration 173/1000 | Loss: 0.00002341
Iteration 174/1000 | Loss: 0.00002341
Iteration 175/1000 | Loss: 0.00002341
Iteration 176/1000 | Loss: 0.00002341
Iteration 177/1000 | Loss: 0.00002341
Iteration 178/1000 | Loss: 0.00002340
Iteration 179/1000 | Loss: 0.00002340
Iteration 180/1000 | Loss: 0.00002340
Iteration 181/1000 | Loss: 0.00002340
Iteration 182/1000 | Loss: 0.00002340
Iteration 183/1000 | Loss: 0.00002340
Iteration 184/1000 | Loss: 0.00002339
Iteration 185/1000 | Loss: 0.00002339
Iteration 186/1000 | Loss: 0.00002339
Iteration 187/1000 | Loss: 0.00002339
Iteration 188/1000 | Loss: 0.00002338
Iteration 189/1000 | Loss: 0.00002337
Iteration 190/1000 | Loss: 0.00002337
Iteration 191/1000 | Loss: 0.00002336
Iteration 192/1000 | Loss: 0.00002336
Iteration 193/1000 | Loss: 0.00002336
Iteration 194/1000 | Loss: 0.00002335
Iteration 195/1000 | Loss: 0.00002335
Iteration 196/1000 | Loss: 0.00002335
Iteration 197/1000 | Loss: 0.00002335
Iteration 198/1000 | Loss: 0.00002335
Iteration 199/1000 | Loss: 0.00002335
Iteration 200/1000 | Loss: 0.00002335
Iteration 201/1000 | Loss: 0.00002334
Iteration 202/1000 | Loss: 0.00002334
Iteration 203/1000 | Loss: 0.00002334
Iteration 204/1000 | Loss: 0.00002334
Iteration 205/1000 | Loss: 0.00002334
Iteration 206/1000 | Loss: 0.00002334
Iteration 207/1000 | Loss: 0.00002334
Iteration 208/1000 | Loss: 0.00002334
Iteration 209/1000 | Loss: 0.00002334
Iteration 210/1000 | Loss: 0.00002334
Iteration 211/1000 | Loss: 0.00002334
Iteration 212/1000 | Loss: 0.00002334
Iteration 213/1000 | Loss: 0.00002334
Iteration 214/1000 | Loss: 0.00002334
Iteration 215/1000 | Loss: 0.00002333
Iteration 216/1000 | Loss: 0.00002333
Iteration 217/1000 | Loss: 0.00002333
Iteration 218/1000 | Loss: 0.00002333
Iteration 219/1000 | Loss: 0.00002333
Iteration 220/1000 | Loss: 0.00002333
Iteration 221/1000 | Loss: 0.00002333
Iteration 222/1000 | Loss: 0.00002333
Iteration 223/1000 | Loss: 0.00002333
Iteration 224/1000 | Loss: 0.00002333
Iteration 225/1000 | Loss: 0.00002332
Iteration 226/1000 | Loss: 0.00002332
Iteration 227/1000 | Loss: 0.00002332
Iteration 228/1000 | Loss: 0.00002332
Iteration 229/1000 | Loss: 0.00002332
Iteration 230/1000 | Loss: 0.00002332
Iteration 231/1000 | Loss: 0.00002332
Iteration 232/1000 | Loss: 0.00002332
Iteration 233/1000 | Loss: 0.00002332
Iteration 234/1000 | Loss: 0.00002332
Iteration 235/1000 | Loss: 0.00002332
Iteration 236/1000 | Loss: 0.00002332
Iteration 237/1000 | Loss: 0.00002332
Iteration 238/1000 | Loss: 0.00002332
Iteration 239/1000 | Loss: 0.00002332
Iteration 240/1000 | Loss: 0.00002332
Iteration 241/1000 | Loss: 0.00002331
Iteration 242/1000 | Loss: 0.00002331
Iteration 243/1000 | Loss: 0.00002331
Iteration 244/1000 | Loss: 0.00002331
Iteration 245/1000 | Loss: 0.00002331
Iteration 246/1000 | Loss: 0.00002331
Iteration 247/1000 | Loss: 0.00002331
Iteration 248/1000 | Loss: 0.00002331
Iteration 249/1000 | Loss: 0.00002331
Iteration 250/1000 | Loss: 0.00002331
Iteration 251/1000 | Loss: 0.00002331
Iteration 252/1000 | Loss: 0.00002331
Iteration 253/1000 | Loss: 0.00002331
Iteration 254/1000 | Loss: 0.00002330
Iteration 255/1000 | Loss: 0.00002330
Iteration 256/1000 | Loss: 0.00002330
Iteration 257/1000 | Loss: 0.00002330
Iteration 258/1000 | Loss: 0.00002330
Iteration 259/1000 | Loss: 0.00002330
Iteration 260/1000 | Loss: 0.00002330
Iteration 261/1000 | Loss: 0.00002330
Iteration 262/1000 | Loss: 0.00002330
Iteration 263/1000 | Loss: 0.00002330
Iteration 264/1000 | Loss: 0.00002330
Iteration 265/1000 | Loss: 0.00002330
Iteration 266/1000 | Loss: 0.00002330
Iteration 267/1000 | Loss: 0.00002330
Iteration 268/1000 | Loss: 0.00002330
Iteration 269/1000 | Loss: 0.00002330
Iteration 270/1000 | Loss: 0.00002330
Iteration 271/1000 | Loss: 0.00002330
Iteration 272/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [2.3296786821447313e-05, 2.3296786821447313e-05, 2.3296786821447313e-05, 2.3296786821447313e-05, 2.3296786821447313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3296786821447313e-05

Optimization complete. Final v2v error: 3.849236488342285 mm

Highest mean error: 5.37304162979126 mm for frame 86

Lowest mean error: 2.734076738357544 mm for frame 28

Saving results

Total time: 274.07495188713074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842662
Iteration 2/25 | Loss: 0.00198967
Iteration 3/25 | Loss: 0.00122246
Iteration 4/25 | Loss: 0.00096196
Iteration 5/25 | Loss: 0.00088795
Iteration 6/25 | Loss: 0.00083574
Iteration 7/25 | Loss: 0.00082332
Iteration 8/25 | Loss: 0.00082055
Iteration 9/25 | Loss: 0.00082034
Iteration 10/25 | Loss: 0.00082033
Iteration 11/25 | Loss: 0.00082033
Iteration 12/25 | Loss: 0.00082032
Iteration 13/25 | Loss: 0.00082032
Iteration 14/25 | Loss: 0.00082032
Iteration 15/25 | Loss: 0.00082032
Iteration 16/25 | Loss: 0.00082032
Iteration 17/25 | Loss: 0.00082032
Iteration 18/25 | Loss: 0.00082032
Iteration 19/25 | Loss: 0.00082032
Iteration 20/25 | Loss: 0.00082032
Iteration 21/25 | Loss: 0.00082032
Iteration 22/25 | Loss: 0.00082032
Iteration 23/25 | Loss: 0.00082032
Iteration 24/25 | Loss: 0.00082032
Iteration 25/25 | Loss: 0.00082032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52631867
Iteration 2/25 | Loss: 0.00129905
Iteration 3/25 | Loss: 0.00129903
Iteration 4/25 | Loss: 0.00129903
Iteration 5/25 | Loss: 0.00129903
Iteration 6/25 | Loss: 0.00129903
Iteration 7/25 | Loss: 0.00129903
Iteration 8/25 | Loss: 0.00129903
Iteration 9/25 | Loss: 0.00129903
Iteration 10/25 | Loss: 0.00129903
Iteration 11/25 | Loss: 0.00129903
Iteration 12/25 | Loss: 0.00129903
Iteration 13/25 | Loss: 0.00129903
Iteration 14/25 | Loss: 0.00129903
Iteration 15/25 | Loss: 0.00129903
Iteration 16/25 | Loss: 0.00129903
Iteration 17/25 | Loss: 0.00129903
Iteration 18/25 | Loss: 0.00129903
Iteration 19/25 | Loss: 0.00129903
Iteration 20/25 | Loss: 0.00129903
Iteration 21/25 | Loss: 0.00129903
Iteration 22/25 | Loss: 0.00129903
Iteration 23/25 | Loss: 0.00129903
Iteration 24/25 | Loss: 0.00129903
Iteration 25/25 | Loss: 0.00129903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129903
Iteration 2/1000 | Loss: 0.00002481
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001470
Iteration 12/1000 | Loss: 0.00001465
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001448
Iteration 17/1000 | Loss: 0.00001443
Iteration 18/1000 | Loss: 0.00001439
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001432
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001432
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001430
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001429
Iteration 29/1000 | Loss: 0.00001429
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001428
Iteration 32/1000 | Loss: 0.00001428
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001427
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001424
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001418
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001417
Iteration 66/1000 | Loss: 0.00001417
Iteration 67/1000 | Loss: 0.00001417
Iteration 68/1000 | Loss: 0.00001417
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001417
Iteration 71/1000 | Loss: 0.00001416
Iteration 72/1000 | Loss: 0.00001416
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001415
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001413
Iteration 80/1000 | Loss: 0.00001413
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001410
Iteration 87/1000 | Loss: 0.00001410
Iteration 88/1000 | Loss: 0.00001410
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001409
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001408
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001408
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001408
Iteration 117/1000 | Loss: 0.00001408
Iteration 118/1000 | Loss: 0.00001408
Iteration 119/1000 | Loss: 0.00001408
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001408
Iteration 122/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.407893705618335e-05, 1.407893705618335e-05, 1.407893705618335e-05, 1.407893705618335e-05, 1.407893705618335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.407893705618335e-05

Optimization complete. Final v2v error: 3.170279026031494 mm

Highest mean error: 3.5694572925567627 mm for frame 3

Lowest mean error: 3.011180877685547 mm for frame 66

Saving results

Total time: 39.03349995613098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428336
Iteration 2/25 | Loss: 0.00092019
Iteration 3/25 | Loss: 0.00077299
Iteration 4/25 | Loss: 0.00074989
Iteration 5/25 | Loss: 0.00074365
Iteration 6/25 | Loss: 0.00074189
Iteration 7/25 | Loss: 0.00074162
Iteration 8/25 | Loss: 0.00074162
Iteration 9/25 | Loss: 0.00074162
Iteration 10/25 | Loss: 0.00074162
Iteration 11/25 | Loss: 0.00074162
Iteration 12/25 | Loss: 0.00074162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007416187436319888, 0.0007416187436319888, 0.0007416187436319888, 0.0007416187436319888, 0.0007416187436319888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007416187436319888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57347047
Iteration 2/25 | Loss: 0.00131722
Iteration 3/25 | Loss: 0.00131722
Iteration 4/25 | Loss: 0.00131722
Iteration 5/25 | Loss: 0.00131722
Iteration 6/25 | Loss: 0.00131722
Iteration 7/25 | Loss: 0.00131722
Iteration 8/25 | Loss: 0.00131722
Iteration 9/25 | Loss: 0.00131722
Iteration 10/25 | Loss: 0.00131722
Iteration 11/25 | Loss: 0.00131722
Iteration 12/25 | Loss: 0.00131722
Iteration 13/25 | Loss: 0.00131722
Iteration 14/25 | Loss: 0.00131722
Iteration 15/25 | Loss: 0.00131722
Iteration 16/25 | Loss: 0.00131722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013172207400202751, 0.0013172207400202751, 0.0013172207400202751, 0.0013172207400202751, 0.0013172207400202751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013172207400202751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131722
Iteration 2/1000 | Loss: 0.00002566
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001280
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001217
Iteration 83/1000 | Loss: 0.00001217
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Iteration 143/1000 | Loss: 0.00001210
Iteration 144/1000 | Loss: 0.00001210
Iteration 145/1000 | Loss: 0.00001210
Iteration 146/1000 | Loss: 0.00001209
Iteration 147/1000 | Loss: 0.00001209
Iteration 148/1000 | Loss: 0.00001209
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00001209
Iteration 151/1000 | Loss: 0.00001208
Iteration 152/1000 | Loss: 0.00001208
Iteration 153/1000 | Loss: 0.00001208
Iteration 154/1000 | Loss: 0.00001208
Iteration 155/1000 | Loss: 0.00001208
Iteration 156/1000 | Loss: 0.00001208
Iteration 157/1000 | Loss: 0.00001208
Iteration 158/1000 | Loss: 0.00001207
Iteration 159/1000 | Loss: 0.00001207
Iteration 160/1000 | Loss: 0.00001207
Iteration 161/1000 | Loss: 0.00001207
Iteration 162/1000 | Loss: 0.00001207
Iteration 163/1000 | Loss: 0.00001206
Iteration 164/1000 | Loss: 0.00001206
Iteration 165/1000 | Loss: 0.00001206
Iteration 166/1000 | Loss: 0.00001206
Iteration 167/1000 | Loss: 0.00001206
Iteration 168/1000 | Loss: 0.00001206
Iteration 169/1000 | Loss: 0.00001206
Iteration 170/1000 | Loss: 0.00001206
Iteration 171/1000 | Loss: 0.00001206
Iteration 172/1000 | Loss: 0.00001205
Iteration 173/1000 | Loss: 0.00001205
Iteration 174/1000 | Loss: 0.00001205
Iteration 175/1000 | Loss: 0.00001205
Iteration 176/1000 | Loss: 0.00001205
Iteration 177/1000 | Loss: 0.00001205
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001205
Iteration 180/1000 | Loss: 0.00001205
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001205
Iteration 184/1000 | Loss: 0.00001205
Iteration 185/1000 | Loss: 0.00001205
Iteration 186/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2052228157699574e-05, 1.2052228157699574e-05, 1.2052228157699574e-05, 1.2052228157699574e-05, 1.2052228157699574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2052228157699574e-05

Optimization complete. Final v2v error: 2.9580395221710205 mm

Highest mean error: 3.8234992027282715 mm for frame 58

Lowest mean error: 2.7063024044036865 mm for frame 88

Saving results

Total time: 43.28870964050293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806645
Iteration 2/25 | Loss: 0.00091757
Iteration 3/25 | Loss: 0.00076745
Iteration 4/25 | Loss: 0.00074957
Iteration 5/25 | Loss: 0.00074412
Iteration 6/25 | Loss: 0.00074262
Iteration 7/25 | Loss: 0.00074262
Iteration 8/25 | Loss: 0.00074262
Iteration 9/25 | Loss: 0.00074262
Iteration 10/25 | Loss: 0.00074262
Iteration 11/25 | Loss: 0.00074262
Iteration 12/25 | Loss: 0.00074262
Iteration 13/25 | Loss: 0.00074262
Iteration 14/25 | Loss: 0.00074262
Iteration 15/25 | Loss: 0.00074262
Iteration 16/25 | Loss: 0.00074262
Iteration 17/25 | Loss: 0.00074262
Iteration 18/25 | Loss: 0.00074262
Iteration 19/25 | Loss: 0.00074262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007426167721860111, 0.0007426167721860111, 0.0007426167721860111, 0.0007426167721860111, 0.0007426167721860111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007426167721860111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59699738
Iteration 2/25 | Loss: 0.00123062
Iteration 3/25 | Loss: 0.00123062
Iteration 4/25 | Loss: 0.00123062
Iteration 5/25 | Loss: 0.00123062
Iteration 6/25 | Loss: 0.00123062
Iteration 7/25 | Loss: 0.00123061
Iteration 8/25 | Loss: 0.00123061
Iteration 9/25 | Loss: 0.00123061
Iteration 10/25 | Loss: 0.00123061
Iteration 11/25 | Loss: 0.00123061
Iteration 12/25 | Loss: 0.00123061
Iteration 13/25 | Loss: 0.00123061
Iteration 14/25 | Loss: 0.00123061
Iteration 15/25 | Loss: 0.00123061
Iteration 16/25 | Loss: 0.00123061
Iteration 17/25 | Loss: 0.00123061
Iteration 18/25 | Loss: 0.00123061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001230613561347127, 0.001230613561347127, 0.001230613561347127, 0.001230613561347127, 0.001230613561347127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001230613561347127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123061
Iteration 2/1000 | Loss: 0.00002178
Iteration 3/1000 | Loss: 0.00001382
Iteration 4/1000 | Loss: 0.00001249
Iteration 5/1000 | Loss: 0.00001159
Iteration 6/1000 | Loss: 0.00001121
Iteration 7/1000 | Loss: 0.00001095
Iteration 8/1000 | Loss: 0.00001082
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001073
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001058
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001049
Iteration 23/1000 | Loss: 0.00001049
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001048
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001047
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001046
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001045
Iteration 43/1000 | Loss: 0.00001045
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001044
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001044
Iteration 50/1000 | Loss: 0.00001043
Iteration 51/1000 | Loss: 0.00001043
Iteration 52/1000 | Loss: 0.00001043
Iteration 53/1000 | Loss: 0.00001042
Iteration 54/1000 | Loss: 0.00001042
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001042
Iteration 57/1000 | Loss: 0.00001042
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001041
Iteration 62/1000 | Loss: 0.00001041
Iteration 63/1000 | Loss: 0.00001041
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001041
Iteration 68/1000 | Loss: 0.00001041
Iteration 69/1000 | Loss: 0.00001041
Iteration 70/1000 | Loss: 0.00001041
Iteration 71/1000 | Loss: 0.00001041
Iteration 72/1000 | Loss: 0.00001041
Iteration 73/1000 | Loss: 0.00001041
Iteration 74/1000 | Loss: 0.00001041
Iteration 75/1000 | Loss: 0.00001041
Iteration 76/1000 | Loss: 0.00001041
Iteration 77/1000 | Loss: 0.00001041
Iteration 78/1000 | Loss: 0.00001041
Iteration 79/1000 | Loss: 0.00001041
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001041
Iteration 83/1000 | Loss: 0.00001041
Iteration 84/1000 | Loss: 0.00001041
Iteration 85/1000 | Loss: 0.00001041
Iteration 86/1000 | Loss: 0.00001041
Iteration 87/1000 | Loss: 0.00001041
Iteration 88/1000 | Loss: 0.00001041
Iteration 89/1000 | Loss: 0.00001041
Iteration 90/1000 | Loss: 0.00001041
Iteration 91/1000 | Loss: 0.00001041
Iteration 92/1000 | Loss: 0.00001041
Iteration 93/1000 | Loss: 0.00001041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.041366886056494e-05, 1.041366886056494e-05, 1.041366886056494e-05, 1.041366886056494e-05, 1.041366886056494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.041366886056494e-05

Optimization complete. Final v2v error: 2.7215816974639893 mm

Highest mean error: 2.874448776245117 mm for frame 72

Lowest mean error: 2.5782504081726074 mm for frame 123

Saving results

Total time: 31.860751628875732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00692392
Iteration 2/25 | Loss: 0.00165366
Iteration 3/25 | Loss: 0.00106532
Iteration 4/25 | Loss: 0.00098510
Iteration 5/25 | Loss: 0.00096794
Iteration 6/25 | Loss: 0.00094222
Iteration 7/25 | Loss: 0.00092784
Iteration 8/25 | Loss: 0.00092325
Iteration 9/25 | Loss: 0.00092156
Iteration 10/25 | Loss: 0.00091988
Iteration 11/25 | Loss: 0.00091936
Iteration 12/25 | Loss: 0.00091851
Iteration 13/25 | Loss: 0.00091684
Iteration 14/25 | Loss: 0.00091627
Iteration 15/25 | Loss: 0.00091594
Iteration 16/25 | Loss: 0.00091575
Iteration 17/25 | Loss: 0.00091564
Iteration 18/25 | Loss: 0.00091562
Iteration 19/25 | Loss: 0.00091562
Iteration 20/25 | Loss: 0.00091562
Iteration 21/25 | Loss: 0.00091562
Iteration 22/25 | Loss: 0.00091562
Iteration 23/25 | Loss: 0.00091562
Iteration 24/25 | Loss: 0.00091562
Iteration 25/25 | Loss: 0.00091562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63849211
Iteration 2/25 | Loss: 0.00120796
Iteration 3/25 | Loss: 0.00120796
Iteration 4/25 | Loss: 0.00120796
Iteration 5/25 | Loss: 0.00120796
Iteration 6/25 | Loss: 0.00120796
Iteration 7/25 | Loss: 0.00120796
Iteration 8/25 | Loss: 0.00120796
Iteration 9/25 | Loss: 0.00120796
Iteration 10/25 | Loss: 0.00120796
Iteration 11/25 | Loss: 0.00120796
Iteration 12/25 | Loss: 0.00120796
Iteration 13/25 | Loss: 0.00120796
Iteration 14/25 | Loss: 0.00120796
Iteration 15/25 | Loss: 0.00120796
Iteration 16/25 | Loss: 0.00120796
Iteration 17/25 | Loss: 0.00120796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001207962166517973, 0.001207962166517973, 0.001207962166517973, 0.001207962166517973, 0.001207962166517973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001207962166517973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120796
Iteration 2/1000 | Loss: 0.00004492
Iteration 3/1000 | Loss: 0.00003334
Iteration 4/1000 | Loss: 0.00003085
Iteration 5/1000 | Loss: 0.00002963
Iteration 6/1000 | Loss: 0.00002883
Iteration 7/1000 | Loss: 0.00002825
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002729
Iteration 10/1000 | Loss: 0.00002699
Iteration 11/1000 | Loss: 0.00002676
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002652
Iteration 14/1000 | Loss: 0.00002645
Iteration 15/1000 | Loss: 0.00002639
Iteration 16/1000 | Loss: 0.00002637
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002635
Iteration 19/1000 | Loss: 0.00002635
Iteration 20/1000 | Loss: 0.00002634
Iteration 21/1000 | Loss: 0.00002633
Iteration 22/1000 | Loss: 0.00002632
Iteration 23/1000 | Loss: 0.00002631
Iteration 24/1000 | Loss: 0.00002631
Iteration 25/1000 | Loss: 0.00002631
Iteration 26/1000 | Loss: 0.00002629
Iteration 27/1000 | Loss: 0.00002629
Iteration 28/1000 | Loss: 0.00002629
Iteration 29/1000 | Loss: 0.00002629
Iteration 30/1000 | Loss: 0.00002629
Iteration 31/1000 | Loss: 0.00002628
Iteration 32/1000 | Loss: 0.00002628
Iteration 33/1000 | Loss: 0.00002628
Iteration 34/1000 | Loss: 0.00002627
Iteration 35/1000 | Loss: 0.00002627
Iteration 36/1000 | Loss: 0.00002627
Iteration 37/1000 | Loss: 0.00002626
Iteration 38/1000 | Loss: 0.00002626
Iteration 39/1000 | Loss: 0.00002626
Iteration 40/1000 | Loss: 0.00002626
Iteration 41/1000 | Loss: 0.00002625
Iteration 42/1000 | Loss: 0.00002625
Iteration 43/1000 | Loss: 0.00002625
Iteration 44/1000 | Loss: 0.00002625
Iteration 45/1000 | Loss: 0.00002625
Iteration 46/1000 | Loss: 0.00002625
Iteration 47/1000 | Loss: 0.00002625
Iteration 48/1000 | Loss: 0.00002625
Iteration 49/1000 | Loss: 0.00002625
Iteration 50/1000 | Loss: 0.00002624
Iteration 51/1000 | Loss: 0.00002624
Iteration 52/1000 | Loss: 0.00002624
Iteration 53/1000 | Loss: 0.00002624
Iteration 54/1000 | Loss: 0.00002623
Iteration 55/1000 | Loss: 0.00002623
Iteration 56/1000 | Loss: 0.00002623
Iteration 57/1000 | Loss: 0.00002623
Iteration 58/1000 | Loss: 0.00002622
Iteration 59/1000 | Loss: 0.00002622
Iteration 60/1000 | Loss: 0.00002622
Iteration 61/1000 | Loss: 0.00002622
Iteration 62/1000 | Loss: 0.00002621
Iteration 63/1000 | Loss: 0.00002621
Iteration 64/1000 | Loss: 0.00002621
Iteration 65/1000 | Loss: 0.00002621
Iteration 66/1000 | Loss: 0.00002620
Iteration 67/1000 | Loss: 0.00002620
Iteration 68/1000 | Loss: 0.00002620
Iteration 69/1000 | Loss: 0.00002620
Iteration 70/1000 | Loss: 0.00002619
Iteration 71/1000 | Loss: 0.00002619
Iteration 72/1000 | Loss: 0.00002619
Iteration 73/1000 | Loss: 0.00002618
Iteration 74/1000 | Loss: 0.00002618
Iteration 75/1000 | Loss: 0.00002618
Iteration 76/1000 | Loss: 0.00002618
Iteration 77/1000 | Loss: 0.00002618
Iteration 78/1000 | Loss: 0.00002618
Iteration 79/1000 | Loss: 0.00002618
Iteration 80/1000 | Loss: 0.00002618
Iteration 81/1000 | Loss: 0.00002617
Iteration 82/1000 | Loss: 0.00002617
Iteration 83/1000 | Loss: 0.00002617
Iteration 84/1000 | Loss: 0.00002617
Iteration 85/1000 | Loss: 0.00002617
Iteration 86/1000 | Loss: 0.00002617
Iteration 87/1000 | Loss: 0.00002617
Iteration 88/1000 | Loss: 0.00002617
Iteration 89/1000 | Loss: 0.00002617
Iteration 90/1000 | Loss: 0.00002617
Iteration 91/1000 | Loss: 0.00002617
Iteration 92/1000 | Loss: 0.00002617
Iteration 93/1000 | Loss: 0.00002617
Iteration 94/1000 | Loss: 0.00002617
Iteration 95/1000 | Loss: 0.00002617
Iteration 96/1000 | Loss: 0.00002617
Iteration 97/1000 | Loss: 0.00002617
Iteration 98/1000 | Loss: 0.00002617
Iteration 99/1000 | Loss: 0.00002617
Iteration 100/1000 | Loss: 0.00002617
Iteration 101/1000 | Loss: 0.00002617
Iteration 102/1000 | Loss: 0.00002617
Iteration 103/1000 | Loss: 0.00002617
Iteration 104/1000 | Loss: 0.00002617
Iteration 105/1000 | Loss: 0.00002617
Iteration 106/1000 | Loss: 0.00002617
Iteration 107/1000 | Loss: 0.00002617
Iteration 108/1000 | Loss: 0.00002617
Iteration 109/1000 | Loss: 0.00002617
Iteration 110/1000 | Loss: 0.00002617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.6172176148975268e-05, 2.6172176148975268e-05, 2.6172176148975268e-05, 2.6172176148975268e-05, 2.6172176148975268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6172176148975268e-05

Optimization complete. Final v2v error: 4.253312587738037 mm

Highest mean error: 5.479960918426514 mm for frame 44

Lowest mean error: 3.3304548263549805 mm for frame 136

Saving results

Total time: 61.77613186836243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085010
Iteration 2/25 | Loss: 0.01085010
Iteration 3/25 | Loss: 0.01085010
Iteration 4/25 | Loss: 0.01085010
Iteration 5/25 | Loss: 0.01085010
Iteration 6/25 | Loss: 0.01085010
Iteration 7/25 | Loss: 0.01085009
Iteration 8/25 | Loss: 0.01085009
Iteration 9/25 | Loss: 0.01085009
Iteration 10/25 | Loss: 0.01085009
Iteration 11/25 | Loss: 0.01085009
Iteration 12/25 | Loss: 0.01085009
Iteration 13/25 | Loss: 0.01085009
Iteration 14/25 | Loss: 0.01085009
Iteration 15/25 | Loss: 0.01085009
Iteration 16/25 | Loss: 0.01085009
Iteration 17/25 | Loss: 0.01085008
Iteration 18/25 | Loss: 0.01085008
Iteration 19/25 | Loss: 0.01085008
Iteration 20/25 | Loss: 0.01085008
Iteration 21/25 | Loss: 0.01085008
Iteration 22/25 | Loss: 0.01085008
Iteration 23/25 | Loss: 0.01085008
Iteration 24/25 | Loss: 0.01085008
Iteration 25/25 | Loss: 0.01085007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28488827
Iteration 2/25 | Loss: 0.07621358
Iteration 3/25 | Loss: 0.07621355
Iteration 4/25 | Loss: 0.07621353
Iteration 5/25 | Loss: 0.07621353
Iteration 6/25 | Loss: 0.07621352
Iteration 7/25 | Loss: 0.07621352
Iteration 8/25 | Loss: 0.07621351
Iteration 9/25 | Loss: 0.07621351
Iteration 10/25 | Loss: 0.07621351
Iteration 11/25 | Loss: 0.07621352
Iteration 12/25 | Loss: 0.07621351
Iteration 13/25 | Loss: 0.07621350
Iteration 14/25 | Loss: 0.07621350
Iteration 15/25 | Loss: 0.07621350
Iteration 16/25 | Loss: 0.07621350
Iteration 17/25 | Loss: 0.07621350
Iteration 18/25 | Loss: 0.07621350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.07621350139379501, 0.07621350139379501, 0.07621350139379501, 0.07621350139379501, 0.07621350139379501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07621350139379501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07621350
Iteration 2/1000 | Loss: 0.00124536
Iteration 3/1000 | Loss: 0.00048519
Iteration 4/1000 | Loss: 0.00142739
Iteration 5/1000 | Loss: 0.00049844
Iteration 6/1000 | Loss: 0.00035243
Iteration 7/1000 | Loss: 0.00033721
Iteration 8/1000 | Loss: 0.00016973
Iteration 9/1000 | Loss: 0.00019805
Iteration 10/1000 | Loss: 0.00111236
Iteration 11/1000 | Loss: 0.00008197
Iteration 12/1000 | Loss: 0.00015303
Iteration 13/1000 | Loss: 0.00004696
Iteration 14/1000 | Loss: 0.00003962
Iteration 15/1000 | Loss: 0.00005409
Iteration 16/1000 | Loss: 0.00027684
Iteration 17/1000 | Loss: 0.00003494
Iteration 18/1000 | Loss: 0.00011891
Iteration 19/1000 | Loss: 0.00005641
Iteration 20/1000 | Loss: 0.00002621
Iteration 21/1000 | Loss: 0.00007954
Iteration 22/1000 | Loss: 0.00003933
Iteration 23/1000 | Loss: 0.00004669
Iteration 24/1000 | Loss: 0.00005742
Iteration 25/1000 | Loss: 0.00008781
Iteration 26/1000 | Loss: 0.00002154
Iteration 27/1000 | Loss: 0.00006754
Iteration 28/1000 | Loss: 0.00011136
Iteration 29/1000 | Loss: 0.00026464
Iteration 30/1000 | Loss: 0.00004598
Iteration 31/1000 | Loss: 0.00002783
Iteration 32/1000 | Loss: 0.00001986
Iteration 33/1000 | Loss: 0.00009684
Iteration 34/1000 | Loss: 0.00003787
Iteration 35/1000 | Loss: 0.00003597
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00008427
Iteration 38/1000 | Loss: 0.00001809
Iteration 39/1000 | Loss: 0.00011495
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00006247
Iteration 42/1000 | Loss: 0.00004539
Iteration 43/1000 | Loss: 0.00002172
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00002910
Iteration 46/1000 | Loss: 0.00003268
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001609
Iteration 49/1000 | Loss: 0.00001609
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001602
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001743
Iteration 56/1000 | Loss: 0.00006270
Iteration 57/1000 | Loss: 0.00002394
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001600
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00004143
Iteration 103/1000 | Loss: 0.00004143
Iteration 104/1000 | Loss: 0.00004143
Iteration 105/1000 | Loss: 0.00018600
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001610
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00002789
Iteration 113/1000 | Loss: 0.00003487
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001542
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.5390693079098128e-05, 1.5390693079098128e-05, 1.5390693079098128e-05, 1.5390693079098128e-05, 1.5390693079098128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5390693079098128e-05

Optimization complete. Final v2v error: 3.2956759929656982 mm

Highest mean error: 4.42899227142334 mm for frame 114

Lowest mean error: 2.669203281402588 mm for frame 198

Saving results

Total time: 111.5164623260498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551100
Iteration 2/25 | Loss: 0.00128273
Iteration 3/25 | Loss: 0.00084358
Iteration 4/25 | Loss: 0.00080711
Iteration 5/25 | Loss: 0.00079799
Iteration 6/25 | Loss: 0.00079439
Iteration 7/25 | Loss: 0.00079358
Iteration 8/25 | Loss: 0.00079348
Iteration 9/25 | Loss: 0.00079348
Iteration 10/25 | Loss: 0.00079348
Iteration 11/25 | Loss: 0.00079348
Iteration 12/25 | Loss: 0.00079348
Iteration 13/25 | Loss: 0.00079348
Iteration 14/25 | Loss: 0.00079348
Iteration 15/25 | Loss: 0.00079348
Iteration 16/25 | Loss: 0.00079348
Iteration 17/25 | Loss: 0.00079348
Iteration 18/25 | Loss: 0.00079348
Iteration 19/25 | Loss: 0.00079348
Iteration 20/25 | Loss: 0.00079348
Iteration 21/25 | Loss: 0.00079348
Iteration 22/25 | Loss: 0.00079348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00079348060535267, 0.00079348060535267, 0.00079348060535267, 0.00079348060535267, 0.00079348060535267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00079348060535267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85129350
Iteration 2/25 | Loss: 0.00093674
Iteration 3/25 | Loss: 0.00093673
Iteration 4/25 | Loss: 0.00093673
Iteration 5/25 | Loss: 0.00093673
Iteration 6/25 | Loss: 0.00093673
Iteration 7/25 | Loss: 0.00093673
Iteration 8/25 | Loss: 0.00093673
Iteration 9/25 | Loss: 0.00093673
Iteration 10/25 | Loss: 0.00093673
Iteration 11/25 | Loss: 0.00093673
Iteration 12/25 | Loss: 0.00093673
Iteration 13/25 | Loss: 0.00093673
Iteration 14/25 | Loss: 0.00093673
Iteration 15/25 | Loss: 0.00093673
Iteration 16/25 | Loss: 0.00093673
Iteration 17/25 | Loss: 0.00093673
Iteration 18/25 | Loss: 0.00093673
Iteration 19/25 | Loss: 0.00093673
Iteration 20/25 | Loss: 0.00093673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009367286693304777, 0.0009367286693304777, 0.0009367286693304777, 0.0009367286693304777, 0.0009367286693304777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009367286693304777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093673
Iteration 2/1000 | Loss: 0.00003384
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001948
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001741
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001626
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001592
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001588
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001587
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001578
Iteration 31/1000 | Loss: 0.00001576
Iteration 32/1000 | Loss: 0.00001576
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001568
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001566
Iteration 67/1000 | Loss: 0.00001566
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001566
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.5661724319215864e-05, 1.5661724319215864e-05, 1.5661724319215864e-05, 1.5661724319215864e-05, 1.5661724319215864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5661724319215864e-05

Optimization complete. Final v2v error: 3.2984514236450195 mm

Highest mean error: 4.103750705718994 mm for frame 10

Lowest mean error: 3.026263952255249 mm for frame 45

Saving results

Total time: 39.86888337135315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023853
Iteration 2/25 | Loss: 0.00408876
Iteration 3/25 | Loss: 0.00286863
Iteration 4/25 | Loss: 0.00214505
Iteration 5/25 | Loss: 0.00195019
Iteration 6/25 | Loss: 0.00192121
Iteration 7/25 | Loss: 0.00185378
Iteration 8/25 | Loss: 0.00177885
Iteration 9/25 | Loss: 0.00171713
Iteration 10/25 | Loss: 0.00169483
Iteration 11/25 | Loss: 0.00161203
Iteration 12/25 | Loss: 0.00154201
Iteration 13/25 | Loss: 0.00153703
Iteration 14/25 | Loss: 0.00146410
Iteration 15/25 | Loss: 0.00142381
Iteration 16/25 | Loss: 0.00138679
Iteration 17/25 | Loss: 0.00136977
Iteration 18/25 | Loss: 0.00134831
Iteration 19/25 | Loss: 0.00134212
Iteration 20/25 | Loss: 0.00136503
Iteration 21/25 | Loss: 0.00133996
Iteration 22/25 | Loss: 0.00134093
Iteration 23/25 | Loss: 0.00134189
Iteration 24/25 | Loss: 0.00134284
Iteration 25/25 | Loss: 0.00133639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20202947
Iteration 2/25 | Loss: 0.00795736
Iteration 3/25 | Loss: 0.00702243
Iteration 4/25 | Loss: 0.00702242
Iteration 5/25 | Loss: 0.00702242
Iteration 6/25 | Loss: 0.00702242
Iteration 7/25 | Loss: 0.00702242
Iteration 8/25 | Loss: 0.00702242
Iteration 9/25 | Loss: 0.00702242
Iteration 10/25 | Loss: 0.00702242
Iteration 11/25 | Loss: 0.00702242
Iteration 12/25 | Loss: 0.00702242
Iteration 13/25 | Loss: 0.00702242
Iteration 14/25 | Loss: 0.00702242
Iteration 15/25 | Loss: 0.00702242
Iteration 16/25 | Loss: 0.00702242
Iteration 17/25 | Loss: 0.00702242
Iteration 18/25 | Loss: 0.00702242
Iteration 19/25 | Loss: 0.00702242
Iteration 20/25 | Loss: 0.00702242
Iteration 21/25 | Loss: 0.00702242
Iteration 22/25 | Loss: 0.00702242
Iteration 23/25 | Loss: 0.00702242
Iteration 24/25 | Loss: 0.00702242
Iteration 25/25 | Loss: 0.00702242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00702242
Iteration 2/1000 | Loss: 0.00301409
Iteration 3/1000 | Loss: 0.00453272
Iteration 4/1000 | Loss: 0.00711293
Iteration 5/1000 | Loss: 0.00881580
Iteration 6/1000 | Loss: 0.00386398
Iteration 7/1000 | Loss: 0.00228225
Iteration 8/1000 | Loss: 0.00264545
Iteration 9/1000 | Loss: 0.01126384
Iteration 10/1000 | Loss: 0.00248391
Iteration 11/1000 | Loss: 0.01415563
Iteration 12/1000 | Loss: 0.00530616
Iteration 13/1000 | Loss: 0.00494721
Iteration 14/1000 | Loss: 0.00443580
Iteration 15/1000 | Loss: 0.00376188
Iteration 16/1000 | Loss: 0.01240454
Iteration 17/1000 | Loss: 0.00334689
Iteration 18/1000 | Loss: 0.00416253
Iteration 19/1000 | Loss: 0.00422992
Iteration 20/1000 | Loss: 0.00754894
Iteration 21/1000 | Loss: 0.00619663
Iteration 22/1000 | Loss: 0.01049733
Iteration 23/1000 | Loss: 0.00218194
Iteration 24/1000 | Loss: 0.00767762
Iteration 25/1000 | Loss: 0.00165428
Iteration 26/1000 | Loss: 0.00097304
Iteration 27/1000 | Loss: 0.00246757
Iteration 28/1000 | Loss: 0.00082324
Iteration 29/1000 | Loss: 0.00389339
Iteration 30/1000 | Loss: 0.00173822
Iteration 31/1000 | Loss: 0.00375886
Iteration 32/1000 | Loss: 0.00127587
Iteration 33/1000 | Loss: 0.00251280
Iteration 34/1000 | Loss: 0.00304167
Iteration 35/1000 | Loss: 0.00101916
Iteration 36/1000 | Loss: 0.00427643
Iteration 37/1000 | Loss: 0.00215515
Iteration 38/1000 | Loss: 0.00049223
Iteration 39/1000 | Loss: 0.00014068
Iteration 40/1000 | Loss: 0.00931020
Iteration 41/1000 | Loss: 0.00453148
Iteration 42/1000 | Loss: 0.00537231
Iteration 43/1000 | Loss: 0.00452998
Iteration 44/1000 | Loss: 0.00100259
Iteration 45/1000 | Loss: 0.00014312
Iteration 46/1000 | Loss: 0.00194135
Iteration 47/1000 | Loss: 0.00093377
Iteration 48/1000 | Loss: 0.00011111
Iteration 49/1000 | Loss: 0.00142394
Iteration 50/1000 | Loss: 0.00010625
Iteration 51/1000 | Loss: 0.00007610
Iteration 52/1000 | Loss: 0.00238412
Iteration 53/1000 | Loss: 0.00569158
Iteration 54/1000 | Loss: 0.00024261
Iteration 55/1000 | Loss: 0.00258527
Iteration 56/1000 | Loss: 0.00034952
Iteration 57/1000 | Loss: 0.00011969
Iteration 58/1000 | Loss: 0.00017448
Iteration 59/1000 | Loss: 0.00016728
Iteration 60/1000 | Loss: 0.00006111
Iteration 61/1000 | Loss: 0.00225150
Iteration 62/1000 | Loss: 0.00085814
Iteration 63/1000 | Loss: 0.00080651
Iteration 64/1000 | Loss: 0.00005248
Iteration 65/1000 | Loss: 0.00004473
Iteration 66/1000 | Loss: 0.00173136
Iteration 67/1000 | Loss: 0.00055912
Iteration 68/1000 | Loss: 0.00004286
Iteration 69/1000 | Loss: 0.00137494
Iteration 70/1000 | Loss: 0.00019383
Iteration 71/1000 | Loss: 0.00045807
Iteration 72/1000 | Loss: 0.00358206
Iteration 73/1000 | Loss: 0.00089864
Iteration 74/1000 | Loss: 0.00004422
Iteration 75/1000 | Loss: 0.00003631
Iteration 76/1000 | Loss: 0.00279564
Iteration 77/1000 | Loss: 0.00026277
Iteration 78/1000 | Loss: 0.00099453
Iteration 79/1000 | Loss: 0.00011247
Iteration 80/1000 | Loss: 0.00056121
Iteration 81/1000 | Loss: 0.00004652
Iteration 82/1000 | Loss: 0.00003658
Iteration 83/1000 | Loss: 0.00003250
Iteration 84/1000 | Loss: 0.00003069
Iteration 85/1000 | Loss: 0.00002980
Iteration 86/1000 | Loss: 0.00002905
Iteration 87/1000 | Loss: 0.00074399
Iteration 88/1000 | Loss: 0.00003446
Iteration 89/1000 | Loss: 0.00002957
Iteration 90/1000 | Loss: 0.00002758
Iteration 91/1000 | Loss: 0.00002617
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002430
Iteration 94/1000 | Loss: 0.00002379
Iteration 95/1000 | Loss: 0.00002352
Iteration 96/1000 | Loss: 0.00002338
Iteration 97/1000 | Loss: 0.00002332
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002325
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002306
Iteration 102/1000 | Loss: 0.00002303
Iteration 103/1000 | Loss: 0.00002302
Iteration 104/1000 | Loss: 0.00002292
Iteration 105/1000 | Loss: 0.00002291
Iteration 106/1000 | Loss: 0.00002291
Iteration 107/1000 | Loss: 0.00002289
Iteration 108/1000 | Loss: 0.00002286
Iteration 109/1000 | Loss: 0.00002279
Iteration 110/1000 | Loss: 0.00002272
Iteration 111/1000 | Loss: 0.00002268
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002264
Iteration 115/1000 | Loss: 0.00002264
Iteration 116/1000 | Loss: 0.00002264
Iteration 117/1000 | Loss: 0.00002263
Iteration 118/1000 | Loss: 0.00002262
Iteration 119/1000 | Loss: 0.00002261
Iteration 120/1000 | Loss: 0.00002261
Iteration 121/1000 | Loss: 0.00002260
Iteration 122/1000 | Loss: 0.00002259
Iteration 123/1000 | Loss: 0.00002259
Iteration 124/1000 | Loss: 0.00002259
Iteration 125/1000 | Loss: 0.00002258
Iteration 126/1000 | Loss: 0.00002258
Iteration 127/1000 | Loss: 0.00002256
Iteration 128/1000 | Loss: 0.00002255
Iteration 129/1000 | Loss: 0.00002255
Iteration 130/1000 | Loss: 0.00002253
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002252
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002250
Iteration 137/1000 | Loss: 0.00002250
Iteration 138/1000 | Loss: 0.00002250
Iteration 139/1000 | Loss: 0.00002249
Iteration 140/1000 | Loss: 0.00002249
Iteration 141/1000 | Loss: 0.00002248
Iteration 142/1000 | Loss: 0.00002248
Iteration 143/1000 | Loss: 0.00002246
Iteration 144/1000 | Loss: 0.00002246
Iteration 145/1000 | Loss: 0.00002245
Iteration 146/1000 | Loss: 0.00002245
Iteration 147/1000 | Loss: 0.00002245
Iteration 148/1000 | Loss: 0.00002245
Iteration 149/1000 | Loss: 0.00002244
Iteration 150/1000 | Loss: 0.00002244
Iteration 151/1000 | Loss: 0.00002244
Iteration 152/1000 | Loss: 0.00002244
Iteration 153/1000 | Loss: 0.00002244
Iteration 154/1000 | Loss: 0.00002244
Iteration 155/1000 | Loss: 0.00002243
Iteration 156/1000 | Loss: 0.00002243
Iteration 157/1000 | Loss: 0.00002243
Iteration 158/1000 | Loss: 0.00002243
Iteration 159/1000 | Loss: 0.00002243
Iteration 160/1000 | Loss: 0.00002242
Iteration 161/1000 | Loss: 0.00002242
Iteration 162/1000 | Loss: 0.00002242
Iteration 163/1000 | Loss: 0.00002242
Iteration 164/1000 | Loss: 0.00002242
Iteration 165/1000 | Loss: 0.00002241
Iteration 166/1000 | Loss: 0.00002241
Iteration 167/1000 | Loss: 0.00002241
Iteration 168/1000 | Loss: 0.00002241
Iteration 169/1000 | Loss: 0.00002240
Iteration 170/1000 | Loss: 0.00002240
Iteration 171/1000 | Loss: 0.00002240
Iteration 172/1000 | Loss: 0.00002240
Iteration 173/1000 | Loss: 0.00002240
Iteration 174/1000 | Loss: 0.00002240
Iteration 175/1000 | Loss: 0.00002240
Iteration 176/1000 | Loss: 0.00002240
Iteration 177/1000 | Loss: 0.00002240
Iteration 178/1000 | Loss: 0.00002240
Iteration 179/1000 | Loss: 0.00002240
Iteration 180/1000 | Loss: 0.00002240
Iteration 181/1000 | Loss: 0.00002239
Iteration 182/1000 | Loss: 0.00002239
Iteration 183/1000 | Loss: 0.00002239
Iteration 184/1000 | Loss: 0.00002239
Iteration 185/1000 | Loss: 0.00002239
Iteration 186/1000 | Loss: 0.00002239
Iteration 187/1000 | Loss: 0.00002239
Iteration 188/1000 | Loss: 0.00002239
Iteration 189/1000 | Loss: 0.00002239
Iteration 190/1000 | Loss: 0.00002239
Iteration 191/1000 | Loss: 0.00002239
Iteration 192/1000 | Loss: 0.00002239
Iteration 193/1000 | Loss: 0.00002239
Iteration 194/1000 | Loss: 0.00002239
Iteration 195/1000 | Loss: 0.00002239
Iteration 196/1000 | Loss: 0.00002238
Iteration 197/1000 | Loss: 0.00002238
Iteration 198/1000 | Loss: 0.00002238
Iteration 199/1000 | Loss: 0.00002238
Iteration 200/1000 | Loss: 0.00002238
Iteration 201/1000 | Loss: 0.00002238
Iteration 202/1000 | Loss: 0.00002238
Iteration 203/1000 | Loss: 0.00002238
Iteration 204/1000 | Loss: 0.00002238
Iteration 205/1000 | Loss: 0.00002238
Iteration 206/1000 | Loss: 0.00002238
Iteration 207/1000 | Loss: 0.00002238
Iteration 208/1000 | Loss: 0.00002237
Iteration 209/1000 | Loss: 0.00002237
Iteration 210/1000 | Loss: 0.00002237
Iteration 211/1000 | Loss: 0.00002237
Iteration 212/1000 | Loss: 0.00002237
Iteration 213/1000 | Loss: 0.00002237
Iteration 214/1000 | Loss: 0.00002237
Iteration 215/1000 | Loss: 0.00002237
Iteration 216/1000 | Loss: 0.00002237
Iteration 217/1000 | Loss: 0.00002236
Iteration 218/1000 | Loss: 0.00002236
Iteration 219/1000 | Loss: 0.00002236
Iteration 220/1000 | Loss: 0.00002236
Iteration 221/1000 | Loss: 0.00002236
Iteration 222/1000 | Loss: 0.00002236
Iteration 223/1000 | Loss: 0.00002236
Iteration 224/1000 | Loss: 0.00002236
Iteration 225/1000 | Loss: 0.00002235
Iteration 226/1000 | Loss: 0.00002235
Iteration 227/1000 | Loss: 0.00002235
Iteration 228/1000 | Loss: 0.00002235
Iteration 229/1000 | Loss: 0.00002235
Iteration 230/1000 | Loss: 0.00002235
Iteration 231/1000 | Loss: 0.00002234
Iteration 232/1000 | Loss: 0.00002234
Iteration 233/1000 | Loss: 0.00002234
Iteration 234/1000 | Loss: 0.00002234
Iteration 235/1000 | Loss: 0.00002234
Iteration 236/1000 | Loss: 0.00002233
Iteration 237/1000 | Loss: 0.00002233
Iteration 238/1000 | Loss: 0.00002233
Iteration 239/1000 | Loss: 0.00002233
Iteration 240/1000 | Loss: 0.00002233
Iteration 241/1000 | Loss: 0.00002233
Iteration 242/1000 | Loss: 0.00002233
Iteration 243/1000 | Loss: 0.00002233
Iteration 244/1000 | Loss: 0.00002232
Iteration 245/1000 | Loss: 0.00002232
Iteration 246/1000 | Loss: 0.00002232
Iteration 247/1000 | Loss: 0.00002232
Iteration 248/1000 | Loss: 0.00002232
Iteration 249/1000 | Loss: 0.00002232
Iteration 250/1000 | Loss: 0.00002232
Iteration 251/1000 | Loss: 0.00002232
Iteration 252/1000 | Loss: 0.00002232
Iteration 253/1000 | Loss: 0.00002232
Iteration 254/1000 | Loss: 0.00002232
Iteration 255/1000 | Loss: 0.00002231
Iteration 256/1000 | Loss: 0.00002231
Iteration 257/1000 | Loss: 0.00002231
Iteration 258/1000 | Loss: 0.00002231
Iteration 259/1000 | Loss: 0.00002231
Iteration 260/1000 | Loss: 0.00002231
Iteration 261/1000 | Loss: 0.00002231
Iteration 262/1000 | Loss: 0.00002231
Iteration 263/1000 | Loss: 0.00002231
Iteration 264/1000 | Loss: 0.00002231
Iteration 265/1000 | Loss: 0.00002231
Iteration 266/1000 | Loss: 0.00002231
Iteration 267/1000 | Loss: 0.00002230
Iteration 268/1000 | Loss: 0.00002230
Iteration 269/1000 | Loss: 0.00002230
Iteration 270/1000 | Loss: 0.00002230
Iteration 271/1000 | Loss: 0.00002230
Iteration 272/1000 | Loss: 0.00002230
Iteration 273/1000 | Loss: 0.00002230
Iteration 274/1000 | Loss: 0.00002230
Iteration 275/1000 | Loss: 0.00002230
Iteration 276/1000 | Loss: 0.00002230
Iteration 277/1000 | Loss: 0.00002230
Iteration 278/1000 | Loss: 0.00002230
Iteration 279/1000 | Loss: 0.00002230
Iteration 280/1000 | Loss: 0.00002230
Iteration 281/1000 | Loss: 0.00002229
Iteration 282/1000 | Loss: 0.00002229
Iteration 283/1000 | Loss: 0.00002229
Iteration 284/1000 | Loss: 0.00002229
Iteration 285/1000 | Loss: 0.00002229
Iteration 286/1000 | Loss: 0.00002229
Iteration 287/1000 | Loss: 0.00002229
Iteration 288/1000 | Loss: 0.00002229
Iteration 289/1000 | Loss: 0.00002229
Iteration 290/1000 | Loss: 0.00002229
Iteration 291/1000 | Loss: 0.00002229
Iteration 292/1000 | Loss: 0.00002229
Iteration 293/1000 | Loss: 0.00002229
Iteration 294/1000 | Loss: 0.00002229
Iteration 295/1000 | Loss: 0.00002229
Iteration 296/1000 | Loss: 0.00002228
Iteration 297/1000 | Loss: 0.00002228
Iteration 298/1000 | Loss: 0.00002228
Iteration 299/1000 | Loss: 0.00002228
Iteration 300/1000 | Loss: 0.00002228
Iteration 301/1000 | Loss: 0.00002228
Iteration 302/1000 | Loss: 0.00002228
Iteration 303/1000 | Loss: 0.00002228
Iteration 304/1000 | Loss: 0.00002228
Iteration 305/1000 | Loss: 0.00002228
Iteration 306/1000 | Loss: 0.00002228
Iteration 307/1000 | Loss: 0.00002228
Iteration 308/1000 | Loss: 0.00002228
Iteration 309/1000 | Loss: 0.00002228
Iteration 310/1000 | Loss: 0.00002228
Iteration 311/1000 | Loss: 0.00002228
Iteration 312/1000 | Loss: 0.00002228
Iteration 313/1000 | Loss: 0.00002228
Iteration 314/1000 | Loss: 0.00002228
Iteration 315/1000 | Loss: 0.00002228
Iteration 316/1000 | Loss: 0.00002228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [2.2284277292783372e-05, 2.2284277292783372e-05, 2.2284277292783372e-05, 2.2284277292783372e-05, 2.2284277292783372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2284277292783372e-05

Optimization complete. Final v2v error: 3.8770763874053955 mm

Highest mean error: 5.49932861328125 mm for frame 45

Lowest mean error: 2.942267894744873 mm for frame 91

Saving results

Total time: 202.63996362686157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876062
Iteration 2/25 | Loss: 0.00102984
Iteration 3/25 | Loss: 0.00085661
Iteration 4/25 | Loss: 0.00082354
Iteration 5/25 | Loss: 0.00081347
Iteration 6/25 | Loss: 0.00081029
Iteration 7/25 | Loss: 0.00080924
Iteration 8/25 | Loss: 0.00080875
Iteration 9/25 | Loss: 0.00081115
Iteration 10/25 | Loss: 0.00081054
Iteration 11/25 | Loss: 0.00080798
Iteration 12/25 | Loss: 0.00081022
Iteration 13/25 | Loss: 0.00081013
Iteration 14/25 | Loss: 0.00080966
Iteration 15/25 | Loss: 0.00081016
Iteration 16/25 | Loss: 0.00080999
Iteration 17/25 | Loss: 0.00080888
Iteration 18/25 | Loss: 0.00080959
Iteration 19/25 | Loss: 0.00080913
Iteration 20/25 | Loss: 0.00080957
Iteration 21/25 | Loss: 0.00080956
Iteration 22/25 | Loss: 0.00080956
Iteration 23/25 | Loss: 0.00080975
Iteration 24/25 | Loss: 0.00080721
Iteration 25/25 | Loss: 0.00080637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59499753
Iteration 2/25 | Loss: 0.00191415
Iteration 3/25 | Loss: 0.00191415
Iteration 4/25 | Loss: 0.00191415
Iteration 5/25 | Loss: 0.00191414
Iteration 6/25 | Loss: 0.00191414
Iteration 7/25 | Loss: 0.00191414
Iteration 8/25 | Loss: 0.00191414
Iteration 9/25 | Loss: 0.00191414
Iteration 10/25 | Loss: 0.00191414
Iteration 11/25 | Loss: 0.00191414
Iteration 12/25 | Loss: 0.00191414
Iteration 13/25 | Loss: 0.00191414
Iteration 14/25 | Loss: 0.00191414
Iteration 15/25 | Loss: 0.00191414
Iteration 16/25 | Loss: 0.00191414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019141433294862509, 0.0019141433294862509, 0.0019141433294862509, 0.0019141433294862509, 0.0019141433294862509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019141433294862509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191414
Iteration 2/1000 | Loss: 0.00012018
Iteration 3/1000 | Loss: 0.00008574
Iteration 4/1000 | Loss: 0.00007428
Iteration 5/1000 | Loss: 0.00006585
Iteration 6/1000 | Loss: 0.00006250
Iteration 7/1000 | Loss: 0.00006034
Iteration 8/1000 | Loss: 0.00005810
Iteration 9/1000 | Loss: 0.00005621
Iteration 10/1000 | Loss: 0.00005489
Iteration 11/1000 | Loss: 0.00005420
Iteration 12/1000 | Loss: 0.00005358
Iteration 13/1000 | Loss: 0.00005290
Iteration 14/1000 | Loss: 0.00005227
Iteration 15/1000 | Loss: 0.00005175
Iteration 16/1000 | Loss: 0.00005127
Iteration 17/1000 | Loss: 0.00005087
Iteration 18/1000 | Loss: 0.00005045
Iteration 19/1000 | Loss: 0.00048989
Iteration 20/1000 | Loss: 0.00015181
Iteration 21/1000 | Loss: 0.00005040
Iteration 22/1000 | Loss: 0.00097105
Iteration 23/1000 | Loss: 0.00061774
Iteration 24/1000 | Loss: 0.00008007
Iteration 25/1000 | Loss: 0.00006628
Iteration 26/1000 | Loss: 0.00006096
Iteration 27/1000 | Loss: 0.00057026
Iteration 28/1000 | Loss: 0.00113322
Iteration 29/1000 | Loss: 0.00041225
Iteration 30/1000 | Loss: 0.00017776
Iteration 31/1000 | Loss: 0.00005695
Iteration 32/1000 | Loss: 0.00005338
Iteration 33/1000 | Loss: 0.00042789
Iteration 34/1000 | Loss: 0.00007966
Iteration 35/1000 | Loss: 0.00017699
Iteration 36/1000 | Loss: 0.00005129
Iteration 37/1000 | Loss: 0.00004877
Iteration 38/1000 | Loss: 0.00004757
Iteration 39/1000 | Loss: 0.00051217
Iteration 40/1000 | Loss: 0.00004803
Iteration 41/1000 | Loss: 0.00069491
Iteration 42/1000 | Loss: 0.00016961
Iteration 43/1000 | Loss: 0.00020415
Iteration 44/1000 | Loss: 0.00038406
Iteration 45/1000 | Loss: 0.00032861
Iteration 46/1000 | Loss: 0.00035345
Iteration 47/1000 | Loss: 0.00015102
Iteration 48/1000 | Loss: 0.00005784
Iteration 49/1000 | Loss: 0.00005212
Iteration 50/1000 | Loss: 0.00017153
Iteration 51/1000 | Loss: 0.00017895
Iteration 52/1000 | Loss: 0.00014684
Iteration 53/1000 | Loss: 0.00015705
Iteration 54/1000 | Loss: 0.00018945
Iteration 55/1000 | Loss: 0.00005093
Iteration 56/1000 | Loss: 0.00025833
Iteration 57/1000 | Loss: 0.00020638
Iteration 58/1000 | Loss: 0.00012989
Iteration 59/1000 | Loss: 0.00005185
Iteration 60/1000 | Loss: 0.00011129
Iteration 61/1000 | Loss: 0.00014313
Iteration 62/1000 | Loss: 0.00011023
Iteration 63/1000 | Loss: 0.00008190
Iteration 64/1000 | Loss: 0.00007119
Iteration 65/1000 | Loss: 0.00004437
Iteration 66/1000 | Loss: 0.00004319
Iteration 67/1000 | Loss: 0.00004254
Iteration 68/1000 | Loss: 0.00004198
Iteration 69/1000 | Loss: 0.00004146
Iteration 70/1000 | Loss: 0.00048876
Iteration 71/1000 | Loss: 0.00004512
Iteration 72/1000 | Loss: 0.00004067
Iteration 73/1000 | Loss: 0.00004005
Iteration 74/1000 | Loss: 0.00003960
Iteration 75/1000 | Loss: 0.00003928
Iteration 76/1000 | Loss: 0.00003902
Iteration 77/1000 | Loss: 0.00003883
Iteration 78/1000 | Loss: 0.00003876
Iteration 79/1000 | Loss: 0.00003869
Iteration 80/1000 | Loss: 0.00003866
Iteration 81/1000 | Loss: 0.00003865
Iteration 82/1000 | Loss: 0.00003865
Iteration 83/1000 | Loss: 0.00003865
Iteration 84/1000 | Loss: 0.00003865
Iteration 85/1000 | Loss: 0.00003865
Iteration 86/1000 | Loss: 0.00003865
Iteration 87/1000 | Loss: 0.00003864
Iteration 88/1000 | Loss: 0.00003864
Iteration 89/1000 | Loss: 0.00003864
Iteration 90/1000 | Loss: 0.00003863
Iteration 91/1000 | Loss: 0.00003863
Iteration 92/1000 | Loss: 0.00003863
Iteration 93/1000 | Loss: 0.00003862
Iteration 94/1000 | Loss: 0.00003861
Iteration 95/1000 | Loss: 0.00003861
Iteration 96/1000 | Loss: 0.00003860
Iteration 97/1000 | Loss: 0.00003860
Iteration 98/1000 | Loss: 0.00003859
Iteration 99/1000 | Loss: 0.00003859
Iteration 100/1000 | Loss: 0.00003859
Iteration 101/1000 | Loss: 0.00003858
Iteration 102/1000 | Loss: 0.00003858
Iteration 103/1000 | Loss: 0.00003858
Iteration 104/1000 | Loss: 0.00003857
Iteration 105/1000 | Loss: 0.00003857
Iteration 106/1000 | Loss: 0.00003857
Iteration 107/1000 | Loss: 0.00003857
Iteration 108/1000 | Loss: 0.00003857
Iteration 109/1000 | Loss: 0.00003857
Iteration 110/1000 | Loss: 0.00003856
Iteration 111/1000 | Loss: 0.00003856
Iteration 112/1000 | Loss: 0.00003856
Iteration 113/1000 | Loss: 0.00003855
Iteration 114/1000 | Loss: 0.00003854
Iteration 115/1000 | Loss: 0.00003854
Iteration 116/1000 | Loss: 0.00003853
Iteration 117/1000 | Loss: 0.00003851
Iteration 118/1000 | Loss: 0.00003851
Iteration 119/1000 | Loss: 0.00003850
Iteration 120/1000 | Loss: 0.00003850
Iteration 121/1000 | Loss: 0.00003849
Iteration 122/1000 | Loss: 0.00003848
Iteration 123/1000 | Loss: 0.00003847
Iteration 124/1000 | Loss: 0.00003847
Iteration 125/1000 | Loss: 0.00003846
Iteration 126/1000 | Loss: 0.00003846
Iteration 127/1000 | Loss: 0.00003846
Iteration 128/1000 | Loss: 0.00003845
Iteration 129/1000 | Loss: 0.00003845
Iteration 130/1000 | Loss: 0.00003845
Iteration 131/1000 | Loss: 0.00003845
Iteration 132/1000 | Loss: 0.00003844
Iteration 133/1000 | Loss: 0.00003844
Iteration 134/1000 | Loss: 0.00003844
Iteration 135/1000 | Loss: 0.00003844
Iteration 136/1000 | Loss: 0.00003844
Iteration 137/1000 | Loss: 0.00003843
Iteration 138/1000 | Loss: 0.00003843
Iteration 139/1000 | Loss: 0.00003843
Iteration 140/1000 | Loss: 0.00003842
Iteration 141/1000 | Loss: 0.00003842
Iteration 142/1000 | Loss: 0.00003842
Iteration 143/1000 | Loss: 0.00003842
Iteration 144/1000 | Loss: 0.00003841
Iteration 145/1000 | Loss: 0.00003841
Iteration 146/1000 | Loss: 0.00003841
Iteration 147/1000 | Loss: 0.00003840
Iteration 148/1000 | Loss: 0.00003840
Iteration 149/1000 | Loss: 0.00003839
Iteration 150/1000 | Loss: 0.00003839
Iteration 151/1000 | Loss: 0.00003839
Iteration 152/1000 | Loss: 0.00003839
Iteration 153/1000 | Loss: 0.00003839
Iteration 154/1000 | Loss: 0.00003839
Iteration 155/1000 | Loss: 0.00003839
Iteration 156/1000 | Loss: 0.00003839
Iteration 157/1000 | Loss: 0.00003839
Iteration 158/1000 | Loss: 0.00003839
Iteration 159/1000 | Loss: 0.00003839
Iteration 160/1000 | Loss: 0.00003838
Iteration 161/1000 | Loss: 0.00003838
Iteration 162/1000 | Loss: 0.00003838
Iteration 163/1000 | Loss: 0.00003838
Iteration 164/1000 | Loss: 0.00003838
Iteration 165/1000 | Loss: 0.00003838
Iteration 166/1000 | Loss: 0.00003838
Iteration 167/1000 | Loss: 0.00003838
Iteration 168/1000 | Loss: 0.00003838
Iteration 169/1000 | Loss: 0.00003838
Iteration 170/1000 | Loss: 0.00003838
Iteration 171/1000 | Loss: 0.00003837
Iteration 172/1000 | Loss: 0.00003837
Iteration 173/1000 | Loss: 0.00003837
Iteration 174/1000 | Loss: 0.00003837
Iteration 175/1000 | Loss: 0.00003837
Iteration 176/1000 | Loss: 0.00003837
Iteration 177/1000 | Loss: 0.00003837
Iteration 178/1000 | Loss: 0.00003837
Iteration 179/1000 | Loss: 0.00003836
Iteration 180/1000 | Loss: 0.00003836
Iteration 181/1000 | Loss: 0.00003836
Iteration 182/1000 | Loss: 0.00003836
Iteration 183/1000 | Loss: 0.00003836
Iteration 184/1000 | Loss: 0.00003836
Iteration 185/1000 | Loss: 0.00003836
Iteration 186/1000 | Loss: 0.00003836
Iteration 187/1000 | Loss: 0.00003835
Iteration 188/1000 | Loss: 0.00003835
Iteration 189/1000 | Loss: 0.00003835
Iteration 190/1000 | Loss: 0.00003835
Iteration 191/1000 | Loss: 0.00003835
Iteration 192/1000 | Loss: 0.00003835
Iteration 193/1000 | Loss: 0.00003835
Iteration 194/1000 | Loss: 0.00003835
Iteration 195/1000 | Loss: 0.00003835
Iteration 196/1000 | Loss: 0.00003834
Iteration 197/1000 | Loss: 0.00003834
Iteration 198/1000 | Loss: 0.00003834
Iteration 199/1000 | Loss: 0.00003834
Iteration 200/1000 | Loss: 0.00003833
Iteration 201/1000 | Loss: 0.00003833
Iteration 202/1000 | Loss: 0.00003833
Iteration 203/1000 | Loss: 0.00003833
Iteration 204/1000 | Loss: 0.00003833
Iteration 205/1000 | Loss: 0.00003833
Iteration 206/1000 | Loss: 0.00003832
Iteration 207/1000 | Loss: 0.00003832
Iteration 208/1000 | Loss: 0.00003832
Iteration 209/1000 | Loss: 0.00003832
Iteration 210/1000 | Loss: 0.00003832
Iteration 211/1000 | Loss: 0.00003832
Iteration 212/1000 | Loss: 0.00003831
Iteration 213/1000 | Loss: 0.00003831
Iteration 214/1000 | Loss: 0.00003831
Iteration 215/1000 | Loss: 0.00003831
Iteration 216/1000 | Loss: 0.00003831
Iteration 217/1000 | Loss: 0.00003831
Iteration 218/1000 | Loss: 0.00003831
Iteration 219/1000 | Loss: 0.00003831
Iteration 220/1000 | Loss: 0.00003831
Iteration 221/1000 | Loss: 0.00003830
Iteration 222/1000 | Loss: 0.00003830
Iteration 223/1000 | Loss: 0.00003830
Iteration 224/1000 | Loss: 0.00003830
Iteration 225/1000 | Loss: 0.00003830
Iteration 226/1000 | Loss: 0.00003830
Iteration 227/1000 | Loss: 0.00003830
Iteration 228/1000 | Loss: 0.00003830
Iteration 229/1000 | Loss: 0.00003830
Iteration 230/1000 | Loss: 0.00003830
Iteration 231/1000 | Loss: 0.00003830
Iteration 232/1000 | Loss: 0.00003830
Iteration 233/1000 | Loss: 0.00003830
Iteration 234/1000 | Loss: 0.00003830
Iteration 235/1000 | Loss: 0.00003830
Iteration 236/1000 | Loss: 0.00003830
Iteration 237/1000 | Loss: 0.00003830
Iteration 238/1000 | Loss: 0.00003830
Iteration 239/1000 | Loss: 0.00003830
Iteration 240/1000 | Loss: 0.00003830
Iteration 241/1000 | Loss: 0.00003830
Iteration 242/1000 | Loss: 0.00003830
Iteration 243/1000 | Loss: 0.00003830
Iteration 244/1000 | Loss: 0.00003830
Iteration 245/1000 | Loss: 0.00003830
Iteration 246/1000 | Loss: 0.00003830
Iteration 247/1000 | Loss: 0.00003830
Iteration 248/1000 | Loss: 0.00003830
Iteration 249/1000 | Loss: 0.00003830
Iteration 250/1000 | Loss: 0.00003830
Iteration 251/1000 | Loss: 0.00003830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [3.829763227258809e-05, 3.829763227258809e-05, 3.829763227258809e-05, 3.829763227258809e-05, 3.829763227258809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.829763227258809e-05

Optimization complete. Final v2v error: 3.1958115100860596 mm

Highest mean error: 11.13433837890625 mm for frame 126

Lowest mean error: 2.4961676597595215 mm for frame 85

Saving results

Total time: 192.53286504745483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043140
Iteration 2/25 | Loss: 0.01043140
Iteration 3/25 | Loss: 0.00236413
Iteration 4/25 | Loss: 0.00151226
Iteration 5/25 | Loss: 0.00132802
Iteration 6/25 | Loss: 0.00131891
Iteration 7/25 | Loss: 0.00133548
Iteration 8/25 | Loss: 0.00125400
Iteration 9/25 | Loss: 0.00108843
Iteration 10/25 | Loss: 0.00102871
Iteration 11/25 | Loss: 0.00095978
Iteration 12/25 | Loss: 0.00092459
Iteration 13/25 | Loss: 0.00091058
Iteration 14/25 | Loss: 0.00090468
Iteration 15/25 | Loss: 0.00090056
Iteration 16/25 | Loss: 0.00089910
Iteration 17/25 | Loss: 0.00089843
Iteration 18/25 | Loss: 0.00089823
Iteration 19/25 | Loss: 0.00089819
Iteration 20/25 | Loss: 0.00089819
Iteration 21/25 | Loss: 0.00089819
Iteration 22/25 | Loss: 0.00089818
Iteration 23/25 | Loss: 0.00089818
Iteration 24/25 | Loss: 0.00089818
Iteration 25/25 | Loss: 0.00089818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59597063
Iteration 2/25 | Loss: 0.00131509
Iteration 3/25 | Loss: 0.00131509
Iteration 4/25 | Loss: 0.00131509
Iteration 5/25 | Loss: 0.00131509
Iteration 6/25 | Loss: 0.00131509
Iteration 7/25 | Loss: 0.00131509
Iteration 8/25 | Loss: 0.00131509
Iteration 9/25 | Loss: 0.00131509
Iteration 10/25 | Loss: 0.00131509
Iteration 11/25 | Loss: 0.00131509
Iteration 12/25 | Loss: 0.00131509
Iteration 13/25 | Loss: 0.00131509
Iteration 14/25 | Loss: 0.00131509
Iteration 15/25 | Loss: 0.00131509
Iteration 16/25 | Loss: 0.00131509
Iteration 17/25 | Loss: 0.00131509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013150882441550493, 0.0013150882441550493, 0.0013150882441550493, 0.0013150882441550493, 0.0013150882441550493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013150882441550493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131509
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002349
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002225
Iteration 7/1000 | Loss: 0.00002200
Iteration 8/1000 | Loss: 0.00002184
Iteration 9/1000 | Loss: 0.00002162
Iteration 10/1000 | Loss: 0.00002133
Iteration 11/1000 | Loss: 0.00002127
Iteration 12/1000 | Loss: 0.00002115
Iteration 13/1000 | Loss: 0.00002115
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002115
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002113
Iteration 20/1000 | Loss: 0.00002113
Iteration 21/1000 | Loss: 0.00002113
Iteration 22/1000 | Loss: 0.00002113
Iteration 23/1000 | Loss: 0.00002113
Iteration 24/1000 | Loss: 0.00002113
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002113
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002112
Iteration 30/1000 | Loss: 0.00002112
Iteration 31/1000 | Loss: 0.00002112
Iteration 32/1000 | Loss: 0.00002112
Iteration 33/1000 | Loss: 0.00002111
Iteration 34/1000 | Loss: 0.00002111
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002110
Iteration 37/1000 | Loss: 0.00002109
Iteration 38/1000 | Loss: 0.00002109
Iteration 39/1000 | Loss: 0.00002109
Iteration 40/1000 | Loss: 0.00002108
Iteration 41/1000 | Loss: 0.00002108
Iteration 42/1000 | Loss: 0.00002108
Iteration 43/1000 | Loss: 0.00002108
Iteration 44/1000 | Loss: 0.00002108
Iteration 45/1000 | Loss: 0.00002108
Iteration 46/1000 | Loss: 0.00002107
Iteration 47/1000 | Loss: 0.00002105
Iteration 48/1000 | Loss: 0.00002104
Iteration 49/1000 | Loss: 0.00002104
Iteration 50/1000 | Loss: 0.00002103
Iteration 51/1000 | Loss: 0.00002103
Iteration 52/1000 | Loss: 0.00002098
Iteration 53/1000 | Loss: 0.00002098
Iteration 54/1000 | Loss: 0.00002098
Iteration 55/1000 | Loss: 0.00002097
Iteration 56/1000 | Loss: 0.00002097
Iteration 57/1000 | Loss: 0.00002097
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002095
Iteration 60/1000 | Loss: 0.00002095
Iteration 61/1000 | Loss: 0.00002095
Iteration 62/1000 | Loss: 0.00002095
Iteration 63/1000 | Loss: 0.00002094
Iteration 64/1000 | Loss: 0.00002094
Iteration 65/1000 | Loss: 0.00002094
Iteration 66/1000 | Loss: 0.00002094
Iteration 67/1000 | Loss: 0.00002094
Iteration 68/1000 | Loss: 0.00002094
Iteration 69/1000 | Loss: 0.00002094
Iteration 70/1000 | Loss: 0.00002094
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002094
Iteration 73/1000 | Loss: 0.00002094
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002093
Iteration 78/1000 | Loss: 0.00002093
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002093
Iteration 85/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.0933575797243975e-05, 2.0933575797243975e-05, 2.0933575797243975e-05, 2.0933575797243975e-05, 2.0933575797243975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0933575797243975e-05

Optimization complete. Final v2v error: 3.738929033279419 mm

Highest mean error: 3.784712076187134 mm for frame 53

Lowest mean error: 3.683972120285034 mm for frame 92

Saving results

Total time: 59.23460817337036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849714
Iteration 2/25 | Loss: 0.00128217
Iteration 3/25 | Loss: 0.00089803
Iteration 4/25 | Loss: 0.00085432
Iteration 5/25 | Loss: 0.00083774
Iteration 6/25 | Loss: 0.00084104
Iteration 7/25 | Loss: 0.00084445
Iteration 8/25 | Loss: 0.00083776
Iteration 9/25 | Loss: 0.00082997
Iteration 10/25 | Loss: 0.00082467
Iteration 11/25 | Loss: 0.00082171
Iteration 12/25 | Loss: 0.00082119
Iteration 13/25 | Loss: 0.00081868
Iteration 14/25 | Loss: 0.00081796
Iteration 15/25 | Loss: 0.00081930
Iteration 16/25 | Loss: 0.00081909
Iteration 17/25 | Loss: 0.00081913
Iteration 18/25 | Loss: 0.00081981
Iteration 19/25 | Loss: 0.00081987
Iteration 20/25 | Loss: 0.00081917
Iteration 21/25 | Loss: 0.00082008
Iteration 22/25 | Loss: 0.00081961
Iteration 23/25 | Loss: 0.00081956
Iteration 24/25 | Loss: 0.00081939
Iteration 25/25 | Loss: 0.00081930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74536848
Iteration 2/25 | Loss: 0.00133390
Iteration 3/25 | Loss: 0.00133389
Iteration 4/25 | Loss: 0.00133389
Iteration 5/25 | Loss: 0.00133389
Iteration 6/25 | Loss: 0.00133389
Iteration 7/25 | Loss: 0.00133389
Iteration 8/25 | Loss: 0.00133389
Iteration 9/25 | Loss: 0.00133389
Iteration 10/25 | Loss: 0.00133389
Iteration 11/25 | Loss: 0.00133389
Iteration 12/25 | Loss: 0.00133389
Iteration 13/25 | Loss: 0.00133389
Iteration 14/25 | Loss: 0.00133389
Iteration 15/25 | Loss: 0.00133389
Iteration 16/25 | Loss: 0.00133389
Iteration 17/25 | Loss: 0.00133389
Iteration 18/25 | Loss: 0.00133389
Iteration 19/25 | Loss: 0.00133389
Iteration 20/25 | Loss: 0.00133389
Iteration 21/25 | Loss: 0.00133389
Iteration 22/25 | Loss: 0.00133389
Iteration 23/25 | Loss: 0.00133389
Iteration 24/25 | Loss: 0.00133389
Iteration 25/25 | Loss: 0.00133389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133389
Iteration 2/1000 | Loss: 0.00004537
Iteration 3/1000 | Loss: 0.00003899
Iteration 4/1000 | Loss: 0.00003154
Iteration 5/1000 | Loss: 0.00028116
Iteration 6/1000 | Loss: 0.00003339
Iteration 7/1000 | Loss: 0.00004077
Iteration 8/1000 | Loss: 0.00004397
Iteration 9/1000 | Loss: 0.00003047
Iteration 10/1000 | Loss: 0.00002709
Iteration 11/1000 | Loss: 0.00003883
Iteration 12/1000 | Loss: 0.00002677
Iteration 13/1000 | Loss: 0.00004274
Iteration 14/1000 | Loss: 0.00003474
Iteration 15/1000 | Loss: 0.00003872
Iteration 16/1000 | Loss: 0.00002960
Iteration 17/1000 | Loss: 0.00004002
Iteration 18/1000 | Loss: 0.00004357
Iteration 19/1000 | Loss: 0.00003974
Iteration 20/1000 | Loss: 0.00004550
Iteration 21/1000 | Loss: 0.00004401
Iteration 22/1000 | Loss: 0.00004131
Iteration 23/1000 | Loss: 0.00005293
Iteration 24/1000 | Loss: 0.00002491
Iteration 25/1000 | Loss: 0.00002347
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002226
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002157
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00002141
Iteration 33/1000 | Loss: 0.00002134
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00002129
Iteration 38/1000 | Loss: 0.00002129
Iteration 39/1000 | Loss: 0.00002129
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002127
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002126
Iteration 47/1000 | Loss: 0.00002126
Iteration 48/1000 | Loss: 0.00002589
Iteration 49/1000 | Loss: 0.00002152
Iteration 50/1000 | Loss: 0.00002125
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002117
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00002117
Iteration 63/1000 | Loss: 0.00002117
Iteration 64/1000 | Loss: 0.00002117
Iteration 65/1000 | Loss: 0.00002117
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002116
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002361
Iteration 76/1000 | Loss: 0.00002117
Iteration 77/1000 | Loss: 0.00002117
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002116
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002113
Iteration 86/1000 | Loss: 0.00002113
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002110
Iteration 106/1000 | Loss: 0.00002110
Iteration 107/1000 | Loss: 0.00002110
Iteration 108/1000 | Loss: 0.00002110
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002109
Iteration 111/1000 | Loss: 0.00002109
Iteration 112/1000 | Loss: 0.00002109
Iteration 113/1000 | Loss: 0.00002109
Iteration 114/1000 | Loss: 0.00002109
Iteration 115/1000 | Loss: 0.00002109
Iteration 116/1000 | Loss: 0.00002109
Iteration 117/1000 | Loss: 0.00002109
Iteration 118/1000 | Loss: 0.00002109
Iteration 119/1000 | Loss: 0.00002109
Iteration 120/1000 | Loss: 0.00002109
Iteration 121/1000 | Loss: 0.00002108
Iteration 122/1000 | Loss: 0.00002108
Iteration 123/1000 | Loss: 0.00002108
Iteration 124/1000 | Loss: 0.00002108
Iteration 125/1000 | Loss: 0.00002108
Iteration 126/1000 | Loss: 0.00002108
Iteration 127/1000 | Loss: 0.00002108
Iteration 128/1000 | Loss: 0.00002107
Iteration 129/1000 | Loss: 0.00002107
Iteration 130/1000 | Loss: 0.00002107
Iteration 131/1000 | Loss: 0.00002107
Iteration 132/1000 | Loss: 0.00002107
Iteration 133/1000 | Loss: 0.00002107
Iteration 134/1000 | Loss: 0.00002107
Iteration 135/1000 | Loss: 0.00002107
Iteration 136/1000 | Loss: 0.00002107
Iteration 137/1000 | Loss: 0.00002107
Iteration 138/1000 | Loss: 0.00002107
Iteration 139/1000 | Loss: 0.00002107
Iteration 140/1000 | Loss: 0.00002107
Iteration 141/1000 | Loss: 0.00002107
Iteration 142/1000 | Loss: 0.00002107
Iteration 143/1000 | Loss: 0.00002107
Iteration 144/1000 | Loss: 0.00002107
Iteration 145/1000 | Loss: 0.00002107
Iteration 146/1000 | Loss: 0.00002107
Iteration 147/1000 | Loss: 0.00002107
Iteration 148/1000 | Loss: 0.00002107
Iteration 149/1000 | Loss: 0.00002107
Iteration 150/1000 | Loss: 0.00002107
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002107
Iteration 155/1000 | Loss: 0.00002107
Iteration 156/1000 | Loss: 0.00002107
Iteration 157/1000 | Loss: 0.00002107
Iteration 158/1000 | Loss: 0.00002107
Iteration 159/1000 | Loss: 0.00002107
Iteration 160/1000 | Loss: 0.00002107
Iteration 161/1000 | Loss: 0.00002107
Iteration 162/1000 | Loss: 0.00002107
Iteration 163/1000 | Loss: 0.00002107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.1065774490125477e-05, 2.1065774490125477e-05, 2.1065774490125477e-05, 2.1065774490125477e-05, 2.1065774490125477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1065774490125477e-05

Optimization complete. Final v2v error: 3.841031074523926 mm

Highest mean error: 4.98030948638916 mm for frame 110

Lowest mean error: 3.1290106773376465 mm for frame 71

Saving results

Total time: 109.68133878707886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100462
Iteration 2/25 | Loss: 0.00219681
Iteration 3/25 | Loss: 0.00154457
Iteration 4/25 | Loss: 0.00157850
Iteration 5/25 | Loss: 0.00133823
Iteration 6/25 | Loss: 0.00121184
Iteration 7/25 | Loss: 0.00124488
Iteration 8/25 | Loss: 0.00113676
Iteration 9/25 | Loss: 0.00108426
Iteration 10/25 | Loss: 0.00100972
Iteration 11/25 | Loss: 0.00098235
Iteration 12/25 | Loss: 0.00097096
Iteration 13/25 | Loss: 0.00097279
Iteration 14/25 | Loss: 0.00096716
Iteration 15/25 | Loss: 0.00097008
Iteration 16/25 | Loss: 0.00096503
Iteration 17/25 | Loss: 0.00096642
Iteration 18/25 | Loss: 0.00097163
Iteration 19/25 | Loss: 0.00096764
Iteration 20/25 | Loss: 0.00096540
Iteration 21/25 | Loss: 0.00096885
Iteration 22/25 | Loss: 0.00097242
Iteration 23/25 | Loss: 0.00096956
Iteration 24/25 | Loss: 0.00097169
Iteration 25/25 | Loss: 0.00096696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94956017
Iteration 2/25 | Loss: 0.00181970
Iteration 3/25 | Loss: 0.00181946
Iteration 4/25 | Loss: 0.00181945
Iteration 5/25 | Loss: 0.00181945
Iteration 6/25 | Loss: 0.00181945
Iteration 7/25 | Loss: 0.00181945
Iteration 8/25 | Loss: 0.00181945
Iteration 9/25 | Loss: 0.00181945
Iteration 10/25 | Loss: 0.00181945
Iteration 11/25 | Loss: 0.00181945
Iteration 12/25 | Loss: 0.00181945
Iteration 13/25 | Loss: 0.00181945
Iteration 14/25 | Loss: 0.00181945
Iteration 15/25 | Loss: 0.00181945
Iteration 16/25 | Loss: 0.00181945
Iteration 17/25 | Loss: 0.00181945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001819453202188015, 0.001819453202188015, 0.001819453202188015, 0.001819453202188015, 0.001819453202188015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001819453202188015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181945
Iteration 2/1000 | Loss: 0.00077624
Iteration 3/1000 | Loss: 0.00072002
Iteration 4/1000 | Loss: 0.00069916
Iteration 5/1000 | Loss: 0.01034039
Iteration 6/1000 | Loss: 0.00191425
Iteration 7/1000 | Loss: 0.00112550
Iteration 8/1000 | Loss: 0.00095808
Iteration 9/1000 | Loss: 0.00008322
Iteration 10/1000 | Loss: 0.00006765
Iteration 11/1000 | Loss: 0.00097618
Iteration 12/1000 | Loss: 0.00070996
Iteration 13/1000 | Loss: 0.00073373
Iteration 14/1000 | Loss: 0.00061589
Iteration 15/1000 | Loss: 0.00031286
Iteration 16/1000 | Loss: 0.00005691
Iteration 17/1000 | Loss: 0.00004909
Iteration 18/1000 | Loss: 0.00004678
Iteration 19/1000 | Loss: 0.00004519
Iteration 20/1000 | Loss: 0.00004435
Iteration 21/1000 | Loss: 0.00004316
Iteration 22/1000 | Loss: 0.00004245
Iteration 23/1000 | Loss: 0.00004184
Iteration 24/1000 | Loss: 0.00004136
Iteration 25/1000 | Loss: 0.00004109
Iteration 26/1000 | Loss: 0.00004088
Iteration 27/1000 | Loss: 0.00004063
Iteration 28/1000 | Loss: 0.00004028
Iteration 29/1000 | Loss: 0.00004010
Iteration 30/1000 | Loss: 0.00003994
Iteration 31/1000 | Loss: 0.00003985
Iteration 32/1000 | Loss: 0.00003982
Iteration 33/1000 | Loss: 0.00003979
Iteration 34/1000 | Loss: 0.00003973
Iteration 35/1000 | Loss: 0.00003970
Iteration 36/1000 | Loss: 0.00003969
Iteration 37/1000 | Loss: 0.00003969
Iteration 38/1000 | Loss: 0.00003969
Iteration 39/1000 | Loss: 0.00003968
Iteration 40/1000 | Loss: 0.00003968
Iteration 41/1000 | Loss: 0.00003968
Iteration 42/1000 | Loss: 0.00003967
Iteration 43/1000 | Loss: 0.00003967
Iteration 44/1000 | Loss: 0.00003966
Iteration 45/1000 | Loss: 0.00003966
Iteration 46/1000 | Loss: 0.00003966
Iteration 47/1000 | Loss: 0.00003965
Iteration 48/1000 | Loss: 0.00003965
Iteration 49/1000 | Loss: 0.00003965
Iteration 50/1000 | Loss: 0.00003964
Iteration 51/1000 | Loss: 0.00003964
Iteration 52/1000 | Loss: 0.00003963
Iteration 53/1000 | Loss: 0.00003963
Iteration 54/1000 | Loss: 0.00003963
Iteration 55/1000 | Loss: 0.00003962
Iteration 56/1000 | Loss: 0.00003962
Iteration 57/1000 | Loss: 0.00003962
Iteration 58/1000 | Loss: 0.00003961
Iteration 59/1000 | Loss: 0.00003961
Iteration 60/1000 | Loss: 0.00003960
Iteration 61/1000 | Loss: 0.00003954
Iteration 62/1000 | Loss: 0.00003949
Iteration 63/1000 | Loss: 0.00003949
Iteration 64/1000 | Loss: 0.00003949
Iteration 65/1000 | Loss: 0.00003949
Iteration 66/1000 | Loss: 0.00003949
Iteration 67/1000 | Loss: 0.00003949
Iteration 68/1000 | Loss: 0.00003949
Iteration 69/1000 | Loss: 0.00003949
Iteration 70/1000 | Loss: 0.00003948
Iteration 71/1000 | Loss: 0.00003948
Iteration 72/1000 | Loss: 0.00003948
Iteration 73/1000 | Loss: 0.00003948
Iteration 74/1000 | Loss: 0.00003946
Iteration 75/1000 | Loss: 0.00003945
Iteration 76/1000 | Loss: 0.00003945
Iteration 77/1000 | Loss: 0.00003945
Iteration 78/1000 | Loss: 0.00003945
Iteration 79/1000 | Loss: 0.00003945
Iteration 80/1000 | Loss: 0.00003945
Iteration 81/1000 | Loss: 0.00003945
Iteration 82/1000 | Loss: 0.00003944
Iteration 83/1000 | Loss: 0.00003944
Iteration 84/1000 | Loss: 0.00003943
Iteration 85/1000 | Loss: 0.00003943
Iteration 86/1000 | Loss: 0.00003943
Iteration 87/1000 | Loss: 0.00003942
Iteration 88/1000 | Loss: 0.00003942
Iteration 89/1000 | Loss: 0.00003941
Iteration 90/1000 | Loss: 0.00003941
Iteration 91/1000 | Loss: 0.00003940
Iteration 92/1000 | Loss: 0.00003940
Iteration 93/1000 | Loss: 0.00003939
Iteration 94/1000 | Loss: 0.00003939
Iteration 95/1000 | Loss: 0.00003939
Iteration 96/1000 | Loss: 0.00003938
Iteration 97/1000 | Loss: 0.00003938
Iteration 98/1000 | Loss: 0.00003938
Iteration 99/1000 | Loss: 0.00003937
Iteration 100/1000 | Loss: 0.00003936
Iteration 101/1000 | Loss: 0.00003934
Iteration 102/1000 | Loss: 0.00003931
Iteration 103/1000 | Loss: 0.00003929
Iteration 104/1000 | Loss: 0.00003928
Iteration 105/1000 | Loss: 0.00003928
Iteration 106/1000 | Loss: 0.00003928
Iteration 107/1000 | Loss: 0.00003928
Iteration 108/1000 | Loss: 0.00003928
Iteration 109/1000 | Loss: 0.00003928
Iteration 110/1000 | Loss: 0.00003928
Iteration 111/1000 | Loss: 0.00003927
Iteration 112/1000 | Loss: 0.00003927
Iteration 113/1000 | Loss: 0.00003926
Iteration 114/1000 | Loss: 0.00003926
Iteration 115/1000 | Loss: 0.00003926
Iteration 116/1000 | Loss: 0.00003925
Iteration 117/1000 | Loss: 0.00003925
Iteration 118/1000 | Loss: 0.00003925
Iteration 119/1000 | Loss: 0.00003925
Iteration 120/1000 | Loss: 0.00003925
Iteration 121/1000 | Loss: 0.00003924
Iteration 122/1000 | Loss: 0.00003924
Iteration 123/1000 | Loss: 0.00003924
Iteration 124/1000 | Loss: 0.00003924
Iteration 125/1000 | Loss: 0.00003924
Iteration 126/1000 | Loss: 0.00003924
Iteration 127/1000 | Loss: 0.00003924
Iteration 128/1000 | Loss: 0.00003924
Iteration 129/1000 | Loss: 0.00003924
Iteration 130/1000 | Loss: 0.00003924
Iteration 131/1000 | Loss: 0.00003924
Iteration 132/1000 | Loss: 0.00003924
Iteration 133/1000 | Loss: 0.00003924
Iteration 134/1000 | Loss: 0.00003924
Iteration 135/1000 | Loss: 0.00003924
Iteration 136/1000 | Loss: 0.00003924
Iteration 137/1000 | Loss: 0.00003924
Iteration 138/1000 | Loss: 0.00003924
Iteration 139/1000 | Loss: 0.00003924
Iteration 140/1000 | Loss: 0.00003924
Iteration 141/1000 | Loss: 0.00003924
Iteration 142/1000 | Loss: 0.00003924
Iteration 143/1000 | Loss: 0.00003924
Iteration 144/1000 | Loss: 0.00003924
Iteration 145/1000 | Loss: 0.00003924
Iteration 146/1000 | Loss: 0.00003924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.9238453609868884e-05, 3.9238453609868884e-05, 3.9238453609868884e-05, 3.9238453609868884e-05, 3.9238453609868884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9238453609868884e-05

Optimization complete. Final v2v error: 5.050094127655029 mm

Highest mean error: 6.563381671905518 mm for frame 87

Lowest mean error: 3.5749623775482178 mm for frame 124

Saving results

Total time: 101.08189058303833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013163
Iteration 2/25 | Loss: 0.00354153
Iteration 3/25 | Loss: 0.00207966
Iteration 4/25 | Loss: 0.00178514
Iteration 5/25 | Loss: 0.00165611
Iteration 6/25 | Loss: 0.00150106
Iteration 7/25 | Loss: 0.00138773
Iteration 8/25 | Loss: 0.00142852
Iteration 9/25 | Loss: 0.00135171
Iteration 10/25 | Loss: 0.00125089
Iteration 11/25 | Loss: 0.00117151
Iteration 12/25 | Loss: 0.00113746
Iteration 13/25 | Loss: 0.00111294
Iteration 14/25 | Loss: 0.00107846
Iteration 15/25 | Loss: 0.00107350
Iteration 16/25 | Loss: 0.00106117
Iteration 17/25 | Loss: 0.00103245
Iteration 18/25 | Loss: 0.00102185
Iteration 19/25 | Loss: 0.00101609
Iteration 20/25 | Loss: 0.00100227
Iteration 21/25 | Loss: 0.00099298
Iteration 22/25 | Loss: 0.00099528
Iteration 23/25 | Loss: 0.00099063
Iteration 24/25 | Loss: 0.00099280
Iteration 25/25 | Loss: 0.00099168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61673379
Iteration 2/25 | Loss: 0.00416718
Iteration 3/25 | Loss: 0.00380434
Iteration 4/25 | Loss: 0.00380434
Iteration 5/25 | Loss: 0.00380434
Iteration 6/25 | Loss: 0.00380434
Iteration 7/25 | Loss: 0.00380434
Iteration 8/25 | Loss: 0.00380434
Iteration 9/25 | Loss: 0.00380434
Iteration 10/25 | Loss: 0.00380434
Iteration 11/25 | Loss: 0.00380434
Iteration 12/25 | Loss: 0.00380434
Iteration 13/25 | Loss: 0.00380434
Iteration 14/25 | Loss: 0.00380434
Iteration 15/25 | Loss: 0.00380434
Iteration 16/25 | Loss: 0.00380434
Iteration 17/25 | Loss: 0.00380434
Iteration 18/25 | Loss: 0.00380434
Iteration 19/25 | Loss: 0.00380434
Iteration 20/25 | Loss: 0.00380434
Iteration 21/25 | Loss: 0.00380434
Iteration 22/25 | Loss: 0.00380434
Iteration 23/25 | Loss: 0.00380434
Iteration 24/25 | Loss: 0.00380434
Iteration 25/25 | Loss: 0.00380434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00380434
Iteration 2/1000 | Loss: 0.00214412
Iteration 3/1000 | Loss: 0.00062221
Iteration 4/1000 | Loss: 0.00039117
Iteration 5/1000 | Loss: 0.00059606
Iteration 6/1000 | Loss: 0.00140748
Iteration 7/1000 | Loss: 0.00031396
Iteration 8/1000 | Loss: 0.00042997
Iteration 9/1000 | Loss: 0.00048690
Iteration 10/1000 | Loss: 0.00133945
Iteration 11/1000 | Loss: 0.00025042
Iteration 12/1000 | Loss: 0.00069881
Iteration 13/1000 | Loss: 0.00054268
Iteration 14/1000 | Loss: 0.00030305
Iteration 15/1000 | Loss: 0.00019659
Iteration 16/1000 | Loss: 0.00064330
Iteration 17/1000 | Loss: 0.00024756
Iteration 18/1000 | Loss: 0.00101935
Iteration 19/1000 | Loss: 0.00041026
Iteration 20/1000 | Loss: 0.00066783
Iteration 21/1000 | Loss: 0.00052102
Iteration 22/1000 | Loss: 0.00062882
Iteration 23/1000 | Loss: 0.00097296
Iteration 24/1000 | Loss: 0.00022680
Iteration 25/1000 | Loss: 0.00048668
Iteration 26/1000 | Loss: 0.00020710
Iteration 27/1000 | Loss: 0.00012591
Iteration 28/1000 | Loss: 0.00037622
Iteration 29/1000 | Loss: 0.00017237
Iteration 30/1000 | Loss: 0.00022787
Iteration 31/1000 | Loss: 0.00056793
Iteration 32/1000 | Loss: 0.00020597
Iteration 33/1000 | Loss: 0.00012541
Iteration 34/1000 | Loss: 0.00009429
Iteration 35/1000 | Loss: 0.00008885
Iteration 36/1000 | Loss: 0.00022024
Iteration 37/1000 | Loss: 0.00020496
Iteration 38/1000 | Loss: 0.00020458
Iteration 39/1000 | Loss: 0.00034091
Iteration 40/1000 | Loss: 0.00029973
Iteration 41/1000 | Loss: 0.00020204
Iteration 42/1000 | Loss: 0.00021784
Iteration 43/1000 | Loss: 0.00008725
Iteration 44/1000 | Loss: 0.00012562
Iteration 45/1000 | Loss: 0.00022698
Iteration 46/1000 | Loss: 0.00032618
Iteration 47/1000 | Loss: 0.00016830
Iteration 48/1000 | Loss: 0.00016097
Iteration 49/1000 | Loss: 0.00005638
Iteration 50/1000 | Loss: 0.00009446
Iteration 51/1000 | Loss: 0.00013734
Iteration 52/1000 | Loss: 0.00011184
Iteration 53/1000 | Loss: 0.00007820
Iteration 54/1000 | Loss: 0.00009683
Iteration 55/1000 | Loss: 0.00009730
Iteration 56/1000 | Loss: 0.00011523
Iteration 57/1000 | Loss: 0.00015437
Iteration 58/1000 | Loss: 0.00010474
Iteration 59/1000 | Loss: 0.00015328
Iteration 60/1000 | Loss: 0.00014670
Iteration 61/1000 | Loss: 0.00021779
Iteration 62/1000 | Loss: 0.00035678
Iteration 63/1000 | Loss: 0.00015000
Iteration 64/1000 | Loss: 0.00010623
Iteration 65/1000 | Loss: 0.00011954
Iteration 66/1000 | Loss: 0.00013306
Iteration 67/1000 | Loss: 0.00011413
Iteration 68/1000 | Loss: 0.00011177
Iteration 69/1000 | Loss: 0.00011476
Iteration 70/1000 | Loss: 0.00014385
Iteration 71/1000 | Loss: 0.00011564
Iteration 72/1000 | Loss: 0.00013188
Iteration 73/1000 | Loss: 0.00011968
Iteration 74/1000 | Loss: 0.00012351
Iteration 75/1000 | Loss: 0.00011565
Iteration 76/1000 | Loss: 0.00011285
Iteration 77/1000 | Loss: 0.00012410
Iteration 78/1000 | Loss: 0.00011263
Iteration 79/1000 | Loss: 0.00011723
Iteration 80/1000 | Loss: 0.00015031
Iteration 81/1000 | Loss: 0.00028233
Iteration 82/1000 | Loss: 0.00011605
Iteration 83/1000 | Loss: 0.00010408
Iteration 84/1000 | Loss: 0.00012542
Iteration 85/1000 | Loss: 0.00010917
Iteration 86/1000 | Loss: 0.00011605
Iteration 87/1000 | Loss: 0.00011298
Iteration 88/1000 | Loss: 0.00014008
Iteration 89/1000 | Loss: 0.00006809
Iteration 90/1000 | Loss: 0.00009023
Iteration 91/1000 | Loss: 0.00013295
Iteration 92/1000 | Loss: 0.00009238
Iteration 93/1000 | Loss: 0.00008257
Iteration 94/1000 | Loss: 0.00009697
Iteration 95/1000 | Loss: 0.00011482
Iteration 96/1000 | Loss: 0.00011470
Iteration 97/1000 | Loss: 0.00064551
Iteration 98/1000 | Loss: 0.00027817
Iteration 99/1000 | Loss: 0.00059816
Iteration 100/1000 | Loss: 0.00010352
Iteration 101/1000 | Loss: 0.00031867
Iteration 102/1000 | Loss: 0.00009186
Iteration 103/1000 | Loss: 0.00012331
Iteration 104/1000 | Loss: 0.00011067
Iteration 105/1000 | Loss: 0.00039999
Iteration 106/1000 | Loss: 0.00014123
Iteration 107/1000 | Loss: 0.00018915
Iteration 108/1000 | Loss: 0.00011779
Iteration 109/1000 | Loss: 0.00010038
Iteration 110/1000 | Loss: 0.00011725
Iteration 111/1000 | Loss: 0.00008885
Iteration 112/1000 | Loss: 0.00010992
Iteration 113/1000 | Loss: 0.00008907
Iteration 114/1000 | Loss: 0.00011305
Iteration 115/1000 | Loss: 0.00016235
Iteration 116/1000 | Loss: 0.00006354
Iteration 117/1000 | Loss: 0.00016616
Iteration 118/1000 | Loss: 0.00004640
Iteration 119/1000 | Loss: 0.00009237
Iteration 120/1000 | Loss: 0.00004268
Iteration 121/1000 | Loss: 0.00076535
Iteration 122/1000 | Loss: 0.00007966
Iteration 123/1000 | Loss: 0.00004662
Iteration 124/1000 | Loss: 0.00009944
Iteration 125/1000 | Loss: 0.00004930
Iteration 126/1000 | Loss: 0.00051747
Iteration 127/1000 | Loss: 0.00008350
Iteration 128/1000 | Loss: 0.00010202
Iteration 129/1000 | Loss: 0.00003374
Iteration 130/1000 | Loss: 0.00004304
Iteration 131/1000 | Loss: 0.00003182
Iteration 132/1000 | Loss: 0.00051538
Iteration 133/1000 | Loss: 0.00007076
Iteration 134/1000 | Loss: 0.00003737
Iteration 135/1000 | Loss: 0.00030858
Iteration 136/1000 | Loss: 0.00004155
Iteration 137/1000 | Loss: 0.00003318
Iteration 138/1000 | Loss: 0.00006552
Iteration 139/1000 | Loss: 0.00003116
Iteration 140/1000 | Loss: 0.00003936
Iteration 141/1000 | Loss: 0.00002738
Iteration 142/1000 | Loss: 0.00002694
Iteration 143/1000 | Loss: 0.00003623
Iteration 144/1000 | Loss: 0.00002927
Iteration 145/1000 | Loss: 0.00002609
Iteration 146/1000 | Loss: 0.00002582
Iteration 147/1000 | Loss: 0.00002565
Iteration 148/1000 | Loss: 0.00003801
Iteration 149/1000 | Loss: 0.00002546
Iteration 150/1000 | Loss: 0.00002539
Iteration 151/1000 | Loss: 0.00002534
Iteration 152/1000 | Loss: 0.00002532
Iteration 153/1000 | Loss: 0.00005375
Iteration 154/1000 | Loss: 0.00004168
Iteration 155/1000 | Loss: 0.00002519
Iteration 156/1000 | Loss: 0.00002518
Iteration 157/1000 | Loss: 0.00002518
Iteration 158/1000 | Loss: 0.00002518
Iteration 159/1000 | Loss: 0.00002517
Iteration 160/1000 | Loss: 0.00002517
Iteration 161/1000 | Loss: 0.00002517
Iteration 162/1000 | Loss: 0.00002517
Iteration 163/1000 | Loss: 0.00002516
Iteration 164/1000 | Loss: 0.00002516
Iteration 165/1000 | Loss: 0.00002516
Iteration 166/1000 | Loss: 0.00002516
Iteration 167/1000 | Loss: 0.00002516
Iteration 168/1000 | Loss: 0.00002516
Iteration 169/1000 | Loss: 0.00002516
Iteration 170/1000 | Loss: 0.00002516
Iteration 171/1000 | Loss: 0.00002515
Iteration 172/1000 | Loss: 0.00002515
Iteration 173/1000 | Loss: 0.00002515
Iteration 174/1000 | Loss: 0.00002515
Iteration 175/1000 | Loss: 0.00002515
Iteration 176/1000 | Loss: 0.00002515
Iteration 177/1000 | Loss: 0.00002515
Iteration 178/1000 | Loss: 0.00002514
Iteration 179/1000 | Loss: 0.00002514
Iteration 180/1000 | Loss: 0.00002513
Iteration 181/1000 | Loss: 0.00002512
Iteration 182/1000 | Loss: 0.00002511
Iteration 183/1000 | Loss: 0.00002511
Iteration 184/1000 | Loss: 0.00002511
Iteration 185/1000 | Loss: 0.00002510
Iteration 186/1000 | Loss: 0.00002510
Iteration 187/1000 | Loss: 0.00002509
Iteration 188/1000 | Loss: 0.00002509
Iteration 189/1000 | Loss: 0.00003600
Iteration 190/1000 | Loss: 0.00003864
Iteration 191/1000 | Loss: 0.00002509
Iteration 192/1000 | Loss: 0.00002503
Iteration 193/1000 | Loss: 0.00002503
Iteration 194/1000 | Loss: 0.00002503
Iteration 195/1000 | Loss: 0.00002503
Iteration 196/1000 | Loss: 0.00002503
Iteration 197/1000 | Loss: 0.00002503
Iteration 198/1000 | Loss: 0.00002503
Iteration 199/1000 | Loss: 0.00002503
Iteration 200/1000 | Loss: 0.00002503
Iteration 201/1000 | Loss: 0.00002503
Iteration 202/1000 | Loss: 0.00002502
Iteration 203/1000 | Loss: 0.00002502
Iteration 204/1000 | Loss: 0.00002502
Iteration 205/1000 | Loss: 0.00003560
Iteration 206/1000 | Loss: 0.00002503
Iteration 207/1000 | Loss: 0.00002502
Iteration 208/1000 | Loss: 0.00002502
Iteration 209/1000 | Loss: 0.00002502
Iteration 210/1000 | Loss: 0.00002502
Iteration 211/1000 | Loss: 0.00002502
Iteration 212/1000 | Loss: 0.00002502
Iteration 213/1000 | Loss: 0.00002502
Iteration 214/1000 | Loss: 0.00002502
Iteration 215/1000 | Loss: 0.00002502
Iteration 216/1000 | Loss: 0.00002501
Iteration 217/1000 | Loss: 0.00002501
Iteration 218/1000 | Loss: 0.00002501
Iteration 219/1000 | Loss: 0.00002501
Iteration 220/1000 | Loss: 0.00002501
Iteration 221/1000 | Loss: 0.00002501
Iteration 222/1000 | Loss: 0.00002501
Iteration 223/1000 | Loss: 0.00003656
Iteration 224/1000 | Loss: 0.00002817
Iteration 225/1000 | Loss: 0.00002891
Iteration 226/1000 | Loss: 0.00004514
Iteration 227/1000 | Loss: 0.00002745
Iteration 228/1000 | Loss: 0.00002500
Iteration 229/1000 | Loss: 0.00002500
Iteration 230/1000 | Loss: 0.00002500
Iteration 231/1000 | Loss: 0.00002500
Iteration 232/1000 | Loss: 0.00002500
Iteration 233/1000 | Loss: 0.00002499
Iteration 234/1000 | Loss: 0.00002499
Iteration 235/1000 | Loss: 0.00002499
Iteration 236/1000 | Loss: 0.00002499
Iteration 237/1000 | Loss: 0.00002499
Iteration 238/1000 | Loss: 0.00002498
Iteration 239/1000 | Loss: 0.00002498
Iteration 240/1000 | Loss: 0.00002498
Iteration 241/1000 | Loss: 0.00002498
Iteration 242/1000 | Loss: 0.00002498
Iteration 243/1000 | Loss: 0.00002498
Iteration 244/1000 | Loss: 0.00002498
Iteration 245/1000 | Loss: 0.00002498
Iteration 246/1000 | Loss: 0.00002498
Iteration 247/1000 | Loss: 0.00002498
Iteration 248/1000 | Loss: 0.00002498
Iteration 249/1000 | Loss: 0.00002498
Iteration 250/1000 | Loss: 0.00002497
Iteration 251/1000 | Loss: 0.00002497
Iteration 252/1000 | Loss: 0.00002497
Iteration 253/1000 | Loss: 0.00002497
Iteration 254/1000 | Loss: 0.00002497
Iteration 255/1000 | Loss: 0.00002497
Iteration 256/1000 | Loss: 0.00002497
Iteration 257/1000 | Loss: 0.00002497
Iteration 258/1000 | Loss: 0.00002497
Iteration 259/1000 | Loss: 0.00002497
Iteration 260/1000 | Loss: 0.00002496
Iteration 261/1000 | Loss: 0.00002496
Iteration 262/1000 | Loss: 0.00002496
Iteration 263/1000 | Loss: 0.00002496
Iteration 264/1000 | Loss: 0.00002496
Iteration 265/1000 | Loss: 0.00002496
Iteration 266/1000 | Loss: 0.00002496
Iteration 267/1000 | Loss: 0.00002495
Iteration 268/1000 | Loss: 0.00002495
Iteration 269/1000 | Loss: 0.00002495
Iteration 270/1000 | Loss: 0.00002495
Iteration 271/1000 | Loss: 0.00002495
Iteration 272/1000 | Loss: 0.00002495
Iteration 273/1000 | Loss: 0.00002495
Iteration 274/1000 | Loss: 0.00002495
Iteration 275/1000 | Loss: 0.00002495
Iteration 276/1000 | Loss: 0.00002494
Iteration 277/1000 | Loss: 0.00002494
Iteration 278/1000 | Loss: 0.00002494
Iteration 279/1000 | Loss: 0.00002494
Iteration 280/1000 | Loss: 0.00002494
Iteration 281/1000 | Loss: 0.00002494
Iteration 282/1000 | Loss: 0.00002494
Iteration 283/1000 | Loss: 0.00002494
Iteration 284/1000 | Loss: 0.00002494
Iteration 285/1000 | Loss: 0.00002494
Iteration 286/1000 | Loss: 0.00002494
Iteration 287/1000 | Loss: 0.00002494
Iteration 288/1000 | Loss: 0.00002493
Iteration 289/1000 | Loss: 0.00002493
Iteration 290/1000 | Loss: 0.00002493
Iteration 291/1000 | Loss: 0.00002493
Iteration 292/1000 | Loss: 0.00002493
Iteration 293/1000 | Loss: 0.00002492
Iteration 294/1000 | Loss: 0.00002492
Iteration 295/1000 | Loss: 0.00002492
Iteration 296/1000 | Loss: 0.00003318
Iteration 297/1000 | Loss: 0.00002505
Iteration 298/1000 | Loss: 0.00006129
Iteration 299/1000 | Loss: 0.00004875
Iteration 300/1000 | Loss: 0.00002550
Iteration 301/1000 | Loss: 0.00002489
Iteration 302/1000 | Loss: 0.00002487
Iteration 303/1000 | Loss: 0.00002487
Iteration 304/1000 | Loss: 0.00002487
Iteration 305/1000 | Loss: 0.00002487
Iteration 306/1000 | Loss: 0.00002486
Iteration 307/1000 | Loss: 0.00039645
Iteration 308/1000 | Loss: 0.00096034
Iteration 309/1000 | Loss: 0.00021742
Iteration 310/1000 | Loss: 0.00007114
Iteration 311/1000 | Loss: 0.00003038
Iteration 312/1000 | Loss: 0.00002959
Iteration 313/1000 | Loss: 0.00002126
Iteration 314/1000 | Loss: 0.00005861
Iteration 315/1000 | Loss: 0.00003904
Iteration 316/1000 | Loss: 0.00006453
Iteration 317/1000 | Loss: 0.00003448
Iteration 318/1000 | Loss: 0.00002954
Iteration 319/1000 | Loss: 0.00004632
Iteration 320/1000 | Loss: 0.00002064
Iteration 321/1000 | Loss: 0.00001741
Iteration 322/1000 | Loss: 0.00001927
Iteration 323/1000 | Loss: 0.00001662
Iteration 324/1000 | Loss: 0.00001645
Iteration 325/1000 | Loss: 0.00001644
Iteration 326/1000 | Loss: 0.00001642
Iteration 327/1000 | Loss: 0.00001641
Iteration 328/1000 | Loss: 0.00001640
Iteration 329/1000 | Loss: 0.00001638
Iteration 330/1000 | Loss: 0.00001638
Iteration 331/1000 | Loss: 0.00001637
Iteration 332/1000 | Loss: 0.00001634
Iteration 333/1000 | Loss: 0.00001633
Iteration 334/1000 | Loss: 0.00001632
Iteration 335/1000 | Loss: 0.00002280
Iteration 336/1000 | Loss: 0.00001623
Iteration 337/1000 | Loss: 0.00001623
Iteration 338/1000 | Loss: 0.00001623
Iteration 339/1000 | Loss: 0.00001623
Iteration 340/1000 | Loss: 0.00001623
Iteration 341/1000 | Loss: 0.00001623
Iteration 342/1000 | Loss: 0.00001623
Iteration 343/1000 | Loss: 0.00001623
Iteration 344/1000 | Loss: 0.00001623
Iteration 345/1000 | Loss: 0.00001623
Iteration 346/1000 | Loss: 0.00001623
Iteration 347/1000 | Loss: 0.00001622
Iteration 348/1000 | Loss: 0.00001622
Iteration 349/1000 | Loss: 0.00001622
Iteration 350/1000 | Loss: 0.00001622
Iteration 351/1000 | Loss: 0.00001622
Iteration 352/1000 | Loss: 0.00001622
Iteration 353/1000 | Loss: 0.00001622
Iteration 354/1000 | Loss: 0.00001622
Iteration 355/1000 | Loss: 0.00001622
Iteration 356/1000 | Loss: 0.00001622
Iteration 357/1000 | Loss: 0.00001621
Iteration 358/1000 | Loss: 0.00001621
Iteration 359/1000 | Loss: 0.00001621
Iteration 360/1000 | Loss: 0.00001621
Iteration 361/1000 | Loss: 0.00001620
Iteration 362/1000 | Loss: 0.00001620
Iteration 363/1000 | Loss: 0.00001620
Iteration 364/1000 | Loss: 0.00001619
Iteration 365/1000 | Loss: 0.00001619
Iteration 366/1000 | Loss: 0.00001618
Iteration 367/1000 | Loss: 0.00001618
Iteration 368/1000 | Loss: 0.00001618
Iteration 369/1000 | Loss: 0.00001617
Iteration 370/1000 | Loss: 0.00001617
Iteration 371/1000 | Loss: 0.00001617
Iteration 372/1000 | Loss: 0.00001617
Iteration 373/1000 | Loss: 0.00001617
Iteration 374/1000 | Loss: 0.00001617
Iteration 375/1000 | Loss: 0.00001617
Iteration 376/1000 | Loss: 0.00001617
Iteration 377/1000 | Loss: 0.00001616
Iteration 378/1000 | Loss: 0.00001616
Iteration 379/1000 | Loss: 0.00001616
Iteration 380/1000 | Loss: 0.00001616
Iteration 381/1000 | Loss: 0.00001616
Iteration 382/1000 | Loss: 0.00003420
Iteration 383/1000 | Loss: 0.00002694
Iteration 384/1000 | Loss: 0.00001622
Iteration 385/1000 | Loss: 0.00001616
Iteration 386/1000 | Loss: 0.00001616
Iteration 387/1000 | Loss: 0.00001616
Iteration 388/1000 | Loss: 0.00001616
Iteration 389/1000 | Loss: 0.00001615
Iteration 390/1000 | Loss: 0.00002292
Iteration 391/1000 | Loss: 0.00001808
Iteration 392/1000 | Loss: 0.00001615
Iteration 393/1000 | Loss: 0.00001615
Iteration 394/1000 | Loss: 0.00001615
Iteration 395/1000 | Loss: 0.00001752
Iteration 396/1000 | Loss: 0.00001612
Iteration 397/1000 | Loss: 0.00001612
Iteration 398/1000 | Loss: 0.00001612
Iteration 399/1000 | Loss: 0.00001612
Iteration 400/1000 | Loss: 0.00001612
Iteration 401/1000 | Loss: 0.00001612
Iteration 402/1000 | Loss: 0.00001612
Iteration 403/1000 | Loss: 0.00001611
Iteration 404/1000 | Loss: 0.00001611
Iteration 405/1000 | Loss: 0.00001611
Iteration 406/1000 | Loss: 0.00001611
Iteration 407/1000 | Loss: 0.00001611
Iteration 408/1000 | Loss: 0.00001611
Iteration 409/1000 | Loss: 0.00001611
Iteration 410/1000 | Loss: 0.00001611
Iteration 411/1000 | Loss: 0.00001611
Iteration 412/1000 | Loss: 0.00001611
Iteration 413/1000 | Loss: 0.00001611
Iteration 414/1000 | Loss: 0.00001610
Iteration 415/1000 | Loss: 0.00001610
Iteration 416/1000 | Loss: 0.00001610
Iteration 417/1000 | Loss: 0.00001610
Iteration 418/1000 | Loss: 0.00001610
Iteration 419/1000 | Loss: 0.00001610
Iteration 420/1000 | Loss: 0.00001610
Iteration 421/1000 | Loss: 0.00001610
Iteration 422/1000 | Loss: 0.00001610
Iteration 423/1000 | Loss: 0.00001610
Iteration 424/1000 | Loss: 0.00001610
Iteration 425/1000 | Loss: 0.00001610
Iteration 426/1000 | Loss: 0.00001610
Iteration 427/1000 | Loss: 0.00001610
Iteration 428/1000 | Loss: 0.00001610
Iteration 429/1000 | Loss: 0.00001610
Iteration 430/1000 | Loss: 0.00001610
Iteration 431/1000 | Loss: 0.00001610
Iteration 432/1000 | Loss: 0.00001610
Iteration 433/1000 | Loss: 0.00001610
Iteration 434/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 434. Stopping optimization.
Last 5 losses: [1.610489925951697e-05, 1.610489925951697e-05, 1.610489925951697e-05, 1.610489925951697e-05, 1.610489925951697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.610489925951697e-05

Optimization complete. Final v2v error: 3.1630821228027344 mm

Highest mean error: 11.549960136413574 mm for frame 15

Lowest mean error: 2.7949492931365967 mm for frame 68

Saving results

Total time: 372.0620255470276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820291
Iteration 2/25 | Loss: 0.00085658
Iteration 3/25 | Loss: 0.00076309
Iteration 4/25 | Loss: 0.00074329
Iteration 5/25 | Loss: 0.00074052
Iteration 6/25 | Loss: 0.00073943
Iteration 7/25 | Loss: 0.00073932
Iteration 8/25 | Loss: 0.00073932
Iteration 9/25 | Loss: 0.00073932
Iteration 10/25 | Loss: 0.00073932
Iteration 11/25 | Loss: 0.00073932
Iteration 12/25 | Loss: 0.00073932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007393186679109931, 0.0007393186679109931, 0.0007393186679109931, 0.0007393186679109931, 0.0007393186679109931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007393186679109931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63605750
Iteration 2/25 | Loss: 0.00129764
Iteration 3/25 | Loss: 0.00129764
Iteration 4/25 | Loss: 0.00129764
Iteration 5/25 | Loss: 0.00129764
Iteration 6/25 | Loss: 0.00129764
Iteration 7/25 | Loss: 0.00129764
Iteration 8/25 | Loss: 0.00129764
Iteration 9/25 | Loss: 0.00129764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012976363068446517, 0.0012976363068446517, 0.0012976363068446517, 0.0012976363068446517, 0.0012976363068446517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012976363068446517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129764
Iteration 2/1000 | Loss: 0.00002448
Iteration 3/1000 | Loss: 0.00001546
Iteration 4/1000 | Loss: 0.00001446
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001314
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001253
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001234
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001233
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001224
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.2244073332112748e-05, 1.2244073332112748e-05, 1.2244073332112748e-05, 1.2244073332112748e-05, 1.2244073332112748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2244073332112748e-05

Optimization complete. Final v2v error: 2.9956655502319336 mm

Highest mean error: 3.1601953506469727 mm for frame 116

Lowest mean error: 2.86970591545105 mm for frame 44

Saving results

Total time: 28.391194105148315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402231
Iteration 2/25 | Loss: 0.00087068
Iteration 3/25 | Loss: 0.00075955
Iteration 4/25 | Loss: 0.00074020
Iteration 5/25 | Loss: 0.00073586
Iteration 6/25 | Loss: 0.00073461
Iteration 7/25 | Loss: 0.00073454
Iteration 8/25 | Loss: 0.00073454
Iteration 9/25 | Loss: 0.00073454
Iteration 10/25 | Loss: 0.00073454
Iteration 11/25 | Loss: 0.00073454
Iteration 12/25 | Loss: 0.00073454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007345362682826817, 0.0007345362682826817, 0.0007345362682826817, 0.0007345362682826817, 0.0007345362682826817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007345362682826817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96872830
Iteration 2/25 | Loss: 0.00121145
Iteration 3/25 | Loss: 0.00121145
Iteration 4/25 | Loss: 0.00121145
Iteration 5/25 | Loss: 0.00121145
Iteration 6/25 | Loss: 0.00121145
Iteration 7/25 | Loss: 0.00121145
Iteration 8/25 | Loss: 0.00121144
Iteration 9/25 | Loss: 0.00121144
Iteration 10/25 | Loss: 0.00121144
Iteration 11/25 | Loss: 0.00121144
Iteration 12/25 | Loss: 0.00121144
Iteration 13/25 | Loss: 0.00121144
Iteration 14/25 | Loss: 0.00121144
Iteration 15/25 | Loss: 0.00121144
Iteration 16/25 | Loss: 0.00121144
Iteration 17/25 | Loss: 0.00121144
Iteration 18/25 | Loss: 0.00121144
Iteration 19/25 | Loss: 0.00121144
Iteration 20/25 | Loss: 0.00121144
Iteration 21/25 | Loss: 0.00121144
Iteration 22/25 | Loss: 0.00121144
Iteration 23/25 | Loss: 0.00121144
Iteration 24/25 | Loss: 0.00121144
Iteration 25/25 | Loss: 0.00121144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121144
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001643
Iteration 5/1000 | Loss: 0.00001546
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001400
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001389
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001380
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001378
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001377
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001368
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001364
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001364
Iteration 80/1000 | Loss: 0.00001364
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001363
Iteration 84/1000 | Loss: 0.00001363
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001360
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001357
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.356783013761742e-05, 1.356783013761742e-05, 1.356783013761742e-05, 1.356783013761742e-05, 1.356783013761742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.356783013761742e-05

Optimization complete. Final v2v error: 3.143882989883423 mm

Highest mean error: 3.4515457153320312 mm for frame 124

Lowest mean error: 2.9963150024414062 mm for frame 148

Saving results

Total time: 35.47727942466736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972271
Iteration 2/25 | Loss: 0.00167490
Iteration 3/25 | Loss: 0.00121633
Iteration 4/25 | Loss: 0.00111357
Iteration 5/25 | Loss: 0.00107550
Iteration 6/25 | Loss: 0.00107148
Iteration 7/25 | Loss: 0.00106559
Iteration 8/25 | Loss: 0.00105559
Iteration 9/25 | Loss: 0.00105104
Iteration 10/25 | Loss: 0.00104809
Iteration 11/25 | Loss: 0.00104728
Iteration 12/25 | Loss: 0.00104639
Iteration 13/25 | Loss: 0.00104955
Iteration 14/25 | Loss: 0.00104836
Iteration 15/25 | Loss: 0.00104675
Iteration 16/25 | Loss: 0.00104385
Iteration 17/25 | Loss: 0.00104272
Iteration 18/25 | Loss: 0.00104165
Iteration 19/25 | Loss: 0.00104069
Iteration 20/25 | Loss: 0.00104437
Iteration 21/25 | Loss: 0.00104198
Iteration 22/25 | Loss: 0.00103908
Iteration 23/25 | Loss: 0.00103793
Iteration 24/25 | Loss: 0.00103692
Iteration 25/25 | Loss: 0.00103673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65670359
Iteration 2/25 | Loss: 0.00410784
Iteration 3/25 | Loss: 0.00410782
Iteration 4/25 | Loss: 0.00410782
Iteration 5/25 | Loss: 0.00410781
Iteration 6/25 | Loss: 0.00410781
Iteration 7/25 | Loss: 0.00410781
Iteration 8/25 | Loss: 0.00410781
Iteration 9/25 | Loss: 0.00410781
Iteration 10/25 | Loss: 0.00410781
Iteration 11/25 | Loss: 0.00410781
Iteration 12/25 | Loss: 0.00410781
Iteration 13/25 | Loss: 0.00410781
Iteration 14/25 | Loss: 0.00410781
Iteration 15/25 | Loss: 0.00410781
Iteration 16/25 | Loss: 0.00410781
Iteration 17/25 | Loss: 0.00410781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0041078124195337296, 0.0041078124195337296, 0.0041078124195337296, 0.0041078124195337296, 0.0041078124195337296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0041078124195337296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00410781
Iteration 2/1000 | Loss: 0.00026744
Iteration 3/1000 | Loss: 0.00019376
Iteration 4/1000 | Loss: 0.00016453
Iteration 5/1000 | Loss: 0.00015452
Iteration 6/1000 | Loss: 0.00014731
Iteration 7/1000 | Loss: 0.00014244
Iteration 8/1000 | Loss: 0.00013809
Iteration 9/1000 | Loss: 0.00269554
Iteration 10/1000 | Loss: 0.02142344
Iteration 11/1000 | Loss: 0.00016212
Iteration 12/1000 | Loss: 0.00010832
Iteration 13/1000 | Loss: 0.00008552
Iteration 14/1000 | Loss: 0.00006125
Iteration 15/1000 | Loss: 0.00004655
Iteration 16/1000 | Loss: 0.00004072
Iteration 17/1000 | Loss: 0.00003663
Iteration 18/1000 | Loss: 0.00003345
Iteration 19/1000 | Loss: 0.00003076
Iteration 20/1000 | Loss: 0.00002888
Iteration 21/1000 | Loss: 0.00002743
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002524
Iteration 24/1000 | Loss: 0.00002451
Iteration 25/1000 | Loss: 0.00002379
Iteration 26/1000 | Loss: 0.00002347
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002289
Iteration 29/1000 | Loss: 0.00002284
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002255
Iteration 32/1000 | Loss: 0.00002253
Iteration 33/1000 | Loss: 0.00002240
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002211
Iteration 40/1000 | Loss: 0.00002209
Iteration 41/1000 | Loss: 0.00002209
Iteration 42/1000 | Loss: 0.00002208
Iteration 43/1000 | Loss: 0.00002208
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002207
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002206
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00002205
Iteration 51/1000 | Loss: 0.00002204
Iteration 52/1000 | Loss: 0.00002204
Iteration 53/1000 | Loss: 0.00002204
Iteration 54/1000 | Loss: 0.00002204
Iteration 55/1000 | Loss: 0.00002204
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002204
Iteration 58/1000 | Loss: 0.00002204
Iteration 59/1000 | Loss: 0.00002204
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002203
Iteration 65/1000 | Loss: 0.00002199
Iteration 66/1000 | Loss: 0.00002199
Iteration 67/1000 | Loss: 0.00002199
Iteration 68/1000 | Loss: 0.00002199
Iteration 69/1000 | Loss: 0.00002199
Iteration 70/1000 | Loss: 0.00002199
Iteration 71/1000 | Loss: 0.00002199
Iteration 72/1000 | Loss: 0.00002199
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002195
Iteration 75/1000 | Loss: 0.00002195
Iteration 76/1000 | Loss: 0.00002194
Iteration 77/1000 | Loss: 0.00002194
Iteration 78/1000 | Loss: 0.00002194
Iteration 79/1000 | Loss: 0.00002194
Iteration 80/1000 | Loss: 0.00002193
Iteration 81/1000 | Loss: 0.00002193
Iteration 82/1000 | Loss: 0.00002192
Iteration 83/1000 | Loss: 0.00002192
Iteration 84/1000 | Loss: 0.00002192
Iteration 85/1000 | Loss: 0.00002191
Iteration 86/1000 | Loss: 0.00002191
Iteration 87/1000 | Loss: 0.00002191
Iteration 88/1000 | Loss: 0.00002190
Iteration 89/1000 | Loss: 0.00002190
Iteration 90/1000 | Loss: 0.00002190
Iteration 91/1000 | Loss: 0.00002190
Iteration 92/1000 | Loss: 0.00002190
Iteration 93/1000 | Loss: 0.00002190
Iteration 94/1000 | Loss: 0.00002190
Iteration 95/1000 | Loss: 0.00002189
Iteration 96/1000 | Loss: 0.00002189
Iteration 97/1000 | Loss: 0.00002188
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002188
Iteration 100/1000 | Loss: 0.00002188
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002187
Iteration 106/1000 | Loss: 0.00002186
Iteration 107/1000 | Loss: 0.00002186
Iteration 108/1000 | Loss: 0.00002186
Iteration 109/1000 | Loss: 0.00002186
Iteration 110/1000 | Loss: 0.00002186
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002184
Iteration 115/1000 | Loss: 0.00002184
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002184
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002183
Iteration 120/1000 | Loss: 0.00002183
Iteration 121/1000 | Loss: 0.00002183
Iteration 122/1000 | Loss: 0.00002183
Iteration 123/1000 | Loss: 0.00002183
Iteration 124/1000 | Loss: 0.00002183
Iteration 125/1000 | Loss: 0.00002183
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002181
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002181
Iteration 134/1000 | Loss: 0.00002181
Iteration 135/1000 | Loss: 0.00002181
Iteration 136/1000 | Loss: 0.00002180
Iteration 137/1000 | Loss: 0.00002180
Iteration 138/1000 | Loss: 0.00002180
Iteration 139/1000 | Loss: 0.00002180
Iteration 140/1000 | Loss: 0.00002180
Iteration 141/1000 | Loss: 0.00002180
Iteration 142/1000 | Loss: 0.00002180
Iteration 143/1000 | Loss: 0.00002180
Iteration 144/1000 | Loss: 0.00002180
Iteration 145/1000 | Loss: 0.00002179
Iteration 146/1000 | Loss: 0.00002179
Iteration 147/1000 | Loss: 0.00002179
Iteration 148/1000 | Loss: 0.00002179
Iteration 149/1000 | Loss: 0.00002179
Iteration 150/1000 | Loss: 0.00002179
Iteration 151/1000 | Loss: 0.00002178
Iteration 152/1000 | Loss: 0.00002178
Iteration 153/1000 | Loss: 0.00002178
Iteration 154/1000 | Loss: 0.00002178
Iteration 155/1000 | Loss: 0.00002178
Iteration 156/1000 | Loss: 0.00002178
Iteration 157/1000 | Loss: 0.00002178
Iteration 158/1000 | Loss: 0.00002178
Iteration 159/1000 | Loss: 0.00002178
Iteration 160/1000 | Loss: 0.00002178
Iteration 161/1000 | Loss: 0.00002178
Iteration 162/1000 | Loss: 0.00002178
Iteration 163/1000 | Loss: 0.00002178
Iteration 164/1000 | Loss: 0.00002178
Iteration 165/1000 | Loss: 0.00002178
Iteration 166/1000 | Loss: 0.00002178
Iteration 167/1000 | Loss: 0.00002178
Iteration 168/1000 | Loss: 0.00002178
Iteration 169/1000 | Loss: 0.00002178
Iteration 170/1000 | Loss: 0.00002178
Iteration 171/1000 | Loss: 0.00002178
Iteration 172/1000 | Loss: 0.00002178
Iteration 173/1000 | Loss: 0.00002178
Iteration 174/1000 | Loss: 0.00002178
Iteration 175/1000 | Loss: 0.00002178
Iteration 176/1000 | Loss: 0.00002178
Iteration 177/1000 | Loss: 0.00002178
Iteration 178/1000 | Loss: 0.00002178
Iteration 179/1000 | Loss: 0.00002178
Iteration 180/1000 | Loss: 0.00002178
Iteration 181/1000 | Loss: 0.00002178
Iteration 182/1000 | Loss: 0.00002178
Iteration 183/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.1779260350740515e-05, 2.1779260350740515e-05, 2.1779260350740515e-05, 2.1779260350740515e-05, 2.1779260350740515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1779260350740515e-05

Optimization complete. Final v2v error: 3.8647713661193848 mm

Highest mean error: 4.327162742614746 mm for frame 142

Lowest mean error: 3.564793586730957 mm for frame 73

Saving results

Total time: 104.72160530090332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500493
Iteration 2/25 | Loss: 0.00101883
Iteration 3/25 | Loss: 0.00089823
Iteration 4/25 | Loss: 0.00085278
Iteration 5/25 | Loss: 0.00083759
Iteration 6/25 | Loss: 0.00083477
Iteration 7/25 | Loss: 0.00083349
Iteration 8/25 | Loss: 0.00083335
Iteration 9/25 | Loss: 0.00083335
Iteration 10/25 | Loss: 0.00083335
Iteration 11/25 | Loss: 0.00083335
Iteration 12/25 | Loss: 0.00083335
Iteration 13/25 | Loss: 0.00083335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008333470323123038, 0.0008333470323123038, 0.0008333470323123038, 0.0008333470323123038, 0.0008333470323123038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008333470323123038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53087568
Iteration 2/25 | Loss: 0.00121924
Iteration 3/25 | Loss: 0.00121924
Iteration 4/25 | Loss: 0.00121924
Iteration 5/25 | Loss: 0.00121924
Iteration 6/25 | Loss: 0.00121924
Iteration 7/25 | Loss: 0.00121924
Iteration 8/25 | Loss: 0.00121924
Iteration 9/25 | Loss: 0.00121924
Iteration 10/25 | Loss: 0.00121924
Iteration 11/25 | Loss: 0.00121924
Iteration 12/25 | Loss: 0.00121924
Iteration 13/25 | Loss: 0.00121924
Iteration 14/25 | Loss: 0.00121924
Iteration 15/25 | Loss: 0.00121924
Iteration 16/25 | Loss: 0.00121924
Iteration 17/25 | Loss: 0.00121924
Iteration 18/25 | Loss: 0.00121924
Iteration 19/25 | Loss: 0.00121924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012192382710054517, 0.0012192382710054517, 0.0012192382710054517, 0.0012192382710054517, 0.0012192382710054517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012192382710054517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121924
Iteration 2/1000 | Loss: 0.00005300
Iteration 3/1000 | Loss: 0.00003643
Iteration 4/1000 | Loss: 0.00003236
Iteration 5/1000 | Loss: 0.00003090
Iteration 6/1000 | Loss: 0.00002946
Iteration 7/1000 | Loss: 0.00002873
Iteration 8/1000 | Loss: 0.00002796
Iteration 9/1000 | Loss: 0.00002760
Iteration 10/1000 | Loss: 0.00002731
Iteration 11/1000 | Loss: 0.00002708
Iteration 12/1000 | Loss: 0.00002694
Iteration 13/1000 | Loss: 0.00002677
Iteration 14/1000 | Loss: 0.00002675
Iteration 15/1000 | Loss: 0.00002675
Iteration 16/1000 | Loss: 0.00002674
Iteration 17/1000 | Loss: 0.00002674
Iteration 18/1000 | Loss: 0.00002673
Iteration 19/1000 | Loss: 0.00002673
Iteration 20/1000 | Loss: 0.00002672
Iteration 21/1000 | Loss: 0.00002672
Iteration 22/1000 | Loss: 0.00002671
Iteration 23/1000 | Loss: 0.00002671
Iteration 24/1000 | Loss: 0.00002671
Iteration 25/1000 | Loss: 0.00002669
Iteration 26/1000 | Loss: 0.00002669
Iteration 27/1000 | Loss: 0.00002668
Iteration 28/1000 | Loss: 0.00002667
Iteration 29/1000 | Loss: 0.00002667
Iteration 30/1000 | Loss: 0.00002667
Iteration 31/1000 | Loss: 0.00002666
Iteration 32/1000 | Loss: 0.00002666
Iteration 33/1000 | Loss: 0.00002665
Iteration 34/1000 | Loss: 0.00002665
Iteration 35/1000 | Loss: 0.00002664
Iteration 36/1000 | Loss: 0.00002663
Iteration 37/1000 | Loss: 0.00002663
Iteration 38/1000 | Loss: 0.00002663
Iteration 39/1000 | Loss: 0.00002663
Iteration 40/1000 | Loss: 0.00002662
Iteration 41/1000 | Loss: 0.00002662
Iteration 42/1000 | Loss: 0.00002662
Iteration 43/1000 | Loss: 0.00002662
Iteration 44/1000 | Loss: 0.00002662
Iteration 45/1000 | Loss: 0.00002661
Iteration 46/1000 | Loss: 0.00002661
Iteration 47/1000 | Loss: 0.00002661
Iteration 48/1000 | Loss: 0.00002661
Iteration 49/1000 | Loss: 0.00002661
Iteration 50/1000 | Loss: 0.00002661
Iteration 51/1000 | Loss: 0.00002661
Iteration 52/1000 | Loss: 0.00002661
Iteration 53/1000 | Loss: 0.00002661
Iteration 54/1000 | Loss: 0.00002661
Iteration 55/1000 | Loss: 0.00002660
Iteration 56/1000 | Loss: 0.00002660
Iteration 57/1000 | Loss: 0.00002660
Iteration 58/1000 | Loss: 0.00002658
Iteration 59/1000 | Loss: 0.00002658
Iteration 60/1000 | Loss: 0.00002657
Iteration 61/1000 | Loss: 0.00002657
Iteration 62/1000 | Loss: 0.00002657
Iteration 63/1000 | Loss: 0.00002657
Iteration 64/1000 | Loss: 0.00002656
Iteration 65/1000 | Loss: 0.00002656
Iteration 66/1000 | Loss: 0.00002654
Iteration 67/1000 | Loss: 0.00002654
Iteration 68/1000 | Loss: 0.00002653
Iteration 69/1000 | Loss: 0.00002653
Iteration 70/1000 | Loss: 0.00002652
Iteration 71/1000 | Loss: 0.00002652
Iteration 72/1000 | Loss: 0.00002652
Iteration 73/1000 | Loss: 0.00002651
Iteration 74/1000 | Loss: 0.00002651
Iteration 75/1000 | Loss: 0.00002651
Iteration 76/1000 | Loss: 0.00002650
Iteration 77/1000 | Loss: 0.00002650
Iteration 78/1000 | Loss: 0.00002649
Iteration 79/1000 | Loss: 0.00002649
Iteration 80/1000 | Loss: 0.00002649
Iteration 81/1000 | Loss: 0.00002648
Iteration 82/1000 | Loss: 0.00002648
Iteration 83/1000 | Loss: 0.00002648
Iteration 84/1000 | Loss: 0.00002647
Iteration 85/1000 | Loss: 0.00002647
Iteration 86/1000 | Loss: 0.00002646
Iteration 87/1000 | Loss: 0.00002646
Iteration 88/1000 | Loss: 0.00002646
Iteration 89/1000 | Loss: 0.00002645
Iteration 90/1000 | Loss: 0.00002645
Iteration 91/1000 | Loss: 0.00002645
Iteration 92/1000 | Loss: 0.00002645
Iteration 93/1000 | Loss: 0.00002645
Iteration 94/1000 | Loss: 0.00002644
Iteration 95/1000 | Loss: 0.00002644
Iteration 96/1000 | Loss: 0.00002644
Iteration 97/1000 | Loss: 0.00002643
Iteration 98/1000 | Loss: 0.00002643
Iteration 99/1000 | Loss: 0.00002643
Iteration 100/1000 | Loss: 0.00002643
Iteration 101/1000 | Loss: 0.00002642
Iteration 102/1000 | Loss: 0.00002642
Iteration 103/1000 | Loss: 0.00002642
Iteration 104/1000 | Loss: 0.00002642
Iteration 105/1000 | Loss: 0.00002641
Iteration 106/1000 | Loss: 0.00002641
Iteration 107/1000 | Loss: 0.00002641
Iteration 108/1000 | Loss: 0.00002641
Iteration 109/1000 | Loss: 0.00002641
Iteration 110/1000 | Loss: 0.00002640
Iteration 111/1000 | Loss: 0.00002640
Iteration 112/1000 | Loss: 0.00002640
Iteration 113/1000 | Loss: 0.00002639
Iteration 114/1000 | Loss: 0.00002639
Iteration 115/1000 | Loss: 0.00002639
Iteration 116/1000 | Loss: 0.00002639
Iteration 117/1000 | Loss: 0.00002639
Iteration 118/1000 | Loss: 0.00002639
Iteration 119/1000 | Loss: 0.00002638
Iteration 120/1000 | Loss: 0.00002638
Iteration 121/1000 | Loss: 0.00002638
Iteration 122/1000 | Loss: 0.00002638
Iteration 123/1000 | Loss: 0.00002638
Iteration 124/1000 | Loss: 0.00002637
Iteration 125/1000 | Loss: 0.00002637
Iteration 126/1000 | Loss: 0.00002637
Iteration 127/1000 | Loss: 0.00002637
Iteration 128/1000 | Loss: 0.00002637
Iteration 129/1000 | Loss: 0.00002636
Iteration 130/1000 | Loss: 0.00002636
Iteration 131/1000 | Loss: 0.00002635
Iteration 132/1000 | Loss: 0.00002635
Iteration 133/1000 | Loss: 0.00002635
Iteration 134/1000 | Loss: 0.00002635
Iteration 135/1000 | Loss: 0.00002635
Iteration 136/1000 | Loss: 0.00002635
Iteration 137/1000 | Loss: 0.00002635
Iteration 138/1000 | Loss: 0.00002635
Iteration 139/1000 | Loss: 0.00002634
Iteration 140/1000 | Loss: 0.00002634
Iteration 141/1000 | Loss: 0.00002634
Iteration 142/1000 | Loss: 0.00002634
Iteration 143/1000 | Loss: 0.00002634
Iteration 144/1000 | Loss: 0.00002634
Iteration 145/1000 | Loss: 0.00002634
Iteration 146/1000 | Loss: 0.00002633
Iteration 147/1000 | Loss: 0.00002633
Iteration 148/1000 | Loss: 0.00002633
Iteration 149/1000 | Loss: 0.00002633
Iteration 150/1000 | Loss: 0.00002633
Iteration 151/1000 | Loss: 0.00002633
Iteration 152/1000 | Loss: 0.00002633
Iteration 153/1000 | Loss: 0.00002633
Iteration 154/1000 | Loss: 0.00002633
Iteration 155/1000 | Loss: 0.00002632
Iteration 156/1000 | Loss: 0.00002632
Iteration 157/1000 | Loss: 0.00002632
Iteration 158/1000 | Loss: 0.00002632
Iteration 159/1000 | Loss: 0.00002632
Iteration 160/1000 | Loss: 0.00002632
Iteration 161/1000 | Loss: 0.00002632
Iteration 162/1000 | Loss: 0.00002632
Iteration 163/1000 | Loss: 0.00002632
Iteration 164/1000 | Loss: 0.00002632
Iteration 165/1000 | Loss: 0.00002632
Iteration 166/1000 | Loss: 0.00002632
Iteration 167/1000 | Loss: 0.00002632
Iteration 168/1000 | Loss: 0.00002632
Iteration 169/1000 | Loss: 0.00002632
Iteration 170/1000 | Loss: 0.00002632
Iteration 171/1000 | Loss: 0.00002632
Iteration 172/1000 | Loss: 0.00002632
Iteration 173/1000 | Loss: 0.00002632
Iteration 174/1000 | Loss: 0.00002632
Iteration 175/1000 | Loss: 0.00002632
Iteration 176/1000 | Loss: 0.00002632
Iteration 177/1000 | Loss: 0.00002632
Iteration 178/1000 | Loss: 0.00002632
Iteration 179/1000 | Loss: 0.00002632
Iteration 180/1000 | Loss: 0.00002632
Iteration 181/1000 | Loss: 0.00002632
Iteration 182/1000 | Loss: 0.00002632
Iteration 183/1000 | Loss: 0.00002632
Iteration 184/1000 | Loss: 0.00002632
Iteration 185/1000 | Loss: 0.00002632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.631595816637855e-05, 2.631595816637855e-05, 2.631595816637855e-05, 2.631595816637855e-05, 2.631595816637855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.631595816637855e-05

Optimization complete. Final v2v error: 4.277984142303467 mm

Highest mean error: 5.0073065757751465 mm for frame 17

Lowest mean error: 3.567488670349121 mm for frame 2

Saving results

Total time: 43.30253481864929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872175
Iteration 2/25 | Loss: 0.00090166
Iteration 3/25 | Loss: 0.00079388
Iteration 4/25 | Loss: 0.00076628
Iteration 5/25 | Loss: 0.00076225
Iteration 6/25 | Loss: 0.00076097
Iteration 7/25 | Loss: 0.00076085
Iteration 8/25 | Loss: 0.00076085
Iteration 9/25 | Loss: 0.00076085
Iteration 10/25 | Loss: 0.00076085
Iteration 11/25 | Loss: 0.00076085
Iteration 12/25 | Loss: 0.00076085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007608534651808441, 0.0007608534651808441, 0.0007608534651808441, 0.0007608534651808441, 0.0007608534651808441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007608534651808441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59990096
Iteration 2/25 | Loss: 0.00112097
Iteration 3/25 | Loss: 0.00112097
Iteration 4/25 | Loss: 0.00112096
Iteration 5/25 | Loss: 0.00112096
Iteration 6/25 | Loss: 0.00112096
Iteration 7/25 | Loss: 0.00112096
Iteration 8/25 | Loss: 0.00112096
Iteration 9/25 | Loss: 0.00112096
Iteration 10/25 | Loss: 0.00112096
Iteration 11/25 | Loss: 0.00112096
Iteration 12/25 | Loss: 0.00112096
Iteration 13/25 | Loss: 0.00112096
Iteration 14/25 | Loss: 0.00112096
Iteration 15/25 | Loss: 0.00112096
Iteration 16/25 | Loss: 0.00112096
Iteration 17/25 | Loss: 0.00112096
Iteration 18/25 | Loss: 0.00112096
Iteration 19/25 | Loss: 0.00112096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011209622025489807, 0.0011209622025489807, 0.0011209622025489807, 0.0011209622025489807, 0.0011209622025489807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011209622025489807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112096
Iteration 2/1000 | Loss: 0.00003349
Iteration 3/1000 | Loss: 0.00002252
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001746
Iteration 12/1000 | Loss: 0.00001732
Iteration 13/1000 | Loss: 0.00001730
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001721
Iteration 17/1000 | Loss: 0.00001721
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001715
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001714
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001711
Iteration 43/1000 | Loss: 0.00001711
Iteration 44/1000 | Loss: 0.00001711
Iteration 45/1000 | Loss: 0.00001711
Iteration 46/1000 | Loss: 0.00001711
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001710
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.7080690668080933e-05, 1.7080690668080933e-05, 1.7080690668080933e-05, 1.7080690668080933e-05, 1.7080690668080933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7080690668080933e-05

Optimization complete. Final v2v error: 3.461092472076416 mm

Highest mean error: 4.019077301025391 mm for frame 97

Lowest mean error: 3.3364298343658447 mm for frame 155

Saving results

Total time: 30.343462228775024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382396
Iteration 2/25 | Loss: 0.00083137
Iteration 3/25 | Loss: 0.00074035
Iteration 4/25 | Loss: 0.00071650
Iteration 5/25 | Loss: 0.00070958
Iteration 6/25 | Loss: 0.00070790
Iteration 7/25 | Loss: 0.00070745
Iteration 8/25 | Loss: 0.00070745
Iteration 9/25 | Loss: 0.00070745
Iteration 10/25 | Loss: 0.00070745
Iteration 11/25 | Loss: 0.00070745
Iteration 12/25 | Loss: 0.00070745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007074542809277773, 0.0007074542809277773, 0.0007074542809277773, 0.0007074542809277773, 0.0007074542809277773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007074542809277773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67325008
Iteration 2/25 | Loss: 0.00119472
Iteration 3/25 | Loss: 0.00119472
Iteration 4/25 | Loss: 0.00119472
Iteration 5/25 | Loss: 0.00119472
Iteration 6/25 | Loss: 0.00119472
Iteration 7/25 | Loss: 0.00119472
Iteration 8/25 | Loss: 0.00119472
Iteration 9/25 | Loss: 0.00119472
Iteration 10/25 | Loss: 0.00119472
Iteration 11/25 | Loss: 0.00119472
Iteration 12/25 | Loss: 0.00119472
Iteration 13/25 | Loss: 0.00119472
Iteration 14/25 | Loss: 0.00119472
Iteration 15/25 | Loss: 0.00119472
Iteration 16/25 | Loss: 0.00119472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011947226012125611, 0.0011947226012125611, 0.0011947226012125611, 0.0011947226012125611, 0.0011947226012125611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011947226012125611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119472
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001392
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001207
Iteration 6/1000 | Loss: 0.00001172
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001124
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001115
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001111
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00001109
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001109
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001105
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001103
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001097
Iteration 60/1000 | Loss: 0.00001097
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001096
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001095
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001095
Iteration 81/1000 | Loss: 0.00001095
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.0945539543172345e-05, 1.0945539543172345e-05, 1.0945539543172345e-05, 1.0945539543172345e-05, 1.0945539543172345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0945539543172345e-05

Optimization complete. Final v2v error: 2.8262202739715576 mm

Highest mean error: 2.933548927307129 mm for frame 24

Lowest mean error: 2.737795352935791 mm for frame 161

Saving results

Total time: 29.255739450454712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471469
Iteration 2/25 | Loss: 0.00117356
Iteration 3/25 | Loss: 0.00083536
Iteration 4/25 | Loss: 0.00077436
Iteration 5/25 | Loss: 0.00076092
Iteration 6/25 | Loss: 0.00075687
Iteration 7/25 | Loss: 0.00075568
Iteration 8/25 | Loss: 0.00075545
Iteration 9/25 | Loss: 0.00075545
Iteration 10/25 | Loss: 0.00075545
Iteration 11/25 | Loss: 0.00075545
Iteration 12/25 | Loss: 0.00075545
Iteration 13/25 | Loss: 0.00075543
Iteration 14/25 | Loss: 0.00075543
Iteration 15/25 | Loss: 0.00075543
Iteration 16/25 | Loss: 0.00075543
Iteration 17/25 | Loss: 0.00075543
Iteration 18/25 | Loss: 0.00075543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007554331095889211, 0.0007554331095889211, 0.0007554331095889211, 0.0007554331095889211, 0.0007554331095889211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007554331095889211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59391487
Iteration 2/25 | Loss: 0.00117462
Iteration 3/25 | Loss: 0.00117462
Iteration 4/25 | Loss: 0.00117462
Iteration 5/25 | Loss: 0.00117462
Iteration 6/25 | Loss: 0.00117462
Iteration 7/25 | Loss: 0.00117462
Iteration 8/25 | Loss: 0.00117462
Iteration 9/25 | Loss: 0.00117462
Iteration 10/25 | Loss: 0.00117462
Iteration 11/25 | Loss: 0.00117462
Iteration 12/25 | Loss: 0.00117462
Iteration 13/25 | Loss: 0.00117462
Iteration 14/25 | Loss: 0.00117462
Iteration 15/25 | Loss: 0.00117462
Iteration 16/25 | Loss: 0.00117462
Iteration 17/25 | Loss: 0.00117462
Iteration 18/25 | Loss: 0.00117462
Iteration 19/25 | Loss: 0.00117462
Iteration 20/25 | Loss: 0.00117462
Iteration 21/25 | Loss: 0.00117462
Iteration 22/25 | Loss: 0.00117462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001174616627395153, 0.001174616627395153, 0.001174616627395153, 0.001174616627395153, 0.001174616627395153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001174616627395153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117462
Iteration 2/1000 | Loss: 0.00002583
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001222
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001162
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001152
Iteration 15/1000 | Loss: 0.00001151
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001133
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001133
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001131
Iteration 35/1000 | Loss: 0.00001130
Iteration 36/1000 | Loss: 0.00001130
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001129
Iteration 39/1000 | Loss: 0.00001129
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001124
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001124
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001123
Iteration 67/1000 | Loss: 0.00001123
Iteration 68/1000 | Loss: 0.00001123
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001122
Iteration 71/1000 | Loss: 0.00001122
Iteration 72/1000 | Loss: 0.00001122
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001122
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001116
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001114
Iteration 130/1000 | Loss: 0.00001114
Iteration 131/1000 | Loss: 0.00001114
Iteration 132/1000 | Loss: 0.00001114
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001113
Iteration 135/1000 | Loss: 0.00001113
Iteration 136/1000 | Loss: 0.00001113
Iteration 137/1000 | Loss: 0.00001113
Iteration 138/1000 | Loss: 0.00001113
Iteration 139/1000 | Loss: 0.00001113
Iteration 140/1000 | Loss: 0.00001113
Iteration 141/1000 | Loss: 0.00001113
Iteration 142/1000 | Loss: 0.00001113
Iteration 143/1000 | Loss: 0.00001112
Iteration 144/1000 | Loss: 0.00001112
Iteration 145/1000 | Loss: 0.00001112
Iteration 146/1000 | Loss: 0.00001112
Iteration 147/1000 | Loss: 0.00001112
Iteration 148/1000 | Loss: 0.00001112
Iteration 149/1000 | Loss: 0.00001112
Iteration 150/1000 | Loss: 0.00001112
Iteration 151/1000 | Loss: 0.00001112
Iteration 152/1000 | Loss: 0.00001112
Iteration 153/1000 | Loss: 0.00001111
Iteration 154/1000 | Loss: 0.00001111
Iteration 155/1000 | Loss: 0.00001111
Iteration 156/1000 | Loss: 0.00001111
Iteration 157/1000 | Loss: 0.00001111
Iteration 158/1000 | Loss: 0.00001111
Iteration 159/1000 | Loss: 0.00001111
Iteration 160/1000 | Loss: 0.00001111
Iteration 161/1000 | Loss: 0.00001111
Iteration 162/1000 | Loss: 0.00001111
Iteration 163/1000 | Loss: 0.00001111
Iteration 164/1000 | Loss: 0.00001111
Iteration 165/1000 | Loss: 0.00001111
Iteration 166/1000 | Loss: 0.00001111
Iteration 167/1000 | Loss: 0.00001111
Iteration 168/1000 | Loss: 0.00001111
Iteration 169/1000 | Loss: 0.00001111
Iteration 170/1000 | Loss: 0.00001111
Iteration 171/1000 | Loss: 0.00001111
Iteration 172/1000 | Loss: 0.00001111
Iteration 173/1000 | Loss: 0.00001111
Iteration 174/1000 | Loss: 0.00001111
Iteration 175/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.1110560080851428e-05, 1.1110560080851428e-05, 1.1110560080851428e-05, 1.1110560080851428e-05, 1.1110560080851428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1110560080851428e-05

Optimization complete. Final v2v error: 2.7943015098571777 mm

Highest mean error: 3.523221969604492 mm for frame 73

Lowest mean error: 2.543576717376709 mm for frame 129

Saving results

Total time: 38.538172006607056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00670773
Iteration 2/25 | Loss: 0.00186320
Iteration 3/25 | Loss: 0.00163034
Iteration 4/25 | Loss: 0.00160799
Iteration 5/25 | Loss: 0.00160502
Iteration 6/25 | Loss: 0.00160433
Iteration 7/25 | Loss: 0.00160433
Iteration 8/25 | Loss: 0.00160433
Iteration 9/25 | Loss: 0.00160433
Iteration 10/25 | Loss: 0.00160433
Iteration 11/25 | Loss: 0.00160433
Iteration 12/25 | Loss: 0.00160433
Iteration 13/25 | Loss: 0.00160433
Iteration 14/25 | Loss: 0.00160433
Iteration 15/25 | Loss: 0.00160433
Iteration 16/25 | Loss: 0.00160433
Iteration 17/25 | Loss: 0.00160433
Iteration 18/25 | Loss: 0.00160433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016043325886130333, 0.0016043325886130333, 0.0016043325886130333, 0.0016043325886130333, 0.0016043325886130333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016043325886130333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28613126
Iteration 2/25 | Loss: 0.00373295
Iteration 3/25 | Loss: 0.00373295
Iteration 4/25 | Loss: 0.00373295
Iteration 5/25 | Loss: 0.00373295
Iteration 6/25 | Loss: 0.00373295
Iteration 7/25 | Loss: 0.00373295
Iteration 8/25 | Loss: 0.00373295
Iteration 9/25 | Loss: 0.00373295
Iteration 10/25 | Loss: 0.00373295
Iteration 11/25 | Loss: 0.00373295
Iteration 12/25 | Loss: 0.00373295
Iteration 13/25 | Loss: 0.00373295
Iteration 14/25 | Loss: 0.00373295
Iteration 15/25 | Loss: 0.00373295
Iteration 16/25 | Loss: 0.00373295
Iteration 17/25 | Loss: 0.00373295
Iteration 18/25 | Loss: 0.00373295
Iteration 19/25 | Loss: 0.00373295
Iteration 20/25 | Loss: 0.00373295
Iteration 21/25 | Loss: 0.00373295
Iteration 22/25 | Loss: 0.00373295
Iteration 23/25 | Loss: 0.00373295
Iteration 24/25 | Loss: 0.00373295
Iteration 25/25 | Loss: 0.00373295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003732951357960701, 0.003732951357960701, 0.003732951357960701, 0.003732951357960701, 0.003732951357960701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003732951357960701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00373295
Iteration 2/1000 | Loss: 0.00007645
Iteration 3/1000 | Loss: 0.00005009
Iteration 4/1000 | Loss: 0.00004375
Iteration 5/1000 | Loss: 0.00004091
Iteration 6/1000 | Loss: 0.00003965
Iteration 7/1000 | Loss: 0.00003880
Iteration 8/1000 | Loss: 0.00003832
Iteration 9/1000 | Loss: 0.00003797
Iteration 10/1000 | Loss: 0.00003780
Iteration 11/1000 | Loss: 0.00003760
Iteration 12/1000 | Loss: 0.00003750
Iteration 13/1000 | Loss: 0.00003741
Iteration 14/1000 | Loss: 0.00003738
Iteration 15/1000 | Loss: 0.00003736
Iteration 16/1000 | Loss: 0.00003736
Iteration 17/1000 | Loss: 0.00003735
Iteration 18/1000 | Loss: 0.00003735
Iteration 19/1000 | Loss: 0.00003735
Iteration 20/1000 | Loss: 0.00003735
Iteration 21/1000 | Loss: 0.00003735
Iteration 22/1000 | Loss: 0.00003735
Iteration 23/1000 | Loss: 0.00003735
Iteration 24/1000 | Loss: 0.00003735
Iteration 25/1000 | Loss: 0.00003735
Iteration 26/1000 | Loss: 0.00003735
Iteration 27/1000 | Loss: 0.00003735
Iteration 28/1000 | Loss: 0.00003735
Iteration 29/1000 | Loss: 0.00003735
Iteration 30/1000 | Loss: 0.00003735
Iteration 31/1000 | Loss: 0.00003735
Iteration 32/1000 | Loss: 0.00003734
Iteration 33/1000 | Loss: 0.00003734
Iteration 34/1000 | Loss: 0.00003734
Iteration 35/1000 | Loss: 0.00003734
Iteration 36/1000 | Loss: 0.00003734
Iteration 37/1000 | Loss: 0.00003728
Iteration 38/1000 | Loss: 0.00003728
Iteration 39/1000 | Loss: 0.00003728
Iteration 40/1000 | Loss: 0.00003728
Iteration 41/1000 | Loss: 0.00003728
Iteration 42/1000 | Loss: 0.00003728
Iteration 43/1000 | Loss: 0.00003728
Iteration 44/1000 | Loss: 0.00003728
Iteration 45/1000 | Loss: 0.00003727
Iteration 46/1000 | Loss: 0.00003727
Iteration 47/1000 | Loss: 0.00003727
Iteration 48/1000 | Loss: 0.00003727
Iteration 49/1000 | Loss: 0.00003722
Iteration 50/1000 | Loss: 0.00003722
Iteration 51/1000 | Loss: 0.00003721
Iteration 52/1000 | Loss: 0.00003720
Iteration 53/1000 | Loss: 0.00003720
Iteration 54/1000 | Loss: 0.00003720
Iteration 55/1000 | Loss: 0.00003720
Iteration 56/1000 | Loss: 0.00003719
Iteration 57/1000 | Loss: 0.00003719
Iteration 58/1000 | Loss: 0.00003719
Iteration 59/1000 | Loss: 0.00003719
Iteration 60/1000 | Loss: 0.00003719
Iteration 61/1000 | Loss: 0.00003719
Iteration 62/1000 | Loss: 0.00003719
Iteration 63/1000 | Loss: 0.00003719
Iteration 64/1000 | Loss: 0.00003719
Iteration 65/1000 | Loss: 0.00003719
Iteration 66/1000 | Loss: 0.00003719
Iteration 67/1000 | Loss: 0.00003719
Iteration 68/1000 | Loss: 0.00003719
Iteration 69/1000 | Loss: 0.00003719
Iteration 70/1000 | Loss: 0.00003719
Iteration 71/1000 | Loss: 0.00003718
Iteration 72/1000 | Loss: 0.00003718
Iteration 73/1000 | Loss: 0.00003718
Iteration 74/1000 | Loss: 0.00003718
Iteration 75/1000 | Loss: 0.00003718
Iteration 76/1000 | Loss: 0.00003718
Iteration 77/1000 | Loss: 0.00003718
Iteration 78/1000 | Loss: 0.00003717
Iteration 79/1000 | Loss: 0.00003717
Iteration 80/1000 | Loss: 0.00003717
Iteration 81/1000 | Loss: 0.00003717
Iteration 82/1000 | Loss: 0.00003717
Iteration 83/1000 | Loss: 0.00003717
Iteration 84/1000 | Loss: 0.00003717
Iteration 85/1000 | Loss: 0.00003717
Iteration 86/1000 | Loss: 0.00003717
Iteration 87/1000 | Loss: 0.00003717
Iteration 88/1000 | Loss: 0.00003717
Iteration 89/1000 | Loss: 0.00003716
Iteration 90/1000 | Loss: 0.00003716
Iteration 91/1000 | Loss: 0.00003716
Iteration 92/1000 | Loss: 0.00003716
Iteration 93/1000 | Loss: 0.00003716
Iteration 94/1000 | Loss: 0.00003716
Iteration 95/1000 | Loss: 0.00003715
Iteration 96/1000 | Loss: 0.00003715
Iteration 97/1000 | Loss: 0.00003715
Iteration 98/1000 | Loss: 0.00003715
Iteration 99/1000 | Loss: 0.00003715
Iteration 100/1000 | Loss: 0.00003715
Iteration 101/1000 | Loss: 0.00003715
Iteration 102/1000 | Loss: 0.00003715
Iteration 103/1000 | Loss: 0.00003715
Iteration 104/1000 | Loss: 0.00003715
Iteration 105/1000 | Loss: 0.00003715
Iteration 106/1000 | Loss: 0.00003715
Iteration 107/1000 | Loss: 0.00003715
Iteration 108/1000 | Loss: 0.00003715
Iteration 109/1000 | Loss: 0.00003715
Iteration 110/1000 | Loss: 0.00003715
Iteration 111/1000 | Loss: 0.00003714
Iteration 112/1000 | Loss: 0.00003714
Iteration 113/1000 | Loss: 0.00003714
Iteration 114/1000 | Loss: 0.00003714
Iteration 115/1000 | Loss: 0.00003714
Iteration 116/1000 | Loss: 0.00003714
Iteration 117/1000 | Loss: 0.00003714
Iteration 118/1000 | Loss: 0.00003714
Iteration 119/1000 | Loss: 0.00003714
Iteration 120/1000 | Loss: 0.00003714
Iteration 121/1000 | Loss: 0.00003714
Iteration 122/1000 | Loss: 0.00003714
Iteration 123/1000 | Loss: 0.00003713
Iteration 124/1000 | Loss: 0.00003713
Iteration 125/1000 | Loss: 0.00003713
Iteration 126/1000 | Loss: 0.00003713
Iteration 127/1000 | Loss: 0.00003713
Iteration 128/1000 | Loss: 0.00003713
Iteration 129/1000 | Loss: 0.00003713
Iteration 130/1000 | Loss: 0.00003713
Iteration 131/1000 | Loss: 0.00003713
Iteration 132/1000 | Loss: 0.00003713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.713241676450707e-05, 3.713241676450707e-05, 3.713241676450707e-05, 3.713241676450707e-05, 3.713241676450707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.713241676450707e-05

Optimization complete. Final v2v error: 5.22231912612915 mm

Highest mean error: 5.308712005615234 mm for frame 38

Lowest mean error: 5.148874759674072 mm for frame 101

Saving results

Total time: 39.46709585189819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868029
Iteration 2/25 | Loss: 0.00208298
Iteration 3/25 | Loss: 0.00185318
Iteration 4/25 | Loss: 0.00180476
Iteration 5/25 | Loss: 0.00178893
Iteration 6/25 | Loss: 0.00178590
Iteration 7/25 | Loss: 0.00178569
Iteration 8/25 | Loss: 0.00178569
Iteration 9/25 | Loss: 0.00178569
Iteration 10/25 | Loss: 0.00178569
Iteration 11/25 | Loss: 0.00178569
Iteration 12/25 | Loss: 0.00178569
Iteration 13/25 | Loss: 0.00178569
Iteration 14/25 | Loss: 0.00178569
Iteration 15/25 | Loss: 0.00178569
Iteration 16/25 | Loss: 0.00178569
Iteration 17/25 | Loss: 0.00178569
Iteration 18/25 | Loss: 0.00178569
Iteration 19/25 | Loss: 0.00178569
Iteration 20/25 | Loss: 0.00178569
Iteration 21/25 | Loss: 0.00178569
Iteration 22/25 | Loss: 0.00178569
Iteration 23/25 | Loss: 0.00178569
Iteration 24/25 | Loss: 0.00178569
Iteration 25/25 | Loss: 0.00178569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07416677
Iteration 2/25 | Loss: 0.00317925
Iteration 3/25 | Loss: 0.00317925
Iteration 4/25 | Loss: 0.00317925
Iteration 5/25 | Loss: 0.00317925
Iteration 6/25 | Loss: 0.00317925
Iteration 7/25 | Loss: 0.00317925
Iteration 8/25 | Loss: 0.00317925
Iteration 9/25 | Loss: 0.00317925
Iteration 10/25 | Loss: 0.00317925
Iteration 11/25 | Loss: 0.00317925
Iteration 12/25 | Loss: 0.00317925
Iteration 13/25 | Loss: 0.00317925
Iteration 14/25 | Loss: 0.00317925
Iteration 15/25 | Loss: 0.00317925
Iteration 16/25 | Loss: 0.00317925
Iteration 17/25 | Loss: 0.00317925
Iteration 18/25 | Loss: 0.00317925
Iteration 19/25 | Loss: 0.00317925
Iteration 20/25 | Loss: 0.00317925
Iteration 21/25 | Loss: 0.00317925
Iteration 22/25 | Loss: 0.00317925
Iteration 23/25 | Loss: 0.00317925
Iteration 24/25 | Loss: 0.00317925
Iteration 25/25 | Loss: 0.00317925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00317925
Iteration 2/1000 | Loss: 0.00012619
Iteration 3/1000 | Loss: 0.00009898
Iteration 4/1000 | Loss: 0.00008663
Iteration 5/1000 | Loss: 0.00007899
Iteration 6/1000 | Loss: 0.00007529
Iteration 7/1000 | Loss: 0.00007320
Iteration 8/1000 | Loss: 0.00007182
Iteration 9/1000 | Loss: 0.00007072
Iteration 10/1000 | Loss: 0.00006982
Iteration 11/1000 | Loss: 0.00006907
Iteration 12/1000 | Loss: 0.00006840
Iteration 13/1000 | Loss: 0.00006773
Iteration 14/1000 | Loss: 0.00006721
Iteration 15/1000 | Loss: 0.00006678
Iteration 16/1000 | Loss: 0.00006649
Iteration 17/1000 | Loss: 0.00006624
Iteration 18/1000 | Loss: 0.00006620
Iteration 19/1000 | Loss: 0.00006604
Iteration 20/1000 | Loss: 0.00006589
Iteration 21/1000 | Loss: 0.00006579
Iteration 22/1000 | Loss: 0.00006575
Iteration 23/1000 | Loss: 0.00006560
Iteration 24/1000 | Loss: 0.00006559
Iteration 25/1000 | Loss: 0.00006558
Iteration 26/1000 | Loss: 0.00006553
Iteration 27/1000 | Loss: 0.00006553
Iteration 28/1000 | Loss: 0.00006553
Iteration 29/1000 | Loss: 0.00006553
Iteration 30/1000 | Loss: 0.00006551
Iteration 31/1000 | Loss: 0.00006550
Iteration 32/1000 | Loss: 0.00006550
Iteration 33/1000 | Loss: 0.00006549
Iteration 34/1000 | Loss: 0.00006549
Iteration 35/1000 | Loss: 0.00006548
Iteration 36/1000 | Loss: 0.00006547
Iteration 37/1000 | Loss: 0.00006547
Iteration 38/1000 | Loss: 0.00006546
Iteration 39/1000 | Loss: 0.00006546
Iteration 40/1000 | Loss: 0.00006545
Iteration 41/1000 | Loss: 0.00006545
Iteration 42/1000 | Loss: 0.00006544
Iteration 43/1000 | Loss: 0.00006544
Iteration 44/1000 | Loss: 0.00006543
Iteration 45/1000 | Loss: 0.00006543
Iteration 46/1000 | Loss: 0.00006542
Iteration 47/1000 | Loss: 0.00006542
Iteration 48/1000 | Loss: 0.00006542
Iteration 49/1000 | Loss: 0.00006541
Iteration 50/1000 | Loss: 0.00006539
Iteration 51/1000 | Loss: 0.00006539
Iteration 52/1000 | Loss: 0.00006536
Iteration 53/1000 | Loss: 0.00006536
Iteration 54/1000 | Loss: 0.00006533
Iteration 55/1000 | Loss: 0.00006533
Iteration 56/1000 | Loss: 0.00006522
Iteration 57/1000 | Loss: 0.00006518
Iteration 58/1000 | Loss: 0.00006516
Iteration 59/1000 | Loss: 0.00006514
Iteration 60/1000 | Loss: 0.00006514
Iteration 61/1000 | Loss: 0.00006513
Iteration 62/1000 | Loss: 0.00006513
Iteration 63/1000 | Loss: 0.00006512
Iteration 64/1000 | Loss: 0.00006512
Iteration 65/1000 | Loss: 0.00006512
Iteration 66/1000 | Loss: 0.00006510
Iteration 67/1000 | Loss: 0.00006510
Iteration 68/1000 | Loss: 0.00006509
Iteration 69/1000 | Loss: 0.00006509
Iteration 70/1000 | Loss: 0.00006508
Iteration 71/1000 | Loss: 0.00006508
Iteration 72/1000 | Loss: 0.00006508
Iteration 73/1000 | Loss: 0.00006507
Iteration 74/1000 | Loss: 0.00006507
Iteration 75/1000 | Loss: 0.00006507
Iteration 76/1000 | Loss: 0.00006507
Iteration 77/1000 | Loss: 0.00006507
Iteration 78/1000 | Loss: 0.00006507
Iteration 79/1000 | Loss: 0.00006507
Iteration 80/1000 | Loss: 0.00006507
Iteration 81/1000 | Loss: 0.00006507
Iteration 82/1000 | Loss: 0.00006507
Iteration 83/1000 | Loss: 0.00006507
Iteration 84/1000 | Loss: 0.00006507
Iteration 85/1000 | Loss: 0.00006506
Iteration 86/1000 | Loss: 0.00006506
Iteration 87/1000 | Loss: 0.00006506
Iteration 88/1000 | Loss: 0.00006506
Iteration 89/1000 | Loss: 0.00006505
Iteration 90/1000 | Loss: 0.00006505
Iteration 91/1000 | Loss: 0.00006505
Iteration 92/1000 | Loss: 0.00006505
Iteration 93/1000 | Loss: 0.00006505
Iteration 94/1000 | Loss: 0.00006504
Iteration 95/1000 | Loss: 0.00006504
Iteration 96/1000 | Loss: 0.00006504
Iteration 97/1000 | Loss: 0.00006504
Iteration 98/1000 | Loss: 0.00006504
Iteration 99/1000 | Loss: 0.00006504
Iteration 100/1000 | Loss: 0.00006504
Iteration 101/1000 | Loss: 0.00006504
Iteration 102/1000 | Loss: 0.00006504
Iteration 103/1000 | Loss: 0.00006504
Iteration 104/1000 | Loss: 0.00006503
Iteration 105/1000 | Loss: 0.00006503
Iteration 106/1000 | Loss: 0.00006503
Iteration 107/1000 | Loss: 0.00006503
Iteration 108/1000 | Loss: 0.00006503
Iteration 109/1000 | Loss: 0.00006502
Iteration 110/1000 | Loss: 0.00006502
Iteration 111/1000 | Loss: 0.00006502
Iteration 112/1000 | Loss: 0.00006502
Iteration 113/1000 | Loss: 0.00006502
Iteration 114/1000 | Loss: 0.00006502
Iteration 115/1000 | Loss: 0.00006502
Iteration 116/1000 | Loss: 0.00006501
Iteration 117/1000 | Loss: 0.00006501
Iteration 118/1000 | Loss: 0.00006501
Iteration 119/1000 | Loss: 0.00006501
Iteration 120/1000 | Loss: 0.00006501
Iteration 121/1000 | Loss: 0.00006500
Iteration 122/1000 | Loss: 0.00006500
Iteration 123/1000 | Loss: 0.00006500
Iteration 124/1000 | Loss: 0.00006500
Iteration 125/1000 | Loss: 0.00006499
Iteration 126/1000 | Loss: 0.00006499
Iteration 127/1000 | Loss: 0.00006499
Iteration 128/1000 | Loss: 0.00006499
Iteration 129/1000 | Loss: 0.00006499
Iteration 130/1000 | Loss: 0.00006499
Iteration 131/1000 | Loss: 0.00006499
Iteration 132/1000 | Loss: 0.00006499
Iteration 133/1000 | Loss: 0.00006499
Iteration 134/1000 | Loss: 0.00006499
Iteration 135/1000 | Loss: 0.00006498
Iteration 136/1000 | Loss: 0.00006498
Iteration 137/1000 | Loss: 0.00006498
Iteration 138/1000 | Loss: 0.00006498
Iteration 139/1000 | Loss: 0.00006498
Iteration 140/1000 | Loss: 0.00006498
Iteration 141/1000 | Loss: 0.00006498
Iteration 142/1000 | Loss: 0.00006498
Iteration 143/1000 | Loss: 0.00006498
Iteration 144/1000 | Loss: 0.00006498
Iteration 145/1000 | Loss: 0.00006497
Iteration 146/1000 | Loss: 0.00006497
Iteration 147/1000 | Loss: 0.00006497
Iteration 148/1000 | Loss: 0.00006497
Iteration 149/1000 | Loss: 0.00006497
Iteration 150/1000 | Loss: 0.00006497
Iteration 151/1000 | Loss: 0.00006497
Iteration 152/1000 | Loss: 0.00006497
Iteration 153/1000 | Loss: 0.00006497
Iteration 154/1000 | Loss: 0.00006496
Iteration 155/1000 | Loss: 0.00006496
Iteration 156/1000 | Loss: 0.00006496
Iteration 157/1000 | Loss: 0.00006496
Iteration 158/1000 | Loss: 0.00006496
Iteration 159/1000 | Loss: 0.00006496
Iteration 160/1000 | Loss: 0.00006496
Iteration 161/1000 | Loss: 0.00006496
Iteration 162/1000 | Loss: 0.00006496
Iteration 163/1000 | Loss: 0.00006496
Iteration 164/1000 | Loss: 0.00006496
Iteration 165/1000 | Loss: 0.00006496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [6.496222340501845e-05, 6.496222340501845e-05, 6.496222340501845e-05, 6.496222340501845e-05, 6.496222340501845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.496222340501845e-05

Optimization complete. Final v2v error: 6.684444427490234 mm

Highest mean error: 8.422246932983398 mm for frame 120

Lowest mean error: 5.875677585601807 mm for frame 103

Saving results

Total time: 61.15360879898071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948452
Iteration 2/25 | Loss: 0.00349143
Iteration 3/25 | Loss: 0.00243626
Iteration 4/25 | Loss: 0.00188755
Iteration 5/25 | Loss: 0.00181326
Iteration 6/25 | Loss: 0.00170479
Iteration 7/25 | Loss: 0.00167969
Iteration 8/25 | Loss: 0.00166954
Iteration 9/25 | Loss: 0.00160047
Iteration 10/25 | Loss: 0.00156806
Iteration 11/25 | Loss: 0.00153502
Iteration 12/25 | Loss: 0.00154661
Iteration 13/25 | Loss: 0.00154067
Iteration 14/25 | Loss: 0.00151556
Iteration 15/25 | Loss: 0.00149597
Iteration 16/25 | Loss: 0.00149178
Iteration 17/25 | Loss: 0.00149895
Iteration 18/25 | Loss: 0.00148686
Iteration 19/25 | Loss: 0.00148321
Iteration 20/25 | Loss: 0.00148168
Iteration 21/25 | Loss: 0.00148056
Iteration 22/25 | Loss: 0.00147988
Iteration 23/25 | Loss: 0.00148628
Iteration 24/25 | Loss: 0.00147297
Iteration 25/25 | Loss: 0.00147233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72309399
Iteration 2/25 | Loss: 0.00672002
Iteration 3/25 | Loss: 0.00650645
Iteration 4/25 | Loss: 0.00650644
Iteration 5/25 | Loss: 0.00650644
Iteration 6/25 | Loss: 0.00650644
Iteration 7/25 | Loss: 0.00650644
Iteration 8/25 | Loss: 0.00650644
Iteration 9/25 | Loss: 0.00650644
Iteration 10/25 | Loss: 0.00650644
Iteration 11/25 | Loss: 0.00650644
Iteration 12/25 | Loss: 0.00650644
Iteration 13/25 | Loss: 0.00650644
Iteration 14/25 | Loss: 0.00650644
Iteration 15/25 | Loss: 0.00650644
Iteration 16/25 | Loss: 0.00650644
Iteration 17/25 | Loss: 0.00650644
Iteration 18/25 | Loss: 0.00650644
Iteration 19/25 | Loss: 0.00650644
Iteration 20/25 | Loss: 0.00650644
Iteration 21/25 | Loss: 0.00650644
Iteration 22/25 | Loss: 0.00650644
Iteration 23/25 | Loss: 0.00650644
Iteration 24/25 | Loss: 0.00650644
Iteration 25/25 | Loss: 0.00650644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00650644
Iteration 2/1000 | Loss: 0.00095184
Iteration 3/1000 | Loss: 0.00617487
Iteration 4/1000 | Loss: 0.00326578
Iteration 5/1000 | Loss: 0.01259807
Iteration 6/1000 | Loss: 0.00892886
Iteration 7/1000 | Loss: 0.00542572
Iteration 8/1000 | Loss: 0.00116525
Iteration 9/1000 | Loss: 0.00122804
Iteration 10/1000 | Loss: 0.00286385
Iteration 11/1000 | Loss: 0.00547634
Iteration 12/1000 | Loss: 0.00262957
Iteration 13/1000 | Loss: 0.00298045
Iteration 14/1000 | Loss: 0.00164813
Iteration 15/1000 | Loss: 0.00525050
Iteration 16/1000 | Loss: 0.00344518
Iteration 17/1000 | Loss: 0.00273883
Iteration 18/1000 | Loss: 0.00149994
Iteration 19/1000 | Loss: 0.00232545
Iteration 20/1000 | Loss: 0.00113692
Iteration 21/1000 | Loss: 0.00160390
Iteration 22/1000 | Loss: 0.00075531
Iteration 23/1000 | Loss: 0.00173460
Iteration 24/1000 | Loss: 0.00097777
Iteration 25/1000 | Loss: 0.00181380
Iteration 26/1000 | Loss: 0.00109569
Iteration 27/1000 | Loss: 0.00121651
Iteration 28/1000 | Loss: 0.00152001
Iteration 29/1000 | Loss: 0.00055644
Iteration 30/1000 | Loss: 0.00040140
Iteration 31/1000 | Loss: 0.00028061
Iteration 32/1000 | Loss: 0.00159793
Iteration 33/1000 | Loss: 0.00043082
Iteration 34/1000 | Loss: 0.00067002
Iteration 35/1000 | Loss: 0.00197645
Iteration 36/1000 | Loss: 0.00125038
Iteration 37/1000 | Loss: 0.00090423
Iteration 38/1000 | Loss: 0.00114610
Iteration 39/1000 | Loss: 0.00158203
Iteration 40/1000 | Loss: 0.00278616
Iteration 41/1000 | Loss: 0.00072825
Iteration 42/1000 | Loss: 0.00138229
Iteration 43/1000 | Loss: 0.00027849
Iteration 44/1000 | Loss: 0.00019294
Iteration 45/1000 | Loss: 0.00016742
Iteration 46/1000 | Loss: 0.00040303
Iteration 47/1000 | Loss: 0.00111245
Iteration 48/1000 | Loss: 0.00019128
Iteration 49/1000 | Loss: 0.00034007
Iteration 50/1000 | Loss: 0.00127003
Iteration 51/1000 | Loss: 0.00055682
Iteration 52/1000 | Loss: 0.00015077
Iteration 53/1000 | Loss: 0.00013788
Iteration 54/1000 | Loss: 0.00031166
Iteration 55/1000 | Loss: 0.00010579
Iteration 56/1000 | Loss: 0.00068373
Iteration 57/1000 | Loss: 0.00151694
Iteration 58/1000 | Loss: 0.00066483
Iteration 59/1000 | Loss: 0.00017469
Iteration 60/1000 | Loss: 0.00036346
Iteration 61/1000 | Loss: 0.00028178
Iteration 62/1000 | Loss: 0.00030354
Iteration 63/1000 | Loss: 0.00099236
Iteration 64/1000 | Loss: 0.00258104
Iteration 65/1000 | Loss: 0.00089555
Iteration 66/1000 | Loss: 0.00130626
Iteration 67/1000 | Loss: 0.00071179
Iteration 68/1000 | Loss: 0.00048056
Iteration 69/1000 | Loss: 0.00049985
Iteration 70/1000 | Loss: 0.00017862
Iteration 71/1000 | Loss: 0.00009992
Iteration 72/1000 | Loss: 0.00029022
Iteration 73/1000 | Loss: 0.00028903
Iteration 74/1000 | Loss: 0.00017823
Iteration 75/1000 | Loss: 0.00018596
Iteration 76/1000 | Loss: 0.00025536
Iteration 77/1000 | Loss: 0.00048325
Iteration 78/1000 | Loss: 0.00072259
Iteration 79/1000 | Loss: 0.00088938
Iteration 80/1000 | Loss: 0.00024645
Iteration 81/1000 | Loss: 0.00018072
Iteration 82/1000 | Loss: 0.00012782
Iteration 83/1000 | Loss: 0.00018190
Iteration 84/1000 | Loss: 0.00014627
Iteration 85/1000 | Loss: 0.00019152
Iteration 86/1000 | Loss: 0.00128194
Iteration 87/1000 | Loss: 0.00053657
Iteration 88/1000 | Loss: 0.00035747
Iteration 89/1000 | Loss: 0.00039973
Iteration 90/1000 | Loss: 0.00015221
Iteration 91/1000 | Loss: 0.00035827
Iteration 92/1000 | Loss: 0.00007017
Iteration 93/1000 | Loss: 0.00022912
Iteration 94/1000 | Loss: 0.00007661
Iteration 95/1000 | Loss: 0.00006893
Iteration 96/1000 | Loss: 0.00006514
Iteration 97/1000 | Loss: 0.00041699
Iteration 98/1000 | Loss: 0.00031293
Iteration 99/1000 | Loss: 0.00031308
Iteration 100/1000 | Loss: 0.00039826
Iteration 101/1000 | Loss: 0.00062729
Iteration 102/1000 | Loss: 0.00012345
Iteration 103/1000 | Loss: 0.00048150
Iteration 104/1000 | Loss: 0.00040533
Iteration 105/1000 | Loss: 0.00018275
Iteration 106/1000 | Loss: 0.00019930
Iteration 107/1000 | Loss: 0.00091835
Iteration 108/1000 | Loss: 0.00028203
Iteration 109/1000 | Loss: 0.00032580
Iteration 110/1000 | Loss: 0.00025231
Iteration 111/1000 | Loss: 0.00007317
Iteration 112/1000 | Loss: 0.00006536
Iteration 113/1000 | Loss: 0.00012657
Iteration 114/1000 | Loss: 0.00032101
Iteration 115/1000 | Loss: 0.00047720
Iteration 116/1000 | Loss: 0.00041913
Iteration 117/1000 | Loss: 0.00015839
Iteration 118/1000 | Loss: 0.00006251
Iteration 119/1000 | Loss: 0.00036216
Iteration 120/1000 | Loss: 0.00039984
Iteration 121/1000 | Loss: 0.00043282
Iteration 122/1000 | Loss: 0.00182972
Iteration 123/1000 | Loss: 0.00054851
Iteration 124/1000 | Loss: 0.00020193
Iteration 125/1000 | Loss: 0.00089960
Iteration 126/1000 | Loss: 0.00045589
Iteration 127/1000 | Loss: 0.00019197
Iteration 128/1000 | Loss: 0.00052098
Iteration 129/1000 | Loss: 0.00021893
Iteration 130/1000 | Loss: 0.00055418
Iteration 131/1000 | Loss: 0.00015056
Iteration 132/1000 | Loss: 0.00012477
Iteration 133/1000 | Loss: 0.00041447
Iteration 134/1000 | Loss: 0.00006938
Iteration 135/1000 | Loss: 0.00009596
Iteration 136/1000 | Loss: 0.00006893
Iteration 137/1000 | Loss: 0.00069646
Iteration 138/1000 | Loss: 0.00044134
Iteration 139/1000 | Loss: 0.00065251
Iteration 140/1000 | Loss: 0.00067959
Iteration 141/1000 | Loss: 0.00008587
Iteration 142/1000 | Loss: 0.00058829
Iteration 143/1000 | Loss: 0.00042583
Iteration 144/1000 | Loss: 0.00074499
Iteration 145/1000 | Loss: 0.00056023
Iteration 146/1000 | Loss: 0.00014173
Iteration 147/1000 | Loss: 0.00018038
Iteration 148/1000 | Loss: 0.00006987
Iteration 149/1000 | Loss: 0.00019416
Iteration 150/1000 | Loss: 0.00006942
Iteration 151/1000 | Loss: 0.00006393
Iteration 152/1000 | Loss: 0.00065019
Iteration 153/1000 | Loss: 0.00024719
Iteration 154/1000 | Loss: 0.00017982
Iteration 155/1000 | Loss: 0.00005931
Iteration 156/1000 | Loss: 0.00005520
Iteration 157/1000 | Loss: 0.00064085
Iteration 158/1000 | Loss: 0.00006573
Iteration 159/1000 | Loss: 0.00009061
Iteration 160/1000 | Loss: 0.00005853
Iteration 161/1000 | Loss: 0.00005394
Iteration 162/1000 | Loss: 0.00005448
Iteration 163/1000 | Loss: 0.00064238
Iteration 164/1000 | Loss: 0.00006302
Iteration 165/1000 | Loss: 0.00042722
Iteration 166/1000 | Loss: 0.00007908
Iteration 167/1000 | Loss: 0.00005497
Iteration 168/1000 | Loss: 0.00005093
Iteration 169/1000 | Loss: 0.00004934
Iteration 170/1000 | Loss: 0.00005602
Iteration 171/1000 | Loss: 0.00005066
Iteration 172/1000 | Loss: 0.00124261
Iteration 173/1000 | Loss: 0.00188026
Iteration 174/1000 | Loss: 0.00131619
Iteration 175/1000 | Loss: 0.00106527
Iteration 176/1000 | Loss: 0.00032602
Iteration 177/1000 | Loss: 0.00006997
Iteration 178/1000 | Loss: 0.00005890
Iteration 179/1000 | Loss: 0.00006526
Iteration 180/1000 | Loss: 0.00005343
Iteration 181/1000 | Loss: 0.00004934
Iteration 182/1000 | Loss: 0.00086758
Iteration 183/1000 | Loss: 0.00170612
Iteration 184/1000 | Loss: 0.00031319
Iteration 185/1000 | Loss: 0.00064879
Iteration 186/1000 | Loss: 0.00009506
Iteration 187/1000 | Loss: 0.00005119
Iteration 188/1000 | Loss: 0.00004756
Iteration 189/1000 | Loss: 0.00004689
Iteration 190/1000 | Loss: 0.00056168
Iteration 191/1000 | Loss: 0.00025991
Iteration 192/1000 | Loss: 0.00033258
Iteration 193/1000 | Loss: 0.00014585
Iteration 194/1000 | Loss: 0.00004578
Iteration 195/1000 | Loss: 0.00011974
Iteration 196/1000 | Loss: 0.00004431
Iteration 197/1000 | Loss: 0.00004790
Iteration 198/1000 | Loss: 0.00004231
Iteration 199/1000 | Loss: 0.00004171
Iteration 200/1000 | Loss: 0.00004135
Iteration 201/1000 | Loss: 0.00021181
Iteration 202/1000 | Loss: 0.00050001
Iteration 203/1000 | Loss: 0.00019848
Iteration 204/1000 | Loss: 0.00008193
Iteration 205/1000 | Loss: 0.00004370
Iteration 206/1000 | Loss: 0.00006004
Iteration 207/1000 | Loss: 0.00004840
Iteration 208/1000 | Loss: 0.00004182
Iteration 209/1000 | Loss: 0.00064907
Iteration 210/1000 | Loss: 0.00117834
Iteration 211/1000 | Loss: 0.00020625
Iteration 212/1000 | Loss: 0.00036116
Iteration 213/1000 | Loss: 0.00004920
Iteration 214/1000 | Loss: 0.00004427
Iteration 215/1000 | Loss: 0.00022475
Iteration 216/1000 | Loss: 0.00134593
Iteration 217/1000 | Loss: 0.00010045
Iteration 218/1000 | Loss: 0.00014936
Iteration 219/1000 | Loss: 0.00012665
Iteration 220/1000 | Loss: 0.00004464
Iteration 221/1000 | Loss: 0.00015023
Iteration 222/1000 | Loss: 0.00070524
Iteration 223/1000 | Loss: 0.00014792
Iteration 224/1000 | Loss: 0.00008289
Iteration 225/1000 | Loss: 0.00055026
Iteration 226/1000 | Loss: 0.00040231
Iteration 227/1000 | Loss: 0.00005292
Iteration 228/1000 | Loss: 0.00038531
Iteration 229/1000 | Loss: 0.00029078
Iteration 230/1000 | Loss: 0.00039596
Iteration 231/1000 | Loss: 0.00019352
Iteration 232/1000 | Loss: 0.00012610
Iteration 233/1000 | Loss: 0.00050885
Iteration 234/1000 | Loss: 0.00040684
Iteration 235/1000 | Loss: 0.00033161
Iteration 236/1000 | Loss: 0.00028218
Iteration 237/1000 | Loss: 0.00033830
Iteration 238/1000 | Loss: 0.00014129
Iteration 239/1000 | Loss: 0.00034950
Iteration 240/1000 | Loss: 0.00026542
Iteration 241/1000 | Loss: 0.00081677
Iteration 242/1000 | Loss: 0.00024070
Iteration 243/1000 | Loss: 0.00009610
Iteration 244/1000 | Loss: 0.00036812
Iteration 245/1000 | Loss: 0.00093770
Iteration 246/1000 | Loss: 0.00035314
Iteration 247/1000 | Loss: 0.00027755
Iteration 248/1000 | Loss: 0.00073327
Iteration 249/1000 | Loss: 0.00019670
Iteration 250/1000 | Loss: 0.00053886
Iteration 251/1000 | Loss: 0.00048477
Iteration 252/1000 | Loss: 0.00005588
Iteration 253/1000 | Loss: 0.00008382
Iteration 254/1000 | Loss: 0.00003899
Iteration 255/1000 | Loss: 0.00034329
Iteration 256/1000 | Loss: 0.00025068
Iteration 257/1000 | Loss: 0.00058945
Iteration 258/1000 | Loss: 0.00090661
Iteration 259/1000 | Loss: 0.00004719
Iteration 260/1000 | Loss: 0.00003564
Iteration 261/1000 | Loss: 0.00003313
Iteration 262/1000 | Loss: 0.00003165
Iteration 263/1000 | Loss: 0.00019467
Iteration 264/1000 | Loss: 0.00003021
Iteration 265/1000 | Loss: 0.00002963
Iteration 266/1000 | Loss: 0.00002923
Iteration 267/1000 | Loss: 0.00002901
Iteration 268/1000 | Loss: 0.00002894
Iteration 269/1000 | Loss: 0.00002891
Iteration 270/1000 | Loss: 0.00002883
Iteration 271/1000 | Loss: 0.00002880
Iteration 272/1000 | Loss: 0.00002879
Iteration 273/1000 | Loss: 0.00002877
Iteration 274/1000 | Loss: 0.00002876
Iteration 275/1000 | Loss: 0.00002875
Iteration 276/1000 | Loss: 0.00002875
Iteration 277/1000 | Loss: 0.00002874
Iteration 278/1000 | Loss: 0.00002874
Iteration 279/1000 | Loss: 0.00002873
Iteration 280/1000 | Loss: 0.00002873
Iteration 281/1000 | Loss: 0.00002873
Iteration 282/1000 | Loss: 0.00002873
Iteration 283/1000 | Loss: 0.00002873
Iteration 284/1000 | Loss: 0.00002872
Iteration 285/1000 | Loss: 0.00002872
Iteration 286/1000 | Loss: 0.00002872
Iteration 287/1000 | Loss: 0.00002872
Iteration 288/1000 | Loss: 0.00002872
Iteration 289/1000 | Loss: 0.00002872
Iteration 290/1000 | Loss: 0.00002872
Iteration 291/1000 | Loss: 0.00002872
Iteration 292/1000 | Loss: 0.00002871
Iteration 293/1000 | Loss: 0.00002871
Iteration 294/1000 | Loss: 0.00002871
Iteration 295/1000 | Loss: 0.00002870
Iteration 296/1000 | Loss: 0.00002870
Iteration 297/1000 | Loss: 0.00002870
Iteration 298/1000 | Loss: 0.00002869
Iteration 299/1000 | Loss: 0.00002869
Iteration 300/1000 | Loss: 0.00002869
Iteration 301/1000 | Loss: 0.00002868
Iteration 302/1000 | Loss: 0.00002868
Iteration 303/1000 | Loss: 0.00002868
Iteration 304/1000 | Loss: 0.00002867
Iteration 305/1000 | Loss: 0.00002867
Iteration 306/1000 | Loss: 0.00002867
Iteration 307/1000 | Loss: 0.00002867
Iteration 308/1000 | Loss: 0.00002867
Iteration 309/1000 | Loss: 0.00002866
Iteration 310/1000 | Loss: 0.00002866
Iteration 311/1000 | Loss: 0.00002866
Iteration 312/1000 | Loss: 0.00002866
Iteration 313/1000 | Loss: 0.00002866
Iteration 314/1000 | Loss: 0.00002866
Iteration 315/1000 | Loss: 0.00002865
Iteration 316/1000 | Loss: 0.00002865
Iteration 317/1000 | Loss: 0.00002865
Iteration 318/1000 | Loss: 0.00002865
Iteration 319/1000 | Loss: 0.00002865
Iteration 320/1000 | Loss: 0.00002865
Iteration 321/1000 | Loss: 0.00002865
Iteration 322/1000 | Loss: 0.00002865
Iteration 323/1000 | Loss: 0.00002865
Iteration 324/1000 | Loss: 0.00002864
Iteration 325/1000 | Loss: 0.00002864
Iteration 326/1000 | Loss: 0.00002864
Iteration 327/1000 | Loss: 0.00002864
Iteration 328/1000 | Loss: 0.00002863
Iteration 329/1000 | Loss: 0.00002863
Iteration 330/1000 | Loss: 0.00002863
Iteration 331/1000 | Loss: 0.00002862
Iteration 332/1000 | Loss: 0.00002862
Iteration 333/1000 | Loss: 0.00002862
Iteration 334/1000 | Loss: 0.00002862
Iteration 335/1000 | Loss: 0.00002862
Iteration 336/1000 | Loss: 0.00002861
Iteration 337/1000 | Loss: 0.00002861
Iteration 338/1000 | Loss: 0.00002861
Iteration 339/1000 | Loss: 0.00002861
Iteration 340/1000 | Loss: 0.00002861
Iteration 341/1000 | Loss: 0.00002861
Iteration 342/1000 | Loss: 0.00002861
Iteration 343/1000 | Loss: 0.00002860
Iteration 344/1000 | Loss: 0.00002860
Iteration 345/1000 | Loss: 0.00002860
Iteration 346/1000 | Loss: 0.00002860
Iteration 347/1000 | Loss: 0.00002860
Iteration 348/1000 | Loss: 0.00002860
Iteration 349/1000 | Loss: 0.00002860
Iteration 350/1000 | Loss: 0.00002860
Iteration 351/1000 | Loss: 0.00002860
Iteration 352/1000 | Loss: 0.00002860
Iteration 353/1000 | Loss: 0.00002860
Iteration 354/1000 | Loss: 0.00002860
Iteration 355/1000 | Loss: 0.00002859
Iteration 356/1000 | Loss: 0.00002859
Iteration 357/1000 | Loss: 0.00002859
Iteration 358/1000 | Loss: 0.00002859
Iteration 359/1000 | Loss: 0.00002859
Iteration 360/1000 | Loss: 0.00002858
Iteration 361/1000 | Loss: 0.00002858
Iteration 362/1000 | Loss: 0.00002858
Iteration 363/1000 | Loss: 0.00002858
Iteration 364/1000 | Loss: 0.00002858
Iteration 365/1000 | Loss: 0.00002857
Iteration 366/1000 | Loss: 0.00002857
Iteration 367/1000 | Loss: 0.00002857
Iteration 368/1000 | Loss: 0.00002857
Iteration 369/1000 | Loss: 0.00002857
Iteration 370/1000 | Loss: 0.00002857
Iteration 371/1000 | Loss: 0.00002857
Iteration 372/1000 | Loss: 0.00002857
Iteration 373/1000 | Loss: 0.00002857
Iteration 374/1000 | Loss: 0.00002857
Iteration 375/1000 | Loss: 0.00002856
Iteration 376/1000 | Loss: 0.00002856
Iteration 377/1000 | Loss: 0.00002856
Iteration 378/1000 | Loss: 0.00002856
Iteration 379/1000 | Loss: 0.00002856
Iteration 380/1000 | Loss: 0.00002856
Iteration 381/1000 | Loss: 0.00002856
Iteration 382/1000 | Loss: 0.00002856
Iteration 383/1000 | Loss: 0.00002856
Iteration 384/1000 | Loss: 0.00002856
Iteration 385/1000 | Loss: 0.00002856
Iteration 386/1000 | Loss: 0.00002856
Iteration 387/1000 | Loss: 0.00002856
Iteration 388/1000 | Loss: 0.00002856
Iteration 389/1000 | Loss: 0.00002856
Iteration 390/1000 | Loss: 0.00002856
Iteration 391/1000 | Loss: 0.00002855
Iteration 392/1000 | Loss: 0.00002855
Iteration 393/1000 | Loss: 0.00002855
Iteration 394/1000 | Loss: 0.00002855
Iteration 395/1000 | Loss: 0.00002855
Iteration 396/1000 | Loss: 0.00002855
Iteration 397/1000 | Loss: 0.00002855
Iteration 398/1000 | Loss: 0.00002855
Iteration 399/1000 | Loss: 0.00002854
Iteration 400/1000 | Loss: 0.00002854
Iteration 401/1000 | Loss: 0.00002854
Iteration 402/1000 | Loss: 0.00002854
Iteration 403/1000 | Loss: 0.00002854
Iteration 404/1000 | Loss: 0.00002854
Iteration 405/1000 | Loss: 0.00002854
Iteration 406/1000 | Loss: 0.00002854
Iteration 407/1000 | Loss: 0.00002854
Iteration 408/1000 | Loss: 0.00002854
Iteration 409/1000 | Loss: 0.00002854
Iteration 410/1000 | Loss: 0.00002854
Iteration 411/1000 | Loss: 0.00002854
Iteration 412/1000 | Loss: 0.00002854
Iteration 413/1000 | Loss: 0.00002854
Iteration 414/1000 | Loss: 0.00002854
Iteration 415/1000 | Loss: 0.00002854
Iteration 416/1000 | Loss: 0.00002854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 416. Stopping optimization.
Last 5 losses: [2.853696969395969e-05, 2.853696969395969e-05, 2.853696969395969e-05, 2.853696969395969e-05, 2.853696969395969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.853696969395969e-05

Optimization complete. Final v2v error: 4.409313201904297 mm

Highest mean error: 9.834510803222656 mm for frame 53

Lowest mean error: 3.362738609313965 mm for frame 24

Saving results

Total time: 488.0925545692444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454026
Iteration 2/25 | Loss: 0.00170896
Iteration 3/25 | Loss: 0.00157214
Iteration 4/25 | Loss: 0.00154405
Iteration 5/25 | Loss: 0.00153669
Iteration 6/25 | Loss: 0.00153485
Iteration 7/25 | Loss: 0.00153437
Iteration 8/25 | Loss: 0.00153437
Iteration 9/25 | Loss: 0.00153437
Iteration 10/25 | Loss: 0.00153437
Iteration 11/25 | Loss: 0.00153437
Iteration 12/25 | Loss: 0.00153437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015343718696385622, 0.0015343718696385622, 0.0015343718696385622, 0.0015343718696385622, 0.0015343718696385622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015343718696385622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70716512
Iteration 2/25 | Loss: 0.00461354
Iteration 3/25 | Loss: 0.00461354
Iteration 4/25 | Loss: 0.00461354
Iteration 5/25 | Loss: 0.00461354
Iteration 6/25 | Loss: 0.00461354
Iteration 7/25 | Loss: 0.00461354
Iteration 8/25 | Loss: 0.00461354
Iteration 9/25 | Loss: 0.00461354
Iteration 10/25 | Loss: 0.00461354
Iteration 11/25 | Loss: 0.00461354
Iteration 12/25 | Loss: 0.00461354
Iteration 13/25 | Loss: 0.00461354
Iteration 14/25 | Loss: 0.00461354
Iteration 15/25 | Loss: 0.00461354
Iteration 16/25 | Loss: 0.00461354
Iteration 17/25 | Loss: 0.00461354
Iteration 18/25 | Loss: 0.00461354
Iteration 19/25 | Loss: 0.00461354
Iteration 20/25 | Loss: 0.00461354
Iteration 21/25 | Loss: 0.00461354
Iteration 22/25 | Loss: 0.00461354
Iteration 23/25 | Loss: 0.00461354
Iteration 24/25 | Loss: 0.00461354
Iteration 25/25 | Loss: 0.00461354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00461354
Iteration 2/1000 | Loss: 0.00012093
Iteration 3/1000 | Loss: 0.00007274
Iteration 4/1000 | Loss: 0.00005477
Iteration 5/1000 | Loss: 0.00004700
Iteration 6/1000 | Loss: 0.00004337
Iteration 7/1000 | Loss: 0.00004144
Iteration 8/1000 | Loss: 0.00003962
Iteration 9/1000 | Loss: 0.00003845
Iteration 10/1000 | Loss: 0.00003761
Iteration 11/1000 | Loss: 0.00003703
Iteration 12/1000 | Loss: 0.00003665
Iteration 13/1000 | Loss: 0.00003636
Iteration 14/1000 | Loss: 0.00003609
Iteration 15/1000 | Loss: 0.00003585
Iteration 16/1000 | Loss: 0.00003573
Iteration 17/1000 | Loss: 0.00003570
Iteration 18/1000 | Loss: 0.00003569
Iteration 19/1000 | Loss: 0.00003569
Iteration 20/1000 | Loss: 0.00003566
Iteration 21/1000 | Loss: 0.00003566
Iteration 22/1000 | Loss: 0.00003564
Iteration 23/1000 | Loss: 0.00003563
Iteration 24/1000 | Loss: 0.00003563
Iteration 25/1000 | Loss: 0.00003560
Iteration 26/1000 | Loss: 0.00003557
Iteration 27/1000 | Loss: 0.00003556
Iteration 28/1000 | Loss: 0.00003556
Iteration 29/1000 | Loss: 0.00003554
Iteration 30/1000 | Loss: 0.00003553
Iteration 31/1000 | Loss: 0.00003553
Iteration 32/1000 | Loss: 0.00003552
Iteration 33/1000 | Loss: 0.00003552
Iteration 34/1000 | Loss: 0.00003552
Iteration 35/1000 | Loss: 0.00003551
Iteration 36/1000 | Loss: 0.00003551
Iteration 37/1000 | Loss: 0.00003551
Iteration 38/1000 | Loss: 0.00003550
Iteration 39/1000 | Loss: 0.00003550
Iteration 40/1000 | Loss: 0.00003550
Iteration 41/1000 | Loss: 0.00003549
Iteration 42/1000 | Loss: 0.00003549
Iteration 43/1000 | Loss: 0.00003549
Iteration 44/1000 | Loss: 0.00003549
Iteration 45/1000 | Loss: 0.00003548
Iteration 46/1000 | Loss: 0.00003548
Iteration 47/1000 | Loss: 0.00003548
Iteration 48/1000 | Loss: 0.00003547
Iteration 49/1000 | Loss: 0.00003547
Iteration 50/1000 | Loss: 0.00003547
Iteration 51/1000 | Loss: 0.00003547
Iteration 52/1000 | Loss: 0.00003546
Iteration 53/1000 | Loss: 0.00003546
Iteration 54/1000 | Loss: 0.00003546
Iteration 55/1000 | Loss: 0.00003546
Iteration 56/1000 | Loss: 0.00003545
Iteration 57/1000 | Loss: 0.00003545
Iteration 58/1000 | Loss: 0.00003545
Iteration 59/1000 | Loss: 0.00003545
Iteration 60/1000 | Loss: 0.00003545
Iteration 61/1000 | Loss: 0.00003544
Iteration 62/1000 | Loss: 0.00003544
Iteration 63/1000 | Loss: 0.00003544
Iteration 64/1000 | Loss: 0.00003544
Iteration 65/1000 | Loss: 0.00003543
Iteration 66/1000 | Loss: 0.00003543
Iteration 67/1000 | Loss: 0.00003543
Iteration 68/1000 | Loss: 0.00003543
Iteration 69/1000 | Loss: 0.00003542
Iteration 70/1000 | Loss: 0.00003542
Iteration 71/1000 | Loss: 0.00003542
Iteration 72/1000 | Loss: 0.00003542
Iteration 73/1000 | Loss: 0.00003541
Iteration 74/1000 | Loss: 0.00003541
Iteration 75/1000 | Loss: 0.00003541
Iteration 76/1000 | Loss: 0.00003541
Iteration 77/1000 | Loss: 0.00003541
Iteration 78/1000 | Loss: 0.00003540
Iteration 79/1000 | Loss: 0.00003540
Iteration 80/1000 | Loss: 0.00003540
Iteration 81/1000 | Loss: 0.00003540
Iteration 82/1000 | Loss: 0.00003540
Iteration 83/1000 | Loss: 0.00003540
Iteration 84/1000 | Loss: 0.00003539
Iteration 85/1000 | Loss: 0.00003539
Iteration 86/1000 | Loss: 0.00003539
Iteration 87/1000 | Loss: 0.00003538
Iteration 88/1000 | Loss: 0.00003538
Iteration 89/1000 | Loss: 0.00003538
Iteration 90/1000 | Loss: 0.00003538
Iteration 91/1000 | Loss: 0.00003538
Iteration 92/1000 | Loss: 0.00003538
Iteration 93/1000 | Loss: 0.00003538
Iteration 94/1000 | Loss: 0.00003537
Iteration 95/1000 | Loss: 0.00003537
Iteration 96/1000 | Loss: 0.00003537
Iteration 97/1000 | Loss: 0.00003537
Iteration 98/1000 | Loss: 0.00003537
Iteration 99/1000 | Loss: 0.00003537
Iteration 100/1000 | Loss: 0.00003536
Iteration 101/1000 | Loss: 0.00003536
Iteration 102/1000 | Loss: 0.00003536
Iteration 103/1000 | Loss: 0.00003536
Iteration 104/1000 | Loss: 0.00003536
Iteration 105/1000 | Loss: 0.00003536
Iteration 106/1000 | Loss: 0.00003536
Iteration 107/1000 | Loss: 0.00003536
Iteration 108/1000 | Loss: 0.00003536
Iteration 109/1000 | Loss: 0.00003536
Iteration 110/1000 | Loss: 0.00003536
Iteration 111/1000 | Loss: 0.00003535
Iteration 112/1000 | Loss: 0.00003535
Iteration 113/1000 | Loss: 0.00003535
Iteration 114/1000 | Loss: 0.00003535
Iteration 115/1000 | Loss: 0.00003535
Iteration 116/1000 | Loss: 0.00003535
Iteration 117/1000 | Loss: 0.00003535
Iteration 118/1000 | Loss: 0.00003535
Iteration 119/1000 | Loss: 0.00003535
Iteration 120/1000 | Loss: 0.00003534
Iteration 121/1000 | Loss: 0.00003534
Iteration 122/1000 | Loss: 0.00003534
Iteration 123/1000 | Loss: 0.00003534
Iteration 124/1000 | Loss: 0.00003534
Iteration 125/1000 | Loss: 0.00003534
Iteration 126/1000 | Loss: 0.00003534
Iteration 127/1000 | Loss: 0.00003534
Iteration 128/1000 | Loss: 0.00003534
Iteration 129/1000 | Loss: 0.00003534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [3.53394789271988e-05, 3.53394789271988e-05, 3.53394789271988e-05, 3.53394789271988e-05, 3.53394789271988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.53394789271988e-05

Optimization complete. Final v2v error: 5.084433078765869 mm

Highest mean error: 5.506765842437744 mm for frame 80

Lowest mean error: 4.451816082000732 mm for frame 69

Saving results

Total time: 42.07844352722168
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164956
Iteration 2/25 | Loss: 0.01164956
Iteration 3/25 | Loss: 0.01164956
Iteration 4/25 | Loss: 0.01164955
Iteration 5/25 | Loss: 0.00228526
Iteration 6/25 | Loss: 0.00209081
Iteration 7/25 | Loss: 0.00150549
Iteration 8/25 | Loss: 0.00145330
Iteration 9/25 | Loss: 0.00142823
Iteration 10/25 | Loss: 0.00142967
Iteration 11/25 | Loss: 0.00142090
Iteration 12/25 | Loss: 0.00142039
Iteration 13/25 | Loss: 0.00141235
Iteration 14/25 | Loss: 0.00140264
Iteration 15/25 | Loss: 0.00140009
Iteration 16/25 | Loss: 0.00140711
Iteration 17/25 | Loss: 0.00140971
Iteration 18/25 | Loss: 0.00139767
Iteration 19/25 | Loss: 0.00139082
Iteration 20/25 | Loss: 0.00139418
Iteration 21/25 | Loss: 0.00138614
Iteration 22/25 | Loss: 0.00138261
Iteration 23/25 | Loss: 0.00138197
Iteration 24/25 | Loss: 0.00138232
Iteration 25/25 | Loss: 0.00138137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76750052
Iteration 2/25 | Loss: 0.00481896
Iteration 3/25 | Loss: 0.00481895
Iteration 4/25 | Loss: 0.00481895
Iteration 5/25 | Loss: 0.00481895
Iteration 6/25 | Loss: 0.00481895
Iteration 7/25 | Loss: 0.00481895
Iteration 8/25 | Loss: 0.00481895
Iteration 9/25 | Loss: 0.00481895
Iteration 10/25 | Loss: 0.00481895
Iteration 11/25 | Loss: 0.00481895
Iteration 12/25 | Loss: 0.00481895
Iteration 13/25 | Loss: 0.00481895
Iteration 14/25 | Loss: 0.00481895
Iteration 15/25 | Loss: 0.00481895
Iteration 16/25 | Loss: 0.00481895
Iteration 17/25 | Loss: 0.00481895
Iteration 18/25 | Loss: 0.00481895
Iteration 19/25 | Loss: 0.00481895
Iteration 20/25 | Loss: 0.00481895
Iteration 21/25 | Loss: 0.00481895
Iteration 22/25 | Loss: 0.00481895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00481894938275218, 0.00481894938275218, 0.00481894938275218, 0.00481894938275218, 0.00481894938275218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00481894938275218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00481895
Iteration 2/1000 | Loss: 0.00012913
Iteration 3/1000 | Loss: 0.00008416
Iteration 4/1000 | Loss: 0.00007516
Iteration 5/1000 | Loss: 0.00039066
Iteration 6/1000 | Loss: 0.00029833
Iteration 7/1000 | Loss: 0.00005242
Iteration 8/1000 | Loss: 0.00006240
Iteration 9/1000 | Loss: 0.00010045
Iteration 10/1000 | Loss: 0.00004315
Iteration 11/1000 | Loss: 0.00006512
Iteration 12/1000 | Loss: 0.00004536
Iteration 13/1000 | Loss: 0.00008600
Iteration 14/1000 | Loss: 0.00003428
Iteration 15/1000 | Loss: 0.00006569
Iteration 16/1000 | Loss: 0.00003367
Iteration 17/1000 | Loss: 0.00006396
Iteration 18/1000 | Loss: 0.00003313
Iteration 19/1000 | Loss: 0.00003827
Iteration 20/1000 | Loss: 0.00014964
Iteration 21/1000 | Loss: 0.00003254
Iteration 22/1000 | Loss: 0.00003240
Iteration 23/1000 | Loss: 0.00003240
Iteration 24/1000 | Loss: 0.00003239
Iteration 25/1000 | Loss: 0.00006171
Iteration 26/1000 | Loss: 0.00003214
Iteration 27/1000 | Loss: 0.00003198
Iteration 28/1000 | Loss: 0.00003194
Iteration 29/1000 | Loss: 0.00003194
Iteration 30/1000 | Loss: 0.00003194
Iteration 31/1000 | Loss: 0.00003194
Iteration 32/1000 | Loss: 0.00003194
Iteration 33/1000 | Loss: 0.00003194
Iteration 34/1000 | Loss: 0.00003193
Iteration 35/1000 | Loss: 0.00003193
Iteration 36/1000 | Loss: 0.00003193
Iteration 37/1000 | Loss: 0.00003191
Iteration 38/1000 | Loss: 0.00008176
Iteration 39/1000 | Loss: 0.00015263
Iteration 40/1000 | Loss: 0.00003521
Iteration 41/1000 | Loss: 0.00003181
Iteration 42/1000 | Loss: 0.00003180
Iteration 43/1000 | Loss: 0.00003177
Iteration 44/1000 | Loss: 0.00003177
Iteration 45/1000 | Loss: 0.00003176
Iteration 46/1000 | Loss: 0.00003175
Iteration 47/1000 | Loss: 0.00003175
Iteration 48/1000 | Loss: 0.00003175
Iteration 49/1000 | Loss: 0.00003174
Iteration 50/1000 | Loss: 0.00003172
Iteration 51/1000 | Loss: 0.00003172
Iteration 52/1000 | Loss: 0.00003172
Iteration 53/1000 | Loss: 0.00003171
Iteration 54/1000 | Loss: 0.00003171
Iteration 55/1000 | Loss: 0.00003171
Iteration 56/1000 | Loss: 0.00003171
Iteration 57/1000 | Loss: 0.00003171
Iteration 58/1000 | Loss: 0.00003171
Iteration 59/1000 | Loss: 0.00003171
Iteration 60/1000 | Loss: 0.00003171
Iteration 61/1000 | Loss: 0.00003171
Iteration 62/1000 | Loss: 0.00007258
Iteration 63/1000 | Loss: 0.00003738
Iteration 64/1000 | Loss: 0.00005103
Iteration 65/1000 | Loss: 0.00003183
Iteration 66/1000 | Loss: 0.00003167
Iteration 67/1000 | Loss: 0.00003167
Iteration 68/1000 | Loss: 0.00003167
Iteration 69/1000 | Loss: 0.00003167
Iteration 70/1000 | Loss: 0.00003167
Iteration 71/1000 | Loss: 0.00003272
Iteration 72/1000 | Loss: 0.00003167
Iteration 73/1000 | Loss: 0.00005944
Iteration 74/1000 | Loss: 0.00003208
Iteration 75/1000 | Loss: 0.00003341
Iteration 76/1000 | Loss: 0.00003174
Iteration 77/1000 | Loss: 0.00003599
Iteration 78/1000 | Loss: 0.00003261
Iteration 79/1000 | Loss: 0.00003231
Iteration 80/1000 | Loss: 0.00003172
Iteration 81/1000 | Loss: 0.00003172
Iteration 82/1000 | Loss: 0.00003198
Iteration 83/1000 | Loss: 0.00003164
Iteration 84/1000 | Loss: 0.00003164
Iteration 85/1000 | Loss: 0.00003164
Iteration 86/1000 | Loss: 0.00003164
Iteration 87/1000 | Loss: 0.00003164
Iteration 88/1000 | Loss: 0.00003164
Iteration 89/1000 | Loss: 0.00003164
Iteration 90/1000 | Loss: 0.00003164
Iteration 91/1000 | Loss: 0.00003164
Iteration 92/1000 | Loss: 0.00003164
Iteration 93/1000 | Loss: 0.00003164
Iteration 94/1000 | Loss: 0.00003164
Iteration 95/1000 | Loss: 0.00003164
Iteration 96/1000 | Loss: 0.00003164
Iteration 97/1000 | Loss: 0.00003164
Iteration 98/1000 | Loss: 0.00003164
Iteration 99/1000 | Loss: 0.00003164
Iteration 100/1000 | Loss: 0.00003164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [3.163839937769808e-05, 3.163839937769808e-05, 3.163839937769808e-05, 3.163839937769808e-05, 3.163839937769808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.163839937769808e-05

Optimization complete. Final v2v error: 4.291589736938477 mm

Highest mean error: 18.125280380249023 mm for frame 49

Lowest mean error: 3.7434194087982178 mm for frame 174

Saving results

Total time: 100.24753832817078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444625
Iteration 2/25 | Loss: 0.00169001
Iteration 3/25 | Loss: 0.00157460
Iteration 4/25 | Loss: 0.00155777
Iteration 5/25 | Loss: 0.00155199
Iteration 6/25 | Loss: 0.00155044
Iteration 7/25 | Loss: 0.00154986
Iteration 8/25 | Loss: 0.00154986
Iteration 9/25 | Loss: 0.00154986
Iteration 10/25 | Loss: 0.00154986
Iteration 11/25 | Loss: 0.00154986
Iteration 12/25 | Loss: 0.00154986
Iteration 13/25 | Loss: 0.00154986
Iteration 14/25 | Loss: 0.00154986
Iteration 15/25 | Loss: 0.00154986
Iteration 16/25 | Loss: 0.00154986
Iteration 17/25 | Loss: 0.00154986
Iteration 18/25 | Loss: 0.00154986
Iteration 19/25 | Loss: 0.00154986
Iteration 20/25 | Loss: 0.00154986
Iteration 21/25 | Loss: 0.00154986
Iteration 22/25 | Loss: 0.00154986
Iteration 23/25 | Loss: 0.00154986
Iteration 24/25 | Loss: 0.00154986
Iteration 25/25 | Loss: 0.00154986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71967912
Iteration 2/25 | Loss: 0.00499133
Iteration 3/25 | Loss: 0.00499133
Iteration 4/25 | Loss: 0.00499133
Iteration 5/25 | Loss: 0.00499133
Iteration 6/25 | Loss: 0.00499133
Iteration 7/25 | Loss: 0.00499133
Iteration 8/25 | Loss: 0.00499133
Iteration 9/25 | Loss: 0.00499133
Iteration 10/25 | Loss: 0.00499133
Iteration 11/25 | Loss: 0.00499133
Iteration 12/25 | Loss: 0.00499133
Iteration 13/25 | Loss: 0.00499132
Iteration 14/25 | Loss: 0.00499133
Iteration 15/25 | Loss: 0.00499132
Iteration 16/25 | Loss: 0.00499132
Iteration 17/25 | Loss: 0.00499132
Iteration 18/25 | Loss: 0.00499132
Iteration 19/25 | Loss: 0.00499132
Iteration 20/25 | Loss: 0.00499132
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004991324618458748, 0.004991324618458748, 0.004991324618458748, 0.004991324618458748, 0.004991324618458748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004991324618458748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00499132
Iteration 2/1000 | Loss: 0.00012567
Iteration 3/1000 | Loss: 0.00007780
Iteration 4/1000 | Loss: 0.00005353
Iteration 5/1000 | Loss: 0.00004456
Iteration 6/1000 | Loss: 0.00003978
Iteration 7/1000 | Loss: 0.00003775
Iteration 8/1000 | Loss: 0.00003603
Iteration 9/1000 | Loss: 0.00003496
Iteration 10/1000 | Loss: 0.00003418
Iteration 11/1000 | Loss: 0.00003349
Iteration 12/1000 | Loss: 0.00003306
Iteration 13/1000 | Loss: 0.00003272
Iteration 14/1000 | Loss: 0.00003247
Iteration 15/1000 | Loss: 0.00003220
Iteration 16/1000 | Loss: 0.00003212
Iteration 17/1000 | Loss: 0.00003207
Iteration 18/1000 | Loss: 0.00003201
Iteration 19/1000 | Loss: 0.00003199
Iteration 20/1000 | Loss: 0.00003198
Iteration 21/1000 | Loss: 0.00003198
Iteration 22/1000 | Loss: 0.00003197
Iteration 23/1000 | Loss: 0.00003196
Iteration 24/1000 | Loss: 0.00003196
Iteration 25/1000 | Loss: 0.00003195
Iteration 26/1000 | Loss: 0.00003194
Iteration 27/1000 | Loss: 0.00003194
Iteration 28/1000 | Loss: 0.00003192
Iteration 29/1000 | Loss: 0.00003191
Iteration 30/1000 | Loss: 0.00003188
Iteration 31/1000 | Loss: 0.00003186
Iteration 32/1000 | Loss: 0.00003185
Iteration 33/1000 | Loss: 0.00003185
Iteration 34/1000 | Loss: 0.00003184
Iteration 35/1000 | Loss: 0.00003183
Iteration 36/1000 | Loss: 0.00003183
Iteration 37/1000 | Loss: 0.00003182
Iteration 38/1000 | Loss: 0.00003182
Iteration 39/1000 | Loss: 0.00003182
Iteration 40/1000 | Loss: 0.00003182
Iteration 41/1000 | Loss: 0.00003182
Iteration 42/1000 | Loss: 0.00003181
Iteration 43/1000 | Loss: 0.00003181
Iteration 44/1000 | Loss: 0.00003181
Iteration 45/1000 | Loss: 0.00003181
Iteration 46/1000 | Loss: 0.00003180
Iteration 47/1000 | Loss: 0.00003180
Iteration 48/1000 | Loss: 0.00003179
Iteration 49/1000 | Loss: 0.00003179
Iteration 50/1000 | Loss: 0.00003179
Iteration 51/1000 | Loss: 0.00003179
Iteration 52/1000 | Loss: 0.00003178
Iteration 53/1000 | Loss: 0.00003178
Iteration 54/1000 | Loss: 0.00003178
Iteration 55/1000 | Loss: 0.00003178
Iteration 56/1000 | Loss: 0.00003178
Iteration 57/1000 | Loss: 0.00003177
Iteration 58/1000 | Loss: 0.00003177
Iteration 59/1000 | Loss: 0.00003176
Iteration 60/1000 | Loss: 0.00003176
Iteration 61/1000 | Loss: 0.00003176
Iteration 62/1000 | Loss: 0.00003175
Iteration 63/1000 | Loss: 0.00003175
Iteration 64/1000 | Loss: 0.00003174
Iteration 65/1000 | Loss: 0.00003174
Iteration 66/1000 | Loss: 0.00003174
Iteration 67/1000 | Loss: 0.00003173
Iteration 68/1000 | Loss: 0.00003173
Iteration 69/1000 | Loss: 0.00003173
Iteration 70/1000 | Loss: 0.00003173
Iteration 71/1000 | Loss: 0.00003172
Iteration 72/1000 | Loss: 0.00003172
Iteration 73/1000 | Loss: 0.00003172
Iteration 74/1000 | Loss: 0.00003172
Iteration 75/1000 | Loss: 0.00003171
Iteration 76/1000 | Loss: 0.00003171
Iteration 77/1000 | Loss: 0.00003171
Iteration 78/1000 | Loss: 0.00003170
Iteration 79/1000 | Loss: 0.00003170
Iteration 80/1000 | Loss: 0.00003170
Iteration 81/1000 | Loss: 0.00003170
Iteration 82/1000 | Loss: 0.00003170
Iteration 83/1000 | Loss: 0.00003169
Iteration 84/1000 | Loss: 0.00003169
Iteration 85/1000 | Loss: 0.00003169
Iteration 86/1000 | Loss: 0.00003168
Iteration 87/1000 | Loss: 0.00003168
Iteration 88/1000 | Loss: 0.00003167
Iteration 89/1000 | Loss: 0.00003167
Iteration 90/1000 | Loss: 0.00003167
Iteration 91/1000 | Loss: 0.00003167
Iteration 92/1000 | Loss: 0.00003167
Iteration 93/1000 | Loss: 0.00003166
Iteration 94/1000 | Loss: 0.00003166
Iteration 95/1000 | Loss: 0.00003166
Iteration 96/1000 | Loss: 0.00003166
Iteration 97/1000 | Loss: 0.00003166
Iteration 98/1000 | Loss: 0.00003166
Iteration 99/1000 | Loss: 0.00003166
Iteration 100/1000 | Loss: 0.00003166
Iteration 101/1000 | Loss: 0.00003166
Iteration 102/1000 | Loss: 0.00003166
Iteration 103/1000 | Loss: 0.00003166
Iteration 104/1000 | Loss: 0.00003166
Iteration 105/1000 | Loss: 0.00003165
Iteration 106/1000 | Loss: 0.00003165
Iteration 107/1000 | Loss: 0.00003165
Iteration 108/1000 | Loss: 0.00003165
Iteration 109/1000 | Loss: 0.00003164
Iteration 110/1000 | Loss: 0.00003164
Iteration 111/1000 | Loss: 0.00003164
Iteration 112/1000 | Loss: 0.00003164
Iteration 113/1000 | Loss: 0.00003164
Iteration 114/1000 | Loss: 0.00003164
Iteration 115/1000 | Loss: 0.00003164
Iteration 116/1000 | Loss: 0.00003164
Iteration 117/1000 | Loss: 0.00003163
Iteration 118/1000 | Loss: 0.00003163
Iteration 119/1000 | Loss: 0.00003163
Iteration 120/1000 | Loss: 0.00003163
Iteration 121/1000 | Loss: 0.00003163
Iteration 122/1000 | Loss: 0.00003163
Iteration 123/1000 | Loss: 0.00003163
Iteration 124/1000 | Loss: 0.00003163
Iteration 125/1000 | Loss: 0.00003163
Iteration 126/1000 | Loss: 0.00003163
Iteration 127/1000 | Loss: 0.00003163
Iteration 128/1000 | Loss: 0.00003162
Iteration 129/1000 | Loss: 0.00003162
Iteration 130/1000 | Loss: 0.00003162
Iteration 131/1000 | Loss: 0.00003162
Iteration 132/1000 | Loss: 0.00003162
Iteration 133/1000 | Loss: 0.00003161
Iteration 134/1000 | Loss: 0.00003161
Iteration 135/1000 | Loss: 0.00003161
Iteration 136/1000 | Loss: 0.00003161
Iteration 137/1000 | Loss: 0.00003161
Iteration 138/1000 | Loss: 0.00003160
Iteration 139/1000 | Loss: 0.00003160
Iteration 140/1000 | Loss: 0.00003160
Iteration 141/1000 | Loss: 0.00003160
Iteration 142/1000 | Loss: 0.00003160
Iteration 143/1000 | Loss: 0.00003160
Iteration 144/1000 | Loss: 0.00003160
Iteration 145/1000 | Loss: 0.00003160
Iteration 146/1000 | Loss: 0.00003160
Iteration 147/1000 | Loss: 0.00003160
Iteration 148/1000 | Loss: 0.00003159
Iteration 149/1000 | Loss: 0.00003159
Iteration 150/1000 | Loss: 0.00003159
Iteration 151/1000 | Loss: 0.00003159
Iteration 152/1000 | Loss: 0.00003159
Iteration 153/1000 | Loss: 0.00003159
Iteration 154/1000 | Loss: 0.00003159
Iteration 155/1000 | Loss: 0.00003159
Iteration 156/1000 | Loss: 0.00003159
Iteration 157/1000 | Loss: 0.00003159
Iteration 158/1000 | Loss: 0.00003159
Iteration 159/1000 | Loss: 0.00003159
Iteration 160/1000 | Loss: 0.00003158
Iteration 161/1000 | Loss: 0.00003158
Iteration 162/1000 | Loss: 0.00003158
Iteration 163/1000 | Loss: 0.00003158
Iteration 164/1000 | Loss: 0.00003158
Iteration 165/1000 | Loss: 0.00003158
Iteration 166/1000 | Loss: 0.00003158
Iteration 167/1000 | Loss: 0.00003158
Iteration 168/1000 | Loss: 0.00003158
Iteration 169/1000 | Loss: 0.00003157
Iteration 170/1000 | Loss: 0.00003157
Iteration 171/1000 | Loss: 0.00003157
Iteration 172/1000 | Loss: 0.00003157
Iteration 173/1000 | Loss: 0.00003157
Iteration 174/1000 | Loss: 0.00003157
Iteration 175/1000 | Loss: 0.00003157
Iteration 176/1000 | Loss: 0.00003157
Iteration 177/1000 | Loss: 0.00003157
Iteration 178/1000 | Loss: 0.00003157
Iteration 179/1000 | Loss: 0.00003157
Iteration 180/1000 | Loss: 0.00003156
Iteration 181/1000 | Loss: 0.00003156
Iteration 182/1000 | Loss: 0.00003156
Iteration 183/1000 | Loss: 0.00003156
Iteration 184/1000 | Loss: 0.00003156
Iteration 185/1000 | Loss: 0.00003156
Iteration 186/1000 | Loss: 0.00003156
Iteration 187/1000 | Loss: 0.00003156
Iteration 188/1000 | Loss: 0.00003156
Iteration 189/1000 | Loss: 0.00003156
Iteration 190/1000 | Loss: 0.00003156
Iteration 191/1000 | Loss: 0.00003156
Iteration 192/1000 | Loss: 0.00003156
Iteration 193/1000 | Loss: 0.00003155
Iteration 194/1000 | Loss: 0.00003155
Iteration 195/1000 | Loss: 0.00003155
Iteration 196/1000 | Loss: 0.00003155
Iteration 197/1000 | Loss: 0.00003155
Iteration 198/1000 | Loss: 0.00003155
Iteration 199/1000 | Loss: 0.00003155
Iteration 200/1000 | Loss: 0.00003155
Iteration 201/1000 | Loss: 0.00003155
Iteration 202/1000 | Loss: 0.00003155
Iteration 203/1000 | Loss: 0.00003155
Iteration 204/1000 | Loss: 0.00003155
Iteration 205/1000 | Loss: 0.00003155
Iteration 206/1000 | Loss: 0.00003155
Iteration 207/1000 | Loss: 0.00003155
Iteration 208/1000 | Loss: 0.00003155
Iteration 209/1000 | Loss: 0.00003155
Iteration 210/1000 | Loss: 0.00003155
Iteration 211/1000 | Loss: 0.00003154
Iteration 212/1000 | Loss: 0.00003154
Iteration 213/1000 | Loss: 0.00003154
Iteration 214/1000 | Loss: 0.00003154
Iteration 215/1000 | Loss: 0.00003154
Iteration 216/1000 | Loss: 0.00003154
Iteration 217/1000 | Loss: 0.00003154
Iteration 218/1000 | Loss: 0.00003154
Iteration 219/1000 | Loss: 0.00003154
Iteration 220/1000 | Loss: 0.00003154
Iteration 221/1000 | Loss: 0.00003154
Iteration 222/1000 | Loss: 0.00003154
Iteration 223/1000 | Loss: 0.00003154
Iteration 224/1000 | Loss: 0.00003154
Iteration 225/1000 | Loss: 0.00003154
Iteration 226/1000 | Loss: 0.00003154
Iteration 227/1000 | Loss: 0.00003154
Iteration 228/1000 | Loss: 0.00003154
Iteration 229/1000 | Loss: 0.00003154
Iteration 230/1000 | Loss: 0.00003153
Iteration 231/1000 | Loss: 0.00003153
Iteration 232/1000 | Loss: 0.00003153
Iteration 233/1000 | Loss: 0.00003153
Iteration 234/1000 | Loss: 0.00003153
Iteration 235/1000 | Loss: 0.00003153
Iteration 236/1000 | Loss: 0.00003153
Iteration 237/1000 | Loss: 0.00003153
Iteration 238/1000 | Loss: 0.00003153
Iteration 239/1000 | Loss: 0.00003153
Iteration 240/1000 | Loss: 0.00003153
Iteration 241/1000 | Loss: 0.00003153
Iteration 242/1000 | Loss: 0.00003153
Iteration 243/1000 | Loss: 0.00003153
Iteration 244/1000 | Loss: 0.00003153
Iteration 245/1000 | Loss: 0.00003153
Iteration 246/1000 | Loss: 0.00003153
Iteration 247/1000 | Loss: 0.00003152
Iteration 248/1000 | Loss: 0.00003152
Iteration 249/1000 | Loss: 0.00003152
Iteration 250/1000 | Loss: 0.00003152
Iteration 251/1000 | Loss: 0.00003152
Iteration 252/1000 | Loss: 0.00003152
Iteration 253/1000 | Loss: 0.00003152
Iteration 254/1000 | Loss: 0.00003152
Iteration 255/1000 | Loss: 0.00003152
Iteration 256/1000 | Loss: 0.00003152
Iteration 257/1000 | Loss: 0.00003152
Iteration 258/1000 | Loss: 0.00003152
Iteration 259/1000 | Loss: 0.00003152
Iteration 260/1000 | Loss: 0.00003152
Iteration 261/1000 | Loss: 0.00003152
Iteration 262/1000 | Loss: 0.00003151
Iteration 263/1000 | Loss: 0.00003151
Iteration 264/1000 | Loss: 0.00003151
Iteration 265/1000 | Loss: 0.00003151
Iteration 266/1000 | Loss: 0.00003151
Iteration 267/1000 | Loss: 0.00003151
Iteration 268/1000 | Loss: 0.00003151
Iteration 269/1000 | Loss: 0.00003151
Iteration 270/1000 | Loss: 0.00003151
Iteration 271/1000 | Loss: 0.00003151
Iteration 272/1000 | Loss: 0.00003151
Iteration 273/1000 | Loss: 0.00003151
Iteration 274/1000 | Loss: 0.00003151
Iteration 275/1000 | Loss: 0.00003151
Iteration 276/1000 | Loss: 0.00003151
Iteration 277/1000 | Loss: 0.00003151
Iteration 278/1000 | Loss: 0.00003151
Iteration 279/1000 | Loss: 0.00003151
Iteration 280/1000 | Loss: 0.00003151
Iteration 281/1000 | Loss: 0.00003151
Iteration 282/1000 | Loss: 0.00003151
Iteration 283/1000 | Loss: 0.00003151
Iteration 284/1000 | Loss: 0.00003151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [3.151034252368845e-05, 3.151034252368845e-05, 3.151034252368845e-05, 3.151034252368845e-05, 3.151034252368845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.151034252368845e-05

Optimization complete. Final v2v error: 4.807238578796387 mm

Highest mean error: 5.3436408042907715 mm for frame 112

Lowest mean error: 4.251554012298584 mm for frame 100

Saving results

Total time: 51.36081314086914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01147086
Iteration 2/25 | Loss: 0.00339311
Iteration 3/25 | Loss: 0.00250000
Iteration 4/25 | Loss: 0.00230563
Iteration 5/25 | Loss: 0.00223865
Iteration 6/25 | Loss: 0.00225166
Iteration 7/25 | Loss: 0.00227012
Iteration 8/25 | Loss: 0.00223037
Iteration 9/25 | Loss: 0.00214879
Iteration 10/25 | Loss: 0.00212668
Iteration 11/25 | Loss: 0.00212081
Iteration 12/25 | Loss: 0.00211363
Iteration 13/25 | Loss: 0.00210050
Iteration 14/25 | Loss: 0.00208666
Iteration 15/25 | Loss: 0.00206651
Iteration 16/25 | Loss: 0.00206158
Iteration 17/25 | Loss: 0.00205491
Iteration 18/25 | Loss: 0.00205143
Iteration 19/25 | Loss: 0.00204688
Iteration 20/25 | Loss: 0.00204287
Iteration 21/25 | Loss: 0.00204029
Iteration 22/25 | Loss: 0.00203979
Iteration 23/25 | Loss: 0.00203943
Iteration 24/25 | Loss: 0.00203967
Iteration 25/25 | Loss: 0.00203936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.66076374
Iteration 2/25 | Loss: 0.01307002
Iteration 3/25 | Loss: 0.01209132
Iteration 4/25 | Loss: 0.01209132
Iteration 5/25 | Loss: 0.01209132
Iteration 6/25 | Loss: 0.01209132
Iteration 7/25 | Loss: 0.01209131
Iteration 8/25 | Loss: 0.01209131
Iteration 9/25 | Loss: 0.01209131
Iteration 10/25 | Loss: 0.01209131
Iteration 11/25 | Loss: 0.01209131
Iteration 12/25 | Loss: 0.01209131
Iteration 13/25 | Loss: 0.01209131
Iteration 14/25 | Loss: 0.01209131
Iteration 15/25 | Loss: 0.01209131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.012091312557458878, 0.012091312557458878, 0.012091312557458878, 0.012091312557458878, 0.012091312557458878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.012091312557458878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01209131
Iteration 2/1000 | Loss: 0.00207275
Iteration 3/1000 | Loss: 0.00069893
Iteration 4/1000 | Loss: 0.00240856
Iteration 5/1000 | Loss: 0.00054536
Iteration 6/1000 | Loss: 0.00068194
Iteration 7/1000 | Loss: 0.01573513
Iteration 8/1000 | Loss: 0.00408212
Iteration 9/1000 | Loss: 0.00097043
Iteration 10/1000 | Loss: 0.00098435
Iteration 11/1000 | Loss: 0.00105026
Iteration 12/1000 | Loss: 0.00089912
Iteration 13/1000 | Loss: 0.00084014
Iteration 14/1000 | Loss: 0.00070806
Iteration 15/1000 | Loss: 0.00168554
Iteration 16/1000 | Loss: 0.00079828
Iteration 17/1000 | Loss: 0.00064940
Iteration 18/1000 | Loss: 0.00074104
Iteration 19/1000 | Loss: 0.00067289
Iteration 20/1000 | Loss: 0.00038564
Iteration 21/1000 | Loss: 0.00010616
Iteration 22/1000 | Loss: 0.00008333
Iteration 23/1000 | Loss: 0.00038708
Iteration 24/1000 | Loss: 0.00015881
Iteration 25/1000 | Loss: 0.00006790
Iteration 26/1000 | Loss: 0.00005295
Iteration 27/1000 | Loss: 0.00009424
Iteration 28/1000 | Loss: 0.00004167
Iteration 29/1000 | Loss: 0.00008577
Iteration 30/1000 | Loss: 0.00003905
Iteration 31/1000 | Loss: 0.00003680
Iteration 32/1000 | Loss: 0.00022642
Iteration 33/1000 | Loss: 0.00005595
Iteration 34/1000 | Loss: 0.00004293
Iteration 35/1000 | Loss: 0.00003523
Iteration 36/1000 | Loss: 0.00003400
Iteration 37/1000 | Loss: 0.00033147
Iteration 38/1000 | Loss: 0.00050188
Iteration 39/1000 | Loss: 0.00034803
Iteration 40/1000 | Loss: 0.00027341
Iteration 41/1000 | Loss: 0.00034347
Iteration 42/1000 | Loss: 0.00025175
Iteration 43/1000 | Loss: 0.00064106
Iteration 44/1000 | Loss: 0.00022359
Iteration 45/1000 | Loss: 0.00003533
Iteration 46/1000 | Loss: 0.00003414
Iteration 47/1000 | Loss: 0.00003316
Iteration 48/1000 | Loss: 0.00003188
Iteration 49/1000 | Loss: 0.00003229
Iteration 50/1000 | Loss: 0.00003176
Iteration 51/1000 | Loss: 0.00019643
Iteration 52/1000 | Loss: 0.00016244
Iteration 53/1000 | Loss: 0.00019567
Iteration 54/1000 | Loss: 0.00017089
Iteration 55/1000 | Loss: 0.00003189
Iteration 56/1000 | Loss: 0.00022502
Iteration 57/1000 | Loss: 0.00021054
Iteration 58/1000 | Loss: 0.00012369
Iteration 59/1000 | Loss: 0.00003217
Iteration 60/1000 | Loss: 0.00003142
Iteration 61/1000 | Loss: 0.00003180
Iteration 62/1000 | Loss: 0.00003112
Iteration 63/1000 | Loss: 0.00003111
Iteration 64/1000 | Loss: 0.00003111
Iteration 65/1000 | Loss: 0.00003109
Iteration 66/1000 | Loss: 0.00003109
Iteration 67/1000 | Loss: 0.00003108
Iteration 68/1000 | Loss: 0.00003106
Iteration 69/1000 | Loss: 0.00003170
Iteration 70/1000 | Loss: 0.00003161
Iteration 71/1000 | Loss: 0.00003169
Iteration 72/1000 | Loss: 0.00003167
Iteration 73/1000 | Loss: 0.00003439
Iteration 74/1000 | Loss: 0.00003302
Iteration 75/1000 | Loss: 0.00003522
Iteration 76/1000 | Loss: 0.00003214
Iteration 77/1000 | Loss: 0.00003178
Iteration 78/1000 | Loss: 0.00003168
Iteration 79/1000 | Loss: 0.00003107
Iteration 80/1000 | Loss: 0.00003106
Iteration 81/1000 | Loss: 0.00003106
Iteration 82/1000 | Loss: 0.00003106
Iteration 83/1000 | Loss: 0.00003106
Iteration 84/1000 | Loss: 0.00003106
Iteration 85/1000 | Loss: 0.00003106
Iteration 86/1000 | Loss: 0.00003130
Iteration 87/1000 | Loss: 0.00003129
Iteration 88/1000 | Loss: 0.00003145
Iteration 89/1000 | Loss: 0.00003144
Iteration 90/1000 | Loss: 0.00003090
Iteration 91/1000 | Loss: 0.00003090
Iteration 92/1000 | Loss: 0.00003090
Iteration 93/1000 | Loss: 0.00003089
Iteration 94/1000 | Loss: 0.00003089
Iteration 95/1000 | Loss: 0.00003114
Iteration 96/1000 | Loss: 0.00003114
Iteration 97/1000 | Loss: 0.00003120
Iteration 98/1000 | Loss: 0.00003119
Iteration 99/1000 | Loss: 0.00003146
Iteration 100/1000 | Loss: 0.00003111
Iteration 101/1000 | Loss: 0.00003120
Iteration 102/1000 | Loss: 0.00003249
Iteration 103/1000 | Loss: 0.00003194
Iteration 104/1000 | Loss: 0.00003184
Iteration 105/1000 | Loss: 0.00003120
Iteration 106/1000 | Loss: 0.00003139
Iteration 107/1000 | Loss: 0.00003139
Iteration 108/1000 | Loss: 0.00003139
Iteration 109/1000 | Loss: 0.00003536
Iteration 110/1000 | Loss: 0.00003181
Iteration 111/1000 | Loss: 0.00003173
Iteration 112/1000 | Loss: 0.00003172
Iteration 113/1000 | Loss: 0.00003298
Iteration 114/1000 | Loss: 0.00003298
Iteration 115/1000 | Loss: 0.00003171
Iteration 116/1000 | Loss: 0.00003148
Iteration 117/1000 | Loss: 0.00003133
Iteration 118/1000 | Loss: 0.00003146
Iteration 119/1000 | Loss: 0.00003308
Iteration 120/1000 | Loss: 0.00003162
Iteration 121/1000 | Loss: 0.00003173
Iteration 122/1000 | Loss: 0.00003348
Iteration 123/1000 | Loss: 0.00003154
Iteration 124/1000 | Loss: 0.00003160
Iteration 125/1000 | Loss: 0.00003352
Iteration 126/1000 | Loss: 0.00003151
Iteration 127/1000 | Loss: 0.00003177
Iteration 128/1000 | Loss: 0.00003552
Iteration 129/1000 | Loss: 0.00003179
Iteration 130/1000 | Loss: 0.00003168
Iteration 131/1000 | Loss: 0.00003344
Iteration 132/1000 | Loss: 0.00003178
Iteration 133/1000 | Loss: 0.00003166
Iteration 134/1000 | Loss: 0.00003453
Iteration 135/1000 | Loss: 0.00003179
Iteration 136/1000 | Loss: 0.00003516
Iteration 137/1000 | Loss: 0.00003180
Iteration 138/1000 | Loss: 0.00003207
Iteration 139/1000 | Loss: 0.00003199
Iteration 140/1000 | Loss: 0.00003120
Iteration 141/1000 | Loss: 0.00003130
Iteration 142/1000 | Loss: 0.00003086
Iteration 143/1000 | Loss: 0.00003086
Iteration 144/1000 | Loss: 0.00003086
Iteration 145/1000 | Loss: 0.00003086
Iteration 146/1000 | Loss: 0.00003086
Iteration 147/1000 | Loss: 0.00003086
Iteration 148/1000 | Loss: 0.00003086
Iteration 149/1000 | Loss: 0.00003086
Iteration 150/1000 | Loss: 0.00003086
Iteration 151/1000 | Loss: 0.00003086
Iteration 152/1000 | Loss: 0.00003243
Iteration 153/1000 | Loss: 0.00003214
Iteration 154/1000 | Loss: 0.00003172
Iteration 155/1000 | Loss: 0.00003333
Iteration 156/1000 | Loss: 0.00003346
Iteration 157/1000 | Loss: 0.00003376
Iteration 158/1000 | Loss: 0.00003301
Iteration 159/1000 | Loss: 0.00003406
Iteration 160/1000 | Loss: 0.00003308
Iteration 161/1000 | Loss: 0.00003390
Iteration 162/1000 | Loss: 0.00003261
Iteration 163/1000 | Loss: 0.00003505
Iteration 164/1000 | Loss: 0.00003327
Iteration 165/1000 | Loss: 0.00003155
Iteration 166/1000 | Loss: 0.00003269
Iteration 167/1000 | Loss: 0.00003195
Iteration 168/1000 | Loss: 0.00003376
Iteration 169/1000 | Loss: 0.00003205
Iteration 170/1000 | Loss: 0.00003205
Iteration 171/1000 | Loss: 0.00003380
Iteration 172/1000 | Loss: 0.00003230
Iteration 173/1000 | Loss: 0.00003224
Iteration 174/1000 | Loss: 0.00003085
Iteration 175/1000 | Loss: 0.00003100
Iteration 176/1000 | Loss: 0.00003106
Iteration 177/1000 | Loss: 0.00003106
Iteration 178/1000 | Loss: 0.00003215
Iteration 179/1000 | Loss: 0.00003288
Iteration 180/1000 | Loss: 0.00003241
Iteration 181/1000 | Loss: 0.00003306
Iteration 182/1000 | Loss: 0.00004178
Iteration 183/1000 | Loss: 0.00003171
Iteration 184/1000 | Loss: 0.00003166
Iteration 185/1000 | Loss: 0.00003381
Iteration 186/1000 | Loss: 0.00003225
Iteration 187/1000 | Loss: 0.00003258
Iteration 188/1000 | Loss: 0.00003146
Iteration 189/1000 | Loss: 0.00003291
Iteration 190/1000 | Loss: 0.00003187
Iteration 191/1000 | Loss: 0.00003390
Iteration 192/1000 | Loss: 0.00003255
Iteration 193/1000 | Loss: 0.00003430
Iteration 194/1000 | Loss: 0.00003229
Iteration 195/1000 | Loss: 0.00003193
Iteration 196/1000 | Loss: 0.00003143
Iteration 197/1000 | Loss: 0.00003142
Iteration 198/1000 | Loss: 0.00003128
Iteration 199/1000 | Loss: 0.00003116
Iteration 200/1000 | Loss: 0.00003102
Iteration 201/1000 | Loss: 0.00003398
Iteration 202/1000 | Loss: 0.00003184
Iteration 203/1000 | Loss: 0.00003194
Iteration 204/1000 | Loss: 0.00003098
Iteration 205/1000 | Loss: 0.00003105
Iteration 206/1000 | Loss: 0.00003216
Iteration 207/1000 | Loss: 0.00003153
Iteration 208/1000 | Loss: 0.00003143
Iteration 209/1000 | Loss: 0.00003128
Iteration 210/1000 | Loss: 0.00003144
Iteration 211/1000 | Loss: 0.00003126
Iteration 212/1000 | Loss: 0.00003126
Iteration 213/1000 | Loss: 0.00003126
Iteration 214/1000 | Loss: 0.00003125
Iteration 215/1000 | Loss: 0.00003133
Iteration 216/1000 | Loss: 0.00003332
Iteration 217/1000 | Loss: 0.00003183
Iteration 218/1000 | Loss: 0.00003183
Iteration 219/1000 | Loss: 0.00003180
Iteration 220/1000 | Loss: 0.00003113
Iteration 221/1000 | Loss: 0.00003154
Iteration 222/1000 | Loss: 0.00003175
Iteration 223/1000 | Loss: 0.00003159
Iteration 224/1000 | Loss: 0.00003166
Iteration 225/1000 | Loss: 0.00003137
Iteration 226/1000 | Loss: 0.00003136
Iteration 227/1000 | Loss: 0.00003284
Iteration 228/1000 | Loss: 0.00003207
Iteration 229/1000 | Loss: 0.00003167
Iteration 230/1000 | Loss: 0.00003128
Iteration 231/1000 | Loss: 0.00003128
Iteration 232/1000 | Loss: 0.00003114
Iteration 233/1000 | Loss: 0.00003114
Iteration 234/1000 | Loss: 0.00003426
Iteration 235/1000 | Loss: 0.00003196
Iteration 236/1000 | Loss: 0.00003116
Iteration 237/1000 | Loss: 0.00003115
Iteration 238/1000 | Loss: 0.00003115
Iteration 239/1000 | Loss: 0.00003214
Iteration 240/1000 | Loss: 0.00003131
Iteration 241/1000 | Loss: 0.00003135
Iteration 242/1000 | Loss: 0.00003119
Iteration 243/1000 | Loss: 0.00003111
Iteration 244/1000 | Loss: 0.00003159
Iteration 245/1000 | Loss: 0.00003157
Iteration 246/1000 | Loss: 0.00003157
Iteration 247/1000 | Loss: 0.00003156
Iteration 248/1000 | Loss: 0.00003154
Iteration 249/1000 | Loss: 0.00003132
Iteration 250/1000 | Loss: 0.00003235
Iteration 251/1000 | Loss: 0.00003177
Iteration 252/1000 | Loss: 0.00003177
Iteration 253/1000 | Loss: 0.00003149
Iteration 254/1000 | Loss: 0.00003193
Iteration 255/1000 | Loss: 0.00003178
Iteration 256/1000 | Loss: 0.00003266
Iteration 257/1000 | Loss: 0.00003177
Iteration 258/1000 | Loss: 0.00003101
Iteration 259/1000 | Loss: 0.00003255
Iteration 260/1000 | Loss: 0.00003190
Iteration 261/1000 | Loss: 0.00003131
Iteration 262/1000 | Loss: 0.00003144
Iteration 263/1000 | Loss: 0.00003143
Iteration 264/1000 | Loss: 0.00003151
Iteration 265/1000 | Loss: 0.00003122
Iteration 266/1000 | Loss: 0.00003122
Iteration 267/1000 | Loss: 0.00003319
Iteration 268/1000 | Loss: 0.00003246
Iteration 269/1000 | Loss: 0.00003150
Iteration 270/1000 | Loss: 0.00003416
Iteration 271/1000 | Loss: 0.00003212
Iteration 272/1000 | Loss: 0.00003159
Iteration 273/1000 | Loss: 0.00003415
Iteration 274/1000 | Loss: 0.00003254
Iteration 275/1000 | Loss: 0.00003187
Iteration 276/1000 | Loss: 0.00003187
Iteration 277/1000 | Loss: 0.00003410
Iteration 278/1000 | Loss: 0.00003180
Iteration 279/1000 | Loss: 0.00003193
Iteration 280/1000 | Loss: 0.00003513
Iteration 281/1000 | Loss: 0.00003201
Iteration 282/1000 | Loss: 0.00003520
Iteration 283/1000 | Loss: 0.00003211
Iteration 284/1000 | Loss: 0.00003175
Iteration 285/1000 | Loss: 0.00003271
Iteration 286/1000 | Loss: 0.00003223
Iteration 287/1000 | Loss: 0.00003365
Iteration 288/1000 | Loss: 0.00003232
Iteration 289/1000 | Loss: 0.00003154
Iteration 290/1000 | Loss: 0.00003274
Iteration 291/1000 | Loss: 0.00003252
Iteration 292/1000 | Loss: 0.00003172
Iteration 293/1000 | Loss: 0.00003276
Iteration 294/1000 | Loss: 0.00003222
Iteration 295/1000 | Loss: 0.00003154
Iteration 296/1000 | Loss: 0.00003149
Iteration 297/1000 | Loss: 0.00003177
Iteration 298/1000 | Loss: 0.00003233
Iteration 299/1000 | Loss: 0.00003232
Iteration 300/1000 | Loss: 0.00003410
Iteration 301/1000 | Loss: 0.00003201
Iteration 302/1000 | Loss: 0.00003144
Iteration 303/1000 | Loss: 0.00003479
Iteration 304/1000 | Loss: 0.00003262
Iteration 305/1000 | Loss: 0.00003465
Iteration 306/1000 | Loss: 0.00003216
Iteration 307/1000 | Loss: 0.00003150
Iteration 308/1000 | Loss: 0.00003278
Iteration 309/1000 | Loss: 0.00003266
Iteration 310/1000 | Loss: 0.00003179
Iteration 311/1000 | Loss: 0.00003113
Iteration 312/1000 | Loss: 0.00003134
Iteration 313/1000 | Loss: 0.00003172
Iteration 314/1000 | Loss: 0.00003172
Iteration 315/1000 | Loss: 0.00003094
Iteration 316/1000 | Loss: 0.00003408
Iteration 317/1000 | Loss: 0.00003206
Iteration 318/1000 | Loss: 0.00003145
Iteration 319/1000 | Loss: 0.00003305
Iteration 320/1000 | Loss: 0.00003229
Iteration 321/1000 | Loss: 0.00003171
Iteration 322/1000 | Loss: 0.00003158
Iteration 323/1000 | Loss: 0.00003354
Iteration 324/1000 | Loss: 0.00003259
Iteration 325/1000 | Loss: 0.00003446
Iteration 326/1000 | Loss: 0.00003445
Iteration 327/1000 | Loss: 0.00003445
Iteration 328/1000 | Loss: 0.00003445
Iteration 329/1000 | Loss: 0.00003445
Iteration 330/1000 | Loss: 0.00003445
Iteration 331/1000 | Loss: 0.00003445
Iteration 332/1000 | Loss: 0.00003445
Iteration 333/1000 | Loss: 0.00003410
Iteration 334/1000 | Loss: 0.00003553
Iteration 335/1000 | Loss: 0.00003944
Iteration 336/1000 | Loss: 0.00003160
Iteration 337/1000 | Loss: 0.00003220
Iteration 338/1000 | Loss: 0.00003135
Iteration 339/1000 | Loss: 0.00003179
Iteration 340/1000 | Loss: 0.00003154
Iteration 341/1000 | Loss: 0.00003154
Iteration 342/1000 | Loss: 0.00003162
Iteration 343/1000 | Loss: 0.00003107
Iteration 344/1000 | Loss: 0.00003117
Iteration 345/1000 | Loss: 0.00003200
Iteration 346/1000 | Loss: 0.00003155
Iteration 347/1000 | Loss: 0.00003187
Iteration 348/1000 | Loss: 0.00003215
Iteration 349/1000 | Loss: 0.00003649
Iteration 350/1000 | Loss: 0.00003166
Iteration 351/1000 | Loss: 0.00003200
Iteration 352/1000 | Loss: 0.00003233
Iteration 353/1000 | Loss: 0.00003185
Iteration 354/1000 | Loss: 0.00003197
Iteration 355/1000 | Loss: 0.00003220
Iteration 356/1000 | Loss: 0.00003219
Iteration 357/1000 | Loss: 0.00003219
Iteration 358/1000 | Loss: 0.00003484
Iteration 359/1000 | Loss: 0.00003154
Iteration 360/1000 | Loss: 0.00003224
Iteration 361/1000 | Loss: 0.00003142
Iteration 362/1000 | Loss: 0.00003126
Iteration 363/1000 | Loss: 0.00003138
Iteration 364/1000 | Loss: 0.00003254
Iteration 365/1000 | Loss: 0.00003177
Iteration 366/1000 | Loss: 0.00003177
Iteration 367/1000 | Loss: 0.00003181
Iteration 368/1000 | Loss: 0.00003199
Iteration 369/1000 | Loss: 0.00003120
Iteration 370/1000 | Loss: 0.00003413
Iteration 371/1000 | Loss: 0.00003262
Iteration 372/1000 | Loss: 0.00003178
Iteration 373/1000 | Loss: 0.00003171
Iteration 374/1000 | Loss: 0.00003209
Iteration 375/1000 | Loss: 0.00003341
Iteration 376/1000 | Loss: 0.00003317
Iteration 377/1000 | Loss: 0.00003194
Iteration 378/1000 | Loss: 0.00003128
Iteration 379/1000 | Loss: 0.00003138
Iteration 380/1000 | Loss: 0.00003370
Iteration 381/1000 | Loss: 0.00003266
Iteration 382/1000 | Loss: 0.00003242
Iteration 383/1000 | Loss: 0.00003230
Iteration 384/1000 | Loss: 0.00003319
Iteration 385/1000 | Loss: 0.00003209
Iteration 386/1000 | Loss: 0.00003180
Iteration 387/1000 | Loss: 0.00003629
Iteration 388/1000 | Loss: 0.00003314
Iteration 389/1000 | Loss: 0.00003394
Iteration 390/1000 | Loss: 0.00003207
Iteration 391/1000 | Loss: 0.00003260
Iteration 392/1000 | Loss: 0.00003239
Iteration 393/1000 | Loss: 0.00003394
Iteration 394/1000 | Loss: 0.00003243
Iteration 395/1000 | Loss: 0.00003918
Iteration 396/1000 | Loss: 0.00003764
Iteration 397/1000 | Loss: 0.00003256
Iteration 398/1000 | Loss: 0.00003194
Iteration 399/1000 | Loss: 0.00003122
Iteration 400/1000 | Loss: 0.00003145
Iteration 401/1000 | Loss: 0.00003471
Iteration 402/1000 | Loss: 0.00003396
Iteration 403/1000 | Loss: 0.00003359
Iteration 404/1000 | Loss: 0.00003301
Iteration 405/1000 | Loss: 0.00003393
Iteration 406/1000 | Loss: 0.00003266
Iteration 407/1000 | Loss: 0.00003454
Iteration 408/1000 | Loss: 0.00003359
Iteration 409/1000 | Loss: 0.00003203
Iteration 410/1000 | Loss: 0.00003203
Iteration 411/1000 | Loss: 0.00003513
Iteration 412/1000 | Loss: 0.00003315
Iteration 413/1000 | Loss: 0.00003136
Iteration 414/1000 | Loss: 0.00003096
Iteration 415/1000 | Loss: 0.00003102
Iteration 416/1000 | Loss: 0.00003093
Iteration 417/1000 | Loss: 0.00003099
Iteration 418/1000 | Loss: 0.00003091
Iteration 419/1000 | Loss: 0.00003090
Iteration 420/1000 | Loss: 0.00003127
Iteration 421/1000 | Loss: 0.00003117
Iteration 422/1000 | Loss: 0.00003116
Iteration 423/1000 | Loss: 0.00003076
Iteration 424/1000 | Loss: 0.00003076
Iteration 425/1000 | Loss: 0.00003075
Iteration 426/1000 | Loss: 0.00003124
Iteration 427/1000 | Loss: 0.00003113
Iteration 428/1000 | Loss: 0.00003132
Iteration 429/1000 | Loss: 0.00003157
Iteration 430/1000 | Loss: 0.00003132
Iteration 431/1000 | Loss: 0.00003125
Iteration 432/1000 | Loss: 0.00003122
Iteration 433/1000 | Loss: 0.00003174
Iteration 434/1000 | Loss: 0.00003161
Iteration 435/1000 | Loss: 0.00003106
Iteration 436/1000 | Loss: 0.00003156
Iteration 437/1000 | Loss: 0.00003155
Iteration 438/1000 | Loss: 0.00003165
Iteration 439/1000 | Loss: 0.00003102
Iteration 440/1000 | Loss: 0.00003102
Iteration 441/1000 | Loss: 0.00003158
Iteration 442/1000 | Loss: 0.00003175
Iteration 443/1000 | Loss: 0.00003342
Iteration 444/1000 | Loss: 0.00003195
Iteration 445/1000 | Loss: 0.00003128
Iteration 446/1000 | Loss: 0.00003215
Iteration 447/1000 | Loss: 0.00003165
Iteration 448/1000 | Loss: 0.00003114
Iteration 449/1000 | Loss: 0.00003112
Iteration 450/1000 | Loss: 0.00003137
Iteration 451/1000 | Loss: 0.00003194
Iteration 452/1000 | Loss: 0.00003168
Iteration 453/1000 | Loss: 0.00003168
Iteration 454/1000 | Loss: 0.00003178
Iteration 455/1000 | Loss: 0.00003155
Iteration 456/1000 | Loss: 0.00003156
Iteration 457/1000 | Loss: 0.00003204
Iteration 458/1000 | Loss: 0.00003156
Iteration 459/1000 | Loss: 0.00003170
Iteration 460/1000 | Loss: 0.00003162
Iteration 461/1000 | Loss: 0.00003087
Iteration 462/1000 | Loss: 0.00003087
Iteration 463/1000 | Loss: 0.00003101
Iteration 464/1000 | Loss: 0.00003212
Iteration 465/1000 | Loss: 0.00003276
Iteration 466/1000 | Loss: 0.00003277
Iteration 467/1000 | Loss: 0.00003182
Iteration 468/1000 | Loss: 0.00003215
Iteration 469/1000 | Loss: 0.00003199
Iteration 470/1000 | Loss: 0.00003190
Iteration 471/1000 | Loss: 0.00003162
Iteration 472/1000 | Loss: 0.00003266
Iteration 473/1000 | Loss: 0.00003165
Iteration 474/1000 | Loss: 0.00003178
Iteration 475/1000 | Loss: 0.00003181
Iteration 476/1000 | Loss: 0.00003154
Iteration 477/1000 | Loss: 0.00003194
Iteration 478/1000 | Loss: 0.00003409
Iteration 479/1000 | Loss: 0.00003358
Iteration 480/1000 | Loss: 0.00003373
Iteration 481/1000 | Loss: 0.00003330
Iteration 482/1000 | Loss: 0.00003353
Iteration 483/1000 | Loss: 0.00003260
Iteration 484/1000 | Loss: 0.00003392
Iteration 485/1000 | Loss: 0.00003271
Iteration 486/1000 | Loss: 0.00003181
Iteration 487/1000 | Loss: 0.00003128
Iteration 488/1000 | Loss: 0.00003094
Iteration 489/1000 | Loss: 0.00003093
Iteration 490/1000 | Loss: 0.00003125
Iteration 491/1000 | Loss: 0.00003170
Iteration 492/1000 | Loss: 0.00003167
Iteration 493/1000 | Loss: 0.00003493
Iteration 494/1000 | Loss: 0.00003274
Iteration 495/1000 | Loss: 0.00003161
Iteration 496/1000 | Loss: 0.00003238
Iteration 497/1000 | Loss: 0.00003205
Iteration 498/1000 | Loss: 0.00003257
Iteration 499/1000 | Loss: 0.00003206
Iteration 500/1000 | Loss: 0.00003265
Iteration 501/1000 | Loss: 0.00003188
Iteration 502/1000 | Loss: 0.00003164
Iteration 503/1000 | Loss: 0.00003189
Iteration 504/1000 | Loss: 0.00003234
Iteration 505/1000 | Loss: 0.00003191
Iteration 506/1000 | Loss: 0.00003290
Iteration 507/1000 | Loss: 0.00003186
Iteration 508/1000 | Loss: 0.00003198
Iteration 509/1000 | Loss: 0.00003453
Iteration 510/1000 | Loss: 0.00003118
Iteration 511/1000 | Loss: 0.00003267
Iteration 512/1000 | Loss: 0.00003253
Iteration 513/1000 | Loss: 0.00003234
Iteration 514/1000 | Loss: 0.00003202
Iteration 515/1000 | Loss: 0.00003197
Iteration 516/1000 | Loss: 0.00003234
Iteration 517/1000 | Loss: 0.00003219
Iteration 518/1000 | Loss: 0.00003225
Iteration 519/1000 | Loss: 0.00003225
Iteration 520/1000 | Loss: 0.00003360
Iteration 521/1000 | Loss: 0.00003244
Iteration 522/1000 | Loss: 0.00003398
Iteration 523/1000 | Loss: 0.00003301
Iteration 524/1000 | Loss: 0.00003243
Iteration 525/1000 | Loss: 0.00003289
Iteration 526/1000 | Loss: 0.00003212
Iteration 527/1000 | Loss: 0.00003170
Iteration 528/1000 | Loss: 0.00003472
Iteration 529/1000 | Loss: 0.00003320
Iteration 530/1000 | Loss: 0.00003291
Iteration 531/1000 | Loss: 0.00003176
Iteration 532/1000 | Loss: 0.00003243
Iteration 533/1000 | Loss: 0.00003201
Iteration 534/1000 | Loss: 0.00003200
Iteration 535/1000 | Loss: 0.00003099
Iteration 536/1000 | Loss: 0.00003568
Iteration 537/1000 | Loss: 0.00003379
Iteration 538/1000 | Loss: 0.00003153
Iteration 539/1000 | Loss: 0.00003298
Iteration 540/1000 | Loss: 0.00003572
Iteration 541/1000 | Loss: 0.00003336
Iteration 542/1000 | Loss: 0.00003508
Iteration 543/1000 | Loss: 0.00003341
Iteration 544/1000 | Loss: 0.00003138
Iteration 545/1000 | Loss: 0.00003124
Iteration 546/1000 | Loss: 0.00003233
Iteration 547/1000 | Loss: 0.00003248
Iteration 548/1000 | Loss: 0.00003150
Iteration 549/1000 | Loss: 0.00003148
Iteration 550/1000 | Loss: 0.00003143
Iteration 551/1000 | Loss: 0.00003075
Iteration 552/1000 | Loss: 0.00003075
Iteration 553/1000 | Loss: 0.00003075
Iteration 554/1000 | Loss: 0.00003075
Iteration 555/1000 | Loss: 0.00003075
Iteration 556/1000 | Loss: 0.00003075
Iteration 557/1000 | Loss: 0.00003075
Iteration 558/1000 | Loss: 0.00003075
Iteration 559/1000 | Loss: 0.00003075
Iteration 560/1000 | Loss: 0.00003127
Iteration 561/1000 | Loss: 0.00003207
Iteration 562/1000 | Loss: 0.00003195
Iteration 563/1000 | Loss: 0.00003464
Iteration 564/1000 | Loss: 0.00003254
Iteration 565/1000 | Loss: 0.00003143
Iteration 566/1000 | Loss: 0.00003143
Iteration 567/1000 | Loss: 0.00003075
Iteration 568/1000 | Loss: 0.00003075
Iteration 569/1000 | Loss: 0.00003075
Iteration 570/1000 | Loss: 0.00003075
Iteration 571/1000 | Loss: 0.00003074
Iteration 572/1000 | Loss: 0.00003074
Iteration 573/1000 | Loss: 0.00003074
Iteration 574/1000 | Loss: 0.00003074
Iteration 575/1000 | Loss: 0.00003074
Iteration 576/1000 | Loss: 0.00003074
Iteration 577/1000 | Loss: 0.00003074
Iteration 578/1000 | Loss: 0.00003074
Iteration 579/1000 | Loss: 0.00003074
Iteration 580/1000 | Loss: 0.00003074
Iteration 581/1000 | Loss: 0.00003074
Iteration 582/1000 | Loss: 0.00003074
Iteration 583/1000 | Loss: 0.00003073
Iteration 584/1000 | Loss: 0.00003073
Iteration 585/1000 | Loss: 0.00003073
Iteration 586/1000 | Loss: 0.00003073
Iteration 587/1000 | Loss: 0.00003073
Iteration 588/1000 | Loss: 0.00003107
Iteration 589/1000 | Loss: 0.00003163
Iteration 590/1000 | Loss: 0.00003228
Iteration 591/1000 | Loss: 0.00003119
Iteration 592/1000 | Loss: 0.00003109
Iteration 593/1000 | Loss: 0.00003141
Iteration 594/1000 | Loss: 0.00003509
Iteration 595/1000 | Loss: 0.00003184
Iteration 596/1000 | Loss: 0.00003076
Iteration 597/1000 | Loss: 0.00003075
Iteration 598/1000 | Loss: 0.00003075
Iteration 599/1000 | Loss: 0.00003075
Iteration 600/1000 | Loss: 0.00003075
Iteration 601/1000 | Loss: 0.00003075
Iteration 602/1000 | Loss: 0.00003122
Iteration 603/1000 | Loss: 0.00003192
Iteration 604/1000 | Loss: 0.00003421
Iteration 605/1000 | Loss: 0.00003342
Iteration 606/1000 | Loss: 0.00003149
Iteration 607/1000 | Loss: 0.00003437
Iteration 608/1000 | Loss: 0.00003340
Iteration 609/1000 | Loss: 0.00003480
Iteration 610/1000 | Loss: 0.00003344
Iteration 611/1000 | Loss: 0.00003137
Iteration 612/1000 | Loss: 0.00003163
Iteration 613/1000 | Loss: 0.00003432
Iteration 614/1000 | Loss: 0.00003288
Iteration 615/1000 | Loss: 0.00003312
Iteration 616/1000 | Loss: 0.00003200
Iteration 617/1000 | Loss: 0.00003179
Iteration 618/1000 | Loss: 0.00003131
Iteration 619/1000 | Loss: 0.00003113
Iteration 620/1000 | Loss: 0.00003129
Iteration 621/1000 | Loss: 0.00003163
Iteration 622/1000 | Loss: 0.00003163
Iteration 623/1000 | Loss: 0.00003163
Iteration 624/1000 | Loss: 0.00003422
Iteration 625/1000 | Loss: 0.00003176
Iteration 626/1000 | Loss: 0.00003197
Iteration 627/1000 | Loss: 0.00003205
Iteration 628/1000 | Loss: 0.00003186
Iteration 629/1000 | Loss: 0.00003185
Iteration 630/1000 | Loss: 0.00003185
Iteration 631/1000 | Loss: 0.00003185
Iteration 632/1000 | Loss: 0.00003185
Iteration 633/1000 | Loss: 0.00003184
Iteration 634/1000 | Loss: 0.00003216
Iteration 635/1000 | Loss: 0.00003240
Iteration 636/1000 | Loss: 0.00003344
Iteration 637/1000 | Loss: 0.00003255
Iteration 638/1000 | Loss: 0.00003264
Iteration 639/1000 | Loss: 0.00003220
Iteration 640/1000 | Loss: 0.00003193
Iteration 641/1000 | Loss: 0.00003200
Iteration 642/1000 | Loss: 0.00003199
Iteration 643/1000 | Loss: 0.00003199
Iteration 644/1000 | Loss: 0.00003108
Iteration 645/1000 | Loss: 0.00003144
Iteration 646/1000 | Loss: 0.00003167
Iteration 647/1000 | Loss: 0.00003228
Iteration 648/1000 | Loss: 0.00003173
Iteration 649/1000 | Loss: 0.00003214
Iteration 650/1000 | Loss: 0.00003390
Iteration 651/1000 | Loss: 0.00003137
Iteration 652/1000 | Loss: 0.00003333
Iteration 653/1000 | Loss: 0.00003147
Iteration 654/1000 | Loss: 0.00003158
Iteration 655/1000 | Loss: 0.00003107
Iteration 656/1000 | Loss: 0.00003120
Iteration 657/1000 | Loss: 0.00003156
Iteration 658/1000 | Loss: 0.00003096
Iteration 659/1000 | Loss: 0.00003095
Iteration 660/1000 | Loss: 0.00003095
Iteration 661/1000 | Loss: 0.00003100
Iteration 662/1000 | Loss: 0.00003131
Iteration 663/1000 | Loss: 0.00003184
Iteration 664/1000 | Loss: 0.00003484
Iteration 665/1000 | Loss: 0.00003331
Iteration 666/1000 | Loss: 0.00003211
Iteration 667/1000 | Loss: 0.00003164
Iteration 668/1000 | Loss: 0.00003078
Iteration 669/1000 | Loss: 0.00003110
Iteration 670/1000 | Loss: 0.00003117
Iteration 671/1000 | Loss: 0.00003180
Iteration 672/1000 | Loss: 0.00003236
Iteration 673/1000 | Loss: 0.00003132
Iteration 674/1000 | Loss: 0.00003132
Iteration 675/1000 | Loss: 0.00003339
Iteration 676/1000 | Loss: 0.00003374
Iteration 677/1000 | Loss: 0.00003161
Iteration 678/1000 | Loss: 0.00003156
Iteration 679/1000 | Loss: 0.00003087
Iteration 680/1000 | Loss: 0.00003087
Iteration 681/1000 | Loss: 0.00003087
Iteration 682/1000 | Loss: 0.00003086
Iteration 683/1000 | Loss: 0.00003086
Iteration 684/1000 | Loss: 0.00003086
Iteration 685/1000 | Loss: 0.00003086
Iteration 686/1000 | Loss: 0.00003086
Iteration 687/1000 | Loss: 0.00003086
Iteration 688/1000 | Loss: 0.00003086
Iteration 689/1000 | Loss: 0.00003086
Iteration 690/1000 | Loss: 0.00003086
Iteration 691/1000 | Loss: 0.00003086
Iteration 692/1000 | Loss: 0.00003086
Iteration 693/1000 | Loss: 0.00003086
Iteration 694/1000 | Loss: 0.00003086
Iteration 695/1000 | Loss: 0.00003086
Iteration 696/1000 | Loss: 0.00003111
Iteration 697/1000 | Loss: 0.00003107
Iteration 698/1000 | Loss: 0.00003125
Iteration 699/1000 | Loss: 0.00003124
Iteration 700/1000 | Loss: 0.00003385
Iteration 701/1000 | Loss: 0.00003659
Iteration 702/1000 | Loss: 0.00003159
Iteration 703/1000 | Loss: 0.00003200
Iteration 704/1000 | Loss: 0.00003101
Iteration 705/1000 | Loss: 0.00003310
Iteration 706/1000 | Loss: 0.00003236
Iteration 707/1000 | Loss: 0.00003165
Iteration 708/1000 | Loss: 0.00003292
Iteration 709/1000 | Loss: 0.00003157
Iteration 710/1000 | Loss: 0.00003239
Iteration 711/1000 | Loss: 0.00003164
Iteration 712/1000 | Loss: 0.00003344
Iteration 713/1000 | Loss: 0.00003161
Iteration 714/1000 | Loss: 0.00003382
Iteration 715/1000 | Loss: 0.00003165
Iteration 716/1000 | Loss: 0.00003390
Iteration 717/1000 | Loss: 0.00003162
Iteration 718/1000 | Loss: 0.00003108
Iteration 719/1000 | Loss: 0.00003291
Iteration 720/1000 | Loss: 0.00003137
Iteration 721/1000 | Loss: 0.00003110
Iteration 722/1000 | Loss: 0.00003280
Iteration 723/1000 | Loss: 0.00003315
Iteration 724/1000 | Loss: 0.00003314
Iteration 725/1000 | Loss: 0.00003241
Iteration 726/1000 | Loss: 0.00003169
Iteration 727/1000 | Loss: 0.00003314
Iteration 728/1000 | Loss: 0.00003361
Iteration 729/1000 | Loss: 0.00003143
Iteration 730/1000 | Loss: 0.00003125
Iteration 731/1000 | Loss: 0.00003209
Iteration 732/1000 | Loss: 0.00003382
Iteration 733/1000 | Loss: 0.00003391
Iteration 734/1000 | Loss: 0.00003178
Iteration 735/1000 | Loss: 0.00003508
Iteration 736/1000 | Loss: 0.00003254
Iteration 737/1000 | Loss: 0.00003158
Iteration 738/1000 | Loss: 0.00003139
Iteration 739/1000 | Loss: 0.00003081
Iteration 740/1000 | Loss: 0.00003081
Iteration 741/1000 | Loss: 0.00003081
Iteration 742/1000 | Loss: 0.00003157
Iteration 743/1000 | Loss: 0.00003217
Iteration 744/1000 | Loss: 0.00003245
Iteration 745/1000 | Loss: 0.00003131
Iteration 746/1000 | Loss: 0.00003120
Iteration 747/1000 | Loss: 0.00003165
Iteration 748/1000 | Loss: 0.00003167
Iteration 749/1000 | Loss: 0.00003125
Iteration 750/1000 | Loss: 0.00003125
Iteration 751/1000 | Loss: 0.00003125
Iteration 752/1000 | Loss: 0.00003125
Iteration 753/1000 | Loss: 0.00003124
Iteration 754/1000 | Loss: 0.00003597
Iteration 755/1000 | Loss: 0.00003339
Iteration 756/1000 | Loss: 0.00003130
Iteration 757/1000 | Loss: 0.00003408
Iteration 758/1000 | Loss: 0.00003565
Iteration 759/1000 | Loss: 0.00003129
Iteration 760/1000 | Loss: 0.00003251
Iteration 761/1000 | Loss: 0.00003252
Iteration 762/1000 | Loss: 0.00003104
Iteration 763/1000 | Loss: 0.00003104
Iteration 764/1000 | Loss: 0.00003291
Iteration 765/1000 | Loss: 0.00003199
Iteration 766/1000 | Loss: 0.00003112
Iteration 767/1000 | Loss: 0.00003207
Iteration 768/1000 | Loss: 0.00003280
Iteration 769/1000 | Loss: 0.00003101
Iteration 770/1000 | Loss: 0.00003214
Iteration 771/1000 | Loss: 0.00003244
Iteration 772/1000 | Loss: 0.00003109
Iteration 773/1000 | Loss: 0.00003412
Iteration 774/1000 | Loss: 0.00003321
Iteration 775/1000 | Loss: 0.00003108
Iteration 776/1000 | Loss: 0.00003106
Iteration 777/1000 | Loss: 0.00003103
Iteration 778/1000 | Loss: 0.00003103
Iteration 779/1000 | Loss: 0.00003222
Iteration 780/1000 | Loss: 0.00003117
Iteration 781/1000 | Loss: 0.00003401
Iteration 782/1000 | Loss: 0.00003342
Iteration 783/1000 | Loss: 0.00003102
Iteration 784/1000 | Loss: 0.00003502
Iteration 785/1000 | Loss: 0.00003454
Iteration 786/1000 | Loss: 0.00003212
Iteration 787/1000 | Loss: 0.00003255
Iteration 788/1000 | Loss: 0.00003254
Iteration 789/1000 | Loss: 0.00003716
Iteration 790/1000 | Loss: 0.00003137
Iteration 791/1000 | Loss: 0.00003203
Iteration 792/1000 | Loss: 0.00003216
Iteration 793/1000 | Loss: 0.00003122
Iteration 794/1000 | Loss: 0.00003231
Iteration 795/1000 | Loss: 0.00003171
Iteration 796/1000 | Loss: 0.00003128
Iteration 797/1000 | Loss: 0.00003177
Iteration 798/1000 | Loss: 0.00003156
Iteration 799/1000 | Loss: 0.00003108
Iteration 800/1000 | Loss: 0.00003122
Iteration 801/1000 | Loss: 0.00003280
Iteration 802/1000 | Loss: 0.00003241
Iteration 803/1000 | Loss: 0.00003241
Iteration 804/1000 | Loss: 0.00003190
Iteration 805/1000 | Loss: 0.00003466
Iteration 806/1000 | Loss: 0.00003351
Iteration 807/1000 | Loss: 0.00003122
Iteration 808/1000 | Loss: 0.00003203
Iteration 809/1000 | Loss: 0.00003234
Iteration 810/1000 | Loss: 0.00003281
Iteration 811/1000 | Loss: 0.00003281
Iteration 812/1000 | Loss: 0.00003138
Iteration 813/1000 | Loss: 0.00003133
Iteration 814/1000 | Loss: 0.00003187
Iteration 815/1000 | Loss: 0.00003153
Iteration 816/1000 | Loss: 0.00003101
Iteration 817/1000 | Loss: 0.00003195
Iteration 818/1000 | Loss: 0.00003171
Iteration 819/1000 | Loss: 0.00003174
Iteration 820/1000 | Loss: 0.00003117
Iteration 821/1000 | Loss: 0.00003154
Iteration 822/1000 | Loss: 0.00003100
Iteration 823/1000 | Loss: 0.00003278
Iteration 824/1000 | Loss: 0.00003172
Iteration 825/1000 | Loss: 0.00003237
Iteration 826/1000 | Loss: 0.00003172
Iteration 827/1000 | Loss: 0.00003257
Iteration 828/1000 | Loss: 0.00003256
Iteration 829/1000 | Loss: 0.00003256
Iteration 830/1000 | Loss: 0.00003255
Iteration 831/1000 | Loss: 0.00003255
Iteration 832/1000 | Loss: 0.00003255
Iteration 833/1000 | Loss: 0.00003254
Iteration 834/1000 | Loss: 0.00003254
Iteration 835/1000 | Loss: 0.00003254
Iteration 836/1000 | Loss: 0.00003253
Iteration 837/1000 | Loss: 0.00003253
Iteration 838/1000 | Loss: 0.00003253
Iteration 839/1000 | Loss: 0.00003252
Iteration 840/1000 | Loss: 0.00003396
Iteration 841/1000 | Loss: 0.00003613
Iteration 842/1000 | Loss: 0.00003297
Iteration 843/1000 | Loss: 0.00003249
Iteration 844/1000 | Loss: 0.00003123
Iteration 845/1000 | Loss: 0.00003344
Iteration 846/1000 | Loss: 0.00003094
Iteration 847/1000 | Loss: 0.00003087
Iteration 848/1000 | Loss: 0.00003086
Iteration 849/1000 | Loss: 0.00003158
Iteration 850/1000 | Loss: 0.00003139
Iteration 851/1000 | Loss: 0.00003143
Iteration 852/1000 | Loss: 0.00003122
Iteration 853/1000 | Loss: 0.00003184
Iteration 854/1000 | Loss: 0.00003148
Iteration 855/1000 | Loss: 0.00003154
Iteration 856/1000 | Loss: 0.00003363
Iteration 857/1000 | Loss: 0.00003270
Iteration 858/1000 | Loss: 0.00003096
Iteration 859/1000 | Loss: 0.00003387
Iteration 860/1000 | Loss: 0.00003264
Iteration 861/1000 | Loss: 0.00003143
Iteration 862/1000 | Loss: 0.00003203
Iteration 863/1000 | Loss: 0.00003297
Iteration 864/1000 | Loss: 0.00003191
Iteration 865/1000 | Loss: 0.00003567
Iteration 866/1000 | Loss: 0.00003137
Iteration 867/1000 | Loss: 0.00003199
Iteration 868/1000 | Loss: 0.00003331
Iteration 869/1000 | Loss: 0.00003152
Iteration 870/1000 | Loss: 0.00003183
Iteration 871/1000 | Loss: 0.00003470
Iteration 872/1000 | Loss: 0.00003144
Iteration 873/1000 | Loss: 0.00003196
Iteration 874/1000 | Loss: 0.00003497
Iteration 875/1000 | Loss: 0.00003453
Iteration 876/1000 | Loss: 0.00003145
Iteration 877/1000 | Loss: 0.00003390
Iteration 878/1000 | Loss: 0.00003573
Iteration 879/1000 | Loss: 0.00003270
Iteration 880/1000 | Loss: 0.00003269
Iteration 881/1000 | Loss: 0.00003334
Iteration 882/1000 | Loss: 0.00003133
Iteration 883/1000 | Loss: 0.00003098
Iteration 884/1000 | Loss: 0.00003155
Iteration 885/1000 | Loss: 0.00003557
Iteration 886/1000 | Loss: 0.00003214
Iteration 887/1000 | Loss: 0.00003164
Iteration 888/1000 | Loss: 0.00003601
Iteration 889/1000 | Loss: 0.00003199
Iteration 890/1000 | Loss: 0.00003165
Iteration 891/1000 | Loss: 0.00003424
Iteration 892/1000 | Loss: 0.00003245
Iteration 893/1000 | Loss: 0.00003075
Iteration 894/1000 | Loss: 0.00003075
Iteration 895/1000 | Loss: 0.00003134
Iteration 896/1000 | Loss: 0.00003182
Iteration 897/1000 | Loss: 0.00003353
Iteration 898/1000 | Loss: 0.00003172
Iteration 899/1000 | Loss: 0.00003074
Iteration 900/1000 | Loss: 0.00003139
Iteration 901/1000 | Loss: 0.00003438
Iteration 902/1000 | Loss: 0.00003364
Iteration 903/1000 | Loss: 0.00003105
Iteration 904/1000 | Loss: 0.00003103
Iteration 905/1000 | Loss: 0.00003103
Iteration 906/1000 | Loss: 0.00003102
Iteration 907/1000 | Loss: 0.00003187
Iteration 908/1000 | Loss: 0.00003348
Iteration 909/1000 | Loss: 0.00003169
Iteration 910/1000 | Loss: 0.00003160
Iteration 911/1000 | Loss: 0.00003340
Iteration 912/1000 | Loss: 0.00003137
Iteration 913/1000 | Loss: 0.00003178
Iteration 914/1000 | Loss: 0.00003381
Iteration 915/1000 | Loss: 0.00003145
Iteration 916/1000 | Loss: 0.00003124
Iteration 917/1000 | Loss: 0.00003417
Iteration 918/1000 | Loss: 0.00003150
Iteration 919/1000 | Loss: 0.00003158
Iteration 920/1000 | Loss: 0.00003396
Iteration 921/1000 | Loss: 0.00003149
Iteration 922/1000 | Loss: 0.00003183
Iteration 923/1000 | Loss: 0.00003372
Iteration 924/1000 | Loss: 0.00003306
Iteration 925/1000 | Loss: 0.00003382
Iteration 926/1000 | Loss: 0.00003364
Iteration 927/1000 | Loss: 0.00003221
Iteration 928/1000 | Loss: 0.00003238
Iteration 929/1000 | Loss: 0.00003237
Iteration 930/1000 | Loss: 0.00003464
Iteration 931/1000 | Loss: 0.00003186
Iteration 932/1000 | Loss: 0.00003211
Iteration 933/1000 | Loss: 0.00003444
Iteration 934/1000 | Loss: 0.00003164
Iteration 935/1000 | Loss: 0.00003209
Iteration 936/1000 | Loss: 0.00003554
Iteration 937/1000 | Loss: 0.00003183
Iteration 938/1000 | Loss: 0.00003218
Iteration 939/1000 | Loss: 0.00003404
Iteration 940/1000 | Loss: 0.00003188
Iteration 941/1000 | Loss: 0.00003188
Iteration 942/1000 | Loss: 0.00003543
Iteration 943/1000 | Loss: 0.00003179
Iteration 944/1000 | Loss: 0.00003091
Iteration 945/1000 | Loss: 0.00003150
Iteration 946/1000 | Loss: 0.00003150
Iteration 947/1000 | Loss: 0.00003189
Iteration 948/1000 | Loss: 0.00003136
Iteration 949/1000 | Loss: 0.00003180
Iteration 950/1000 | Loss: 0.00003170
Iteration 951/1000 | Loss: 0.00003143
Iteration 952/1000 | Loss: 0.00003343
Iteration 953/1000 | Loss: 0.00003204
Iteration 954/1000 | Loss: 0.00003245
Iteration 955/1000 | Loss: 0.00003196
Iteration 956/1000 | Loss: 0.00003217
Iteration 957/1000 | Loss: 0.00003157
Iteration 958/1000 | Loss: 0.00003238
Iteration 959/1000 | Loss: 0.00003226
Iteration 960/1000 | Loss: 0.00003258
Iteration 961/1000 | Loss: 0.00003200
Iteration 962/1000 | Loss: 0.00003223
Iteration 963/1000 | Loss: 0.00003237
Iteration 964/1000 | Loss: 0.00003208
Iteration 965/1000 | Loss: 0.00003195
Iteration 966/1000 | Loss: 0.00003247
Iteration 967/1000 | Loss: 0.00003199
Iteration 968/1000 | Loss: 0.00003239
Iteration 969/1000 | Loss: 0.00003126
Iteration 970/1000 | Loss: 0.00003214
Iteration 971/1000 | Loss: 0.00003145
Iteration 972/1000 | Loss: 0.00003315
Iteration 973/1000 | Loss: 0.00003218
Iteration 974/1000 | Loss: 0.00003209
Iteration 975/1000 | Loss: 0.00003340
Iteration 976/1000 | Loss: 0.00003199
Iteration 977/1000 | Loss: 0.00003105
Iteration 978/1000 | Loss: 0.00003122
Iteration 979/1000 | Loss: 0.00003202
Iteration 980/1000 | Loss: 0.00003119
Iteration 981/1000 | Loss: 0.00003119
Iteration 982/1000 | Loss: 0.00003172
Iteration 983/1000 | Loss: 0.00003328
Iteration 984/1000 | Loss: 0.00003331
Iteration 985/1000 | Loss: 0.00003274
Iteration 986/1000 | Loss: 0.00003273
Iteration 987/1000 | Loss: 0.00003133
Iteration 988/1000 | Loss: 0.00003171
Iteration 989/1000 | Loss: 0.00003108
Iteration 990/1000 | Loss: 0.00003108
Iteration 991/1000 | Loss: 0.00003249
Iteration 992/1000 | Loss: 0.00003155
Iteration 993/1000 | Loss: 0.00003184
Iteration 994/1000 | Loss: 0.00003212
Iteration 995/1000 | Loss: 0.00003337
Iteration 996/1000 | Loss: 0.00003405
Iteration 997/1000 | Loss: 0.00003288
Iteration 998/1000 | Loss: 0.00003118
Iteration 999/1000 | Loss: 0.00003107
Iteration 1000/1000 | Loss: 0.00003150

Optimization complete. Final v2v error: 4.666121959686279 mm

Highest mean error: 12.579593658447266 mm for frame 59

Lowest mean error: 3.903501033782959 mm for frame 131

Saving results

Total time: 1038.3853106498718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398843
Iteration 2/25 | Loss: 0.00176302
Iteration 3/25 | Loss: 0.00162358
Iteration 4/25 | Loss: 0.00159743
Iteration 5/25 | Loss: 0.00158793
Iteration 6/25 | Loss: 0.00158576
Iteration 7/25 | Loss: 0.00158497
Iteration 8/25 | Loss: 0.00158497
Iteration 9/25 | Loss: 0.00158497
Iteration 10/25 | Loss: 0.00158497
Iteration 11/25 | Loss: 0.00158497
Iteration 12/25 | Loss: 0.00158497
Iteration 13/25 | Loss: 0.00158497
Iteration 14/25 | Loss: 0.00158497
Iteration 15/25 | Loss: 0.00158497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001584965968504548, 0.001584965968504548, 0.001584965968504548, 0.001584965968504548, 0.001584965968504548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001584965968504548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66911101
Iteration 2/25 | Loss: 0.00496554
Iteration 3/25 | Loss: 0.00496554
Iteration 4/25 | Loss: 0.00496554
Iteration 5/25 | Loss: 0.00496554
Iteration 6/25 | Loss: 0.00496554
Iteration 7/25 | Loss: 0.00496554
Iteration 8/25 | Loss: 0.00496554
Iteration 9/25 | Loss: 0.00496554
Iteration 10/25 | Loss: 0.00496554
Iteration 11/25 | Loss: 0.00496554
Iteration 12/25 | Loss: 0.00496554
Iteration 13/25 | Loss: 0.00496554
Iteration 14/25 | Loss: 0.00496554
Iteration 15/25 | Loss: 0.00496554
Iteration 16/25 | Loss: 0.00496554
Iteration 17/25 | Loss: 0.00496554
Iteration 18/25 | Loss: 0.00496554
Iteration 19/25 | Loss: 0.00496554
Iteration 20/25 | Loss: 0.00496554
Iteration 21/25 | Loss: 0.00496554
Iteration 22/25 | Loss: 0.00496554
Iteration 23/25 | Loss: 0.00496554
Iteration 24/25 | Loss: 0.00496554
Iteration 25/25 | Loss: 0.00496554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00496554
Iteration 2/1000 | Loss: 0.00009312
Iteration 3/1000 | Loss: 0.00005611
Iteration 4/1000 | Loss: 0.00004241
Iteration 5/1000 | Loss: 0.00003789
Iteration 6/1000 | Loss: 0.00003566
Iteration 7/1000 | Loss: 0.00003401
Iteration 8/1000 | Loss: 0.00003286
Iteration 9/1000 | Loss: 0.00003204
Iteration 10/1000 | Loss: 0.00003136
Iteration 11/1000 | Loss: 0.00003098
Iteration 12/1000 | Loss: 0.00003068
Iteration 13/1000 | Loss: 0.00003042
Iteration 14/1000 | Loss: 0.00003022
Iteration 15/1000 | Loss: 0.00003013
Iteration 16/1000 | Loss: 0.00003007
Iteration 17/1000 | Loss: 0.00003005
Iteration 18/1000 | Loss: 0.00003004
Iteration 19/1000 | Loss: 0.00003004
Iteration 20/1000 | Loss: 0.00003003
Iteration 21/1000 | Loss: 0.00003001
Iteration 22/1000 | Loss: 0.00003000
Iteration 23/1000 | Loss: 0.00002999
Iteration 24/1000 | Loss: 0.00002995
Iteration 25/1000 | Loss: 0.00002995
Iteration 26/1000 | Loss: 0.00002992
Iteration 27/1000 | Loss: 0.00002992
Iteration 28/1000 | Loss: 0.00002992
Iteration 29/1000 | Loss: 0.00002989
Iteration 30/1000 | Loss: 0.00002986
Iteration 31/1000 | Loss: 0.00002986
Iteration 32/1000 | Loss: 0.00002985
Iteration 33/1000 | Loss: 0.00002984
Iteration 34/1000 | Loss: 0.00002984
Iteration 35/1000 | Loss: 0.00002983
Iteration 36/1000 | Loss: 0.00002983
Iteration 37/1000 | Loss: 0.00002982
Iteration 38/1000 | Loss: 0.00002982
Iteration 39/1000 | Loss: 0.00002981
Iteration 40/1000 | Loss: 0.00002981
Iteration 41/1000 | Loss: 0.00002981
Iteration 42/1000 | Loss: 0.00002980
Iteration 43/1000 | Loss: 0.00002980
Iteration 44/1000 | Loss: 0.00002979
Iteration 45/1000 | Loss: 0.00002979
Iteration 46/1000 | Loss: 0.00002979
Iteration 47/1000 | Loss: 0.00002978
Iteration 48/1000 | Loss: 0.00002978
Iteration 49/1000 | Loss: 0.00002978
Iteration 50/1000 | Loss: 0.00002977
Iteration 51/1000 | Loss: 0.00002977
Iteration 52/1000 | Loss: 0.00002976
Iteration 53/1000 | Loss: 0.00002976
Iteration 54/1000 | Loss: 0.00002976
Iteration 55/1000 | Loss: 0.00002976
Iteration 56/1000 | Loss: 0.00002975
Iteration 57/1000 | Loss: 0.00002975
Iteration 58/1000 | Loss: 0.00002975
Iteration 59/1000 | Loss: 0.00002975
Iteration 60/1000 | Loss: 0.00002975
Iteration 61/1000 | Loss: 0.00002974
Iteration 62/1000 | Loss: 0.00002974
Iteration 63/1000 | Loss: 0.00002974
Iteration 64/1000 | Loss: 0.00002974
Iteration 65/1000 | Loss: 0.00002974
Iteration 66/1000 | Loss: 0.00002974
Iteration 67/1000 | Loss: 0.00002973
Iteration 68/1000 | Loss: 0.00002973
Iteration 69/1000 | Loss: 0.00002973
Iteration 70/1000 | Loss: 0.00002973
Iteration 71/1000 | Loss: 0.00002973
Iteration 72/1000 | Loss: 0.00002973
Iteration 73/1000 | Loss: 0.00002973
Iteration 74/1000 | Loss: 0.00002972
Iteration 75/1000 | Loss: 0.00002972
Iteration 76/1000 | Loss: 0.00002972
Iteration 77/1000 | Loss: 0.00002972
Iteration 78/1000 | Loss: 0.00002972
Iteration 79/1000 | Loss: 0.00002971
Iteration 80/1000 | Loss: 0.00002971
Iteration 81/1000 | Loss: 0.00002971
Iteration 82/1000 | Loss: 0.00002971
Iteration 83/1000 | Loss: 0.00002970
Iteration 84/1000 | Loss: 0.00002970
Iteration 85/1000 | Loss: 0.00002970
Iteration 86/1000 | Loss: 0.00002970
Iteration 87/1000 | Loss: 0.00002970
Iteration 88/1000 | Loss: 0.00002969
Iteration 89/1000 | Loss: 0.00002969
Iteration 90/1000 | Loss: 0.00002969
Iteration 91/1000 | Loss: 0.00002969
Iteration 92/1000 | Loss: 0.00002969
Iteration 93/1000 | Loss: 0.00002968
Iteration 94/1000 | Loss: 0.00002968
Iteration 95/1000 | Loss: 0.00002968
Iteration 96/1000 | Loss: 0.00002968
Iteration 97/1000 | Loss: 0.00002968
Iteration 98/1000 | Loss: 0.00002968
Iteration 99/1000 | Loss: 0.00002968
Iteration 100/1000 | Loss: 0.00002968
Iteration 101/1000 | Loss: 0.00002968
Iteration 102/1000 | Loss: 0.00002968
Iteration 103/1000 | Loss: 0.00002968
Iteration 104/1000 | Loss: 0.00002968
Iteration 105/1000 | Loss: 0.00002968
Iteration 106/1000 | Loss: 0.00002968
Iteration 107/1000 | Loss: 0.00002968
Iteration 108/1000 | Loss: 0.00002968
Iteration 109/1000 | Loss: 0.00002968
Iteration 110/1000 | Loss: 0.00002968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.9679138606297784e-05, 2.9679138606297784e-05, 2.9679138606297784e-05, 2.9679138606297784e-05, 2.9679138606297784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9679138606297784e-05

Optimization complete. Final v2v error: 4.654894828796387 mm

Highest mean error: 5.3205647468566895 mm for frame 203

Lowest mean error: 3.9498536586761475 mm for frame 111

Saving results

Total time: 45.433430433273315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479677
Iteration 2/25 | Loss: 0.00172653
Iteration 3/25 | Loss: 0.00155134
Iteration 4/25 | Loss: 0.00151811
Iteration 5/25 | Loss: 0.00151226
Iteration 6/25 | Loss: 0.00151104
Iteration 7/25 | Loss: 0.00151067
Iteration 8/25 | Loss: 0.00151067
Iteration 9/25 | Loss: 0.00151067
Iteration 10/25 | Loss: 0.00151067
Iteration 11/25 | Loss: 0.00151067
Iteration 12/25 | Loss: 0.00151067
Iteration 13/25 | Loss: 0.00151067
Iteration 14/25 | Loss: 0.00151067
Iteration 15/25 | Loss: 0.00151067
Iteration 16/25 | Loss: 0.00151067
Iteration 17/25 | Loss: 0.00151067
Iteration 18/25 | Loss: 0.00151067
Iteration 19/25 | Loss: 0.00151067
Iteration 20/25 | Loss: 0.00151067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015106667997315526, 0.0015106667997315526, 0.0015106667997315526, 0.0015106667997315526, 0.0015106667997315526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015106667997315526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.21269941
Iteration 2/25 | Loss: 0.00389665
Iteration 3/25 | Loss: 0.00389663
Iteration 4/25 | Loss: 0.00389662
Iteration 5/25 | Loss: 0.00389662
Iteration 6/25 | Loss: 0.00389662
Iteration 7/25 | Loss: 0.00389662
Iteration 8/25 | Loss: 0.00389662
Iteration 9/25 | Loss: 0.00389662
Iteration 10/25 | Loss: 0.00389662
Iteration 11/25 | Loss: 0.00389662
Iteration 12/25 | Loss: 0.00389662
Iteration 13/25 | Loss: 0.00389662
Iteration 14/25 | Loss: 0.00389662
Iteration 15/25 | Loss: 0.00389662
Iteration 16/25 | Loss: 0.00389662
Iteration 17/25 | Loss: 0.00389662
Iteration 18/25 | Loss: 0.00389662
Iteration 19/25 | Loss: 0.00389662
Iteration 20/25 | Loss: 0.00389662
Iteration 21/25 | Loss: 0.00389662
Iteration 22/25 | Loss: 0.00389662
Iteration 23/25 | Loss: 0.00389662
Iteration 24/25 | Loss: 0.00389662
Iteration 25/25 | Loss: 0.00389662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003896618727594614, 0.003896618727594614, 0.003896618727594614, 0.003896618727594614, 0.003896618727594614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003896618727594614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389662
Iteration 2/1000 | Loss: 0.00007507
Iteration 3/1000 | Loss: 0.00005594
Iteration 4/1000 | Loss: 0.00004478
Iteration 5/1000 | Loss: 0.00004024
Iteration 6/1000 | Loss: 0.00003764
Iteration 7/1000 | Loss: 0.00003634
Iteration 8/1000 | Loss: 0.00003552
Iteration 9/1000 | Loss: 0.00003501
Iteration 10/1000 | Loss: 0.00003466
Iteration 11/1000 | Loss: 0.00003431
Iteration 12/1000 | Loss: 0.00003408
Iteration 13/1000 | Loss: 0.00003403
Iteration 14/1000 | Loss: 0.00003398
Iteration 15/1000 | Loss: 0.00003398
Iteration 16/1000 | Loss: 0.00003398
Iteration 17/1000 | Loss: 0.00003397
Iteration 18/1000 | Loss: 0.00003396
Iteration 19/1000 | Loss: 0.00003394
Iteration 20/1000 | Loss: 0.00003393
Iteration 21/1000 | Loss: 0.00003393
Iteration 22/1000 | Loss: 0.00003393
Iteration 23/1000 | Loss: 0.00003393
Iteration 24/1000 | Loss: 0.00003393
Iteration 25/1000 | Loss: 0.00003392
Iteration 26/1000 | Loss: 0.00003391
Iteration 27/1000 | Loss: 0.00003391
Iteration 28/1000 | Loss: 0.00003390
Iteration 29/1000 | Loss: 0.00003390
Iteration 30/1000 | Loss: 0.00003390
Iteration 31/1000 | Loss: 0.00003390
Iteration 32/1000 | Loss: 0.00003390
Iteration 33/1000 | Loss: 0.00003390
Iteration 34/1000 | Loss: 0.00003390
Iteration 35/1000 | Loss: 0.00003390
Iteration 36/1000 | Loss: 0.00003389
Iteration 37/1000 | Loss: 0.00003389
Iteration 38/1000 | Loss: 0.00003388
Iteration 39/1000 | Loss: 0.00003388
Iteration 40/1000 | Loss: 0.00003388
Iteration 41/1000 | Loss: 0.00003388
Iteration 42/1000 | Loss: 0.00003388
Iteration 43/1000 | Loss: 0.00003388
Iteration 44/1000 | Loss: 0.00003388
Iteration 45/1000 | Loss: 0.00003387
Iteration 46/1000 | Loss: 0.00003387
Iteration 47/1000 | Loss: 0.00003387
Iteration 48/1000 | Loss: 0.00003386
Iteration 49/1000 | Loss: 0.00003386
Iteration 50/1000 | Loss: 0.00003386
Iteration 51/1000 | Loss: 0.00003385
Iteration 52/1000 | Loss: 0.00003385
Iteration 53/1000 | Loss: 0.00003385
Iteration 54/1000 | Loss: 0.00003385
Iteration 55/1000 | Loss: 0.00003385
Iteration 56/1000 | Loss: 0.00003385
Iteration 57/1000 | Loss: 0.00003385
Iteration 58/1000 | Loss: 0.00003385
Iteration 59/1000 | Loss: 0.00003385
Iteration 60/1000 | Loss: 0.00003385
Iteration 61/1000 | Loss: 0.00003385
Iteration 62/1000 | Loss: 0.00003384
Iteration 63/1000 | Loss: 0.00003384
Iteration 64/1000 | Loss: 0.00003384
Iteration 65/1000 | Loss: 0.00003384
Iteration 66/1000 | Loss: 0.00003383
Iteration 67/1000 | Loss: 0.00003383
Iteration 68/1000 | Loss: 0.00003383
Iteration 69/1000 | Loss: 0.00003383
Iteration 70/1000 | Loss: 0.00003382
Iteration 71/1000 | Loss: 0.00003382
Iteration 72/1000 | Loss: 0.00003382
Iteration 73/1000 | Loss: 0.00003382
Iteration 74/1000 | Loss: 0.00003382
Iteration 75/1000 | Loss: 0.00003382
Iteration 76/1000 | Loss: 0.00003381
Iteration 77/1000 | Loss: 0.00003381
Iteration 78/1000 | Loss: 0.00003381
Iteration 79/1000 | Loss: 0.00003380
Iteration 80/1000 | Loss: 0.00003380
Iteration 81/1000 | Loss: 0.00003379
Iteration 82/1000 | Loss: 0.00003379
Iteration 83/1000 | Loss: 0.00003379
Iteration 84/1000 | Loss: 0.00003379
Iteration 85/1000 | Loss: 0.00003378
Iteration 86/1000 | Loss: 0.00003378
Iteration 87/1000 | Loss: 0.00003378
Iteration 88/1000 | Loss: 0.00003377
Iteration 89/1000 | Loss: 0.00003377
Iteration 90/1000 | Loss: 0.00003377
Iteration 91/1000 | Loss: 0.00003377
Iteration 92/1000 | Loss: 0.00003377
Iteration 93/1000 | Loss: 0.00003377
Iteration 94/1000 | Loss: 0.00003376
Iteration 95/1000 | Loss: 0.00003376
Iteration 96/1000 | Loss: 0.00003376
Iteration 97/1000 | Loss: 0.00003376
Iteration 98/1000 | Loss: 0.00003376
Iteration 99/1000 | Loss: 0.00003376
Iteration 100/1000 | Loss: 0.00003376
Iteration 101/1000 | Loss: 0.00003376
Iteration 102/1000 | Loss: 0.00003376
Iteration 103/1000 | Loss: 0.00003376
Iteration 104/1000 | Loss: 0.00003375
Iteration 105/1000 | Loss: 0.00003375
Iteration 106/1000 | Loss: 0.00003375
Iteration 107/1000 | Loss: 0.00003375
Iteration 108/1000 | Loss: 0.00003375
Iteration 109/1000 | Loss: 0.00003375
Iteration 110/1000 | Loss: 0.00003374
Iteration 111/1000 | Loss: 0.00003374
Iteration 112/1000 | Loss: 0.00003374
Iteration 113/1000 | Loss: 0.00003374
Iteration 114/1000 | Loss: 0.00003374
Iteration 115/1000 | Loss: 0.00003374
Iteration 116/1000 | Loss: 0.00003373
Iteration 117/1000 | Loss: 0.00003373
Iteration 118/1000 | Loss: 0.00003373
Iteration 119/1000 | Loss: 0.00003373
Iteration 120/1000 | Loss: 0.00003373
Iteration 121/1000 | Loss: 0.00003373
Iteration 122/1000 | Loss: 0.00003372
Iteration 123/1000 | Loss: 0.00003372
Iteration 124/1000 | Loss: 0.00003372
Iteration 125/1000 | Loss: 0.00003372
Iteration 126/1000 | Loss: 0.00003371
Iteration 127/1000 | Loss: 0.00003371
Iteration 128/1000 | Loss: 0.00003371
Iteration 129/1000 | Loss: 0.00003371
Iteration 130/1000 | Loss: 0.00003370
Iteration 131/1000 | Loss: 0.00003370
Iteration 132/1000 | Loss: 0.00003370
Iteration 133/1000 | Loss: 0.00003370
Iteration 134/1000 | Loss: 0.00003370
Iteration 135/1000 | Loss: 0.00003370
Iteration 136/1000 | Loss: 0.00003370
Iteration 137/1000 | Loss: 0.00003370
Iteration 138/1000 | Loss: 0.00003369
Iteration 139/1000 | Loss: 0.00003369
Iteration 140/1000 | Loss: 0.00003369
Iteration 141/1000 | Loss: 0.00003369
Iteration 142/1000 | Loss: 0.00003369
Iteration 143/1000 | Loss: 0.00003369
Iteration 144/1000 | Loss: 0.00003369
Iteration 145/1000 | Loss: 0.00003369
Iteration 146/1000 | Loss: 0.00003369
Iteration 147/1000 | Loss: 0.00003369
Iteration 148/1000 | Loss: 0.00003369
Iteration 149/1000 | Loss: 0.00003368
Iteration 150/1000 | Loss: 0.00003368
Iteration 151/1000 | Loss: 0.00003368
Iteration 152/1000 | Loss: 0.00003368
Iteration 153/1000 | Loss: 0.00003368
Iteration 154/1000 | Loss: 0.00003368
Iteration 155/1000 | Loss: 0.00003368
Iteration 156/1000 | Loss: 0.00003368
Iteration 157/1000 | Loss: 0.00003368
Iteration 158/1000 | Loss: 0.00003367
Iteration 159/1000 | Loss: 0.00003367
Iteration 160/1000 | Loss: 0.00003367
Iteration 161/1000 | Loss: 0.00003367
Iteration 162/1000 | Loss: 0.00003367
Iteration 163/1000 | Loss: 0.00003367
Iteration 164/1000 | Loss: 0.00003367
Iteration 165/1000 | Loss: 0.00003367
Iteration 166/1000 | Loss: 0.00003367
Iteration 167/1000 | Loss: 0.00003367
Iteration 168/1000 | Loss: 0.00003367
Iteration 169/1000 | Loss: 0.00003367
Iteration 170/1000 | Loss: 0.00003367
Iteration 171/1000 | Loss: 0.00003367
Iteration 172/1000 | Loss: 0.00003366
Iteration 173/1000 | Loss: 0.00003366
Iteration 174/1000 | Loss: 0.00003366
Iteration 175/1000 | Loss: 0.00003366
Iteration 176/1000 | Loss: 0.00003366
Iteration 177/1000 | Loss: 0.00003366
Iteration 178/1000 | Loss: 0.00003366
Iteration 179/1000 | Loss: 0.00003366
Iteration 180/1000 | Loss: 0.00003366
Iteration 181/1000 | Loss: 0.00003366
Iteration 182/1000 | Loss: 0.00003366
Iteration 183/1000 | Loss: 0.00003366
Iteration 184/1000 | Loss: 0.00003366
Iteration 185/1000 | Loss: 0.00003366
Iteration 186/1000 | Loss: 0.00003366
Iteration 187/1000 | Loss: 0.00003366
Iteration 188/1000 | Loss: 0.00003366
Iteration 189/1000 | Loss: 0.00003366
Iteration 190/1000 | Loss: 0.00003366
Iteration 191/1000 | Loss: 0.00003366
Iteration 192/1000 | Loss: 0.00003366
Iteration 193/1000 | Loss: 0.00003366
Iteration 194/1000 | Loss: 0.00003366
Iteration 195/1000 | Loss: 0.00003366
Iteration 196/1000 | Loss: 0.00003366
Iteration 197/1000 | Loss: 0.00003366
Iteration 198/1000 | Loss: 0.00003366
Iteration 199/1000 | Loss: 0.00003366
Iteration 200/1000 | Loss: 0.00003366
Iteration 201/1000 | Loss: 0.00003366
Iteration 202/1000 | Loss: 0.00003366
Iteration 203/1000 | Loss: 0.00003366
Iteration 204/1000 | Loss: 0.00003366
Iteration 205/1000 | Loss: 0.00003366
Iteration 206/1000 | Loss: 0.00003366
Iteration 207/1000 | Loss: 0.00003366
Iteration 208/1000 | Loss: 0.00003366
Iteration 209/1000 | Loss: 0.00003366
Iteration 210/1000 | Loss: 0.00003366
Iteration 211/1000 | Loss: 0.00003366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.365650263731368e-05, 3.365650263731368e-05, 3.365650263731368e-05, 3.365650263731368e-05, 3.365650263731368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.365650263731368e-05

Optimization complete. Final v2v error: 4.967123031616211 mm

Highest mean error: 5.5516133308410645 mm for frame 23

Lowest mean error: 4.463263511657715 mm for frame 59

Saving results

Total time: 40.39211702346802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01155712
Iteration 2/25 | Loss: 0.00386223
Iteration 3/25 | Loss: 0.00239790
Iteration 4/25 | Loss: 0.00232384
Iteration 5/25 | Loss: 0.00223430
Iteration 6/25 | Loss: 0.00213747
Iteration 7/25 | Loss: 0.00209061
Iteration 8/25 | Loss: 0.00204127
Iteration 9/25 | Loss: 0.00199017
Iteration 10/25 | Loss: 0.00196155
Iteration 11/25 | Loss: 0.00194429
Iteration 12/25 | Loss: 0.00192465
Iteration 13/25 | Loss: 0.00194263
Iteration 14/25 | Loss: 0.00190355
Iteration 15/25 | Loss: 0.00189639
Iteration 16/25 | Loss: 0.00189151
Iteration 17/25 | Loss: 0.00188950
Iteration 18/25 | Loss: 0.00188328
Iteration 19/25 | Loss: 0.00188538
Iteration 20/25 | Loss: 0.00188406
Iteration 21/25 | Loss: 0.00188454
Iteration 22/25 | Loss: 0.00188272
Iteration 23/25 | Loss: 0.00188433
Iteration 24/25 | Loss: 0.00188555
Iteration 25/25 | Loss: 0.00188369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73759949
Iteration 2/25 | Loss: 0.01206167
Iteration 3/25 | Loss: 0.00893567
Iteration 4/25 | Loss: 0.00893566
Iteration 5/25 | Loss: 0.00893566
Iteration 6/25 | Loss: 0.00893566
Iteration 7/25 | Loss: 0.00893566
Iteration 8/25 | Loss: 0.00893566
Iteration 9/25 | Loss: 0.00893566
Iteration 10/25 | Loss: 0.00893566
Iteration 11/25 | Loss: 0.00893566
Iteration 12/25 | Loss: 0.00893565
Iteration 13/25 | Loss: 0.00893565
Iteration 14/25 | Loss: 0.00893565
Iteration 15/25 | Loss: 0.00893565
Iteration 16/25 | Loss: 0.00893565
Iteration 17/25 | Loss: 0.00893565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.008935654535889626, 0.008935654535889626, 0.008935654535889626, 0.008935654535889626, 0.008935654535889626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008935654535889626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00893565
Iteration 2/1000 | Loss: 0.00447239
Iteration 3/1000 | Loss: 0.00637861
Iteration 4/1000 | Loss: 0.00195675
Iteration 5/1000 | Loss: 0.00781938
Iteration 6/1000 | Loss: 0.00366451
Iteration 7/1000 | Loss: 0.00257450
Iteration 8/1000 | Loss: 0.00425160
Iteration 9/1000 | Loss: 0.00341722
Iteration 10/1000 | Loss: 0.00403762
Iteration 11/1000 | Loss: 0.00357105
Iteration 12/1000 | Loss: 0.00522413
Iteration 13/1000 | Loss: 0.00576806
Iteration 14/1000 | Loss: 0.00085286
Iteration 15/1000 | Loss: 0.00236840
Iteration 16/1000 | Loss: 0.00198430
Iteration 17/1000 | Loss: 0.00086470
Iteration 18/1000 | Loss: 0.00159934
Iteration 19/1000 | Loss: 0.00145908
Iteration 20/1000 | Loss: 0.00202275
Iteration 21/1000 | Loss: 0.00144540
Iteration 22/1000 | Loss: 0.00309889
Iteration 23/1000 | Loss: 0.00101822
Iteration 24/1000 | Loss: 0.00092734
Iteration 25/1000 | Loss: 0.00131616
Iteration 26/1000 | Loss: 0.00236123
Iteration 27/1000 | Loss: 0.00198130
Iteration 28/1000 | Loss: 0.00070652
Iteration 29/1000 | Loss: 0.00125782
Iteration 30/1000 | Loss: 0.00089556
Iteration 31/1000 | Loss: 0.00201739
Iteration 32/1000 | Loss: 0.00183953
Iteration 33/1000 | Loss: 0.00120278
Iteration 34/1000 | Loss: 0.00142457
Iteration 35/1000 | Loss: 0.00191310
Iteration 36/1000 | Loss: 0.00044089
Iteration 37/1000 | Loss: 0.00034107
Iteration 38/1000 | Loss: 0.00048198
Iteration 39/1000 | Loss: 0.00128401
Iteration 40/1000 | Loss: 0.00062870
Iteration 41/1000 | Loss: 0.00129231
Iteration 42/1000 | Loss: 0.00079798
Iteration 43/1000 | Loss: 0.00043764
Iteration 44/1000 | Loss: 0.00061660
Iteration 45/1000 | Loss: 0.00100177
Iteration 46/1000 | Loss: 0.00029529
Iteration 47/1000 | Loss: 0.00048044
Iteration 48/1000 | Loss: 0.00030190
Iteration 49/1000 | Loss: 0.00025977
Iteration 50/1000 | Loss: 0.00100415
Iteration 51/1000 | Loss: 0.00091883
Iteration 52/1000 | Loss: 0.00096986
Iteration 53/1000 | Loss: 0.00152409
Iteration 54/1000 | Loss: 0.00039838
Iteration 55/1000 | Loss: 0.00072567
Iteration 56/1000 | Loss: 0.00055021
Iteration 57/1000 | Loss: 0.00031283
Iteration 58/1000 | Loss: 0.00043199
Iteration 59/1000 | Loss: 0.00116206
Iteration 60/1000 | Loss: 0.00057394
Iteration 61/1000 | Loss: 0.00073034
Iteration 62/1000 | Loss: 0.00031064
Iteration 63/1000 | Loss: 0.00121712
Iteration 64/1000 | Loss: 0.00085376
Iteration 65/1000 | Loss: 0.00068228
Iteration 66/1000 | Loss: 0.00104143
Iteration 67/1000 | Loss: 0.00053244
Iteration 68/1000 | Loss: 0.00120973
Iteration 69/1000 | Loss: 0.00035637
Iteration 70/1000 | Loss: 0.00064613
Iteration 71/1000 | Loss: 0.00046572
Iteration 72/1000 | Loss: 0.00028487
Iteration 73/1000 | Loss: 0.00013235
Iteration 74/1000 | Loss: 0.00022417
Iteration 75/1000 | Loss: 0.00014921
Iteration 76/1000 | Loss: 0.00032282
Iteration 77/1000 | Loss: 0.00020532
Iteration 78/1000 | Loss: 0.00015955
Iteration 79/1000 | Loss: 0.00040860
Iteration 80/1000 | Loss: 0.00019590
Iteration 81/1000 | Loss: 0.00018122
Iteration 82/1000 | Loss: 0.00011703
Iteration 83/1000 | Loss: 0.00012080
Iteration 84/1000 | Loss: 0.00011171
Iteration 85/1000 | Loss: 0.00048586
Iteration 86/1000 | Loss: 0.00057465
Iteration 87/1000 | Loss: 0.00058198
Iteration 88/1000 | Loss: 0.00057770
Iteration 89/1000 | Loss: 0.00060493
Iteration 90/1000 | Loss: 0.00064208
Iteration 91/1000 | Loss: 0.00046365
Iteration 92/1000 | Loss: 0.00060610
Iteration 93/1000 | Loss: 0.00066632
Iteration 94/1000 | Loss: 0.00020506
Iteration 95/1000 | Loss: 0.00019632
Iteration 96/1000 | Loss: 0.00010975
Iteration 97/1000 | Loss: 0.00012086
Iteration 98/1000 | Loss: 0.00016606
Iteration 99/1000 | Loss: 0.00010256
Iteration 100/1000 | Loss: 0.00010612
Iteration 101/1000 | Loss: 0.00010582
Iteration 102/1000 | Loss: 0.00017858
Iteration 103/1000 | Loss: 0.00013699
Iteration 104/1000 | Loss: 0.00011487
Iteration 105/1000 | Loss: 0.00010758
Iteration 106/1000 | Loss: 0.00022585
Iteration 107/1000 | Loss: 0.00035371
Iteration 108/1000 | Loss: 0.00010325
Iteration 109/1000 | Loss: 0.00025606
Iteration 110/1000 | Loss: 0.00021157
Iteration 111/1000 | Loss: 0.00074846
Iteration 112/1000 | Loss: 0.00010690
Iteration 113/1000 | Loss: 0.00009542
Iteration 114/1000 | Loss: 0.00014700
Iteration 115/1000 | Loss: 0.00009883
Iteration 116/1000 | Loss: 0.00008581
Iteration 117/1000 | Loss: 0.00009303
Iteration 118/1000 | Loss: 0.00008913
Iteration 119/1000 | Loss: 0.00008401
Iteration 120/1000 | Loss: 0.00008459
Iteration 121/1000 | Loss: 0.00007922
Iteration 122/1000 | Loss: 0.00008409
Iteration 123/1000 | Loss: 0.00008356
Iteration 124/1000 | Loss: 0.00010662
Iteration 125/1000 | Loss: 0.00013284
Iteration 126/1000 | Loss: 0.00022754
Iteration 127/1000 | Loss: 0.00024154
Iteration 128/1000 | Loss: 0.00017976
Iteration 129/1000 | Loss: 0.00010212
Iteration 130/1000 | Loss: 0.00009391
Iteration 131/1000 | Loss: 0.00012033
Iteration 132/1000 | Loss: 0.00014512
Iteration 133/1000 | Loss: 0.00009305
Iteration 134/1000 | Loss: 0.00007929
Iteration 135/1000 | Loss: 0.00008387
Iteration 136/1000 | Loss: 0.00008350
Iteration 137/1000 | Loss: 0.00008457
Iteration 138/1000 | Loss: 0.00008320
Iteration 139/1000 | Loss: 0.00008380
Iteration 140/1000 | Loss: 0.00007849
Iteration 141/1000 | Loss: 0.00008319
Iteration 142/1000 | Loss: 0.00008434
Iteration 143/1000 | Loss: 0.00007751
Iteration 144/1000 | Loss: 0.00026732
Iteration 145/1000 | Loss: 0.00040014
Iteration 146/1000 | Loss: 0.00038601
Iteration 147/1000 | Loss: 0.00012991
Iteration 148/1000 | Loss: 0.00043783
Iteration 149/1000 | Loss: 0.00016650
Iteration 150/1000 | Loss: 0.00009561
Iteration 151/1000 | Loss: 0.00007488
Iteration 152/1000 | Loss: 0.00007274
Iteration 153/1000 | Loss: 0.00007165
Iteration 154/1000 | Loss: 0.00014914
Iteration 155/1000 | Loss: 0.00018504
Iteration 156/1000 | Loss: 0.00046561
Iteration 157/1000 | Loss: 0.00011082
Iteration 158/1000 | Loss: 0.00018486
Iteration 159/1000 | Loss: 0.00025811
Iteration 160/1000 | Loss: 0.00007209
Iteration 161/1000 | Loss: 0.00014198
Iteration 162/1000 | Loss: 0.00010296
Iteration 163/1000 | Loss: 0.00006913
Iteration 164/1000 | Loss: 0.00006830
Iteration 165/1000 | Loss: 0.00016485
Iteration 166/1000 | Loss: 0.00008102
Iteration 167/1000 | Loss: 0.00010141
Iteration 168/1000 | Loss: 0.00006729
Iteration 169/1000 | Loss: 0.00006707
Iteration 170/1000 | Loss: 0.00006706
Iteration 171/1000 | Loss: 0.00007699
Iteration 172/1000 | Loss: 0.00015120
Iteration 173/1000 | Loss: 0.00008517
Iteration 174/1000 | Loss: 0.00011353
Iteration 175/1000 | Loss: 0.00011163
Iteration 176/1000 | Loss: 0.00008814
Iteration 177/1000 | Loss: 0.00006685
Iteration 178/1000 | Loss: 0.00008738
Iteration 179/1000 | Loss: 0.00007044
Iteration 180/1000 | Loss: 0.00006676
Iteration 181/1000 | Loss: 0.00006676
Iteration 182/1000 | Loss: 0.00006675
Iteration 183/1000 | Loss: 0.00006675
Iteration 184/1000 | Loss: 0.00006675
Iteration 185/1000 | Loss: 0.00006675
Iteration 186/1000 | Loss: 0.00006675
Iteration 187/1000 | Loss: 0.00006675
Iteration 188/1000 | Loss: 0.00006675
Iteration 189/1000 | Loss: 0.00006675
Iteration 190/1000 | Loss: 0.00006675
Iteration 191/1000 | Loss: 0.00006675
Iteration 192/1000 | Loss: 0.00006675
Iteration 193/1000 | Loss: 0.00006675
Iteration 194/1000 | Loss: 0.00006675
Iteration 195/1000 | Loss: 0.00006674
Iteration 196/1000 | Loss: 0.00006674
Iteration 197/1000 | Loss: 0.00006674
Iteration 198/1000 | Loss: 0.00006674
Iteration 199/1000 | Loss: 0.00006674
Iteration 200/1000 | Loss: 0.00006674
Iteration 201/1000 | Loss: 0.00006674
Iteration 202/1000 | Loss: 0.00006674
Iteration 203/1000 | Loss: 0.00006674
Iteration 204/1000 | Loss: 0.00006674
Iteration 205/1000 | Loss: 0.00006674
Iteration 206/1000 | Loss: 0.00006674
Iteration 207/1000 | Loss: 0.00006674
Iteration 208/1000 | Loss: 0.00006674
Iteration 209/1000 | Loss: 0.00006674
Iteration 210/1000 | Loss: 0.00006674
Iteration 211/1000 | Loss: 0.00006674
Iteration 212/1000 | Loss: 0.00006673
Iteration 213/1000 | Loss: 0.00006673
Iteration 214/1000 | Loss: 0.00006673
Iteration 215/1000 | Loss: 0.00006673
Iteration 216/1000 | Loss: 0.00006673
Iteration 217/1000 | Loss: 0.00006672
Iteration 218/1000 | Loss: 0.00007321
Iteration 219/1000 | Loss: 0.00006671
Iteration 220/1000 | Loss: 0.00006670
Iteration 221/1000 | Loss: 0.00006670
Iteration 222/1000 | Loss: 0.00006670
Iteration 223/1000 | Loss: 0.00006669
Iteration 224/1000 | Loss: 0.00006669
Iteration 225/1000 | Loss: 0.00006669
Iteration 226/1000 | Loss: 0.00006669
Iteration 227/1000 | Loss: 0.00006668
Iteration 228/1000 | Loss: 0.00006668
Iteration 229/1000 | Loss: 0.00006668
Iteration 230/1000 | Loss: 0.00006668
Iteration 231/1000 | Loss: 0.00006668
Iteration 232/1000 | Loss: 0.00006668
Iteration 233/1000 | Loss: 0.00006668
Iteration 234/1000 | Loss: 0.00008380
Iteration 235/1000 | Loss: 0.00006672
Iteration 236/1000 | Loss: 0.00007137
Iteration 237/1000 | Loss: 0.00006668
Iteration 238/1000 | Loss: 0.00006668
Iteration 239/1000 | Loss: 0.00006667
Iteration 240/1000 | Loss: 0.00006667
Iteration 241/1000 | Loss: 0.00006667
Iteration 242/1000 | Loss: 0.00006667
Iteration 243/1000 | Loss: 0.00006667
Iteration 244/1000 | Loss: 0.00006667
Iteration 245/1000 | Loss: 0.00006667
Iteration 246/1000 | Loss: 0.00006667
Iteration 247/1000 | Loss: 0.00006667
Iteration 248/1000 | Loss: 0.00006667
Iteration 249/1000 | Loss: 0.00006667
Iteration 250/1000 | Loss: 0.00006667
Iteration 251/1000 | Loss: 0.00006667
Iteration 252/1000 | Loss: 0.00006667
Iteration 253/1000 | Loss: 0.00006667
Iteration 254/1000 | Loss: 0.00006667
Iteration 255/1000 | Loss: 0.00006667
Iteration 256/1000 | Loss: 0.00006667
Iteration 257/1000 | Loss: 0.00006667
Iteration 258/1000 | Loss: 0.00006667
Iteration 259/1000 | Loss: 0.00006667
Iteration 260/1000 | Loss: 0.00006667
Iteration 261/1000 | Loss: 0.00006667
Iteration 262/1000 | Loss: 0.00006667
Iteration 263/1000 | Loss: 0.00006667
Iteration 264/1000 | Loss: 0.00006667
Iteration 265/1000 | Loss: 0.00006667
Iteration 266/1000 | Loss: 0.00006667
Iteration 267/1000 | Loss: 0.00006667
Iteration 268/1000 | Loss: 0.00006667
Iteration 269/1000 | Loss: 0.00006667
Iteration 270/1000 | Loss: 0.00006667
Iteration 271/1000 | Loss: 0.00006667
Iteration 272/1000 | Loss: 0.00006667
Iteration 273/1000 | Loss: 0.00006667
Iteration 274/1000 | Loss: 0.00006667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [6.66695850668475e-05, 6.66695850668475e-05, 6.66695850668475e-05, 6.66695850668475e-05, 6.66695850668475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.66695850668475e-05

Optimization complete. Final v2v error: 4.975440502166748 mm

Highest mean error: 14.619447708129883 mm for frame 142

Lowest mean error: 3.844855785369873 mm for frame 9

Saving results

Total time: 350.21720933914185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826870
Iteration 2/25 | Loss: 0.00172578
Iteration 3/25 | Loss: 0.00154321
Iteration 4/25 | Loss: 0.00150942
Iteration 5/25 | Loss: 0.00149914
Iteration 6/25 | Loss: 0.00149334
Iteration 7/25 | Loss: 0.00148616
Iteration 8/25 | Loss: 0.00148991
Iteration 9/25 | Loss: 0.00148577
Iteration 10/25 | Loss: 0.00148574
Iteration 11/25 | Loss: 0.00148574
Iteration 12/25 | Loss: 0.00148574
Iteration 13/25 | Loss: 0.00148574
Iteration 14/25 | Loss: 0.00148574
Iteration 15/25 | Loss: 0.00148574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014857399510219693, 0.0014857399510219693, 0.0014857399510219693, 0.0014857399510219693, 0.0014857399510219693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014857399510219693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28202796
Iteration 2/25 | Loss: 0.00378099
Iteration 3/25 | Loss: 0.00378099
Iteration 4/25 | Loss: 0.00378099
Iteration 5/25 | Loss: 0.00378099
Iteration 6/25 | Loss: 0.00378099
Iteration 7/25 | Loss: 0.00378099
Iteration 8/25 | Loss: 0.00378099
Iteration 9/25 | Loss: 0.00378099
Iteration 10/25 | Loss: 0.00378099
Iteration 11/25 | Loss: 0.00378099
Iteration 12/25 | Loss: 0.00378099
Iteration 13/25 | Loss: 0.00378099
Iteration 14/25 | Loss: 0.00378099
Iteration 15/25 | Loss: 0.00378099
Iteration 16/25 | Loss: 0.00378099
Iteration 17/25 | Loss: 0.00378099
Iteration 18/25 | Loss: 0.00378099
Iteration 19/25 | Loss: 0.00378099
Iteration 20/25 | Loss: 0.00378099
Iteration 21/25 | Loss: 0.00378099
Iteration 22/25 | Loss: 0.00378099
Iteration 23/25 | Loss: 0.00378099
Iteration 24/25 | Loss: 0.00378099
Iteration 25/25 | Loss: 0.00378099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00378099
Iteration 2/1000 | Loss: 0.00004868
Iteration 3/1000 | Loss: 0.00003225
Iteration 4/1000 | Loss: 0.00002904
Iteration 5/1000 | Loss: 0.00002795
Iteration 6/1000 | Loss: 0.00002710
Iteration 7/1000 | Loss: 0.00002658
Iteration 8/1000 | Loss: 0.00002613
Iteration 9/1000 | Loss: 0.00002582
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002549
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002545
Iteration 15/1000 | Loss: 0.00002544
Iteration 16/1000 | Loss: 0.00002543
Iteration 17/1000 | Loss: 0.00002543
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002542
Iteration 20/1000 | Loss: 0.00002542
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002542
Iteration 23/1000 | Loss: 0.00002542
Iteration 24/1000 | Loss: 0.00002541
Iteration 25/1000 | Loss: 0.00002541
Iteration 26/1000 | Loss: 0.00002541
Iteration 27/1000 | Loss: 0.00002541
Iteration 28/1000 | Loss: 0.00002540
Iteration 29/1000 | Loss: 0.00002540
Iteration 30/1000 | Loss: 0.00002540
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00002539
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002538
Iteration 35/1000 | Loss: 0.00002537
Iteration 36/1000 | Loss: 0.00002536
Iteration 37/1000 | Loss: 0.00002536
Iteration 38/1000 | Loss: 0.00002535
Iteration 39/1000 | Loss: 0.00002535
Iteration 40/1000 | Loss: 0.00002533
Iteration 41/1000 | Loss: 0.00002532
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002531
Iteration 44/1000 | Loss: 0.00002531
Iteration 45/1000 | Loss: 0.00002531
Iteration 46/1000 | Loss: 0.00002531
Iteration 47/1000 | Loss: 0.00002531
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002531
Iteration 50/1000 | Loss: 0.00002530
Iteration 51/1000 | Loss: 0.00002530
Iteration 52/1000 | Loss: 0.00002529
Iteration 53/1000 | Loss: 0.00002529
Iteration 54/1000 | Loss: 0.00002529
Iteration 55/1000 | Loss: 0.00002528
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00002528
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002528
Iteration 60/1000 | Loss: 0.00002526
Iteration 61/1000 | Loss: 0.00002526
Iteration 62/1000 | Loss: 0.00002526
Iteration 63/1000 | Loss: 0.00002526
Iteration 64/1000 | Loss: 0.00002526
Iteration 65/1000 | Loss: 0.00002526
Iteration 66/1000 | Loss: 0.00002526
Iteration 67/1000 | Loss: 0.00002525
Iteration 68/1000 | Loss: 0.00002525
Iteration 69/1000 | Loss: 0.00002525
Iteration 70/1000 | Loss: 0.00002525
Iteration 71/1000 | Loss: 0.00002525
Iteration 72/1000 | Loss: 0.00002525
Iteration 73/1000 | Loss: 0.00002525
Iteration 74/1000 | Loss: 0.00002525
Iteration 75/1000 | Loss: 0.00002525
Iteration 76/1000 | Loss: 0.00002525
Iteration 77/1000 | Loss: 0.00002525
Iteration 78/1000 | Loss: 0.00002523
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00002523
Iteration 81/1000 | Loss: 0.00002523
Iteration 82/1000 | Loss: 0.00002522
Iteration 83/1000 | Loss: 0.00002522
Iteration 84/1000 | Loss: 0.00002522
Iteration 85/1000 | Loss: 0.00002522
Iteration 86/1000 | Loss: 0.00002522
Iteration 87/1000 | Loss: 0.00002522
Iteration 88/1000 | Loss: 0.00002522
Iteration 89/1000 | Loss: 0.00002522
Iteration 90/1000 | Loss: 0.00002522
Iteration 91/1000 | Loss: 0.00002521
Iteration 92/1000 | Loss: 0.00002521
Iteration 93/1000 | Loss: 0.00002521
Iteration 94/1000 | Loss: 0.00002520
Iteration 95/1000 | Loss: 0.00002520
Iteration 96/1000 | Loss: 0.00002520
Iteration 97/1000 | Loss: 0.00002520
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00002520
Iteration 100/1000 | Loss: 0.00002520
Iteration 101/1000 | Loss: 0.00002520
Iteration 102/1000 | Loss: 0.00002520
Iteration 103/1000 | Loss: 0.00002520
Iteration 104/1000 | Loss: 0.00002520
Iteration 105/1000 | Loss: 0.00002520
Iteration 106/1000 | Loss: 0.00002520
Iteration 107/1000 | Loss: 0.00002520
Iteration 108/1000 | Loss: 0.00002520
Iteration 109/1000 | Loss: 0.00002520
Iteration 110/1000 | Loss: 0.00002520
Iteration 111/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.519530062272679e-05, 2.519530062272679e-05, 2.519530062272679e-05, 2.519530062272679e-05, 2.519530062272679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.519530062272679e-05

Optimization complete. Final v2v error: 4.392847061157227 mm

Highest mean error: 4.831106662750244 mm for frame 149

Lowest mean error: 4.160567760467529 mm for frame 219

Saving results

Total time: 43.68166470527649
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01186808
Iteration 2/25 | Loss: 0.01186808
Iteration 3/25 | Loss: 0.01186808
Iteration 4/25 | Loss: 0.00328203
Iteration 5/25 | Loss: 0.00233971
Iteration 6/25 | Loss: 0.00224283
Iteration 7/25 | Loss: 0.00223921
Iteration 8/25 | Loss: 0.00224678
Iteration 9/25 | Loss: 0.00210697
Iteration 10/25 | Loss: 0.00199821
Iteration 11/25 | Loss: 0.00190618
Iteration 12/25 | Loss: 0.00187695
Iteration 13/25 | Loss: 0.00186124
Iteration 14/25 | Loss: 0.00184558
Iteration 15/25 | Loss: 0.00184257
Iteration 16/25 | Loss: 0.00183723
Iteration 17/25 | Loss: 0.00183070
Iteration 18/25 | Loss: 0.00182356
Iteration 19/25 | Loss: 0.00181971
Iteration 20/25 | Loss: 0.00181378
Iteration 21/25 | Loss: 0.00181334
Iteration 22/25 | Loss: 0.00181320
Iteration 23/25 | Loss: 0.00181312
Iteration 24/25 | Loss: 0.00181310
Iteration 25/25 | Loss: 0.00181309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65215254
Iteration 2/25 | Loss: 0.00870365
Iteration 3/25 | Loss: 0.00738377
Iteration 4/25 | Loss: 0.00738377
Iteration 5/25 | Loss: 0.00738376
Iteration 6/25 | Loss: 0.00738376
Iteration 7/25 | Loss: 0.00738376
Iteration 8/25 | Loss: 0.00738376
Iteration 9/25 | Loss: 0.00738376
Iteration 10/25 | Loss: 0.00738376
Iteration 11/25 | Loss: 0.00738376
Iteration 12/25 | Loss: 0.00738376
Iteration 13/25 | Loss: 0.00738376
Iteration 14/25 | Loss: 0.00738376
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007383761927485466, 0.007383761927485466, 0.007383761927485466, 0.007383761927485466, 0.007383761927485466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007383761927485466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00738376
Iteration 2/1000 | Loss: 0.00247099
Iteration 3/1000 | Loss: 0.00367560
Iteration 4/1000 | Loss: 0.00458446
Iteration 5/1000 | Loss: 0.00261451
Iteration 6/1000 | Loss: 0.00258296
Iteration 7/1000 | Loss: 0.00138327
Iteration 8/1000 | Loss: 0.00085323
Iteration 9/1000 | Loss: 0.00121816
Iteration 10/1000 | Loss: 0.01153978
Iteration 11/1000 | Loss: 0.00407719
Iteration 12/1000 | Loss: 0.00796150
Iteration 13/1000 | Loss: 0.00179015
Iteration 14/1000 | Loss: 0.00449043
Iteration 15/1000 | Loss: 0.00231132
Iteration 16/1000 | Loss: 0.00092594
Iteration 17/1000 | Loss: 0.00332480
Iteration 18/1000 | Loss: 0.00138682
Iteration 19/1000 | Loss: 0.00151764
Iteration 20/1000 | Loss: 0.00058428
Iteration 21/1000 | Loss: 0.00411356
Iteration 22/1000 | Loss: 0.00101359
Iteration 23/1000 | Loss: 0.00185461
Iteration 24/1000 | Loss: 0.00107146
Iteration 25/1000 | Loss: 0.00163817
Iteration 26/1000 | Loss: 0.00293230
Iteration 27/1000 | Loss: 0.00322838
Iteration 28/1000 | Loss: 0.00068534
Iteration 29/1000 | Loss: 0.00153780
Iteration 30/1000 | Loss: 0.00151008
Iteration 31/1000 | Loss: 0.00044058
Iteration 32/1000 | Loss: 0.00195451
Iteration 33/1000 | Loss: 0.00202649
Iteration 34/1000 | Loss: 0.00066885
Iteration 35/1000 | Loss: 0.00192508
Iteration 36/1000 | Loss: 0.00269974
Iteration 37/1000 | Loss: 0.00183653
Iteration 38/1000 | Loss: 0.00160200
Iteration 39/1000 | Loss: 0.00084792
Iteration 40/1000 | Loss: 0.00210154
Iteration 41/1000 | Loss: 0.00274104
Iteration 42/1000 | Loss: 0.00260654
Iteration 43/1000 | Loss: 0.00146128
Iteration 44/1000 | Loss: 0.00157986
Iteration 45/1000 | Loss: 0.00063661
Iteration 46/1000 | Loss: 0.00024669
Iteration 47/1000 | Loss: 0.00071081
Iteration 48/1000 | Loss: 0.00126901
Iteration 49/1000 | Loss: 0.00030128
Iteration 50/1000 | Loss: 0.00022048
Iteration 51/1000 | Loss: 0.00015429
Iteration 52/1000 | Loss: 0.00197637
Iteration 53/1000 | Loss: 0.00085724
Iteration 54/1000 | Loss: 0.00043313
Iteration 55/1000 | Loss: 0.00099376
Iteration 56/1000 | Loss: 0.00173464
Iteration 57/1000 | Loss: 0.00020371
Iteration 58/1000 | Loss: 0.00016706
Iteration 59/1000 | Loss: 0.00158532
Iteration 60/1000 | Loss: 0.00366203
Iteration 61/1000 | Loss: 0.00037486
Iteration 62/1000 | Loss: 0.00021433
Iteration 63/1000 | Loss: 0.00015017
Iteration 64/1000 | Loss: 0.00239135
Iteration 65/1000 | Loss: 0.00229091
Iteration 66/1000 | Loss: 0.00323600
Iteration 67/1000 | Loss: 0.00082422
Iteration 68/1000 | Loss: 0.00015793
Iteration 69/1000 | Loss: 0.00017396
Iteration 70/1000 | Loss: 0.00052459
Iteration 71/1000 | Loss: 0.00047646
Iteration 72/1000 | Loss: 0.00015249
Iteration 73/1000 | Loss: 0.00017758
Iteration 74/1000 | Loss: 0.00034277
Iteration 75/1000 | Loss: 0.00064729
Iteration 76/1000 | Loss: 0.00192743
Iteration 77/1000 | Loss: 0.00151535
Iteration 78/1000 | Loss: 0.00041918
Iteration 79/1000 | Loss: 0.00030710
Iteration 80/1000 | Loss: 0.00118233
Iteration 81/1000 | Loss: 0.00058657
Iteration 82/1000 | Loss: 0.00033907
Iteration 83/1000 | Loss: 0.00026086
Iteration 84/1000 | Loss: 0.00019679
Iteration 85/1000 | Loss: 0.00018458
Iteration 86/1000 | Loss: 0.00015348
Iteration 87/1000 | Loss: 0.00010240
Iteration 88/1000 | Loss: 0.00016794
Iteration 89/1000 | Loss: 0.00009814
Iteration 90/1000 | Loss: 0.00009638
Iteration 91/1000 | Loss: 0.00009523
Iteration 92/1000 | Loss: 0.00111540
Iteration 93/1000 | Loss: 0.00024018
Iteration 94/1000 | Loss: 0.00012864
Iteration 95/1000 | Loss: 0.00009357
Iteration 96/1000 | Loss: 0.00010048
Iteration 97/1000 | Loss: 0.00107716
Iteration 98/1000 | Loss: 0.00080466
Iteration 99/1000 | Loss: 0.00061111
Iteration 100/1000 | Loss: 0.00025294
Iteration 101/1000 | Loss: 0.00161860
Iteration 102/1000 | Loss: 0.00054236
Iteration 103/1000 | Loss: 0.00013602
Iteration 104/1000 | Loss: 0.00104414
Iteration 105/1000 | Loss: 0.00036734
Iteration 106/1000 | Loss: 0.00010125
Iteration 107/1000 | Loss: 0.00010268
Iteration 108/1000 | Loss: 0.00009853
Iteration 109/1000 | Loss: 0.00070112
Iteration 110/1000 | Loss: 0.00020074
Iteration 111/1000 | Loss: 0.00012007
Iteration 112/1000 | Loss: 0.00009767
Iteration 113/1000 | Loss: 0.00013517
Iteration 114/1000 | Loss: 0.00009387
Iteration 115/1000 | Loss: 0.00017732
Iteration 116/1000 | Loss: 0.00009230
Iteration 117/1000 | Loss: 0.00009179
Iteration 118/1000 | Loss: 0.00104632
Iteration 119/1000 | Loss: 0.00028013
Iteration 120/1000 | Loss: 0.00009915
Iteration 121/1000 | Loss: 0.00010564
Iteration 122/1000 | Loss: 0.00009147
Iteration 123/1000 | Loss: 0.00009093
Iteration 124/1000 | Loss: 0.00010941
Iteration 125/1000 | Loss: 0.00009055
Iteration 126/1000 | Loss: 0.00009041
Iteration 127/1000 | Loss: 0.00009040
Iteration 128/1000 | Loss: 0.00009040
Iteration 129/1000 | Loss: 0.00009032
Iteration 130/1000 | Loss: 0.00009013
Iteration 131/1000 | Loss: 0.00008994
Iteration 132/1000 | Loss: 0.00008994
Iteration 133/1000 | Loss: 0.00008986
Iteration 134/1000 | Loss: 0.00008967
Iteration 135/1000 | Loss: 0.00008949
Iteration 136/1000 | Loss: 0.00010241
Iteration 137/1000 | Loss: 0.00008919
Iteration 138/1000 | Loss: 0.00008909
Iteration 139/1000 | Loss: 0.00010798
Iteration 140/1000 | Loss: 0.00008895
Iteration 141/1000 | Loss: 0.00008871
Iteration 142/1000 | Loss: 0.00008869
Iteration 143/1000 | Loss: 0.00008869
Iteration 144/1000 | Loss: 0.00008869
Iteration 145/1000 | Loss: 0.00008854
Iteration 146/1000 | Loss: 0.00008849
Iteration 147/1000 | Loss: 0.00008848
Iteration 148/1000 | Loss: 0.00008848
Iteration 149/1000 | Loss: 0.00008845
Iteration 150/1000 | Loss: 0.00008844
Iteration 151/1000 | Loss: 0.00008832
Iteration 152/1000 | Loss: 0.00008832
Iteration 153/1000 | Loss: 0.00008827
Iteration 154/1000 | Loss: 0.00008827
Iteration 155/1000 | Loss: 0.00008826
Iteration 156/1000 | Loss: 0.00008826
Iteration 157/1000 | Loss: 0.00008826
Iteration 158/1000 | Loss: 0.00008825
Iteration 159/1000 | Loss: 0.00008824
Iteration 160/1000 | Loss: 0.00008823
Iteration 161/1000 | Loss: 0.00008821
Iteration 162/1000 | Loss: 0.00008820
Iteration 163/1000 | Loss: 0.00008819
Iteration 164/1000 | Loss: 0.00008819
Iteration 165/1000 | Loss: 0.00008819
Iteration 166/1000 | Loss: 0.00008817
Iteration 167/1000 | Loss: 0.00008815
Iteration 168/1000 | Loss: 0.00008815
Iteration 169/1000 | Loss: 0.00008815
Iteration 170/1000 | Loss: 0.00008814
Iteration 171/1000 | Loss: 0.00008813
Iteration 172/1000 | Loss: 0.00008812
Iteration 173/1000 | Loss: 0.00008811
Iteration 174/1000 | Loss: 0.00008811
Iteration 175/1000 | Loss: 0.00008811
Iteration 176/1000 | Loss: 0.00008811
Iteration 177/1000 | Loss: 0.00008811
Iteration 178/1000 | Loss: 0.00008811
Iteration 179/1000 | Loss: 0.00008811
Iteration 180/1000 | Loss: 0.00008811
Iteration 181/1000 | Loss: 0.00008811
Iteration 182/1000 | Loss: 0.00008811
Iteration 183/1000 | Loss: 0.00008811
Iteration 184/1000 | Loss: 0.00008810
Iteration 185/1000 | Loss: 0.00008810
Iteration 186/1000 | Loss: 0.00008810
Iteration 187/1000 | Loss: 0.00008810
Iteration 188/1000 | Loss: 0.00008809
Iteration 189/1000 | Loss: 0.00008809
Iteration 190/1000 | Loss: 0.00008808
Iteration 191/1000 | Loss: 0.00008808
Iteration 192/1000 | Loss: 0.00008808
Iteration 193/1000 | Loss: 0.00008806
Iteration 194/1000 | Loss: 0.00008805
Iteration 195/1000 | Loss: 0.00008805
Iteration 196/1000 | Loss: 0.00008805
Iteration 197/1000 | Loss: 0.00008805
Iteration 198/1000 | Loss: 0.00008805
Iteration 199/1000 | Loss: 0.00008805
Iteration 200/1000 | Loss: 0.00008803
Iteration 201/1000 | Loss: 0.00008802
Iteration 202/1000 | Loss: 0.00008802
Iteration 203/1000 | Loss: 0.00008802
Iteration 204/1000 | Loss: 0.00008801
Iteration 205/1000 | Loss: 0.00008801
Iteration 206/1000 | Loss: 0.00008801
Iteration 207/1000 | Loss: 0.00008801
Iteration 208/1000 | Loss: 0.00008801
Iteration 209/1000 | Loss: 0.00008801
Iteration 210/1000 | Loss: 0.00008800
Iteration 211/1000 | Loss: 0.00008800
Iteration 212/1000 | Loss: 0.00008800
Iteration 213/1000 | Loss: 0.00127261
Iteration 214/1000 | Loss: 0.00155221
Iteration 215/1000 | Loss: 0.00085179
Iteration 216/1000 | Loss: 0.00012020
Iteration 217/1000 | Loss: 0.00011552
Iteration 218/1000 | Loss: 0.00009751
Iteration 219/1000 | Loss: 0.00012368
Iteration 220/1000 | Loss: 0.00014652
Iteration 221/1000 | Loss: 0.00008742
Iteration 222/1000 | Loss: 0.00018311
Iteration 223/1000 | Loss: 0.00010211
Iteration 224/1000 | Loss: 0.00008424
Iteration 225/1000 | Loss: 0.00008362
Iteration 226/1000 | Loss: 0.00010057
Iteration 227/1000 | Loss: 0.00008296
Iteration 228/1000 | Loss: 0.00008272
Iteration 229/1000 | Loss: 0.00008271
Iteration 230/1000 | Loss: 0.00008256
Iteration 231/1000 | Loss: 0.00008253
Iteration 232/1000 | Loss: 0.00008253
Iteration 233/1000 | Loss: 0.00008253
Iteration 234/1000 | Loss: 0.00008252
Iteration 235/1000 | Loss: 0.00008246
Iteration 236/1000 | Loss: 0.00012160
Iteration 237/1000 | Loss: 0.00008365
Iteration 238/1000 | Loss: 0.00008256
Iteration 239/1000 | Loss: 0.00008240
Iteration 240/1000 | Loss: 0.00008237
Iteration 241/1000 | Loss: 0.00008236
Iteration 242/1000 | Loss: 0.00008235
Iteration 243/1000 | Loss: 0.00008235
Iteration 244/1000 | Loss: 0.00008235
Iteration 245/1000 | Loss: 0.00008235
Iteration 246/1000 | Loss: 0.00008235
Iteration 247/1000 | Loss: 0.00008234
Iteration 248/1000 | Loss: 0.00008234
Iteration 249/1000 | Loss: 0.00008234
Iteration 250/1000 | Loss: 0.00008234
Iteration 251/1000 | Loss: 0.00008234
Iteration 252/1000 | Loss: 0.00008234
Iteration 253/1000 | Loss: 0.00008234
Iteration 254/1000 | Loss: 0.00008233
Iteration 255/1000 | Loss: 0.00008233
Iteration 256/1000 | Loss: 0.00008233
Iteration 257/1000 | Loss: 0.00008233
Iteration 258/1000 | Loss: 0.00008233
Iteration 259/1000 | Loss: 0.00008233
Iteration 260/1000 | Loss: 0.00008233
Iteration 261/1000 | Loss: 0.00008233
Iteration 262/1000 | Loss: 0.00008233
Iteration 263/1000 | Loss: 0.00008233
Iteration 264/1000 | Loss: 0.00008233
Iteration 265/1000 | Loss: 0.00008233
Iteration 266/1000 | Loss: 0.00008233
Iteration 267/1000 | Loss: 0.00008233
Iteration 268/1000 | Loss: 0.00008233
Iteration 269/1000 | Loss: 0.00008233
Iteration 270/1000 | Loss: 0.00008233
Iteration 271/1000 | Loss: 0.00008233
Iteration 272/1000 | Loss: 0.00008233
Iteration 273/1000 | Loss: 0.00008233
Iteration 274/1000 | Loss: 0.00008233
Iteration 275/1000 | Loss: 0.00008233
Iteration 276/1000 | Loss: 0.00008232
Iteration 277/1000 | Loss: 0.00008232
Iteration 278/1000 | Loss: 0.00008232
Iteration 279/1000 | Loss: 0.00008232
Iteration 280/1000 | Loss: 0.00008232
Iteration 281/1000 | Loss: 0.00008232
Iteration 282/1000 | Loss: 0.00008232
Iteration 283/1000 | Loss: 0.00008232
Iteration 284/1000 | Loss: 0.00008232
Iteration 285/1000 | Loss: 0.00008232
Iteration 286/1000 | Loss: 0.00008232
Iteration 287/1000 | Loss: 0.00008232
Iteration 288/1000 | Loss: 0.00008232
Iteration 289/1000 | Loss: 0.00008232
Iteration 290/1000 | Loss: 0.00008232
Iteration 291/1000 | Loss: 0.00008232
Iteration 292/1000 | Loss: 0.00008232
Iteration 293/1000 | Loss: 0.00008232
Iteration 294/1000 | Loss: 0.00008232
Iteration 295/1000 | Loss: 0.00008231
Iteration 296/1000 | Loss: 0.00008231
Iteration 297/1000 | Loss: 0.00008231
Iteration 298/1000 | Loss: 0.00008231
Iteration 299/1000 | Loss: 0.00008231
Iteration 300/1000 | Loss: 0.00008231
Iteration 301/1000 | Loss: 0.00008231
Iteration 302/1000 | Loss: 0.00008231
Iteration 303/1000 | Loss: 0.00008231
Iteration 304/1000 | Loss: 0.00008231
Iteration 305/1000 | Loss: 0.00008231
Iteration 306/1000 | Loss: 0.00008231
Iteration 307/1000 | Loss: 0.00008231
Iteration 308/1000 | Loss: 0.00008231
Iteration 309/1000 | Loss: 0.00008231
Iteration 310/1000 | Loss: 0.00008231
Iteration 311/1000 | Loss: 0.00008231
Iteration 312/1000 | Loss: 0.00008231
Iteration 313/1000 | Loss: 0.00008230
Iteration 314/1000 | Loss: 0.00008230
Iteration 315/1000 | Loss: 0.00008230
Iteration 316/1000 | Loss: 0.00008230
Iteration 317/1000 | Loss: 0.00008230
Iteration 318/1000 | Loss: 0.00008230
Iteration 319/1000 | Loss: 0.00008230
Iteration 320/1000 | Loss: 0.00008230
Iteration 321/1000 | Loss: 0.00008230
Iteration 322/1000 | Loss: 0.00008230
Iteration 323/1000 | Loss: 0.00008230
Iteration 324/1000 | Loss: 0.00008230
Iteration 325/1000 | Loss: 0.00008230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [8.23030059109442e-05, 8.23030059109442e-05, 8.23030059109442e-05, 8.23030059109442e-05, 8.23030059109442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.23030059109442e-05

Optimization complete. Final v2v error: 5.674376964569092 mm

Highest mean error: 13.663175582885742 mm for frame 61

Lowest mean error: 4.168392658233643 mm for frame 78

Saving results

Total time: 288.4907648563385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01191916
Iteration 2/25 | Loss: 0.00293255
Iteration 3/25 | Loss: 0.00247000
Iteration 4/25 | Loss: 0.00235551
Iteration 5/25 | Loss: 0.00234727
Iteration 6/25 | Loss: 0.00232367
Iteration 7/25 | Loss: 0.00225028
Iteration 8/25 | Loss: 0.00217614
Iteration 9/25 | Loss: 0.00216027
Iteration 10/25 | Loss: 0.00212290
Iteration 11/25 | Loss: 0.00209896
Iteration 12/25 | Loss: 0.00209930
Iteration 13/25 | Loss: 0.00208110
Iteration 14/25 | Loss: 0.00208094
Iteration 15/25 | Loss: 0.00208024
Iteration 16/25 | Loss: 0.00207858
Iteration 17/25 | Loss: 0.00207716
Iteration 18/25 | Loss: 0.00206854
Iteration 19/25 | Loss: 0.00207191
Iteration 20/25 | Loss: 0.00207024
Iteration 21/25 | Loss: 0.00207440
Iteration 22/25 | Loss: 0.00207586
Iteration 23/25 | Loss: 0.00207378
Iteration 24/25 | Loss: 0.00207761
Iteration 25/25 | Loss: 0.00207132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71509230
Iteration 2/25 | Loss: 0.01110230
Iteration 3/25 | Loss: 0.00914385
Iteration 4/25 | Loss: 0.00914385
Iteration 5/25 | Loss: 0.00914385
Iteration 6/25 | Loss: 0.00914385
Iteration 7/25 | Loss: 0.00914385
Iteration 8/25 | Loss: 0.00914384
Iteration 9/25 | Loss: 0.00914384
Iteration 10/25 | Loss: 0.00914385
Iteration 11/25 | Loss: 0.00914384
Iteration 12/25 | Loss: 0.00914384
Iteration 13/25 | Loss: 0.00914384
Iteration 14/25 | Loss: 0.00914384
Iteration 15/25 | Loss: 0.00914384
Iteration 16/25 | Loss: 0.00914384
Iteration 17/25 | Loss: 0.00914384
Iteration 18/25 | Loss: 0.00914384
Iteration 19/25 | Loss: 0.00914384
Iteration 20/25 | Loss: 0.00914384
Iteration 21/25 | Loss: 0.00914384
Iteration 22/25 | Loss: 0.00914384
Iteration 23/25 | Loss: 0.00914384
Iteration 24/25 | Loss: 0.00914384
Iteration 25/25 | Loss: 0.00914384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.009143844246864319, 0.009143844246864319, 0.009143844246864319, 0.009143844246864319, 0.009143844246864319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009143844246864319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00914384
Iteration 2/1000 | Loss: 0.00319542
Iteration 3/1000 | Loss: 0.00294445
Iteration 4/1000 | Loss: 0.00432401
Iteration 5/1000 | Loss: 0.00119294
Iteration 6/1000 | Loss: 0.00286290
Iteration 7/1000 | Loss: 0.00205844
Iteration 8/1000 | Loss: 0.00218542
Iteration 9/1000 | Loss: 0.00106150
Iteration 10/1000 | Loss: 0.00157407
Iteration 11/1000 | Loss: 0.00082511
Iteration 12/1000 | Loss: 0.00214542
Iteration 13/1000 | Loss: 0.00244607
Iteration 14/1000 | Loss: 0.00088799
Iteration 15/1000 | Loss: 0.00132972
Iteration 16/1000 | Loss: 0.00803098
Iteration 17/1000 | Loss: 0.00784632
Iteration 18/1000 | Loss: 0.01320571
Iteration 19/1000 | Loss: 0.00352737
Iteration 20/1000 | Loss: 0.01703629
Iteration 21/1000 | Loss: 0.02860278
Iteration 22/1000 | Loss: 0.03516353
Iteration 23/1000 | Loss: 0.03390036
Iteration 24/1000 | Loss: 0.02401887
Iteration 25/1000 | Loss: 0.01700608
Iteration 26/1000 | Loss: 0.00697703
Iteration 27/1000 | Loss: 0.00749896
Iteration 28/1000 | Loss: 0.00762621
Iteration 29/1000 | Loss: 0.00792364
Iteration 30/1000 | Loss: 0.00373498
Iteration 31/1000 | Loss: 0.00539112
Iteration 32/1000 | Loss: 0.00395982
Iteration 33/1000 | Loss: 0.00268681
Iteration 34/1000 | Loss: 0.00055703
Iteration 35/1000 | Loss: 0.00271902
Iteration 36/1000 | Loss: 0.00473264
Iteration 37/1000 | Loss: 0.00390189
Iteration 38/1000 | Loss: 0.00129957
Iteration 39/1000 | Loss: 0.00240992
Iteration 40/1000 | Loss: 0.00231748
Iteration 41/1000 | Loss: 0.00214175
Iteration 42/1000 | Loss: 0.00085836
Iteration 43/1000 | Loss: 0.00072041
Iteration 44/1000 | Loss: 0.00052299
Iteration 45/1000 | Loss: 0.00152202
Iteration 46/1000 | Loss: 0.00237133
Iteration 47/1000 | Loss: 0.00099061
Iteration 48/1000 | Loss: 0.00145876
Iteration 49/1000 | Loss: 0.00110523
Iteration 50/1000 | Loss: 0.00083517
Iteration 51/1000 | Loss: 0.00150257
Iteration 52/1000 | Loss: 0.00147410
Iteration 53/1000 | Loss: 0.00165621
Iteration 54/1000 | Loss: 0.00219663
Iteration 55/1000 | Loss: 0.00215696
Iteration 56/1000 | Loss: 0.00249251
Iteration 57/1000 | Loss: 0.00139083
Iteration 58/1000 | Loss: 0.00286432
Iteration 59/1000 | Loss: 0.00178301
Iteration 60/1000 | Loss: 0.00260990
Iteration 61/1000 | Loss: 0.00182119
Iteration 62/1000 | Loss: 0.00089278
Iteration 63/1000 | Loss: 0.00023854
Iteration 64/1000 | Loss: 0.00197001
Iteration 65/1000 | Loss: 0.00134488
Iteration 66/1000 | Loss: 0.00214818
Iteration 67/1000 | Loss: 0.00129474
Iteration 68/1000 | Loss: 0.00078020
Iteration 69/1000 | Loss: 0.00124721
Iteration 70/1000 | Loss: 0.00048168
Iteration 71/1000 | Loss: 0.00115595
Iteration 72/1000 | Loss: 0.00043217
Iteration 73/1000 | Loss: 0.00045955
Iteration 74/1000 | Loss: 0.00113546
Iteration 75/1000 | Loss: 0.00151949
Iteration 76/1000 | Loss: 0.00042579
Iteration 77/1000 | Loss: 0.00019764
Iteration 78/1000 | Loss: 0.00056269
Iteration 79/1000 | Loss: 0.00053506
Iteration 80/1000 | Loss: 0.00013000
Iteration 81/1000 | Loss: 0.00007654
Iteration 82/1000 | Loss: 0.00322652
Iteration 83/1000 | Loss: 0.00147861
Iteration 84/1000 | Loss: 0.00198557
Iteration 85/1000 | Loss: 0.00202805
Iteration 86/1000 | Loss: 0.00110153
Iteration 87/1000 | Loss: 0.00116369
Iteration 88/1000 | Loss: 0.00136146
Iteration 89/1000 | Loss: 0.00096042
Iteration 90/1000 | Loss: 0.00076555
Iteration 91/1000 | Loss: 0.00083638
Iteration 92/1000 | Loss: 0.00088024
Iteration 93/1000 | Loss: 0.00188870
Iteration 94/1000 | Loss: 0.00051071
Iteration 95/1000 | Loss: 0.00118124
Iteration 96/1000 | Loss: 0.00041214
Iteration 97/1000 | Loss: 0.00266408
Iteration 98/1000 | Loss: 0.00136422
Iteration 99/1000 | Loss: 0.00029675
Iteration 100/1000 | Loss: 0.00042322
Iteration 101/1000 | Loss: 0.00018872
Iteration 102/1000 | Loss: 0.00023064
Iteration 103/1000 | Loss: 0.00050851
Iteration 104/1000 | Loss: 0.00041939
Iteration 105/1000 | Loss: 0.00078731
Iteration 106/1000 | Loss: 0.00071622
Iteration 107/1000 | Loss: 0.00042844
Iteration 108/1000 | Loss: 0.00033103
Iteration 109/1000 | Loss: 0.00050209
Iteration 110/1000 | Loss: 0.00028094
Iteration 111/1000 | Loss: 0.00021298
Iteration 112/1000 | Loss: 0.00043225
Iteration 113/1000 | Loss: 0.00058006
Iteration 114/1000 | Loss: 0.00022177
Iteration 115/1000 | Loss: 0.00017917
Iteration 116/1000 | Loss: 0.00027563
Iteration 117/1000 | Loss: 0.00016234
Iteration 118/1000 | Loss: 0.00022369
Iteration 119/1000 | Loss: 0.00010533
Iteration 120/1000 | Loss: 0.00022027
Iteration 121/1000 | Loss: 0.00074615
Iteration 122/1000 | Loss: 0.00047725
Iteration 123/1000 | Loss: 0.00110196
Iteration 124/1000 | Loss: 0.00076534
Iteration 125/1000 | Loss: 0.00049126
Iteration 126/1000 | Loss: 0.00034348
Iteration 127/1000 | Loss: 0.00028219
Iteration 128/1000 | Loss: 0.00045730
Iteration 129/1000 | Loss: 0.00037255
Iteration 130/1000 | Loss: 0.00064253
Iteration 131/1000 | Loss: 0.00043625
Iteration 132/1000 | Loss: 0.00043257
Iteration 133/1000 | Loss: 0.00065225
Iteration 134/1000 | Loss: 0.00049373
Iteration 135/1000 | Loss: 0.00078435
Iteration 136/1000 | Loss: 0.00054883
Iteration 137/1000 | Loss: 0.00075173
Iteration 138/1000 | Loss: 0.00068259
Iteration 139/1000 | Loss: 0.00036936
Iteration 140/1000 | Loss: 0.00006128
Iteration 141/1000 | Loss: 0.00078335
Iteration 142/1000 | Loss: 0.00046044
Iteration 143/1000 | Loss: 0.00011505
Iteration 144/1000 | Loss: 0.00107845
Iteration 145/1000 | Loss: 0.00058156
Iteration 146/1000 | Loss: 0.00056753
Iteration 147/1000 | Loss: 0.00137042
Iteration 148/1000 | Loss: 0.00030363
Iteration 149/1000 | Loss: 0.00022364
Iteration 150/1000 | Loss: 0.00018310
Iteration 151/1000 | Loss: 0.00005687
Iteration 152/1000 | Loss: 0.00027160
Iteration 153/1000 | Loss: 0.00031669
Iteration 154/1000 | Loss: 0.00004798
Iteration 155/1000 | Loss: 0.00083479
Iteration 156/1000 | Loss: 0.00032734
Iteration 157/1000 | Loss: 0.00057469
Iteration 158/1000 | Loss: 0.00073464
Iteration 159/1000 | Loss: 0.00079546
Iteration 160/1000 | Loss: 0.00051384
Iteration 161/1000 | Loss: 0.00086667
Iteration 162/1000 | Loss: 0.00093088
Iteration 163/1000 | Loss: 0.00049379
Iteration 164/1000 | Loss: 0.00033017
Iteration 165/1000 | Loss: 0.00089249
Iteration 166/1000 | Loss: 0.00036652
Iteration 167/1000 | Loss: 0.00087287
Iteration 168/1000 | Loss: 0.00064528
Iteration 169/1000 | Loss: 0.00096003
Iteration 170/1000 | Loss: 0.00074294
Iteration 171/1000 | Loss: 0.00125361
Iteration 172/1000 | Loss: 0.00094323
Iteration 173/1000 | Loss: 0.00123410
Iteration 174/1000 | Loss: 0.00053883
Iteration 175/1000 | Loss: 0.00070277
Iteration 176/1000 | Loss: 0.00085130
Iteration 177/1000 | Loss: 0.00064443
Iteration 178/1000 | Loss: 0.00071313
Iteration 179/1000 | Loss: 0.00100541
Iteration 180/1000 | Loss: 0.00069335
Iteration 181/1000 | Loss: 0.00038263
Iteration 182/1000 | Loss: 0.00179619
Iteration 183/1000 | Loss: 0.00150069
Iteration 184/1000 | Loss: 0.00050716
Iteration 185/1000 | Loss: 0.00070239
Iteration 186/1000 | Loss: 0.00075611
Iteration 187/1000 | Loss: 0.00081760
Iteration 188/1000 | Loss: 0.00052566
Iteration 189/1000 | Loss: 0.00091472
Iteration 190/1000 | Loss: 0.00057447
Iteration 191/1000 | Loss: 0.00039735
Iteration 192/1000 | Loss: 0.00051001
Iteration 193/1000 | Loss: 0.00015549
Iteration 194/1000 | Loss: 0.00060809
Iteration 195/1000 | Loss: 0.00020990
Iteration 196/1000 | Loss: 0.00055727
Iteration 197/1000 | Loss: 0.00019877
Iteration 198/1000 | Loss: 0.00054001
Iteration 199/1000 | Loss: 0.00005227
Iteration 200/1000 | Loss: 0.00082765
Iteration 201/1000 | Loss: 0.00084048
Iteration 202/1000 | Loss: 0.00021135
Iteration 203/1000 | Loss: 0.00010447
Iteration 204/1000 | Loss: 0.00054598
Iteration 205/1000 | Loss: 0.00005499
Iteration 206/1000 | Loss: 0.00005345
Iteration 207/1000 | Loss: 0.00006277
Iteration 208/1000 | Loss: 0.00009863
Iteration 209/1000 | Loss: 0.00057500
Iteration 210/1000 | Loss: 0.00062927
Iteration 211/1000 | Loss: 0.00103738
Iteration 212/1000 | Loss: 0.00061478
Iteration 213/1000 | Loss: 0.00071328
Iteration 214/1000 | Loss: 0.00004910
Iteration 215/1000 | Loss: 0.00003310
Iteration 216/1000 | Loss: 0.00003025
Iteration 217/1000 | Loss: 0.00002824
Iteration 218/1000 | Loss: 0.00004086
Iteration 219/1000 | Loss: 0.00002610
Iteration 220/1000 | Loss: 0.00007240
Iteration 221/1000 | Loss: 0.00004869
Iteration 222/1000 | Loss: 0.00003104
Iteration 223/1000 | Loss: 0.00008930
Iteration 224/1000 | Loss: 0.00002684
Iteration 225/1000 | Loss: 0.00002816
Iteration 226/1000 | Loss: 0.00003686
Iteration 227/1000 | Loss: 0.00002397
Iteration 228/1000 | Loss: 0.00003362
Iteration 229/1000 | Loss: 0.00002349
Iteration 230/1000 | Loss: 0.00004085
Iteration 231/1000 | Loss: 0.00004085
Iteration 232/1000 | Loss: 0.00002698
Iteration 233/1000 | Loss: 0.00002438
Iteration 234/1000 | Loss: 0.00002331
Iteration 235/1000 | Loss: 0.00002311
Iteration 236/1000 | Loss: 0.00002307
Iteration 237/1000 | Loss: 0.00002294
Iteration 238/1000 | Loss: 0.00002289
Iteration 239/1000 | Loss: 0.00002287
Iteration 240/1000 | Loss: 0.00002286
Iteration 241/1000 | Loss: 0.00002286
Iteration 242/1000 | Loss: 0.00002286
Iteration 243/1000 | Loss: 0.00002286
Iteration 244/1000 | Loss: 0.00002286
Iteration 245/1000 | Loss: 0.00002286
Iteration 246/1000 | Loss: 0.00002286
Iteration 247/1000 | Loss: 0.00002285
Iteration 248/1000 | Loss: 0.00002285
Iteration 249/1000 | Loss: 0.00002285
Iteration 250/1000 | Loss: 0.00002285
Iteration 251/1000 | Loss: 0.00002284
Iteration 252/1000 | Loss: 0.00002284
Iteration 253/1000 | Loss: 0.00002284
Iteration 254/1000 | Loss: 0.00002284
Iteration 255/1000 | Loss: 0.00002284
Iteration 256/1000 | Loss: 0.00002284
Iteration 257/1000 | Loss: 0.00002284
Iteration 258/1000 | Loss: 0.00002284
Iteration 259/1000 | Loss: 0.00002284
Iteration 260/1000 | Loss: 0.00002284
Iteration 261/1000 | Loss: 0.00002283
Iteration 262/1000 | Loss: 0.00002283
Iteration 263/1000 | Loss: 0.00002283
Iteration 264/1000 | Loss: 0.00002283
Iteration 265/1000 | Loss: 0.00002283
Iteration 266/1000 | Loss: 0.00002283
Iteration 267/1000 | Loss: 0.00002282
Iteration 268/1000 | Loss: 0.00002282
Iteration 269/1000 | Loss: 0.00002282
Iteration 270/1000 | Loss: 0.00002282
Iteration 271/1000 | Loss: 0.00002282
Iteration 272/1000 | Loss: 0.00002282
Iteration 273/1000 | Loss: 0.00002281
Iteration 274/1000 | Loss: 0.00002281
Iteration 275/1000 | Loss: 0.00002281
Iteration 276/1000 | Loss: 0.00002281
Iteration 277/1000 | Loss: 0.00002281
Iteration 278/1000 | Loss: 0.00002281
Iteration 279/1000 | Loss: 0.00002281
Iteration 280/1000 | Loss: 0.00002281
Iteration 281/1000 | Loss: 0.00002281
Iteration 282/1000 | Loss: 0.00002281
Iteration 283/1000 | Loss: 0.00002280
Iteration 284/1000 | Loss: 0.00002280
Iteration 285/1000 | Loss: 0.00002280
Iteration 286/1000 | Loss: 0.00002279
Iteration 287/1000 | Loss: 0.00002279
Iteration 288/1000 | Loss: 0.00002278
Iteration 289/1000 | Loss: 0.00002278
Iteration 290/1000 | Loss: 0.00002278
Iteration 291/1000 | Loss: 0.00002278
Iteration 292/1000 | Loss: 0.00002278
Iteration 293/1000 | Loss: 0.00002277
Iteration 294/1000 | Loss: 0.00002277
Iteration 295/1000 | Loss: 0.00002277
Iteration 296/1000 | Loss: 0.00002277
Iteration 297/1000 | Loss: 0.00002277
Iteration 298/1000 | Loss: 0.00002277
Iteration 299/1000 | Loss: 0.00002277
Iteration 300/1000 | Loss: 0.00002277
Iteration 301/1000 | Loss: 0.00002276
Iteration 302/1000 | Loss: 0.00002276
Iteration 303/1000 | Loss: 0.00002276
Iteration 304/1000 | Loss: 0.00002276
Iteration 305/1000 | Loss: 0.00002276
Iteration 306/1000 | Loss: 0.00002276
Iteration 307/1000 | Loss: 0.00002276
Iteration 308/1000 | Loss: 0.00002276
Iteration 309/1000 | Loss: 0.00002275
Iteration 310/1000 | Loss: 0.00002275
Iteration 311/1000 | Loss: 0.00002275
Iteration 312/1000 | Loss: 0.00002275
Iteration 313/1000 | Loss: 0.00002275
Iteration 314/1000 | Loss: 0.00002275
Iteration 315/1000 | Loss: 0.00002275
Iteration 316/1000 | Loss: 0.00002275
Iteration 317/1000 | Loss: 0.00002274
Iteration 318/1000 | Loss: 0.00002274
Iteration 319/1000 | Loss: 0.00002274
Iteration 320/1000 | Loss: 0.00002274
Iteration 321/1000 | Loss: 0.00002274
Iteration 322/1000 | Loss: 0.00002274
Iteration 323/1000 | Loss: 0.00002274
Iteration 324/1000 | Loss: 0.00002273
Iteration 325/1000 | Loss: 0.00002273
Iteration 326/1000 | Loss: 0.00002273
Iteration 327/1000 | Loss: 0.00002273
Iteration 328/1000 | Loss: 0.00002273
Iteration 329/1000 | Loss: 0.00002272
Iteration 330/1000 | Loss: 0.00002272
Iteration 331/1000 | Loss: 0.00002272
Iteration 332/1000 | Loss: 0.00002272
Iteration 333/1000 | Loss: 0.00002272
Iteration 334/1000 | Loss: 0.00002272
Iteration 335/1000 | Loss: 0.00002272
Iteration 336/1000 | Loss: 0.00002272
Iteration 337/1000 | Loss: 0.00002272
Iteration 338/1000 | Loss: 0.00002272
Iteration 339/1000 | Loss: 0.00002272
Iteration 340/1000 | Loss: 0.00002272
Iteration 341/1000 | Loss: 0.00002272
Iteration 342/1000 | Loss: 0.00002272
Iteration 343/1000 | Loss: 0.00002272
Iteration 344/1000 | Loss: 0.00002272
Iteration 345/1000 | Loss: 0.00002272
Iteration 346/1000 | Loss: 0.00002272
Iteration 347/1000 | Loss: 0.00002272
Iteration 348/1000 | Loss: 0.00002272
Iteration 349/1000 | Loss: 0.00002272
Iteration 350/1000 | Loss: 0.00002272
Iteration 351/1000 | Loss: 0.00002272
Iteration 352/1000 | Loss: 0.00002272
Iteration 353/1000 | Loss: 0.00002272
Iteration 354/1000 | Loss: 0.00002272
Iteration 355/1000 | Loss: 0.00002272
Iteration 356/1000 | Loss: 0.00002272
Iteration 357/1000 | Loss: 0.00002272
Iteration 358/1000 | Loss: 0.00002272
Iteration 359/1000 | Loss: 0.00002272
Iteration 360/1000 | Loss: 0.00002272
Iteration 361/1000 | Loss: 0.00002272
Iteration 362/1000 | Loss: 0.00002272
Iteration 363/1000 | Loss: 0.00002272
Iteration 364/1000 | Loss: 0.00002272
Iteration 365/1000 | Loss: 0.00002272
Iteration 366/1000 | Loss: 0.00002272
Iteration 367/1000 | Loss: 0.00002272
Iteration 368/1000 | Loss: 0.00002272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [2.27196142077446e-05, 2.27196142077446e-05, 2.27196142077446e-05, 2.27196142077446e-05, 2.27196142077446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.27196142077446e-05

Optimization complete. Final v2v error: 4.22257661819458 mm

Highest mean error: 4.659775733947754 mm for frame 0

Lowest mean error: 3.9052984714508057 mm for frame 27

Saving results

Total time: 375.02620029449463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607876
Iteration 2/25 | Loss: 0.00158929
Iteration 3/25 | Loss: 0.00152352
Iteration 4/25 | Loss: 0.00150739
Iteration 5/25 | Loss: 0.00150331
Iteration 6/25 | Loss: 0.00150175
Iteration 7/25 | Loss: 0.00150174
Iteration 8/25 | Loss: 0.00150174
Iteration 9/25 | Loss: 0.00150174
Iteration 10/25 | Loss: 0.00150174
Iteration 11/25 | Loss: 0.00150174
Iteration 12/25 | Loss: 0.00150174
Iteration 13/25 | Loss: 0.00150174
Iteration 14/25 | Loss: 0.00150174
Iteration 15/25 | Loss: 0.00150174
Iteration 16/25 | Loss: 0.00150174
Iteration 17/25 | Loss: 0.00150174
Iteration 18/25 | Loss: 0.00150174
Iteration 19/25 | Loss: 0.00150174
Iteration 20/25 | Loss: 0.00150174
Iteration 21/25 | Loss: 0.00150174
Iteration 22/25 | Loss: 0.00150174
Iteration 23/25 | Loss: 0.00150174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015017361147329211, 0.0015017361147329211, 0.0015017361147329211, 0.0015017361147329211, 0.0015017361147329211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015017361147329211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.98200083
Iteration 2/25 | Loss: 0.00391906
Iteration 3/25 | Loss: 0.00391906
Iteration 4/25 | Loss: 0.00391905
Iteration 5/25 | Loss: 0.00391905
Iteration 6/25 | Loss: 0.00391905
Iteration 7/25 | Loss: 0.00391905
Iteration 8/25 | Loss: 0.00391905
Iteration 9/25 | Loss: 0.00391905
Iteration 10/25 | Loss: 0.00391905
Iteration 11/25 | Loss: 0.00391905
Iteration 12/25 | Loss: 0.00391905
Iteration 13/25 | Loss: 0.00391905
Iteration 14/25 | Loss: 0.00391905
Iteration 15/25 | Loss: 0.00391905
Iteration 16/25 | Loss: 0.00391905
Iteration 17/25 | Loss: 0.00391905
Iteration 18/25 | Loss: 0.00391905
Iteration 19/25 | Loss: 0.00391905
Iteration 20/25 | Loss: 0.00391905
Iteration 21/25 | Loss: 0.00391905
Iteration 22/25 | Loss: 0.00391905
Iteration 23/25 | Loss: 0.00391905
Iteration 24/25 | Loss: 0.00391905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00391905102878809, 0.00391905102878809, 0.00391905102878809, 0.00391905102878809, 0.00391905102878809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00391905102878809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00391905
Iteration 2/1000 | Loss: 0.00006041
Iteration 3/1000 | Loss: 0.00004163
Iteration 4/1000 | Loss: 0.00003664
Iteration 5/1000 | Loss: 0.00003374
Iteration 6/1000 | Loss: 0.00003252
Iteration 7/1000 | Loss: 0.00003149
Iteration 8/1000 | Loss: 0.00003095
Iteration 9/1000 | Loss: 0.00003055
Iteration 10/1000 | Loss: 0.00003025
Iteration 11/1000 | Loss: 0.00003003
Iteration 12/1000 | Loss: 0.00003002
Iteration 13/1000 | Loss: 0.00002999
Iteration 14/1000 | Loss: 0.00002999
Iteration 15/1000 | Loss: 0.00002999
Iteration 16/1000 | Loss: 0.00002998
Iteration 17/1000 | Loss: 0.00002998
Iteration 18/1000 | Loss: 0.00002997
Iteration 19/1000 | Loss: 0.00002997
Iteration 20/1000 | Loss: 0.00002997
Iteration 21/1000 | Loss: 0.00002996
Iteration 22/1000 | Loss: 0.00002996
Iteration 23/1000 | Loss: 0.00002996
Iteration 24/1000 | Loss: 0.00002995
Iteration 25/1000 | Loss: 0.00002995
Iteration 26/1000 | Loss: 0.00002995
Iteration 27/1000 | Loss: 0.00002993
Iteration 28/1000 | Loss: 0.00002993
Iteration 29/1000 | Loss: 0.00002992
Iteration 30/1000 | Loss: 0.00002991
Iteration 31/1000 | Loss: 0.00002990
Iteration 32/1000 | Loss: 0.00002990
Iteration 33/1000 | Loss: 0.00002989
Iteration 34/1000 | Loss: 0.00002989
Iteration 35/1000 | Loss: 0.00002988
Iteration 36/1000 | Loss: 0.00002988
Iteration 37/1000 | Loss: 0.00002985
Iteration 38/1000 | Loss: 0.00002984
Iteration 39/1000 | Loss: 0.00002980
Iteration 40/1000 | Loss: 0.00002980
Iteration 41/1000 | Loss: 0.00002980
Iteration 42/1000 | Loss: 0.00002980
Iteration 43/1000 | Loss: 0.00002978
Iteration 44/1000 | Loss: 0.00002977
Iteration 45/1000 | Loss: 0.00002977
Iteration 46/1000 | Loss: 0.00002977
Iteration 47/1000 | Loss: 0.00002977
Iteration 48/1000 | Loss: 0.00002977
Iteration 49/1000 | Loss: 0.00002977
Iteration 50/1000 | Loss: 0.00002977
Iteration 51/1000 | Loss: 0.00002977
Iteration 52/1000 | Loss: 0.00002977
Iteration 53/1000 | Loss: 0.00002977
Iteration 54/1000 | Loss: 0.00002977
Iteration 55/1000 | Loss: 0.00002976
Iteration 56/1000 | Loss: 0.00002976
Iteration 57/1000 | Loss: 0.00002976
Iteration 58/1000 | Loss: 0.00002976
Iteration 59/1000 | Loss: 0.00002976
Iteration 60/1000 | Loss: 0.00002976
Iteration 61/1000 | Loss: 0.00002975
Iteration 62/1000 | Loss: 0.00002975
Iteration 63/1000 | Loss: 0.00002974
Iteration 64/1000 | Loss: 0.00002974
Iteration 65/1000 | Loss: 0.00002974
Iteration 66/1000 | Loss: 0.00002973
Iteration 67/1000 | Loss: 0.00002973
Iteration 68/1000 | Loss: 0.00002973
Iteration 69/1000 | Loss: 0.00002973
Iteration 70/1000 | Loss: 0.00002973
Iteration 71/1000 | Loss: 0.00002973
Iteration 72/1000 | Loss: 0.00002973
Iteration 73/1000 | Loss: 0.00002972
Iteration 74/1000 | Loss: 0.00002972
Iteration 75/1000 | Loss: 0.00002972
Iteration 76/1000 | Loss: 0.00002971
Iteration 77/1000 | Loss: 0.00002971
Iteration 78/1000 | Loss: 0.00002971
Iteration 79/1000 | Loss: 0.00002971
Iteration 80/1000 | Loss: 0.00002971
Iteration 81/1000 | Loss: 0.00002971
Iteration 82/1000 | Loss: 0.00002971
Iteration 83/1000 | Loss: 0.00002971
Iteration 84/1000 | Loss: 0.00002971
Iteration 85/1000 | Loss: 0.00002971
Iteration 86/1000 | Loss: 0.00002971
Iteration 87/1000 | Loss: 0.00002970
Iteration 88/1000 | Loss: 0.00002970
Iteration 89/1000 | Loss: 0.00002970
Iteration 90/1000 | Loss: 0.00002970
Iteration 91/1000 | Loss: 0.00002970
Iteration 92/1000 | Loss: 0.00002969
Iteration 93/1000 | Loss: 0.00002969
Iteration 94/1000 | Loss: 0.00002969
Iteration 95/1000 | Loss: 0.00002969
Iteration 96/1000 | Loss: 0.00002969
Iteration 97/1000 | Loss: 0.00002969
Iteration 98/1000 | Loss: 0.00002969
Iteration 99/1000 | Loss: 0.00002969
Iteration 100/1000 | Loss: 0.00002969
Iteration 101/1000 | Loss: 0.00002969
Iteration 102/1000 | Loss: 0.00002969
Iteration 103/1000 | Loss: 0.00002969
Iteration 104/1000 | Loss: 0.00002969
Iteration 105/1000 | Loss: 0.00002969
Iteration 106/1000 | Loss: 0.00002968
Iteration 107/1000 | Loss: 0.00002968
Iteration 108/1000 | Loss: 0.00002968
Iteration 109/1000 | Loss: 0.00002968
Iteration 110/1000 | Loss: 0.00002968
Iteration 111/1000 | Loss: 0.00002968
Iteration 112/1000 | Loss: 0.00002968
Iteration 113/1000 | Loss: 0.00002968
Iteration 114/1000 | Loss: 0.00002968
Iteration 115/1000 | Loss: 0.00002968
Iteration 116/1000 | Loss: 0.00002968
Iteration 117/1000 | Loss: 0.00002968
Iteration 118/1000 | Loss: 0.00002968
Iteration 119/1000 | Loss: 0.00002968
Iteration 120/1000 | Loss: 0.00002968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.9681525120395236e-05, 2.9681525120395236e-05, 2.9681525120395236e-05, 2.9681525120395236e-05, 2.9681525120395236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9681525120395236e-05

Optimization complete. Final v2v error: 4.767879009246826 mm

Highest mean error: 5.220054626464844 mm for frame 67

Lowest mean error: 4.468906402587891 mm for frame 133

Saving results

Total time: 34.69061899185181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100770
Iteration 2/25 | Loss: 0.00531236
Iteration 3/25 | Loss: 0.00320693
Iteration 4/25 | Loss: 0.00306823
Iteration 5/25 | Loss: 0.00251326
Iteration 6/25 | Loss: 0.00233210
Iteration 7/25 | Loss: 0.00226717
Iteration 8/25 | Loss: 0.00215462
Iteration 9/25 | Loss: 0.00215576
Iteration 10/25 | Loss: 0.00209796
Iteration 11/25 | Loss: 0.00207957
Iteration 12/25 | Loss: 0.00208385
Iteration 13/25 | Loss: 0.00203844
Iteration 14/25 | Loss: 0.00206977
Iteration 15/25 | Loss: 0.00203245
Iteration 16/25 | Loss: 0.00202643
Iteration 17/25 | Loss: 0.00196713
Iteration 18/25 | Loss: 0.00198966
Iteration 19/25 | Loss: 0.00200208
Iteration 20/25 | Loss: 0.00198055
Iteration 21/25 | Loss: 0.00195956
Iteration 22/25 | Loss: 0.00195991
Iteration 23/25 | Loss: 0.00195181
Iteration 24/25 | Loss: 0.00197372
Iteration 25/25 | Loss: 0.00194820

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72937059
Iteration 2/25 | Loss: 0.01820311
Iteration 3/25 | Loss: 0.00674249
Iteration 4/25 | Loss: 0.00674249
Iteration 5/25 | Loss: 0.00674249
Iteration 6/25 | Loss: 0.00674249
Iteration 7/25 | Loss: 0.00674249
Iteration 8/25 | Loss: 0.00674249
Iteration 9/25 | Loss: 0.00674249
Iteration 10/25 | Loss: 0.00674249
Iteration 11/25 | Loss: 0.00674249
Iteration 12/25 | Loss: 0.00674249
Iteration 13/25 | Loss: 0.00674249
Iteration 14/25 | Loss: 0.00674249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.006742486730217934, 0.006742486730217934, 0.006742486730217934, 0.006742486730217934, 0.006742486730217934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006742486730217934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00674249
Iteration 2/1000 | Loss: 0.01246484
Iteration 3/1000 | Loss: 0.00083676
Iteration 4/1000 | Loss: 0.00294631
Iteration 5/1000 | Loss: 0.00087476
Iteration 6/1000 | Loss: 0.00078809
Iteration 7/1000 | Loss: 0.00493127
Iteration 8/1000 | Loss: 0.00136630
Iteration 9/1000 | Loss: 0.00058618
Iteration 10/1000 | Loss: 0.00067796
Iteration 11/1000 | Loss: 0.02640470
Iteration 12/1000 | Loss: 0.00422056
Iteration 13/1000 | Loss: 0.00231977
Iteration 14/1000 | Loss: 0.00108394
Iteration 15/1000 | Loss: 0.00071353
Iteration 16/1000 | Loss: 0.00314327
Iteration 17/1000 | Loss: 0.00504598
Iteration 18/1000 | Loss: 0.00026519
Iteration 19/1000 | Loss: 0.00028311
Iteration 20/1000 | Loss: 0.00028287
Iteration 21/1000 | Loss: 0.00037223
Iteration 22/1000 | Loss: 0.00034159
Iteration 23/1000 | Loss: 0.00008655
Iteration 24/1000 | Loss: 0.00011568
Iteration 25/1000 | Loss: 0.00005967
Iteration 26/1000 | Loss: 0.00085907
Iteration 27/1000 | Loss: 0.00009600
Iteration 28/1000 | Loss: 0.00053723
Iteration 29/1000 | Loss: 0.00175985
Iteration 30/1000 | Loss: 0.00011902
Iteration 31/1000 | Loss: 0.00188359
Iteration 32/1000 | Loss: 0.00024986
Iteration 33/1000 | Loss: 0.00037837
Iteration 34/1000 | Loss: 0.00007154
Iteration 35/1000 | Loss: 0.00006958
Iteration 36/1000 | Loss: 0.00003284
Iteration 37/1000 | Loss: 0.00029739
Iteration 38/1000 | Loss: 0.00159433
Iteration 39/1000 | Loss: 0.00033489
Iteration 40/1000 | Loss: 0.00007962
Iteration 41/1000 | Loss: 0.00009234
Iteration 42/1000 | Loss: 0.00002833
Iteration 43/1000 | Loss: 0.00026051
Iteration 44/1000 | Loss: 0.00009903
Iteration 45/1000 | Loss: 0.00012479
Iteration 46/1000 | Loss: 0.00002673
Iteration 47/1000 | Loss: 0.00007011
Iteration 48/1000 | Loss: 0.00035104
Iteration 49/1000 | Loss: 0.00002628
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002559
Iteration 52/1000 | Loss: 0.00016673
Iteration 53/1000 | Loss: 0.00014525
Iteration 54/1000 | Loss: 0.00009211
Iteration 55/1000 | Loss: 0.00002529
Iteration 56/1000 | Loss: 0.00006191
Iteration 57/1000 | Loss: 0.00002445
Iteration 58/1000 | Loss: 0.00002551
Iteration 59/1000 | Loss: 0.00002426
Iteration 60/1000 | Loss: 0.00002963
Iteration 61/1000 | Loss: 0.00002600
Iteration 62/1000 | Loss: 0.00003180
Iteration 63/1000 | Loss: 0.00002657
Iteration 64/1000 | Loss: 0.00003269
Iteration 65/1000 | Loss: 0.00002727
Iteration 66/1000 | Loss: 0.00003353
Iteration 67/1000 | Loss: 0.00002849
Iteration 68/1000 | Loss: 0.00003812
Iteration 69/1000 | Loss: 0.00002989
Iteration 70/1000 | Loss: 0.00016020
Iteration 71/1000 | Loss: 0.00002946
Iteration 72/1000 | Loss: 0.00013925
Iteration 73/1000 | Loss: 0.00027326
Iteration 74/1000 | Loss: 0.00015978
Iteration 75/1000 | Loss: 0.00003128
Iteration 76/1000 | Loss: 0.00039800
Iteration 77/1000 | Loss: 0.00002627
Iteration 78/1000 | Loss: 0.00002443
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00004671
Iteration 81/1000 | Loss: 0.00020730
Iteration 82/1000 | Loss: 0.00014540
Iteration 83/1000 | Loss: 0.00029679
Iteration 84/1000 | Loss: 0.00007555
Iteration 85/1000 | Loss: 0.00018819
Iteration 86/1000 | Loss: 0.00008738
Iteration 87/1000 | Loss: 0.00002493
Iteration 88/1000 | Loss: 0.00005110
Iteration 89/1000 | Loss: 0.00002369
Iteration 90/1000 | Loss: 0.00015708
Iteration 91/1000 | Loss: 0.00022333
Iteration 92/1000 | Loss: 0.00014061
Iteration 93/1000 | Loss: 0.00002369
Iteration 94/1000 | Loss: 0.00004545
Iteration 95/1000 | Loss: 0.00002345
Iteration 96/1000 | Loss: 0.00002330
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00014665
Iteration 99/1000 | Loss: 0.00042741
Iteration 100/1000 | Loss: 0.00002344
Iteration 101/1000 | Loss: 0.00002326
Iteration 102/1000 | Loss: 0.00002325
Iteration 103/1000 | Loss: 0.00002324
Iteration 104/1000 | Loss: 0.00002324
Iteration 105/1000 | Loss: 0.00002324
Iteration 106/1000 | Loss: 0.00002324
Iteration 107/1000 | Loss: 0.00002324
Iteration 108/1000 | Loss: 0.00002323
Iteration 109/1000 | Loss: 0.00002323
Iteration 110/1000 | Loss: 0.00002323
Iteration 111/1000 | Loss: 0.00002322
Iteration 112/1000 | Loss: 0.00002322
Iteration 113/1000 | Loss: 0.00002322
Iteration 114/1000 | Loss: 0.00002322
Iteration 115/1000 | Loss: 0.00002322
Iteration 116/1000 | Loss: 0.00002322
Iteration 117/1000 | Loss: 0.00002322
Iteration 118/1000 | Loss: 0.00002322
Iteration 119/1000 | Loss: 0.00002321
Iteration 120/1000 | Loss: 0.00002321
Iteration 121/1000 | Loss: 0.00002321
Iteration 122/1000 | Loss: 0.00002321
Iteration 123/1000 | Loss: 0.00002321
Iteration 124/1000 | Loss: 0.00002321
Iteration 125/1000 | Loss: 0.00002321
Iteration 126/1000 | Loss: 0.00002321
Iteration 127/1000 | Loss: 0.00002321
Iteration 128/1000 | Loss: 0.00002321
Iteration 129/1000 | Loss: 0.00002321
Iteration 130/1000 | Loss: 0.00002321
Iteration 131/1000 | Loss: 0.00002320
Iteration 132/1000 | Loss: 0.00002320
Iteration 133/1000 | Loss: 0.00002320
Iteration 134/1000 | Loss: 0.00002320
Iteration 135/1000 | Loss: 0.00002320
Iteration 136/1000 | Loss: 0.00002320
Iteration 137/1000 | Loss: 0.00002320
Iteration 138/1000 | Loss: 0.00002320
Iteration 139/1000 | Loss: 0.00002320
Iteration 140/1000 | Loss: 0.00002320
Iteration 141/1000 | Loss: 0.00002320
Iteration 142/1000 | Loss: 0.00002320
Iteration 143/1000 | Loss: 0.00002320
Iteration 144/1000 | Loss: 0.00002320
Iteration 145/1000 | Loss: 0.00002320
Iteration 146/1000 | Loss: 0.00002320
Iteration 147/1000 | Loss: 0.00002320
Iteration 148/1000 | Loss: 0.00002320
Iteration 149/1000 | Loss: 0.00002319
Iteration 150/1000 | Loss: 0.00002319
Iteration 151/1000 | Loss: 0.00002319
Iteration 152/1000 | Loss: 0.00002319
Iteration 153/1000 | Loss: 0.00002319
Iteration 154/1000 | Loss: 0.00002319
Iteration 155/1000 | Loss: 0.00002319
Iteration 156/1000 | Loss: 0.00002319
Iteration 157/1000 | Loss: 0.00002319
Iteration 158/1000 | Loss: 0.00002319
Iteration 159/1000 | Loss: 0.00002319
Iteration 160/1000 | Loss: 0.00002319
Iteration 161/1000 | Loss: 0.00002319
Iteration 162/1000 | Loss: 0.00002319
Iteration 163/1000 | Loss: 0.00002319
Iteration 164/1000 | Loss: 0.00002319
Iteration 165/1000 | Loss: 0.00002319
Iteration 166/1000 | Loss: 0.00002319
Iteration 167/1000 | Loss: 0.00002319
Iteration 168/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.3186090402305126e-05, 2.3186090402305126e-05, 2.3186090402305126e-05, 2.3186090402305126e-05, 2.3186090402305126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3186090402305126e-05

Optimization complete. Final v2v error: 4.050937652587891 mm

Highest mean error: 10.814970970153809 mm for frame 71

Lowest mean error: 3.7394187450408936 mm for frame 46

Saving results

Total time: 182.1629137992859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520526
Iteration 2/25 | Loss: 0.00180781
Iteration 3/25 | Loss: 0.00149697
Iteration 4/25 | Loss: 0.00146865
Iteration 5/25 | Loss: 0.00146300
Iteration 6/25 | Loss: 0.00146111
Iteration 7/25 | Loss: 0.00146087
Iteration 8/25 | Loss: 0.00146087
Iteration 9/25 | Loss: 0.00146087
Iteration 10/25 | Loss: 0.00146087
Iteration 11/25 | Loss: 0.00146087
Iteration 12/25 | Loss: 0.00146087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014608664205297828, 0.0014608664205297828, 0.0014608664205297828, 0.0014608664205297828, 0.0014608664205297828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014608664205297828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82158303
Iteration 2/25 | Loss: 0.00373035
Iteration 3/25 | Loss: 0.00373035
Iteration 4/25 | Loss: 0.00373035
Iteration 5/25 | Loss: 0.00373035
Iteration 6/25 | Loss: 0.00373035
Iteration 7/25 | Loss: 0.00373035
Iteration 8/25 | Loss: 0.00373035
Iteration 9/25 | Loss: 0.00373035
Iteration 10/25 | Loss: 0.00373035
Iteration 11/25 | Loss: 0.00373035
Iteration 12/25 | Loss: 0.00373035
Iteration 13/25 | Loss: 0.00373035
Iteration 14/25 | Loss: 0.00373035
Iteration 15/25 | Loss: 0.00373035
Iteration 16/25 | Loss: 0.00373035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0037303483113646507, 0.0037303483113646507, 0.0037303483113646507, 0.0037303483113646507, 0.0037303483113646507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037303483113646507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00373035
Iteration 2/1000 | Loss: 0.00008730
Iteration 3/1000 | Loss: 0.00005438
Iteration 4/1000 | Loss: 0.00004207
Iteration 5/1000 | Loss: 0.00003762
Iteration 6/1000 | Loss: 0.00003524
Iteration 7/1000 | Loss: 0.00003412
Iteration 8/1000 | Loss: 0.00003344
Iteration 9/1000 | Loss: 0.00003288
Iteration 10/1000 | Loss: 0.00003254
Iteration 11/1000 | Loss: 0.00003212
Iteration 12/1000 | Loss: 0.00003171
Iteration 13/1000 | Loss: 0.00003145
Iteration 14/1000 | Loss: 0.00003134
Iteration 15/1000 | Loss: 0.00003134
Iteration 16/1000 | Loss: 0.00003129
Iteration 17/1000 | Loss: 0.00003128
Iteration 18/1000 | Loss: 0.00003114
Iteration 19/1000 | Loss: 0.00003102
Iteration 20/1000 | Loss: 0.00003096
Iteration 21/1000 | Loss: 0.00003092
Iteration 22/1000 | Loss: 0.00003092
Iteration 23/1000 | Loss: 0.00003087
Iteration 24/1000 | Loss: 0.00003079
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003074
Iteration 27/1000 | Loss: 0.00003074
Iteration 28/1000 | Loss: 0.00003074
Iteration 29/1000 | Loss: 0.00003073
Iteration 30/1000 | Loss: 0.00003073
Iteration 31/1000 | Loss: 0.00003071
Iteration 32/1000 | Loss: 0.00003071
Iteration 33/1000 | Loss: 0.00003070
Iteration 34/1000 | Loss: 0.00003070
Iteration 35/1000 | Loss: 0.00003070
Iteration 36/1000 | Loss: 0.00003070
Iteration 37/1000 | Loss: 0.00003069
Iteration 38/1000 | Loss: 0.00003069
Iteration 39/1000 | Loss: 0.00003069
Iteration 40/1000 | Loss: 0.00003069
Iteration 41/1000 | Loss: 0.00003069
Iteration 42/1000 | Loss: 0.00003068
Iteration 43/1000 | Loss: 0.00003067
Iteration 44/1000 | Loss: 0.00003067
Iteration 45/1000 | Loss: 0.00003067
Iteration 46/1000 | Loss: 0.00003066
Iteration 47/1000 | Loss: 0.00003066
Iteration 48/1000 | Loss: 0.00003066
Iteration 49/1000 | Loss: 0.00003066
Iteration 50/1000 | Loss: 0.00003066
Iteration 51/1000 | Loss: 0.00003066
Iteration 52/1000 | Loss: 0.00003066
Iteration 53/1000 | Loss: 0.00003066
Iteration 54/1000 | Loss: 0.00003066
Iteration 55/1000 | Loss: 0.00003066
Iteration 56/1000 | Loss: 0.00003066
Iteration 57/1000 | Loss: 0.00003066
Iteration 58/1000 | Loss: 0.00003065
Iteration 59/1000 | Loss: 0.00003065
Iteration 60/1000 | Loss: 0.00003065
Iteration 61/1000 | Loss: 0.00003065
Iteration 62/1000 | Loss: 0.00003065
Iteration 63/1000 | Loss: 0.00003064
Iteration 64/1000 | Loss: 0.00003063
Iteration 65/1000 | Loss: 0.00003063
Iteration 66/1000 | Loss: 0.00003063
Iteration 67/1000 | Loss: 0.00003063
Iteration 68/1000 | Loss: 0.00003062
Iteration 69/1000 | Loss: 0.00003062
Iteration 70/1000 | Loss: 0.00003062
Iteration 71/1000 | Loss: 0.00003062
Iteration 72/1000 | Loss: 0.00003062
Iteration 73/1000 | Loss: 0.00003062
Iteration 74/1000 | Loss: 0.00003061
Iteration 75/1000 | Loss: 0.00003061
Iteration 76/1000 | Loss: 0.00003061
Iteration 77/1000 | Loss: 0.00003060
Iteration 78/1000 | Loss: 0.00003060
Iteration 79/1000 | Loss: 0.00003060
Iteration 80/1000 | Loss: 0.00003058
Iteration 81/1000 | Loss: 0.00003058
Iteration 82/1000 | Loss: 0.00003058
Iteration 83/1000 | Loss: 0.00003058
Iteration 84/1000 | Loss: 0.00003058
Iteration 85/1000 | Loss: 0.00003058
Iteration 86/1000 | Loss: 0.00003058
Iteration 87/1000 | Loss: 0.00003058
Iteration 88/1000 | Loss: 0.00003058
Iteration 89/1000 | Loss: 0.00003057
Iteration 90/1000 | Loss: 0.00003057
Iteration 91/1000 | Loss: 0.00003057
Iteration 92/1000 | Loss: 0.00003057
Iteration 93/1000 | Loss: 0.00003057
Iteration 94/1000 | Loss: 0.00003057
Iteration 95/1000 | Loss: 0.00003057
Iteration 96/1000 | Loss: 0.00003056
Iteration 97/1000 | Loss: 0.00003056
Iteration 98/1000 | Loss: 0.00003055
Iteration 99/1000 | Loss: 0.00003055
Iteration 100/1000 | Loss: 0.00003054
Iteration 101/1000 | Loss: 0.00003054
Iteration 102/1000 | Loss: 0.00003054
Iteration 103/1000 | Loss: 0.00003054
Iteration 104/1000 | Loss: 0.00003053
Iteration 105/1000 | Loss: 0.00003053
Iteration 106/1000 | Loss: 0.00003053
Iteration 107/1000 | Loss: 0.00003052
Iteration 108/1000 | Loss: 0.00003052
Iteration 109/1000 | Loss: 0.00003052
Iteration 110/1000 | Loss: 0.00003052
Iteration 111/1000 | Loss: 0.00003052
Iteration 112/1000 | Loss: 0.00003051
Iteration 113/1000 | Loss: 0.00003051
Iteration 114/1000 | Loss: 0.00003051
Iteration 115/1000 | Loss: 0.00003051
Iteration 116/1000 | Loss: 0.00003051
Iteration 117/1000 | Loss: 0.00003051
Iteration 118/1000 | Loss: 0.00003051
Iteration 119/1000 | Loss: 0.00003051
Iteration 120/1000 | Loss: 0.00003051
Iteration 121/1000 | Loss: 0.00003051
Iteration 122/1000 | Loss: 0.00003051
Iteration 123/1000 | Loss: 0.00003050
Iteration 124/1000 | Loss: 0.00003050
Iteration 125/1000 | Loss: 0.00003050
Iteration 126/1000 | Loss: 0.00003050
Iteration 127/1000 | Loss: 0.00003050
Iteration 128/1000 | Loss: 0.00003050
Iteration 129/1000 | Loss: 0.00003050
Iteration 130/1000 | Loss: 0.00003050
Iteration 131/1000 | Loss: 0.00003049
Iteration 132/1000 | Loss: 0.00003049
Iteration 133/1000 | Loss: 0.00003049
Iteration 134/1000 | Loss: 0.00003049
Iteration 135/1000 | Loss: 0.00003049
Iteration 136/1000 | Loss: 0.00003049
Iteration 137/1000 | Loss: 0.00003049
Iteration 138/1000 | Loss: 0.00003049
Iteration 139/1000 | Loss: 0.00003049
Iteration 140/1000 | Loss: 0.00003048
Iteration 141/1000 | Loss: 0.00003048
Iteration 142/1000 | Loss: 0.00003048
Iteration 143/1000 | Loss: 0.00003048
Iteration 144/1000 | Loss: 0.00003048
Iteration 145/1000 | Loss: 0.00003048
Iteration 146/1000 | Loss: 0.00003048
Iteration 147/1000 | Loss: 0.00003048
Iteration 148/1000 | Loss: 0.00003048
Iteration 149/1000 | Loss: 0.00003048
Iteration 150/1000 | Loss: 0.00003047
Iteration 151/1000 | Loss: 0.00003047
Iteration 152/1000 | Loss: 0.00003047
Iteration 153/1000 | Loss: 0.00003047
Iteration 154/1000 | Loss: 0.00003047
Iteration 155/1000 | Loss: 0.00003047
Iteration 156/1000 | Loss: 0.00003047
Iteration 157/1000 | Loss: 0.00003047
Iteration 158/1000 | Loss: 0.00003047
Iteration 159/1000 | Loss: 0.00003047
Iteration 160/1000 | Loss: 0.00003047
Iteration 161/1000 | Loss: 0.00003047
Iteration 162/1000 | Loss: 0.00003047
Iteration 163/1000 | Loss: 0.00003047
Iteration 164/1000 | Loss: 0.00003047
Iteration 165/1000 | Loss: 0.00003046
Iteration 166/1000 | Loss: 0.00003046
Iteration 167/1000 | Loss: 0.00003046
Iteration 168/1000 | Loss: 0.00003046
Iteration 169/1000 | Loss: 0.00003046
Iteration 170/1000 | Loss: 0.00003046
Iteration 171/1000 | Loss: 0.00003046
Iteration 172/1000 | Loss: 0.00003046
Iteration 173/1000 | Loss: 0.00003046
Iteration 174/1000 | Loss: 0.00003046
Iteration 175/1000 | Loss: 0.00003046
Iteration 176/1000 | Loss: 0.00003046
Iteration 177/1000 | Loss: 0.00003046
Iteration 178/1000 | Loss: 0.00003046
Iteration 179/1000 | Loss: 0.00003046
Iteration 180/1000 | Loss: 0.00003046
Iteration 181/1000 | Loss: 0.00003046
Iteration 182/1000 | Loss: 0.00003046
Iteration 183/1000 | Loss: 0.00003046
Iteration 184/1000 | Loss: 0.00003046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.046008168894332e-05, 3.046008168894332e-05, 3.046008168894332e-05, 3.046008168894332e-05, 3.046008168894332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.046008168894332e-05

Optimization complete. Final v2v error: 4.758432865142822 mm

Highest mean error: 5.329133033752441 mm for frame 21

Lowest mean error: 4.026834964752197 mm for frame 1

Saving results

Total time: 46.08982586860657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353950
Iteration 2/25 | Loss: 0.00173898
Iteration 3/25 | Loss: 0.00157663
Iteration 4/25 | Loss: 0.00154530
Iteration 5/25 | Loss: 0.00153780
Iteration 6/25 | Loss: 0.00153721
Iteration 7/25 | Loss: 0.00153721
Iteration 8/25 | Loss: 0.00153721
Iteration 9/25 | Loss: 0.00153721
Iteration 10/25 | Loss: 0.00153721
Iteration 11/25 | Loss: 0.00153721
Iteration 12/25 | Loss: 0.00153721
Iteration 13/25 | Loss: 0.00153721
Iteration 14/25 | Loss: 0.00153721
Iteration 15/25 | Loss: 0.00153721
Iteration 16/25 | Loss: 0.00153721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015372083289548755, 0.0015372083289548755, 0.0015372083289548755, 0.0015372083289548755, 0.0015372083289548755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015372083289548755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80713677
Iteration 2/25 | Loss: 0.00535404
Iteration 3/25 | Loss: 0.00535404
Iteration 4/25 | Loss: 0.00535404
Iteration 5/25 | Loss: 0.00535404
Iteration 6/25 | Loss: 0.00535403
Iteration 7/25 | Loss: 0.00535403
Iteration 8/25 | Loss: 0.00535403
Iteration 9/25 | Loss: 0.00535403
Iteration 10/25 | Loss: 0.00535403
Iteration 11/25 | Loss: 0.00535403
Iteration 12/25 | Loss: 0.00535403
Iteration 13/25 | Loss: 0.00535403
Iteration 14/25 | Loss: 0.00535403
Iteration 15/25 | Loss: 0.00535403
Iteration 16/25 | Loss: 0.00535403
Iteration 17/25 | Loss: 0.00535403
Iteration 18/25 | Loss: 0.00535403
Iteration 19/25 | Loss: 0.00535403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005354033317416906, 0.005354033317416906, 0.005354033317416906, 0.005354033317416906, 0.005354033317416906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005354033317416906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00535403
Iteration 2/1000 | Loss: 0.00009192
Iteration 3/1000 | Loss: 0.00006009
Iteration 4/1000 | Loss: 0.00004449
Iteration 5/1000 | Loss: 0.00003747
Iteration 6/1000 | Loss: 0.00003519
Iteration 7/1000 | Loss: 0.00003305
Iteration 8/1000 | Loss: 0.00003206
Iteration 9/1000 | Loss: 0.00003123
Iteration 10/1000 | Loss: 0.00003058
Iteration 11/1000 | Loss: 0.00003012
Iteration 12/1000 | Loss: 0.00002984
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002926
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002913
Iteration 17/1000 | Loss: 0.00002906
Iteration 18/1000 | Loss: 0.00002904
Iteration 19/1000 | Loss: 0.00002894
Iteration 20/1000 | Loss: 0.00002890
Iteration 21/1000 | Loss: 0.00002889
Iteration 22/1000 | Loss: 0.00002889
Iteration 23/1000 | Loss: 0.00002887
Iteration 24/1000 | Loss: 0.00002886
Iteration 25/1000 | Loss: 0.00002886
Iteration 26/1000 | Loss: 0.00002884
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00002878
Iteration 29/1000 | Loss: 0.00002875
Iteration 30/1000 | Loss: 0.00002875
Iteration 31/1000 | Loss: 0.00002875
Iteration 32/1000 | Loss: 0.00002875
Iteration 33/1000 | Loss: 0.00002875
Iteration 34/1000 | Loss: 0.00002875
Iteration 35/1000 | Loss: 0.00002875
Iteration 36/1000 | Loss: 0.00002874
Iteration 37/1000 | Loss: 0.00002874
Iteration 38/1000 | Loss: 0.00002874
Iteration 39/1000 | Loss: 0.00002874
Iteration 40/1000 | Loss: 0.00002874
Iteration 41/1000 | Loss: 0.00002874
Iteration 42/1000 | Loss: 0.00002874
Iteration 43/1000 | Loss: 0.00002874
Iteration 44/1000 | Loss: 0.00002874
Iteration 45/1000 | Loss: 0.00002873
Iteration 46/1000 | Loss: 0.00002873
Iteration 47/1000 | Loss: 0.00002873
Iteration 48/1000 | Loss: 0.00002873
Iteration 49/1000 | Loss: 0.00002873
Iteration 50/1000 | Loss: 0.00002873
Iteration 51/1000 | Loss: 0.00002873
Iteration 52/1000 | Loss: 0.00002873
Iteration 53/1000 | Loss: 0.00002873
Iteration 54/1000 | Loss: 0.00002873
Iteration 55/1000 | Loss: 0.00002873
Iteration 56/1000 | Loss: 0.00002873
Iteration 57/1000 | Loss: 0.00002873
Iteration 58/1000 | Loss: 0.00002873
Iteration 59/1000 | Loss: 0.00002872
Iteration 60/1000 | Loss: 0.00002872
Iteration 61/1000 | Loss: 0.00002872
Iteration 62/1000 | Loss: 0.00002872
Iteration 63/1000 | Loss: 0.00002872
Iteration 64/1000 | Loss: 0.00002872
Iteration 65/1000 | Loss: 0.00002872
Iteration 66/1000 | Loss: 0.00002871
Iteration 67/1000 | Loss: 0.00002871
Iteration 68/1000 | Loss: 0.00002871
Iteration 69/1000 | Loss: 0.00002871
Iteration 70/1000 | Loss: 0.00002871
Iteration 71/1000 | Loss: 0.00002871
Iteration 72/1000 | Loss: 0.00002871
Iteration 73/1000 | Loss: 0.00002871
Iteration 74/1000 | Loss: 0.00002871
Iteration 75/1000 | Loss: 0.00002871
Iteration 76/1000 | Loss: 0.00002871
Iteration 77/1000 | Loss: 0.00002871
Iteration 78/1000 | Loss: 0.00002871
Iteration 79/1000 | Loss: 0.00002871
Iteration 80/1000 | Loss: 0.00002871
Iteration 81/1000 | Loss: 0.00002871
Iteration 82/1000 | Loss: 0.00002871
Iteration 83/1000 | Loss: 0.00002871
Iteration 84/1000 | Loss: 0.00002871
Iteration 85/1000 | Loss: 0.00002871
Iteration 86/1000 | Loss: 0.00002871
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002871
Iteration 89/1000 | Loss: 0.00002871
Iteration 90/1000 | Loss: 0.00002871
Iteration 91/1000 | Loss: 0.00002871
Iteration 92/1000 | Loss: 0.00002871
Iteration 93/1000 | Loss: 0.00002871
Iteration 94/1000 | Loss: 0.00002871
Iteration 95/1000 | Loss: 0.00002871
Iteration 96/1000 | Loss: 0.00002871
Iteration 97/1000 | Loss: 0.00002871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.871359174605459e-05, 2.871359174605459e-05, 2.871359174605459e-05, 2.871359174605459e-05, 2.871359174605459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.871359174605459e-05

Optimization complete. Final v2v error: 4.720764636993408 mm

Highest mean error: 5.017043113708496 mm for frame 209

Lowest mean error: 4.514659404754639 mm for frame 5

Saving results

Total time: 42.588987588882446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164864
Iteration 2/25 | Loss: 0.00226841
Iteration 3/25 | Loss: 0.00174540
Iteration 4/25 | Loss: 0.00168760
Iteration 5/25 | Loss: 0.00167077
Iteration 6/25 | Loss: 0.00166513
Iteration 7/25 | Loss: 0.00166144
Iteration 8/25 | Loss: 0.00165702
Iteration 9/25 | Loss: 0.00164965
Iteration 10/25 | Loss: 0.00164719
Iteration 11/25 | Loss: 0.00164675
Iteration 12/25 | Loss: 0.00164669
Iteration 13/25 | Loss: 0.00164669
Iteration 14/25 | Loss: 0.00164669
Iteration 15/25 | Loss: 0.00164669
Iteration 16/25 | Loss: 0.00164669
Iteration 17/25 | Loss: 0.00164669
Iteration 18/25 | Loss: 0.00164668
Iteration 19/25 | Loss: 0.00164668
Iteration 20/25 | Loss: 0.00164668
Iteration 21/25 | Loss: 0.00164668
Iteration 22/25 | Loss: 0.00164668
Iteration 23/25 | Loss: 0.00164668
Iteration 24/25 | Loss: 0.00164668
Iteration 25/25 | Loss: 0.00164668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09886074
Iteration 2/25 | Loss: 0.00286719
Iteration 3/25 | Loss: 0.00286716
Iteration 4/25 | Loss: 0.00286716
Iteration 5/25 | Loss: 0.00286716
Iteration 6/25 | Loss: 0.00286716
Iteration 7/25 | Loss: 0.00286716
Iteration 8/25 | Loss: 0.00286716
Iteration 9/25 | Loss: 0.00286716
Iteration 10/25 | Loss: 0.00286716
Iteration 11/25 | Loss: 0.00286716
Iteration 12/25 | Loss: 0.00286716
Iteration 13/25 | Loss: 0.00286716
Iteration 14/25 | Loss: 0.00286716
Iteration 15/25 | Loss: 0.00286716
Iteration 16/25 | Loss: 0.00286716
Iteration 17/25 | Loss: 0.00286716
Iteration 18/25 | Loss: 0.00286716
Iteration 19/25 | Loss: 0.00286716
Iteration 20/25 | Loss: 0.00286716
Iteration 21/25 | Loss: 0.00286716
Iteration 22/25 | Loss: 0.00286716
Iteration 23/25 | Loss: 0.00286716
Iteration 24/25 | Loss: 0.00286716
Iteration 25/25 | Loss: 0.00286716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286716
Iteration 2/1000 | Loss: 0.00012907
Iteration 3/1000 | Loss: 0.00008571
Iteration 4/1000 | Loss: 0.00006436
Iteration 5/1000 | Loss: 0.00005951
Iteration 6/1000 | Loss: 0.00005588
Iteration 7/1000 | Loss: 0.00005387
Iteration 8/1000 | Loss: 0.00005291
Iteration 9/1000 | Loss: 0.00005207
Iteration 10/1000 | Loss: 0.00005151
Iteration 11/1000 | Loss: 0.00005108
Iteration 12/1000 | Loss: 0.00005074
Iteration 13/1000 | Loss: 0.00005055
Iteration 14/1000 | Loss: 0.00005041
Iteration 15/1000 | Loss: 0.00005034
Iteration 16/1000 | Loss: 0.00005029
Iteration 17/1000 | Loss: 0.00005029
Iteration 18/1000 | Loss: 0.00005016
Iteration 19/1000 | Loss: 0.00005002
Iteration 20/1000 | Loss: 0.00004992
Iteration 21/1000 | Loss: 0.00004989
Iteration 22/1000 | Loss: 0.00004983
Iteration 23/1000 | Loss: 0.00004981
Iteration 24/1000 | Loss: 0.00004980
Iteration 25/1000 | Loss: 0.00004977
Iteration 26/1000 | Loss: 0.00004977
Iteration 27/1000 | Loss: 0.00004972
Iteration 28/1000 | Loss: 0.00004972
Iteration 29/1000 | Loss: 0.00004970
Iteration 30/1000 | Loss: 0.00004969
Iteration 31/1000 | Loss: 0.00004969
Iteration 32/1000 | Loss: 0.00004963
Iteration 33/1000 | Loss: 0.00004960
Iteration 34/1000 | Loss: 0.00004960
Iteration 35/1000 | Loss: 0.00004958
Iteration 36/1000 | Loss: 0.00004958
Iteration 37/1000 | Loss: 0.00004958
Iteration 38/1000 | Loss: 0.00004957
Iteration 39/1000 | Loss: 0.00004956
Iteration 40/1000 | Loss: 0.00004956
Iteration 41/1000 | Loss: 0.00004955
Iteration 42/1000 | Loss: 0.00004955
Iteration 43/1000 | Loss: 0.00004954
Iteration 44/1000 | Loss: 0.00004952
Iteration 45/1000 | Loss: 0.00004952
Iteration 46/1000 | Loss: 0.00004951
Iteration 47/1000 | Loss: 0.00004951
Iteration 48/1000 | Loss: 0.00004950
Iteration 49/1000 | Loss: 0.00004950
Iteration 50/1000 | Loss: 0.00004950
Iteration 51/1000 | Loss: 0.00004949
Iteration 52/1000 | Loss: 0.00004949
Iteration 53/1000 | Loss: 0.00004948
Iteration 54/1000 | Loss: 0.00004948
Iteration 55/1000 | Loss: 0.00004948
Iteration 56/1000 | Loss: 0.00004947
Iteration 57/1000 | Loss: 0.00004947
Iteration 58/1000 | Loss: 0.00004946
Iteration 59/1000 | Loss: 0.00004946
Iteration 60/1000 | Loss: 0.00004946
Iteration 61/1000 | Loss: 0.00004946
Iteration 62/1000 | Loss: 0.00004946
Iteration 63/1000 | Loss: 0.00004946
Iteration 64/1000 | Loss: 0.00004946
Iteration 65/1000 | Loss: 0.00004946
Iteration 66/1000 | Loss: 0.00004946
Iteration 67/1000 | Loss: 0.00004946
Iteration 68/1000 | Loss: 0.00004946
Iteration 69/1000 | Loss: 0.00004946
Iteration 70/1000 | Loss: 0.00004945
Iteration 71/1000 | Loss: 0.00004945
Iteration 72/1000 | Loss: 0.00004945
Iteration 73/1000 | Loss: 0.00004945
Iteration 74/1000 | Loss: 0.00004944
Iteration 75/1000 | Loss: 0.00004944
Iteration 76/1000 | Loss: 0.00004944
Iteration 77/1000 | Loss: 0.00004944
Iteration 78/1000 | Loss: 0.00004944
Iteration 79/1000 | Loss: 0.00004944
Iteration 80/1000 | Loss: 0.00004944
Iteration 81/1000 | Loss: 0.00004944
Iteration 82/1000 | Loss: 0.00004944
Iteration 83/1000 | Loss: 0.00004943
Iteration 84/1000 | Loss: 0.00004943
Iteration 85/1000 | Loss: 0.00004943
Iteration 86/1000 | Loss: 0.00004943
Iteration 87/1000 | Loss: 0.00004943
Iteration 88/1000 | Loss: 0.00004943
Iteration 89/1000 | Loss: 0.00004943
Iteration 90/1000 | Loss: 0.00004943
Iteration 91/1000 | Loss: 0.00004943
Iteration 92/1000 | Loss: 0.00004943
Iteration 93/1000 | Loss: 0.00004943
Iteration 94/1000 | Loss: 0.00004943
Iteration 95/1000 | Loss: 0.00004943
Iteration 96/1000 | Loss: 0.00004943
Iteration 97/1000 | Loss: 0.00004943
Iteration 98/1000 | Loss: 0.00004943
Iteration 99/1000 | Loss: 0.00004943
Iteration 100/1000 | Loss: 0.00004943
Iteration 101/1000 | Loss: 0.00004943
Iteration 102/1000 | Loss: 0.00004943
Iteration 103/1000 | Loss: 0.00004943
Iteration 104/1000 | Loss: 0.00004943
Iteration 105/1000 | Loss: 0.00004943
Iteration 106/1000 | Loss: 0.00004943
Iteration 107/1000 | Loss: 0.00004943
Iteration 108/1000 | Loss: 0.00004943
Iteration 109/1000 | Loss: 0.00004943
Iteration 110/1000 | Loss: 0.00004943
Iteration 111/1000 | Loss: 0.00004943
Iteration 112/1000 | Loss: 0.00004943
Iteration 113/1000 | Loss: 0.00004943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [4.943093881593086e-05, 4.943093881593086e-05, 4.943093881593086e-05, 4.943093881593086e-05, 4.943093881593086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.943093881593086e-05

Optimization complete. Final v2v error: 5.767370700836182 mm

Highest mean error: 6.746028900146484 mm for frame 67

Lowest mean error: 5.0935893058776855 mm for frame 0

Saving results

Total time: 59.93292045593262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922718
Iteration 2/25 | Loss: 0.00163439
Iteration 3/25 | Loss: 0.00154953
Iteration 4/25 | Loss: 0.00153062
Iteration 5/25 | Loss: 0.00152535
Iteration 6/25 | Loss: 0.00152426
Iteration 7/25 | Loss: 0.00152426
Iteration 8/25 | Loss: 0.00152426
Iteration 9/25 | Loss: 0.00152426
Iteration 10/25 | Loss: 0.00152426
Iteration 11/25 | Loss: 0.00152426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015242600347846746, 0.0015242600347846746, 0.0015242600347846746, 0.0015242600347846746, 0.0015242600347846746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015242600347846746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.80722952
Iteration 2/25 | Loss: 0.00408260
Iteration 3/25 | Loss: 0.00408257
Iteration 4/25 | Loss: 0.00408257
Iteration 5/25 | Loss: 0.00408257
Iteration 6/25 | Loss: 0.00408257
Iteration 7/25 | Loss: 0.00408257
Iteration 8/25 | Loss: 0.00408257
Iteration 9/25 | Loss: 0.00408257
Iteration 10/25 | Loss: 0.00408257
Iteration 11/25 | Loss: 0.00408257
Iteration 12/25 | Loss: 0.00408257
Iteration 13/25 | Loss: 0.00408257
Iteration 14/25 | Loss: 0.00408257
Iteration 15/25 | Loss: 0.00408257
Iteration 16/25 | Loss: 0.00408257
Iteration 17/25 | Loss: 0.00408257
Iteration 18/25 | Loss: 0.00408257
Iteration 19/25 | Loss: 0.00408257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004082568921148777, 0.004082568921148777, 0.004082568921148777, 0.004082568921148777, 0.004082568921148777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004082568921148777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00408257
Iteration 2/1000 | Loss: 0.00005730
Iteration 3/1000 | Loss: 0.00004277
Iteration 4/1000 | Loss: 0.00003691
Iteration 5/1000 | Loss: 0.00003415
Iteration 6/1000 | Loss: 0.00003273
Iteration 7/1000 | Loss: 0.00003157
Iteration 8/1000 | Loss: 0.00003080
Iteration 9/1000 | Loss: 0.00003030
Iteration 10/1000 | Loss: 0.00002983
Iteration 11/1000 | Loss: 0.00002963
Iteration 12/1000 | Loss: 0.00002959
Iteration 13/1000 | Loss: 0.00002955
Iteration 14/1000 | Loss: 0.00002951
Iteration 15/1000 | Loss: 0.00002951
Iteration 16/1000 | Loss: 0.00002949
Iteration 17/1000 | Loss: 0.00002948
Iteration 18/1000 | Loss: 0.00002948
Iteration 19/1000 | Loss: 0.00002948
Iteration 20/1000 | Loss: 0.00002948
Iteration 21/1000 | Loss: 0.00002947
Iteration 22/1000 | Loss: 0.00002947
Iteration 23/1000 | Loss: 0.00002946
Iteration 24/1000 | Loss: 0.00002943
Iteration 25/1000 | Loss: 0.00002943
Iteration 26/1000 | Loss: 0.00002943
Iteration 27/1000 | Loss: 0.00002942
Iteration 28/1000 | Loss: 0.00002941
Iteration 29/1000 | Loss: 0.00002941
Iteration 30/1000 | Loss: 0.00002940
Iteration 31/1000 | Loss: 0.00002940
Iteration 32/1000 | Loss: 0.00002939
Iteration 33/1000 | Loss: 0.00002939
Iteration 34/1000 | Loss: 0.00002939
Iteration 35/1000 | Loss: 0.00002938
Iteration 36/1000 | Loss: 0.00002938
Iteration 37/1000 | Loss: 0.00002936
Iteration 38/1000 | Loss: 0.00002935
Iteration 39/1000 | Loss: 0.00002935
Iteration 40/1000 | Loss: 0.00002935
Iteration 41/1000 | Loss: 0.00002933
Iteration 42/1000 | Loss: 0.00002931
Iteration 43/1000 | Loss: 0.00002931
Iteration 44/1000 | Loss: 0.00002931
Iteration 45/1000 | Loss: 0.00002930
Iteration 46/1000 | Loss: 0.00002930
Iteration 47/1000 | Loss: 0.00002930
Iteration 48/1000 | Loss: 0.00002930
Iteration 49/1000 | Loss: 0.00002929
Iteration 50/1000 | Loss: 0.00002929
Iteration 51/1000 | Loss: 0.00002929
Iteration 52/1000 | Loss: 0.00002929
Iteration 53/1000 | Loss: 0.00002928
Iteration 54/1000 | Loss: 0.00002928
Iteration 55/1000 | Loss: 0.00002928
Iteration 56/1000 | Loss: 0.00002928
Iteration 57/1000 | Loss: 0.00002927
Iteration 58/1000 | Loss: 0.00002927
Iteration 59/1000 | Loss: 0.00002927
Iteration 60/1000 | Loss: 0.00002926
Iteration 61/1000 | Loss: 0.00002926
Iteration 62/1000 | Loss: 0.00002926
Iteration 63/1000 | Loss: 0.00002926
Iteration 64/1000 | Loss: 0.00002926
Iteration 65/1000 | Loss: 0.00002926
Iteration 66/1000 | Loss: 0.00002925
Iteration 67/1000 | Loss: 0.00002925
Iteration 68/1000 | Loss: 0.00002925
Iteration 69/1000 | Loss: 0.00002924
Iteration 70/1000 | Loss: 0.00002924
Iteration 71/1000 | Loss: 0.00002924
Iteration 72/1000 | Loss: 0.00002924
Iteration 73/1000 | Loss: 0.00002924
Iteration 74/1000 | Loss: 0.00002924
Iteration 75/1000 | Loss: 0.00002923
Iteration 76/1000 | Loss: 0.00002923
Iteration 77/1000 | Loss: 0.00002923
Iteration 78/1000 | Loss: 0.00002923
Iteration 79/1000 | Loss: 0.00002922
Iteration 80/1000 | Loss: 0.00002922
Iteration 81/1000 | Loss: 0.00002922
Iteration 82/1000 | Loss: 0.00002922
Iteration 83/1000 | Loss: 0.00002922
Iteration 84/1000 | Loss: 0.00002922
Iteration 85/1000 | Loss: 0.00002922
Iteration 86/1000 | Loss: 0.00002922
Iteration 87/1000 | Loss: 0.00002921
Iteration 88/1000 | Loss: 0.00002921
Iteration 89/1000 | Loss: 0.00002921
Iteration 90/1000 | Loss: 0.00002921
Iteration 91/1000 | Loss: 0.00002921
Iteration 92/1000 | Loss: 0.00002921
Iteration 93/1000 | Loss: 0.00002921
Iteration 94/1000 | Loss: 0.00002921
Iteration 95/1000 | Loss: 0.00002921
Iteration 96/1000 | Loss: 0.00002921
Iteration 97/1000 | Loss: 0.00002921
Iteration 98/1000 | Loss: 0.00002921
Iteration 99/1000 | Loss: 0.00002921
Iteration 100/1000 | Loss: 0.00002921
Iteration 101/1000 | Loss: 0.00002921
Iteration 102/1000 | Loss: 0.00002920
Iteration 103/1000 | Loss: 0.00002920
Iteration 104/1000 | Loss: 0.00002920
Iteration 105/1000 | Loss: 0.00002920
Iteration 106/1000 | Loss: 0.00002920
Iteration 107/1000 | Loss: 0.00002920
Iteration 108/1000 | Loss: 0.00002919
Iteration 109/1000 | Loss: 0.00002919
Iteration 110/1000 | Loss: 0.00002919
Iteration 111/1000 | Loss: 0.00002919
Iteration 112/1000 | Loss: 0.00002919
Iteration 113/1000 | Loss: 0.00002919
Iteration 114/1000 | Loss: 0.00002919
Iteration 115/1000 | Loss: 0.00002919
Iteration 116/1000 | Loss: 0.00002919
Iteration 117/1000 | Loss: 0.00002919
Iteration 118/1000 | Loss: 0.00002919
Iteration 119/1000 | Loss: 0.00002919
Iteration 120/1000 | Loss: 0.00002919
Iteration 121/1000 | Loss: 0.00002919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.918981044786051e-05, 2.918981044786051e-05, 2.918981044786051e-05, 2.918981044786051e-05, 2.918981044786051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.918981044786051e-05

Optimization complete. Final v2v error: 4.651237487792969 mm

Highest mean error: 5.401325225830078 mm for frame 146

Lowest mean error: 4.153976917266846 mm for frame 40

Saving results

Total time: 34.78935623168945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491845
Iteration 2/25 | Loss: 0.00160704
Iteration 3/25 | Loss: 0.00145664
Iteration 4/25 | Loss: 0.00143702
Iteration 5/25 | Loss: 0.00143246
Iteration 6/25 | Loss: 0.00143158
Iteration 7/25 | Loss: 0.00143158
Iteration 8/25 | Loss: 0.00143158
Iteration 9/25 | Loss: 0.00143158
Iteration 10/25 | Loss: 0.00143158
Iteration 11/25 | Loss: 0.00143158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014315842418000102, 0.0014315842418000102, 0.0014315842418000102, 0.0014315842418000102, 0.0014315842418000102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014315842418000102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71188259
Iteration 2/25 | Loss: 0.00392810
Iteration 3/25 | Loss: 0.00392810
Iteration 4/25 | Loss: 0.00392810
Iteration 5/25 | Loss: 0.00392810
Iteration 6/25 | Loss: 0.00392810
Iteration 7/25 | Loss: 0.00392810
Iteration 8/25 | Loss: 0.00392810
Iteration 9/25 | Loss: 0.00392810
Iteration 10/25 | Loss: 0.00392810
Iteration 11/25 | Loss: 0.00392810
Iteration 12/25 | Loss: 0.00392810
Iteration 13/25 | Loss: 0.00392810
Iteration 14/25 | Loss: 0.00392810
Iteration 15/25 | Loss: 0.00392810
Iteration 16/25 | Loss: 0.00392810
Iteration 17/25 | Loss: 0.00392810
Iteration 18/25 | Loss: 0.00392810
Iteration 19/25 | Loss: 0.00392810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003928098361939192, 0.003928098361939192, 0.003928098361939192, 0.003928098361939192, 0.003928098361939192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003928098361939192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00392810
Iteration 2/1000 | Loss: 0.00006638
Iteration 3/1000 | Loss: 0.00003876
Iteration 4/1000 | Loss: 0.00003325
Iteration 5/1000 | Loss: 0.00002958
Iteration 6/1000 | Loss: 0.00002830
Iteration 7/1000 | Loss: 0.00002758
Iteration 8/1000 | Loss: 0.00002722
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00002675
Iteration 11/1000 | Loss: 0.00002663
Iteration 12/1000 | Loss: 0.00002645
Iteration 13/1000 | Loss: 0.00002637
Iteration 14/1000 | Loss: 0.00002636
Iteration 15/1000 | Loss: 0.00002636
Iteration 16/1000 | Loss: 0.00002635
Iteration 17/1000 | Loss: 0.00002631
Iteration 18/1000 | Loss: 0.00002630
Iteration 19/1000 | Loss: 0.00002630
Iteration 20/1000 | Loss: 0.00002628
Iteration 21/1000 | Loss: 0.00002627
Iteration 22/1000 | Loss: 0.00002627
Iteration 23/1000 | Loss: 0.00002626
Iteration 24/1000 | Loss: 0.00002626
Iteration 25/1000 | Loss: 0.00002626
Iteration 26/1000 | Loss: 0.00002625
Iteration 27/1000 | Loss: 0.00002625
Iteration 28/1000 | Loss: 0.00002624
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002623
Iteration 31/1000 | Loss: 0.00002622
Iteration 32/1000 | Loss: 0.00002622
Iteration 33/1000 | Loss: 0.00002622
Iteration 34/1000 | Loss: 0.00002622
Iteration 35/1000 | Loss: 0.00002622
Iteration 36/1000 | Loss: 0.00002622
Iteration 37/1000 | Loss: 0.00002622
Iteration 38/1000 | Loss: 0.00002621
Iteration 39/1000 | Loss: 0.00002621
Iteration 40/1000 | Loss: 0.00002621
Iteration 41/1000 | Loss: 0.00002621
Iteration 42/1000 | Loss: 0.00002621
Iteration 43/1000 | Loss: 0.00002621
Iteration 44/1000 | Loss: 0.00002621
Iteration 45/1000 | Loss: 0.00002621
Iteration 46/1000 | Loss: 0.00002621
Iteration 47/1000 | Loss: 0.00002621
Iteration 48/1000 | Loss: 0.00002620
Iteration 49/1000 | Loss: 0.00002619
Iteration 50/1000 | Loss: 0.00002619
Iteration 51/1000 | Loss: 0.00002619
Iteration 52/1000 | Loss: 0.00002619
Iteration 53/1000 | Loss: 0.00002619
Iteration 54/1000 | Loss: 0.00002619
Iteration 55/1000 | Loss: 0.00002619
Iteration 56/1000 | Loss: 0.00002619
Iteration 57/1000 | Loss: 0.00002619
Iteration 58/1000 | Loss: 0.00002619
Iteration 59/1000 | Loss: 0.00002619
Iteration 60/1000 | Loss: 0.00002618
Iteration 61/1000 | Loss: 0.00002618
Iteration 62/1000 | Loss: 0.00002618
Iteration 63/1000 | Loss: 0.00002618
Iteration 64/1000 | Loss: 0.00002618
Iteration 65/1000 | Loss: 0.00002617
Iteration 66/1000 | Loss: 0.00002617
Iteration 67/1000 | Loss: 0.00002617
Iteration 68/1000 | Loss: 0.00002617
Iteration 69/1000 | Loss: 0.00002617
Iteration 70/1000 | Loss: 0.00002617
Iteration 71/1000 | Loss: 0.00002616
Iteration 72/1000 | Loss: 0.00002616
Iteration 73/1000 | Loss: 0.00002616
Iteration 74/1000 | Loss: 0.00002616
Iteration 75/1000 | Loss: 0.00002616
Iteration 76/1000 | Loss: 0.00002615
Iteration 77/1000 | Loss: 0.00002615
Iteration 78/1000 | Loss: 0.00002615
Iteration 79/1000 | Loss: 0.00002614
Iteration 80/1000 | Loss: 0.00002614
Iteration 81/1000 | Loss: 0.00002614
Iteration 82/1000 | Loss: 0.00002614
Iteration 83/1000 | Loss: 0.00002613
Iteration 84/1000 | Loss: 0.00002613
Iteration 85/1000 | Loss: 0.00002613
Iteration 86/1000 | Loss: 0.00002613
Iteration 87/1000 | Loss: 0.00002613
Iteration 88/1000 | Loss: 0.00002613
Iteration 89/1000 | Loss: 0.00002613
Iteration 90/1000 | Loss: 0.00002613
Iteration 91/1000 | Loss: 0.00002613
Iteration 92/1000 | Loss: 0.00002613
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002612
Iteration 95/1000 | Loss: 0.00002612
Iteration 96/1000 | Loss: 0.00002612
Iteration 97/1000 | Loss: 0.00002612
Iteration 98/1000 | Loss: 0.00002612
Iteration 99/1000 | Loss: 0.00002612
Iteration 100/1000 | Loss: 0.00002612
Iteration 101/1000 | Loss: 0.00002612
Iteration 102/1000 | Loss: 0.00002612
Iteration 103/1000 | Loss: 0.00002612
Iteration 104/1000 | Loss: 0.00002612
Iteration 105/1000 | Loss: 0.00002611
Iteration 106/1000 | Loss: 0.00002611
Iteration 107/1000 | Loss: 0.00002611
Iteration 108/1000 | Loss: 0.00002611
Iteration 109/1000 | Loss: 0.00002611
Iteration 110/1000 | Loss: 0.00002611
Iteration 111/1000 | Loss: 0.00002611
Iteration 112/1000 | Loss: 0.00002611
Iteration 113/1000 | Loss: 0.00002611
Iteration 114/1000 | Loss: 0.00002611
Iteration 115/1000 | Loss: 0.00002611
Iteration 116/1000 | Loss: 0.00002611
Iteration 117/1000 | Loss: 0.00002611
Iteration 118/1000 | Loss: 0.00002611
Iteration 119/1000 | Loss: 0.00002611
Iteration 120/1000 | Loss: 0.00002611
Iteration 121/1000 | Loss: 0.00002611
Iteration 122/1000 | Loss: 0.00002611
Iteration 123/1000 | Loss: 0.00002611
Iteration 124/1000 | Loss: 0.00002611
Iteration 125/1000 | Loss: 0.00002611
Iteration 126/1000 | Loss: 0.00002611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.6112045816262253e-05, 2.6112045816262253e-05, 2.6112045816262253e-05, 2.6112045816262253e-05, 2.6112045816262253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6112045816262253e-05

Optimization complete. Final v2v error: 4.384196758270264 mm

Highest mean error: 4.772096157073975 mm for frame 8

Lowest mean error: 4.14409875869751 mm for frame 137

Saving results

Total time: 34.322275161743164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01200225
Iteration 2/25 | Loss: 0.01200225
Iteration 3/25 | Loss: 0.01200225
Iteration 4/25 | Loss: 0.01200225
Iteration 5/25 | Loss: 0.01200225
Iteration 6/25 | Loss: 0.01200225
Iteration 7/25 | Loss: 0.01200224
Iteration 8/25 | Loss: 0.01200224
Iteration 9/25 | Loss: 0.01200224
Iteration 10/25 | Loss: 0.01200224
Iteration 11/25 | Loss: 0.01200224
Iteration 12/25 | Loss: 0.01200224
Iteration 13/25 | Loss: 0.01200224
Iteration 14/25 | Loss: 0.01200224
Iteration 15/25 | Loss: 0.01200224
Iteration 16/25 | Loss: 0.01200224
Iteration 17/25 | Loss: 0.01200224
Iteration 18/25 | Loss: 0.01200223
Iteration 19/25 | Loss: 0.01200223
Iteration 20/25 | Loss: 0.01200223
Iteration 21/25 | Loss: 0.01200223
Iteration 22/25 | Loss: 0.01200223
Iteration 23/25 | Loss: 0.01200223
Iteration 24/25 | Loss: 0.01200223
Iteration 25/25 | Loss: 0.01200223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94067621
Iteration 2/25 | Loss: 0.08570074
Iteration 3/25 | Loss: 0.08563969
Iteration 4/25 | Loss: 0.08564271
Iteration 5/25 | Loss: 0.08561673
Iteration 6/25 | Loss: 0.08561671
Iteration 7/25 | Loss: 0.08561670
Iteration 8/25 | Loss: 0.08561670
Iteration 9/25 | Loss: 0.08561670
Iteration 10/25 | Loss: 0.08561669
Iteration 11/25 | Loss: 0.08561669
Iteration 12/25 | Loss: 0.08561669
Iteration 13/25 | Loss: 0.08561669
Iteration 14/25 | Loss: 0.08561669
Iteration 15/25 | Loss: 0.08561669
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08561669290065765, 0.08561669290065765, 0.08561669290065765, 0.08561669290065765, 0.08561669290065765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08561669290065765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08561669
Iteration 2/1000 | Loss: 0.00091540
Iteration 3/1000 | Loss: 0.00019483
Iteration 4/1000 | Loss: 0.00008951
Iteration 5/1000 | Loss: 0.00011564
Iteration 6/1000 | Loss: 0.00007360
Iteration 7/1000 | Loss: 0.00009371
Iteration 8/1000 | Loss: 0.00004677
Iteration 9/1000 | Loss: 0.00008907
Iteration 10/1000 | Loss: 0.00003986
Iteration 11/1000 | Loss: 0.00003550
Iteration 12/1000 | Loss: 0.00005873
Iteration 13/1000 | Loss: 0.00003210
Iteration 14/1000 | Loss: 0.00003036
Iteration 15/1000 | Loss: 0.00004383
Iteration 16/1000 | Loss: 0.00003518
Iteration 17/1000 | Loss: 0.00002721
Iteration 18/1000 | Loss: 0.00002644
Iteration 19/1000 | Loss: 0.00002559
Iteration 20/1000 | Loss: 0.00002500
Iteration 21/1000 | Loss: 0.00002450
Iteration 22/1000 | Loss: 0.00002408
Iteration 23/1000 | Loss: 0.00006590
Iteration 24/1000 | Loss: 0.00002720
Iteration 25/1000 | Loss: 0.00003145
Iteration 26/1000 | Loss: 0.00002880
Iteration 27/1000 | Loss: 0.00002347
Iteration 28/1000 | Loss: 0.00002343
Iteration 29/1000 | Loss: 0.00002591
Iteration 30/1000 | Loss: 0.00002580
Iteration 31/1000 | Loss: 0.00002554
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002357
Iteration 35/1000 | Loss: 0.00002309
Iteration 36/1000 | Loss: 0.00002309
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002308
Iteration 41/1000 | Loss: 0.00002308
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00002308
Iteration 44/1000 | Loss: 0.00002308
Iteration 45/1000 | Loss: 0.00002308
Iteration 46/1000 | Loss: 0.00002308
Iteration 47/1000 | Loss: 0.00002308
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00002308
Iteration 50/1000 | Loss: 0.00002308
Iteration 51/1000 | Loss: 0.00002307
Iteration 52/1000 | Loss: 0.00002307
Iteration 53/1000 | Loss: 0.00002306
Iteration 54/1000 | Loss: 0.00002306
Iteration 55/1000 | Loss: 0.00002306
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002305
Iteration 59/1000 | Loss: 0.00002305
Iteration 60/1000 | Loss: 0.00002305
Iteration 61/1000 | Loss: 0.00002305
Iteration 62/1000 | Loss: 0.00002305
Iteration 63/1000 | Loss: 0.00002305
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002304
Iteration 68/1000 | Loss: 0.00002304
Iteration 69/1000 | Loss: 0.00002304
Iteration 70/1000 | Loss: 0.00002302
Iteration 71/1000 | Loss: 0.00002302
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002302
Iteration 76/1000 | Loss: 0.00002302
Iteration 77/1000 | Loss: 0.00002302
Iteration 78/1000 | Loss: 0.00002423
Iteration 79/1000 | Loss: 0.00002422
Iteration 80/1000 | Loss: 0.00002422
Iteration 81/1000 | Loss: 0.00002422
Iteration 82/1000 | Loss: 0.00002422
Iteration 83/1000 | Loss: 0.00002402
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00002341
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002362
Iteration 88/1000 | Loss: 0.00002389
Iteration 89/1000 | Loss: 0.00002350
Iteration 90/1000 | Loss: 0.00002346
Iteration 91/1000 | Loss: 0.00002333
Iteration 92/1000 | Loss: 0.00002351
Iteration 93/1000 | Loss: 0.00002346
Iteration 94/1000 | Loss: 0.00002345
Iteration 95/1000 | Loss: 0.00002345
Iteration 96/1000 | Loss: 0.00002370
Iteration 97/1000 | Loss: 0.00002333
Iteration 98/1000 | Loss: 0.00002360
Iteration 99/1000 | Loss: 0.00002336
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002364
Iteration 102/1000 | Loss: 0.00002375
Iteration 103/1000 | Loss: 0.00002364
Iteration 104/1000 | Loss: 0.00002370
Iteration 105/1000 | Loss: 0.00002386
Iteration 106/1000 | Loss: 0.00002348
Iteration 107/1000 | Loss: 0.00002367
Iteration 108/1000 | Loss: 0.00002351
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002354
Iteration 111/1000 | Loss: 0.00002367
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002363
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002370
Iteration 116/1000 | Loss: 0.00002359
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002358
Iteration 119/1000 | Loss: 0.00002376
Iteration 120/1000 | Loss: 0.00002368
Iteration 121/1000 | Loss: 0.00002373
Iteration 122/1000 | Loss: 0.00002373
Iteration 123/1000 | Loss: 0.00002371
Iteration 124/1000 | Loss: 0.00002301
Iteration 125/1000 | Loss: 0.00002360
Iteration 126/1000 | Loss: 0.00002368
Iteration 127/1000 | Loss: 0.00002300
Iteration 128/1000 | Loss: 0.00002299
Iteration 129/1000 | Loss: 0.00002298
Iteration 130/1000 | Loss: 0.00002358
Iteration 131/1000 | Loss: 0.00002365
Iteration 132/1000 | Loss: 0.00002319
Iteration 133/1000 | Loss: 0.00002304
Iteration 134/1000 | Loss: 0.00002300
Iteration 135/1000 | Loss: 0.00002298
Iteration 136/1000 | Loss: 0.00002297
Iteration 137/1000 | Loss: 0.00002297
Iteration 138/1000 | Loss: 0.00002297
Iteration 139/1000 | Loss: 0.00002297
Iteration 140/1000 | Loss: 0.00002297
Iteration 141/1000 | Loss: 0.00002296
Iteration 142/1000 | Loss: 0.00002296
Iteration 143/1000 | Loss: 0.00002296
Iteration 144/1000 | Loss: 0.00002296
Iteration 145/1000 | Loss: 0.00002295
Iteration 146/1000 | Loss: 0.00002295
Iteration 147/1000 | Loss: 0.00002295
Iteration 148/1000 | Loss: 0.00002295
Iteration 149/1000 | Loss: 0.00002294
Iteration 150/1000 | Loss: 0.00002294
Iteration 151/1000 | Loss: 0.00002294
Iteration 152/1000 | Loss: 0.00002294
Iteration 153/1000 | Loss: 0.00002294
Iteration 154/1000 | Loss: 0.00002294
Iteration 155/1000 | Loss: 0.00002293
Iteration 156/1000 | Loss: 0.00002293
Iteration 157/1000 | Loss: 0.00002293
Iteration 158/1000 | Loss: 0.00002293
Iteration 159/1000 | Loss: 0.00002293
Iteration 160/1000 | Loss: 0.00002293
Iteration 161/1000 | Loss: 0.00002293
Iteration 162/1000 | Loss: 0.00002293
Iteration 163/1000 | Loss: 0.00002293
Iteration 164/1000 | Loss: 0.00002293
Iteration 165/1000 | Loss: 0.00002293
Iteration 166/1000 | Loss: 0.00002293
Iteration 167/1000 | Loss: 0.00002293
Iteration 168/1000 | Loss: 0.00002293
Iteration 169/1000 | Loss: 0.00002293
Iteration 170/1000 | Loss: 0.00002293
Iteration 171/1000 | Loss: 0.00002293
Iteration 172/1000 | Loss: 0.00002293
Iteration 173/1000 | Loss: 0.00002293
Iteration 174/1000 | Loss: 0.00002293
Iteration 175/1000 | Loss: 0.00002293
Iteration 176/1000 | Loss: 0.00002292
Iteration 177/1000 | Loss: 0.00002292
Iteration 178/1000 | Loss: 0.00002292
Iteration 179/1000 | Loss: 0.00002292
Iteration 180/1000 | Loss: 0.00002292
Iteration 181/1000 | Loss: 0.00002292
Iteration 182/1000 | Loss: 0.00002292
Iteration 183/1000 | Loss: 0.00002292
Iteration 184/1000 | Loss: 0.00002292
Iteration 185/1000 | Loss: 0.00002292
Iteration 186/1000 | Loss: 0.00002292
Iteration 187/1000 | Loss: 0.00002292
Iteration 188/1000 | Loss: 0.00002292
Iteration 189/1000 | Loss: 0.00002292
Iteration 190/1000 | Loss: 0.00002292
Iteration 191/1000 | Loss: 0.00002292
Iteration 192/1000 | Loss: 0.00002292
Iteration 193/1000 | Loss: 0.00002292
Iteration 194/1000 | Loss: 0.00002292
Iteration 195/1000 | Loss: 0.00002292
Iteration 196/1000 | Loss: 0.00002292
Iteration 197/1000 | Loss: 0.00002291
Iteration 198/1000 | Loss: 0.00002291
Iteration 199/1000 | Loss: 0.00002291
Iteration 200/1000 | Loss: 0.00002291
Iteration 201/1000 | Loss: 0.00002291
Iteration 202/1000 | Loss: 0.00002291
Iteration 203/1000 | Loss: 0.00002291
Iteration 204/1000 | Loss: 0.00002291
Iteration 205/1000 | Loss: 0.00002291
Iteration 206/1000 | Loss: 0.00002291
Iteration 207/1000 | Loss: 0.00002291
Iteration 208/1000 | Loss: 0.00002291
Iteration 209/1000 | Loss: 0.00002291
Iteration 210/1000 | Loss: 0.00002291
Iteration 211/1000 | Loss: 0.00002291
Iteration 212/1000 | Loss: 0.00002291
Iteration 213/1000 | Loss: 0.00002291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.2912536223884672e-05, 2.2912536223884672e-05, 2.2912536223884672e-05, 2.2912536223884672e-05, 2.2912536223884672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2912536223884672e-05

Optimization complete. Final v2v error: 4.216707706451416 mm

Highest mean error: 11.14537525177002 mm for frame 66

Lowest mean error: 3.9590060710906982 mm for frame 98

Saving results

Total time: 123.82814478874207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478884
Iteration 2/25 | Loss: 0.00161308
Iteration 3/25 | Loss: 0.00153662
Iteration 4/25 | Loss: 0.00151314
Iteration 5/25 | Loss: 0.00150922
Iteration 6/25 | Loss: 0.00150881
Iteration 7/25 | Loss: 0.00150881
Iteration 8/25 | Loss: 0.00150881
Iteration 9/25 | Loss: 0.00150881
Iteration 10/25 | Loss: 0.00150881
Iteration 11/25 | Loss: 0.00150881
Iteration 12/25 | Loss: 0.00150881
Iteration 13/25 | Loss: 0.00150881
Iteration 14/25 | Loss: 0.00150881
Iteration 15/25 | Loss: 0.00150881
Iteration 16/25 | Loss: 0.00150881
Iteration 17/25 | Loss: 0.00150881
Iteration 18/25 | Loss: 0.00150881
Iteration 19/25 | Loss: 0.00150881
Iteration 20/25 | Loss: 0.00150881
Iteration 21/25 | Loss: 0.00150881
Iteration 22/25 | Loss: 0.00150881
Iteration 23/25 | Loss: 0.00150881
Iteration 24/25 | Loss: 0.00150881
Iteration 25/25 | Loss: 0.00150881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70288026
Iteration 2/25 | Loss: 0.00390993
Iteration 3/25 | Loss: 0.00390993
Iteration 4/25 | Loss: 0.00390993
Iteration 5/25 | Loss: 0.00390993
Iteration 6/25 | Loss: 0.00390992
Iteration 7/25 | Loss: 0.00390992
Iteration 8/25 | Loss: 0.00390992
Iteration 9/25 | Loss: 0.00390992
Iteration 10/25 | Loss: 0.00390992
Iteration 11/25 | Loss: 0.00390992
Iteration 12/25 | Loss: 0.00390992
Iteration 13/25 | Loss: 0.00390992
Iteration 14/25 | Loss: 0.00390992
Iteration 15/25 | Loss: 0.00390992
Iteration 16/25 | Loss: 0.00390992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003909923601895571, 0.003909923601895571, 0.003909923601895571, 0.003909923601895571, 0.003909923601895571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003909923601895571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390992
Iteration 2/1000 | Loss: 0.00006240
Iteration 3/1000 | Loss: 0.00003931
Iteration 4/1000 | Loss: 0.00003508
Iteration 5/1000 | Loss: 0.00003307
Iteration 6/1000 | Loss: 0.00003155
Iteration 7/1000 | Loss: 0.00003063
Iteration 8/1000 | Loss: 0.00003020
Iteration 9/1000 | Loss: 0.00003016
Iteration 10/1000 | Loss: 0.00002997
Iteration 11/1000 | Loss: 0.00002976
Iteration 12/1000 | Loss: 0.00002966
Iteration 13/1000 | Loss: 0.00002966
Iteration 14/1000 | Loss: 0.00002965
Iteration 15/1000 | Loss: 0.00002965
Iteration 16/1000 | Loss: 0.00002964
Iteration 17/1000 | Loss: 0.00002961
Iteration 18/1000 | Loss: 0.00002960
Iteration 19/1000 | Loss: 0.00002960
Iteration 20/1000 | Loss: 0.00002960
Iteration 21/1000 | Loss: 0.00002959
Iteration 22/1000 | Loss: 0.00002959
Iteration 23/1000 | Loss: 0.00002959
Iteration 24/1000 | Loss: 0.00002958
Iteration 25/1000 | Loss: 0.00002958
Iteration 26/1000 | Loss: 0.00002958
Iteration 27/1000 | Loss: 0.00002958
Iteration 28/1000 | Loss: 0.00002958
Iteration 29/1000 | Loss: 0.00002958
Iteration 30/1000 | Loss: 0.00002958
Iteration 31/1000 | Loss: 0.00002958
Iteration 32/1000 | Loss: 0.00002958
Iteration 33/1000 | Loss: 0.00002958
Iteration 34/1000 | Loss: 0.00002957
Iteration 35/1000 | Loss: 0.00002957
Iteration 36/1000 | Loss: 0.00002956
Iteration 37/1000 | Loss: 0.00002955
Iteration 38/1000 | Loss: 0.00002955
Iteration 39/1000 | Loss: 0.00002955
Iteration 40/1000 | Loss: 0.00002954
Iteration 41/1000 | Loss: 0.00002954
Iteration 42/1000 | Loss: 0.00002954
Iteration 43/1000 | Loss: 0.00002953
Iteration 44/1000 | Loss: 0.00002953
Iteration 45/1000 | Loss: 0.00002953
Iteration 46/1000 | Loss: 0.00002953
Iteration 47/1000 | Loss: 0.00002952
Iteration 48/1000 | Loss: 0.00002952
Iteration 49/1000 | Loss: 0.00002952
Iteration 50/1000 | Loss: 0.00002951
Iteration 51/1000 | Loss: 0.00002951
Iteration 52/1000 | Loss: 0.00002950
Iteration 53/1000 | Loss: 0.00002950
Iteration 54/1000 | Loss: 0.00002950
Iteration 55/1000 | Loss: 0.00002949
Iteration 56/1000 | Loss: 0.00002949
Iteration 57/1000 | Loss: 0.00002949
Iteration 58/1000 | Loss: 0.00002948
Iteration 59/1000 | Loss: 0.00002948
Iteration 60/1000 | Loss: 0.00002947
Iteration 61/1000 | Loss: 0.00002947
Iteration 62/1000 | Loss: 0.00002947
Iteration 63/1000 | Loss: 0.00002946
Iteration 64/1000 | Loss: 0.00002946
Iteration 65/1000 | Loss: 0.00002945
Iteration 66/1000 | Loss: 0.00002944
Iteration 67/1000 | Loss: 0.00002944
Iteration 68/1000 | Loss: 0.00002944
Iteration 69/1000 | Loss: 0.00002943
Iteration 70/1000 | Loss: 0.00002942
Iteration 71/1000 | Loss: 0.00002942
Iteration 72/1000 | Loss: 0.00002941
Iteration 73/1000 | Loss: 0.00002941
Iteration 74/1000 | Loss: 0.00002941
Iteration 75/1000 | Loss: 0.00002941
Iteration 76/1000 | Loss: 0.00002941
Iteration 77/1000 | Loss: 0.00002941
Iteration 78/1000 | Loss: 0.00002941
Iteration 79/1000 | Loss: 0.00002941
Iteration 80/1000 | Loss: 0.00002941
Iteration 81/1000 | Loss: 0.00002941
Iteration 82/1000 | Loss: 0.00002941
Iteration 83/1000 | Loss: 0.00002941
Iteration 84/1000 | Loss: 0.00002941
Iteration 85/1000 | Loss: 0.00002940
Iteration 86/1000 | Loss: 0.00002940
Iteration 87/1000 | Loss: 0.00002940
Iteration 88/1000 | Loss: 0.00002940
Iteration 89/1000 | Loss: 0.00002939
Iteration 90/1000 | Loss: 0.00002939
Iteration 91/1000 | Loss: 0.00002939
Iteration 92/1000 | Loss: 0.00002938
Iteration 93/1000 | Loss: 0.00002938
Iteration 94/1000 | Loss: 0.00002938
Iteration 95/1000 | Loss: 0.00002938
Iteration 96/1000 | Loss: 0.00002938
Iteration 97/1000 | Loss: 0.00002938
Iteration 98/1000 | Loss: 0.00002937
Iteration 99/1000 | Loss: 0.00002937
Iteration 100/1000 | Loss: 0.00002937
Iteration 101/1000 | Loss: 0.00002937
Iteration 102/1000 | Loss: 0.00002937
Iteration 103/1000 | Loss: 0.00002937
Iteration 104/1000 | Loss: 0.00002937
Iteration 105/1000 | Loss: 0.00002937
Iteration 106/1000 | Loss: 0.00002937
Iteration 107/1000 | Loss: 0.00002937
Iteration 108/1000 | Loss: 0.00002937
Iteration 109/1000 | Loss: 0.00002937
Iteration 110/1000 | Loss: 0.00002937
Iteration 111/1000 | Loss: 0.00002937
Iteration 112/1000 | Loss: 0.00002937
Iteration 113/1000 | Loss: 0.00002937
Iteration 114/1000 | Loss: 0.00002937
Iteration 115/1000 | Loss: 0.00002937
Iteration 116/1000 | Loss: 0.00002937
Iteration 117/1000 | Loss: 0.00002937
Iteration 118/1000 | Loss: 0.00002937
Iteration 119/1000 | Loss: 0.00002937
Iteration 120/1000 | Loss: 0.00002937
Iteration 121/1000 | Loss: 0.00002937
Iteration 122/1000 | Loss: 0.00002937
Iteration 123/1000 | Loss: 0.00002937
Iteration 124/1000 | Loss: 0.00002937
Iteration 125/1000 | Loss: 0.00002937
Iteration 126/1000 | Loss: 0.00002937
Iteration 127/1000 | Loss: 0.00002937
Iteration 128/1000 | Loss: 0.00002937
Iteration 129/1000 | Loss: 0.00002937
Iteration 130/1000 | Loss: 0.00002937
Iteration 131/1000 | Loss: 0.00002937
Iteration 132/1000 | Loss: 0.00002937
Iteration 133/1000 | Loss: 0.00002937
Iteration 134/1000 | Loss: 0.00002937
Iteration 135/1000 | Loss: 0.00002937
Iteration 136/1000 | Loss: 0.00002937
Iteration 137/1000 | Loss: 0.00002937
Iteration 138/1000 | Loss: 0.00002937
Iteration 139/1000 | Loss: 0.00002937
Iteration 140/1000 | Loss: 0.00002937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.9366241506068036e-05, 2.9366241506068036e-05, 2.9366241506068036e-05, 2.9366241506068036e-05, 2.9366241506068036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9366241506068036e-05

Optimization complete. Final v2v error: 4.7349724769592285 mm

Highest mean error: 4.995062828063965 mm for frame 110

Lowest mean error: 4.5956711769104 mm for frame 157

Saving results

Total time: 31.70929217338562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921793
Iteration 2/25 | Loss: 0.00161176
Iteration 3/25 | Loss: 0.00147124
Iteration 4/25 | Loss: 0.00145025
Iteration 5/25 | Loss: 0.00144708
Iteration 6/25 | Loss: 0.00144691
Iteration 7/25 | Loss: 0.00144691
Iteration 8/25 | Loss: 0.00144691
Iteration 9/25 | Loss: 0.00144691
Iteration 10/25 | Loss: 0.00144691
Iteration 11/25 | Loss: 0.00144691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014469072921201587, 0.0014469072921201587, 0.0014469072921201587, 0.0014469072921201587, 0.0014469072921201587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014469072921201587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74578190
Iteration 2/25 | Loss: 0.00430005
Iteration 3/25 | Loss: 0.00430005
Iteration 4/25 | Loss: 0.00430005
Iteration 5/25 | Loss: 0.00430005
Iteration 6/25 | Loss: 0.00430005
Iteration 7/25 | Loss: 0.00430005
Iteration 8/25 | Loss: 0.00430005
Iteration 9/25 | Loss: 0.00430005
Iteration 10/25 | Loss: 0.00430005
Iteration 11/25 | Loss: 0.00430005
Iteration 12/25 | Loss: 0.00430005
Iteration 13/25 | Loss: 0.00430005
Iteration 14/25 | Loss: 0.00430005
Iteration 15/25 | Loss: 0.00430005
Iteration 16/25 | Loss: 0.00430005
Iteration 17/25 | Loss: 0.00430005
Iteration 18/25 | Loss: 0.00430005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004300050437450409, 0.004300050437450409, 0.004300050437450409, 0.004300050437450409, 0.004300050437450409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004300050437450409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00430005
Iteration 2/1000 | Loss: 0.00005285
Iteration 3/1000 | Loss: 0.00003125
Iteration 4/1000 | Loss: 0.00002683
Iteration 5/1000 | Loss: 0.00002475
Iteration 6/1000 | Loss: 0.00002360
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002238
Iteration 9/1000 | Loss: 0.00002208
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002177
Iteration 12/1000 | Loss: 0.00002175
Iteration 13/1000 | Loss: 0.00002171
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002155
Iteration 16/1000 | Loss: 0.00002152
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00002141
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00002137
Iteration 23/1000 | Loss: 0.00002137
Iteration 24/1000 | Loss: 0.00002137
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002137
Iteration 27/1000 | Loss: 0.00002136
Iteration 28/1000 | Loss: 0.00002136
Iteration 29/1000 | Loss: 0.00002136
Iteration 30/1000 | Loss: 0.00002136
Iteration 31/1000 | Loss: 0.00002136
Iteration 32/1000 | Loss: 0.00002135
Iteration 33/1000 | Loss: 0.00002134
Iteration 34/1000 | Loss: 0.00002134
Iteration 35/1000 | Loss: 0.00002133
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002132
Iteration 38/1000 | Loss: 0.00002132
Iteration 39/1000 | Loss: 0.00002131
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002128
Iteration 44/1000 | Loss: 0.00002127
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002126
Iteration 47/1000 | Loss: 0.00002126
Iteration 48/1000 | Loss: 0.00002125
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00002125
Iteration 51/1000 | Loss: 0.00002125
Iteration 52/1000 | Loss: 0.00002125
Iteration 53/1000 | Loss: 0.00002124
Iteration 54/1000 | Loss: 0.00002124
Iteration 55/1000 | Loss: 0.00002124
Iteration 56/1000 | Loss: 0.00002124
Iteration 57/1000 | Loss: 0.00002124
Iteration 58/1000 | Loss: 0.00002124
Iteration 59/1000 | Loss: 0.00002124
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002123
Iteration 71/1000 | Loss: 0.00002123
Iteration 72/1000 | Loss: 0.00002123
Iteration 73/1000 | Loss: 0.00002123
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002123
Iteration 79/1000 | Loss: 0.00002123
Iteration 80/1000 | Loss: 0.00002123
Iteration 81/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.1234305677353404e-05, 2.1234305677353404e-05, 2.1234305677353404e-05, 2.1234305677353404e-05, 2.1234305677353404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1234305677353404e-05

Optimization complete. Final v2v error: 4.048666954040527 mm

Highest mean error: 4.201645374298096 mm for frame 112

Lowest mean error: 3.871314764022827 mm for frame 23

Saving results

Total time: 35.46101140975952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876276
Iteration 2/25 | Loss: 0.00198968
Iteration 3/25 | Loss: 0.00183655
Iteration 4/25 | Loss: 0.00172129
Iteration 5/25 | Loss: 0.00164980
Iteration 6/25 | Loss: 0.00164187
Iteration 7/25 | Loss: 0.00163919
Iteration 8/25 | Loss: 0.00163850
Iteration 9/25 | Loss: 0.00163831
Iteration 10/25 | Loss: 0.00163827
Iteration 11/25 | Loss: 0.00163826
Iteration 12/25 | Loss: 0.00163826
Iteration 13/25 | Loss: 0.00163826
Iteration 14/25 | Loss: 0.00163826
Iteration 15/25 | Loss: 0.00163826
Iteration 16/25 | Loss: 0.00163826
Iteration 17/25 | Loss: 0.00163826
Iteration 18/25 | Loss: 0.00163826
Iteration 19/25 | Loss: 0.00163826
Iteration 20/25 | Loss: 0.00163826
Iteration 21/25 | Loss: 0.00163826
Iteration 22/25 | Loss: 0.00163826
Iteration 23/25 | Loss: 0.00163826
Iteration 24/25 | Loss: 0.00163826
Iteration 25/25 | Loss: 0.00163826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17829275
Iteration 2/25 | Loss: 0.00515922
Iteration 3/25 | Loss: 0.00493118
Iteration 4/25 | Loss: 0.00493118
Iteration 5/25 | Loss: 0.00493118
Iteration 6/25 | Loss: 0.00493118
Iteration 7/25 | Loss: 0.00493118
Iteration 8/25 | Loss: 0.00493118
Iteration 9/25 | Loss: 0.00493118
Iteration 10/25 | Loss: 0.00493118
Iteration 11/25 | Loss: 0.00493118
Iteration 12/25 | Loss: 0.00493118
Iteration 13/25 | Loss: 0.00493118
Iteration 14/25 | Loss: 0.00493118
Iteration 15/25 | Loss: 0.00493118
Iteration 16/25 | Loss: 0.00493118
Iteration 17/25 | Loss: 0.00493118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004931181203573942, 0.004931181203573942, 0.004931181203573942, 0.004931181203573942, 0.004931181203573942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004931181203573942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00493118
Iteration 2/1000 | Loss: 0.00012909
Iteration 3/1000 | Loss: 0.00007690
Iteration 4/1000 | Loss: 0.00005512
Iteration 5/1000 | Loss: 0.00004744
Iteration 6/1000 | Loss: 0.00004273
Iteration 7/1000 | Loss: 0.00025820
Iteration 8/1000 | Loss: 0.00004014
Iteration 9/1000 | Loss: 0.00003931
Iteration 10/1000 | Loss: 0.00003839
Iteration 11/1000 | Loss: 0.00003774
Iteration 12/1000 | Loss: 0.00003744
Iteration 13/1000 | Loss: 0.00003707
Iteration 14/1000 | Loss: 0.00003681
Iteration 15/1000 | Loss: 0.00003679
Iteration 16/1000 | Loss: 0.00003671
Iteration 17/1000 | Loss: 0.00003665
Iteration 18/1000 | Loss: 0.00003663
Iteration 19/1000 | Loss: 0.00003660
Iteration 20/1000 | Loss: 0.00003659
Iteration 21/1000 | Loss: 0.00003656
Iteration 22/1000 | Loss: 0.00003653
Iteration 23/1000 | Loss: 0.00003650
Iteration 24/1000 | Loss: 0.00003650
Iteration 25/1000 | Loss: 0.00003649
Iteration 26/1000 | Loss: 0.00003647
Iteration 27/1000 | Loss: 0.00003647
Iteration 28/1000 | Loss: 0.00003644
Iteration 29/1000 | Loss: 0.00003644
Iteration 30/1000 | Loss: 0.00003643
Iteration 31/1000 | Loss: 0.00003643
Iteration 32/1000 | Loss: 0.00003642
Iteration 33/1000 | Loss: 0.00003642
Iteration 34/1000 | Loss: 0.00003642
Iteration 35/1000 | Loss: 0.00003642
Iteration 36/1000 | Loss: 0.00003641
Iteration 37/1000 | Loss: 0.00003641
Iteration 38/1000 | Loss: 0.00003639
Iteration 39/1000 | Loss: 0.00003639
Iteration 40/1000 | Loss: 0.00003638
Iteration 41/1000 | Loss: 0.00003637
Iteration 42/1000 | Loss: 0.00003637
Iteration 43/1000 | Loss: 0.00003637
Iteration 44/1000 | Loss: 0.00003636
Iteration 45/1000 | Loss: 0.00003636
Iteration 46/1000 | Loss: 0.00003635
Iteration 47/1000 | Loss: 0.00003635
Iteration 48/1000 | Loss: 0.00003634
Iteration 49/1000 | Loss: 0.00003630
Iteration 50/1000 | Loss: 0.00003630
Iteration 51/1000 | Loss: 0.00003629
Iteration 52/1000 | Loss: 0.00003629
Iteration 53/1000 | Loss: 0.00003628
Iteration 54/1000 | Loss: 0.00003626
Iteration 55/1000 | Loss: 0.00003625
Iteration 56/1000 | Loss: 0.00003625
Iteration 57/1000 | Loss: 0.00003624
Iteration 58/1000 | Loss: 0.00003624
Iteration 59/1000 | Loss: 0.00003623
Iteration 60/1000 | Loss: 0.00003623
Iteration 61/1000 | Loss: 0.00003622
Iteration 62/1000 | Loss: 0.00003622
Iteration 63/1000 | Loss: 0.00003622
Iteration 64/1000 | Loss: 0.00003622
Iteration 65/1000 | Loss: 0.00003621
Iteration 66/1000 | Loss: 0.00003620
Iteration 67/1000 | Loss: 0.00003620
Iteration 68/1000 | Loss: 0.00003619
Iteration 69/1000 | Loss: 0.00003619
Iteration 70/1000 | Loss: 0.00003619
Iteration 71/1000 | Loss: 0.00003619
Iteration 72/1000 | Loss: 0.00003618
Iteration 73/1000 | Loss: 0.00003618
Iteration 74/1000 | Loss: 0.00003618
Iteration 75/1000 | Loss: 0.00003617
Iteration 76/1000 | Loss: 0.00003617
Iteration 77/1000 | Loss: 0.00003617
Iteration 78/1000 | Loss: 0.00003616
Iteration 79/1000 | Loss: 0.00003616
Iteration 80/1000 | Loss: 0.00003616
Iteration 81/1000 | Loss: 0.00003615
Iteration 82/1000 | Loss: 0.00003614
Iteration 83/1000 | Loss: 0.00003613
Iteration 84/1000 | Loss: 0.00003612
Iteration 85/1000 | Loss: 0.00003612
Iteration 86/1000 | Loss: 0.00003612
Iteration 87/1000 | Loss: 0.00003612
Iteration 88/1000 | Loss: 0.00003612
Iteration 89/1000 | Loss: 0.00003612
Iteration 90/1000 | Loss: 0.00003612
Iteration 91/1000 | Loss: 0.00003612
Iteration 92/1000 | Loss: 0.00003611
Iteration 93/1000 | Loss: 0.00003611
Iteration 94/1000 | Loss: 0.00003608
Iteration 95/1000 | Loss: 0.00003607
Iteration 96/1000 | Loss: 0.00003607
Iteration 97/1000 | Loss: 0.00003607
Iteration 98/1000 | Loss: 0.00003606
Iteration 99/1000 | Loss: 0.00003606
Iteration 100/1000 | Loss: 0.00003606
Iteration 101/1000 | Loss: 0.00003605
Iteration 102/1000 | Loss: 0.00003605
Iteration 103/1000 | Loss: 0.00003605
Iteration 104/1000 | Loss: 0.00003605
Iteration 105/1000 | Loss: 0.00003605
Iteration 106/1000 | Loss: 0.00003604
Iteration 107/1000 | Loss: 0.00003604
Iteration 108/1000 | Loss: 0.00003604
Iteration 109/1000 | Loss: 0.00003604
Iteration 110/1000 | Loss: 0.00003604
Iteration 111/1000 | Loss: 0.00003604
Iteration 112/1000 | Loss: 0.00003603
Iteration 113/1000 | Loss: 0.00003603
Iteration 114/1000 | Loss: 0.00003603
Iteration 115/1000 | Loss: 0.00003603
Iteration 116/1000 | Loss: 0.00003603
Iteration 117/1000 | Loss: 0.00003603
Iteration 118/1000 | Loss: 0.00003603
Iteration 119/1000 | Loss: 0.00003603
Iteration 120/1000 | Loss: 0.00003603
Iteration 121/1000 | Loss: 0.00003603
Iteration 122/1000 | Loss: 0.00003602
Iteration 123/1000 | Loss: 0.00003602
Iteration 124/1000 | Loss: 0.00003602
Iteration 125/1000 | Loss: 0.00003602
Iteration 126/1000 | Loss: 0.00003602
Iteration 127/1000 | Loss: 0.00003602
Iteration 128/1000 | Loss: 0.00003602
Iteration 129/1000 | Loss: 0.00003602
Iteration 130/1000 | Loss: 0.00003602
Iteration 131/1000 | Loss: 0.00003602
Iteration 132/1000 | Loss: 0.00003602
Iteration 133/1000 | Loss: 0.00003601
Iteration 134/1000 | Loss: 0.00003601
Iteration 135/1000 | Loss: 0.00003601
Iteration 136/1000 | Loss: 0.00003601
Iteration 137/1000 | Loss: 0.00003600
Iteration 138/1000 | Loss: 0.00003600
Iteration 139/1000 | Loss: 0.00003600
Iteration 140/1000 | Loss: 0.00003600
Iteration 141/1000 | Loss: 0.00003600
Iteration 142/1000 | Loss: 0.00003600
Iteration 143/1000 | Loss: 0.00003600
Iteration 144/1000 | Loss: 0.00003600
Iteration 145/1000 | Loss: 0.00003600
Iteration 146/1000 | Loss: 0.00003600
Iteration 147/1000 | Loss: 0.00003599
Iteration 148/1000 | Loss: 0.00003599
Iteration 149/1000 | Loss: 0.00003599
Iteration 150/1000 | Loss: 0.00003599
Iteration 151/1000 | Loss: 0.00003599
Iteration 152/1000 | Loss: 0.00003599
Iteration 153/1000 | Loss: 0.00003599
Iteration 154/1000 | Loss: 0.00003599
Iteration 155/1000 | Loss: 0.00003599
Iteration 156/1000 | Loss: 0.00003599
Iteration 157/1000 | Loss: 0.00003598
Iteration 158/1000 | Loss: 0.00003598
Iteration 159/1000 | Loss: 0.00003598
Iteration 160/1000 | Loss: 0.00003598
Iteration 161/1000 | Loss: 0.00003598
Iteration 162/1000 | Loss: 0.00003598
Iteration 163/1000 | Loss: 0.00003598
Iteration 164/1000 | Loss: 0.00003598
Iteration 165/1000 | Loss: 0.00003598
Iteration 166/1000 | Loss: 0.00003598
Iteration 167/1000 | Loss: 0.00003598
Iteration 168/1000 | Loss: 0.00003597
Iteration 169/1000 | Loss: 0.00003597
Iteration 170/1000 | Loss: 0.00003597
Iteration 171/1000 | Loss: 0.00003597
Iteration 172/1000 | Loss: 0.00003596
Iteration 173/1000 | Loss: 0.00003596
Iteration 174/1000 | Loss: 0.00003596
Iteration 175/1000 | Loss: 0.00003596
Iteration 176/1000 | Loss: 0.00003596
Iteration 177/1000 | Loss: 0.00003595
Iteration 178/1000 | Loss: 0.00003595
Iteration 179/1000 | Loss: 0.00003595
Iteration 180/1000 | Loss: 0.00003595
Iteration 181/1000 | Loss: 0.00003595
Iteration 182/1000 | Loss: 0.00003595
Iteration 183/1000 | Loss: 0.00003595
Iteration 184/1000 | Loss: 0.00003595
Iteration 185/1000 | Loss: 0.00003595
Iteration 186/1000 | Loss: 0.00003594
Iteration 187/1000 | Loss: 0.00003594
Iteration 188/1000 | Loss: 0.00003594
Iteration 189/1000 | Loss: 0.00003594
Iteration 190/1000 | Loss: 0.00003594
Iteration 191/1000 | Loss: 0.00003593
Iteration 192/1000 | Loss: 0.00003593
Iteration 193/1000 | Loss: 0.00003593
Iteration 194/1000 | Loss: 0.00003593
Iteration 195/1000 | Loss: 0.00003593
Iteration 196/1000 | Loss: 0.00003593
Iteration 197/1000 | Loss: 0.00003593
Iteration 198/1000 | Loss: 0.00003593
Iteration 199/1000 | Loss: 0.00003593
Iteration 200/1000 | Loss: 0.00003593
Iteration 201/1000 | Loss: 0.00003593
Iteration 202/1000 | Loss: 0.00003593
Iteration 203/1000 | Loss: 0.00003593
Iteration 204/1000 | Loss: 0.00003593
Iteration 205/1000 | Loss: 0.00003593
Iteration 206/1000 | Loss: 0.00003592
Iteration 207/1000 | Loss: 0.00003592
Iteration 208/1000 | Loss: 0.00003592
Iteration 209/1000 | Loss: 0.00003592
Iteration 210/1000 | Loss: 0.00003592
Iteration 211/1000 | Loss: 0.00003592
Iteration 212/1000 | Loss: 0.00003592
Iteration 213/1000 | Loss: 0.00003592
Iteration 214/1000 | Loss: 0.00003592
Iteration 215/1000 | Loss: 0.00003592
Iteration 216/1000 | Loss: 0.00003592
Iteration 217/1000 | Loss: 0.00003592
Iteration 218/1000 | Loss: 0.00003591
Iteration 219/1000 | Loss: 0.00003591
Iteration 220/1000 | Loss: 0.00003591
Iteration 221/1000 | Loss: 0.00003591
Iteration 222/1000 | Loss: 0.00003591
Iteration 223/1000 | Loss: 0.00003591
Iteration 224/1000 | Loss: 0.00003591
Iteration 225/1000 | Loss: 0.00003591
Iteration 226/1000 | Loss: 0.00003591
Iteration 227/1000 | Loss: 0.00003591
Iteration 228/1000 | Loss: 0.00003591
Iteration 229/1000 | Loss: 0.00003591
Iteration 230/1000 | Loss: 0.00003591
Iteration 231/1000 | Loss: 0.00003591
Iteration 232/1000 | Loss: 0.00003591
Iteration 233/1000 | Loss: 0.00003591
Iteration 234/1000 | Loss: 0.00003591
Iteration 235/1000 | Loss: 0.00003591
Iteration 236/1000 | Loss: 0.00003591
Iteration 237/1000 | Loss: 0.00003591
Iteration 238/1000 | Loss: 0.00003591
Iteration 239/1000 | Loss: 0.00003591
Iteration 240/1000 | Loss: 0.00003591
Iteration 241/1000 | Loss: 0.00003591
Iteration 242/1000 | Loss: 0.00003591
Iteration 243/1000 | Loss: 0.00003591
Iteration 244/1000 | Loss: 0.00003591
Iteration 245/1000 | Loss: 0.00003591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [3.590566120692529e-05, 3.590566120692529e-05, 3.590566120692529e-05, 3.590566120692529e-05, 3.590566120692529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.590566120692529e-05

Optimization complete. Final v2v error: 5.113668441772461 mm

Highest mean error: 5.7027482986450195 mm for frame 4

Lowest mean error: 4.581724166870117 mm for frame 22

Saving results

Total time: 56.29199576377869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_1443/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_1443/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826278
Iteration 2/25 | Loss: 0.00191817
Iteration 3/25 | Loss: 0.00172542
Iteration 4/25 | Loss: 0.00168040
Iteration 5/25 | Loss: 0.00167242
Iteration 6/25 | Loss: 0.00166617
Iteration 7/25 | Loss: 0.00166475
Iteration 8/25 | Loss: 0.00166100
Iteration 9/25 | Loss: 0.00165808
Iteration 10/25 | Loss: 0.00165529
Iteration 11/25 | Loss: 0.00165648
Iteration 12/25 | Loss: 0.00165593
Iteration 13/25 | Loss: 0.00165723
Iteration 14/25 | Loss: 0.00165688
Iteration 15/25 | Loss: 0.00165589
Iteration 16/25 | Loss: 0.00165593
Iteration 17/25 | Loss: 0.00165629
Iteration 18/25 | Loss: 0.00165394
Iteration 19/25 | Loss: 0.00165655
Iteration 20/25 | Loss: 0.00165665
Iteration 21/25 | Loss: 0.00165196
Iteration 22/25 | Loss: 0.00165151
Iteration 23/25 | Loss: 0.00165138
Iteration 24/25 | Loss: 0.00165137
Iteration 25/25 | Loss: 0.00165137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61605740
Iteration 2/25 | Loss: 0.00481609
Iteration 3/25 | Loss: 0.00481605
Iteration 4/25 | Loss: 0.00481605
Iteration 5/25 | Loss: 0.00481605
Iteration 6/25 | Loss: 0.00481605
Iteration 7/25 | Loss: 0.00481605
Iteration 8/25 | Loss: 0.00481605
Iteration 9/25 | Loss: 0.00481605
Iteration 10/25 | Loss: 0.00481605
Iteration 11/25 | Loss: 0.00481605
Iteration 12/25 | Loss: 0.00481605
Iteration 13/25 | Loss: 0.00481605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004816046915948391, 0.004816046915948391, 0.004816046915948391, 0.004816046915948391, 0.004816046915948391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004816046915948391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00481605
Iteration 2/1000 | Loss: 0.00017459
Iteration 3/1000 | Loss: 0.00011361
Iteration 4/1000 | Loss: 0.00008605
Iteration 5/1000 | Loss: 0.00007536
Iteration 6/1000 | Loss: 0.00081977
Iteration 7/1000 | Loss: 0.00177517
Iteration 8/1000 | Loss: 0.00200370
Iteration 9/1000 | Loss: 0.00043787
Iteration 10/1000 | Loss: 0.00008659
Iteration 11/1000 | Loss: 0.00148944
Iteration 12/1000 | Loss: 0.00092037
Iteration 13/1000 | Loss: 0.00100652
Iteration 14/1000 | Loss: 0.00079211
Iteration 15/1000 | Loss: 0.00027268
Iteration 16/1000 | Loss: 0.00025593
Iteration 17/1000 | Loss: 0.00023951
Iteration 18/1000 | Loss: 0.00007246
Iteration 19/1000 | Loss: 0.00006338
Iteration 20/1000 | Loss: 0.00005810
Iteration 21/1000 | Loss: 0.00005610
Iteration 22/1000 | Loss: 0.00059112
Iteration 23/1000 | Loss: 0.00035668
Iteration 24/1000 | Loss: 0.00020674
Iteration 25/1000 | Loss: 0.00031479
Iteration 26/1000 | Loss: 0.00018683
Iteration 27/1000 | Loss: 0.00028644
Iteration 28/1000 | Loss: 0.00005984
Iteration 29/1000 | Loss: 0.00023154
Iteration 30/1000 | Loss: 0.00005133
Iteration 31/1000 | Loss: 0.00004847
Iteration 32/1000 | Loss: 0.00004661
Iteration 33/1000 | Loss: 0.00004550
Iteration 34/1000 | Loss: 0.00004476
Iteration 35/1000 | Loss: 0.00004439
Iteration 36/1000 | Loss: 0.00004407
Iteration 37/1000 | Loss: 0.00004373
Iteration 38/1000 | Loss: 0.00004352
Iteration 39/1000 | Loss: 0.00004346
Iteration 40/1000 | Loss: 0.00004337
Iteration 41/1000 | Loss: 0.00004321
Iteration 42/1000 | Loss: 0.00004311
Iteration 43/1000 | Loss: 0.00004309
Iteration 44/1000 | Loss: 0.00004306
Iteration 45/1000 | Loss: 0.00004305
Iteration 46/1000 | Loss: 0.00004305
Iteration 47/1000 | Loss: 0.00004300
Iteration 48/1000 | Loss: 0.00004300
Iteration 49/1000 | Loss: 0.00004300
Iteration 50/1000 | Loss: 0.00004300
Iteration 51/1000 | Loss: 0.00004300
Iteration 52/1000 | Loss: 0.00004300
Iteration 53/1000 | Loss: 0.00004300
Iteration 54/1000 | Loss: 0.00004300
Iteration 55/1000 | Loss: 0.00004300
Iteration 56/1000 | Loss: 0.00004300
Iteration 57/1000 | Loss: 0.00004299
Iteration 58/1000 | Loss: 0.00004299
Iteration 59/1000 | Loss: 0.00004299
Iteration 60/1000 | Loss: 0.00004299
Iteration 61/1000 | Loss: 0.00004298
Iteration 62/1000 | Loss: 0.00004298
Iteration 63/1000 | Loss: 0.00004297
Iteration 64/1000 | Loss: 0.00004297
Iteration 65/1000 | Loss: 0.00004297
Iteration 66/1000 | Loss: 0.00004297
Iteration 67/1000 | Loss: 0.00004296
Iteration 68/1000 | Loss: 0.00004296
Iteration 69/1000 | Loss: 0.00004296
Iteration 70/1000 | Loss: 0.00004296
Iteration 71/1000 | Loss: 0.00004296
Iteration 72/1000 | Loss: 0.00004296
Iteration 73/1000 | Loss: 0.00004295
Iteration 74/1000 | Loss: 0.00004295
Iteration 75/1000 | Loss: 0.00004295
Iteration 76/1000 | Loss: 0.00004295
Iteration 77/1000 | Loss: 0.00004295
Iteration 78/1000 | Loss: 0.00004294
Iteration 79/1000 | Loss: 0.00004293
Iteration 80/1000 | Loss: 0.00004293
Iteration 81/1000 | Loss: 0.00004293
Iteration 82/1000 | Loss: 0.00004293
Iteration 83/1000 | Loss: 0.00004293
Iteration 84/1000 | Loss: 0.00004293
Iteration 85/1000 | Loss: 0.00004293
Iteration 86/1000 | Loss: 0.00004292
Iteration 87/1000 | Loss: 0.00004292
Iteration 88/1000 | Loss: 0.00004292
Iteration 89/1000 | Loss: 0.00004292
Iteration 90/1000 | Loss: 0.00004292
Iteration 91/1000 | Loss: 0.00004292
Iteration 92/1000 | Loss: 0.00004292
Iteration 93/1000 | Loss: 0.00004292
Iteration 94/1000 | Loss: 0.00004292
Iteration 95/1000 | Loss: 0.00004291
Iteration 96/1000 | Loss: 0.00004291
Iteration 97/1000 | Loss: 0.00004291
Iteration 98/1000 | Loss: 0.00004291
Iteration 99/1000 | Loss: 0.00004291
Iteration 100/1000 | Loss: 0.00004291
Iteration 101/1000 | Loss: 0.00004291
Iteration 102/1000 | Loss: 0.00004291
Iteration 103/1000 | Loss: 0.00004291
Iteration 104/1000 | Loss: 0.00004290
Iteration 105/1000 | Loss: 0.00004290
Iteration 106/1000 | Loss: 0.00004290
Iteration 107/1000 | Loss: 0.00004290
Iteration 108/1000 | Loss: 0.00004290
Iteration 109/1000 | Loss: 0.00004290
Iteration 110/1000 | Loss: 0.00004290
Iteration 111/1000 | Loss: 0.00004290
Iteration 112/1000 | Loss: 0.00004290
Iteration 113/1000 | Loss: 0.00004289
Iteration 114/1000 | Loss: 0.00004289
Iteration 115/1000 | Loss: 0.00004289
Iteration 116/1000 | Loss: 0.00004288
Iteration 117/1000 | Loss: 0.00004288
Iteration 118/1000 | Loss: 0.00004288
Iteration 119/1000 | Loss: 0.00004288
Iteration 120/1000 | Loss: 0.00004288
Iteration 121/1000 | Loss: 0.00004288
Iteration 122/1000 | Loss: 0.00004288
Iteration 123/1000 | Loss: 0.00004287
Iteration 124/1000 | Loss: 0.00004287
Iteration 125/1000 | Loss: 0.00004287
Iteration 126/1000 | Loss: 0.00004287
Iteration 127/1000 | Loss: 0.00004287
Iteration 128/1000 | Loss: 0.00004287
Iteration 129/1000 | Loss: 0.00004287
Iteration 130/1000 | Loss: 0.00004287
Iteration 131/1000 | Loss: 0.00004287
Iteration 132/1000 | Loss: 0.00004287
Iteration 133/1000 | Loss: 0.00004287
Iteration 134/1000 | Loss: 0.00004287
Iteration 135/1000 | Loss: 0.00004287
Iteration 136/1000 | Loss: 0.00004287
Iteration 137/1000 | Loss: 0.00004287
Iteration 138/1000 | Loss: 0.00004286
Iteration 139/1000 | Loss: 0.00004286
Iteration 140/1000 | Loss: 0.00004286
Iteration 141/1000 | Loss: 0.00004286
Iteration 142/1000 | Loss: 0.00004286
Iteration 143/1000 | Loss: 0.00004286
Iteration 144/1000 | Loss: 0.00004286
Iteration 145/1000 | Loss: 0.00004286
Iteration 146/1000 | Loss: 0.00004286
Iteration 147/1000 | Loss: 0.00004286
Iteration 148/1000 | Loss: 0.00004286
Iteration 149/1000 | Loss: 0.00004286
Iteration 150/1000 | Loss: 0.00004286
Iteration 151/1000 | Loss: 0.00004286
Iteration 152/1000 | Loss: 0.00004286
Iteration 153/1000 | Loss: 0.00004286
Iteration 154/1000 | Loss: 0.00004286
Iteration 155/1000 | Loss: 0.00004286
Iteration 156/1000 | Loss: 0.00004286
Iteration 157/1000 | Loss: 0.00004286
Iteration 158/1000 | Loss: 0.00004286
Iteration 159/1000 | Loss: 0.00004286
Iteration 160/1000 | Loss: 0.00004286
Iteration 161/1000 | Loss: 0.00004286
Iteration 162/1000 | Loss: 0.00004286
Iteration 163/1000 | Loss: 0.00004286
Iteration 164/1000 | Loss: 0.00004286
Iteration 165/1000 | Loss: 0.00004286
Iteration 166/1000 | Loss: 0.00004286
Iteration 167/1000 | Loss: 0.00004286
Iteration 168/1000 | Loss: 0.00004286
Iteration 169/1000 | Loss: 0.00004286
Iteration 170/1000 | Loss: 0.00004286
Iteration 171/1000 | Loss: 0.00004286
Iteration 172/1000 | Loss: 0.00004286
Iteration 173/1000 | Loss: 0.00004286
Iteration 174/1000 | Loss: 0.00004286
Iteration 175/1000 | Loss: 0.00004286
Iteration 176/1000 | Loss: 0.00004286
Iteration 177/1000 | Loss: 0.00004286
Iteration 178/1000 | Loss: 0.00004286
Iteration 179/1000 | Loss: 0.00004286
Iteration 180/1000 | Loss: 0.00004286
Iteration 181/1000 | Loss: 0.00004286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [4.286158218747005e-05, 4.286158218747005e-05, 4.286158218747005e-05, 4.286158218747005e-05, 4.286158218747005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.286158218747005e-05

Optimization complete. Final v2v error: 4.978832721710205 mm

Highest mean error: 13.690885543823242 mm for frame 143

Lowest mean error: 4.318663120269775 mm for frame 34

Saving results

Total time: 121.31399512290955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730965
Iteration 2/25 | Loss: 0.00172820
Iteration 3/25 | Loss: 0.00145974
Iteration 4/25 | Loss: 0.00144456
Iteration 5/25 | Loss: 0.00147264
Iteration 6/25 | Loss: 0.00143892
Iteration 7/25 | Loss: 0.00143325
Iteration 8/25 | Loss: 0.00142563
Iteration 9/25 | Loss: 0.00142569
Iteration 10/25 | Loss: 0.00143185
Iteration 11/25 | Loss: 0.00142320
Iteration 12/25 | Loss: 0.00141534
Iteration 13/25 | Loss: 0.00141327
Iteration 14/25 | Loss: 0.00141295
Iteration 15/25 | Loss: 0.00141285
Iteration 16/25 | Loss: 0.00141285
Iteration 17/25 | Loss: 0.00141285
Iteration 18/25 | Loss: 0.00141285
Iteration 19/25 | Loss: 0.00141285
Iteration 20/25 | Loss: 0.00141284
Iteration 21/25 | Loss: 0.00141284
Iteration 22/25 | Loss: 0.00141284
Iteration 23/25 | Loss: 0.00141284
Iteration 24/25 | Loss: 0.00141284
Iteration 25/25 | Loss: 0.00141284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71728563
Iteration 2/25 | Loss: 0.00102917
Iteration 3/25 | Loss: 0.00102917
Iteration 4/25 | Loss: 0.00102917
Iteration 5/25 | Loss: 0.00102917
Iteration 6/25 | Loss: 0.00102917
Iteration 7/25 | Loss: 0.00102917
Iteration 8/25 | Loss: 0.00102917
Iteration 9/25 | Loss: 0.00102917
Iteration 10/25 | Loss: 0.00102917
Iteration 11/25 | Loss: 0.00102917
Iteration 12/25 | Loss: 0.00102917
Iteration 13/25 | Loss: 0.00102917
Iteration 14/25 | Loss: 0.00102917
Iteration 15/25 | Loss: 0.00102917
Iteration 16/25 | Loss: 0.00102917
Iteration 17/25 | Loss: 0.00102917
Iteration 18/25 | Loss: 0.00102917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010291683720424771, 0.0010291683720424771, 0.0010291683720424771, 0.0010291683720424771, 0.0010291683720424771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010291683720424771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102917
Iteration 2/1000 | Loss: 0.00006417
Iteration 3/1000 | Loss: 0.00003870
Iteration 4/1000 | Loss: 0.00003335
Iteration 5/1000 | Loss: 0.00003119
Iteration 6/1000 | Loss: 0.00002961
Iteration 7/1000 | Loss: 0.00002835
Iteration 8/1000 | Loss: 0.00002760
Iteration 9/1000 | Loss: 0.00002709
Iteration 10/1000 | Loss: 0.00002677
Iteration 11/1000 | Loss: 0.00002646
Iteration 12/1000 | Loss: 0.00002625
Iteration 13/1000 | Loss: 0.00002601
Iteration 14/1000 | Loss: 0.00002583
Iteration 15/1000 | Loss: 0.00002581
Iteration 16/1000 | Loss: 0.00002578
Iteration 17/1000 | Loss: 0.00002577
Iteration 18/1000 | Loss: 0.00002576
Iteration 19/1000 | Loss: 0.00002576
Iteration 20/1000 | Loss: 0.00002573
Iteration 21/1000 | Loss: 0.00002568
Iteration 22/1000 | Loss: 0.00002561
Iteration 23/1000 | Loss: 0.00002561
Iteration 24/1000 | Loss: 0.00002560
Iteration 25/1000 | Loss: 0.00002559
Iteration 26/1000 | Loss: 0.00002558
Iteration 27/1000 | Loss: 0.00002557
Iteration 28/1000 | Loss: 0.00002557
Iteration 29/1000 | Loss: 0.00002557
Iteration 30/1000 | Loss: 0.00002557
Iteration 31/1000 | Loss: 0.00002556
Iteration 32/1000 | Loss: 0.00002556
Iteration 33/1000 | Loss: 0.00002553
Iteration 34/1000 | Loss: 0.00002552
Iteration 35/1000 | Loss: 0.00002552
Iteration 36/1000 | Loss: 0.00002550
Iteration 37/1000 | Loss: 0.00002549
Iteration 38/1000 | Loss: 0.00002548
Iteration 39/1000 | Loss: 0.00002548
Iteration 40/1000 | Loss: 0.00002547
Iteration 41/1000 | Loss: 0.00002547
Iteration 42/1000 | Loss: 0.00002546
Iteration 43/1000 | Loss: 0.00002546
Iteration 44/1000 | Loss: 0.00002545
Iteration 45/1000 | Loss: 0.00002545
Iteration 46/1000 | Loss: 0.00002545
Iteration 47/1000 | Loss: 0.00002544
Iteration 48/1000 | Loss: 0.00002544
Iteration 49/1000 | Loss: 0.00002543
Iteration 50/1000 | Loss: 0.00002543
Iteration 51/1000 | Loss: 0.00002543
Iteration 52/1000 | Loss: 0.00002542
Iteration 53/1000 | Loss: 0.00002542
Iteration 54/1000 | Loss: 0.00002542
Iteration 55/1000 | Loss: 0.00002541
Iteration 56/1000 | Loss: 0.00002541
Iteration 57/1000 | Loss: 0.00002541
Iteration 58/1000 | Loss: 0.00002540
Iteration 59/1000 | Loss: 0.00002540
Iteration 60/1000 | Loss: 0.00002539
Iteration 61/1000 | Loss: 0.00002539
Iteration 62/1000 | Loss: 0.00002539
Iteration 63/1000 | Loss: 0.00002539
Iteration 64/1000 | Loss: 0.00002538
Iteration 65/1000 | Loss: 0.00002538
Iteration 66/1000 | Loss: 0.00002538
Iteration 67/1000 | Loss: 0.00002537
Iteration 68/1000 | Loss: 0.00002537
Iteration 69/1000 | Loss: 0.00002537
Iteration 70/1000 | Loss: 0.00002536
Iteration 71/1000 | Loss: 0.00002536
Iteration 72/1000 | Loss: 0.00002536
Iteration 73/1000 | Loss: 0.00002536
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002535
Iteration 76/1000 | Loss: 0.00002535
Iteration 77/1000 | Loss: 0.00002534
Iteration 78/1000 | Loss: 0.00002534
Iteration 79/1000 | Loss: 0.00002534
Iteration 80/1000 | Loss: 0.00002533
Iteration 81/1000 | Loss: 0.00002533
Iteration 82/1000 | Loss: 0.00002533
Iteration 83/1000 | Loss: 0.00002533
Iteration 84/1000 | Loss: 0.00002533
Iteration 85/1000 | Loss: 0.00002532
Iteration 86/1000 | Loss: 0.00002532
Iteration 87/1000 | Loss: 0.00002532
Iteration 88/1000 | Loss: 0.00002532
Iteration 89/1000 | Loss: 0.00002531
Iteration 90/1000 | Loss: 0.00002531
Iteration 91/1000 | Loss: 0.00002531
Iteration 92/1000 | Loss: 0.00002531
Iteration 93/1000 | Loss: 0.00002531
Iteration 94/1000 | Loss: 0.00002531
Iteration 95/1000 | Loss: 0.00002530
Iteration 96/1000 | Loss: 0.00002530
Iteration 97/1000 | Loss: 0.00002530
Iteration 98/1000 | Loss: 0.00002530
Iteration 99/1000 | Loss: 0.00002530
Iteration 100/1000 | Loss: 0.00002530
Iteration 101/1000 | Loss: 0.00002530
Iteration 102/1000 | Loss: 0.00002529
Iteration 103/1000 | Loss: 0.00002529
Iteration 104/1000 | Loss: 0.00002529
Iteration 105/1000 | Loss: 0.00002529
Iteration 106/1000 | Loss: 0.00002529
Iteration 107/1000 | Loss: 0.00002529
Iteration 108/1000 | Loss: 0.00002529
Iteration 109/1000 | Loss: 0.00002529
Iteration 110/1000 | Loss: 0.00002529
Iteration 111/1000 | Loss: 0.00002529
Iteration 112/1000 | Loss: 0.00002529
Iteration 113/1000 | Loss: 0.00002529
Iteration 114/1000 | Loss: 0.00002529
Iteration 115/1000 | Loss: 0.00002529
Iteration 116/1000 | Loss: 0.00002529
Iteration 117/1000 | Loss: 0.00002529
Iteration 118/1000 | Loss: 0.00002529
Iteration 119/1000 | Loss: 0.00002529
Iteration 120/1000 | Loss: 0.00002529
Iteration 121/1000 | Loss: 0.00002529
Iteration 122/1000 | Loss: 0.00002529
Iteration 123/1000 | Loss: 0.00002529
Iteration 124/1000 | Loss: 0.00002529
Iteration 125/1000 | Loss: 0.00002529
Iteration 126/1000 | Loss: 0.00002529
Iteration 127/1000 | Loss: 0.00002529
Iteration 128/1000 | Loss: 0.00002529
Iteration 129/1000 | Loss: 0.00002529
Iteration 130/1000 | Loss: 0.00002529
Iteration 131/1000 | Loss: 0.00002529
Iteration 132/1000 | Loss: 0.00002529
Iteration 133/1000 | Loss: 0.00002529
Iteration 134/1000 | Loss: 0.00002529
Iteration 135/1000 | Loss: 0.00002529
Iteration 136/1000 | Loss: 0.00002529
Iteration 137/1000 | Loss: 0.00002529
Iteration 138/1000 | Loss: 0.00002529
Iteration 139/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.529361518099904e-05, 2.529361518099904e-05, 2.529361518099904e-05, 2.529361518099904e-05, 2.529361518099904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.529361518099904e-05

Optimization complete. Final v2v error: 4.052680015563965 mm

Highest mean error: 5.255306720733643 mm for frame 77

Lowest mean error: 3.2842390537261963 mm for frame 188

Saving results

Total time: 64.32638907432556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398619
Iteration 2/25 | Loss: 0.00131692
Iteration 3/25 | Loss: 0.00124847
Iteration 4/25 | Loss: 0.00124015
Iteration 5/25 | Loss: 0.00123769
Iteration 6/25 | Loss: 0.00123745
Iteration 7/25 | Loss: 0.00123745
Iteration 8/25 | Loss: 0.00123745
Iteration 9/25 | Loss: 0.00123745
Iteration 10/25 | Loss: 0.00123745
Iteration 11/25 | Loss: 0.00123745
Iteration 12/25 | Loss: 0.00123745
Iteration 13/25 | Loss: 0.00123745
Iteration 14/25 | Loss: 0.00123745
Iteration 15/25 | Loss: 0.00123745
Iteration 16/25 | Loss: 0.00123745
Iteration 17/25 | Loss: 0.00123745
Iteration 18/25 | Loss: 0.00123745
Iteration 19/25 | Loss: 0.00123745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012374479556456208, 0.0012374479556456208, 0.0012374479556456208, 0.0012374479556456208, 0.0012374479556456208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012374479556456208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43329334
Iteration 2/25 | Loss: 0.00087719
Iteration 3/25 | Loss: 0.00087719
Iteration 4/25 | Loss: 0.00087718
Iteration 5/25 | Loss: 0.00087718
Iteration 6/25 | Loss: 0.00087718
Iteration 7/25 | Loss: 0.00087718
Iteration 8/25 | Loss: 0.00087718
Iteration 9/25 | Loss: 0.00087718
Iteration 10/25 | Loss: 0.00087718
Iteration 11/25 | Loss: 0.00087718
Iteration 12/25 | Loss: 0.00087718
Iteration 13/25 | Loss: 0.00087718
Iteration 14/25 | Loss: 0.00087718
Iteration 15/25 | Loss: 0.00087718
Iteration 16/25 | Loss: 0.00087718
Iteration 17/25 | Loss: 0.00087718
Iteration 18/25 | Loss: 0.00087718
Iteration 19/25 | Loss: 0.00087718
Iteration 20/25 | Loss: 0.00087718
Iteration 21/25 | Loss: 0.00087718
Iteration 22/25 | Loss: 0.00087718
Iteration 23/25 | Loss: 0.00087718
Iteration 24/25 | Loss: 0.00087718
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008771816501393914, 0.0008771816501393914, 0.0008771816501393914, 0.0008771816501393914, 0.0008771816501393914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008771816501393914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087718
Iteration 2/1000 | Loss: 0.00003200
Iteration 3/1000 | Loss: 0.00001922
Iteration 4/1000 | Loss: 0.00001534
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001249
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001159
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001143
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001140
Iteration 34/1000 | Loss: 0.00001139
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001138
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001129
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001128
Iteration 68/1000 | Loss: 0.00001128
Iteration 69/1000 | Loss: 0.00001127
Iteration 70/1000 | Loss: 0.00001127
Iteration 71/1000 | Loss: 0.00001127
Iteration 72/1000 | Loss: 0.00001127
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001126
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001125
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001123
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001122
Iteration 93/1000 | Loss: 0.00001122
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001121
Iteration 96/1000 | Loss: 0.00001121
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001121
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001120
Iteration 105/1000 | Loss: 0.00001120
Iteration 106/1000 | Loss: 0.00001120
Iteration 107/1000 | Loss: 0.00001120
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001119
Iteration 110/1000 | Loss: 0.00001119
Iteration 111/1000 | Loss: 0.00001119
Iteration 112/1000 | Loss: 0.00001118
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001113
Iteration 128/1000 | Loss: 0.00001113
Iteration 129/1000 | Loss: 0.00001113
Iteration 130/1000 | Loss: 0.00001112
Iteration 131/1000 | Loss: 0.00001112
Iteration 132/1000 | Loss: 0.00001112
Iteration 133/1000 | Loss: 0.00001112
Iteration 134/1000 | Loss: 0.00001112
Iteration 135/1000 | Loss: 0.00001112
Iteration 136/1000 | Loss: 0.00001112
Iteration 137/1000 | Loss: 0.00001111
Iteration 138/1000 | Loss: 0.00001111
Iteration 139/1000 | Loss: 0.00001111
Iteration 140/1000 | Loss: 0.00001111
Iteration 141/1000 | Loss: 0.00001111
Iteration 142/1000 | Loss: 0.00001111
Iteration 143/1000 | Loss: 0.00001111
Iteration 144/1000 | Loss: 0.00001111
Iteration 145/1000 | Loss: 0.00001111
Iteration 146/1000 | Loss: 0.00001111
Iteration 147/1000 | Loss: 0.00001110
Iteration 148/1000 | Loss: 0.00001110
Iteration 149/1000 | Loss: 0.00001110
Iteration 150/1000 | Loss: 0.00001110
Iteration 151/1000 | Loss: 0.00001110
Iteration 152/1000 | Loss: 0.00001110
Iteration 153/1000 | Loss: 0.00001110
Iteration 154/1000 | Loss: 0.00001110
Iteration 155/1000 | Loss: 0.00001110
Iteration 156/1000 | Loss: 0.00001109
Iteration 157/1000 | Loss: 0.00001109
Iteration 158/1000 | Loss: 0.00001109
Iteration 159/1000 | Loss: 0.00001109
Iteration 160/1000 | Loss: 0.00001108
Iteration 161/1000 | Loss: 0.00001108
Iteration 162/1000 | Loss: 0.00001108
Iteration 163/1000 | Loss: 0.00001108
Iteration 164/1000 | Loss: 0.00001108
Iteration 165/1000 | Loss: 0.00001108
Iteration 166/1000 | Loss: 0.00001108
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001107
Iteration 171/1000 | Loss: 0.00001107
Iteration 172/1000 | Loss: 0.00001107
Iteration 173/1000 | Loss: 0.00001107
Iteration 174/1000 | Loss: 0.00001107
Iteration 175/1000 | Loss: 0.00001107
Iteration 176/1000 | Loss: 0.00001107
Iteration 177/1000 | Loss: 0.00001107
Iteration 178/1000 | Loss: 0.00001107
Iteration 179/1000 | Loss: 0.00001107
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001106
Iteration 184/1000 | Loss: 0.00001106
Iteration 185/1000 | Loss: 0.00001106
Iteration 186/1000 | Loss: 0.00001106
Iteration 187/1000 | Loss: 0.00001106
Iteration 188/1000 | Loss: 0.00001106
Iteration 189/1000 | Loss: 0.00001106
Iteration 190/1000 | Loss: 0.00001106
Iteration 191/1000 | Loss: 0.00001106
Iteration 192/1000 | Loss: 0.00001105
Iteration 193/1000 | Loss: 0.00001105
Iteration 194/1000 | Loss: 0.00001105
Iteration 195/1000 | Loss: 0.00001105
Iteration 196/1000 | Loss: 0.00001105
Iteration 197/1000 | Loss: 0.00001105
Iteration 198/1000 | Loss: 0.00001105
Iteration 199/1000 | Loss: 0.00001105
Iteration 200/1000 | Loss: 0.00001105
Iteration 201/1000 | Loss: 0.00001105
Iteration 202/1000 | Loss: 0.00001105
Iteration 203/1000 | Loss: 0.00001105
Iteration 204/1000 | Loss: 0.00001105
Iteration 205/1000 | Loss: 0.00001105
Iteration 206/1000 | Loss: 0.00001105
Iteration 207/1000 | Loss: 0.00001105
Iteration 208/1000 | Loss: 0.00001105
Iteration 209/1000 | Loss: 0.00001105
Iteration 210/1000 | Loss: 0.00001105
Iteration 211/1000 | Loss: 0.00001105
Iteration 212/1000 | Loss: 0.00001105
Iteration 213/1000 | Loss: 0.00001105
Iteration 214/1000 | Loss: 0.00001105
Iteration 215/1000 | Loss: 0.00001105
Iteration 216/1000 | Loss: 0.00001105
Iteration 217/1000 | Loss: 0.00001105
Iteration 218/1000 | Loss: 0.00001105
Iteration 219/1000 | Loss: 0.00001105
Iteration 220/1000 | Loss: 0.00001105
Iteration 221/1000 | Loss: 0.00001105
Iteration 222/1000 | Loss: 0.00001105
Iteration 223/1000 | Loss: 0.00001105
Iteration 224/1000 | Loss: 0.00001105
Iteration 225/1000 | Loss: 0.00001105
Iteration 226/1000 | Loss: 0.00001105
Iteration 227/1000 | Loss: 0.00001105
Iteration 228/1000 | Loss: 0.00001105
Iteration 229/1000 | Loss: 0.00001105
Iteration 230/1000 | Loss: 0.00001105
Iteration 231/1000 | Loss: 0.00001105
Iteration 232/1000 | Loss: 0.00001105
Iteration 233/1000 | Loss: 0.00001105
Iteration 234/1000 | Loss: 0.00001105
Iteration 235/1000 | Loss: 0.00001105
Iteration 236/1000 | Loss: 0.00001105
Iteration 237/1000 | Loss: 0.00001105
Iteration 238/1000 | Loss: 0.00001105
Iteration 239/1000 | Loss: 0.00001105
Iteration 240/1000 | Loss: 0.00001105
Iteration 241/1000 | Loss: 0.00001105
Iteration 242/1000 | Loss: 0.00001105
Iteration 243/1000 | Loss: 0.00001105
Iteration 244/1000 | Loss: 0.00001105
Iteration 245/1000 | Loss: 0.00001105
Iteration 246/1000 | Loss: 0.00001105
Iteration 247/1000 | Loss: 0.00001105
Iteration 248/1000 | Loss: 0.00001105
Iteration 249/1000 | Loss: 0.00001105
Iteration 250/1000 | Loss: 0.00001105
Iteration 251/1000 | Loss: 0.00001105
Iteration 252/1000 | Loss: 0.00001105
Iteration 253/1000 | Loss: 0.00001105
Iteration 254/1000 | Loss: 0.00001105
Iteration 255/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.1051801266148686e-05, 1.1051801266148686e-05, 1.1051801266148686e-05, 1.1051801266148686e-05, 1.1051801266148686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1051801266148686e-05

Optimization complete. Final v2v error: 2.84804630279541 mm

Highest mean error: 3.6854841709136963 mm for frame 64

Lowest mean error: 2.6898245811462402 mm for frame 151

Saving results

Total time: 41.8977108001709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985519
Iteration 2/25 | Loss: 0.00985519
Iteration 3/25 | Loss: 0.00985518
Iteration 4/25 | Loss: 0.00254016
Iteration 5/25 | Loss: 0.00191828
Iteration 6/25 | Loss: 0.00183323
Iteration 7/25 | Loss: 0.00180281
Iteration 8/25 | Loss: 0.00176973
Iteration 9/25 | Loss: 0.00175111
Iteration 10/25 | Loss: 0.00172364
Iteration 11/25 | Loss: 0.00173914
Iteration 12/25 | Loss: 0.00169446
Iteration 13/25 | Loss: 0.00168895
Iteration 14/25 | Loss: 0.00167643
Iteration 15/25 | Loss: 0.00166727
Iteration 16/25 | Loss: 0.00166779
Iteration 17/25 | Loss: 0.00166142
Iteration 18/25 | Loss: 0.00165614
Iteration 19/25 | Loss: 0.00165547
Iteration 20/25 | Loss: 0.00165540
Iteration 21/25 | Loss: 0.00165540
Iteration 22/25 | Loss: 0.00165540
Iteration 23/25 | Loss: 0.00165540
Iteration 24/25 | Loss: 0.00165540
Iteration 25/25 | Loss: 0.00165539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38885081
Iteration 2/25 | Loss: 0.00291476
Iteration 3/25 | Loss: 0.00285483
Iteration 4/25 | Loss: 0.00285483
Iteration 5/25 | Loss: 0.00285483
Iteration 6/25 | Loss: 0.00285483
Iteration 7/25 | Loss: 0.00285483
Iteration 8/25 | Loss: 0.00285483
Iteration 9/25 | Loss: 0.00285483
Iteration 10/25 | Loss: 0.00285483
Iteration 11/25 | Loss: 0.00285483
Iteration 12/25 | Loss: 0.00285483
Iteration 13/25 | Loss: 0.00285483
Iteration 14/25 | Loss: 0.00285483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002854827092960477, 0.002854827092960477, 0.002854827092960477, 0.002854827092960477, 0.002854827092960477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002854827092960477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285483
Iteration 2/1000 | Loss: 0.00046214
Iteration 3/1000 | Loss: 0.00042590
Iteration 4/1000 | Loss: 0.00027320
Iteration 5/1000 | Loss: 0.00024483
Iteration 6/1000 | Loss: 0.00022590
Iteration 7/1000 | Loss: 0.00020788
Iteration 8/1000 | Loss: 0.00019323
Iteration 9/1000 | Loss: 0.00018306
Iteration 10/1000 | Loss: 0.00017569
Iteration 11/1000 | Loss: 0.00016989
Iteration 12/1000 | Loss: 0.00016519
Iteration 13/1000 | Loss: 0.00030063
Iteration 14/1000 | Loss: 0.00030158
Iteration 15/1000 | Loss: 0.00037913
Iteration 16/1000 | Loss: 0.00089940
Iteration 17/1000 | Loss: 0.00442879
Iteration 18/1000 | Loss: 0.00508375
Iteration 19/1000 | Loss: 0.00036979
Iteration 20/1000 | Loss: 0.00026181
Iteration 21/1000 | Loss: 0.00018994
Iteration 22/1000 | Loss: 0.00012835
Iteration 23/1000 | Loss: 0.00012481
Iteration 24/1000 | Loss: 0.00007249
Iteration 25/1000 | Loss: 0.00005294
Iteration 26/1000 | Loss: 0.00004020
Iteration 27/1000 | Loss: 0.00003506
Iteration 28/1000 | Loss: 0.00002957
Iteration 29/1000 | Loss: 0.00002633
Iteration 30/1000 | Loss: 0.00002344
Iteration 31/1000 | Loss: 0.00002153
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00001607
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001457
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001436
Iteration 54/1000 | Loss: 0.00001436
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001433
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001424
Iteration 94/1000 | Loss: 0.00001424
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001421
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001420
Iteration 113/1000 | Loss: 0.00001420
Iteration 114/1000 | Loss: 0.00001420
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001419
Iteration 132/1000 | Loss: 0.00001419
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001418
Iteration 144/1000 | Loss: 0.00001418
Iteration 145/1000 | Loss: 0.00001418
Iteration 146/1000 | Loss: 0.00001418
Iteration 147/1000 | Loss: 0.00001418
Iteration 148/1000 | Loss: 0.00001418
Iteration 149/1000 | Loss: 0.00001418
Iteration 150/1000 | Loss: 0.00001418
Iteration 151/1000 | Loss: 0.00001418
Iteration 152/1000 | Loss: 0.00001418
Iteration 153/1000 | Loss: 0.00001418
Iteration 154/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.4178644960338715e-05, 1.4178644960338715e-05, 1.4178644960338715e-05, 1.4178644960338715e-05, 1.4178644960338715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4178644960338715e-05

Optimization complete. Final v2v error: 3.253072500228882 mm

Highest mean error: 3.485241413116455 mm for frame 164

Lowest mean error: 3.055773973464966 mm for frame 221

Saving results

Total time: 106.81544256210327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773237
Iteration 2/25 | Loss: 0.00170545
Iteration 3/25 | Loss: 0.00138850
Iteration 4/25 | Loss: 0.00136087
Iteration 5/25 | Loss: 0.00135608
Iteration 6/25 | Loss: 0.00135550
Iteration 7/25 | Loss: 0.00135550
Iteration 8/25 | Loss: 0.00135550
Iteration 9/25 | Loss: 0.00135550
Iteration 10/25 | Loss: 0.00135550
Iteration 11/25 | Loss: 0.00135550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013554979814216495, 0.0013554979814216495, 0.0013554979814216495, 0.0013554979814216495, 0.0013554979814216495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013554979814216495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40288365
Iteration 2/25 | Loss: 0.00073080
Iteration 3/25 | Loss: 0.00073079
Iteration 4/25 | Loss: 0.00073078
Iteration 5/25 | Loss: 0.00073078
Iteration 6/25 | Loss: 0.00073078
Iteration 7/25 | Loss: 0.00073078
Iteration 8/25 | Loss: 0.00073078
Iteration 9/25 | Loss: 0.00073078
Iteration 10/25 | Loss: 0.00073078
Iteration 11/25 | Loss: 0.00073078
Iteration 12/25 | Loss: 0.00073078
Iteration 13/25 | Loss: 0.00073078
Iteration 14/25 | Loss: 0.00073078
Iteration 15/25 | Loss: 0.00073078
Iteration 16/25 | Loss: 0.00073078
Iteration 17/25 | Loss: 0.00073078
Iteration 18/25 | Loss: 0.00073078
Iteration 19/25 | Loss: 0.00073078
Iteration 20/25 | Loss: 0.00073078
Iteration 21/25 | Loss: 0.00073078
Iteration 22/25 | Loss: 0.00073078
Iteration 23/25 | Loss: 0.00073078
Iteration 24/25 | Loss: 0.00073078
Iteration 25/25 | Loss: 0.00073078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073078
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00002925
Iteration 4/1000 | Loss: 0.00002688
Iteration 5/1000 | Loss: 0.00002556
Iteration 6/1000 | Loss: 0.00002445
Iteration 7/1000 | Loss: 0.00002392
Iteration 8/1000 | Loss: 0.00002346
Iteration 9/1000 | Loss: 0.00002298
Iteration 10/1000 | Loss: 0.00002272
Iteration 11/1000 | Loss: 0.00002246
Iteration 12/1000 | Loss: 0.00002239
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002209
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002201
Iteration 17/1000 | Loss: 0.00002200
Iteration 18/1000 | Loss: 0.00002199
Iteration 19/1000 | Loss: 0.00002197
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002195
Iteration 22/1000 | Loss: 0.00002195
Iteration 23/1000 | Loss: 0.00002194
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002193
Iteration 26/1000 | Loss: 0.00002193
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002192
Iteration 29/1000 | Loss: 0.00002192
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002191
Iteration 32/1000 | Loss: 0.00002191
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002191
Iteration 35/1000 | Loss: 0.00002191
Iteration 36/1000 | Loss: 0.00002191
Iteration 37/1000 | Loss: 0.00002191
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002191
Iteration 40/1000 | Loss: 0.00002190
Iteration 41/1000 | Loss: 0.00002190
Iteration 42/1000 | Loss: 0.00002190
Iteration 43/1000 | Loss: 0.00002189
Iteration 44/1000 | Loss: 0.00002189
Iteration 45/1000 | Loss: 0.00002188
Iteration 46/1000 | Loss: 0.00002188
Iteration 47/1000 | Loss: 0.00002188
Iteration 48/1000 | Loss: 0.00002188
Iteration 49/1000 | Loss: 0.00002188
Iteration 50/1000 | Loss: 0.00002188
Iteration 51/1000 | Loss: 0.00002187
Iteration 52/1000 | Loss: 0.00002187
Iteration 53/1000 | Loss: 0.00002187
Iteration 54/1000 | Loss: 0.00002187
Iteration 55/1000 | Loss: 0.00002187
Iteration 56/1000 | Loss: 0.00002187
Iteration 57/1000 | Loss: 0.00002187
Iteration 58/1000 | Loss: 0.00002187
Iteration 59/1000 | Loss: 0.00002186
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00002186
Iteration 62/1000 | Loss: 0.00002186
Iteration 63/1000 | Loss: 0.00002186
Iteration 64/1000 | Loss: 0.00002185
Iteration 65/1000 | Loss: 0.00002185
Iteration 66/1000 | Loss: 0.00002185
Iteration 67/1000 | Loss: 0.00002185
Iteration 68/1000 | Loss: 0.00002185
Iteration 69/1000 | Loss: 0.00002185
Iteration 70/1000 | Loss: 0.00002184
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002184
Iteration 73/1000 | Loss: 0.00002184
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002183
Iteration 78/1000 | Loss: 0.00002183
Iteration 79/1000 | Loss: 0.00002183
Iteration 80/1000 | Loss: 0.00002183
Iteration 81/1000 | Loss: 0.00002183
Iteration 82/1000 | Loss: 0.00002183
Iteration 83/1000 | Loss: 0.00002183
Iteration 84/1000 | Loss: 0.00002182
Iteration 85/1000 | Loss: 0.00002182
Iteration 86/1000 | Loss: 0.00002182
Iteration 87/1000 | Loss: 0.00002182
Iteration 88/1000 | Loss: 0.00002182
Iteration 89/1000 | Loss: 0.00002182
Iteration 90/1000 | Loss: 0.00002182
Iteration 91/1000 | Loss: 0.00002181
Iteration 92/1000 | Loss: 0.00002181
Iteration 93/1000 | Loss: 0.00002181
Iteration 94/1000 | Loss: 0.00002181
Iteration 95/1000 | Loss: 0.00002181
Iteration 96/1000 | Loss: 0.00002181
Iteration 97/1000 | Loss: 0.00002181
Iteration 98/1000 | Loss: 0.00002181
Iteration 99/1000 | Loss: 0.00002181
Iteration 100/1000 | Loss: 0.00002181
Iteration 101/1000 | Loss: 0.00002181
Iteration 102/1000 | Loss: 0.00002181
Iteration 103/1000 | Loss: 0.00002181
Iteration 104/1000 | Loss: 0.00002181
Iteration 105/1000 | Loss: 0.00002181
Iteration 106/1000 | Loss: 0.00002181
Iteration 107/1000 | Loss: 0.00002181
Iteration 108/1000 | Loss: 0.00002181
Iteration 109/1000 | Loss: 0.00002180
Iteration 110/1000 | Loss: 0.00002180
Iteration 111/1000 | Loss: 0.00002180
Iteration 112/1000 | Loss: 0.00002180
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002179
Iteration 116/1000 | Loss: 0.00002179
Iteration 117/1000 | Loss: 0.00002179
Iteration 118/1000 | Loss: 0.00002179
Iteration 119/1000 | Loss: 0.00002179
Iteration 120/1000 | Loss: 0.00002179
Iteration 121/1000 | Loss: 0.00002179
Iteration 122/1000 | Loss: 0.00002178
Iteration 123/1000 | Loss: 0.00002178
Iteration 124/1000 | Loss: 0.00002178
Iteration 125/1000 | Loss: 0.00002177
Iteration 126/1000 | Loss: 0.00002177
Iteration 127/1000 | Loss: 0.00002176
Iteration 128/1000 | Loss: 0.00002176
Iteration 129/1000 | Loss: 0.00002176
Iteration 130/1000 | Loss: 0.00002176
Iteration 131/1000 | Loss: 0.00002176
Iteration 132/1000 | Loss: 0.00002176
Iteration 133/1000 | Loss: 0.00002176
Iteration 134/1000 | Loss: 0.00002176
Iteration 135/1000 | Loss: 0.00002176
Iteration 136/1000 | Loss: 0.00002175
Iteration 137/1000 | Loss: 0.00002175
Iteration 138/1000 | Loss: 0.00002175
Iteration 139/1000 | Loss: 0.00002175
Iteration 140/1000 | Loss: 0.00002175
Iteration 141/1000 | Loss: 0.00002175
Iteration 142/1000 | Loss: 0.00002175
Iteration 143/1000 | Loss: 0.00002175
Iteration 144/1000 | Loss: 0.00002175
Iteration 145/1000 | Loss: 0.00002174
Iteration 146/1000 | Loss: 0.00002174
Iteration 147/1000 | Loss: 0.00002174
Iteration 148/1000 | Loss: 0.00002174
Iteration 149/1000 | Loss: 0.00002174
Iteration 150/1000 | Loss: 0.00002174
Iteration 151/1000 | Loss: 0.00002174
Iteration 152/1000 | Loss: 0.00002174
Iteration 153/1000 | Loss: 0.00002174
Iteration 154/1000 | Loss: 0.00002174
Iteration 155/1000 | Loss: 0.00002174
Iteration 156/1000 | Loss: 0.00002173
Iteration 157/1000 | Loss: 0.00002173
Iteration 158/1000 | Loss: 0.00002173
Iteration 159/1000 | Loss: 0.00002173
Iteration 160/1000 | Loss: 0.00002173
Iteration 161/1000 | Loss: 0.00002173
Iteration 162/1000 | Loss: 0.00002173
Iteration 163/1000 | Loss: 0.00002172
Iteration 164/1000 | Loss: 0.00002172
Iteration 165/1000 | Loss: 0.00002172
Iteration 166/1000 | Loss: 0.00002172
Iteration 167/1000 | Loss: 0.00002172
Iteration 168/1000 | Loss: 0.00002172
Iteration 169/1000 | Loss: 0.00002172
Iteration 170/1000 | Loss: 0.00002172
Iteration 171/1000 | Loss: 0.00002172
Iteration 172/1000 | Loss: 0.00002172
Iteration 173/1000 | Loss: 0.00002172
Iteration 174/1000 | Loss: 0.00002171
Iteration 175/1000 | Loss: 0.00002171
Iteration 176/1000 | Loss: 0.00002171
Iteration 177/1000 | Loss: 0.00002171
Iteration 178/1000 | Loss: 0.00002171
Iteration 179/1000 | Loss: 0.00002171
Iteration 180/1000 | Loss: 0.00002171
Iteration 181/1000 | Loss: 0.00002171
Iteration 182/1000 | Loss: 0.00002171
Iteration 183/1000 | Loss: 0.00002170
Iteration 184/1000 | Loss: 0.00002170
Iteration 185/1000 | Loss: 0.00002170
Iteration 186/1000 | Loss: 0.00002170
Iteration 187/1000 | Loss: 0.00002170
Iteration 188/1000 | Loss: 0.00002170
Iteration 189/1000 | Loss: 0.00002170
Iteration 190/1000 | Loss: 0.00002170
Iteration 191/1000 | Loss: 0.00002170
Iteration 192/1000 | Loss: 0.00002170
Iteration 193/1000 | Loss: 0.00002170
Iteration 194/1000 | Loss: 0.00002170
Iteration 195/1000 | Loss: 0.00002170
Iteration 196/1000 | Loss: 0.00002170
Iteration 197/1000 | Loss: 0.00002170
Iteration 198/1000 | Loss: 0.00002170
Iteration 199/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.1703333914047107e-05, 2.1703333914047107e-05, 2.1703333914047107e-05, 2.1703333914047107e-05, 2.1703333914047107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1703333914047107e-05

Optimization complete. Final v2v error: 3.8869011402130127 mm

Highest mean error: 4.067856788635254 mm for frame 212

Lowest mean error: 3.6592745780944824 mm for frame 0

Saving results

Total time: 43.8571515083313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039713
Iteration 2/25 | Loss: 0.00236494
Iteration 3/25 | Loss: 0.00167118
Iteration 4/25 | Loss: 0.00162292
Iteration 5/25 | Loss: 0.00157746
Iteration 6/25 | Loss: 0.00155811
Iteration 7/25 | Loss: 0.00156296
Iteration 8/25 | Loss: 0.00157134
Iteration 9/25 | Loss: 0.00152288
Iteration 10/25 | Loss: 0.00149078
Iteration 11/25 | Loss: 0.00147712
Iteration 12/25 | Loss: 0.00146968
Iteration 13/25 | Loss: 0.00146624
Iteration 14/25 | Loss: 0.00146571
Iteration 15/25 | Loss: 0.00146525
Iteration 16/25 | Loss: 0.00149103
Iteration 17/25 | Loss: 0.00146484
Iteration 18/25 | Loss: 0.00146370
Iteration 19/25 | Loss: 0.00146091
Iteration 20/25 | Loss: 0.00145862
Iteration 21/25 | Loss: 0.00145606
Iteration 22/25 | Loss: 0.00144580
Iteration 23/25 | Loss: 0.00144457
Iteration 24/25 | Loss: 0.00144439
Iteration 25/25 | Loss: 0.00144438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47795391
Iteration 2/25 | Loss: 0.00100238
Iteration 3/25 | Loss: 0.00100234
Iteration 4/25 | Loss: 0.00100234
Iteration 5/25 | Loss: 0.00100234
Iteration 6/25 | Loss: 0.00100234
Iteration 7/25 | Loss: 0.00100234
Iteration 8/25 | Loss: 0.00100234
Iteration 9/25 | Loss: 0.00100234
Iteration 10/25 | Loss: 0.00100234
Iteration 11/25 | Loss: 0.00100234
Iteration 12/25 | Loss: 0.00100234
Iteration 13/25 | Loss: 0.00100234
Iteration 14/25 | Loss: 0.00100234
Iteration 15/25 | Loss: 0.00100234
Iteration 16/25 | Loss: 0.00100234
Iteration 17/25 | Loss: 0.00100234
Iteration 18/25 | Loss: 0.00100234
Iteration 19/25 | Loss: 0.00100234
Iteration 20/25 | Loss: 0.00100234
Iteration 21/25 | Loss: 0.00100234
Iteration 22/25 | Loss: 0.00100234
Iteration 23/25 | Loss: 0.00100234
Iteration 24/25 | Loss: 0.00100234
Iteration 25/25 | Loss: 0.00100234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100234
Iteration 2/1000 | Loss: 0.00020452
Iteration 3/1000 | Loss: 0.00004494
Iteration 4/1000 | Loss: 0.00003485
Iteration 5/1000 | Loss: 0.00003223
Iteration 6/1000 | Loss: 0.00003072
Iteration 7/1000 | Loss: 0.00022249
Iteration 8/1000 | Loss: 0.00019869
Iteration 9/1000 | Loss: 0.00002915
Iteration 10/1000 | Loss: 0.00002863
Iteration 11/1000 | Loss: 0.00002813
Iteration 12/1000 | Loss: 0.00002775
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00010493
Iteration 15/1000 | Loss: 0.00002902
Iteration 16/1000 | Loss: 0.00002748
Iteration 17/1000 | Loss: 0.00014219
Iteration 18/1000 | Loss: 0.00002665
Iteration 19/1000 | Loss: 0.00015080
Iteration 20/1000 | Loss: 0.00002613
Iteration 21/1000 | Loss: 0.00017719
Iteration 22/1000 | Loss: 0.00002643
Iteration 23/1000 | Loss: 0.00002555
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002533
Iteration 26/1000 | Loss: 0.00002529
Iteration 27/1000 | Loss: 0.00002522
Iteration 28/1000 | Loss: 0.00002514
Iteration 29/1000 | Loss: 0.00002507
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002500
Iteration 33/1000 | Loss: 0.00002499
Iteration 34/1000 | Loss: 0.00002498
Iteration 35/1000 | Loss: 0.00002497
Iteration 36/1000 | Loss: 0.00002497
Iteration 37/1000 | Loss: 0.00002497
Iteration 38/1000 | Loss: 0.00002496
Iteration 39/1000 | Loss: 0.00002495
Iteration 40/1000 | Loss: 0.00002494
Iteration 41/1000 | Loss: 0.00002494
Iteration 42/1000 | Loss: 0.00002493
Iteration 43/1000 | Loss: 0.00002493
Iteration 44/1000 | Loss: 0.00002492
Iteration 45/1000 | Loss: 0.00002492
Iteration 46/1000 | Loss: 0.00002492
Iteration 47/1000 | Loss: 0.00002492
Iteration 48/1000 | Loss: 0.00002492
Iteration 49/1000 | Loss: 0.00002492
Iteration 50/1000 | Loss: 0.00002491
Iteration 51/1000 | Loss: 0.00002491
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002490
Iteration 54/1000 | Loss: 0.00002490
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002489
Iteration 57/1000 | Loss: 0.00002489
Iteration 58/1000 | Loss: 0.00002488
Iteration 59/1000 | Loss: 0.00002488
Iteration 60/1000 | Loss: 0.00002488
Iteration 61/1000 | Loss: 0.00002487
Iteration 62/1000 | Loss: 0.00002487
Iteration 63/1000 | Loss: 0.00002487
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002485
Iteration 67/1000 | Loss: 0.00002485
Iteration 68/1000 | Loss: 0.00002484
Iteration 69/1000 | Loss: 0.00002484
Iteration 70/1000 | Loss: 0.00002483
Iteration 71/1000 | Loss: 0.00002482
Iteration 72/1000 | Loss: 0.00002482
Iteration 73/1000 | Loss: 0.00002482
Iteration 74/1000 | Loss: 0.00002481
Iteration 75/1000 | Loss: 0.00002481
Iteration 76/1000 | Loss: 0.00002481
Iteration 77/1000 | Loss: 0.00002481
Iteration 78/1000 | Loss: 0.00002481
Iteration 79/1000 | Loss: 0.00002481
Iteration 80/1000 | Loss: 0.00002481
Iteration 81/1000 | Loss: 0.00002481
Iteration 82/1000 | Loss: 0.00002481
Iteration 83/1000 | Loss: 0.00002481
Iteration 84/1000 | Loss: 0.00002480
Iteration 85/1000 | Loss: 0.00002480
Iteration 86/1000 | Loss: 0.00002480
Iteration 87/1000 | Loss: 0.00002480
Iteration 88/1000 | Loss: 0.00002480
Iteration 89/1000 | Loss: 0.00002480
Iteration 90/1000 | Loss: 0.00002479
Iteration 91/1000 | Loss: 0.00002479
Iteration 92/1000 | Loss: 0.00002479
Iteration 93/1000 | Loss: 0.00002479
Iteration 94/1000 | Loss: 0.00002479
Iteration 95/1000 | Loss: 0.00002479
Iteration 96/1000 | Loss: 0.00002479
Iteration 97/1000 | Loss: 0.00002478
Iteration 98/1000 | Loss: 0.00002478
Iteration 99/1000 | Loss: 0.00002478
Iteration 100/1000 | Loss: 0.00002477
Iteration 101/1000 | Loss: 0.00002477
Iteration 102/1000 | Loss: 0.00002477
Iteration 103/1000 | Loss: 0.00002476
Iteration 104/1000 | Loss: 0.00002476
Iteration 105/1000 | Loss: 0.00002476
Iteration 106/1000 | Loss: 0.00002476
Iteration 107/1000 | Loss: 0.00002475
Iteration 108/1000 | Loss: 0.00002475
Iteration 109/1000 | Loss: 0.00002475
Iteration 110/1000 | Loss: 0.00002474
Iteration 111/1000 | Loss: 0.00002474
Iteration 112/1000 | Loss: 0.00002474
Iteration 113/1000 | Loss: 0.00002474
Iteration 114/1000 | Loss: 0.00002474
Iteration 115/1000 | Loss: 0.00002474
Iteration 116/1000 | Loss: 0.00002473
Iteration 117/1000 | Loss: 0.00002473
Iteration 118/1000 | Loss: 0.00002473
Iteration 119/1000 | Loss: 0.00002473
Iteration 120/1000 | Loss: 0.00002473
Iteration 121/1000 | Loss: 0.00002473
Iteration 122/1000 | Loss: 0.00002473
Iteration 123/1000 | Loss: 0.00002473
Iteration 124/1000 | Loss: 0.00002473
Iteration 125/1000 | Loss: 0.00002472
Iteration 126/1000 | Loss: 0.00002472
Iteration 127/1000 | Loss: 0.00002472
Iteration 128/1000 | Loss: 0.00002472
Iteration 129/1000 | Loss: 0.00002472
Iteration 130/1000 | Loss: 0.00002472
Iteration 131/1000 | Loss: 0.00002472
Iteration 132/1000 | Loss: 0.00002472
Iteration 133/1000 | Loss: 0.00002472
Iteration 134/1000 | Loss: 0.00002472
Iteration 135/1000 | Loss: 0.00002472
Iteration 136/1000 | Loss: 0.00002472
Iteration 137/1000 | Loss: 0.00002472
Iteration 138/1000 | Loss: 0.00002472
Iteration 139/1000 | Loss: 0.00002472
Iteration 140/1000 | Loss: 0.00002471
Iteration 141/1000 | Loss: 0.00002471
Iteration 142/1000 | Loss: 0.00002471
Iteration 143/1000 | Loss: 0.00002471
Iteration 144/1000 | Loss: 0.00002471
Iteration 145/1000 | Loss: 0.00002471
Iteration 146/1000 | Loss: 0.00002471
Iteration 147/1000 | Loss: 0.00002471
Iteration 148/1000 | Loss: 0.00002471
Iteration 149/1000 | Loss: 0.00002471
Iteration 150/1000 | Loss: 0.00002471
Iteration 151/1000 | Loss: 0.00002471
Iteration 152/1000 | Loss: 0.00002471
Iteration 153/1000 | Loss: 0.00002471
Iteration 154/1000 | Loss: 0.00002471
Iteration 155/1000 | Loss: 0.00002470
Iteration 156/1000 | Loss: 0.00002470
Iteration 157/1000 | Loss: 0.00002470
Iteration 158/1000 | Loss: 0.00002470
Iteration 159/1000 | Loss: 0.00002470
Iteration 160/1000 | Loss: 0.00002470
Iteration 161/1000 | Loss: 0.00002470
Iteration 162/1000 | Loss: 0.00002470
Iteration 163/1000 | Loss: 0.00002470
Iteration 164/1000 | Loss: 0.00002470
Iteration 165/1000 | Loss: 0.00002470
Iteration 166/1000 | Loss: 0.00002470
Iteration 167/1000 | Loss: 0.00002470
Iteration 168/1000 | Loss: 0.00002470
Iteration 169/1000 | Loss: 0.00002470
Iteration 170/1000 | Loss: 0.00002470
Iteration 171/1000 | Loss: 0.00002470
Iteration 172/1000 | Loss: 0.00002470
Iteration 173/1000 | Loss: 0.00002470
Iteration 174/1000 | Loss: 0.00002469
Iteration 175/1000 | Loss: 0.00002469
Iteration 176/1000 | Loss: 0.00002469
Iteration 177/1000 | Loss: 0.00002469
Iteration 178/1000 | Loss: 0.00002469
Iteration 179/1000 | Loss: 0.00002469
Iteration 180/1000 | Loss: 0.00002469
Iteration 181/1000 | Loss: 0.00002469
Iteration 182/1000 | Loss: 0.00002469
Iteration 183/1000 | Loss: 0.00002469
Iteration 184/1000 | Loss: 0.00002469
Iteration 185/1000 | Loss: 0.00002469
Iteration 186/1000 | Loss: 0.00002469
Iteration 187/1000 | Loss: 0.00002469
Iteration 188/1000 | Loss: 0.00002469
Iteration 189/1000 | Loss: 0.00002469
Iteration 190/1000 | Loss: 0.00002469
Iteration 191/1000 | Loss: 0.00002469
Iteration 192/1000 | Loss: 0.00002469
Iteration 193/1000 | Loss: 0.00002469
Iteration 194/1000 | Loss: 0.00002469
Iteration 195/1000 | Loss: 0.00002469
Iteration 196/1000 | Loss: 0.00002469
Iteration 197/1000 | Loss: 0.00002469
Iteration 198/1000 | Loss: 0.00002469
Iteration 199/1000 | Loss: 0.00002469
Iteration 200/1000 | Loss: 0.00002469
Iteration 201/1000 | Loss: 0.00002469
Iteration 202/1000 | Loss: 0.00002469
Iteration 203/1000 | Loss: 0.00002469
Iteration 204/1000 | Loss: 0.00002469
Iteration 205/1000 | Loss: 0.00002469
Iteration 206/1000 | Loss: 0.00002469
Iteration 207/1000 | Loss: 0.00002469
Iteration 208/1000 | Loss: 0.00002469
Iteration 209/1000 | Loss: 0.00002469
Iteration 210/1000 | Loss: 0.00002469
Iteration 211/1000 | Loss: 0.00002469
Iteration 212/1000 | Loss: 0.00002469
Iteration 213/1000 | Loss: 0.00002469
Iteration 214/1000 | Loss: 0.00002469
Iteration 215/1000 | Loss: 0.00002469
Iteration 216/1000 | Loss: 0.00002469
Iteration 217/1000 | Loss: 0.00002469
Iteration 218/1000 | Loss: 0.00002469
Iteration 219/1000 | Loss: 0.00002469
Iteration 220/1000 | Loss: 0.00002469
Iteration 221/1000 | Loss: 0.00002469
Iteration 222/1000 | Loss: 0.00002469
Iteration 223/1000 | Loss: 0.00002469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.4685768948984332e-05, 2.4685768948984332e-05, 2.4685768948984332e-05, 2.4685768948984332e-05, 2.4685768948984332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4685768948984332e-05

Optimization complete. Final v2v error: 4.061647891998291 mm

Highest mean error: 4.550942897796631 mm for frame 1

Lowest mean error: 3.6225855350494385 mm for frame 142

Saving results

Total time: 93.38465237617493
