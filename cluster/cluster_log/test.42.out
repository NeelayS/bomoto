Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=42, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2352-2407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418942
Iteration 2/25 | Loss: 0.00092175
Iteration 3/25 | Loss: 0.00080773
Iteration 4/25 | Loss: 0.00079442
Iteration 5/25 | Loss: 0.00078982
Iteration 6/25 | Loss: 0.00078850
Iteration 7/25 | Loss: 0.00078827
Iteration 8/25 | Loss: 0.00078827
Iteration 9/25 | Loss: 0.00078827
Iteration 10/25 | Loss: 0.00078827
Iteration 11/25 | Loss: 0.00078827
Iteration 12/25 | Loss: 0.00078827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007882742211222649, 0.0007882742211222649, 0.0007882742211222649, 0.0007882742211222649, 0.0007882742211222649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007882742211222649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88475847
Iteration 2/25 | Loss: 0.00077332
Iteration 3/25 | Loss: 0.00077330
Iteration 4/25 | Loss: 0.00077330
Iteration 5/25 | Loss: 0.00077330
Iteration 6/25 | Loss: 0.00077330
Iteration 7/25 | Loss: 0.00077330
Iteration 8/25 | Loss: 0.00077330
Iteration 9/25 | Loss: 0.00077330
Iteration 10/25 | Loss: 0.00077330
Iteration 11/25 | Loss: 0.00077330
Iteration 12/25 | Loss: 0.00077330
Iteration 13/25 | Loss: 0.00077330
Iteration 14/25 | Loss: 0.00077330
Iteration 15/25 | Loss: 0.00077330
Iteration 16/25 | Loss: 0.00077330
Iteration 17/25 | Loss: 0.00077330
Iteration 18/25 | Loss: 0.00077330
Iteration 19/25 | Loss: 0.00077330
Iteration 20/25 | Loss: 0.00077330
Iteration 21/25 | Loss: 0.00077330
Iteration 22/25 | Loss: 0.00077330
Iteration 23/25 | Loss: 0.00077330
Iteration 24/25 | Loss: 0.00077330
Iteration 25/25 | Loss: 0.00077330

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077330
Iteration 2/1000 | Loss: 0.00003225
Iteration 3/1000 | Loss: 0.00002567
Iteration 4/1000 | Loss: 0.00002408
Iteration 5/1000 | Loss: 0.00002305
Iteration 6/1000 | Loss: 0.00002232
Iteration 7/1000 | Loss: 0.00002166
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002124
Iteration 10/1000 | Loss: 0.00002119
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002117
Iteration 13/1000 | Loss: 0.00002116
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002115
Iteration 16/1000 | Loss: 0.00002115
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002112
Iteration 19/1000 | Loss: 0.00002112
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002110
Iteration 22/1000 | Loss: 0.00002104
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002097
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002093
Iteration 32/1000 | Loss: 0.00002093
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002092
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002092
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002091
Iteration 41/1000 | Loss: 0.00002091
Iteration 42/1000 | Loss: 0.00002091
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002090
Iteration 45/1000 | Loss: 0.00002090
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002090
Iteration 48/1000 | Loss: 0.00002090
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002090
Iteration 51/1000 | Loss: 0.00002089
Iteration 52/1000 | Loss: 0.00002089
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002089
Iteration 55/1000 | Loss: 0.00002089
Iteration 56/1000 | Loss: 0.00002089
Iteration 57/1000 | Loss: 0.00002089
Iteration 58/1000 | Loss: 0.00002089
Iteration 59/1000 | Loss: 0.00002088
Iteration 60/1000 | Loss: 0.00002088
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002088
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002086
Iteration 70/1000 | Loss: 0.00002086
Iteration 71/1000 | Loss: 0.00002085
Iteration 72/1000 | Loss: 0.00002085
Iteration 73/1000 | Loss: 0.00002085
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002084
Iteration 79/1000 | Loss: 0.00002084
Iteration 80/1000 | Loss: 0.00002084
Iteration 81/1000 | Loss: 0.00002083
Iteration 82/1000 | Loss: 0.00002083
Iteration 83/1000 | Loss: 0.00002083
Iteration 84/1000 | Loss: 0.00002083
Iteration 85/1000 | Loss: 0.00002083
Iteration 86/1000 | Loss: 0.00002083
Iteration 87/1000 | Loss: 0.00002082
Iteration 88/1000 | Loss: 0.00002082
Iteration 89/1000 | Loss: 0.00002082
Iteration 90/1000 | Loss: 0.00002082
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00002081
Iteration 93/1000 | Loss: 0.00002081
Iteration 94/1000 | Loss: 0.00002081
Iteration 95/1000 | Loss: 0.00002081
Iteration 96/1000 | Loss: 0.00002081
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002080
Iteration 103/1000 | Loss: 0.00002080
Iteration 104/1000 | Loss: 0.00002080
Iteration 105/1000 | Loss: 0.00002080
Iteration 106/1000 | Loss: 0.00002080
Iteration 107/1000 | Loss: 0.00002080
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002079
Iteration 110/1000 | Loss: 0.00002079
Iteration 111/1000 | Loss: 0.00002079
Iteration 112/1000 | Loss: 0.00002079
Iteration 113/1000 | Loss: 0.00002079
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002079
Iteration 117/1000 | Loss: 0.00002079
Iteration 118/1000 | Loss: 0.00002079
Iteration 119/1000 | Loss: 0.00002079
Iteration 120/1000 | Loss: 0.00002079
Iteration 121/1000 | Loss: 0.00002078
Iteration 122/1000 | Loss: 0.00002078
Iteration 123/1000 | Loss: 0.00002078
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002077
Iteration 128/1000 | Loss: 0.00002077
Iteration 129/1000 | Loss: 0.00002077
Iteration 130/1000 | Loss: 0.00002077
Iteration 131/1000 | Loss: 0.00002077
Iteration 132/1000 | Loss: 0.00002077
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002077
Iteration 136/1000 | Loss: 0.00002077
Iteration 137/1000 | Loss: 0.00002077
Iteration 138/1000 | Loss: 0.00002077
Iteration 139/1000 | Loss: 0.00002077
Iteration 140/1000 | Loss: 0.00002077
Iteration 141/1000 | Loss: 0.00002077
Iteration 142/1000 | Loss: 0.00002077
Iteration 143/1000 | Loss: 0.00002077
Iteration 144/1000 | Loss: 0.00002077
Iteration 145/1000 | Loss: 0.00002077
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002077
Iteration 148/1000 | Loss: 0.00002077
Iteration 149/1000 | Loss: 0.00002077
Iteration 150/1000 | Loss: 0.00002077
Iteration 151/1000 | Loss: 0.00002077
Iteration 152/1000 | Loss: 0.00002077
Iteration 153/1000 | Loss: 0.00002077
Iteration 154/1000 | Loss: 0.00002077
Iteration 155/1000 | Loss: 0.00002077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0772322386619635e-05, 2.0772322386619635e-05, 2.0772322386619635e-05, 2.0772322386619635e-05, 2.0772322386619635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0772322386619635e-05

Optimization complete. Final v2v error: 3.9462838172912598 mm

Highest mean error: 4.313362121582031 mm for frame 71

Lowest mean error: 3.6628990173339844 mm for frame 135

Saving results

Total time: 35.32206583023071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462241
Iteration 2/25 | Loss: 0.00106998
Iteration 3/25 | Loss: 0.00081286
Iteration 4/25 | Loss: 0.00078548
Iteration 5/25 | Loss: 0.00077791
Iteration 6/25 | Loss: 0.00077558
Iteration 7/25 | Loss: 0.00077508
Iteration 8/25 | Loss: 0.00077508
Iteration 9/25 | Loss: 0.00077508
Iteration 10/25 | Loss: 0.00077508
Iteration 11/25 | Loss: 0.00077508
Iteration 12/25 | Loss: 0.00077508
Iteration 13/25 | Loss: 0.00077508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007750789518468082, 0.0007750789518468082, 0.0007750789518468082, 0.0007750789518468082, 0.0007750789518468082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007750789518468082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50337172
Iteration 2/25 | Loss: 0.00076279
Iteration 3/25 | Loss: 0.00076279
Iteration 4/25 | Loss: 0.00076279
Iteration 5/25 | Loss: 0.00076279
Iteration 6/25 | Loss: 0.00076279
Iteration 7/25 | Loss: 0.00076279
Iteration 8/25 | Loss: 0.00076279
Iteration 9/25 | Loss: 0.00076279
Iteration 10/25 | Loss: 0.00076278
Iteration 11/25 | Loss: 0.00076278
Iteration 12/25 | Loss: 0.00076278
Iteration 13/25 | Loss: 0.00076278
Iteration 14/25 | Loss: 0.00076278
Iteration 15/25 | Loss: 0.00076278
Iteration 16/25 | Loss: 0.00076278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007627845625393093, 0.0007627845625393093, 0.0007627845625393093, 0.0007627845625393093, 0.0007627845625393093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007627845625393093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076278
Iteration 2/1000 | Loss: 0.00003769
Iteration 3/1000 | Loss: 0.00002880
Iteration 4/1000 | Loss: 0.00002660
Iteration 5/1000 | Loss: 0.00002518
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00002309
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002266
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002251
Iteration 13/1000 | Loss: 0.00002248
Iteration 14/1000 | Loss: 0.00002247
Iteration 15/1000 | Loss: 0.00002246
Iteration 16/1000 | Loss: 0.00002246
Iteration 17/1000 | Loss: 0.00002242
Iteration 18/1000 | Loss: 0.00002241
Iteration 19/1000 | Loss: 0.00002239
Iteration 20/1000 | Loss: 0.00002239
Iteration 21/1000 | Loss: 0.00002238
Iteration 22/1000 | Loss: 0.00002238
Iteration 23/1000 | Loss: 0.00002237
Iteration 24/1000 | Loss: 0.00002235
Iteration 25/1000 | Loss: 0.00002235
Iteration 26/1000 | Loss: 0.00002235
Iteration 27/1000 | Loss: 0.00002235
Iteration 28/1000 | Loss: 0.00002235
Iteration 29/1000 | Loss: 0.00002234
Iteration 30/1000 | Loss: 0.00002234
Iteration 31/1000 | Loss: 0.00002231
Iteration 32/1000 | Loss: 0.00002231
Iteration 33/1000 | Loss: 0.00002230
Iteration 34/1000 | Loss: 0.00002230
Iteration 35/1000 | Loss: 0.00002229
Iteration 36/1000 | Loss: 0.00002227
Iteration 37/1000 | Loss: 0.00002227
Iteration 38/1000 | Loss: 0.00002227
Iteration 39/1000 | Loss: 0.00002226
Iteration 40/1000 | Loss: 0.00002224
Iteration 41/1000 | Loss: 0.00002224
Iteration 42/1000 | Loss: 0.00002224
Iteration 43/1000 | Loss: 0.00002223
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00002223
Iteration 46/1000 | Loss: 0.00002222
Iteration 47/1000 | Loss: 0.00002221
Iteration 48/1000 | Loss: 0.00002221
Iteration 49/1000 | Loss: 0.00002220
Iteration 50/1000 | Loss: 0.00002220
Iteration 51/1000 | Loss: 0.00002219
Iteration 52/1000 | Loss: 0.00002219
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002219
Iteration 55/1000 | Loss: 0.00002218
Iteration 56/1000 | Loss: 0.00002218
Iteration 57/1000 | Loss: 0.00002218
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002218
Iteration 60/1000 | Loss: 0.00002218
Iteration 61/1000 | Loss: 0.00002217
Iteration 62/1000 | Loss: 0.00002217
Iteration 63/1000 | Loss: 0.00002217
Iteration 64/1000 | Loss: 0.00002216
Iteration 65/1000 | Loss: 0.00002216
Iteration 66/1000 | Loss: 0.00002216
Iteration 67/1000 | Loss: 0.00002216
Iteration 68/1000 | Loss: 0.00002216
Iteration 69/1000 | Loss: 0.00002215
Iteration 70/1000 | Loss: 0.00002215
Iteration 71/1000 | Loss: 0.00002215
Iteration 72/1000 | Loss: 0.00002215
Iteration 73/1000 | Loss: 0.00002215
Iteration 74/1000 | Loss: 0.00002215
Iteration 75/1000 | Loss: 0.00002215
Iteration 76/1000 | Loss: 0.00002215
Iteration 77/1000 | Loss: 0.00002215
Iteration 78/1000 | Loss: 0.00002215
Iteration 79/1000 | Loss: 0.00002214
Iteration 80/1000 | Loss: 0.00002214
Iteration 81/1000 | Loss: 0.00002214
Iteration 82/1000 | Loss: 0.00002214
Iteration 83/1000 | Loss: 0.00002214
Iteration 84/1000 | Loss: 0.00002214
Iteration 85/1000 | Loss: 0.00002214
Iteration 86/1000 | Loss: 0.00002213
Iteration 87/1000 | Loss: 0.00002213
Iteration 88/1000 | Loss: 0.00002213
Iteration 89/1000 | Loss: 0.00002213
Iteration 90/1000 | Loss: 0.00002213
Iteration 91/1000 | Loss: 0.00002213
Iteration 92/1000 | Loss: 0.00002213
Iteration 93/1000 | Loss: 0.00002213
Iteration 94/1000 | Loss: 0.00002213
Iteration 95/1000 | Loss: 0.00002213
Iteration 96/1000 | Loss: 0.00002213
Iteration 97/1000 | Loss: 0.00002213
Iteration 98/1000 | Loss: 0.00002213
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00002213
Iteration 101/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.212546496593859e-05, 2.212546496593859e-05, 2.212546496593859e-05, 2.212546496593859e-05, 2.212546496593859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.212546496593859e-05

Optimization complete. Final v2v error: 4.031414031982422 mm

Highest mean error: 4.612661361694336 mm for frame 115

Lowest mean error: 3.4315333366394043 mm for frame 201

Saving results

Total time: 39.67038345336914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836698
Iteration 2/25 | Loss: 0.00197099
Iteration 3/25 | Loss: 0.00110859
Iteration 4/25 | Loss: 0.00094958
Iteration 5/25 | Loss: 0.00094861
Iteration 6/25 | Loss: 0.00085767
Iteration 7/25 | Loss: 0.00086180
Iteration 8/25 | Loss: 0.00085495
Iteration 9/25 | Loss: 0.00084993
Iteration 10/25 | Loss: 0.00083059
Iteration 11/25 | Loss: 0.00082878
Iteration 12/25 | Loss: 0.00080643
Iteration 13/25 | Loss: 0.00080589
Iteration 14/25 | Loss: 0.00080115
Iteration 15/25 | Loss: 0.00080600
Iteration 16/25 | Loss: 0.00080124
Iteration 17/25 | Loss: 0.00079682
Iteration 18/25 | Loss: 0.00079640
Iteration 19/25 | Loss: 0.00079626
Iteration 20/25 | Loss: 0.00079625
Iteration 21/25 | Loss: 0.00079625
Iteration 22/25 | Loss: 0.00079625
Iteration 23/25 | Loss: 0.00079625
Iteration 24/25 | Loss: 0.00079625
Iteration 25/25 | Loss: 0.00079625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54975319
Iteration 2/25 | Loss: 0.00075761
Iteration 3/25 | Loss: 0.00075761
Iteration 4/25 | Loss: 0.00075761
Iteration 5/25 | Loss: 0.00075761
Iteration 6/25 | Loss: 0.00075761
Iteration 7/25 | Loss: 0.00075761
Iteration 8/25 | Loss: 0.00075761
Iteration 9/25 | Loss: 0.00075761
Iteration 10/25 | Loss: 0.00075760
Iteration 11/25 | Loss: 0.00069476
Iteration 12/25 | Loss: 0.00069476
Iteration 13/25 | Loss: 0.00069476
Iteration 14/25 | Loss: 0.00069476
Iteration 15/25 | Loss: 0.00069476
Iteration 16/25 | Loss: 0.00069476
Iteration 17/25 | Loss: 0.00069476
Iteration 18/25 | Loss: 0.00069476
Iteration 19/25 | Loss: 0.00069476
Iteration 20/25 | Loss: 0.00069476
Iteration 21/25 | Loss: 0.00069476
Iteration 22/25 | Loss: 0.00069476
Iteration 23/25 | Loss: 0.00069476
Iteration 24/25 | Loss: 0.00069476
Iteration 25/25 | Loss: 0.00069476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069476
Iteration 2/1000 | Loss: 0.00003932
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002434
Iteration 6/1000 | Loss: 0.00002341
Iteration 7/1000 | Loss: 0.00002269
Iteration 8/1000 | Loss: 0.00002231
Iteration 9/1000 | Loss: 0.00002208
Iteration 10/1000 | Loss: 0.00002198
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002183
Iteration 13/1000 | Loss: 0.00002182
Iteration 14/1000 | Loss: 0.00002181
Iteration 15/1000 | Loss: 0.00002174
Iteration 16/1000 | Loss: 0.00002172
Iteration 17/1000 | Loss: 0.00002171
Iteration 18/1000 | Loss: 0.00002169
Iteration 19/1000 | Loss: 0.00002168
Iteration 20/1000 | Loss: 0.00002167
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002166
Iteration 23/1000 | Loss: 0.00002164
Iteration 24/1000 | Loss: 0.00002164
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002162
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002161
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002160
Iteration 33/1000 | Loss: 0.00002159
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002159
Iteration 36/1000 | Loss: 0.00002159
Iteration 37/1000 | Loss: 0.00002159
Iteration 38/1000 | Loss: 0.00002158
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002158
Iteration 41/1000 | Loss: 0.00002158
Iteration 42/1000 | Loss: 0.00002157
Iteration 43/1000 | Loss: 0.00002157
Iteration 44/1000 | Loss: 0.00002157
Iteration 45/1000 | Loss: 0.00002156
Iteration 46/1000 | Loss: 0.00002156
Iteration 47/1000 | Loss: 0.00002156
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002155
Iteration 50/1000 | Loss: 0.00002155
Iteration 51/1000 | Loss: 0.00002154
Iteration 52/1000 | Loss: 0.00002154
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002154
Iteration 55/1000 | Loss: 0.00002153
Iteration 56/1000 | Loss: 0.00002152
Iteration 57/1000 | Loss: 0.00002152
Iteration 58/1000 | Loss: 0.00002152
Iteration 59/1000 | Loss: 0.00002152
Iteration 60/1000 | Loss: 0.00002152
Iteration 61/1000 | Loss: 0.00002152
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002151
Iteration 64/1000 | Loss: 0.00002151
Iteration 65/1000 | Loss: 0.00002151
Iteration 66/1000 | Loss: 0.00002150
Iteration 67/1000 | Loss: 0.00002150
Iteration 68/1000 | Loss: 0.00002149
Iteration 69/1000 | Loss: 0.00002149
Iteration 70/1000 | Loss: 0.00002149
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002148
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002146
Iteration 80/1000 | Loss: 0.00002146
Iteration 81/1000 | Loss: 0.00002146
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002145
Iteration 87/1000 | Loss: 0.00002145
Iteration 88/1000 | Loss: 0.00002145
Iteration 89/1000 | Loss: 0.00002144
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002144
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002144
Iteration 99/1000 | Loss: 0.00002144
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00002143
Iteration 105/1000 | Loss: 0.00002143
Iteration 106/1000 | Loss: 0.00002143
Iteration 107/1000 | Loss: 0.00002143
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002143
Iteration 113/1000 | Loss: 0.00002143
Iteration 114/1000 | Loss: 0.00002143
Iteration 115/1000 | Loss: 0.00002142
Iteration 116/1000 | Loss: 0.00002142
Iteration 117/1000 | Loss: 0.00002142
Iteration 118/1000 | Loss: 0.00002142
Iteration 119/1000 | Loss: 0.00002142
Iteration 120/1000 | Loss: 0.00002142
Iteration 121/1000 | Loss: 0.00002142
Iteration 122/1000 | Loss: 0.00002142
Iteration 123/1000 | Loss: 0.00002142
Iteration 124/1000 | Loss: 0.00002142
Iteration 125/1000 | Loss: 0.00002142
Iteration 126/1000 | Loss: 0.00002142
Iteration 127/1000 | Loss: 0.00002142
Iteration 128/1000 | Loss: 0.00002142
Iteration 129/1000 | Loss: 0.00002142
Iteration 130/1000 | Loss: 0.00002142
Iteration 131/1000 | Loss: 0.00002142
Iteration 132/1000 | Loss: 0.00002142
Iteration 133/1000 | Loss: 0.00002142
Iteration 134/1000 | Loss: 0.00002142
Iteration 135/1000 | Loss: 0.00002142
Iteration 136/1000 | Loss: 0.00002142
Iteration 137/1000 | Loss: 0.00002142
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.141601544281002e-05, 2.141601544281002e-05, 2.141601544281002e-05, 2.141601544281002e-05, 2.141601544281002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.141601544281002e-05

Optimization complete. Final v2v error: 3.967867136001587 mm

Highest mean error: 4.506010055541992 mm for frame 83

Lowest mean error: 3.4371871948242188 mm for frame 137

Saving results

Total time: 63.205185413360596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515412
Iteration 2/25 | Loss: 0.00111645
Iteration 3/25 | Loss: 0.00090570
Iteration 4/25 | Loss: 0.00086654
Iteration 5/25 | Loss: 0.00085497
Iteration 6/25 | Loss: 0.00085176
Iteration 7/25 | Loss: 0.00085079
Iteration 8/25 | Loss: 0.00085079
Iteration 9/25 | Loss: 0.00085079
Iteration 10/25 | Loss: 0.00085079
Iteration 11/25 | Loss: 0.00085079
Iteration 12/25 | Loss: 0.00085079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008507936145178974, 0.0008507936145178974, 0.0008507936145178974, 0.0008507936145178974, 0.0008507936145178974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008507936145178974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83957529
Iteration 2/25 | Loss: 0.00082417
Iteration 3/25 | Loss: 0.00082417
Iteration 4/25 | Loss: 0.00082417
Iteration 5/25 | Loss: 0.00082417
Iteration 6/25 | Loss: 0.00082417
Iteration 7/25 | Loss: 0.00082417
Iteration 8/25 | Loss: 0.00082417
Iteration 9/25 | Loss: 0.00082417
Iteration 10/25 | Loss: 0.00082417
Iteration 11/25 | Loss: 0.00082417
Iteration 12/25 | Loss: 0.00082417
Iteration 13/25 | Loss: 0.00082417
Iteration 14/25 | Loss: 0.00082417
Iteration 15/25 | Loss: 0.00082417
Iteration 16/25 | Loss: 0.00082417
Iteration 17/25 | Loss: 0.00082417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008241687901318073, 0.0008241687901318073, 0.0008241687901318073, 0.0008241687901318073, 0.0008241687901318073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008241687901318073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082417
Iteration 2/1000 | Loss: 0.00004638
Iteration 3/1000 | Loss: 0.00003380
Iteration 4/1000 | Loss: 0.00003134
Iteration 5/1000 | Loss: 0.00003008
Iteration 6/1000 | Loss: 0.00002923
Iteration 7/1000 | Loss: 0.00002840
Iteration 8/1000 | Loss: 0.00002779
Iteration 9/1000 | Loss: 0.00002737
Iteration 10/1000 | Loss: 0.00002701
Iteration 11/1000 | Loss: 0.00002675
Iteration 12/1000 | Loss: 0.00002662
Iteration 13/1000 | Loss: 0.00002662
Iteration 14/1000 | Loss: 0.00002661
Iteration 15/1000 | Loss: 0.00002660
Iteration 16/1000 | Loss: 0.00002660
Iteration 17/1000 | Loss: 0.00002647
Iteration 18/1000 | Loss: 0.00002645
Iteration 19/1000 | Loss: 0.00002638
Iteration 20/1000 | Loss: 0.00002632
Iteration 21/1000 | Loss: 0.00002627
Iteration 22/1000 | Loss: 0.00002626
Iteration 23/1000 | Loss: 0.00002625
Iteration 24/1000 | Loss: 0.00002625
Iteration 25/1000 | Loss: 0.00002625
Iteration 26/1000 | Loss: 0.00002625
Iteration 27/1000 | Loss: 0.00002625
Iteration 28/1000 | Loss: 0.00002624
Iteration 29/1000 | Loss: 0.00002624
Iteration 30/1000 | Loss: 0.00002624
Iteration 31/1000 | Loss: 0.00002624
Iteration 32/1000 | Loss: 0.00002624
Iteration 33/1000 | Loss: 0.00002624
Iteration 34/1000 | Loss: 0.00002624
Iteration 35/1000 | Loss: 0.00002624
Iteration 36/1000 | Loss: 0.00002623
Iteration 37/1000 | Loss: 0.00002623
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002617
Iteration 40/1000 | Loss: 0.00002617
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002616
Iteration 43/1000 | Loss: 0.00002615
Iteration 44/1000 | Loss: 0.00002614
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002614
Iteration 47/1000 | Loss: 0.00002613
Iteration 48/1000 | Loss: 0.00002613
Iteration 49/1000 | Loss: 0.00002613
Iteration 50/1000 | Loss: 0.00002613
Iteration 51/1000 | Loss: 0.00002613
Iteration 52/1000 | Loss: 0.00002612
Iteration 53/1000 | Loss: 0.00002612
Iteration 54/1000 | Loss: 0.00002612
Iteration 55/1000 | Loss: 0.00002611
Iteration 56/1000 | Loss: 0.00002611
Iteration 57/1000 | Loss: 0.00002611
Iteration 58/1000 | Loss: 0.00002611
Iteration 59/1000 | Loss: 0.00002611
Iteration 60/1000 | Loss: 0.00002610
Iteration 61/1000 | Loss: 0.00002610
Iteration 62/1000 | Loss: 0.00002610
Iteration 63/1000 | Loss: 0.00002610
Iteration 64/1000 | Loss: 0.00002610
Iteration 65/1000 | Loss: 0.00002609
Iteration 66/1000 | Loss: 0.00002609
Iteration 67/1000 | Loss: 0.00002609
Iteration 68/1000 | Loss: 0.00002609
Iteration 69/1000 | Loss: 0.00002609
Iteration 70/1000 | Loss: 0.00002608
Iteration 71/1000 | Loss: 0.00002608
Iteration 72/1000 | Loss: 0.00002608
Iteration 73/1000 | Loss: 0.00002608
Iteration 74/1000 | Loss: 0.00002608
Iteration 75/1000 | Loss: 0.00002608
Iteration 76/1000 | Loss: 0.00002608
Iteration 77/1000 | Loss: 0.00002608
Iteration 78/1000 | Loss: 0.00002607
Iteration 79/1000 | Loss: 0.00002607
Iteration 80/1000 | Loss: 0.00002607
Iteration 81/1000 | Loss: 0.00002607
Iteration 82/1000 | Loss: 0.00002607
Iteration 83/1000 | Loss: 0.00002607
Iteration 84/1000 | Loss: 0.00002607
Iteration 85/1000 | Loss: 0.00002607
Iteration 86/1000 | Loss: 0.00002607
Iteration 87/1000 | Loss: 0.00002607
Iteration 88/1000 | Loss: 0.00002607
Iteration 89/1000 | Loss: 0.00002606
Iteration 90/1000 | Loss: 0.00002606
Iteration 91/1000 | Loss: 0.00002606
Iteration 92/1000 | Loss: 0.00002606
Iteration 93/1000 | Loss: 0.00002606
Iteration 94/1000 | Loss: 0.00002606
Iteration 95/1000 | Loss: 0.00002605
Iteration 96/1000 | Loss: 0.00002605
Iteration 97/1000 | Loss: 0.00002605
Iteration 98/1000 | Loss: 0.00002605
Iteration 99/1000 | Loss: 0.00002605
Iteration 100/1000 | Loss: 0.00002605
Iteration 101/1000 | Loss: 0.00002605
Iteration 102/1000 | Loss: 0.00002605
Iteration 103/1000 | Loss: 0.00002604
Iteration 104/1000 | Loss: 0.00002604
Iteration 105/1000 | Loss: 0.00002604
Iteration 106/1000 | Loss: 0.00002604
Iteration 107/1000 | Loss: 0.00002603
Iteration 108/1000 | Loss: 0.00002603
Iteration 109/1000 | Loss: 0.00002603
Iteration 110/1000 | Loss: 0.00002603
Iteration 111/1000 | Loss: 0.00002603
Iteration 112/1000 | Loss: 0.00002603
Iteration 113/1000 | Loss: 0.00002603
Iteration 114/1000 | Loss: 0.00002603
Iteration 115/1000 | Loss: 0.00002602
Iteration 116/1000 | Loss: 0.00002602
Iteration 117/1000 | Loss: 0.00002602
Iteration 118/1000 | Loss: 0.00002602
Iteration 119/1000 | Loss: 0.00002602
Iteration 120/1000 | Loss: 0.00002602
Iteration 121/1000 | Loss: 0.00002602
Iteration 122/1000 | Loss: 0.00002602
Iteration 123/1000 | Loss: 0.00002602
Iteration 124/1000 | Loss: 0.00002602
Iteration 125/1000 | Loss: 0.00002601
Iteration 126/1000 | Loss: 0.00002601
Iteration 127/1000 | Loss: 0.00002601
Iteration 128/1000 | Loss: 0.00002601
Iteration 129/1000 | Loss: 0.00002601
Iteration 130/1000 | Loss: 0.00002601
Iteration 131/1000 | Loss: 0.00002601
Iteration 132/1000 | Loss: 0.00002600
Iteration 133/1000 | Loss: 0.00002600
Iteration 134/1000 | Loss: 0.00002600
Iteration 135/1000 | Loss: 0.00002600
Iteration 136/1000 | Loss: 0.00002600
Iteration 137/1000 | Loss: 0.00002600
Iteration 138/1000 | Loss: 0.00002600
Iteration 139/1000 | Loss: 0.00002600
Iteration 140/1000 | Loss: 0.00002600
Iteration 141/1000 | Loss: 0.00002600
Iteration 142/1000 | Loss: 0.00002600
Iteration 143/1000 | Loss: 0.00002600
Iteration 144/1000 | Loss: 0.00002600
Iteration 145/1000 | Loss: 0.00002600
Iteration 146/1000 | Loss: 0.00002600
Iteration 147/1000 | Loss: 0.00002600
Iteration 148/1000 | Loss: 0.00002600
Iteration 149/1000 | Loss: 0.00002600
Iteration 150/1000 | Loss: 0.00002600
Iteration 151/1000 | Loss: 0.00002600
Iteration 152/1000 | Loss: 0.00002600
Iteration 153/1000 | Loss: 0.00002600
Iteration 154/1000 | Loss: 0.00002600
Iteration 155/1000 | Loss: 0.00002600
Iteration 156/1000 | Loss: 0.00002600
Iteration 157/1000 | Loss: 0.00002600
Iteration 158/1000 | Loss: 0.00002600
Iteration 159/1000 | Loss: 0.00002600
Iteration 160/1000 | Loss: 0.00002600
Iteration 161/1000 | Loss: 0.00002600
Iteration 162/1000 | Loss: 0.00002600
Iteration 163/1000 | Loss: 0.00002600
Iteration 164/1000 | Loss: 0.00002600
Iteration 165/1000 | Loss: 0.00002600
Iteration 166/1000 | Loss: 0.00002600
Iteration 167/1000 | Loss: 0.00002600
Iteration 168/1000 | Loss: 0.00002600
Iteration 169/1000 | Loss: 0.00002600
Iteration 170/1000 | Loss: 0.00002600
Iteration 171/1000 | Loss: 0.00002600
Iteration 172/1000 | Loss: 0.00002600
Iteration 173/1000 | Loss: 0.00002600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.5995768737629987e-05, 2.5995768737629987e-05, 2.5995768737629987e-05, 2.5995768737629987e-05, 2.5995768737629987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5995768737629987e-05

Optimization complete. Final v2v error: 4.368084907531738 mm

Highest mean error: 5.037281513214111 mm for frame 9

Lowest mean error: 4.135432243347168 mm for frame 79

Saving results

Total time: 47.42887830734253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457413
Iteration 2/25 | Loss: 0.00116652
Iteration 3/25 | Loss: 0.00081253
Iteration 4/25 | Loss: 0.00076878
Iteration 5/25 | Loss: 0.00076077
Iteration 6/25 | Loss: 0.00075862
Iteration 7/25 | Loss: 0.00075804
Iteration 8/25 | Loss: 0.00075802
Iteration 9/25 | Loss: 0.00075802
Iteration 10/25 | Loss: 0.00075802
Iteration 11/25 | Loss: 0.00075802
Iteration 12/25 | Loss: 0.00075802
Iteration 13/25 | Loss: 0.00075802
Iteration 14/25 | Loss: 0.00075802
Iteration 15/25 | Loss: 0.00075802
Iteration 16/25 | Loss: 0.00075802
Iteration 17/25 | Loss: 0.00075802
Iteration 18/25 | Loss: 0.00075802
Iteration 19/25 | Loss: 0.00075802
Iteration 20/25 | Loss: 0.00075802
Iteration 21/25 | Loss: 0.00075802
Iteration 22/25 | Loss: 0.00075802
Iteration 23/25 | Loss: 0.00075802
Iteration 24/25 | Loss: 0.00075802
Iteration 25/25 | Loss: 0.00075802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60612631
Iteration 2/25 | Loss: 0.00089867
Iteration 3/25 | Loss: 0.00089866
Iteration 4/25 | Loss: 0.00089866
Iteration 5/25 | Loss: 0.00089866
Iteration 6/25 | Loss: 0.00089866
Iteration 7/25 | Loss: 0.00089865
Iteration 8/25 | Loss: 0.00089865
Iteration 9/25 | Loss: 0.00089865
Iteration 10/25 | Loss: 0.00089865
Iteration 11/25 | Loss: 0.00089865
Iteration 12/25 | Loss: 0.00089865
Iteration 13/25 | Loss: 0.00089865
Iteration 14/25 | Loss: 0.00089865
Iteration 15/25 | Loss: 0.00089865
Iteration 16/25 | Loss: 0.00089865
Iteration 17/25 | Loss: 0.00089865
Iteration 18/25 | Loss: 0.00089865
Iteration 19/25 | Loss: 0.00089865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008986538159660995, 0.0008986538159660995, 0.0008986538159660995, 0.0008986538159660995, 0.0008986538159660995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008986538159660995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089865
Iteration 2/1000 | Loss: 0.00003362
Iteration 3/1000 | Loss: 0.00002455
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00001996
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001786
Iteration 15/1000 | Loss: 0.00001785
Iteration 16/1000 | Loss: 0.00001784
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001783
Iteration 19/1000 | Loss: 0.00001780
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001774
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001769
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001765
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001764
Iteration 34/1000 | Loss: 0.00001763
Iteration 35/1000 | Loss: 0.00001761
Iteration 36/1000 | Loss: 0.00001761
Iteration 37/1000 | Loss: 0.00001761
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001760
Iteration 40/1000 | Loss: 0.00001760
Iteration 41/1000 | Loss: 0.00001760
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001759
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001758
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001757
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001756
Iteration 54/1000 | Loss: 0.00001756
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001756
Iteration 59/1000 | Loss: 0.00001756
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001755
Iteration 63/1000 | Loss: 0.00001755
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001754
Iteration 66/1000 | Loss: 0.00001754
Iteration 67/1000 | Loss: 0.00001754
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001750
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001750
Iteration 93/1000 | Loss: 0.00001750
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001750
Iteration 96/1000 | Loss: 0.00001750
Iteration 97/1000 | Loss: 0.00001750
Iteration 98/1000 | Loss: 0.00001750
Iteration 99/1000 | Loss: 0.00001750
Iteration 100/1000 | Loss: 0.00001750
Iteration 101/1000 | Loss: 0.00001750
Iteration 102/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.749549301166553e-05, 1.749549301166553e-05, 1.749549301166553e-05, 1.749549301166553e-05, 1.749549301166553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.749549301166553e-05

Optimization complete. Final v2v error: 3.628988027572632 mm

Highest mean error: 4.2330217361450195 mm for frame 36

Lowest mean error: 3.123814344406128 mm for frame 122

Saving results

Total time: 33.18130922317505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164264
Iteration 2/25 | Loss: 0.00261460
Iteration 3/25 | Loss: 0.00180808
Iteration 4/25 | Loss: 0.00177493
Iteration 5/25 | Loss: 0.00138074
Iteration 6/25 | Loss: 0.00104712
Iteration 7/25 | Loss: 0.00096790
Iteration 8/25 | Loss: 0.00093782
Iteration 9/25 | Loss: 0.00092258
Iteration 10/25 | Loss: 0.00092105
Iteration 11/25 | Loss: 0.00092791
Iteration 12/25 | Loss: 0.00093487
Iteration 13/25 | Loss: 0.00093315
Iteration 14/25 | Loss: 0.00091632
Iteration 15/25 | Loss: 0.00090297
Iteration 16/25 | Loss: 0.00090476
Iteration 17/25 | Loss: 0.00090161
Iteration 18/25 | Loss: 0.00089441
Iteration 19/25 | Loss: 0.00089195
Iteration 20/25 | Loss: 0.00089067
Iteration 21/25 | Loss: 0.00088984
Iteration 22/25 | Loss: 0.00088688
Iteration 23/25 | Loss: 0.00088751
Iteration 24/25 | Loss: 0.00088520
Iteration 25/25 | Loss: 0.00088490

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45947683
Iteration 2/25 | Loss: 0.00067905
Iteration 3/25 | Loss: 0.00067905
Iteration 4/25 | Loss: 0.00067905
Iteration 5/25 | Loss: 0.00067905
Iteration 6/25 | Loss: 0.00067905
Iteration 7/25 | Loss: 0.00067905
Iteration 8/25 | Loss: 0.00067905
Iteration 9/25 | Loss: 0.00067905
Iteration 10/25 | Loss: 0.00067905
Iteration 11/25 | Loss: 0.00067905
Iteration 12/25 | Loss: 0.00067905
Iteration 13/25 | Loss: 0.00067905
Iteration 14/25 | Loss: 0.00067905
Iteration 15/25 | Loss: 0.00067905
Iteration 16/25 | Loss: 0.00067905
Iteration 17/25 | Loss: 0.00067905
Iteration 18/25 | Loss: 0.00067905
Iteration 19/25 | Loss: 0.00067905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006790453335270286, 0.0006790453335270286, 0.0006790453335270286, 0.0006790453335270286, 0.0006790453335270286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006790453335270286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067905
Iteration 2/1000 | Loss: 0.00004519
Iteration 3/1000 | Loss: 0.00002978
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002589
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00002437
Iteration 8/1000 | Loss: 0.00002388
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00013131
Iteration 11/1000 | Loss: 0.00003132
Iteration 12/1000 | Loss: 0.00002699
Iteration 13/1000 | Loss: 0.00002599
Iteration 14/1000 | Loss: 0.00002525
Iteration 15/1000 | Loss: 0.00018039
Iteration 16/1000 | Loss: 0.00003099
Iteration 17/1000 | Loss: 0.00002716
Iteration 18/1000 | Loss: 0.00002514
Iteration 19/1000 | Loss: 0.00002428
Iteration 20/1000 | Loss: 0.00002372
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002312
Iteration 23/1000 | Loss: 0.00002293
Iteration 24/1000 | Loss: 0.00002290
Iteration 25/1000 | Loss: 0.00002287
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00002284
Iteration 28/1000 | Loss: 0.00002283
Iteration 29/1000 | Loss: 0.00003269
Iteration 30/1000 | Loss: 0.00002923
Iteration 31/1000 | Loss: 0.00003238
Iteration 32/1000 | Loss: 0.00002683
Iteration 33/1000 | Loss: 0.00003086
Iteration 34/1000 | Loss: 0.00002779
Iteration 35/1000 | Loss: 0.00003072
Iteration 36/1000 | Loss: 0.00002356
Iteration 37/1000 | Loss: 0.00002300
Iteration 38/1000 | Loss: 0.00002280
Iteration 39/1000 | Loss: 0.00002263
Iteration 40/1000 | Loss: 0.00002259
Iteration 41/1000 | Loss: 0.00002257
Iteration 42/1000 | Loss: 0.00002257
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002255
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00002254
Iteration 52/1000 | Loss: 0.00002254
Iteration 53/1000 | Loss: 0.00002254
Iteration 54/1000 | Loss: 0.00002254
Iteration 55/1000 | Loss: 0.00002254
Iteration 56/1000 | Loss: 0.00002254
Iteration 57/1000 | Loss: 0.00002254
Iteration 58/1000 | Loss: 0.00002253
Iteration 59/1000 | Loss: 0.00002253
Iteration 60/1000 | Loss: 0.00002253
Iteration 61/1000 | Loss: 0.00002253
Iteration 62/1000 | Loss: 0.00002253
Iteration 63/1000 | Loss: 0.00002253
Iteration 64/1000 | Loss: 0.00002253
Iteration 65/1000 | Loss: 0.00002253
Iteration 66/1000 | Loss: 0.00002253
Iteration 67/1000 | Loss: 0.00002253
Iteration 68/1000 | Loss: 0.00002253
Iteration 69/1000 | Loss: 0.00002253
Iteration 70/1000 | Loss: 0.00002252
Iteration 71/1000 | Loss: 0.00002252
Iteration 72/1000 | Loss: 0.00002252
Iteration 73/1000 | Loss: 0.00002252
Iteration 74/1000 | Loss: 0.00002252
Iteration 75/1000 | Loss: 0.00002252
Iteration 76/1000 | Loss: 0.00002252
Iteration 77/1000 | Loss: 0.00002252
Iteration 78/1000 | Loss: 0.00002252
Iteration 79/1000 | Loss: 0.00002252
Iteration 80/1000 | Loss: 0.00002252
Iteration 81/1000 | Loss: 0.00002252
Iteration 82/1000 | Loss: 0.00002252
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002251
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00002251
Iteration 88/1000 | Loss: 0.00002251
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002250
Iteration 91/1000 | Loss: 0.00002250
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002248
Iteration 100/1000 | Loss: 0.00002248
Iteration 101/1000 | Loss: 0.00002248
Iteration 102/1000 | Loss: 0.00002248
Iteration 103/1000 | Loss: 0.00002248
Iteration 104/1000 | Loss: 0.00002248
Iteration 105/1000 | Loss: 0.00002248
Iteration 106/1000 | Loss: 0.00002248
Iteration 107/1000 | Loss: 0.00002248
Iteration 108/1000 | Loss: 0.00002248
Iteration 109/1000 | Loss: 0.00002247
Iteration 110/1000 | Loss: 0.00002247
Iteration 111/1000 | Loss: 0.00002247
Iteration 112/1000 | Loss: 0.00002247
Iteration 113/1000 | Loss: 0.00002247
Iteration 114/1000 | Loss: 0.00002247
Iteration 115/1000 | Loss: 0.00002247
Iteration 116/1000 | Loss: 0.00002247
Iteration 117/1000 | Loss: 0.00002247
Iteration 118/1000 | Loss: 0.00002247
Iteration 119/1000 | Loss: 0.00002247
Iteration 120/1000 | Loss: 0.00002247
Iteration 121/1000 | Loss: 0.00002247
Iteration 122/1000 | Loss: 0.00002247
Iteration 123/1000 | Loss: 0.00002247
Iteration 124/1000 | Loss: 0.00002247
Iteration 125/1000 | Loss: 0.00002247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.2469217583420686e-05, 2.2469217583420686e-05, 2.2469217583420686e-05, 2.2469217583420686e-05, 2.2469217583420686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2469217583420686e-05

Optimization complete. Final v2v error: 3.9493978023529053 mm

Highest mean error: 5.474276065826416 mm for frame 66

Lowest mean error: 3.4821369647979736 mm for frame 218

Saving results

Total time: 108.78529214859009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006265
Iteration 2/25 | Loss: 0.00189206
Iteration 3/25 | Loss: 0.00137476
Iteration 4/25 | Loss: 0.00125228
Iteration 5/25 | Loss: 0.00132304
Iteration 6/25 | Loss: 0.00117650
Iteration 7/25 | Loss: 0.00115395
Iteration 8/25 | Loss: 0.00112555
Iteration 9/25 | Loss: 0.00113695
Iteration 10/25 | Loss: 0.00112603
Iteration 11/25 | Loss: 0.00116391
Iteration 12/25 | Loss: 0.00114310
Iteration 13/25 | Loss: 0.00112193
Iteration 14/25 | Loss: 0.00112573
Iteration 15/25 | Loss: 0.00111970
Iteration 16/25 | Loss: 0.00114018
Iteration 17/25 | Loss: 0.00112664
Iteration 18/25 | Loss: 0.00110268
Iteration 19/25 | Loss: 0.00108494
Iteration 20/25 | Loss: 0.00108104
Iteration 21/25 | Loss: 0.00106794
Iteration 22/25 | Loss: 0.00106076
Iteration 23/25 | Loss: 0.00106398
Iteration 24/25 | Loss: 0.00104910
Iteration 25/25 | Loss: 0.00106737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82750869
Iteration 2/25 | Loss: 0.00177873
Iteration 3/25 | Loss: 0.00177858
Iteration 4/25 | Loss: 0.00177858
Iteration 5/25 | Loss: 0.00177858
Iteration 6/25 | Loss: 0.00177858
Iteration 7/25 | Loss: 0.00177858
Iteration 8/25 | Loss: 0.00177858
Iteration 9/25 | Loss: 0.00177858
Iteration 10/25 | Loss: 0.00177858
Iteration 11/25 | Loss: 0.00177858
Iteration 12/25 | Loss: 0.00177858
Iteration 13/25 | Loss: 0.00177858
Iteration 14/25 | Loss: 0.00177858
Iteration 15/25 | Loss: 0.00177858
Iteration 16/25 | Loss: 0.00177858
Iteration 17/25 | Loss: 0.00177858
Iteration 18/25 | Loss: 0.00177858
Iteration 19/25 | Loss: 0.00177858
Iteration 20/25 | Loss: 0.00177858
Iteration 21/25 | Loss: 0.00177858
Iteration 22/25 | Loss: 0.00177858
Iteration 23/25 | Loss: 0.00177858
Iteration 24/25 | Loss: 0.00177858
Iteration 25/25 | Loss: 0.00177858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177858
Iteration 2/1000 | Loss: 0.00210191
Iteration 3/1000 | Loss: 0.00958179
Iteration 4/1000 | Loss: 0.01208668
Iteration 5/1000 | Loss: 0.00625458
Iteration 6/1000 | Loss: 0.00327673
Iteration 7/1000 | Loss: 0.00276741
Iteration 8/1000 | Loss: 0.00359841
Iteration 9/1000 | Loss: 0.00147501
Iteration 10/1000 | Loss: 0.00470681
Iteration 11/1000 | Loss: 0.00233838
Iteration 12/1000 | Loss: 0.00294251
Iteration 13/1000 | Loss: 0.00248645
Iteration 14/1000 | Loss: 0.00190130
Iteration 15/1000 | Loss: 0.00310842
Iteration 16/1000 | Loss: 0.00171147
Iteration 17/1000 | Loss: 0.00491471
Iteration 18/1000 | Loss: 0.00137401
Iteration 19/1000 | Loss: 0.00074561
Iteration 20/1000 | Loss: 0.00207145
Iteration 21/1000 | Loss: 0.00204087
Iteration 22/1000 | Loss: 0.00188591
Iteration 23/1000 | Loss: 0.00302368
Iteration 24/1000 | Loss: 0.00158257
Iteration 25/1000 | Loss: 0.00116726
Iteration 26/1000 | Loss: 0.00111731
Iteration 27/1000 | Loss: 0.00158812
Iteration 28/1000 | Loss: 0.00174071
Iteration 29/1000 | Loss: 0.00331728
Iteration 30/1000 | Loss: 0.00322518
Iteration 31/1000 | Loss: 0.00210672
Iteration 32/1000 | Loss: 0.00104446
Iteration 33/1000 | Loss: 0.00126985
Iteration 34/1000 | Loss: 0.00209211
Iteration 35/1000 | Loss: 0.00268306
Iteration 36/1000 | Loss: 0.00260484
Iteration 37/1000 | Loss: 0.00809597
Iteration 38/1000 | Loss: 0.00187851
Iteration 39/1000 | Loss: 0.00404215
Iteration 40/1000 | Loss: 0.00261029
Iteration 41/1000 | Loss: 0.00249540
Iteration 42/1000 | Loss: 0.00555586
Iteration 43/1000 | Loss: 0.00487668
Iteration 44/1000 | Loss: 0.00258676
Iteration 45/1000 | Loss: 0.00346010
Iteration 46/1000 | Loss: 0.00171883
Iteration 47/1000 | Loss: 0.00147508
Iteration 48/1000 | Loss: 0.00102575
Iteration 49/1000 | Loss: 0.00482283
Iteration 50/1000 | Loss: 0.00077530
Iteration 51/1000 | Loss: 0.00008415
Iteration 52/1000 | Loss: 0.00006844
Iteration 53/1000 | Loss: 0.00006245
Iteration 54/1000 | Loss: 0.00005911
Iteration 55/1000 | Loss: 0.00005581
Iteration 56/1000 | Loss: 0.00005320
Iteration 57/1000 | Loss: 0.00005129
Iteration 58/1000 | Loss: 0.00127813
Iteration 59/1000 | Loss: 0.00026380
Iteration 60/1000 | Loss: 0.00005429
Iteration 61/1000 | Loss: 0.00076371
Iteration 62/1000 | Loss: 0.00031893
Iteration 63/1000 | Loss: 0.00078241
Iteration 64/1000 | Loss: 0.00009518
Iteration 65/1000 | Loss: 0.00006106
Iteration 66/1000 | Loss: 0.00006546
Iteration 67/1000 | Loss: 0.00005907
Iteration 68/1000 | Loss: 0.00231108
Iteration 69/1000 | Loss: 0.00148008
Iteration 70/1000 | Loss: 0.00087042
Iteration 71/1000 | Loss: 0.00125365
Iteration 72/1000 | Loss: 0.00033186
Iteration 73/1000 | Loss: 0.00005165
Iteration 74/1000 | Loss: 0.00005003
Iteration 75/1000 | Loss: 0.00142419
Iteration 76/1000 | Loss: 0.00184064
Iteration 77/1000 | Loss: 0.00287179
Iteration 78/1000 | Loss: 0.00220897
Iteration 79/1000 | Loss: 0.00080776
Iteration 80/1000 | Loss: 0.00222599
Iteration 81/1000 | Loss: 0.00159231
Iteration 82/1000 | Loss: 0.00010090
Iteration 83/1000 | Loss: 0.00006300
Iteration 84/1000 | Loss: 0.00005319
Iteration 85/1000 | Loss: 0.00163514
Iteration 86/1000 | Loss: 0.00239939
Iteration 87/1000 | Loss: 0.00296975
Iteration 88/1000 | Loss: 0.00317691
Iteration 89/1000 | Loss: 0.00124579
Iteration 90/1000 | Loss: 0.00066922
Iteration 91/1000 | Loss: 0.00186684
Iteration 92/1000 | Loss: 0.00295055
Iteration 93/1000 | Loss: 0.00296387
Iteration 94/1000 | Loss: 0.00345119
Iteration 95/1000 | Loss: 0.00543640
Iteration 96/1000 | Loss: 0.00023393
Iteration 97/1000 | Loss: 0.00268422
Iteration 98/1000 | Loss: 0.00323133
Iteration 99/1000 | Loss: 0.00322879
Iteration 100/1000 | Loss: 0.00419817
Iteration 101/1000 | Loss: 0.00462961
Iteration 102/1000 | Loss: 0.00298821
Iteration 103/1000 | Loss: 0.00010680
Iteration 104/1000 | Loss: 0.00006964
Iteration 105/1000 | Loss: 0.00238896
Iteration 106/1000 | Loss: 0.00031158
Iteration 107/1000 | Loss: 0.00005281
Iteration 108/1000 | Loss: 0.00132044
Iteration 109/1000 | Loss: 0.00137643
Iteration 110/1000 | Loss: 0.00175375
Iteration 111/1000 | Loss: 0.00124590
Iteration 112/1000 | Loss: 0.00099446
Iteration 113/1000 | Loss: 0.00034397
Iteration 114/1000 | Loss: 0.00152880
Iteration 115/1000 | Loss: 0.00008954
Iteration 116/1000 | Loss: 0.00005780
Iteration 117/1000 | Loss: 0.00114389
Iteration 118/1000 | Loss: 0.00031330
Iteration 119/1000 | Loss: 0.00005324
Iteration 120/1000 | Loss: 0.00005122
Iteration 121/1000 | Loss: 0.00016770
Iteration 122/1000 | Loss: 0.00005156
Iteration 123/1000 | Loss: 0.00004922
Iteration 124/1000 | Loss: 0.00004674
Iteration 125/1000 | Loss: 0.00004520
Iteration 126/1000 | Loss: 0.00097694
Iteration 127/1000 | Loss: 0.00027213
Iteration 128/1000 | Loss: 0.00006467
Iteration 129/1000 | Loss: 0.00004361
Iteration 130/1000 | Loss: 0.00108714
Iteration 131/1000 | Loss: 0.00026171
Iteration 132/1000 | Loss: 0.00004683
Iteration 133/1000 | Loss: 0.00006216
Iteration 134/1000 | Loss: 0.00004590
Iteration 135/1000 | Loss: 0.00004485
Iteration 136/1000 | Loss: 0.00132899
Iteration 137/1000 | Loss: 0.00066987
Iteration 138/1000 | Loss: 0.00114263
Iteration 139/1000 | Loss: 0.00063288
Iteration 140/1000 | Loss: 0.00004746
Iteration 141/1000 | Loss: 0.00129071
Iteration 142/1000 | Loss: 0.00049175
Iteration 143/1000 | Loss: 0.00007217
Iteration 144/1000 | Loss: 0.00005190
Iteration 145/1000 | Loss: 0.00075952
Iteration 146/1000 | Loss: 0.00094417
Iteration 147/1000 | Loss: 0.00007958
Iteration 148/1000 | Loss: 0.00005101
Iteration 149/1000 | Loss: 0.00004798
Iteration 150/1000 | Loss: 0.00107329
Iteration 151/1000 | Loss: 0.00033646
Iteration 152/1000 | Loss: 0.00004602
Iteration 153/1000 | Loss: 0.00004472
Iteration 154/1000 | Loss: 0.00006071
Iteration 155/1000 | Loss: 0.00006191
Iteration 156/1000 | Loss: 0.00006375
Iteration 157/1000 | Loss: 0.00004638
Iteration 158/1000 | Loss: 0.00004521
Iteration 159/1000 | Loss: 0.00129382
Iteration 160/1000 | Loss: 0.00020042
Iteration 161/1000 | Loss: 0.00008522
Iteration 162/1000 | Loss: 0.00068504
Iteration 163/1000 | Loss: 0.00031320
Iteration 164/1000 | Loss: 0.00005983
Iteration 165/1000 | Loss: 0.00005088
Iteration 166/1000 | Loss: 0.00028036
Iteration 167/1000 | Loss: 0.00037897
Iteration 168/1000 | Loss: 0.00026068
Iteration 169/1000 | Loss: 0.00037374
Iteration 170/1000 | Loss: 0.00021604
Iteration 171/1000 | Loss: 0.00032203
Iteration 172/1000 | Loss: 0.00043055
Iteration 173/1000 | Loss: 0.00076733
Iteration 174/1000 | Loss: 0.00020486
Iteration 175/1000 | Loss: 0.00046292
Iteration 176/1000 | Loss: 0.00017470
Iteration 177/1000 | Loss: 0.00025909
Iteration 178/1000 | Loss: 0.00019807
Iteration 179/1000 | Loss: 0.00029438
Iteration 180/1000 | Loss: 0.00041874
Iteration 181/1000 | Loss: 0.00023312
Iteration 182/1000 | Loss: 0.00050869
Iteration 183/1000 | Loss: 0.00037863
Iteration 184/1000 | Loss: 0.00032641
Iteration 185/1000 | Loss: 0.00036740
Iteration 186/1000 | Loss: 0.00020267
Iteration 187/1000 | Loss: 0.00034288
Iteration 188/1000 | Loss: 0.00025657
Iteration 189/1000 | Loss: 0.00035119
Iteration 190/1000 | Loss: 0.00050780
Iteration 191/1000 | Loss: 0.00051785
Iteration 192/1000 | Loss: 0.00016442
Iteration 193/1000 | Loss: 0.00006317
Iteration 194/1000 | Loss: 0.00057581
Iteration 195/1000 | Loss: 0.00031400
Iteration 196/1000 | Loss: 0.00053016
Iteration 197/1000 | Loss: 0.00022465
Iteration 198/1000 | Loss: 0.00044801
Iteration 199/1000 | Loss: 0.00030268
Iteration 200/1000 | Loss: 0.00005624
Iteration 201/1000 | Loss: 0.00004773
Iteration 202/1000 | Loss: 0.00018264
Iteration 203/1000 | Loss: 0.00146196
Iteration 204/1000 | Loss: 0.00024737
Iteration 205/1000 | Loss: 0.00005521
Iteration 206/1000 | Loss: 0.00005075
Iteration 207/1000 | Loss: 0.00004871
Iteration 208/1000 | Loss: 0.00004675
Iteration 209/1000 | Loss: 0.00004429
Iteration 210/1000 | Loss: 0.00004135
Iteration 211/1000 | Loss: 0.00005437
Iteration 212/1000 | Loss: 0.00004313
Iteration 213/1000 | Loss: 0.00004192
Iteration 214/1000 | Loss: 0.00004113
Iteration 215/1000 | Loss: 0.00004022
Iteration 216/1000 | Loss: 0.00004404
Iteration 217/1000 | Loss: 0.00004171
Iteration 218/1000 | Loss: 0.00004018
Iteration 219/1000 | Loss: 0.00003894
Iteration 220/1000 | Loss: 0.00003815
Iteration 221/1000 | Loss: 0.00003775
Iteration 222/1000 | Loss: 0.00003741
Iteration 223/1000 | Loss: 0.00090681
Iteration 224/1000 | Loss: 0.00064654
Iteration 225/1000 | Loss: 0.00014505
Iteration 226/1000 | Loss: 0.00006461
Iteration 227/1000 | Loss: 0.00077072
Iteration 228/1000 | Loss: 0.00011378
Iteration 229/1000 | Loss: 0.00004937
Iteration 230/1000 | Loss: 0.00004346
Iteration 231/1000 | Loss: 0.00005381
Iteration 232/1000 | Loss: 0.00005042
Iteration 233/1000 | Loss: 0.00004585
Iteration 234/1000 | Loss: 0.00003711
Iteration 235/1000 | Loss: 0.00003660
Iteration 236/1000 | Loss: 0.00003597
Iteration 237/1000 | Loss: 0.00003566
Iteration 238/1000 | Loss: 0.00003557
Iteration 239/1000 | Loss: 0.00003542
Iteration 240/1000 | Loss: 0.00003538
Iteration 241/1000 | Loss: 0.00003533
Iteration 242/1000 | Loss: 0.00003533
Iteration 243/1000 | Loss: 0.00003531
Iteration 244/1000 | Loss: 0.00003530
Iteration 245/1000 | Loss: 0.00003529
Iteration 246/1000 | Loss: 0.00003528
Iteration 247/1000 | Loss: 0.00003525
Iteration 248/1000 | Loss: 0.00003525
Iteration 249/1000 | Loss: 0.00003525
Iteration 250/1000 | Loss: 0.00003525
Iteration 251/1000 | Loss: 0.00003524
Iteration 252/1000 | Loss: 0.00003524
Iteration 253/1000 | Loss: 0.00003524
Iteration 254/1000 | Loss: 0.00003523
Iteration 255/1000 | Loss: 0.00006874
Iteration 256/1000 | Loss: 0.00004175
Iteration 257/1000 | Loss: 0.00003751
Iteration 258/1000 | Loss: 0.00003605
Iteration 259/1000 | Loss: 0.00003521
Iteration 260/1000 | Loss: 0.00003514
Iteration 261/1000 | Loss: 0.00003508
Iteration 262/1000 | Loss: 0.00003507
Iteration 263/1000 | Loss: 0.00003506
Iteration 264/1000 | Loss: 0.00003506
Iteration 265/1000 | Loss: 0.00003506
Iteration 266/1000 | Loss: 0.00003505
Iteration 267/1000 | Loss: 0.00003505
Iteration 268/1000 | Loss: 0.00003505
Iteration 269/1000 | Loss: 0.00003504
Iteration 270/1000 | Loss: 0.00003504
Iteration 271/1000 | Loss: 0.00003504
Iteration 272/1000 | Loss: 0.00003503
Iteration 273/1000 | Loss: 0.00003503
Iteration 274/1000 | Loss: 0.00003503
Iteration 275/1000 | Loss: 0.00003502
Iteration 276/1000 | Loss: 0.00003502
Iteration 277/1000 | Loss: 0.00003502
Iteration 278/1000 | Loss: 0.00003501
Iteration 279/1000 | Loss: 0.00003501
Iteration 280/1000 | Loss: 0.00003501
Iteration 281/1000 | Loss: 0.00003501
Iteration 282/1000 | Loss: 0.00003500
Iteration 283/1000 | Loss: 0.00003500
Iteration 284/1000 | Loss: 0.00003499
Iteration 285/1000 | Loss: 0.00003499
Iteration 286/1000 | Loss: 0.00003499
Iteration 287/1000 | Loss: 0.00003498
Iteration 288/1000 | Loss: 0.00003498
Iteration 289/1000 | Loss: 0.00003498
Iteration 290/1000 | Loss: 0.00003498
Iteration 291/1000 | Loss: 0.00003498
Iteration 292/1000 | Loss: 0.00003498
Iteration 293/1000 | Loss: 0.00003498
Iteration 294/1000 | Loss: 0.00003498
Iteration 295/1000 | Loss: 0.00003498
Iteration 296/1000 | Loss: 0.00003498
Iteration 297/1000 | Loss: 0.00003498
Iteration 298/1000 | Loss: 0.00003497
Iteration 299/1000 | Loss: 0.00003497
Iteration 300/1000 | Loss: 0.00003497
Iteration 301/1000 | Loss: 0.00003497
Iteration 302/1000 | Loss: 0.00003497
Iteration 303/1000 | Loss: 0.00003497
Iteration 304/1000 | Loss: 0.00003496
Iteration 305/1000 | Loss: 0.00003496
Iteration 306/1000 | Loss: 0.00003496
Iteration 307/1000 | Loss: 0.00003496
Iteration 308/1000 | Loss: 0.00003496
Iteration 309/1000 | Loss: 0.00003496
Iteration 310/1000 | Loss: 0.00003496
Iteration 311/1000 | Loss: 0.00003496
Iteration 312/1000 | Loss: 0.00003495
Iteration 313/1000 | Loss: 0.00003495
Iteration 314/1000 | Loss: 0.00003495
Iteration 315/1000 | Loss: 0.00003495
Iteration 316/1000 | Loss: 0.00003495
Iteration 317/1000 | Loss: 0.00003495
Iteration 318/1000 | Loss: 0.00003495
Iteration 319/1000 | Loss: 0.00003494
Iteration 320/1000 | Loss: 0.00003494
Iteration 321/1000 | Loss: 0.00003494
Iteration 322/1000 | Loss: 0.00003494
Iteration 323/1000 | Loss: 0.00003494
Iteration 324/1000 | Loss: 0.00003494
Iteration 325/1000 | Loss: 0.00003494
Iteration 326/1000 | Loss: 0.00003494
Iteration 327/1000 | Loss: 0.00003493
Iteration 328/1000 | Loss: 0.00003493
Iteration 329/1000 | Loss: 0.00003493
Iteration 330/1000 | Loss: 0.00003493
Iteration 331/1000 | Loss: 0.00003493
Iteration 332/1000 | Loss: 0.00003493
Iteration 333/1000 | Loss: 0.00003493
Iteration 334/1000 | Loss: 0.00003493
Iteration 335/1000 | Loss: 0.00003493
Iteration 336/1000 | Loss: 0.00003492
Iteration 337/1000 | Loss: 0.00003492
Iteration 338/1000 | Loss: 0.00003492
Iteration 339/1000 | Loss: 0.00003492
Iteration 340/1000 | Loss: 0.00003491
Iteration 341/1000 | Loss: 0.00003491
Iteration 342/1000 | Loss: 0.00003491
Iteration 343/1000 | Loss: 0.00003491
Iteration 344/1000 | Loss: 0.00003491
Iteration 345/1000 | Loss: 0.00003491
Iteration 346/1000 | Loss: 0.00003490
Iteration 347/1000 | Loss: 0.00003490
Iteration 348/1000 | Loss: 0.00003490
Iteration 349/1000 | Loss: 0.00003490
Iteration 350/1000 | Loss: 0.00003490
Iteration 351/1000 | Loss: 0.00003490
Iteration 352/1000 | Loss: 0.00003489
Iteration 353/1000 | Loss: 0.00003489
Iteration 354/1000 | Loss: 0.00003489
Iteration 355/1000 | Loss: 0.00003489
Iteration 356/1000 | Loss: 0.00003489
Iteration 357/1000 | Loss: 0.00003489
Iteration 358/1000 | Loss: 0.00003489
Iteration 359/1000 | Loss: 0.00003489
Iteration 360/1000 | Loss: 0.00003489
Iteration 361/1000 | Loss: 0.00003489
Iteration 362/1000 | Loss: 0.00003489
Iteration 363/1000 | Loss: 0.00003489
Iteration 364/1000 | Loss: 0.00003489
Iteration 365/1000 | Loss: 0.00003489
Iteration 366/1000 | Loss: 0.00003489
Iteration 367/1000 | Loss: 0.00003489
Iteration 368/1000 | Loss: 0.00003489
Iteration 369/1000 | Loss: 0.00003489
Iteration 370/1000 | Loss: 0.00003489
Iteration 371/1000 | Loss: 0.00003489
Iteration 372/1000 | Loss: 0.00003489
Iteration 373/1000 | Loss: 0.00003489
Iteration 374/1000 | Loss: 0.00003489
Iteration 375/1000 | Loss: 0.00003489
Iteration 376/1000 | Loss: 0.00003489
Iteration 377/1000 | Loss: 0.00003489
Iteration 378/1000 | Loss: 0.00003489
Iteration 379/1000 | Loss: 0.00003489
Iteration 380/1000 | Loss: 0.00003489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 380. Stopping optimization.
Last 5 losses: [3.488701622700319e-05, 3.488701622700319e-05, 3.488701622700319e-05, 3.488701622700319e-05, 3.488701622700319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.488701622700319e-05

Optimization complete. Final v2v error: 4.843282222747803 mm

Highest mean error: 6.411889553070068 mm for frame 196

Lowest mean error: 3.5323867797851562 mm for frame 3

Saving results

Total time: 450.48592805862427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479038
Iteration 2/25 | Loss: 0.00093977
Iteration 3/25 | Loss: 0.00082029
Iteration 4/25 | Loss: 0.00080742
Iteration 5/25 | Loss: 0.00080353
Iteration 6/25 | Loss: 0.00080248
Iteration 7/25 | Loss: 0.00080235
Iteration 8/25 | Loss: 0.00080235
Iteration 9/25 | Loss: 0.00080235
Iteration 10/25 | Loss: 0.00080235
Iteration 11/25 | Loss: 0.00080235
Iteration 12/25 | Loss: 0.00080235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008023511036299169, 0.0008023511036299169, 0.0008023511036299169, 0.0008023511036299169, 0.0008023511036299169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008023511036299169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.27726698
Iteration 2/25 | Loss: 0.00084757
Iteration 3/25 | Loss: 0.00084756
Iteration 4/25 | Loss: 0.00084755
Iteration 5/25 | Loss: 0.00084755
Iteration 6/25 | Loss: 0.00084755
Iteration 7/25 | Loss: 0.00084755
Iteration 8/25 | Loss: 0.00084755
Iteration 9/25 | Loss: 0.00084755
Iteration 10/25 | Loss: 0.00084755
Iteration 11/25 | Loss: 0.00084755
Iteration 12/25 | Loss: 0.00084755
Iteration 13/25 | Loss: 0.00084755
Iteration 14/25 | Loss: 0.00084755
Iteration 15/25 | Loss: 0.00084755
Iteration 16/25 | Loss: 0.00084755
Iteration 17/25 | Loss: 0.00084755
Iteration 18/25 | Loss: 0.00084755
Iteration 19/25 | Loss: 0.00084755
Iteration 20/25 | Loss: 0.00084755
Iteration 21/25 | Loss: 0.00084755
Iteration 22/25 | Loss: 0.00084755
Iteration 23/25 | Loss: 0.00084755
Iteration 24/25 | Loss: 0.00084755
Iteration 25/25 | Loss: 0.00084755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084755
Iteration 2/1000 | Loss: 0.00003258
Iteration 3/1000 | Loss: 0.00002613
Iteration 4/1000 | Loss: 0.00002253
Iteration 5/1000 | Loss: 0.00002109
Iteration 6/1000 | Loss: 0.00002004
Iteration 7/1000 | Loss: 0.00001950
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001852
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001850
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001837
Iteration 16/1000 | Loss: 0.00001837
Iteration 17/1000 | Loss: 0.00001837
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001836
Iteration 21/1000 | Loss: 0.00001833
Iteration 22/1000 | Loss: 0.00001833
Iteration 23/1000 | Loss: 0.00001833
Iteration 24/1000 | Loss: 0.00001832
Iteration 25/1000 | Loss: 0.00001832
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001832
Iteration 28/1000 | Loss: 0.00001831
Iteration 29/1000 | Loss: 0.00001831
Iteration 30/1000 | Loss: 0.00001830
Iteration 31/1000 | Loss: 0.00001830
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001829
Iteration 34/1000 | Loss: 0.00001829
Iteration 35/1000 | Loss: 0.00001829
Iteration 36/1000 | Loss: 0.00001829
Iteration 37/1000 | Loss: 0.00001829
Iteration 38/1000 | Loss: 0.00001829
Iteration 39/1000 | Loss: 0.00001829
Iteration 40/1000 | Loss: 0.00001829
Iteration 41/1000 | Loss: 0.00001829
Iteration 42/1000 | Loss: 0.00001829
Iteration 43/1000 | Loss: 0.00001828
Iteration 44/1000 | Loss: 0.00001828
Iteration 45/1000 | Loss: 0.00001828
Iteration 46/1000 | Loss: 0.00001828
Iteration 47/1000 | Loss: 0.00001828
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001827
Iteration 58/1000 | Loss: 0.00001827
Iteration 59/1000 | Loss: 0.00001827
Iteration 60/1000 | Loss: 0.00001827
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001826
Iteration 64/1000 | Loss: 0.00001826
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001826
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001825
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001825
Iteration 85/1000 | Loss: 0.00001825
Iteration 86/1000 | Loss: 0.00001825
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001824
Iteration 91/1000 | Loss: 0.00001824
Iteration 92/1000 | Loss: 0.00001824
Iteration 93/1000 | Loss: 0.00001824
Iteration 94/1000 | Loss: 0.00001824
Iteration 95/1000 | Loss: 0.00001824
Iteration 96/1000 | Loss: 0.00001824
Iteration 97/1000 | Loss: 0.00001824
Iteration 98/1000 | Loss: 0.00001823
Iteration 99/1000 | Loss: 0.00001823
Iteration 100/1000 | Loss: 0.00001823
Iteration 101/1000 | Loss: 0.00001823
Iteration 102/1000 | Loss: 0.00001823
Iteration 103/1000 | Loss: 0.00001823
Iteration 104/1000 | Loss: 0.00001823
Iteration 105/1000 | Loss: 0.00001823
Iteration 106/1000 | Loss: 0.00001823
Iteration 107/1000 | Loss: 0.00001822
Iteration 108/1000 | Loss: 0.00001822
Iteration 109/1000 | Loss: 0.00001822
Iteration 110/1000 | Loss: 0.00001822
Iteration 111/1000 | Loss: 0.00001822
Iteration 112/1000 | Loss: 0.00001822
Iteration 113/1000 | Loss: 0.00001822
Iteration 114/1000 | Loss: 0.00001822
Iteration 115/1000 | Loss: 0.00001822
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001822
Iteration 121/1000 | Loss: 0.00001822
Iteration 122/1000 | Loss: 0.00001822
Iteration 123/1000 | Loss: 0.00001821
Iteration 124/1000 | Loss: 0.00001821
Iteration 125/1000 | Loss: 0.00001821
Iteration 126/1000 | Loss: 0.00001821
Iteration 127/1000 | Loss: 0.00001821
Iteration 128/1000 | Loss: 0.00001821
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001821
Iteration 134/1000 | Loss: 0.00001821
Iteration 135/1000 | Loss: 0.00001821
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001820
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00001820
Iteration 146/1000 | Loss: 0.00001820
Iteration 147/1000 | Loss: 0.00001820
Iteration 148/1000 | Loss: 0.00001820
Iteration 149/1000 | Loss: 0.00001820
Iteration 150/1000 | Loss: 0.00001820
Iteration 151/1000 | Loss: 0.00001820
Iteration 152/1000 | Loss: 0.00001820
Iteration 153/1000 | Loss: 0.00001820
Iteration 154/1000 | Loss: 0.00001820
Iteration 155/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.8200831618742086e-05, 1.8200831618742086e-05, 1.8200831618742086e-05, 1.8200831618742086e-05, 1.8200831618742086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8200831618742086e-05

Optimization complete. Final v2v error: 3.705000162124634 mm

Highest mean error: 4.141722679138184 mm for frame 108

Lowest mean error: 3.1686758995056152 mm for frame 135

Saving results

Total time: 33.184152126312256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978578
Iteration 2/25 | Loss: 0.00317360
Iteration 3/25 | Loss: 0.00197949
Iteration 4/25 | Loss: 0.00144248
Iteration 5/25 | Loss: 0.00154408
Iteration 6/25 | Loss: 0.00124735
Iteration 7/25 | Loss: 0.00121066
Iteration 8/25 | Loss: 0.00118780
Iteration 9/25 | Loss: 0.00123376
Iteration 10/25 | Loss: 0.00117058
Iteration 11/25 | Loss: 0.00117099
Iteration 12/25 | Loss: 0.00117403
Iteration 13/25 | Loss: 0.00116073
Iteration 14/25 | Loss: 0.00115668
Iteration 15/25 | Loss: 0.00115327
Iteration 16/25 | Loss: 0.00114961
Iteration 17/25 | Loss: 0.00115137
Iteration 18/25 | Loss: 0.00114782
Iteration 19/25 | Loss: 0.00114963
Iteration 20/25 | Loss: 0.00114663
Iteration 21/25 | Loss: 0.00114579
Iteration 22/25 | Loss: 0.00114451
Iteration 23/25 | Loss: 0.00116891
Iteration 24/25 | Loss: 0.00114661
Iteration 25/25 | Loss: 0.00116889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.25983000
Iteration 2/25 | Loss: 0.01114000
Iteration 3/25 | Loss: 0.00869141
Iteration 4/25 | Loss: 0.00615955
Iteration 5/25 | Loss: 0.00429623
Iteration 6/25 | Loss: 0.00419979
Iteration 7/25 | Loss: 0.00419968
Iteration 8/25 | Loss: 0.00419968
Iteration 9/25 | Loss: 0.00419968
Iteration 10/25 | Loss: 0.00419968
Iteration 11/25 | Loss: 0.00419968
Iteration 12/25 | Loss: 0.00419968
Iteration 13/25 | Loss: 0.00419968
Iteration 14/25 | Loss: 0.00419968
Iteration 15/25 | Loss: 0.00419968
Iteration 16/25 | Loss: 0.00419968
Iteration 17/25 | Loss: 0.00419968
Iteration 18/25 | Loss: 0.00419968
Iteration 19/25 | Loss: 0.00419968
Iteration 20/25 | Loss: 0.00419968
Iteration 21/25 | Loss: 0.00419968
Iteration 22/25 | Loss: 0.00419968
Iteration 23/25 | Loss: 0.00419968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004199676681309938, 0.004199676681309938, 0.004199676681309938, 0.004199676681309938, 0.004199676681309938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004199676681309938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00419968
Iteration 2/1000 | Loss: 0.00797639
Iteration 3/1000 | Loss: 0.00541941
Iteration 4/1000 | Loss: 0.00303572
Iteration 5/1000 | Loss: 0.00691033
Iteration 6/1000 | Loss: 0.00395788
Iteration 7/1000 | Loss: 0.01106796
Iteration 8/1000 | Loss: 0.00622324
Iteration 9/1000 | Loss: 0.00774944
Iteration 10/1000 | Loss: 0.00617532
Iteration 11/1000 | Loss: 0.00293906
Iteration 12/1000 | Loss: 0.00270946
Iteration 13/1000 | Loss: 0.00192019
Iteration 14/1000 | Loss: 0.00084985
Iteration 15/1000 | Loss: 0.00210845
Iteration 16/1000 | Loss: 0.00318476
Iteration 17/1000 | Loss: 0.00138120
Iteration 18/1000 | Loss: 0.00108186
Iteration 19/1000 | Loss: 0.00102631
Iteration 20/1000 | Loss: 0.00118731
Iteration 21/1000 | Loss: 0.00328163
Iteration 22/1000 | Loss: 0.00120912
Iteration 23/1000 | Loss: 0.00355267
Iteration 24/1000 | Loss: 0.00136020
Iteration 25/1000 | Loss: 0.00075717
Iteration 26/1000 | Loss: 0.00098859
Iteration 27/1000 | Loss: 0.00210887
Iteration 28/1000 | Loss: 0.00156673
Iteration 29/1000 | Loss: 0.00228123
Iteration 30/1000 | Loss: 0.00138582
Iteration 31/1000 | Loss: 0.00096480
Iteration 32/1000 | Loss: 0.00202479
Iteration 33/1000 | Loss: 0.00283471
Iteration 34/1000 | Loss: 0.00177027
Iteration 35/1000 | Loss: 0.00116768
Iteration 36/1000 | Loss: 0.00331021
Iteration 37/1000 | Loss: 0.00186814
Iteration 38/1000 | Loss: 0.00101601
Iteration 39/1000 | Loss: 0.00233010
Iteration 40/1000 | Loss: 0.00225811
Iteration 41/1000 | Loss: 0.00225521
Iteration 42/1000 | Loss: 0.00084314
Iteration 43/1000 | Loss: 0.00182365
Iteration 44/1000 | Loss: 0.00153500
Iteration 45/1000 | Loss: 0.00185618
Iteration 46/1000 | Loss: 0.00111021
Iteration 47/1000 | Loss: 0.00213407
Iteration 48/1000 | Loss: 0.00180891
Iteration 49/1000 | Loss: 0.00177923
Iteration 50/1000 | Loss: 0.00237582
Iteration 51/1000 | Loss: 0.00129965
Iteration 52/1000 | Loss: 0.00169842
Iteration 53/1000 | Loss: 0.00245521
Iteration 54/1000 | Loss: 0.00132360
Iteration 55/1000 | Loss: 0.00211898
Iteration 56/1000 | Loss: 0.00168163
Iteration 57/1000 | Loss: 0.00152838
Iteration 58/1000 | Loss: 0.00317262
Iteration 59/1000 | Loss: 0.00251716
Iteration 60/1000 | Loss: 0.00120649
Iteration 61/1000 | Loss: 0.00074575
Iteration 62/1000 | Loss: 0.00146489
Iteration 63/1000 | Loss: 0.00117803
Iteration 64/1000 | Loss: 0.00213067
Iteration 65/1000 | Loss: 0.00190717
Iteration 66/1000 | Loss: 0.00234638
Iteration 67/1000 | Loss: 0.00242697
Iteration 68/1000 | Loss: 0.00127526
Iteration 69/1000 | Loss: 0.00262503
Iteration 70/1000 | Loss: 0.00294252
Iteration 71/1000 | Loss: 0.00257628
Iteration 72/1000 | Loss: 0.00279874
Iteration 73/1000 | Loss: 0.00358491
Iteration 74/1000 | Loss: 0.00135997
Iteration 75/1000 | Loss: 0.00221926
Iteration 76/1000 | Loss: 0.00243525
Iteration 77/1000 | Loss: 0.00240807
Iteration 78/1000 | Loss: 0.00200140
Iteration 79/1000 | Loss: 0.00235835
Iteration 80/1000 | Loss: 0.00261827
Iteration 81/1000 | Loss: 0.00287617
Iteration 82/1000 | Loss: 0.00235666
Iteration 83/1000 | Loss: 0.00212044
Iteration 84/1000 | Loss: 0.00265312
Iteration 85/1000 | Loss: 0.00294746
Iteration 86/1000 | Loss: 0.00311943
Iteration 87/1000 | Loss: 0.00273902
Iteration 88/1000 | Loss: 0.00271587
Iteration 89/1000 | Loss: 0.00223942
Iteration 90/1000 | Loss: 0.00321406
Iteration 91/1000 | Loss: 0.00334837
Iteration 92/1000 | Loss: 0.00289140
Iteration 93/1000 | Loss: 0.00345607
Iteration 94/1000 | Loss: 0.00359874
Iteration 95/1000 | Loss: 0.00301120
Iteration 96/1000 | Loss: 0.00283994
Iteration 97/1000 | Loss: 0.00238858
Iteration 98/1000 | Loss: 0.00263905
Iteration 99/1000 | Loss: 0.00303283
Iteration 100/1000 | Loss: 0.00289809
Iteration 101/1000 | Loss: 0.00296203
Iteration 102/1000 | Loss: 0.00233148
Iteration 103/1000 | Loss: 0.00300966
Iteration 104/1000 | Loss: 0.00427611
Iteration 105/1000 | Loss: 0.00309348
Iteration 106/1000 | Loss: 0.00220138
Iteration 107/1000 | Loss: 0.00274124
Iteration 108/1000 | Loss: 0.00262159
Iteration 109/1000 | Loss: 0.00227646
Iteration 110/1000 | Loss: 0.00084409
Iteration 111/1000 | Loss: 0.00241775
Iteration 112/1000 | Loss: 0.00210168
Iteration 113/1000 | Loss: 0.00244148
Iteration 114/1000 | Loss: 0.00233968
Iteration 115/1000 | Loss: 0.00243295
Iteration 116/1000 | Loss: 0.00404568
Iteration 117/1000 | Loss: 0.00287467
Iteration 118/1000 | Loss: 0.00247961
Iteration 119/1000 | Loss: 0.00229583
Iteration 120/1000 | Loss: 0.00182059
Iteration 121/1000 | Loss: 0.00126681
Iteration 122/1000 | Loss: 0.00161807
Iteration 123/1000 | Loss: 0.00326587
Iteration 124/1000 | Loss: 0.00253997
Iteration 125/1000 | Loss: 0.00219213
Iteration 126/1000 | Loss: 0.00267387
Iteration 127/1000 | Loss: 0.00213782
Iteration 128/1000 | Loss: 0.00151062
Iteration 129/1000 | Loss: 0.00164251
Iteration 130/1000 | Loss: 0.00111864
Iteration 131/1000 | Loss: 0.00143430
Iteration 132/1000 | Loss: 0.00113088
Iteration 133/1000 | Loss: 0.00227382
Iteration 134/1000 | Loss: 0.00148841
Iteration 135/1000 | Loss: 0.00146558
Iteration 136/1000 | Loss: 0.00103261
Iteration 137/1000 | Loss: 0.00088092
Iteration 138/1000 | Loss: 0.00078980
Iteration 139/1000 | Loss: 0.00071093
Iteration 140/1000 | Loss: 0.00068643
Iteration 141/1000 | Loss: 0.00150702
Iteration 142/1000 | Loss: 0.00138999
Iteration 143/1000 | Loss: 0.00123789
Iteration 144/1000 | Loss: 0.00114035
Iteration 145/1000 | Loss: 0.00096755
Iteration 146/1000 | Loss: 0.00029825
Iteration 147/1000 | Loss: 0.00071840
Iteration 148/1000 | Loss: 0.00108908
Iteration 149/1000 | Loss: 0.00084683
Iteration 150/1000 | Loss: 0.00083537
Iteration 151/1000 | Loss: 0.00037356
Iteration 152/1000 | Loss: 0.00017513
Iteration 153/1000 | Loss: 0.00109194
Iteration 154/1000 | Loss: 0.00072748
Iteration 155/1000 | Loss: 0.00009082
Iteration 156/1000 | Loss: 0.00068643
Iteration 157/1000 | Loss: 0.00026030
Iteration 158/1000 | Loss: 0.00012802
Iteration 159/1000 | Loss: 0.00079834
Iteration 160/1000 | Loss: 0.00073872
Iteration 161/1000 | Loss: 0.00123068
Iteration 162/1000 | Loss: 0.00058649
Iteration 163/1000 | Loss: 0.00026000
Iteration 164/1000 | Loss: 0.00097250
Iteration 165/1000 | Loss: 0.00051575
Iteration 166/1000 | Loss: 0.00071732
Iteration 167/1000 | Loss: 0.00071997
Iteration 168/1000 | Loss: 0.00081377
Iteration 169/1000 | Loss: 0.00090118
Iteration 170/1000 | Loss: 0.00114784
Iteration 171/1000 | Loss: 0.00064010
Iteration 172/1000 | Loss: 0.00087668
Iteration 173/1000 | Loss: 0.00090280
Iteration 174/1000 | Loss: 0.00042251
Iteration 175/1000 | Loss: 0.00045070
Iteration 176/1000 | Loss: 0.00006952
Iteration 177/1000 | Loss: 0.00036168
Iteration 178/1000 | Loss: 0.00087999
Iteration 179/1000 | Loss: 0.00066872
Iteration 180/1000 | Loss: 0.00063958
Iteration 181/1000 | Loss: 0.00061321
Iteration 182/1000 | Loss: 0.00036173
Iteration 183/1000 | Loss: 0.00019378
Iteration 184/1000 | Loss: 0.00074324
Iteration 185/1000 | Loss: 0.00021002
Iteration 186/1000 | Loss: 0.00006218
Iteration 187/1000 | Loss: 0.00057248
Iteration 188/1000 | Loss: 0.00014448
Iteration 189/1000 | Loss: 0.00049787
Iteration 190/1000 | Loss: 0.00040074
Iteration 191/1000 | Loss: 0.00065063
Iteration 192/1000 | Loss: 0.00014943
Iteration 193/1000 | Loss: 0.00008410
Iteration 194/1000 | Loss: 0.00010833
Iteration 195/1000 | Loss: 0.00026327
Iteration 196/1000 | Loss: 0.00082141
Iteration 197/1000 | Loss: 0.00046125
Iteration 198/1000 | Loss: 0.00045686
Iteration 199/1000 | Loss: 0.00024894
Iteration 200/1000 | Loss: 0.00031938
Iteration 201/1000 | Loss: 0.00027164
Iteration 202/1000 | Loss: 0.00032929
Iteration 203/1000 | Loss: 0.00018836
Iteration 204/1000 | Loss: 0.00048406
Iteration 205/1000 | Loss: 0.00069261
Iteration 206/1000 | Loss: 0.00045645
Iteration 207/1000 | Loss: 0.00017135
Iteration 208/1000 | Loss: 0.00028204
Iteration 209/1000 | Loss: 0.00046600
Iteration 210/1000 | Loss: 0.00050756
Iteration 211/1000 | Loss: 0.00036271
Iteration 212/1000 | Loss: 0.00032386
Iteration 213/1000 | Loss: 0.00022515
Iteration 214/1000 | Loss: 0.00029433
Iteration 215/1000 | Loss: 0.00027618
Iteration 216/1000 | Loss: 0.00027152
Iteration 217/1000 | Loss: 0.00012020
Iteration 218/1000 | Loss: 0.00005717
Iteration 219/1000 | Loss: 0.00011347
Iteration 220/1000 | Loss: 0.00004201
Iteration 221/1000 | Loss: 0.00013982
Iteration 222/1000 | Loss: 0.00014913
Iteration 223/1000 | Loss: 0.00003813
Iteration 224/1000 | Loss: 0.00005027
Iteration 225/1000 | Loss: 0.00004062
Iteration 226/1000 | Loss: 0.00003378
Iteration 227/1000 | Loss: 0.00068984
Iteration 228/1000 | Loss: 0.00040923
Iteration 229/1000 | Loss: 0.00088154
Iteration 230/1000 | Loss: 0.00094802
Iteration 231/1000 | Loss: 0.00004138
Iteration 232/1000 | Loss: 0.00037228
Iteration 233/1000 | Loss: 0.00057458
Iteration 234/1000 | Loss: 0.00053918
Iteration 235/1000 | Loss: 0.00042834
Iteration 236/1000 | Loss: 0.00007727
Iteration 237/1000 | Loss: 0.00004114
Iteration 238/1000 | Loss: 0.00003546
Iteration 239/1000 | Loss: 0.00037562
Iteration 240/1000 | Loss: 0.00054737
Iteration 241/1000 | Loss: 0.00055386
Iteration 242/1000 | Loss: 0.00028713
Iteration 243/1000 | Loss: 0.00005657
Iteration 244/1000 | Loss: 0.00004124
Iteration 245/1000 | Loss: 0.00015428
Iteration 246/1000 | Loss: 0.00008291
Iteration 247/1000 | Loss: 0.00014552
Iteration 248/1000 | Loss: 0.00003732
Iteration 249/1000 | Loss: 0.00004568
Iteration 250/1000 | Loss: 0.00046727
Iteration 251/1000 | Loss: 0.00019752
Iteration 252/1000 | Loss: 0.00004890
Iteration 253/1000 | Loss: 0.00003340
Iteration 254/1000 | Loss: 0.00045147
Iteration 255/1000 | Loss: 0.00023737
Iteration 256/1000 | Loss: 0.00048174
Iteration 257/1000 | Loss: 0.00064536
Iteration 258/1000 | Loss: 0.00048867
Iteration 259/1000 | Loss: 0.00045660
Iteration 260/1000 | Loss: 0.00041481
Iteration 261/1000 | Loss: 0.00025079
Iteration 262/1000 | Loss: 0.00045473
Iteration 263/1000 | Loss: 0.00027673
Iteration 264/1000 | Loss: 0.00047974
Iteration 265/1000 | Loss: 0.00038078
Iteration 266/1000 | Loss: 0.00042041
Iteration 267/1000 | Loss: 0.00041119
Iteration 268/1000 | Loss: 0.00043607
Iteration 269/1000 | Loss: 0.00036807
Iteration 270/1000 | Loss: 0.00030123
Iteration 271/1000 | Loss: 0.00005469
Iteration 272/1000 | Loss: 0.00007487
Iteration 273/1000 | Loss: 0.00011164
Iteration 274/1000 | Loss: 0.00003971
Iteration 275/1000 | Loss: 0.00004923
Iteration 276/1000 | Loss: 0.00004232
Iteration 277/1000 | Loss: 0.00040959
Iteration 278/1000 | Loss: 0.00034479
Iteration 279/1000 | Loss: 0.00050960
Iteration 280/1000 | Loss: 0.00036160
Iteration 281/1000 | Loss: 0.00011728
Iteration 282/1000 | Loss: 0.00016280
Iteration 283/1000 | Loss: 0.00028132
Iteration 284/1000 | Loss: 0.00019618
Iteration 285/1000 | Loss: 0.00008208
Iteration 286/1000 | Loss: 0.00004841
Iteration 287/1000 | Loss: 0.00054711
Iteration 288/1000 | Loss: 0.00004611
Iteration 289/1000 | Loss: 0.00004869
Iteration 290/1000 | Loss: 0.00003828
Iteration 291/1000 | Loss: 0.00021540
Iteration 292/1000 | Loss: 0.00003502
Iteration 293/1000 | Loss: 0.00007186
Iteration 294/1000 | Loss: 0.00003300
Iteration 295/1000 | Loss: 0.00007138
Iteration 296/1000 | Loss: 0.00041996
Iteration 297/1000 | Loss: 0.00071821
Iteration 298/1000 | Loss: 0.00022299
Iteration 299/1000 | Loss: 0.00004299
Iteration 300/1000 | Loss: 0.00012000
Iteration 301/1000 | Loss: 0.00003812
Iteration 302/1000 | Loss: 0.00004592
Iteration 303/1000 | Loss: 0.00025135
Iteration 304/1000 | Loss: 0.00003495
Iteration 305/1000 | Loss: 0.00003897
Iteration 306/1000 | Loss: 0.00003231
Iteration 307/1000 | Loss: 0.00005568
Iteration 308/1000 | Loss: 0.00021166
Iteration 309/1000 | Loss: 0.00003144
Iteration 310/1000 | Loss: 0.00004456
Iteration 311/1000 | Loss: 0.00002902
Iteration 312/1000 | Loss: 0.00002831
Iteration 313/1000 | Loss: 0.00004275
Iteration 314/1000 | Loss: 0.00004761
Iteration 315/1000 | Loss: 0.00015328
Iteration 316/1000 | Loss: 0.00004363
Iteration 317/1000 | Loss: 0.00032924
Iteration 318/1000 | Loss: 0.00006314
Iteration 319/1000 | Loss: 0.00003980
Iteration 320/1000 | Loss: 0.00018255
Iteration 321/1000 | Loss: 0.00005701
Iteration 322/1000 | Loss: 0.00004379
Iteration 323/1000 | Loss: 0.00005137
Iteration 324/1000 | Loss: 0.00002721
Iteration 325/1000 | Loss: 0.00002677
Iteration 326/1000 | Loss: 0.00005755
Iteration 327/1000 | Loss: 0.00002695
Iteration 328/1000 | Loss: 0.00002624
Iteration 329/1000 | Loss: 0.00002606
Iteration 330/1000 | Loss: 0.00002604
Iteration 331/1000 | Loss: 0.00002602
Iteration 332/1000 | Loss: 0.00037348
Iteration 333/1000 | Loss: 0.00068069
Iteration 334/1000 | Loss: 0.00011541
Iteration 335/1000 | Loss: 0.00007791
Iteration 336/1000 | Loss: 0.00004597
Iteration 337/1000 | Loss: 0.00003952
Iteration 338/1000 | Loss: 0.00004848
Iteration 339/1000 | Loss: 0.00005262
Iteration 340/1000 | Loss: 0.00006208
Iteration 341/1000 | Loss: 0.00003857
Iteration 342/1000 | Loss: 0.00003283
Iteration 343/1000 | Loss: 0.00009042
Iteration 344/1000 | Loss: 0.00010308
Iteration 345/1000 | Loss: 0.00002811
Iteration 346/1000 | Loss: 0.00005218
Iteration 347/1000 | Loss: 0.00003814
Iteration 348/1000 | Loss: 0.00006320
Iteration 349/1000 | Loss: 0.00002682
Iteration 350/1000 | Loss: 0.00002641
Iteration 351/1000 | Loss: 0.00006570
Iteration 352/1000 | Loss: 0.00003106
Iteration 353/1000 | Loss: 0.00003597
Iteration 354/1000 | Loss: 0.00002714
Iteration 355/1000 | Loss: 0.00002582
Iteration 356/1000 | Loss: 0.00002580
Iteration 357/1000 | Loss: 0.00002580
Iteration 358/1000 | Loss: 0.00002579
Iteration 359/1000 | Loss: 0.00002579
Iteration 360/1000 | Loss: 0.00002579
Iteration 361/1000 | Loss: 0.00002579
Iteration 362/1000 | Loss: 0.00002579
Iteration 363/1000 | Loss: 0.00002579
Iteration 364/1000 | Loss: 0.00002577
Iteration 365/1000 | Loss: 0.00002574
Iteration 366/1000 | Loss: 0.00002574
Iteration 367/1000 | Loss: 0.00002574
Iteration 368/1000 | Loss: 0.00002574
Iteration 369/1000 | Loss: 0.00002574
Iteration 370/1000 | Loss: 0.00002574
Iteration 371/1000 | Loss: 0.00002574
Iteration 372/1000 | Loss: 0.00002574
Iteration 373/1000 | Loss: 0.00002574
Iteration 374/1000 | Loss: 0.00002574
Iteration 375/1000 | Loss: 0.00002573
Iteration 376/1000 | Loss: 0.00002573
Iteration 377/1000 | Loss: 0.00002573
Iteration 378/1000 | Loss: 0.00002573
Iteration 379/1000 | Loss: 0.00002573
Iteration 380/1000 | Loss: 0.00002573
Iteration 381/1000 | Loss: 0.00002573
Iteration 382/1000 | Loss: 0.00002573
Iteration 383/1000 | Loss: 0.00002572
Iteration 384/1000 | Loss: 0.00002572
Iteration 385/1000 | Loss: 0.00002572
Iteration 386/1000 | Loss: 0.00002572
Iteration 387/1000 | Loss: 0.00002572
Iteration 388/1000 | Loss: 0.00002572
Iteration 389/1000 | Loss: 0.00002572
Iteration 390/1000 | Loss: 0.00002572
Iteration 391/1000 | Loss: 0.00002571
Iteration 392/1000 | Loss: 0.00002571
Iteration 393/1000 | Loss: 0.00002571
Iteration 394/1000 | Loss: 0.00002571
Iteration 395/1000 | Loss: 0.00002571
Iteration 396/1000 | Loss: 0.00002571
Iteration 397/1000 | Loss: 0.00002571
Iteration 398/1000 | Loss: 0.00002571
Iteration 399/1000 | Loss: 0.00002570
Iteration 400/1000 | Loss: 0.00002570
Iteration 401/1000 | Loss: 0.00002570
Iteration 402/1000 | Loss: 0.00002570
Iteration 403/1000 | Loss: 0.00002570
Iteration 404/1000 | Loss: 0.00002570
Iteration 405/1000 | Loss: 0.00002570
Iteration 406/1000 | Loss: 0.00002570
Iteration 407/1000 | Loss: 0.00002570
Iteration 408/1000 | Loss: 0.00002569
Iteration 409/1000 | Loss: 0.00002569
Iteration 410/1000 | Loss: 0.00002569
Iteration 411/1000 | Loss: 0.00002569
Iteration 412/1000 | Loss: 0.00002569
Iteration 413/1000 | Loss: 0.00002569
Iteration 414/1000 | Loss: 0.00002569
Iteration 415/1000 | Loss: 0.00002569
Iteration 416/1000 | Loss: 0.00002569
Iteration 417/1000 | Loss: 0.00002568
Iteration 418/1000 | Loss: 0.00002568
Iteration 419/1000 | Loss: 0.00002568
Iteration 420/1000 | Loss: 0.00002567
Iteration 421/1000 | Loss: 0.00002567
Iteration 422/1000 | Loss: 0.00002567
Iteration 423/1000 | Loss: 0.00002567
Iteration 424/1000 | Loss: 0.00002567
Iteration 425/1000 | Loss: 0.00002567
Iteration 426/1000 | Loss: 0.00002567
Iteration 427/1000 | Loss: 0.00002567
Iteration 428/1000 | Loss: 0.00002566
Iteration 429/1000 | Loss: 0.00002566
Iteration 430/1000 | Loss: 0.00002566
Iteration 431/1000 | Loss: 0.00002566
Iteration 432/1000 | Loss: 0.00003453
Iteration 433/1000 | Loss: 0.00002570
Iteration 434/1000 | Loss: 0.00002568
Iteration 435/1000 | Loss: 0.00002568
Iteration 436/1000 | Loss: 0.00002568
Iteration 437/1000 | Loss: 0.00002568
Iteration 438/1000 | Loss: 0.00002568
Iteration 439/1000 | Loss: 0.00002567
Iteration 440/1000 | Loss: 0.00002567
Iteration 441/1000 | Loss: 0.00002567
Iteration 442/1000 | Loss: 0.00002567
Iteration 443/1000 | Loss: 0.00002567
Iteration 444/1000 | Loss: 0.00002567
Iteration 445/1000 | Loss: 0.00002567
Iteration 446/1000 | Loss: 0.00002566
Iteration 447/1000 | Loss: 0.00002566
Iteration 448/1000 | Loss: 0.00002962
Iteration 449/1000 | Loss: 0.00002622
Iteration 450/1000 | Loss: 0.00002564
Iteration 451/1000 | Loss: 0.00002564
Iteration 452/1000 | Loss: 0.00002564
Iteration 453/1000 | Loss: 0.00002564
Iteration 454/1000 | Loss: 0.00002563
Iteration 455/1000 | Loss: 0.00002563
Iteration 456/1000 | Loss: 0.00002563
Iteration 457/1000 | Loss: 0.00002563
Iteration 458/1000 | Loss: 0.00002563
Iteration 459/1000 | Loss: 0.00002563
Iteration 460/1000 | Loss: 0.00002563
Iteration 461/1000 | Loss: 0.00002563
Iteration 462/1000 | Loss: 0.00002563
Iteration 463/1000 | Loss: 0.00002563
Iteration 464/1000 | Loss: 0.00002563
Iteration 465/1000 | Loss: 0.00002563
Iteration 466/1000 | Loss: 0.00002563
Iteration 467/1000 | Loss: 0.00003638
Iteration 468/1000 | Loss: 0.00003419
Iteration 469/1000 | Loss: 0.00003419
Iteration 470/1000 | Loss: 0.00002570
Iteration 471/1000 | Loss: 0.00002569
Iteration 472/1000 | Loss: 0.00003351
Iteration 473/1000 | Loss: 0.00003057
Iteration 474/1000 | Loss: 0.00002569
Iteration 475/1000 | Loss: 0.00002561
Iteration 476/1000 | Loss: 0.00002560
Iteration 477/1000 | Loss: 0.00002560
Iteration 478/1000 | Loss: 0.00002559
Iteration 479/1000 | Loss: 0.00002559
Iteration 480/1000 | Loss: 0.00002559
Iteration 481/1000 | Loss: 0.00002558
Iteration 482/1000 | Loss: 0.00002558
Iteration 483/1000 | Loss: 0.00002558
Iteration 484/1000 | Loss: 0.00002558
Iteration 485/1000 | Loss: 0.00004447
Iteration 486/1000 | Loss: 0.00007936
Iteration 487/1000 | Loss: 0.00002748
Iteration 488/1000 | Loss: 0.00002694
Iteration 489/1000 | Loss: 0.00004941
Iteration 490/1000 | Loss: 0.00003240
Iteration 491/1000 | Loss: 0.00002812
Iteration 492/1000 | Loss: 0.00002563
Iteration 493/1000 | Loss: 0.00004681
Iteration 494/1000 | Loss: 0.00003796
Iteration 495/1000 | Loss: 0.00003014
Iteration 496/1000 | Loss: 0.00004665
Iteration 497/1000 | Loss: 0.00002945
Iteration 498/1000 | Loss: 0.00004450
Iteration 499/1000 | Loss: 0.00002874
Iteration 500/1000 | Loss: 0.00003092
Iteration 501/1000 | Loss: 0.00003127
Iteration 502/1000 | Loss: 0.00005251
Iteration 503/1000 | Loss: 0.00003152
Iteration 504/1000 | Loss: 0.00003213
Iteration 505/1000 | Loss: 0.00004734
Iteration 506/1000 | Loss: 0.00003050
Iteration 507/1000 | Loss: 0.00004437
Iteration 508/1000 | Loss: 0.00002793
Iteration 509/1000 | Loss: 0.00002571
Iteration 510/1000 | Loss: 0.00004029
Iteration 511/1000 | Loss: 0.00006323
Iteration 512/1000 | Loss: 0.00002605
Iteration 513/1000 | Loss: 0.00002553
Iteration 514/1000 | Loss: 0.00003901
Iteration 515/1000 | Loss: 0.00004165
Iteration 516/1000 | Loss: 0.00003020
Iteration 517/1000 | Loss: 0.00002503
Iteration 518/1000 | Loss: 0.00003418
Iteration 519/1000 | Loss: 0.00003406
Iteration 520/1000 | Loss: 0.00005077
Iteration 521/1000 | Loss: 0.00002520
Iteration 522/1000 | Loss: 0.00002486
Iteration 523/1000 | Loss: 0.00002486
Iteration 524/1000 | Loss: 0.00002485
Iteration 525/1000 | Loss: 0.00002485
Iteration 526/1000 | Loss: 0.00002485
Iteration 527/1000 | Loss: 0.00002483
Iteration 528/1000 | Loss: 0.00002483
Iteration 529/1000 | Loss: 0.00002480
Iteration 530/1000 | Loss: 0.00002480
Iteration 531/1000 | Loss: 0.00002479
Iteration 532/1000 | Loss: 0.00002478
Iteration 533/1000 | Loss: 0.00002478
Iteration 534/1000 | Loss: 0.00002477
Iteration 535/1000 | Loss: 0.00002477
Iteration 536/1000 | Loss: 0.00003646
Iteration 537/1000 | Loss: 0.00002515
Iteration 538/1000 | Loss: 0.00004524
Iteration 539/1000 | Loss: 0.00002842
Iteration 540/1000 | Loss: 0.00003420
Iteration 541/1000 | Loss: 0.00002466
Iteration 542/1000 | Loss: 0.00002465
Iteration 543/1000 | Loss: 0.00002465
Iteration 544/1000 | Loss: 0.00002465
Iteration 545/1000 | Loss: 0.00002464
Iteration 546/1000 | Loss: 0.00002464
Iteration 547/1000 | Loss: 0.00002464
Iteration 548/1000 | Loss: 0.00002463
Iteration 549/1000 | Loss: 0.00002463
Iteration 550/1000 | Loss: 0.00002463
Iteration 551/1000 | Loss: 0.00002462
Iteration 552/1000 | Loss: 0.00002462
Iteration 553/1000 | Loss: 0.00002462
Iteration 554/1000 | Loss: 0.00002462
Iteration 555/1000 | Loss: 0.00002462
Iteration 556/1000 | Loss: 0.00002462
Iteration 557/1000 | Loss: 0.00002461
Iteration 558/1000 | Loss: 0.00002461
Iteration 559/1000 | Loss: 0.00002461
Iteration 560/1000 | Loss: 0.00002461
Iteration 561/1000 | Loss: 0.00002461
Iteration 562/1000 | Loss: 0.00002461
Iteration 563/1000 | Loss: 0.00002461
Iteration 564/1000 | Loss: 0.00002461
Iteration 565/1000 | Loss: 0.00002461
Iteration 566/1000 | Loss: 0.00002461
Iteration 567/1000 | Loss: 0.00002461
Iteration 568/1000 | Loss: 0.00002461
Iteration 569/1000 | Loss: 0.00002461
Iteration 570/1000 | Loss: 0.00002461
Iteration 571/1000 | Loss: 0.00002461
Iteration 572/1000 | Loss: 0.00002461
Iteration 573/1000 | Loss: 0.00002461
Iteration 574/1000 | Loss: 0.00002461
Iteration 575/1000 | Loss: 0.00002460
Iteration 576/1000 | Loss: 0.00002460
Iteration 577/1000 | Loss: 0.00002460
Iteration 578/1000 | Loss: 0.00002460
Iteration 579/1000 | Loss: 0.00002460
Iteration 580/1000 | Loss: 0.00002460
Iteration 581/1000 | Loss: 0.00002460
Iteration 582/1000 | Loss: 0.00002460
Iteration 583/1000 | Loss: 0.00002460
Iteration 584/1000 | Loss: 0.00002460
Iteration 585/1000 | Loss: 0.00002460
Iteration 586/1000 | Loss: 0.00002460
Iteration 587/1000 | Loss: 0.00002460
Iteration 588/1000 | Loss: 0.00002460
Iteration 589/1000 | Loss: 0.00002460
Iteration 590/1000 | Loss: 0.00002460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 590. Stopping optimization.
Last 5 losses: [2.4601316908956505e-05, 2.4601316908956505e-05, 2.4601316908956505e-05, 2.4601316908956505e-05, 2.4601316908956505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4601316908956505e-05

Optimization complete. Final v2v error: 3.9007515907287598 mm

Highest mean error: 8.260598182678223 mm for frame 28

Lowest mean error: 2.686156988143921 mm for frame 24

Saving results

Total time: 655.416650056839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01180000
Iteration 2/25 | Loss: 0.01179999
Iteration 3/25 | Loss: 0.01179999
Iteration 4/25 | Loss: 0.01179999
Iteration 5/25 | Loss: 0.01179999
Iteration 6/25 | Loss: 0.01179999
Iteration 7/25 | Loss: 0.01179999
Iteration 8/25 | Loss: 0.01179999
Iteration 9/25 | Loss: 0.01179999
Iteration 10/25 | Loss: 0.01179998
Iteration 11/25 | Loss: 0.01179998
Iteration 12/25 | Loss: 0.01179998
Iteration 13/25 | Loss: 0.01179998
Iteration 14/25 | Loss: 0.01179998
Iteration 15/25 | Loss: 0.01179998
Iteration 16/25 | Loss: 0.01179998
Iteration 17/25 | Loss: 0.01179998
Iteration 18/25 | Loss: 0.01179997
Iteration 19/25 | Loss: 0.01179997
Iteration 20/25 | Loss: 0.01179997
Iteration 21/25 | Loss: 0.01179997
Iteration 22/25 | Loss: 0.01179997
Iteration 23/25 | Loss: 0.01179997
Iteration 24/25 | Loss: 0.01179997
Iteration 25/25 | Loss: 0.01179997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83694088
Iteration 2/25 | Loss: 0.14493905
Iteration 3/25 | Loss: 0.14490961
Iteration 4/25 | Loss: 0.14490958
Iteration 5/25 | Loss: 0.14490958
Iteration 6/25 | Loss: 0.14490958
Iteration 7/25 | Loss: 0.14490958
Iteration 8/25 | Loss: 0.14490958
Iteration 9/25 | Loss: 0.14490958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.1449095755815506, 0.1449095755815506, 0.1449095755815506, 0.1449095755815506, 0.1449095755815506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1449095755815506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14490958
Iteration 2/1000 | Loss: 0.00422921
Iteration 3/1000 | Loss: 0.00059176
Iteration 4/1000 | Loss: 0.00093888
Iteration 5/1000 | Loss: 0.00025778
Iteration 6/1000 | Loss: 0.00013761
Iteration 7/1000 | Loss: 0.00090344
Iteration 8/1000 | Loss: 0.00008005
Iteration 9/1000 | Loss: 0.00014069
Iteration 10/1000 | Loss: 0.00006594
Iteration 11/1000 | Loss: 0.00015511
Iteration 12/1000 | Loss: 0.00009759
Iteration 13/1000 | Loss: 0.00005322
Iteration 14/1000 | Loss: 0.00004908
Iteration 15/1000 | Loss: 0.00021371
Iteration 16/1000 | Loss: 0.00010595
Iteration 17/1000 | Loss: 0.00009407
Iteration 18/1000 | Loss: 0.00012832
Iteration 19/1000 | Loss: 0.00015955
Iteration 20/1000 | Loss: 0.00016844
Iteration 21/1000 | Loss: 0.00022189
Iteration 22/1000 | Loss: 0.00004601
Iteration 23/1000 | Loss: 0.00004907
Iteration 24/1000 | Loss: 0.00004039
Iteration 25/1000 | Loss: 0.00024876
Iteration 26/1000 | Loss: 0.00044261
Iteration 27/1000 | Loss: 0.00005193
Iteration 28/1000 | Loss: 0.00011121
Iteration 29/1000 | Loss: 0.00009719
Iteration 30/1000 | Loss: 0.00031783
Iteration 31/1000 | Loss: 0.00008239
Iteration 32/1000 | Loss: 0.00011337
Iteration 33/1000 | Loss: 0.00009228
Iteration 34/1000 | Loss: 0.00003688
Iteration 35/1000 | Loss: 0.00013522
Iteration 36/1000 | Loss: 0.00004975
Iteration 37/1000 | Loss: 0.00014515
Iteration 38/1000 | Loss: 0.00004842
Iteration 39/1000 | Loss: 0.00003613
Iteration 40/1000 | Loss: 0.00003408
Iteration 41/1000 | Loss: 0.00004725
Iteration 42/1000 | Loss: 0.00003340
Iteration 43/1000 | Loss: 0.00003300
Iteration 44/1000 | Loss: 0.00011425
Iteration 45/1000 | Loss: 0.00009316
Iteration 46/1000 | Loss: 0.00003291
Iteration 47/1000 | Loss: 0.00003326
Iteration 48/1000 | Loss: 0.00003285
Iteration 49/1000 | Loss: 0.00012030
Iteration 50/1000 | Loss: 0.00004051
Iteration 51/1000 | Loss: 0.00003759
Iteration 52/1000 | Loss: 0.00003266
Iteration 53/1000 | Loss: 0.00003585
Iteration 54/1000 | Loss: 0.00004716
Iteration 55/1000 | Loss: 0.00003520
Iteration 56/1000 | Loss: 0.00012031
Iteration 57/1000 | Loss: 0.00003605
Iteration 58/1000 | Loss: 0.00013227
Iteration 59/1000 | Loss: 0.00005254
Iteration 60/1000 | Loss: 0.00006905
Iteration 61/1000 | Loss: 0.00003856
Iteration 62/1000 | Loss: 0.00004014
Iteration 63/1000 | Loss: 0.00003275
Iteration 64/1000 | Loss: 0.00003245
Iteration 65/1000 | Loss: 0.00004342
Iteration 66/1000 | Loss: 0.00003245
Iteration 67/1000 | Loss: 0.00003302
Iteration 68/1000 | Loss: 0.00003235
Iteration 69/1000 | Loss: 0.00003235
Iteration 70/1000 | Loss: 0.00003235
Iteration 71/1000 | Loss: 0.00003235
Iteration 72/1000 | Loss: 0.00003235
Iteration 73/1000 | Loss: 0.00003235
Iteration 74/1000 | Loss: 0.00003235
Iteration 75/1000 | Loss: 0.00003235
Iteration 76/1000 | Loss: 0.00003235
Iteration 77/1000 | Loss: 0.00003235
Iteration 78/1000 | Loss: 0.00003235
Iteration 79/1000 | Loss: 0.00003235
Iteration 80/1000 | Loss: 0.00003235
Iteration 81/1000 | Loss: 0.00003235
Iteration 82/1000 | Loss: 0.00003235
Iteration 83/1000 | Loss: 0.00004591
Iteration 84/1000 | Loss: 0.00003239
Iteration 85/1000 | Loss: 0.00006421
Iteration 86/1000 | Loss: 0.00003248
Iteration 87/1000 | Loss: 0.00003238
Iteration 88/1000 | Loss: 0.00003237
Iteration 89/1000 | Loss: 0.00003267
Iteration 90/1000 | Loss: 0.00004152
Iteration 91/1000 | Loss: 0.00009872
Iteration 92/1000 | Loss: 0.00003255
Iteration 93/1000 | Loss: 0.00003277
Iteration 94/1000 | Loss: 0.00003229
Iteration 95/1000 | Loss: 0.00003229
Iteration 96/1000 | Loss: 0.00003229
Iteration 97/1000 | Loss: 0.00003229
Iteration 98/1000 | Loss: 0.00003229
Iteration 99/1000 | Loss: 0.00003229
Iteration 100/1000 | Loss: 0.00003228
Iteration 101/1000 | Loss: 0.00003230
Iteration 102/1000 | Loss: 0.00003230
Iteration 103/1000 | Loss: 0.00003230
Iteration 104/1000 | Loss: 0.00003229
Iteration 105/1000 | Loss: 0.00003227
Iteration 106/1000 | Loss: 0.00003227
Iteration 107/1000 | Loss: 0.00003227
Iteration 108/1000 | Loss: 0.00003231
Iteration 109/1000 | Loss: 0.00003227
Iteration 110/1000 | Loss: 0.00003227
Iteration 111/1000 | Loss: 0.00003227
Iteration 112/1000 | Loss: 0.00003227
Iteration 113/1000 | Loss: 0.00003227
Iteration 114/1000 | Loss: 0.00003226
Iteration 115/1000 | Loss: 0.00003226
Iteration 116/1000 | Loss: 0.00003226
Iteration 117/1000 | Loss: 0.00003226
Iteration 118/1000 | Loss: 0.00003226
Iteration 119/1000 | Loss: 0.00003226
Iteration 120/1000 | Loss: 0.00003226
Iteration 121/1000 | Loss: 0.00003226
Iteration 122/1000 | Loss: 0.00003226
Iteration 123/1000 | Loss: 0.00003226
Iteration 124/1000 | Loss: 0.00003226
Iteration 125/1000 | Loss: 0.00003226
Iteration 126/1000 | Loss: 0.00003226
Iteration 127/1000 | Loss: 0.00003226
Iteration 128/1000 | Loss: 0.00003226
Iteration 129/1000 | Loss: 0.00003226
Iteration 130/1000 | Loss: 0.00003226
Iteration 131/1000 | Loss: 0.00003226
Iteration 132/1000 | Loss: 0.00003226
Iteration 133/1000 | Loss: 0.00003226
Iteration 134/1000 | Loss: 0.00003226
Iteration 135/1000 | Loss: 0.00003226
Iteration 136/1000 | Loss: 0.00003226
Iteration 137/1000 | Loss: 0.00003226
Iteration 138/1000 | Loss: 0.00003226
Iteration 139/1000 | Loss: 0.00003226
Iteration 140/1000 | Loss: 0.00003226
Iteration 141/1000 | Loss: 0.00003226
Iteration 142/1000 | Loss: 0.00003226
Iteration 143/1000 | Loss: 0.00003226
Iteration 144/1000 | Loss: 0.00003226
Iteration 145/1000 | Loss: 0.00003226
Iteration 146/1000 | Loss: 0.00003226
Iteration 147/1000 | Loss: 0.00003226
Iteration 148/1000 | Loss: 0.00003226
Iteration 149/1000 | Loss: 0.00003226
Iteration 150/1000 | Loss: 0.00003226
Iteration 151/1000 | Loss: 0.00003226
Iteration 152/1000 | Loss: 0.00003226
Iteration 153/1000 | Loss: 0.00003226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.226466287742369e-05, 3.226466287742369e-05, 3.226466287742369e-05, 3.226466287742369e-05, 3.226466287742369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.226466287742369e-05

Optimization complete. Final v2v error: 4.817848205566406 mm

Highest mean error: 9.586787223815918 mm for frame 15

Lowest mean error: 4.488499641418457 mm for frame 118

Saving results

Total time: 123.04330825805664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830766
Iteration 2/25 | Loss: 0.00135472
Iteration 3/25 | Loss: 0.00104113
Iteration 4/25 | Loss: 0.00098150
Iteration 5/25 | Loss: 0.00096731
Iteration 6/25 | Loss: 0.00096353
Iteration 7/25 | Loss: 0.00096199
Iteration 8/25 | Loss: 0.00096160
Iteration 9/25 | Loss: 0.00096160
Iteration 10/25 | Loss: 0.00096160
Iteration 11/25 | Loss: 0.00096160
Iteration 12/25 | Loss: 0.00096160
Iteration 13/25 | Loss: 0.00096160
Iteration 14/25 | Loss: 0.00096160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009616019087843597, 0.0009616019087843597, 0.0009616019087843597, 0.0009616019087843597, 0.0009616019087843597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009616019087843597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56316543
Iteration 2/25 | Loss: 0.00140877
Iteration 3/25 | Loss: 0.00140877
Iteration 4/25 | Loss: 0.00140877
Iteration 5/25 | Loss: 0.00140877
Iteration 6/25 | Loss: 0.00140877
Iteration 7/25 | Loss: 0.00140877
Iteration 8/25 | Loss: 0.00140877
Iteration 9/25 | Loss: 0.00140877
Iteration 10/25 | Loss: 0.00140877
Iteration 11/25 | Loss: 0.00140877
Iteration 12/25 | Loss: 0.00140877
Iteration 13/25 | Loss: 0.00140877
Iteration 14/25 | Loss: 0.00140877
Iteration 15/25 | Loss: 0.00140877
Iteration 16/25 | Loss: 0.00140877
Iteration 17/25 | Loss: 0.00140877
Iteration 18/25 | Loss: 0.00140877
Iteration 19/25 | Loss: 0.00140877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001408766838721931, 0.001408766838721931, 0.001408766838721931, 0.001408766838721931, 0.001408766838721931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001408766838721931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140877
Iteration 2/1000 | Loss: 0.00005680
Iteration 3/1000 | Loss: 0.00003884
Iteration 4/1000 | Loss: 0.00003161
Iteration 5/1000 | Loss: 0.00002903
Iteration 6/1000 | Loss: 0.00002699
Iteration 7/1000 | Loss: 0.00002594
Iteration 8/1000 | Loss: 0.00002509
Iteration 9/1000 | Loss: 0.00002438
Iteration 10/1000 | Loss: 0.00002388
Iteration 11/1000 | Loss: 0.00002362
Iteration 12/1000 | Loss: 0.00002338
Iteration 13/1000 | Loss: 0.00002319
Iteration 14/1000 | Loss: 0.00002302
Iteration 15/1000 | Loss: 0.00002298
Iteration 16/1000 | Loss: 0.00002297
Iteration 17/1000 | Loss: 0.00002296
Iteration 18/1000 | Loss: 0.00002295
Iteration 19/1000 | Loss: 0.00002294
Iteration 20/1000 | Loss: 0.00002293
Iteration 21/1000 | Loss: 0.00002291
Iteration 22/1000 | Loss: 0.00002291
Iteration 23/1000 | Loss: 0.00002290
Iteration 24/1000 | Loss: 0.00002286
Iteration 25/1000 | Loss: 0.00002283
Iteration 26/1000 | Loss: 0.00002283
Iteration 27/1000 | Loss: 0.00002280
Iteration 28/1000 | Loss: 0.00002279
Iteration 29/1000 | Loss: 0.00002278
Iteration 30/1000 | Loss: 0.00002278
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002276
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002274
Iteration 39/1000 | Loss: 0.00002274
Iteration 40/1000 | Loss: 0.00002274
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002273
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002272
Iteration 46/1000 | Loss: 0.00002272
Iteration 47/1000 | Loss: 0.00002272
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00002271
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002270
Iteration 53/1000 | Loss: 0.00002270
Iteration 54/1000 | Loss: 0.00002270
Iteration 55/1000 | Loss: 0.00002270
Iteration 56/1000 | Loss: 0.00002270
Iteration 57/1000 | Loss: 0.00002270
Iteration 58/1000 | Loss: 0.00002270
Iteration 59/1000 | Loss: 0.00002270
Iteration 60/1000 | Loss: 0.00002270
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002269
Iteration 63/1000 | Loss: 0.00002269
Iteration 64/1000 | Loss: 0.00002269
Iteration 65/1000 | Loss: 0.00002269
Iteration 66/1000 | Loss: 0.00002269
Iteration 67/1000 | Loss: 0.00002269
Iteration 68/1000 | Loss: 0.00002269
Iteration 69/1000 | Loss: 0.00002269
Iteration 70/1000 | Loss: 0.00002268
Iteration 71/1000 | Loss: 0.00002268
Iteration 72/1000 | Loss: 0.00002268
Iteration 73/1000 | Loss: 0.00002268
Iteration 74/1000 | Loss: 0.00002268
Iteration 75/1000 | Loss: 0.00002268
Iteration 76/1000 | Loss: 0.00002268
Iteration 77/1000 | Loss: 0.00002268
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002268
Iteration 82/1000 | Loss: 0.00002268
Iteration 83/1000 | Loss: 0.00002268
Iteration 84/1000 | Loss: 0.00002268
Iteration 85/1000 | Loss: 0.00002268
Iteration 86/1000 | Loss: 0.00002268
Iteration 87/1000 | Loss: 0.00002268
Iteration 88/1000 | Loss: 0.00002268
Iteration 89/1000 | Loss: 0.00002268
Iteration 90/1000 | Loss: 0.00002268
Iteration 91/1000 | Loss: 0.00002268
Iteration 92/1000 | Loss: 0.00002268
Iteration 93/1000 | Loss: 0.00002268
Iteration 94/1000 | Loss: 0.00002268
Iteration 95/1000 | Loss: 0.00002268
Iteration 96/1000 | Loss: 0.00002268
Iteration 97/1000 | Loss: 0.00002268
Iteration 98/1000 | Loss: 0.00002268
Iteration 99/1000 | Loss: 0.00002268
Iteration 100/1000 | Loss: 0.00002268
Iteration 101/1000 | Loss: 0.00002268
Iteration 102/1000 | Loss: 0.00002268
Iteration 103/1000 | Loss: 0.00002268
Iteration 104/1000 | Loss: 0.00002268
Iteration 105/1000 | Loss: 0.00002268
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00002268
Iteration 108/1000 | Loss: 0.00002268
Iteration 109/1000 | Loss: 0.00002268
Iteration 110/1000 | Loss: 0.00002268
Iteration 111/1000 | Loss: 0.00002268
Iteration 112/1000 | Loss: 0.00002268
Iteration 113/1000 | Loss: 0.00002268
Iteration 114/1000 | Loss: 0.00002268
Iteration 115/1000 | Loss: 0.00002268
Iteration 116/1000 | Loss: 0.00002268
Iteration 117/1000 | Loss: 0.00002268
Iteration 118/1000 | Loss: 0.00002268
Iteration 119/1000 | Loss: 0.00002268
Iteration 120/1000 | Loss: 0.00002268
Iteration 121/1000 | Loss: 0.00002268
Iteration 122/1000 | Loss: 0.00002268
Iteration 123/1000 | Loss: 0.00002268
Iteration 124/1000 | Loss: 0.00002268
Iteration 125/1000 | Loss: 0.00002268
Iteration 126/1000 | Loss: 0.00002268
Iteration 127/1000 | Loss: 0.00002268
Iteration 128/1000 | Loss: 0.00002268
Iteration 129/1000 | Loss: 0.00002268
Iteration 130/1000 | Loss: 0.00002268
Iteration 131/1000 | Loss: 0.00002268
Iteration 132/1000 | Loss: 0.00002268
Iteration 133/1000 | Loss: 0.00002268
Iteration 134/1000 | Loss: 0.00002268
Iteration 135/1000 | Loss: 0.00002268
Iteration 136/1000 | Loss: 0.00002268
Iteration 137/1000 | Loss: 0.00002268
Iteration 138/1000 | Loss: 0.00002268
Iteration 139/1000 | Loss: 0.00002268
Iteration 140/1000 | Loss: 0.00002268
Iteration 141/1000 | Loss: 0.00002268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2676991648040712e-05, 2.2676991648040712e-05, 2.2676991648040712e-05, 2.2676991648040712e-05, 2.2676991648040712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2676991648040712e-05

Optimization complete. Final v2v error: 3.9617624282836914 mm

Highest mean error: 5.062007904052734 mm for frame 74

Lowest mean error: 3.465160608291626 mm for frame 168

Saving results

Total time: 39.58793568611145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918859
Iteration 2/25 | Loss: 0.00087172
Iteration 3/25 | Loss: 0.00079073
Iteration 4/25 | Loss: 0.00077596
Iteration 5/25 | Loss: 0.00077150
Iteration 6/25 | Loss: 0.00077042
Iteration 7/25 | Loss: 0.00077032
Iteration 8/25 | Loss: 0.00077032
Iteration 9/25 | Loss: 0.00077032
Iteration 10/25 | Loss: 0.00077032
Iteration 11/25 | Loss: 0.00077032
Iteration 12/25 | Loss: 0.00077032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000770318612921983, 0.000770318612921983, 0.000770318612921983, 0.000770318612921983, 0.000770318612921983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000770318612921983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49445605
Iteration 2/25 | Loss: 0.00073164
Iteration 3/25 | Loss: 0.00073164
Iteration 4/25 | Loss: 0.00073164
Iteration 5/25 | Loss: 0.00073164
Iteration 6/25 | Loss: 0.00073164
Iteration 7/25 | Loss: 0.00073164
Iteration 8/25 | Loss: 0.00073164
Iteration 9/25 | Loss: 0.00073164
Iteration 10/25 | Loss: 0.00073164
Iteration 11/25 | Loss: 0.00073164
Iteration 12/25 | Loss: 0.00073164
Iteration 13/25 | Loss: 0.00073164
Iteration 14/25 | Loss: 0.00073164
Iteration 15/25 | Loss: 0.00073164
Iteration 16/25 | Loss: 0.00073164
Iteration 17/25 | Loss: 0.00073164
Iteration 18/25 | Loss: 0.00073164
Iteration 19/25 | Loss: 0.00073164
Iteration 20/25 | Loss: 0.00073164
Iteration 21/25 | Loss: 0.00073164
Iteration 22/25 | Loss: 0.00073164
Iteration 23/25 | Loss: 0.00073164
Iteration 24/25 | Loss: 0.00073164
Iteration 25/25 | Loss: 0.00073164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073164
Iteration 2/1000 | Loss: 0.00003014
Iteration 3/1000 | Loss: 0.00002445
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00002014
Iteration 9/1000 | Loss: 0.00002007
Iteration 10/1000 | Loss: 0.00002006
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00002006
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00002005
Iteration 16/1000 | Loss: 0.00002002
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001999
Iteration 19/1000 | Loss: 0.00001999
Iteration 20/1000 | Loss: 0.00001999
Iteration 21/1000 | Loss: 0.00001998
Iteration 22/1000 | Loss: 0.00001998
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00001994
Iteration 25/1000 | Loss: 0.00001994
Iteration 26/1000 | Loss: 0.00001994
Iteration 27/1000 | Loss: 0.00001993
Iteration 28/1000 | Loss: 0.00001993
Iteration 29/1000 | Loss: 0.00001992
Iteration 30/1000 | Loss: 0.00001992
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001987
Iteration 34/1000 | Loss: 0.00001987
Iteration 35/1000 | Loss: 0.00001986
Iteration 36/1000 | Loss: 0.00001986
Iteration 37/1000 | Loss: 0.00001986
Iteration 38/1000 | Loss: 0.00001985
Iteration 39/1000 | Loss: 0.00001985
Iteration 40/1000 | Loss: 0.00001984
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001983
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001982
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001981
Iteration 50/1000 | Loss: 0.00001981
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001980
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001980
Iteration 56/1000 | Loss: 0.00001979
Iteration 57/1000 | Loss: 0.00001979
Iteration 58/1000 | Loss: 0.00001979
Iteration 59/1000 | Loss: 0.00001979
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001978
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001977
Iteration 66/1000 | Loss: 0.00001977
Iteration 67/1000 | Loss: 0.00001977
Iteration 68/1000 | Loss: 0.00001977
Iteration 69/1000 | Loss: 0.00001977
Iteration 70/1000 | Loss: 0.00001977
Iteration 71/1000 | Loss: 0.00001976
Iteration 72/1000 | Loss: 0.00001976
Iteration 73/1000 | Loss: 0.00001976
Iteration 74/1000 | Loss: 0.00001976
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001975
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.9695080482051708e-05, 1.9695080482051708e-05, 1.9695080482051708e-05, 1.9695080482051708e-05, 1.9695080482051708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9695080482051708e-05

Optimization complete. Final v2v error: 3.7936127185821533 mm

Highest mean error: 4.026034355163574 mm for frame 70

Lowest mean error: 3.4972102642059326 mm for frame 112

Saving results

Total time: 27.571346521377563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776997
Iteration 2/25 | Loss: 0.00097138
Iteration 3/25 | Loss: 0.00085837
Iteration 4/25 | Loss: 0.00083933
Iteration 5/25 | Loss: 0.00083470
Iteration 6/25 | Loss: 0.00083333
Iteration 7/25 | Loss: 0.00083327
Iteration 8/25 | Loss: 0.00083327
Iteration 9/25 | Loss: 0.00083327
Iteration 10/25 | Loss: 0.00083327
Iteration 11/25 | Loss: 0.00083327
Iteration 12/25 | Loss: 0.00083327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008332731667906046, 0.0008332731667906046, 0.0008332731667906046, 0.0008332731667906046, 0.0008332731667906046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008332731667906046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.58581972
Iteration 2/25 | Loss: 0.00067855
Iteration 3/25 | Loss: 0.00067854
Iteration 4/25 | Loss: 0.00067854
Iteration 5/25 | Loss: 0.00067854
Iteration 6/25 | Loss: 0.00067854
Iteration 7/25 | Loss: 0.00067854
Iteration 8/25 | Loss: 0.00067854
Iteration 9/25 | Loss: 0.00067854
Iteration 10/25 | Loss: 0.00067854
Iteration 11/25 | Loss: 0.00067854
Iteration 12/25 | Loss: 0.00067854
Iteration 13/25 | Loss: 0.00067854
Iteration 14/25 | Loss: 0.00067854
Iteration 15/25 | Loss: 0.00067854
Iteration 16/25 | Loss: 0.00067854
Iteration 17/25 | Loss: 0.00067854
Iteration 18/25 | Loss: 0.00067854
Iteration 19/25 | Loss: 0.00067854
Iteration 20/25 | Loss: 0.00067854
Iteration 21/25 | Loss: 0.00067854
Iteration 22/25 | Loss: 0.00067854
Iteration 23/25 | Loss: 0.00067854
Iteration 24/25 | Loss: 0.00067854
Iteration 25/25 | Loss: 0.00067854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067854
Iteration 2/1000 | Loss: 0.00004195
Iteration 3/1000 | Loss: 0.00002884
Iteration 4/1000 | Loss: 0.00002643
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002453
Iteration 7/1000 | Loss: 0.00002375
Iteration 8/1000 | Loss: 0.00002321
Iteration 9/1000 | Loss: 0.00002296
Iteration 10/1000 | Loss: 0.00002287
Iteration 11/1000 | Loss: 0.00002282
Iteration 12/1000 | Loss: 0.00002281
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002280
Iteration 15/1000 | Loss: 0.00002275
Iteration 16/1000 | Loss: 0.00002274
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002269
Iteration 19/1000 | Loss: 0.00002269
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002269
Iteration 22/1000 | Loss: 0.00002269
Iteration 23/1000 | Loss: 0.00002269
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002269
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002268
Iteration 29/1000 | Loss: 0.00002268
Iteration 30/1000 | Loss: 0.00002268
Iteration 31/1000 | Loss: 0.00002267
Iteration 32/1000 | Loss: 0.00002265
Iteration 33/1000 | Loss: 0.00002265
Iteration 34/1000 | Loss: 0.00002265
Iteration 35/1000 | Loss: 0.00002264
Iteration 36/1000 | Loss: 0.00002264
Iteration 37/1000 | Loss: 0.00002263
Iteration 38/1000 | Loss: 0.00002262
Iteration 39/1000 | Loss: 0.00002262
Iteration 40/1000 | Loss: 0.00002262
Iteration 41/1000 | Loss: 0.00002262
Iteration 42/1000 | Loss: 0.00002262
Iteration 43/1000 | Loss: 0.00002262
Iteration 44/1000 | Loss: 0.00002262
Iteration 45/1000 | Loss: 0.00002262
Iteration 46/1000 | Loss: 0.00002261
Iteration 47/1000 | Loss: 0.00002261
Iteration 48/1000 | Loss: 0.00002261
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002261
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002260
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002258
Iteration 60/1000 | Loss: 0.00002258
Iteration 61/1000 | Loss: 0.00002258
Iteration 62/1000 | Loss: 0.00002258
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002257
Iteration 67/1000 | Loss: 0.00002257
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002257
Iteration 71/1000 | Loss: 0.00002257
Iteration 72/1000 | Loss: 0.00002257
Iteration 73/1000 | Loss: 0.00002257
Iteration 74/1000 | Loss: 0.00002257
Iteration 75/1000 | Loss: 0.00002257
Iteration 76/1000 | Loss: 0.00002257
Iteration 77/1000 | Loss: 0.00002257
Iteration 78/1000 | Loss: 0.00002257
Iteration 79/1000 | Loss: 0.00002257
Iteration 80/1000 | Loss: 0.00002257
Iteration 81/1000 | Loss: 0.00002257
Iteration 82/1000 | Loss: 0.00002257
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002257
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002257
Iteration 94/1000 | Loss: 0.00002257
Iteration 95/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.2569718566956e-05, 2.2569718566956e-05, 2.2569718566956e-05, 2.2569718566956e-05, 2.2569718566956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2569718566956e-05

Optimization complete. Final v2v error: 4.043200969696045 mm

Highest mean error: 4.355724811553955 mm for frame 110

Lowest mean error: 3.662717342376709 mm for frame 31

Saving results

Total time: 28.89616560935974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00562116
Iteration 2/25 | Loss: 0.00118616
Iteration 3/25 | Loss: 0.00085757
Iteration 4/25 | Loss: 0.00082343
Iteration 5/25 | Loss: 0.00081084
Iteration 6/25 | Loss: 0.00080591
Iteration 7/25 | Loss: 0.00081085
Iteration 8/25 | Loss: 0.00080159
Iteration 9/25 | Loss: 0.00080103
Iteration 10/25 | Loss: 0.00079762
Iteration 11/25 | Loss: 0.00079611
Iteration 12/25 | Loss: 0.00079885
Iteration 13/25 | Loss: 0.00079832
Iteration 14/25 | Loss: 0.00079719
Iteration 15/25 | Loss: 0.00079663
Iteration 16/25 | Loss: 0.00079634
Iteration 17/25 | Loss: 0.00079772
Iteration 18/25 | Loss: 0.00079549
Iteration 19/25 | Loss: 0.00079438
Iteration 20/25 | Loss: 0.00079339
Iteration 21/25 | Loss: 0.00079306
Iteration 22/25 | Loss: 0.00079297
Iteration 23/25 | Loss: 0.00079294
Iteration 24/25 | Loss: 0.00079294
Iteration 25/25 | Loss: 0.00079294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76191318
Iteration 2/25 | Loss: 0.00089619
Iteration 3/25 | Loss: 0.00089619
Iteration 4/25 | Loss: 0.00089619
Iteration 5/25 | Loss: 0.00089619
Iteration 6/25 | Loss: 0.00089619
Iteration 7/25 | Loss: 0.00089619
Iteration 8/25 | Loss: 0.00089619
Iteration 9/25 | Loss: 0.00089619
Iteration 10/25 | Loss: 0.00089619
Iteration 11/25 | Loss: 0.00089619
Iteration 12/25 | Loss: 0.00089619
Iteration 13/25 | Loss: 0.00089619
Iteration 14/25 | Loss: 0.00089619
Iteration 15/25 | Loss: 0.00089619
Iteration 16/25 | Loss: 0.00089619
Iteration 17/25 | Loss: 0.00089619
Iteration 18/25 | Loss: 0.00089619
Iteration 19/25 | Loss: 0.00089619
Iteration 20/25 | Loss: 0.00089619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008961888961493969, 0.0008961888961493969, 0.0008961888961493969, 0.0008961888961493969, 0.0008961888961493969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008961888961493969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089619
Iteration 2/1000 | Loss: 0.00003705
Iteration 3/1000 | Loss: 0.00002703
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00002034
Iteration 7/1000 | Loss: 0.00001970
Iteration 8/1000 | Loss: 0.00001906
Iteration 9/1000 | Loss: 0.00001868
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001831
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001823
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001819
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001817
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001817
Iteration 28/1000 | Loss: 0.00001816
Iteration 29/1000 | Loss: 0.00001816
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001803
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001801
Iteration 89/1000 | Loss: 0.00001801
Iteration 90/1000 | Loss: 0.00001801
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001801
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001801
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001801
Iteration 99/1000 | Loss: 0.00001801
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001800
Iteration 102/1000 | Loss: 0.00001800
Iteration 103/1000 | Loss: 0.00001800
Iteration 104/1000 | Loss: 0.00001800
Iteration 105/1000 | Loss: 0.00001799
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001798
Iteration 110/1000 | Loss: 0.00001798
Iteration 111/1000 | Loss: 0.00001798
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001798
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001797
Iteration 119/1000 | Loss: 0.00001797
Iteration 120/1000 | Loss: 0.00001797
Iteration 121/1000 | Loss: 0.00001797
Iteration 122/1000 | Loss: 0.00001797
Iteration 123/1000 | Loss: 0.00001796
Iteration 124/1000 | Loss: 0.00001796
Iteration 125/1000 | Loss: 0.00001796
Iteration 126/1000 | Loss: 0.00001796
Iteration 127/1000 | Loss: 0.00001796
Iteration 128/1000 | Loss: 0.00001796
Iteration 129/1000 | Loss: 0.00001796
Iteration 130/1000 | Loss: 0.00001796
Iteration 131/1000 | Loss: 0.00001796
Iteration 132/1000 | Loss: 0.00001796
Iteration 133/1000 | Loss: 0.00001796
Iteration 134/1000 | Loss: 0.00001796
Iteration 135/1000 | Loss: 0.00001796
Iteration 136/1000 | Loss: 0.00001796
Iteration 137/1000 | Loss: 0.00001795
Iteration 138/1000 | Loss: 0.00001795
Iteration 139/1000 | Loss: 0.00001795
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001795
Iteration 142/1000 | Loss: 0.00001795
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00001795
Iteration 145/1000 | Loss: 0.00001795
Iteration 146/1000 | Loss: 0.00001795
Iteration 147/1000 | Loss: 0.00001795
Iteration 148/1000 | Loss: 0.00001795
Iteration 149/1000 | Loss: 0.00001795
Iteration 150/1000 | Loss: 0.00001795
Iteration 151/1000 | Loss: 0.00001795
Iteration 152/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.7946325897355564e-05, 1.7946325897355564e-05, 1.7946325897355564e-05, 1.7946325897355564e-05, 1.7946325897355564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7946325897355564e-05

Optimization complete. Final v2v error: 3.6495156288146973 mm

Highest mean error: 4.244869232177734 mm for frame 61

Lowest mean error: 3.269317150115967 mm for frame 183

Saving results

Total time: 72.31498527526855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412141
Iteration 2/25 | Loss: 0.00088458
Iteration 3/25 | Loss: 0.00078269
Iteration 4/25 | Loss: 0.00076421
Iteration 5/25 | Loss: 0.00075736
Iteration 6/25 | Loss: 0.00075524
Iteration 7/25 | Loss: 0.00075451
Iteration 8/25 | Loss: 0.00075449
Iteration 9/25 | Loss: 0.00075449
Iteration 10/25 | Loss: 0.00075449
Iteration 11/25 | Loss: 0.00075449
Iteration 12/25 | Loss: 0.00075449
Iteration 13/25 | Loss: 0.00075449
Iteration 14/25 | Loss: 0.00075449
Iteration 15/25 | Loss: 0.00075449
Iteration 16/25 | Loss: 0.00075449
Iteration 17/25 | Loss: 0.00075449
Iteration 18/25 | Loss: 0.00075449
Iteration 19/25 | Loss: 0.00075449
Iteration 20/25 | Loss: 0.00075449
Iteration 21/25 | Loss: 0.00075449
Iteration 22/25 | Loss: 0.00075449
Iteration 23/25 | Loss: 0.00075449
Iteration 24/25 | Loss: 0.00075449
Iteration 25/25 | Loss: 0.00075449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.97844219
Iteration 2/25 | Loss: 0.00072409
Iteration 3/25 | Loss: 0.00072409
Iteration 4/25 | Loss: 0.00072409
Iteration 5/25 | Loss: 0.00072409
Iteration 6/25 | Loss: 0.00072409
Iteration 7/25 | Loss: 0.00072409
Iteration 8/25 | Loss: 0.00072409
Iteration 9/25 | Loss: 0.00072409
Iteration 10/25 | Loss: 0.00072409
Iteration 11/25 | Loss: 0.00072409
Iteration 12/25 | Loss: 0.00072409
Iteration 13/25 | Loss: 0.00072409
Iteration 14/25 | Loss: 0.00072409
Iteration 15/25 | Loss: 0.00072409
Iteration 16/25 | Loss: 0.00072409
Iteration 17/25 | Loss: 0.00072409
Iteration 18/25 | Loss: 0.00072409
Iteration 19/25 | Loss: 0.00072409
Iteration 20/25 | Loss: 0.00072409
Iteration 21/25 | Loss: 0.00072409
Iteration 22/25 | Loss: 0.00072409
Iteration 23/25 | Loss: 0.00072409
Iteration 24/25 | Loss: 0.00072409
Iteration 25/25 | Loss: 0.00072409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072409
Iteration 2/1000 | Loss: 0.00002761
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001992
Iteration 5/1000 | Loss: 0.00001899
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001765
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001755
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001749
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001733
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001728
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001717
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001713
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001710
Iteration 106/1000 | Loss: 0.00001710
Iteration 107/1000 | Loss: 0.00001710
Iteration 108/1000 | Loss: 0.00001710
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001709
Iteration 114/1000 | Loss: 0.00001709
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001709
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001708
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001708
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001708
Iteration 144/1000 | Loss: 0.00001708
Iteration 145/1000 | Loss: 0.00001708
Iteration 146/1000 | Loss: 0.00001708
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.708413583401125e-05, 1.708413583401125e-05, 1.708413583401125e-05, 1.708413583401125e-05, 1.708413583401125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.708413583401125e-05

Optimization complete. Final v2v error: 3.530423402786255 mm

Highest mean error: 4.063429355621338 mm for frame 125

Lowest mean error: 2.815492868423462 mm for frame 183

Saving results

Total time: 34.721457958221436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894788
Iteration 2/25 | Loss: 0.00123018
Iteration 3/25 | Loss: 0.00094985
Iteration 4/25 | Loss: 0.00087591
Iteration 5/25 | Loss: 0.00085581
Iteration 6/25 | Loss: 0.00084783
Iteration 7/25 | Loss: 0.00084625
Iteration 8/25 | Loss: 0.00084632
Iteration 9/25 | Loss: 0.00084445
Iteration 10/25 | Loss: 0.00084247
Iteration 11/25 | Loss: 0.00084097
Iteration 12/25 | Loss: 0.00084057
Iteration 13/25 | Loss: 0.00084123
Iteration 14/25 | Loss: 0.00084675
Iteration 15/25 | Loss: 0.00083317
Iteration 16/25 | Loss: 0.00083089
Iteration 17/25 | Loss: 0.00083071
Iteration 18/25 | Loss: 0.00083070
Iteration 19/25 | Loss: 0.00083070
Iteration 20/25 | Loss: 0.00083070
Iteration 21/25 | Loss: 0.00083070
Iteration 22/25 | Loss: 0.00083070
Iteration 23/25 | Loss: 0.00083070
Iteration 24/25 | Loss: 0.00083070
Iteration 25/25 | Loss: 0.00083070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97396588
Iteration 2/25 | Loss: 0.00073745
Iteration 3/25 | Loss: 0.00073745
Iteration 4/25 | Loss: 0.00073745
Iteration 5/25 | Loss: 0.00073745
Iteration 6/25 | Loss: 0.00073745
Iteration 7/25 | Loss: 0.00073745
Iteration 8/25 | Loss: 0.00073745
Iteration 9/25 | Loss: 0.00073745
Iteration 10/25 | Loss: 0.00073745
Iteration 11/25 | Loss: 0.00073745
Iteration 12/25 | Loss: 0.00073745
Iteration 13/25 | Loss: 0.00073745
Iteration 14/25 | Loss: 0.00073745
Iteration 15/25 | Loss: 0.00073745
Iteration 16/25 | Loss: 0.00073745
Iteration 17/25 | Loss: 0.00073745
Iteration 18/25 | Loss: 0.00073745
Iteration 19/25 | Loss: 0.00073745
Iteration 20/25 | Loss: 0.00073745
Iteration 21/25 | Loss: 0.00073745
Iteration 22/25 | Loss: 0.00073745
Iteration 23/25 | Loss: 0.00073745
Iteration 24/25 | Loss: 0.00073745
Iteration 25/25 | Loss: 0.00073745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073745
Iteration 2/1000 | Loss: 0.00003724
Iteration 3/1000 | Loss: 0.00003072
Iteration 4/1000 | Loss: 0.00002813
Iteration 5/1000 | Loss: 0.00026541
Iteration 6/1000 | Loss: 0.00002991
Iteration 7/1000 | Loss: 0.00002774
Iteration 8/1000 | Loss: 0.00002603
Iteration 9/1000 | Loss: 0.00029436
Iteration 10/1000 | Loss: 0.00034993
Iteration 11/1000 | Loss: 0.00028110
Iteration 12/1000 | Loss: 0.00002763
Iteration 13/1000 | Loss: 0.00002435
Iteration 14/1000 | Loss: 0.00002284
Iteration 15/1000 | Loss: 0.00002225
Iteration 16/1000 | Loss: 0.00002178
Iteration 17/1000 | Loss: 0.00002159
Iteration 18/1000 | Loss: 0.00002158
Iteration 19/1000 | Loss: 0.00002157
Iteration 20/1000 | Loss: 0.00002156
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002152
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002152
Iteration 27/1000 | Loss: 0.00002151
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002150
Iteration 30/1000 | Loss: 0.00002149
Iteration 31/1000 | Loss: 0.00002148
Iteration 32/1000 | Loss: 0.00002148
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00002145
Iteration 35/1000 | Loss: 0.00002145
Iteration 36/1000 | Loss: 0.00002144
Iteration 37/1000 | Loss: 0.00002144
Iteration 38/1000 | Loss: 0.00002144
Iteration 39/1000 | Loss: 0.00002144
Iteration 40/1000 | Loss: 0.00002144
Iteration 41/1000 | Loss: 0.00002144
Iteration 42/1000 | Loss: 0.00002144
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002144
Iteration 45/1000 | Loss: 0.00002143
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002143
Iteration 48/1000 | Loss: 0.00002141
Iteration 49/1000 | Loss: 0.00002140
Iteration 50/1000 | Loss: 0.00002140
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002135
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002133
Iteration 68/1000 | Loss: 0.00002133
Iteration 69/1000 | Loss: 0.00002133
Iteration 70/1000 | Loss: 0.00002133
Iteration 71/1000 | Loss: 0.00002132
Iteration 72/1000 | Loss: 0.00002132
Iteration 73/1000 | Loss: 0.00002132
Iteration 74/1000 | Loss: 0.00002132
Iteration 75/1000 | Loss: 0.00002132
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00002131
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002130
Iteration 82/1000 | Loss: 0.00002130
Iteration 83/1000 | Loss: 0.00002130
Iteration 84/1000 | Loss: 0.00002130
Iteration 85/1000 | Loss: 0.00002130
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002129
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002127
Iteration 92/1000 | Loss: 0.00002126
Iteration 93/1000 | Loss: 0.00002126
Iteration 94/1000 | Loss: 0.00002126
Iteration 95/1000 | Loss: 0.00002126
Iteration 96/1000 | Loss: 0.00002126
Iteration 97/1000 | Loss: 0.00002126
Iteration 98/1000 | Loss: 0.00002126
Iteration 99/1000 | Loss: 0.00002126
Iteration 100/1000 | Loss: 0.00002126
Iteration 101/1000 | Loss: 0.00002126
Iteration 102/1000 | Loss: 0.00002126
Iteration 103/1000 | Loss: 0.00002126
Iteration 104/1000 | Loss: 0.00002126
Iteration 105/1000 | Loss: 0.00002126
Iteration 106/1000 | Loss: 0.00002125
Iteration 107/1000 | Loss: 0.00002125
Iteration 108/1000 | Loss: 0.00002125
Iteration 109/1000 | Loss: 0.00002125
Iteration 110/1000 | Loss: 0.00002125
Iteration 111/1000 | Loss: 0.00002125
Iteration 112/1000 | Loss: 0.00002125
Iteration 113/1000 | Loss: 0.00002125
Iteration 114/1000 | Loss: 0.00002125
Iteration 115/1000 | Loss: 0.00002125
Iteration 116/1000 | Loss: 0.00002125
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002123
Iteration 135/1000 | Loss: 0.00002123
Iteration 136/1000 | Loss: 0.00002122
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002122
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002122
Iteration 142/1000 | Loss: 0.00002122
Iteration 143/1000 | Loss: 0.00002122
Iteration 144/1000 | Loss: 0.00002122
Iteration 145/1000 | Loss: 0.00002122
Iteration 146/1000 | Loss: 0.00002121
Iteration 147/1000 | Loss: 0.00002121
Iteration 148/1000 | Loss: 0.00002121
Iteration 149/1000 | Loss: 0.00002121
Iteration 150/1000 | Loss: 0.00002121
Iteration 151/1000 | Loss: 0.00002121
Iteration 152/1000 | Loss: 0.00002121
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002120
Iteration 155/1000 | Loss: 0.00002120
Iteration 156/1000 | Loss: 0.00002120
Iteration 157/1000 | Loss: 0.00002120
Iteration 158/1000 | Loss: 0.00002120
Iteration 159/1000 | Loss: 0.00002120
Iteration 160/1000 | Loss: 0.00002120
Iteration 161/1000 | Loss: 0.00002120
Iteration 162/1000 | Loss: 0.00002120
Iteration 163/1000 | Loss: 0.00002120
Iteration 164/1000 | Loss: 0.00002120
Iteration 165/1000 | Loss: 0.00002120
Iteration 166/1000 | Loss: 0.00002120
Iteration 167/1000 | Loss: 0.00002120
Iteration 168/1000 | Loss: 0.00002119
Iteration 169/1000 | Loss: 0.00002119
Iteration 170/1000 | Loss: 0.00002119
Iteration 171/1000 | Loss: 0.00002119
Iteration 172/1000 | Loss: 0.00002119
Iteration 173/1000 | Loss: 0.00002119
Iteration 174/1000 | Loss: 0.00002119
Iteration 175/1000 | Loss: 0.00002119
Iteration 176/1000 | Loss: 0.00002119
Iteration 177/1000 | Loss: 0.00002119
Iteration 178/1000 | Loss: 0.00002119
Iteration 179/1000 | Loss: 0.00002119
Iteration 180/1000 | Loss: 0.00002119
Iteration 181/1000 | Loss: 0.00002119
Iteration 182/1000 | Loss: 0.00002119
Iteration 183/1000 | Loss: 0.00002119
Iteration 184/1000 | Loss: 0.00002119
Iteration 185/1000 | Loss: 0.00002119
Iteration 186/1000 | Loss: 0.00002119
Iteration 187/1000 | Loss: 0.00002119
Iteration 188/1000 | Loss: 0.00002119
Iteration 189/1000 | Loss: 0.00002119
Iteration 190/1000 | Loss: 0.00002119
Iteration 191/1000 | Loss: 0.00002119
Iteration 192/1000 | Loss: 0.00002119
Iteration 193/1000 | Loss: 0.00002119
Iteration 194/1000 | Loss: 0.00002119
Iteration 195/1000 | Loss: 0.00002119
Iteration 196/1000 | Loss: 0.00002119
Iteration 197/1000 | Loss: 0.00002119
Iteration 198/1000 | Loss: 0.00002119
Iteration 199/1000 | Loss: 0.00002119
Iteration 200/1000 | Loss: 0.00002119
Iteration 201/1000 | Loss: 0.00002119
Iteration 202/1000 | Loss: 0.00002119
Iteration 203/1000 | Loss: 0.00002119
Iteration 204/1000 | Loss: 0.00002119
Iteration 205/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.118777956638951e-05, 2.118777956638951e-05, 2.118777956638951e-05, 2.118777956638951e-05, 2.118777956638951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.118777956638951e-05

Optimization complete. Final v2v error: 3.930008888244629 mm

Highest mean error: 4.434318542480469 mm for frame 50

Lowest mean error: 3.622751474380493 mm for frame 128

Saving results

Total time: 74.92228078842163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508953
Iteration 2/25 | Loss: 0.00094723
Iteration 3/25 | Loss: 0.00081322
Iteration 4/25 | Loss: 0.00078890
Iteration 5/25 | Loss: 0.00078119
Iteration 6/25 | Loss: 0.00077900
Iteration 7/25 | Loss: 0.00077857
Iteration 8/25 | Loss: 0.00077857
Iteration 9/25 | Loss: 0.00077857
Iteration 10/25 | Loss: 0.00077857
Iteration 11/25 | Loss: 0.00077857
Iteration 12/25 | Loss: 0.00077857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007785724592395127, 0.0007785724592395127, 0.0007785724592395127, 0.0007785724592395127, 0.0007785724592395127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007785724592395127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51535618
Iteration 2/25 | Loss: 0.00075951
Iteration 3/25 | Loss: 0.00075946
Iteration 4/25 | Loss: 0.00075946
Iteration 5/25 | Loss: 0.00075946
Iteration 6/25 | Loss: 0.00075946
Iteration 7/25 | Loss: 0.00075946
Iteration 8/25 | Loss: 0.00075946
Iteration 9/25 | Loss: 0.00075946
Iteration 10/25 | Loss: 0.00075946
Iteration 11/25 | Loss: 0.00075946
Iteration 12/25 | Loss: 0.00075946
Iteration 13/25 | Loss: 0.00075946
Iteration 14/25 | Loss: 0.00075946
Iteration 15/25 | Loss: 0.00075946
Iteration 16/25 | Loss: 0.00075946
Iteration 17/25 | Loss: 0.00075946
Iteration 18/25 | Loss: 0.00075946
Iteration 19/25 | Loss: 0.00075946
Iteration 20/25 | Loss: 0.00075946
Iteration 21/25 | Loss: 0.00075946
Iteration 22/25 | Loss: 0.00075946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007594609633088112, 0.0007594609633088112, 0.0007594609633088112, 0.0007594609633088112, 0.0007594609633088112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007594609633088112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075946
Iteration 2/1000 | Loss: 0.00003248
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001828
Iteration 6/1000 | Loss: 0.00001754
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001633
Iteration 17/1000 | Loss: 0.00001633
Iteration 18/1000 | Loss: 0.00001633
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001631
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001630
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001630
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001630
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001617
Iteration 84/1000 | Loss: 0.00001617
Iteration 85/1000 | Loss: 0.00001617
Iteration 86/1000 | Loss: 0.00001617
Iteration 87/1000 | Loss: 0.00001617
Iteration 88/1000 | Loss: 0.00001617
Iteration 89/1000 | Loss: 0.00001617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.6174533811863512e-05, 1.6174533811863512e-05, 1.6174533811863512e-05, 1.6174533811863512e-05, 1.6174533811863512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6174533811863512e-05

Optimization complete. Final v2v error: 3.50854754447937 mm

Highest mean error: 3.856959819793701 mm for frame 137

Lowest mean error: 3.120084047317505 mm for frame 57

Saving results

Total time: 28.27284002304077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915079
Iteration 2/25 | Loss: 0.00137318
Iteration 3/25 | Loss: 0.00095898
Iteration 4/25 | Loss: 0.00091499
Iteration 5/25 | Loss: 0.00090870
Iteration 6/25 | Loss: 0.00090783
Iteration 7/25 | Loss: 0.00090783
Iteration 8/25 | Loss: 0.00090783
Iteration 9/25 | Loss: 0.00090783
Iteration 10/25 | Loss: 0.00090783
Iteration 11/25 | Loss: 0.00090783
Iteration 12/25 | Loss: 0.00090783
Iteration 13/25 | Loss: 0.00090783
Iteration 14/25 | Loss: 0.00090783
Iteration 15/25 | Loss: 0.00090783
Iteration 16/25 | Loss: 0.00090783
Iteration 17/25 | Loss: 0.00090783
Iteration 18/25 | Loss: 0.00090783
Iteration 19/25 | Loss: 0.00090783
Iteration 20/25 | Loss: 0.00090783
Iteration 21/25 | Loss: 0.00090783
Iteration 22/25 | Loss: 0.00090783
Iteration 23/25 | Loss: 0.00090783
Iteration 24/25 | Loss: 0.00090783
Iteration 25/25 | Loss: 0.00090783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43439806
Iteration 2/25 | Loss: 0.00100308
Iteration 3/25 | Loss: 0.00100308
Iteration 4/25 | Loss: 0.00100307
Iteration 5/25 | Loss: 0.00100307
Iteration 6/25 | Loss: 0.00100307
Iteration 7/25 | Loss: 0.00100307
Iteration 8/25 | Loss: 0.00100307
Iteration 9/25 | Loss: 0.00100307
Iteration 10/25 | Loss: 0.00100307
Iteration 11/25 | Loss: 0.00100307
Iteration 12/25 | Loss: 0.00100307
Iteration 13/25 | Loss: 0.00100307
Iteration 14/25 | Loss: 0.00100307
Iteration 15/25 | Loss: 0.00100307
Iteration 16/25 | Loss: 0.00100307
Iteration 17/25 | Loss: 0.00100307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010030724806711078, 0.0010030724806711078, 0.0010030724806711078, 0.0010030724806711078, 0.0010030724806711078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010030724806711078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100307
Iteration 2/1000 | Loss: 0.00004218
Iteration 3/1000 | Loss: 0.00003608
Iteration 4/1000 | Loss: 0.00003338
Iteration 5/1000 | Loss: 0.00003218
Iteration 6/1000 | Loss: 0.00003131
Iteration 7/1000 | Loss: 0.00003054
Iteration 8/1000 | Loss: 0.00003002
Iteration 9/1000 | Loss: 0.00002973
Iteration 10/1000 | Loss: 0.00002970
Iteration 11/1000 | Loss: 0.00002961
Iteration 12/1000 | Loss: 0.00002960
Iteration 13/1000 | Loss: 0.00002950
Iteration 14/1000 | Loss: 0.00002949
Iteration 15/1000 | Loss: 0.00002948
Iteration 16/1000 | Loss: 0.00002941
Iteration 17/1000 | Loss: 0.00002940
Iteration 18/1000 | Loss: 0.00002940
Iteration 19/1000 | Loss: 0.00002940
Iteration 20/1000 | Loss: 0.00002940
Iteration 21/1000 | Loss: 0.00002940
Iteration 22/1000 | Loss: 0.00002940
Iteration 23/1000 | Loss: 0.00002940
Iteration 24/1000 | Loss: 0.00002940
Iteration 25/1000 | Loss: 0.00002938
Iteration 26/1000 | Loss: 0.00002938
Iteration 27/1000 | Loss: 0.00002938
Iteration 28/1000 | Loss: 0.00002937
Iteration 29/1000 | Loss: 0.00002937
Iteration 30/1000 | Loss: 0.00002937
Iteration 31/1000 | Loss: 0.00002936
Iteration 32/1000 | Loss: 0.00002936
Iteration 33/1000 | Loss: 0.00002936
Iteration 34/1000 | Loss: 0.00002935
Iteration 35/1000 | Loss: 0.00002935
Iteration 36/1000 | Loss: 0.00002935
Iteration 37/1000 | Loss: 0.00002935
Iteration 38/1000 | Loss: 0.00002935
Iteration 39/1000 | Loss: 0.00002935
Iteration 40/1000 | Loss: 0.00002935
Iteration 41/1000 | Loss: 0.00002935
Iteration 42/1000 | Loss: 0.00002935
Iteration 43/1000 | Loss: 0.00002935
Iteration 44/1000 | Loss: 0.00002935
Iteration 45/1000 | Loss: 0.00002935
Iteration 46/1000 | Loss: 0.00002935
Iteration 47/1000 | Loss: 0.00002935
Iteration 48/1000 | Loss: 0.00002935
Iteration 49/1000 | Loss: 0.00002935
Iteration 50/1000 | Loss: 0.00002934
Iteration 51/1000 | Loss: 0.00002934
Iteration 52/1000 | Loss: 0.00002934
Iteration 53/1000 | Loss: 0.00002934
Iteration 54/1000 | Loss: 0.00002934
Iteration 55/1000 | Loss: 0.00002934
Iteration 56/1000 | Loss: 0.00002934
Iteration 57/1000 | Loss: 0.00002934
Iteration 58/1000 | Loss: 0.00002934
Iteration 59/1000 | Loss: 0.00002934
Iteration 60/1000 | Loss: 0.00002934
Iteration 61/1000 | Loss: 0.00002934
Iteration 62/1000 | Loss: 0.00002934
Iteration 63/1000 | Loss: 0.00002934
Iteration 64/1000 | Loss: 0.00002934
Iteration 65/1000 | Loss: 0.00002934
Iteration 66/1000 | Loss: 0.00002934
Iteration 67/1000 | Loss: 0.00002934
Iteration 68/1000 | Loss: 0.00002934
Iteration 69/1000 | Loss: 0.00002934
Iteration 70/1000 | Loss: 0.00002934
Iteration 71/1000 | Loss: 0.00002934
Iteration 72/1000 | Loss: 0.00002934
Iteration 73/1000 | Loss: 0.00002934
Iteration 74/1000 | Loss: 0.00002934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [2.934395342890639e-05, 2.934395342890639e-05, 2.934395342890639e-05, 2.934395342890639e-05, 2.934395342890639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.934395342890639e-05

Optimization complete. Final v2v error: 4.615773677825928 mm

Highest mean error: 5.3243513107299805 mm for frame 137

Lowest mean error: 3.7925126552581787 mm for frame 1

Saving results

Total time: 31.800574779510498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050189
Iteration 2/25 | Loss: 0.00177441
Iteration 3/25 | Loss: 0.00109981
Iteration 4/25 | Loss: 0.00104614
Iteration 5/25 | Loss: 0.00102718
Iteration 6/25 | Loss: 0.00102134
Iteration 7/25 | Loss: 0.00101992
Iteration 8/25 | Loss: 0.00101978
Iteration 9/25 | Loss: 0.00101978
Iteration 10/25 | Loss: 0.00101978
Iteration 11/25 | Loss: 0.00101978
Iteration 12/25 | Loss: 0.00101978
Iteration 13/25 | Loss: 0.00101978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010197757510468364, 0.0010197757510468364, 0.0010197757510468364, 0.0010197757510468364, 0.0010197757510468364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010197757510468364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61241877
Iteration 2/25 | Loss: 0.00056543
Iteration 3/25 | Loss: 0.00056543
Iteration 4/25 | Loss: 0.00056543
Iteration 5/25 | Loss: 0.00056543
Iteration 6/25 | Loss: 0.00056543
Iteration 7/25 | Loss: 0.00056543
Iteration 8/25 | Loss: 0.00056543
Iteration 9/25 | Loss: 0.00056543
Iteration 10/25 | Loss: 0.00056543
Iteration 11/25 | Loss: 0.00056542
Iteration 12/25 | Loss: 0.00056542
Iteration 13/25 | Loss: 0.00056542
Iteration 14/25 | Loss: 0.00056542
Iteration 15/25 | Loss: 0.00056542
Iteration 16/25 | Loss: 0.00056542
Iteration 17/25 | Loss: 0.00056542
Iteration 18/25 | Loss: 0.00056542
Iteration 19/25 | Loss: 0.00056542
Iteration 20/25 | Loss: 0.00056542
Iteration 21/25 | Loss: 0.00056542
Iteration 22/25 | Loss: 0.00056542
Iteration 23/25 | Loss: 0.00056542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005654249689541757, 0.0005654249689541757, 0.0005654249689541757, 0.0005654249689541757, 0.0005654249689541757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005654249689541757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056542
Iteration 2/1000 | Loss: 0.00007037
Iteration 3/1000 | Loss: 0.00005400
Iteration 4/1000 | Loss: 0.00004833
Iteration 5/1000 | Loss: 0.00004662
Iteration 6/1000 | Loss: 0.00004476
Iteration 7/1000 | Loss: 0.00004375
Iteration 8/1000 | Loss: 0.00004270
Iteration 9/1000 | Loss: 0.00004196
Iteration 10/1000 | Loss: 0.00004149
Iteration 11/1000 | Loss: 0.00004111
Iteration 12/1000 | Loss: 0.00004079
Iteration 13/1000 | Loss: 0.00004059
Iteration 14/1000 | Loss: 0.00004046
Iteration 15/1000 | Loss: 0.00004044
Iteration 16/1000 | Loss: 0.00004043
Iteration 17/1000 | Loss: 0.00004042
Iteration 18/1000 | Loss: 0.00004042
Iteration 19/1000 | Loss: 0.00004036
Iteration 20/1000 | Loss: 0.00004028
Iteration 21/1000 | Loss: 0.00004028
Iteration 22/1000 | Loss: 0.00004025
Iteration 23/1000 | Loss: 0.00004022
Iteration 24/1000 | Loss: 0.00004021
Iteration 25/1000 | Loss: 0.00004021
Iteration 26/1000 | Loss: 0.00004015
Iteration 27/1000 | Loss: 0.00004015
Iteration 28/1000 | Loss: 0.00004014
Iteration 29/1000 | Loss: 0.00004012
Iteration 30/1000 | Loss: 0.00004011
Iteration 31/1000 | Loss: 0.00004011
Iteration 32/1000 | Loss: 0.00004007
Iteration 33/1000 | Loss: 0.00004007
Iteration 34/1000 | Loss: 0.00004007
Iteration 35/1000 | Loss: 0.00004007
Iteration 36/1000 | Loss: 0.00004004
Iteration 37/1000 | Loss: 0.00004004
Iteration 38/1000 | Loss: 0.00004004
Iteration 39/1000 | Loss: 0.00004004
Iteration 40/1000 | Loss: 0.00004003
Iteration 41/1000 | Loss: 0.00004002
Iteration 42/1000 | Loss: 0.00004002
Iteration 43/1000 | Loss: 0.00004001
Iteration 44/1000 | Loss: 0.00004000
Iteration 45/1000 | Loss: 0.00004000
Iteration 46/1000 | Loss: 0.00004000
Iteration 47/1000 | Loss: 0.00003999
Iteration 48/1000 | Loss: 0.00003999
Iteration 49/1000 | Loss: 0.00003999
Iteration 50/1000 | Loss: 0.00003999
Iteration 51/1000 | Loss: 0.00003999
Iteration 52/1000 | Loss: 0.00003999
Iteration 53/1000 | Loss: 0.00003999
Iteration 54/1000 | Loss: 0.00003999
Iteration 55/1000 | Loss: 0.00003999
Iteration 56/1000 | Loss: 0.00003999
Iteration 57/1000 | Loss: 0.00003998
Iteration 58/1000 | Loss: 0.00003998
Iteration 59/1000 | Loss: 0.00003998
Iteration 60/1000 | Loss: 0.00003998
Iteration 61/1000 | Loss: 0.00003997
Iteration 62/1000 | Loss: 0.00003997
Iteration 63/1000 | Loss: 0.00003997
Iteration 64/1000 | Loss: 0.00003997
Iteration 65/1000 | Loss: 0.00003997
Iteration 66/1000 | Loss: 0.00003997
Iteration 67/1000 | Loss: 0.00003997
Iteration 68/1000 | Loss: 0.00003997
Iteration 69/1000 | Loss: 0.00003997
Iteration 70/1000 | Loss: 0.00003997
Iteration 71/1000 | Loss: 0.00003997
Iteration 72/1000 | Loss: 0.00003996
Iteration 73/1000 | Loss: 0.00003996
Iteration 74/1000 | Loss: 0.00003996
Iteration 75/1000 | Loss: 0.00003996
Iteration 76/1000 | Loss: 0.00003996
Iteration 77/1000 | Loss: 0.00003995
Iteration 78/1000 | Loss: 0.00003995
Iteration 79/1000 | Loss: 0.00003995
Iteration 80/1000 | Loss: 0.00003995
Iteration 81/1000 | Loss: 0.00003995
Iteration 82/1000 | Loss: 0.00003994
Iteration 83/1000 | Loss: 0.00003994
Iteration 84/1000 | Loss: 0.00003994
Iteration 85/1000 | Loss: 0.00003994
Iteration 86/1000 | Loss: 0.00003994
Iteration 87/1000 | Loss: 0.00003994
Iteration 88/1000 | Loss: 0.00003994
Iteration 89/1000 | Loss: 0.00003994
Iteration 90/1000 | Loss: 0.00003994
Iteration 91/1000 | Loss: 0.00003994
Iteration 92/1000 | Loss: 0.00003993
Iteration 93/1000 | Loss: 0.00003993
Iteration 94/1000 | Loss: 0.00003993
Iteration 95/1000 | Loss: 0.00003993
Iteration 96/1000 | Loss: 0.00003993
Iteration 97/1000 | Loss: 0.00003993
Iteration 98/1000 | Loss: 0.00003992
Iteration 99/1000 | Loss: 0.00003992
Iteration 100/1000 | Loss: 0.00003992
Iteration 101/1000 | Loss: 0.00003992
Iteration 102/1000 | Loss: 0.00003992
Iteration 103/1000 | Loss: 0.00003992
Iteration 104/1000 | Loss: 0.00003991
Iteration 105/1000 | Loss: 0.00003991
Iteration 106/1000 | Loss: 0.00003991
Iteration 107/1000 | Loss: 0.00003991
Iteration 108/1000 | Loss: 0.00003991
Iteration 109/1000 | Loss: 0.00003991
Iteration 110/1000 | Loss: 0.00003991
Iteration 111/1000 | Loss: 0.00003990
Iteration 112/1000 | Loss: 0.00003990
Iteration 113/1000 | Loss: 0.00003990
Iteration 114/1000 | Loss: 0.00003990
Iteration 115/1000 | Loss: 0.00003990
Iteration 116/1000 | Loss: 0.00003990
Iteration 117/1000 | Loss: 0.00003990
Iteration 118/1000 | Loss: 0.00003990
Iteration 119/1000 | Loss: 0.00003990
Iteration 120/1000 | Loss: 0.00003990
Iteration 121/1000 | Loss: 0.00003990
Iteration 122/1000 | Loss: 0.00003990
Iteration 123/1000 | Loss: 0.00003990
Iteration 124/1000 | Loss: 0.00003990
Iteration 125/1000 | Loss: 0.00003990
Iteration 126/1000 | Loss: 0.00003990
Iteration 127/1000 | Loss: 0.00003990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [3.989772449131124e-05, 3.989772449131124e-05, 3.989772449131124e-05, 3.989772449131124e-05, 3.989772449131124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.989772449131124e-05

Optimization complete. Final v2v error: 5.285764217376709 mm

Highest mean error: 5.688803195953369 mm for frame 97

Lowest mean error: 4.474030494689941 mm for frame 0

Saving results

Total time: 42.056272983551025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432589
Iteration 2/25 | Loss: 0.00106694
Iteration 3/25 | Loss: 0.00082277
Iteration 4/25 | Loss: 0.00078781
Iteration 5/25 | Loss: 0.00077680
Iteration 6/25 | Loss: 0.00077443
Iteration 7/25 | Loss: 0.00077377
Iteration 8/25 | Loss: 0.00077377
Iteration 9/25 | Loss: 0.00077377
Iteration 10/25 | Loss: 0.00077377
Iteration 11/25 | Loss: 0.00077377
Iteration 12/25 | Loss: 0.00077377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000773768057115376, 0.000773768057115376, 0.000773768057115376, 0.000773768057115376, 0.000773768057115376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000773768057115376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56186044
Iteration 2/25 | Loss: 0.00078840
Iteration 3/25 | Loss: 0.00078840
Iteration 4/25 | Loss: 0.00078840
Iteration 5/25 | Loss: 0.00078840
Iteration 6/25 | Loss: 0.00078840
Iteration 7/25 | Loss: 0.00078840
Iteration 8/25 | Loss: 0.00078839
Iteration 9/25 | Loss: 0.00078839
Iteration 10/25 | Loss: 0.00078839
Iteration 11/25 | Loss: 0.00078839
Iteration 12/25 | Loss: 0.00078839
Iteration 13/25 | Loss: 0.00078839
Iteration 14/25 | Loss: 0.00078839
Iteration 15/25 | Loss: 0.00078839
Iteration 16/25 | Loss: 0.00078839
Iteration 17/25 | Loss: 0.00078839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007883942453190684, 0.0007883942453190684, 0.0007883942453190684, 0.0007883942453190684, 0.0007883942453190684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007883942453190684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078839
Iteration 2/1000 | Loss: 0.00003275
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00002402
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002213
Iteration 7/1000 | Loss: 0.00002145
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002078
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002065
Iteration 13/1000 | Loss: 0.00002061
Iteration 14/1000 | Loss: 0.00002057
Iteration 15/1000 | Loss: 0.00002057
Iteration 16/1000 | Loss: 0.00002056
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002054
Iteration 23/1000 | Loss: 0.00002054
Iteration 24/1000 | Loss: 0.00002054
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00002054
Iteration 27/1000 | Loss: 0.00002054
Iteration 28/1000 | Loss: 0.00002054
Iteration 29/1000 | Loss: 0.00002054
Iteration 30/1000 | Loss: 0.00002054
Iteration 31/1000 | Loss: 0.00002054
Iteration 32/1000 | Loss: 0.00002054
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002053
Iteration 36/1000 | Loss: 0.00002053
Iteration 37/1000 | Loss: 0.00002052
Iteration 38/1000 | Loss: 0.00002052
Iteration 39/1000 | Loss: 0.00002052
Iteration 40/1000 | Loss: 0.00002052
Iteration 41/1000 | Loss: 0.00002052
Iteration 42/1000 | Loss: 0.00002052
Iteration 43/1000 | Loss: 0.00002051
Iteration 44/1000 | Loss: 0.00002051
Iteration 45/1000 | Loss: 0.00002051
Iteration 46/1000 | Loss: 0.00002050
Iteration 47/1000 | Loss: 0.00002050
Iteration 48/1000 | Loss: 0.00002050
Iteration 49/1000 | Loss: 0.00002049
Iteration 50/1000 | Loss: 0.00002049
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002048
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002046
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002046
Iteration 60/1000 | Loss: 0.00002045
Iteration 61/1000 | Loss: 0.00002045
Iteration 62/1000 | Loss: 0.00002045
Iteration 63/1000 | Loss: 0.00002045
Iteration 64/1000 | Loss: 0.00002044
Iteration 65/1000 | Loss: 0.00002044
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002044
Iteration 69/1000 | Loss: 0.00002044
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002043
Iteration 75/1000 | Loss: 0.00002043
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002042
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002041
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002040
Iteration 86/1000 | Loss: 0.00002040
Iteration 87/1000 | Loss: 0.00002040
Iteration 88/1000 | Loss: 0.00002040
Iteration 89/1000 | Loss: 0.00002039
Iteration 90/1000 | Loss: 0.00002039
Iteration 91/1000 | Loss: 0.00002039
Iteration 92/1000 | Loss: 0.00002039
Iteration 93/1000 | Loss: 0.00002039
Iteration 94/1000 | Loss: 0.00002039
Iteration 95/1000 | Loss: 0.00002038
Iteration 96/1000 | Loss: 0.00002038
Iteration 97/1000 | Loss: 0.00002038
Iteration 98/1000 | Loss: 0.00002038
Iteration 99/1000 | Loss: 0.00002037
Iteration 100/1000 | Loss: 0.00002037
Iteration 101/1000 | Loss: 0.00002037
Iteration 102/1000 | Loss: 0.00002037
Iteration 103/1000 | Loss: 0.00002037
Iteration 104/1000 | Loss: 0.00002037
Iteration 105/1000 | Loss: 0.00002037
Iteration 106/1000 | Loss: 0.00002037
Iteration 107/1000 | Loss: 0.00002037
Iteration 108/1000 | Loss: 0.00002037
Iteration 109/1000 | Loss: 0.00002036
Iteration 110/1000 | Loss: 0.00002036
Iteration 111/1000 | Loss: 0.00002036
Iteration 112/1000 | Loss: 0.00002036
Iteration 113/1000 | Loss: 0.00002036
Iteration 114/1000 | Loss: 0.00002036
Iteration 115/1000 | Loss: 0.00002036
Iteration 116/1000 | Loss: 0.00002036
Iteration 117/1000 | Loss: 0.00002036
Iteration 118/1000 | Loss: 0.00002036
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002034
Iteration 125/1000 | Loss: 0.00002034
Iteration 126/1000 | Loss: 0.00002034
Iteration 127/1000 | Loss: 0.00002034
Iteration 128/1000 | Loss: 0.00002034
Iteration 129/1000 | Loss: 0.00002033
Iteration 130/1000 | Loss: 0.00002033
Iteration 131/1000 | Loss: 0.00002032
Iteration 132/1000 | Loss: 0.00002032
Iteration 133/1000 | Loss: 0.00002032
Iteration 134/1000 | Loss: 0.00002032
Iteration 135/1000 | Loss: 0.00002032
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00002031
Iteration 140/1000 | Loss: 0.00002031
Iteration 141/1000 | Loss: 0.00002031
Iteration 142/1000 | Loss: 0.00002031
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002031
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002030
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002030
Iteration 157/1000 | Loss: 0.00002030
Iteration 158/1000 | Loss: 0.00002030
Iteration 159/1000 | Loss: 0.00002030
Iteration 160/1000 | Loss: 0.00002030
Iteration 161/1000 | Loss: 0.00002030
Iteration 162/1000 | Loss: 0.00002030
Iteration 163/1000 | Loss: 0.00002029
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002029
Iteration 166/1000 | Loss: 0.00002029
Iteration 167/1000 | Loss: 0.00002029
Iteration 168/1000 | Loss: 0.00002029
Iteration 169/1000 | Loss: 0.00002029
Iteration 170/1000 | Loss: 0.00002029
Iteration 171/1000 | Loss: 0.00002029
Iteration 172/1000 | Loss: 0.00002029
Iteration 173/1000 | Loss: 0.00002029
Iteration 174/1000 | Loss: 0.00002029
Iteration 175/1000 | Loss: 0.00002029
Iteration 176/1000 | Loss: 0.00002029
Iteration 177/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.0289464373490773e-05, 2.0289464373490773e-05, 2.0289464373490773e-05, 2.0289464373490773e-05, 2.0289464373490773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0289464373490773e-05

Optimization complete. Final v2v error: 3.83003306388855 mm

Highest mean error: 4.45263147354126 mm for frame 79

Lowest mean error: 3.2751827239990234 mm for frame 165

Saving results

Total time: 41.17483854293823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853106
Iteration 2/25 | Loss: 0.00116210
Iteration 3/25 | Loss: 0.00085105
Iteration 4/25 | Loss: 0.00082819
Iteration 5/25 | Loss: 0.00082305
Iteration 6/25 | Loss: 0.00082247
Iteration 7/25 | Loss: 0.00082247
Iteration 8/25 | Loss: 0.00082247
Iteration 9/25 | Loss: 0.00082247
Iteration 10/25 | Loss: 0.00082247
Iteration 11/25 | Loss: 0.00082247
Iteration 12/25 | Loss: 0.00082247
Iteration 13/25 | Loss: 0.00082247
Iteration 14/25 | Loss: 0.00082247
Iteration 15/25 | Loss: 0.00082247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008224715129472315, 0.0008224715129472315, 0.0008224715129472315, 0.0008224715129472315, 0.0008224715129472315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008224715129472315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47726715
Iteration 2/25 | Loss: 0.00091890
Iteration 3/25 | Loss: 0.00091890
Iteration 4/25 | Loss: 0.00091890
Iteration 5/25 | Loss: 0.00091890
Iteration 6/25 | Loss: 0.00091890
Iteration 7/25 | Loss: 0.00091890
Iteration 8/25 | Loss: 0.00091890
Iteration 9/25 | Loss: 0.00091890
Iteration 10/25 | Loss: 0.00091890
Iteration 11/25 | Loss: 0.00091890
Iteration 12/25 | Loss: 0.00091890
Iteration 13/25 | Loss: 0.00091890
Iteration 14/25 | Loss: 0.00091890
Iteration 15/25 | Loss: 0.00091890
Iteration 16/25 | Loss: 0.00091890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009188990225084126, 0.0009188990225084126, 0.0009188990225084126, 0.0009188990225084126, 0.0009188990225084126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009188990225084126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091890
Iteration 2/1000 | Loss: 0.00002728
Iteration 3/1000 | Loss: 0.00002382
Iteration 4/1000 | Loss: 0.00002232
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00002026
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002017
Iteration 15/1000 | Loss: 0.00002016
Iteration 16/1000 | Loss: 0.00002016
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002015
Iteration 19/1000 | Loss: 0.00002015
Iteration 20/1000 | Loss: 0.00002015
Iteration 21/1000 | Loss: 0.00002014
Iteration 22/1000 | Loss: 0.00002014
Iteration 23/1000 | Loss: 0.00002014
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002012
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00002008
Iteration 35/1000 | Loss: 0.00002008
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002007
Iteration 38/1000 | Loss: 0.00002007
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002006
Iteration 44/1000 | Loss: 0.00002006
Iteration 45/1000 | Loss: 0.00002005
Iteration 46/1000 | Loss: 0.00002005
Iteration 47/1000 | Loss: 0.00002004
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002002
Iteration 55/1000 | Loss: 0.00002002
Iteration 56/1000 | Loss: 0.00002002
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002002
Iteration 59/1000 | Loss: 0.00002002
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002001
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002001
Iteration 72/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.001094981096685e-05, 2.001094981096685e-05, 2.001094981096685e-05, 2.001094981096685e-05, 2.001094981096685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.001094981096685e-05

Optimization complete. Final v2v error: 3.7540221214294434 mm

Highest mean error: 4.17284631729126 mm for frame 23

Lowest mean error: 3.4624416828155518 mm for frame 195

Saving results

Total time: 27.126578330993652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402957
Iteration 2/25 | Loss: 0.00091822
Iteration 3/25 | Loss: 0.00081799
Iteration 4/25 | Loss: 0.00079937
Iteration 5/25 | Loss: 0.00079089
Iteration 6/25 | Loss: 0.00078898
Iteration 7/25 | Loss: 0.00078872
Iteration 8/25 | Loss: 0.00078872
Iteration 9/25 | Loss: 0.00078872
Iteration 10/25 | Loss: 0.00078872
Iteration 11/25 | Loss: 0.00078872
Iteration 12/25 | Loss: 0.00078872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007887225365266204, 0.0007887225365266204, 0.0007887225365266204, 0.0007887225365266204, 0.0007887225365266204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007887225365266204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51208758
Iteration 2/25 | Loss: 0.00072499
Iteration 3/25 | Loss: 0.00072498
Iteration 4/25 | Loss: 0.00072498
Iteration 5/25 | Loss: 0.00072498
Iteration 6/25 | Loss: 0.00072498
Iteration 7/25 | Loss: 0.00072498
Iteration 8/25 | Loss: 0.00072498
Iteration 9/25 | Loss: 0.00072498
Iteration 10/25 | Loss: 0.00072498
Iteration 11/25 | Loss: 0.00072498
Iteration 12/25 | Loss: 0.00072498
Iteration 13/25 | Loss: 0.00072498
Iteration 14/25 | Loss: 0.00072498
Iteration 15/25 | Loss: 0.00072498
Iteration 16/25 | Loss: 0.00072498
Iteration 17/25 | Loss: 0.00072498
Iteration 18/25 | Loss: 0.00072498
Iteration 19/25 | Loss: 0.00072498
Iteration 20/25 | Loss: 0.00072498
Iteration 21/25 | Loss: 0.00072498
Iteration 22/25 | Loss: 0.00072498
Iteration 23/25 | Loss: 0.00072498
Iteration 24/25 | Loss: 0.00072498
Iteration 25/25 | Loss: 0.00072498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072498
Iteration 2/1000 | Loss: 0.00003743
Iteration 3/1000 | Loss: 0.00002932
Iteration 4/1000 | Loss: 0.00002743
Iteration 5/1000 | Loss: 0.00002608
Iteration 6/1000 | Loss: 0.00002483
Iteration 7/1000 | Loss: 0.00002438
Iteration 8/1000 | Loss: 0.00002412
Iteration 9/1000 | Loss: 0.00002406
Iteration 10/1000 | Loss: 0.00002405
Iteration 11/1000 | Loss: 0.00002405
Iteration 12/1000 | Loss: 0.00002404
Iteration 13/1000 | Loss: 0.00002399
Iteration 14/1000 | Loss: 0.00002399
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002398
Iteration 17/1000 | Loss: 0.00002392
Iteration 18/1000 | Loss: 0.00002391
Iteration 19/1000 | Loss: 0.00002391
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002387
Iteration 23/1000 | Loss: 0.00002387
Iteration 24/1000 | Loss: 0.00002386
Iteration 25/1000 | Loss: 0.00002385
Iteration 26/1000 | Loss: 0.00002381
Iteration 27/1000 | Loss: 0.00002380
Iteration 28/1000 | Loss: 0.00002380
Iteration 29/1000 | Loss: 0.00002379
Iteration 30/1000 | Loss: 0.00002378
Iteration 31/1000 | Loss: 0.00002377
Iteration 32/1000 | Loss: 0.00002377
Iteration 33/1000 | Loss: 0.00002377
Iteration 34/1000 | Loss: 0.00002377
Iteration 35/1000 | Loss: 0.00002377
Iteration 36/1000 | Loss: 0.00002377
Iteration 37/1000 | Loss: 0.00002377
Iteration 38/1000 | Loss: 0.00002377
Iteration 39/1000 | Loss: 0.00002376
Iteration 40/1000 | Loss: 0.00002376
Iteration 41/1000 | Loss: 0.00002376
Iteration 42/1000 | Loss: 0.00002375
Iteration 43/1000 | Loss: 0.00002374
Iteration 44/1000 | Loss: 0.00002374
Iteration 45/1000 | Loss: 0.00002374
Iteration 46/1000 | Loss: 0.00002374
Iteration 47/1000 | Loss: 0.00002374
Iteration 48/1000 | Loss: 0.00002374
Iteration 49/1000 | Loss: 0.00002374
Iteration 50/1000 | Loss: 0.00002374
Iteration 51/1000 | Loss: 0.00002374
Iteration 52/1000 | Loss: 0.00002374
Iteration 53/1000 | Loss: 0.00002374
Iteration 54/1000 | Loss: 0.00002374
Iteration 55/1000 | Loss: 0.00002373
Iteration 56/1000 | Loss: 0.00002373
Iteration 57/1000 | Loss: 0.00002373
Iteration 58/1000 | Loss: 0.00002373
Iteration 59/1000 | Loss: 0.00002372
Iteration 60/1000 | Loss: 0.00002372
Iteration 61/1000 | Loss: 0.00002372
Iteration 62/1000 | Loss: 0.00002371
Iteration 63/1000 | Loss: 0.00002371
Iteration 64/1000 | Loss: 0.00002370
Iteration 65/1000 | Loss: 0.00002369
Iteration 66/1000 | Loss: 0.00002369
Iteration 67/1000 | Loss: 0.00002368
Iteration 68/1000 | Loss: 0.00002368
Iteration 69/1000 | Loss: 0.00002368
Iteration 70/1000 | Loss: 0.00002367
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002366
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002363
Iteration 82/1000 | Loss: 0.00002363
Iteration 83/1000 | Loss: 0.00002363
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002363
Iteration 86/1000 | Loss: 0.00002363
Iteration 87/1000 | Loss: 0.00002363
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002362
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002361
Iteration 93/1000 | Loss: 0.00002361
Iteration 94/1000 | Loss: 0.00002361
Iteration 95/1000 | Loss: 0.00002361
Iteration 96/1000 | Loss: 0.00002361
Iteration 97/1000 | Loss: 0.00002360
Iteration 98/1000 | Loss: 0.00002360
Iteration 99/1000 | Loss: 0.00002360
Iteration 100/1000 | Loss: 0.00002359
Iteration 101/1000 | Loss: 0.00002359
Iteration 102/1000 | Loss: 0.00002358
Iteration 103/1000 | Loss: 0.00002358
Iteration 104/1000 | Loss: 0.00002358
Iteration 105/1000 | Loss: 0.00002358
Iteration 106/1000 | Loss: 0.00002358
Iteration 107/1000 | Loss: 0.00002358
Iteration 108/1000 | Loss: 0.00002358
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002358
Iteration 111/1000 | Loss: 0.00002358
Iteration 112/1000 | Loss: 0.00002358
Iteration 113/1000 | Loss: 0.00002358
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00002358
Iteration 116/1000 | Loss: 0.00002358
Iteration 117/1000 | Loss: 0.00002358
Iteration 118/1000 | Loss: 0.00002358
Iteration 119/1000 | Loss: 0.00002358
Iteration 120/1000 | Loss: 0.00002358
Iteration 121/1000 | Loss: 0.00002357
Iteration 122/1000 | Loss: 0.00002357
Iteration 123/1000 | Loss: 0.00002357
Iteration 124/1000 | Loss: 0.00002357
Iteration 125/1000 | Loss: 0.00002357
Iteration 126/1000 | Loss: 0.00002357
Iteration 127/1000 | Loss: 0.00002357
Iteration 128/1000 | Loss: 0.00002357
Iteration 129/1000 | Loss: 0.00002357
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002357
Iteration 136/1000 | Loss: 0.00002357
Iteration 137/1000 | Loss: 0.00002357
Iteration 138/1000 | Loss: 0.00002357
Iteration 139/1000 | Loss: 0.00002357
Iteration 140/1000 | Loss: 0.00002357
Iteration 141/1000 | Loss: 0.00002357
Iteration 142/1000 | Loss: 0.00002357
Iteration 143/1000 | Loss: 0.00002357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.3574824808747508e-05, 2.3574824808747508e-05, 2.3574824808747508e-05, 2.3574824808747508e-05, 2.3574824808747508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3574824808747508e-05

Optimization complete. Final v2v error: 4.201075553894043 mm

Highest mean error: 4.444793224334717 mm for frame 42

Lowest mean error: 4.041928291320801 mm for frame 0

Saving results

Total time: 31.880229473114014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868128
Iteration 2/25 | Loss: 0.00101296
Iteration 3/25 | Loss: 0.00086474
Iteration 4/25 | Loss: 0.00083701
Iteration 5/25 | Loss: 0.00082705
Iteration 6/25 | Loss: 0.00082440
Iteration 7/25 | Loss: 0.00082337
Iteration 8/25 | Loss: 0.00082319
Iteration 9/25 | Loss: 0.00082319
Iteration 10/25 | Loss: 0.00082319
Iteration 11/25 | Loss: 0.00082319
Iteration 12/25 | Loss: 0.00082319
Iteration 13/25 | Loss: 0.00082319
Iteration 14/25 | Loss: 0.00082319
Iteration 15/25 | Loss: 0.00082319
Iteration 16/25 | Loss: 0.00082319
Iteration 17/25 | Loss: 0.00082319
Iteration 18/25 | Loss: 0.00082319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008231948013417423, 0.0008231948013417423, 0.0008231948013417423, 0.0008231948013417423, 0.0008231948013417423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008231948013417423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54999673
Iteration 2/25 | Loss: 0.00100550
Iteration 3/25 | Loss: 0.00100550
Iteration 4/25 | Loss: 0.00100550
Iteration 5/25 | Loss: 0.00100550
Iteration 6/25 | Loss: 0.00100550
Iteration 7/25 | Loss: 0.00100550
Iteration 8/25 | Loss: 0.00100550
Iteration 9/25 | Loss: 0.00100550
Iteration 10/25 | Loss: 0.00100550
Iteration 11/25 | Loss: 0.00100550
Iteration 12/25 | Loss: 0.00100550
Iteration 13/25 | Loss: 0.00100550
Iteration 14/25 | Loss: 0.00100550
Iteration 15/25 | Loss: 0.00100550
Iteration 16/25 | Loss: 0.00100550
Iteration 17/25 | Loss: 0.00100550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010054975282400846, 0.0010054975282400846, 0.0010054975282400846, 0.0010054975282400846, 0.0010054975282400846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010054975282400846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100550
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00002711
Iteration 4/1000 | Loss: 0.00002256
Iteration 5/1000 | Loss: 0.00002079
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001958
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001855
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00001854
Iteration 16/1000 | Loss: 0.00001853
Iteration 17/1000 | Loss: 0.00001851
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001844
Iteration 26/1000 | Loss: 0.00001842
Iteration 27/1000 | Loss: 0.00001840
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001840
Iteration 30/1000 | Loss: 0.00001838
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001832
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001829
Iteration 59/1000 | Loss: 0.00001829
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00001828
Iteration 63/1000 | Loss: 0.00001828
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001828
Iteration 66/1000 | Loss: 0.00001828
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001824
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001821
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001821
Iteration 107/1000 | Loss: 0.00001821
Iteration 108/1000 | Loss: 0.00001821
Iteration 109/1000 | Loss: 0.00001821
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001820
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001820
Iteration 119/1000 | Loss: 0.00001820
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001820
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001819
Iteration 130/1000 | Loss: 0.00001819
Iteration 131/1000 | Loss: 0.00001819
Iteration 132/1000 | Loss: 0.00001819
Iteration 133/1000 | Loss: 0.00001819
Iteration 134/1000 | Loss: 0.00001819
Iteration 135/1000 | Loss: 0.00001819
Iteration 136/1000 | Loss: 0.00001819
Iteration 137/1000 | Loss: 0.00001819
Iteration 138/1000 | Loss: 0.00001819
Iteration 139/1000 | Loss: 0.00001819
Iteration 140/1000 | Loss: 0.00001819
Iteration 141/1000 | Loss: 0.00001819
Iteration 142/1000 | Loss: 0.00001819
Iteration 143/1000 | Loss: 0.00001819
Iteration 144/1000 | Loss: 0.00001819
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001819
Iteration 147/1000 | Loss: 0.00001819
Iteration 148/1000 | Loss: 0.00001819
Iteration 149/1000 | Loss: 0.00001819
Iteration 150/1000 | Loss: 0.00001819
Iteration 151/1000 | Loss: 0.00001819
Iteration 152/1000 | Loss: 0.00001819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.8186145098297857e-05, 1.8186145098297857e-05, 1.8186145098297857e-05, 1.8186145098297857e-05, 1.8186145098297857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8186145098297857e-05

Optimization complete. Final v2v error: 3.58211088180542 mm

Highest mean error: 4.1137003898620605 mm for frame 80

Lowest mean error: 2.9655215740203857 mm for frame 103

Saving results

Total time: 35.8306405544281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997755
Iteration 2/25 | Loss: 0.00997755
Iteration 3/25 | Loss: 0.00997754
Iteration 4/25 | Loss: 0.00997754
Iteration 5/25 | Loss: 0.00997754
Iteration 6/25 | Loss: 0.00997754
Iteration 7/25 | Loss: 0.00997754
Iteration 8/25 | Loss: 0.00997753
Iteration 9/25 | Loss: 0.00997753
Iteration 10/25 | Loss: 0.00997753
Iteration 11/25 | Loss: 0.00997753
Iteration 12/25 | Loss: 0.00997752
Iteration 13/25 | Loss: 0.00997752
Iteration 14/25 | Loss: 0.00997752
Iteration 15/25 | Loss: 0.00997752
Iteration 16/25 | Loss: 0.00997751
Iteration 17/25 | Loss: 0.00997751
Iteration 18/25 | Loss: 0.00997751
Iteration 19/25 | Loss: 0.00997750
Iteration 20/25 | Loss: 0.00997750
Iteration 21/25 | Loss: 0.00997750
Iteration 22/25 | Loss: 0.00997750
Iteration 23/25 | Loss: 0.00997750
Iteration 24/25 | Loss: 0.00997749
Iteration 25/25 | Loss: 0.00997749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44972062
Iteration 2/25 | Loss: 0.12453815
Iteration 3/25 | Loss: 0.12359473
Iteration 4/25 | Loss: 0.12354059
Iteration 5/25 | Loss: 0.12354057
Iteration 6/25 | Loss: 0.12354057
Iteration 7/25 | Loss: 0.12354057
Iteration 8/25 | Loss: 0.12354057
Iteration 9/25 | Loss: 0.12354057
Iteration 10/25 | Loss: 0.12354057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.12354056537151337, 0.12354056537151337, 0.12354056537151337, 0.12354056537151337, 0.12354056537151337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12354056537151337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12354057
Iteration 2/1000 | Loss: 0.00216046
Iteration 3/1000 | Loss: 0.00169483
Iteration 4/1000 | Loss: 0.00059903
Iteration 5/1000 | Loss: 0.00029195
Iteration 6/1000 | Loss: 0.00054731
Iteration 7/1000 | Loss: 0.00012218
Iteration 8/1000 | Loss: 0.00025821
Iteration 9/1000 | Loss: 0.00013667
Iteration 10/1000 | Loss: 0.00030061
Iteration 11/1000 | Loss: 0.00013038
Iteration 12/1000 | Loss: 0.00015450
Iteration 13/1000 | Loss: 0.00009995
Iteration 14/1000 | Loss: 0.00024885
Iteration 15/1000 | Loss: 0.00041329
Iteration 16/1000 | Loss: 0.00008575
Iteration 17/1000 | Loss: 0.00005063
Iteration 18/1000 | Loss: 0.00003716
Iteration 19/1000 | Loss: 0.00002869
Iteration 20/1000 | Loss: 0.00005202
Iteration 21/1000 | Loss: 0.00007413
Iteration 22/1000 | Loss: 0.00008821
Iteration 23/1000 | Loss: 0.00017326
Iteration 24/1000 | Loss: 0.00003232
Iteration 25/1000 | Loss: 0.00003893
Iteration 26/1000 | Loss: 0.00003505
Iteration 27/1000 | Loss: 0.00003020
Iteration 28/1000 | Loss: 0.00004025
Iteration 29/1000 | Loss: 0.00015015
Iteration 30/1000 | Loss: 0.00013802
Iteration 31/1000 | Loss: 0.00003664
Iteration 32/1000 | Loss: 0.00002168
Iteration 33/1000 | Loss: 0.00005210
Iteration 34/1000 | Loss: 0.00002549
Iteration 35/1000 | Loss: 0.00003310
Iteration 36/1000 | Loss: 0.00002100
Iteration 37/1000 | Loss: 0.00006076
Iteration 38/1000 | Loss: 0.00003857
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002279
Iteration 41/1000 | Loss: 0.00003900
Iteration 42/1000 | Loss: 0.00010109
Iteration 43/1000 | Loss: 0.00006206
Iteration 44/1000 | Loss: 0.00012137
Iteration 45/1000 | Loss: 0.00002551
Iteration 46/1000 | Loss: 0.00002060
Iteration 47/1000 | Loss: 0.00005720
Iteration 48/1000 | Loss: 0.00002671
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00003695
Iteration 52/1000 | Loss: 0.00005170
Iteration 53/1000 | Loss: 0.00002017
Iteration 54/1000 | Loss: 0.00001935
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001928
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00009560
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002402
Iteration 65/1000 | Loss: 0.00002144
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001892
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001878
Iteration 96/1000 | Loss: 0.00001878
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00005519
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00003124
Iteration 105/1000 | Loss: 0.00001877
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00002479
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001871
Iteration 125/1000 | Loss: 0.00001871
Iteration 126/1000 | Loss: 0.00001871
Iteration 127/1000 | Loss: 0.00001871
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001869
Iteration 135/1000 | Loss: 0.00001869
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001868
Iteration 141/1000 | Loss: 0.00001868
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001866
Iteration 144/1000 | Loss: 0.00001866
Iteration 145/1000 | Loss: 0.00001866
Iteration 146/1000 | Loss: 0.00001866
Iteration 147/1000 | Loss: 0.00001865
Iteration 148/1000 | Loss: 0.00001865
Iteration 149/1000 | Loss: 0.00001864
Iteration 150/1000 | Loss: 0.00001864
Iteration 151/1000 | Loss: 0.00001951
Iteration 152/1000 | Loss: 0.00001867
Iteration 153/1000 | Loss: 0.00001863
Iteration 154/1000 | Loss: 0.00001863
Iteration 155/1000 | Loss: 0.00001862
Iteration 156/1000 | Loss: 0.00001862
Iteration 157/1000 | Loss: 0.00001862
Iteration 158/1000 | Loss: 0.00001862
Iteration 159/1000 | Loss: 0.00001861
Iteration 160/1000 | Loss: 0.00001861
Iteration 161/1000 | Loss: 0.00001861
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001861
Iteration 165/1000 | Loss: 0.00001861
Iteration 166/1000 | Loss: 0.00001861
Iteration 167/1000 | Loss: 0.00001861
Iteration 168/1000 | Loss: 0.00001861
Iteration 169/1000 | Loss: 0.00001861
Iteration 170/1000 | Loss: 0.00001860
Iteration 171/1000 | Loss: 0.00001860
Iteration 172/1000 | Loss: 0.00001860
Iteration 173/1000 | Loss: 0.00001860
Iteration 174/1000 | Loss: 0.00001860
Iteration 175/1000 | Loss: 0.00001860
Iteration 176/1000 | Loss: 0.00001860
Iteration 177/1000 | Loss: 0.00001860
Iteration 178/1000 | Loss: 0.00001860
Iteration 179/1000 | Loss: 0.00001860
Iteration 180/1000 | Loss: 0.00001860
Iteration 181/1000 | Loss: 0.00001860
Iteration 182/1000 | Loss: 0.00001860
Iteration 183/1000 | Loss: 0.00001860
Iteration 184/1000 | Loss: 0.00001859
Iteration 185/1000 | Loss: 0.00001859
Iteration 186/1000 | Loss: 0.00001859
Iteration 187/1000 | Loss: 0.00001858
Iteration 188/1000 | Loss: 0.00001857
Iteration 189/1000 | Loss: 0.00001856
Iteration 190/1000 | Loss: 0.00001856
Iteration 191/1000 | Loss: 0.00001856
Iteration 192/1000 | Loss: 0.00001856
Iteration 193/1000 | Loss: 0.00001856
Iteration 194/1000 | Loss: 0.00001856
Iteration 195/1000 | Loss: 0.00001856
Iteration 196/1000 | Loss: 0.00001856
Iteration 197/1000 | Loss: 0.00001856
Iteration 198/1000 | Loss: 0.00001856
Iteration 199/1000 | Loss: 0.00001856
Iteration 200/1000 | Loss: 0.00001855
Iteration 201/1000 | Loss: 0.00001855
Iteration 202/1000 | Loss: 0.00001855
Iteration 203/1000 | Loss: 0.00001855
Iteration 204/1000 | Loss: 0.00001854
Iteration 205/1000 | Loss: 0.00001854
Iteration 206/1000 | Loss: 0.00001854
Iteration 207/1000 | Loss: 0.00001853
Iteration 208/1000 | Loss: 0.00001853
Iteration 209/1000 | Loss: 0.00001853
Iteration 210/1000 | Loss: 0.00001853
Iteration 211/1000 | Loss: 0.00001852
Iteration 212/1000 | Loss: 0.00001852
Iteration 213/1000 | Loss: 0.00001852
Iteration 214/1000 | Loss: 0.00001852
Iteration 215/1000 | Loss: 0.00001852
Iteration 216/1000 | Loss: 0.00001852
Iteration 217/1000 | Loss: 0.00001852
Iteration 218/1000 | Loss: 0.00001851
Iteration 219/1000 | Loss: 0.00001851
Iteration 220/1000 | Loss: 0.00001851
Iteration 221/1000 | Loss: 0.00001851
Iteration 222/1000 | Loss: 0.00001851
Iteration 223/1000 | Loss: 0.00001851
Iteration 224/1000 | Loss: 0.00001851
Iteration 225/1000 | Loss: 0.00001851
Iteration 226/1000 | Loss: 0.00001851
Iteration 227/1000 | Loss: 0.00001851
Iteration 228/1000 | Loss: 0.00001851
Iteration 229/1000 | Loss: 0.00001851
Iteration 230/1000 | Loss: 0.00001851
Iteration 231/1000 | Loss: 0.00001851
Iteration 232/1000 | Loss: 0.00001850
Iteration 233/1000 | Loss: 0.00001850
Iteration 234/1000 | Loss: 0.00001850
Iteration 235/1000 | Loss: 0.00001850
Iteration 236/1000 | Loss: 0.00001850
Iteration 237/1000 | Loss: 0.00001850
Iteration 238/1000 | Loss: 0.00001850
Iteration 239/1000 | Loss: 0.00001850
Iteration 240/1000 | Loss: 0.00001850
Iteration 241/1000 | Loss: 0.00001850
Iteration 242/1000 | Loss: 0.00001850
Iteration 243/1000 | Loss: 0.00001850
Iteration 244/1000 | Loss: 0.00001850
Iteration 245/1000 | Loss: 0.00001850
Iteration 246/1000 | Loss: 0.00001850
Iteration 247/1000 | Loss: 0.00001850
Iteration 248/1000 | Loss: 0.00001850
Iteration 249/1000 | Loss: 0.00001850
Iteration 250/1000 | Loss: 0.00001850
Iteration 251/1000 | Loss: 0.00001850
Iteration 252/1000 | Loss: 0.00001850
Iteration 253/1000 | Loss: 0.00001850
Iteration 254/1000 | Loss: 0.00001850
Iteration 255/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.8498763893148862e-05, 1.8498763893148862e-05, 1.8498763893148862e-05, 1.8498763893148862e-05, 1.8498763893148862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8498763893148862e-05

Optimization complete. Final v2v error: 3.733060121536255 mm

Highest mean error: 4.0960307121276855 mm for frame 198

Lowest mean error: 3.4271280765533447 mm for frame 110

Saving results

Total time: 122.7063844203949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818348
Iteration 2/25 | Loss: 0.00149733
Iteration 3/25 | Loss: 0.00134529
Iteration 4/25 | Loss: 0.00132063
Iteration 5/25 | Loss: 0.00131444
Iteration 6/25 | Loss: 0.00131424
Iteration 7/25 | Loss: 0.00131424
Iteration 8/25 | Loss: 0.00131424
Iteration 9/25 | Loss: 0.00131424
Iteration 10/25 | Loss: 0.00131424
Iteration 11/25 | Loss: 0.00131424
Iteration 12/25 | Loss: 0.00131424
Iteration 13/25 | Loss: 0.00131424
Iteration 14/25 | Loss: 0.00131424
Iteration 15/25 | Loss: 0.00131424
Iteration 16/25 | Loss: 0.00131424
Iteration 17/25 | Loss: 0.00131424
Iteration 18/25 | Loss: 0.00131424
Iteration 19/25 | Loss: 0.00131424
Iteration 20/25 | Loss: 0.00131424
Iteration 21/25 | Loss: 0.00131424
Iteration 22/25 | Loss: 0.00131424
Iteration 23/25 | Loss: 0.00131424
Iteration 24/25 | Loss: 0.00131424
Iteration 25/25 | Loss: 0.00131424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87830776
Iteration 2/25 | Loss: 0.00142006
Iteration 3/25 | Loss: 0.00142006
Iteration 4/25 | Loss: 0.00142006
Iteration 5/25 | Loss: 0.00142006
Iteration 6/25 | Loss: 0.00142006
Iteration 7/25 | Loss: 0.00142006
Iteration 8/25 | Loss: 0.00142006
Iteration 9/25 | Loss: 0.00142006
Iteration 10/25 | Loss: 0.00142006
Iteration 11/25 | Loss: 0.00142006
Iteration 12/25 | Loss: 0.00142006
Iteration 13/25 | Loss: 0.00142006
Iteration 14/25 | Loss: 0.00142006
Iteration 15/25 | Loss: 0.00142006
Iteration 16/25 | Loss: 0.00142006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014200571458786726, 0.0014200571458786726, 0.0014200571458786726, 0.0014200571458786726, 0.0014200571458786726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014200571458786726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142006
Iteration 2/1000 | Loss: 0.00003386
Iteration 3/1000 | Loss: 0.00002764
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002458
Iteration 6/1000 | Loss: 0.00002373
Iteration 7/1000 | Loss: 0.00002312
Iteration 8/1000 | Loss: 0.00002268
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002183
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002129
Iteration 13/1000 | Loss: 0.00002108
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002081
Iteration 16/1000 | Loss: 0.00002079
Iteration 17/1000 | Loss: 0.00002078
Iteration 18/1000 | Loss: 0.00002077
Iteration 19/1000 | Loss: 0.00002075
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00002074
Iteration 22/1000 | Loss: 0.00002072
Iteration 23/1000 | Loss: 0.00002071
Iteration 24/1000 | Loss: 0.00002067
Iteration 25/1000 | Loss: 0.00002067
Iteration 26/1000 | Loss: 0.00002067
Iteration 27/1000 | Loss: 0.00002067
Iteration 28/1000 | Loss: 0.00002067
Iteration 29/1000 | Loss: 0.00002067
Iteration 30/1000 | Loss: 0.00002067
Iteration 31/1000 | Loss: 0.00002067
Iteration 32/1000 | Loss: 0.00002066
Iteration 33/1000 | Loss: 0.00002066
Iteration 34/1000 | Loss: 0.00002066
Iteration 35/1000 | Loss: 0.00002066
Iteration 36/1000 | Loss: 0.00002066
Iteration 37/1000 | Loss: 0.00002066
Iteration 38/1000 | Loss: 0.00002066
Iteration 39/1000 | Loss: 0.00002066
Iteration 40/1000 | Loss: 0.00002066
Iteration 41/1000 | Loss: 0.00002066
Iteration 42/1000 | Loss: 0.00002066
Iteration 43/1000 | Loss: 0.00002066
Iteration 44/1000 | Loss: 0.00002066
Iteration 45/1000 | Loss: 0.00002065
Iteration 46/1000 | Loss: 0.00002065
Iteration 47/1000 | Loss: 0.00002065
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002063
Iteration 50/1000 | Loss: 0.00002062
Iteration 51/1000 | Loss: 0.00002061
Iteration 52/1000 | Loss: 0.00002059
Iteration 53/1000 | Loss: 0.00002058
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002055
Iteration 56/1000 | Loss: 0.00002053
Iteration 57/1000 | Loss: 0.00002053
Iteration 58/1000 | Loss: 0.00002052
Iteration 59/1000 | Loss: 0.00002052
Iteration 60/1000 | Loss: 0.00002052
Iteration 61/1000 | Loss: 0.00002052
Iteration 62/1000 | Loss: 0.00002052
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002051
Iteration 65/1000 | Loss: 0.00002050
Iteration 66/1000 | Loss: 0.00002050
Iteration 67/1000 | Loss: 0.00002048
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002045
Iteration 73/1000 | Loss: 0.00002045
Iteration 74/1000 | Loss: 0.00002041
Iteration 75/1000 | Loss: 0.00002040
Iteration 76/1000 | Loss: 0.00002040
Iteration 77/1000 | Loss: 0.00002040
Iteration 78/1000 | Loss: 0.00002040
Iteration 79/1000 | Loss: 0.00002040
Iteration 80/1000 | Loss: 0.00002040
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00002040
Iteration 83/1000 | Loss: 0.00002040
Iteration 84/1000 | Loss: 0.00002039
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002039
Iteration 87/1000 | Loss: 0.00002039
Iteration 88/1000 | Loss: 0.00002039
Iteration 89/1000 | Loss: 0.00002039
Iteration 90/1000 | Loss: 0.00002039
Iteration 91/1000 | Loss: 0.00002039
Iteration 92/1000 | Loss: 0.00002039
Iteration 93/1000 | Loss: 0.00002039
Iteration 94/1000 | Loss: 0.00002038
Iteration 95/1000 | Loss: 0.00002038
Iteration 96/1000 | Loss: 0.00002038
Iteration 97/1000 | Loss: 0.00002038
Iteration 98/1000 | Loss: 0.00002038
Iteration 99/1000 | Loss: 0.00002038
Iteration 100/1000 | Loss: 0.00002038
Iteration 101/1000 | Loss: 0.00002038
Iteration 102/1000 | Loss: 0.00002038
Iteration 103/1000 | Loss: 0.00002038
Iteration 104/1000 | Loss: 0.00002037
Iteration 105/1000 | Loss: 0.00002037
Iteration 106/1000 | Loss: 0.00002037
Iteration 107/1000 | Loss: 0.00002037
Iteration 108/1000 | Loss: 0.00002037
Iteration 109/1000 | Loss: 0.00002037
Iteration 110/1000 | Loss: 0.00002036
Iteration 111/1000 | Loss: 0.00002036
Iteration 112/1000 | Loss: 0.00002036
Iteration 113/1000 | Loss: 0.00002036
Iteration 114/1000 | Loss: 0.00002035
Iteration 115/1000 | Loss: 0.00002035
Iteration 116/1000 | Loss: 0.00002035
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002035
Iteration 119/1000 | Loss: 0.00002034
Iteration 120/1000 | Loss: 0.00002034
Iteration 121/1000 | Loss: 0.00002034
Iteration 122/1000 | Loss: 0.00002034
Iteration 123/1000 | Loss: 0.00002034
Iteration 124/1000 | Loss: 0.00002034
Iteration 125/1000 | Loss: 0.00002034
Iteration 126/1000 | Loss: 0.00002033
Iteration 127/1000 | Loss: 0.00002033
Iteration 128/1000 | Loss: 0.00002033
Iteration 129/1000 | Loss: 0.00002033
Iteration 130/1000 | Loss: 0.00002033
Iteration 131/1000 | Loss: 0.00002033
Iteration 132/1000 | Loss: 0.00002033
Iteration 133/1000 | Loss: 0.00002033
Iteration 134/1000 | Loss: 0.00002033
Iteration 135/1000 | Loss: 0.00002033
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00002032
Iteration 140/1000 | Loss: 0.00002032
Iteration 141/1000 | Loss: 0.00002032
Iteration 142/1000 | Loss: 0.00002032
Iteration 143/1000 | Loss: 0.00002032
Iteration 144/1000 | Loss: 0.00002032
Iteration 145/1000 | Loss: 0.00002032
Iteration 146/1000 | Loss: 0.00002032
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002031
Iteration 152/1000 | Loss: 0.00002031
Iteration 153/1000 | Loss: 0.00002031
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002030
Iteration 157/1000 | Loss: 0.00002030
Iteration 158/1000 | Loss: 0.00002030
Iteration 159/1000 | Loss: 0.00002030
Iteration 160/1000 | Loss: 0.00002029
Iteration 161/1000 | Loss: 0.00002029
Iteration 162/1000 | Loss: 0.00002029
Iteration 163/1000 | Loss: 0.00002029
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002029
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002028
Iteration 168/1000 | Loss: 0.00002028
Iteration 169/1000 | Loss: 0.00002028
Iteration 170/1000 | Loss: 0.00002027
Iteration 171/1000 | Loss: 0.00002027
Iteration 172/1000 | Loss: 0.00002027
Iteration 173/1000 | Loss: 0.00002027
Iteration 174/1000 | Loss: 0.00002027
Iteration 175/1000 | Loss: 0.00002027
Iteration 176/1000 | Loss: 0.00002027
Iteration 177/1000 | Loss: 0.00002027
Iteration 178/1000 | Loss: 0.00002027
Iteration 179/1000 | Loss: 0.00002027
Iteration 180/1000 | Loss: 0.00002026
Iteration 181/1000 | Loss: 0.00002026
Iteration 182/1000 | Loss: 0.00002026
Iteration 183/1000 | Loss: 0.00002026
Iteration 184/1000 | Loss: 0.00002026
Iteration 185/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.026435140578542e-05, 2.026435140578542e-05, 2.026435140578542e-05, 2.026435140578542e-05, 2.026435140578542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.026435140578542e-05

Optimization complete. Final v2v error: 3.9166789054870605 mm

Highest mean error: 4.576992511749268 mm for frame 239

Lowest mean error: 3.492048740386963 mm for frame 2

Saving results

Total time: 48.57715964317322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422296
Iteration 2/25 | Loss: 0.00139532
Iteration 3/25 | Loss: 0.00132958
Iteration 4/25 | Loss: 0.00131870
Iteration 5/25 | Loss: 0.00131520
Iteration 6/25 | Loss: 0.00131433
Iteration 7/25 | Loss: 0.00131433
Iteration 8/25 | Loss: 0.00131433
Iteration 9/25 | Loss: 0.00131433
Iteration 10/25 | Loss: 0.00131433
Iteration 11/25 | Loss: 0.00131433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001314328401349485, 0.001314328401349485, 0.001314328401349485, 0.001314328401349485, 0.001314328401349485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001314328401349485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59232032
Iteration 2/25 | Loss: 0.00222293
Iteration 3/25 | Loss: 0.00222293
Iteration 4/25 | Loss: 0.00222293
Iteration 5/25 | Loss: 0.00222293
Iteration 6/25 | Loss: 0.00222293
Iteration 7/25 | Loss: 0.00222292
Iteration 8/25 | Loss: 0.00222292
Iteration 9/25 | Loss: 0.00222292
Iteration 10/25 | Loss: 0.00222292
Iteration 11/25 | Loss: 0.00222292
Iteration 12/25 | Loss: 0.00222292
Iteration 13/25 | Loss: 0.00222292
Iteration 14/25 | Loss: 0.00222292
Iteration 15/25 | Loss: 0.00222292
Iteration 16/25 | Loss: 0.00222292
Iteration 17/25 | Loss: 0.00222292
Iteration 18/25 | Loss: 0.00222292
Iteration 19/25 | Loss: 0.00222292
Iteration 20/25 | Loss: 0.00222292
Iteration 21/25 | Loss: 0.00222292
Iteration 22/25 | Loss: 0.00222292
Iteration 23/25 | Loss: 0.00222292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0022229221649467945, 0.0022229221649467945, 0.0022229221649467945, 0.0022229221649467945, 0.0022229221649467945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022229221649467945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222292
Iteration 2/1000 | Loss: 0.00003024
Iteration 3/1000 | Loss: 0.00002168
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001580
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001435
Iteration 8/1000 | Loss: 0.00001378
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001238
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001206
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001200
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001182
Iteration 66/1000 | Loss: 0.00001182
Iteration 67/1000 | Loss: 0.00001182
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001181
Iteration 72/1000 | Loss: 0.00001181
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001179
Iteration 77/1000 | Loss: 0.00001179
Iteration 78/1000 | Loss: 0.00001179
Iteration 79/1000 | Loss: 0.00001179
Iteration 80/1000 | Loss: 0.00001179
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001172
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001167
Iteration 148/1000 | Loss: 0.00001167
Iteration 149/1000 | Loss: 0.00001167
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001165
Iteration 156/1000 | Loss: 0.00001165
Iteration 157/1000 | Loss: 0.00001165
Iteration 158/1000 | Loss: 0.00001165
Iteration 159/1000 | Loss: 0.00001165
Iteration 160/1000 | Loss: 0.00001165
Iteration 161/1000 | Loss: 0.00001165
Iteration 162/1000 | Loss: 0.00001165
Iteration 163/1000 | Loss: 0.00001165
Iteration 164/1000 | Loss: 0.00001164
Iteration 165/1000 | Loss: 0.00001164
Iteration 166/1000 | Loss: 0.00001164
Iteration 167/1000 | Loss: 0.00001164
Iteration 168/1000 | Loss: 0.00001164
Iteration 169/1000 | Loss: 0.00001164
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001164
Iteration 174/1000 | Loss: 0.00001164
Iteration 175/1000 | Loss: 0.00001164
Iteration 176/1000 | Loss: 0.00001164
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001163
Iteration 182/1000 | Loss: 0.00001163
Iteration 183/1000 | Loss: 0.00001163
Iteration 184/1000 | Loss: 0.00001163
Iteration 185/1000 | Loss: 0.00001163
Iteration 186/1000 | Loss: 0.00001162
Iteration 187/1000 | Loss: 0.00001162
Iteration 188/1000 | Loss: 0.00001162
Iteration 189/1000 | Loss: 0.00001162
Iteration 190/1000 | Loss: 0.00001162
Iteration 191/1000 | Loss: 0.00001162
Iteration 192/1000 | Loss: 0.00001162
Iteration 193/1000 | Loss: 0.00001161
Iteration 194/1000 | Loss: 0.00001161
Iteration 195/1000 | Loss: 0.00001161
Iteration 196/1000 | Loss: 0.00001161
Iteration 197/1000 | Loss: 0.00001161
Iteration 198/1000 | Loss: 0.00001161
Iteration 199/1000 | Loss: 0.00001161
Iteration 200/1000 | Loss: 0.00001161
Iteration 201/1000 | Loss: 0.00001161
Iteration 202/1000 | Loss: 0.00001161
Iteration 203/1000 | Loss: 0.00001161
Iteration 204/1000 | Loss: 0.00001161
Iteration 205/1000 | Loss: 0.00001161
Iteration 206/1000 | Loss: 0.00001161
Iteration 207/1000 | Loss: 0.00001161
Iteration 208/1000 | Loss: 0.00001161
Iteration 209/1000 | Loss: 0.00001161
Iteration 210/1000 | Loss: 0.00001161
Iteration 211/1000 | Loss: 0.00001161
Iteration 212/1000 | Loss: 0.00001161
Iteration 213/1000 | Loss: 0.00001161
Iteration 214/1000 | Loss: 0.00001161
Iteration 215/1000 | Loss: 0.00001161
Iteration 216/1000 | Loss: 0.00001161
Iteration 217/1000 | Loss: 0.00001161
Iteration 218/1000 | Loss: 0.00001161
Iteration 219/1000 | Loss: 0.00001161
Iteration 220/1000 | Loss: 0.00001161
Iteration 221/1000 | Loss: 0.00001161
Iteration 222/1000 | Loss: 0.00001161
Iteration 223/1000 | Loss: 0.00001161
Iteration 224/1000 | Loss: 0.00001161
Iteration 225/1000 | Loss: 0.00001161
Iteration 226/1000 | Loss: 0.00001161
Iteration 227/1000 | Loss: 0.00001161
Iteration 228/1000 | Loss: 0.00001161
Iteration 229/1000 | Loss: 0.00001161
Iteration 230/1000 | Loss: 0.00001161
Iteration 231/1000 | Loss: 0.00001161
Iteration 232/1000 | Loss: 0.00001161
Iteration 233/1000 | Loss: 0.00001161
Iteration 234/1000 | Loss: 0.00001161
Iteration 235/1000 | Loss: 0.00001161
Iteration 236/1000 | Loss: 0.00001161
Iteration 237/1000 | Loss: 0.00001161
Iteration 238/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.1608864951995201e-05, 1.1608864951995201e-05, 1.1608864951995201e-05, 1.1608864951995201e-05, 1.1608864951995201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1608864951995201e-05

Optimization complete. Final v2v error: 2.9436135292053223 mm

Highest mean error: 3.870148181915283 mm for frame 74

Lowest mean error: 2.6594841480255127 mm for frame 97

Saving results

Total time: 45.563637256622314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726392
Iteration 2/25 | Loss: 0.00198975
Iteration 3/25 | Loss: 0.00168889
Iteration 4/25 | Loss: 0.00162568
Iteration 5/25 | Loss: 0.00148550
Iteration 6/25 | Loss: 0.00142288
Iteration 7/25 | Loss: 0.00138622
Iteration 8/25 | Loss: 0.00135585
Iteration 9/25 | Loss: 0.00134734
Iteration 10/25 | Loss: 0.00134091
Iteration 11/25 | Loss: 0.00133592
Iteration 12/25 | Loss: 0.00133262
Iteration 13/25 | Loss: 0.00133178
Iteration 14/25 | Loss: 0.00133174
Iteration 15/25 | Loss: 0.00133174
Iteration 16/25 | Loss: 0.00133174
Iteration 17/25 | Loss: 0.00133174
Iteration 18/25 | Loss: 0.00133174
Iteration 19/25 | Loss: 0.00133174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013317374978214502, 0.0013317374978214502, 0.0013317374978214502, 0.0013317374978214502, 0.0013317374978214502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013317374978214502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19528556
Iteration 2/25 | Loss: 0.00184536
Iteration 3/25 | Loss: 0.00184535
Iteration 4/25 | Loss: 0.00184535
Iteration 5/25 | Loss: 0.00184535
Iteration 6/25 | Loss: 0.00184535
Iteration 7/25 | Loss: 0.00184535
Iteration 8/25 | Loss: 0.00184535
Iteration 9/25 | Loss: 0.00184535
Iteration 10/25 | Loss: 0.00184535
Iteration 11/25 | Loss: 0.00184535
Iteration 12/25 | Loss: 0.00184535
Iteration 13/25 | Loss: 0.00184535
Iteration 14/25 | Loss: 0.00184535
Iteration 15/25 | Loss: 0.00184535
Iteration 16/25 | Loss: 0.00184535
Iteration 17/25 | Loss: 0.00184535
Iteration 18/25 | Loss: 0.00184535
Iteration 19/25 | Loss: 0.00184535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018453499069437385, 0.0018453499069437385, 0.0018453499069437385, 0.0018453499069437385, 0.0018453499069437385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018453499069437385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184535
Iteration 2/1000 | Loss: 0.00002729
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001763
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001546
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001311
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001286
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00001285
Iteration 44/1000 | Loss: 0.00001285
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001284
Iteration 48/1000 | Loss: 0.00001284
Iteration 49/1000 | Loss: 0.00001284
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001283
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001282
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001281
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001276
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001273
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001264
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001262
Iteration 134/1000 | Loss: 0.00001262
Iteration 135/1000 | Loss: 0.00001262
Iteration 136/1000 | Loss: 0.00001262
Iteration 137/1000 | Loss: 0.00001262
Iteration 138/1000 | Loss: 0.00001262
Iteration 139/1000 | Loss: 0.00001262
Iteration 140/1000 | Loss: 0.00001262
Iteration 141/1000 | Loss: 0.00001262
Iteration 142/1000 | Loss: 0.00001262
Iteration 143/1000 | Loss: 0.00001262
Iteration 144/1000 | Loss: 0.00001262
Iteration 145/1000 | Loss: 0.00001262
Iteration 146/1000 | Loss: 0.00001262
Iteration 147/1000 | Loss: 0.00001262
Iteration 148/1000 | Loss: 0.00001262
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001262
Iteration 151/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.2618040273082443e-05, 1.2618040273082443e-05, 1.2618040273082443e-05, 1.2618040273082443e-05, 1.2618040273082443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2618040273082443e-05

Optimization complete. Final v2v error: 3.0777018070220947 mm

Highest mean error: 3.2421514987945557 mm for frame 221

Lowest mean error: 2.961594343185425 mm for frame 76

Saving results

Total time: 61.44963264465332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389723
Iteration 2/25 | Loss: 0.00141911
Iteration 3/25 | Loss: 0.00133161
Iteration 4/25 | Loss: 0.00132112
Iteration 5/25 | Loss: 0.00131753
Iteration 6/25 | Loss: 0.00131702
Iteration 7/25 | Loss: 0.00131702
Iteration 8/25 | Loss: 0.00131702
Iteration 9/25 | Loss: 0.00131702
Iteration 10/25 | Loss: 0.00131702
Iteration 11/25 | Loss: 0.00131702
Iteration 12/25 | Loss: 0.00131702
Iteration 13/25 | Loss: 0.00131702
Iteration 14/25 | Loss: 0.00131702
Iteration 15/25 | Loss: 0.00131702
Iteration 16/25 | Loss: 0.00131702
Iteration 17/25 | Loss: 0.00131702
Iteration 18/25 | Loss: 0.00131702
Iteration 19/25 | Loss: 0.00131702
Iteration 20/25 | Loss: 0.00131702
Iteration 21/25 | Loss: 0.00131702
Iteration 22/25 | Loss: 0.00131702
Iteration 23/25 | Loss: 0.00131702
Iteration 24/25 | Loss: 0.00131702
Iteration 25/25 | Loss: 0.00131702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19123602
Iteration 2/25 | Loss: 0.00248430
Iteration 3/25 | Loss: 0.00248430
Iteration 4/25 | Loss: 0.00248430
Iteration 5/25 | Loss: 0.00248430
Iteration 6/25 | Loss: 0.00248430
Iteration 7/25 | Loss: 0.00248430
Iteration 8/25 | Loss: 0.00248430
Iteration 9/25 | Loss: 0.00248430
Iteration 10/25 | Loss: 0.00248430
Iteration 11/25 | Loss: 0.00248430
Iteration 12/25 | Loss: 0.00248430
Iteration 13/25 | Loss: 0.00248430
Iteration 14/25 | Loss: 0.00248430
Iteration 15/25 | Loss: 0.00248430
Iteration 16/25 | Loss: 0.00248430
Iteration 17/25 | Loss: 0.00248430
Iteration 18/25 | Loss: 0.00248430
Iteration 19/25 | Loss: 0.00248430
Iteration 20/25 | Loss: 0.00248430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002484299475327134, 0.002484299475327134, 0.002484299475327134, 0.002484299475327134, 0.002484299475327134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002484299475327134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248430
Iteration 2/1000 | Loss: 0.00004579
Iteration 3/1000 | Loss: 0.00003338
Iteration 4/1000 | Loss: 0.00002584
Iteration 5/1000 | Loss: 0.00002332
Iteration 6/1000 | Loss: 0.00002162
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001901
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001714
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001690
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001689
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001686
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001686
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001684
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001682
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001682
Iteration 69/1000 | Loss: 0.00001682
Iteration 70/1000 | Loss: 0.00001681
Iteration 71/1000 | Loss: 0.00001681
Iteration 72/1000 | Loss: 0.00001681
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001679
Iteration 80/1000 | Loss: 0.00001679
Iteration 81/1000 | Loss: 0.00001679
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001678
Iteration 85/1000 | Loss: 0.00001678
Iteration 86/1000 | Loss: 0.00001677
Iteration 87/1000 | Loss: 0.00001677
Iteration 88/1000 | Loss: 0.00001677
Iteration 89/1000 | Loss: 0.00001677
Iteration 90/1000 | Loss: 0.00001677
Iteration 91/1000 | Loss: 0.00001677
Iteration 92/1000 | Loss: 0.00001677
Iteration 93/1000 | Loss: 0.00001677
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001676
Iteration 99/1000 | Loss: 0.00001676
Iteration 100/1000 | Loss: 0.00001676
Iteration 101/1000 | Loss: 0.00001675
Iteration 102/1000 | Loss: 0.00001675
Iteration 103/1000 | Loss: 0.00001675
Iteration 104/1000 | Loss: 0.00001675
Iteration 105/1000 | Loss: 0.00001674
Iteration 106/1000 | Loss: 0.00001674
Iteration 107/1000 | Loss: 0.00001674
Iteration 108/1000 | Loss: 0.00001674
Iteration 109/1000 | Loss: 0.00001673
Iteration 110/1000 | Loss: 0.00001673
Iteration 111/1000 | Loss: 0.00001673
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001672
Iteration 115/1000 | Loss: 0.00001672
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001671
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001669
Iteration 125/1000 | Loss: 0.00001669
Iteration 126/1000 | Loss: 0.00001669
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001668
Iteration 130/1000 | Loss: 0.00001667
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001666
Iteration 134/1000 | Loss: 0.00001666
Iteration 135/1000 | Loss: 0.00001666
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001665
Iteration 141/1000 | Loss: 0.00001664
Iteration 142/1000 | Loss: 0.00001664
Iteration 143/1000 | Loss: 0.00001664
Iteration 144/1000 | Loss: 0.00001664
Iteration 145/1000 | Loss: 0.00001663
Iteration 146/1000 | Loss: 0.00001663
Iteration 147/1000 | Loss: 0.00001663
Iteration 148/1000 | Loss: 0.00001663
Iteration 149/1000 | Loss: 0.00001662
Iteration 150/1000 | Loss: 0.00001662
Iteration 151/1000 | Loss: 0.00001662
Iteration 152/1000 | Loss: 0.00001662
Iteration 153/1000 | Loss: 0.00001662
Iteration 154/1000 | Loss: 0.00001662
Iteration 155/1000 | Loss: 0.00001662
Iteration 156/1000 | Loss: 0.00001662
Iteration 157/1000 | Loss: 0.00001661
Iteration 158/1000 | Loss: 0.00001661
Iteration 159/1000 | Loss: 0.00001661
Iteration 160/1000 | Loss: 0.00001661
Iteration 161/1000 | Loss: 0.00001661
Iteration 162/1000 | Loss: 0.00001660
Iteration 163/1000 | Loss: 0.00001660
Iteration 164/1000 | Loss: 0.00001660
Iteration 165/1000 | Loss: 0.00001660
Iteration 166/1000 | Loss: 0.00001660
Iteration 167/1000 | Loss: 0.00001660
Iteration 168/1000 | Loss: 0.00001659
Iteration 169/1000 | Loss: 0.00001659
Iteration 170/1000 | Loss: 0.00001659
Iteration 171/1000 | Loss: 0.00001659
Iteration 172/1000 | Loss: 0.00001659
Iteration 173/1000 | Loss: 0.00001659
Iteration 174/1000 | Loss: 0.00001659
Iteration 175/1000 | Loss: 0.00001659
Iteration 176/1000 | Loss: 0.00001659
Iteration 177/1000 | Loss: 0.00001658
Iteration 178/1000 | Loss: 0.00001658
Iteration 179/1000 | Loss: 0.00001658
Iteration 180/1000 | Loss: 0.00001658
Iteration 181/1000 | Loss: 0.00001658
Iteration 182/1000 | Loss: 0.00001658
Iteration 183/1000 | Loss: 0.00001658
Iteration 184/1000 | Loss: 0.00001658
Iteration 185/1000 | Loss: 0.00001658
Iteration 186/1000 | Loss: 0.00001657
Iteration 187/1000 | Loss: 0.00001657
Iteration 188/1000 | Loss: 0.00001657
Iteration 189/1000 | Loss: 0.00001657
Iteration 190/1000 | Loss: 0.00001656
Iteration 191/1000 | Loss: 0.00001656
Iteration 192/1000 | Loss: 0.00001656
Iteration 193/1000 | Loss: 0.00001656
Iteration 194/1000 | Loss: 0.00001656
Iteration 195/1000 | Loss: 0.00001655
Iteration 196/1000 | Loss: 0.00001655
Iteration 197/1000 | Loss: 0.00001655
Iteration 198/1000 | Loss: 0.00001655
Iteration 199/1000 | Loss: 0.00001655
Iteration 200/1000 | Loss: 0.00001655
Iteration 201/1000 | Loss: 0.00001655
Iteration 202/1000 | Loss: 0.00001655
Iteration 203/1000 | Loss: 0.00001655
Iteration 204/1000 | Loss: 0.00001655
Iteration 205/1000 | Loss: 0.00001655
Iteration 206/1000 | Loss: 0.00001655
Iteration 207/1000 | Loss: 0.00001655
Iteration 208/1000 | Loss: 0.00001655
Iteration 209/1000 | Loss: 0.00001655
Iteration 210/1000 | Loss: 0.00001655
Iteration 211/1000 | Loss: 0.00001655
Iteration 212/1000 | Loss: 0.00001655
Iteration 213/1000 | Loss: 0.00001655
Iteration 214/1000 | Loss: 0.00001655
Iteration 215/1000 | Loss: 0.00001655
Iteration 216/1000 | Loss: 0.00001655
Iteration 217/1000 | Loss: 0.00001655
Iteration 218/1000 | Loss: 0.00001655
Iteration 219/1000 | Loss: 0.00001655
Iteration 220/1000 | Loss: 0.00001655
Iteration 221/1000 | Loss: 0.00001655
Iteration 222/1000 | Loss: 0.00001655
Iteration 223/1000 | Loss: 0.00001655
Iteration 224/1000 | Loss: 0.00001655
Iteration 225/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.6547146515222266e-05, 1.6547146515222266e-05, 1.6547146515222266e-05, 1.6547146515222266e-05, 1.6547146515222266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6547146515222266e-05

Optimization complete. Final v2v error: 3.477572441101074 mm

Highest mean error: 3.854776382446289 mm for frame 57

Lowest mean error: 2.921327590942383 mm for frame 13

Saving results

Total time: 46.26655864715576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017738
Iteration 2/25 | Loss: 0.01017738
Iteration 3/25 | Loss: 0.01017738
Iteration 4/25 | Loss: 0.01017738
Iteration 5/25 | Loss: 0.01017738
Iteration 6/25 | Loss: 0.01017738
Iteration 7/25 | Loss: 0.01017738
Iteration 8/25 | Loss: 0.01017737
Iteration 9/25 | Loss: 0.01017737
Iteration 10/25 | Loss: 0.01017737
Iteration 11/25 | Loss: 0.01017737
Iteration 12/25 | Loss: 0.01017737
Iteration 13/25 | Loss: 0.01017737
Iteration 14/25 | Loss: 0.01017737
Iteration 15/25 | Loss: 0.01017737
Iteration 16/25 | Loss: 0.01017737
Iteration 17/25 | Loss: 0.01017737
Iteration 18/25 | Loss: 0.01017737
Iteration 19/25 | Loss: 0.01017737
Iteration 20/25 | Loss: 0.01017737
Iteration 21/25 | Loss: 0.01017737
Iteration 22/25 | Loss: 0.01017736
Iteration 23/25 | Loss: 0.01017736
Iteration 24/25 | Loss: 0.01017736
Iteration 25/25 | Loss: 0.01017736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82363582
Iteration 2/25 | Loss: 0.07718373
Iteration 3/25 | Loss: 0.07718363
Iteration 4/25 | Loss: 0.07718363
Iteration 5/25 | Loss: 0.07718361
Iteration 6/25 | Loss: 0.07718360
Iteration 7/25 | Loss: 0.07718360
Iteration 8/25 | Loss: 0.07718360
Iteration 9/25 | Loss: 0.07718360
Iteration 10/25 | Loss: 0.07718360
Iteration 11/25 | Loss: 0.07718360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.07718360424041748, 0.07718360424041748, 0.07718360424041748, 0.07718360424041748, 0.07718360424041748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07718360424041748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07718360
Iteration 2/1000 | Loss: 0.00124511
Iteration 3/1000 | Loss: 0.00058572
Iteration 4/1000 | Loss: 0.00083711
Iteration 5/1000 | Loss: 0.00044074
Iteration 6/1000 | Loss: 0.00056728
Iteration 7/1000 | Loss: 0.00015202
Iteration 8/1000 | Loss: 0.00014322
Iteration 9/1000 | Loss: 0.00009868
Iteration 10/1000 | Loss: 0.00010580
Iteration 11/1000 | Loss: 0.00008948
Iteration 12/1000 | Loss: 0.00004039
Iteration 13/1000 | Loss: 0.00004556
Iteration 14/1000 | Loss: 0.00004676
Iteration 15/1000 | Loss: 0.00019174
Iteration 16/1000 | Loss: 0.00007021
Iteration 17/1000 | Loss: 0.00005673
Iteration 18/1000 | Loss: 0.00004358
Iteration 19/1000 | Loss: 0.00006887
Iteration 20/1000 | Loss: 0.00002952
Iteration 21/1000 | Loss: 0.00007197
Iteration 22/1000 | Loss: 0.00012575
Iteration 23/1000 | Loss: 0.00037818
Iteration 24/1000 | Loss: 0.00003290
Iteration 25/1000 | Loss: 0.00003551
Iteration 26/1000 | Loss: 0.00002597
Iteration 27/1000 | Loss: 0.00009908
Iteration 28/1000 | Loss: 0.00002791
Iteration 29/1000 | Loss: 0.00005163
Iteration 30/1000 | Loss: 0.00010629
Iteration 31/1000 | Loss: 0.00002064
Iteration 32/1000 | Loss: 0.00003048
Iteration 33/1000 | Loss: 0.00008132
Iteration 34/1000 | Loss: 0.00009815
Iteration 35/1000 | Loss: 0.00003202
Iteration 36/1000 | Loss: 0.00002339
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00004480
Iteration 40/1000 | Loss: 0.00006227
Iteration 41/1000 | Loss: 0.00005484
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00004708
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00004099
Iteration 46/1000 | Loss: 0.00038340
Iteration 47/1000 | Loss: 0.00003095
Iteration 48/1000 | Loss: 0.00002599
Iteration 49/1000 | Loss: 0.00010360
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002545
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00002087
Iteration 54/1000 | Loss: 0.00003925
Iteration 55/1000 | Loss: 0.00002133
Iteration 56/1000 | Loss: 0.00001668
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00003922
Iteration 62/1000 | Loss: 0.00015912
Iteration 63/1000 | Loss: 0.00002002
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00002204
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001736
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001615
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00006419
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00001633
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001597
Iteration 105/1000 | Loss: 0.00001597
Iteration 106/1000 | Loss: 0.00001597
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00003201
Iteration 110/1000 | Loss: 0.00001593
Iteration 111/1000 | Loss: 0.00001593
Iteration 112/1000 | Loss: 0.00001593
Iteration 113/1000 | Loss: 0.00001593
Iteration 114/1000 | Loss: 0.00001593
Iteration 115/1000 | Loss: 0.00001593
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001592
Iteration 118/1000 | Loss: 0.00001592
Iteration 119/1000 | Loss: 0.00001592
Iteration 120/1000 | Loss: 0.00001592
Iteration 121/1000 | Loss: 0.00001592
Iteration 122/1000 | Loss: 0.00001592
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001591
Iteration 129/1000 | Loss: 0.00001591
Iteration 130/1000 | Loss: 0.00001591
Iteration 131/1000 | Loss: 0.00001590
Iteration 132/1000 | Loss: 0.00001590
Iteration 133/1000 | Loss: 0.00001590
Iteration 134/1000 | Loss: 0.00001590
Iteration 135/1000 | Loss: 0.00001589
Iteration 136/1000 | Loss: 0.00001589
Iteration 137/1000 | Loss: 0.00001972
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00004698
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001718
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001578
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001577
Iteration 149/1000 | Loss: 0.00001577
Iteration 150/1000 | Loss: 0.00001577
Iteration 151/1000 | Loss: 0.00001577
Iteration 152/1000 | Loss: 0.00001577
Iteration 153/1000 | Loss: 0.00001577
Iteration 154/1000 | Loss: 0.00001577
Iteration 155/1000 | Loss: 0.00001577
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001577
Iteration 158/1000 | Loss: 0.00001577
Iteration 159/1000 | Loss: 0.00001576
Iteration 160/1000 | Loss: 0.00001576
Iteration 161/1000 | Loss: 0.00001576
Iteration 162/1000 | Loss: 0.00001576
Iteration 163/1000 | Loss: 0.00001576
Iteration 164/1000 | Loss: 0.00001576
Iteration 165/1000 | Loss: 0.00001576
Iteration 166/1000 | Loss: 0.00001576
Iteration 167/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.5760069800307974e-05, 1.5760069800307974e-05, 1.5760069800307974e-05, 1.5760069800307974e-05, 1.5760069800307974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5760069800307974e-05

Optimization complete. Final v2v error: 3.4126367568969727 mm

Highest mean error: 4.645310878753662 mm for frame 114

Lowest mean error: 2.727189064025879 mm for frame 69

Saving results

Total time: 128.37946915626526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360009
Iteration 2/25 | Loss: 0.00146225
Iteration 3/25 | Loss: 0.00134081
Iteration 4/25 | Loss: 0.00132645
Iteration 5/25 | Loss: 0.00132356
Iteration 6/25 | Loss: 0.00132292
Iteration 7/25 | Loss: 0.00132292
Iteration 8/25 | Loss: 0.00132292
Iteration 9/25 | Loss: 0.00132292
Iteration 10/25 | Loss: 0.00132292
Iteration 11/25 | Loss: 0.00132292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001322916359640658, 0.001322916359640658, 0.001322916359640658, 0.001322916359640658, 0.001322916359640658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322916359640658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25825047
Iteration 2/25 | Loss: 0.00255978
Iteration 3/25 | Loss: 0.00255978
Iteration 4/25 | Loss: 0.00255978
Iteration 5/25 | Loss: 0.00255978
Iteration 6/25 | Loss: 0.00255978
Iteration 7/25 | Loss: 0.00255978
Iteration 8/25 | Loss: 0.00255978
Iteration 9/25 | Loss: 0.00255978
Iteration 10/25 | Loss: 0.00255978
Iteration 11/25 | Loss: 0.00255978
Iteration 12/25 | Loss: 0.00255978
Iteration 13/25 | Loss: 0.00255978
Iteration 14/25 | Loss: 0.00255978
Iteration 15/25 | Loss: 0.00255978
Iteration 16/25 | Loss: 0.00255978
Iteration 17/25 | Loss: 0.00255978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002559780376031995, 0.002559780376031995, 0.002559780376031995, 0.002559780376031995, 0.002559780376031995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002559780376031995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255978
Iteration 2/1000 | Loss: 0.00003240
Iteration 3/1000 | Loss: 0.00002074
Iteration 4/1000 | Loss: 0.00001741
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001312
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001288
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001276
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001266
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001261
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001252
Iteration 78/1000 | Loss: 0.00001252
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001251
Iteration 83/1000 | Loss: 0.00001251
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001245
Iteration 101/1000 | Loss: 0.00001245
Iteration 102/1000 | Loss: 0.00001245
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001244
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001240
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001240
Iteration 119/1000 | Loss: 0.00001240
Iteration 120/1000 | Loss: 0.00001240
Iteration 121/1000 | Loss: 0.00001240
Iteration 122/1000 | Loss: 0.00001240
Iteration 123/1000 | Loss: 0.00001240
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001238
Iteration 129/1000 | Loss: 0.00001238
Iteration 130/1000 | Loss: 0.00001238
Iteration 131/1000 | Loss: 0.00001238
Iteration 132/1000 | Loss: 0.00001238
Iteration 133/1000 | Loss: 0.00001238
Iteration 134/1000 | Loss: 0.00001238
Iteration 135/1000 | Loss: 0.00001238
Iteration 136/1000 | Loss: 0.00001238
Iteration 137/1000 | Loss: 0.00001238
Iteration 138/1000 | Loss: 0.00001238
Iteration 139/1000 | Loss: 0.00001238
Iteration 140/1000 | Loss: 0.00001238
Iteration 141/1000 | Loss: 0.00001238
Iteration 142/1000 | Loss: 0.00001238
Iteration 143/1000 | Loss: 0.00001238
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001238
Iteration 146/1000 | Loss: 0.00001238
Iteration 147/1000 | Loss: 0.00001238
Iteration 148/1000 | Loss: 0.00001238
Iteration 149/1000 | Loss: 0.00001238
Iteration 150/1000 | Loss: 0.00001238
Iteration 151/1000 | Loss: 0.00001238
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001238
Iteration 158/1000 | Loss: 0.00001238
Iteration 159/1000 | Loss: 0.00001238
Iteration 160/1000 | Loss: 0.00001238
Iteration 161/1000 | Loss: 0.00001238
Iteration 162/1000 | Loss: 0.00001238
Iteration 163/1000 | Loss: 0.00001238
Iteration 164/1000 | Loss: 0.00001238
Iteration 165/1000 | Loss: 0.00001238
Iteration 166/1000 | Loss: 0.00001238
Iteration 167/1000 | Loss: 0.00001238
Iteration 168/1000 | Loss: 0.00001238
Iteration 169/1000 | Loss: 0.00001238
Iteration 170/1000 | Loss: 0.00001238
Iteration 171/1000 | Loss: 0.00001238
Iteration 172/1000 | Loss: 0.00001238
Iteration 173/1000 | Loss: 0.00001238
Iteration 174/1000 | Loss: 0.00001238
Iteration 175/1000 | Loss: 0.00001238
Iteration 176/1000 | Loss: 0.00001238
Iteration 177/1000 | Loss: 0.00001238
Iteration 178/1000 | Loss: 0.00001238
Iteration 179/1000 | Loss: 0.00001238
Iteration 180/1000 | Loss: 0.00001238
Iteration 181/1000 | Loss: 0.00001238
Iteration 182/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2375094229355454e-05, 1.2375094229355454e-05, 1.2375094229355454e-05, 1.2375094229355454e-05, 1.2375094229355454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2375094229355454e-05

Optimization complete. Final v2v error: 3.0352065563201904 mm

Highest mean error: 3.3285863399505615 mm for frame 14

Lowest mean error: 2.8407087326049805 mm for frame 67

Saving results

Total time: 40.82769775390625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869275
Iteration 2/25 | Loss: 0.00181121
Iteration 3/25 | Loss: 0.00151783
Iteration 4/25 | Loss: 0.00153950
Iteration 5/25 | Loss: 0.00147868
Iteration 6/25 | Loss: 0.00146887
Iteration 7/25 | Loss: 0.00146184
Iteration 8/25 | Loss: 0.00144988
Iteration 9/25 | Loss: 0.00143289
Iteration 10/25 | Loss: 0.00142420
Iteration 11/25 | Loss: 0.00142009
Iteration 12/25 | Loss: 0.00141993
Iteration 13/25 | Loss: 0.00141977
Iteration 14/25 | Loss: 0.00141761
Iteration 15/25 | Loss: 0.00141691
Iteration 16/25 | Loss: 0.00141709
Iteration 17/25 | Loss: 0.00141247
Iteration 18/25 | Loss: 0.00141874
Iteration 19/25 | Loss: 0.00142055
Iteration 20/25 | Loss: 0.00141827
Iteration 21/25 | Loss: 0.00141660
Iteration 22/25 | Loss: 0.00141509
Iteration 23/25 | Loss: 0.00141828
Iteration 24/25 | Loss: 0.00141472
Iteration 25/25 | Loss: 0.00141944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.54919767
Iteration 2/25 | Loss: 0.00336413
Iteration 3/25 | Loss: 0.00336413
Iteration 4/25 | Loss: 0.00336413
Iteration 5/25 | Loss: 0.00336413
Iteration 6/25 | Loss: 0.00336413
Iteration 7/25 | Loss: 0.00336413
Iteration 8/25 | Loss: 0.00336413
Iteration 9/25 | Loss: 0.00336413
Iteration 10/25 | Loss: 0.00336413
Iteration 11/25 | Loss: 0.00336413
Iteration 12/25 | Loss: 0.00336413
Iteration 13/25 | Loss: 0.00336413
Iteration 14/25 | Loss: 0.00336413
Iteration 15/25 | Loss: 0.00336413
Iteration 16/25 | Loss: 0.00336413
Iteration 17/25 | Loss: 0.00336413
Iteration 18/25 | Loss: 0.00336413
Iteration 19/25 | Loss: 0.00336413
Iteration 20/25 | Loss: 0.00336413
Iteration 21/25 | Loss: 0.00336413
Iteration 22/25 | Loss: 0.00336413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0033641261979937553, 0.0033641261979937553, 0.0033641261979937553, 0.0033641261979937553, 0.0033641261979937553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033641261979937553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336413
Iteration 2/1000 | Loss: 0.00012490
Iteration 3/1000 | Loss: 0.00008292
Iteration 4/1000 | Loss: 0.00005798
Iteration 5/1000 | Loss: 0.00008067
Iteration 6/1000 | Loss: 0.00006720
Iteration 7/1000 | Loss: 0.00007065
Iteration 8/1000 | Loss: 0.00003925
Iteration 9/1000 | Loss: 0.00003638
Iteration 10/1000 | Loss: 0.00005449
Iteration 11/1000 | Loss: 0.00005271
Iteration 12/1000 | Loss: 0.00006162
Iteration 13/1000 | Loss: 0.00003938
Iteration 14/1000 | Loss: 0.00004161
Iteration 15/1000 | Loss: 0.00004461
Iteration 16/1000 | Loss: 0.00003283
Iteration 17/1000 | Loss: 0.00003519
Iteration 18/1000 | Loss: 0.00004405
Iteration 19/1000 | Loss: 0.00003674
Iteration 20/1000 | Loss: 0.00003943
Iteration 21/1000 | Loss: 0.00003195
Iteration 22/1000 | Loss: 0.00003802
Iteration 23/1000 | Loss: 0.00004379
Iteration 24/1000 | Loss: 0.00004385
Iteration 25/1000 | Loss: 0.00004671
Iteration 26/1000 | Loss: 0.00003825
Iteration 27/1000 | Loss: 0.00003509
Iteration 28/1000 | Loss: 0.00004217
Iteration 29/1000 | Loss: 0.00004755
Iteration 30/1000 | Loss: 0.00004647
Iteration 31/1000 | Loss: 0.00004563
Iteration 32/1000 | Loss: 0.00005036
Iteration 33/1000 | Loss: 0.00004519
Iteration 34/1000 | Loss: 0.00004618
Iteration 35/1000 | Loss: 0.00004562
Iteration 36/1000 | Loss: 0.00004065
Iteration 37/1000 | Loss: 0.00004660
Iteration 38/1000 | Loss: 0.00003528
Iteration 39/1000 | Loss: 0.00003960
Iteration 40/1000 | Loss: 0.00003176
Iteration 41/1000 | Loss: 0.00003611
Iteration 42/1000 | Loss: 0.00003699
Iteration 43/1000 | Loss: 0.00003963
Iteration 44/1000 | Loss: 0.00004522
Iteration 45/1000 | Loss: 0.00003842
Iteration 46/1000 | Loss: 0.00004395
Iteration 47/1000 | Loss: 0.00003942
Iteration 48/1000 | Loss: 0.00005376
Iteration 49/1000 | Loss: 0.00006087
Iteration 50/1000 | Loss: 0.00004156
Iteration 51/1000 | Loss: 0.00004386
Iteration 52/1000 | Loss: 0.00003829
Iteration 53/1000 | Loss: 0.00004581
Iteration 54/1000 | Loss: 0.00004333
Iteration 55/1000 | Loss: 0.00004449
Iteration 56/1000 | Loss: 0.00004312
Iteration 57/1000 | Loss: 0.00004259
Iteration 58/1000 | Loss: 0.00004155
Iteration 59/1000 | Loss: 0.00004392
Iteration 60/1000 | Loss: 0.00004698
Iteration 61/1000 | Loss: 0.00005830
Iteration 62/1000 | Loss: 0.00003061
Iteration 63/1000 | Loss: 0.00004012
Iteration 64/1000 | Loss: 0.00004801
Iteration 65/1000 | Loss: 0.00003810
Iteration 66/1000 | Loss: 0.00004396
Iteration 67/1000 | Loss: 0.00004944
Iteration 68/1000 | Loss: 0.00004572
Iteration 69/1000 | Loss: 0.00004801
Iteration 70/1000 | Loss: 0.00005089
Iteration 71/1000 | Loss: 0.00004420
Iteration 72/1000 | Loss: 0.00004985
Iteration 73/1000 | Loss: 0.00004508
Iteration 74/1000 | Loss: 0.00003312
Iteration 75/1000 | Loss: 0.00003159
Iteration 76/1000 | Loss: 0.00004060
Iteration 77/1000 | Loss: 0.00005266
Iteration 78/1000 | Loss: 0.00004181
Iteration 79/1000 | Loss: 0.00005322
Iteration 80/1000 | Loss: 0.00003827
Iteration 81/1000 | Loss: 0.00004099
Iteration 82/1000 | Loss: 0.00004428
Iteration 83/1000 | Loss: 0.00004847
Iteration 84/1000 | Loss: 0.00004037
Iteration 85/1000 | Loss: 0.00004805
Iteration 86/1000 | Loss: 0.00004217
Iteration 87/1000 | Loss: 0.00004232
Iteration 88/1000 | Loss: 0.00004084
Iteration 89/1000 | Loss: 0.00003637
Iteration 90/1000 | Loss: 0.00005015
Iteration 91/1000 | Loss: 0.00003673
Iteration 92/1000 | Loss: 0.00004900
Iteration 93/1000 | Loss: 0.00003622
Iteration 94/1000 | Loss: 0.00003201
Iteration 95/1000 | Loss: 0.00003086
Iteration 96/1000 | Loss: 0.00003022
Iteration 97/1000 | Loss: 0.00002931
Iteration 98/1000 | Loss: 0.00002894
Iteration 99/1000 | Loss: 0.00002858
Iteration 100/1000 | Loss: 0.00002834
Iteration 101/1000 | Loss: 0.00002831
Iteration 102/1000 | Loss: 0.00002828
Iteration 103/1000 | Loss: 0.00002827
Iteration 104/1000 | Loss: 0.00002827
Iteration 105/1000 | Loss: 0.00002826
Iteration 106/1000 | Loss: 0.00002818
Iteration 107/1000 | Loss: 0.00002817
Iteration 108/1000 | Loss: 0.00002815
Iteration 109/1000 | Loss: 0.00002812
Iteration 110/1000 | Loss: 0.00002811
Iteration 111/1000 | Loss: 0.00002811
Iteration 112/1000 | Loss: 0.00002810
Iteration 113/1000 | Loss: 0.00002810
Iteration 114/1000 | Loss: 0.00002809
Iteration 115/1000 | Loss: 0.00002808
Iteration 116/1000 | Loss: 0.00002807
Iteration 117/1000 | Loss: 0.00002805
Iteration 118/1000 | Loss: 0.00002804
Iteration 119/1000 | Loss: 0.00002804
Iteration 120/1000 | Loss: 0.00002804
Iteration 121/1000 | Loss: 0.00002803
Iteration 122/1000 | Loss: 0.00002803
Iteration 123/1000 | Loss: 0.00002803
Iteration 124/1000 | Loss: 0.00002803
Iteration 125/1000 | Loss: 0.00002803
Iteration 126/1000 | Loss: 0.00002803
Iteration 127/1000 | Loss: 0.00002802
Iteration 128/1000 | Loss: 0.00002802
Iteration 129/1000 | Loss: 0.00002802
Iteration 130/1000 | Loss: 0.00002802
Iteration 131/1000 | Loss: 0.00002802
Iteration 132/1000 | Loss: 0.00002802
Iteration 133/1000 | Loss: 0.00002802
Iteration 134/1000 | Loss: 0.00002802
Iteration 135/1000 | Loss: 0.00002802
Iteration 136/1000 | Loss: 0.00002802
Iteration 137/1000 | Loss: 0.00002801
Iteration 138/1000 | Loss: 0.00002801
Iteration 139/1000 | Loss: 0.00002800
Iteration 140/1000 | Loss: 0.00002800
Iteration 141/1000 | Loss: 0.00002800
Iteration 142/1000 | Loss: 0.00002800
Iteration 143/1000 | Loss: 0.00002800
Iteration 144/1000 | Loss: 0.00002800
Iteration 145/1000 | Loss: 0.00002800
Iteration 146/1000 | Loss: 0.00002799
Iteration 147/1000 | Loss: 0.00002799
Iteration 148/1000 | Loss: 0.00002799
Iteration 149/1000 | Loss: 0.00002798
Iteration 150/1000 | Loss: 0.00002797
Iteration 151/1000 | Loss: 0.00002797
Iteration 152/1000 | Loss: 0.00002797
Iteration 153/1000 | Loss: 0.00002796
Iteration 154/1000 | Loss: 0.00002795
Iteration 155/1000 | Loss: 0.00002794
Iteration 156/1000 | Loss: 0.00002794
Iteration 157/1000 | Loss: 0.00002794
Iteration 158/1000 | Loss: 0.00002793
Iteration 159/1000 | Loss: 0.00002793
Iteration 160/1000 | Loss: 0.00002792
Iteration 161/1000 | Loss: 0.00002792
Iteration 162/1000 | Loss: 0.00002792
Iteration 163/1000 | Loss: 0.00002792
Iteration 164/1000 | Loss: 0.00002792
Iteration 165/1000 | Loss: 0.00002792
Iteration 166/1000 | Loss: 0.00002792
Iteration 167/1000 | Loss: 0.00002792
Iteration 168/1000 | Loss: 0.00002792
Iteration 169/1000 | Loss: 0.00002792
Iteration 170/1000 | Loss: 0.00002792
Iteration 171/1000 | Loss: 0.00002791
Iteration 172/1000 | Loss: 0.00002791
Iteration 173/1000 | Loss: 0.00002791
Iteration 174/1000 | Loss: 0.00002791
Iteration 175/1000 | Loss: 0.00002791
Iteration 176/1000 | Loss: 0.00002787
Iteration 177/1000 | Loss: 0.00002782
Iteration 178/1000 | Loss: 0.00002777
Iteration 179/1000 | Loss: 0.00002776
Iteration 180/1000 | Loss: 0.00002776
Iteration 181/1000 | Loss: 0.00002776
Iteration 182/1000 | Loss: 0.00002775
Iteration 183/1000 | Loss: 0.00002774
Iteration 184/1000 | Loss: 0.00002774
Iteration 185/1000 | Loss: 0.00002771
Iteration 186/1000 | Loss: 0.00002771
Iteration 187/1000 | Loss: 0.00002770
Iteration 188/1000 | Loss: 0.00002770
Iteration 189/1000 | Loss: 0.00002769
Iteration 190/1000 | Loss: 0.00002769
Iteration 191/1000 | Loss: 0.00002768
Iteration 192/1000 | Loss: 0.00002768
Iteration 193/1000 | Loss: 0.00002768
Iteration 194/1000 | Loss: 0.00002768
Iteration 195/1000 | Loss: 0.00002768
Iteration 196/1000 | Loss: 0.00002768
Iteration 197/1000 | Loss: 0.00002768
Iteration 198/1000 | Loss: 0.00002768
Iteration 199/1000 | Loss: 0.00002767
Iteration 200/1000 | Loss: 0.00002767
Iteration 201/1000 | Loss: 0.00002767
Iteration 202/1000 | Loss: 0.00002766
Iteration 203/1000 | Loss: 0.00002766
Iteration 204/1000 | Loss: 0.00002766
Iteration 205/1000 | Loss: 0.00002766
Iteration 206/1000 | Loss: 0.00002766
Iteration 207/1000 | Loss: 0.00002766
Iteration 208/1000 | Loss: 0.00002766
Iteration 209/1000 | Loss: 0.00002765
Iteration 210/1000 | Loss: 0.00002765
Iteration 211/1000 | Loss: 0.00002765
Iteration 212/1000 | Loss: 0.00002764
Iteration 213/1000 | Loss: 0.00002764
Iteration 214/1000 | Loss: 0.00002764
Iteration 215/1000 | Loss: 0.00002764
Iteration 216/1000 | Loss: 0.00002763
Iteration 217/1000 | Loss: 0.00002763
Iteration 218/1000 | Loss: 0.00002763
Iteration 219/1000 | Loss: 0.00002763
Iteration 220/1000 | Loss: 0.00002763
Iteration 221/1000 | Loss: 0.00002762
Iteration 222/1000 | Loss: 0.00002762
Iteration 223/1000 | Loss: 0.00002762
Iteration 224/1000 | Loss: 0.00002762
Iteration 225/1000 | Loss: 0.00002762
Iteration 226/1000 | Loss: 0.00002761
Iteration 227/1000 | Loss: 0.00002761
Iteration 228/1000 | Loss: 0.00002761
Iteration 229/1000 | Loss: 0.00002761
Iteration 230/1000 | Loss: 0.00002761
Iteration 231/1000 | Loss: 0.00002761
Iteration 232/1000 | Loss: 0.00002760
Iteration 233/1000 | Loss: 0.00002760
Iteration 234/1000 | Loss: 0.00002760
Iteration 235/1000 | Loss: 0.00002760
Iteration 236/1000 | Loss: 0.00002760
Iteration 237/1000 | Loss: 0.00002760
Iteration 238/1000 | Loss: 0.00002760
Iteration 239/1000 | Loss: 0.00002760
Iteration 240/1000 | Loss: 0.00002760
Iteration 241/1000 | Loss: 0.00002759
Iteration 242/1000 | Loss: 0.00002759
Iteration 243/1000 | Loss: 0.00002759
Iteration 244/1000 | Loss: 0.00002759
Iteration 245/1000 | Loss: 0.00002759
Iteration 246/1000 | Loss: 0.00002759
Iteration 247/1000 | Loss: 0.00002759
Iteration 248/1000 | Loss: 0.00002759
Iteration 249/1000 | Loss: 0.00002759
Iteration 250/1000 | Loss: 0.00002759
Iteration 251/1000 | Loss: 0.00002759
Iteration 252/1000 | Loss: 0.00002759
Iteration 253/1000 | Loss: 0.00002759
Iteration 254/1000 | Loss: 0.00002759
Iteration 255/1000 | Loss: 0.00002758
Iteration 256/1000 | Loss: 0.00002758
Iteration 257/1000 | Loss: 0.00002758
Iteration 258/1000 | Loss: 0.00002758
Iteration 259/1000 | Loss: 0.00002758
Iteration 260/1000 | Loss: 0.00002758
Iteration 261/1000 | Loss: 0.00002758
Iteration 262/1000 | Loss: 0.00002758
Iteration 263/1000 | Loss: 0.00002758
Iteration 264/1000 | Loss: 0.00002758
Iteration 265/1000 | Loss: 0.00002758
Iteration 266/1000 | Loss: 0.00002758
Iteration 267/1000 | Loss: 0.00002758
Iteration 268/1000 | Loss: 0.00002757
Iteration 269/1000 | Loss: 0.00002757
Iteration 270/1000 | Loss: 0.00002757
Iteration 271/1000 | Loss: 0.00002757
Iteration 272/1000 | Loss: 0.00002757
Iteration 273/1000 | Loss: 0.00002757
Iteration 274/1000 | Loss: 0.00002757
Iteration 275/1000 | Loss: 0.00002757
Iteration 276/1000 | Loss: 0.00002757
Iteration 277/1000 | Loss: 0.00002757
Iteration 278/1000 | Loss: 0.00002757
Iteration 279/1000 | Loss: 0.00002757
Iteration 280/1000 | Loss: 0.00002757
Iteration 281/1000 | Loss: 0.00002757
Iteration 282/1000 | Loss: 0.00002757
Iteration 283/1000 | Loss: 0.00002757
Iteration 284/1000 | Loss: 0.00002757
Iteration 285/1000 | Loss: 0.00002757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.7569867597776465e-05, 2.7569867597776465e-05, 2.7569867597776465e-05, 2.7569867597776465e-05, 2.7569867597776465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7569867597776465e-05

Optimization complete. Final v2v error: 4.342382431030273 mm

Highest mean error: 6.793735980987549 mm for frame 43

Lowest mean error: 3.063488006591797 mm for frame 137

Saving results

Total time: 225.46830940246582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826462
Iteration 2/25 | Loss: 0.00175158
Iteration 3/25 | Loss: 0.00144711
Iteration 4/25 | Loss: 0.00142208
Iteration 5/25 | Loss: 0.00141849
Iteration 6/25 | Loss: 0.00141849
Iteration 7/25 | Loss: 0.00141849
Iteration 8/25 | Loss: 0.00141849
Iteration 9/25 | Loss: 0.00141849
Iteration 10/25 | Loss: 0.00141849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001418488216586411, 0.001418488216586411, 0.001418488216586411, 0.001418488216586411, 0.001418488216586411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001418488216586411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02769768
Iteration 2/25 | Loss: 0.00189913
Iteration 3/25 | Loss: 0.00189911
Iteration 4/25 | Loss: 0.00189911
Iteration 5/25 | Loss: 0.00189911
Iteration 6/25 | Loss: 0.00189911
Iteration 7/25 | Loss: 0.00189911
Iteration 8/25 | Loss: 0.00189911
Iteration 9/25 | Loss: 0.00189911
Iteration 10/25 | Loss: 0.00189911
Iteration 11/25 | Loss: 0.00189911
Iteration 12/25 | Loss: 0.00189911
Iteration 13/25 | Loss: 0.00189911
Iteration 14/25 | Loss: 0.00189911
Iteration 15/25 | Loss: 0.00189911
Iteration 16/25 | Loss: 0.00189911
Iteration 17/25 | Loss: 0.00189911
Iteration 18/25 | Loss: 0.00189911
Iteration 19/25 | Loss: 0.00189911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018991067772731185, 0.0018991067772731185, 0.0018991067772731185, 0.0018991067772731185, 0.0018991067772731185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018991067772731185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189911
Iteration 2/1000 | Loss: 0.00004079
Iteration 3/1000 | Loss: 0.00002744
Iteration 4/1000 | Loss: 0.00002388
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002045
Iteration 8/1000 | Loss: 0.00001998
Iteration 9/1000 | Loss: 0.00001949
Iteration 10/1000 | Loss: 0.00001912
Iteration 11/1000 | Loss: 0.00001882
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001823
Iteration 17/1000 | Loss: 0.00001822
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001807
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001806
Iteration 24/1000 | Loss: 0.00001805
Iteration 25/1000 | Loss: 0.00001805
Iteration 26/1000 | Loss: 0.00001805
Iteration 27/1000 | Loss: 0.00001804
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00001803
Iteration 32/1000 | Loss: 0.00001802
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001800
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001796
Iteration 45/1000 | Loss: 0.00001796
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001794
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001790
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001789
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001788
Iteration 82/1000 | Loss: 0.00001788
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001788
Iteration 85/1000 | Loss: 0.00001788
Iteration 86/1000 | Loss: 0.00001788
Iteration 87/1000 | Loss: 0.00001788
Iteration 88/1000 | Loss: 0.00001787
Iteration 89/1000 | Loss: 0.00001787
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001786
Iteration 92/1000 | Loss: 0.00001786
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001785
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001783
Iteration 117/1000 | Loss: 0.00001783
Iteration 118/1000 | Loss: 0.00001783
Iteration 119/1000 | Loss: 0.00001783
Iteration 120/1000 | Loss: 0.00001783
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001782
Iteration 124/1000 | Loss: 0.00001782
Iteration 125/1000 | Loss: 0.00001782
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Iteration 138/1000 | Loss: 0.00001781
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001781
Iteration 149/1000 | Loss: 0.00001781
Iteration 150/1000 | Loss: 0.00001781
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001781
Iteration 153/1000 | Loss: 0.00001781
Iteration 154/1000 | Loss: 0.00001781
Iteration 155/1000 | Loss: 0.00001781
Iteration 156/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.7807553376769647e-05, 1.7807553376769647e-05, 1.7807553376769647e-05, 1.7807553376769647e-05, 1.7807553376769647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7807553376769647e-05

Optimization complete. Final v2v error: 3.566406726837158 mm

Highest mean error: 3.9847073554992676 mm for frame 1

Lowest mean error: 3.171051502227783 mm for frame 78

Saving results

Total time: 42.82495737075806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811384
Iteration 2/25 | Loss: 0.00147459
Iteration 3/25 | Loss: 0.00137325
Iteration 4/25 | Loss: 0.00135616
Iteration 5/25 | Loss: 0.00135122
Iteration 6/25 | Loss: 0.00135057
Iteration 7/25 | Loss: 0.00135057
Iteration 8/25 | Loss: 0.00135057
Iteration 9/25 | Loss: 0.00135057
Iteration 10/25 | Loss: 0.00135057
Iteration 11/25 | Loss: 0.00135057
Iteration 12/25 | Loss: 0.00135057
Iteration 13/25 | Loss: 0.00135057
Iteration 14/25 | Loss: 0.00135057
Iteration 15/25 | Loss: 0.00135057
Iteration 16/25 | Loss: 0.00135057
Iteration 17/25 | Loss: 0.00135057
Iteration 18/25 | Loss: 0.00135057
Iteration 19/25 | Loss: 0.00135057
Iteration 20/25 | Loss: 0.00135057
Iteration 21/25 | Loss: 0.00135057
Iteration 22/25 | Loss: 0.00135057
Iteration 23/25 | Loss: 0.00135057
Iteration 24/25 | Loss: 0.00135057
Iteration 25/25 | Loss: 0.00135057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62993836
Iteration 2/25 | Loss: 0.00233769
Iteration 3/25 | Loss: 0.00233766
Iteration 4/25 | Loss: 0.00233766
Iteration 5/25 | Loss: 0.00233766
Iteration 6/25 | Loss: 0.00233765
Iteration 7/25 | Loss: 0.00233765
Iteration 8/25 | Loss: 0.00233765
Iteration 9/25 | Loss: 0.00233765
Iteration 10/25 | Loss: 0.00233765
Iteration 11/25 | Loss: 0.00233765
Iteration 12/25 | Loss: 0.00233765
Iteration 13/25 | Loss: 0.00233765
Iteration 14/25 | Loss: 0.00233765
Iteration 15/25 | Loss: 0.00233765
Iteration 16/25 | Loss: 0.00233765
Iteration 17/25 | Loss: 0.00233765
Iteration 18/25 | Loss: 0.00233765
Iteration 19/25 | Loss: 0.00233765
Iteration 20/25 | Loss: 0.00233765
Iteration 21/25 | Loss: 0.00233765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023376536555588245, 0.0023376536555588245, 0.0023376536555588245, 0.0023376536555588245, 0.0023376536555588245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023376536555588245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233765
Iteration 2/1000 | Loss: 0.00003703
Iteration 3/1000 | Loss: 0.00002310
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001752
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001658
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001585
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001490
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001468
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001466
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001463
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001462
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001458
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001456
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001455
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001449
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001438
Iteration 111/1000 | Loss: 0.00001438
Iteration 112/1000 | Loss: 0.00001438
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001437
Iteration 117/1000 | Loss: 0.00001437
Iteration 118/1000 | Loss: 0.00001437
Iteration 119/1000 | Loss: 0.00001437
Iteration 120/1000 | Loss: 0.00001437
Iteration 121/1000 | Loss: 0.00001437
Iteration 122/1000 | Loss: 0.00001437
Iteration 123/1000 | Loss: 0.00001437
Iteration 124/1000 | Loss: 0.00001437
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001436
Iteration 156/1000 | Loss: 0.00001436
Iteration 157/1000 | Loss: 0.00001436
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001436
Iteration 161/1000 | Loss: 0.00001436
Iteration 162/1000 | Loss: 0.00001436
Iteration 163/1000 | Loss: 0.00001436
Iteration 164/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4358960470417514e-05, 1.4358960470417514e-05, 1.4358960470417514e-05, 1.4358960470417514e-05, 1.4358960470417514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4358960470417514e-05

Optimization complete. Final v2v error: 3.2421514987945557 mm

Highest mean error: 3.4852287769317627 mm for frame 114

Lowest mean error: 2.97628116607666 mm for frame 3

Saving results

Total time: 38.8263738155365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452380
Iteration 2/25 | Loss: 0.00144745
Iteration 3/25 | Loss: 0.00135615
Iteration 4/25 | Loss: 0.00134388
Iteration 5/25 | Loss: 0.00134163
Iteration 6/25 | Loss: 0.00134163
Iteration 7/25 | Loss: 0.00134163
Iteration 8/25 | Loss: 0.00134163
Iteration 9/25 | Loss: 0.00134163
Iteration 10/25 | Loss: 0.00134163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001341632567346096, 0.001341632567346096, 0.001341632567346096, 0.001341632567346096, 0.001341632567346096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001341632567346096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.54658031
Iteration 2/25 | Loss: 0.00187340
Iteration 3/25 | Loss: 0.00187340
Iteration 4/25 | Loss: 0.00187340
Iteration 5/25 | Loss: 0.00187340
Iteration 6/25 | Loss: 0.00187340
Iteration 7/25 | Loss: 0.00187340
Iteration 8/25 | Loss: 0.00187340
Iteration 9/25 | Loss: 0.00187340
Iteration 10/25 | Loss: 0.00187339
Iteration 11/25 | Loss: 0.00187339
Iteration 12/25 | Loss: 0.00187339
Iteration 13/25 | Loss: 0.00187339
Iteration 14/25 | Loss: 0.00187339
Iteration 15/25 | Loss: 0.00187339
Iteration 16/25 | Loss: 0.00187339
Iteration 17/25 | Loss: 0.00187339
Iteration 18/25 | Loss: 0.00187339
Iteration 19/25 | Loss: 0.00187339
Iteration 20/25 | Loss: 0.00187339
Iteration 21/25 | Loss: 0.00187339
Iteration 22/25 | Loss: 0.00187339
Iteration 23/25 | Loss: 0.00187339
Iteration 24/25 | Loss: 0.00187339
Iteration 25/25 | Loss: 0.00187339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001873393077403307, 0.001873393077403307, 0.001873393077403307, 0.001873393077403307, 0.001873393077403307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001873393077403307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187339
Iteration 2/1000 | Loss: 0.00002377
Iteration 3/1000 | Loss: 0.00001961
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001735
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001456
Iteration 12/1000 | Loss: 0.00001435
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001327
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3265936104289722e-05, 1.3265936104289722e-05, 1.3265936104289722e-05, 1.3265936104289722e-05, 1.3265936104289722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3265936104289722e-05

Optimization complete. Final v2v error: 3.1201024055480957 mm

Highest mean error: 3.4457619190216064 mm for frame 87

Lowest mean error: 2.850208282470703 mm for frame 10

Saving results

Total time: 41.822320222854614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733453
Iteration 2/25 | Loss: 0.00187333
Iteration 3/25 | Loss: 0.00145944
Iteration 4/25 | Loss: 0.00139668
Iteration 5/25 | Loss: 0.00138789
Iteration 6/25 | Loss: 0.00138748
Iteration 7/25 | Loss: 0.00138748
Iteration 8/25 | Loss: 0.00138748
Iteration 9/25 | Loss: 0.00138748
Iteration 10/25 | Loss: 0.00138748
Iteration 11/25 | Loss: 0.00138748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013874828582629561, 0.0013874828582629561, 0.0013874828582629561, 0.0013874828582629561, 0.0013874828582629561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013874828582629561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57696414
Iteration 2/25 | Loss: 0.00189265
Iteration 3/25 | Loss: 0.00189261
Iteration 4/25 | Loss: 0.00189261
Iteration 5/25 | Loss: 0.00189261
Iteration 6/25 | Loss: 0.00189260
Iteration 7/25 | Loss: 0.00189260
Iteration 8/25 | Loss: 0.00189260
Iteration 9/25 | Loss: 0.00189260
Iteration 10/25 | Loss: 0.00189260
Iteration 11/25 | Loss: 0.00189260
Iteration 12/25 | Loss: 0.00189260
Iteration 13/25 | Loss: 0.00189260
Iteration 14/25 | Loss: 0.00189260
Iteration 15/25 | Loss: 0.00189260
Iteration 16/25 | Loss: 0.00189260
Iteration 17/25 | Loss: 0.00189260
Iteration 18/25 | Loss: 0.00189260
Iteration 19/25 | Loss: 0.00189260
Iteration 20/25 | Loss: 0.00189260
Iteration 21/25 | Loss: 0.00189260
Iteration 22/25 | Loss: 0.00189260
Iteration 23/25 | Loss: 0.00189260
Iteration 24/25 | Loss: 0.00189260
Iteration 25/25 | Loss: 0.00189260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189260
Iteration 2/1000 | Loss: 0.00003788
Iteration 3/1000 | Loss: 0.00002745
Iteration 4/1000 | Loss: 0.00002586
Iteration 5/1000 | Loss: 0.00002486
Iteration 6/1000 | Loss: 0.00002399
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00002300
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002223
Iteration 11/1000 | Loss: 0.00002187
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002139
Iteration 14/1000 | Loss: 0.00002138
Iteration 15/1000 | Loss: 0.00002124
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002118
Iteration 18/1000 | Loss: 0.00002118
Iteration 19/1000 | Loss: 0.00002117
Iteration 20/1000 | Loss: 0.00002113
Iteration 21/1000 | Loss: 0.00002113
Iteration 22/1000 | Loss: 0.00002112
Iteration 23/1000 | Loss: 0.00002109
Iteration 24/1000 | Loss: 0.00002108
Iteration 25/1000 | Loss: 0.00002107
Iteration 26/1000 | Loss: 0.00002107
Iteration 27/1000 | Loss: 0.00002106
Iteration 28/1000 | Loss: 0.00002106
Iteration 29/1000 | Loss: 0.00002105
Iteration 30/1000 | Loss: 0.00002104
Iteration 31/1000 | Loss: 0.00002104
Iteration 32/1000 | Loss: 0.00002103
Iteration 33/1000 | Loss: 0.00002103
Iteration 34/1000 | Loss: 0.00002103
Iteration 35/1000 | Loss: 0.00002103
Iteration 36/1000 | Loss: 0.00002102
Iteration 37/1000 | Loss: 0.00002102
Iteration 38/1000 | Loss: 0.00002101
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00002100
Iteration 41/1000 | Loss: 0.00002099
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002097
Iteration 46/1000 | Loss: 0.00002097
Iteration 47/1000 | Loss: 0.00002097
Iteration 48/1000 | Loss: 0.00002096
Iteration 49/1000 | Loss: 0.00002096
Iteration 50/1000 | Loss: 0.00002096
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002094
Iteration 53/1000 | Loss: 0.00002094
Iteration 54/1000 | Loss: 0.00002094
Iteration 55/1000 | Loss: 0.00002094
Iteration 56/1000 | Loss: 0.00002094
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002091
Iteration 61/1000 | Loss: 0.00002090
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002089
Iteration 64/1000 | Loss: 0.00002089
Iteration 65/1000 | Loss: 0.00002089
Iteration 66/1000 | Loss: 0.00002089
Iteration 67/1000 | Loss: 0.00002088
Iteration 68/1000 | Loss: 0.00002088
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002087
Iteration 75/1000 | Loss: 0.00002087
Iteration 76/1000 | Loss: 0.00002086
Iteration 77/1000 | Loss: 0.00002086
Iteration 78/1000 | Loss: 0.00002086
Iteration 79/1000 | Loss: 0.00002086
Iteration 80/1000 | Loss: 0.00002086
Iteration 81/1000 | Loss: 0.00002086
Iteration 82/1000 | Loss: 0.00002086
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002085
Iteration 89/1000 | Loss: 0.00002085
Iteration 90/1000 | Loss: 0.00002085
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002083
Iteration 97/1000 | Loss: 0.00002083
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002083
Iteration 100/1000 | Loss: 0.00002083
Iteration 101/1000 | Loss: 0.00002082
Iteration 102/1000 | Loss: 0.00002082
Iteration 103/1000 | Loss: 0.00002082
Iteration 104/1000 | Loss: 0.00002082
Iteration 105/1000 | Loss: 0.00002082
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002081
Iteration 108/1000 | Loss: 0.00002081
Iteration 109/1000 | Loss: 0.00002081
Iteration 110/1000 | Loss: 0.00002081
Iteration 111/1000 | Loss: 0.00002080
Iteration 112/1000 | Loss: 0.00002080
Iteration 113/1000 | Loss: 0.00002080
Iteration 114/1000 | Loss: 0.00002080
Iteration 115/1000 | Loss: 0.00002080
Iteration 116/1000 | Loss: 0.00002080
Iteration 117/1000 | Loss: 0.00002080
Iteration 118/1000 | Loss: 0.00002080
Iteration 119/1000 | Loss: 0.00002080
Iteration 120/1000 | Loss: 0.00002080
Iteration 121/1000 | Loss: 0.00002080
Iteration 122/1000 | Loss: 0.00002080
Iteration 123/1000 | Loss: 0.00002080
Iteration 124/1000 | Loss: 0.00002080
Iteration 125/1000 | Loss: 0.00002080
Iteration 126/1000 | Loss: 0.00002080
Iteration 127/1000 | Loss: 0.00002080
Iteration 128/1000 | Loss: 0.00002080
Iteration 129/1000 | Loss: 0.00002080
Iteration 130/1000 | Loss: 0.00002080
Iteration 131/1000 | Loss: 0.00002080
Iteration 132/1000 | Loss: 0.00002080
Iteration 133/1000 | Loss: 0.00002080
Iteration 134/1000 | Loss: 0.00002080
Iteration 135/1000 | Loss: 0.00002080
Iteration 136/1000 | Loss: 0.00002080
Iteration 137/1000 | Loss: 0.00002080
Iteration 138/1000 | Loss: 0.00002080
Iteration 139/1000 | Loss: 0.00002080
Iteration 140/1000 | Loss: 0.00002080
Iteration 141/1000 | Loss: 0.00002080
Iteration 142/1000 | Loss: 0.00002080
Iteration 143/1000 | Loss: 0.00002080
Iteration 144/1000 | Loss: 0.00002080
Iteration 145/1000 | Loss: 0.00002080
Iteration 146/1000 | Loss: 0.00002080
Iteration 147/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.0800032871193253e-05, 2.0800032871193253e-05, 2.0800032871193253e-05, 2.0800032871193253e-05, 2.0800032871193253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0800032871193253e-05

Optimization complete. Final v2v error: 3.8204846382141113 mm

Highest mean error: 4.108745098114014 mm for frame 235

Lowest mean error: 3.592349052429199 mm for frame 125

Saving results

Total time: 42.44682788848877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753110
Iteration 2/25 | Loss: 0.00143322
Iteration 3/25 | Loss: 0.00136065
Iteration 4/25 | Loss: 0.00135262
Iteration 5/25 | Loss: 0.00135149
Iteration 6/25 | Loss: 0.00135149
Iteration 7/25 | Loss: 0.00135149
Iteration 8/25 | Loss: 0.00135149
Iteration 9/25 | Loss: 0.00135149
Iteration 10/25 | Loss: 0.00135149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013514856109395623, 0.0013514856109395623, 0.0013514856109395623, 0.0013514856109395623, 0.0013514856109395623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013514856109395623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53309822
Iteration 2/25 | Loss: 0.00175046
Iteration 3/25 | Loss: 0.00175043
Iteration 4/25 | Loss: 0.00175043
Iteration 5/25 | Loss: 0.00175043
Iteration 6/25 | Loss: 0.00175043
Iteration 7/25 | Loss: 0.00175042
Iteration 8/25 | Loss: 0.00175042
Iteration 9/25 | Loss: 0.00175042
Iteration 10/25 | Loss: 0.00175042
Iteration 11/25 | Loss: 0.00175042
Iteration 12/25 | Loss: 0.00175042
Iteration 13/25 | Loss: 0.00175042
Iteration 14/25 | Loss: 0.00175042
Iteration 15/25 | Loss: 0.00175042
Iteration 16/25 | Loss: 0.00175042
Iteration 17/25 | Loss: 0.00175042
Iteration 18/25 | Loss: 0.00175042
Iteration 19/25 | Loss: 0.00175042
Iteration 20/25 | Loss: 0.00175042
Iteration 21/25 | Loss: 0.00175042
Iteration 22/25 | Loss: 0.00175042
Iteration 23/25 | Loss: 0.00175042
Iteration 24/25 | Loss: 0.00175042
Iteration 25/25 | Loss: 0.00175042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175042
Iteration 2/1000 | Loss: 0.00002959
Iteration 3/1000 | Loss: 0.00002237
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001743
Iteration 9/1000 | Loss: 0.00001705
Iteration 10/1000 | Loss: 0.00001666
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001628
Iteration 14/1000 | Loss: 0.00001622
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001588
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001572
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001550
Iteration 96/1000 | Loss: 0.00001550
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001547
Iteration 118/1000 | Loss: 0.00001547
Iteration 119/1000 | Loss: 0.00001547
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001547
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001547
Iteration 143/1000 | Loss: 0.00001547
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.547205465612933e-05, 1.547205465612933e-05, 1.547205465612933e-05, 1.547205465612933e-05, 1.547205465612933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.547205465612933e-05

Optimization complete. Final v2v error: 3.3811426162719727 mm

Highest mean error: 3.7577147483825684 mm for frame 146

Lowest mean error: 3.090550661087036 mm for frame 212

Saving results

Total time: 41.54882097244263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409532
Iteration 2/25 | Loss: 0.00139009
Iteration 3/25 | Loss: 0.00132994
Iteration 4/25 | Loss: 0.00132042
Iteration 5/25 | Loss: 0.00131735
Iteration 6/25 | Loss: 0.00131668
Iteration 7/25 | Loss: 0.00131668
Iteration 8/25 | Loss: 0.00131668
Iteration 9/25 | Loss: 0.00131668
Iteration 10/25 | Loss: 0.00131668
Iteration 11/25 | Loss: 0.00131668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013166788266971707, 0.0013166788266971707, 0.0013166788266971707, 0.0013166788266971707, 0.0013166788266971707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013166788266971707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24379778
Iteration 2/25 | Loss: 0.00209866
Iteration 3/25 | Loss: 0.00209865
Iteration 4/25 | Loss: 0.00209865
Iteration 5/25 | Loss: 0.00209865
Iteration 6/25 | Loss: 0.00209865
Iteration 7/25 | Loss: 0.00209865
Iteration 8/25 | Loss: 0.00209865
Iteration 9/25 | Loss: 0.00209865
Iteration 10/25 | Loss: 0.00209865
Iteration 11/25 | Loss: 0.00209865
Iteration 12/25 | Loss: 0.00209865
Iteration 13/25 | Loss: 0.00209865
Iteration 14/25 | Loss: 0.00209865
Iteration 15/25 | Loss: 0.00209865
Iteration 16/25 | Loss: 0.00209865
Iteration 17/25 | Loss: 0.00209865
Iteration 18/25 | Loss: 0.00209865
Iteration 19/25 | Loss: 0.00209865
Iteration 20/25 | Loss: 0.00209865
Iteration 21/25 | Loss: 0.00209865
Iteration 22/25 | Loss: 0.00209865
Iteration 23/25 | Loss: 0.00209865
Iteration 24/25 | Loss: 0.00209865
Iteration 25/25 | Loss: 0.00209865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002098650671541691, 0.002098650671541691, 0.002098650671541691, 0.002098650671541691, 0.002098650671541691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002098650671541691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209865
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001192
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001164
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001158
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001145
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001132
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001132
Iteration 30/1000 | Loss: 0.00001131
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001128
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001122
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001114
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001112
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001108
Iteration 143/1000 | Loss: 0.00001108
Iteration 144/1000 | Loss: 0.00001108
Iteration 145/1000 | Loss: 0.00001108
Iteration 146/1000 | Loss: 0.00001108
Iteration 147/1000 | Loss: 0.00001108
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001106
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001106
Iteration 184/1000 | Loss: 0.00001106
Iteration 185/1000 | Loss: 0.00001106
Iteration 186/1000 | Loss: 0.00001106
Iteration 187/1000 | Loss: 0.00001106
Iteration 188/1000 | Loss: 0.00001106
Iteration 189/1000 | Loss: 0.00001106
Iteration 190/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.1056403309339657e-05, 1.1056403309339657e-05, 1.1056403309339657e-05, 1.1056403309339657e-05, 1.1056403309339657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1056403309339657e-05

Optimization complete. Final v2v error: 2.8943166732788086 mm

Highest mean error: 3.0806000232696533 mm for frame 105

Lowest mean error: 2.754981279373169 mm for frame 88

Saving results

Total time: 39.348042726516724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792571
Iteration 2/25 | Loss: 0.00139438
Iteration 3/25 | Loss: 0.00133432
Iteration 4/25 | Loss: 0.00132486
Iteration 5/25 | Loss: 0.00132232
Iteration 6/25 | Loss: 0.00132232
Iteration 7/25 | Loss: 0.00132232
Iteration 8/25 | Loss: 0.00132232
Iteration 9/25 | Loss: 0.00132232
Iteration 10/25 | Loss: 0.00132232
Iteration 11/25 | Loss: 0.00132232
Iteration 12/25 | Loss: 0.00132232
Iteration 13/25 | Loss: 0.00132232
Iteration 14/25 | Loss: 0.00132232
Iteration 15/25 | Loss: 0.00132232
Iteration 16/25 | Loss: 0.00132232
Iteration 17/25 | Loss: 0.00132232
Iteration 18/25 | Loss: 0.00132232
Iteration 19/25 | Loss: 0.00132232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013223205460235476, 0.0013223205460235476, 0.0013223205460235476, 0.0013223205460235476, 0.0013223205460235476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013223205460235476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.36180162
Iteration 2/25 | Loss: 0.00223665
Iteration 3/25 | Loss: 0.00223663
Iteration 4/25 | Loss: 0.00223662
Iteration 5/25 | Loss: 0.00223662
Iteration 6/25 | Loss: 0.00223662
Iteration 7/25 | Loss: 0.00223662
Iteration 8/25 | Loss: 0.00223662
Iteration 9/25 | Loss: 0.00223662
Iteration 10/25 | Loss: 0.00223662
Iteration 11/25 | Loss: 0.00223662
Iteration 12/25 | Loss: 0.00223662
Iteration 13/25 | Loss: 0.00223662
Iteration 14/25 | Loss: 0.00223662
Iteration 15/25 | Loss: 0.00223662
Iteration 16/25 | Loss: 0.00223662
Iteration 17/25 | Loss: 0.00223662
Iteration 18/25 | Loss: 0.00223662
Iteration 19/25 | Loss: 0.00223662
Iteration 20/25 | Loss: 0.00223662
Iteration 21/25 | Loss: 0.00223662
Iteration 22/25 | Loss: 0.00223662
Iteration 23/25 | Loss: 0.00223662
Iteration 24/25 | Loss: 0.00223662
Iteration 25/25 | Loss: 0.00223662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223662
Iteration 2/1000 | Loss: 0.00003268
Iteration 3/1000 | Loss: 0.00002433
Iteration 4/1000 | Loss: 0.00002096
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001695
Iteration 9/1000 | Loss: 0.00001642
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001496
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00001458
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001456
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001426
Iteration 113/1000 | Loss: 0.00001426
Iteration 114/1000 | Loss: 0.00001426
Iteration 115/1000 | Loss: 0.00001426
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001424
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001424
Iteration 124/1000 | Loss: 0.00001423
Iteration 125/1000 | Loss: 0.00001423
Iteration 126/1000 | Loss: 0.00001423
Iteration 127/1000 | Loss: 0.00001423
Iteration 128/1000 | Loss: 0.00001423
Iteration 129/1000 | Loss: 0.00001422
Iteration 130/1000 | Loss: 0.00001422
Iteration 131/1000 | Loss: 0.00001422
Iteration 132/1000 | Loss: 0.00001422
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001421
Iteration 137/1000 | Loss: 0.00001421
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001421
Iteration 145/1000 | Loss: 0.00001421
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001419
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Iteration 159/1000 | Loss: 0.00001419
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001419
Iteration 163/1000 | Loss: 0.00001419
Iteration 164/1000 | Loss: 0.00001419
Iteration 165/1000 | Loss: 0.00001419
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001418
Iteration 182/1000 | Loss: 0.00001418
Iteration 183/1000 | Loss: 0.00001418
Iteration 184/1000 | Loss: 0.00001418
Iteration 185/1000 | Loss: 0.00001418
Iteration 186/1000 | Loss: 0.00001418
Iteration 187/1000 | Loss: 0.00001418
Iteration 188/1000 | Loss: 0.00001418
Iteration 189/1000 | Loss: 0.00001418
Iteration 190/1000 | Loss: 0.00001418
Iteration 191/1000 | Loss: 0.00001418
Iteration 192/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.418118699803017e-05, 1.418118699803017e-05, 1.418118699803017e-05, 1.418118699803017e-05, 1.418118699803017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.418118699803017e-05

Optimization complete. Final v2v error: 3.2193260192871094 mm

Highest mean error: 3.92570161819458 mm for frame 146

Lowest mean error: 2.894639253616333 mm for frame 24

Saving results

Total time: 43.7925968170166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525984
Iteration 2/25 | Loss: 0.00156492
Iteration 3/25 | Loss: 0.00137728
Iteration 4/25 | Loss: 0.00136873
Iteration 5/25 | Loss: 0.00136817
Iteration 6/25 | Loss: 0.00136817
Iteration 7/25 | Loss: 0.00136817
Iteration 8/25 | Loss: 0.00136817
Iteration 9/25 | Loss: 0.00136817
Iteration 10/25 | Loss: 0.00136817
Iteration 11/25 | Loss: 0.00136817
Iteration 12/25 | Loss: 0.00136817
Iteration 13/25 | Loss: 0.00136817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013681702548637986, 0.0013681702548637986, 0.0013681702548637986, 0.0013681702548637986, 0.0013681702548637986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013681702548637986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22914386
Iteration 2/25 | Loss: 0.00187842
Iteration 3/25 | Loss: 0.00187842
Iteration 4/25 | Loss: 0.00187842
Iteration 5/25 | Loss: 0.00187842
Iteration 6/25 | Loss: 0.00187842
Iteration 7/25 | Loss: 0.00187842
Iteration 8/25 | Loss: 0.00187842
Iteration 9/25 | Loss: 0.00187841
Iteration 10/25 | Loss: 0.00187841
Iteration 11/25 | Loss: 0.00187841
Iteration 12/25 | Loss: 0.00187841
Iteration 13/25 | Loss: 0.00187841
Iteration 14/25 | Loss: 0.00187841
Iteration 15/25 | Loss: 0.00187841
Iteration 16/25 | Loss: 0.00187841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018784143030643463, 0.0018784143030643463, 0.0018784143030643463, 0.0018784143030643463, 0.0018784143030643463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018784143030643463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187841
Iteration 2/1000 | Loss: 0.00003678
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00002416
Iteration 5/1000 | Loss: 0.00002256
Iteration 6/1000 | Loss: 0.00002170
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001915
Iteration 12/1000 | Loss: 0.00001898
Iteration 13/1000 | Loss: 0.00001884
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001880
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001846
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001845
Iteration 25/1000 | Loss: 0.00001844
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001841
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001840
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001839
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001836
Iteration 34/1000 | Loss: 0.00001836
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001835
Iteration 38/1000 | Loss: 0.00001835
Iteration 39/1000 | Loss: 0.00001835
Iteration 40/1000 | Loss: 0.00001835
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001835
Iteration 43/1000 | Loss: 0.00001835
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001833
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001830
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001826
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001825
Iteration 95/1000 | Loss: 0.00001825
Iteration 96/1000 | Loss: 0.00001825
Iteration 97/1000 | Loss: 0.00001825
Iteration 98/1000 | Loss: 0.00001825
Iteration 99/1000 | Loss: 0.00001824
Iteration 100/1000 | Loss: 0.00001824
Iteration 101/1000 | Loss: 0.00001824
Iteration 102/1000 | Loss: 0.00001823
Iteration 103/1000 | Loss: 0.00001823
Iteration 104/1000 | Loss: 0.00001823
Iteration 105/1000 | Loss: 0.00001823
Iteration 106/1000 | Loss: 0.00001823
Iteration 107/1000 | Loss: 0.00001823
Iteration 108/1000 | Loss: 0.00001823
Iteration 109/1000 | Loss: 0.00001823
Iteration 110/1000 | Loss: 0.00001822
Iteration 111/1000 | Loss: 0.00001822
Iteration 112/1000 | Loss: 0.00001822
Iteration 113/1000 | Loss: 0.00001822
Iteration 114/1000 | Loss: 0.00001822
Iteration 115/1000 | Loss: 0.00001822
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001821
Iteration 119/1000 | Loss: 0.00001821
Iteration 120/1000 | Loss: 0.00001821
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001820
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001819
Iteration 134/1000 | Loss: 0.00001819
Iteration 135/1000 | Loss: 0.00001819
Iteration 136/1000 | Loss: 0.00001818
Iteration 137/1000 | Loss: 0.00001818
Iteration 138/1000 | Loss: 0.00001818
Iteration 139/1000 | Loss: 0.00001818
Iteration 140/1000 | Loss: 0.00001818
Iteration 141/1000 | Loss: 0.00001818
Iteration 142/1000 | Loss: 0.00001818
Iteration 143/1000 | Loss: 0.00001818
Iteration 144/1000 | Loss: 0.00001818
Iteration 145/1000 | Loss: 0.00001818
Iteration 146/1000 | Loss: 0.00001818
Iteration 147/1000 | Loss: 0.00001818
Iteration 148/1000 | Loss: 0.00001817
Iteration 149/1000 | Loss: 0.00001817
Iteration 150/1000 | Loss: 0.00001817
Iteration 151/1000 | Loss: 0.00001817
Iteration 152/1000 | Loss: 0.00001817
Iteration 153/1000 | Loss: 0.00001816
Iteration 154/1000 | Loss: 0.00001816
Iteration 155/1000 | Loss: 0.00001816
Iteration 156/1000 | Loss: 0.00001816
Iteration 157/1000 | Loss: 0.00001816
Iteration 158/1000 | Loss: 0.00001816
Iteration 159/1000 | Loss: 0.00001815
Iteration 160/1000 | Loss: 0.00001815
Iteration 161/1000 | Loss: 0.00001815
Iteration 162/1000 | Loss: 0.00001815
Iteration 163/1000 | Loss: 0.00001815
Iteration 164/1000 | Loss: 0.00001815
Iteration 165/1000 | Loss: 0.00001815
Iteration 166/1000 | Loss: 0.00001815
Iteration 167/1000 | Loss: 0.00001815
Iteration 168/1000 | Loss: 0.00001815
Iteration 169/1000 | Loss: 0.00001815
Iteration 170/1000 | Loss: 0.00001815
Iteration 171/1000 | Loss: 0.00001815
Iteration 172/1000 | Loss: 0.00001815
Iteration 173/1000 | Loss: 0.00001814
Iteration 174/1000 | Loss: 0.00001814
Iteration 175/1000 | Loss: 0.00001814
Iteration 176/1000 | Loss: 0.00001814
Iteration 177/1000 | Loss: 0.00001814
Iteration 178/1000 | Loss: 0.00001814
Iteration 179/1000 | Loss: 0.00001814
Iteration 180/1000 | Loss: 0.00001814
Iteration 181/1000 | Loss: 0.00001814
Iteration 182/1000 | Loss: 0.00001814
Iteration 183/1000 | Loss: 0.00001814
Iteration 184/1000 | Loss: 0.00001814
Iteration 185/1000 | Loss: 0.00001814
Iteration 186/1000 | Loss: 0.00001814
Iteration 187/1000 | Loss: 0.00001814
Iteration 188/1000 | Loss: 0.00001814
Iteration 189/1000 | Loss: 0.00001814
Iteration 190/1000 | Loss: 0.00001814
Iteration 191/1000 | Loss: 0.00001814
Iteration 192/1000 | Loss: 0.00001814
Iteration 193/1000 | Loss: 0.00001814
Iteration 194/1000 | Loss: 0.00001814
Iteration 195/1000 | Loss: 0.00001814
Iteration 196/1000 | Loss: 0.00001814
Iteration 197/1000 | Loss: 0.00001814
Iteration 198/1000 | Loss: 0.00001814
Iteration 199/1000 | Loss: 0.00001814
Iteration 200/1000 | Loss: 0.00001814
Iteration 201/1000 | Loss: 0.00001814
Iteration 202/1000 | Loss: 0.00001814
Iteration 203/1000 | Loss: 0.00001814
Iteration 204/1000 | Loss: 0.00001814
Iteration 205/1000 | Loss: 0.00001814
Iteration 206/1000 | Loss: 0.00001814
Iteration 207/1000 | Loss: 0.00001814
Iteration 208/1000 | Loss: 0.00001814
Iteration 209/1000 | Loss: 0.00001814
Iteration 210/1000 | Loss: 0.00001814
Iteration 211/1000 | Loss: 0.00001814
Iteration 212/1000 | Loss: 0.00001814
Iteration 213/1000 | Loss: 0.00001814
Iteration 214/1000 | Loss: 0.00001814
Iteration 215/1000 | Loss: 0.00001814
Iteration 216/1000 | Loss: 0.00001814
Iteration 217/1000 | Loss: 0.00001814
Iteration 218/1000 | Loss: 0.00001814
Iteration 219/1000 | Loss: 0.00001814
Iteration 220/1000 | Loss: 0.00001814
Iteration 221/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.8142356566386297e-05, 1.8142356566386297e-05, 1.8142356566386297e-05, 1.8142356566386297e-05, 1.8142356566386297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8142356566386297e-05

Optimization complete. Final v2v error: 3.64436674118042 mm

Highest mean error: 4.021440505981445 mm for frame 165

Lowest mean error: 3.4077048301696777 mm for frame 21

Saving results

Total time: 42.56763005256653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963970
Iteration 2/25 | Loss: 0.00375360
Iteration 3/25 | Loss: 0.00290062
Iteration 4/25 | Loss: 0.00265624
Iteration 5/25 | Loss: 0.00263143
Iteration 6/25 | Loss: 0.00247585
Iteration 7/25 | Loss: 0.00237911
Iteration 8/25 | Loss: 0.00222633
Iteration 9/25 | Loss: 0.00199450
Iteration 10/25 | Loss: 0.00187477
Iteration 11/25 | Loss: 0.00184829
Iteration 12/25 | Loss: 0.00183811
Iteration 13/25 | Loss: 0.00177416
Iteration 14/25 | Loss: 0.00176852
Iteration 15/25 | Loss: 0.00174790
Iteration 16/25 | Loss: 0.00174584
Iteration 17/25 | Loss: 0.00174504
Iteration 18/25 | Loss: 0.00174413
Iteration 19/25 | Loss: 0.00174135
Iteration 20/25 | Loss: 0.00176689
Iteration 21/25 | Loss: 0.00173502
Iteration 22/25 | Loss: 0.00171468
Iteration 23/25 | Loss: 0.00171166
Iteration 24/25 | Loss: 0.00171091
Iteration 25/25 | Loss: 0.00171480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01734495
Iteration 2/25 | Loss: 0.01111746
Iteration 3/25 | Loss: 0.00549617
Iteration 4/25 | Loss: 0.00547504
Iteration 5/25 | Loss: 0.00547503
Iteration 6/25 | Loss: 0.00547503
Iteration 7/25 | Loss: 0.00547503
Iteration 8/25 | Loss: 0.00547503
Iteration 9/25 | Loss: 0.00547503
Iteration 10/25 | Loss: 0.00547503
Iteration 11/25 | Loss: 0.00547503
Iteration 12/25 | Loss: 0.00547503
Iteration 13/25 | Loss: 0.00547503
Iteration 14/25 | Loss: 0.00547503
Iteration 15/25 | Loss: 0.00547503
Iteration 16/25 | Loss: 0.00547503
Iteration 17/25 | Loss: 0.00547503
Iteration 18/25 | Loss: 0.00547503
Iteration 19/25 | Loss: 0.00547503
Iteration 20/25 | Loss: 0.00547503
Iteration 21/25 | Loss: 0.00547503
Iteration 22/25 | Loss: 0.00547503
Iteration 23/25 | Loss: 0.00547503
Iteration 24/25 | Loss: 0.00547503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0054750288836658, 0.0054750288836658, 0.0054750288836658, 0.0054750288836658, 0.0054750288836658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0054750288836658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00547503
Iteration 2/1000 | Loss: 0.00123017
Iteration 3/1000 | Loss: 0.00111088
Iteration 4/1000 | Loss: 0.00214703
Iteration 5/1000 | Loss: 0.00183091
Iteration 6/1000 | Loss: 0.00167488
Iteration 7/1000 | Loss: 0.00069222
Iteration 8/1000 | Loss: 0.00232198
Iteration 9/1000 | Loss: 0.00278706
Iteration 10/1000 | Loss: 0.00407760
Iteration 11/1000 | Loss: 0.00075732
Iteration 12/1000 | Loss: 0.00264168
Iteration 13/1000 | Loss: 0.00531699
Iteration 14/1000 | Loss: 0.00045539
Iteration 15/1000 | Loss: 0.00079753
Iteration 16/1000 | Loss: 0.00146676
Iteration 17/1000 | Loss: 0.00115457
Iteration 18/1000 | Loss: 0.00481015
Iteration 19/1000 | Loss: 0.00076592
Iteration 20/1000 | Loss: 0.00157631
Iteration 21/1000 | Loss: 0.00042811
Iteration 22/1000 | Loss: 0.00095017
Iteration 23/1000 | Loss: 0.00048305
Iteration 24/1000 | Loss: 0.00016080
Iteration 25/1000 | Loss: 0.00058888
Iteration 26/1000 | Loss: 0.00014552
Iteration 27/1000 | Loss: 0.00013720
Iteration 28/1000 | Loss: 0.00012777
Iteration 29/1000 | Loss: 0.00013262
Iteration 30/1000 | Loss: 0.00036158
Iteration 31/1000 | Loss: 0.00036213
Iteration 32/1000 | Loss: 0.00017369
Iteration 33/1000 | Loss: 0.00063596
Iteration 34/1000 | Loss: 0.00014262
Iteration 35/1000 | Loss: 0.00033285
Iteration 36/1000 | Loss: 0.00336420
Iteration 37/1000 | Loss: 0.00025641
Iteration 38/1000 | Loss: 0.00037260
Iteration 39/1000 | Loss: 0.00035433
Iteration 40/1000 | Loss: 0.00133253
Iteration 41/1000 | Loss: 0.00033982
Iteration 42/1000 | Loss: 0.00034726
Iteration 43/1000 | Loss: 0.00012896
Iteration 44/1000 | Loss: 0.00012465
Iteration 45/1000 | Loss: 0.00191689
Iteration 46/1000 | Loss: 0.00071796
Iteration 47/1000 | Loss: 0.00024367
Iteration 48/1000 | Loss: 0.00013117
Iteration 49/1000 | Loss: 0.00102217
Iteration 50/1000 | Loss: 0.00044146
Iteration 51/1000 | Loss: 0.00013827
Iteration 52/1000 | Loss: 0.00011224
Iteration 53/1000 | Loss: 0.00069975
Iteration 54/1000 | Loss: 0.00075479
Iteration 55/1000 | Loss: 0.00090545
Iteration 56/1000 | Loss: 0.00015789
Iteration 57/1000 | Loss: 0.00014570
Iteration 58/1000 | Loss: 0.00105724
Iteration 59/1000 | Loss: 0.00101791
Iteration 60/1000 | Loss: 0.00110936
Iteration 61/1000 | Loss: 0.00032173
Iteration 62/1000 | Loss: 0.00018417
Iteration 63/1000 | Loss: 0.00012022
Iteration 64/1000 | Loss: 0.00012563
Iteration 65/1000 | Loss: 0.00036272
Iteration 66/1000 | Loss: 0.00035102
Iteration 67/1000 | Loss: 0.00012596
Iteration 68/1000 | Loss: 0.00051472
Iteration 69/1000 | Loss: 0.00026402
Iteration 70/1000 | Loss: 0.00141175
Iteration 71/1000 | Loss: 0.00027923
Iteration 72/1000 | Loss: 0.00042934
Iteration 73/1000 | Loss: 0.00026012
Iteration 74/1000 | Loss: 0.00037785
Iteration 75/1000 | Loss: 0.00015703
Iteration 76/1000 | Loss: 0.00059102
Iteration 77/1000 | Loss: 0.00469407
Iteration 78/1000 | Loss: 0.00184901
Iteration 79/1000 | Loss: 0.00158596
Iteration 80/1000 | Loss: 0.00026911
Iteration 81/1000 | Loss: 0.00018086
Iteration 82/1000 | Loss: 0.00066509
Iteration 83/1000 | Loss: 0.00063322
Iteration 84/1000 | Loss: 0.00044288
Iteration 85/1000 | Loss: 0.00012717
Iteration 86/1000 | Loss: 0.00130912
Iteration 87/1000 | Loss: 0.00144777
Iteration 88/1000 | Loss: 0.00132465
Iteration 89/1000 | Loss: 0.00401990
Iteration 90/1000 | Loss: 0.00085147
Iteration 91/1000 | Loss: 0.00053915
Iteration 92/1000 | Loss: 0.00048946
Iteration 93/1000 | Loss: 0.00165765
Iteration 94/1000 | Loss: 0.00100209
Iteration 95/1000 | Loss: 0.00043202
Iteration 96/1000 | Loss: 0.00048549
Iteration 97/1000 | Loss: 0.00031578
Iteration 98/1000 | Loss: 0.00038768
Iteration 99/1000 | Loss: 0.00106797
Iteration 100/1000 | Loss: 0.00015311
Iteration 101/1000 | Loss: 0.00011129
Iteration 102/1000 | Loss: 0.00010976
Iteration 103/1000 | Loss: 0.00036440
Iteration 104/1000 | Loss: 0.00081019
Iteration 105/1000 | Loss: 0.00338953
Iteration 106/1000 | Loss: 0.00103753
Iteration 107/1000 | Loss: 0.00172279
Iteration 108/1000 | Loss: 0.00093238
Iteration 109/1000 | Loss: 0.00078554
Iteration 110/1000 | Loss: 0.00061141
Iteration 111/1000 | Loss: 0.00072995
Iteration 112/1000 | Loss: 0.00009574
Iteration 113/1000 | Loss: 0.00009547
Iteration 114/1000 | Loss: 0.00008404
Iteration 115/1000 | Loss: 0.00049533
Iteration 116/1000 | Loss: 0.00099202
Iteration 117/1000 | Loss: 0.00050993
Iteration 118/1000 | Loss: 0.00011404
Iteration 119/1000 | Loss: 0.00079530
Iteration 120/1000 | Loss: 0.00084265
Iteration 121/1000 | Loss: 0.00072598
Iteration 122/1000 | Loss: 0.00051360
Iteration 123/1000 | Loss: 0.00227761
Iteration 124/1000 | Loss: 0.00020569
Iteration 125/1000 | Loss: 0.00026355
Iteration 126/1000 | Loss: 0.00007601
Iteration 127/1000 | Loss: 0.00008862
Iteration 128/1000 | Loss: 0.00007221
Iteration 129/1000 | Loss: 0.00007350
Iteration 130/1000 | Loss: 0.00068896
Iteration 131/1000 | Loss: 0.00007767
Iteration 132/1000 | Loss: 0.00093529
Iteration 133/1000 | Loss: 0.00043261
Iteration 134/1000 | Loss: 0.00006975
Iteration 135/1000 | Loss: 0.00007676
Iteration 136/1000 | Loss: 0.00007808
Iteration 137/1000 | Loss: 0.00007938
Iteration 138/1000 | Loss: 0.00007761
Iteration 139/1000 | Loss: 0.00206451
Iteration 140/1000 | Loss: 0.00134511
Iteration 141/1000 | Loss: 0.00186394
Iteration 142/1000 | Loss: 0.00024900
Iteration 143/1000 | Loss: 0.00047834
Iteration 144/1000 | Loss: 0.00007546
Iteration 145/1000 | Loss: 0.00007880
Iteration 146/1000 | Loss: 0.00110164
Iteration 147/1000 | Loss: 0.00109259
Iteration 148/1000 | Loss: 0.00010040
Iteration 149/1000 | Loss: 0.00005588
Iteration 150/1000 | Loss: 0.00062982
Iteration 151/1000 | Loss: 0.00112577
Iteration 152/1000 | Loss: 0.00008225
Iteration 153/1000 | Loss: 0.00007102
Iteration 154/1000 | Loss: 0.00006602
Iteration 155/1000 | Loss: 0.00064921
Iteration 156/1000 | Loss: 0.00167744
Iteration 157/1000 | Loss: 0.00007813
Iteration 158/1000 | Loss: 0.00034669
Iteration 159/1000 | Loss: 0.00011269
Iteration 160/1000 | Loss: 0.00006465
Iteration 161/1000 | Loss: 0.00004600
Iteration 162/1000 | Loss: 0.00004439
Iteration 163/1000 | Loss: 0.00004115
Iteration 164/1000 | Loss: 0.00064859
Iteration 165/1000 | Loss: 0.00004815
Iteration 166/1000 | Loss: 0.00005084
Iteration 167/1000 | Loss: 0.00061745
Iteration 168/1000 | Loss: 0.00005090
Iteration 169/1000 | Loss: 0.00004843
Iteration 170/1000 | Loss: 0.00004532
Iteration 171/1000 | Loss: 0.00004081
Iteration 172/1000 | Loss: 0.00006547
Iteration 173/1000 | Loss: 0.00003963
Iteration 174/1000 | Loss: 0.00061735
Iteration 175/1000 | Loss: 0.00004712
Iteration 176/1000 | Loss: 0.00007659
Iteration 177/1000 | Loss: 0.00040870
Iteration 178/1000 | Loss: 0.00003310
Iteration 179/1000 | Loss: 0.00002788
Iteration 180/1000 | Loss: 0.00002618
Iteration 181/1000 | Loss: 0.00002504
Iteration 182/1000 | Loss: 0.00002391
Iteration 183/1000 | Loss: 0.00002290
Iteration 184/1000 | Loss: 0.00002234
Iteration 185/1000 | Loss: 0.00064182
Iteration 186/1000 | Loss: 0.00004015
Iteration 187/1000 | Loss: 0.00003031
Iteration 188/1000 | Loss: 0.00002679
Iteration 189/1000 | Loss: 0.00002378
Iteration 190/1000 | Loss: 0.00002260
Iteration 191/1000 | Loss: 0.00002162
Iteration 192/1000 | Loss: 0.00002095
Iteration 193/1000 | Loss: 0.00002043
Iteration 194/1000 | Loss: 0.00043846
Iteration 195/1000 | Loss: 0.00002849
Iteration 196/1000 | Loss: 0.00002195
Iteration 197/1000 | Loss: 0.00001981
Iteration 198/1000 | Loss: 0.00001864
Iteration 199/1000 | Loss: 0.00001802
Iteration 200/1000 | Loss: 0.00001766
Iteration 201/1000 | Loss: 0.00001740
Iteration 202/1000 | Loss: 0.00001729
Iteration 203/1000 | Loss: 0.00001722
Iteration 204/1000 | Loss: 0.00001720
Iteration 205/1000 | Loss: 0.00001718
Iteration 206/1000 | Loss: 0.00001717
Iteration 207/1000 | Loss: 0.00001716
Iteration 208/1000 | Loss: 0.00001716
Iteration 209/1000 | Loss: 0.00001715
Iteration 210/1000 | Loss: 0.00001714
Iteration 211/1000 | Loss: 0.00001713
Iteration 212/1000 | Loss: 0.00001712
Iteration 213/1000 | Loss: 0.00001709
Iteration 214/1000 | Loss: 0.00001703
Iteration 215/1000 | Loss: 0.00001702
Iteration 216/1000 | Loss: 0.00001701
Iteration 217/1000 | Loss: 0.00001700
Iteration 218/1000 | Loss: 0.00001700
Iteration 219/1000 | Loss: 0.00001699
Iteration 220/1000 | Loss: 0.00001698
Iteration 221/1000 | Loss: 0.00001695
Iteration 222/1000 | Loss: 0.00001692
Iteration 223/1000 | Loss: 0.00001692
Iteration 224/1000 | Loss: 0.00001692
Iteration 225/1000 | Loss: 0.00001691
Iteration 226/1000 | Loss: 0.00001691
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001689
Iteration 229/1000 | Loss: 0.00001689
Iteration 230/1000 | Loss: 0.00001688
Iteration 231/1000 | Loss: 0.00001688
Iteration 232/1000 | Loss: 0.00001688
Iteration 233/1000 | Loss: 0.00001688
Iteration 234/1000 | Loss: 0.00001688
Iteration 235/1000 | Loss: 0.00001688
Iteration 236/1000 | Loss: 0.00001687
Iteration 237/1000 | Loss: 0.00001687
Iteration 238/1000 | Loss: 0.00001687
Iteration 239/1000 | Loss: 0.00001687
Iteration 240/1000 | Loss: 0.00001686
Iteration 241/1000 | Loss: 0.00001686
Iteration 242/1000 | Loss: 0.00001686
Iteration 243/1000 | Loss: 0.00001685
Iteration 244/1000 | Loss: 0.00001685
Iteration 245/1000 | Loss: 0.00001685
Iteration 246/1000 | Loss: 0.00001684
Iteration 247/1000 | Loss: 0.00001684
Iteration 248/1000 | Loss: 0.00001684
Iteration 249/1000 | Loss: 0.00001683
Iteration 250/1000 | Loss: 0.00001683
Iteration 251/1000 | Loss: 0.00001683
Iteration 252/1000 | Loss: 0.00001682
Iteration 253/1000 | Loss: 0.00001682
Iteration 254/1000 | Loss: 0.00001682
Iteration 255/1000 | Loss: 0.00001681
Iteration 256/1000 | Loss: 0.00001681
Iteration 257/1000 | Loss: 0.00001681
Iteration 258/1000 | Loss: 0.00001680
Iteration 259/1000 | Loss: 0.00001680
Iteration 260/1000 | Loss: 0.00001680
Iteration 261/1000 | Loss: 0.00001680
Iteration 262/1000 | Loss: 0.00001680
Iteration 263/1000 | Loss: 0.00001679
Iteration 264/1000 | Loss: 0.00001679
Iteration 265/1000 | Loss: 0.00001679
Iteration 266/1000 | Loss: 0.00001679
Iteration 267/1000 | Loss: 0.00001679
Iteration 268/1000 | Loss: 0.00001679
Iteration 269/1000 | Loss: 0.00001679
Iteration 270/1000 | Loss: 0.00001678
Iteration 271/1000 | Loss: 0.00001678
Iteration 272/1000 | Loss: 0.00001678
Iteration 273/1000 | Loss: 0.00001678
Iteration 274/1000 | Loss: 0.00001678
Iteration 275/1000 | Loss: 0.00001678
Iteration 276/1000 | Loss: 0.00001677
Iteration 277/1000 | Loss: 0.00001677
Iteration 278/1000 | Loss: 0.00001677
Iteration 279/1000 | Loss: 0.00001677
Iteration 280/1000 | Loss: 0.00001677
Iteration 281/1000 | Loss: 0.00001676
Iteration 282/1000 | Loss: 0.00001676
Iteration 283/1000 | Loss: 0.00001676
Iteration 284/1000 | Loss: 0.00001676
Iteration 285/1000 | Loss: 0.00001676
Iteration 286/1000 | Loss: 0.00001676
Iteration 287/1000 | Loss: 0.00001676
Iteration 288/1000 | Loss: 0.00001676
Iteration 289/1000 | Loss: 0.00001675
Iteration 290/1000 | Loss: 0.00001675
Iteration 291/1000 | Loss: 0.00001675
Iteration 292/1000 | Loss: 0.00001675
Iteration 293/1000 | Loss: 0.00001675
Iteration 294/1000 | Loss: 0.00001675
Iteration 295/1000 | Loss: 0.00001674
Iteration 296/1000 | Loss: 0.00001674
Iteration 297/1000 | Loss: 0.00001674
Iteration 298/1000 | Loss: 0.00001674
Iteration 299/1000 | Loss: 0.00001674
Iteration 300/1000 | Loss: 0.00001673
Iteration 301/1000 | Loss: 0.00001673
Iteration 302/1000 | Loss: 0.00001672
Iteration 303/1000 | Loss: 0.00001672
Iteration 304/1000 | Loss: 0.00001672
Iteration 305/1000 | Loss: 0.00001672
Iteration 306/1000 | Loss: 0.00001672
Iteration 307/1000 | Loss: 0.00001671
Iteration 308/1000 | Loss: 0.00001671
Iteration 309/1000 | Loss: 0.00001670
Iteration 310/1000 | Loss: 0.00001670
Iteration 311/1000 | Loss: 0.00001670
Iteration 312/1000 | Loss: 0.00001670
Iteration 313/1000 | Loss: 0.00001669
Iteration 314/1000 | Loss: 0.00001669
Iteration 315/1000 | Loss: 0.00001668
Iteration 316/1000 | Loss: 0.00001668
Iteration 317/1000 | Loss: 0.00001668
Iteration 318/1000 | Loss: 0.00001668
Iteration 319/1000 | Loss: 0.00001667
Iteration 320/1000 | Loss: 0.00001667
Iteration 321/1000 | Loss: 0.00001667
Iteration 322/1000 | Loss: 0.00001667
Iteration 323/1000 | Loss: 0.00001667
Iteration 324/1000 | Loss: 0.00001667
Iteration 325/1000 | Loss: 0.00001667
Iteration 326/1000 | Loss: 0.00001667
Iteration 327/1000 | Loss: 0.00001666
Iteration 328/1000 | Loss: 0.00001666
Iteration 329/1000 | Loss: 0.00001666
Iteration 330/1000 | Loss: 0.00001666
Iteration 331/1000 | Loss: 0.00001666
Iteration 332/1000 | Loss: 0.00001666
Iteration 333/1000 | Loss: 0.00001666
Iteration 334/1000 | Loss: 0.00001666
Iteration 335/1000 | Loss: 0.00001666
Iteration 336/1000 | Loss: 0.00001665
Iteration 337/1000 | Loss: 0.00001665
Iteration 338/1000 | Loss: 0.00001665
Iteration 339/1000 | Loss: 0.00001665
Iteration 340/1000 | Loss: 0.00001665
Iteration 341/1000 | Loss: 0.00001665
Iteration 342/1000 | Loss: 0.00001665
Iteration 343/1000 | Loss: 0.00001664
Iteration 344/1000 | Loss: 0.00001664
Iteration 345/1000 | Loss: 0.00001664
Iteration 346/1000 | Loss: 0.00001664
Iteration 347/1000 | Loss: 0.00001664
Iteration 348/1000 | Loss: 0.00001664
Iteration 349/1000 | Loss: 0.00001664
Iteration 350/1000 | Loss: 0.00001664
Iteration 351/1000 | Loss: 0.00001664
Iteration 352/1000 | Loss: 0.00001663
Iteration 353/1000 | Loss: 0.00001663
Iteration 354/1000 | Loss: 0.00001663
Iteration 355/1000 | Loss: 0.00001662
Iteration 356/1000 | Loss: 0.00001662
Iteration 357/1000 | Loss: 0.00001662
Iteration 358/1000 | Loss: 0.00001662
Iteration 359/1000 | Loss: 0.00001661
Iteration 360/1000 | Loss: 0.00001661
Iteration 361/1000 | Loss: 0.00001661
Iteration 362/1000 | Loss: 0.00001661
Iteration 363/1000 | Loss: 0.00001661
Iteration 364/1000 | Loss: 0.00001661
Iteration 365/1000 | Loss: 0.00001661
Iteration 366/1000 | Loss: 0.00001661
Iteration 367/1000 | Loss: 0.00001661
Iteration 368/1000 | Loss: 0.00001661
Iteration 369/1000 | Loss: 0.00001661
Iteration 370/1000 | Loss: 0.00001661
Iteration 371/1000 | Loss: 0.00001661
Iteration 372/1000 | Loss: 0.00001661
Iteration 373/1000 | Loss: 0.00001661
Iteration 374/1000 | Loss: 0.00001661
Iteration 375/1000 | Loss: 0.00001661
Iteration 376/1000 | Loss: 0.00001661
Iteration 377/1000 | Loss: 0.00001661
Iteration 378/1000 | Loss: 0.00001661
Iteration 379/1000 | Loss: 0.00001661
Iteration 380/1000 | Loss: 0.00001661
Iteration 381/1000 | Loss: 0.00001661
Iteration 382/1000 | Loss: 0.00001661
Iteration 383/1000 | Loss: 0.00001661
Iteration 384/1000 | Loss: 0.00001661
Iteration 385/1000 | Loss: 0.00001661
Iteration 386/1000 | Loss: 0.00001661
Iteration 387/1000 | Loss: 0.00001661
Iteration 388/1000 | Loss: 0.00001661
Iteration 389/1000 | Loss: 0.00001661
Iteration 390/1000 | Loss: 0.00001661
Iteration 391/1000 | Loss: 0.00001661
Iteration 392/1000 | Loss: 0.00001661
Iteration 393/1000 | Loss: 0.00001661
Iteration 394/1000 | Loss: 0.00001661
Iteration 395/1000 | Loss: 0.00001661
Iteration 396/1000 | Loss: 0.00001661
Iteration 397/1000 | Loss: 0.00001661
Iteration 398/1000 | Loss: 0.00001661
Iteration 399/1000 | Loss: 0.00001661
Iteration 400/1000 | Loss: 0.00001661
Iteration 401/1000 | Loss: 0.00001661
Iteration 402/1000 | Loss: 0.00001661
Iteration 403/1000 | Loss: 0.00001661
Iteration 404/1000 | Loss: 0.00001661
Iteration 405/1000 | Loss: 0.00001661
Iteration 406/1000 | Loss: 0.00001661
Iteration 407/1000 | Loss: 0.00001661
Iteration 408/1000 | Loss: 0.00001661
Iteration 409/1000 | Loss: 0.00001661
Iteration 410/1000 | Loss: 0.00001661
Iteration 411/1000 | Loss: 0.00001661
Iteration 412/1000 | Loss: 0.00001661
Iteration 413/1000 | Loss: 0.00001661
Iteration 414/1000 | Loss: 0.00001661
Iteration 415/1000 | Loss: 0.00001661
Iteration 416/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 416. Stopping optimization.
Last 5 losses: [1.6608089936198667e-05, 1.6608089936198667e-05, 1.6608089936198667e-05, 1.6608089936198667e-05, 1.6608089936198667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6608089936198667e-05

Optimization complete. Final v2v error: 3.4614105224609375 mm

Highest mean error: 5.042283058166504 mm for frame 45

Lowest mean error: 2.6947133541107178 mm for frame 101

Saving results

Total time: 338.30953574180603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00355988
Iteration 2/25 | Loss: 0.00134835
Iteration 3/25 | Loss: 0.00128831
Iteration 4/25 | Loss: 0.00128019
Iteration 5/25 | Loss: 0.00127727
Iteration 6/25 | Loss: 0.00127695
Iteration 7/25 | Loss: 0.00127695
Iteration 8/25 | Loss: 0.00127695
Iteration 9/25 | Loss: 0.00127695
Iteration 10/25 | Loss: 0.00127695
Iteration 11/25 | Loss: 0.00127695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012769490713253617, 0.0012769490713253617, 0.0012769490713253617, 0.0012769490713253617, 0.0012769490713253617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012769490713253617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25227237
Iteration 2/25 | Loss: 0.00221977
Iteration 3/25 | Loss: 0.00221977
Iteration 4/25 | Loss: 0.00221977
Iteration 5/25 | Loss: 0.00221977
Iteration 6/25 | Loss: 0.00221977
Iteration 7/25 | Loss: 0.00221977
Iteration 8/25 | Loss: 0.00221977
Iteration 9/25 | Loss: 0.00221977
Iteration 10/25 | Loss: 0.00221977
Iteration 11/25 | Loss: 0.00221977
Iteration 12/25 | Loss: 0.00221977
Iteration 13/25 | Loss: 0.00221977
Iteration 14/25 | Loss: 0.00221977
Iteration 15/25 | Loss: 0.00221977
Iteration 16/25 | Loss: 0.00221977
Iteration 17/25 | Loss: 0.00221977
Iteration 18/25 | Loss: 0.00221977
Iteration 19/25 | Loss: 0.00221977
Iteration 20/25 | Loss: 0.00221977
Iteration 21/25 | Loss: 0.00221977
Iteration 22/25 | Loss: 0.00221977
Iteration 23/25 | Loss: 0.00221977
Iteration 24/25 | Loss: 0.00221977
Iteration 25/25 | Loss: 0.00221977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221977
Iteration 2/1000 | Loss: 0.00001797
Iteration 3/1000 | Loss: 0.00001329
Iteration 4/1000 | Loss: 0.00001213
Iteration 5/1000 | Loss: 0.00001134
Iteration 6/1000 | Loss: 0.00001068
Iteration 7/1000 | Loss: 0.00001029
Iteration 8/1000 | Loss: 0.00000997
Iteration 9/1000 | Loss: 0.00000970
Iteration 10/1000 | Loss: 0.00000949
Iteration 11/1000 | Loss: 0.00000937
Iteration 12/1000 | Loss: 0.00000931
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000927
Iteration 16/1000 | Loss: 0.00000926
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000909
Iteration 19/1000 | Loss: 0.00000904
Iteration 20/1000 | Loss: 0.00000892
Iteration 21/1000 | Loss: 0.00000892
Iteration 22/1000 | Loss: 0.00000892
Iteration 23/1000 | Loss: 0.00000881
Iteration 24/1000 | Loss: 0.00000875
Iteration 25/1000 | Loss: 0.00000874
Iteration 26/1000 | Loss: 0.00000874
Iteration 27/1000 | Loss: 0.00000874
Iteration 28/1000 | Loss: 0.00000874
Iteration 29/1000 | Loss: 0.00000871
Iteration 30/1000 | Loss: 0.00000871
Iteration 31/1000 | Loss: 0.00000871
Iteration 32/1000 | Loss: 0.00000870
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000869
Iteration 35/1000 | Loss: 0.00000868
Iteration 36/1000 | Loss: 0.00000867
Iteration 37/1000 | Loss: 0.00000867
Iteration 38/1000 | Loss: 0.00000866
Iteration 39/1000 | Loss: 0.00000866
Iteration 40/1000 | Loss: 0.00000866
Iteration 41/1000 | Loss: 0.00000866
Iteration 42/1000 | Loss: 0.00000866
Iteration 43/1000 | Loss: 0.00000865
Iteration 44/1000 | Loss: 0.00000865
Iteration 45/1000 | Loss: 0.00000864
Iteration 46/1000 | Loss: 0.00000864
Iteration 47/1000 | Loss: 0.00000864
Iteration 48/1000 | Loss: 0.00000864
Iteration 49/1000 | Loss: 0.00000863
Iteration 50/1000 | Loss: 0.00000863
Iteration 51/1000 | Loss: 0.00000863
Iteration 52/1000 | Loss: 0.00000863
Iteration 53/1000 | Loss: 0.00000863
Iteration 54/1000 | Loss: 0.00000863
Iteration 55/1000 | Loss: 0.00000862
Iteration 56/1000 | Loss: 0.00000862
Iteration 57/1000 | Loss: 0.00000862
Iteration 58/1000 | Loss: 0.00000861
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000859
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000859
Iteration 66/1000 | Loss: 0.00000859
Iteration 67/1000 | Loss: 0.00000859
Iteration 68/1000 | Loss: 0.00000859
Iteration 69/1000 | Loss: 0.00000859
Iteration 70/1000 | Loss: 0.00000859
Iteration 71/1000 | Loss: 0.00000859
Iteration 72/1000 | Loss: 0.00000858
Iteration 73/1000 | Loss: 0.00000858
Iteration 74/1000 | Loss: 0.00000858
Iteration 75/1000 | Loss: 0.00000858
Iteration 76/1000 | Loss: 0.00000858
Iteration 77/1000 | Loss: 0.00000858
Iteration 78/1000 | Loss: 0.00000858
Iteration 79/1000 | Loss: 0.00000858
Iteration 80/1000 | Loss: 0.00000858
Iteration 81/1000 | Loss: 0.00000858
Iteration 82/1000 | Loss: 0.00000858
Iteration 83/1000 | Loss: 0.00000858
Iteration 84/1000 | Loss: 0.00000857
Iteration 85/1000 | Loss: 0.00000856
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000856
Iteration 88/1000 | Loss: 0.00000856
Iteration 89/1000 | Loss: 0.00000856
Iteration 90/1000 | Loss: 0.00000856
Iteration 91/1000 | Loss: 0.00000856
Iteration 92/1000 | Loss: 0.00000856
Iteration 93/1000 | Loss: 0.00000856
Iteration 94/1000 | Loss: 0.00000856
Iteration 95/1000 | Loss: 0.00000854
Iteration 96/1000 | Loss: 0.00000854
Iteration 97/1000 | Loss: 0.00000854
Iteration 98/1000 | Loss: 0.00000854
Iteration 99/1000 | Loss: 0.00000853
Iteration 100/1000 | Loss: 0.00000853
Iteration 101/1000 | Loss: 0.00000853
Iteration 102/1000 | Loss: 0.00000853
Iteration 103/1000 | Loss: 0.00000853
Iteration 104/1000 | Loss: 0.00000853
Iteration 105/1000 | Loss: 0.00000853
Iteration 106/1000 | Loss: 0.00000853
Iteration 107/1000 | Loss: 0.00000853
Iteration 108/1000 | Loss: 0.00000852
Iteration 109/1000 | Loss: 0.00000852
Iteration 110/1000 | Loss: 0.00000852
Iteration 111/1000 | Loss: 0.00000852
Iteration 112/1000 | Loss: 0.00000852
Iteration 113/1000 | Loss: 0.00000852
Iteration 114/1000 | Loss: 0.00000852
Iteration 115/1000 | Loss: 0.00000852
Iteration 116/1000 | Loss: 0.00000852
Iteration 117/1000 | Loss: 0.00000851
Iteration 118/1000 | Loss: 0.00000851
Iteration 119/1000 | Loss: 0.00000851
Iteration 120/1000 | Loss: 0.00000851
Iteration 121/1000 | Loss: 0.00000851
Iteration 122/1000 | Loss: 0.00000851
Iteration 123/1000 | Loss: 0.00000851
Iteration 124/1000 | Loss: 0.00000851
Iteration 125/1000 | Loss: 0.00000851
Iteration 126/1000 | Loss: 0.00000851
Iteration 127/1000 | Loss: 0.00000851
Iteration 128/1000 | Loss: 0.00000851
Iteration 129/1000 | Loss: 0.00000851
Iteration 130/1000 | Loss: 0.00000851
Iteration 131/1000 | Loss: 0.00000851
Iteration 132/1000 | Loss: 0.00000851
Iteration 133/1000 | Loss: 0.00000850
Iteration 134/1000 | Loss: 0.00000850
Iteration 135/1000 | Loss: 0.00000850
Iteration 136/1000 | Loss: 0.00000850
Iteration 137/1000 | Loss: 0.00000850
Iteration 138/1000 | Loss: 0.00000850
Iteration 139/1000 | Loss: 0.00000850
Iteration 140/1000 | Loss: 0.00000850
Iteration 141/1000 | Loss: 0.00000850
Iteration 142/1000 | Loss: 0.00000850
Iteration 143/1000 | Loss: 0.00000850
Iteration 144/1000 | Loss: 0.00000850
Iteration 145/1000 | Loss: 0.00000850
Iteration 146/1000 | Loss: 0.00000850
Iteration 147/1000 | Loss: 0.00000850
Iteration 148/1000 | Loss: 0.00000850
Iteration 149/1000 | Loss: 0.00000850
Iteration 150/1000 | Loss: 0.00000850
Iteration 151/1000 | Loss: 0.00000850
Iteration 152/1000 | Loss: 0.00000850
Iteration 153/1000 | Loss: 0.00000850
Iteration 154/1000 | Loss: 0.00000850
Iteration 155/1000 | Loss: 0.00000850
Iteration 156/1000 | Loss: 0.00000850
Iteration 157/1000 | Loss: 0.00000850
Iteration 158/1000 | Loss: 0.00000850
Iteration 159/1000 | Loss: 0.00000850
Iteration 160/1000 | Loss: 0.00000850
Iteration 161/1000 | Loss: 0.00000850
Iteration 162/1000 | Loss: 0.00000850
Iteration 163/1000 | Loss: 0.00000850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.504368452122435e-06, 8.504368452122435e-06, 8.504368452122435e-06, 8.504368452122435e-06, 8.504368452122435e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.504368452122435e-06

Optimization complete. Final v2v error: 2.5530474185943604 mm

Highest mean error: 2.6455881595611572 mm for frame 131

Lowest mean error: 2.5102291107177734 mm for frame 106

Saving results

Total time: 37.50278925895691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403950
Iteration 2/25 | Loss: 0.00142665
Iteration 3/25 | Loss: 0.00133539
Iteration 4/25 | Loss: 0.00132510
Iteration 5/25 | Loss: 0.00132303
Iteration 6/25 | Loss: 0.00132247
Iteration 7/25 | Loss: 0.00132231
Iteration 8/25 | Loss: 0.00132231
Iteration 9/25 | Loss: 0.00132231
Iteration 10/25 | Loss: 0.00132231
Iteration 11/25 | Loss: 0.00132231
Iteration 12/25 | Loss: 0.00132231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001322312862612307, 0.001322312862612307, 0.001322312862612307, 0.001322312862612307, 0.001322312862612307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322312862612307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35371649
Iteration 2/25 | Loss: 0.00203259
Iteration 3/25 | Loss: 0.00203259
Iteration 4/25 | Loss: 0.00203259
Iteration 5/25 | Loss: 0.00203258
Iteration 6/25 | Loss: 0.00203258
Iteration 7/25 | Loss: 0.00203258
Iteration 8/25 | Loss: 0.00203258
Iteration 9/25 | Loss: 0.00203258
Iteration 10/25 | Loss: 0.00203258
Iteration 11/25 | Loss: 0.00203258
Iteration 12/25 | Loss: 0.00203258
Iteration 13/25 | Loss: 0.00203258
Iteration 14/25 | Loss: 0.00203258
Iteration 15/25 | Loss: 0.00203258
Iteration 16/25 | Loss: 0.00203258
Iteration 17/25 | Loss: 0.00203258
Iteration 18/25 | Loss: 0.00203258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020325828809291124, 0.0020325828809291124, 0.0020325828809291124, 0.0020325828809291124, 0.0020325828809291124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020325828809291124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203258
Iteration 2/1000 | Loss: 0.00003334
Iteration 3/1000 | Loss: 0.00002298
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001341
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001209
Iteration 30/1000 | Loss: 0.00001209
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001204
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001178
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001177
Iteration 100/1000 | Loss: 0.00001177
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001176
Iteration 103/1000 | Loss: 0.00001176
Iteration 104/1000 | Loss: 0.00001176
Iteration 105/1000 | Loss: 0.00001176
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001173
Iteration 130/1000 | Loss: 0.00001173
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001170
Iteration 157/1000 | Loss: 0.00001170
Iteration 158/1000 | Loss: 0.00001170
Iteration 159/1000 | Loss: 0.00001170
Iteration 160/1000 | Loss: 0.00001170
Iteration 161/1000 | Loss: 0.00001170
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001169
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001168
Iteration 173/1000 | Loss: 0.00001168
Iteration 174/1000 | Loss: 0.00001167
Iteration 175/1000 | Loss: 0.00001167
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001164
Iteration 189/1000 | Loss: 0.00001163
Iteration 190/1000 | Loss: 0.00001163
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Iteration 196/1000 | Loss: 0.00001162
Iteration 197/1000 | Loss: 0.00001162
Iteration 198/1000 | Loss: 0.00001162
Iteration 199/1000 | Loss: 0.00001162
Iteration 200/1000 | Loss: 0.00001162
Iteration 201/1000 | Loss: 0.00001162
Iteration 202/1000 | Loss: 0.00001162
Iteration 203/1000 | Loss: 0.00001161
Iteration 204/1000 | Loss: 0.00001161
Iteration 205/1000 | Loss: 0.00001161
Iteration 206/1000 | Loss: 0.00001161
Iteration 207/1000 | Loss: 0.00001161
Iteration 208/1000 | Loss: 0.00001160
Iteration 209/1000 | Loss: 0.00001160
Iteration 210/1000 | Loss: 0.00001160
Iteration 211/1000 | Loss: 0.00001159
Iteration 212/1000 | Loss: 0.00001159
Iteration 213/1000 | Loss: 0.00001159
Iteration 214/1000 | Loss: 0.00001159
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001157
Iteration 219/1000 | Loss: 0.00001157
Iteration 220/1000 | Loss: 0.00001157
Iteration 221/1000 | Loss: 0.00001157
Iteration 222/1000 | Loss: 0.00001157
Iteration 223/1000 | Loss: 0.00001157
Iteration 224/1000 | Loss: 0.00001157
Iteration 225/1000 | Loss: 0.00001157
Iteration 226/1000 | Loss: 0.00001157
Iteration 227/1000 | Loss: 0.00001157
Iteration 228/1000 | Loss: 0.00001157
Iteration 229/1000 | Loss: 0.00001157
Iteration 230/1000 | Loss: 0.00001157
Iteration 231/1000 | Loss: 0.00001156
Iteration 232/1000 | Loss: 0.00001156
Iteration 233/1000 | Loss: 0.00001156
Iteration 234/1000 | Loss: 0.00001156
Iteration 235/1000 | Loss: 0.00001156
Iteration 236/1000 | Loss: 0.00001156
Iteration 237/1000 | Loss: 0.00001156
Iteration 238/1000 | Loss: 0.00001156
Iteration 239/1000 | Loss: 0.00001156
Iteration 240/1000 | Loss: 0.00001156
Iteration 241/1000 | Loss: 0.00001156
Iteration 242/1000 | Loss: 0.00001156
Iteration 243/1000 | Loss: 0.00001156
Iteration 244/1000 | Loss: 0.00001156
Iteration 245/1000 | Loss: 0.00001155
Iteration 246/1000 | Loss: 0.00001155
Iteration 247/1000 | Loss: 0.00001155
Iteration 248/1000 | Loss: 0.00001155
Iteration 249/1000 | Loss: 0.00001155
Iteration 250/1000 | Loss: 0.00001154
Iteration 251/1000 | Loss: 0.00001154
Iteration 252/1000 | Loss: 0.00001154
Iteration 253/1000 | Loss: 0.00001154
Iteration 254/1000 | Loss: 0.00001154
Iteration 255/1000 | Loss: 0.00001154
Iteration 256/1000 | Loss: 0.00001154
Iteration 257/1000 | Loss: 0.00001154
Iteration 258/1000 | Loss: 0.00001153
Iteration 259/1000 | Loss: 0.00001153
Iteration 260/1000 | Loss: 0.00001153
Iteration 261/1000 | Loss: 0.00001153
Iteration 262/1000 | Loss: 0.00001152
Iteration 263/1000 | Loss: 0.00001152
Iteration 264/1000 | Loss: 0.00001152
Iteration 265/1000 | Loss: 0.00001152
Iteration 266/1000 | Loss: 0.00001151
Iteration 267/1000 | Loss: 0.00001151
Iteration 268/1000 | Loss: 0.00001151
Iteration 269/1000 | Loss: 0.00001151
Iteration 270/1000 | Loss: 0.00001151
Iteration 271/1000 | Loss: 0.00001151
Iteration 272/1000 | Loss: 0.00001151
Iteration 273/1000 | Loss: 0.00001151
Iteration 274/1000 | Loss: 0.00001151
Iteration 275/1000 | Loss: 0.00001151
Iteration 276/1000 | Loss: 0.00001151
Iteration 277/1000 | Loss: 0.00001151
Iteration 278/1000 | Loss: 0.00001150
Iteration 279/1000 | Loss: 0.00001150
Iteration 280/1000 | Loss: 0.00001150
Iteration 281/1000 | Loss: 0.00001149
Iteration 282/1000 | Loss: 0.00001149
Iteration 283/1000 | Loss: 0.00001149
Iteration 284/1000 | Loss: 0.00001149
Iteration 285/1000 | Loss: 0.00001149
Iteration 286/1000 | Loss: 0.00001149
Iteration 287/1000 | Loss: 0.00001149
Iteration 288/1000 | Loss: 0.00001149
Iteration 289/1000 | Loss: 0.00001149
Iteration 290/1000 | Loss: 0.00001148
Iteration 291/1000 | Loss: 0.00001148
Iteration 292/1000 | Loss: 0.00001148
Iteration 293/1000 | Loss: 0.00001148
Iteration 294/1000 | Loss: 0.00001148
Iteration 295/1000 | Loss: 0.00001148
Iteration 296/1000 | Loss: 0.00001148
Iteration 297/1000 | Loss: 0.00001148
Iteration 298/1000 | Loss: 0.00001148
Iteration 299/1000 | Loss: 0.00001148
Iteration 300/1000 | Loss: 0.00001148
Iteration 301/1000 | Loss: 0.00001147
Iteration 302/1000 | Loss: 0.00001147
Iteration 303/1000 | Loss: 0.00001147
Iteration 304/1000 | Loss: 0.00001147
Iteration 305/1000 | Loss: 0.00001147
Iteration 306/1000 | Loss: 0.00001147
Iteration 307/1000 | Loss: 0.00001147
Iteration 308/1000 | Loss: 0.00001147
Iteration 309/1000 | Loss: 0.00001147
Iteration 310/1000 | Loss: 0.00001147
Iteration 311/1000 | Loss: 0.00001147
Iteration 312/1000 | Loss: 0.00001147
Iteration 313/1000 | Loss: 0.00001147
Iteration 314/1000 | Loss: 0.00001147
Iteration 315/1000 | Loss: 0.00001147
Iteration 316/1000 | Loss: 0.00001147
Iteration 317/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [1.1467860531411134e-05, 1.1467860531411134e-05, 1.1467860531411134e-05, 1.1467860531411134e-05, 1.1467860531411134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1467860531411134e-05

Optimization complete. Final v2v error: 2.8674964904785156 mm

Highest mean error: 4.249499797821045 mm for frame 60

Lowest mean error: 2.6319632530212402 mm for frame 102

Saving results

Total time: 53.79862833023071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789883
Iteration 2/25 | Loss: 0.00169414
Iteration 3/25 | Loss: 0.00137934
Iteration 4/25 | Loss: 0.00135761
Iteration 5/25 | Loss: 0.00135601
Iteration 6/25 | Loss: 0.00135601
Iteration 7/25 | Loss: 0.00135601
Iteration 8/25 | Loss: 0.00135601
Iteration 9/25 | Loss: 0.00135601
Iteration 10/25 | Loss: 0.00135601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013560065999627113, 0.0013560065999627113, 0.0013560065999627113, 0.0013560065999627113, 0.0013560065999627113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013560065999627113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24737072
Iteration 2/25 | Loss: 0.00198001
Iteration 3/25 | Loss: 0.00198001
Iteration 4/25 | Loss: 0.00198001
Iteration 5/25 | Loss: 0.00198001
Iteration 6/25 | Loss: 0.00198001
Iteration 7/25 | Loss: 0.00198001
Iteration 8/25 | Loss: 0.00198001
Iteration 9/25 | Loss: 0.00198001
Iteration 10/25 | Loss: 0.00198001
Iteration 11/25 | Loss: 0.00198001
Iteration 12/25 | Loss: 0.00198001
Iteration 13/25 | Loss: 0.00198001
Iteration 14/25 | Loss: 0.00198001
Iteration 15/25 | Loss: 0.00198001
Iteration 16/25 | Loss: 0.00198001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019800078589469194, 0.0019800078589469194, 0.0019800078589469194, 0.0019800078589469194, 0.0019800078589469194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019800078589469194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198001
Iteration 2/1000 | Loss: 0.00003831
Iteration 3/1000 | Loss: 0.00002680
Iteration 4/1000 | Loss: 0.00002340
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00002055
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001934
Iteration 9/1000 | Loss: 0.00001879
Iteration 10/1000 | Loss: 0.00001839
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001800
Iteration 13/1000 | Loss: 0.00001781
Iteration 14/1000 | Loss: 0.00001780
Iteration 15/1000 | Loss: 0.00001760
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001740
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001694
Iteration 56/1000 | Loss: 0.00001694
Iteration 57/1000 | Loss: 0.00001693
Iteration 58/1000 | Loss: 0.00001693
Iteration 59/1000 | Loss: 0.00001693
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001683
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001682
Iteration 86/1000 | Loss: 0.00001682
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001682
Iteration 92/1000 | Loss: 0.00001682
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001682
Iteration 95/1000 | Loss: 0.00001682
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001682
Iteration 105/1000 | Loss: 0.00001682
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001680
Iteration 110/1000 | Loss: 0.00001680
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001679
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001678
Iteration 119/1000 | Loss: 0.00001678
Iteration 120/1000 | Loss: 0.00001678
Iteration 121/1000 | Loss: 0.00001678
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001677
Iteration 124/1000 | Loss: 0.00001677
Iteration 125/1000 | Loss: 0.00001677
Iteration 126/1000 | Loss: 0.00001677
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00001676
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001676
Iteration 140/1000 | Loss: 0.00001675
Iteration 141/1000 | Loss: 0.00001675
Iteration 142/1000 | Loss: 0.00001675
Iteration 143/1000 | Loss: 0.00001675
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001674
Iteration 159/1000 | Loss: 0.00001673
Iteration 160/1000 | Loss: 0.00001673
Iteration 161/1000 | Loss: 0.00001673
Iteration 162/1000 | Loss: 0.00001673
Iteration 163/1000 | Loss: 0.00001673
Iteration 164/1000 | Loss: 0.00001673
Iteration 165/1000 | Loss: 0.00001673
Iteration 166/1000 | Loss: 0.00001673
Iteration 167/1000 | Loss: 0.00001673
Iteration 168/1000 | Loss: 0.00001673
Iteration 169/1000 | Loss: 0.00001673
Iteration 170/1000 | Loss: 0.00001673
Iteration 171/1000 | Loss: 0.00001673
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001673
Iteration 174/1000 | Loss: 0.00001673
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00001673
Iteration 177/1000 | Loss: 0.00001673
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001673
Iteration 182/1000 | Loss: 0.00001673
Iteration 183/1000 | Loss: 0.00001673
Iteration 184/1000 | Loss: 0.00001673
Iteration 185/1000 | Loss: 0.00001673
Iteration 186/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.6734622477088124e-05, 1.6734622477088124e-05, 1.6734622477088124e-05, 1.6734622477088124e-05, 1.6734622477088124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6734622477088124e-05

Optimization complete. Final v2v error: 3.4578065872192383 mm

Highest mean error: 4.12490701675415 mm for frame 24

Lowest mean error: 3.1778600215911865 mm for frame 0

Saving results

Total time: 46.43907833099365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952068
Iteration 2/25 | Loss: 0.00315040
Iteration 3/25 | Loss: 0.00225423
Iteration 4/25 | Loss: 0.00206843
Iteration 5/25 | Loss: 0.00194153
Iteration 6/25 | Loss: 0.00194053
Iteration 7/25 | Loss: 0.00185668
Iteration 8/25 | Loss: 0.00183894
Iteration 9/25 | Loss: 0.00182000
Iteration 10/25 | Loss: 0.00178674
Iteration 11/25 | Loss: 0.00174154
Iteration 12/25 | Loss: 0.00170661
Iteration 13/25 | Loss: 0.00169174
Iteration 14/25 | Loss: 0.00168394
Iteration 15/25 | Loss: 0.00169156
Iteration 16/25 | Loss: 0.00167207
Iteration 17/25 | Loss: 0.00165542
Iteration 18/25 | Loss: 0.00165412
Iteration 19/25 | Loss: 0.00164213
Iteration 20/25 | Loss: 0.00163689
Iteration 21/25 | Loss: 0.00163389
Iteration 22/25 | Loss: 0.00163265
Iteration 23/25 | Loss: 0.00163163
Iteration 24/25 | Loss: 0.00162408
Iteration 25/25 | Loss: 0.00162879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18184638
Iteration 2/25 | Loss: 0.00654808
Iteration 3/25 | Loss: 0.00512559
Iteration 4/25 | Loss: 0.00512558
Iteration 5/25 | Loss: 0.00512558
Iteration 6/25 | Loss: 0.00512558
Iteration 7/25 | Loss: 0.00512558
Iteration 8/25 | Loss: 0.00512558
Iteration 9/25 | Loss: 0.00512558
Iteration 10/25 | Loss: 0.00512558
Iteration 11/25 | Loss: 0.00512558
Iteration 12/25 | Loss: 0.00512558
Iteration 13/25 | Loss: 0.00512558
Iteration 14/25 | Loss: 0.00512558
Iteration 15/25 | Loss: 0.00512558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005125576630234718, 0.005125576630234718, 0.005125576630234718, 0.005125576630234718, 0.005125576630234718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005125576630234718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00512558
Iteration 2/1000 | Loss: 0.00171651
Iteration 3/1000 | Loss: 0.00441893
Iteration 4/1000 | Loss: 0.00045887
Iteration 5/1000 | Loss: 0.00108973
Iteration 6/1000 | Loss: 0.00041305
Iteration 7/1000 | Loss: 0.00031372
Iteration 8/1000 | Loss: 0.00017221
Iteration 9/1000 | Loss: 0.00031806
Iteration 10/1000 | Loss: 0.00014830
Iteration 11/1000 | Loss: 0.00031831
Iteration 12/1000 | Loss: 0.00018262
Iteration 13/1000 | Loss: 0.00030794
Iteration 14/1000 | Loss: 0.00013276
Iteration 15/1000 | Loss: 0.00029473
Iteration 16/1000 | Loss: 0.00014808
Iteration 17/1000 | Loss: 0.00012951
Iteration 18/1000 | Loss: 0.00012506
Iteration 19/1000 | Loss: 0.00013314
Iteration 20/1000 | Loss: 0.00012166
Iteration 21/1000 | Loss: 0.00011971
Iteration 22/1000 | Loss: 0.00013513
Iteration 23/1000 | Loss: 0.00011695
Iteration 24/1000 | Loss: 0.00031763
Iteration 25/1000 | Loss: 0.00017578
Iteration 26/1000 | Loss: 0.00011704
Iteration 27/1000 | Loss: 0.00045932
Iteration 28/1000 | Loss: 0.00012444
Iteration 29/1000 | Loss: 0.00011548
Iteration 30/1000 | Loss: 0.00011269
Iteration 31/1000 | Loss: 0.00011048
Iteration 32/1000 | Loss: 0.00030215
Iteration 33/1000 | Loss: 0.00037887
Iteration 34/1000 | Loss: 0.00025641
Iteration 35/1000 | Loss: 0.00011563
Iteration 36/1000 | Loss: 0.00010884
Iteration 37/1000 | Loss: 0.00025702
Iteration 38/1000 | Loss: 0.00013760
Iteration 39/1000 | Loss: 0.00019108
Iteration 40/1000 | Loss: 0.00017991
Iteration 41/1000 | Loss: 0.00032305
Iteration 42/1000 | Loss: 0.00034151
Iteration 43/1000 | Loss: 0.00037974
Iteration 44/1000 | Loss: 0.00032200
Iteration 45/1000 | Loss: 0.00010997
Iteration 46/1000 | Loss: 0.00020698
Iteration 47/1000 | Loss: 0.00011251
Iteration 48/1000 | Loss: 0.00012128
Iteration 49/1000 | Loss: 0.00019149
Iteration 50/1000 | Loss: 0.00015766
Iteration 51/1000 | Loss: 0.00010486
Iteration 52/1000 | Loss: 0.00009498
Iteration 53/1000 | Loss: 0.00009337
Iteration 54/1000 | Loss: 0.00009191
Iteration 55/1000 | Loss: 0.00009073
Iteration 56/1000 | Loss: 0.00008954
Iteration 57/1000 | Loss: 0.00008857
Iteration 58/1000 | Loss: 0.00008792
Iteration 59/1000 | Loss: 0.00008742
Iteration 60/1000 | Loss: 0.00008696
Iteration 61/1000 | Loss: 0.00008664
Iteration 62/1000 | Loss: 0.00008627
Iteration 63/1000 | Loss: 0.00008596
Iteration 64/1000 | Loss: 0.00008569
Iteration 65/1000 | Loss: 0.00008549
Iteration 66/1000 | Loss: 0.00008545
Iteration 67/1000 | Loss: 0.00008544
Iteration 68/1000 | Loss: 0.00008529
Iteration 69/1000 | Loss: 0.00008517
Iteration 70/1000 | Loss: 0.00008511
Iteration 71/1000 | Loss: 0.00008510
Iteration 72/1000 | Loss: 0.00008510
Iteration 73/1000 | Loss: 0.00008503
Iteration 74/1000 | Loss: 0.00008502
Iteration 75/1000 | Loss: 0.00008500
Iteration 76/1000 | Loss: 0.00008498
Iteration 77/1000 | Loss: 0.00008498
Iteration 78/1000 | Loss: 0.00008497
Iteration 79/1000 | Loss: 0.00008497
Iteration 80/1000 | Loss: 0.00008497
Iteration 81/1000 | Loss: 0.00008494
Iteration 82/1000 | Loss: 0.00008494
Iteration 83/1000 | Loss: 0.00008493
Iteration 84/1000 | Loss: 0.00008493
Iteration 85/1000 | Loss: 0.00008493
Iteration 86/1000 | Loss: 0.00008492
Iteration 87/1000 | Loss: 0.00008488
Iteration 88/1000 | Loss: 0.00008474
Iteration 89/1000 | Loss: 0.00008473
Iteration 90/1000 | Loss: 0.00008471
Iteration 91/1000 | Loss: 0.00008467
Iteration 92/1000 | Loss: 0.00008467
Iteration 93/1000 | Loss: 0.00008466
Iteration 94/1000 | Loss: 0.00008460
Iteration 95/1000 | Loss: 0.00008455
Iteration 96/1000 | Loss: 0.00008448
Iteration 97/1000 | Loss: 0.00008447
Iteration 98/1000 | Loss: 0.00008447
Iteration 99/1000 | Loss: 0.00008447
Iteration 100/1000 | Loss: 0.00008447
Iteration 101/1000 | Loss: 0.00008447
Iteration 102/1000 | Loss: 0.00008447
Iteration 103/1000 | Loss: 0.00008447
Iteration 104/1000 | Loss: 0.00008446
Iteration 105/1000 | Loss: 0.00008446
Iteration 106/1000 | Loss: 0.00008443
Iteration 107/1000 | Loss: 0.00008442
Iteration 108/1000 | Loss: 0.00008439
Iteration 109/1000 | Loss: 0.00008438
Iteration 110/1000 | Loss: 0.00008434
Iteration 111/1000 | Loss: 0.00008434
Iteration 112/1000 | Loss: 0.00008433
Iteration 113/1000 | Loss: 0.00008428
Iteration 114/1000 | Loss: 0.00008421
Iteration 115/1000 | Loss: 0.00008414
Iteration 116/1000 | Loss: 0.00008402
Iteration 117/1000 | Loss: 0.00008399
Iteration 118/1000 | Loss: 0.00008386
Iteration 119/1000 | Loss: 0.00008359
Iteration 120/1000 | Loss: 0.00008295
Iteration 121/1000 | Loss: 0.00008215
Iteration 122/1000 | Loss: 0.00008118
Iteration 123/1000 | Loss: 0.00007982
Iteration 124/1000 | Loss: 0.00021714
Iteration 125/1000 | Loss: 0.00008517
Iteration 126/1000 | Loss: 0.00007999
Iteration 127/1000 | Loss: 0.00038378
Iteration 128/1000 | Loss: 0.00008933
Iteration 129/1000 | Loss: 0.00008154
Iteration 130/1000 | Loss: 0.00023158
Iteration 131/1000 | Loss: 0.00059381
Iteration 132/1000 | Loss: 0.00034533
Iteration 133/1000 | Loss: 0.00009476
Iteration 134/1000 | Loss: 0.00045524
Iteration 135/1000 | Loss: 0.00034993
Iteration 136/1000 | Loss: 0.00022856
Iteration 137/1000 | Loss: 0.00023653
Iteration 138/1000 | Loss: 0.00011271
Iteration 139/1000 | Loss: 0.00040485
Iteration 140/1000 | Loss: 0.00024457
Iteration 141/1000 | Loss: 0.00056553
Iteration 142/1000 | Loss: 0.00038366
Iteration 143/1000 | Loss: 0.00008188
Iteration 144/1000 | Loss: 0.00060195
Iteration 145/1000 | Loss: 0.00036698
Iteration 146/1000 | Loss: 0.00055890
Iteration 147/1000 | Loss: 0.00060202
Iteration 148/1000 | Loss: 0.00077317
Iteration 149/1000 | Loss: 0.00011109
Iteration 150/1000 | Loss: 0.00012242
Iteration 151/1000 | Loss: 0.00008627
Iteration 152/1000 | Loss: 0.00007824
Iteration 153/1000 | Loss: 0.00007622
Iteration 154/1000 | Loss: 0.00024272
Iteration 155/1000 | Loss: 0.00118210
Iteration 156/1000 | Loss: 0.00032873
Iteration 157/1000 | Loss: 0.00027210
Iteration 158/1000 | Loss: 0.00015318
Iteration 159/1000 | Loss: 0.00020884
Iteration 160/1000 | Loss: 0.00009698
Iteration 161/1000 | Loss: 0.00025658
Iteration 162/1000 | Loss: 0.00075686
Iteration 163/1000 | Loss: 0.00024743
Iteration 164/1000 | Loss: 0.00020663
Iteration 165/1000 | Loss: 0.00009529
Iteration 166/1000 | Loss: 0.00044844
Iteration 167/1000 | Loss: 0.00052800
Iteration 168/1000 | Loss: 0.00060776
Iteration 169/1000 | Loss: 0.00028032
Iteration 170/1000 | Loss: 0.00022083
Iteration 171/1000 | Loss: 0.00047090
Iteration 172/1000 | Loss: 0.00036530
Iteration 173/1000 | Loss: 0.00009412
Iteration 174/1000 | Loss: 0.00008800
Iteration 175/1000 | Loss: 0.00008419
Iteration 176/1000 | Loss: 0.00031158
Iteration 177/1000 | Loss: 0.00037268
Iteration 178/1000 | Loss: 0.00009498
Iteration 179/1000 | Loss: 0.00008634
Iteration 180/1000 | Loss: 0.00026339
Iteration 181/1000 | Loss: 0.00047589
Iteration 182/1000 | Loss: 0.00024217
Iteration 183/1000 | Loss: 0.00009232
Iteration 184/1000 | Loss: 0.00008422
Iteration 185/1000 | Loss: 0.00043896
Iteration 186/1000 | Loss: 0.00022891
Iteration 187/1000 | Loss: 0.00018936
Iteration 188/1000 | Loss: 0.00008793
Iteration 189/1000 | Loss: 0.00008124
Iteration 190/1000 | Loss: 0.00008001
Iteration 191/1000 | Loss: 0.00007823
Iteration 192/1000 | Loss: 0.00007706
Iteration 193/1000 | Loss: 0.00007580
Iteration 194/1000 | Loss: 0.00007481
Iteration 195/1000 | Loss: 0.00007340
Iteration 196/1000 | Loss: 0.00042844
Iteration 197/1000 | Loss: 0.00034255
Iteration 198/1000 | Loss: 0.00045461
Iteration 199/1000 | Loss: 0.00037960
Iteration 200/1000 | Loss: 0.00024312
Iteration 201/1000 | Loss: 0.00016347
Iteration 202/1000 | Loss: 0.00016070
Iteration 203/1000 | Loss: 0.00011478
Iteration 204/1000 | Loss: 0.00013793
Iteration 205/1000 | Loss: 0.00007240
Iteration 206/1000 | Loss: 0.00006945
Iteration 207/1000 | Loss: 0.00006765
Iteration 208/1000 | Loss: 0.00060252
Iteration 209/1000 | Loss: 0.00036025
Iteration 210/1000 | Loss: 0.00046337
Iteration 211/1000 | Loss: 0.00025833
Iteration 212/1000 | Loss: 0.00020674
Iteration 213/1000 | Loss: 0.00023777
Iteration 214/1000 | Loss: 0.00007195
Iteration 215/1000 | Loss: 0.00006878
Iteration 216/1000 | Loss: 0.00006651
Iteration 217/1000 | Loss: 0.00025004
Iteration 218/1000 | Loss: 0.00007340
Iteration 219/1000 | Loss: 0.00006675
Iteration 220/1000 | Loss: 0.00025281
Iteration 221/1000 | Loss: 0.00020945
Iteration 222/1000 | Loss: 0.00006513
Iteration 223/1000 | Loss: 0.00079476
Iteration 224/1000 | Loss: 0.00059903
Iteration 225/1000 | Loss: 0.00039441
Iteration 226/1000 | Loss: 0.00021077
Iteration 227/1000 | Loss: 0.00074111
Iteration 228/1000 | Loss: 0.00027995
Iteration 229/1000 | Loss: 0.00025904
Iteration 230/1000 | Loss: 0.00048926
Iteration 231/1000 | Loss: 0.00033101
Iteration 232/1000 | Loss: 0.00028630
Iteration 233/1000 | Loss: 0.00011773
Iteration 234/1000 | Loss: 0.00007880
Iteration 235/1000 | Loss: 0.00007371
Iteration 236/1000 | Loss: 0.00061820
Iteration 237/1000 | Loss: 0.00046297
Iteration 238/1000 | Loss: 0.00061941
Iteration 239/1000 | Loss: 0.00085546
Iteration 240/1000 | Loss: 0.00029480
Iteration 241/1000 | Loss: 0.00022659
Iteration 242/1000 | Loss: 0.00008571
Iteration 243/1000 | Loss: 0.00007549
Iteration 244/1000 | Loss: 0.00007146
Iteration 245/1000 | Loss: 0.00016595
Iteration 246/1000 | Loss: 0.00007062
Iteration 247/1000 | Loss: 0.00024972
Iteration 248/1000 | Loss: 0.00026820
Iteration 249/1000 | Loss: 0.00008466
Iteration 250/1000 | Loss: 0.00007153
Iteration 251/1000 | Loss: 0.00006745
Iteration 252/1000 | Loss: 0.00024430
Iteration 253/1000 | Loss: 0.00006918
Iteration 254/1000 | Loss: 0.00006600
Iteration 255/1000 | Loss: 0.00006457
Iteration 256/1000 | Loss: 0.00006330
Iteration 257/1000 | Loss: 0.00024831
Iteration 258/1000 | Loss: 0.00006939
Iteration 259/1000 | Loss: 0.00006448
Iteration 260/1000 | Loss: 0.00006321
Iteration 261/1000 | Loss: 0.00006208
Iteration 262/1000 | Loss: 0.00006127
Iteration 263/1000 | Loss: 0.00006039
Iteration 264/1000 | Loss: 0.00005950
Iteration 265/1000 | Loss: 0.00024295
Iteration 266/1000 | Loss: 0.00019600
Iteration 267/1000 | Loss: 0.00028887
Iteration 268/1000 | Loss: 0.00007537
Iteration 269/1000 | Loss: 0.00005984
Iteration 270/1000 | Loss: 0.00005848
Iteration 271/1000 | Loss: 0.00056735
Iteration 272/1000 | Loss: 0.00043715
Iteration 273/1000 | Loss: 0.00059246
Iteration 274/1000 | Loss: 0.00038126
Iteration 275/1000 | Loss: 0.00034091
Iteration 276/1000 | Loss: 0.00028033
Iteration 277/1000 | Loss: 0.00036916
Iteration 278/1000 | Loss: 0.00008480
Iteration 279/1000 | Loss: 0.00006761
Iteration 280/1000 | Loss: 0.00006215
Iteration 281/1000 | Loss: 0.00024056
Iteration 282/1000 | Loss: 0.00021001
Iteration 283/1000 | Loss: 0.00024190
Iteration 284/1000 | Loss: 0.00042081
Iteration 285/1000 | Loss: 0.00021143
Iteration 286/1000 | Loss: 0.00021206
Iteration 287/1000 | Loss: 0.00020125
Iteration 288/1000 | Loss: 0.00033171
Iteration 289/1000 | Loss: 0.00008658
Iteration 290/1000 | Loss: 0.00009317
Iteration 291/1000 | Loss: 0.00006091
Iteration 292/1000 | Loss: 0.00005704
Iteration 293/1000 | Loss: 0.00023108
Iteration 294/1000 | Loss: 0.00064094
Iteration 295/1000 | Loss: 0.00061318
Iteration 296/1000 | Loss: 0.00043488
Iteration 297/1000 | Loss: 0.00009012
Iteration 298/1000 | Loss: 0.00007145
Iteration 299/1000 | Loss: 0.00006560
Iteration 300/1000 | Loss: 0.00022093
Iteration 301/1000 | Loss: 0.00045054
Iteration 302/1000 | Loss: 0.00021398
Iteration 303/1000 | Loss: 0.00019959
Iteration 304/1000 | Loss: 0.00038524
Iteration 305/1000 | Loss: 0.00008825
Iteration 306/1000 | Loss: 0.00008371
Iteration 307/1000 | Loss: 0.00007652
Iteration 308/1000 | Loss: 0.00007125
Iteration 309/1000 | Loss: 0.00008340
Iteration 310/1000 | Loss: 0.00007280
Iteration 311/1000 | Loss: 0.00007333
Iteration 312/1000 | Loss: 0.00039839
Iteration 313/1000 | Loss: 0.00044854
Iteration 314/1000 | Loss: 0.00007821
Iteration 315/1000 | Loss: 0.00007632
Iteration 316/1000 | Loss: 0.00007322
Iteration 317/1000 | Loss: 0.00007193
Iteration 318/1000 | Loss: 0.00006915
Iteration 319/1000 | Loss: 0.00006669
Iteration 320/1000 | Loss: 0.00006909
Iteration 321/1000 | Loss: 0.00027522
Iteration 322/1000 | Loss: 0.00049107
Iteration 323/1000 | Loss: 0.00035901
Iteration 324/1000 | Loss: 0.00044856
Iteration 325/1000 | Loss: 0.00021673
Iteration 326/1000 | Loss: 0.00010587
Iteration 327/1000 | Loss: 0.00010312
Iteration 328/1000 | Loss: 0.00007591
Iteration 329/1000 | Loss: 0.00024198
Iteration 330/1000 | Loss: 0.00008582
Iteration 331/1000 | Loss: 0.00007315
Iteration 332/1000 | Loss: 0.00007128
Iteration 333/1000 | Loss: 0.00006769
Iteration 334/1000 | Loss: 0.00007195
Iteration 335/1000 | Loss: 0.00006912
Iteration 336/1000 | Loss: 0.00006133
Iteration 337/1000 | Loss: 0.00007042
Iteration 338/1000 | Loss: 0.00006010
Iteration 339/1000 | Loss: 0.00006539
Iteration 340/1000 | Loss: 0.00006488
Iteration 341/1000 | Loss: 0.00006530
Iteration 342/1000 | Loss: 0.00007437
Iteration 343/1000 | Loss: 0.00006629
Iteration 344/1000 | Loss: 0.00015910
Iteration 345/1000 | Loss: 0.00007136
Iteration 346/1000 | Loss: 0.00007556
Iteration 347/1000 | Loss: 0.00007450
Iteration 348/1000 | Loss: 0.00011387
Iteration 349/1000 | Loss: 0.00005872
Iteration 350/1000 | Loss: 0.00025807
Iteration 351/1000 | Loss: 0.00006958
Iteration 352/1000 | Loss: 0.00007597
Iteration 353/1000 | Loss: 0.00006671
Iteration 354/1000 | Loss: 0.00007476
Iteration 355/1000 | Loss: 0.00006509
Iteration 356/1000 | Loss: 0.00006190
Iteration 357/1000 | Loss: 0.00005906
Iteration 358/1000 | Loss: 0.00005925
Iteration 359/1000 | Loss: 0.00006809
Iteration 360/1000 | Loss: 0.00007307
Iteration 361/1000 | Loss: 0.00006941
Iteration 362/1000 | Loss: 0.00007259
Iteration 363/1000 | Loss: 0.00006702
Iteration 364/1000 | Loss: 0.00006883
Iteration 365/1000 | Loss: 0.00007479
Iteration 366/1000 | Loss: 0.00006830
Iteration 367/1000 | Loss: 0.00007336
Iteration 368/1000 | Loss: 0.00007201
Iteration 369/1000 | Loss: 0.00007234
Iteration 370/1000 | Loss: 0.00007027
Iteration 371/1000 | Loss: 0.00007114
Iteration 372/1000 | Loss: 0.00007272
Iteration 373/1000 | Loss: 0.00005613
Iteration 374/1000 | Loss: 0.00006489
Iteration 375/1000 | Loss: 0.00005404
Iteration 376/1000 | Loss: 0.00024062
Iteration 377/1000 | Loss: 0.00019920
Iteration 378/1000 | Loss: 0.00026888
Iteration 379/1000 | Loss: 0.00019556
Iteration 380/1000 | Loss: 0.00023742
Iteration 381/1000 | Loss: 0.00035527
Iteration 382/1000 | Loss: 0.00006952
Iteration 383/1000 | Loss: 0.00007329
Iteration 384/1000 | Loss: 0.00007429
Iteration 385/1000 | Loss: 0.00006449
Iteration 386/1000 | Loss: 0.00005471
Iteration 387/1000 | Loss: 0.00005190
Iteration 388/1000 | Loss: 0.00024094
Iteration 389/1000 | Loss: 0.00020367
Iteration 390/1000 | Loss: 0.00025604
Iteration 391/1000 | Loss: 0.00015912
Iteration 392/1000 | Loss: 0.00017854
Iteration 393/1000 | Loss: 0.00013079
Iteration 394/1000 | Loss: 0.00018792
Iteration 395/1000 | Loss: 0.00012648
Iteration 396/1000 | Loss: 0.00017973
Iteration 397/1000 | Loss: 0.00011603
Iteration 398/1000 | Loss: 0.00005963
Iteration 399/1000 | Loss: 0.00006880
Iteration 400/1000 | Loss: 0.00006665
Iteration 401/1000 | Loss: 0.00006452
Iteration 402/1000 | Loss: 0.00005431
Iteration 403/1000 | Loss: 0.00014543
Iteration 404/1000 | Loss: 0.00007715
Iteration 405/1000 | Loss: 0.00006375
Iteration 406/1000 | Loss: 0.00022907
Iteration 407/1000 | Loss: 0.00009316
Iteration 408/1000 | Loss: 0.00021950
Iteration 409/1000 | Loss: 0.00009076
Iteration 410/1000 | Loss: 0.00034779
Iteration 411/1000 | Loss: 0.00020818
Iteration 412/1000 | Loss: 0.00040388
Iteration 413/1000 | Loss: 0.00051039
Iteration 414/1000 | Loss: 0.00038881
Iteration 415/1000 | Loss: 0.00025675
Iteration 416/1000 | Loss: 0.00006999
Iteration 417/1000 | Loss: 0.00006765
Iteration 418/1000 | Loss: 0.00022584
Iteration 419/1000 | Loss: 0.00018055
Iteration 420/1000 | Loss: 0.00010441
Iteration 421/1000 | Loss: 0.00007223
Iteration 422/1000 | Loss: 0.00018218
Iteration 423/1000 | Loss: 0.00025555
Iteration 424/1000 | Loss: 0.00006786
Iteration 425/1000 | Loss: 0.00006442
Iteration 426/1000 | Loss: 0.00006284
Iteration 427/1000 | Loss: 0.00014395
Iteration 428/1000 | Loss: 0.00012029
Iteration 429/1000 | Loss: 0.00005638
Iteration 430/1000 | Loss: 0.00005572
Iteration 431/1000 | Loss: 0.00006150
Iteration 432/1000 | Loss: 0.00050910
Iteration 433/1000 | Loss: 0.00058558
Iteration 434/1000 | Loss: 0.00034532
Iteration 435/1000 | Loss: 0.00008297
Iteration 436/1000 | Loss: 0.00010372
Iteration 437/1000 | Loss: 0.00010198
Iteration 438/1000 | Loss: 0.00005586
Iteration 439/1000 | Loss: 0.00005249
Iteration 440/1000 | Loss: 0.00005080
Iteration 441/1000 | Loss: 0.00028357
Iteration 442/1000 | Loss: 0.00005521
Iteration 443/1000 | Loss: 0.00005156
Iteration 444/1000 | Loss: 0.00023023
Iteration 445/1000 | Loss: 0.00006433
Iteration 446/1000 | Loss: 0.00005840
Iteration 447/1000 | Loss: 0.00005132
Iteration 448/1000 | Loss: 0.00005872
Iteration 449/1000 | Loss: 0.00005635
Iteration 450/1000 | Loss: 0.00005380
Iteration 451/1000 | Loss: 0.00005348
Iteration 452/1000 | Loss: 0.00005244
Iteration 453/1000 | Loss: 0.00004998
Iteration 454/1000 | Loss: 0.00018231
Iteration 455/1000 | Loss: 0.00024394
Iteration 456/1000 | Loss: 0.00005698
Iteration 457/1000 | Loss: 0.00005111
Iteration 458/1000 | Loss: 0.00004847
Iteration 459/1000 | Loss: 0.00004710
Iteration 460/1000 | Loss: 0.00004592
Iteration 461/1000 | Loss: 0.00004529
Iteration 462/1000 | Loss: 0.00004492
Iteration 463/1000 | Loss: 0.00004443
Iteration 464/1000 | Loss: 0.00004415
Iteration 465/1000 | Loss: 0.00021467
Iteration 466/1000 | Loss: 0.00018169
Iteration 467/1000 | Loss: 0.00020726
Iteration 468/1000 | Loss: 0.00004737
Iteration 469/1000 | Loss: 0.00004561
Iteration 470/1000 | Loss: 0.00004504
Iteration 471/1000 | Loss: 0.00004441
Iteration 472/1000 | Loss: 0.00004384
Iteration 473/1000 | Loss: 0.00020369
Iteration 474/1000 | Loss: 0.00017163
Iteration 475/1000 | Loss: 0.00017139
Iteration 476/1000 | Loss: 0.00004475
Iteration 477/1000 | Loss: 0.00021813
Iteration 478/1000 | Loss: 0.00020216
Iteration 479/1000 | Loss: 0.00021398
Iteration 480/1000 | Loss: 0.00017687
Iteration 481/1000 | Loss: 0.00058724
Iteration 482/1000 | Loss: 0.00041508
Iteration 483/1000 | Loss: 0.00071934
Iteration 484/1000 | Loss: 0.00019162
Iteration 485/1000 | Loss: 0.00022331
Iteration 486/1000 | Loss: 0.00007108
Iteration 487/1000 | Loss: 0.00005739
Iteration 488/1000 | Loss: 0.00004818
Iteration 489/1000 | Loss: 0.00007997
Iteration 490/1000 | Loss: 0.00004536
Iteration 491/1000 | Loss: 0.00004438
Iteration 492/1000 | Loss: 0.00004358
Iteration 493/1000 | Loss: 0.00004317
Iteration 494/1000 | Loss: 0.00004274
Iteration 495/1000 | Loss: 0.00040793
Iteration 496/1000 | Loss: 0.00007492
Iteration 497/1000 | Loss: 0.00005276
Iteration 498/1000 | Loss: 0.00004638
Iteration 499/1000 | Loss: 0.00004300
Iteration 500/1000 | Loss: 0.00004130
Iteration 501/1000 | Loss: 0.00004070
Iteration 502/1000 | Loss: 0.00004017
Iteration 503/1000 | Loss: 0.00003997
Iteration 504/1000 | Loss: 0.00003979
Iteration 505/1000 | Loss: 0.00003958
Iteration 506/1000 | Loss: 0.00003931
Iteration 507/1000 | Loss: 0.00003925
Iteration 508/1000 | Loss: 0.00003905
Iteration 509/1000 | Loss: 0.00022651
Iteration 510/1000 | Loss: 0.00015148
Iteration 511/1000 | Loss: 0.00003951
Iteration 512/1000 | Loss: 0.00022783
Iteration 513/1000 | Loss: 0.00014578
Iteration 514/1000 | Loss: 0.00039902
Iteration 515/1000 | Loss: 0.00016385
Iteration 516/1000 | Loss: 0.00018858
Iteration 517/1000 | Loss: 0.00016066
Iteration 518/1000 | Loss: 0.00004110
Iteration 519/1000 | Loss: 0.00004038
Iteration 520/1000 | Loss: 0.00004010
Iteration 521/1000 | Loss: 0.00021250
Iteration 522/1000 | Loss: 0.00022706
Iteration 523/1000 | Loss: 0.00004244
Iteration 524/1000 | Loss: 0.00003981
Iteration 525/1000 | Loss: 0.00003900
Iteration 526/1000 | Loss: 0.00020180
Iteration 527/1000 | Loss: 0.00012735
Iteration 528/1000 | Loss: 0.00019918
Iteration 529/1000 | Loss: 0.00004234
Iteration 530/1000 | Loss: 0.00004080
Iteration 531/1000 | Loss: 0.00020934
Iteration 532/1000 | Loss: 0.00023643
Iteration 533/1000 | Loss: 0.00016348
Iteration 534/1000 | Loss: 0.00004755
Iteration 535/1000 | Loss: 0.00004301
Iteration 536/1000 | Loss: 0.00004170
Iteration 537/1000 | Loss: 0.00004131
Iteration 538/1000 | Loss: 0.00004096
Iteration 539/1000 | Loss: 0.00004066
Iteration 540/1000 | Loss: 0.00021369
Iteration 541/1000 | Loss: 0.00016410
Iteration 542/1000 | Loss: 0.00020926
Iteration 543/1000 | Loss: 0.00014213
Iteration 544/1000 | Loss: 0.00006913
Iteration 545/1000 | Loss: 0.00009771
Iteration 546/1000 | Loss: 0.00021989
Iteration 547/1000 | Loss: 0.00006999
Iteration 548/1000 | Loss: 0.00004126
Iteration 549/1000 | Loss: 0.00004039
Iteration 550/1000 | Loss: 0.00020624
Iteration 551/1000 | Loss: 0.00005751
Iteration 552/1000 | Loss: 0.00004361
Iteration 553/1000 | Loss: 0.00004115
Iteration 554/1000 | Loss: 0.00004042
Iteration 555/1000 | Loss: 0.00004025
Iteration 556/1000 | Loss: 0.00004017
Iteration 557/1000 | Loss: 0.00022983
Iteration 558/1000 | Loss: 0.00009942
Iteration 559/1000 | Loss: 0.00010782
Iteration 560/1000 | Loss: 0.00013558
Iteration 561/1000 | Loss: 0.00012043
Iteration 562/1000 | Loss: 0.00013097
Iteration 563/1000 | Loss: 0.00015103
Iteration 564/1000 | Loss: 0.00005279
Iteration 565/1000 | Loss: 0.00012971
Iteration 566/1000 | Loss: 0.00014276
Iteration 567/1000 | Loss: 0.00004559
Iteration 568/1000 | Loss: 0.00004443
Iteration 569/1000 | Loss: 0.00004339
Iteration 570/1000 | Loss: 0.00004272
Iteration 571/1000 | Loss: 0.00004234
Iteration 572/1000 | Loss: 0.00004165
Iteration 573/1000 | Loss: 0.00023700
Iteration 574/1000 | Loss: 0.00005183
Iteration 575/1000 | Loss: 0.00004324
Iteration 576/1000 | Loss: 0.00004148
Iteration 577/1000 | Loss: 0.00004065
Iteration 578/1000 | Loss: 0.00003982
Iteration 579/1000 | Loss: 0.00003934
Iteration 580/1000 | Loss: 0.00003911
Iteration 581/1000 | Loss: 0.00003890
Iteration 582/1000 | Loss: 0.00003868
Iteration 583/1000 | Loss: 0.00003862
Iteration 584/1000 | Loss: 0.00003859
Iteration 585/1000 | Loss: 0.00003841
Iteration 586/1000 | Loss: 0.00003837
Iteration 587/1000 | Loss: 0.00003822
Iteration 588/1000 | Loss: 0.00003805
Iteration 589/1000 | Loss: 0.00003795
Iteration 590/1000 | Loss: 0.00003775
Iteration 591/1000 | Loss: 0.00003762
Iteration 592/1000 | Loss: 0.00022987
Iteration 593/1000 | Loss: 0.00016637
Iteration 594/1000 | Loss: 0.00037557
Iteration 595/1000 | Loss: 0.00013957
Iteration 596/1000 | Loss: 0.00015405
Iteration 597/1000 | Loss: 0.00004446
Iteration 598/1000 | Loss: 0.00003951
Iteration 599/1000 | Loss: 0.00003830
Iteration 600/1000 | Loss: 0.00003792
Iteration 601/1000 | Loss: 0.00003770
Iteration 602/1000 | Loss: 0.00021094
Iteration 603/1000 | Loss: 0.00013702
Iteration 604/1000 | Loss: 0.00026794
Iteration 605/1000 | Loss: 0.00012687
Iteration 606/1000 | Loss: 0.00026810
Iteration 607/1000 | Loss: 0.00016210
Iteration 608/1000 | Loss: 0.00022615
Iteration 609/1000 | Loss: 0.00011447
Iteration 610/1000 | Loss: 0.00009690
Iteration 611/1000 | Loss: 0.00004776
Iteration 612/1000 | Loss: 0.00003822
Iteration 613/1000 | Loss: 0.00021736
Iteration 614/1000 | Loss: 0.00005574
Iteration 615/1000 | Loss: 0.00004582
Iteration 616/1000 | Loss: 0.00004004
Iteration 617/1000 | Loss: 0.00003729
Iteration 618/1000 | Loss: 0.00003646
Iteration 619/1000 | Loss: 0.00003605
Iteration 620/1000 | Loss: 0.00003595
Iteration 621/1000 | Loss: 0.00003580
Iteration 622/1000 | Loss: 0.00003574
Iteration 623/1000 | Loss: 0.00003574
Iteration 624/1000 | Loss: 0.00003573
Iteration 625/1000 | Loss: 0.00003573
Iteration 626/1000 | Loss: 0.00003572
Iteration 627/1000 | Loss: 0.00003572
Iteration 628/1000 | Loss: 0.00003572
Iteration 629/1000 | Loss: 0.00003572
Iteration 630/1000 | Loss: 0.00003571
Iteration 631/1000 | Loss: 0.00003571
Iteration 632/1000 | Loss: 0.00003571
Iteration 633/1000 | Loss: 0.00003570
Iteration 634/1000 | Loss: 0.00003570
Iteration 635/1000 | Loss: 0.00003570
Iteration 636/1000 | Loss: 0.00003570
Iteration 637/1000 | Loss: 0.00003569
Iteration 638/1000 | Loss: 0.00003569
Iteration 639/1000 | Loss: 0.00003568
Iteration 640/1000 | Loss: 0.00003568
Iteration 641/1000 | Loss: 0.00003568
Iteration 642/1000 | Loss: 0.00003567
Iteration 643/1000 | Loss: 0.00003567
Iteration 644/1000 | Loss: 0.00003567
Iteration 645/1000 | Loss: 0.00003567
Iteration 646/1000 | Loss: 0.00003567
Iteration 647/1000 | Loss: 0.00003567
Iteration 648/1000 | Loss: 0.00003567
Iteration 649/1000 | Loss: 0.00003566
Iteration 650/1000 | Loss: 0.00003566
Iteration 651/1000 | Loss: 0.00003566
Iteration 652/1000 | Loss: 0.00003566
Iteration 653/1000 | Loss: 0.00003566
Iteration 654/1000 | Loss: 0.00003566
Iteration 655/1000 | Loss: 0.00003566
Iteration 656/1000 | Loss: 0.00003566
Iteration 657/1000 | Loss: 0.00003566
Iteration 658/1000 | Loss: 0.00003566
Iteration 659/1000 | Loss: 0.00003566
Iteration 660/1000 | Loss: 0.00003565
Iteration 661/1000 | Loss: 0.00003565
Iteration 662/1000 | Loss: 0.00003565
Iteration 663/1000 | Loss: 0.00003565
Iteration 664/1000 | Loss: 0.00003565
Iteration 665/1000 | Loss: 0.00003565
Iteration 666/1000 | Loss: 0.00003565
Iteration 667/1000 | Loss: 0.00003564
Iteration 668/1000 | Loss: 0.00003564
Iteration 669/1000 | Loss: 0.00003564
Iteration 670/1000 | Loss: 0.00003564
Iteration 671/1000 | Loss: 0.00003564
Iteration 672/1000 | Loss: 0.00003564
Iteration 673/1000 | Loss: 0.00003564
Iteration 674/1000 | Loss: 0.00003564
Iteration 675/1000 | Loss: 0.00003564
Iteration 676/1000 | Loss: 0.00003563
Iteration 677/1000 | Loss: 0.00003563
Iteration 678/1000 | Loss: 0.00003563
Iteration 679/1000 | Loss: 0.00003563
Iteration 680/1000 | Loss: 0.00003563
Iteration 681/1000 | Loss: 0.00003563
Iteration 682/1000 | Loss: 0.00003563
Iteration 683/1000 | Loss: 0.00003563
Iteration 684/1000 | Loss: 0.00003563
Iteration 685/1000 | Loss: 0.00003563
Iteration 686/1000 | Loss: 0.00003563
Iteration 687/1000 | Loss: 0.00003563
Iteration 688/1000 | Loss: 0.00003563
Iteration 689/1000 | Loss: 0.00003563
Iteration 690/1000 | Loss: 0.00003562
Iteration 691/1000 | Loss: 0.00003562
Iteration 692/1000 | Loss: 0.00003562
Iteration 693/1000 | Loss: 0.00003562
Iteration 694/1000 | Loss: 0.00003561
Iteration 695/1000 | Loss: 0.00003561
Iteration 696/1000 | Loss: 0.00003561
Iteration 697/1000 | Loss: 0.00003561
Iteration 698/1000 | Loss: 0.00003561
Iteration 699/1000 | Loss: 0.00003561
Iteration 700/1000 | Loss: 0.00003560
Iteration 701/1000 | Loss: 0.00003560
Iteration 702/1000 | Loss: 0.00003560
Iteration 703/1000 | Loss: 0.00003559
Iteration 704/1000 | Loss: 0.00003559
Iteration 705/1000 | Loss: 0.00003559
Iteration 706/1000 | Loss: 0.00003559
Iteration 707/1000 | Loss: 0.00003559
Iteration 708/1000 | Loss: 0.00003559
Iteration 709/1000 | Loss: 0.00003559
Iteration 710/1000 | Loss: 0.00003559
Iteration 711/1000 | Loss: 0.00003559
Iteration 712/1000 | Loss: 0.00003559
Iteration 713/1000 | Loss: 0.00003559
Iteration 714/1000 | Loss: 0.00003559
Iteration 715/1000 | Loss: 0.00003559
Iteration 716/1000 | Loss: 0.00003558
Iteration 717/1000 | Loss: 0.00003558
Iteration 718/1000 | Loss: 0.00003558
Iteration 719/1000 | Loss: 0.00003558
Iteration 720/1000 | Loss: 0.00003558
Iteration 721/1000 | Loss: 0.00003558
Iteration 722/1000 | Loss: 0.00003558
Iteration 723/1000 | Loss: 0.00003557
Iteration 724/1000 | Loss: 0.00003557
Iteration 725/1000 | Loss: 0.00003557
Iteration 726/1000 | Loss: 0.00003557
Iteration 727/1000 | Loss: 0.00003557
Iteration 728/1000 | Loss: 0.00003557
Iteration 729/1000 | Loss: 0.00003557
Iteration 730/1000 | Loss: 0.00003557
Iteration 731/1000 | Loss: 0.00003557
Iteration 732/1000 | Loss: 0.00003557
Iteration 733/1000 | Loss: 0.00003557
Iteration 734/1000 | Loss: 0.00003557
Iteration 735/1000 | Loss: 0.00003556
Iteration 736/1000 | Loss: 0.00003556
Iteration 737/1000 | Loss: 0.00003556
Iteration 738/1000 | Loss: 0.00003556
Iteration 739/1000 | Loss: 0.00003556
Iteration 740/1000 | Loss: 0.00003556
Iteration 741/1000 | Loss: 0.00003556
Iteration 742/1000 | Loss: 0.00003556
Iteration 743/1000 | Loss: 0.00003556
Iteration 744/1000 | Loss: 0.00003556
Iteration 745/1000 | Loss: 0.00003556
Iteration 746/1000 | Loss: 0.00003556
Iteration 747/1000 | Loss: 0.00003556
Iteration 748/1000 | Loss: 0.00003556
Iteration 749/1000 | Loss: 0.00003556
Iteration 750/1000 | Loss: 0.00003556
Iteration 751/1000 | Loss: 0.00003556
Iteration 752/1000 | Loss: 0.00003556
Iteration 753/1000 | Loss: 0.00003555
Iteration 754/1000 | Loss: 0.00003555
Iteration 755/1000 | Loss: 0.00003555
Iteration 756/1000 | Loss: 0.00003555
Iteration 757/1000 | Loss: 0.00003555
Iteration 758/1000 | Loss: 0.00003555
Iteration 759/1000 | Loss: 0.00003555
Iteration 760/1000 | Loss: 0.00003555
Iteration 761/1000 | Loss: 0.00003555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 761. Stopping optimization.
Last 5 losses: [3.555346847861074e-05, 3.555346847861074e-05, 3.555346847861074e-05, 3.555346847861074e-05, 3.555346847861074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.555346847861074e-05

Optimization complete. Final v2v error: 3.9687318801879883 mm

Highest mean error: 12.399345397949219 mm for frame 148

Lowest mean error: 3.1947712898254395 mm for frame 99

Saving results

Total time: 980.2280375957489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387752
Iteration 2/25 | Loss: 0.00141306
Iteration 3/25 | Loss: 0.00132410
Iteration 4/25 | Loss: 0.00131749
Iteration 5/25 | Loss: 0.00131726
Iteration 6/25 | Loss: 0.00131726
Iteration 7/25 | Loss: 0.00131726
Iteration 8/25 | Loss: 0.00131726
Iteration 9/25 | Loss: 0.00131726
Iteration 10/25 | Loss: 0.00131726
Iteration 11/25 | Loss: 0.00131726
Iteration 12/25 | Loss: 0.00131726
Iteration 13/25 | Loss: 0.00131726
Iteration 14/25 | Loss: 0.00131726
Iteration 15/25 | Loss: 0.00131726
Iteration 16/25 | Loss: 0.00131726
Iteration 17/25 | Loss: 0.00131726
Iteration 18/25 | Loss: 0.00131726
Iteration 19/25 | Loss: 0.00131726
Iteration 20/25 | Loss: 0.00131726
Iteration 21/25 | Loss: 0.00131726
Iteration 22/25 | Loss: 0.00131726
Iteration 23/25 | Loss: 0.00131726
Iteration 24/25 | Loss: 0.00131726
Iteration 25/25 | Loss: 0.00131726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50375819
Iteration 2/25 | Loss: 0.00210382
Iteration 3/25 | Loss: 0.00210382
Iteration 4/25 | Loss: 0.00210382
Iteration 5/25 | Loss: 0.00210382
Iteration 6/25 | Loss: 0.00210382
Iteration 7/25 | Loss: 0.00210382
Iteration 8/25 | Loss: 0.00210382
Iteration 9/25 | Loss: 0.00210382
Iteration 10/25 | Loss: 0.00210382
Iteration 11/25 | Loss: 0.00210382
Iteration 12/25 | Loss: 0.00210382
Iteration 13/25 | Loss: 0.00210382
Iteration 14/25 | Loss: 0.00210382
Iteration 15/25 | Loss: 0.00210382
Iteration 16/25 | Loss: 0.00210382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021038150880485773, 0.0021038150880485773, 0.0021038150880485773, 0.0021038150880485773, 0.0021038150880485773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021038150880485773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210381
Iteration 2/1000 | Loss: 0.00002883
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001600
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001368
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001253
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001120
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001096
Iteration 17/1000 | Loss: 0.00001095
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001071
Iteration 23/1000 | Loss: 0.00001070
Iteration 24/1000 | Loss: 0.00001063
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001039
Iteration 35/1000 | Loss: 0.00001039
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001038
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001036
Iteration 41/1000 | Loss: 0.00001036
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001033
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001032
Iteration 47/1000 | Loss: 0.00001031
Iteration 48/1000 | Loss: 0.00001030
Iteration 49/1000 | Loss: 0.00001030
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001027
Iteration 55/1000 | Loss: 0.00001027
Iteration 56/1000 | Loss: 0.00001027
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001021
Iteration 67/1000 | Loss: 0.00001021
Iteration 68/1000 | Loss: 0.00001020
Iteration 69/1000 | Loss: 0.00001020
Iteration 70/1000 | Loss: 0.00001019
Iteration 71/1000 | Loss: 0.00001018
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001018
Iteration 74/1000 | Loss: 0.00001017
Iteration 75/1000 | Loss: 0.00001017
Iteration 76/1000 | Loss: 0.00001016
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001011
Iteration 87/1000 | Loss: 0.00001011
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001008
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001007
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001007
Iteration 98/1000 | Loss: 0.00001007
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001006
Iteration 102/1000 | Loss: 0.00001006
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001006
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001005
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001002
Iteration 117/1000 | Loss: 0.00001002
Iteration 118/1000 | Loss: 0.00001002
Iteration 119/1000 | Loss: 0.00001001
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001000
Iteration 123/1000 | Loss: 0.00001000
Iteration 124/1000 | Loss: 0.00001000
Iteration 125/1000 | Loss: 0.00001000
Iteration 126/1000 | Loss: 0.00001000
Iteration 127/1000 | Loss: 0.00001000
Iteration 128/1000 | Loss: 0.00001000
Iteration 129/1000 | Loss: 0.00001000
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00000999
Iteration 132/1000 | Loss: 0.00000999
Iteration 133/1000 | Loss: 0.00000999
Iteration 134/1000 | Loss: 0.00000999
Iteration 135/1000 | Loss: 0.00000999
Iteration 136/1000 | Loss: 0.00000998
Iteration 137/1000 | Loss: 0.00000998
Iteration 138/1000 | Loss: 0.00000998
Iteration 139/1000 | Loss: 0.00000998
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [9.972664884116966e-06, 9.972664884116966e-06, 9.972664884116966e-06, 9.972664884116966e-06, 9.972664884116966e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.972664884116966e-06

Optimization complete. Final v2v error: 2.726590633392334 mm

Highest mean error: 2.941101312637329 mm for frame 148

Lowest mean error: 2.600498676300049 mm for frame 138

Saving results

Total time: 46.73946404457092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508367
Iteration 2/25 | Loss: 0.00142648
Iteration 3/25 | Loss: 0.00135277
Iteration 4/25 | Loss: 0.00134206
Iteration 5/25 | Loss: 0.00133865
Iteration 6/25 | Loss: 0.00133826
Iteration 7/25 | Loss: 0.00133826
Iteration 8/25 | Loss: 0.00133826
Iteration 9/25 | Loss: 0.00133826
Iteration 10/25 | Loss: 0.00133826
Iteration 11/25 | Loss: 0.00133826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013382569886744022, 0.0013382569886744022, 0.0013382569886744022, 0.0013382569886744022, 0.0013382569886744022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013382569886744022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.89760303
Iteration 2/25 | Loss: 0.00226320
Iteration 3/25 | Loss: 0.00226320
Iteration 4/25 | Loss: 0.00226320
Iteration 5/25 | Loss: 0.00226320
Iteration 6/25 | Loss: 0.00226320
Iteration 7/25 | Loss: 0.00226320
Iteration 8/25 | Loss: 0.00226320
Iteration 9/25 | Loss: 0.00226320
Iteration 10/25 | Loss: 0.00226320
Iteration 11/25 | Loss: 0.00226320
Iteration 12/25 | Loss: 0.00226320
Iteration 13/25 | Loss: 0.00226320
Iteration 14/25 | Loss: 0.00226320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0022631995379924774, 0.0022631995379924774, 0.0022631995379924774, 0.0022631995379924774, 0.0022631995379924774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022631995379924774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226320
Iteration 2/1000 | Loss: 0.00003992
Iteration 3/1000 | Loss: 0.00002615
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002113
Iteration 7/1000 | Loss: 0.00002054
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001906
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001862
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001816
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001805
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001796
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001790
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001787
Iteration 28/1000 | Loss: 0.00001787
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001778
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001774
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001771
Iteration 53/1000 | Loss: 0.00001771
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001770
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001767
Iteration 67/1000 | Loss: 0.00001767
Iteration 68/1000 | Loss: 0.00001766
Iteration 69/1000 | Loss: 0.00001766
Iteration 70/1000 | Loss: 0.00001766
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001766
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001766
Iteration 78/1000 | Loss: 0.00001766
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001765
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001760
Iteration 89/1000 | Loss: 0.00001760
Iteration 90/1000 | Loss: 0.00001760
Iteration 91/1000 | Loss: 0.00001760
Iteration 92/1000 | Loss: 0.00001759
Iteration 93/1000 | Loss: 0.00001759
Iteration 94/1000 | Loss: 0.00001759
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001752
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001751
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001747
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001745
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001744
Iteration 155/1000 | Loss: 0.00001744
Iteration 156/1000 | Loss: 0.00001744
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001744
Iteration 163/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7440805095247924e-05, 1.7440805095247924e-05, 1.7440805095247924e-05, 1.7440805095247924e-05, 1.7440805095247924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7440805095247924e-05

Optimization complete. Final v2v error: 3.5439956188201904 mm

Highest mean error: 4.676688194274902 mm for frame 67

Lowest mean error: 3.072190046310425 mm for frame 19

Saving results

Total time: 41.53605651855469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591164
Iteration 2/25 | Loss: 0.00169604
Iteration 3/25 | Loss: 0.00142309
Iteration 4/25 | Loss: 0.00139151
Iteration 5/25 | Loss: 0.00138828
Iteration 6/25 | Loss: 0.00138737
Iteration 7/25 | Loss: 0.00138737
Iteration 8/25 | Loss: 0.00138737
Iteration 9/25 | Loss: 0.00138737
Iteration 10/25 | Loss: 0.00138737
Iteration 11/25 | Loss: 0.00138737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013873664429411292, 0.0013873664429411292, 0.0013873664429411292, 0.0013873664429411292, 0.0013873664429411292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013873664429411292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06786788
Iteration 2/25 | Loss: 0.00187074
Iteration 3/25 | Loss: 0.00187072
Iteration 4/25 | Loss: 0.00187072
Iteration 5/25 | Loss: 0.00187072
Iteration 6/25 | Loss: 0.00187072
Iteration 7/25 | Loss: 0.00187072
Iteration 8/25 | Loss: 0.00187072
Iteration 9/25 | Loss: 0.00187072
Iteration 10/25 | Loss: 0.00187072
Iteration 11/25 | Loss: 0.00187072
Iteration 12/25 | Loss: 0.00187072
Iteration 13/25 | Loss: 0.00187072
Iteration 14/25 | Loss: 0.00187072
Iteration 15/25 | Loss: 0.00187072
Iteration 16/25 | Loss: 0.00187072
Iteration 17/25 | Loss: 0.00187072
Iteration 18/25 | Loss: 0.00187072
Iteration 19/25 | Loss: 0.00187072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001870716456323862, 0.001870716456323862, 0.001870716456323862, 0.001870716456323862, 0.001870716456323862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001870716456323862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187072
Iteration 2/1000 | Loss: 0.00004520
Iteration 3/1000 | Loss: 0.00002967
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001800
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001737
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001675
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001670
Iteration 18/1000 | Loss: 0.00001669
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001643
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001628
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001606
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001605
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001604
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001603
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001602
Iteration 41/1000 | Loss: 0.00001602
Iteration 42/1000 | Loss: 0.00001602
Iteration 43/1000 | Loss: 0.00001601
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001599
Iteration 55/1000 | Loss: 0.00001599
Iteration 56/1000 | Loss: 0.00001599
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001599
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001599
Iteration 65/1000 | Loss: 0.00001599
Iteration 66/1000 | Loss: 0.00001599
Iteration 67/1000 | Loss: 0.00001599
Iteration 68/1000 | Loss: 0.00001599
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001598
Iteration 71/1000 | Loss: 0.00001598
Iteration 72/1000 | Loss: 0.00001598
Iteration 73/1000 | Loss: 0.00001598
Iteration 74/1000 | Loss: 0.00001598
Iteration 75/1000 | Loss: 0.00001598
Iteration 76/1000 | Loss: 0.00001598
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001598
Iteration 79/1000 | Loss: 0.00001598
Iteration 80/1000 | Loss: 0.00001598
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001598
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001598
Iteration 93/1000 | Loss: 0.00001598
Iteration 94/1000 | Loss: 0.00001598
Iteration 95/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.5983714547473937e-05, 1.5983714547473937e-05, 1.5983714547473937e-05, 1.5983714547473937e-05, 1.5983714547473937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5983714547473937e-05

Optimization complete. Final v2v error: 3.280080795288086 mm

Highest mean error: 4.797427654266357 mm for frame 54

Lowest mean error: 2.9258217811584473 mm for frame 138

Saving results

Total time: 35.892738580703735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403866
Iteration 2/25 | Loss: 0.00144677
Iteration 3/25 | Loss: 0.00133021
Iteration 4/25 | Loss: 0.00131526
Iteration 5/25 | Loss: 0.00131317
Iteration 6/25 | Loss: 0.00131305
Iteration 7/25 | Loss: 0.00131305
Iteration 8/25 | Loss: 0.00131305
Iteration 9/25 | Loss: 0.00131305
Iteration 10/25 | Loss: 0.00131305
Iteration 11/25 | Loss: 0.00131305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013130533043295145, 0.0013130533043295145, 0.0013130533043295145, 0.0013130533043295145, 0.0013130533043295145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013130533043295145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22547650
Iteration 2/25 | Loss: 0.00192343
Iteration 3/25 | Loss: 0.00192343
Iteration 4/25 | Loss: 0.00192343
Iteration 5/25 | Loss: 0.00192343
Iteration 6/25 | Loss: 0.00192343
Iteration 7/25 | Loss: 0.00192343
Iteration 8/25 | Loss: 0.00192343
Iteration 9/25 | Loss: 0.00192343
Iteration 10/25 | Loss: 0.00192343
Iteration 11/25 | Loss: 0.00192343
Iteration 12/25 | Loss: 0.00192342
Iteration 13/25 | Loss: 0.00192342
Iteration 14/25 | Loss: 0.00192342
Iteration 15/25 | Loss: 0.00192342
Iteration 16/25 | Loss: 0.00192342
Iteration 17/25 | Loss: 0.00192342
Iteration 18/25 | Loss: 0.00192342
Iteration 19/25 | Loss: 0.00192342
Iteration 20/25 | Loss: 0.00192342
Iteration 21/25 | Loss: 0.00192342
Iteration 22/25 | Loss: 0.00192342
Iteration 23/25 | Loss: 0.00192342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019234243081882596, 0.0019234243081882596, 0.0019234243081882596, 0.0019234243081882596, 0.0019234243081882596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019234243081882596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192342
Iteration 2/1000 | Loss: 0.00002358
Iteration 3/1000 | Loss: 0.00001801
Iteration 4/1000 | Loss: 0.00001652
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001323
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001189
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001185
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001181
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001179
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001167
Iteration 110/1000 | Loss: 0.00001167
Iteration 111/1000 | Loss: 0.00001167
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001164
Iteration 128/1000 | Loss: 0.00001164
Iteration 129/1000 | Loss: 0.00001164
Iteration 130/1000 | Loss: 0.00001164
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001164
Iteration 136/1000 | Loss: 0.00001164
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001164
Iteration 139/1000 | Loss: 0.00001164
Iteration 140/1000 | Loss: 0.00001164
Iteration 141/1000 | Loss: 0.00001164
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001163
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001163
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001163
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001162
Iteration 162/1000 | Loss: 0.00001162
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001162
Iteration 165/1000 | Loss: 0.00001162
Iteration 166/1000 | Loss: 0.00001162
Iteration 167/1000 | Loss: 0.00001162
Iteration 168/1000 | Loss: 0.00001162
Iteration 169/1000 | Loss: 0.00001162
Iteration 170/1000 | Loss: 0.00001162
Iteration 171/1000 | Loss: 0.00001162
Iteration 172/1000 | Loss: 0.00001162
Iteration 173/1000 | Loss: 0.00001162
Iteration 174/1000 | Loss: 0.00001162
Iteration 175/1000 | Loss: 0.00001162
Iteration 176/1000 | Loss: 0.00001162
Iteration 177/1000 | Loss: 0.00001162
Iteration 178/1000 | Loss: 0.00001162
Iteration 179/1000 | Loss: 0.00001162
Iteration 180/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.161557884188369e-05, 1.161557884188369e-05, 1.161557884188369e-05, 1.161557884188369e-05, 1.161557884188369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.161557884188369e-05

Optimization complete. Final v2v error: 2.930783987045288 mm

Highest mean error: 3.2369165420532227 mm for frame 82

Lowest mean error: 2.73136305809021 mm for frame 16

Saving results

Total time: 42.976829528808594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840137
Iteration 2/25 | Loss: 0.00188761
Iteration 3/25 | Loss: 0.00164031
Iteration 4/25 | Loss: 0.00157194
Iteration 5/25 | Loss: 0.00156355
Iteration 6/25 | Loss: 0.00156708
Iteration 7/25 | Loss: 0.00155555
Iteration 8/25 | Loss: 0.00155023
Iteration 9/25 | Loss: 0.00154956
Iteration 10/25 | Loss: 0.00154907
Iteration 11/25 | Loss: 0.00154907
Iteration 12/25 | Loss: 0.00154791
Iteration 13/25 | Loss: 0.00154769
Iteration 14/25 | Loss: 0.00154756
Iteration 15/25 | Loss: 0.00154755
Iteration 16/25 | Loss: 0.00154755
Iteration 17/25 | Loss: 0.00154755
Iteration 18/25 | Loss: 0.00154754
Iteration 19/25 | Loss: 0.00154754
Iteration 20/25 | Loss: 0.00154753
Iteration 21/25 | Loss: 0.00154744
Iteration 22/25 | Loss: 0.00154740
Iteration 23/25 | Loss: 0.00154740
Iteration 24/25 | Loss: 0.00154740
Iteration 25/25 | Loss: 0.00154740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23740315
Iteration 2/25 | Loss: 0.00200972
Iteration 3/25 | Loss: 0.00200972
Iteration 4/25 | Loss: 0.00200971
Iteration 5/25 | Loss: 0.00200971
Iteration 6/25 | Loss: 0.00200971
Iteration 7/25 | Loss: 0.00200971
Iteration 8/25 | Loss: 0.00200971
Iteration 9/25 | Loss: 0.00200971
Iteration 10/25 | Loss: 0.00200971
Iteration 11/25 | Loss: 0.00200971
Iteration 12/25 | Loss: 0.00200971
Iteration 13/25 | Loss: 0.00200971
Iteration 14/25 | Loss: 0.00200971
Iteration 15/25 | Loss: 0.00200971
Iteration 16/25 | Loss: 0.00200971
Iteration 17/25 | Loss: 0.00200971
Iteration 18/25 | Loss: 0.00200971
Iteration 19/25 | Loss: 0.00200971
Iteration 20/25 | Loss: 0.00200971
Iteration 21/25 | Loss: 0.00200971
Iteration 22/25 | Loss: 0.00200971
Iteration 23/25 | Loss: 0.00200971
Iteration 24/25 | Loss: 0.00200971
Iteration 25/25 | Loss: 0.00200971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200971
Iteration 2/1000 | Loss: 0.00007671
Iteration 3/1000 | Loss: 0.00056877
Iteration 4/1000 | Loss: 0.00033927
Iteration 5/1000 | Loss: 0.00006994
Iteration 6/1000 | Loss: 0.00004691
Iteration 7/1000 | Loss: 0.00004193
Iteration 8/1000 | Loss: 0.00003788
Iteration 9/1000 | Loss: 0.00011746
Iteration 10/1000 | Loss: 0.00019847
Iteration 11/1000 | Loss: 0.00005081
Iteration 12/1000 | Loss: 0.00004801
Iteration 13/1000 | Loss: 0.00003567
Iteration 14/1000 | Loss: 0.00003484
Iteration 15/1000 | Loss: 0.00003442
Iteration 16/1000 | Loss: 0.00027987
Iteration 17/1000 | Loss: 0.00029020
Iteration 18/1000 | Loss: 0.00006051
Iteration 19/1000 | Loss: 0.00003402
Iteration 20/1000 | Loss: 0.00004768
Iteration 21/1000 | Loss: 0.00003330
Iteration 22/1000 | Loss: 0.00003298
Iteration 23/1000 | Loss: 0.00017901
Iteration 24/1000 | Loss: 0.00003266
Iteration 25/1000 | Loss: 0.00011321
Iteration 26/1000 | Loss: 0.00003234
Iteration 27/1000 | Loss: 0.00005515
Iteration 28/1000 | Loss: 0.00007383
Iteration 29/1000 | Loss: 0.00005032
Iteration 30/1000 | Loss: 0.00005791
Iteration 31/1000 | Loss: 0.00003845
Iteration 32/1000 | Loss: 0.00003093
Iteration 33/1000 | Loss: 0.00017948
Iteration 34/1000 | Loss: 0.00004813
Iteration 35/1000 | Loss: 0.00004097
Iteration 36/1000 | Loss: 0.00004899
Iteration 37/1000 | Loss: 0.00008603
Iteration 38/1000 | Loss: 0.00008473
Iteration 39/1000 | Loss: 0.00004427
Iteration 40/1000 | Loss: 0.00004663
Iteration 41/1000 | Loss: 0.00003696
Iteration 42/1000 | Loss: 0.00003041
Iteration 43/1000 | Loss: 0.00003022
Iteration 44/1000 | Loss: 0.00003011
Iteration 45/1000 | Loss: 0.00003010
Iteration 46/1000 | Loss: 0.00003010
Iteration 47/1000 | Loss: 0.00003010
Iteration 48/1000 | Loss: 0.00003010
Iteration 49/1000 | Loss: 0.00003010
Iteration 50/1000 | Loss: 0.00003009
Iteration 51/1000 | Loss: 0.00003009
Iteration 52/1000 | Loss: 0.00003009
Iteration 53/1000 | Loss: 0.00003009
Iteration 54/1000 | Loss: 0.00003009
Iteration 55/1000 | Loss: 0.00003008
Iteration 56/1000 | Loss: 0.00003006
Iteration 57/1000 | Loss: 0.00003006
Iteration 58/1000 | Loss: 0.00003004
Iteration 59/1000 | Loss: 0.00003004
Iteration 60/1000 | Loss: 0.00003004
Iteration 61/1000 | Loss: 0.00003003
Iteration 62/1000 | Loss: 0.00003003
Iteration 63/1000 | Loss: 0.00003002
Iteration 64/1000 | Loss: 0.00003001
Iteration 65/1000 | Loss: 0.00003000
Iteration 66/1000 | Loss: 0.00003000
Iteration 67/1000 | Loss: 0.00007172
Iteration 68/1000 | Loss: 0.00013689
Iteration 69/1000 | Loss: 0.00003564
Iteration 70/1000 | Loss: 0.00003027
Iteration 71/1000 | Loss: 0.00003001
Iteration 72/1000 | Loss: 0.00002995
Iteration 73/1000 | Loss: 0.00002992
Iteration 74/1000 | Loss: 0.00002992
Iteration 75/1000 | Loss: 0.00002991
Iteration 76/1000 | Loss: 0.00002991
Iteration 77/1000 | Loss: 0.00002991
Iteration 78/1000 | Loss: 0.00002991
Iteration 79/1000 | Loss: 0.00002991
Iteration 80/1000 | Loss: 0.00002991
Iteration 81/1000 | Loss: 0.00002991
Iteration 82/1000 | Loss: 0.00002991
Iteration 83/1000 | Loss: 0.00002991
Iteration 84/1000 | Loss: 0.00002990
Iteration 85/1000 | Loss: 0.00002990
Iteration 86/1000 | Loss: 0.00002990
Iteration 87/1000 | Loss: 0.00002990
Iteration 88/1000 | Loss: 0.00002990
Iteration 89/1000 | Loss: 0.00002990
Iteration 90/1000 | Loss: 0.00002989
Iteration 91/1000 | Loss: 0.00002989
Iteration 92/1000 | Loss: 0.00002989
Iteration 93/1000 | Loss: 0.00002989
Iteration 94/1000 | Loss: 0.00002989
Iteration 95/1000 | Loss: 0.00002989
Iteration 96/1000 | Loss: 0.00002989
Iteration 97/1000 | Loss: 0.00002989
Iteration 98/1000 | Loss: 0.00002989
Iteration 99/1000 | Loss: 0.00002989
Iteration 100/1000 | Loss: 0.00002989
Iteration 101/1000 | Loss: 0.00002989
Iteration 102/1000 | Loss: 0.00002989
Iteration 103/1000 | Loss: 0.00002989
Iteration 104/1000 | Loss: 0.00002989
Iteration 105/1000 | Loss: 0.00002988
Iteration 106/1000 | Loss: 0.00002988
Iteration 107/1000 | Loss: 0.00002988
Iteration 108/1000 | Loss: 0.00002988
Iteration 109/1000 | Loss: 0.00002988
Iteration 110/1000 | Loss: 0.00002988
Iteration 111/1000 | Loss: 0.00002988
Iteration 112/1000 | Loss: 0.00002988
Iteration 113/1000 | Loss: 0.00002988
Iteration 114/1000 | Loss: 0.00002988
Iteration 115/1000 | Loss: 0.00002988
Iteration 116/1000 | Loss: 0.00002988
Iteration 117/1000 | Loss: 0.00002987
Iteration 118/1000 | Loss: 0.00002987
Iteration 119/1000 | Loss: 0.00002987
Iteration 120/1000 | Loss: 0.00002987
Iteration 121/1000 | Loss: 0.00002987
Iteration 122/1000 | Loss: 0.00002987
Iteration 123/1000 | Loss: 0.00002987
Iteration 124/1000 | Loss: 0.00002987
Iteration 125/1000 | Loss: 0.00002987
Iteration 126/1000 | Loss: 0.00002986
Iteration 127/1000 | Loss: 0.00002986
Iteration 128/1000 | Loss: 0.00002986
Iteration 129/1000 | Loss: 0.00002986
Iteration 130/1000 | Loss: 0.00002986
Iteration 131/1000 | Loss: 0.00002985
Iteration 132/1000 | Loss: 0.00002985
Iteration 133/1000 | Loss: 0.00002985
Iteration 134/1000 | Loss: 0.00002985
Iteration 135/1000 | Loss: 0.00002985
Iteration 136/1000 | Loss: 0.00002985
Iteration 137/1000 | Loss: 0.00002985
Iteration 138/1000 | Loss: 0.00002985
Iteration 139/1000 | Loss: 0.00002985
Iteration 140/1000 | Loss: 0.00002985
Iteration 141/1000 | Loss: 0.00002985
Iteration 142/1000 | Loss: 0.00002985
Iteration 143/1000 | Loss: 0.00002985
Iteration 144/1000 | Loss: 0.00002985
Iteration 145/1000 | Loss: 0.00002985
Iteration 146/1000 | Loss: 0.00002985
Iteration 147/1000 | Loss: 0.00002985
Iteration 148/1000 | Loss: 0.00002985
Iteration 149/1000 | Loss: 0.00002985
Iteration 150/1000 | Loss: 0.00002985
Iteration 151/1000 | Loss: 0.00002985
Iteration 152/1000 | Loss: 0.00002984
Iteration 153/1000 | Loss: 0.00002984
Iteration 154/1000 | Loss: 0.00002984
Iteration 155/1000 | Loss: 0.00002984
Iteration 156/1000 | Loss: 0.00002984
Iteration 157/1000 | Loss: 0.00002984
Iteration 158/1000 | Loss: 0.00002984
Iteration 159/1000 | Loss: 0.00002984
Iteration 160/1000 | Loss: 0.00002984
Iteration 161/1000 | Loss: 0.00002984
Iteration 162/1000 | Loss: 0.00002984
Iteration 163/1000 | Loss: 0.00002984
Iteration 164/1000 | Loss: 0.00002984
Iteration 165/1000 | Loss: 0.00002984
Iteration 166/1000 | Loss: 0.00002984
Iteration 167/1000 | Loss: 0.00002984
Iteration 168/1000 | Loss: 0.00002984
Iteration 169/1000 | Loss: 0.00002984
Iteration 170/1000 | Loss: 0.00002984
Iteration 171/1000 | Loss: 0.00002984
Iteration 172/1000 | Loss: 0.00002984
Iteration 173/1000 | Loss: 0.00002983
Iteration 174/1000 | Loss: 0.00002983
Iteration 175/1000 | Loss: 0.00002983
Iteration 176/1000 | Loss: 0.00002983
Iteration 177/1000 | Loss: 0.00002983
Iteration 178/1000 | Loss: 0.00002983
Iteration 179/1000 | Loss: 0.00002983
Iteration 180/1000 | Loss: 0.00002983
Iteration 181/1000 | Loss: 0.00002983
Iteration 182/1000 | Loss: 0.00002983
Iteration 183/1000 | Loss: 0.00002983
Iteration 184/1000 | Loss: 0.00002983
Iteration 185/1000 | Loss: 0.00002983
Iteration 186/1000 | Loss: 0.00002983
Iteration 187/1000 | Loss: 0.00002983
Iteration 188/1000 | Loss: 0.00002983
Iteration 189/1000 | Loss: 0.00002983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.9833250664523803e-05, 2.9833250664523803e-05, 2.9833250664523803e-05, 2.9833250664523803e-05, 2.9833250664523803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9833250664523803e-05

Optimization complete. Final v2v error: 4.4079508781433105 mm

Highest mean error: 5.4654083251953125 mm for frame 224

Lowest mean error: 3.8998560905456543 mm for frame 1

Saving results

Total time: 117.09400796890259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812076
Iteration 2/25 | Loss: 0.00157565
Iteration 3/25 | Loss: 0.00136717
Iteration 4/25 | Loss: 0.00134771
Iteration 5/25 | Loss: 0.00134535
Iteration 6/25 | Loss: 0.00134535
Iteration 7/25 | Loss: 0.00134535
Iteration 8/25 | Loss: 0.00134535
Iteration 9/25 | Loss: 0.00134535
Iteration 10/25 | Loss: 0.00134535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013453519204631448, 0.0013453519204631448, 0.0013453519204631448, 0.0013453519204631448, 0.0013453519204631448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013453519204631448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87620014
Iteration 2/25 | Loss: 0.00127861
Iteration 3/25 | Loss: 0.00127860
Iteration 4/25 | Loss: 0.00127860
Iteration 5/25 | Loss: 0.00127860
Iteration 6/25 | Loss: 0.00127860
Iteration 7/25 | Loss: 0.00127860
Iteration 8/25 | Loss: 0.00127860
Iteration 9/25 | Loss: 0.00127860
Iteration 10/25 | Loss: 0.00127860
Iteration 11/25 | Loss: 0.00127860
Iteration 12/25 | Loss: 0.00127860
Iteration 13/25 | Loss: 0.00127860
Iteration 14/25 | Loss: 0.00127860
Iteration 15/25 | Loss: 0.00127860
Iteration 16/25 | Loss: 0.00127860
Iteration 17/25 | Loss: 0.00127860
Iteration 18/25 | Loss: 0.00127860
Iteration 19/25 | Loss: 0.00127860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012785954168066382, 0.0012785954168066382, 0.0012785954168066382, 0.0012785954168066382, 0.0012785954168066382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012785954168066382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127860
Iteration 2/1000 | Loss: 0.00003031
Iteration 3/1000 | Loss: 0.00002400
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001960
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001857
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001776
Iteration 19/1000 | Loss: 0.00001772
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001772
Iteration 23/1000 | Loss: 0.00001772
Iteration 24/1000 | Loss: 0.00001771
Iteration 25/1000 | Loss: 0.00001771
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001757
Iteration 37/1000 | Loss: 0.00001757
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001755
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001753
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001748
Iteration 57/1000 | Loss: 0.00001746
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001742
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001740
Iteration 75/1000 | Loss: 0.00001739
Iteration 76/1000 | Loss: 0.00001739
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001737
Iteration 83/1000 | Loss: 0.00001737
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001736
Iteration 87/1000 | Loss: 0.00001736
Iteration 88/1000 | Loss: 0.00001736
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001736
Iteration 92/1000 | Loss: 0.00001736
Iteration 93/1000 | Loss: 0.00001736
Iteration 94/1000 | Loss: 0.00001736
Iteration 95/1000 | Loss: 0.00001735
Iteration 96/1000 | Loss: 0.00001735
Iteration 97/1000 | Loss: 0.00001735
Iteration 98/1000 | Loss: 0.00001735
Iteration 99/1000 | Loss: 0.00001735
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001735
Iteration 106/1000 | Loss: 0.00001735
Iteration 107/1000 | Loss: 0.00001735
Iteration 108/1000 | Loss: 0.00001735
Iteration 109/1000 | Loss: 0.00001735
Iteration 110/1000 | Loss: 0.00001735
Iteration 111/1000 | Loss: 0.00001735
Iteration 112/1000 | Loss: 0.00001735
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001735
Iteration 118/1000 | Loss: 0.00001735
Iteration 119/1000 | Loss: 0.00001735
Iteration 120/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.734642864903435e-05, 1.734642864903435e-05, 1.734642864903435e-05, 1.734642864903435e-05, 1.734642864903435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.734642864903435e-05

Optimization complete. Final v2v error: 3.540353298187256 mm

Highest mean error: 3.9011785984039307 mm for frame 26

Lowest mean error: 3.3580873012542725 mm for frame 140

Saving results

Total time: 36.80906629562378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003173
Iteration 2/25 | Loss: 0.00174188
Iteration 3/25 | Loss: 0.00148783
Iteration 4/25 | Loss: 0.00146517
Iteration 5/25 | Loss: 0.00145787
Iteration 6/25 | Loss: 0.00145690
Iteration 7/25 | Loss: 0.00145690
Iteration 8/25 | Loss: 0.00145690
Iteration 9/25 | Loss: 0.00145690
Iteration 10/25 | Loss: 0.00145690
Iteration 11/25 | Loss: 0.00145690
Iteration 12/25 | Loss: 0.00145690
Iteration 13/25 | Loss: 0.00145690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014568985207006335, 0.0014568985207006335, 0.0014568985207006335, 0.0014568985207006335, 0.0014568985207006335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014568985207006335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88478112
Iteration 2/25 | Loss: 0.00225261
Iteration 3/25 | Loss: 0.00225259
Iteration 4/25 | Loss: 0.00225259
Iteration 5/25 | Loss: 0.00225259
Iteration 6/25 | Loss: 0.00225259
Iteration 7/25 | Loss: 0.00225259
Iteration 8/25 | Loss: 0.00225258
Iteration 9/25 | Loss: 0.00225258
Iteration 10/25 | Loss: 0.00225258
Iteration 11/25 | Loss: 0.00225258
Iteration 12/25 | Loss: 0.00225258
Iteration 13/25 | Loss: 0.00225258
Iteration 14/25 | Loss: 0.00225258
Iteration 15/25 | Loss: 0.00225258
Iteration 16/25 | Loss: 0.00225258
Iteration 17/25 | Loss: 0.00225258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022525836247950792, 0.0022525836247950792, 0.0022525836247950792, 0.0022525836247950792, 0.0022525836247950792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022525836247950792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225258
Iteration 2/1000 | Loss: 0.00007298
Iteration 3/1000 | Loss: 0.00004314
Iteration 4/1000 | Loss: 0.00003243
Iteration 5/1000 | Loss: 0.00003035
Iteration 6/1000 | Loss: 0.00002913
Iteration 7/1000 | Loss: 0.00002824
Iteration 8/1000 | Loss: 0.00002749
Iteration 9/1000 | Loss: 0.00002697
Iteration 10/1000 | Loss: 0.00002641
Iteration 11/1000 | Loss: 0.00002607
Iteration 12/1000 | Loss: 0.00002584
Iteration 13/1000 | Loss: 0.00002561
Iteration 14/1000 | Loss: 0.00002537
Iteration 15/1000 | Loss: 0.00002519
Iteration 16/1000 | Loss: 0.00002505
Iteration 17/1000 | Loss: 0.00002504
Iteration 18/1000 | Loss: 0.00002488
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002470
Iteration 21/1000 | Loss: 0.00002469
Iteration 22/1000 | Loss: 0.00002461
Iteration 23/1000 | Loss: 0.00002458
Iteration 24/1000 | Loss: 0.00002458
Iteration 25/1000 | Loss: 0.00002457
Iteration 26/1000 | Loss: 0.00002457
Iteration 27/1000 | Loss: 0.00002455
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002453
Iteration 30/1000 | Loss: 0.00002453
Iteration 31/1000 | Loss: 0.00002452
Iteration 32/1000 | Loss: 0.00002452
Iteration 33/1000 | Loss: 0.00002450
Iteration 34/1000 | Loss: 0.00002449
Iteration 35/1000 | Loss: 0.00002447
Iteration 36/1000 | Loss: 0.00002446
Iteration 37/1000 | Loss: 0.00002446
Iteration 38/1000 | Loss: 0.00002445
Iteration 39/1000 | Loss: 0.00002445
Iteration 40/1000 | Loss: 0.00002444
Iteration 41/1000 | Loss: 0.00002444
Iteration 42/1000 | Loss: 0.00002444
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002443
Iteration 45/1000 | Loss: 0.00002443
Iteration 46/1000 | Loss: 0.00002442
Iteration 47/1000 | Loss: 0.00002442
Iteration 48/1000 | Loss: 0.00002441
Iteration 49/1000 | Loss: 0.00002441
Iteration 50/1000 | Loss: 0.00002440
Iteration 51/1000 | Loss: 0.00002440
Iteration 52/1000 | Loss: 0.00002439
Iteration 53/1000 | Loss: 0.00002439
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002438
Iteration 56/1000 | Loss: 0.00002438
Iteration 57/1000 | Loss: 0.00002438
Iteration 58/1000 | Loss: 0.00002437
Iteration 59/1000 | Loss: 0.00002437
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002435
Iteration 62/1000 | Loss: 0.00002435
Iteration 63/1000 | Loss: 0.00002435
Iteration 64/1000 | Loss: 0.00002434
Iteration 65/1000 | Loss: 0.00002434
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002434
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002433
Iteration 71/1000 | Loss: 0.00002433
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002432
Iteration 74/1000 | Loss: 0.00002432
Iteration 75/1000 | Loss: 0.00002432
Iteration 76/1000 | Loss: 0.00002432
Iteration 77/1000 | Loss: 0.00002432
Iteration 78/1000 | Loss: 0.00002432
Iteration 79/1000 | Loss: 0.00002432
Iteration 80/1000 | Loss: 0.00002431
Iteration 81/1000 | Loss: 0.00002431
Iteration 82/1000 | Loss: 0.00002431
Iteration 83/1000 | Loss: 0.00002431
Iteration 84/1000 | Loss: 0.00002431
Iteration 85/1000 | Loss: 0.00002431
Iteration 86/1000 | Loss: 0.00002431
Iteration 87/1000 | Loss: 0.00002430
Iteration 88/1000 | Loss: 0.00002430
Iteration 89/1000 | Loss: 0.00002430
Iteration 90/1000 | Loss: 0.00002430
Iteration 91/1000 | Loss: 0.00002429
Iteration 92/1000 | Loss: 0.00002429
Iteration 93/1000 | Loss: 0.00002429
Iteration 94/1000 | Loss: 0.00002429
Iteration 95/1000 | Loss: 0.00002428
Iteration 96/1000 | Loss: 0.00002428
Iteration 97/1000 | Loss: 0.00002428
Iteration 98/1000 | Loss: 0.00002428
Iteration 99/1000 | Loss: 0.00002428
Iteration 100/1000 | Loss: 0.00002428
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002427
Iteration 104/1000 | Loss: 0.00002427
Iteration 105/1000 | Loss: 0.00002427
Iteration 106/1000 | Loss: 0.00002427
Iteration 107/1000 | Loss: 0.00002426
Iteration 108/1000 | Loss: 0.00002426
Iteration 109/1000 | Loss: 0.00002426
Iteration 110/1000 | Loss: 0.00002426
Iteration 111/1000 | Loss: 0.00002426
Iteration 112/1000 | Loss: 0.00002426
Iteration 113/1000 | Loss: 0.00002426
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002426
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002425
Iteration 121/1000 | Loss: 0.00002425
Iteration 122/1000 | Loss: 0.00002425
Iteration 123/1000 | Loss: 0.00002425
Iteration 124/1000 | Loss: 0.00002425
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.4249327907455154e-05, 2.4249327907455154e-05, 2.4249327907455154e-05, 2.4249327907455154e-05, 2.4249327907455154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4249327907455154e-05

Optimization complete. Final v2v error: 4.1304097175598145 mm

Highest mean error: 4.858774185180664 mm for frame 65

Lowest mean error: 3.45613169670105 mm for frame 27

Saving results

Total time: 46.92560124397278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985712
Iteration 2/25 | Loss: 0.00433939
Iteration 3/25 | Loss: 0.00291598
Iteration 4/25 | Loss: 0.00217532
Iteration 5/25 | Loss: 0.00208863
Iteration 6/25 | Loss: 0.00184933
Iteration 7/25 | Loss: 0.00155374
Iteration 8/25 | Loss: 0.00140141
Iteration 9/25 | Loss: 0.00134775
Iteration 10/25 | Loss: 0.00133813
Iteration 11/25 | Loss: 0.00132196
Iteration 12/25 | Loss: 0.00132156
Iteration 13/25 | Loss: 0.00131421
Iteration 14/25 | Loss: 0.00131283
Iteration 15/25 | Loss: 0.00131250
Iteration 16/25 | Loss: 0.00131238
Iteration 17/25 | Loss: 0.00131234
Iteration 18/25 | Loss: 0.00131234
Iteration 19/25 | Loss: 0.00131234
Iteration 20/25 | Loss: 0.00131234
Iteration 21/25 | Loss: 0.00131233
Iteration 22/25 | Loss: 0.00131233
Iteration 23/25 | Loss: 0.00131233
Iteration 24/25 | Loss: 0.00131233
Iteration 25/25 | Loss: 0.00131233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24244261
Iteration 2/25 | Loss: 0.00194605
Iteration 3/25 | Loss: 0.00194605
Iteration 4/25 | Loss: 0.00194605
Iteration 5/25 | Loss: 0.00194605
Iteration 6/25 | Loss: 0.00194605
Iteration 7/25 | Loss: 0.00194605
Iteration 8/25 | Loss: 0.00194605
Iteration 9/25 | Loss: 0.00194605
Iteration 10/25 | Loss: 0.00194605
Iteration 11/25 | Loss: 0.00194605
Iteration 12/25 | Loss: 0.00194605
Iteration 13/25 | Loss: 0.00194605
Iteration 14/25 | Loss: 0.00194605
Iteration 15/25 | Loss: 0.00194605
Iteration 16/25 | Loss: 0.00194605
Iteration 17/25 | Loss: 0.00194605
Iteration 18/25 | Loss: 0.00194605
Iteration 19/25 | Loss: 0.00194605
Iteration 20/25 | Loss: 0.00194605
Iteration 21/25 | Loss: 0.00194605
Iteration 22/25 | Loss: 0.00194605
Iteration 23/25 | Loss: 0.00194605
Iteration 24/25 | Loss: 0.00194605
Iteration 25/25 | Loss: 0.00194605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194605
Iteration 2/1000 | Loss: 0.00002997
Iteration 3/1000 | Loss: 0.00002255
Iteration 4/1000 | Loss: 0.00002026
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001820
Iteration 8/1000 | Loss: 0.00001739
Iteration 9/1000 | Loss: 0.00001701
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00115017
Iteration 13/1000 | Loss: 0.00005465
Iteration 14/1000 | Loss: 0.00162538
Iteration 15/1000 | Loss: 0.00092612
Iteration 16/1000 | Loss: 0.00129309
Iteration 17/1000 | Loss: 0.00076138
Iteration 18/1000 | Loss: 0.00107101
Iteration 19/1000 | Loss: 0.00073568
Iteration 20/1000 | Loss: 0.00002072
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001105
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001088
Iteration 39/1000 | Loss: 0.00001087
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001085
Iteration 42/1000 | Loss: 0.00001084
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001084
Iteration 45/1000 | Loss: 0.00001083
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001082
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001079
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001079
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001079
Iteration 65/1000 | Loss: 0.00001078
Iteration 66/1000 | Loss: 0.00001078
Iteration 67/1000 | Loss: 0.00001078
Iteration 68/1000 | Loss: 0.00001078
Iteration 69/1000 | Loss: 0.00001078
Iteration 70/1000 | Loss: 0.00001077
Iteration 71/1000 | Loss: 0.00001077
Iteration 72/1000 | Loss: 0.00001077
Iteration 73/1000 | Loss: 0.00001077
Iteration 74/1000 | Loss: 0.00001077
Iteration 75/1000 | Loss: 0.00001077
Iteration 76/1000 | Loss: 0.00001077
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.0770459994091652e-05, 1.0770459994091652e-05, 1.0770459994091652e-05, 1.0770459994091652e-05, 1.0770459994091652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0770459994091652e-05

Optimization complete. Final v2v error: 2.879063606262207 mm

Highest mean error: 3.782615900039673 mm for frame 138

Lowest mean error: 2.7878952026367188 mm for frame 142

Saving results

Total time: 72.1930525302887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028060
Iteration 2/25 | Loss: 0.00211880
Iteration 3/25 | Loss: 0.00160974
Iteration 4/25 | Loss: 0.00151622
Iteration 5/25 | Loss: 0.00151028
Iteration 6/25 | Loss: 0.00150276
Iteration 7/25 | Loss: 0.00148469
Iteration 8/25 | Loss: 0.00146087
Iteration 9/25 | Loss: 0.00144995
Iteration 10/25 | Loss: 0.00144664
Iteration 11/25 | Loss: 0.00144648
Iteration 12/25 | Loss: 0.00144647
Iteration 13/25 | Loss: 0.00144646
Iteration 14/25 | Loss: 0.00144646
Iteration 15/25 | Loss: 0.00144646
Iteration 16/25 | Loss: 0.00144646
Iteration 17/25 | Loss: 0.00144646
Iteration 18/25 | Loss: 0.00144646
Iteration 19/25 | Loss: 0.00144646
Iteration 20/25 | Loss: 0.00144646
Iteration 21/25 | Loss: 0.00144646
Iteration 22/25 | Loss: 0.00144646
Iteration 23/25 | Loss: 0.00144645
Iteration 24/25 | Loss: 0.00144645
Iteration 25/25 | Loss: 0.00144645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24901700
Iteration 2/25 | Loss: 0.00229522
Iteration 3/25 | Loss: 0.00220034
Iteration 4/25 | Loss: 0.00220034
Iteration 5/25 | Loss: 0.00220033
Iteration 6/25 | Loss: 0.00220033
Iteration 7/25 | Loss: 0.00220033
Iteration 8/25 | Loss: 0.00220033
Iteration 9/25 | Loss: 0.00220033
Iteration 10/25 | Loss: 0.00220033
Iteration 11/25 | Loss: 0.00220033
Iteration 12/25 | Loss: 0.00220033
Iteration 13/25 | Loss: 0.00220033
Iteration 14/25 | Loss: 0.00220033
Iteration 15/25 | Loss: 0.00220033
Iteration 16/25 | Loss: 0.00220033
Iteration 17/25 | Loss: 0.00220033
Iteration 18/25 | Loss: 0.00220033
Iteration 19/25 | Loss: 0.00220033
Iteration 20/25 | Loss: 0.00220033
Iteration 21/25 | Loss: 0.00220033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002200330374762416, 0.002200330374762416, 0.002200330374762416, 0.002200330374762416, 0.002200330374762416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002200330374762416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220033
Iteration 2/1000 | Loss: 0.00028553
Iteration 3/1000 | Loss: 0.00003647
Iteration 4/1000 | Loss: 0.00012845
Iteration 5/1000 | Loss: 0.00013478
Iteration 6/1000 | Loss: 0.00007450
Iteration 7/1000 | Loss: 0.00012845
Iteration 8/1000 | Loss: 0.00069764
Iteration 9/1000 | Loss: 0.00004156
Iteration 10/1000 | Loss: 0.00011492
Iteration 11/1000 | Loss: 0.00004006
Iteration 12/1000 | Loss: 0.00005021
Iteration 13/1000 | Loss: 0.00002919
Iteration 14/1000 | Loss: 0.00009584
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00002834
Iteration 17/1000 | Loss: 0.00010238
Iteration 18/1000 | Loss: 0.00002773
Iteration 19/1000 | Loss: 0.00002738
Iteration 20/1000 | Loss: 0.00002708
Iteration 21/1000 | Loss: 0.00002697
Iteration 22/1000 | Loss: 0.00002677
Iteration 23/1000 | Loss: 0.00005628
Iteration 24/1000 | Loss: 0.00002665
Iteration 25/1000 | Loss: 0.00005109
Iteration 26/1000 | Loss: 0.00019011
Iteration 27/1000 | Loss: 0.00007808
Iteration 28/1000 | Loss: 0.00005624
Iteration 29/1000 | Loss: 0.00005435
Iteration 30/1000 | Loss: 0.00002648
Iteration 31/1000 | Loss: 0.00002641
Iteration 32/1000 | Loss: 0.00002640
Iteration 33/1000 | Loss: 0.00003876
Iteration 34/1000 | Loss: 0.00004043
Iteration 35/1000 | Loss: 0.00003700
Iteration 36/1000 | Loss: 0.00005799
Iteration 37/1000 | Loss: 0.00006469
Iteration 38/1000 | Loss: 0.00002903
Iteration 39/1000 | Loss: 0.00002637
Iteration 40/1000 | Loss: 0.00002635
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002629
Iteration 43/1000 | Loss: 0.00002629
Iteration 44/1000 | Loss: 0.00002628
Iteration 45/1000 | Loss: 0.00002628
Iteration 46/1000 | Loss: 0.00002628
Iteration 47/1000 | Loss: 0.00002627
Iteration 48/1000 | Loss: 0.00002627
Iteration 49/1000 | Loss: 0.00002627
Iteration 50/1000 | Loss: 0.00002627
Iteration 51/1000 | Loss: 0.00002627
Iteration 52/1000 | Loss: 0.00002627
Iteration 53/1000 | Loss: 0.00002627
Iteration 54/1000 | Loss: 0.00002626
Iteration 55/1000 | Loss: 0.00002626
Iteration 56/1000 | Loss: 0.00002626
Iteration 57/1000 | Loss: 0.00002626
Iteration 58/1000 | Loss: 0.00002625
Iteration 59/1000 | Loss: 0.00002625
Iteration 60/1000 | Loss: 0.00002625
Iteration 61/1000 | Loss: 0.00002625
Iteration 62/1000 | Loss: 0.00002625
Iteration 63/1000 | Loss: 0.00002625
Iteration 64/1000 | Loss: 0.00002625
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002625
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002625
Iteration 70/1000 | Loss: 0.00002624
Iteration 71/1000 | Loss: 0.00002624
Iteration 72/1000 | Loss: 0.00002624
Iteration 73/1000 | Loss: 0.00002624
Iteration 74/1000 | Loss: 0.00002624
Iteration 75/1000 | Loss: 0.00008258
Iteration 76/1000 | Loss: 0.00002631
Iteration 77/1000 | Loss: 0.00002630
Iteration 78/1000 | Loss: 0.00002626
Iteration 79/1000 | Loss: 0.00002626
Iteration 80/1000 | Loss: 0.00002625
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002624
Iteration 83/1000 | Loss: 0.00002624
Iteration 84/1000 | Loss: 0.00002623
Iteration 85/1000 | Loss: 0.00002623
Iteration 86/1000 | Loss: 0.00002622
Iteration 87/1000 | Loss: 0.00002622
Iteration 88/1000 | Loss: 0.00002622
Iteration 89/1000 | Loss: 0.00002622
Iteration 90/1000 | Loss: 0.00002622
Iteration 91/1000 | Loss: 0.00002621
Iteration 92/1000 | Loss: 0.00004365
Iteration 93/1000 | Loss: 0.00002625
Iteration 94/1000 | Loss: 0.00004024
Iteration 95/1000 | Loss: 0.00002621
Iteration 96/1000 | Loss: 0.00002621
Iteration 97/1000 | Loss: 0.00002620
Iteration 98/1000 | Loss: 0.00002620
Iteration 99/1000 | Loss: 0.00002620
Iteration 100/1000 | Loss: 0.00002620
Iteration 101/1000 | Loss: 0.00002620
Iteration 102/1000 | Loss: 0.00002620
Iteration 103/1000 | Loss: 0.00002620
Iteration 104/1000 | Loss: 0.00002620
Iteration 105/1000 | Loss: 0.00002620
Iteration 106/1000 | Loss: 0.00002620
Iteration 107/1000 | Loss: 0.00002620
Iteration 108/1000 | Loss: 0.00002620
Iteration 109/1000 | Loss: 0.00002620
Iteration 110/1000 | Loss: 0.00002620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.6202513254247606e-05, 2.6202513254247606e-05, 2.6202513254247606e-05, 2.6202513254247606e-05, 2.6202513254247606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6202513254247606e-05

Optimization complete. Final v2v error: 4.25909423828125 mm

Highest mean error: 4.952066898345947 mm for frame 45

Lowest mean error: 3.9380788803100586 mm for frame 130

Saving results

Total time: 76.0221779346466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599035
Iteration 2/25 | Loss: 0.00173159
Iteration 3/25 | Loss: 0.00145951
Iteration 4/25 | Loss: 0.00144867
Iteration 5/25 | Loss: 0.00144539
Iteration 6/25 | Loss: 0.00144464
Iteration 7/25 | Loss: 0.00144464
Iteration 8/25 | Loss: 0.00144464
Iteration 9/25 | Loss: 0.00144464
Iteration 10/25 | Loss: 0.00144464
Iteration 11/25 | Loss: 0.00144464
Iteration 12/25 | Loss: 0.00144464
Iteration 13/25 | Loss: 0.00144464
Iteration 14/25 | Loss: 0.00144464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014446397544816136, 0.0014446397544816136, 0.0014446397544816136, 0.0014446397544816136, 0.0014446397544816136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014446397544816136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99584866
Iteration 2/25 | Loss: 0.00173351
Iteration 3/25 | Loss: 0.00173350
Iteration 4/25 | Loss: 0.00173350
Iteration 5/25 | Loss: 0.00173350
Iteration 6/25 | Loss: 0.00173350
Iteration 7/25 | Loss: 0.00173350
Iteration 8/25 | Loss: 0.00173350
Iteration 9/25 | Loss: 0.00173350
Iteration 10/25 | Loss: 0.00173350
Iteration 11/25 | Loss: 0.00173350
Iteration 12/25 | Loss: 0.00173349
Iteration 13/25 | Loss: 0.00173349
Iteration 14/25 | Loss: 0.00173349
Iteration 15/25 | Loss: 0.00173349
Iteration 16/25 | Loss: 0.00173349
Iteration 17/25 | Loss: 0.00173349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017334948061034083, 0.0017334948061034083, 0.0017334948061034083, 0.0017334948061034083, 0.0017334948061034083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017334948061034083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173349
Iteration 2/1000 | Loss: 0.00008039
Iteration 3/1000 | Loss: 0.00005139
Iteration 4/1000 | Loss: 0.00004048
Iteration 5/1000 | Loss: 0.00003767
Iteration 6/1000 | Loss: 0.00003633
Iteration 7/1000 | Loss: 0.00003576
Iteration 8/1000 | Loss: 0.00003500
Iteration 9/1000 | Loss: 0.00003416
Iteration 10/1000 | Loss: 0.00003367
Iteration 11/1000 | Loss: 0.00003324
Iteration 12/1000 | Loss: 0.00003283
Iteration 13/1000 | Loss: 0.00003245
Iteration 14/1000 | Loss: 0.00003213
Iteration 15/1000 | Loss: 0.00003187
Iteration 16/1000 | Loss: 0.00003162
Iteration 17/1000 | Loss: 0.00003144
Iteration 18/1000 | Loss: 0.00003127
Iteration 19/1000 | Loss: 0.00003108
Iteration 20/1000 | Loss: 0.00003094
Iteration 21/1000 | Loss: 0.00003084
Iteration 22/1000 | Loss: 0.00003077
Iteration 23/1000 | Loss: 0.00003075
Iteration 24/1000 | Loss: 0.00003074
Iteration 25/1000 | Loss: 0.00003074
Iteration 26/1000 | Loss: 0.00003068
Iteration 27/1000 | Loss: 0.00003068
Iteration 28/1000 | Loss: 0.00003065
Iteration 29/1000 | Loss: 0.00003064
Iteration 30/1000 | Loss: 0.00003064
Iteration 31/1000 | Loss: 0.00003064
Iteration 32/1000 | Loss: 0.00003064
Iteration 33/1000 | Loss: 0.00003063
Iteration 34/1000 | Loss: 0.00003063
Iteration 35/1000 | Loss: 0.00003063
Iteration 36/1000 | Loss: 0.00003063
Iteration 37/1000 | Loss: 0.00003063
Iteration 38/1000 | Loss: 0.00003063
Iteration 39/1000 | Loss: 0.00003063
Iteration 40/1000 | Loss: 0.00003063
Iteration 41/1000 | Loss: 0.00003063
Iteration 42/1000 | Loss: 0.00003063
Iteration 43/1000 | Loss: 0.00003063
Iteration 44/1000 | Loss: 0.00003062
Iteration 45/1000 | Loss: 0.00003062
Iteration 46/1000 | Loss: 0.00003062
Iteration 47/1000 | Loss: 0.00003062
Iteration 48/1000 | Loss: 0.00003062
Iteration 49/1000 | Loss: 0.00003062
Iteration 50/1000 | Loss: 0.00003062
Iteration 51/1000 | Loss: 0.00003062
Iteration 52/1000 | Loss: 0.00003062
Iteration 53/1000 | Loss: 0.00003062
Iteration 54/1000 | Loss: 0.00003062
Iteration 55/1000 | Loss: 0.00003060
Iteration 56/1000 | Loss: 0.00003060
Iteration 57/1000 | Loss: 0.00003060
Iteration 58/1000 | Loss: 0.00003060
Iteration 59/1000 | Loss: 0.00003060
Iteration 60/1000 | Loss: 0.00003060
Iteration 61/1000 | Loss: 0.00003060
Iteration 62/1000 | Loss: 0.00003060
Iteration 63/1000 | Loss: 0.00003060
Iteration 64/1000 | Loss: 0.00003060
Iteration 65/1000 | Loss: 0.00003060
Iteration 66/1000 | Loss: 0.00003060
Iteration 67/1000 | Loss: 0.00003059
Iteration 68/1000 | Loss: 0.00003058
Iteration 69/1000 | Loss: 0.00003058
Iteration 70/1000 | Loss: 0.00003058
Iteration 71/1000 | Loss: 0.00003058
Iteration 72/1000 | Loss: 0.00003057
Iteration 73/1000 | Loss: 0.00003057
Iteration 74/1000 | Loss: 0.00003057
Iteration 75/1000 | Loss: 0.00003057
Iteration 76/1000 | Loss: 0.00003057
Iteration 77/1000 | Loss: 0.00003057
Iteration 78/1000 | Loss: 0.00003057
Iteration 79/1000 | Loss: 0.00003057
Iteration 80/1000 | Loss: 0.00003057
Iteration 81/1000 | Loss: 0.00003057
Iteration 82/1000 | Loss: 0.00003057
Iteration 83/1000 | Loss: 0.00003057
Iteration 84/1000 | Loss: 0.00003057
Iteration 85/1000 | Loss: 0.00003057
Iteration 86/1000 | Loss: 0.00003057
Iteration 87/1000 | Loss: 0.00003057
Iteration 88/1000 | Loss: 0.00003057
Iteration 89/1000 | Loss: 0.00003057
Iteration 90/1000 | Loss: 0.00003057
Iteration 91/1000 | Loss: 0.00003057
Iteration 92/1000 | Loss: 0.00003057
Iteration 93/1000 | Loss: 0.00003057
Iteration 94/1000 | Loss: 0.00003057
Iteration 95/1000 | Loss: 0.00003057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [3.0569448426831514e-05, 3.0569448426831514e-05, 3.0569448426831514e-05, 3.0569448426831514e-05, 3.0569448426831514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0569448426831514e-05

Optimization complete. Final v2v error: 4.229028224945068 mm

Highest mean error: 5.263830661773682 mm for frame 136

Lowest mean error: 3.2195496559143066 mm for frame 50

Saving results

Total time: 44.198089838027954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062893
Iteration 2/25 | Loss: 0.00234188
Iteration 3/25 | Loss: 0.00173699
Iteration 4/25 | Loss: 0.00163146
Iteration 5/25 | Loss: 0.00160107
Iteration 6/25 | Loss: 0.00158238
Iteration 7/25 | Loss: 0.00156325
Iteration 8/25 | Loss: 0.00155290
Iteration 9/25 | Loss: 0.00154882
Iteration 10/25 | Loss: 0.00154780
Iteration 11/25 | Loss: 0.00154648
Iteration 12/25 | Loss: 0.00154601
Iteration 13/25 | Loss: 0.00154728
Iteration 14/25 | Loss: 0.00154664
Iteration 15/25 | Loss: 0.00154573
Iteration 16/25 | Loss: 0.00154512
Iteration 17/25 | Loss: 0.00154445
Iteration 18/25 | Loss: 0.00154413
Iteration 19/25 | Loss: 0.00154411
Iteration 20/25 | Loss: 0.00154411
Iteration 21/25 | Loss: 0.00154411
Iteration 22/25 | Loss: 0.00154410
Iteration 23/25 | Loss: 0.00154410
Iteration 24/25 | Loss: 0.00154410
Iteration 25/25 | Loss: 0.00154410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57903481
Iteration 2/25 | Loss: 0.00306116
Iteration 3/25 | Loss: 0.00306116
Iteration 4/25 | Loss: 0.00306116
Iteration 5/25 | Loss: 0.00306116
Iteration 6/25 | Loss: 0.00306116
Iteration 7/25 | Loss: 0.00306116
Iteration 8/25 | Loss: 0.00306116
Iteration 9/25 | Loss: 0.00306116
Iteration 10/25 | Loss: 0.00306116
Iteration 11/25 | Loss: 0.00306116
Iteration 12/25 | Loss: 0.00306116
Iteration 13/25 | Loss: 0.00306116
Iteration 14/25 | Loss: 0.00306116
Iteration 15/25 | Loss: 0.00306116
Iteration 16/25 | Loss: 0.00306116
Iteration 17/25 | Loss: 0.00306116
Iteration 18/25 | Loss: 0.00306116
Iteration 19/25 | Loss: 0.00306116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0030611578840762377, 0.0030611578840762377, 0.0030611578840762377, 0.0030611578840762377, 0.0030611578840762377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030611578840762377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306116
Iteration 2/1000 | Loss: 0.00021329
Iteration 3/1000 | Loss: 0.00030393
Iteration 4/1000 | Loss: 0.00024467
Iteration 5/1000 | Loss: 0.00012156
Iteration 6/1000 | Loss: 0.00028197
Iteration 7/1000 | Loss: 0.00028787
Iteration 8/1000 | Loss: 0.00011252
Iteration 9/1000 | Loss: 0.00008652
Iteration 10/1000 | Loss: 0.00006316
Iteration 11/1000 | Loss: 0.00005730
Iteration 12/1000 | Loss: 0.00005279
Iteration 13/1000 | Loss: 0.00011540
Iteration 14/1000 | Loss: 0.00008354
Iteration 15/1000 | Loss: 0.00007292
Iteration 16/1000 | Loss: 0.00004812
Iteration 17/1000 | Loss: 0.00004510
Iteration 18/1000 | Loss: 0.00005723
Iteration 19/1000 | Loss: 0.00005326
Iteration 20/1000 | Loss: 0.00005647
Iteration 21/1000 | Loss: 0.00006143
Iteration 22/1000 | Loss: 0.00005641
Iteration 23/1000 | Loss: 0.00005165
Iteration 24/1000 | Loss: 0.00004251
Iteration 25/1000 | Loss: 0.00004741
Iteration 26/1000 | Loss: 0.00008604
Iteration 27/1000 | Loss: 0.00005836
Iteration 28/1000 | Loss: 0.00004266
Iteration 29/1000 | Loss: 0.00004990
Iteration 30/1000 | Loss: 0.00004380
Iteration 31/1000 | Loss: 0.00004672
Iteration 32/1000 | Loss: 0.00006059
Iteration 33/1000 | Loss: 0.00004426
Iteration 34/1000 | Loss: 0.00005162
Iteration 35/1000 | Loss: 0.00004911
Iteration 36/1000 | Loss: 0.00004970
Iteration 37/1000 | Loss: 0.00005723
Iteration 38/1000 | Loss: 0.00005501
Iteration 39/1000 | Loss: 0.00005693
Iteration 40/1000 | Loss: 0.00005504
Iteration 41/1000 | Loss: 0.00010592
Iteration 42/1000 | Loss: 0.00006382
Iteration 43/1000 | Loss: 0.00005465
Iteration 44/1000 | Loss: 0.00005107
Iteration 45/1000 | Loss: 0.00005344
Iteration 46/1000 | Loss: 0.00005189
Iteration 47/1000 | Loss: 0.00005194
Iteration 48/1000 | Loss: 0.00005600
Iteration 49/1000 | Loss: 0.00005157
Iteration 50/1000 | Loss: 0.00005229
Iteration 51/1000 | Loss: 0.00005003
Iteration 52/1000 | Loss: 0.00005631
Iteration 53/1000 | Loss: 0.00006103
Iteration 54/1000 | Loss: 0.00005133
Iteration 55/1000 | Loss: 0.00005348
Iteration 56/1000 | Loss: 0.00006508
Iteration 57/1000 | Loss: 0.00012076
Iteration 58/1000 | Loss: 0.00008993
Iteration 59/1000 | Loss: 0.00006856
Iteration 60/1000 | Loss: 0.00005992
Iteration 61/1000 | Loss: 0.00006290
Iteration 62/1000 | Loss: 0.00004196
Iteration 63/1000 | Loss: 0.00004169
Iteration 64/1000 | Loss: 0.00004475
Iteration 65/1000 | Loss: 0.00008505
Iteration 66/1000 | Loss: 0.00004973
Iteration 67/1000 | Loss: 0.00004875
Iteration 68/1000 | Loss: 0.00005224
Iteration 69/1000 | Loss: 0.00010548
Iteration 70/1000 | Loss: 0.00005124
Iteration 71/1000 | Loss: 0.00004566
Iteration 72/1000 | Loss: 0.00004760
Iteration 73/1000 | Loss: 0.00005456
Iteration 74/1000 | Loss: 0.00005654
Iteration 75/1000 | Loss: 0.00006162
Iteration 76/1000 | Loss: 0.00005908
Iteration 77/1000 | Loss: 0.00009947
Iteration 78/1000 | Loss: 0.00007927
Iteration 79/1000 | Loss: 0.00006584
Iteration 80/1000 | Loss: 0.00005361
Iteration 81/1000 | Loss: 0.00004905
Iteration 82/1000 | Loss: 0.00010024
Iteration 83/1000 | Loss: 0.00006776
Iteration 84/1000 | Loss: 0.00006479
Iteration 85/1000 | Loss: 0.00004638
Iteration 86/1000 | Loss: 0.00006013
Iteration 87/1000 | Loss: 0.00006007
Iteration 88/1000 | Loss: 0.00005219
Iteration 89/1000 | Loss: 0.00004088
Iteration 90/1000 | Loss: 0.00004483
Iteration 91/1000 | Loss: 0.00004961
Iteration 92/1000 | Loss: 0.00004460
Iteration 93/1000 | Loss: 0.00010987
Iteration 94/1000 | Loss: 0.00004419
Iteration 95/1000 | Loss: 0.00005001
Iteration 96/1000 | Loss: 0.00009709
Iteration 97/1000 | Loss: 0.00004619
Iteration 98/1000 | Loss: 0.00004456
Iteration 99/1000 | Loss: 0.00009918
Iteration 100/1000 | Loss: 0.00004207
Iteration 101/1000 | Loss: 0.00004072
Iteration 102/1000 | Loss: 0.00003968
Iteration 103/1000 | Loss: 0.00004330
Iteration 104/1000 | Loss: 0.00003868
Iteration 105/1000 | Loss: 0.00008040
Iteration 106/1000 | Loss: 0.00005125
Iteration 107/1000 | Loss: 0.00004238
Iteration 108/1000 | Loss: 0.00004029
Iteration 109/1000 | Loss: 0.00007949
Iteration 110/1000 | Loss: 0.00005180
Iteration 111/1000 | Loss: 0.00004322
Iteration 112/1000 | Loss: 0.00007538
Iteration 113/1000 | Loss: 0.00004155
Iteration 114/1000 | Loss: 0.00003856
Iteration 115/1000 | Loss: 0.00004545
Iteration 116/1000 | Loss: 0.00005580
Iteration 117/1000 | Loss: 0.00005079
Iteration 118/1000 | Loss: 0.00004412
Iteration 119/1000 | Loss: 0.00004920
Iteration 120/1000 | Loss: 0.00004147
Iteration 121/1000 | Loss: 0.00003990
Iteration 122/1000 | Loss: 0.00004515
Iteration 123/1000 | Loss: 0.00004284
Iteration 124/1000 | Loss: 0.00004440
Iteration 125/1000 | Loss: 0.00004439
Iteration 126/1000 | Loss: 0.00004439
Iteration 127/1000 | Loss: 0.00004042
Iteration 128/1000 | Loss: 0.00005321
Iteration 129/1000 | Loss: 0.00003790
Iteration 130/1000 | Loss: 0.00003739
Iteration 131/1000 | Loss: 0.00003708
Iteration 132/1000 | Loss: 0.00005278
Iteration 133/1000 | Loss: 0.00005307
Iteration 134/1000 | Loss: 0.00003719
Iteration 135/1000 | Loss: 0.00005401
Iteration 136/1000 | Loss: 0.00005537
Iteration 137/1000 | Loss: 0.00003788
Iteration 138/1000 | Loss: 0.00003709
Iteration 139/1000 | Loss: 0.00003679
Iteration 140/1000 | Loss: 0.00005442
Iteration 141/1000 | Loss: 0.00006505
Iteration 142/1000 | Loss: 0.00003858
Iteration 143/1000 | Loss: 0.00003757
Iteration 144/1000 | Loss: 0.00003733
Iteration 145/1000 | Loss: 0.00003703
Iteration 146/1000 | Loss: 0.00003674
Iteration 147/1000 | Loss: 0.00003655
Iteration 148/1000 | Loss: 0.00003633
Iteration 149/1000 | Loss: 0.00003620
Iteration 150/1000 | Loss: 0.00003616
Iteration 151/1000 | Loss: 0.00003616
Iteration 152/1000 | Loss: 0.00003616
Iteration 153/1000 | Loss: 0.00003615
Iteration 154/1000 | Loss: 0.00003615
Iteration 155/1000 | Loss: 0.00003614
Iteration 156/1000 | Loss: 0.00003614
Iteration 157/1000 | Loss: 0.00003609
Iteration 158/1000 | Loss: 0.00003608
Iteration 159/1000 | Loss: 0.00003608
Iteration 160/1000 | Loss: 0.00003607
Iteration 161/1000 | Loss: 0.00003607
Iteration 162/1000 | Loss: 0.00003607
Iteration 163/1000 | Loss: 0.00003607
Iteration 164/1000 | Loss: 0.00003607
Iteration 165/1000 | Loss: 0.00003607
Iteration 166/1000 | Loss: 0.00003607
Iteration 167/1000 | Loss: 0.00003607
Iteration 168/1000 | Loss: 0.00003607
Iteration 169/1000 | Loss: 0.00003607
Iteration 170/1000 | Loss: 0.00003607
Iteration 171/1000 | Loss: 0.00003606
Iteration 172/1000 | Loss: 0.00003606
Iteration 173/1000 | Loss: 0.00003606
Iteration 174/1000 | Loss: 0.00003605
Iteration 175/1000 | Loss: 0.00003604
Iteration 176/1000 | Loss: 0.00003603
Iteration 177/1000 | Loss: 0.00003603
Iteration 178/1000 | Loss: 0.00003603
Iteration 179/1000 | Loss: 0.00003602
Iteration 180/1000 | Loss: 0.00003602
Iteration 181/1000 | Loss: 0.00003602
Iteration 182/1000 | Loss: 0.00003602
Iteration 183/1000 | Loss: 0.00003602
Iteration 184/1000 | Loss: 0.00003602
Iteration 185/1000 | Loss: 0.00003601
Iteration 186/1000 | Loss: 0.00003601
Iteration 187/1000 | Loss: 0.00003601
Iteration 188/1000 | Loss: 0.00003601
Iteration 189/1000 | Loss: 0.00003600
Iteration 190/1000 | Loss: 0.00003600
Iteration 191/1000 | Loss: 0.00003600
Iteration 192/1000 | Loss: 0.00003600
Iteration 193/1000 | Loss: 0.00003600
Iteration 194/1000 | Loss: 0.00003599
Iteration 195/1000 | Loss: 0.00003599
Iteration 196/1000 | Loss: 0.00003599
Iteration 197/1000 | Loss: 0.00003599
Iteration 198/1000 | Loss: 0.00003599
Iteration 199/1000 | Loss: 0.00003599
Iteration 200/1000 | Loss: 0.00003599
Iteration 201/1000 | Loss: 0.00003599
Iteration 202/1000 | Loss: 0.00003598
Iteration 203/1000 | Loss: 0.00003598
Iteration 204/1000 | Loss: 0.00003598
Iteration 205/1000 | Loss: 0.00003598
Iteration 206/1000 | Loss: 0.00003598
Iteration 207/1000 | Loss: 0.00003598
Iteration 208/1000 | Loss: 0.00003598
Iteration 209/1000 | Loss: 0.00003597
Iteration 210/1000 | Loss: 0.00003597
Iteration 211/1000 | Loss: 0.00003597
Iteration 212/1000 | Loss: 0.00003597
Iteration 213/1000 | Loss: 0.00003597
Iteration 214/1000 | Loss: 0.00003597
Iteration 215/1000 | Loss: 0.00003597
Iteration 216/1000 | Loss: 0.00003597
Iteration 217/1000 | Loss: 0.00003597
Iteration 218/1000 | Loss: 0.00003597
Iteration 219/1000 | Loss: 0.00003597
Iteration 220/1000 | Loss: 0.00003597
Iteration 221/1000 | Loss: 0.00003597
Iteration 222/1000 | Loss: 0.00003597
Iteration 223/1000 | Loss: 0.00003597
Iteration 224/1000 | Loss: 0.00003597
Iteration 225/1000 | Loss: 0.00003597
Iteration 226/1000 | Loss: 0.00003597
Iteration 227/1000 | Loss: 0.00003597
Iteration 228/1000 | Loss: 0.00003597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [3.596965689212084e-05, 3.596965689212084e-05, 3.596965689212084e-05, 3.596965689212084e-05, 3.596965689212084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.596965689212084e-05

Optimization complete. Final v2v error: 4.8934783935546875 mm

Highest mean error: 7.461009502410889 mm for frame 206

Lowest mean error: 3.7714617252349854 mm for frame 21

Saving results

Total time: 274.75510716438293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881935
Iteration 2/25 | Loss: 0.00154233
Iteration 3/25 | Loss: 0.00140602
Iteration 4/25 | Loss: 0.00139320
Iteration 5/25 | Loss: 0.00138977
Iteration 6/25 | Loss: 0.00138965
Iteration 7/25 | Loss: 0.00138965
Iteration 8/25 | Loss: 0.00138965
Iteration 9/25 | Loss: 0.00138965
Iteration 10/25 | Loss: 0.00138965
Iteration 11/25 | Loss: 0.00138965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013896548189222813, 0.0013896548189222813, 0.0013896548189222813, 0.0013896548189222813, 0.0013896548189222813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013896548189222813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65135688
Iteration 2/25 | Loss: 0.00171557
Iteration 3/25 | Loss: 0.00171557
Iteration 4/25 | Loss: 0.00171557
Iteration 5/25 | Loss: 0.00171557
Iteration 6/25 | Loss: 0.00171557
Iteration 7/25 | Loss: 0.00171557
Iteration 8/25 | Loss: 0.00171557
Iteration 9/25 | Loss: 0.00171557
Iteration 10/25 | Loss: 0.00171557
Iteration 11/25 | Loss: 0.00171557
Iteration 12/25 | Loss: 0.00171557
Iteration 13/25 | Loss: 0.00171557
Iteration 14/25 | Loss: 0.00171557
Iteration 15/25 | Loss: 0.00171557
Iteration 16/25 | Loss: 0.00171557
Iteration 17/25 | Loss: 0.00171557
Iteration 18/25 | Loss: 0.00171557
Iteration 19/25 | Loss: 0.00171557
Iteration 20/25 | Loss: 0.00171557
Iteration 21/25 | Loss: 0.00171557
Iteration 22/25 | Loss: 0.00171557
Iteration 23/25 | Loss: 0.00171557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017155699897557497, 0.0017155699897557497, 0.0017155699897557497, 0.0017155699897557497, 0.0017155699897557497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017155699897557497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171557
Iteration 2/1000 | Loss: 0.00004778
Iteration 3/1000 | Loss: 0.00003221
Iteration 4/1000 | Loss: 0.00002793
Iteration 5/1000 | Loss: 0.00002639
Iteration 6/1000 | Loss: 0.00002504
Iteration 7/1000 | Loss: 0.00002424
Iteration 8/1000 | Loss: 0.00002342
Iteration 9/1000 | Loss: 0.00002293
Iteration 10/1000 | Loss: 0.00002250
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002081
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001906
Iteration 19/1000 | Loss: 0.00001889
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001869
Iteration 22/1000 | Loss: 0.00001869
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001859
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001856
Iteration 42/1000 | Loss: 0.00001855
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00001854
Iteration 46/1000 | Loss: 0.00001854
Iteration 47/1000 | Loss: 0.00001854
Iteration 48/1000 | Loss: 0.00001854
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001849
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001843
Iteration 65/1000 | Loss: 0.00001843
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001838
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001837
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001836
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001835
Iteration 74/1000 | Loss: 0.00001835
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00001833
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001832
Iteration 84/1000 | Loss: 0.00001832
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001830
Iteration 95/1000 | Loss: 0.00001830
Iteration 96/1000 | Loss: 0.00001830
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001827
Iteration 110/1000 | Loss: 0.00001827
Iteration 111/1000 | Loss: 0.00001827
Iteration 112/1000 | Loss: 0.00001827
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001827
Iteration 115/1000 | Loss: 0.00001827
Iteration 116/1000 | Loss: 0.00001827
Iteration 117/1000 | Loss: 0.00001827
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001827
Iteration 120/1000 | Loss: 0.00001827
Iteration 121/1000 | Loss: 0.00001827
Iteration 122/1000 | Loss: 0.00001827
Iteration 123/1000 | Loss: 0.00001827
Iteration 124/1000 | Loss: 0.00001827
Iteration 125/1000 | Loss: 0.00001827
Iteration 126/1000 | Loss: 0.00001827
Iteration 127/1000 | Loss: 0.00001827
Iteration 128/1000 | Loss: 0.00001827
Iteration 129/1000 | Loss: 0.00001827
Iteration 130/1000 | Loss: 0.00001827
Iteration 131/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.826590596465394e-05, 1.826590596465394e-05, 1.826590596465394e-05, 1.826590596465394e-05, 1.826590596465394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826590596465394e-05

Optimization complete. Final v2v error: 3.5496408939361572 mm

Highest mean error: 3.754011392593384 mm for frame 58

Lowest mean error: 3.3344812393188477 mm for frame 161

Saving results

Total time: 44.956401348114014
