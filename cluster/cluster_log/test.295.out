Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=295, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16520-16575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743755
Iteration 2/25 | Loss: 0.00149639
Iteration 3/25 | Loss: 0.00136522
Iteration 4/25 | Loss: 0.00135658
Iteration 5/25 | Loss: 0.00135411
Iteration 6/25 | Loss: 0.00135411
Iteration 7/25 | Loss: 0.00135411
Iteration 8/25 | Loss: 0.00135411
Iteration 9/25 | Loss: 0.00135411
Iteration 10/25 | Loss: 0.00135411
Iteration 11/25 | Loss: 0.00135411
Iteration 12/25 | Loss: 0.00135411
Iteration 13/25 | Loss: 0.00135411
Iteration 14/25 | Loss: 0.00135411
Iteration 15/25 | Loss: 0.00135411
Iteration 16/25 | Loss: 0.00135411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013541127555072308, 0.0013541127555072308, 0.0013541127555072308, 0.0013541127555072308, 0.0013541127555072308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013541127555072308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.29174232
Iteration 2/25 | Loss: 0.00081770
Iteration 3/25 | Loss: 0.00081770
Iteration 4/25 | Loss: 0.00081770
Iteration 5/25 | Loss: 0.00081769
Iteration 6/25 | Loss: 0.00081769
Iteration 7/25 | Loss: 0.00081769
Iteration 8/25 | Loss: 0.00081769
Iteration 9/25 | Loss: 0.00081769
Iteration 10/25 | Loss: 0.00081769
Iteration 11/25 | Loss: 0.00081769
Iteration 12/25 | Loss: 0.00081769
Iteration 13/25 | Loss: 0.00081769
Iteration 14/25 | Loss: 0.00081769
Iteration 15/25 | Loss: 0.00081769
Iteration 16/25 | Loss: 0.00081769
Iteration 17/25 | Loss: 0.00081769
Iteration 18/25 | Loss: 0.00081769
Iteration 19/25 | Loss: 0.00081769
Iteration 20/25 | Loss: 0.00081769
Iteration 21/25 | Loss: 0.00081769
Iteration 22/25 | Loss: 0.00081769
Iteration 23/25 | Loss: 0.00081769
Iteration 24/25 | Loss: 0.00081769
Iteration 25/25 | Loss: 0.00081769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081769
Iteration 2/1000 | Loss: 0.00007128
Iteration 3/1000 | Loss: 0.00003786
Iteration 4/1000 | Loss: 0.00003090
Iteration 5/1000 | Loss: 0.00002892
Iteration 6/1000 | Loss: 0.00002788
Iteration 7/1000 | Loss: 0.00002704
Iteration 8/1000 | Loss: 0.00002650
Iteration 9/1000 | Loss: 0.00002613
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002536
Iteration 13/1000 | Loss: 0.00002515
Iteration 14/1000 | Loss: 0.00002515
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00002505
Iteration 17/1000 | Loss: 0.00002505
Iteration 18/1000 | Loss: 0.00002498
Iteration 19/1000 | Loss: 0.00002490
Iteration 20/1000 | Loss: 0.00002486
Iteration 21/1000 | Loss: 0.00002486
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002485
Iteration 24/1000 | Loss: 0.00002484
Iteration 25/1000 | Loss: 0.00002481
Iteration 26/1000 | Loss: 0.00002480
Iteration 27/1000 | Loss: 0.00002480
Iteration 28/1000 | Loss: 0.00002479
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002478
Iteration 31/1000 | Loss: 0.00002477
Iteration 32/1000 | Loss: 0.00002476
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002475
Iteration 37/1000 | Loss: 0.00002474
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002472
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002471
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00002471
Iteration 45/1000 | Loss: 0.00002471
Iteration 46/1000 | Loss: 0.00002471
Iteration 47/1000 | Loss: 0.00002471
Iteration 48/1000 | Loss: 0.00002471
Iteration 49/1000 | Loss: 0.00002471
Iteration 50/1000 | Loss: 0.00002471
Iteration 51/1000 | Loss: 0.00002471
Iteration 52/1000 | Loss: 0.00002471
Iteration 53/1000 | Loss: 0.00002470
Iteration 54/1000 | Loss: 0.00002470
Iteration 55/1000 | Loss: 0.00002470
Iteration 56/1000 | Loss: 0.00002470
Iteration 57/1000 | Loss: 0.00002470
Iteration 58/1000 | Loss: 0.00002469
Iteration 59/1000 | Loss: 0.00002466
Iteration 60/1000 | Loss: 0.00002466
Iteration 61/1000 | Loss: 0.00002466
Iteration 62/1000 | Loss: 0.00002466
Iteration 63/1000 | Loss: 0.00002466
Iteration 64/1000 | Loss: 0.00002466
Iteration 65/1000 | Loss: 0.00002466
Iteration 66/1000 | Loss: 0.00002465
Iteration 67/1000 | Loss: 0.00002465
Iteration 68/1000 | Loss: 0.00002465
Iteration 69/1000 | Loss: 0.00002465
Iteration 70/1000 | Loss: 0.00002465
Iteration 71/1000 | Loss: 0.00002465
Iteration 72/1000 | Loss: 0.00002464
Iteration 73/1000 | Loss: 0.00002463
Iteration 74/1000 | Loss: 0.00002463
Iteration 75/1000 | Loss: 0.00002463
Iteration 76/1000 | Loss: 0.00002462
Iteration 77/1000 | Loss: 0.00002462
Iteration 78/1000 | Loss: 0.00002462
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002461
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002460
Iteration 83/1000 | Loss: 0.00002460
Iteration 84/1000 | Loss: 0.00002459
Iteration 85/1000 | Loss: 0.00002459
Iteration 86/1000 | Loss: 0.00002459
Iteration 87/1000 | Loss: 0.00002459
Iteration 88/1000 | Loss: 0.00002459
Iteration 89/1000 | Loss: 0.00002459
Iteration 90/1000 | Loss: 0.00002459
Iteration 91/1000 | Loss: 0.00002459
Iteration 92/1000 | Loss: 0.00002459
Iteration 93/1000 | Loss: 0.00002459
Iteration 94/1000 | Loss: 0.00002458
Iteration 95/1000 | Loss: 0.00002458
Iteration 96/1000 | Loss: 0.00002458
Iteration 97/1000 | Loss: 0.00002457
Iteration 98/1000 | Loss: 0.00002456
Iteration 99/1000 | Loss: 0.00002456
Iteration 100/1000 | Loss: 0.00002456
Iteration 101/1000 | Loss: 0.00002456
Iteration 102/1000 | Loss: 0.00002456
Iteration 103/1000 | Loss: 0.00002456
Iteration 104/1000 | Loss: 0.00002455
Iteration 105/1000 | Loss: 0.00002455
Iteration 106/1000 | Loss: 0.00002455
Iteration 107/1000 | Loss: 0.00002455
Iteration 108/1000 | Loss: 0.00002455
Iteration 109/1000 | Loss: 0.00002455
Iteration 110/1000 | Loss: 0.00002454
Iteration 111/1000 | Loss: 0.00002454
Iteration 112/1000 | Loss: 0.00002454
Iteration 113/1000 | Loss: 0.00002454
Iteration 114/1000 | Loss: 0.00002454
Iteration 115/1000 | Loss: 0.00002453
Iteration 116/1000 | Loss: 0.00002453
Iteration 117/1000 | Loss: 0.00002453
Iteration 118/1000 | Loss: 0.00002452
Iteration 119/1000 | Loss: 0.00002452
Iteration 120/1000 | Loss: 0.00002452
Iteration 121/1000 | Loss: 0.00002452
Iteration 122/1000 | Loss: 0.00002452
Iteration 123/1000 | Loss: 0.00002452
Iteration 124/1000 | Loss: 0.00002451
Iteration 125/1000 | Loss: 0.00002451
Iteration 126/1000 | Loss: 0.00002451
Iteration 127/1000 | Loss: 0.00002450
Iteration 128/1000 | Loss: 0.00002450
Iteration 129/1000 | Loss: 0.00002450
Iteration 130/1000 | Loss: 0.00002450
Iteration 131/1000 | Loss: 0.00002450
Iteration 132/1000 | Loss: 0.00002450
Iteration 133/1000 | Loss: 0.00002450
Iteration 134/1000 | Loss: 0.00002450
Iteration 135/1000 | Loss: 0.00002450
Iteration 136/1000 | Loss: 0.00002450
Iteration 137/1000 | Loss: 0.00002450
Iteration 138/1000 | Loss: 0.00002449
Iteration 139/1000 | Loss: 0.00002449
Iteration 140/1000 | Loss: 0.00002449
Iteration 141/1000 | Loss: 0.00002449
Iteration 142/1000 | Loss: 0.00002449
Iteration 143/1000 | Loss: 0.00002449
Iteration 144/1000 | Loss: 0.00002449
Iteration 145/1000 | Loss: 0.00002449
Iteration 146/1000 | Loss: 0.00002449
Iteration 147/1000 | Loss: 0.00002449
Iteration 148/1000 | Loss: 0.00002449
Iteration 149/1000 | Loss: 0.00002448
Iteration 150/1000 | Loss: 0.00002448
Iteration 151/1000 | Loss: 0.00002448
Iteration 152/1000 | Loss: 0.00002448
Iteration 153/1000 | Loss: 0.00002448
Iteration 154/1000 | Loss: 0.00002448
Iteration 155/1000 | Loss: 0.00002448
Iteration 156/1000 | Loss: 0.00002448
Iteration 157/1000 | Loss: 0.00002448
Iteration 158/1000 | Loss: 0.00002448
Iteration 159/1000 | Loss: 0.00002448
Iteration 160/1000 | Loss: 0.00002448
Iteration 161/1000 | Loss: 0.00002448
Iteration 162/1000 | Loss: 0.00002447
Iteration 163/1000 | Loss: 0.00002447
Iteration 164/1000 | Loss: 0.00002447
Iteration 165/1000 | Loss: 0.00002447
Iteration 166/1000 | Loss: 0.00002447
Iteration 167/1000 | Loss: 0.00002447
Iteration 168/1000 | Loss: 0.00002447
Iteration 169/1000 | Loss: 0.00002447
Iteration 170/1000 | Loss: 0.00002447
Iteration 171/1000 | Loss: 0.00002447
Iteration 172/1000 | Loss: 0.00002447
Iteration 173/1000 | Loss: 0.00002447
Iteration 174/1000 | Loss: 0.00002447
Iteration 175/1000 | Loss: 0.00002447
Iteration 176/1000 | Loss: 0.00002447
Iteration 177/1000 | Loss: 0.00002446
Iteration 178/1000 | Loss: 0.00002446
Iteration 179/1000 | Loss: 0.00002446
Iteration 180/1000 | Loss: 0.00002446
Iteration 181/1000 | Loss: 0.00002446
Iteration 182/1000 | Loss: 0.00002446
Iteration 183/1000 | Loss: 0.00002446
Iteration 184/1000 | Loss: 0.00002446
Iteration 185/1000 | Loss: 0.00002446
Iteration 186/1000 | Loss: 0.00002446
Iteration 187/1000 | Loss: 0.00002446
Iteration 188/1000 | Loss: 0.00002446
Iteration 189/1000 | Loss: 0.00002446
Iteration 190/1000 | Loss: 0.00002446
Iteration 191/1000 | Loss: 0.00002446
Iteration 192/1000 | Loss: 0.00002445
Iteration 193/1000 | Loss: 0.00002445
Iteration 194/1000 | Loss: 0.00002445
Iteration 195/1000 | Loss: 0.00002445
Iteration 196/1000 | Loss: 0.00002445
Iteration 197/1000 | Loss: 0.00002445
Iteration 198/1000 | Loss: 0.00002445
Iteration 199/1000 | Loss: 0.00002445
Iteration 200/1000 | Loss: 0.00002445
Iteration 201/1000 | Loss: 0.00002445
Iteration 202/1000 | Loss: 0.00002445
Iteration 203/1000 | Loss: 0.00002445
Iteration 204/1000 | Loss: 0.00002445
Iteration 205/1000 | Loss: 0.00002445
Iteration 206/1000 | Loss: 0.00002445
Iteration 207/1000 | Loss: 0.00002445
Iteration 208/1000 | Loss: 0.00002444
Iteration 209/1000 | Loss: 0.00002444
Iteration 210/1000 | Loss: 0.00002444
Iteration 211/1000 | Loss: 0.00002444
Iteration 212/1000 | Loss: 0.00002444
Iteration 213/1000 | Loss: 0.00002444
Iteration 214/1000 | Loss: 0.00002444
Iteration 215/1000 | Loss: 0.00002444
Iteration 216/1000 | Loss: 0.00002444
Iteration 217/1000 | Loss: 0.00002444
Iteration 218/1000 | Loss: 0.00002444
Iteration 219/1000 | Loss: 0.00002444
Iteration 220/1000 | Loss: 0.00002444
Iteration 221/1000 | Loss: 0.00002444
Iteration 222/1000 | Loss: 0.00002444
Iteration 223/1000 | Loss: 0.00002444
Iteration 224/1000 | Loss: 0.00002444
Iteration 225/1000 | Loss: 0.00002444
Iteration 226/1000 | Loss: 0.00002444
Iteration 227/1000 | Loss: 0.00002444
Iteration 228/1000 | Loss: 0.00002444
Iteration 229/1000 | Loss: 0.00002444
Iteration 230/1000 | Loss: 0.00002444
Iteration 231/1000 | Loss: 0.00002444
Iteration 232/1000 | Loss: 0.00002444
Iteration 233/1000 | Loss: 0.00002444
Iteration 234/1000 | Loss: 0.00002444
Iteration 235/1000 | Loss: 0.00002444
Iteration 236/1000 | Loss: 0.00002444
Iteration 237/1000 | Loss: 0.00002444
Iteration 238/1000 | Loss: 0.00002444
Iteration 239/1000 | Loss: 0.00002444
Iteration 240/1000 | Loss: 0.00002444
Iteration 241/1000 | Loss: 0.00002444
Iteration 242/1000 | Loss: 0.00002444
Iteration 243/1000 | Loss: 0.00002444
Iteration 244/1000 | Loss: 0.00002444
Iteration 245/1000 | Loss: 0.00002444
Iteration 246/1000 | Loss: 0.00002444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.4444125301670283e-05, 2.4444125301670283e-05, 2.4444125301670283e-05, 2.4444125301670283e-05, 2.4444125301670283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4444125301670283e-05

Optimization complete. Final v2v error: 3.9849765300750732 mm

Highest mean error: 4.77475643157959 mm for frame 16

Lowest mean error: 3.2557520866394043 mm for frame 83

Saving results

Total time: 45.602673292160034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414097
Iteration 2/25 | Loss: 0.00126004
Iteration 3/25 | Loss: 0.00117025
Iteration 4/25 | Loss: 0.00116396
Iteration 5/25 | Loss: 0.00116302
Iteration 6/25 | Loss: 0.00116302
Iteration 7/25 | Loss: 0.00116302
Iteration 8/25 | Loss: 0.00116302
Iteration 9/25 | Loss: 0.00116302
Iteration 10/25 | Loss: 0.00116302
Iteration 11/25 | Loss: 0.00116302
Iteration 12/25 | Loss: 0.00116302
Iteration 13/25 | Loss: 0.00116302
Iteration 14/25 | Loss: 0.00116302
Iteration 15/25 | Loss: 0.00116302
Iteration 16/25 | Loss: 0.00116302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011630221270024776, 0.0011630221270024776, 0.0011630221270024776, 0.0011630221270024776, 0.0011630221270024776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011630221270024776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37362826
Iteration 2/25 | Loss: 0.00078070
Iteration 3/25 | Loss: 0.00078069
Iteration 4/25 | Loss: 0.00078069
Iteration 5/25 | Loss: 0.00078069
Iteration 6/25 | Loss: 0.00078069
Iteration 7/25 | Loss: 0.00078069
Iteration 8/25 | Loss: 0.00078069
Iteration 9/25 | Loss: 0.00078069
Iteration 10/25 | Loss: 0.00078069
Iteration 11/25 | Loss: 0.00078069
Iteration 12/25 | Loss: 0.00078069
Iteration 13/25 | Loss: 0.00078069
Iteration 14/25 | Loss: 0.00078069
Iteration 15/25 | Loss: 0.00078069
Iteration 16/25 | Loss: 0.00078069
Iteration 17/25 | Loss: 0.00078069
Iteration 18/25 | Loss: 0.00078069
Iteration 19/25 | Loss: 0.00078069
Iteration 20/25 | Loss: 0.00078069
Iteration 21/25 | Loss: 0.00078069
Iteration 22/25 | Loss: 0.00078069
Iteration 23/25 | Loss: 0.00078069
Iteration 24/25 | Loss: 0.00078069
Iteration 25/25 | Loss: 0.00078069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078069
Iteration 2/1000 | Loss: 0.00002931
Iteration 3/1000 | Loss: 0.00001922
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001388
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001370
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001344
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001344
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.3435772416414693e-05, 1.3435772416414693e-05, 1.3435772416414693e-05, 1.3435772416414693e-05, 1.3435772416414693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3435772416414693e-05

Optimization complete. Final v2v error: 3.127922534942627 mm

Highest mean error: 3.5115928649902344 mm for frame 73

Lowest mean error: 2.7826929092407227 mm for frame 200

Saving results

Total time: 26.841890573501587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470127
Iteration 2/25 | Loss: 0.00130548
Iteration 3/25 | Loss: 0.00121830
Iteration 4/25 | Loss: 0.00120192
Iteration 5/25 | Loss: 0.00119576
Iteration 6/25 | Loss: 0.00119431
Iteration 7/25 | Loss: 0.00119431
Iteration 8/25 | Loss: 0.00119431
Iteration 9/25 | Loss: 0.00119431
Iteration 10/25 | Loss: 0.00119431
Iteration 11/25 | Loss: 0.00119431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001194308279082179, 0.001194308279082179, 0.001194308279082179, 0.001194308279082179, 0.001194308279082179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194308279082179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11397147
Iteration 2/25 | Loss: 0.00092041
Iteration 3/25 | Loss: 0.00092040
Iteration 4/25 | Loss: 0.00092040
Iteration 5/25 | Loss: 0.00092040
Iteration 6/25 | Loss: 0.00092040
Iteration 7/25 | Loss: 0.00092040
Iteration 8/25 | Loss: 0.00092040
Iteration 9/25 | Loss: 0.00092040
Iteration 10/25 | Loss: 0.00092040
Iteration 11/25 | Loss: 0.00092040
Iteration 12/25 | Loss: 0.00092040
Iteration 13/25 | Loss: 0.00092040
Iteration 14/25 | Loss: 0.00092040
Iteration 15/25 | Loss: 0.00092040
Iteration 16/25 | Loss: 0.00092040
Iteration 17/25 | Loss: 0.00092040
Iteration 18/25 | Loss: 0.00092040
Iteration 19/25 | Loss: 0.00092040
Iteration 20/25 | Loss: 0.00092040
Iteration 21/25 | Loss: 0.00092040
Iteration 22/25 | Loss: 0.00092040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009203969384543598, 0.0009203969384543598, 0.0009203969384543598, 0.0009203969384543598, 0.0009203969384543598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009203969384543598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092040
Iteration 2/1000 | Loss: 0.00002897
Iteration 3/1000 | Loss: 0.00002096
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001889
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001850
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001805
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001795
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001782
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001777
Iteration 24/1000 | Loss: 0.00001776
Iteration 25/1000 | Loss: 0.00001775
Iteration 26/1000 | Loss: 0.00001775
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001773
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001771
Iteration 32/1000 | Loss: 0.00001771
Iteration 33/1000 | Loss: 0.00001771
Iteration 34/1000 | Loss: 0.00001771
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001768
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001768
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001767
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001766
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001765
Iteration 46/1000 | Loss: 0.00001765
Iteration 47/1000 | Loss: 0.00001765
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001764
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001755
Iteration 80/1000 | Loss: 0.00001755
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001754
Iteration 85/1000 | Loss: 0.00001754
Iteration 86/1000 | Loss: 0.00001754
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001753
Iteration 95/1000 | Loss: 0.00001753
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001752
Iteration 105/1000 | Loss: 0.00001752
Iteration 106/1000 | Loss: 0.00001752
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001751
Iteration 110/1000 | Loss: 0.00001751
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001751
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001748
Iteration 125/1000 | Loss: 0.00001748
Iteration 126/1000 | Loss: 0.00001748
Iteration 127/1000 | Loss: 0.00001748
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001744
Iteration 163/1000 | Loss: 0.00001744
Iteration 164/1000 | Loss: 0.00001744
Iteration 165/1000 | Loss: 0.00001744
Iteration 166/1000 | Loss: 0.00001744
Iteration 167/1000 | Loss: 0.00001744
Iteration 168/1000 | Loss: 0.00001744
Iteration 169/1000 | Loss: 0.00001744
Iteration 170/1000 | Loss: 0.00001744
Iteration 171/1000 | Loss: 0.00001744
Iteration 172/1000 | Loss: 0.00001744
Iteration 173/1000 | Loss: 0.00001744
Iteration 174/1000 | Loss: 0.00001743
Iteration 175/1000 | Loss: 0.00001743
Iteration 176/1000 | Loss: 0.00001743
Iteration 177/1000 | Loss: 0.00001743
Iteration 178/1000 | Loss: 0.00001743
Iteration 179/1000 | Loss: 0.00001743
Iteration 180/1000 | Loss: 0.00001743
Iteration 181/1000 | Loss: 0.00001743
Iteration 182/1000 | Loss: 0.00001743
Iteration 183/1000 | Loss: 0.00001743
Iteration 184/1000 | Loss: 0.00001743
Iteration 185/1000 | Loss: 0.00001743
Iteration 186/1000 | Loss: 0.00001743
Iteration 187/1000 | Loss: 0.00001742
Iteration 188/1000 | Loss: 0.00001742
Iteration 189/1000 | Loss: 0.00001742
Iteration 190/1000 | Loss: 0.00001742
Iteration 191/1000 | Loss: 0.00001742
Iteration 192/1000 | Loss: 0.00001742
Iteration 193/1000 | Loss: 0.00001742
Iteration 194/1000 | Loss: 0.00001741
Iteration 195/1000 | Loss: 0.00001741
Iteration 196/1000 | Loss: 0.00001741
Iteration 197/1000 | Loss: 0.00001741
Iteration 198/1000 | Loss: 0.00001741
Iteration 199/1000 | Loss: 0.00001741
Iteration 200/1000 | Loss: 0.00001741
Iteration 201/1000 | Loss: 0.00001741
Iteration 202/1000 | Loss: 0.00001741
Iteration 203/1000 | Loss: 0.00001741
Iteration 204/1000 | Loss: 0.00001741
Iteration 205/1000 | Loss: 0.00001741
Iteration 206/1000 | Loss: 0.00001741
Iteration 207/1000 | Loss: 0.00001741
Iteration 208/1000 | Loss: 0.00001741
Iteration 209/1000 | Loss: 0.00001741
Iteration 210/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.7412339730071835e-05, 1.7412339730071835e-05, 1.7412339730071835e-05, 1.7412339730071835e-05, 1.7412339730071835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7412339730071835e-05

Optimization complete. Final v2v error: 3.4997715950012207 mm

Highest mean error: 4.222717761993408 mm for frame 183

Lowest mean error: 3.350829839706421 mm for frame 125

Saving results

Total time: 41.58073592185974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844468
Iteration 2/25 | Loss: 0.00170614
Iteration 3/25 | Loss: 0.00140603
Iteration 4/25 | Loss: 0.00135539
Iteration 5/25 | Loss: 0.00134525
Iteration 6/25 | Loss: 0.00131499
Iteration 7/25 | Loss: 0.00130652
Iteration 8/25 | Loss: 0.00128755
Iteration 9/25 | Loss: 0.00128362
Iteration 10/25 | Loss: 0.00127266
Iteration 11/25 | Loss: 0.00126701
Iteration 12/25 | Loss: 0.00127024
Iteration 13/25 | Loss: 0.00126628
Iteration 14/25 | Loss: 0.00126195
Iteration 15/25 | Loss: 0.00126158
Iteration 16/25 | Loss: 0.00126165
Iteration 17/25 | Loss: 0.00125309
Iteration 18/25 | Loss: 0.00125390
Iteration 19/25 | Loss: 0.00125401
Iteration 20/25 | Loss: 0.00124899
Iteration 21/25 | Loss: 0.00125588
Iteration 22/25 | Loss: 0.00125175
Iteration 23/25 | Loss: 0.00124655
Iteration 24/25 | Loss: 0.00124750
Iteration 25/25 | Loss: 0.00124555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74452615
Iteration 2/25 | Loss: 0.00098267
Iteration 3/25 | Loss: 0.00098264
Iteration 4/25 | Loss: 0.00098264
Iteration 5/25 | Loss: 0.00098264
Iteration 6/25 | Loss: 0.00098264
Iteration 7/25 | Loss: 0.00098264
Iteration 8/25 | Loss: 0.00098264
Iteration 9/25 | Loss: 0.00098264
Iteration 10/25 | Loss: 0.00098264
Iteration 11/25 | Loss: 0.00098264
Iteration 12/25 | Loss: 0.00098264
Iteration 13/25 | Loss: 0.00098264
Iteration 14/25 | Loss: 0.00098264
Iteration 15/25 | Loss: 0.00098264
Iteration 16/25 | Loss: 0.00098264
Iteration 17/25 | Loss: 0.00098264
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009826415916904807, 0.0009826415916904807, 0.0009826415916904807, 0.0009826415916904807, 0.0009826415916904807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009826415916904807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098264
Iteration 2/1000 | Loss: 0.00012042
Iteration 3/1000 | Loss: 0.00010745
Iteration 4/1000 | Loss: 0.00007032
Iteration 5/1000 | Loss: 0.00026819
Iteration 6/1000 | Loss: 0.00022864
Iteration 7/1000 | Loss: 0.00004370
Iteration 8/1000 | Loss: 0.00003768
Iteration 9/1000 | Loss: 0.00003914
Iteration 10/1000 | Loss: 0.00003254
Iteration 11/1000 | Loss: 0.00004188
Iteration 12/1000 | Loss: 0.00003116
Iteration 13/1000 | Loss: 0.00007418
Iteration 14/1000 | Loss: 0.00003068
Iteration 15/1000 | Loss: 0.00005038
Iteration 16/1000 | Loss: 0.00003529
Iteration 17/1000 | Loss: 0.00002961
Iteration 18/1000 | Loss: 0.00002921
Iteration 19/1000 | Loss: 0.00002894
Iteration 20/1000 | Loss: 0.00002861
Iteration 21/1000 | Loss: 0.00002844
Iteration 22/1000 | Loss: 0.00002822
Iteration 23/1000 | Loss: 0.00002807
Iteration 24/1000 | Loss: 0.00002798
Iteration 25/1000 | Loss: 0.00002792
Iteration 26/1000 | Loss: 0.00002780
Iteration 27/1000 | Loss: 0.00002780
Iteration 28/1000 | Loss: 0.00002779
Iteration 29/1000 | Loss: 0.00002779
Iteration 30/1000 | Loss: 0.00002778
Iteration 31/1000 | Loss: 0.00002778
Iteration 32/1000 | Loss: 0.00002776
Iteration 33/1000 | Loss: 0.00002776
Iteration 34/1000 | Loss: 0.00002776
Iteration 35/1000 | Loss: 0.00002776
Iteration 36/1000 | Loss: 0.00002776
Iteration 37/1000 | Loss: 0.00002774
Iteration 38/1000 | Loss: 0.00002773
Iteration 39/1000 | Loss: 0.00002771
Iteration 40/1000 | Loss: 0.00002771
Iteration 41/1000 | Loss: 0.00002771
Iteration 42/1000 | Loss: 0.00002771
Iteration 43/1000 | Loss: 0.00002771
Iteration 44/1000 | Loss: 0.00002771
Iteration 45/1000 | Loss: 0.00002771
Iteration 46/1000 | Loss: 0.00002771
Iteration 47/1000 | Loss: 0.00002770
Iteration 48/1000 | Loss: 0.00002770
Iteration 49/1000 | Loss: 0.00002770
Iteration 50/1000 | Loss: 0.00002770
Iteration 51/1000 | Loss: 0.00002770
Iteration 52/1000 | Loss: 0.00002769
Iteration 53/1000 | Loss: 0.00002768
Iteration 54/1000 | Loss: 0.00002767
Iteration 55/1000 | Loss: 0.00002767
Iteration 56/1000 | Loss: 0.00002767
Iteration 57/1000 | Loss: 0.00002766
Iteration 58/1000 | Loss: 0.00002766
Iteration 59/1000 | Loss: 0.00002766
Iteration 60/1000 | Loss: 0.00002765
Iteration 61/1000 | Loss: 0.00002765
Iteration 62/1000 | Loss: 0.00002765
Iteration 63/1000 | Loss: 0.00002765
Iteration 64/1000 | Loss: 0.00002764
Iteration 65/1000 | Loss: 0.00002764
Iteration 66/1000 | Loss: 0.00002764
Iteration 67/1000 | Loss: 0.00002764
Iteration 68/1000 | Loss: 0.00002764
Iteration 69/1000 | Loss: 0.00002763
Iteration 70/1000 | Loss: 0.00002763
Iteration 71/1000 | Loss: 0.00002763
Iteration 72/1000 | Loss: 0.00002762
Iteration 73/1000 | Loss: 0.00002762
Iteration 74/1000 | Loss: 0.00002762
Iteration 75/1000 | Loss: 0.00002762
Iteration 76/1000 | Loss: 0.00002761
Iteration 77/1000 | Loss: 0.00002761
Iteration 78/1000 | Loss: 0.00002761
Iteration 79/1000 | Loss: 0.00002761
Iteration 80/1000 | Loss: 0.00002760
Iteration 81/1000 | Loss: 0.00002760
Iteration 82/1000 | Loss: 0.00002760
Iteration 83/1000 | Loss: 0.00002760
Iteration 84/1000 | Loss: 0.00002760
Iteration 85/1000 | Loss: 0.00002760
Iteration 86/1000 | Loss: 0.00002760
Iteration 87/1000 | Loss: 0.00002760
Iteration 88/1000 | Loss: 0.00002760
Iteration 89/1000 | Loss: 0.00002759
Iteration 90/1000 | Loss: 0.00002759
Iteration 91/1000 | Loss: 0.00002759
Iteration 92/1000 | Loss: 0.00002759
Iteration 93/1000 | Loss: 0.00002759
Iteration 94/1000 | Loss: 0.00002759
Iteration 95/1000 | Loss: 0.00002759
Iteration 96/1000 | Loss: 0.00002759
Iteration 97/1000 | Loss: 0.00002759
Iteration 98/1000 | Loss: 0.00002759
Iteration 99/1000 | Loss: 0.00002759
Iteration 100/1000 | Loss: 0.00002759
Iteration 101/1000 | Loss: 0.00002759
Iteration 102/1000 | Loss: 0.00002759
Iteration 103/1000 | Loss: 0.00002759
Iteration 104/1000 | Loss: 0.00002758
Iteration 105/1000 | Loss: 0.00002758
Iteration 106/1000 | Loss: 0.00002758
Iteration 107/1000 | Loss: 0.00002758
Iteration 108/1000 | Loss: 0.00002758
Iteration 109/1000 | Loss: 0.00002757
Iteration 110/1000 | Loss: 0.00002757
Iteration 111/1000 | Loss: 0.00002757
Iteration 112/1000 | Loss: 0.00002757
Iteration 113/1000 | Loss: 0.00002757
Iteration 114/1000 | Loss: 0.00002756
Iteration 115/1000 | Loss: 0.00002756
Iteration 116/1000 | Loss: 0.00002756
Iteration 117/1000 | Loss: 0.00002756
Iteration 118/1000 | Loss: 0.00002756
Iteration 119/1000 | Loss: 0.00002756
Iteration 120/1000 | Loss: 0.00002756
Iteration 121/1000 | Loss: 0.00002756
Iteration 122/1000 | Loss: 0.00002755
Iteration 123/1000 | Loss: 0.00002755
Iteration 124/1000 | Loss: 0.00002755
Iteration 125/1000 | Loss: 0.00002755
Iteration 126/1000 | Loss: 0.00002755
Iteration 127/1000 | Loss: 0.00002755
Iteration 128/1000 | Loss: 0.00002755
Iteration 129/1000 | Loss: 0.00002755
Iteration 130/1000 | Loss: 0.00002755
Iteration 131/1000 | Loss: 0.00002754
Iteration 132/1000 | Loss: 0.00002754
Iteration 133/1000 | Loss: 0.00002754
Iteration 134/1000 | Loss: 0.00002754
Iteration 135/1000 | Loss: 0.00002754
Iteration 136/1000 | Loss: 0.00002754
Iteration 137/1000 | Loss: 0.00002753
Iteration 138/1000 | Loss: 0.00002753
Iteration 139/1000 | Loss: 0.00002753
Iteration 140/1000 | Loss: 0.00002753
Iteration 141/1000 | Loss: 0.00002753
Iteration 142/1000 | Loss: 0.00002753
Iteration 143/1000 | Loss: 0.00002753
Iteration 144/1000 | Loss: 0.00002753
Iteration 145/1000 | Loss: 0.00002753
Iteration 146/1000 | Loss: 0.00002753
Iteration 147/1000 | Loss: 0.00002753
Iteration 148/1000 | Loss: 0.00002752
Iteration 149/1000 | Loss: 0.00002752
Iteration 150/1000 | Loss: 0.00002752
Iteration 151/1000 | Loss: 0.00002752
Iteration 152/1000 | Loss: 0.00002752
Iteration 153/1000 | Loss: 0.00002752
Iteration 154/1000 | Loss: 0.00002752
Iteration 155/1000 | Loss: 0.00002752
Iteration 156/1000 | Loss: 0.00002752
Iteration 157/1000 | Loss: 0.00002752
Iteration 158/1000 | Loss: 0.00002752
Iteration 159/1000 | Loss: 0.00002752
Iteration 160/1000 | Loss: 0.00002752
Iteration 161/1000 | Loss: 0.00002752
Iteration 162/1000 | Loss: 0.00002752
Iteration 163/1000 | Loss: 0.00002752
Iteration 164/1000 | Loss: 0.00002752
Iteration 165/1000 | Loss: 0.00002752
Iteration 166/1000 | Loss: 0.00002752
Iteration 167/1000 | Loss: 0.00002752
Iteration 168/1000 | Loss: 0.00002752
Iteration 169/1000 | Loss: 0.00002752
Iteration 170/1000 | Loss: 0.00002752
Iteration 171/1000 | Loss: 0.00002752
Iteration 172/1000 | Loss: 0.00002752
Iteration 173/1000 | Loss: 0.00002752
Iteration 174/1000 | Loss: 0.00002752
Iteration 175/1000 | Loss: 0.00002752
Iteration 176/1000 | Loss: 0.00002752
Iteration 177/1000 | Loss: 0.00002752
Iteration 178/1000 | Loss: 0.00002752
Iteration 179/1000 | Loss: 0.00002752
Iteration 180/1000 | Loss: 0.00002752
Iteration 181/1000 | Loss: 0.00002752
Iteration 182/1000 | Loss: 0.00002752
Iteration 183/1000 | Loss: 0.00002752
Iteration 184/1000 | Loss: 0.00002752
Iteration 185/1000 | Loss: 0.00002752
Iteration 186/1000 | Loss: 0.00002752
Iteration 187/1000 | Loss: 0.00002752
Iteration 188/1000 | Loss: 0.00002752
Iteration 189/1000 | Loss: 0.00002752
Iteration 190/1000 | Loss: 0.00002752
Iteration 191/1000 | Loss: 0.00002752
Iteration 192/1000 | Loss: 0.00002752
Iteration 193/1000 | Loss: 0.00002752
Iteration 194/1000 | Loss: 0.00002752
Iteration 195/1000 | Loss: 0.00002752
Iteration 196/1000 | Loss: 0.00002752
Iteration 197/1000 | Loss: 0.00002752
Iteration 198/1000 | Loss: 0.00002752
Iteration 199/1000 | Loss: 0.00002752
Iteration 200/1000 | Loss: 0.00002752
Iteration 201/1000 | Loss: 0.00002752
Iteration 202/1000 | Loss: 0.00002752
Iteration 203/1000 | Loss: 0.00002752
Iteration 204/1000 | Loss: 0.00002752
Iteration 205/1000 | Loss: 0.00002752
Iteration 206/1000 | Loss: 0.00002752
Iteration 207/1000 | Loss: 0.00002752
Iteration 208/1000 | Loss: 0.00002752
Iteration 209/1000 | Loss: 0.00002752
Iteration 210/1000 | Loss: 0.00002752
Iteration 211/1000 | Loss: 0.00002752
Iteration 212/1000 | Loss: 0.00002752
Iteration 213/1000 | Loss: 0.00002752
Iteration 214/1000 | Loss: 0.00002752
Iteration 215/1000 | Loss: 0.00002752
Iteration 216/1000 | Loss: 0.00002752
Iteration 217/1000 | Loss: 0.00002752
Iteration 218/1000 | Loss: 0.00002752
Iteration 219/1000 | Loss: 0.00002752
Iteration 220/1000 | Loss: 0.00002752
Iteration 221/1000 | Loss: 0.00002752
Iteration 222/1000 | Loss: 0.00002752
Iteration 223/1000 | Loss: 0.00002752
Iteration 224/1000 | Loss: 0.00002752
Iteration 225/1000 | Loss: 0.00002752
Iteration 226/1000 | Loss: 0.00002752
Iteration 227/1000 | Loss: 0.00002752
Iteration 228/1000 | Loss: 0.00002752
Iteration 229/1000 | Loss: 0.00002752
Iteration 230/1000 | Loss: 0.00002752
Iteration 231/1000 | Loss: 0.00002752
Iteration 232/1000 | Loss: 0.00002752
Iteration 233/1000 | Loss: 0.00002752
Iteration 234/1000 | Loss: 0.00002752
Iteration 235/1000 | Loss: 0.00002752
Iteration 236/1000 | Loss: 0.00002752
Iteration 237/1000 | Loss: 0.00002752
Iteration 238/1000 | Loss: 0.00002752
Iteration 239/1000 | Loss: 0.00002752
Iteration 240/1000 | Loss: 0.00002752
Iteration 241/1000 | Loss: 0.00002752
Iteration 242/1000 | Loss: 0.00002752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.752240288828034e-05, 2.752240288828034e-05, 2.752240288828034e-05, 2.752240288828034e-05, 2.752240288828034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.752240288828034e-05

Optimization complete. Final v2v error: 4.361660003662109 mm

Highest mean error: 5.817991256713867 mm for frame 98

Lowest mean error: 3.442709445953369 mm for frame 41

Saving results

Total time: 104.24132585525513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812250
Iteration 2/25 | Loss: 0.00126077
Iteration 3/25 | Loss: 0.00115070
Iteration 4/25 | Loss: 0.00113936
Iteration 5/25 | Loss: 0.00113675
Iteration 6/25 | Loss: 0.00113649
Iteration 7/25 | Loss: 0.00113649
Iteration 8/25 | Loss: 0.00113649
Iteration 9/25 | Loss: 0.00113649
Iteration 10/25 | Loss: 0.00113649
Iteration 11/25 | Loss: 0.00113649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011364876991137862, 0.0011364876991137862, 0.0011364876991137862, 0.0011364876991137862, 0.0011364876991137862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011364876991137862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36966956
Iteration 2/25 | Loss: 0.00084827
Iteration 3/25 | Loss: 0.00084826
Iteration 4/25 | Loss: 0.00084826
Iteration 5/25 | Loss: 0.00084826
Iteration 6/25 | Loss: 0.00084826
Iteration 7/25 | Loss: 0.00084826
Iteration 8/25 | Loss: 0.00084826
Iteration 9/25 | Loss: 0.00084826
Iteration 10/25 | Loss: 0.00084826
Iteration 11/25 | Loss: 0.00084826
Iteration 12/25 | Loss: 0.00084826
Iteration 13/25 | Loss: 0.00084826
Iteration 14/25 | Loss: 0.00084826
Iteration 15/25 | Loss: 0.00084826
Iteration 16/25 | Loss: 0.00084826
Iteration 17/25 | Loss: 0.00084826
Iteration 18/25 | Loss: 0.00084826
Iteration 19/25 | Loss: 0.00084826
Iteration 20/25 | Loss: 0.00084826
Iteration 21/25 | Loss: 0.00084826
Iteration 22/25 | Loss: 0.00084826
Iteration 23/25 | Loss: 0.00084826
Iteration 24/25 | Loss: 0.00084826
Iteration 25/25 | Loss: 0.00084826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084826
Iteration 2/1000 | Loss: 0.00002945
Iteration 3/1000 | Loss: 0.00002068
Iteration 4/1000 | Loss: 0.00001609
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001355
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001239
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001207
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001129
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001122
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001122
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001121
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001119
Iteration 50/1000 | Loss: 0.00001118
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001117
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001111
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001109
Iteration 79/1000 | Loss: 0.00001109
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001107
Iteration 86/1000 | Loss: 0.00001107
Iteration 87/1000 | Loss: 0.00001107
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001106
Iteration 90/1000 | Loss: 0.00001106
Iteration 91/1000 | Loss: 0.00001106
Iteration 92/1000 | Loss: 0.00001105
Iteration 93/1000 | Loss: 0.00001105
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001105
Iteration 96/1000 | Loss: 0.00001104
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001103
Iteration 106/1000 | Loss: 0.00001103
Iteration 107/1000 | Loss: 0.00001103
Iteration 108/1000 | Loss: 0.00001103
Iteration 109/1000 | Loss: 0.00001103
Iteration 110/1000 | Loss: 0.00001103
Iteration 111/1000 | Loss: 0.00001103
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001102
Iteration 114/1000 | Loss: 0.00001102
Iteration 115/1000 | Loss: 0.00001102
Iteration 116/1000 | Loss: 0.00001102
Iteration 117/1000 | Loss: 0.00001102
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001101
Iteration 121/1000 | Loss: 0.00001101
Iteration 122/1000 | Loss: 0.00001101
Iteration 123/1000 | Loss: 0.00001101
Iteration 124/1000 | Loss: 0.00001101
Iteration 125/1000 | Loss: 0.00001100
Iteration 126/1000 | Loss: 0.00001100
Iteration 127/1000 | Loss: 0.00001100
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001097
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001096
Iteration 142/1000 | Loss: 0.00001096
Iteration 143/1000 | Loss: 0.00001096
Iteration 144/1000 | Loss: 0.00001096
Iteration 145/1000 | Loss: 0.00001096
Iteration 146/1000 | Loss: 0.00001095
Iteration 147/1000 | Loss: 0.00001095
Iteration 148/1000 | Loss: 0.00001095
Iteration 149/1000 | Loss: 0.00001095
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001094
Iteration 152/1000 | Loss: 0.00001094
Iteration 153/1000 | Loss: 0.00001094
Iteration 154/1000 | Loss: 0.00001094
Iteration 155/1000 | Loss: 0.00001094
Iteration 156/1000 | Loss: 0.00001094
Iteration 157/1000 | Loss: 0.00001094
Iteration 158/1000 | Loss: 0.00001094
Iteration 159/1000 | Loss: 0.00001094
Iteration 160/1000 | Loss: 0.00001094
Iteration 161/1000 | Loss: 0.00001094
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001094
Iteration 164/1000 | Loss: 0.00001093
Iteration 165/1000 | Loss: 0.00001093
Iteration 166/1000 | Loss: 0.00001093
Iteration 167/1000 | Loss: 0.00001093
Iteration 168/1000 | Loss: 0.00001093
Iteration 169/1000 | Loss: 0.00001093
Iteration 170/1000 | Loss: 0.00001093
Iteration 171/1000 | Loss: 0.00001093
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001092
Iteration 174/1000 | Loss: 0.00001092
Iteration 175/1000 | Loss: 0.00001092
Iteration 176/1000 | Loss: 0.00001092
Iteration 177/1000 | Loss: 0.00001092
Iteration 178/1000 | Loss: 0.00001092
Iteration 179/1000 | Loss: 0.00001092
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001091
Iteration 183/1000 | Loss: 0.00001091
Iteration 184/1000 | Loss: 0.00001091
Iteration 185/1000 | Loss: 0.00001091
Iteration 186/1000 | Loss: 0.00001091
Iteration 187/1000 | Loss: 0.00001091
Iteration 188/1000 | Loss: 0.00001091
Iteration 189/1000 | Loss: 0.00001091
Iteration 190/1000 | Loss: 0.00001091
Iteration 191/1000 | Loss: 0.00001091
Iteration 192/1000 | Loss: 0.00001091
Iteration 193/1000 | Loss: 0.00001091
Iteration 194/1000 | Loss: 0.00001091
Iteration 195/1000 | Loss: 0.00001091
Iteration 196/1000 | Loss: 0.00001091
Iteration 197/1000 | Loss: 0.00001090
Iteration 198/1000 | Loss: 0.00001090
Iteration 199/1000 | Loss: 0.00001090
Iteration 200/1000 | Loss: 0.00001090
Iteration 201/1000 | Loss: 0.00001090
Iteration 202/1000 | Loss: 0.00001090
Iteration 203/1000 | Loss: 0.00001090
Iteration 204/1000 | Loss: 0.00001090
Iteration 205/1000 | Loss: 0.00001090
Iteration 206/1000 | Loss: 0.00001090
Iteration 207/1000 | Loss: 0.00001090
Iteration 208/1000 | Loss: 0.00001089
Iteration 209/1000 | Loss: 0.00001089
Iteration 210/1000 | Loss: 0.00001089
Iteration 211/1000 | Loss: 0.00001089
Iteration 212/1000 | Loss: 0.00001089
Iteration 213/1000 | Loss: 0.00001089
Iteration 214/1000 | Loss: 0.00001089
Iteration 215/1000 | Loss: 0.00001089
Iteration 216/1000 | Loss: 0.00001089
Iteration 217/1000 | Loss: 0.00001089
Iteration 218/1000 | Loss: 0.00001089
Iteration 219/1000 | Loss: 0.00001089
Iteration 220/1000 | Loss: 0.00001089
Iteration 221/1000 | Loss: 0.00001089
Iteration 222/1000 | Loss: 0.00001089
Iteration 223/1000 | Loss: 0.00001089
Iteration 224/1000 | Loss: 0.00001089
Iteration 225/1000 | Loss: 0.00001088
Iteration 226/1000 | Loss: 0.00001088
Iteration 227/1000 | Loss: 0.00001088
Iteration 228/1000 | Loss: 0.00001088
Iteration 229/1000 | Loss: 0.00001088
Iteration 230/1000 | Loss: 0.00001088
Iteration 231/1000 | Loss: 0.00001088
Iteration 232/1000 | Loss: 0.00001088
Iteration 233/1000 | Loss: 0.00001088
Iteration 234/1000 | Loss: 0.00001088
Iteration 235/1000 | Loss: 0.00001088
Iteration 236/1000 | Loss: 0.00001088
Iteration 237/1000 | Loss: 0.00001088
Iteration 238/1000 | Loss: 0.00001088
Iteration 239/1000 | Loss: 0.00001088
Iteration 240/1000 | Loss: 0.00001088
Iteration 241/1000 | Loss: 0.00001088
Iteration 242/1000 | Loss: 0.00001088
Iteration 243/1000 | Loss: 0.00001088
Iteration 244/1000 | Loss: 0.00001088
Iteration 245/1000 | Loss: 0.00001088
Iteration 246/1000 | Loss: 0.00001087
Iteration 247/1000 | Loss: 0.00001087
Iteration 248/1000 | Loss: 0.00001087
Iteration 249/1000 | Loss: 0.00001087
Iteration 250/1000 | Loss: 0.00001087
Iteration 251/1000 | Loss: 0.00001087
Iteration 252/1000 | Loss: 0.00001087
Iteration 253/1000 | Loss: 0.00001087
Iteration 254/1000 | Loss: 0.00001087
Iteration 255/1000 | Loss: 0.00001087
Iteration 256/1000 | Loss: 0.00001087
Iteration 257/1000 | Loss: 0.00001087
Iteration 258/1000 | Loss: 0.00001087
Iteration 259/1000 | Loss: 0.00001087
Iteration 260/1000 | Loss: 0.00001087
Iteration 261/1000 | Loss: 0.00001087
Iteration 262/1000 | Loss: 0.00001087
Iteration 263/1000 | Loss: 0.00001087
Iteration 264/1000 | Loss: 0.00001087
Iteration 265/1000 | Loss: 0.00001087
Iteration 266/1000 | Loss: 0.00001087
Iteration 267/1000 | Loss: 0.00001087
Iteration 268/1000 | Loss: 0.00001087
Iteration 269/1000 | Loss: 0.00001087
Iteration 270/1000 | Loss: 0.00001087
Iteration 271/1000 | Loss: 0.00001087
Iteration 272/1000 | Loss: 0.00001087
Iteration 273/1000 | Loss: 0.00001087
Iteration 274/1000 | Loss: 0.00001087
Iteration 275/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.0866793672903441e-05, 1.0866793672903441e-05, 1.0866793672903441e-05, 1.0866793672903441e-05, 1.0866793672903441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0866793672903441e-05

Optimization complete. Final v2v error: 2.7725517749786377 mm

Highest mean error: 3.8155710697174072 mm for frame 61

Lowest mean error: 2.4676172733306885 mm for frame 153

Saving results

Total time: 45.23968505859375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083495
Iteration 2/25 | Loss: 0.00211885
Iteration 3/25 | Loss: 0.00157610
Iteration 4/25 | Loss: 0.00146262
Iteration 5/25 | Loss: 0.00143778
Iteration 6/25 | Loss: 0.00142608
Iteration 7/25 | Loss: 0.00141243
Iteration 8/25 | Loss: 0.00140182
Iteration 9/25 | Loss: 0.00139900
Iteration 10/25 | Loss: 0.00139843
Iteration 11/25 | Loss: 0.00139684
Iteration 12/25 | Loss: 0.00139653
Iteration 13/25 | Loss: 0.00139495
Iteration 14/25 | Loss: 0.00139479
Iteration 15/25 | Loss: 0.00139477
Iteration 16/25 | Loss: 0.00139477
Iteration 17/25 | Loss: 0.00139474
Iteration 18/25 | Loss: 0.00139474
Iteration 19/25 | Loss: 0.00139474
Iteration 20/25 | Loss: 0.00139474
Iteration 21/25 | Loss: 0.00139474
Iteration 22/25 | Loss: 0.00139474
Iteration 23/25 | Loss: 0.00139474
Iteration 24/25 | Loss: 0.00139474
Iteration 25/25 | Loss: 0.00139474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61087155
Iteration 2/25 | Loss: 0.00182986
Iteration 3/25 | Loss: 0.00182973
Iteration 4/25 | Loss: 0.00182973
Iteration 5/25 | Loss: 0.00182973
Iteration 6/25 | Loss: 0.00182972
Iteration 7/25 | Loss: 0.00182972
Iteration 8/25 | Loss: 0.00182972
Iteration 9/25 | Loss: 0.00182972
Iteration 10/25 | Loss: 0.00182972
Iteration 11/25 | Loss: 0.00182972
Iteration 12/25 | Loss: 0.00182972
Iteration 13/25 | Loss: 0.00182972
Iteration 14/25 | Loss: 0.00182972
Iteration 15/25 | Loss: 0.00182972
Iteration 16/25 | Loss: 0.00182972
Iteration 17/25 | Loss: 0.00182972
Iteration 18/25 | Loss: 0.00182972
Iteration 19/25 | Loss: 0.00182972
Iteration 20/25 | Loss: 0.00182972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018297232454642653, 0.0018297232454642653, 0.0018297232454642653, 0.0018297232454642653, 0.0018297232454642653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018297232454642653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182972
Iteration 2/1000 | Loss: 0.00023930
Iteration 3/1000 | Loss: 0.00031441
Iteration 4/1000 | Loss: 0.00026256
Iteration 5/1000 | Loss: 0.00009917
Iteration 6/1000 | Loss: 0.00007016
Iteration 7/1000 | Loss: 0.00005564
Iteration 8/1000 | Loss: 0.00005120
Iteration 9/1000 | Loss: 0.00004847
Iteration 10/1000 | Loss: 0.00005853
Iteration 11/1000 | Loss: 0.00004751
Iteration 12/1000 | Loss: 0.00004525
Iteration 13/1000 | Loss: 0.00006055
Iteration 14/1000 | Loss: 0.00004738
Iteration 15/1000 | Loss: 0.00004526
Iteration 16/1000 | Loss: 0.00004399
Iteration 17/1000 | Loss: 0.00007170
Iteration 18/1000 | Loss: 0.00007295
Iteration 19/1000 | Loss: 0.00004386
Iteration 20/1000 | Loss: 0.00004187
Iteration 21/1000 | Loss: 0.00005040
Iteration 22/1000 | Loss: 0.00004165
Iteration 23/1000 | Loss: 0.00007228
Iteration 24/1000 | Loss: 0.00006651
Iteration 25/1000 | Loss: 0.00004705
Iteration 26/1000 | Loss: 0.00004513
Iteration 27/1000 | Loss: 0.00004945
Iteration 28/1000 | Loss: 0.00007093
Iteration 29/1000 | Loss: 0.00006581
Iteration 30/1000 | Loss: 0.00006880
Iteration 31/1000 | Loss: 0.00006359
Iteration 32/1000 | Loss: 0.00007737
Iteration 33/1000 | Loss: 0.00007651
Iteration 34/1000 | Loss: 0.00005837
Iteration 35/1000 | Loss: 0.00006109
Iteration 36/1000 | Loss: 0.00006311
Iteration 37/1000 | Loss: 0.00004557
Iteration 38/1000 | Loss: 0.00005056
Iteration 39/1000 | Loss: 0.00005764
Iteration 40/1000 | Loss: 0.00008922
Iteration 41/1000 | Loss: 0.00006149
Iteration 42/1000 | Loss: 0.00006212
Iteration 43/1000 | Loss: 0.00006595
Iteration 44/1000 | Loss: 0.00006295
Iteration 45/1000 | Loss: 0.00006537
Iteration 46/1000 | Loss: 0.00004845
Iteration 47/1000 | Loss: 0.00004818
Iteration 48/1000 | Loss: 0.00005336
Iteration 49/1000 | Loss: 0.00004877
Iteration 50/1000 | Loss: 0.00005397
Iteration 51/1000 | Loss: 0.00004984
Iteration 52/1000 | Loss: 0.00005456
Iteration 53/1000 | Loss: 0.00004886
Iteration 54/1000 | Loss: 0.00005098
Iteration 55/1000 | Loss: 0.00005034
Iteration 56/1000 | Loss: 0.00005898
Iteration 57/1000 | Loss: 0.00005004
Iteration 58/1000 | Loss: 0.00005175
Iteration 59/1000 | Loss: 0.00005718
Iteration 60/1000 | Loss: 0.00005594
Iteration 61/1000 | Loss: 0.00005420
Iteration 62/1000 | Loss: 0.00004781
Iteration 63/1000 | Loss: 0.00004531
Iteration 64/1000 | Loss: 0.00005221
Iteration 65/1000 | Loss: 0.00004705
Iteration 66/1000 | Loss: 0.00004113
Iteration 67/1000 | Loss: 0.00005544
Iteration 68/1000 | Loss: 0.00004101
Iteration 69/1000 | Loss: 0.00005144
Iteration 70/1000 | Loss: 0.00004119
Iteration 71/1000 | Loss: 0.00005464
Iteration 72/1000 | Loss: 0.00004569
Iteration 73/1000 | Loss: 0.00005514
Iteration 74/1000 | Loss: 0.00005561
Iteration 75/1000 | Loss: 0.00005438
Iteration 76/1000 | Loss: 0.00003894
Iteration 77/1000 | Loss: 0.00004050
Iteration 78/1000 | Loss: 0.00005189
Iteration 79/1000 | Loss: 0.00004501
Iteration 80/1000 | Loss: 0.00004152
Iteration 81/1000 | Loss: 0.00004055
Iteration 82/1000 | Loss: 0.00004890
Iteration 83/1000 | Loss: 0.00005649
Iteration 84/1000 | Loss: 0.00004101
Iteration 85/1000 | Loss: 0.00003885
Iteration 86/1000 | Loss: 0.00003827
Iteration 87/1000 | Loss: 0.00003777
Iteration 88/1000 | Loss: 0.00003747
Iteration 89/1000 | Loss: 0.00003737
Iteration 90/1000 | Loss: 0.00003729
Iteration 91/1000 | Loss: 0.00003728
Iteration 92/1000 | Loss: 0.00003727
Iteration 93/1000 | Loss: 0.00003726
Iteration 94/1000 | Loss: 0.00003726
Iteration 95/1000 | Loss: 0.00003726
Iteration 96/1000 | Loss: 0.00003726
Iteration 97/1000 | Loss: 0.00003725
Iteration 98/1000 | Loss: 0.00003725
Iteration 99/1000 | Loss: 0.00003725
Iteration 100/1000 | Loss: 0.00003725
Iteration 101/1000 | Loss: 0.00003725
Iteration 102/1000 | Loss: 0.00003724
Iteration 103/1000 | Loss: 0.00003724
Iteration 104/1000 | Loss: 0.00003721
Iteration 105/1000 | Loss: 0.00003721
Iteration 106/1000 | Loss: 0.00003721
Iteration 107/1000 | Loss: 0.00003721
Iteration 108/1000 | Loss: 0.00003720
Iteration 109/1000 | Loss: 0.00003720
Iteration 110/1000 | Loss: 0.00003720
Iteration 111/1000 | Loss: 0.00003720
Iteration 112/1000 | Loss: 0.00003720
Iteration 113/1000 | Loss: 0.00003720
Iteration 114/1000 | Loss: 0.00003720
Iteration 115/1000 | Loss: 0.00003720
Iteration 116/1000 | Loss: 0.00003719
Iteration 117/1000 | Loss: 0.00003719
Iteration 118/1000 | Loss: 0.00003718
Iteration 119/1000 | Loss: 0.00003718
Iteration 120/1000 | Loss: 0.00003718
Iteration 121/1000 | Loss: 0.00003718
Iteration 122/1000 | Loss: 0.00003718
Iteration 123/1000 | Loss: 0.00003718
Iteration 124/1000 | Loss: 0.00003717
Iteration 125/1000 | Loss: 0.00003717
Iteration 126/1000 | Loss: 0.00003714
Iteration 127/1000 | Loss: 0.00003714
Iteration 128/1000 | Loss: 0.00003714
Iteration 129/1000 | Loss: 0.00003714
Iteration 130/1000 | Loss: 0.00003714
Iteration 131/1000 | Loss: 0.00003714
Iteration 132/1000 | Loss: 0.00003714
Iteration 133/1000 | Loss: 0.00003714
Iteration 134/1000 | Loss: 0.00003714
Iteration 135/1000 | Loss: 0.00003714
Iteration 136/1000 | Loss: 0.00003713
Iteration 137/1000 | Loss: 0.00003713
Iteration 138/1000 | Loss: 0.00003713
Iteration 139/1000 | Loss: 0.00003711
Iteration 140/1000 | Loss: 0.00003709
Iteration 141/1000 | Loss: 0.00003709
Iteration 142/1000 | Loss: 0.00003709
Iteration 143/1000 | Loss: 0.00003709
Iteration 144/1000 | Loss: 0.00003709
Iteration 145/1000 | Loss: 0.00003709
Iteration 146/1000 | Loss: 0.00003709
Iteration 147/1000 | Loss: 0.00003709
Iteration 148/1000 | Loss: 0.00003709
Iteration 149/1000 | Loss: 0.00003708
Iteration 150/1000 | Loss: 0.00003708
Iteration 151/1000 | Loss: 0.00003707
Iteration 152/1000 | Loss: 0.00003707
Iteration 153/1000 | Loss: 0.00003707
Iteration 154/1000 | Loss: 0.00003707
Iteration 155/1000 | Loss: 0.00003707
Iteration 156/1000 | Loss: 0.00003707
Iteration 157/1000 | Loss: 0.00003707
Iteration 158/1000 | Loss: 0.00003707
Iteration 159/1000 | Loss: 0.00003707
Iteration 160/1000 | Loss: 0.00003707
Iteration 161/1000 | Loss: 0.00003707
Iteration 162/1000 | Loss: 0.00003707
Iteration 163/1000 | Loss: 0.00003706
Iteration 164/1000 | Loss: 0.00003706
Iteration 165/1000 | Loss: 0.00003706
Iteration 166/1000 | Loss: 0.00003706
Iteration 167/1000 | Loss: 0.00003706
Iteration 168/1000 | Loss: 0.00003706
Iteration 169/1000 | Loss: 0.00003705
Iteration 170/1000 | Loss: 0.00003705
Iteration 171/1000 | Loss: 0.00003705
Iteration 172/1000 | Loss: 0.00003705
Iteration 173/1000 | Loss: 0.00003705
Iteration 174/1000 | Loss: 0.00003703
Iteration 175/1000 | Loss: 0.00003703
Iteration 176/1000 | Loss: 0.00003703
Iteration 177/1000 | Loss: 0.00003703
Iteration 178/1000 | Loss: 0.00003703
Iteration 179/1000 | Loss: 0.00003702
Iteration 180/1000 | Loss: 0.00003702
Iteration 181/1000 | Loss: 0.00003701
Iteration 182/1000 | Loss: 0.00003701
Iteration 183/1000 | Loss: 0.00003701
Iteration 184/1000 | Loss: 0.00003701
Iteration 185/1000 | Loss: 0.00003700
Iteration 186/1000 | Loss: 0.00003700
Iteration 187/1000 | Loss: 0.00003700
Iteration 188/1000 | Loss: 0.00003697
Iteration 189/1000 | Loss: 0.00003696
Iteration 190/1000 | Loss: 0.00003695
Iteration 191/1000 | Loss: 0.00003695
Iteration 192/1000 | Loss: 0.00003695
Iteration 193/1000 | Loss: 0.00003694
Iteration 194/1000 | Loss: 0.00003694
Iteration 195/1000 | Loss: 0.00003694
Iteration 196/1000 | Loss: 0.00003694
Iteration 197/1000 | Loss: 0.00003693
Iteration 198/1000 | Loss: 0.00003693
Iteration 199/1000 | Loss: 0.00003693
Iteration 200/1000 | Loss: 0.00003693
Iteration 201/1000 | Loss: 0.00003693
Iteration 202/1000 | Loss: 0.00003692
Iteration 203/1000 | Loss: 0.00003692
Iteration 204/1000 | Loss: 0.00003692
Iteration 205/1000 | Loss: 0.00003692
Iteration 206/1000 | Loss: 0.00003691
Iteration 207/1000 | Loss: 0.00003691
Iteration 208/1000 | Loss: 0.00003691
Iteration 209/1000 | Loss: 0.00003691
Iteration 210/1000 | Loss: 0.00003691
Iteration 211/1000 | Loss: 0.00003691
Iteration 212/1000 | Loss: 0.00003691
Iteration 213/1000 | Loss: 0.00003691
Iteration 214/1000 | Loss: 0.00003690
Iteration 215/1000 | Loss: 0.00003690
Iteration 216/1000 | Loss: 0.00003690
Iteration 217/1000 | Loss: 0.00003690
Iteration 218/1000 | Loss: 0.00003689
Iteration 219/1000 | Loss: 0.00003689
Iteration 220/1000 | Loss: 0.00003689
Iteration 221/1000 | Loss: 0.00003689
Iteration 222/1000 | Loss: 0.00003689
Iteration 223/1000 | Loss: 0.00003689
Iteration 224/1000 | Loss: 0.00003689
Iteration 225/1000 | Loss: 0.00003689
Iteration 226/1000 | Loss: 0.00003688
Iteration 227/1000 | Loss: 0.00003688
Iteration 228/1000 | Loss: 0.00003688
Iteration 229/1000 | Loss: 0.00003688
Iteration 230/1000 | Loss: 0.00003688
Iteration 231/1000 | Loss: 0.00003688
Iteration 232/1000 | Loss: 0.00003687
Iteration 233/1000 | Loss: 0.00003687
Iteration 234/1000 | Loss: 0.00003687
Iteration 235/1000 | Loss: 0.00003687
Iteration 236/1000 | Loss: 0.00003687
Iteration 237/1000 | Loss: 0.00003687
Iteration 238/1000 | Loss: 0.00003687
Iteration 239/1000 | Loss: 0.00003687
Iteration 240/1000 | Loss: 0.00003686
Iteration 241/1000 | Loss: 0.00003686
Iteration 242/1000 | Loss: 0.00003686
Iteration 243/1000 | Loss: 0.00003686
Iteration 244/1000 | Loss: 0.00003685
Iteration 245/1000 | Loss: 0.00003685
Iteration 246/1000 | Loss: 0.00003685
Iteration 247/1000 | Loss: 0.00003684
Iteration 248/1000 | Loss: 0.00003684
Iteration 249/1000 | Loss: 0.00003684
Iteration 250/1000 | Loss: 0.00003683
Iteration 251/1000 | Loss: 0.00003683
Iteration 252/1000 | Loss: 0.00003683
Iteration 253/1000 | Loss: 0.00003683
Iteration 254/1000 | Loss: 0.00003683
Iteration 255/1000 | Loss: 0.00003682
Iteration 256/1000 | Loss: 0.00003682
Iteration 257/1000 | Loss: 0.00003682
Iteration 258/1000 | Loss: 0.00003682
Iteration 259/1000 | Loss: 0.00003682
Iteration 260/1000 | Loss: 0.00003681
Iteration 261/1000 | Loss: 0.00003681
Iteration 262/1000 | Loss: 0.00003681
Iteration 263/1000 | Loss: 0.00003681
Iteration 264/1000 | Loss: 0.00003681
Iteration 265/1000 | Loss: 0.00003681
Iteration 266/1000 | Loss: 0.00003680
Iteration 267/1000 | Loss: 0.00003680
Iteration 268/1000 | Loss: 0.00003680
Iteration 269/1000 | Loss: 0.00003680
Iteration 270/1000 | Loss: 0.00003680
Iteration 271/1000 | Loss: 0.00003680
Iteration 272/1000 | Loss: 0.00003680
Iteration 273/1000 | Loss: 0.00003680
Iteration 274/1000 | Loss: 0.00003679
Iteration 275/1000 | Loss: 0.00003679
Iteration 276/1000 | Loss: 0.00003679
Iteration 277/1000 | Loss: 0.00003679
Iteration 278/1000 | Loss: 0.00003679
Iteration 279/1000 | Loss: 0.00003679
Iteration 280/1000 | Loss: 0.00003678
Iteration 281/1000 | Loss: 0.00003678
Iteration 282/1000 | Loss: 0.00003678
Iteration 283/1000 | Loss: 0.00003678
Iteration 284/1000 | Loss: 0.00003678
Iteration 285/1000 | Loss: 0.00003677
Iteration 286/1000 | Loss: 0.00003677
Iteration 287/1000 | Loss: 0.00003676
Iteration 288/1000 | Loss: 0.00003676
Iteration 289/1000 | Loss: 0.00003675
Iteration 290/1000 | Loss: 0.00003675
Iteration 291/1000 | Loss: 0.00003675
Iteration 292/1000 | Loss: 0.00003675
Iteration 293/1000 | Loss: 0.00003674
Iteration 294/1000 | Loss: 0.00003674
Iteration 295/1000 | Loss: 0.00003674
Iteration 296/1000 | Loss: 0.00003673
Iteration 297/1000 | Loss: 0.00003673
Iteration 298/1000 | Loss: 0.00003673
Iteration 299/1000 | Loss: 0.00003672
Iteration 300/1000 | Loss: 0.00003671
Iteration 301/1000 | Loss: 0.00003671
Iteration 302/1000 | Loss: 0.00003671
Iteration 303/1000 | Loss: 0.00003671
Iteration 304/1000 | Loss: 0.00003671
Iteration 305/1000 | Loss: 0.00003671
Iteration 306/1000 | Loss: 0.00003670
Iteration 307/1000 | Loss: 0.00003670
Iteration 308/1000 | Loss: 0.00003670
Iteration 309/1000 | Loss: 0.00003670
Iteration 310/1000 | Loss: 0.00003670
Iteration 311/1000 | Loss: 0.00003670
Iteration 312/1000 | Loss: 0.00003670
Iteration 313/1000 | Loss: 0.00003669
Iteration 314/1000 | Loss: 0.00003669
Iteration 315/1000 | Loss: 0.00003669
Iteration 316/1000 | Loss: 0.00003669
Iteration 317/1000 | Loss: 0.00003668
Iteration 318/1000 | Loss: 0.00003668
Iteration 319/1000 | Loss: 0.00003668
Iteration 320/1000 | Loss: 0.00003668
Iteration 321/1000 | Loss: 0.00003668
Iteration 322/1000 | Loss: 0.00003668
Iteration 323/1000 | Loss: 0.00003667
Iteration 324/1000 | Loss: 0.00003667
Iteration 325/1000 | Loss: 0.00003667
Iteration 326/1000 | Loss: 0.00003667
Iteration 327/1000 | Loss: 0.00003667
Iteration 328/1000 | Loss: 0.00003667
Iteration 329/1000 | Loss: 0.00003667
Iteration 330/1000 | Loss: 0.00003667
Iteration 331/1000 | Loss: 0.00003666
Iteration 332/1000 | Loss: 0.00003666
Iteration 333/1000 | Loss: 0.00003666
Iteration 334/1000 | Loss: 0.00003666
Iteration 335/1000 | Loss: 0.00003666
Iteration 336/1000 | Loss: 0.00003666
Iteration 337/1000 | Loss: 0.00003666
Iteration 338/1000 | Loss: 0.00003666
Iteration 339/1000 | Loss: 0.00003666
Iteration 340/1000 | Loss: 0.00003666
Iteration 341/1000 | Loss: 0.00003666
Iteration 342/1000 | Loss: 0.00003665
Iteration 343/1000 | Loss: 0.00003665
Iteration 344/1000 | Loss: 0.00003665
Iteration 345/1000 | Loss: 0.00003665
Iteration 346/1000 | Loss: 0.00003665
Iteration 347/1000 | Loss: 0.00003665
Iteration 348/1000 | Loss: 0.00003665
Iteration 349/1000 | Loss: 0.00003665
Iteration 350/1000 | Loss: 0.00003665
Iteration 351/1000 | Loss: 0.00003665
Iteration 352/1000 | Loss: 0.00003665
Iteration 353/1000 | Loss: 0.00003665
Iteration 354/1000 | Loss: 0.00003665
Iteration 355/1000 | Loss: 0.00003665
Iteration 356/1000 | Loss: 0.00003665
Iteration 357/1000 | Loss: 0.00003664
Iteration 358/1000 | Loss: 0.00003664
Iteration 359/1000 | Loss: 0.00003664
Iteration 360/1000 | Loss: 0.00003664
Iteration 361/1000 | Loss: 0.00003664
Iteration 362/1000 | Loss: 0.00003664
Iteration 363/1000 | Loss: 0.00003664
Iteration 364/1000 | Loss: 0.00003664
Iteration 365/1000 | Loss: 0.00003664
Iteration 366/1000 | Loss: 0.00003664
Iteration 367/1000 | Loss: 0.00003664
Iteration 368/1000 | Loss: 0.00003664
Iteration 369/1000 | Loss: 0.00003664
Iteration 370/1000 | Loss: 0.00003664
Iteration 371/1000 | Loss: 0.00003663
Iteration 372/1000 | Loss: 0.00003663
Iteration 373/1000 | Loss: 0.00003663
Iteration 374/1000 | Loss: 0.00003663
Iteration 375/1000 | Loss: 0.00003663
Iteration 376/1000 | Loss: 0.00003663
Iteration 377/1000 | Loss: 0.00003663
Iteration 378/1000 | Loss: 0.00003663
Iteration 379/1000 | Loss: 0.00003663
Iteration 380/1000 | Loss: 0.00003663
Iteration 381/1000 | Loss: 0.00003663
Iteration 382/1000 | Loss: 0.00003663
Iteration 383/1000 | Loss: 0.00003663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 383. Stopping optimization.
Last 5 losses: [3.663252937258221e-05, 3.663252937258221e-05, 3.663252937258221e-05, 3.663252937258221e-05, 3.663252937258221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.663252937258221e-05

Optimization complete. Final v2v error: 4.910465240478516 mm

Highest mean error: 6.24023962020874 mm for frame 207

Lowest mean error: 3.7181944847106934 mm for frame 21

Saving results

Total time: 196.10186433792114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708566
Iteration 2/25 | Loss: 0.00143779
Iteration 3/25 | Loss: 0.00128501
Iteration 4/25 | Loss: 0.00126846
Iteration 5/25 | Loss: 0.00126464
Iteration 6/25 | Loss: 0.00126464
Iteration 7/25 | Loss: 0.00126464
Iteration 8/25 | Loss: 0.00126464
Iteration 9/25 | Loss: 0.00126464
Iteration 10/25 | Loss: 0.00126464
Iteration 11/25 | Loss: 0.00126464
Iteration 12/25 | Loss: 0.00126464
Iteration 13/25 | Loss: 0.00126464
Iteration 14/25 | Loss: 0.00126464
Iteration 15/25 | Loss: 0.00126464
Iteration 16/25 | Loss: 0.00126464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001264636986888945, 0.001264636986888945, 0.001264636986888945, 0.001264636986888945, 0.001264636986888945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001264636986888945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.00859785
Iteration 2/25 | Loss: 0.00105362
Iteration 3/25 | Loss: 0.00105360
Iteration 4/25 | Loss: 0.00105360
Iteration 5/25 | Loss: 0.00105360
Iteration 6/25 | Loss: 0.00105360
Iteration 7/25 | Loss: 0.00105360
Iteration 8/25 | Loss: 0.00105360
Iteration 9/25 | Loss: 0.00105360
Iteration 10/25 | Loss: 0.00105360
Iteration 11/25 | Loss: 0.00105360
Iteration 12/25 | Loss: 0.00105360
Iteration 13/25 | Loss: 0.00105360
Iteration 14/25 | Loss: 0.00105360
Iteration 15/25 | Loss: 0.00105360
Iteration 16/25 | Loss: 0.00105360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001053595100529492, 0.001053595100529492, 0.001053595100529492, 0.001053595100529492, 0.001053595100529492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001053595100529492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105360
Iteration 2/1000 | Loss: 0.00004046
Iteration 3/1000 | Loss: 0.00002921
Iteration 4/1000 | Loss: 0.00002653
Iteration 5/1000 | Loss: 0.00002491
Iteration 6/1000 | Loss: 0.00002388
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002312
Iteration 9/1000 | Loss: 0.00002282
Iteration 10/1000 | Loss: 0.00002261
Iteration 11/1000 | Loss: 0.00002256
Iteration 12/1000 | Loss: 0.00002241
Iteration 13/1000 | Loss: 0.00002226
Iteration 14/1000 | Loss: 0.00002216
Iteration 15/1000 | Loss: 0.00002205
Iteration 16/1000 | Loss: 0.00002204
Iteration 17/1000 | Loss: 0.00002204
Iteration 18/1000 | Loss: 0.00002204
Iteration 19/1000 | Loss: 0.00002204
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002201
Iteration 22/1000 | Loss: 0.00002201
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00002200
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002200
Iteration 27/1000 | Loss: 0.00002200
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002200
Iteration 30/1000 | Loss: 0.00002199
Iteration 31/1000 | Loss: 0.00002199
Iteration 32/1000 | Loss: 0.00002199
Iteration 33/1000 | Loss: 0.00002198
Iteration 34/1000 | Loss: 0.00002198
Iteration 35/1000 | Loss: 0.00002198
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002197
Iteration 38/1000 | Loss: 0.00002197
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002196
Iteration 42/1000 | Loss: 0.00002196
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002195
Iteration 47/1000 | Loss: 0.00002195
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002193
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002193
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002192
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002190
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002189
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002189
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002186
Iteration 74/1000 | Loss: 0.00002186
Iteration 75/1000 | Loss: 0.00002185
Iteration 76/1000 | Loss: 0.00002185
Iteration 77/1000 | Loss: 0.00002185
Iteration 78/1000 | Loss: 0.00002184
Iteration 79/1000 | Loss: 0.00002184
Iteration 80/1000 | Loss: 0.00002184
Iteration 81/1000 | Loss: 0.00002184
Iteration 82/1000 | Loss: 0.00002183
Iteration 83/1000 | Loss: 0.00002181
Iteration 84/1000 | Loss: 0.00002180
Iteration 85/1000 | Loss: 0.00002179
Iteration 86/1000 | Loss: 0.00002179
Iteration 87/1000 | Loss: 0.00002178
Iteration 88/1000 | Loss: 0.00002178
Iteration 89/1000 | Loss: 0.00002177
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002175
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002169
Iteration 98/1000 | Loss: 0.00002169
Iteration 99/1000 | Loss: 0.00002169
Iteration 100/1000 | Loss: 0.00002169
Iteration 101/1000 | Loss: 0.00002169
Iteration 102/1000 | Loss: 0.00002169
Iteration 103/1000 | Loss: 0.00002169
Iteration 104/1000 | Loss: 0.00002169
Iteration 105/1000 | Loss: 0.00002169
Iteration 106/1000 | Loss: 0.00002168
Iteration 107/1000 | Loss: 0.00002168
Iteration 108/1000 | Loss: 0.00002166
Iteration 109/1000 | Loss: 0.00002166
Iteration 110/1000 | Loss: 0.00002166
Iteration 111/1000 | Loss: 0.00002166
Iteration 112/1000 | Loss: 0.00002166
Iteration 113/1000 | Loss: 0.00002166
Iteration 114/1000 | Loss: 0.00002166
Iteration 115/1000 | Loss: 0.00002166
Iteration 116/1000 | Loss: 0.00002166
Iteration 117/1000 | Loss: 0.00002166
Iteration 118/1000 | Loss: 0.00002165
Iteration 119/1000 | Loss: 0.00002165
Iteration 120/1000 | Loss: 0.00002165
Iteration 121/1000 | Loss: 0.00002164
Iteration 122/1000 | Loss: 0.00002164
Iteration 123/1000 | Loss: 0.00002164
Iteration 124/1000 | Loss: 0.00002163
Iteration 125/1000 | Loss: 0.00002163
Iteration 126/1000 | Loss: 0.00002163
Iteration 127/1000 | Loss: 0.00002163
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002162
Iteration 132/1000 | Loss: 0.00002162
Iteration 133/1000 | Loss: 0.00002162
Iteration 134/1000 | Loss: 0.00002162
Iteration 135/1000 | Loss: 0.00002162
Iteration 136/1000 | Loss: 0.00002162
Iteration 137/1000 | Loss: 0.00002162
Iteration 138/1000 | Loss: 0.00002162
Iteration 139/1000 | Loss: 0.00002162
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002161
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00002160
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00002160
Iteration 153/1000 | Loss: 0.00002160
Iteration 154/1000 | Loss: 0.00002159
Iteration 155/1000 | Loss: 0.00002159
Iteration 156/1000 | Loss: 0.00002159
Iteration 157/1000 | Loss: 0.00002159
Iteration 158/1000 | Loss: 0.00002159
Iteration 159/1000 | Loss: 0.00002159
Iteration 160/1000 | Loss: 0.00002159
Iteration 161/1000 | Loss: 0.00002159
Iteration 162/1000 | Loss: 0.00002158
Iteration 163/1000 | Loss: 0.00002158
Iteration 164/1000 | Loss: 0.00002158
Iteration 165/1000 | Loss: 0.00002158
Iteration 166/1000 | Loss: 0.00002158
Iteration 167/1000 | Loss: 0.00002158
Iteration 168/1000 | Loss: 0.00002158
Iteration 169/1000 | Loss: 0.00002158
Iteration 170/1000 | Loss: 0.00002158
Iteration 171/1000 | Loss: 0.00002158
Iteration 172/1000 | Loss: 0.00002158
Iteration 173/1000 | Loss: 0.00002157
Iteration 174/1000 | Loss: 0.00002157
Iteration 175/1000 | Loss: 0.00002157
Iteration 176/1000 | Loss: 0.00002157
Iteration 177/1000 | Loss: 0.00002157
Iteration 178/1000 | Loss: 0.00002157
Iteration 179/1000 | Loss: 0.00002157
Iteration 180/1000 | Loss: 0.00002157
Iteration 181/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.157234303012956e-05, 2.157234303012956e-05, 2.157234303012956e-05, 2.157234303012956e-05, 2.157234303012956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.157234303012956e-05

Optimization complete. Final v2v error: 3.8604466915130615 mm

Highest mean error: 4.242262840270996 mm for frame 27

Lowest mean error: 3.3987655639648438 mm for frame 239

Saving results

Total time: 46.3142466545105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901118
Iteration 2/25 | Loss: 0.00175439
Iteration 3/25 | Loss: 0.00142664
Iteration 4/25 | Loss: 0.00136456
Iteration 5/25 | Loss: 0.00136766
Iteration 6/25 | Loss: 0.00135498
Iteration 7/25 | Loss: 0.00133984
Iteration 8/25 | Loss: 0.00134597
Iteration 9/25 | Loss: 0.00133638
Iteration 10/25 | Loss: 0.00133191
Iteration 11/25 | Loss: 0.00133936
Iteration 12/25 | Loss: 0.00133865
Iteration 13/25 | Loss: 0.00131654
Iteration 14/25 | Loss: 0.00131559
Iteration 15/25 | Loss: 0.00131197
Iteration 16/25 | Loss: 0.00130621
Iteration 17/25 | Loss: 0.00130535
Iteration 18/25 | Loss: 0.00130668
Iteration 19/25 | Loss: 0.00130208
Iteration 20/25 | Loss: 0.00130262
Iteration 21/25 | Loss: 0.00130722
Iteration 22/25 | Loss: 0.00130215
Iteration 23/25 | Loss: 0.00130400
Iteration 24/25 | Loss: 0.00130383
Iteration 25/25 | Loss: 0.00129886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31072664
Iteration 2/25 | Loss: 0.00123949
Iteration 3/25 | Loss: 0.00123924
Iteration 4/25 | Loss: 0.00123924
Iteration 5/25 | Loss: 0.00123924
Iteration 6/25 | Loss: 0.00123923
Iteration 7/25 | Loss: 0.00123923
Iteration 8/25 | Loss: 0.00123923
Iteration 9/25 | Loss: 0.00123923
Iteration 10/25 | Loss: 0.00123923
Iteration 11/25 | Loss: 0.00123923
Iteration 12/25 | Loss: 0.00123923
Iteration 13/25 | Loss: 0.00123923
Iteration 14/25 | Loss: 0.00123923
Iteration 15/25 | Loss: 0.00123923
Iteration 16/25 | Loss: 0.00123923
Iteration 17/25 | Loss: 0.00123923
Iteration 18/25 | Loss: 0.00123923
Iteration 19/25 | Loss: 0.00123923
Iteration 20/25 | Loss: 0.00123923
Iteration 21/25 | Loss: 0.00123923
Iteration 22/25 | Loss: 0.00123923
Iteration 23/25 | Loss: 0.00123923
Iteration 24/25 | Loss: 0.00123923
Iteration 25/25 | Loss: 0.00123923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123923
Iteration 2/1000 | Loss: 0.00101667
Iteration 3/1000 | Loss: 0.01070270
Iteration 4/1000 | Loss: 0.00022554
Iteration 5/1000 | Loss: 0.00392662
Iteration 6/1000 | Loss: 0.00453107
Iteration 7/1000 | Loss: 0.00272574
Iteration 8/1000 | Loss: 0.00034101
Iteration 9/1000 | Loss: 0.00026575
Iteration 10/1000 | Loss: 0.00053775
Iteration 11/1000 | Loss: 0.00021969
Iteration 12/1000 | Loss: 0.00038583
Iteration 13/1000 | Loss: 0.00027938
Iteration 14/1000 | Loss: 0.00036291
Iteration 15/1000 | Loss: 0.00043747
Iteration 16/1000 | Loss: 0.00031304
Iteration 17/1000 | Loss: 0.00029293
Iteration 18/1000 | Loss: 0.00031663
Iteration 19/1000 | Loss: 0.00030847
Iteration 20/1000 | Loss: 0.00025509
Iteration 21/1000 | Loss: 0.00010245
Iteration 22/1000 | Loss: 0.00022968
Iteration 23/1000 | Loss: 0.00022381
Iteration 24/1000 | Loss: 0.00028128
Iteration 25/1000 | Loss: 0.00025829
Iteration 26/1000 | Loss: 0.00030222
Iteration 27/1000 | Loss: 0.00025744
Iteration 28/1000 | Loss: 0.00021768
Iteration 29/1000 | Loss: 0.00026905
Iteration 30/1000 | Loss: 0.00022009
Iteration 31/1000 | Loss: 0.00027404
Iteration 32/1000 | Loss: 0.00093516
Iteration 33/1000 | Loss: 0.00089103
Iteration 34/1000 | Loss: 0.00100966
Iteration 35/1000 | Loss: 0.00148294
Iteration 36/1000 | Loss: 0.00080512
Iteration 37/1000 | Loss: 0.00062205
Iteration 38/1000 | Loss: 0.00047588
Iteration 39/1000 | Loss: 0.00063321
Iteration 40/1000 | Loss: 0.00013765
Iteration 41/1000 | Loss: 0.00020783
Iteration 42/1000 | Loss: 0.00022853
Iteration 43/1000 | Loss: 0.00084854
Iteration 44/1000 | Loss: 0.00031905
Iteration 45/1000 | Loss: 0.00039796
Iteration 46/1000 | Loss: 0.00032702
Iteration 47/1000 | Loss: 0.00036298
Iteration 48/1000 | Loss: 0.00035746
Iteration 49/1000 | Loss: 0.00038879
Iteration 50/1000 | Loss: 0.00037916
Iteration 51/1000 | Loss: 0.00038223
Iteration 52/1000 | Loss: 0.00033542
Iteration 53/1000 | Loss: 0.00032217
Iteration 54/1000 | Loss: 0.00048321
Iteration 55/1000 | Loss: 0.00011782
Iteration 56/1000 | Loss: 0.00021892
Iteration 57/1000 | Loss: 0.00064533
Iteration 58/1000 | Loss: 0.00017172
Iteration 59/1000 | Loss: 0.00013149
Iteration 60/1000 | Loss: 0.00054969
Iteration 61/1000 | Loss: 0.00010741
Iteration 62/1000 | Loss: 0.00008794
Iteration 63/1000 | Loss: 0.00008158
Iteration 64/1000 | Loss: 0.00006584
Iteration 65/1000 | Loss: 0.00005966
Iteration 66/1000 | Loss: 0.00006365
Iteration 67/1000 | Loss: 0.00020241
Iteration 68/1000 | Loss: 0.00004374
Iteration 69/1000 | Loss: 0.00003810
Iteration 70/1000 | Loss: 0.00007789
Iteration 71/1000 | Loss: 0.00007245
Iteration 72/1000 | Loss: 0.00007056
Iteration 73/1000 | Loss: 0.00006730
Iteration 74/1000 | Loss: 0.00003089
Iteration 75/1000 | Loss: 0.00005705
Iteration 76/1000 | Loss: 0.00007461
Iteration 77/1000 | Loss: 0.00007069
Iteration 78/1000 | Loss: 0.00007825
Iteration 79/1000 | Loss: 0.00025172
Iteration 80/1000 | Loss: 0.00017710
Iteration 81/1000 | Loss: 0.00013996
Iteration 82/1000 | Loss: 0.00009667
Iteration 83/1000 | Loss: 0.00004387
Iteration 84/1000 | Loss: 0.00019743
Iteration 85/1000 | Loss: 0.00009069
Iteration 86/1000 | Loss: 0.00004166
Iteration 87/1000 | Loss: 0.00054500
Iteration 88/1000 | Loss: 0.00045661
Iteration 89/1000 | Loss: 0.00011735
Iteration 90/1000 | Loss: 0.00060345
Iteration 91/1000 | Loss: 0.00013809
Iteration 92/1000 | Loss: 0.00007127
Iteration 93/1000 | Loss: 0.00066579
Iteration 94/1000 | Loss: 0.00008720
Iteration 95/1000 | Loss: 0.00008632
Iteration 96/1000 | Loss: 0.00010204
Iteration 97/1000 | Loss: 0.00003782
Iteration 98/1000 | Loss: 0.00004144
Iteration 99/1000 | Loss: 0.00007257
Iteration 100/1000 | Loss: 0.00010691
Iteration 101/1000 | Loss: 0.00004327
Iteration 102/1000 | Loss: 0.00007350
Iteration 103/1000 | Loss: 0.00010128
Iteration 104/1000 | Loss: 0.00004327
Iteration 105/1000 | Loss: 0.00007732
Iteration 106/1000 | Loss: 0.00010181
Iteration 107/1000 | Loss: 0.00026144
Iteration 108/1000 | Loss: 0.00008099
Iteration 109/1000 | Loss: 0.00016787
Iteration 110/1000 | Loss: 0.00019943
Iteration 111/1000 | Loss: 0.00010610
Iteration 112/1000 | Loss: 0.00030994
Iteration 113/1000 | Loss: 0.00021143
Iteration 114/1000 | Loss: 0.00014520
Iteration 115/1000 | Loss: 0.00019488
Iteration 116/1000 | Loss: 0.00020591
Iteration 117/1000 | Loss: 0.00016232
Iteration 118/1000 | Loss: 0.00008301
Iteration 119/1000 | Loss: 0.00020505
Iteration 120/1000 | Loss: 0.00006555
Iteration 121/1000 | Loss: 0.00017122
Iteration 122/1000 | Loss: 0.00019472
Iteration 123/1000 | Loss: 0.00018933
Iteration 124/1000 | Loss: 0.00023221
Iteration 125/1000 | Loss: 0.00020029
Iteration 126/1000 | Loss: 0.00018378
Iteration 127/1000 | Loss: 0.00021780
Iteration 128/1000 | Loss: 0.00026441
Iteration 129/1000 | Loss: 0.00016063
Iteration 130/1000 | Loss: 0.00016735
Iteration 131/1000 | Loss: 0.00017703
Iteration 132/1000 | Loss: 0.00019277
Iteration 133/1000 | Loss: 0.00021616
Iteration 134/1000 | Loss: 0.00017768
Iteration 135/1000 | Loss: 0.00019643
Iteration 136/1000 | Loss: 0.00015927
Iteration 137/1000 | Loss: 0.00020736
Iteration 138/1000 | Loss: 0.00015294
Iteration 139/1000 | Loss: 0.00006317
Iteration 140/1000 | Loss: 0.00003219
Iteration 141/1000 | Loss: 0.00003576
Iteration 142/1000 | Loss: 0.00003444
Iteration 143/1000 | Loss: 0.00003913
Iteration 144/1000 | Loss: 0.00004112
Iteration 145/1000 | Loss: 0.00003942
Iteration 146/1000 | Loss: 0.00004227
Iteration 147/1000 | Loss: 0.00003908
Iteration 148/1000 | Loss: 0.00004201
Iteration 149/1000 | Loss: 0.00003656
Iteration 150/1000 | Loss: 0.00003560
Iteration 151/1000 | Loss: 0.00003918
Iteration 152/1000 | Loss: 0.00003405
Iteration 153/1000 | Loss: 0.00002777
Iteration 154/1000 | Loss: 0.00003296
Iteration 155/1000 | Loss: 0.00003880
Iteration 156/1000 | Loss: 0.00003877
Iteration 157/1000 | Loss: 0.00003271
Iteration 158/1000 | Loss: 0.00003668
Iteration 159/1000 | Loss: 0.00003335
Iteration 160/1000 | Loss: 0.00003663
Iteration 161/1000 | Loss: 0.00003361
Iteration 162/1000 | Loss: 0.00004214
Iteration 163/1000 | Loss: 0.00003855
Iteration 164/1000 | Loss: 0.00003715
Iteration 165/1000 | Loss: 0.00003324
Iteration 166/1000 | Loss: 0.00003022
Iteration 167/1000 | Loss: 0.00002907
Iteration 168/1000 | Loss: 0.00003617
Iteration 169/1000 | Loss: 0.00004217
Iteration 170/1000 | Loss: 0.00003538
Iteration 171/1000 | Loss: 0.00003810
Iteration 172/1000 | Loss: 0.00004174
Iteration 173/1000 | Loss: 0.00003568
Iteration 174/1000 | Loss: 0.00003369
Iteration 175/1000 | Loss: 0.00003655
Iteration 176/1000 | Loss: 0.00004056
Iteration 177/1000 | Loss: 0.00003268
Iteration 178/1000 | Loss: 0.00003379
Iteration 179/1000 | Loss: 0.00004582
Iteration 180/1000 | Loss: 0.00003810
Iteration 181/1000 | Loss: 0.00003514
Iteration 182/1000 | Loss: 0.00003565
Iteration 183/1000 | Loss: 0.00003487
Iteration 184/1000 | Loss: 0.00003630
Iteration 185/1000 | Loss: 0.00004131
Iteration 186/1000 | Loss: 0.00004047
Iteration 187/1000 | Loss: 0.00003320
Iteration 188/1000 | Loss: 0.00003956
Iteration 189/1000 | Loss: 0.00003862
Iteration 190/1000 | Loss: 0.00004761
Iteration 191/1000 | Loss: 0.00003935
Iteration 192/1000 | Loss: 0.00004639
Iteration 193/1000 | Loss: 0.00003971
Iteration 194/1000 | Loss: 0.00003291
Iteration 195/1000 | Loss: 0.00002983
Iteration 196/1000 | Loss: 0.00004439
Iteration 197/1000 | Loss: 0.00003029
Iteration 198/1000 | Loss: 0.00002908
Iteration 199/1000 | Loss: 0.00002606
Iteration 200/1000 | Loss: 0.00002510
Iteration 201/1000 | Loss: 0.00002469
Iteration 202/1000 | Loss: 0.00002441
Iteration 203/1000 | Loss: 0.00002424
Iteration 204/1000 | Loss: 0.00002409
Iteration 205/1000 | Loss: 0.00002408
Iteration 206/1000 | Loss: 0.00002402
Iteration 207/1000 | Loss: 0.00002400
Iteration 208/1000 | Loss: 0.00002399
Iteration 209/1000 | Loss: 0.00002398
Iteration 210/1000 | Loss: 0.00002397
Iteration 211/1000 | Loss: 0.00002395
Iteration 212/1000 | Loss: 0.00002394
Iteration 213/1000 | Loss: 0.00002392
Iteration 214/1000 | Loss: 0.00002392
Iteration 215/1000 | Loss: 0.00002391
Iteration 216/1000 | Loss: 0.00002391
Iteration 217/1000 | Loss: 0.00002390
Iteration 218/1000 | Loss: 0.00002390
Iteration 219/1000 | Loss: 0.00002389
Iteration 220/1000 | Loss: 0.00002388
Iteration 221/1000 | Loss: 0.00002388
Iteration 222/1000 | Loss: 0.00002387
Iteration 223/1000 | Loss: 0.00002387
Iteration 224/1000 | Loss: 0.00002386
Iteration 225/1000 | Loss: 0.00002386
Iteration 226/1000 | Loss: 0.00002385
Iteration 227/1000 | Loss: 0.00002385
Iteration 228/1000 | Loss: 0.00002384
Iteration 229/1000 | Loss: 0.00002384
Iteration 230/1000 | Loss: 0.00002384
Iteration 231/1000 | Loss: 0.00002383
Iteration 232/1000 | Loss: 0.00002383
Iteration 233/1000 | Loss: 0.00002382
Iteration 234/1000 | Loss: 0.00002379
Iteration 235/1000 | Loss: 0.00002379
Iteration 236/1000 | Loss: 0.00002378
Iteration 237/1000 | Loss: 0.00002377
Iteration 238/1000 | Loss: 0.00002376
Iteration 239/1000 | Loss: 0.00002376
Iteration 240/1000 | Loss: 0.00002375
Iteration 241/1000 | Loss: 0.00002375
Iteration 242/1000 | Loss: 0.00002375
Iteration 243/1000 | Loss: 0.00002375
Iteration 244/1000 | Loss: 0.00002375
Iteration 245/1000 | Loss: 0.00002375
Iteration 246/1000 | Loss: 0.00002375
Iteration 247/1000 | Loss: 0.00002375
Iteration 248/1000 | Loss: 0.00002375
Iteration 249/1000 | Loss: 0.00002375
Iteration 250/1000 | Loss: 0.00002374
Iteration 251/1000 | Loss: 0.00002374
Iteration 252/1000 | Loss: 0.00002374
Iteration 253/1000 | Loss: 0.00002373
Iteration 254/1000 | Loss: 0.00002373
Iteration 255/1000 | Loss: 0.00002372
Iteration 256/1000 | Loss: 0.00002372
Iteration 257/1000 | Loss: 0.00002371
Iteration 258/1000 | Loss: 0.00002371
Iteration 259/1000 | Loss: 0.00002371
Iteration 260/1000 | Loss: 0.00002371
Iteration 261/1000 | Loss: 0.00002371
Iteration 262/1000 | Loss: 0.00002370
Iteration 263/1000 | Loss: 0.00002370
Iteration 264/1000 | Loss: 0.00002370
Iteration 265/1000 | Loss: 0.00002370
Iteration 266/1000 | Loss: 0.00002370
Iteration 267/1000 | Loss: 0.00002369
Iteration 268/1000 | Loss: 0.00002369
Iteration 269/1000 | Loss: 0.00002369
Iteration 270/1000 | Loss: 0.00002368
Iteration 271/1000 | Loss: 0.00002368
Iteration 272/1000 | Loss: 0.00002368
Iteration 273/1000 | Loss: 0.00002368
Iteration 274/1000 | Loss: 0.00002368
Iteration 275/1000 | Loss: 0.00002368
Iteration 276/1000 | Loss: 0.00002368
Iteration 277/1000 | Loss: 0.00002367
Iteration 278/1000 | Loss: 0.00002367
Iteration 279/1000 | Loss: 0.00002367
Iteration 280/1000 | Loss: 0.00002367
Iteration 281/1000 | Loss: 0.00002367
Iteration 282/1000 | Loss: 0.00002366
Iteration 283/1000 | Loss: 0.00002366
Iteration 284/1000 | Loss: 0.00002366
Iteration 285/1000 | Loss: 0.00002366
Iteration 286/1000 | Loss: 0.00002366
Iteration 287/1000 | Loss: 0.00002366
Iteration 288/1000 | Loss: 0.00002366
Iteration 289/1000 | Loss: 0.00002365
Iteration 290/1000 | Loss: 0.00002365
Iteration 291/1000 | Loss: 0.00002365
Iteration 292/1000 | Loss: 0.00002365
Iteration 293/1000 | Loss: 0.00002365
Iteration 294/1000 | Loss: 0.00002365
Iteration 295/1000 | Loss: 0.00002365
Iteration 296/1000 | Loss: 0.00002365
Iteration 297/1000 | Loss: 0.00002365
Iteration 298/1000 | Loss: 0.00002365
Iteration 299/1000 | Loss: 0.00002365
Iteration 300/1000 | Loss: 0.00002365
Iteration 301/1000 | Loss: 0.00002365
Iteration 302/1000 | Loss: 0.00002365
Iteration 303/1000 | Loss: 0.00002365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [2.3654039978282526e-05, 2.3654039978282526e-05, 2.3654039978282526e-05, 2.3654039978282526e-05, 2.3654039978282526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3654039978282526e-05

Optimization complete. Final v2v error: 4.021574974060059 mm

Highest mean error: 5.890540599822998 mm for frame 97

Lowest mean error: 3.0486419200897217 mm for frame 144

Saving results

Total time: 335.2565007209778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423342
Iteration 2/25 | Loss: 0.00129976
Iteration 3/25 | Loss: 0.00120315
Iteration 4/25 | Loss: 0.00119354
Iteration 5/25 | Loss: 0.00119140
Iteration 6/25 | Loss: 0.00119089
Iteration 7/25 | Loss: 0.00119089
Iteration 8/25 | Loss: 0.00119089
Iteration 9/25 | Loss: 0.00119089
Iteration 10/25 | Loss: 0.00119089
Iteration 11/25 | Loss: 0.00119089
Iteration 12/25 | Loss: 0.00119089
Iteration 13/25 | Loss: 0.00119089
Iteration 14/25 | Loss: 0.00119089
Iteration 15/25 | Loss: 0.00119089
Iteration 16/25 | Loss: 0.00119089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011908870656043291, 0.0011908870656043291, 0.0011908870656043291, 0.0011908870656043291, 0.0011908870656043291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011908870656043291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40327549
Iteration 2/25 | Loss: 0.00083711
Iteration 3/25 | Loss: 0.00083710
Iteration 4/25 | Loss: 0.00083710
Iteration 5/25 | Loss: 0.00083710
Iteration 6/25 | Loss: 0.00083710
Iteration 7/25 | Loss: 0.00083710
Iteration 8/25 | Loss: 0.00083710
Iteration 9/25 | Loss: 0.00083710
Iteration 10/25 | Loss: 0.00083710
Iteration 11/25 | Loss: 0.00083710
Iteration 12/25 | Loss: 0.00083710
Iteration 13/25 | Loss: 0.00083710
Iteration 14/25 | Loss: 0.00083710
Iteration 15/25 | Loss: 0.00083710
Iteration 16/25 | Loss: 0.00083710
Iteration 17/25 | Loss: 0.00083710
Iteration 18/25 | Loss: 0.00083710
Iteration 19/25 | Loss: 0.00083710
Iteration 20/25 | Loss: 0.00083710
Iteration 21/25 | Loss: 0.00083710
Iteration 22/25 | Loss: 0.00083710
Iteration 23/25 | Loss: 0.00083710
Iteration 24/25 | Loss: 0.00083710
Iteration 25/25 | Loss: 0.00083710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083710
Iteration 2/1000 | Loss: 0.00004215
Iteration 3/1000 | Loss: 0.00002219
Iteration 4/1000 | Loss: 0.00001890
Iteration 5/1000 | Loss: 0.00001794
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001616
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001592
Iteration 20/1000 | Loss: 0.00001591
Iteration 21/1000 | Loss: 0.00001590
Iteration 22/1000 | Loss: 0.00001589
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001579
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001573
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001564
Iteration 52/1000 | Loss: 0.00001564
Iteration 53/1000 | Loss: 0.00001564
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001562
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001561
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001560
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001559
Iteration 79/1000 | Loss: 0.00001559
Iteration 80/1000 | Loss: 0.00001559
Iteration 81/1000 | Loss: 0.00001559
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001559
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001558
Iteration 87/1000 | Loss: 0.00001558
Iteration 88/1000 | Loss: 0.00001558
Iteration 89/1000 | Loss: 0.00001558
Iteration 90/1000 | Loss: 0.00001558
Iteration 91/1000 | Loss: 0.00001558
Iteration 92/1000 | Loss: 0.00001558
Iteration 93/1000 | Loss: 0.00001558
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001557
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001557
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001557
Iteration 101/1000 | Loss: 0.00001557
Iteration 102/1000 | Loss: 0.00001557
Iteration 103/1000 | Loss: 0.00001557
Iteration 104/1000 | Loss: 0.00001557
Iteration 105/1000 | Loss: 0.00001557
Iteration 106/1000 | Loss: 0.00001557
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001556
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Iteration 114/1000 | Loss: 0.00001556
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001555
Iteration 125/1000 | Loss: 0.00001555
Iteration 126/1000 | Loss: 0.00001555
Iteration 127/1000 | Loss: 0.00001555
Iteration 128/1000 | Loss: 0.00001555
Iteration 129/1000 | Loss: 0.00001555
Iteration 130/1000 | Loss: 0.00001555
Iteration 131/1000 | Loss: 0.00001555
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001555
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Iteration 137/1000 | Loss: 0.00001554
Iteration 138/1000 | Loss: 0.00001554
Iteration 139/1000 | Loss: 0.00001554
Iteration 140/1000 | Loss: 0.00001554
Iteration 141/1000 | Loss: 0.00001554
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001553
Iteration 144/1000 | Loss: 0.00001553
Iteration 145/1000 | Loss: 0.00001553
Iteration 146/1000 | Loss: 0.00001553
Iteration 147/1000 | Loss: 0.00001553
Iteration 148/1000 | Loss: 0.00001553
Iteration 149/1000 | Loss: 0.00001553
Iteration 150/1000 | Loss: 0.00001553
Iteration 151/1000 | Loss: 0.00001553
Iteration 152/1000 | Loss: 0.00001553
Iteration 153/1000 | Loss: 0.00001553
Iteration 154/1000 | Loss: 0.00001553
Iteration 155/1000 | Loss: 0.00001553
Iteration 156/1000 | Loss: 0.00001553
Iteration 157/1000 | Loss: 0.00001552
Iteration 158/1000 | Loss: 0.00001552
Iteration 159/1000 | Loss: 0.00001552
Iteration 160/1000 | Loss: 0.00001552
Iteration 161/1000 | Loss: 0.00001552
Iteration 162/1000 | Loss: 0.00001552
Iteration 163/1000 | Loss: 0.00001552
Iteration 164/1000 | Loss: 0.00001552
Iteration 165/1000 | Loss: 0.00001552
Iteration 166/1000 | Loss: 0.00001552
Iteration 167/1000 | Loss: 0.00001552
Iteration 168/1000 | Loss: 0.00001552
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Iteration 171/1000 | Loss: 0.00001551
Iteration 172/1000 | Loss: 0.00001551
Iteration 173/1000 | Loss: 0.00001551
Iteration 174/1000 | Loss: 0.00001551
Iteration 175/1000 | Loss: 0.00001551
Iteration 176/1000 | Loss: 0.00001551
Iteration 177/1000 | Loss: 0.00001551
Iteration 178/1000 | Loss: 0.00001551
Iteration 179/1000 | Loss: 0.00001551
Iteration 180/1000 | Loss: 0.00001551
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001550
Iteration 184/1000 | Loss: 0.00001550
Iteration 185/1000 | Loss: 0.00001550
Iteration 186/1000 | Loss: 0.00001550
Iteration 187/1000 | Loss: 0.00001550
Iteration 188/1000 | Loss: 0.00001550
Iteration 189/1000 | Loss: 0.00001550
Iteration 190/1000 | Loss: 0.00001550
Iteration 191/1000 | Loss: 0.00001550
Iteration 192/1000 | Loss: 0.00001550
Iteration 193/1000 | Loss: 0.00001549
Iteration 194/1000 | Loss: 0.00001549
Iteration 195/1000 | Loss: 0.00001549
Iteration 196/1000 | Loss: 0.00001549
Iteration 197/1000 | Loss: 0.00001549
Iteration 198/1000 | Loss: 0.00001549
Iteration 199/1000 | Loss: 0.00001549
Iteration 200/1000 | Loss: 0.00001549
Iteration 201/1000 | Loss: 0.00001549
Iteration 202/1000 | Loss: 0.00001548
Iteration 203/1000 | Loss: 0.00001548
Iteration 204/1000 | Loss: 0.00001548
Iteration 205/1000 | Loss: 0.00001548
Iteration 206/1000 | Loss: 0.00001548
Iteration 207/1000 | Loss: 0.00001548
Iteration 208/1000 | Loss: 0.00001548
Iteration 209/1000 | Loss: 0.00001548
Iteration 210/1000 | Loss: 0.00001548
Iteration 211/1000 | Loss: 0.00001548
Iteration 212/1000 | Loss: 0.00001548
Iteration 213/1000 | Loss: 0.00001548
Iteration 214/1000 | Loss: 0.00001548
Iteration 215/1000 | Loss: 0.00001547
Iteration 216/1000 | Loss: 0.00001547
Iteration 217/1000 | Loss: 0.00001547
Iteration 218/1000 | Loss: 0.00001547
Iteration 219/1000 | Loss: 0.00001547
Iteration 220/1000 | Loss: 0.00001547
Iteration 221/1000 | Loss: 0.00001547
Iteration 222/1000 | Loss: 0.00001547
Iteration 223/1000 | Loss: 0.00001547
Iteration 224/1000 | Loss: 0.00001547
Iteration 225/1000 | Loss: 0.00001547
Iteration 226/1000 | Loss: 0.00001547
Iteration 227/1000 | Loss: 0.00001547
Iteration 228/1000 | Loss: 0.00001547
Iteration 229/1000 | Loss: 0.00001547
Iteration 230/1000 | Loss: 0.00001547
Iteration 231/1000 | Loss: 0.00001547
Iteration 232/1000 | Loss: 0.00001546
Iteration 233/1000 | Loss: 0.00001546
Iteration 234/1000 | Loss: 0.00001546
Iteration 235/1000 | Loss: 0.00001546
Iteration 236/1000 | Loss: 0.00001546
Iteration 237/1000 | Loss: 0.00001546
Iteration 238/1000 | Loss: 0.00001546
Iteration 239/1000 | Loss: 0.00001546
Iteration 240/1000 | Loss: 0.00001546
Iteration 241/1000 | Loss: 0.00001546
Iteration 242/1000 | Loss: 0.00001546
Iteration 243/1000 | Loss: 0.00001546
Iteration 244/1000 | Loss: 0.00001545
Iteration 245/1000 | Loss: 0.00001545
Iteration 246/1000 | Loss: 0.00001545
Iteration 247/1000 | Loss: 0.00001545
Iteration 248/1000 | Loss: 0.00001545
Iteration 249/1000 | Loss: 0.00001545
Iteration 250/1000 | Loss: 0.00001545
Iteration 251/1000 | Loss: 0.00001545
Iteration 252/1000 | Loss: 0.00001545
Iteration 253/1000 | Loss: 0.00001545
Iteration 254/1000 | Loss: 0.00001545
Iteration 255/1000 | Loss: 0.00001544
Iteration 256/1000 | Loss: 0.00001544
Iteration 257/1000 | Loss: 0.00001544
Iteration 258/1000 | Loss: 0.00001544
Iteration 259/1000 | Loss: 0.00001544
Iteration 260/1000 | Loss: 0.00001544
Iteration 261/1000 | Loss: 0.00001544
Iteration 262/1000 | Loss: 0.00001544
Iteration 263/1000 | Loss: 0.00001544
Iteration 264/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.5443371012224816e-05, 1.5443371012224816e-05, 1.5443371012224816e-05, 1.5443371012224816e-05, 1.5443371012224816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5443371012224816e-05

Optimization complete. Final v2v error: 3.192795991897583 mm

Highest mean error: 4.5159220695495605 mm for frame 38

Lowest mean error: 2.77547550201416 mm for frame 83

Saving results

Total time: 41.671377182006836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647913
Iteration 2/25 | Loss: 0.00129201
Iteration 3/25 | Loss: 0.00119104
Iteration 4/25 | Loss: 0.00117898
Iteration 5/25 | Loss: 0.00117567
Iteration 6/25 | Loss: 0.00117547
Iteration 7/25 | Loss: 0.00117547
Iteration 8/25 | Loss: 0.00117547
Iteration 9/25 | Loss: 0.00117547
Iteration 10/25 | Loss: 0.00117547
Iteration 11/25 | Loss: 0.00117547
Iteration 12/25 | Loss: 0.00117547
Iteration 13/25 | Loss: 0.00117547
Iteration 14/25 | Loss: 0.00117547
Iteration 15/25 | Loss: 0.00117547
Iteration 16/25 | Loss: 0.00117547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011754728620871902, 0.0011754728620871902, 0.0011754728620871902, 0.0011754728620871902, 0.0011754728620871902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011754728620871902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.08251190
Iteration 2/25 | Loss: 0.00083237
Iteration 3/25 | Loss: 0.00083236
Iteration 4/25 | Loss: 0.00083236
Iteration 5/25 | Loss: 0.00083236
Iteration 6/25 | Loss: 0.00083236
Iteration 7/25 | Loss: 0.00083236
Iteration 8/25 | Loss: 0.00083236
Iteration 9/25 | Loss: 0.00083236
Iteration 10/25 | Loss: 0.00083236
Iteration 11/25 | Loss: 0.00083236
Iteration 12/25 | Loss: 0.00083236
Iteration 13/25 | Loss: 0.00083236
Iteration 14/25 | Loss: 0.00083236
Iteration 15/25 | Loss: 0.00083236
Iteration 16/25 | Loss: 0.00083236
Iteration 17/25 | Loss: 0.00083236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008323597721755505, 0.0008323597721755505, 0.0008323597721755505, 0.0008323597721755505, 0.0008323597721755505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008323597721755505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083236
Iteration 2/1000 | Loss: 0.00003742
Iteration 3/1000 | Loss: 0.00002322
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001689
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001587
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001524
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001493
Iteration 25/1000 | Loss: 0.00001492
Iteration 26/1000 | Loss: 0.00001492
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001480
Iteration 37/1000 | Loss: 0.00001479
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001477
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001468
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001464
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001457
Iteration 88/1000 | Loss: 0.00001457
Iteration 89/1000 | Loss: 0.00001457
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001455
Iteration 102/1000 | Loss: 0.00001455
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001454
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001453
Iteration 115/1000 | Loss: 0.00001453
Iteration 116/1000 | Loss: 0.00001453
Iteration 117/1000 | Loss: 0.00001453
Iteration 118/1000 | Loss: 0.00001452
Iteration 119/1000 | Loss: 0.00001452
Iteration 120/1000 | Loss: 0.00001452
Iteration 121/1000 | Loss: 0.00001452
Iteration 122/1000 | Loss: 0.00001452
Iteration 123/1000 | Loss: 0.00001452
Iteration 124/1000 | Loss: 0.00001451
Iteration 125/1000 | Loss: 0.00001451
Iteration 126/1000 | Loss: 0.00001451
Iteration 127/1000 | Loss: 0.00001451
Iteration 128/1000 | Loss: 0.00001451
Iteration 129/1000 | Loss: 0.00001451
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001449
Iteration 135/1000 | Loss: 0.00001449
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001447
Iteration 149/1000 | Loss: 0.00001447
Iteration 150/1000 | Loss: 0.00001447
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001446
Iteration 155/1000 | Loss: 0.00001446
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Iteration 158/1000 | Loss: 0.00001446
Iteration 159/1000 | Loss: 0.00001445
Iteration 160/1000 | Loss: 0.00001445
Iteration 161/1000 | Loss: 0.00001445
Iteration 162/1000 | Loss: 0.00001445
Iteration 163/1000 | Loss: 0.00001445
Iteration 164/1000 | Loss: 0.00001445
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001444
Iteration 167/1000 | Loss: 0.00001444
Iteration 168/1000 | Loss: 0.00001444
Iteration 169/1000 | Loss: 0.00001444
Iteration 170/1000 | Loss: 0.00001444
Iteration 171/1000 | Loss: 0.00001444
Iteration 172/1000 | Loss: 0.00001444
Iteration 173/1000 | Loss: 0.00001444
Iteration 174/1000 | Loss: 0.00001444
Iteration 175/1000 | Loss: 0.00001444
Iteration 176/1000 | Loss: 0.00001443
Iteration 177/1000 | Loss: 0.00001443
Iteration 178/1000 | Loss: 0.00001443
Iteration 179/1000 | Loss: 0.00001443
Iteration 180/1000 | Loss: 0.00001443
Iteration 181/1000 | Loss: 0.00001443
Iteration 182/1000 | Loss: 0.00001443
Iteration 183/1000 | Loss: 0.00001443
Iteration 184/1000 | Loss: 0.00001443
Iteration 185/1000 | Loss: 0.00001443
Iteration 186/1000 | Loss: 0.00001443
Iteration 187/1000 | Loss: 0.00001443
Iteration 188/1000 | Loss: 0.00001443
Iteration 189/1000 | Loss: 0.00001443
Iteration 190/1000 | Loss: 0.00001443
Iteration 191/1000 | Loss: 0.00001443
Iteration 192/1000 | Loss: 0.00001443
Iteration 193/1000 | Loss: 0.00001443
Iteration 194/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.4430044757318683e-05, 1.4430044757318683e-05, 1.4430044757318683e-05, 1.4430044757318683e-05, 1.4430044757318683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4430044757318683e-05

Optimization complete. Final v2v error: 3.156437635421753 mm

Highest mean error: 3.6615164279937744 mm for frame 195

Lowest mean error: 2.7140562534332275 mm for frame 90

Saving results

Total time: 49.43775916099548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387142
Iteration 2/25 | Loss: 0.00120373
Iteration 3/25 | Loss: 0.00113801
Iteration 4/25 | Loss: 0.00112936
Iteration 5/25 | Loss: 0.00112860
Iteration 6/25 | Loss: 0.00112860
Iteration 7/25 | Loss: 0.00112860
Iteration 8/25 | Loss: 0.00112860
Iteration 9/25 | Loss: 0.00112860
Iteration 10/25 | Loss: 0.00112860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011286032386124134, 0.0011286032386124134, 0.0011286032386124134, 0.0011286032386124134, 0.0011286032386124134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011286032386124134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37078166
Iteration 2/25 | Loss: 0.00078827
Iteration 3/25 | Loss: 0.00078827
Iteration 4/25 | Loss: 0.00078827
Iteration 5/25 | Loss: 0.00078827
Iteration 6/25 | Loss: 0.00078827
Iteration 7/25 | Loss: 0.00078827
Iteration 8/25 | Loss: 0.00078827
Iteration 9/25 | Loss: 0.00078827
Iteration 10/25 | Loss: 0.00078827
Iteration 11/25 | Loss: 0.00078827
Iteration 12/25 | Loss: 0.00078827
Iteration 13/25 | Loss: 0.00078827
Iteration 14/25 | Loss: 0.00078827
Iteration 15/25 | Loss: 0.00078827
Iteration 16/25 | Loss: 0.00078827
Iteration 17/25 | Loss: 0.00078827
Iteration 18/25 | Loss: 0.00078827
Iteration 19/25 | Loss: 0.00078827
Iteration 20/25 | Loss: 0.00078827
Iteration 21/25 | Loss: 0.00078827
Iteration 22/25 | Loss: 0.00078827
Iteration 23/25 | Loss: 0.00078827
Iteration 24/25 | Loss: 0.00078827
Iteration 25/25 | Loss: 0.00078827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078827
Iteration 2/1000 | Loss: 0.00001662
Iteration 3/1000 | Loss: 0.00001320
Iteration 4/1000 | Loss: 0.00001235
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001144
Iteration 7/1000 | Loss: 0.00001118
Iteration 8/1000 | Loss: 0.00001087
Iteration 9/1000 | Loss: 0.00001063
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001048
Iteration 13/1000 | Loss: 0.00001047
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001015
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000988
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000976
Iteration 22/1000 | Loss: 0.00000976
Iteration 23/1000 | Loss: 0.00000975
Iteration 24/1000 | Loss: 0.00000975
Iteration 25/1000 | Loss: 0.00000975
Iteration 26/1000 | Loss: 0.00000975
Iteration 27/1000 | Loss: 0.00000974
Iteration 28/1000 | Loss: 0.00000972
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000970
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000965
Iteration 34/1000 | Loss: 0.00000965
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000964
Iteration 37/1000 | Loss: 0.00000964
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000964
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000956
Iteration 45/1000 | Loss: 0.00000956
Iteration 46/1000 | Loss: 0.00000956
Iteration 47/1000 | Loss: 0.00000956
Iteration 48/1000 | Loss: 0.00000955
Iteration 49/1000 | Loss: 0.00000955
Iteration 50/1000 | Loss: 0.00000955
Iteration 51/1000 | Loss: 0.00000953
Iteration 52/1000 | Loss: 0.00000953
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000952
Iteration 55/1000 | Loss: 0.00000951
Iteration 56/1000 | Loss: 0.00000951
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000948
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000944
Iteration 79/1000 | Loss: 0.00000943
Iteration 80/1000 | Loss: 0.00000943
Iteration 81/1000 | Loss: 0.00000942
Iteration 82/1000 | Loss: 0.00000942
Iteration 83/1000 | Loss: 0.00000942
Iteration 84/1000 | Loss: 0.00000942
Iteration 85/1000 | Loss: 0.00000942
Iteration 86/1000 | Loss: 0.00000942
Iteration 87/1000 | Loss: 0.00000942
Iteration 88/1000 | Loss: 0.00000942
Iteration 89/1000 | Loss: 0.00000942
Iteration 90/1000 | Loss: 0.00000941
Iteration 91/1000 | Loss: 0.00000941
Iteration 92/1000 | Loss: 0.00000941
Iteration 93/1000 | Loss: 0.00000940
Iteration 94/1000 | Loss: 0.00000940
Iteration 95/1000 | Loss: 0.00000940
Iteration 96/1000 | Loss: 0.00000940
Iteration 97/1000 | Loss: 0.00000940
Iteration 98/1000 | Loss: 0.00000940
Iteration 99/1000 | Loss: 0.00000940
Iteration 100/1000 | Loss: 0.00000940
Iteration 101/1000 | Loss: 0.00000940
Iteration 102/1000 | Loss: 0.00000940
Iteration 103/1000 | Loss: 0.00000940
Iteration 104/1000 | Loss: 0.00000940
Iteration 105/1000 | Loss: 0.00000940
Iteration 106/1000 | Loss: 0.00000940
Iteration 107/1000 | Loss: 0.00000940
Iteration 108/1000 | Loss: 0.00000940
Iteration 109/1000 | Loss: 0.00000940
Iteration 110/1000 | Loss: 0.00000940
Iteration 111/1000 | Loss: 0.00000940
Iteration 112/1000 | Loss: 0.00000940
Iteration 113/1000 | Loss: 0.00000940
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [9.401614988746587e-06, 9.401614988746587e-06, 9.401614988746587e-06, 9.401614988746587e-06, 9.401614988746587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.401614988746587e-06

Optimization complete. Final v2v error: 2.6617157459259033 mm

Highest mean error: 2.7018001079559326 mm for frame 37

Lowest mean error: 2.6144931316375732 mm for frame 198

Saving results

Total time: 38.34083962440491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808619
Iteration 2/25 | Loss: 0.00140326
Iteration 3/25 | Loss: 0.00118040
Iteration 4/25 | Loss: 0.00115443
Iteration 5/25 | Loss: 0.00115070
Iteration 6/25 | Loss: 0.00114977
Iteration 7/25 | Loss: 0.00114934
Iteration 8/25 | Loss: 0.00114909
Iteration 9/25 | Loss: 0.00114895
Iteration 10/25 | Loss: 0.00114878
Iteration 11/25 | Loss: 0.00115272
Iteration 12/25 | Loss: 0.00115037
Iteration 13/25 | Loss: 0.00115156
Iteration 14/25 | Loss: 0.00115175
Iteration 15/25 | Loss: 0.00114897
Iteration 16/25 | Loss: 0.00114630
Iteration 17/25 | Loss: 0.00114449
Iteration 18/25 | Loss: 0.00114384
Iteration 19/25 | Loss: 0.00114368
Iteration 20/25 | Loss: 0.00114366
Iteration 21/25 | Loss: 0.00114365
Iteration 22/25 | Loss: 0.00114365
Iteration 23/25 | Loss: 0.00114365
Iteration 24/25 | Loss: 0.00114365
Iteration 25/25 | Loss: 0.00114365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36682868
Iteration 2/25 | Loss: 0.00083094
Iteration 3/25 | Loss: 0.00083094
Iteration 4/25 | Loss: 0.00083094
Iteration 5/25 | Loss: 0.00083094
Iteration 6/25 | Loss: 0.00083094
Iteration 7/25 | Loss: 0.00083094
Iteration 8/25 | Loss: 0.00083094
Iteration 9/25 | Loss: 0.00083094
Iteration 10/25 | Loss: 0.00083094
Iteration 11/25 | Loss: 0.00083094
Iteration 12/25 | Loss: 0.00083094
Iteration 13/25 | Loss: 0.00083094
Iteration 14/25 | Loss: 0.00083094
Iteration 15/25 | Loss: 0.00083094
Iteration 16/25 | Loss: 0.00083094
Iteration 17/25 | Loss: 0.00083094
Iteration 18/25 | Loss: 0.00083094
Iteration 19/25 | Loss: 0.00083094
Iteration 20/25 | Loss: 0.00083094
Iteration 21/25 | Loss: 0.00083094
Iteration 22/25 | Loss: 0.00083094
Iteration 23/25 | Loss: 0.00083094
Iteration 24/25 | Loss: 0.00083094
Iteration 25/25 | Loss: 0.00083094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083094
Iteration 2/1000 | Loss: 0.00002395
Iteration 3/1000 | Loss: 0.00001476
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00001180
Iteration 7/1000 | Loss: 0.00001139
Iteration 8/1000 | Loss: 0.00001112
Iteration 9/1000 | Loss: 0.00001091
Iteration 10/1000 | Loss: 0.00001067
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001060
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001057
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001053
Iteration 24/1000 | Loss: 0.00001052
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001042
Iteration 34/1000 | Loss: 0.00001042
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001041
Iteration 37/1000 | Loss: 0.00001040
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001039
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001038
Iteration 42/1000 | Loss: 0.00001038
Iteration 43/1000 | Loss: 0.00001038
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001035
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001035
Iteration 52/1000 | Loss: 0.00001035
Iteration 53/1000 | Loss: 0.00001035
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001034
Iteration 59/1000 | Loss: 0.00001034
Iteration 60/1000 | Loss: 0.00001033
Iteration 61/1000 | Loss: 0.00001033
Iteration 62/1000 | Loss: 0.00001032
Iteration 63/1000 | Loss: 0.00001032
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001032
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001031
Iteration 71/1000 | Loss: 0.00001031
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001030
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001030
Iteration 85/1000 | Loss: 0.00001030
Iteration 86/1000 | Loss: 0.00001030
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001029
Iteration 90/1000 | Loss: 0.00001029
Iteration 91/1000 | Loss: 0.00001029
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001029
Iteration 99/1000 | Loss: 0.00001029
Iteration 100/1000 | Loss: 0.00001029
Iteration 101/1000 | Loss: 0.00001029
Iteration 102/1000 | Loss: 0.00001029
Iteration 103/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.028785300150048e-05, 1.028785300150048e-05, 1.028785300150048e-05, 1.028785300150048e-05, 1.028785300150048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.028785300150048e-05

Optimization complete. Final v2v error: 2.7711212635040283 mm

Highest mean error: 3.054941177368164 mm for frame 97

Lowest mean error: 2.650552749633789 mm for frame 4

Saving results

Total time: 56.238640785217285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903917
Iteration 2/25 | Loss: 0.00168020
Iteration 3/25 | Loss: 0.00131958
Iteration 4/25 | Loss: 0.00128930
Iteration 5/25 | Loss: 0.00127909
Iteration 6/25 | Loss: 0.00127691
Iteration 7/25 | Loss: 0.00127691
Iteration 8/25 | Loss: 0.00127691
Iteration 9/25 | Loss: 0.00127691
Iteration 10/25 | Loss: 0.00127691
Iteration 11/25 | Loss: 0.00127691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001276909140869975, 0.001276909140869975, 0.001276909140869975, 0.001276909140869975, 0.001276909140869975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001276909140869975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05841815
Iteration 2/25 | Loss: 0.00102774
Iteration 3/25 | Loss: 0.00102774
Iteration 4/25 | Loss: 0.00102774
Iteration 5/25 | Loss: 0.00102774
Iteration 6/25 | Loss: 0.00102774
Iteration 7/25 | Loss: 0.00102774
Iteration 8/25 | Loss: 0.00102774
Iteration 9/25 | Loss: 0.00102774
Iteration 10/25 | Loss: 0.00102774
Iteration 11/25 | Loss: 0.00102774
Iteration 12/25 | Loss: 0.00102774
Iteration 13/25 | Loss: 0.00102774
Iteration 14/25 | Loss: 0.00102774
Iteration 15/25 | Loss: 0.00102774
Iteration 16/25 | Loss: 0.00102774
Iteration 17/25 | Loss: 0.00102774
Iteration 18/25 | Loss: 0.00102774
Iteration 19/25 | Loss: 0.00102774
Iteration 20/25 | Loss: 0.00102774
Iteration 21/25 | Loss: 0.00102774
Iteration 22/25 | Loss: 0.00102774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010277365799993277, 0.0010277365799993277, 0.0010277365799993277, 0.0010277365799993277, 0.0010277365799993277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010277365799993277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102774
Iteration 2/1000 | Loss: 0.00005816
Iteration 3/1000 | Loss: 0.00004035
Iteration 4/1000 | Loss: 0.00003712
Iteration 5/1000 | Loss: 0.00003509
Iteration 6/1000 | Loss: 0.00003384
Iteration 7/1000 | Loss: 0.00003293
Iteration 8/1000 | Loss: 0.00003230
Iteration 9/1000 | Loss: 0.00003187
Iteration 10/1000 | Loss: 0.00003152
Iteration 11/1000 | Loss: 0.00003123
Iteration 12/1000 | Loss: 0.00003093
Iteration 13/1000 | Loss: 0.00003065
Iteration 14/1000 | Loss: 0.00003044
Iteration 15/1000 | Loss: 0.00003023
Iteration 16/1000 | Loss: 0.00003003
Iteration 17/1000 | Loss: 0.00002985
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002966
Iteration 20/1000 | Loss: 0.00002965
Iteration 21/1000 | Loss: 0.00002964
Iteration 22/1000 | Loss: 0.00002960
Iteration 23/1000 | Loss: 0.00002957
Iteration 24/1000 | Loss: 0.00002954
Iteration 25/1000 | Loss: 0.00002953
Iteration 26/1000 | Loss: 0.00002952
Iteration 27/1000 | Loss: 0.00002951
Iteration 28/1000 | Loss: 0.00002951
Iteration 29/1000 | Loss: 0.00002950
Iteration 30/1000 | Loss: 0.00002950
Iteration 31/1000 | Loss: 0.00002949
Iteration 32/1000 | Loss: 0.00002949
Iteration 33/1000 | Loss: 0.00002947
Iteration 34/1000 | Loss: 0.00002946
Iteration 35/1000 | Loss: 0.00002946
Iteration 36/1000 | Loss: 0.00002944
Iteration 37/1000 | Loss: 0.00002942
Iteration 38/1000 | Loss: 0.00002942
Iteration 39/1000 | Loss: 0.00002942
Iteration 40/1000 | Loss: 0.00002942
Iteration 41/1000 | Loss: 0.00002942
Iteration 42/1000 | Loss: 0.00002942
Iteration 43/1000 | Loss: 0.00002941
Iteration 44/1000 | Loss: 0.00002941
Iteration 45/1000 | Loss: 0.00002941
Iteration 46/1000 | Loss: 0.00002941
Iteration 47/1000 | Loss: 0.00002940
Iteration 48/1000 | Loss: 0.00002940
Iteration 49/1000 | Loss: 0.00002939
Iteration 50/1000 | Loss: 0.00002939
Iteration 51/1000 | Loss: 0.00002939
Iteration 52/1000 | Loss: 0.00002939
Iteration 53/1000 | Loss: 0.00002937
Iteration 54/1000 | Loss: 0.00002937
Iteration 55/1000 | Loss: 0.00002937
Iteration 56/1000 | Loss: 0.00002936
Iteration 57/1000 | Loss: 0.00002936
Iteration 58/1000 | Loss: 0.00002936
Iteration 59/1000 | Loss: 0.00002936
Iteration 60/1000 | Loss: 0.00002935
Iteration 61/1000 | Loss: 0.00002935
Iteration 62/1000 | Loss: 0.00002934
Iteration 63/1000 | Loss: 0.00002934
Iteration 64/1000 | Loss: 0.00002934
Iteration 65/1000 | Loss: 0.00002934
Iteration 66/1000 | Loss: 0.00002934
Iteration 67/1000 | Loss: 0.00002934
Iteration 68/1000 | Loss: 0.00002934
Iteration 69/1000 | Loss: 0.00002934
Iteration 70/1000 | Loss: 0.00002934
Iteration 71/1000 | Loss: 0.00002933
Iteration 72/1000 | Loss: 0.00002933
Iteration 73/1000 | Loss: 0.00002932
Iteration 74/1000 | Loss: 0.00002932
Iteration 75/1000 | Loss: 0.00002932
Iteration 76/1000 | Loss: 0.00002932
Iteration 77/1000 | Loss: 0.00002931
Iteration 78/1000 | Loss: 0.00002931
Iteration 79/1000 | Loss: 0.00002931
Iteration 80/1000 | Loss: 0.00002931
Iteration 81/1000 | Loss: 0.00002931
Iteration 82/1000 | Loss: 0.00002931
Iteration 83/1000 | Loss: 0.00002930
Iteration 84/1000 | Loss: 0.00002930
Iteration 85/1000 | Loss: 0.00002930
Iteration 86/1000 | Loss: 0.00002930
Iteration 87/1000 | Loss: 0.00002930
Iteration 88/1000 | Loss: 0.00002930
Iteration 89/1000 | Loss: 0.00002929
Iteration 90/1000 | Loss: 0.00002929
Iteration 91/1000 | Loss: 0.00002929
Iteration 92/1000 | Loss: 0.00002929
Iteration 93/1000 | Loss: 0.00002929
Iteration 94/1000 | Loss: 0.00002929
Iteration 95/1000 | Loss: 0.00002929
Iteration 96/1000 | Loss: 0.00002928
Iteration 97/1000 | Loss: 0.00002928
Iteration 98/1000 | Loss: 0.00002928
Iteration 99/1000 | Loss: 0.00002928
Iteration 100/1000 | Loss: 0.00002928
Iteration 101/1000 | Loss: 0.00002927
Iteration 102/1000 | Loss: 0.00002927
Iteration 103/1000 | Loss: 0.00002927
Iteration 104/1000 | Loss: 0.00002927
Iteration 105/1000 | Loss: 0.00002927
Iteration 106/1000 | Loss: 0.00002927
Iteration 107/1000 | Loss: 0.00002927
Iteration 108/1000 | Loss: 0.00002927
Iteration 109/1000 | Loss: 0.00002927
Iteration 110/1000 | Loss: 0.00002927
Iteration 111/1000 | Loss: 0.00002927
Iteration 112/1000 | Loss: 0.00002927
Iteration 113/1000 | Loss: 0.00002926
Iteration 114/1000 | Loss: 0.00002926
Iteration 115/1000 | Loss: 0.00002926
Iteration 116/1000 | Loss: 0.00002926
Iteration 117/1000 | Loss: 0.00002926
Iteration 118/1000 | Loss: 0.00002926
Iteration 119/1000 | Loss: 0.00002926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.9264978365972638e-05, 2.9264978365972638e-05, 2.9264978365972638e-05, 2.9264978365972638e-05, 2.9264978365972638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9264978365972638e-05

Optimization complete. Final v2v error: 4.414638996124268 mm

Highest mean error: 5.636920928955078 mm for frame 144

Lowest mean error: 3.6488070487976074 mm for frame 10

Saving results

Total time: 51.48682188987732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469737
Iteration 2/25 | Loss: 0.00135609
Iteration 3/25 | Loss: 0.00120939
Iteration 4/25 | Loss: 0.00119847
Iteration 5/25 | Loss: 0.00119594
Iteration 6/25 | Loss: 0.00119546
Iteration 7/25 | Loss: 0.00119546
Iteration 8/25 | Loss: 0.00119546
Iteration 9/25 | Loss: 0.00119546
Iteration 10/25 | Loss: 0.00119546
Iteration 11/25 | Loss: 0.00119546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011954638175666332, 0.0011954638175666332, 0.0011954638175666332, 0.0011954638175666332, 0.0011954638175666332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011954638175666332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42202210
Iteration 2/25 | Loss: 0.00077712
Iteration 3/25 | Loss: 0.00077710
Iteration 4/25 | Loss: 0.00077710
Iteration 5/25 | Loss: 0.00077710
Iteration 6/25 | Loss: 0.00077710
Iteration 7/25 | Loss: 0.00077710
Iteration 8/25 | Loss: 0.00077710
Iteration 9/25 | Loss: 0.00077710
Iteration 10/25 | Loss: 0.00077710
Iteration 11/25 | Loss: 0.00077710
Iteration 12/25 | Loss: 0.00077710
Iteration 13/25 | Loss: 0.00077710
Iteration 14/25 | Loss: 0.00077710
Iteration 15/25 | Loss: 0.00077710
Iteration 16/25 | Loss: 0.00077710
Iteration 17/25 | Loss: 0.00077710
Iteration 18/25 | Loss: 0.00077710
Iteration 19/25 | Loss: 0.00077710
Iteration 20/25 | Loss: 0.00077710
Iteration 21/25 | Loss: 0.00077710
Iteration 22/25 | Loss: 0.00077710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007770967204123735, 0.0007770967204123735, 0.0007770967204123735, 0.0007770967204123735, 0.0007770967204123735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007770967204123735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077710
Iteration 2/1000 | Loss: 0.00003032
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001838
Iteration 5/1000 | Loss: 0.00001724
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001561
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001523
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001517
Iteration 14/1000 | Loss: 0.00001511
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001491
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001484
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001470
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001466
Iteration 48/1000 | Loss: 0.00001466
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001455
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001448
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001444
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001443
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001438
Iteration 129/1000 | Loss: 0.00001438
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001435
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001435
Iteration 163/1000 | Loss: 0.00001435
Iteration 164/1000 | Loss: 0.00001435
Iteration 165/1000 | Loss: 0.00001434
Iteration 166/1000 | Loss: 0.00001434
Iteration 167/1000 | Loss: 0.00001434
Iteration 168/1000 | Loss: 0.00001434
Iteration 169/1000 | Loss: 0.00001434
Iteration 170/1000 | Loss: 0.00001434
Iteration 171/1000 | Loss: 0.00001434
Iteration 172/1000 | Loss: 0.00001434
Iteration 173/1000 | Loss: 0.00001434
Iteration 174/1000 | Loss: 0.00001434
Iteration 175/1000 | Loss: 0.00001434
Iteration 176/1000 | Loss: 0.00001434
Iteration 177/1000 | Loss: 0.00001434
Iteration 178/1000 | Loss: 0.00001434
Iteration 179/1000 | Loss: 0.00001434
Iteration 180/1000 | Loss: 0.00001434
Iteration 181/1000 | Loss: 0.00001434
Iteration 182/1000 | Loss: 0.00001434
Iteration 183/1000 | Loss: 0.00001434
Iteration 184/1000 | Loss: 0.00001434
Iteration 185/1000 | Loss: 0.00001434
Iteration 186/1000 | Loss: 0.00001434
Iteration 187/1000 | Loss: 0.00001434
Iteration 188/1000 | Loss: 0.00001434
Iteration 189/1000 | Loss: 0.00001434
Iteration 190/1000 | Loss: 0.00001434
Iteration 191/1000 | Loss: 0.00001434
Iteration 192/1000 | Loss: 0.00001434
Iteration 193/1000 | Loss: 0.00001434
Iteration 194/1000 | Loss: 0.00001434
Iteration 195/1000 | Loss: 0.00001434
Iteration 196/1000 | Loss: 0.00001434
Iteration 197/1000 | Loss: 0.00001434
Iteration 198/1000 | Loss: 0.00001434
Iteration 199/1000 | Loss: 0.00001434
Iteration 200/1000 | Loss: 0.00001434
Iteration 201/1000 | Loss: 0.00001434
Iteration 202/1000 | Loss: 0.00001434
Iteration 203/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.43437682709191e-05, 1.43437682709191e-05, 1.43437682709191e-05, 1.43437682709191e-05, 1.43437682709191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.43437682709191e-05

Optimization complete. Final v2v error: 3.128190755844116 mm

Highest mean error: 3.8184800148010254 mm for frame 70

Lowest mean error: 2.525688648223877 mm for frame 147

Saving results

Total time: 40.69018292427063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007416
Iteration 2/25 | Loss: 0.00220421
Iteration 3/25 | Loss: 0.00202774
Iteration 4/25 | Loss: 0.00162997
Iteration 5/25 | Loss: 0.00153012
Iteration 6/25 | Loss: 0.00172461
Iteration 7/25 | Loss: 0.00152238
Iteration 8/25 | Loss: 0.00140125
Iteration 9/25 | Loss: 0.00139966
Iteration 10/25 | Loss: 0.00131857
Iteration 11/25 | Loss: 0.00130947
Iteration 12/25 | Loss: 0.00133301
Iteration 13/25 | Loss: 0.00127461
Iteration 14/25 | Loss: 0.00126141
Iteration 15/25 | Loss: 0.00125609
Iteration 16/25 | Loss: 0.00125241
Iteration 17/25 | Loss: 0.00125668
Iteration 18/25 | Loss: 0.00125042
Iteration 19/25 | Loss: 0.00124489
Iteration 20/25 | Loss: 0.00124732
Iteration 21/25 | Loss: 0.00124781
Iteration 22/25 | Loss: 0.00124925
Iteration 23/25 | Loss: 0.00124482
Iteration 24/25 | Loss: 0.00124365
Iteration 25/25 | Loss: 0.00124304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39465272
Iteration 2/25 | Loss: 0.00119052
Iteration 3/25 | Loss: 0.00119052
Iteration 4/25 | Loss: 0.00119052
Iteration 5/25 | Loss: 0.00119052
Iteration 6/25 | Loss: 0.00119052
Iteration 7/25 | Loss: 0.00119052
Iteration 8/25 | Loss: 0.00119052
Iteration 9/25 | Loss: 0.00119052
Iteration 10/25 | Loss: 0.00119052
Iteration 11/25 | Loss: 0.00119052
Iteration 12/25 | Loss: 0.00119052
Iteration 13/25 | Loss: 0.00119052
Iteration 14/25 | Loss: 0.00119052
Iteration 15/25 | Loss: 0.00119052
Iteration 16/25 | Loss: 0.00119052
Iteration 17/25 | Loss: 0.00119052
Iteration 18/25 | Loss: 0.00119052
Iteration 19/25 | Loss: 0.00119052
Iteration 20/25 | Loss: 0.00119052
Iteration 21/25 | Loss: 0.00119052
Iteration 22/25 | Loss: 0.00119052
Iteration 23/25 | Loss: 0.00119052
Iteration 24/25 | Loss: 0.00119052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011905154678970575, 0.0011905154678970575, 0.0011905154678970575, 0.0011905154678970575, 0.0011905154678970575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011905154678970575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119052
Iteration 2/1000 | Loss: 0.00008789
Iteration 3/1000 | Loss: 0.00006335
Iteration 4/1000 | Loss: 0.00005156
Iteration 5/1000 | Loss: 0.00004627
Iteration 6/1000 | Loss: 0.00004375
Iteration 7/1000 | Loss: 0.00031708
Iteration 8/1000 | Loss: 0.00031722
Iteration 9/1000 | Loss: 0.00006436
Iteration 10/1000 | Loss: 0.00004586
Iteration 11/1000 | Loss: 0.00003794
Iteration 12/1000 | Loss: 0.00003253
Iteration 13/1000 | Loss: 0.00002985
Iteration 14/1000 | Loss: 0.00002871
Iteration 15/1000 | Loss: 0.00002755
Iteration 16/1000 | Loss: 0.00002663
Iteration 17/1000 | Loss: 0.00002589
Iteration 18/1000 | Loss: 0.00002509
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002410
Iteration 22/1000 | Loss: 0.00102441
Iteration 23/1000 | Loss: 0.00131329
Iteration 24/1000 | Loss: 0.00080755
Iteration 25/1000 | Loss: 0.00056047
Iteration 26/1000 | Loss: 0.00133575
Iteration 27/1000 | Loss: 0.00079310
Iteration 28/1000 | Loss: 0.00004480
Iteration 29/1000 | Loss: 0.00002956
Iteration 30/1000 | Loss: 0.00002373
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001829
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001717
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001669
Iteration 42/1000 | Loss: 0.00001668
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001666
Iteration 45/1000 | Loss: 0.00001666
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001658
Iteration 60/1000 | Loss: 0.00001658
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001658
Iteration 63/1000 | Loss: 0.00001657
Iteration 64/1000 | Loss: 0.00001657
Iteration 65/1000 | Loss: 0.00001657
Iteration 66/1000 | Loss: 0.00001657
Iteration 67/1000 | Loss: 0.00001657
Iteration 68/1000 | Loss: 0.00001656
Iteration 69/1000 | Loss: 0.00001656
Iteration 70/1000 | Loss: 0.00001656
Iteration 71/1000 | Loss: 0.00001656
Iteration 72/1000 | Loss: 0.00001656
Iteration 73/1000 | Loss: 0.00001655
Iteration 74/1000 | Loss: 0.00001655
Iteration 75/1000 | Loss: 0.00001655
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001653
Iteration 88/1000 | Loss: 0.00001652
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001651
Iteration 92/1000 | Loss: 0.00001651
Iteration 93/1000 | Loss: 0.00001651
Iteration 94/1000 | Loss: 0.00001651
Iteration 95/1000 | Loss: 0.00001651
Iteration 96/1000 | Loss: 0.00001651
Iteration 97/1000 | Loss: 0.00001650
Iteration 98/1000 | Loss: 0.00001650
Iteration 99/1000 | Loss: 0.00001650
Iteration 100/1000 | Loss: 0.00001650
Iteration 101/1000 | Loss: 0.00001650
Iteration 102/1000 | Loss: 0.00001650
Iteration 103/1000 | Loss: 0.00001650
Iteration 104/1000 | Loss: 0.00001650
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001649
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001649
Iteration 115/1000 | Loss: 0.00001649
Iteration 116/1000 | Loss: 0.00001649
Iteration 117/1000 | Loss: 0.00001649
Iteration 118/1000 | Loss: 0.00001648
Iteration 119/1000 | Loss: 0.00001648
Iteration 120/1000 | Loss: 0.00001648
Iteration 121/1000 | Loss: 0.00001648
Iteration 122/1000 | Loss: 0.00001648
Iteration 123/1000 | Loss: 0.00001647
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001647
Iteration 126/1000 | Loss: 0.00001647
Iteration 127/1000 | Loss: 0.00001647
Iteration 128/1000 | Loss: 0.00001647
Iteration 129/1000 | Loss: 0.00001647
Iteration 130/1000 | Loss: 0.00001647
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001647
Iteration 134/1000 | Loss: 0.00001647
Iteration 135/1000 | Loss: 0.00001647
Iteration 136/1000 | Loss: 0.00001647
Iteration 137/1000 | Loss: 0.00001647
Iteration 138/1000 | Loss: 0.00001646
Iteration 139/1000 | Loss: 0.00001646
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001646
Iteration 144/1000 | Loss: 0.00001646
Iteration 145/1000 | Loss: 0.00001646
Iteration 146/1000 | Loss: 0.00001646
Iteration 147/1000 | Loss: 0.00001646
Iteration 148/1000 | Loss: 0.00001646
Iteration 149/1000 | Loss: 0.00001646
Iteration 150/1000 | Loss: 0.00001646
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001646
Iteration 153/1000 | Loss: 0.00001646
Iteration 154/1000 | Loss: 0.00001646
Iteration 155/1000 | Loss: 0.00001645
Iteration 156/1000 | Loss: 0.00001645
Iteration 157/1000 | Loss: 0.00001645
Iteration 158/1000 | Loss: 0.00001645
Iteration 159/1000 | Loss: 0.00001645
Iteration 160/1000 | Loss: 0.00001645
Iteration 161/1000 | Loss: 0.00001645
Iteration 162/1000 | Loss: 0.00001645
Iteration 163/1000 | Loss: 0.00001645
Iteration 164/1000 | Loss: 0.00001645
Iteration 165/1000 | Loss: 0.00001645
Iteration 166/1000 | Loss: 0.00001645
Iteration 167/1000 | Loss: 0.00001645
Iteration 168/1000 | Loss: 0.00001645
Iteration 169/1000 | Loss: 0.00001645
Iteration 170/1000 | Loss: 0.00001645
Iteration 171/1000 | Loss: 0.00001645
Iteration 172/1000 | Loss: 0.00001645
Iteration 173/1000 | Loss: 0.00001645
Iteration 174/1000 | Loss: 0.00001645
Iteration 175/1000 | Loss: 0.00001645
Iteration 176/1000 | Loss: 0.00001644
Iteration 177/1000 | Loss: 0.00001644
Iteration 178/1000 | Loss: 0.00001644
Iteration 179/1000 | Loss: 0.00001644
Iteration 180/1000 | Loss: 0.00001644
Iteration 181/1000 | Loss: 0.00001644
Iteration 182/1000 | Loss: 0.00001644
Iteration 183/1000 | Loss: 0.00001644
Iteration 184/1000 | Loss: 0.00001644
Iteration 185/1000 | Loss: 0.00001644
Iteration 186/1000 | Loss: 0.00001644
Iteration 187/1000 | Loss: 0.00001644
Iteration 188/1000 | Loss: 0.00001644
Iteration 189/1000 | Loss: 0.00001644
Iteration 190/1000 | Loss: 0.00001644
Iteration 191/1000 | Loss: 0.00001644
Iteration 192/1000 | Loss: 0.00001643
Iteration 193/1000 | Loss: 0.00001643
Iteration 194/1000 | Loss: 0.00001643
Iteration 195/1000 | Loss: 0.00001643
Iteration 196/1000 | Loss: 0.00001643
Iteration 197/1000 | Loss: 0.00001643
Iteration 198/1000 | Loss: 0.00001643
Iteration 199/1000 | Loss: 0.00001643
Iteration 200/1000 | Loss: 0.00001643
Iteration 201/1000 | Loss: 0.00001643
Iteration 202/1000 | Loss: 0.00001643
Iteration 203/1000 | Loss: 0.00001643
Iteration 204/1000 | Loss: 0.00001643
Iteration 205/1000 | Loss: 0.00001643
Iteration 206/1000 | Loss: 0.00001643
Iteration 207/1000 | Loss: 0.00001642
Iteration 208/1000 | Loss: 0.00001642
Iteration 209/1000 | Loss: 0.00001642
Iteration 210/1000 | Loss: 0.00001642
Iteration 211/1000 | Loss: 0.00001642
Iteration 212/1000 | Loss: 0.00001642
Iteration 213/1000 | Loss: 0.00001642
Iteration 214/1000 | Loss: 0.00001642
Iteration 215/1000 | Loss: 0.00001642
Iteration 216/1000 | Loss: 0.00001642
Iteration 217/1000 | Loss: 0.00001642
Iteration 218/1000 | Loss: 0.00001642
Iteration 219/1000 | Loss: 0.00001642
Iteration 220/1000 | Loss: 0.00001642
Iteration 221/1000 | Loss: 0.00001642
Iteration 222/1000 | Loss: 0.00001642
Iteration 223/1000 | Loss: 0.00001642
Iteration 224/1000 | Loss: 0.00001642
Iteration 225/1000 | Loss: 0.00001642
Iteration 226/1000 | Loss: 0.00001642
Iteration 227/1000 | Loss: 0.00001642
Iteration 228/1000 | Loss: 0.00001642
Iteration 229/1000 | Loss: 0.00001642
Iteration 230/1000 | Loss: 0.00001642
Iteration 231/1000 | Loss: 0.00001642
Iteration 232/1000 | Loss: 0.00001642
Iteration 233/1000 | Loss: 0.00001642
Iteration 234/1000 | Loss: 0.00001642
Iteration 235/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.641662311158143e-05, 1.641662311158143e-05, 1.641662311158143e-05, 1.641662311158143e-05, 1.641662311158143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.641662311158143e-05

Optimization complete. Final v2v error: 3.4065561294555664 mm

Highest mean error: 4.399438381195068 mm for frame 66

Lowest mean error: 2.7868247032165527 mm for frame 43

Saving results

Total time: 110.50464034080505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399270
Iteration 2/25 | Loss: 0.00126206
Iteration 3/25 | Loss: 0.00116594
Iteration 4/25 | Loss: 0.00114928
Iteration 5/25 | Loss: 0.00114380
Iteration 6/25 | Loss: 0.00114292
Iteration 7/25 | Loss: 0.00114292
Iteration 8/25 | Loss: 0.00114292
Iteration 9/25 | Loss: 0.00114292
Iteration 10/25 | Loss: 0.00114292
Iteration 11/25 | Loss: 0.00114292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011429209262132645, 0.0011429209262132645, 0.0011429209262132645, 0.0011429209262132645, 0.0011429209262132645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011429209262132645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32176065
Iteration 2/25 | Loss: 0.00084225
Iteration 3/25 | Loss: 0.00084225
Iteration 4/25 | Loss: 0.00084225
Iteration 5/25 | Loss: 0.00084225
Iteration 6/25 | Loss: 0.00084225
Iteration 7/25 | Loss: 0.00084225
Iteration 8/25 | Loss: 0.00084225
Iteration 9/25 | Loss: 0.00084225
Iteration 10/25 | Loss: 0.00084225
Iteration 11/25 | Loss: 0.00084225
Iteration 12/25 | Loss: 0.00084225
Iteration 13/25 | Loss: 0.00084225
Iteration 14/25 | Loss: 0.00084225
Iteration 15/25 | Loss: 0.00084225
Iteration 16/25 | Loss: 0.00084225
Iteration 17/25 | Loss: 0.00084225
Iteration 18/25 | Loss: 0.00084225
Iteration 19/25 | Loss: 0.00084225
Iteration 20/25 | Loss: 0.00084225
Iteration 21/25 | Loss: 0.00084225
Iteration 22/25 | Loss: 0.00084225
Iteration 23/25 | Loss: 0.00084225
Iteration 24/25 | Loss: 0.00084225
Iteration 25/25 | Loss: 0.00084225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084225
Iteration 2/1000 | Loss: 0.00004223
Iteration 3/1000 | Loss: 0.00002833
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002370
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002144
Iteration 8/1000 | Loss: 0.00002090
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002027
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001947
Iteration 17/1000 | Loss: 0.00001935
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00001926
Iteration 21/1000 | Loss: 0.00001925
Iteration 22/1000 | Loss: 0.00001925
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001923
Iteration 27/1000 | Loss: 0.00001921
Iteration 28/1000 | Loss: 0.00001920
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001907
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001905
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001904
Iteration 39/1000 | Loss: 0.00001903
Iteration 40/1000 | Loss: 0.00001903
Iteration 41/1000 | Loss: 0.00001903
Iteration 42/1000 | Loss: 0.00001900
Iteration 43/1000 | Loss: 0.00001900
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001899
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001898
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001898
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001897
Iteration 67/1000 | Loss: 0.00001897
Iteration 68/1000 | Loss: 0.00001896
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001896
Iteration 78/1000 | Loss: 0.00001896
Iteration 79/1000 | Loss: 0.00001896
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001896
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001895
Iteration 85/1000 | Loss: 0.00001895
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001895
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001894
Iteration 97/1000 | Loss: 0.00001894
Iteration 98/1000 | Loss: 0.00001894
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001893
Iteration 106/1000 | Loss: 0.00001893
Iteration 107/1000 | Loss: 0.00001893
Iteration 108/1000 | Loss: 0.00001893
Iteration 109/1000 | Loss: 0.00001893
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001892
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001891
Iteration 119/1000 | Loss: 0.00001891
Iteration 120/1000 | Loss: 0.00001891
Iteration 121/1000 | Loss: 0.00001891
Iteration 122/1000 | Loss: 0.00001891
Iteration 123/1000 | Loss: 0.00001891
Iteration 124/1000 | Loss: 0.00001891
Iteration 125/1000 | Loss: 0.00001890
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001890
Iteration 129/1000 | Loss: 0.00001890
Iteration 130/1000 | Loss: 0.00001890
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001888
Iteration 141/1000 | Loss: 0.00001888
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001887
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001887
Iteration 147/1000 | Loss: 0.00001887
Iteration 148/1000 | Loss: 0.00001886
Iteration 149/1000 | Loss: 0.00001886
Iteration 150/1000 | Loss: 0.00001886
Iteration 151/1000 | Loss: 0.00001886
Iteration 152/1000 | Loss: 0.00001886
Iteration 153/1000 | Loss: 0.00001885
Iteration 154/1000 | Loss: 0.00001885
Iteration 155/1000 | Loss: 0.00001885
Iteration 156/1000 | Loss: 0.00001885
Iteration 157/1000 | Loss: 0.00001884
Iteration 158/1000 | Loss: 0.00001884
Iteration 159/1000 | Loss: 0.00001884
Iteration 160/1000 | Loss: 0.00001884
Iteration 161/1000 | Loss: 0.00001884
Iteration 162/1000 | Loss: 0.00001884
Iteration 163/1000 | Loss: 0.00001884
Iteration 164/1000 | Loss: 0.00001884
Iteration 165/1000 | Loss: 0.00001883
Iteration 166/1000 | Loss: 0.00001883
Iteration 167/1000 | Loss: 0.00001883
Iteration 168/1000 | Loss: 0.00001883
Iteration 169/1000 | Loss: 0.00001883
Iteration 170/1000 | Loss: 0.00001883
Iteration 171/1000 | Loss: 0.00001883
Iteration 172/1000 | Loss: 0.00001882
Iteration 173/1000 | Loss: 0.00001882
Iteration 174/1000 | Loss: 0.00001882
Iteration 175/1000 | Loss: 0.00001882
Iteration 176/1000 | Loss: 0.00001882
Iteration 177/1000 | Loss: 0.00001882
Iteration 178/1000 | Loss: 0.00001882
Iteration 179/1000 | Loss: 0.00001882
Iteration 180/1000 | Loss: 0.00001882
Iteration 181/1000 | Loss: 0.00001882
Iteration 182/1000 | Loss: 0.00001882
Iteration 183/1000 | Loss: 0.00001881
Iteration 184/1000 | Loss: 0.00001881
Iteration 185/1000 | Loss: 0.00001881
Iteration 186/1000 | Loss: 0.00001881
Iteration 187/1000 | Loss: 0.00001881
Iteration 188/1000 | Loss: 0.00001881
Iteration 189/1000 | Loss: 0.00001881
Iteration 190/1000 | Loss: 0.00001881
Iteration 191/1000 | Loss: 0.00001881
Iteration 192/1000 | Loss: 0.00001880
Iteration 193/1000 | Loss: 0.00001880
Iteration 194/1000 | Loss: 0.00001880
Iteration 195/1000 | Loss: 0.00001880
Iteration 196/1000 | Loss: 0.00001880
Iteration 197/1000 | Loss: 0.00001880
Iteration 198/1000 | Loss: 0.00001880
Iteration 199/1000 | Loss: 0.00001880
Iteration 200/1000 | Loss: 0.00001880
Iteration 201/1000 | Loss: 0.00001880
Iteration 202/1000 | Loss: 0.00001880
Iteration 203/1000 | Loss: 0.00001880
Iteration 204/1000 | Loss: 0.00001880
Iteration 205/1000 | Loss: 0.00001880
Iteration 206/1000 | Loss: 0.00001880
Iteration 207/1000 | Loss: 0.00001880
Iteration 208/1000 | Loss: 0.00001880
Iteration 209/1000 | Loss: 0.00001880
Iteration 210/1000 | Loss: 0.00001880
Iteration 211/1000 | Loss: 0.00001880
Iteration 212/1000 | Loss: 0.00001880
Iteration 213/1000 | Loss: 0.00001880
Iteration 214/1000 | Loss: 0.00001880
Iteration 215/1000 | Loss: 0.00001880
Iteration 216/1000 | Loss: 0.00001880
Iteration 217/1000 | Loss: 0.00001880
Iteration 218/1000 | Loss: 0.00001880
Iteration 219/1000 | Loss: 0.00001880
Iteration 220/1000 | Loss: 0.00001880
Iteration 221/1000 | Loss: 0.00001880
Iteration 222/1000 | Loss: 0.00001880
Iteration 223/1000 | Loss: 0.00001880
Iteration 224/1000 | Loss: 0.00001880
Iteration 225/1000 | Loss: 0.00001880
Iteration 226/1000 | Loss: 0.00001880
Iteration 227/1000 | Loss: 0.00001880
Iteration 228/1000 | Loss: 0.00001880
Iteration 229/1000 | Loss: 0.00001880
Iteration 230/1000 | Loss: 0.00001880
Iteration 231/1000 | Loss: 0.00001880
Iteration 232/1000 | Loss: 0.00001880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.8803964849212207e-05, 1.8803964849212207e-05, 1.8803964849212207e-05, 1.8803964849212207e-05, 1.8803964849212207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8803964849212207e-05

Optimization complete. Final v2v error: 3.5530624389648438 mm

Highest mean error: 4.06742525100708 mm for frame 59

Lowest mean error: 3.1979498863220215 mm for frame 131

Saving results

Total time: 45.45320224761963
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472680
Iteration 2/25 | Loss: 0.00120124
Iteration 3/25 | Loss: 0.00113623
Iteration 4/25 | Loss: 0.00112669
Iteration 5/25 | Loss: 0.00112277
Iteration 6/25 | Loss: 0.00112210
Iteration 7/25 | Loss: 0.00112210
Iteration 8/25 | Loss: 0.00112210
Iteration 9/25 | Loss: 0.00112210
Iteration 10/25 | Loss: 0.00112210
Iteration 11/25 | Loss: 0.00112210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011221043532714248, 0.0011221043532714248, 0.0011221043532714248, 0.0011221043532714248, 0.0011221043532714248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011221043532714248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68345499
Iteration 2/25 | Loss: 0.00078829
Iteration 3/25 | Loss: 0.00078829
Iteration 4/25 | Loss: 0.00078828
Iteration 5/25 | Loss: 0.00078828
Iteration 6/25 | Loss: 0.00078828
Iteration 7/25 | Loss: 0.00078828
Iteration 8/25 | Loss: 0.00078828
Iteration 9/25 | Loss: 0.00078828
Iteration 10/25 | Loss: 0.00078828
Iteration 11/25 | Loss: 0.00078828
Iteration 12/25 | Loss: 0.00078828
Iteration 13/25 | Loss: 0.00078828
Iteration 14/25 | Loss: 0.00078828
Iteration 15/25 | Loss: 0.00078828
Iteration 16/25 | Loss: 0.00078828
Iteration 17/25 | Loss: 0.00078828
Iteration 18/25 | Loss: 0.00078828
Iteration 19/25 | Loss: 0.00078828
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007882827194407582, 0.0007882827194407582, 0.0007882827194407582, 0.0007882827194407582, 0.0007882827194407582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007882827194407582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078828
Iteration 2/1000 | Loss: 0.00002670
Iteration 3/1000 | Loss: 0.00001681
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001151
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001093
Iteration 13/1000 | Loss: 0.00001093
Iteration 14/1000 | Loss: 0.00001092
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001084
Iteration 17/1000 | Loss: 0.00001084
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001076
Iteration 24/1000 | Loss: 0.00001073
Iteration 25/1000 | Loss: 0.00001072
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001070
Iteration 28/1000 | Loss: 0.00001070
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001059
Iteration 35/1000 | Loss: 0.00001058
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001057
Iteration 39/1000 | Loss: 0.00001056
Iteration 40/1000 | Loss: 0.00001056
Iteration 41/1000 | Loss: 0.00001056
Iteration 42/1000 | Loss: 0.00001056
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001055
Iteration 45/1000 | Loss: 0.00001052
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001050
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001050
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001046
Iteration 66/1000 | Loss: 0.00001046
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001045
Iteration 70/1000 | Loss: 0.00001045
Iteration 71/1000 | Loss: 0.00001044
Iteration 72/1000 | Loss: 0.00001044
Iteration 73/1000 | Loss: 0.00001044
Iteration 74/1000 | Loss: 0.00001043
Iteration 75/1000 | Loss: 0.00001043
Iteration 76/1000 | Loss: 0.00001043
Iteration 77/1000 | Loss: 0.00001042
Iteration 78/1000 | Loss: 0.00001042
Iteration 79/1000 | Loss: 0.00001041
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001039
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001038
Iteration 89/1000 | Loss: 0.00001038
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001038
Iteration 92/1000 | Loss: 0.00001038
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001033
Iteration 102/1000 | Loss: 0.00001033
Iteration 103/1000 | Loss: 0.00001032
Iteration 104/1000 | Loss: 0.00001031
Iteration 105/1000 | Loss: 0.00001031
Iteration 106/1000 | Loss: 0.00001030
Iteration 107/1000 | Loss: 0.00001030
Iteration 108/1000 | Loss: 0.00001030
Iteration 109/1000 | Loss: 0.00001030
Iteration 110/1000 | Loss: 0.00001029
Iteration 111/1000 | Loss: 0.00001029
Iteration 112/1000 | Loss: 0.00001029
Iteration 113/1000 | Loss: 0.00001029
Iteration 114/1000 | Loss: 0.00001029
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001028
Iteration 121/1000 | Loss: 0.00001028
Iteration 122/1000 | Loss: 0.00001028
Iteration 123/1000 | Loss: 0.00001028
Iteration 124/1000 | Loss: 0.00001027
Iteration 125/1000 | Loss: 0.00001027
Iteration 126/1000 | Loss: 0.00001027
Iteration 127/1000 | Loss: 0.00001027
Iteration 128/1000 | Loss: 0.00001027
Iteration 129/1000 | Loss: 0.00001026
Iteration 130/1000 | Loss: 0.00001026
Iteration 131/1000 | Loss: 0.00001026
Iteration 132/1000 | Loss: 0.00001026
Iteration 133/1000 | Loss: 0.00001026
Iteration 134/1000 | Loss: 0.00001026
Iteration 135/1000 | Loss: 0.00001026
Iteration 136/1000 | Loss: 0.00001026
Iteration 137/1000 | Loss: 0.00001026
Iteration 138/1000 | Loss: 0.00001026
Iteration 139/1000 | Loss: 0.00001026
Iteration 140/1000 | Loss: 0.00001026
Iteration 141/1000 | Loss: 0.00001025
Iteration 142/1000 | Loss: 0.00001025
Iteration 143/1000 | Loss: 0.00001025
Iteration 144/1000 | Loss: 0.00001025
Iteration 145/1000 | Loss: 0.00001025
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001024
Iteration 151/1000 | Loss: 0.00001023
Iteration 152/1000 | Loss: 0.00001023
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001022
Iteration 159/1000 | Loss: 0.00001022
Iteration 160/1000 | Loss: 0.00001022
Iteration 161/1000 | Loss: 0.00001022
Iteration 162/1000 | Loss: 0.00001022
Iteration 163/1000 | Loss: 0.00001022
Iteration 164/1000 | Loss: 0.00001021
Iteration 165/1000 | Loss: 0.00001021
Iteration 166/1000 | Loss: 0.00001021
Iteration 167/1000 | Loss: 0.00001021
Iteration 168/1000 | Loss: 0.00001021
Iteration 169/1000 | Loss: 0.00001021
Iteration 170/1000 | Loss: 0.00001021
Iteration 171/1000 | Loss: 0.00001021
Iteration 172/1000 | Loss: 0.00001021
Iteration 173/1000 | Loss: 0.00001021
Iteration 174/1000 | Loss: 0.00001021
Iteration 175/1000 | Loss: 0.00001021
Iteration 176/1000 | Loss: 0.00001021
Iteration 177/1000 | Loss: 0.00001021
Iteration 178/1000 | Loss: 0.00001021
Iteration 179/1000 | Loss: 0.00001021
Iteration 180/1000 | Loss: 0.00001021
Iteration 181/1000 | Loss: 0.00001021
Iteration 182/1000 | Loss: 0.00001020
Iteration 183/1000 | Loss: 0.00001020
Iteration 184/1000 | Loss: 0.00001020
Iteration 185/1000 | Loss: 0.00001020
Iteration 186/1000 | Loss: 0.00001020
Iteration 187/1000 | Loss: 0.00001020
Iteration 188/1000 | Loss: 0.00001020
Iteration 189/1000 | Loss: 0.00001020
Iteration 190/1000 | Loss: 0.00001020
Iteration 191/1000 | Loss: 0.00001020
Iteration 192/1000 | Loss: 0.00001020
Iteration 193/1000 | Loss: 0.00001020
Iteration 194/1000 | Loss: 0.00001020
Iteration 195/1000 | Loss: 0.00001020
Iteration 196/1000 | Loss: 0.00001020
Iteration 197/1000 | Loss: 0.00001020
Iteration 198/1000 | Loss: 0.00001020
Iteration 199/1000 | Loss: 0.00001020
Iteration 200/1000 | Loss: 0.00001020
Iteration 201/1000 | Loss: 0.00001020
Iteration 202/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.0203275451203808e-05, 1.0203275451203808e-05, 1.0203275451203808e-05, 1.0203275451203808e-05, 1.0203275451203808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0203275451203808e-05

Optimization complete. Final v2v error: 2.748830556869507 mm

Highest mean error: 3.0712735652923584 mm for frame 76

Lowest mean error: 2.4879305362701416 mm for frame 125

Saving results

Total time: 41.00139880180359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993908
Iteration 2/25 | Loss: 0.00280735
Iteration 3/25 | Loss: 0.00207031
Iteration 4/25 | Loss: 0.00205212
Iteration 5/25 | Loss: 0.00200674
Iteration 6/25 | Loss: 0.00142815
Iteration 7/25 | Loss: 0.00135621
Iteration 8/25 | Loss: 0.00127636
Iteration 9/25 | Loss: 0.00126949
Iteration 10/25 | Loss: 0.00126609
Iteration 11/25 | Loss: 0.00126511
Iteration 12/25 | Loss: 0.00126487
Iteration 13/25 | Loss: 0.00126474
Iteration 14/25 | Loss: 0.00126468
Iteration 15/25 | Loss: 0.00126467
Iteration 16/25 | Loss: 0.00126467
Iteration 17/25 | Loss: 0.00126466
Iteration 18/25 | Loss: 0.00126466
Iteration 19/25 | Loss: 0.00126466
Iteration 20/25 | Loss: 0.00126466
Iteration 21/25 | Loss: 0.00126466
Iteration 22/25 | Loss: 0.00126466
Iteration 23/25 | Loss: 0.00126466
Iteration 24/25 | Loss: 0.00126465
Iteration 25/25 | Loss: 0.00126465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36109412
Iteration 2/25 | Loss: 0.00077793
Iteration 3/25 | Loss: 0.00077793
Iteration 4/25 | Loss: 0.00077793
Iteration 5/25 | Loss: 0.00077793
Iteration 6/25 | Loss: 0.00077792
Iteration 7/25 | Loss: 0.00077792
Iteration 8/25 | Loss: 0.00077792
Iteration 9/25 | Loss: 0.00077792
Iteration 10/25 | Loss: 0.00077792
Iteration 11/25 | Loss: 0.00077792
Iteration 12/25 | Loss: 0.00077792
Iteration 13/25 | Loss: 0.00077792
Iteration 14/25 | Loss: 0.00077792
Iteration 15/25 | Loss: 0.00077792
Iteration 16/25 | Loss: 0.00077792
Iteration 17/25 | Loss: 0.00077792
Iteration 18/25 | Loss: 0.00077792
Iteration 19/25 | Loss: 0.00077792
Iteration 20/25 | Loss: 0.00077792
Iteration 21/25 | Loss: 0.00077792
Iteration 22/25 | Loss: 0.00077792
Iteration 23/25 | Loss: 0.00077792
Iteration 24/25 | Loss: 0.00077792
Iteration 25/25 | Loss: 0.00077792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077792
Iteration 2/1000 | Loss: 0.00005502
Iteration 3/1000 | Loss: 0.00003833
Iteration 4/1000 | Loss: 0.00003457
Iteration 5/1000 | Loss: 0.00003322
Iteration 6/1000 | Loss: 0.00003199
Iteration 7/1000 | Loss: 0.00003129
Iteration 8/1000 | Loss: 0.00003067
Iteration 9/1000 | Loss: 0.00003018
Iteration 10/1000 | Loss: 0.00002982
Iteration 11/1000 | Loss: 0.00002947
Iteration 12/1000 | Loss: 0.00065468
Iteration 13/1000 | Loss: 0.00011694
Iteration 14/1000 | Loss: 0.00003344
Iteration 15/1000 | Loss: 0.00002985
Iteration 16/1000 | Loss: 0.00002706
Iteration 17/1000 | Loss: 0.00002446
Iteration 18/1000 | Loss: 0.00002296
Iteration 19/1000 | Loss: 0.00002230
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002140
Iteration 22/1000 | Loss: 0.00002133
Iteration 23/1000 | Loss: 0.00002119
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002080
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002077
Iteration 31/1000 | Loss: 0.00002075
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002074
Iteration 34/1000 | Loss: 0.00002070
Iteration 35/1000 | Loss: 0.00002070
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002068
Iteration 39/1000 | Loss: 0.00002068
Iteration 40/1000 | Loss: 0.00002067
Iteration 41/1000 | Loss: 0.00002067
Iteration 42/1000 | Loss: 0.00002067
Iteration 43/1000 | Loss: 0.00002067
Iteration 44/1000 | Loss: 0.00002067
Iteration 45/1000 | Loss: 0.00002067
Iteration 46/1000 | Loss: 0.00002066
Iteration 47/1000 | Loss: 0.00002066
Iteration 48/1000 | Loss: 0.00002066
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002065
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002065
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002064
Iteration 56/1000 | Loss: 0.00002063
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00002062
Iteration 59/1000 | Loss: 0.00002061
Iteration 60/1000 | Loss: 0.00002061
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002060
Iteration 65/1000 | Loss: 0.00002060
Iteration 66/1000 | Loss: 0.00002059
Iteration 67/1000 | Loss: 0.00002059
Iteration 68/1000 | Loss: 0.00002059
Iteration 69/1000 | Loss: 0.00002059
Iteration 70/1000 | Loss: 0.00002059
Iteration 71/1000 | Loss: 0.00002059
Iteration 72/1000 | Loss: 0.00002059
Iteration 73/1000 | Loss: 0.00002059
Iteration 74/1000 | Loss: 0.00002059
Iteration 75/1000 | Loss: 0.00002058
Iteration 76/1000 | Loss: 0.00002058
Iteration 77/1000 | Loss: 0.00002058
Iteration 78/1000 | Loss: 0.00002058
Iteration 79/1000 | Loss: 0.00002057
Iteration 80/1000 | Loss: 0.00002057
Iteration 81/1000 | Loss: 0.00002057
Iteration 82/1000 | Loss: 0.00002057
Iteration 83/1000 | Loss: 0.00002057
Iteration 84/1000 | Loss: 0.00002056
Iteration 85/1000 | Loss: 0.00002056
Iteration 86/1000 | Loss: 0.00002056
Iteration 87/1000 | Loss: 0.00002056
Iteration 88/1000 | Loss: 0.00002056
Iteration 89/1000 | Loss: 0.00002056
Iteration 90/1000 | Loss: 0.00002056
Iteration 91/1000 | Loss: 0.00002056
Iteration 92/1000 | Loss: 0.00002056
Iteration 93/1000 | Loss: 0.00002056
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002056
Iteration 96/1000 | Loss: 0.00002056
Iteration 97/1000 | Loss: 0.00002056
Iteration 98/1000 | Loss: 0.00002056
Iteration 99/1000 | Loss: 0.00002056
Iteration 100/1000 | Loss: 0.00002056
Iteration 101/1000 | Loss: 0.00002055
Iteration 102/1000 | Loss: 0.00002055
Iteration 103/1000 | Loss: 0.00002055
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002054
Iteration 106/1000 | Loss: 0.00002054
Iteration 107/1000 | Loss: 0.00002054
Iteration 108/1000 | Loss: 0.00002054
Iteration 109/1000 | Loss: 0.00002054
Iteration 110/1000 | Loss: 0.00002054
Iteration 111/1000 | Loss: 0.00002054
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002054
Iteration 115/1000 | Loss: 0.00002054
Iteration 116/1000 | Loss: 0.00002054
Iteration 117/1000 | Loss: 0.00002054
Iteration 118/1000 | Loss: 0.00002054
Iteration 119/1000 | Loss: 0.00002054
Iteration 120/1000 | Loss: 0.00002054
Iteration 121/1000 | Loss: 0.00002054
Iteration 122/1000 | Loss: 0.00002054
Iteration 123/1000 | Loss: 0.00002054
Iteration 124/1000 | Loss: 0.00002054
Iteration 125/1000 | Loss: 0.00002054
Iteration 126/1000 | Loss: 0.00002053
Iteration 127/1000 | Loss: 0.00002053
Iteration 128/1000 | Loss: 0.00002053
Iteration 129/1000 | Loss: 0.00002053
Iteration 130/1000 | Loss: 0.00002053
Iteration 131/1000 | Loss: 0.00002053
Iteration 132/1000 | Loss: 0.00002053
Iteration 133/1000 | Loss: 0.00002053
Iteration 134/1000 | Loss: 0.00002053
Iteration 135/1000 | Loss: 0.00002053
Iteration 136/1000 | Loss: 0.00002053
Iteration 137/1000 | Loss: 0.00002053
Iteration 138/1000 | Loss: 0.00002053
Iteration 139/1000 | Loss: 0.00002053
Iteration 140/1000 | Loss: 0.00002053
Iteration 141/1000 | Loss: 0.00002053
Iteration 142/1000 | Loss: 0.00002053
Iteration 143/1000 | Loss: 0.00002053
Iteration 144/1000 | Loss: 0.00002053
Iteration 145/1000 | Loss: 0.00002053
Iteration 146/1000 | Loss: 0.00002053
Iteration 147/1000 | Loss: 0.00002053
Iteration 148/1000 | Loss: 0.00002053
Iteration 149/1000 | Loss: 0.00002053
Iteration 150/1000 | Loss: 0.00002053
Iteration 151/1000 | Loss: 0.00002053
Iteration 152/1000 | Loss: 0.00002053
Iteration 153/1000 | Loss: 0.00002053
Iteration 154/1000 | Loss: 0.00002053
Iteration 155/1000 | Loss: 0.00002053
Iteration 156/1000 | Loss: 0.00002053
Iteration 157/1000 | Loss: 0.00002053
Iteration 158/1000 | Loss: 0.00002053
Iteration 159/1000 | Loss: 0.00002053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.0528159438981675e-05, 2.0528159438981675e-05, 2.0528159438981675e-05, 2.0528159438981675e-05, 2.0528159438981675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0528159438981675e-05

Optimization complete. Final v2v error: 3.8318285942077637 mm

Highest mean error: 4.108855247497559 mm for frame 148

Lowest mean error: 3.422513008117676 mm for frame 5

Saving results

Total time: 67.64832091331482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594196
Iteration 2/25 | Loss: 0.00131891
Iteration 3/25 | Loss: 0.00122436
Iteration 4/25 | Loss: 0.00120100
Iteration 5/25 | Loss: 0.00119216
Iteration 6/25 | Loss: 0.00118975
Iteration 7/25 | Loss: 0.00118897
Iteration 8/25 | Loss: 0.00118897
Iteration 9/25 | Loss: 0.00118897
Iteration 10/25 | Loss: 0.00118897
Iteration 11/25 | Loss: 0.00118897
Iteration 12/25 | Loss: 0.00118897
Iteration 13/25 | Loss: 0.00118897
Iteration 14/25 | Loss: 0.00118897
Iteration 15/25 | Loss: 0.00118897
Iteration 16/25 | Loss: 0.00118897
Iteration 17/25 | Loss: 0.00118897
Iteration 18/25 | Loss: 0.00118897
Iteration 19/25 | Loss: 0.00118897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011889655143022537, 0.0011889655143022537, 0.0011889655143022537, 0.0011889655143022537, 0.0011889655143022537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011889655143022537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30659962
Iteration 2/25 | Loss: 0.00127022
Iteration 3/25 | Loss: 0.00127022
Iteration 4/25 | Loss: 0.00127022
Iteration 5/25 | Loss: 0.00127022
Iteration 6/25 | Loss: 0.00127022
Iteration 7/25 | Loss: 0.00127022
Iteration 8/25 | Loss: 0.00127022
Iteration 9/25 | Loss: 0.00127022
Iteration 10/25 | Loss: 0.00127022
Iteration 11/25 | Loss: 0.00127022
Iteration 12/25 | Loss: 0.00127022
Iteration 13/25 | Loss: 0.00127022
Iteration 14/25 | Loss: 0.00127022
Iteration 15/25 | Loss: 0.00127022
Iteration 16/25 | Loss: 0.00127022
Iteration 17/25 | Loss: 0.00127022
Iteration 18/25 | Loss: 0.00127022
Iteration 19/25 | Loss: 0.00127022
Iteration 20/25 | Loss: 0.00127022
Iteration 21/25 | Loss: 0.00127022
Iteration 22/25 | Loss: 0.00127022
Iteration 23/25 | Loss: 0.00127022
Iteration 24/25 | Loss: 0.00127022
Iteration 25/25 | Loss: 0.00127022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127022
Iteration 2/1000 | Loss: 0.00005302
Iteration 3/1000 | Loss: 0.00003422
Iteration 4/1000 | Loss: 0.00002917
Iteration 5/1000 | Loss: 0.00002709
Iteration 6/1000 | Loss: 0.00002515
Iteration 7/1000 | Loss: 0.00002415
Iteration 8/1000 | Loss: 0.00002323
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002208
Iteration 11/1000 | Loss: 0.00002173
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002118
Iteration 15/1000 | Loss: 0.00002115
Iteration 16/1000 | Loss: 0.00002112
Iteration 17/1000 | Loss: 0.00002111
Iteration 18/1000 | Loss: 0.00002094
Iteration 19/1000 | Loss: 0.00002093
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002082
Iteration 22/1000 | Loss: 0.00002081
Iteration 23/1000 | Loss: 0.00002081
Iteration 24/1000 | Loss: 0.00002079
Iteration 25/1000 | Loss: 0.00002078
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002078
Iteration 28/1000 | Loss: 0.00002077
Iteration 29/1000 | Loss: 0.00002076
Iteration 30/1000 | Loss: 0.00002074
Iteration 31/1000 | Loss: 0.00002073
Iteration 32/1000 | Loss: 0.00002073
Iteration 33/1000 | Loss: 0.00002073
Iteration 34/1000 | Loss: 0.00002073
Iteration 35/1000 | Loss: 0.00002072
Iteration 36/1000 | Loss: 0.00002072
Iteration 37/1000 | Loss: 0.00002072
Iteration 38/1000 | Loss: 0.00002071
Iteration 39/1000 | Loss: 0.00002071
Iteration 40/1000 | Loss: 0.00002069
Iteration 41/1000 | Loss: 0.00002066
Iteration 42/1000 | Loss: 0.00002066
Iteration 43/1000 | Loss: 0.00002065
Iteration 44/1000 | Loss: 0.00002061
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002061
Iteration 48/1000 | Loss: 0.00002060
Iteration 49/1000 | Loss: 0.00002059
Iteration 50/1000 | Loss: 0.00002058
Iteration 51/1000 | Loss: 0.00002058
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002056
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002055
Iteration 62/1000 | Loss: 0.00002055
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00002054
Iteration 65/1000 | Loss: 0.00002053
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002052
Iteration 69/1000 | Loss: 0.00002052
Iteration 70/1000 | Loss: 0.00002052
Iteration 71/1000 | Loss: 0.00002051
Iteration 72/1000 | Loss: 0.00002051
Iteration 73/1000 | Loss: 0.00002051
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002049
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002047
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002044
Iteration 101/1000 | Loss: 0.00002044
Iteration 102/1000 | Loss: 0.00002044
Iteration 103/1000 | Loss: 0.00002044
Iteration 104/1000 | Loss: 0.00002043
Iteration 105/1000 | Loss: 0.00002043
Iteration 106/1000 | Loss: 0.00002043
Iteration 107/1000 | Loss: 0.00002043
Iteration 108/1000 | Loss: 0.00002043
Iteration 109/1000 | Loss: 0.00002043
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002040
Iteration 117/1000 | Loss: 0.00002040
Iteration 118/1000 | Loss: 0.00002040
Iteration 119/1000 | Loss: 0.00002040
Iteration 120/1000 | Loss: 0.00002040
Iteration 121/1000 | Loss: 0.00002039
Iteration 122/1000 | Loss: 0.00002039
Iteration 123/1000 | Loss: 0.00002038
Iteration 124/1000 | Loss: 0.00002038
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00002038
Iteration 127/1000 | Loss: 0.00002038
Iteration 128/1000 | Loss: 0.00002037
Iteration 129/1000 | Loss: 0.00002037
Iteration 130/1000 | Loss: 0.00002037
Iteration 131/1000 | Loss: 0.00002037
Iteration 132/1000 | Loss: 0.00002037
Iteration 133/1000 | Loss: 0.00002037
Iteration 134/1000 | Loss: 0.00002037
Iteration 135/1000 | Loss: 0.00002037
Iteration 136/1000 | Loss: 0.00002037
Iteration 137/1000 | Loss: 0.00002037
Iteration 138/1000 | Loss: 0.00002037
Iteration 139/1000 | Loss: 0.00002037
Iteration 140/1000 | Loss: 0.00002037
Iteration 141/1000 | Loss: 0.00002037
Iteration 142/1000 | Loss: 0.00002037
Iteration 143/1000 | Loss: 0.00002036
Iteration 144/1000 | Loss: 0.00002036
Iteration 145/1000 | Loss: 0.00002036
Iteration 146/1000 | Loss: 0.00002035
Iteration 147/1000 | Loss: 0.00002035
Iteration 148/1000 | Loss: 0.00002035
Iteration 149/1000 | Loss: 0.00002035
Iteration 150/1000 | Loss: 0.00002035
Iteration 151/1000 | Loss: 0.00002035
Iteration 152/1000 | Loss: 0.00002035
Iteration 153/1000 | Loss: 0.00002035
Iteration 154/1000 | Loss: 0.00002035
Iteration 155/1000 | Loss: 0.00002035
Iteration 156/1000 | Loss: 0.00002035
Iteration 157/1000 | Loss: 0.00002035
Iteration 158/1000 | Loss: 0.00002035
Iteration 159/1000 | Loss: 0.00002035
Iteration 160/1000 | Loss: 0.00002035
Iteration 161/1000 | Loss: 0.00002035
Iteration 162/1000 | Loss: 0.00002035
Iteration 163/1000 | Loss: 0.00002035
Iteration 164/1000 | Loss: 0.00002035
Iteration 165/1000 | Loss: 0.00002035
Iteration 166/1000 | Loss: 0.00002035
Iteration 167/1000 | Loss: 0.00002035
Iteration 168/1000 | Loss: 0.00002035
Iteration 169/1000 | Loss: 0.00002035
Iteration 170/1000 | Loss: 0.00002035
Iteration 171/1000 | Loss: 0.00002035
Iteration 172/1000 | Loss: 0.00002035
Iteration 173/1000 | Loss: 0.00002035
Iteration 174/1000 | Loss: 0.00002035
Iteration 175/1000 | Loss: 0.00002035
Iteration 176/1000 | Loss: 0.00002035
Iteration 177/1000 | Loss: 0.00002035
Iteration 178/1000 | Loss: 0.00002035
Iteration 179/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.034815406659618e-05, 2.034815406659618e-05, 2.034815406659618e-05, 2.034815406659618e-05, 2.034815406659618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.034815406659618e-05

Optimization complete. Final v2v error: 3.852712869644165 mm

Highest mean error: 4.403623104095459 mm for frame 94

Lowest mean error: 3.2066259384155273 mm for frame 57

Saving results

Total time: 43.45219564437866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739999
Iteration 2/25 | Loss: 0.00167237
Iteration 3/25 | Loss: 0.00129560
Iteration 4/25 | Loss: 0.00123695
Iteration 5/25 | Loss: 0.00122647
Iteration 6/25 | Loss: 0.00120464
Iteration 7/25 | Loss: 0.00119563
Iteration 8/25 | Loss: 0.00119371
Iteration 9/25 | Loss: 0.00119283
Iteration 10/25 | Loss: 0.00119238
Iteration 11/25 | Loss: 0.00119228
Iteration 12/25 | Loss: 0.00119359
Iteration 13/25 | Loss: 0.00119536
Iteration 14/25 | Loss: 0.00119602
Iteration 15/25 | Loss: 0.00119343
Iteration 16/25 | Loss: 0.00119086
Iteration 17/25 | Loss: 0.00118800
Iteration 18/25 | Loss: 0.00118674
Iteration 19/25 | Loss: 0.00118597
Iteration 20/25 | Loss: 0.00118555
Iteration 21/25 | Loss: 0.00118533
Iteration 22/25 | Loss: 0.00118517
Iteration 23/25 | Loss: 0.00118823
Iteration 24/25 | Loss: 0.00118736
Iteration 25/25 | Loss: 0.00118433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.82088757
Iteration 2/25 | Loss: 0.00086993
Iteration 3/25 | Loss: 0.00086980
Iteration 4/25 | Loss: 0.00086980
Iteration 5/25 | Loss: 0.00086980
Iteration 6/25 | Loss: 0.00086980
Iteration 7/25 | Loss: 0.00086980
Iteration 8/25 | Loss: 0.00086980
Iteration 9/25 | Loss: 0.00086980
Iteration 10/25 | Loss: 0.00086980
Iteration 11/25 | Loss: 0.00086980
Iteration 12/25 | Loss: 0.00086980
Iteration 13/25 | Loss: 0.00086980
Iteration 14/25 | Loss: 0.00086980
Iteration 15/25 | Loss: 0.00086980
Iteration 16/25 | Loss: 0.00086980
Iteration 17/25 | Loss: 0.00086980
Iteration 18/25 | Loss: 0.00086980
Iteration 19/25 | Loss: 0.00086980
Iteration 20/25 | Loss: 0.00086980
Iteration 21/25 | Loss: 0.00086980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008697970188222826, 0.0008697970188222826, 0.0008697970188222826, 0.0008697970188222826, 0.0008697970188222826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008697970188222826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086980
Iteration 2/1000 | Loss: 0.00004304
Iteration 3/1000 | Loss: 0.00002873
Iteration 4/1000 | Loss: 0.00002478
Iteration 5/1000 | Loss: 0.00009904
Iteration 6/1000 | Loss: 0.00002206
Iteration 7/1000 | Loss: 0.00002086
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00001972
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00002834
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00001982
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001840
Iteration 16/1000 | Loss: 0.00003613
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001817
Iteration 19/1000 | Loss: 0.00001815
Iteration 20/1000 | Loss: 0.00001814
Iteration 21/1000 | Loss: 0.00001813
Iteration 22/1000 | Loss: 0.00001810
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001804
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001803
Iteration 28/1000 | Loss: 0.00001802
Iteration 29/1000 | Loss: 0.00001802
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001800
Iteration 33/1000 | Loss: 0.00001798
Iteration 34/1000 | Loss: 0.00001798
Iteration 35/1000 | Loss: 0.00001797
Iteration 36/1000 | Loss: 0.00001796
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001795
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001793
Iteration 42/1000 | Loss: 0.00001792
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001789
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001786
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001786
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001785
Iteration 66/1000 | Loss: 0.00001785
Iteration 67/1000 | Loss: 0.00001785
Iteration 68/1000 | Loss: 0.00001785
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001783
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001781
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00003245
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001775
Iteration 98/1000 | Loss: 0.00001775
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001774
Iteration 104/1000 | Loss: 0.00001774
Iteration 105/1000 | Loss: 0.00001774
Iteration 106/1000 | Loss: 0.00001774
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001773
Iteration 112/1000 | Loss: 0.00001773
Iteration 113/1000 | Loss: 0.00001773
Iteration 114/1000 | Loss: 0.00001773
Iteration 115/1000 | Loss: 0.00001773
Iteration 116/1000 | Loss: 0.00001773
Iteration 117/1000 | Loss: 0.00001773
Iteration 118/1000 | Loss: 0.00001773
Iteration 119/1000 | Loss: 0.00001773
Iteration 120/1000 | Loss: 0.00001773
Iteration 121/1000 | Loss: 0.00001773
Iteration 122/1000 | Loss: 0.00001772
Iteration 123/1000 | Loss: 0.00001772
Iteration 124/1000 | Loss: 0.00001772
Iteration 125/1000 | Loss: 0.00001772
Iteration 126/1000 | Loss: 0.00001772
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001772
Iteration 129/1000 | Loss: 0.00001772
Iteration 130/1000 | Loss: 0.00001772
Iteration 131/1000 | Loss: 0.00001772
Iteration 132/1000 | Loss: 0.00001772
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00001771
Iteration 135/1000 | Loss: 0.00001771
Iteration 136/1000 | Loss: 0.00001771
Iteration 137/1000 | Loss: 0.00001771
Iteration 138/1000 | Loss: 0.00001771
Iteration 139/1000 | Loss: 0.00001771
Iteration 140/1000 | Loss: 0.00001770
Iteration 141/1000 | Loss: 0.00001770
Iteration 142/1000 | Loss: 0.00001770
Iteration 143/1000 | Loss: 0.00001770
Iteration 144/1000 | Loss: 0.00001770
Iteration 145/1000 | Loss: 0.00001770
Iteration 146/1000 | Loss: 0.00001770
Iteration 147/1000 | Loss: 0.00001770
Iteration 148/1000 | Loss: 0.00001769
Iteration 149/1000 | Loss: 0.00001769
Iteration 150/1000 | Loss: 0.00001769
Iteration 151/1000 | Loss: 0.00001769
Iteration 152/1000 | Loss: 0.00001768
Iteration 153/1000 | Loss: 0.00001768
Iteration 154/1000 | Loss: 0.00001768
Iteration 155/1000 | Loss: 0.00001768
Iteration 156/1000 | Loss: 0.00001768
Iteration 157/1000 | Loss: 0.00001768
Iteration 158/1000 | Loss: 0.00001768
Iteration 159/1000 | Loss: 0.00001767
Iteration 160/1000 | Loss: 0.00001767
Iteration 161/1000 | Loss: 0.00001767
Iteration 162/1000 | Loss: 0.00001767
Iteration 163/1000 | Loss: 0.00001767
Iteration 164/1000 | Loss: 0.00001767
Iteration 165/1000 | Loss: 0.00001767
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001767
Iteration 169/1000 | Loss: 0.00001766
Iteration 170/1000 | Loss: 0.00001766
Iteration 171/1000 | Loss: 0.00001766
Iteration 172/1000 | Loss: 0.00001766
Iteration 173/1000 | Loss: 0.00001766
Iteration 174/1000 | Loss: 0.00001766
Iteration 175/1000 | Loss: 0.00001766
Iteration 176/1000 | Loss: 0.00001766
Iteration 177/1000 | Loss: 0.00001765
Iteration 178/1000 | Loss: 0.00001765
Iteration 179/1000 | Loss: 0.00001765
Iteration 180/1000 | Loss: 0.00001765
Iteration 181/1000 | Loss: 0.00001765
Iteration 182/1000 | Loss: 0.00001765
Iteration 183/1000 | Loss: 0.00001765
Iteration 184/1000 | Loss: 0.00001765
Iteration 185/1000 | Loss: 0.00001765
Iteration 186/1000 | Loss: 0.00001765
Iteration 187/1000 | Loss: 0.00001765
Iteration 188/1000 | Loss: 0.00001765
Iteration 189/1000 | Loss: 0.00001765
Iteration 190/1000 | Loss: 0.00001765
Iteration 191/1000 | Loss: 0.00001764
Iteration 192/1000 | Loss: 0.00003432
Iteration 193/1000 | Loss: 0.00001769
Iteration 194/1000 | Loss: 0.00001764
Iteration 195/1000 | Loss: 0.00001763
Iteration 196/1000 | Loss: 0.00001763
Iteration 197/1000 | Loss: 0.00001763
Iteration 198/1000 | Loss: 0.00001763
Iteration 199/1000 | Loss: 0.00001763
Iteration 200/1000 | Loss: 0.00001763
Iteration 201/1000 | Loss: 0.00001763
Iteration 202/1000 | Loss: 0.00001763
Iteration 203/1000 | Loss: 0.00001763
Iteration 204/1000 | Loss: 0.00001763
Iteration 205/1000 | Loss: 0.00001763
Iteration 206/1000 | Loss: 0.00001763
Iteration 207/1000 | Loss: 0.00001763
Iteration 208/1000 | Loss: 0.00001763
Iteration 209/1000 | Loss: 0.00001762
Iteration 210/1000 | Loss: 0.00001762
Iteration 211/1000 | Loss: 0.00001762
Iteration 212/1000 | Loss: 0.00001762
Iteration 213/1000 | Loss: 0.00001762
Iteration 214/1000 | Loss: 0.00001762
Iteration 215/1000 | Loss: 0.00001762
Iteration 216/1000 | Loss: 0.00001762
Iteration 217/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.7621881852392107e-05, 1.7621881852392107e-05, 1.7621881852392107e-05, 1.7621881852392107e-05, 1.7621881852392107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7621881852392107e-05

Optimization complete. Final v2v error: 3.479877233505249 mm

Highest mean error: 5.418039321899414 mm for frame 173

Lowest mean error: 2.6772701740264893 mm for frame 118

Saving results

Total time: 99.01751732826233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371749
Iteration 2/25 | Loss: 0.00117908
Iteration 3/25 | Loss: 0.00111571
Iteration 4/25 | Loss: 0.00111059
Iteration 5/25 | Loss: 0.00110897
Iteration 6/25 | Loss: 0.00110849
Iteration 7/25 | Loss: 0.00110849
Iteration 8/25 | Loss: 0.00110849
Iteration 9/25 | Loss: 0.00110849
Iteration 10/25 | Loss: 0.00110849
Iteration 11/25 | Loss: 0.00110849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011084874859079719, 0.0011084874859079719, 0.0011084874859079719, 0.0011084874859079719, 0.0011084874859079719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011084874859079719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75661552
Iteration 2/25 | Loss: 0.00087329
Iteration 3/25 | Loss: 0.00087328
Iteration 4/25 | Loss: 0.00087328
Iteration 5/25 | Loss: 0.00087328
Iteration 6/25 | Loss: 0.00087328
Iteration 7/25 | Loss: 0.00087328
Iteration 8/25 | Loss: 0.00087328
Iteration 9/25 | Loss: 0.00087328
Iteration 10/25 | Loss: 0.00087328
Iteration 11/25 | Loss: 0.00087328
Iteration 12/25 | Loss: 0.00087327
Iteration 13/25 | Loss: 0.00087327
Iteration 14/25 | Loss: 0.00087327
Iteration 15/25 | Loss: 0.00087327
Iteration 16/25 | Loss: 0.00087327
Iteration 17/25 | Loss: 0.00087327
Iteration 18/25 | Loss: 0.00087327
Iteration 19/25 | Loss: 0.00087327
Iteration 20/25 | Loss: 0.00087327
Iteration 21/25 | Loss: 0.00087327
Iteration 22/25 | Loss: 0.00087327
Iteration 23/25 | Loss: 0.00087327
Iteration 24/25 | Loss: 0.00087327
Iteration 25/25 | Loss: 0.00087327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087327
Iteration 2/1000 | Loss: 0.00002565
Iteration 3/1000 | Loss: 0.00001559
Iteration 4/1000 | Loss: 0.00001249
Iteration 5/1000 | Loss: 0.00001107
Iteration 6/1000 | Loss: 0.00001040
Iteration 7/1000 | Loss: 0.00000991
Iteration 8/1000 | Loss: 0.00000956
Iteration 9/1000 | Loss: 0.00000953
Iteration 10/1000 | Loss: 0.00000935
Iteration 11/1000 | Loss: 0.00000919
Iteration 12/1000 | Loss: 0.00000913
Iteration 13/1000 | Loss: 0.00000908
Iteration 14/1000 | Loss: 0.00000905
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000897
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000895
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000894
Iteration 21/1000 | Loss: 0.00000893
Iteration 22/1000 | Loss: 0.00000892
Iteration 23/1000 | Loss: 0.00000890
Iteration 24/1000 | Loss: 0.00000890
Iteration 25/1000 | Loss: 0.00000889
Iteration 26/1000 | Loss: 0.00000889
Iteration 27/1000 | Loss: 0.00000888
Iteration 28/1000 | Loss: 0.00000888
Iteration 29/1000 | Loss: 0.00000888
Iteration 30/1000 | Loss: 0.00000886
Iteration 31/1000 | Loss: 0.00000885
Iteration 32/1000 | Loss: 0.00000885
Iteration 33/1000 | Loss: 0.00000885
Iteration 34/1000 | Loss: 0.00000885
Iteration 35/1000 | Loss: 0.00000885
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000885
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000880
Iteration 41/1000 | Loss: 0.00000880
Iteration 42/1000 | Loss: 0.00000879
Iteration 43/1000 | Loss: 0.00000878
Iteration 44/1000 | Loss: 0.00000878
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000878
Iteration 47/1000 | Loss: 0.00000877
Iteration 48/1000 | Loss: 0.00000877
Iteration 49/1000 | Loss: 0.00000877
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000874
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000874
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000872
Iteration 60/1000 | Loss: 0.00000872
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000872
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000871
Iteration 68/1000 | Loss: 0.00000871
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000870
Iteration 75/1000 | Loss: 0.00000870
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000869
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000868
Iteration 80/1000 | Loss: 0.00000868
Iteration 81/1000 | Loss: 0.00000868
Iteration 82/1000 | Loss: 0.00000868
Iteration 83/1000 | Loss: 0.00000867
Iteration 84/1000 | Loss: 0.00000867
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000867
Iteration 87/1000 | Loss: 0.00000867
Iteration 88/1000 | Loss: 0.00000867
Iteration 89/1000 | Loss: 0.00000867
Iteration 90/1000 | Loss: 0.00000867
Iteration 91/1000 | Loss: 0.00000867
Iteration 92/1000 | Loss: 0.00000867
Iteration 93/1000 | Loss: 0.00000867
Iteration 94/1000 | Loss: 0.00000867
Iteration 95/1000 | Loss: 0.00000866
Iteration 96/1000 | Loss: 0.00000866
Iteration 97/1000 | Loss: 0.00000866
Iteration 98/1000 | Loss: 0.00000866
Iteration 99/1000 | Loss: 0.00000865
Iteration 100/1000 | Loss: 0.00000865
Iteration 101/1000 | Loss: 0.00000865
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000865
Iteration 104/1000 | Loss: 0.00000865
Iteration 105/1000 | Loss: 0.00000865
Iteration 106/1000 | Loss: 0.00000865
Iteration 107/1000 | Loss: 0.00000865
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000864
Iteration 110/1000 | Loss: 0.00000864
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000864
Iteration 113/1000 | Loss: 0.00000863
Iteration 114/1000 | Loss: 0.00000863
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000862
Iteration 118/1000 | Loss: 0.00000862
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000860
Iteration 123/1000 | Loss: 0.00000860
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000858
Iteration 142/1000 | Loss: 0.00000858
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000857
Iteration 150/1000 | Loss: 0.00000857
Iteration 151/1000 | Loss: 0.00000857
Iteration 152/1000 | Loss: 0.00000857
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000856
Iteration 155/1000 | Loss: 0.00000856
Iteration 156/1000 | Loss: 0.00000855
Iteration 157/1000 | Loss: 0.00000855
Iteration 158/1000 | Loss: 0.00000855
Iteration 159/1000 | Loss: 0.00000855
Iteration 160/1000 | Loss: 0.00000855
Iteration 161/1000 | Loss: 0.00000855
Iteration 162/1000 | Loss: 0.00000854
Iteration 163/1000 | Loss: 0.00000854
Iteration 164/1000 | Loss: 0.00000854
Iteration 165/1000 | Loss: 0.00000854
Iteration 166/1000 | Loss: 0.00000853
Iteration 167/1000 | Loss: 0.00000853
Iteration 168/1000 | Loss: 0.00000853
Iteration 169/1000 | Loss: 0.00000853
Iteration 170/1000 | Loss: 0.00000853
Iteration 171/1000 | Loss: 0.00000853
Iteration 172/1000 | Loss: 0.00000852
Iteration 173/1000 | Loss: 0.00000852
Iteration 174/1000 | Loss: 0.00000852
Iteration 175/1000 | Loss: 0.00000852
Iteration 176/1000 | Loss: 0.00000852
Iteration 177/1000 | Loss: 0.00000852
Iteration 178/1000 | Loss: 0.00000852
Iteration 179/1000 | Loss: 0.00000852
Iteration 180/1000 | Loss: 0.00000852
Iteration 181/1000 | Loss: 0.00000852
Iteration 182/1000 | Loss: 0.00000851
Iteration 183/1000 | Loss: 0.00000851
Iteration 184/1000 | Loss: 0.00000851
Iteration 185/1000 | Loss: 0.00000851
Iteration 186/1000 | Loss: 0.00000851
Iteration 187/1000 | Loss: 0.00000851
Iteration 188/1000 | Loss: 0.00000851
Iteration 189/1000 | Loss: 0.00000851
Iteration 190/1000 | Loss: 0.00000851
Iteration 191/1000 | Loss: 0.00000851
Iteration 192/1000 | Loss: 0.00000851
Iteration 193/1000 | Loss: 0.00000851
Iteration 194/1000 | Loss: 0.00000851
Iteration 195/1000 | Loss: 0.00000851
Iteration 196/1000 | Loss: 0.00000850
Iteration 197/1000 | Loss: 0.00000850
Iteration 198/1000 | Loss: 0.00000850
Iteration 199/1000 | Loss: 0.00000850
Iteration 200/1000 | Loss: 0.00000850
Iteration 201/1000 | Loss: 0.00000850
Iteration 202/1000 | Loss: 0.00000850
Iteration 203/1000 | Loss: 0.00000850
Iteration 204/1000 | Loss: 0.00000850
Iteration 205/1000 | Loss: 0.00000850
Iteration 206/1000 | Loss: 0.00000850
Iteration 207/1000 | Loss: 0.00000850
Iteration 208/1000 | Loss: 0.00000849
Iteration 209/1000 | Loss: 0.00000849
Iteration 210/1000 | Loss: 0.00000849
Iteration 211/1000 | Loss: 0.00000849
Iteration 212/1000 | Loss: 0.00000849
Iteration 213/1000 | Loss: 0.00000849
Iteration 214/1000 | Loss: 0.00000849
Iteration 215/1000 | Loss: 0.00000849
Iteration 216/1000 | Loss: 0.00000849
Iteration 217/1000 | Loss: 0.00000849
Iteration 218/1000 | Loss: 0.00000849
Iteration 219/1000 | Loss: 0.00000849
Iteration 220/1000 | Loss: 0.00000848
Iteration 221/1000 | Loss: 0.00000848
Iteration 222/1000 | Loss: 0.00000848
Iteration 223/1000 | Loss: 0.00000848
Iteration 224/1000 | Loss: 0.00000848
Iteration 225/1000 | Loss: 0.00000848
Iteration 226/1000 | Loss: 0.00000848
Iteration 227/1000 | Loss: 0.00000848
Iteration 228/1000 | Loss: 0.00000848
Iteration 229/1000 | Loss: 0.00000848
Iteration 230/1000 | Loss: 0.00000847
Iteration 231/1000 | Loss: 0.00000847
Iteration 232/1000 | Loss: 0.00000847
Iteration 233/1000 | Loss: 0.00000847
Iteration 234/1000 | Loss: 0.00000847
Iteration 235/1000 | Loss: 0.00000847
Iteration 236/1000 | Loss: 0.00000847
Iteration 237/1000 | Loss: 0.00000847
Iteration 238/1000 | Loss: 0.00000847
Iteration 239/1000 | Loss: 0.00000847
Iteration 240/1000 | Loss: 0.00000847
Iteration 241/1000 | Loss: 0.00000846
Iteration 242/1000 | Loss: 0.00000846
Iteration 243/1000 | Loss: 0.00000846
Iteration 244/1000 | Loss: 0.00000846
Iteration 245/1000 | Loss: 0.00000846
Iteration 246/1000 | Loss: 0.00000846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [8.464996426482685e-06, 8.464996426482685e-06, 8.464996426482685e-06, 8.464996426482685e-06, 8.464996426482685e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.464996426482685e-06

Optimization complete. Final v2v error: 2.498382806777954 mm

Highest mean error: 2.9555160999298096 mm for frame 78

Lowest mean error: 2.4049229621887207 mm for frame 113

Saving results

Total time: 41.5912868976593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772291
Iteration 2/25 | Loss: 0.00130348
Iteration 3/25 | Loss: 0.00116473
Iteration 4/25 | Loss: 0.00114713
Iteration 5/25 | Loss: 0.00114401
Iteration 6/25 | Loss: 0.00114401
Iteration 7/25 | Loss: 0.00114401
Iteration 8/25 | Loss: 0.00114401
Iteration 9/25 | Loss: 0.00114401
Iteration 10/25 | Loss: 0.00114401
Iteration 11/25 | Loss: 0.00114401
Iteration 12/25 | Loss: 0.00114401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011440098751336336, 0.0011440098751336336, 0.0011440098751336336, 0.0011440098751336336, 0.0011440098751336336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011440098751336336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31836534
Iteration 2/25 | Loss: 0.00059392
Iteration 3/25 | Loss: 0.00059388
Iteration 4/25 | Loss: 0.00059388
Iteration 5/25 | Loss: 0.00059388
Iteration 6/25 | Loss: 0.00059388
Iteration 7/25 | Loss: 0.00059388
Iteration 8/25 | Loss: 0.00059388
Iteration 9/25 | Loss: 0.00059388
Iteration 10/25 | Loss: 0.00059388
Iteration 11/25 | Loss: 0.00059388
Iteration 12/25 | Loss: 0.00059388
Iteration 13/25 | Loss: 0.00059388
Iteration 14/25 | Loss: 0.00059388
Iteration 15/25 | Loss: 0.00059388
Iteration 16/25 | Loss: 0.00059388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005938814138062298, 0.0005938814138062298, 0.0005938814138062298, 0.0005938814138062298, 0.0005938814138062298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005938814138062298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059388
Iteration 2/1000 | Loss: 0.00002700
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001612
Iteration 6/1000 | Loss: 0.00001535
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001403
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001365
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001340
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001299
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001298
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001283
Iteration 88/1000 | Loss: 0.00001283
Iteration 89/1000 | Loss: 0.00001283
Iteration 90/1000 | Loss: 0.00001283
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00001281
Iteration 109/1000 | Loss: 0.00001280
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001278
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001278
Iteration 136/1000 | Loss: 0.00001278
Iteration 137/1000 | Loss: 0.00001278
Iteration 138/1000 | Loss: 0.00001278
Iteration 139/1000 | Loss: 0.00001278
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001277
Iteration 152/1000 | Loss: 0.00001277
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001276
Iteration 161/1000 | Loss: 0.00001276
Iteration 162/1000 | Loss: 0.00001276
Iteration 163/1000 | Loss: 0.00001276
Iteration 164/1000 | Loss: 0.00001276
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001275
Iteration 174/1000 | Loss: 0.00001275
Iteration 175/1000 | Loss: 0.00001275
Iteration 176/1000 | Loss: 0.00001275
Iteration 177/1000 | Loss: 0.00001275
Iteration 178/1000 | Loss: 0.00001275
Iteration 179/1000 | Loss: 0.00001275
Iteration 180/1000 | Loss: 0.00001275
Iteration 181/1000 | Loss: 0.00001275
Iteration 182/1000 | Loss: 0.00001275
Iteration 183/1000 | Loss: 0.00001275
Iteration 184/1000 | Loss: 0.00001275
Iteration 185/1000 | Loss: 0.00001275
Iteration 186/1000 | Loss: 0.00001275
Iteration 187/1000 | Loss: 0.00001274
Iteration 188/1000 | Loss: 0.00001274
Iteration 189/1000 | Loss: 0.00001274
Iteration 190/1000 | Loss: 0.00001274
Iteration 191/1000 | Loss: 0.00001274
Iteration 192/1000 | Loss: 0.00001273
Iteration 193/1000 | Loss: 0.00001273
Iteration 194/1000 | Loss: 0.00001273
Iteration 195/1000 | Loss: 0.00001273
Iteration 196/1000 | Loss: 0.00001273
Iteration 197/1000 | Loss: 0.00001273
Iteration 198/1000 | Loss: 0.00001273
Iteration 199/1000 | Loss: 0.00001273
Iteration 200/1000 | Loss: 0.00001273
Iteration 201/1000 | Loss: 0.00001273
Iteration 202/1000 | Loss: 0.00001273
Iteration 203/1000 | Loss: 0.00001273
Iteration 204/1000 | Loss: 0.00001273
Iteration 205/1000 | Loss: 0.00001273
Iteration 206/1000 | Loss: 0.00001273
Iteration 207/1000 | Loss: 0.00001273
Iteration 208/1000 | Loss: 0.00001273
Iteration 209/1000 | Loss: 0.00001273
Iteration 210/1000 | Loss: 0.00001273
Iteration 211/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.2731710739899427e-05, 1.2731710739899427e-05, 1.2731710739899427e-05, 1.2731710739899427e-05, 1.2731710739899427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2731710739899427e-05

Optimization complete. Final v2v error: 3.0218467712402344 mm

Highest mean error: 3.4148340225219727 mm for frame 112

Lowest mean error: 2.7278213500976562 mm for frame 23

Saving results

Total time: 46.506197690963745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008786
Iteration 2/25 | Loss: 0.00292131
Iteration 3/25 | Loss: 0.00243571
Iteration 4/25 | Loss: 0.00199031
Iteration 5/25 | Loss: 0.00174955
Iteration 6/25 | Loss: 0.00169096
Iteration 7/25 | Loss: 0.00156474
Iteration 8/25 | Loss: 0.00151562
Iteration 9/25 | Loss: 0.00142651
Iteration 10/25 | Loss: 0.00137171
Iteration 11/25 | Loss: 0.00134752
Iteration 12/25 | Loss: 0.00136633
Iteration 13/25 | Loss: 0.00131194
Iteration 14/25 | Loss: 0.00130360
Iteration 15/25 | Loss: 0.00130622
Iteration 16/25 | Loss: 0.00128443
Iteration 17/25 | Loss: 0.00129085
Iteration 18/25 | Loss: 0.00129105
Iteration 19/25 | Loss: 0.00128837
Iteration 20/25 | Loss: 0.00128491
Iteration 21/25 | Loss: 0.00127078
Iteration 22/25 | Loss: 0.00127293
Iteration 23/25 | Loss: 0.00126967
Iteration 24/25 | Loss: 0.00126741
Iteration 25/25 | Loss: 0.00126562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43527853
Iteration 2/25 | Loss: 0.00130434
Iteration 3/25 | Loss: 0.00119105
Iteration 4/25 | Loss: 0.00109904
Iteration 5/25 | Loss: 0.00109904
Iteration 6/25 | Loss: 0.00109903
Iteration 7/25 | Loss: 0.00109903
Iteration 8/25 | Loss: 0.00109902
Iteration 9/25 | Loss: 0.00109902
Iteration 10/25 | Loss: 0.00109902
Iteration 11/25 | Loss: 0.00109902
Iteration 12/25 | Loss: 0.00109902
Iteration 13/25 | Loss: 0.00109902
Iteration 14/25 | Loss: 0.00109902
Iteration 15/25 | Loss: 0.00109902
Iteration 16/25 | Loss: 0.00109902
Iteration 17/25 | Loss: 0.00109902
Iteration 18/25 | Loss: 0.00109902
Iteration 19/25 | Loss: 0.00109902
Iteration 20/25 | Loss: 0.00109902
Iteration 21/25 | Loss: 0.00109902
Iteration 22/25 | Loss: 0.00109902
Iteration 23/25 | Loss: 0.00109902
Iteration 24/25 | Loss: 0.00109902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010990219889208674, 0.0010990219889208674, 0.0010990219889208674, 0.0010990219889208674, 0.0010990219889208674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010990219889208674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109902
Iteration 2/1000 | Loss: 0.00081811
Iteration 3/1000 | Loss: 0.00135505
Iteration 4/1000 | Loss: 0.00184770
Iteration 5/1000 | Loss: 0.00047647
Iteration 6/1000 | Loss: 0.00233141
Iteration 7/1000 | Loss: 0.00072590
Iteration 8/1000 | Loss: 0.00028321
Iteration 9/1000 | Loss: 0.00012138
Iteration 10/1000 | Loss: 0.00038665
Iteration 11/1000 | Loss: 0.00009050
Iteration 12/1000 | Loss: 0.00048369
Iteration 13/1000 | Loss: 0.00031226
Iteration 14/1000 | Loss: 0.00033364
Iteration 15/1000 | Loss: 0.00043678
Iteration 16/1000 | Loss: 0.00044722
Iteration 17/1000 | Loss: 0.00017833
Iteration 18/1000 | Loss: 0.00009747
Iteration 19/1000 | Loss: 0.00013918
Iteration 20/1000 | Loss: 0.00006366
Iteration 21/1000 | Loss: 0.00030609
Iteration 22/1000 | Loss: 0.00006964
Iteration 23/1000 | Loss: 0.00006379
Iteration 24/1000 | Loss: 0.00006521
Iteration 25/1000 | Loss: 0.00005329
Iteration 26/1000 | Loss: 0.00014535
Iteration 27/1000 | Loss: 0.00046304
Iteration 28/1000 | Loss: 0.00015303
Iteration 29/1000 | Loss: 0.00018925
Iteration 30/1000 | Loss: 0.00019708
Iteration 31/1000 | Loss: 0.00009449
Iteration 32/1000 | Loss: 0.00005074
Iteration 33/1000 | Loss: 0.00006541
Iteration 34/1000 | Loss: 0.00008880
Iteration 35/1000 | Loss: 0.00046528
Iteration 36/1000 | Loss: 0.00022697
Iteration 37/1000 | Loss: 0.00021804
Iteration 38/1000 | Loss: 0.00030318
Iteration 39/1000 | Loss: 0.00059363
Iteration 40/1000 | Loss: 0.00031011
Iteration 41/1000 | Loss: 0.00076221
Iteration 42/1000 | Loss: 0.00036996
Iteration 43/1000 | Loss: 0.00006007
Iteration 44/1000 | Loss: 0.00011078
Iteration 45/1000 | Loss: 0.00004435
Iteration 46/1000 | Loss: 0.00005640
Iteration 47/1000 | Loss: 0.00028548
Iteration 48/1000 | Loss: 0.00007838
Iteration 49/1000 | Loss: 0.00004338
Iteration 50/1000 | Loss: 0.00005013
Iteration 51/1000 | Loss: 0.00002980
Iteration 52/1000 | Loss: 0.00010187
Iteration 53/1000 | Loss: 0.00005069
Iteration 54/1000 | Loss: 0.00011378
Iteration 55/1000 | Loss: 0.00002842
Iteration 56/1000 | Loss: 0.00002784
Iteration 57/1000 | Loss: 0.00046671
Iteration 58/1000 | Loss: 0.00009168
Iteration 59/1000 | Loss: 0.00027008
Iteration 60/1000 | Loss: 0.00073640
Iteration 61/1000 | Loss: 0.00071725
Iteration 62/1000 | Loss: 0.00167572
Iteration 63/1000 | Loss: 0.00013287
Iteration 64/1000 | Loss: 0.00025321
Iteration 65/1000 | Loss: 0.00046337
Iteration 66/1000 | Loss: 0.00008215
Iteration 67/1000 | Loss: 0.00002800
Iteration 68/1000 | Loss: 0.00002670
Iteration 69/1000 | Loss: 0.00005847
Iteration 70/1000 | Loss: 0.00002580
Iteration 71/1000 | Loss: 0.00014712
Iteration 72/1000 | Loss: 0.00072110
Iteration 73/1000 | Loss: 0.00078932
Iteration 74/1000 | Loss: 0.00018206
Iteration 75/1000 | Loss: 0.00047091
Iteration 76/1000 | Loss: 0.00004676
Iteration 77/1000 | Loss: 0.00002518
Iteration 78/1000 | Loss: 0.00002493
Iteration 79/1000 | Loss: 0.00006912
Iteration 80/1000 | Loss: 0.00002473
Iteration 81/1000 | Loss: 0.00002464
Iteration 82/1000 | Loss: 0.00002464
Iteration 83/1000 | Loss: 0.00002463
Iteration 84/1000 | Loss: 0.00002463
Iteration 85/1000 | Loss: 0.00002462
Iteration 86/1000 | Loss: 0.00002459
Iteration 87/1000 | Loss: 0.00008292
Iteration 88/1000 | Loss: 0.00014882
Iteration 89/1000 | Loss: 0.00010764
Iteration 90/1000 | Loss: 0.00003287
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002449
Iteration 93/1000 | Loss: 0.00002443
Iteration 94/1000 | Loss: 0.00002443
Iteration 95/1000 | Loss: 0.00002442
Iteration 96/1000 | Loss: 0.00002442
Iteration 97/1000 | Loss: 0.00002442
Iteration 98/1000 | Loss: 0.00002442
Iteration 99/1000 | Loss: 0.00002442
Iteration 100/1000 | Loss: 0.00002442
Iteration 101/1000 | Loss: 0.00002442
Iteration 102/1000 | Loss: 0.00002442
Iteration 103/1000 | Loss: 0.00002442
Iteration 104/1000 | Loss: 0.00002442
Iteration 105/1000 | Loss: 0.00002442
Iteration 106/1000 | Loss: 0.00002441
Iteration 107/1000 | Loss: 0.00002441
Iteration 108/1000 | Loss: 0.00002441
Iteration 109/1000 | Loss: 0.00002441
Iteration 110/1000 | Loss: 0.00002440
Iteration 111/1000 | Loss: 0.00002439
Iteration 112/1000 | Loss: 0.00002438
Iteration 113/1000 | Loss: 0.00002438
Iteration 114/1000 | Loss: 0.00002438
Iteration 115/1000 | Loss: 0.00002437
Iteration 116/1000 | Loss: 0.00002437
Iteration 117/1000 | Loss: 0.00002436
Iteration 118/1000 | Loss: 0.00002435
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002433
Iteration 121/1000 | Loss: 0.00002433
Iteration 122/1000 | Loss: 0.00002433
Iteration 123/1000 | Loss: 0.00002433
Iteration 124/1000 | Loss: 0.00002432
Iteration 125/1000 | Loss: 0.00002432
Iteration 126/1000 | Loss: 0.00002431
Iteration 127/1000 | Loss: 0.00002430
Iteration 128/1000 | Loss: 0.00002430
Iteration 129/1000 | Loss: 0.00002430
Iteration 130/1000 | Loss: 0.00002430
Iteration 131/1000 | Loss: 0.00002430
Iteration 132/1000 | Loss: 0.00002430
Iteration 133/1000 | Loss: 0.00002429
Iteration 134/1000 | Loss: 0.00002429
Iteration 135/1000 | Loss: 0.00002429
Iteration 136/1000 | Loss: 0.00002429
Iteration 137/1000 | Loss: 0.00002429
Iteration 138/1000 | Loss: 0.00002429
Iteration 139/1000 | Loss: 0.00002429
Iteration 140/1000 | Loss: 0.00002429
Iteration 141/1000 | Loss: 0.00002429
Iteration 142/1000 | Loss: 0.00002428
Iteration 143/1000 | Loss: 0.00002428
Iteration 144/1000 | Loss: 0.00002428
Iteration 145/1000 | Loss: 0.00002428
Iteration 146/1000 | Loss: 0.00002427
Iteration 147/1000 | Loss: 0.00002427
Iteration 148/1000 | Loss: 0.00002427
Iteration 149/1000 | Loss: 0.00002427
Iteration 150/1000 | Loss: 0.00002427
Iteration 151/1000 | Loss: 0.00002426
Iteration 152/1000 | Loss: 0.00002426
Iteration 153/1000 | Loss: 0.00002426
Iteration 154/1000 | Loss: 0.00002426
Iteration 155/1000 | Loss: 0.00002426
Iteration 156/1000 | Loss: 0.00002426
Iteration 157/1000 | Loss: 0.00002426
Iteration 158/1000 | Loss: 0.00002426
Iteration 159/1000 | Loss: 0.00002425
Iteration 160/1000 | Loss: 0.00002425
Iteration 161/1000 | Loss: 0.00002425
Iteration 162/1000 | Loss: 0.00002425
Iteration 163/1000 | Loss: 0.00002425
Iteration 164/1000 | Loss: 0.00002425
Iteration 165/1000 | Loss: 0.00002425
Iteration 166/1000 | Loss: 0.00002425
Iteration 167/1000 | Loss: 0.00002425
Iteration 168/1000 | Loss: 0.00002425
Iteration 169/1000 | Loss: 0.00002425
Iteration 170/1000 | Loss: 0.00002424
Iteration 171/1000 | Loss: 0.00002424
Iteration 172/1000 | Loss: 0.00002424
Iteration 173/1000 | Loss: 0.00002424
Iteration 174/1000 | Loss: 0.00002424
Iteration 175/1000 | Loss: 0.00002424
Iteration 176/1000 | Loss: 0.00002424
Iteration 177/1000 | Loss: 0.00002424
Iteration 178/1000 | Loss: 0.00002424
Iteration 179/1000 | Loss: 0.00002424
Iteration 180/1000 | Loss: 0.00002424
Iteration 181/1000 | Loss: 0.00002424
Iteration 182/1000 | Loss: 0.00002424
Iteration 183/1000 | Loss: 0.00002424
Iteration 184/1000 | Loss: 0.00002424
Iteration 185/1000 | Loss: 0.00002424
Iteration 186/1000 | Loss: 0.00002424
Iteration 187/1000 | Loss: 0.00002424
Iteration 188/1000 | Loss: 0.00002423
Iteration 189/1000 | Loss: 0.00002423
Iteration 190/1000 | Loss: 0.00002423
Iteration 191/1000 | Loss: 0.00002423
Iteration 192/1000 | Loss: 0.00002423
Iteration 193/1000 | Loss: 0.00002423
Iteration 194/1000 | Loss: 0.00002423
Iteration 195/1000 | Loss: 0.00002423
Iteration 196/1000 | Loss: 0.00002423
Iteration 197/1000 | Loss: 0.00002423
Iteration 198/1000 | Loss: 0.00002423
Iteration 199/1000 | Loss: 0.00002423
Iteration 200/1000 | Loss: 0.00002423
Iteration 201/1000 | Loss: 0.00002423
Iteration 202/1000 | Loss: 0.00002423
Iteration 203/1000 | Loss: 0.00002423
Iteration 204/1000 | Loss: 0.00002423
Iteration 205/1000 | Loss: 0.00002423
Iteration 206/1000 | Loss: 0.00002423
Iteration 207/1000 | Loss: 0.00002423
Iteration 208/1000 | Loss: 0.00002423
Iteration 209/1000 | Loss: 0.00002423
Iteration 210/1000 | Loss: 0.00002423
Iteration 211/1000 | Loss: 0.00002423
Iteration 212/1000 | Loss: 0.00002423
Iteration 213/1000 | Loss: 0.00002423
Iteration 214/1000 | Loss: 0.00002423
Iteration 215/1000 | Loss: 0.00002423
Iteration 216/1000 | Loss: 0.00002423
Iteration 217/1000 | Loss: 0.00002423
Iteration 218/1000 | Loss: 0.00002423
Iteration 219/1000 | Loss: 0.00002423
Iteration 220/1000 | Loss: 0.00002423
Iteration 221/1000 | Loss: 0.00002423
Iteration 222/1000 | Loss: 0.00002423
Iteration 223/1000 | Loss: 0.00002423
Iteration 224/1000 | Loss: 0.00002423
Iteration 225/1000 | Loss: 0.00002423
Iteration 226/1000 | Loss: 0.00002423
Iteration 227/1000 | Loss: 0.00002423
Iteration 228/1000 | Loss: 0.00002423
Iteration 229/1000 | Loss: 0.00002423
Iteration 230/1000 | Loss: 0.00002423
Iteration 231/1000 | Loss: 0.00002423
Iteration 232/1000 | Loss: 0.00002423
Iteration 233/1000 | Loss: 0.00002423
Iteration 234/1000 | Loss: 0.00002423
Iteration 235/1000 | Loss: 0.00002423
Iteration 236/1000 | Loss: 0.00002423
Iteration 237/1000 | Loss: 0.00002423
Iteration 238/1000 | Loss: 0.00002423
Iteration 239/1000 | Loss: 0.00002423
Iteration 240/1000 | Loss: 0.00002423
Iteration 241/1000 | Loss: 0.00002423
Iteration 242/1000 | Loss: 0.00002423
Iteration 243/1000 | Loss: 0.00002423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.4231541829067282e-05, 2.4231541829067282e-05, 2.4231541829067282e-05, 2.4231541829067282e-05, 2.4231541829067282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4231541829067282e-05

Optimization complete. Final v2v error: 3.732651472091675 mm

Highest mean error: 13.666528701782227 mm for frame 196

Lowest mean error: 3.231096029281616 mm for frame 136

Saving results

Total time: 201.35122108459473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949532
Iteration 2/25 | Loss: 0.00265736
Iteration 3/25 | Loss: 0.00208269
Iteration 4/25 | Loss: 0.00204298
Iteration 5/25 | Loss: 0.00174967
Iteration 6/25 | Loss: 0.00159697
Iteration 7/25 | Loss: 0.00146628
Iteration 8/25 | Loss: 0.00145303
Iteration 9/25 | Loss: 0.00145272
Iteration 10/25 | Loss: 0.00137936
Iteration 11/25 | Loss: 0.00135188
Iteration 12/25 | Loss: 0.00133985
Iteration 13/25 | Loss: 0.00133258
Iteration 14/25 | Loss: 0.00133011
Iteration 15/25 | Loss: 0.00132879
Iteration 16/25 | Loss: 0.00132849
Iteration 17/25 | Loss: 0.00132834
Iteration 18/25 | Loss: 0.00132821
Iteration 19/25 | Loss: 0.00132812
Iteration 20/25 | Loss: 0.00132797
Iteration 21/25 | Loss: 0.00132764
Iteration 22/25 | Loss: 0.00132731
Iteration 23/25 | Loss: 0.00132692
Iteration 24/25 | Loss: 0.00132661
Iteration 25/25 | Loss: 0.00132641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35551882
Iteration 2/25 | Loss: 0.00099906
Iteration 3/25 | Loss: 0.00099906
Iteration 4/25 | Loss: 0.00099906
Iteration 5/25 | Loss: 0.00099906
Iteration 6/25 | Loss: 0.00099906
Iteration 7/25 | Loss: 0.00099906
Iteration 8/25 | Loss: 0.00099906
Iteration 9/25 | Loss: 0.00099906
Iteration 10/25 | Loss: 0.00099906
Iteration 11/25 | Loss: 0.00099906
Iteration 12/25 | Loss: 0.00099906
Iteration 13/25 | Loss: 0.00099906
Iteration 14/25 | Loss: 0.00099906
Iteration 15/25 | Loss: 0.00099905
Iteration 16/25 | Loss: 0.00099905
Iteration 17/25 | Loss: 0.00099905
Iteration 18/25 | Loss: 0.00099906
Iteration 19/25 | Loss: 0.00099906
Iteration 20/25 | Loss: 0.00099906
Iteration 21/25 | Loss: 0.00099905
Iteration 22/25 | Loss: 0.00099905
Iteration 23/25 | Loss: 0.00099905
Iteration 24/25 | Loss: 0.00099905
Iteration 25/25 | Loss: 0.00099905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099905
Iteration 2/1000 | Loss: 0.00074446
Iteration 3/1000 | Loss: 0.00168666
Iteration 4/1000 | Loss: 0.00058294
Iteration 5/1000 | Loss: 0.00024578
Iteration 6/1000 | Loss: 0.00015344
Iteration 7/1000 | Loss: 0.00024057
Iteration 8/1000 | Loss: 0.00011795
Iteration 9/1000 | Loss: 0.00021285
Iteration 10/1000 | Loss: 0.00021682
Iteration 11/1000 | Loss: 0.00041733
Iteration 12/1000 | Loss: 0.00027237
Iteration 13/1000 | Loss: 0.00007748
Iteration 14/1000 | Loss: 0.00005671
Iteration 15/1000 | Loss: 0.00004620
Iteration 16/1000 | Loss: 0.00004123
Iteration 17/1000 | Loss: 0.00039284
Iteration 18/1000 | Loss: 0.00012666
Iteration 19/1000 | Loss: 0.00039353
Iteration 20/1000 | Loss: 0.00004657
Iteration 21/1000 | Loss: 0.00003997
Iteration 22/1000 | Loss: 0.00057339
Iteration 23/1000 | Loss: 0.00026258
Iteration 24/1000 | Loss: 0.00004033
Iteration 25/1000 | Loss: 0.00019214
Iteration 26/1000 | Loss: 0.00003138
Iteration 27/1000 | Loss: 0.00034208
Iteration 28/1000 | Loss: 0.00044879
Iteration 29/1000 | Loss: 0.00005563
Iteration 30/1000 | Loss: 0.00002607
Iteration 31/1000 | Loss: 0.00002492
Iteration 32/1000 | Loss: 0.00039103
Iteration 33/1000 | Loss: 0.00058581
Iteration 34/1000 | Loss: 0.00035811
Iteration 35/1000 | Loss: 0.00005245
Iteration 36/1000 | Loss: 0.00029002
Iteration 37/1000 | Loss: 0.00133439
Iteration 38/1000 | Loss: 0.00113057
Iteration 39/1000 | Loss: 0.00004928
Iteration 40/1000 | Loss: 0.00018823
Iteration 41/1000 | Loss: 0.00003044
Iteration 42/1000 | Loss: 0.00002576
Iteration 43/1000 | Loss: 0.00002384
Iteration 44/1000 | Loss: 0.00002319
Iteration 45/1000 | Loss: 0.00002293
Iteration 46/1000 | Loss: 0.00002273
Iteration 47/1000 | Loss: 0.00002273
Iteration 48/1000 | Loss: 0.00002265
Iteration 49/1000 | Loss: 0.00002265
Iteration 50/1000 | Loss: 0.00002251
Iteration 51/1000 | Loss: 0.00002243
Iteration 52/1000 | Loss: 0.00002239
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002239
Iteration 56/1000 | Loss: 0.00002239
Iteration 57/1000 | Loss: 0.00002238
Iteration 58/1000 | Loss: 0.00002238
Iteration 59/1000 | Loss: 0.00002238
Iteration 60/1000 | Loss: 0.00002238
Iteration 61/1000 | Loss: 0.00002238
Iteration 62/1000 | Loss: 0.00002238
Iteration 63/1000 | Loss: 0.00002238
Iteration 64/1000 | Loss: 0.00002238
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002236
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002235
Iteration 69/1000 | Loss: 0.00002235
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00002233
Iteration 73/1000 | Loss: 0.00002233
Iteration 74/1000 | Loss: 0.00002232
Iteration 75/1000 | Loss: 0.00002232
Iteration 76/1000 | Loss: 0.00002232
Iteration 77/1000 | Loss: 0.00002232
Iteration 78/1000 | Loss: 0.00002232
Iteration 79/1000 | Loss: 0.00002232
Iteration 80/1000 | Loss: 0.00002232
Iteration 81/1000 | Loss: 0.00002232
Iteration 82/1000 | Loss: 0.00002232
Iteration 83/1000 | Loss: 0.00002232
Iteration 84/1000 | Loss: 0.00002232
Iteration 85/1000 | Loss: 0.00002232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.231853432022035e-05, 2.231853432022035e-05, 2.231853432022035e-05, 2.231853432022035e-05, 2.231853432022035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.231853432022035e-05

Optimization complete. Final v2v error: 4.040816783905029 mm

Highest mean error: 4.758831977844238 mm for frame 129

Lowest mean error: 3.877042293548584 mm for frame 99

Saving results

Total time: 109.80402064323425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757377
Iteration 2/25 | Loss: 0.00162250
Iteration 3/25 | Loss: 0.00138783
Iteration 4/25 | Loss: 0.00134287
Iteration 5/25 | Loss: 0.00133334
Iteration 6/25 | Loss: 0.00133093
Iteration 7/25 | Loss: 0.00133093
Iteration 8/25 | Loss: 0.00133093
Iteration 9/25 | Loss: 0.00133093
Iteration 10/25 | Loss: 0.00133093
Iteration 11/25 | Loss: 0.00133093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013309292262420058, 0.0013309292262420058, 0.0013309292262420058, 0.0013309292262420058, 0.0013309292262420058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013309292262420058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13822830
Iteration 2/25 | Loss: 0.00128788
Iteration 3/25 | Loss: 0.00128786
Iteration 4/25 | Loss: 0.00128786
Iteration 5/25 | Loss: 0.00128786
Iteration 6/25 | Loss: 0.00128786
Iteration 7/25 | Loss: 0.00128786
Iteration 8/25 | Loss: 0.00128786
Iteration 9/25 | Loss: 0.00128786
Iteration 10/25 | Loss: 0.00128786
Iteration 11/25 | Loss: 0.00128786
Iteration 12/25 | Loss: 0.00128786
Iteration 13/25 | Loss: 0.00128786
Iteration 14/25 | Loss: 0.00128786
Iteration 15/25 | Loss: 0.00128786
Iteration 16/25 | Loss: 0.00128786
Iteration 17/25 | Loss: 0.00128786
Iteration 18/25 | Loss: 0.00128786
Iteration 19/25 | Loss: 0.00128786
Iteration 20/25 | Loss: 0.00128786
Iteration 21/25 | Loss: 0.00128786
Iteration 22/25 | Loss: 0.00128786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012878583511337638, 0.0012878583511337638, 0.0012878583511337638, 0.0012878583511337638, 0.0012878583511337638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012878583511337638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128786
Iteration 2/1000 | Loss: 0.00008503
Iteration 3/1000 | Loss: 0.00005508
Iteration 4/1000 | Loss: 0.00004619
Iteration 5/1000 | Loss: 0.00004259
Iteration 6/1000 | Loss: 0.00004058
Iteration 7/1000 | Loss: 0.00003930
Iteration 8/1000 | Loss: 0.00003830
Iteration 9/1000 | Loss: 0.00003754
Iteration 10/1000 | Loss: 0.00003703
Iteration 11/1000 | Loss: 0.00003649
Iteration 12/1000 | Loss: 0.00003604
Iteration 13/1000 | Loss: 0.00003574
Iteration 14/1000 | Loss: 0.00003547
Iteration 15/1000 | Loss: 0.00003523
Iteration 16/1000 | Loss: 0.00003511
Iteration 17/1000 | Loss: 0.00003499
Iteration 18/1000 | Loss: 0.00003498
Iteration 19/1000 | Loss: 0.00003494
Iteration 20/1000 | Loss: 0.00003489
Iteration 21/1000 | Loss: 0.00003489
Iteration 22/1000 | Loss: 0.00003488
Iteration 23/1000 | Loss: 0.00003485
Iteration 24/1000 | Loss: 0.00003483
Iteration 25/1000 | Loss: 0.00003482
Iteration 26/1000 | Loss: 0.00003481
Iteration 27/1000 | Loss: 0.00003475
Iteration 28/1000 | Loss: 0.00003472
Iteration 29/1000 | Loss: 0.00003471
Iteration 30/1000 | Loss: 0.00003462
Iteration 31/1000 | Loss: 0.00003461
Iteration 32/1000 | Loss: 0.00003461
Iteration 33/1000 | Loss: 0.00003458
Iteration 34/1000 | Loss: 0.00003457
Iteration 35/1000 | Loss: 0.00003452
Iteration 36/1000 | Loss: 0.00003450
Iteration 37/1000 | Loss: 0.00003450
Iteration 38/1000 | Loss: 0.00003449
Iteration 39/1000 | Loss: 0.00003444
Iteration 40/1000 | Loss: 0.00003444
Iteration 41/1000 | Loss: 0.00003443
Iteration 42/1000 | Loss: 0.00003443
Iteration 43/1000 | Loss: 0.00003443
Iteration 44/1000 | Loss: 0.00003443
Iteration 45/1000 | Loss: 0.00003443
Iteration 46/1000 | Loss: 0.00003443
Iteration 47/1000 | Loss: 0.00003443
Iteration 48/1000 | Loss: 0.00003443
Iteration 49/1000 | Loss: 0.00003443
Iteration 50/1000 | Loss: 0.00003442
Iteration 51/1000 | Loss: 0.00003442
Iteration 52/1000 | Loss: 0.00003441
Iteration 53/1000 | Loss: 0.00003441
Iteration 54/1000 | Loss: 0.00003441
Iteration 55/1000 | Loss: 0.00003440
Iteration 56/1000 | Loss: 0.00003440
Iteration 57/1000 | Loss: 0.00003440
Iteration 58/1000 | Loss: 0.00003440
Iteration 59/1000 | Loss: 0.00003439
Iteration 60/1000 | Loss: 0.00003439
Iteration 61/1000 | Loss: 0.00003439
Iteration 62/1000 | Loss: 0.00003439
Iteration 63/1000 | Loss: 0.00003439
Iteration 64/1000 | Loss: 0.00003439
Iteration 65/1000 | Loss: 0.00003438
Iteration 66/1000 | Loss: 0.00003438
Iteration 67/1000 | Loss: 0.00003438
Iteration 68/1000 | Loss: 0.00003438
Iteration 69/1000 | Loss: 0.00003438
Iteration 70/1000 | Loss: 0.00003438
Iteration 71/1000 | Loss: 0.00003437
Iteration 72/1000 | Loss: 0.00003437
Iteration 73/1000 | Loss: 0.00003437
Iteration 74/1000 | Loss: 0.00003437
Iteration 75/1000 | Loss: 0.00003437
Iteration 76/1000 | Loss: 0.00003437
Iteration 77/1000 | Loss: 0.00003437
Iteration 78/1000 | Loss: 0.00003437
Iteration 79/1000 | Loss: 0.00003437
Iteration 80/1000 | Loss: 0.00003437
Iteration 81/1000 | Loss: 0.00003437
Iteration 82/1000 | Loss: 0.00003437
Iteration 83/1000 | Loss: 0.00003436
Iteration 84/1000 | Loss: 0.00003436
Iteration 85/1000 | Loss: 0.00003436
Iteration 86/1000 | Loss: 0.00003436
Iteration 87/1000 | Loss: 0.00003436
Iteration 88/1000 | Loss: 0.00003435
Iteration 89/1000 | Loss: 0.00003435
Iteration 90/1000 | Loss: 0.00003435
Iteration 91/1000 | Loss: 0.00003434
Iteration 92/1000 | Loss: 0.00003434
Iteration 93/1000 | Loss: 0.00003434
Iteration 94/1000 | Loss: 0.00003434
Iteration 95/1000 | Loss: 0.00003433
Iteration 96/1000 | Loss: 0.00003433
Iteration 97/1000 | Loss: 0.00003433
Iteration 98/1000 | Loss: 0.00003433
Iteration 99/1000 | Loss: 0.00003433
Iteration 100/1000 | Loss: 0.00003433
Iteration 101/1000 | Loss: 0.00003433
Iteration 102/1000 | Loss: 0.00003433
Iteration 103/1000 | Loss: 0.00003433
Iteration 104/1000 | Loss: 0.00003433
Iteration 105/1000 | Loss: 0.00003432
Iteration 106/1000 | Loss: 0.00003432
Iteration 107/1000 | Loss: 0.00003432
Iteration 108/1000 | Loss: 0.00003432
Iteration 109/1000 | Loss: 0.00003432
Iteration 110/1000 | Loss: 0.00003431
Iteration 111/1000 | Loss: 0.00003431
Iteration 112/1000 | Loss: 0.00003431
Iteration 113/1000 | Loss: 0.00003431
Iteration 114/1000 | Loss: 0.00003431
Iteration 115/1000 | Loss: 0.00003431
Iteration 116/1000 | Loss: 0.00003430
Iteration 117/1000 | Loss: 0.00003430
Iteration 118/1000 | Loss: 0.00003430
Iteration 119/1000 | Loss: 0.00003430
Iteration 120/1000 | Loss: 0.00003429
Iteration 121/1000 | Loss: 0.00003429
Iteration 122/1000 | Loss: 0.00003429
Iteration 123/1000 | Loss: 0.00003429
Iteration 124/1000 | Loss: 0.00003429
Iteration 125/1000 | Loss: 0.00003429
Iteration 126/1000 | Loss: 0.00003429
Iteration 127/1000 | Loss: 0.00003429
Iteration 128/1000 | Loss: 0.00003429
Iteration 129/1000 | Loss: 0.00003429
Iteration 130/1000 | Loss: 0.00003428
Iteration 131/1000 | Loss: 0.00003428
Iteration 132/1000 | Loss: 0.00003428
Iteration 133/1000 | Loss: 0.00003428
Iteration 134/1000 | Loss: 0.00003427
Iteration 135/1000 | Loss: 0.00003427
Iteration 136/1000 | Loss: 0.00003427
Iteration 137/1000 | Loss: 0.00003427
Iteration 138/1000 | Loss: 0.00003427
Iteration 139/1000 | Loss: 0.00003427
Iteration 140/1000 | Loss: 0.00003427
Iteration 141/1000 | Loss: 0.00003427
Iteration 142/1000 | Loss: 0.00003427
Iteration 143/1000 | Loss: 0.00003427
Iteration 144/1000 | Loss: 0.00003426
Iteration 145/1000 | Loss: 0.00003426
Iteration 146/1000 | Loss: 0.00003426
Iteration 147/1000 | Loss: 0.00003426
Iteration 148/1000 | Loss: 0.00003426
Iteration 149/1000 | Loss: 0.00003426
Iteration 150/1000 | Loss: 0.00003426
Iteration 151/1000 | Loss: 0.00003426
Iteration 152/1000 | Loss: 0.00003426
Iteration 153/1000 | Loss: 0.00003426
Iteration 154/1000 | Loss: 0.00003426
Iteration 155/1000 | Loss: 0.00003426
Iteration 156/1000 | Loss: 0.00003425
Iteration 157/1000 | Loss: 0.00003425
Iteration 158/1000 | Loss: 0.00003425
Iteration 159/1000 | Loss: 0.00003425
Iteration 160/1000 | Loss: 0.00003425
Iteration 161/1000 | Loss: 0.00003425
Iteration 162/1000 | Loss: 0.00003425
Iteration 163/1000 | Loss: 0.00003425
Iteration 164/1000 | Loss: 0.00003425
Iteration 165/1000 | Loss: 0.00003425
Iteration 166/1000 | Loss: 0.00003424
Iteration 167/1000 | Loss: 0.00003424
Iteration 168/1000 | Loss: 0.00003424
Iteration 169/1000 | Loss: 0.00003424
Iteration 170/1000 | Loss: 0.00003424
Iteration 171/1000 | Loss: 0.00003424
Iteration 172/1000 | Loss: 0.00003424
Iteration 173/1000 | Loss: 0.00003424
Iteration 174/1000 | Loss: 0.00003424
Iteration 175/1000 | Loss: 0.00003424
Iteration 176/1000 | Loss: 0.00003424
Iteration 177/1000 | Loss: 0.00003424
Iteration 178/1000 | Loss: 0.00003424
Iteration 179/1000 | Loss: 0.00003424
Iteration 180/1000 | Loss: 0.00003424
Iteration 181/1000 | Loss: 0.00003423
Iteration 182/1000 | Loss: 0.00003423
Iteration 183/1000 | Loss: 0.00003423
Iteration 184/1000 | Loss: 0.00003423
Iteration 185/1000 | Loss: 0.00003423
Iteration 186/1000 | Loss: 0.00003423
Iteration 187/1000 | Loss: 0.00003423
Iteration 188/1000 | Loss: 0.00003423
Iteration 189/1000 | Loss: 0.00003423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [3.4234799386467785e-05, 3.4234799386467785e-05, 3.4234799386467785e-05, 3.4234799386467785e-05, 3.4234799386467785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4234799386467785e-05

Optimization complete. Final v2v error: 4.797455310821533 mm

Highest mean error: 6.127989292144775 mm for frame 75

Lowest mean error: 3.584270715713501 mm for frame 215

Saving results

Total time: 56.68168497085571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437303
Iteration 2/25 | Loss: 0.00129259
Iteration 3/25 | Loss: 0.00117951
Iteration 4/25 | Loss: 0.00116231
Iteration 5/25 | Loss: 0.00115766
Iteration 6/25 | Loss: 0.00115701
Iteration 7/25 | Loss: 0.00115701
Iteration 8/25 | Loss: 0.00115701
Iteration 9/25 | Loss: 0.00115701
Iteration 10/25 | Loss: 0.00115701
Iteration 11/25 | Loss: 0.00115701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011570118367671967, 0.0011570118367671967, 0.0011570118367671967, 0.0011570118367671967, 0.0011570118367671967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011570118367671967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34282374
Iteration 2/25 | Loss: 0.00066587
Iteration 3/25 | Loss: 0.00066586
Iteration 4/25 | Loss: 0.00066586
Iteration 5/25 | Loss: 0.00066586
Iteration 6/25 | Loss: 0.00066586
Iteration 7/25 | Loss: 0.00066586
Iteration 8/25 | Loss: 0.00066586
Iteration 9/25 | Loss: 0.00066586
Iteration 10/25 | Loss: 0.00066586
Iteration 11/25 | Loss: 0.00066586
Iteration 12/25 | Loss: 0.00066586
Iteration 13/25 | Loss: 0.00066586
Iteration 14/25 | Loss: 0.00066586
Iteration 15/25 | Loss: 0.00066586
Iteration 16/25 | Loss: 0.00066586
Iteration 17/25 | Loss: 0.00066586
Iteration 18/25 | Loss: 0.00066586
Iteration 19/25 | Loss: 0.00066586
Iteration 20/25 | Loss: 0.00066586
Iteration 21/25 | Loss: 0.00066586
Iteration 22/25 | Loss: 0.00066586
Iteration 23/25 | Loss: 0.00066586
Iteration 24/25 | Loss: 0.00066586
Iteration 25/25 | Loss: 0.00066586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066586
Iteration 2/1000 | Loss: 0.00002780
Iteration 3/1000 | Loss: 0.00001978
Iteration 4/1000 | Loss: 0.00001843
Iteration 5/1000 | Loss: 0.00001749
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001589
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001562
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001545
Iteration 14/1000 | Loss: 0.00001544
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001540
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001518
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001516
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001512
Iteration 39/1000 | Loss: 0.00001511
Iteration 40/1000 | Loss: 0.00001511
Iteration 41/1000 | Loss: 0.00001510
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001509
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001508
Iteration 47/1000 | Loss: 0.00001508
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001503
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001503
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001501
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001501
Iteration 92/1000 | Loss: 0.00001501
Iteration 93/1000 | Loss: 0.00001501
Iteration 94/1000 | Loss: 0.00001501
Iteration 95/1000 | Loss: 0.00001501
Iteration 96/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.5007390175014734e-05, 1.5007390175014734e-05, 1.5007390175014734e-05, 1.5007390175014734e-05, 1.5007390175014734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5007390175014734e-05

Optimization complete. Final v2v error: 3.300990104675293 mm

Highest mean error: 3.5204172134399414 mm for frame 178

Lowest mean error: 2.9103007316589355 mm for frame 189

Saving results

Total time: 37.774476528167725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395969
Iteration 2/25 | Loss: 0.00120397
Iteration 3/25 | Loss: 0.00112695
Iteration 4/25 | Loss: 0.00111117
Iteration 5/25 | Loss: 0.00110516
Iteration 6/25 | Loss: 0.00110330
Iteration 7/25 | Loss: 0.00110326
Iteration 8/25 | Loss: 0.00110326
Iteration 9/25 | Loss: 0.00110326
Iteration 10/25 | Loss: 0.00110326
Iteration 11/25 | Loss: 0.00110326
Iteration 12/25 | Loss: 0.00110326
Iteration 13/25 | Loss: 0.00110326
Iteration 14/25 | Loss: 0.00110326
Iteration 15/25 | Loss: 0.00110326
Iteration 16/25 | Loss: 0.00110326
Iteration 17/25 | Loss: 0.00110326
Iteration 18/25 | Loss: 0.00110326
Iteration 19/25 | Loss: 0.00110326
Iteration 20/25 | Loss: 0.00110326
Iteration 21/25 | Loss: 0.00110326
Iteration 22/25 | Loss: 0.00110326
Iteration 23/25 | Loss: 0.00110326
Iteration 24/25 | Loss: 0.00110326
Iteration 25/25 | Loss: 0.00110326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36773002
Iteration 2/25 | Loss: 0.00111336
Iteration 3/25 | Loss: 0.00111336
Iteration 4/25 | Loss: 0.00111336
Iteration 5/25 | Loss: 0.00111336
Iteration 6/25 | Loss: 0.00111336
Iteration 7/25 | Loss: 0.00111336
Iteration 8/25 | Loss: 0.00111336
Iteration 9/25 | Loss: 0.00111336
Iteration 10/25 | Loss: 0.00111336
Iteration 11/25 | Loss: 0.00111335
Iteration 12/25 | Loss: 0.00111335
Iteration 13/25 | Loss: 0.00111335
Iteration 14/25 | Loss: 0.00111336
Iteration 15/25 | Loss: 0.00111336
Iteration 16/25 | Loss: 0.00111335
Iteration 17/25 | Loss: 0.00111335
Iteration 18/25 | Loss: 0.00111336
Iteration 19/25 | Loss: 0.00111336
Iteration 20/25 | Loss: 0.00111336
Iteration 21/25 | Loss: 0.00111335
Iteration 22/25 | Loss: 0.00111335
Iteration 23/25 | Loss: 0.00111336
Iteration 24/25 | Loss: 0.00111336
Iteration 25/25 | Loss: 0.00111336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111336
Iteration 2/1000 | Loss: 0.00002898
Iteration 3/1000 | Loss: 0.00002049
Iteration 4/1000 | Loss: 0.00001885
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001651
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001514
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001465
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00001458
Iteration 27/1000 | Loss: 0.00001457
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001456
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001446
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001433
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001432
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001431
Iteration 70/1000 | Loss: 0.00001431
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.4283987184171565e-05, 1.4283987184171565e-05, 1.4283987184171565e-05, 1.4283987184171565e-05, 1.4283987184171565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4283987184171565e-05

Optimization complete. Final v2v error: 3.094165086746216 mm

Highest mean error: 3.932868480682373 mm for frame 120

Lowest mean error: 2.5498852729797363 mm for frame 27

Saving results

Total time: 40.1275851726532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426915
Iteration 2/25 | Loss: 0.00132166
Iteration 3/25 | Loss: 0.00119087
Iteration 4/25 | Loss: 0.00116891
Iteration 5/25 | Loss: 0.00116442
Iteration 6/25 | Loss: 0.00116379
Iteration 7/25 | Loss: 0.00116379
Iteration 8/25 | Loss: 0.00116379
Iteration 9/25 | Loss: 0.00116379
Iteration 10/25 | Loss: 0.00116379
Iteration 11/25 | Loss: 0.00116379
Iteration 12/25 | Loss: 0.00116379
Iteration 13/25 | Loss: 0.00116379
Iteration 14/25 | Loss: 0.00116379
Iteration 15/25 | Loss: 0.00116379
Iteration 16/25 | Loss: 0.00116379
Iteration 17/25 | Loss: 0.00116379
Iteration 18/25 | Loss: 0.00116379
Iteration 19/25 | Loss: 0.00116379
Iteration 20/25 | Loss: 0.00116379
Iteration 21/25 | Loss: 0.00116379
Iteration 22/25 | Loss: 0.00116379
Iteration 23/25 | Loss: 0.00116379
Iteration 24/25 | Loss: 0.00116379
Iteration 25/25 | Loss: 0.00116379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011637865100055933, 0.0011637865100055933, 0.0011637865100055933, 0.0011637865100055933, 0.0011637865100055933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011637865100055933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38270044
Iteration 2/25 | Loss: 0.00092165
Iteration 3/25 | Loss: 0.00092165
Iteration 4/25 | Loss: 0.00092165
Iteration 5/25 | Loss: 0.00092165
Iteration 6/25 | Loss: 0.00092165
Iteration 7/25 | Loss: 0.00092164
Iteration 8/25 | Loss: 0.00092164
Iteration 9/25 | Loss: 0.00092164
Iteration 10/25 | Loss: 0.00092164
Iteration 11/25 | Loss: 0.00092164
Iteration 12/25 | Loss: 0.00092164
Iteration 13/25 | Loss: 0.00092164
Iteration 14/25 | Loss: 0.00092164
Iteration 15/25 | Loss: 0.00092164
Iteration 16/25 | Loss: 0.00092164
Iteration 17/25 | Loss: 0.00092164
Iteration 18/25 | Loss: 0.00092164
Iteration 19/25 | Loss: 0.00092164
Iteration 20/25 | Loss: 0.00092164
Iteration 21/25 | Loss: 0.00092164
Iteration 22/25 | Loss: 0.00092164
Iteration 23/25 | Loss: 0.00092164
Iteration 24/25 | Loss: 0.00092164
Iteration 25/25 | Loss: 0.00092164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092164
Iteration 2/1000 | Loss: 0.00003230
Iteration 3/1000 | Loss: 0.00001984
Iteration 4/1000 | Loss: 0.00001703
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001437
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001360
Iteration 16/1000 | Loss: 0.00001349
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001320
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001314
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001310
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001306
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001303
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001302
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001301
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001301
Iteration 87/1000 | Loss: 0.00001300
Iteration 88/1000 | Loss: 0.00001300
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001295
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001293
Iteration 132/1000 | Loss: 0.00001293
Iteration 133/1000 | Loss: 0.00001293
Iteration 134/1000 | Loss: 0.00001293
Iteration 135/1000 | Loss: 0.00001293
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001292
Iteration 138/1000 | Loss: 0.00001292
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001289
Iteration 151/1000 | Loss: 0.00001289
Iteration 152/1000 | Loss: 0.00001289
Iteration 153/1000 | Loss: 0.00001289
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001287
Iteration 163/1000 | Loss: 0.00001287
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001287
Iteration 166/1000 | Loss: 0.00001287
Iteration 167/1000 | Loss: 0.00001287
Iteration 168/1000 | Loss: 0.00001287
Iteration 169/1000 | Loss: 0.00001287
Iteration 170/1000 | Loss: 0.00001287
Iteration 171/1000 | Loss: 0.00001287
Iteration 172/1000 | Loss: 0.00001287
Iteration 173/1000 | Loss: 0.00001287
Iteration 174/1000 | Loss: 0.00001286
Iteration 175/1000 | Loss: 0.00001286
Iteration 176/1000 | Loss: 0.00001286
Iteration 177/1000 | Loss: 0.00001286
Iteration 178/1000 | Loss: 0.00001286
Iteration 179/1000 | Loss: 0.00001286
Iteration 180/1000 | Loss: 0.00001286
Iteration 181/1000 | Loss: 0.00001286
Iteration 182/1000 | Loss: 0.00001286
Iteration 183/1000 | Loss: 0.00001286
Iteration 184/1000 | Loss: 0.00001286
Iteration 185/1000 | Loss: 0.00001286
Iteration 186/1000 | Loss: 0.00001286
Iteration 187/1000 | Loss: 0.00001286
Iteration 188/1000 | Loss: 0.00001286
Iteration 189/1000 | Loss: 0.00001286
Iteration 190/1000 | Loss: 0.00001286
Iteration 191/1000 | Loss: 0.00001286
Iteration 192/1000 | Loss: 0.00001286
Iteration 193/1000 | Loss: 0.00001286
Iteration 194/1000 | Loss: 0.00001286
Iteration 195/1000 | Loss: 0.00001286
Iteration 196/1000 | Loss: 0.00001286
Iteration 197/1000 | Loss: 0.00001286
Iteration 198/1000 | Loss: 0.00001286
Iteration 199/1000 | Loss: 0.00001286
Iteration 200/1000 | Loss: 0.00001286
Iteration 201/1000 | Loss: 0.00001286
Iteration 202/1000 | Loss: 0.00001286
Iteration 203/1000 | Loss: 0.00001286
Iteration 204/1000 | Loss: 0.00001286
Iteration 205/1000 | Loss: 0.00001286
Iteration 206/1000 | Loss: 0.00001286
Iteration 207/1000 | Loss: 0.00001286
Iteration 208/1000 | Loss: 0.00001286
Iteration 209/1000 | Loss: 0.00001286
Iteration 210/1000 | Loss: 0.00001286
Iteration 211/1000 | Loss: 0.00001286
Iteration 212/1000 | Loss: 0.00001286
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001286
Iteration 219/1000 | Loss: 0.00001286
Iteration 220/1000 | Loss: 0.00001286
Iteration 221/1000 | Loss: 0.00001286
Iteration 222/1000 | Loss: 0.00001286
Iteration 223/1000 | Loss: 0.00001286
Iteration 224/1000 | Loss: 0.00001286
Iteration 225/1000 | Loss: 0.00001286
Iteration 226/1000 | Loss: 0.00001286
Iteration 227/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.2855806744482834e-05, 1.2855806744482834e-05, 1.2855806744482834e-05, 1.2855806744482834e-05, 1.2855806744482834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2855806744482834e-05

Optimization complete. Final v2v error: 3.00991153717041 mm

Highest mean error: 3.367718458175659 mm for frame 23

Lowest mean error: 2.613340139389038 mm for frame 79

Saving results

Total time: 41.403632164001465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756075
Iteration 2/25 | Loss: 0.00189047
Iteration 3/25 | Loss: 0.00142215
Iteration 4/25 | Loss: 0.00133196
Iteration 5/25 | Loss: 0.00130767
Iteration 6/25 | Loss: 0.00132753
Iteration 7/25 | Loss: 0.00131126
Iteration 8/25 | Loss: 0.00129970
Iteration 9/25 | Loss: 0.00130545
Iteration 10/25 | Loss: 0.00130301
Iteration 11/25 | Loss: 0.00129540
Iteration 12/25 | Loss: 0.00126704
Iteration 13/25 | Loss: 0.00125852
Iteration 14/25 | Loss: 0.00125453
Iteration 15/25 | Loss: 0.00125265
Iteration 16/25 | Loss: 0.00125203
Iteration 17/25 | Loss: 0.00125167
Iteration 18/25 | Loss: 0.00125521
Iteration 19/25 | Loss: 0.00125405
Iteration 20/25 | Loss: 0.00125013
Iteration 21/25 | Loss: 0.00125287
Iteration 22/25 | Loss: 0.00124868
Iteration 23/25 | Loss: 0.00124683
Iteration 24/25 | Loss: 0.00124659
Iteration 25/25 | Loss: 0.00124655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.16035175
Iteration 2/25 | Loss: 0.00071040
Iteration 3/25 | Loss: 0.00071033
Iteration 4/25 | Loss: 0.00071033
Iteration 5/25 | Loss: 0.00071033
Iteration 6/25 | Loss: 0.00071033
Iteration 7/25 | Loss: 0.00071033
Iteration 8/25 | Loss: 0.00071033
Iteration 9/25 | Loss: 0.00071032
Iteration 10/25 | Loss: 0.00071032
Iteration 11/25 | Loss: 0.00071032
Iteration 12/25 | Loss: 0.00071032
Iteration 13/25 | Loss: 0.00071032
Iteration 14/25 | Loss: 0.00071032
Iteration 15/25 | Loss: 0.00071032
Iteration 16/25 | Loss: 0.00071032
Iteration 17/25 | Loss: 0.00071032
Iteration 18/25 | Loss: 0.00071032
Iteration 19/25 | Loss: 0.00071032
Iteration 20/25 | Loss: 0.00071032
Iteration 21/25 | Loss: 0.00071032
Iteration 22/25 | Loss: 0.00071032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007103242678567767, 0.0007103242678567767, 0.0007103242678567767, 0.0007103242678567767, 0.0007103242678567767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007103242678567767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071032
Iteration 2/1000 | Loss: 0.00004311
Iteration 3/1000 | Loss: 0.00002691
Iteration 4/1000 | Loss: 0.00003954
Iteration 5/1000 | Loss: 0.00002678
Iteration 6/1000 | Loss: 0.00001805
Iteration 7/1000 | Loss: 0.00003407
Iteration 8/1000 | Loss: 0.00001733
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00003601
Iteration 11/1000 | Loss: 0.00001694
Iteration 12/1000 | Loss: 0.00002755
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001648
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001643
Iteration 24/1000 | Loss: 0.00001643
Iteration 25/1000 | Loss: 0.00001643
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001642
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001638
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001632
Iteration 44/1000 | Loss: 0.00001632
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001631
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001630
Iteration 63/1000 | Loss: 0.00001630
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001629
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001628
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001627
Iteration 74/1000 | Loss: 0.00001627
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00002967
Iteration 129/1000 | Loss: 0.00001614
Iteration 130/1000 | Loss: 0.00001614
Iteration 131/1000 | Loss: 0.00001614
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00002120
Iteration 135/1000 | Loss: 0.00002013
Iteration 136/1000 | Loss: 0.00001755
Iteration 137/1000 | Loss: 0.00001611
Iteration 138/1000 | Loss: 0.00001611
Iteration 139/1000 | Loss: 0.00001611
Iteration 140/1000 | Loss: 0.00001611
Iteration 141/1000 | Loss: 0.00001611
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001611
Iteration 146/1000 | Loss: 0.00001611
Iteration 147/1000 | Loss: 0.00001611
Iteration 148/1000 | Loss: 0.00001611
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001610
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001610
Iteration 157/1000 | Loss: 0.00001610
Iteration 158/1000 | Loss: 0.00001610
Iteration 159/1000 | Loss: 0.00001610
Iteration 160/1000 | Loss: 0.00001610
Iteration 161/1000 | Loss: 0.00001610
Iteration 162/1000 | Loss: 0.00001610
Iteration 163/1000 | Loss: 0.00001610
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001609
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001609
Iteration 169/1000 | Loss: 0.00001609
Iteration 170/1000 | Loss: 0.00001609
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001607
Iteration 178/1000 | Loss: 0.00001607
Iteration 179/1000 | Loss: 0.00001607
Iteration 180/1000 | Loss: 0.00001607
Iteration 181/1000 | Loss: 0.00001607
Iteration 182/1000 | Loss: 0.00001607
Iteration 183/1000 | Loss: 0.00001606
Iteration 184/1000 | Loss: 0.00001939
Iteration 185/1000 | Loss: 0.00001605
Iteration 186/1000 | Loss: 0.00001605
Iteration 187/1000 | Loss: 0.00001605
Iteration 188/1000 | Loss: 0.00001605
Iteration 189/1000 | Loss: 0.00001605
Iteration 190/1000 | Loss: 0.00001605
Iteration 191/1000 | Loss: 0.00001605
Iteration 192/1000 | Loss: 0.00001604
Iteration 193/1000 | Loss: 0.00001604
Iteration 194/1000 | Loss: 0.00001604
Iteration 195/1000 | Loss: 0.00001604
Iteration 196/1000 | Loss: 0.00001604
Iteration 197/1000 | Loss: 0.00001604
Iteration 198/1000 | Loss: 0.00001604
Iteration 199/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.604414137545973e-05, 1.604414137545973e-05, 1.604414137545973e-05, 1.604414137545973e-05, 1.604414137545973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.604414137545973e-05

Optimization complete. Final v2v error: 3.300854206085205 mm

Highest mean error: 3.875715732574463 mm for frame 177

Lowest mean error: 2.9451022148132324 mm for frame 148

Saving results

Total time: 86.4749162197113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621123
Iteration 2/25 | Loss: 0.00143990
Iteration 3/25 | Loss: 0.00119614
Iteration 4/25 | Loss: 0.00117111
Iteration 5/25 | Loss: 0.00116917
Iteration 6/25 | Loss: 0.00116917
Iteration 7/25 | Loss: 0.00116917
Iteration 8/25 | Loss: 0.00116917
Iteration 9/25 | Loss: 0.00116917
Iteration 10/25 | Loss: 0.00116917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011691659456118941, 0.0011691659456118941, 0.0011691659456118941, 0.0011691659456118941, 0.0011691659456118941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011691659456118941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07446647
Iteration 2/25 | Loss: 0.00064656
Iteration 3/25 | Loss: 0.00064654
Iteration 4/25 | Loss: 0.00064654
Iteration 5/25 | Loss: 0.00064654
Iteration 6/25 | Loss: 0.00064654
Iteration 7/25 | Loss: 0.00064654
Iteration 8/25 | Loss: 0.00064653
Iteration 9/25 | Loss: 0.00064653
Iteration 10/25 | Loss: 0.00064653
Iteration 11/25 | Loss: 0.00064653
Iteration 12/25 | Loss: 0.00064653
Iteration 13/25 | Loss: 0.00064653
Iteration 14/25 | Loss: 0.00064653
Iteration 15/25 | Loss: 0.00064653
Iteration 16/25 | Loss: 0.00064653
Iteration 17/25 | Loss: 0.00064653
Iteration 18/25 | Loss: 0.00064653
Iteration 19/25 | Loss: 0.00064653
Iteration 20/25 | Loss: 0.00064653
Iteration 21/25 | Loss: 0.00064653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006465340266004205, 0.0006465340266004205, 0.0006465340266004205, 0.0006465340266004205, 0.0006465340266004205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006465340266004205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064653
Iteration 2/1000 | Loss: 0.00003177
Iteration 3/1000 | Loss: 0.00001742
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001419
Iteration 7/1000 | Loss: 0.00001378
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001282
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001177
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001176
Iteration 103/1000 | Loss: 0.00001176
Iteration 104/1000 | Loss: 0.00001176
Iteration 105/1000 | Loss: 0.00001176
Iteration 106/1000 | Loss: 0.00001176
Iteration 107/1000 | Loss: 0.00001176
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001173
Iteration 130/1000 | Loss: 0.00001173
Iteration 131/1000 | Loss: 0.00001173
Iteration 132/1000 | Loss: 0.00001173
Iteration 133/1000 | Loss: 0.00001173
Iteration 134/1000 | Loss: 0.00001173
Iteration 135/1000 | Loss: 0.00001173
Iteration 136/1000 | Loss: 0.00001173
Iteration 137/1000 | Loss: 0.00001173
Iteration 138/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.17258259706432e-05, 1.17258259706432e-05, 1.17258259706432e-05, 1.17258259706432e-05, 1.17258259706432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.17258259706432e-05

Optimization complete. Final v2v error: 2.926781415939331 mm

Highest mean error: 3.3933968544006348 mm for frame 231

Lowest mean error: 2.601958990097046 mm for frame 185

Saving results

Total time: 44.20705604553223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488308
Iteration 2/25 | Loss: 0.00109297
Iteration 3/25 | Loss: 0.00094550
Iteration 4/25 | Loss: 0.00090884
Iteration 5/25 | Loss: 0.00090010
Iteration 6/25 | Loss: 0.00089800
Iteration 7/25 | Loss: 0.00089769
Iteration 8/25 | Loss: 0.00089769
Iteration 9/25 | Loss: 0.00089769
Iteration 10/25 | Loss: 0.00089769
Iteration 11/25 | Loss: 0.00089769
Iteration 12/25 | Loss: 0.00089769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008976851822808385, 0.0008976851822808385, 0.0008976851822808385, 0.0008976851822808385, 0.0008976851822808385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008976851822808385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56950140
Iteration 2/25 | Loss: 0.00202529
Iteration 3/25 | Loss: 0.00202525
Iteration 4/25 | Loss: 0.00202525
Iteration 5/25 | Loss: 0.00202525
Iteration 6/25 | Loss: 0.00202525
Iteration 7/25 | Loss: 0.00202525
Iteration 8/25 | Loss: 0.00202525
Iteration 9/25 | Loss: 0.00202525
Iteration 10/25 | Loss: 0.00202525
Iteration 11/25 | Loss: 0.00202525
Iteration 12/25 | Loss: 0.00202525
Iteration 13/25 | Loss: 0.00202525
Iteration 14/25 | Loss: 0.00202525
Iteration 15/25 | Loss: 0.00202525
Iteration 16/25 | Loss: 0.00202525
Iteration 17/25 | Loss: 0.00202525
Iteration 18/25 | Loss: 0.00202525
Iteration 19/25 | Loss: 0.00202525
Iteration 20/25 | Loss: 0.00202525
Iteration 21/25 | Loss: 0.00202525
Iteration 22/25 | Loss: 0.00202525
Iteration 23/25 | Loss: 0.00202525
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020252459216862917, 0.0020252459216862917, 0.0020252459216862917, 0.0020252459216862917, 0.0020252459216862917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020252459216862917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202525
Iteration 2/1000 | Loss: 0.00003199
Iteration 3/1000 | Loss: 0.00002412
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001677
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001633
Iteration 13/1000 | Loss: 0.00001616
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001603
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001593
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001592
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001583
Iteration 32/1000 | Loss: 0.00001583
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001582
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001573
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001571
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001569
Iteration 69/1000 | Loss: 0.00001569
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001563
Iteration 98/1000 | Loss: 0.00001563
Iteration 99/1000 | Loss: 0.00001563
Iteration 100/1000 | Loss: 0.00001563
Iteration 101/1000 | Loss: 0.00001562
Iteration 102/1000 | Loss: 0.00001562
Iteration 103/1000 | Loss: 0.00001562
Iteration 104/1000 | Loss: 0.00001562
Iteration 105/1000 | Loss: 0.00001562
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001562
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001562
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001562
Iteration 117/1000 | Loss: 0.00001562
Iteration 118/1000 | Loss: 0.00001562
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001561
Iteration 121/1000 | Loss: 0.00001561
Iteration 122/1000 | Loss: 0.00001561
Iteration 123/1000 | Loss: 0.00001561
Iteration 124/1000 | Loss: 0.00001561
Iteration 125/1000 | Loss: 0.00001561
Iteration 126/1000 | Loss: 0.00001561
Iteration 127/1000 | Loss: 0.00001561
Iteration 128/1000 | Loss: 0.00001561
Iteration 129/1000 | Loss: 0.00001561
Iteration 130/1000 | Loss: 0.00001561
Iteration 131/1000 | Loss: 0.00001561
Iteration 132/1000 | Loss: 0.00001561
Iteration 133/1000 | Loss: 0.00001561
Iteration 134/1000 | Loss: 0.00001561
Iteration 135/1000 | Loss: 0.00001561
Iteration 136/1000 | Loss: 0.00001561
Iteration 137/1000 | Loss: 0.00001561
Iteration 138/1000 | Loss: 0.00001561
Iteration 139/1000 | Loss: 0.00001561
Iteration 140/1000 | Loss: 0.00001561
Iteration 141/1000 | Loss: 0.00001561
Iteration 142/1000 | Loss: 0.00001561
Iteration 143/1000 | Loss: 0.00001561
Iteration 144/1000 | Loss: 0.00001561
Iteration 145/1000 | Loss: 0.00001561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5610001355526038e-05, 1.5610001355526038e-05, 1.5610001355526038e-05, 1.5610001355526038e-05, 1.5610001355526038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5610001355526038e-05

Optimization complete. Final v2v error: 3.4047062397003174 mm

Highest mean error: 4.19443941116333 mm for frame 91

Lowest mean error: 2.6720752716064453 mm for frame 106

Saving results

Total time: 44.05443549156189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862830
Iteration 2/25 | Loss: 0.00127743
Iteration 3/25 | Loss: 0.00104680
Iteration 4/25 | Loss: 0.00098256
Iteration 5/25 | Loss: 0.00096075
Iteration 6/25 | Loss: 0.00094750
Iteration 7/25 | Loss: 0.00093531
Iteration 8/25 | Loss: 0.00094450
Iteration 9/25 | Loss: 0.00095130
Iteration 10/25 | Loss: 0.00094108
Iteration 11/25 | Loss: 0.00093516
Iteration 12/25 | Loss: 0.00093436
Iteration 13/25 | Loss: 0.00094658
Iteration 14/25 | Loss: 0.00095939
Iteration 15/25 | Loss: 0.00094917
Iteration 16/25 | Loss: 0.00094234
Iteration 17/25 | Loss: 0.00093353
Iteration 18/25 | Loss: 0.00092268
Iteration 19/25 | Loss: 0.00091641
Iteration 20/25 | Loss: 0.00091271
Iteration 21/25 | Loss: 0.00091076
Iteration 22/25 | Loss: 0.00091127
Iteration 23/25 | Loss: 0.00091118
Iteration 24/25 | Loss: 0.00091009
Iteration 25/25 | Loss: 0.00091042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66034198
Iteration 2/25 | Loss: 0.00224859
Iteration 3/25 | Loss: 0.00224857
Iteration 4/25 | Loss: 0.00224857
Iteration 5/25 | Loss: 0.00224857
Iteration 6/25 | Loss: 0.00224857
Iteration 7/25 | Loss: 0.00224857
Iteration 8/25 | Loss: 0.00224857
Iteration 9/25 | Loss: 0.00224857
Iteration 10/25 | Loss: 0.00224857
Iteration 11/25 | Loss: 0.00224857
Iteration 12/25 | Loss: 0.00224857
Iteration 13/25 | Loss: 0.00224857
Iteration 14/25 | Loss: 0.00224857
Iteration 15/25 | Loss: 0.00224857
Iteration 16/25 | Loss: 0.00224857
Iteration 17/25 | Loss: 0.00224857
Iteration 18/25 | Loss: 0.00224857
Iteration 19/25 | Loss: 0.00224857
Iteration 20/25 | Loss: 0.00224857
Iteration 21/25 | Loss: 0.00224857
Iteration 22/25 | Loss: 0.00224857
Iteration 23/25 | Loss: 0.00224857
Iteration 24/25 | Loss: 0.00224857
Iteration 25/25 | Loss: 0.00224857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224857
Iteration 2/1000 | Loss: 0.00023885
Iteration 3/1000 | Loss: 0.00045507
Iteration 4/1000 | Loss: 0.00045311
Iteration 5/1000 | Loss: 0.00025869
Iteration 6/1000 | Loss: 0.00035164
Iteration 7/1000 | Loss: 0.00027688
Iteration 8/1000 | Loss: 0.00007129
Iteration 9/1000 | Loss: 0.00064155
Iteration 10/1000 | Loss: 0.00031956
Iteration 11/1000 | Loss: 0.00005674
Iteration 12/1000 | Loss: 0.00004817
Iteration 13/1000 | Loss: 0.00019865
Iteration 14/1000 | Loss: 0.00023117
Iteration 15/1000 | Loss: 0.00054602
Iteration 16/1000 | Loss: 0.00018216
Iteration 17/1000 | Loss: 0.00018864
Iteration 18/1000 | Loss: 0.00014128
Iteration 19/1000 | Loss: 0.00004189
Iteration 20/1000 | Loss: 0.00003319
Iteration 21/1000 | Loss: 0.00003737
Iteration 22/1000 | Loss: 0.00004815
Iteration 23/1000 | Loss: 0.00002819
Iteration 24/1000 | Loss: 0.00003762
Iteration 25/1000 | Loss: 0.00003350
Iteration 26/1000 | Loss: 0.00003151
Iteration 27/1000 | Loss: 0.00003364
Iteration 28/1000 | Loss: 0.00003198
Iteration 29/1000 | Loss: 0.00003085
Iteration 30/1000 | Loss: 0.00003302
Iteration 31/1000 | Loss: 0.00003150
Iteration 32/1000 | Loss: 0.00003869
Iteration 33/1000 | Loss: 0.00004397
Iteration 34/1000 | Loss: 0.00003771
Iteration 35/1000 | Loss: 0.00029998
Iteration 36/1000 | Loss: 0.00004651
Iteration 37/1000 | Loss: 0.00004371
Iteration 38/1000 | Loss: 0.00004872
Iteration 39/1000 | Loss: 0.00004392
Iteration 40/1000 | Loss: 0.00004215
Iteration 41/1000 | Loss: 0.00004006
Iteration 42/1000 | Loss: 0.00003973
Iteration 43/1000 | Loss: 0.00003923
Iteration 44/1000 | Loss: 0.00003187
Iteration 45/1000 | Loss: 0.00003505
Iteration 46/1000 | Loss: 0.00004040
Iteration 47/1000 | Loss: 0.00003727
Iteration 48/1000 | Loss: 0.00003984
Iteration 49/1000 | Loss: 0.00003676
Iteration 50/1000 | Loss: 0.00004029
Iteration 51/1000 | Loss: 0.00003784
Iteration 52/1000 | Loss: 0.00003826
Iteration 53/1000 | Loss: 0.00003927
Iteration 54/1000 | Loss: 0.00003177
Iteration 55/1000 | Loss: 0.00003300
Iteration 56/1000 | Loss: 0.00003863
Iteration 57/1000 | Loss: 0.00003735
Iteration 58/1000 | Loss: 0.00003099
Iteration 59/1000 | Loss: 0.00003773
Iteration 60/1000 | Loss: 0.00004084
Iteration 61/1000 | Loss: 0.00003598
Iteration 62/1000 | Loss: 0.00003253
Iteration 63/1000 | Loss: 0.00029179
Iteration 64/1000 | Loss: 0.00051493
Iteration 65/1000 | Loss: 0.00006131
Iteration 66/1000 | Loss: 0.00005242
Iteration 67/1000 | Loss: 0.00014251
Iteration 68/1000 | Loss: 0.00004042
Iteration 69/1000 | Loss: 0.00003330
Iteration 70/1000 | Loss: 0.00003006
Iteration 71/1000 | Loss: 0.00002859
Iteration 72/1000 | Loss: 0.00002694
Iteration 73/1000 | Loss: 0.00002614
Iteration 74/1000 | Loss: 0.00002533
Iteration 75/1000 | Loss: 0.00002472
Iteration 76/1000 | Loss: 0.00002943
Iteration 77/1000 | Loss: 0.00002437
Iteration 78/1000 | Loss: 0.00002385
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002318
Iteration 81/1000 | Loss: 0.00002288
Iteration 82/1000 | Loss: 0.00002258
Iteration 83/1000 | Loss: 0.00002249
Iteration 84/1000 | Loss: 0.00002243
Iteration 85/1000 | Loss: 0.00002239
Iteration 86/1000 | Loss: 0.00002239
Iteration 87/1000 | Loss: 0.00002238
Iteration 88/1000 | Loss: 0.00027389
Iteration 89/1000 | Loss: 0.00027643
Iteration 90/1000 | Loss: 0.00013216
Iteration 91/1000 | Loss: 0.00009673
Iteration 92/1000 | Loss: 0.00005169
Iteration 93/1000 | Loss: 0.00003461
Iteration 94/1000 | Loss: 0.00002966
Iteration 95/1000 | Loss: 0.00002847
Iteration 96/1000 | Loss: 0.00002588
Iteration 97/1000 | Loss: 0.00002485
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002365
Iteration 100/1000 | Loss: 0.00025614
Iteration 101/1000 | Loss: 0.00070587
Iteration 102/1000 | Loss: 0.00004319
Iteration 103/1000 | Loss: 0.00003036
Iteration 104/1000 | Loss: 0.00002539
Iteration 105/1000 | Loss: 0.00002381
Iteration 106/1000 | Loss: 0.00002301
Iteration 107/1000 | Loss: 0.00002240
Iteration 108/1000 | Loss: 0.00002190
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002132
Iteration 111/1000 | Loss: 0.00002123
Iteration 112/1000 | Loss: 0.00002120
Iteration 113/1000 | Loss: 0.00002119
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002112
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00002110
Iteration 118/1000 | Loss: 0.00002109
Iteration 119/1000 | Loss: 0.00002108
Iteration 120/1000 | Loss: 0.00002104
Iteration 121/1000 | Loss: 0.00002104
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002099
Iteration 125/1000 | Loss: 0.00002099
Iteration 126/1000 | Loss: 0.00002098
Iteration 127/1000 | Loss: 0.00002097
Iteration 128/1000 | Loss: 0.00002097
Iteration 129/1000 | Loss: 0.00002096
Iteration 130/1000 | Loss: 0.00002096
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002090
Iteration 133/1000 | Loss: 0.00002089
Iteration 134/1000 | Loss: 0.00002087
Iteration 135/1000 | Loss: 0.00002087
Iteration 136/1000 | Loss: 0.00002087
Iteration 137/1000 | Loss: 0.00002086
Iteration 138/1000 | Loss: 0.00002086
Iteration 139/1000 | Loss: 0.00002086
Iteration 140/1000 | Loss: 0.00002086
Iteration 141/1000 | Loss: 0.00002086
Iteration 142/1000 | Loss: 0.00002086
Iteration 143/1000 | Loss: 0.00002086
Iteration 144/1000 | Loss: 0.00002086
Iteration 145/1000 | Loss: 0.00002086
Iteration 146/1000 | Loss: 0.00002086
Iteration 147/1000 | Loss: 0.00002086
Iteration 148/1000 | Loss: 0.00002086
Iteration 149/1000 | Loss: 0.00002085
Iteration 150/1000 | Loss: 0.00002083
Iteration 151/1000 | Loss: 0.00002083
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002083
Iteration 154/1000 | Loss: 0.00002083
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002082
Iteration 157/1000 | Loss: 0.00002081
Iteration 158/1000 | Loss: 0.00002081
Iteration 159/1000 | Loss: 0.00002080
Iteration 160/1000 | Loss: 0.00002080
Iteration 161/1000 | Loss: 0.00002080
Iteration 162/1000 | Loss: 0.00002080
Iteration 163/1000 | Loss: 0.00002080
Iteration 164/1000 | Loss: 0.00002080
Iteration 165/1000 | Loss: 0.00002080
Iteration 166/1000 | Loss: 0.00002080
Iteration 167/1000 | Loss: 0.00002080
Iteration 168/1000 | Loss: 0.00002080
Iteration 169/1000 | Loss: 0.00002080
Iteration 170/1000 | Loss: 0.00002080
Iteration 171/1000 | Loss: 0.00002080
Iteration 172/1000 | Loss: 0.00002079
Iteration 173/1000 | Loss: 0.00002079
Iteration 174/1000 | Loss: 0.00002079
Iteration 175/1000 | Loss: 0.00002079
Iteration 176/1000 | Loss: 0.00002079
Iteration 177/1000 | Loss: 0.00002079
Iteration 178/1000 | Loss: 0.00002079
Iteration 179/1000 | Loss: 0.00002078
Iteration 180/1000 | Loss: 0.00002078
Iteration 181/1000 | Loss: 0.00002078
Iteration 182/1000 | Loss: 0.00002078
Iteration 183/1000 | Loss: 0.00002077
Iteration 184/1000 | Loss: 0.00002077
Iteration 185/1000 | Loss: 0.00002077
Iteration 186/1000 | Loss: 0.00002077
Iteration 187/1000 | Loss: 0.00002077
Iteration 188/1000 | Loss: 0.00002077
Iteration 189/1000 | Loss: 0.00002076
Iteration 190/1000 | Loss: 0.00002076
Iteration 191/1000 | Loss: 0.00002076
Iteration 192/1000 | Loss: 0.00002076
Iteration 193/1000 | Loss: 0.00002076
Iteration 194/1000 | Loss: 0.00002076
Iteration 195/1000 | Loss: 0.00002076
Iteration 196/1000 | Loss: 0.00002075
Iteration 197/1000 | Loss: 0.00002075
Iteration 198/1000 | Loss: 0.00002075
Iteration 199/1000 | Loss: 0.00002075
Iteration 200/1000 | Loss: 0.00002075
Iteration 201/1000 | Loss: 0.00002075
Iteration 202/1000 | Loss: 0.00002075
Iteration 203/1000 | Loss: 0.00002074
Iteration 204/1000 | Loss: 0.00002074
Iteration 205/1000 | Loss: 0.00002074
Iteration 206/1000 | Loss: 0.00002074
Iteration 207/1000 | Loss: 0.00002074
Iteration 208/1000 | Loss: 0.00002074
Iteration 209/1000 | Loss: 0.00002074
Iteration 210/1000 | Loss: 0.00002074
Iteration 211/1000 | Loss: 0.00002074
Iteration 212/1000 | Loss: 0.00002074
Iteration 213/1000 | Loss: 0.00002074
Iteration 214/1000 | Loss: 0.00002074
Iteration 215/1000 | Loss: 0.00002074
Iteration 216/1000 | Loss: 0.00002074
Iteration 217/1000 | Loss: 0.00002074
Iteration 218/1000 | Loss: 0.00002074
Iteration 219/1000 | Loss: 0.00002073
Iteration 220/1000 | Loss: 0.00002073
Iteration 221/1000 | Loss: 0.00002073
Iteration 222/1000 | Loss: 0.00002073
Iteration 223/1000 | Loss: 0.00002073
Iteration 224/1000 | Loss: 0.00002073
Iteration 225/1000 | Loss: 0.00002073
Iteration 226/1000 | Loss: 0.00002073
Iteration 227/1000 | Loss: 0.00002073
Iteration 228/1000 | Loss: 0.00002073
Iteration 229/1000 | Loss: 0.00002073
Iteration 230/1000 | Loss: 0.00002073
Iteration 231/1000 | Loss: 0.00002073
Iteration 232/1000 | Loss: 0.00002073
Iteration 233/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.0731014956254512e-05, 2.0731014956254512e-05, 2.0731014956254512e-05, 2.0731014956254512e-05, 2.0731014956254512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0731014956254512e-05

Optimization complete. Final v2v error: 3.8036317825317383 mm

Highest mean error: 5.4338250160217285 mm for frame 226

Lowest mean error: 3.3026063442230225 mm for frame 56

Saving results

Total time: 235.7287619113922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145635
Iteration 2/25 | Loss: 0.00223853
Iteration 3/25 | Loss: 0.00160596
Iteration 4/25 | Loss: 0.00166307
Iteration 5/25 | Loss: 0.00143825
Iteration 6/25 | Loss: 0.00146710
Iteration 7/25 | Loss: 0.00122776
Iteration 8/25 | Loss: 0.00134397
Iteration 9/25 | Loss: 0.00115474
Iteration 10/25 | Loss: 0.00112556
Iteration 11/25 | Loss: 0.00113066
Iteration 12/25 | Loss: 0.00110224
Iteration 13/25 | Loss: 0.00109517
Iteration 14/25 | Loss: 0.00108906
Iteration 15/25 | Loss: 0.00108942
Iteration 16/25 | Loss: 0.00108798
Iteration 17/25 | Loss: 0.00108746
Iteration 18/25 | Loss: 0.00108718
Iteration 19/25 | Loss: 0.00108581
Iteration 20/25 | Loss: 0.00108520
Iteration 21/25 | Loss: 0.00108416
Iteration 22/25 | Loss: 0.00108497
Iteration 23/25 | Loss: 0.00108326
Iteration 24/25 | Loss: 0.00108288
Iteration 25/25 | Loss: 0.00108450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00631142
Iteration 2/25 | Loss: 0.00242963
Iteration 3/25 | Loss: 0.00242936
Iteration 4/25 | Loss: 0.00242936
Iteration 5/25 | Loss: 0.00242936
Iteration 6/25 | Loss: 0.00242936
Iteration 7/25 | Loss: 0.00242936
Iteration 8/25 | Loss: 0.00242936
Iteration 9/25 | Loss: 0.00242936
Iteration 10/25 | Loss: 0.00242936
Iteration 11/25 | Loss: 0.00242936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0024293619208037853, 0.0024293619208037853, 0.0024293619208037853, 0.0024293619208037853, 0.0024293619208037853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024293619208037853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242936
Iteration 2/1000 | Loss: 0.00031483
Iteration 3/1000 | Loss: 0.00035038
Iteration 4/1000 | Loss: 0.00026849
Iteration 5/1000 | Loss: 0.00014032
Iteration 6/1000 | Loss: 0.00021022
Iteration 7/1000 | Loss: 0.00021032
Iteration 8/1000 | Loss: 0.00020143
Iteration 9/1000 | Loss: 0.00019542
Iteration 10/1000 | Loss: 0.00030671
Iteration 11/1000 | Loss: 0.00018073
Iteration 12/1000 | Loss: 0.00026630
Iteration 13/1000 | Loss: 0.00030648
Iteration 14/1000 | Loss: 0.00022945
Iteration 15/1000 | Loss: 0.00516844
Iteration 16/1000 | Loss: 0.00022239
Iteration 17/1000 | Loss: 0.00016560
Iteration 18/1000 | Loss: 0.00023135
Iteration 19/1000 | Loss: 0.00129429
Iteration 20/1000 | Loss: 0.00050691
Iteration 21/1000 | Loss: 0.00022770
Iteration 22/1000 | Loss: 0.00079600
Iteration 23/1000 | Loss: 0.00039588
Iteration 24/1000 | Loss: 0.00025421
Iteration 25/1000 | Loss: 0.00015977
Iteration 26/1000 | Loss: 0.00027463
Iteration 27/1000 | Loss: 0.00034593
Iteration 28/1000 | Loss: 0.00029593
Iteration 29/1000 | Loss: 0.00030684
Iteration 30/1000 | Loss: 0.00022164
Iteration 31/1000 | Loss: 0.00017401
Iteration 32/1000 | Loss: 0.00025457
Iteration 33/1000 | Loss: 0.00016283
Iteration 34/1000 | Loss: 0.00022293
Iteration 35/1000 | Loss: 0.00029314
Iteration 36/1000 | Loss: 0.00016963
Iteration 37/1000 | Loss: 0.00028085
Iteration 38/1000 | Loss: 0.00024387
Iteration 39/1000 | Loss: 0.00056588
Iteration 40/1000 | Loss: 0.00022712
Iteration 41/1000 | Loss: 0.00037583
Iteration 42/1000 | Loss: 0.00006540
Iteration 43/1000 | Loss: 0.00004990
Iteration 44/1000 | Loss: 0.00004766
Iteration 45/1000 | Loss: 0.00004664
Iteration 46/1000 | Loss: 0.00006727
Iteration 47/1000 | Loss: 0.00005219
Iteration 48/1000 | Loss: 0.00006489
Iteration 49/1000 | Loss: 0.00005602
Iteration 50/1000 | Loss: 0.00004449
Iteration 51/1000 | Loss: 0.00004387
Iteration 52/1000 | Loss: 0.00004317
Iteration 53/1000 | Loss: 0.00006002
Iteration 54/1000 | Loss: 0.00005383
Iteration 55/1000 | Loss: 0.00006665
Iteration 56/1000 | Loss: 0.00006301
Iteration 57/1000 | Loss: 0.00006436
Iteration 58/1000 | Loss: 0.00005650
Iteration 59/1000 | Loss: 0.00005530
Iteration 60/1000 | Loss: 0.00004559
Iteration 61/1000 | Loss: 0.00004312
Iteration 62/1000 | Loss: 0.00004162
Iteration 63/1000 | Loss: 0.00004102
Iteration 64/1000 | Loss: 0.00004069
Iteration 65/1000 | Loss: 0.00004046
Iteration 66/1000 | Loss: 0.00004041
Iteration 67/1000 | Loss: 0.00004038
Iteration 68/1000 | Loss: 0.00004038
Iteration 69/1000 | Loss: 0.00004037
Iteration 70/1000 | Loss: 0.00004036
Iteration 71/1000 | Loss: 0.00004035
Iteration 72/1000 | Loss: 0.00004031
Iteration 73/1000 | Loss: 0.00004025
Iteration 74/1000 | Loss: 0.00004024
Iteration 75/1000 | Loss: 0.00004024
Iteration 76/1000 | Loss: 0.00004023
Iteration 77/1000 | Loss: 0.00004023
Iteration 78/1000 | Loss: 0.00004019
Iteration 79/1000 | Loss: 0.00004019
Iteration 80/1000 | Loss: 0.00004012
Iteration 81/1000 | Loss: 0.00004010
Iteration 82/1000 | Loss: 0.00004009
Iteration 83/1000 | Loss: 0.00004009
Iteration 84/1000 | Loss: 0.00004009
Iteration 85/1000 | Loss: 0.00004009
Iteration 86/1000 | Loss: 0.00004009
Iteration 87/1000 | Loss: 0.00004008
Iteration 88/1000 | Loss: 0.00004001
Iteration 89/1000 | Loss: 0.00003987
Iteration 90/1000 | Loss: 0.00003981
Iteration 91/1000 | Loss: 0.00003980
Iteration 92/1000 | Loss: 0.00003978
Iteration 93/1000 | Loss: 0.00003977
Iteration 94/1000 | Loss: 0.00003977
Iteration 95/1000 | Loss: 0.00003976
Iteration 96/1000 | Loss: 0.00003976
Iteration 97/1000 | Loss: 0.00003976
Iteration 98/1000 | Loss: 0.00003976
Iteration 99/1000 | Loss: 0.00003976
Iteration 100/1000 | Loss: 0.00003976
Iteration 101/1000 | Loss: 0.00003976
Iteration 102/1000 | Loss: 0.00003976
Iteration 103/1000 | Loss: 0.00003976
Iteration 104/1000 | Loss: 0.00003976
Iteration 105/1000 | Loss: 0.00003976
Iteration 106/1000 | Loss: 0.00003976
Iteration 107/1000 | Loss: 0.00003975
Iteration 108/1000 | Loss: 0.00003975
Iteration 109/1000 | Loss: 0.00003975
Iteration 110/1000 | Loss: 0.00003975
Iteration 111/1000 | Loss: 0.00003974
Iteration 112/1000 | Loss: 0.00003974
Iteration 113/1000 | Loss: 0.00003974
Iteration 114/1000 | Loss: 0.00003974
Iteration 115/1000 | Loss: 0.00003973
Iteration 116/1000 | Loss: 0.00003973
Iteration 117/1000 | Loss: 0.00003973
Iteration 118/1000 | Loss: 0.00003973
Iteration 119/1000 | Loss: 0.00003972
Iteration 120/1000 | Loss: 0.00003972
Iteration 121/1000 | Loss: 0.00003972
Iteration 122/1000 | Loss: 0.00003972
Iteration 123/1000 | Loss: 0.00003972
Iteration 124/1000 | Loss: 0.00003972
Iteration 125/1000 | Loss: 0.00003972
Iteration 126/1000 | Loss: 0.00003972
Iteration 127/1000 | Loss: 0.00003972
Iteration 128/1000 | Loss: 0.00003972
Iteration 129/1000 | Loss: 0.00003972
Iteration 130/1000 | Loss: 0.00003972
Iteration 131/1000 | Loss: 0.00003972
Iteration 132/1000 | Loss: 0.00003972
Iteration 133/1000 | Loss: 0.00003972
Iteration 134/1000 | Loss: 0.00003971
Iteration 135/1000 | Loss: 0.00003971
Iteration 136/1000 | Loss: 0.00003971
Iteration 137/1000 | Loss: 0.00003971
Iteration 138/1000 | Loss: 0.00003971
Iteration 139/1000 | Loss: 0.00003971
Iteration 140/1000 | Loss: 0.00003971
Iteration 141/1000 | Loss: 0.00003971
Iteration 142/1000 | Loss: 0.00003971
Iteration 143/1000 | Loss: 0.00003971
Iteration 144/1000 | Loss: 0.00003971
Iteration 145/1000 | Loss: 0.00003970
Iteration 146/1000 | Loss: 0.00003970
Iteration 147/1000 | Loss: 0.00003970
Iteration 148/1000 | Loss: 0.00003970
Iteration 149/1000 | Loss: 0.00003970
Iteration 150/1000 | Loss: 0.00003970
Iteration 151/1000 | Loss: 0.00003970
Iteration 152/1000 | Loss: 0.00003970
Iteration 153/1000 | Loss: 0.00003970
Iteration 154/1000 | Loss: 0.00003970
Iteration 155/1000 | Loss: 0.00003970
Iteration 156/1000 | Loss: 0.00003970
Iteration 157/1000 | Loss: 0.00003970
Iteration 158/1000 | Loss: 0.00003970
Iteration 159/1000 | Loss: 0.00003970
Iteration 160/1000 | Loss: 0.00003970
Iteration 161/1000 | Loss: 0.00003970
Iteration 162/1000 | Loss: 0.00003970
Iteration 163/1000 | Loss: 0.00003970
Iteration 164/1000 | Loss: 0.00003970
Iteration 165/1000 | Loss: 0.00003970
Iteration 166/1000 | Loss: 0.00003970
Iteration 167/1000 | Loss: 0.00003970
Iteration 168/1000 | Loss: 0.00003970
Iteration 169/1000 | Loss: 0.00003970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.9702463254798204e-05, 3.9702463254798204e-05, 3.9702463254798204e-05, 3.9702463254798204e-05, 3.9702463254798204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9702463254798204e-05

Optimization complete. Final v2v error: 5.242159366607666 mm

Highest mean error: 6.437679767608643 mm for frame 91

Lowest mean error: 4.025018692016602 mm for frame 0

Saving results

Total time: 146.7424943447113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870082
Iteration 2/25 | Loss: 0.00136528
Iteration 3/25 | Loss: 0.00115998
Iteration 4/25 | Loss: 0.00110162
Iteration 5/25 | Loss: 0.00104915
Iteration 6/25 | Loss: 0.00104568
Iteration 7/25 | Loss: 0.00104401
Iteration 8/25 | Loss: 0.00104700
Iteration 9/25 | Loss: 0.00104237
Iteration 10/25 | Loss: 0.00103891
Iteration 11/25 | Loss: 0.00103715
Iteration 12/25 | Loss: 0.00103615
Iteration 13/25 | Loss: 0.00103553
Iteration 14/25 | Loss: 0.00103510
Iteration 15/25 | Loss: 0.00103496
Iteration 16/25 | Loss: 0.00103493
Iteration 17/25 | Loss: 0.00103493
Iteration 18/25 | Loss: 0.00103493
Iteration 19/25 | Loss: 0.00103493
Iteration 20/25 | Loss: 0.00103493
Iteration 21/25 | Loss: 0.00103493
Iteration 22/25 | Loss: 0.00103493
Iteration 23/25 | Loss: 0.00103493
Iteration 24/25 | Loss: 0.00103493
Iteration 25/25 | Loss: 0.00103493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79140019
Iteration 2/25 | Loss: 0.00178251
Iteration 3/25 | Loss: 0.00178247
Iteration 4/25 | Loss: 0.00178247
Iteration 5/25 | Loss: 0.00178247
Iteration 6/25 | Loss: 0.00178247
Iteration 7/25 | Loss: 0.00178247
Iteration 8/25 | Loss: 0.00178247
Iteration 9/25 | Loss: 0.00178247
Iteration 10/25 | Loss: 0.00178247
Iteration 11/25 | Loss: 0.00178247
Iteration 12/25 | Loss: 0.00178247
Iteration 13/25 | Loss: 0.00178247
Iteration 14/25 | Loss: 0.00178247
Iteration 15/25 | Loss: 0.00178247
Iteration 16/25 | Loss: 0.00178247
Iteration 17/25 | Loss: 0.00178247
Iteration 18/25 | Loss: 0.00178247
Iteration 19/25 | Loss: 0.00178247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017824716633185744, 0.0017824716633185744, 0.0017824716633185744, 0.0017824716633185744, 0.0017824716633185744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017824716633185744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178247
Iteration 2/1000 | Loss: 0.00004543
Iteration 3/1000 | Loss: 0.00002981
Iteration 4/1000 | Loss: 0.00002763
Iteration 5/1000 | Loss: 0.00002579
Iteration 6/1000 | Loss: 0.00002472
Iteration 7/1000 | Loss: 0.00002410
Iteration 8/1000 | Loss: 0.00002377
Iteration 9/1000 | Loss: 0.00002355
Iteration 10/1000 | Loss: 0.00002349
Iteration 11/1000 | Loss: 0.00002349
Iteration 12/1000 | Loss: 0.00002346
Iteration 13/1000 | Loss: 0.00002338
Iteration 14/1000 | Loss: 0.00002335
Iteration 15/1000 | Loss: 0.00002334
Iteration 16/1000 | Loss: 0.00002334
Iteration 17/1000 | Loss: 0.00002333
Iteration 18/1000 | Loss: 0.00002332
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002331
Iteration 21/1000 | Loss: 0.00002331
Iteration 22/1000 | Loss: 0.00002330
Iteration 23/1000 | Loss: 0.00002330
Iteration 24/1000 | Loss: 0.00002330
Iteration 25/1000 | Loss: 0.00002329
Iteration 26/1000 | Loss: 0.00002329
Iteration 27/1000 | Loss: 0.00002329
Iteration 28/1000 | Loss: 0.00002328
Iteration 29/1000 | Loss: 0.00002328
Iteration 30/1000 | Loss: 0.00002327
Iteration 31/1000 | Loss: 0.00002327
Iteration 32/1000 | Loss: 0.00002327
Iteration 33/1000 | Loss: 0.00002327
Iteration 34/1000 | Loss: 0.00002326
Iteration 35/1000 | Loss: 0.00002326
Iteration 36/1000 | Loss: 0.00002326
Iteration 37/1000 | Loss: 0.00002326
Iteration 38/1000 | Loss: 0.00002326
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002325
Iteration 41/1000 | Loss: 0.00002325
Iteration 42/1000 | Loss: 0.00002324
Iteration 43/1000 | Loss: 0.00002324
Iteration 44/1000 | Loss: 0.00002324
Iteration 45/1000 | Loss: 0.00002323
Iteration 46/1000 | Loss: 0.00002323
Iteration 47/1000 | Loss: 0.00002323
Iteration 48/1000 | Loss: 0.00002323
Iteration 49/1000 | Loss: 0.00002322
Iteration 50/1000 | Loss: 0.00002322
Iteration 51/1000 | Loss: 0.00002322
Iteration 52/1000 | Loss: 0.00002321
Iteration 53/1000 | Loss: 0.00002321
Iteration 54/1000 | Loss: 0.00002321
Iteration 55/1000 | Loss: 0.00002321
Iteration 56/1000 | Loss: 0.00002321
Iteration 57/1000 | Loss: 0.00002320
Iteration 58/1000 | Loss: 0.00002320
Iteration 59/1000 | Loss: 0.00002320
Iteration 60/1000 | Loss: 0.00002320
Iteration 61/1000 | Loss: 0.00002320
Iteration 62/1000 | Loss: 0.00002319
Iteration 63/1000 | Loss: 0.00002319
Iteration 64/1000 | Loss: 0.00002319
Iteration 65/1000 | Loss: 0.00002318
Iteration 66/1000 | Loss: 0.00002318
Iteration 67/1000 | Loss: 0.00002318
Iteration 68/1000 | Loss: 0.00002318
Iteration 69/1000 | Loss: 0.00002317
Iteration 70/1000 | Loss: 0.00002317
Iteration 71/1000 | Loss: 0.00002317
Iteration 72/1000 | Loss: 0.00002317
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002316
Iteration 76/1000 | Loss: 0.00002315
Iteration 77/1000 | Loss: 0.00002315
Iteration 78/1000 | Loss: 0.00002315
Iteration 79/1000 | Loss: 0.00002315
Iteration 80/1000 | Loss: 0.00002314
Iteration 81/1000 | Loss: 0.00002314
Iteration 82/1000 | Loss: 0.00002314
Iteration 83/1000 | Loss: 0.00002314
Iteration 84/1000 | Loss: 0.00002313
Iteration 85/1000 | Loss: 0.00002313
Iteration 86/1000 | Loss: 0.00002313
Iteration 87/1000 | Loss: 0.00002313
Iteration 88/1000 | Loss: 0.00002313
Iteration 89/1000 | Loss: 0.00002313
Iteration 90/1000 | Loss: 0.00002313
Iteration 91/1000 | Loss: 0.00002313
Iteration 92/1000 | Loss: 0.00002313
Iteration 93/1000 | Loss: 0.00002312
Iteration 94/1000 | Loss: 0.00002312
Iteration 95/1000 | Loss: 0.00002312
Iteration 96/1000 | Loss: 0.00002312
Iteration 97/1000 | Loss: 0.00002312
Iteration 98/1000 | Loss: 0.00002312
Iteration 99/1000 | Loss: 0.00002312
Iteration 100/1000 | Loss: 0.00002312
Iteration 101/1000 | Loss: 0.00002312
Iteration 102/1000 | Loss: 0.00002312
Iteration 103/1000 | Loss: 0.00002312
Iteration 104/1000 | Loss: 0.00002312
Iteration 105/1000 | Loss: 0.00002312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.3123095161281526e-05, 2.3123095161281526e-05, 2.3123095161281526e-05, 2.3123095161281526e-05, 2.3123095161281526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3123095161281526e-05

Optimization complete. Final v2v error: 4.0765252113342285 mm

Highest mean error: 4.716004371643066 mm for frame 43

Lowest mean error: 3.687966823577881 mm for frame 123

Saving results

Total time: 53.86265230178833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072179
Iteration 2/25 | Loss: 0.00264111
Iteration 3/25 | Loss: 0.00170668
Iteration 4/25 | Loss: 0.00155926
Iteration 5/25 | Loss: 0.00152134
Iteration 6/25 | Loss: 0.00130114
Iteration 7/25 | Loss: 0.00114999
Iteration 8/25 | Loss: 0.00109504
Iteration 9/25 | Loss: 0.00106540
Iteration 10/25 | Loss: 0.00104863
Iteration 11/25 | Loss: 0.00101531
Iteration 12/25 | Loss: 0.00100461
Iteration 13/25 | Loss: 0.00102234
Iteration 14/25 | Loss: 0.00098041
Iteration 15/25 | Loss: 0.00096657
Iteration 16/25 | Loss: 0.00096543
Iteration 17/25 | Loss: 0.00096134
Iteration 18/25 | Loss: 0.00098424
Iteration 19/25 | Loss: 0.00094713
Iteration 20/25 | Loss: 0.00094556
Iteration 21/25 | Loss: 0.00094221
Iteration 22/25 | Loss: 0.00094283
Iteration 23/25 | Loss: 0.00093195
Iteration 24/25 | Loss: 0.00093094
Iteration 25/25 | Loss: 0.00092143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67221570
Iteration 2/25 | Loss: 0.00300867
Iteration 3/25 | Loss: 0.00275714
Iteration 4/25 | Loss: 0.00275714
Iteration 5/25 | Loss: 0.00275714
Iteration 6/25 | Loss: 0.00275714
Iteration 7/25 | Loss: 0.00275713
Iteration 8/25 | Loss: 0.00275713
Iteration 9/25 | Loss: 0.00275713
Iteration 10/25 | Loss: 0.00275713
Iteration 11/25 | Loss: 0.00275713
Iteration 12/25 | Loss: 0.00275713
Iteration 13/25 | Loss: 0.00275713
Iteration 14/25 | Loss: 0.00275713
Iteration 15/25 | Loss: 0.00275713
Iteration 16/25 | Loss: 0.00275713
Iteration 17/25 | Loss: 0.00275713
Iteration 18/25 | Loss: 0.00275713
Iteration 19/25 | Loss: 0.00275713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002757134148851037, 0.002757134148851037, 0.002757134148851037, 0.002757134148851037, 0.002757134148851037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002757134148851037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275713
Iteration 2/1000 | Loss: 0.00019692
Iteration 3/1000 | Loss: 0.00119908
Iteration 4/1000 | Loss: 0.00080755
Iteration 5/1000 | Loss: 0.00038057
Iteration 6/1000 | Loss: 0.00009607
Iteration 7/1000 | Loss: 0.00019913
Iteration 8/1000 | Loss: 0.00006368
Iteration 9/1000 | Loss: 0.00019153
Iteration 10/1000 | Loss: 0.00004443
Iteration 11/1000 | Loss: 0.00050017
Iteration 12/1000 | Loss: 0.00169229
Iteration 13/1000 | Loss: 0.00007112
Iteration 14/1000 | Loss: 0.00058840
Iteration 15/1000 | Loss: 0.00006008
Iteration 16/1000 | Loss: 0.00003074
Iteration 17/1000 | Loss: 0.00009667
Iteration 18/1000 | Loss: 0.00029501
Iteration 19/1000 | Loss: 0.00006636
Iteration 20/1000 | Loss: 0.00005577
Iteration 21/1000 | Loss: 0.00004629
Iteration 22/1000 | Loss: 0.00127766
Iteration 23/1000 | Loss: 0.00062288
Iteration 24/1000 | Loss: 0.00121897
Iteration 25/1000 | Loss: 0.00041042
Iteration 26/1000 | Loss: 0.00013360
Iteration 27/1000 | Loss: 0.00093564
Iteration 28/1000 | Loss: 0.00016655
Iteration 29/1000 | Loss: 0.00014687
Iteration 30/1000 | Loss: 0.00009274
Iteration 31/1000 | Loss: 0.00022213
Iteration 32/1000 | Loss: 0.00004560
Iteration 33/1000 | Loss: 0.00005247
Iteration 34/1000 | Loss: 0.00021985
Iteration 35/1000 | Loss: 0.00043873
Iteration 36/1000 | Loss: 0.00015151
Iteration 37/1000 | Loss: 0.00016539
Iteration 38/1000 | Loss: 0.00018864
Iteration 39/1000 | Loss: 0.00008655
Iteration 40/1000 | Loss: 0.00008031
Iteration 41/1000 | Loss: 0.00006808
Iteration 42/1000 | Loss: 0.00023759
Iteration 43/1000 | Loss: 0.00004800
Iteration 44/1000 | Loss: 0.00012083
Iteration 45/1000 | Loss: 0.00026515
Iteration 46/1000 | Loss: 0.00029962
Iteration 47/1000 | Loss: 0.00009277
Iteration 48/1000 | Loss: 0.00004987
Iteration 49/1000 | Loss: 0.00014909
Iteration 50/1000 | Loss: 0.00009433
Iteration 51/1000 | Loss: 0.00005418
Iteration 52/1000 | Loss: 0.00005807
Iteration 53/1000 | Loss: 0.00010721
Iteration 54/1000 | Loss: 0.00006359
Iteration 55/1000 | Loss: 0.00004510
Iteration 56/1000 | Loss: 0.00011976
Iteration 57/1000 | Loss: 0.00002645
Iteration 58/1000 | Loss: 0.00005113
Iteration 59/1000 | Loss: 0.00011216
Iteration 60/1000 | Loss: 0.00032530
Iteration 61/1000 | Loss: 0.00005411
Iteration 62/1000 | Loss: 0.00006128
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00042479
Iteration 66/1000 | Loss: 0.00041605
Iteration 67/1000 | Loss: 0.00016328
Iteration 68/1000 | Loss: 0.00022017
Iteration 69/1000 | Loss: 0.00011145
Iteration 70/1000 | Loss: 0.00009878
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00017337
Iteration 74/1000 | Loss: 0.00017450
Iteration 75/1000 | Loss: 0.00002910
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00002452
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00004036
Iteration 83/1000 | Loss: 0.00010940
Iteration 84/1000 | Loss: 0.00002372
Iteration 85/1000 | Loss: 0.00010496
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00004726
Iteration 88/1000 | Loss: 0.00002848
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001460
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00020779
Iteration 95/1000 | Loss: 0.00027658
Iteration 96/1000 | Loss: 0.00006909
Iteration 97/1000 | Loss: 0.00006811
Iteration 98/1000 | Loss: 0.00007081
Iteration 99/1000 | Loss: 0.00024482
Iteration 100/1000 | Loss: 0.00006794
Iteration 101/1000 | Loss: 0.00008166
Iteration 102/1000 | Loss: 0.00009260
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00006608
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00013146
Iteration 107/1000 | Loss: 0.00006001
Iteration 108/1000 | Loss: 0.00007873
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00007867
Iteration 111/1000 | Loss: 0.00001614
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00005410
Iteration 114/1000 | Loss: 0.00001187
Iteration 115/1000 | Loss: 0.00003307
Iteration 116/1000 | Loss: 0.00005666
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001162
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00002276
Iteration 126/1000 | Loss: 0.00001155
Iteration 127/1000 | Loss: 0.00006243
Iteration 128/1000 | Loss: 0.00009930
Iteration 129/1000 | Loss: 0.00002845
Iteration 130/1000 | Loss: 0.00006270
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00002777
Iteration 135/1000 | Loss: 0.00003582
Iteration 136/1000 | Loss: 0.00002239
Iteration 137/1000 | Loss: 0.00001779
Iteration 138/1000 | Loss: 0.00001933
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001145
Iteration 142/1000 | Loss: 0.00001145
Iteration 143/1000 | Loss: 0.00001145
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.1428353900555521e-05, 1.1428353900555521e-05, 1.1428353900555521e-05, 1.1428353900555521e-05, 1.1428353900555521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1428353900555521e-05

Optimization complete. Final v2v error: 2.894603729248047 mm

Highest mean error: 4.968288421630859 mm for frame 50

Lowest mean error: 2.397568702697754 mm for frame 78

Saving results

Total time: 252.34720587730408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410942
Iteration 2/25 | Loss: 0.00102358
Iteration 3/25 | Loss: 0.00088759
Iteration 4/25 | Loss: 0.00086438
Iteration 5/25 | Loss: 0.00085878
Iteration 6/25 | Loss: 0.00085802
Iteration 7/25 | Loss: 0.00085802
Iteration 8/25 | Loss: 0.00085802
Iteration 9/25 | Loss: 0.00085802
Iteration 10/25 | Loss: 0.00085802
Iteration 11/25 | Loss: 0.00085802
Iteration 12/25 | Loss: 0.00085802
Iteration 13/25 | Loss: 0.00085802
Iteration 14/25 | Loss: 0.00085802
Iteration 15/25 | Loss: 0.00085802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008580231224186718, 0.0008580231224186718, 0.0008580231224186718, 0.0008580231224186718, 0.0008580231224186718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008580231224186718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61319697
Iteration 2/25 | Loss: 0.00174863
Iteration 3/25 | Loss: 0.00174863
Iteration 4/25 | Loss: 0.00174863
Iteration 5/25 | Loss: 0.00174863
Iteration 6/25 | Loss: 0.00174863
Iteration 7/25 | Loss: 0.00174863
Iteration 8/25 | Loss: 0.00174863
Iteration 9/25 | Loss: 0.00174863
Iteration 10/25 | Loss: 0.00174863
Iteration 11/25 | Loss: 0.00174863
Iteration 12/25 | Loss: 0.00174863
Iteration 13/25 | Loss: 0.00174863
Iteration 14/25 | Loss: 0.00174863
Iteration 15/25 | Loss: 0.00174863
Iteration 16/25 | Loss: 0.00174863
Iteration 17/25 | Loss: 0.00174863
Iteration 18/25 | Loss: 0.00174863
Iteration 19/25 | Loss: 0.00174863
Iteration 20/25 | Loss: 0.00174863
Iteration 21/25 | Loss: 0.00174863
Iteration 22/25 | Loss: 0.00174863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017486298456788063, 0.0017486298456788063, 0.0017486298456788063, 0.0017486298456788063, 0.0017486298456788063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017486298456788063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174863
Iteration 2/1000 | Loss: 0.00003228
Iteration 3/1000 | Loss: 0.00001914
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001460
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001444
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001437
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001437
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001433
Iteration 24/1000 | Loss: 0.00001433
Iteration 25/1000 | Loss: 0.00001433
Iteration 26/1000 | Loss: 0.00001432
Iteration 27/1000 | Loss: 0.00001432
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001431
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001428
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001428
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001423
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001423
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001421
Iteration 78/1000 | Loss: 0.00001421
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001419
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001419
Iteration 90/1000 | Loss: 0.00001419
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001418
Iteration 93/1000 | Loss: 0.00001418
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001418
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001418
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001417
Iteration 104/1000 | Loss: 0.00001417
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001417
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001415
Iteration 121/1000 | Loss: 0.00001415
Iteration 122/1000 | Loss: 0.00001415
Iteration 123/1000 | Loss: 0.00001415
Iteration 124/1000 | Loss: 0.00001415
Iteration 125/1000 | Loss: 0.00001415
Iteration 126/1000 | Loss: 0.00001415
Iteration 127/1000 | Loss: 0.00001415
Iteration 128/1000 | Loss: 0.00001415
Iteration 129/1000 | Loss: 0.00001415
Iteration 130/1000 | Loss: 0.00001415
Iteration 131/1000 | Loss: 0.00001415
Iteration 132/1000 | Loss: 0.00001415
Iteration 133/1000 | Loss: 0.00001415
Iteration 134/1000 | Loss: 0.00001415
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001414
Iteration 140/1000 | Loss: 0.00001414
Iteration 141/1000 | Loss: 0.00001414
Iteration 142/1000 | Loss: 0.00001414
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001413
Iteration 145/1000 | Loss: 0.00001413
Iteration 146/1000 | Loss: 0.00001413
Iteration 147/1000 | Loss: 0.00001413
Iteration 148/1000 | Loss: 0.00001413
Iteration 149/1000 | Loss: 0.00001413
Iteration 150/1000 | Loss: 0.00001413
Iteration 151/1000 | Loss: 0.00001413
Iteration 152/1000 | Loss: 0.00001413
Iteration 153/1000 | Loss: 0.00001413
Iteration 154/1000 | Loss: 0.00001413
Iteration 155/1000 | Loss: 0.00001413
Iteration 156/1000 | Loss: 0.00001413
Iteration 157/1000 | Loss: 0.00001413
Iteration 158/1000 | Loss: 0.00001413
Iteration 159/1000 | Loss: 0.00001413
Iteration 160/1000 | Loss: 0.00001413
Iteration 161/1000 | Loss: 0.00001413
Iteration 162/1000 | Loss: 0.00001413
Iteration 163/1000 | Loss: 0.00001413
Iteration 164/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4132803698885255e-05, 1.4132803698885255e-05, 1.4132803698885255e-05, 1.4132803698885255e-05, 1.4132803698885255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4132803698885255e-05

Optimization complete. Final v2v error: 3.20892333984375 mm

Highest mean error: 3.587949752807617 mm for frame 113

Lowest mean error: 2.803845167160034 mm for frame 217

Saving results

Total time: 35.80920958518982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897195
Iteration 2/25 | Loss: 0.00125585
Iteration 3/25 | Loss: 0.00091605
Iteration 4/25 | Loss: 0.00086112
Iteration 5/25 | Loss: 0.00084641
Iteration 6/25 | Loss: 0.00084329
Iteration 7/25 | Loss: 0.00084188
Iteration 8/25 | Loss: 0.00084136
Iteration 9/25 | Loss: 0.00084097
Iteration 10/25 | Loss: 0.00084236
Iteration 11/25 | Loss: 0.00084432
Iteration 12/25 | Loss: 0.00084394
Iteration 13/25 | Loss: 0.00084196
Iteration 14/25 | Loss: 0.00084057
Iteration 15/25 | Loss: 0.00084290
Iteration 16/25 | Loss: 0.00084380
Iteration 17/25 | Loss: 0.00084278
Iteration 18/25 | Loss: 0.00084429
Iteration 19/25 | Loss: 0.00084418
Iteration 20/25 | Loss: 0.00084426
Iteration 21/25 | Loss: 0.00084394
Iteration 22/25 | Loss: 0.00084434
Iteration 23/25 | Loss: 0.00084449
Iteration 24/25 | Loss: 0.00084434
Iteration 25/25 | Loss: 0.00084382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62525654
Iteration 2/25 | Loss: 0.00189266
Iteration 3/25 | Loss: 0.00189266
Iteration 4/25 | Loss: 0.00189266
Iteration 5/25 | Loss: 0.00189266
Iteration 6/25 | Loss: 0.00189266
Iteration 7/25 | Loss: 0.00189266
Iteration 8/25 | Loss: 0.00189266
Iteration 9/25 | Loss: 0.00189266
Iteration 10/25 | Loss: 0.00189266
Iteration 11/25 | Loss: 0.00189266
Iteration 12/25 | Loss: 0.00189266
Iteration 13/25 | Loss: 0.00189266
Iteration 14/25 | Loss: 0.00189266
Iteration 15/25 | Loss: 0.00189266
Iteration 16/25 | Loss: 0.00189266
Iteration 17/25 | Loss: 0.00189266
Iteration 18/25 | Loss: 0.00189266
Iteration 19/25 | Loss: 0.00189266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00189265760127455, 0.00189265760127455, 0.00189265760127455, 0.00189265760127455, 0.00189265760127455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00189265760127455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189266
Iteration 2/1000 | Loss: 0.00006329
Iteration 3/1000 | Loss: 0.00005222
Iteration 4/1000 | Loss: 0.00004827
Iteration 5/1000 | Loss: 0.00006840
Iteration 6/1000 | Loss: 0.00006296
Iteration 7/1000 | Loss: 0.00005502
Iteration 8/1000 | Loss: 0.00006829
Iteration 9/1000 | Loss: 0.00006456
Iteration 10/1000 | Loss: 0.00006617
Iteration 11/1000 | Loss: 0.00003212
Iteration 12/1000 | Loss: 0.00004143
Iteration 13/1000 | Loss: 0.00004512
Iteration 14/1000 | Loss: 0.00005166
Iteration 15/1000 | Loss: 0.00004763
Iteration 16/1000 | Loss: 0.00004834
Iteration 17/1000 | Loss: 0.00003754
Iteration 18/1000 | Loss: 0.00004917
Iteration 19/1000 | Loss: 0.00004222
Iteration 20/1000 | Loss: 0.00004818
Iteration 21/1000 | Loss: 0.00003981
Iteration 22/1000 | Loss: 0.00003266
Iteration 23/1000 | Loss: 0.00004292
Iteration 24/1000 | Loss: 0.00003262
Iteration 25/1000 | Loss: 0.00003872
Iteration 26/1000 | Loss: 0.00004857
Iteration 27/1000 | Loss: 0.00003618
Iteration 28/1000 | Loss: 0.00003660
Iteration 29/1000 | Loss: 0.00002266
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001749
Iteration 35/1000 | Loss: 0.00001703
Iteration 36/1000 | Loss: 0.00001662
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00001602
Iteration 41/1000 | Loss: 0.00001601
Iteration 42/1000 | Loss: 0.00001601
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001597
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001596
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001578
Iteration 62/1000 | Loss: 0.00001578
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001578
Iteration 65/1000 | Loss: 0.00001577
Iteration 66/1000 | Loss: 0.00001577
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001569
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001558
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001557
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001557
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001556
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001550
Iteration 120/1000 | Loss: 0.00001550
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001546
Iteration 156/1000 | Loss: 0.00001546
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001546
Iteration 160/1000 | Loss: 0.00001546
Iteration 161/1000 | Loss: 0.00001546
Iteration 162/1000 | Loss: 0.00001546
Iteration 163/1000 | Loss: 0.00001546
Iteration 164/1000 | Loss: 0.00001546
Iteration 165/1000 | Loss: 0.00001546
Iteration 166/1000 | Loss: 0.00001546
Iteration 167/1000 | Loss: 0.00001546
Iteration 168/1000 | Loss: 0.00001546
Iteration 169/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.545970735605806e-05, 1.545970735605806e-05, 1.545970735605806e-05, 1.545970735605806e-05, 1.545970735605806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.545970735605806e-05

Optimization complete. Final v2v error: 3.419711112976074 mm

Highest mean error: 4.77853536605835 mm for frame 62

Lowest mean error: 3.2013983726501465 mm for frame 6

Saving results

Total time: 124.31462955474854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837012
Iteration 2/25 | Loss: 0.00153524
Iteration 3/25 | Loss: 0.00101886
Iteration 4/25 | Loss: 0.00092103
Iteration 5/25 | Loss: 0.00092972
Iteration 6/25 | Loss: 0.00091169
Iteration 7/25 | Loss: 0.00089921
Iteration 8/25 | Loss: 0.00089261
Iteration 9/25 | Loss: 0.00089146
Iteration 10/25 | Loss: 0.00089111
Iteration 11/25 | Loss: 0.00089075
Iteration 12/25 | Loss: 0.00089450
Iteration 13/25 | Loss: 0.00088981
Iteration 14/25 | Loss: 0.00088857
Iteration 15/25 | Loss: 0.00088845
Iteration 16/25 | Loss: 0.00088843
Iteration 17/25 | Loss: 0.00088843
Iteration 18/25 | Loss: 0.00088842
Iteration 19/25 | Loss: 0.00088842
Iteration 20/25 | Loss: 0.00088842
Iteration 21/25 | Loss: 0.00088842
Iteration 22/25 | Loss: 0.00088842
Iteration 23/25 | Loss: 0.00088842
Iteration 24/25 | Loss: 0.00088842
Iteration 25/25 | Loss: 0.00088842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75335574
Iteration 2/25 | Loss: 0.00162548
Iteration 3/25 | Loss: 0.00162547
Iteration 4/25 | Loss: 0.00162547
Iteration 5/25 | Loss: 0.00162547
Iteration 6/25 | Loss: 0.00162546
Iteration 7/25 | Loss: 0.00162546
Iteration 8/25 | Loss: 0.00162546
Iteration 9/25 | Loss: 0.00162546
Iteration 10/25 | Loss: 0.00162546
Iteration 11/25 | Loss: 0.00162546
Iteration 12/25 | Loss: 0.00162546
Iteration 13/25 | Loss: 0.00162546
Iteration 14/25 | Loss: 0.00162546
Iteration 15/25 | Loss: 0.00162546
Iteration 16/25 | Loss: 0.00162546
Iteration 17/25 | Loss: 0.00162546
Iteration 18/25 | Loss: 0.00162546
Iteration 19/25 | Loss: 0.00162546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016254637157544494, 0.0016254637157544494, 0.0016254637157544494, 0.0016254637157544494, 0.0016254637157544494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016254637157544494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162546
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00002254
Iteration 4/1000 | Loss: 0.00002159
Iteration 5/1000 | Loss: 0.00002099
Iteration 6/1000 | Loss: 0.00002045
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00002014
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00002012
Iteration 11/1000 | Loss: 0.00002011
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002006
Iteration 17/1000 | Loss: 0.00002001
Iteration 18/1000 | Loss: 0.00002001
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00002000
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001987
Iteration 26/1000 | Loss: 0.00001986
Iteration 27/1000 | Loss: 0.00001986
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001980
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001977
Iteration 36/1000 | Loss: 0.00001976
Iteration 37/1000 | Loss: 0.00001976
Iteration 38/1000 | Loss: 0.00001976
Iteration 39/1000 | Loss: 0.00001976
Iteration 40/1000 | Loss: 0.00001975
Iteration 41/1000 | Loss: 0.00001975
Iteration 42/1000 | Loss: 0.00001975
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001974
Iteration 45/1000 | Loss: 0.00001974
Iteration 46/1000 | Loss: 0.00001973
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001973
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001971
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001971
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001970
Iteration 63/1000 | Loss: 0.00001970
Iteration 64/1000 | Loss: 0.00001970
Iteration 65/1000 | Loss: 0.00001969
Iteration 66/1000 | Loss: 0.00001969
Iteration 67/1000 | Loss: 0.00001969
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001968
Iteration 72/1000 | Loss: 0.00001968
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001967
Iteration 75/1000 | Loss: 0.00001967
Iteration 76/1000 | Loss: 0.00001967
Iteration 77/1000 | Loss: 0.00001967
Iteration 78/1000 | Loss: 0.00001967
Iteration 79/1000 | Loss: 0.00001967
Iteration 80/1000 | Loss: 0.00001967
Iteration 81/1000 | Loss: 0.00001967
Iteration 82/1000 | Loss: 0.00001967
Iteration 83/1000 | Loss: 0.00001966
Iteration 84/1000 | Loss: 0.00001966
Iteration 85/1000 | Loss: 0.00001966
Iteration 86/1000 | Loss: 0.00001966
Iteration 87/1000 | Loss: 0.00001966
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001965
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00001965
Iteration 104/1000 | Loss: 0.00001965
Iteration 105/1000 | Loss: 0.00001965
Iteration 106/1000 | Loss: 0.00001965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.9648730813059956e-05, 1.9648730813059956e-05, 1.9648730813059956e-05, 1.9648730813059956e-05, 1.9648730813059956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9648730813059956e-05

Optimization complete. Final v2v error: 3.8830316066741943 mm

Highest mean error: 4.221574783325195 mm for frame 55

Lowest mean error: 3.493791103363037 mm for frame 227

Saving results

Total time: 52.7489550113678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457111
Iteration 2/25 | Loss: 0.00101835
Iteration 3/25 | Loss: 0.00090330
Iteration 4/25 | Loss: 0.00086492
Iteration 5/25 | Loss: 0.00085729
Iteration 6/25 | Loss: 0.00085567
Iteration 7/25 | Loss: 0.00085534
Iteration 8/25 | Loss: 0.00085534
Iteration 9/25 | Loss: 0.00085534
Iteration 10/25 | Loss: 0.00085534
Iteration 11/25 | Loss: 0.00085534
Iteration 12/25 | Loss: 0.00085534
Iteration 13/25 | Loss: 0.00085534
Iteration 14/25 | Loss: 0.00085534
Iteration 15/25 | Loss: 0.00085534
Iteration 16/25 | Loss: 0.00085534
Iteration 17/25 | Loss: 0.00085534
Iteration 18/25 | Loss: 0.00085534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008553443476557732, 0.0008553443476557732, 0.0008553443476557732, 0.0008553443476557732, 0.0008553443476557732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008553443476557732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.80469179
Iteration 2/25 | Loss: 0.00174549
Iteration 3/25 | Loss: 0.00174548
Iteration 4/25 | Loss: 0.00174548
Iteration 5/25 | Loss: 0.00174548
Iteration 6/25 | Loss: 0.00174548
Iteration 7/25 | Loss: 0.00174548
Iteration 8/25 | Loss: 0.00174548
Iteration 9/25 | Loss: 0.00174547
Iteration 10/25 | Loss: 0.00174547
Iteration 11/25 | Loss: 0.00174547
Iteration 12/25 | Loss: 0.00174547
Iteration 13/25 | Loss: 0.00174547
Iteration 14/25 | Loss: 0.00174547
Iteration 15/25 | Loss: 0.00174547
Iteration 16/25 | Loss: 0.00174547
Iteration 17/25 | Loss: 0.00174547
Iteration 18/25 | Loss: 0.00174547
Iteration 19/25 | Loss: 0.00174547
Iteration 20/25 | Loss: 0.00174547
Iteration 21/25 | Loss: 0.00174547
Iteration 22/25 | Loss: 0.00174547
Iteration 23/25 | Loss: 0.00174547
Iteration 24/25 | Loss: 0.00174547
Iteration 25/25 | Loss: 0.00174547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174547
Iteration 2/1000 | Loss: 0.00003350
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002074
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001903
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001788
Iteration 11/1000 | Loss: 0.00001783
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001775
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001774
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001772
Iteration 23/1000 | Loss: 0.00001772
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001771
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001770
Iteration 28/1000 | Loss: 0.00001770
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001768
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001767
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001765
Iteration 46/1000 | Loss: 0.00001765
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001764
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001764
Iteration 53/1000 | Loss: 0.00001764
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001764
Iteration 56/1000 | Loss: 0.00001763
Iteration 57/1000 | Loss: 0.00001763
Iteration 58/1000 | Loss: 0.00001763
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001763
Iteration 61/1000 | Loss: 0.00001763
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001760
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001760
Iteration 88/1000 | Loss: 0.00001760
Iteration 89/1000 | Loss: 0.00001760
Iteration 90/1000 | Loss: 0.00001759
Iteration 91/1000 | Loss: 0.00001759
Iteration 92/1000 | Loss: 0.00001759
Iteration 93/1000 | Loss: 0.00001759
Iteration 94/1000 | Loss: 0.00001759
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001759
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001759
Iteration 103/1000 | Loss: 0.00001759
Iteration 104/1000 | Loss: 0.00001758
Iteration 105/1000 | Loss: 0.00001758
Iteration 106/1000 | Loss: 0.00001758
Iteration 107/1000 | Loss: 0.00001758
Iteration 108/1000 | Loss: 0.00001758
Iteration 109/1000 | Loss: 0.00001758
Iteration 110/1000 | Loss: 0.00001758
Iteration 111/1000 | Loss: 0.00001758
Iteration 112/1000 | Loss: 0.00001758
Iteration 113/1000 | Loss: 0.00001758
Iteration 114/1000 | Loss: 0.00001758
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001757
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001757
Iteration 125/1000 | Loss: 0.00001757
Iteration 126/1000 | Loss: 0.00001757
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001756
Iteration 129/1000 | Loss: 0.00001756
Iteration 130/1000 | Loss: 0.00001756
Iteration 131/1000 | Loss: 0.00001756
Iteration 132/1000 | Loss: 0.00001756
Iteration 133/1000 | Loss: 0.00001756
Iteration 134/1000 | Loss: 0.00001756
Iteration 135/1000 | Loss: 0.00001756
Iteration 136/1000 | Loss: 0.00001756
Iteration 137/1000 | Loss: 0.00001756
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001755
Iteration 140/1000 | Loss: 0.00001755
Iteration 141/1000 | Loss: 0.00001755
Iteration 142/1000 | Loss: 0.00001755
Iteration 143/1000 | Loss: 0.00001755
Iteration 144/1000 | Loss: 0.00001755
Iteration 145/1000 | Loss: 0.00001755
Iteration 146/1000 | Loss: 0.00001754
Iteration 147/1000 | Loss: 0.00001754
Iteration 148/1000 | Loss: 0.00001754
Iteration 149/1000 | Loss: 0.00001754
Iteration 150/1000 | Loss: 0.00001754
Iteration 151/1000 | Loss: 0.00001754
Iteration 152/1000 | Loss: 0.00001754
Iteration 153/1000 | Loss: 0.00001753
Iteration 154/1000 | Loss: 0.00001753
Iteration 155/1000 | Loss: 0.00001753
Iteration 156/1000 | Loss: 0.00001753
Iteration 157/1000 | Loss: 0.00001753
Iteration 158/1000 | Loss: 0.00001753
Iteration 159/1000 | Loss: 0.00001753
Iteration 160/1000 | Loss: 0.00001753
Iteration 161/1000 | Loss: 0.00001753
Iteration 162/1000 | Loss: 0.00001753
Iteration 163/1000 | Loss: 0.00001753
Iteration 164/1000 | Loss: 0.00001753
Iteration 165/1000 | Loss: 0.00001753
Iteration 166/1000 | Loss: 0.00001753
Iteration 167/1000 | Loss: 0.00001753
Iteration 168/1000 | Loss: 0.00001752
Iteration 169/1000 | Loss: 0.00001752
Iteration 170/1000 | Loss: 0.00001752
Iteration 171/1000 | Loss: 0.00001752
Iteration 172/1000 | Loss: 0.00001752
Iteration 173/1000 | Loss: 0.00001752
Iteration 174/1000 | Loss: 0.00001752
Iteration 175/1000 | Loss: 0.00001752
Iteration 176/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.752393291098997e-05, 1.752393291098997e-05, 1.752393291098997e-05, 1.752393291098997e-05, 1.752393291098997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.752393291098997e-05

Optimization complete. Final v2v error: 3.591843605041504 mm

Highest mean error: 4.010687351226807 mm for frame 102

Lowest mean error: 3.1619417667388916 mm for frame 40

Saving results

Total time: 38.981226444244385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929004
Iteration 2/25 | Loss: 0.00104525
Iteration 3/25 | Loss: 0.00092486
Iteration 4/25 | Loss: 0.00089330
Iteration 5/25 | Loss: 0.00088843
Iteration 6/25 | Loss: 0.00088760
Iteration 7/25 | Loss: 0.00088760
Iteration 8/25 | Loss: 0.00088760
Iteration 9/25 | Loss: 0.00088760
Iteration 10/25 | Loss: 0.00088760
Iteration 11/25 | Loss: 0.00088760
Iteration 12/25 | Loss: 0.00088760
Iteration 13/25 | Loss: 0.00088760
Iteration 14/25 | Loss: 0.00088760
Iteration 15/25 | Loss: 0.00088760
Iteration 16/25 | Loss: 0.00088760
Iteration 17/25 | Loss: 0.00088760
Iteration 18/25 | Loss: 0.00088760
Iteration 19/25 | Loss: 0.00088760
Iteration 20/25 | Loss: 0.00088760
Iteration 21/25 | Loss: 0.00088760
Iteration 22/25 | Loss: 0.00088760
Iteration 23/25 | Loss: 0.00088760
Iteration 24/25 | Loss: 0.00088760
Iteration 25/25 | Loss: 0.00088760

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63152719
Iteration 2/25 | Loss: 0.00171887
Iteration 3/25 | Loss: 0.00171885
Iteration 4/25 | Loss: 0.00171885
Iteration 5/25 | Loss: 0.00171885
Iteration 6/25 | Loss: 0.00171885
Iteration 7/25 | Loss: 0.00171885
Iteration 8/25 | Loss: 0.00171885
Iteration 9/25 | Loss: 0.00171885
Iteration 10/25 | Loss: 0.00171885
Iteration 11/25 | Loss: 0.00171885
Iteration 12/25 | Loss: 0.00171885
Iteration 13/25 | Loss: 0.00171885
Iteration 14/25 | Loss: 0.00171885
Iteration 15/25 | Loss: 0.00171885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017188478959724307, 0.0017188478959724307, 0.0017188478959724307, 0.0017188478959724307, 0.0017188478959724307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017188478959724307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171885
Iteration 2/1000 | Loss: 0.00003578
Iteration 3/1000 | Loss: 0.00002476
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002177
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001960
Iteration 11/1000 | Loss: 0.00001958
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001956
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001953
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001951
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001950
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001950
Iteration 29/1000 | Loss: 0.00001950
Iteration 30/1000 | Loss: 0.00001950
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001950
Iteration 36/1000 | Loss: 0.00001949
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001949
Iteration 41/1000 | Loss: 0.00001949
Iteration 42/1000 | Loss: 0.00001949
Iteration 43/1000 | Loss: 0.00001949
Iteration 44/1000 | Loss: 0.00001949
Iteration 45/1000 | Loss: 0.00001949
Iteration 46/1000 | Loss: 0.00001949
Iteration 47/1000 | Loss: 0.00001949
Iteration 48/1000 | Loss: 0.00001949
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001948
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001946
Iteration 85/1000 | Loss: 0.00001946
Iteration 86/1000 | Loss: 0.00001946
Iteration 87/1000 | Loss: 0.00001946
Iteration 88/1000 | Loss: 0.00001946
Iteration 89/1000 | Loss: 0.00001946
Iteration 90/1000 | Loss: 0.00001946
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001946
Iteration 93/1000 | Loss: 0.00001946
Iteration 94/1000 | Loss: 0.00001946
Iteration 95/1000 | Loss: 0.00001946
Iteration 96/1000 | Loss: 0.00001946
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001946
Iteration 103/1000 | Loss: 0.00001946
Iteration 104/1000 | Loss: 0.00001946
Iteration 105/1000 | Loss: 0.00001946
Iteration 106/1000 | Loss: 0.00001946
Iteration 107/1000 | Loss: 0.00001946
Iteration 108/1000 | Loss: 0.00001946
Iteration 109/1000 | Loss: 0.00001946
Iteration 110/1000 | Loss: 0.00001946
Iteration 111/1000 | Loss: 0.00001946
Iteration 112/1000 | Loss: 0.00001946
Iteration 113/1000 | Loss: 0.00001946
Iteration 114/1000 | Loss: 0.00001946
Iteration 115/1000 | Loss: 0.00001946
Iteration 116/1000 | Loss: 0.00001946
Iteration 117/1000 | Loss: 0.00001946
Iteration 118/1000 | Loss: 0.00001946
Iteration 119/1000 | Loss: 0.00001946
Iteration 120/1000 | Loss: 0.00001946
Iteration 121/1000 | Loss: 0.00001946
Iteration 122/1000 | Loss: 0.00001946
Iteration 123/1000 | Loss: 0.00001946
Iteration 124/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.9456811060081236e-05, 1.9456811060081236e-05, 1.9456811060081236e-05, 1.9456811060081236e-05, 1.9456811060081236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9456811060081236e-05

Optimization complete. Final v2v error: 3.805384397506714 mm

Highest mean error: 4.170472621917725 mm for frame 113

Lowest mean error: 3.558884620666504 mm for frame 207

Saving results

Total time: 32.726720571517944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535098
Iteration 2/25 | Loss: 0.00104220
Iteration 3/25 | Loss: 0.00088342
Iteration 4/25 | Loss: 0.00086727
Iteration 5/25 | Loss: 0.00086277
Iteration 6/25 | Loss: 0.00086202
Iteration 7/25 | Loss: 0.00086202
Iteration 8/25 | Loss: 0.00086202
Iteration 9/25 | Loss: 0.00086202
Iteration 10/25 | Loss: 0.00086202
Iteration 11/25 | Loss: 0.00086202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008620154112577438, 0.0008620154112577438, 0.0008620154112577438, 0.0008620154112577438, 0.0008620154112577438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008620154112577438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58808446
Iteration 2/25 | Loss: 0.00160712
Iteration 3/25 | Loss: 0.00160710
Iteration 4/25 | Loss: 0.00160710
Iteration 5/25 | Loss: 0.00160709
Iteration 6/25 | Loss: 0.00160709
Iteration 7/25 | Loss: 0.00160709
Iteration 8/25 | Loss: 0.00160709
Iteration 9/25 | Loss: 0.00160709
Iteration 10/25 | Loss: 0.00160709
Iteration 11/25 | Loss: 0.00160709
Iteration 12/25 | Loss: 0.00160709
Iteration 13/25 | Loss: 0.00160709
Iteration 14/25 | Loss: 0.00160709
Iteration 15/25 | Loss: 0.00160709
Iteration 16/25 | Loss: 0.00160709
Iteration 17/25 | Loss: 0.00160709
Iteration 18/25 | Loss: 0.00160709
Iteration 19/25 | Loss: 0.00160709
Iteration 20/25 | Loss: 0.00160709
Iteration 21/25 | Loss: 0.00160709
Iteration 22/25 | Loss: 0.00160709
Iteration 23/25 | Loss: 0.00160709
Iteration 24/25 | Loss: 0.00160709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016070930287241936, 0.0016070930287241936, 0.0016070930287241936, 0.0016070930287241936, 0.0016070930287241936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016070930287241936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160709
Iteration 2/1000 | Loss: 0.00002798
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00001866
Iteration 6/1000 | Loss: 0.00001773
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001684
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001623
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00001615
Iteration 32/1000 | Loss: 0.00001615
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001614
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001612
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001607
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001606
Iteration 87/1000 | Loss: 0.00001606
Iteration 88/1000 | Loss: 0.00001606
Iteration 89/1000 | Loss: 0.00001606
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.605563193152193e-05, 1.605563193152193e-05, 1.605563193152193e-05, 1.605563193152193e-05, 1.605563193152193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.605563193152193e-05

Optimization complete. Final v2v error: 3.4852070808410645 mm

Highest mean error: 3.750311851501465 mm for frame 102

Lowest mean error: 2.9999680519104004 mm for frame 3

Saving results

Total time: 35.11306548118591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437446
Iteration 2/25 | Loss: 0.00100483
Iteration 3/25 | Loss: 0.00091032
Iteration 4/25 | Loss: 0.00088946
Iteration 5/25 | Loss: 0.00088214
Iteration 6/25 | Loss: 0.00088074
Iteration 7/25 | Loss: 0.00088074
Iteration 8/25 | Loss: 0.00088074
Iteration 9/25 | Loss: 0.00088074
Iteration 10/25 | Loss: 0.00088074
Iteration 11/25 | Loss: 0.00088074
Iteration 12/25 | Loss: 0.00088074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008807406411506236, 0.0008807406411506236, 0.0008807406411506236, 0.0008807406411506236, 0.0008807406411506236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008807406411506236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38080788
Iteration 2/25 | Loss: 0.00180147
Iteration 3/25 | Loss: 0.00180146
Iteration 4/25 | Loss: 0.00180146
Iteration 5/25 | Loss: 0.00180146
Iteration 6/25 | Loss: 0.00180146
Iteration 7/25 | Loss: 0.00180146
Iteration 8/25 | Loss: 0.00180146
Iteration 9/25 | Loss: 0.00180146
Iteration 10/25 | Loss: 0.00180146
Iteration 11/25 | Loss: 0.00180146
Iteration 12/25 | Loss: 0.00180146
Iteration 13/25 | Loss: 0.00180146
Iteration 14/25 | Loss: 0.00180146
Iteration 15/25 | Loss: 0.00180146
Iteration 16/25 | Loss: 0.00180146
Iteration 17/25 | Loss: 0.00180146
Iteration 18/25 | Loss: 0.00180146
Iteration 19/25 | Loss: 0.00180146
Iteration 20/25 | Loss: 0.00180146
Iteration 21/25 | Loss: 0.00180146
Iteration 22/25 | Loss: 0.00180146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018014597008004785, 0.0018014597008004785, 0.0018014597008004785, 0.0018014597008004785, 0.0018014597008004785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018014597008004785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180146
Iteration 2/1000 | Loss: 0.00003029
Iteration 3/1000 | Loss: 0.00002587
Iteration 4/1000 | Loss: 0.00002459
Iteration 5/1000 | Loss: 0.00002352
Iteration 6/1000 | Loss: 0.00002287
Iteration 7/1000 | Loss: 0.00002252
Iteration 8/1000 | Loss: 0.00002238
Iteration 9/1000 | Loss: 0.00002238
Iteration 10/1000 | Loss: 0.00002235
Iteration 11/1000 | Loss: 0.00002233
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002232
Iteration 15/1000 | Loss: 0.00002231
Iteration 16/1000 | Loss: 0.00002231
Iteration 17/1000 | Loss: 0.00002231
Iteration 18/1000 | Loss: 0.00002230
Iteration 19/1000 | Loss: 0.00002230
Iteration 20/1000 | Loss: 0.00002229
Iteration 21/1000 | Loss: 0.00002229
Iteration 22/1000 | Loss: 0.00002228
Iteration 23/1000 | Loss: 0.00002228
Iteration 24/1000 | Loss: 0.00002227
Iteration 25/1000 | Loss: 0.00002227
Iteration 26/1000 | Loss: 0.00002226
Iteration 27/1000 | Loss: 0.00002226
Iteration 28/1000 | Loss: 0.00002226
Iteration 29/1000 | Loss: 0.00002226
Iteration 30/1000 | Loss: 0.00002225
Iteration 31/1000 | Loss: 0.00002225
Iteration 32/1000 | Loss: 0.00002223
Iteration 33/1000 | Loss: 0.00002223
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002221
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002218
Iteration 41/1000 | Loss: 0.00002218
Iteration 42/1000 | Loss: 0.00002218
Iteration 43/1000 | Loss: 0.00002218
Iteration 44/1000 | Loss: 0.00002218
Iteration 45/1000 | Loss: 0.00002217
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002217
Iteration 48/1000 | Loss: 0.00002217
Iteration 49/1000 | Loss: 0.00002216
Iteration 50/1000 | Loss: 0.00002216
Iteration 51/1000 | Loss: 0.00002216
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00002215
Iteration 54/1000 | Loss: 0.00002215
Iteration 55/1000 | Loss: 0.00002215
Iteration 56/1000 | Loss: 0.00002215
Iteration 57/1000 | Loss: 0.00002215
Iteration 58/1000 | Loss: 0.00002215
Iteration 59/1000 | Loss: 0.00002215
Iteration 60/1000 | Loss: 0.00002215
Iteration 61/1000 | Loss: 0.00002215
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002214
Iteration 64/1000 | Loss: 0.00002214
Iteration 65/1000 | Loss: 0.00002214
Iteration 66/1000 | Loss: 0.00002214
Iteration 67/1000 | Loss: 0.00002214
Iteration 68/1000 | Loss: 0.00002214
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002214
Iteration 71/1000 | Loss: 0.00002214
Iteration 72/1000 | Loss: 0.00002214
Iteration 73/1000 | Loss: 0.00002214
Iteration 74/1000 | Loss: 0.00002213
Iteration 75/1000 | Loss: 0.00002213
Iteration 76/1000 | Loss: 0.00002213
Iteration 77/1000 | Loss: 0.00002213
Iteration 78/1000 | Loss: 0.00002213
Iteration 79/1000 | Loss: 0.00002213
Iteration 80/1000 | Loss: 0.00002213
Iteration 81/1000 | Loss: 0.00002213
Iteration 82/1000 | Loss: 0.00002213
Iteration 83/1000 | Loss: 0.00002213
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.212904473708477e-05, 2.212904473708477e-05, 2.212904473708477e-05, 2.212904473708477e-05, 2.212904473708477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.212904473708477e-05

Optimization complete. Final v2v error: 3.9988608360290527 mm

Highest mean error: 4.2412519454956055 mm for frame 121

Lowest mean error: 3.732109546661377 mm for frame 100

Saving results

Total time: 28.769843101501465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069433
Iteration 2/25 | Loss: 0.00165340
Iteration 3/25 | Loss: 0.00111500
Iteration 4/25 | Loss: 0.00101948
Iteration 5/25 | Loss: 0.00099520
Iteration 6/25 | Loss: 0.00099135
Iteration 7/25 | Loss: 0.00099125
Iteration 8/25 | Loss: 0.00099125
Iteration 9/25 | Loss: 0.00099125
Iteration 10/25 | Loss: 0.00099125
Iteration 11/25 | Loss: 0.00099125
Iteration 12/25 | Loss: 0.00099125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009912470122799277, 0.0009912470122799277, 0.0009912470122799277, 0.0009912470122799277, 0.0009912470122799277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009912470122799277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.24966478
Iteration 2/25 | Loss: 0.00204409
Iteration 3/25 | Loss: 0.00204409
Iteration 4/25 | Loss: 0.00204409
Iteration 5/25 | Loss: 0.00204409
Iteration 6/25 | Loss: 0.00204408
Iteration 7/25 | Loss: 0.00204408
Iteration 8/25 | Loss: 0.00204408
Iteration 9/25 | Loss: 0.00204408
Iteration 10/25 | Loss: 0.00204408
Iteration 11/25 | Loss: 0.00204408
Iteration 12/25 | Loss: 0.00204408
Iteration 13/25 | Loss: 0.00204408
Iteration 14/25 | Loss: 0.00204408
Iteration 15/25 | Loss: 0.00204408
Iteration 16/25 | Loss: 0.00204408
Iteration 17/25 | Loss: 0.00204408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020440835505723953, 0.0020440835505723953, 0.0020440835505723953, 0.0020440835505723953, 0.0020440835505723953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020440835505723953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204408
Iteration 2/1000 | Loss: 0.00002962
Iteration 3/1000 | Loss: 0.00002441
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002208
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002070
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00002014
Iteration 12/1000 | Loss: 0.00002012
Iteration 13/1000 | Loss: 0.00002012
Iteration 14/1000 | Loss: 0.00002011
Iteration 15/1000 | Loss: 0.00002011
Iteration 16/1000 | Loss: 0.00002010
Iteration 17/1000 | Loss: 0.00002004
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002002
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002002
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002002
Iteration 26/1000 | Loss: 0.00002002
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00002001
Iteration 29/1000 | Loss: 0.00002001
Iteration 30/1000 | Loss: 0.00002001
Iteration 31/1000 | Loss: 0.00002000
Iteration 32/1000 | Loss: 0.00002000
Iteration 33/1000 | Loss: 0.00001999
Iteration 34/1000 | Loss: 0.00001999
Iteration 35/1000 | Loss: 0.00001999
Iteration 36/1000 | Loss: 0.00001999
Iteration 37/1000 | Loss: 0.00001999
Iteration 38/1000 | Loss: 0.00001998
Iteration 39/1000 | Loss: 0.00001998
Iteration 40/1000 | Loss: 0.00001998
Iteration 41/1000 | Loss: 0.00001998
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001998
Iteration 45/1000 | Loss: 0.00001998
Iteration 46/1000 | Loss: 0.00001998
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001998
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001998
Iteration 52/1000 | Loss: 0.00001998
Iteration 53/1000 | Loss: 0.00001998
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001997
Iteration 56/1000 | Loss: 0.00001997
Iteration 57/1000 | Loss: 0.00001997
Iteration 58/1000 | Loss: 0.00001997
Iteration 59/1000 | Loss: 0.00001997
Iteration 60/1000 | Loss: 0.00001997
Iteration 61/1000 | Loss: 0.00001997
Iteration 62/1000 | Loss: 0.00001996
Iteration 63/1000 | Loss: 0.00001996
Iteration 64/1000 | Loss: 0.00001996
Iteration 65/1000 | Loss: 0.00001996
Iteration 66/1000 | Loss: 0.00001996
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001996
Iteration 70/1000 | Loss: 0.00001996
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001995
Iteration 73/1000 | Loss: 0.00001995
Iteration 74/1000 | Loss: 0.00001995
Iteration 75/1000 | Loss: 0.00001995
Iteration 76/1000 | Loss: 0.00001995
Iteration 77/1000 | Loss: 0.00001995
Iteration 78/1000 | Loss: 0.00001995
Iteration 79/1000 | Loss: 0.00001995
Iteration 80/1000 | Loss: 0.00001995
Iteration 81/1000 | Loss: 0.00001995
Iteration 82/1000 | Loss: 0.00001995
Iteration 83/1000 | Loss: 0.00001995
Iteration 84/1000 | Loss: 0.00001994
Iteration 85/1000 | Loss: 0.00001994
Iteration 86/1000 | Loss: 0.00001994
Iteration 87/1000 | Loss: 0.00001994
Iteration 88/1000 | Loss: 0.00001994
Iteration 89/1000 | Loss: 0.00001994
Iteration 90/1000 | Loss: 0.00001994
Iteration 91/1000 | Loss: 0.00001994
Iteration 92/1000 | Loss: 0.00001994
Iteration 93/1000 | Loss: 0.00001993
Iteration 94/1000 | Loss: 0.00001993
Iteration 95/1000 | Loss: 0.00001993
Iteration 96/1000 | Loss: 0.00001993
Iteration 97/1000 | Loss: 0.00001993
Iteration 98/1000 | Loss: 0.00001993
Iteration 99/1000 | Loss: 0.00001993
Iteration 100/1000 | Loss: 0.00001993
Iteration 101/1000 | Loss: 0.00001993
Iteration 102/1000 | Loss: 0.00001993
Iteration 103/1000 | Loss: 0.00001993
Iteration 104/1000 | Loss: 0.00001993
Iteration 105/1000 | Loss: 0.00001993
Iteration 106/1000 | Loss: 0.00001993
Iteration 107/1000 | Loss: 0.00001993
Iteration 108/1000 | Loss: 0.00001993
Iteration 109/1000 | Loss: 0.00001993
Iteration 110/1000 | Loss: 0.00001993
Iteration 111/1000 | Loss: 0.00001993
Iteration 112/1000 | Loss: 0.00001993
Iteration 113/1000 | Loss: 0.00001993
Iteration 114/1000 | Loss: 0.00001993
Iteration 115/1000 | Loss: 0.00001993
Iteration 116/1000 | Loss: 0.00001993
Iteration 117/1000 | Loss: 0.00001993
Iteration 118/1000 | Loss: 0.00001993
Iteration 119/1000 | Loss: 0.00001993
Iteration 120/1000 | Loss: 0.00001993
Iteration 121/1000 | Loss: 0.00001993
Iteration 122/1000 | Loss: 0.00001993
Iteration 123/1000 | Loss: 0.00001993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.9929068002966233e-05, 1.9929068002966233e-05, 1.9929068002966233e-05, 1.9929068002966233e-05, 1.9929068002966233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9929068002966233e-05

Optimization complete. Final v2v error: 3.8963606357574463 mm

Highest mean error: 4.156582355499268 mm for frame 115

Lowest mean error: 3.623964548110962 mm for frame 150

Saving results

Total time: 30.45452642440796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698342
Iteration 2/25 | Loss: 0.00140600
Iteration 3/25 | Loss: 0.00099607
Iteration 4/25 | Loss: 0.00092748
Iteration 5/25 | Loss: 0.00090558
Iteration 6/25 | Loss: 0.00089955
Iteration 7/25 | Loss: 0.00089702
Iteration 8/25 | Loss: 0.00089420
Iteration 9/25 | Loss: 0.00089359
Iteration 10/25 | Loss: 0.00089335
Iteration 11/25 | Loss: 0.00089316
Iteration 12/25 | Loss: 0.00089312
Iteration 13/25 | Loss: 0.00089311
Iteration 14/25 | Loss: 0.00089311
Iteration 15/25 | Loss: 0.00089311
Iteration 16/25 | Loss: 0.00089311
Iteration 17/25 | Loss: 0.00089311
Iteration 18/25 | Loss: 0.00089311
Iteration 19/25 | Loss: 0.00089311
Iteration 20/25 | Loss: 0.00089311
Iteration 21/25 | Loss: 0.00089311
Iteration 22/25 | Loss: 0.00089311
Iteration 23/25 | Loss: 0.00089311
Iteration 24/25 | Loss: 0.00089311
Iteration 25/25 | Loss: 0.00089311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13199353
Iteration 2/25 | Loss: 0.00231248
Iteration 3/25 | Loss: 0.00231248
Iteration 4/25 | Loss: 0.00231248
Iteration 5/25 | Loss: 0.00231248
Iteration 6/25 | Loss: 0.00231248
Iteration 7/25 | Loss: 0.00231247
Iteration 8/25 | Loss: 0.00231247
Iteration 9/25 | Loss: 0.00231247
Iteration 10/25 | Loss: 0.00231247
Iteration 11/25 | Loss: 0.00231247
Iteration 12/25 | Loss: 0.00231247
Iteration 13/25 | Loss: 0.00231247
Iteration 14/25 | Loss: 0.00231247
Iteration 15/25 | Loss: 0.00231247
Iteration 16/25 | Loss: 0.00231247
Iteration 17/25 | Loss: 0.00231247
Iteration 18/25 | Loss: 0.00231247
Iteration 19/25 | Loss: 0.00231247
Iteration 20/25 | Loss: 0.00231247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023124737199395895, 0.0023124737199395895, 0.0023124737199395895, 0.0023124737199395895, 0.0023124737199395895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023124737199395895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231247
Iteration 2/1000 | Loss: 0.00006927
Iteration 3/1000 | Loss: 0.00004743
Iteration 4/1000 | Loss: 0.00015477
Iteration 5/1000 | Loss: 0.00003757
Iteration 6/1000 | Loss: 0.00003280
Iteration 7/1000 | Loss: 0.00043937
Iteration 8/1000 | Loss: 0.00013957
Iteration 9/1000 | Loss: 0.00005889
Iteration 10/1000 | Loss: 0.00037792
Iteration 11/1000 | Loss: 0.00008466
Iteration 12/1000 | Loss: 0.00039999
Iteration 13/1000 | Loss: 0.00009428
Iteration 14/1000 | Loss: 0.00036300
Iteration 15/1000 | Loss: 0.00004542
Iteration 16/1000 | Loss: 0.00003056
Iteration 17/1000 | Loss: 0.00002581
Iteration 18/1000 | Loss: 0.00002386
Iteration 19/1000 | Loss: 0.00062876
Iteration 20/1000 | Loss: 0.00003458
Iteration 21/1000 | Loss: 0.00002477
Iteration 22/1000 | Loss: 0.00002108
Iteration 23/1000 | Loss: 0.00002012
Iteration 24/1000 | Loss: 0.00001948
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001879
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001832
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001829
Iteration 34/1000 | Loss: 0.00001829
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001807
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001799
Iteration 48/1000 | Loss: 0.00001799
Iteration 49/1000 | Loss: 0.00001799
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001799
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001798
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00001797
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001797
Iteration 61/1000 | Loss: 0.00001797
Iteration 62/1000 | Loss: 0.00001797
Iteration 63/1000 | Loss: 0.00001796
Iteration 64/1000 | Loss: 0.00001796
Iteration 65/1000 | Loss: 0.00001796
Iteration 66/1000 | Loss: 0.00001796
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001795
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001794
Iteration 74/1000 | Loss: 0.00001794
Iteration 75/1000 | Loss: 0.00001793
Iteration 76/1000 | Loss: 0.00001793
Iteration 77/1000 | Loss: 0.00001793
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001792
Iteration 80/1000 | Loss: 0.00001792
Iteration 81/1000 | Loss: 0.00001791
Iteration 82/1000 | Loss: 0.00001791
Iteration 83/1000 | Loss: 0.00001791
Iteration 84/1000 | Loss: 0.00001790
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001790
Iteration 87/1000 | Loss: 0.00001790
Iteration 88/1000 | Loss: 0.00001789
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001788
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001788
Iteration 94/1000 | Loss: 0.00001787
Iteration 95/1000 | Loss: 0.00001787
Iteration 96/1000 | Loss: 0.00001787
Iteration 97/1000 | Loss: 0.00001787
Iteration 98/1000 | Loss: 0.00001787
Iteration 99/1000 | Loss: 0.00001787
Iteration 100/1000 | Loss: 0.00001786
Iteration 101/1000 | Loss: 0.00001786
Iteration 102/1000 | Loss: 0.00001786
Iteration 103/1000 | Loss: 0.00001786
Iteration 104/1000 | Loss: 0.00001786
Iteration 105/1000 | Loss: 0.00001786
Iteration 106/1000 | Loss: 0.00001786
Iteration 107/1000 | Loss: 0.00001786
Iteration 108/1000 | Loss: 0.00001786
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001786
Iteration 111/1000 | Loss: 0.00001786
Iteration 112/1000 | Loss: 0.00001786
Iteration 113/1000 | Loss: 0.00001786
Iteration 114/1000 | Loss: 0.00001786
Iteration 115/1000 | Loss: 0.00001786
Iteration 116/1000 | Loss: 0.00001786
Iteration 117/1000 | Loss: 0.00001786
Iteration 118/1000 | Loss: 0.00001786
Iteration 119/1000 | Loss: 0.00001786
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.7855900296126492e-05, 1.7855900296126492e-05, 1.7855900296126492e-05, 1.7855900296126492e-05, 1.7855900296126492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7855900296126492e-05

Optimization complete. Final v2v error: 3.5781900882720947 mm

Highest mean error: 4.538991451263428 mm for frame 176

Lowest mean error: 2.9580047130584717 mm for frame 142

Saving results

Total time: 79.38390326499939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839177
Iteration 2/25 | Loss: 0.00150469
Iteration 3/25 | Loss: 0.00113364
Iteration 4/25 | Loss: 0.00105439
Iteration 5/25 | Loss: 0.00103739
Iteration 6/25 | Loss: 0.00103248
Iteration 7/25 | Loss: 0.00103087
Iteration 8/25 | Loss: 0.00103073
Iteration 9/25 | Loss: 0.00103073
Iteration 10/25 | Loss: 0.00103073
Iteration 11/25 | Loss: 0.00103073
Iteration 12/25 | Loss: 0.00103073
Iteration 13/25 | Loss: 0.00103073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001030728337354958, 0.001030728337354958, 0.001030728337354958, 0.001030728337354958, 0.001030728337354958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001030728337354958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70290112
Iteration 2/25 | Loss: 0.00289269
Iteration 3/25 | Loss: 0.00289269
Iteration 4/25 | Loss: 0.00289269
Iteration 5/25 | Loss: 0.00289269
Iteration 6/25 | Loss: 0.00289269
Iteration 7/25 | Loss: 0.00289269
Iteration 8/25 | Loss: 0.00289269
Iteration 9/25 | Loss: 0.00289269
Iteration 10/25 | Loss: 0.00289269
Iteration 11/25 | Loss: 0.00289269
Iteration 12/25 | Loss: 0.00289269
Iteration 13/25 | Loss: 0.00289269
Iteration 14/25 | Loss: 0.00289269
Iteration 15/25 | Loss: 0.00289269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0028926865197718143, 0.0028926865197718143, 0.0028926865197718143, 0.0028926865197718143, 0.0028926865197718143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028926865197718143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289269
Iteration 2/1000 | Loss: 0.00004322
Iteration 3/1000 | Loss: 0.00003381
Iteration 4/1000 | Loss: 0.00002823
Iteration 5/1000 | Loss: 0.00002666
Iteration 6/1000 | Loss: 0.00002537
Iteration 7/1000 | Loss: 0.00002407
Iteration 8/1000 | Loss: 0.00002339
Iteration 9/1000 | Loss: 0.00002271
Iteration 10/1000 | Loss: 0.00002228
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002179
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002155
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002129
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002124
Iteration 19/1000 | Loss: 0.00002123
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002121
Iteration 23/1000 | Loss: 0.00002120
Iteration 24/1000 | Loss: 0.00002116
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00002116
Iteration 27/1000 | Loss: 0.00002113
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002112
Iteration 30/1000 | Loss: 0.00002112
Iteration 31/1000 | Loss: 0.00002112
Iteration 32/1000 | Loss: 0.00002111
Iteration 33/1000 | Loss: 0.00002111
Iteration 34/1000 | Loss: 0.00002110
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002110
Iteration 37/1000 | Loss: 0.00002109
Iteration 38/1000 | Loss: 0.00002109
Iteration 39/1000 | Loss: 0.00002108
Iteration 40/1000 | Loss: 0.00002108
Iteration 41/1000 | Loss: 0.00002108
Iteration 42/1000 | Loss: 0.00002107
Iteration 43/1000 | Loss: 0.00002107
Iteration 44/1000 | Loss: 0.00002107
Iteration 45/1000 | Loss: 0.00002107
Iteration 46/1000 | Loss: 0.00002106
Iteration 47/1000 | Loss: 0.00002106
Iteration 48/1000 | Loss: 0.00002106
Iteration 49/1000 | Loss: 0.00002106
Iteration 50/1000 | Loss: 0.00002105
Iteration 51/1000 | Loss: 0.00002105
Iteration 52/1000 | Loss: 0.00002105
Iteration 53/1000 | Loss: 0.00002104
Iteration 54/1000 | Loss: 0.00002104
Iteration 55/1000 | Loss: 0.00002104
Iteration 56/1000 | Loss: 0.00002104
Iteration 57/1000 | Loss: 0.00002103
Iteration 58/1000 | Loss: 0.00002103
Iteration 59/1000 | Loss: 0.00002103
Iteration 60/1000 | Loss: 0.00002103
Iteration 61/1000 | Loss: 0.00002102
Iteration 62/1000 | Loss: 0.00002102
Iteration 63/1000 | Loss: 0.00002102
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002102
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002102
Iteration 68/1000 | Loss: 0.00002102
Iteration 69/1000 | Loss: 0.00002102
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00002101
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002101
Iteration 75/1000 | Loss: 0.00002100
Iteration 76/1000 | Loss: 0.00002100
Iteration 77/1000 | Loss: 0.00002100
Iteration 78/1000 | Loss: 0.00002100
Iteration 79/1000 | Loss: 0.00002100
Iteration 80/1000 | Loss: 0.00002099
Iteration 81/1000 | Loss: 0.00002099
Iteration 82/1000 | Loss: 0.00002099
Iteration 83/1000 | Loss: 0.00002099
Iteration 84/1000 | Loss: 0.00002099
Iteration 85/1000 | Loss: 0.00002099
Iteration 86/1000 | Loss: 0.00002099
Iteration 87/1000 | Loss: 0.00002099
Iteration 88/1000 | Loss: 0.00002099
Iteration 89/1000 | Loss: 0.00002099
Iteration 90/1000 | Loss: 0.00002099
Iteration 91/1000 | Loss: 0.00002099
Iteration 92/1000 | Loss: 0.00002099
Iteration 93/1000 | Loss: 0.00002099
Iteration 94/1000 | Loss: 0.00002098
Iteration 95/1000 | Loss: 0.00002098
Iteration 96/1000 | Loss: 0.00002098
Iteration 97/1000 | Loss: 0.00002098
Iteration 98/1000 | Loss: 0.00002098
Iteration 99/1000 | Loss: 0.00002098
Iteration 100/1000 | Loss: 0.00002098
Iteration 101/1000 | Loss: 0.00002098
Iteration 102/1000 | Loss: 0.00002098
Iteration 103/1000 | Loss: 0.00002098
Iteration 104/1000 | Loss: 0.00002098
Iteration 105/1000 | Loss: 0.00002098
Iteration 106/1000 | Loss: 0.00002098
Iteration 107/1000 | Loss: 0.00002097
Iteration 108/1000 | Loss: 0.00002097
Iteration 109/1000 | Loss: 0.00002097
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002097
Iteration 112/1000 | Loss: 0.00002097
Iteration 113/1000 | Loss: 0.00002097
Iteration 114/1000 | Loss: 0.00002097
Iteration 115/1000 | Loss: 0.00002097
Iteration 116/1000 | Loss: 0.00002097
Iteration 117/1000 | Loss: 0.00002097
Iteration 118/1000 | Loss: 0.00002097
Iteration 119/1000 | Loss: 0.00002097
Iteration 120/1000 | Loss: 0.00002097
Iteration 121/1000 | Loss: 0.00002097
Iteration 122/1000 | Loss: 0.00002097
Iteration 123/1000 | Loss: 0.00002097
Iteration 124/1000 | Loss: 0.00002097
Iteration 125/1000 | Loss: 0.00002097
Iteration 126/1000 | Loss: 0.00002097
Iteration 127/1000 | Loss: 0.00002097
Iteration 128/1000 | Loss: 0.00002097
Iteration 129/1000 | Loss: 0.00002097
Iteration 130/1000 | Loss: 0.00002097
Iteration 131/1000 | Loss: 0.00002096
Iteration 132/1000 | Loss: 0.00002096
Iteration 133/1000 | Loss: 0.00002096
Iteration 134/1000 | Loss: 0.00002096
Iteration 135/1000 | Loss: 0.00002096
Iteration 136/1000 | Loss: 0.00002096
Iteration 137/1000 | Loss: 0.00002096
Iteration 138/1000 | Loss: 0.00002096
Iteration 139/1000 | Loss: 0.00002095
Iteration 140/1000 | Loss: 0.00002095
Iteration 141/1000 | Loss: 0.00002095
Iteration 142/1000 | Loss: 0.00002095
Iteration 143/1000 | Loss: 0.00002095
Iteration 144/1000 | Loss: 0.00002095
Iteration 145/1000 | Loss: 0.00002095
Iteration 146/1000 | Loss: 0.00002095
Iteration 147/1000 | Loss: 0.00002095
Iteration 148/1000 | Loss: 0.00002095
Iteration 149/1000 | Loss: 0.00002095
Iteration 150/1000 | Loss: 0.00002095
Iteration 151/1000 | Loss: 0.00002095
Iteration 152/1000 | Loss: 0.00002095
Iteration 153/1000 | Loss: 0.00002095
Iteration 154/1000 | Loss: 0.00002095
Iteration 155/1000 | Loss: 0.00002095
Iteration 156/1000 | Loss: 0.00002094
Iteration 157/1000 | Loss: 0.00002094
Iteration 158/1000 | Loss: 0.00002094
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002094
Iteration 165/1000 | Loss: 0.00002093
Iteration 166/1000 | Loss: 0.00002093
Iteration 167/1000 | Loss: 0.00002093
Iteration 168/1000 | Loss: 0.00002093
Iteration 169/1000 | Loss: 0.00002093
Iteration 170/1000 | Loss: 0.00002093
Iteration 171/1000 | Loss: 0.00002093
Iteration 172/1000 | Loss: 0.00002093
Iteration 173/1000 | Loss: 0.00002093
Iteration 174/1000 | Loss: 0.00002093
Iteration 175/1000 | Loss: 0.00002092
Iteration 176/1000 | Loss: 0.00002092
Iteration 177/1000 | Loss: 0.00002092
Iteration 178/1000 | Loss: 0.00002092
Iteration 179/1000 | Loss: 0.00002092
Iteration 180/1000 | Loss: 0.00002092
Iteration 181/1000 | Loss: 0.00002091
Iteration 182/1000 | Loss: 0.00002091
Iteration 183/1000 | Loss: 0.00002091
Iteration 184/1000 | Loss: 0.00002091
Iteration 185/1000 | Loss: 0.00002091
Iteration 186/1000 | Loss: 0.00002091
Iteration 187/1000 | Loss: 0.00002091
Iteration 188/1000 | Loss: 0.00002091
Iteration 189/1000 | Loss: 0.00002091
Iteration 190/1000 | Loss: 0.00002091
Iteration 191/1000 | Loss: 0.00002091
Iteration 192/1000 | Loss: 0.00002091
Iteration 193/1000 | Loss: 0.00002091
Iteration 194/1000 | Loss: 0.00002091
Iteration 195/1000 | Loss: 0.00002091
Iteration 196/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.0912388208671473e-05, 2.0912388208671473e-05, 2.0912388208671473e-05, 2.0912388208671473e-05, 2.0912388208671473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0912388208671473e-05

Optimization complete. Final v2v error: 3.8283157348632812 mm

Highest mean error: 4.928074359893799 mm for frame 27

Lowest mean error: 3.219367265701294 mm for frame 128

Saving results

Total time: 45.08257532119751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473139
Iteration 2/25 | Loss: 0.00109180
Iteration 3/25 | Loss: 0.00097154
Iteration 4/25 | Loss: 0.00094049
Iteration 5/25 | Loss: 0.00093131
Iteration 6/25 | Loss: 0.00093004
Iteration 7/25 | Loss: 0.00092958
Iteration 8/25 | Loss: 0.00092957
Iteration 9/25 | Loss: 0.00092957
Iteration 10/25 | Loss: 0.00092957
Iteration 11/25 | Loss: 0.00092957
Iteration 12/25 | Loss: 0.00092957
Iteration 13/25 | Loss: 0.00092957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009295728523284197, 0.0009295728523284197, 0.0009295728523284197, 0.0009295728523284197, 0.0009295728523284197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009295728523284197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55271530
Iteration 2/25 | Loss: 0.00177887
Iteration 3/25 | Loss: 0.00177887
Iteration 4/25 | Loss: 0.00177887
Iteration 5/25 | Loss: 0.00177887
Iteration 6/25 | Loss: 0.00177887
Iteration 7/25 | Loss: 0.00177887
Iteration 8/25 | Loss: 0.00177887
Iteration 9/25 | Loss: 0.00177887
Iteration 10/25 | Loss: 0.00177887
Iteration 11/25 | Loss: 0.00177887
Iteration 12/25 | Loss: 0.00177887
Iteration 13/25 | Loss: 0.00177887
Iteration 14/25 | Loss: 0.00177887
Iteration 15/25 | Loss: 0.00177887
Iteration 16/25 | Loss: 0.00177887
Iteration 17/25 | Loss: 0.00177887
Iteration 18/25 | Loss: 0.00177887
Iteration 19/25 | Loss: 0.00177887
Iteration 20/25 | Loss: 0.00177887
Iteration 21/25 | Loss: 0.00177887
Iteration 22/25 | Loss: 0.00177887
Iteration 23/25 | Loss: 0.00177887
Iteration 24/25 | Loss: 0.00177887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017788708209991455, 0.0017788708209991455, 0.0017788708209991455, 0.0017788708209991455, 0.0017788708209991455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017788708209991455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177887
Iteration 2/1000 | Loss: 0.00005261
Iteration 3/1000 | Loss: 0.00003935
Iteration 4/1000 | Loss: 0.00003673
Iteration 5/1000 | Loss: 0.00003529
Iteration 6/1000 | Loss: 0.00003435
Iteration 7/1000 | Loss: 0.00003346
Iteration 8/1000 | Loss: 0.00003282
Iteration 9/1000 | Loss: 0.00003251
Iteration 10/1000 | Loss: 0.00003236
Iteration 11/1000 | Loss: 0.00003231
Iteration 12/1000 | Loss: 0.00003231
Iteration 13/1000 | Loss: 0.00003227
Iteration 14/1000 | Loss: 0.00003222
Iteration 15/1000 | Loss: 0.00003221
Iteration 16/1000 | Loss: 0.00003216
Iteration 17/1000 | Loss: 0.00003213
Iteration 18/1000 | Loss: 0.00003212
Iteration 19/1000 | Loss: 0.00003212
Iteration 20/1000 | Loss: 0.00003212
Iteration 21/1000 | Loss: 0.00003211
Iteration 22/1000 | Loss: 0.00003211
Iteration 23/1000 | Loss: 0.00003211
Iteration 24/1000 | Loss: 0.00003209
Iteration 25/1000 | Loss: 0.00003209
Iteration 26/1000 | Loss: 0.00003208
Iteration 27/1000 | Loss: 0.00003208
Iteration 28/1000 | Loss: 0.00003208
Iteration 29/1000 | Loss: 0.00003207
Iteration 30/1000 | Loss: 0.00003207
Iteration 31/1000 | Loss: 0.00003207
Iteration 32/1000 | Loss: 0.00003206
Iteration 33/1000 | Loss: 0.00003206
Iteration 34/1000 | Loss: 0.00003206
Iteration 35/1000 | Loss: 0.00003206
Iteration 36/1000 | Loss: 0.00003206
Iteration 37/1000 | Loss: 0.00003205
Iteration 38/1000 | Loss: 0.00003205
Iteration 39/1000 | Loss: 0.00003205
Iteration 40/1000 | Loss: 0.00003204
Iteration 41/1000 | Loss: 0.00003203
Iteration 42/1000 | Loss: 0.00003203
Iteration 43/1000 | Loss: 0.00003202
Iteration 44/1000 | Loss: 0.00003202
Iteration 45/1000 | Loss: 0.00003202
Iteration 46/1000 | Loss: 0.00003202
Iteration 47/1000 | Loss: 0.00003201
Iteration 48/1000 | Loss: 0.00003201
Iteration 49/1000 | Loss: 0.00003201
Iteration 50/1000 | Loss: 0.00003200
Iteration 51/1000 | Loss: 0.00003200
Iteration 52/1000 | Loss: 0.00003200
Iteration 53/1000 | Loss: 0.00003200
Iteration 54/1000 | Loss: 0.00003199
Iteration 55/1000 | Loss: 0.00003199
Iteration 56/1000 | Loss: 0.00003199
Iteration 57/1000 | Loss: 0.00003198
Iteration 58/1000 | Loss: 0.00003198
Iteration 59/1000 | Loss: 0.00003198
Iteration 60/1000 | Loss: 0.00003198
Iteration 61/1000 | Loss: 0.00003198
Iteration 62/1000 | Loss: 0.00003198
Iteration 63/1000 | Loss: 0.00003197
Iteration 64/1000 | Loss: 0.00003197
Iteration 65/1000 | Loss: 0.00003197
Iteration 66/1000 | Loss: 0.00003197
Iteration 67/1000 | Loss: 0.00003197
Iteration 68/1000 | Loss: 0.00003197
Iteration 69/1000 | Loss: 0.00003196
Iteration 70/1000 | Loss: 0.00003196
Iteration 71/1000 | Loss: 0.00003196
Iteration 72/1000 | Loss: 0.00003196
Iteration 73/1000 | Loss: 0.00003196
Iteration 74/1000 | Loss: 0.00003196
Iteration 75/1000 | Loss: 0.00003196
Iteration 76/1000 | Loss: 0.00003196
Iteration 77/1000 | Loss: 0.00003196
Iteration 78/1000 | Loss: 0.00003196
Iteration 79/1000 | Loss: 0.00003195
Iteration 80/1000 | Loss: 0.00003195
Iteration 81/1000 | Loss: 0.00003195
Iteration 82/1000 | Loss: 0.00003195
Iteration 83/1000 | Loss: 0.00003195
Iteration 84/1000 | Loss: 0.00003195
Iteration 85/1000 | Loss: 0.00003195
Iteration 86/1000 | Loss: 0.00003195
Iteration 87/1000 | Loss: 0.00003195
Iteration 88/1000 | Loss: 0.00003195
Iteration 89/1000 | Loss: 0.00003195
Iteration 90/1000 | Loss: 0.00003195
Iteration 91/1000 | Loss: 0.00003194
Iteration 92/1000 | Loss: 0.00003194
Iteration 93/1000 | Loss: 0.00003194
Iteration 94/1000 | Loss: 0.00003194
Iteration 95/1000 | Loss: 0.00003194
Iteration 96/1000 | Loss: 0.00003194
Iteration 97/1000 | Loss: 0.00003194
Iteration 98/1000 | Loss: 0.00003194
Iteration 99/1000 | Loss: 0.00003194
Iteration 100/1000 | Loss: 0.00003194
Iteration 101/1000 | Loss: 0.00003194
Iteration 102/1000 | Loss: 0.00003194
Iteration 103/1000 | Loss: 0.00003194
Iteration 104/1000 | Loss: 0.00003194
Iteration 105/1000 | Loss: 0.00003194
Iteration 106/1000 | Loss: 0.00003194
Iteration 107/1000 | Loss: 0.00003194
Iteration 108/1000 | Loss: 0.00003194
Iteration 109/1000 | Loss: 0.00003194
Iteration 110/1000 | Loss: 0.00003194
Iteration 111/1000 | Loss: 0.00003194
Iteration 112/1000 | Loss: 0.00003194
Iteration 113/1000 | Loss: 0.00003194
Iteration 114/1000 | Loss: 0.00003194
Iteration 115/1000 | Loss: 0.00003194
Iteration 116/1000 | Loss: 0.00003194
Iteration 117/1000 | Loss: 0.00003194
Iteration 118/1000 | Loss: 0.00003194
Iteration 119/1000 | Loss: 0.00003194
Iteration 120/1000 | Loss: 0.00003194
Iteration 121/1000 | Loss: 0.00003194
Iteration 122/1000 | Loss: 0.00003194
Iteration 123/1000 | Loss: 0.00003194
Iteration 124/1000 | Loss: 0.00003194
Iteration 125/1000 | Loss: 0.00003194
Iteration 126/1000 | Loss: 0.00003194
Iteration 127/1000 | Loss: 0.00003194
Iteration 128/1000 | Loss: 0.00003194
Iteration 129/1000 | Loss: 0.00003194
Iteration 130/1000 | Loss: 0.00003194
Iteration 131/1000 | Loss: 0.00003194
Iteration 132/1000 | Loss: 0.00003194
Iteration 133/1000 | Loss: 0.00003194
Iteration 134/1000 | Loss: 0.00003194
Iteration 135/1000 | Loss: 0.00003194
Iteration 136/1000 | Loss: 0.00003194
Iteration 137/1000 | Loss: 0.00003194
Iteration 138/1000 | Loss: 0.00003194
Iteration 139/1000 | Loss: 0.00003194
Iteration 140/1000 | Loss: 0.00003194
Iteration 141/1000 | Loss: 0.00003194
Iteration 142/1000 | Loss: 0.00003194
Iteration 143/1000 | Loss: 0.00003194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.193685188307427e-05, 3.193685188307427e-05, 3.193685188307427e-05, 3.193685188307427e-05, 3.193685188307427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.193685188307427e-05

Optimization complete. Final v2v error: 4.631350994110107 mm

Highest mean error: 5.1919145584106445 mm for frame 30

Lowest mean error: 4.1945085525512695 mm for frame 134

Saving results

Total time: 35.254820823669434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121538
Iteration 2/25 | Loss: 0.01121538
Iteration 3/25 | Loss: 0.01121538
Iteration 4/25 | Loss: 0.01121538
Iteration 5/25 | Loss: 0.01121538
Iteration 6/25 | Loss: 0.01121538
Iteration 7/25 | Loss: 0.01121538
Iteration 8/25 | Loss: 0.01121538
Iteration 9/25 | Loss: 0.01121538
Iteration 10/25 | Loss: 0.01121538
Iteration 11/25 | Loss: 0.01121538
Iteration 12/25 | Loss: 0.01121538
Iteration 13/25 | Loss: 0.01121537
Iteration 14/25 | Loss: 0.01121537
Iteration 15/25 | Loss: 0.01121537
Iteration 16/25 | Loss: 0.01121537
Iteration 17/25 | Loss: 0.01121537
Iteration 18/25 | Loss: 0.01121537
Iteration 19/25 | Loss: 0.01121537
Iteration 20/25 | Loss: 0.01121537
Iteration 21/25 | Loss: 0.01121537
Iteration 22/25 | Loss: 0.01121537
Iteration 23/25 | Loss: 0.01121537
Iteration 24/25 | Loss: 0.01121537
Iteration 25/25 | Loss: 0.01121536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81284201
Iteration 2/25 | Loss: 0.08589525
Iteration 3/25 | Loss: 0.08533945
Iteration 4/25 | Loss: 0.08530573
Iteration 5/25 | Loss: 0.08519331
Iteration 6/25 | Loss: 0.08519330
Iteration 7/25 | Loss: 0.08519331
Iteration 8/25 | Loss: 0.08519331
Iteration 9/25 | Loss: 0.08519330
Iteration 10/25 | Loss: 0.08519329
Iteration 11/25 | Loss: 0.08519329
Iteration 12/25 | Loss: 0.08519329
Iteration 13/25 | Loss: 0.08519329
Iteration 14/25 | Loss: 0.08519329
Iteration 15/25 | Loss: 0.08519329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08519329130649567, 0.08519329130649567, 0.08519329130649567, 0.08519329130649567, 0.08519329130649567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08519329130649567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08519329
Iteration 2/1000 | Loss: 0.00610634
Iteration 3/1000 | Loss: 0.00242550
Iteration 4/1000 | Loss: 0.00274587
Iteration 5/1000 | Loss: 0.00339817
Iteration 6/1000 | Loss: 0.00061583
Iteration 7/1000 | Loss: 0.00174574
Iteration 8/1000 | Loss: 0.00090649
Iteration 9/1000 | Loss: 0.00148991
Iteration 10/1000 | Loss: 0.00025270
Iteration 11/1000 | Loss: 0.00034352
Iteration 12/1000 | Loss: 0.00007618
Iteration 13/1000 | Loss: 0.00010359
Iteration 14/1000 | Loss: 0.00055170
Iteration 15/1000 | Loss: 0.00005871
Iteration 16/1000 | Loss: 0.00005849
Iteration 17/1000 | Loss: 0.00004698
Iteration 18/1000 | Loss: 0.00004225
Iteration 19/1000 | Loss: 0.00016234
Iteration 20/1000 | Loss: 0.00085307
Iteration 21/1000 | Loss: 0.00036575
Iteration 22/1000 | Loss: 0.00176922
Iteration 23/1000 | Loss: 0.00018398
Iteration 24/1000 | Loss: 0.00011263
Iteration 25/1000 | Loss: 0.00031668
Iteration 26/1000 | Loss: 0.00003501
Iteration 27/1000 | Loss: 0.00003349
Iteration 28/1000 | Loss: 0.00024711
Iteration 29/1000 | Loss: 0.00112580
Iteration 30/1000 | Loss: 0.00007791
Iteration 31/1000 | Loss: 0.00024259
Iteration 32/1000 | Loss: 0.00004647
Iteration 33/1000 | Loss: 0.00005207
Iteration 34/1000 | Loss: 0.00004914
Iteration 35/1000 | Loss: 0.00004539
Iteration 36/1000 | Loss: 0.00004244
Iteration 37/1000 | Loss: 0.00002780
Iteration 38/1000 | Loss: 0.00002701
Iteration 39/1000 | Loss: 0.00002892
Iteration 40/1000 | Loss: 0.00002637
Iteration 41/1000 | Loss: 0.00002583
Iteration 42/1000 | Loss: 0.00002539
Iteration 43/1000 | Loss: 0.00002521
Iteration 44/1000 | Loss: 0.00006637
Iteration 45/1000 | Loss: 0.00004550
Iteration 46/1000 | Loss: 0.00004478
Iteration 47/1000 | Loss: 0.00004427
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002429
Iteration 50/1000 | Loss: 0.00002428
Iteration 51/1000 | Loss: 0.00002406
Iteration 52/1000 | Loss: 0.00002394
Iteration 53/1000 | Loss: 0.00002392
Iteration 54/1000 | Loss: 0.00002392
Iteration 55/1000 | Loss: 0.00002391
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00002387
Iteration 58/1000 | Loss: 0.00002386
Iteration 59/1000 | Loss: 0.00002386
Iteration 60/1000 | Loss: 0.00002384
Iteration 61/1000 | Loss: 0.00002384
Iteration 62/1000 | Loss: 0.00002383
Iteration 63/1000 | Loss: 0.00002382
Iteration 64/1000 | Loss: 0.00002382
Iteration 65/1000 | Loss: 0.00002381
Iteration 66/1000 | Loss: 0.00002381
Iteration 67/1000 | Loss: 0.00002380
Iteration 68/1000 | Loss: 0.00002380
Iteration 69/1000 | Loss: 0.00002380
Iteration 70/1000 | Loss: 0.00002389
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002379
Iteration 73/1000 | Loss: 0.00002379
Iteration 74/1000 | Loss: 0.00002379
Iteration 75/1000 | Loss: 0.00002379
Iteration 76/1000 | Loss: 0.00002378
Iteration 77/1000 | Loss: 0.00002378
Iteration 78/1000 | Loss: 0.00002387
Iteration 79/1000 | Loss: 0.00002387
Iteration 80/1000 | Loss: 0.00002386
Iteration 81/1000 | Loss: 0.00002386
Iteration 82/1000 | Loss: 0.00002385
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002394
Iteration 85/1000 | Loss: 0.00002393
Iteration 86/1000 | Loss: 0.00002392
Iteration 87/1000 | Loss: 0.00002387
Iteration 88/1000 | Loss: 0.00002387
Iteration 89/1000 | Loss: 0.00002383
Iteration 90/1000 | Loss: 0.00002382
Iteration 91/1000 | Loss: 0.00002382
Iteration 92/1000 | Loss: 0.00002381
Iteration 93/1000 | Loss: 0.00002380
Iteration 94/1000 | Loss: 0.00002380
Iteration 95/1000 | Loss: 0.00002379
Iteration 96/1000 | Loss: 0.00002379
Iteration 97/1000 | Loss: 0.00002374
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002373
Iteration 100/1000 | Loss: 0.00002373
Iteration 101/1000 | Loss: 0.00002378
Iteration 102/1000 | Loss: 0.00002370
Iteration 103/1000 | Loss: 0.00002370
Iteration 104/1000 | Loss: 0.00002369
Iteration 105/1000 | Loss: 0.00004296
Iteration 106/1000 | Loss: 0.00003673
Iteration 107/1000 | Loss: 0.00004237
Iteration 108/1000 | Loss: 0.00007388
Iteration 109/1000 | Loss: 0.00003636
Iteration 110/1000 | Loss: 0.00002382
Iteration 111/1000 | Loss: 0.00002366
Iteration 112/1000 | Loss: 0.00002385
Iteration 113/1000 | Loss: 0.00002385
Iteration 114/1000 | Loss: 0.00002373
Iteration 115/1000 | Loss: 0.00002372
Iteration 116/1000 | Loss: 0.00002376
Iteration 117/1000 | Loss: 0.00004217
Iteration 118/1000 | Loss: 0.00003457
Iteration 119/1000 | Loss: 0.00002406
Iteration 120/1000 | Loss: 0.00004096
Iteration 121/1000 | Loss: 0.00003431
Iteration 122/1000 | Loss: 0.00004150
Iteration 123/1000 | Loss: 0.00003686
Iteration 124/1000 | Loss: 0.00004156
Iteration 125/1000 | Loss: 0.00004161
Iteration 126/1000 | Loss: 0.00003684
Iteration 127/1000 | Loss: 0.00003925
Iteration 128/1000 | Loss: 0.00003776
Iteration 129/1000 | Loss: 0.00004493
Iteration 130/1000 | Loss: 0.00003496
Iteration 131/1000 | Loss: 0.00003453
Iteration 132/1000 | Loss: 0.00003243
Iteration 133/1000 | Loss: 0.00003668
Iteration 134/1000 | Loss: 0.00003636
Iteration 135/1000 | Loss: 0.00003758
Iteration 136/1000 | Loss: 0.00004127
Iteration 137/1000 | Loss: 0.00003661
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002357
Iteration 140/1000 | Loss: 0.00002357
Iteration 141/1000 | Loss: 0.00002356
Iteration 142/1000 | Loss: 0.00002356
Iteration 143/1000 | Loss: 0.00002356
Iteration 144/1000 | Loss: 0.00002355
Iteration 145/1000 | Loss: 0.00002355
Iteration 146/1000 | Loss: 0.00002355
Iteration 147/1000 | Loss: 0.00002353
Iteration 148/1000 | Loss: 0.00002353
Iteration 149/1000 | Loss: 0.00002353
Iteration 150/1000 | Loss: 0.00002352
Iteration 151/1000 | Loss: 0.00002352
Iteration 152/1000 | Loss: 0.00002352
Iteration 153/1000 | Loss: 0.00002352
Iteration 154/1000 | Loss: 0.00002351
Iteration 155/1000 | Loss: 0.00002351
Iteration 156/1000 | Loss: 0.00002351
Iteration 157/1000 | Loss: 0.00002351
Iteration 158/1000 | Loss: 0.00002351
Iteration 159/1000 | Loss: 0.00002351
Iteration 160/1000 | Loss: 0.00002350
Iteration 161/1000 | Loss: 0.00002350
Iteration 162/1000 | Loss: 0.00002349
Iteration 163/1000 | Loss: 0.00002349
Iteration 164/1000 | Loss: 0.00002349
Iteration 165/1000 | Loss: 0.00002349
Iteration 166/1000 | Loss: 0.00002349
Iteration 167/1000 | Loss: 0.00002348
Iteration 168/1000 | Loss: 0.00002348
Iteration 169/1000 | Loss: 0.00002359
Iteration 170/1000 | Loss: 0.00002359
Iteration 171/1000 | Loss: 0.00002359
Iteration 172/1000 | Loss: 0.00002359
Iteration 173/1000 | Loss: 0.00002359
Iteration 174/1000 | Loss: 0.00002358
Iteration 175/1000 | Loss: 0.00002358
Iteration 176/1000 | Loss: 0.00002357
Iteration 177/1000 | Loss: 0.00002356
Iteration 178/1000 | Loss: 0.00002356
Iteration 179/1000 | Loss: 0.00002355
Iteration 180/1000 | Loss: 0.00002355
Iteration 181/1000 | Loss: 0.00002355
Iteration 182/1000 | Loss: 0.00002355
Iteration 183/1000 | Loss: 0.00002355
Iteration 184/1000 | Loss: 0.00002355
Iteration 185/1000 | Loss: 0.00002355
Iteration 186/1000 | Loss: 0.00002355
Iteration 187/1000 | Loss: 0.00002355
Iteration 188/1000 | Loss: 0.00002355
Iteration 189/1000 | Loss: 0.00002357
Iteration 190/1000 | Loss: 0.00002348
Iteration 191/1000 | Loss: 0.00002348
Iteration 192/1000 | Loss: 0.00002347
Iteration 193/1000 | Loss: 0.00002347
Iteration 194/1000 | Loss: 0.00002347
Iteration 195/1000 | Loss: 0.00002347
Iteration 196/1000 | Loss: 0.00002347
Iteration 197/1000 | Loss: 0.00002347
Iteration 198/1000 | Loss: 0.00002347
Iteration 199/1000 | Loss: 0.00002346
Iteration 200/1000 | Loss: 0.00002346
Iteration 201/1000 | Loss: 0.00002346
Iteration 202/1000 | Loss: 0.00002346
Iteration 203/1000 | Loss: 0.00002345
Iteration 204/1000 | Loss: 0.00002345
Iteration 205/1000 | Loss: 0.00002345
Iteration 206/1000 | Loss: 0.00002345
Iteration 207/1000 | Loss: 0.00002345
Iteration 208/1000 | Loss: 0.00002345
Iteration 209/1000 | Loss: 0.00002345
Iteration 210/1000 | Loss: 0.00002345
Iteration 211/1000 | Loss: 0.00002345
Iteration 212/1000 | Loss: 0.00002345
Iteration 213/1000 | Loss: 0.00002345
Iteration 214/1000 | Loss: 0.00002345
Iteration 215/1000 | Loss: 0.00002345
Iteration 216/1000 | Loss: 0.00002345
Iteration 217/1000 | Loss: 0.00002345
Iteration 218/1000 | Loss: 0.00002345
Iteration 219/1000 | Loss: 0.00002345
Iteration 220/1000 | Loss: 0.00002345
Iteration 221/1000 | Loss: 0.00002345
Iteration 222/1000 | Loss: 0.00002345
Iteration 223/1000 | Loss: 0.00002345
Iteration 224/1000 | Loss: 0.00002345
Iteration 225/1000 | Loss: 0.00002345
Iteration 226/1000 | Loss: 0.00002345
Iteration 227/1000 | Loss: 0.00002345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.345172833884135e-05, 2.345172833884135e-05, 2.345172833884135e-05, 2.345172833884135e-05, 2.345172833884135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.345172833884135e-05

Optimization complete. Final v2v error: 3.4863946437835693 mm

Highest mean error: 22.270084381103516 mm for frame 172

Lowest mean error: 2.827925443649292 mm for frame 6

Saving results

Total time: 158.4513828754425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023507
Iteration 2/25 | Loss: 0.00416063
Iteration 3/25 | Loss: 0.00327565
Iteration 4/25 | Loss: 0.00284825
Iteration 5/25 | Loss: 0.00226033
Iteration 6/25 | Loss: 0.00199911
Iteration 7/25 | Loss: 0.00189326
Iteration 8/25 | Loss: 0.00184866
Iteration 9/25 | Loss: 0.00183519
Iteration 10/25 | Loss: 0.00182673
Iteration 11/25 | Loss: 0.00181859
Iteration 12/25 | Loss: 0.00182834
Iteration 13/25 | Loss: 0.00173403
Iteration 14/25 | Loss: 0.00209742
Iteration 15/25 | Loss: 0.00143796
Iteration 16/25 | Loss: 0.00118744
Iteration 17/25 | Loss: 0.00111573
Iteration 18/25 | Loss: 0.00112406
Iteration 19/25 | Loss: 0.00105458
Iteration 20/25 | Loss: 0.00104557
Iteration 21/25 | Loss: 0.00102833
Iteration 22/25 | Loss: 0.00100991
Iteration 23/25 | Loss: 0.00099927
Iteration 24/25 | Loss: 0.00098828
Iteration 25/25 | Loss: 0.00098532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59188735
Iteration 2/25 | Loss: 0.00235428
Iteration 3/25 | Loss: 0.00170644
Iteration 4/25 | Loss: 0.00170644
Iteration 5/25 | Loss: 0.00170644
Iteration 6/25 | Loss: 0.00170644
Iteration 7/25 | Loss: 0.00170644
Iteration 8/25 | Loss: 0.00170644
Iteration 9/25 | Loss: 0.00170644
Iteration 10/25 | Loss: 0.00170644
Iteration 11/25 | Loss: 0.00170644
Iteration 12/25 | Loss: 0.00170644
Iteration 13/25 | Loss: 0.00170644
Iteration 14/25 | Loss: 0.00170644
Iteration 15/25 | Loss: 0.00170644
Iteration 16/25 | Loss: 0.00170644
Iteration 17/25 | Loss: 0.00170644
Iteration 18/25 | Loss: 0.00170644
Iteration 19/25 | Loss: 0.00170644
Iteration 20/25 | Loss: 0.00170644
Iteration 21/25 | Loss: 0.00170644
Iteration 22/25 | Loss: 0.00170644
Iteration 23/25 | Loss: 0.00170644
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017064380226656795, 0.0017064380226656795, 0.0017064380226656795, 0.0017064380226656795, 0.0017064380226656795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017064380226656795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170644
Iteration 2/1000 | Loss: 0.00006119
Iteration 3/1000 | Loss: 0.00004385
Iteration 4/1000 | Loss: 0.00075641
Iteration 5/1000 | Loss: 0.00038737
Iteration 6/1000 | Loss: 0.00068080
Iteration 7/1000 | Loss: 0.00003344
Iteration 8/1000 | Loss: 0.00033520
Iteration 9/1000 | Loss: 0.00005815
Iteration 10/1000 | Loss: 0.00003888
Iteration 11/1000 | Loss: 0.00003091
Iteration 12/1000 | Loss: 0.00002972
Iteration 13/1000 | Loss: 0.00002877
Iteration 14/1000 | Loss: 0.00136940
Iteration 15/1000 | Loss: 0.00240413
Iteration 16/1000 | Loss: 0.00009691
Iteration 17/1000 | Loss: 0.00062970
Iteration 18/1000 | Loss: 0.00003174
Iteration 19/1000 | Loss: 0.00002643
Iteration 20/1000 | Loss: 0.00034835
Iteration 21/1000 | Loss: 0.00002241
Iteration 22/1000 | Loss: 0.00002120
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002019
Iteration 27/1000 | Loss: 0.00002015
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002074
Iteration 30/1000 | Loss: 0.00002011
Iteration 31/1000 | Loss: 0.00002011
Iteration 32/1000 | Loss: 0.00002010
Iteration 33/1000 | Loss: 0.00002010
Iteration 34/1000 | Loss: 0.00002009
Iteration 35/1000 | Loss: 0.00002009
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002009
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002009
Iteration 43/1000 | Loss: 0.00002008
Iteration 44/1000 | Loss: 0.00002008
Iteration 45/1000 | Loss: 0.00002008
Iteration 46/1000 | Loss: 0.00002008
Iteration 47/1000 | Loss: 0.00002008
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002055
Iteration 50/1000 | Loss: 0.00002000
Iteration 51/1000 | Loss: 0.00002000
Iteration 52/1000 | Loss: 0.00002000
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00002000
Iteration 55/1000 | Loss: 0.00002000
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00002000
Iteration 60/1000 | Loss: 0.00002000
Iteration 61/1000 | Loss: 0.00002000
Iteration 62/1000 | Loss: 0.00002000
Iteration 63/1000 | Loss: 0.00002000
Iteration 64/1000 | Loss: 0.00002000
Iteration 65/1000 | Loss: 0.00002000
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.9996819901280105e-05, 1.9996819901280105e-05, 1.9996819901280105e-05, 1.9996819901280105e-05, 1.9996819901280105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9996819901280105e-05

Optimization complete. Final v2v error: 3.812417507171631 mm

Highest mean error: 10.584000587463379 mm for frame 93

Lowest mean error: 3.612745523452759 mm for frame 161

Saving results

Total time: 89.96098136901855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00674144
Iteration 2/25 | Loss: 0.00100894
Iteration 3/25 | Loss: 0.00089501
Iteration 4/25 | Loss: 0.00087364
Iteration 5/25 | Loss: 0.00086573
Iteration 6/25 | Loss: 0.00086387
Iteration 7/25 | Loss: 0.00086358
Iteration 8/25 | Loss: 0.00086358
Iteration 9/25 | Loss: 0.00086358
Iteration 10/25 | Loss: 0.00086358
Iteration 11/25 | Loss: 0.00086358
Iteration 12/25 | Loss: 0.00086358
Iteration 13/25 | Loss: 0.00086358
Iteration 14/25 | Loss: 0.00086358
Iteration 15/25 | Loss: 0.00086358
Iteration 16/25 | Loss: 0.00086358
Iteration 17/25 | Loss: 0.00086358
Iteration 18/25 | Loss: 0.00086358
Iteration 19/25 | Loss: 0.00086358
Iteration 20/25 | Loss: 0.00086358
Iteration 21/25 | Loss: 0.00086358
Iteration 22/25 | Loss: 0.00086358
Iteration 23/25 | Loss: 0.00086358
Iteration 24/25 | Loss: 0.00086358
Iteration 25/25 | Loss: 0.00086358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15397501
Iteration 2/25 | Loss: 0.00175610
Iteration 3/25 | Loss: 0.00175610
Iteration 4/25 | Loss: 0.00175610
Iteration 5/25 | Loss: 0.00175610
Iteration 6/25 | Loss: 0.00175609
Iteration 7/25 | Loss: 0.00175609
Iteration 8/25 | Loss: 0.00175609
Iteration 9/25 | Loss: 0.00175609
Iteration 10/25 | Loss: 0.00175609
Iteration 11/25 | Loss: 0.00175609
Iteration 12/25 | Loss: 0.00175609
Iteration 13/25 | Loss: 0.00175609
Iteration 14/25 | Loss: 0.00175609
Iteration 15/25 | Loss: 0.00175609
Iteration 16/25 | Loss: 0.00175609
Iteration 17/25 | Loss: 0.00175609
Iteration 18/25 | Loss: 0.00175609
Iteration 19/25 | Loss: 0.00175609
Iteration 20/25 | Loss: 0.00175609
Iteration 21/25 | Loss: 0.00175609
Iteration 22/25 | Loss: 0.00175609
Iteration 23/25 | Loss: 0.00175609
Iteration 24/25 | Loss: 0.00175609
Iteration 25/25 | Loss: 0.00175609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175609
Iteration 2/1000 | Loss: 0.00002538
Iteration 3/1000 | Loss: 0.00002098
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001884
Iteration 6/1000 | Loss: 0.00001837
Iteration 7/1000 | Loss: 0.00001797
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001777
Iteration 10/1000 | Loss: 0.00001777
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001764
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001761
Iteration 16/1000 | Loss: 0.00001761
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001759
Iteration 20/1000 | Loss: 0.00001759
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001757
Iteration 27/1000 | Loss: 0.00001756
Iteration 28/1000 | Loss: 0.00001756
Iteration 29/1000 | Loss: 0.00001756
Iteration 30/1000 | Loss: 0.00001754
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001753
Iteration 35/1000 | Loss: 0.00001753
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001753
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001752
Iteration 41/1000 | Loss: 0.00001752
Iteration 42/1000 | Loss: 0.00001752
Iteration 43/1000 | Loss: 0.00001752
Iteration 44/1000 | Loss: 0.00001751
Iteration 45/1000 | Loss: 0.00001751
Iteration 46/1000 | Loss: 0.00001751
Iteration 47/1000 | Loss: 0.00001751
Iteration 48/1000 | Loss: 0.00001751
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001749
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001747
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001747
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.746715497574769e-05, 1.746715497574769e-05, 1.746715497574769e-05, 1.746715497574769e-05, 1.746715497574769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.746715497574769e-05

Optimization complete. Final v2v error: 3.552433967590332 mm

Highest mean error: 4.111546516418457 mm for frame 77

Lowest mean error: 2.8877651691436768 mm for frame 90

Saving results

Total time: 28.9360408782959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512570
Iteration 2/25 | Loss: 0.00114485
Iteration 3/25 | Loss: 0.00092928
Iteration 4/25 | Loss: 0.00089739
Iteration 5/25 | Loss: 0.00089341
Iteration 6/25 | Loss: 0.00089242
Iteration 7/25 | Loss: 0.00089232
Iteration 8/25 | Loss: 0.00089232
Iteration 9/25 | Loss: 0.00089232
Iteration 10/25 | Loss: 0.00089232
Iteration 11/25 | Loss: 0.00089232
Iteration 12/25 | Loss: 0.00089232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008923150016926229, 0.0008923150016926229, 0.0008923150016926229, 0.0008923150016926229, 0.0008923150016926229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008923150016926229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97146595
Iteration 2/25 | Loss: 0.00149698
Iteration 3/25 | Loss: 0.00149697
Iteration 4/25 | Loss: 0.00149697
Iteration 5/25 | Loss: 0.00149697
Iteration 6/25 | Loss: 0.00149696
Iteration 7/25 | Loss: 0.00149696
Iteration 8/25 | Loss: 0.00149696
Iteration 9/25 | Loss: 0.00149696
Iteration 10/25 | Loss: 0.00149696
Iteration 11/25 | Loss: 0.00149696
Iteration 12/25 | Loss: 0.00149696
Iteration 13/25 | Loss: 0.00149696
Iteration 14/25 | Loss: 0.00149696
Iteration 15/25 | Loss: 0.00149696
Iteration 16/25 | Loss: 0.00149696
Iteration 17/25 | Loss: 0.00149696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014969637850299478, 0.0014969637850299478, 0.0014969637850299478, 0.0014969637850299478, 0.0014969637850299478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014969637850299478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149696
Iteration 2/1000 | Loss: 0.00002481
Iteration 3/1000 | Loss: 0.00002031
Iteration 4/1000 | Loss: 0.00001961
Iteration 5/1000 | Loss: 0.00001898
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001832
Iteration 8/1000 | Loss: 0.00001830
Iteration 9/1000 | Loss: 0.00001829
Iteration 10/1000 | Loss: 0.00001826
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001825
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001824
Iteration 15/1000 | Loss: 0.00001823
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001822
Iteration 18/1000 | Loss: 0.00001821
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001820
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001810
Iteration 25/1000 | Loss: 0.00001810
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001810
Iteration 30/1000 | Loss: 0.00001809
Iteration 31/1000 | Loss: 0.00001809
Iteration 32/1000 | Loss: 0.00001809
Iteration 33/1000 | Loss: 0.00001809
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001808
Iteration 42/1000 | Loss: 0.00001808
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001804
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001804
Iteration 49/1000 | Loss: 0.00001804
Iteration 50/1000 | Loss: 0.00001804
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001804
Iteration 53/1000 | Loss: 0.00001804
Iteration 54/1000 | Loss: 0.00001804
Iteration 55/1000 | Loss: 0.00001803
Iteration 56/1000 | Loss: 0.00001803
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001803
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001802
Iteration 61/1000 | Loss: 0.00001802
Iteration 62/1000 | Loss: 0.00001802
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001802
Iteration 65/1000 | Loss: 0.00001802
Iteration 66/1000 | Loss: 0.00001801
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001801
Iteration 70/1000 | Loss: 0.00001801
Iteration 71/1000 | Loss: 0.00001801
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001800
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001799
Iteration 80/1000 | Loss: 0.00001799
Iteration 81/1000 | Loss: 0.00001799
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001798
Iteration 88/1000 | Loss: 0.00001798
Iteration 89/1000 | Loss: 0.00001798
Iteration 90/1000 | Loss: 0.00001798
Iteration 91/1000 | Loss: 0.00001798
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001797
Iteration 94/1000 | Loss: 0.00001797
Iteration 95/1000 | Loss: 0.00001797
Iteration 96/1000 | Loss: 0.00001797
Iteration 97/1000 | Loss: 0.00001797
Iteration 98/1000 | Loss: 0.00001797
Iteration 99/1000 | Loss: 0.00001797
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001794
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001793
Iteration 129/1000 | Loss: 0.00001793
Iteration 130/1000 | Loss: 0.00001793
Iteration 131/1000 | Loss: 0.00001793
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001793
Iteration 136/1000 | Loss: 0.00001793
Iteration 137/1000 | Loss: 0.00001793
Iteration 138/1000 | Loss: 0.00001793
Iteration 139/1000 | Loss: 0.00001793
Iteration 140/1000 | Loss: 0.00001793
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001793
Iteration 143/1000 | Loss: 0.00001793
Iteration 144/1000 | Loss: 0.00001793
Iteration 145/1000 | Loss: 0.00001793
Iteration 146/1000 | Loss: 0.00001793
Iteration 147/1000 | Loss: 0.00001793
Iteration 148/1000 | Loss: 0.00001793
Iteration 149/1000 | Loss: 0.00001793
Iteration 150/1000 | Loss: 0.00001793
Iteration 151/1000 | Loss: 0.00001793
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001793
Iteration 155/1000 | Loss: 0.00001793
Iteration 156/1000 | Loss: 0.00001793
Iteration 157/1000 | Loss: 0.00001793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.7927945009432733e-05, 1.7927945009432733e-05, 1.7927945009432733e-05, 1.7927945009432733e-05, 1.7927945009432733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7927945009432733e-05

Optimization complete. Final v2v error: 3.6587064266204834 mm

Highest mean error: 3.8673970699310303 mm for frame 11

Lowest mean error: 3.456265449523926 mm for frame 33

Saving results

Total time: 30.155750036239624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934615
Iteration 2/25 | Loss: 0.00135706
Iteration 3/25 | Loss: 0.00106865
Iteration 4/25 | Loss: 0.00103118
Iteration 5/25 | Loss: 0.00102127
Iteration 6/25 | Loss: 0.00101980
Iteration 7/25 | Loss: 0.00101971
Iteration 8/25 | Loss: 0.00101971
Iteration 9/25 | Loss: 0.00101971
Iteration 10/25 | Loss: 0.00101971
Iteration 11/25 | Loss: 0.00101971
Iteration 12/25 | Loss: 0.00101971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001019709394313395, 0.001019709394313395, 0.001019709394313395, 0.001019709394313395, 0.001019709394313395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001019709394313395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11520767
Iteration 2/25 | Loss: 0.00164883
Iteration 3/25 | Loss: 0.00164882
Iteration 4/25 | Loss: 0.00164882
Iteration 5/25 | Loss: 0.00164882
Iteration 6/25 | Loss: 0.00164882
Iteration 7/25 | Loss: 0.00164882
Iteration 8/25 | Loss: 0.00164882
Iteration 9/25 | Loss: 0.00164881
Iteration 10/25 | Loss: 0.00164881
Iteration 11/25 | Loss: 0.00164881
Iteration 12/25 | Loss: 0.00164881
Iteration 13/25 | Loss: 0.00164881
Iteration 14/25 | Loss: 0.00164881
Iteration 15/25 | Loss: 0.00164881
Iteration 16/25 | Loss: 0.00164881
Iteration 17/25 | Loss: 0.00164881
Iteration 18/25 | Loss: 0.00164881
Iteration 19/25 | Loss: 0.00164881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016488147666677833, 0.0016488147666677833, 0.0016488147666677833, 0.0016488147666677833, 0.0016488147666677833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016488147666677833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164881
Iteration 2/1000 | Loss: 0.00004813
Iteration 3/1000 | Loss: 0.00003963
Iteration 4/1000 | Loss: 0.00003709
Iteration 5/1000 | Loss: 0.00003535
Iteration 6/1000 | Loss: 0.00003420
Iteration 7/1000 | Loss: 0.00003349
Iteration 8/1000 | Loss: 0.00003292
Iteration 9/1000 | Loss: 0.00003260
Iteration 10/1000 | Loss: 0.00003255
Iteration 11/1000 | Loss: 0.00003255
Iteration 12/1000 | Loss: 0.00003244
Iteration 13/1000 | Loss: 0.00003234
Iteration 14/1000 | Loss: 0.00003234
Iteration 15/1000 | Loss: 0.00003232
Iteration 16/1000 | Loss: 0.00003230
Iteration 17/1000 | Loss: 0.00003230
Iteration 18/1000 | Loss: 0.00003230
Iteration 19/1000 | Loss: 0.00003230
Iteration 20/1000 | Loss: 0.00003230
Iteration 21/1000 | Loss: 0.00003230
Iteration 22/1000 | Loss: 0.00003230
Iteration 23/1000 | Loss: 0.00003229
Iteration 24/1000 | Loss: 0.00003229
Iteration 25/1000 | Loss: 0.00003229
Iteration 26/1000 | Loss: 0.00003228
Iteration 27/1000 | Loss: 0.00003226
Iteration 28/1000 | Loss: 0.00003226
Iteration 29/1000 | Loss: 0.00003226
Iteration 30/1000 | Loss: 0.00003226
Iteration 31/1000 | Loss: 0.00003226
Iteration 32/1000 | Loss: 0.00003226
Iteration 33/1000 | Loss: 0.00003226
Iteration 34/1000 | Loss: 0.00003226
Iteration 35/1000 | Loss: 0.00003226
Iteration 36/1000 | Loss: 0.00003225
Iteration 37/1000 | Loss: 0.00003225
Iteration 38/1000 | Loss: 0.00003225
Iteration 39/1000 | Loss: 0.00003225
Iteration 40/1000 | Loss: 0.00003225
Iteration 41/1000 | Loss: 0.00003225
Iteration 42/1000 | Loss: 0.00003225
Iteration 43/1000 | Loss: 0.00003225
Iteration 44/1000 | Loss: 0.00003225
Iteration 45/1000 | Loss: 0.00003224
Iteration 46/1000 | Loss: 0.00003224
Iteration 47/1000 | Loss: 0.00003224
Iteration 48/1000 | Loss: 0.00003224
Iteration 49/1000 | Loss: 0.00003224
Iteration 50/1000 | Loss: 0.00003224
Iteration 51/1000 | Loss: 0.00003223
Iteration 52/1000 | Loss: 0.00003223
Iteration 53/1000 | Loss: 0.00003223
Iteration 54/1000 | Loss: 0.00003223
Iteration 55/1000 | Loss: 0.00003222
Iteration 56/1000 | Loss: 0.00003221
Iteration 57/1000 | Loss: 0.00003221
Iteration 58/1000 | Loss: 0.00003221
Iteration 59/1000 | Loss: 0.00003220
Iteration 60/1000 | Loss: 0.00003220
Iteration 61/1000 | Loss: 0.00003220
Iteration 62/1000 | Loss: 0.00003220
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003220
Iteration 66/1000 | Loss: 0.00003220
Iteration 67/1000 | Loss: 0.00003220
Iteration 68/1000 | Loss: 0.00003219
Iteration 69/1000 | Loss: 0.00003219
Iteration 70/1000 | Loss: 0.00003219
Iteration 71/1000 | Loss: 0.00003219
Iteration 72/1000 | Loss: 0.00003219
Iteration 73/1000 | Loss: 0.00003219
Iteration 74/1000 | Loss: 0.00003219
Iteration 75/1000 | Loss: 0.00003219
Iteration 76/1000 | Loss: 0.00003219
Iteration 77/1000 | Loss: 0.00003219
Iteration 78/1000 | Loss: 0.00003219
Iteration 79/1000 | Loss: 0.00003219
Iteration 80/1000 | Loss: 0.00003219
Iteration 81/1000 | Loss: 0.00003219
Iteration 82/1000 | Loss: 0.00003219
Iteration 83/1000 | Loss: 0.00003219
Iteration 84/1000 | Loss: 0.00003219
Iteration 85/1000 | Loss: 0.00003219
Iteration 86/1000 | Loss: 0.00003219
Iteration 87/1000 | Loss: 0.00003219
Iteration 88/1000 | Loss: 0.00003219
Iteration 89/1000 | Loss: 0.00003219
Iteration 90/1000 | Loss: 0.00003219
Iteration 91/1000 | Loss: 0.00003219
Iteration 92/1000 | Loss: 0.00003219
Iteration 93/1000 | Loss: 0.00003219
Iteration 94/1000 | Loss: 0.00003219
Iteration 95/1000 | Loss: 0.00003219
Iteration 96/1000 | Loss: 0.00003219
Iteration 97/1000 | Loss: 0.00003219
Iteration 98/1000 | Loss: 0.00003219
Iteration 99/1000 | Loss: 0.00003219
Iteration 100/1000 | Loss: 0.00003219
Iteration 101/1000 | Loss: 0.00003219
Iteration 102/1000 | Loss: 0.00003219
Iteration 103/1000 | Loss: 0.00003219
Iteration 104/1000 | Loss: 0.00003219
Iteration 105/1000 | Loss: 0.00003219
Iteration 106/1000 | Loss: 0.00003219
Iteration 107/1000 | Loss: 0.00003219
Iteration 108/1000 | Loss: 0.00003219
Iteration 109/1000 | Loss: 0.00003219
Iteration 110/1000 | Loss: 0.00003219
Iteration 111/1000 | Loss: 0.00003219
Iteration 112/1000 | Loss: 0.00003219
Iteration 113/1000 | Loss: 0.00003219
Iteration 114/1000 | Loss: 0.00003219
Iteration 115/1000 | Loss: 0.00003219
Iteration 116/1000 | Loss: 0.00003219
Iteration 117/1000 | Loss: 0.00003219
Iteration 118/1000 | Loss: 0.00003219
Iteration 119/1000 | Loss: 0.00003219
Iteration 120/1000 | Loss: 0.00003219
Iteration 121/1000 | Loss: 0.00003219
Iteration 122/1000 | Loss: 0.00003219
Iteration 123/1000 | Loss: 0.00003219
Iteration 124/1000 | Loss: 0.00003219
Iteration 125/1000 | Loss: 0.00003219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.2185060263145715e-05, 3.2185060263145715e-05, 3.2185060263145715e-05, 3.2185060263145715e-05, 3.2185060263145715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2185060263145715e-05

Optimization complete. Final v2v error: 4.778406143188477 mm

Highest mean error: 5.251920700073242 mm for frame 123

Lowest mean error: 4.40631628036499 mm for frame 0

Saving results

Total time: 30.364129781723022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00651107
Iteration 2/25 | Loss: 0.00125131
Iteration 3/25 | Loss: 0.00098860
Iteration 4/25 | Loss: 0.00094968
Iteration 5/25 | Loss: 0.00092847
Iteration 6/25 | Loss: 0.00092679
Iteration 7/25 | Loss: 0.00092379
Iteration 8/25 | Loss: 0.00091761
Iteration 9/25 | Loss: 0.00091466
Iteration 10/25 | Loss: 0.00091323
Iteration 11/25 | Loss: 0.00091701
Iteration 12/25 | Loss: 0.00091613
Iteration 13/25 | Loss: 0.00091478
Iteration 14/25 | Loss: 0.00091429
Iteration 15/25 | Loss: 0.00091316
Iteration 16/25 | Loss: 0.00091304
Iteration 17/25 | Loss: 0.00091294
Iteration 18/25 | Loss: 0.00091276
Iteration 19/25 | Loss: 0.00091271
Iteration 20/25 | Loss: 0.00091271
Iteration 21/25 | Loss: 0.00091270
Iteration 22/25 | Loss: 0.00091270
Iteration 23/25 | Loss: 0.00091270
Iteration 24/25 | Loss: 0.00091270
Iteration 25/25 | Loss: 0.00091270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63716793
Iteration 2/25 | Loss: 0.00214814
Iteration 3/25 | Loss: 0.00214810
Iteration 4/25 | Loss: 0.00214810
Iteration 5/25 | Loss: 0.00214810
Iteration 6/25 | Loss: 0.00214809
Iteration 7/25 | Loss: 0.00214809
Iteration 8/25 | Loss: 0.00214809
Iteration 9/25 | Loss: 0.00214809
Iteration 10/25 | Loss: 0.00214809
Iteration 11/25 | Loss: 0.00214809
Iteration 12/25 | Loss: 0.00214809
Iteration 13/25 | Loss: 0.00214809
Iteration 14/25 | Loss: 0.00214809
Iteration 15/25 | Loss: 0.00214809
Iteration 16/25 | Loss: 0.00214809
Iteration 17/25 | Loss: 0.00214809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021480938885360956, 0.0021480938885360956, 0.0021480938885360956, 0.0021480938885360956, 0.0021480938885360956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021480938885360956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214809
Iteration 2/1000 | Loss: 0.00004199
Iteration 3/1000 | Loss: 0.00002755
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002355
Iteration 6/1000 | Loss: 0.00002265
Iteration 7/1000 | Loss: 0.00002219
Iteration 8/1000 | Loss: 0.00002188
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002132
Iteration 11/1000 | Loss: 0.00002111
Iteration 12/1000 | Loss: 0.00002092
Iteration 13/1000 | Loss: 0.00002087
Iteration 14/1000 | Loss: 0.00002079
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002070
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002067
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002046
Iteration 23/1000 | Loss: 0.00002041
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002032
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002029
Iteration 28/1000 | Loss: 0.00002023
Iteration 29/1000 | Loss: 0.00002023
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002019
Iteration 32/1000 | Loss: 0.00002013
Iteration 33/1000 | Loss: 0.00003182
Iteration 34/1000 | Loss: 0.00002806
Iteration 35/1000 | Loss: 0.00002121
Iteration 36/1000 | Loss: 0.00002061
Iteration 37/1000 | Loss: 0.00002929
Iteration 38/1000 | Loss: 0.00002571
Iteration 39/1000 | Loss: 0.00003257
Iteration 40/1000 | Loss: 0.00002523
Iteration 41/1000 | Loss: 0.00002097
Iteration 42/1000 | Loss: 0.00002074
Iteration 43/1000 | Loss: 0.00002046
Iteration 44/1000 | Loss: 0.00002998
Iteration 45/1000 | Loss: 0.00002018
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001984
Iteration 49/1000 | Loss: 0.00001983
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001981
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001970
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001964
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00001963
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001961
Iteration 70/1000 | Loss: 0.00001961
Iteration 71/1000 | Loss: 0.00001961
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001960
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001957
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001956
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001956
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.9561659428291023e-05, 1.9561659428291023e-05, 1.9561659428291023e-05, 1.9561659428291023e-05, 1.9561659428291023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9561659428291023e-05

Optimization complete. Final v2v error: 3.836320400238037 mm

Highest mean error: 5.4890289306640625 mm for frame 166

Lowest mean error: 3.076294422149658 mm for frame 238

Saving results

Total time: 95.1167585849762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553577
Iteration 2/25 | Loss: 0.00096457
Iteration 3/25 | Loss: 0.00086396
Iteration 4/25 | Loss: 0.00084369
Iteration 5/25 | Loss: 0.00083596
Iteration 6/25 | Loss: 0.00083475
Iteration 7/25 | Loss: 0.00083475
Iteration 8/25 | Loss: 0.00083475
Iteration 9/25 | Loss: 0.00083475
Iteration 10/25 | Loss: 0.00083475
Iteration 11/25 | Loss: 0.00083475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008347510010935366, 0.0008347510010935366, 0.0008347510010935366, 0.0008347510010935366, 0.0008347510010935366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008347510010935366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 18.86557770
Iteration 2/25 | Loss: 0.00180663
Iteration 3/25 | Loss: 0.00180655
Iteration 4/25 | Loss: 0.00180655
Iteration 5/25 | Loss: 0.00180655
Iteration 6/25 | Loss: 0.00180655
Iteration 7/25 | Loss: 0.00180655
Iteration 8/25 | Loss: 0.00180655
Iteration 9/25 | Loss: 0.00180655
Iteration 10/25 | Loss: 0.00180655
Iteration 11/25 | Loss: 0.00180655
Iteration 12/25 | Loss: 0.00180655
Iteration 13/25 | Loss: 0.00180655
Iteration 14/25 | Loss: 0.00180655
Iteration 15/25 | Loss: 0.00180655
Iteration 16/25 | Loss: 0.00180655
Iteration 17/25 | Loss: 0.00180655
Iteration 18/25 | Loss: 0.00180655
Iteration 19/25 | Loss: 0.00180655
Iteration 20/25 | Loss: 0.00180655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018065472831949592, 0.0018065472831949592, 0.0018065472831949592, 0.0018065472831949592, 0.0018065472831949592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018065472831949592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180655
Iteration 2/1000 | Loss: 0.00002601
Iteration 3/1000 | Loss: 0.00001733
Iteration 4/1000 | Loss: 0.00001618
Iteration 5/1000 | Loss: 0.00001530
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001496
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001468
Iteration 10/1000 | Loss: 0.00001467
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001443
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001441
Iteration 47/1000 | Loss: 0.00001440
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001440
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001439
Iteration 61/1000 | Loss: 0.00001439
Iteration 62/1000 | Loss: 0.00001439
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001437
Iteration 99/1000 | Loss: 0.00001437
Iteration 100/1000 | Loss: 0.00001437
Iteration 101/1000 | Loss: 0.00001437
Iteration 102/1000 | Loss: 0.00001437
Iteration 103/1000 | Loss: 0.00001437
Iteration 104/1000 | Loss: 0.00001437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4373269550560508e-05, 1.4373269550560508e-05, 1.4373269550560508e-05, 1.4373269550560508e-05, 1.4373269550560508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4373269550560508e-05

Optimization complete. Final v2v error: 3.3263840675354004 mm

Highest mean error: 3.5943448543548584 mm for frame 137

Lowest mean error: 2.991598606109619 mm for frame 71

Saving results

Total time: 31.424211740493774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771525
Iteration 2/25 | Loss: 0.00131276
Iteration 3/25 | Loss: 0.00101848
Iteration 4/25 | Loss: 0.00097589
Iteration 5/25 | Loss: 0.00096222
Iteration 6/25 | Loss: 0.00095858
Iteration 7/25 | Loss: 0.00095667
Iteration 8/25 | Loss: 0.00094804
Iteration 9/25 | Loss: 0.00093777
Iteration 10/25 | Loss: 0.00093321
Iteration 11/25 | Loss: 0.00093620
Iteration 12/25 | Loss: 0.00093220
Iteration 13/25 | Loss: 0.00092862
Iteration 14/25 | Loss: 0.00092666
Iteration 15/25 | Loss: 0.00092811
Iteration 16/25 | Loss: 0.00092462
Iteration 17/25 | Loss: 0.00092403
Iteration 18/25 | Loss: 0.00092385
Iteration 19/25 | Loss: 0.00092383
Iteration 20/25 | Loss: 0.00092383
Iteration 21/25 | Loss: 0.00092383
Iteration 22/25 | Loss: 0.00092383
Iteration 23/25 | Loss: 0.00092383
Iteration 24/25 | Loss: 0.00092383
Iteration 25/25 | Loss: 0.00092383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60387385
Iteration 2/25 | Loss: 0.00192237
Iteration 3/25 | Loss: 0.00192237
Iteration 4/25 | Loss: 0.00192237
Iteration 5/25 | Loss: 0.00192237
Iteration 6/25 | Loss: 0.00192237
Iteration 7/25 | Loss: 0.00192237
Iteration 8/25 | Loss: 0.00192237
Iteration 9/25 | Loss: 0.00192237
Iteration 10/25 | Loss: 0.00192237
Iteration 11/25 | Loss: 0.00192237
Iteration 12/25 | Loss: 0.00192237
Iteration 13/25 | Loss: 0.00192237
Iteration 14/25 | Loss: 0.00192237
Iteration 15/25 | Loss: 0.00192237
Iteration 16/25 | Loss: 0.00192237
Iteration 17/25 | Loss: 0.00192237
Iteration 18/25 | Loss: 0.00192237
Iteration 19/25 | Loss: 0.00192237
Iteration 20/25 | Loss: 0.00192237
Iteration 21/25 | Loss: 0.00192237
Iteration 22/25 | Loss: 0.00192237
Iteration 23/25 | Loss: 0.00192237
Iteration 24/25 | Loss: 0.00192237
Iteration 25/25 | Loss: 0.00192237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192237
Iteration 2/1000 | Loss: 0.00004897
Iteration 3/1000 | Loss: 0.00003591
Iteration 4/1000 | Loss: 0.00003258
Iteration 5/1000 | Loss: 0.00003066
Iteration 6/1000 | Loss: 0.00002924
Iteration 7/1000 | Loss: 0.00002806
Iteration 8/1000 | Loss: 0.00002721
Iteration 9/1000 | Loss: 0.00046422
Iteration 10/1000 | Loss: 0.00050929
Iteration 11/1000 | Loss: 0.00024389
Iteration 12/1000 | Loss: 0.00039423
Iteration 13/1000 | Loss: 0.00108604
Iteration 14/1000 | Loss: 0.00017256
Iteration 15/1000 | Loss: 0.00027267
Iteration 16/1000 | Loss: 0.00047388
Iteration 17/1000 | Loss: 0.00030908
Iteration 18/1000 | Loss: 0.00003222
Iteration 19/1000 | Loss: 0.00002770
Iteration 20/1000 | Loss: 0.00002440
Iteration 21/1000 | Loss: 0.00002227
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002042
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00001981
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001944
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001939
Iteration 32/1000 | Loss: 0.00001939
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001925
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00026899
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002094
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001742
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001731
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001726
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001724
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001716
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001712
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001711
Iteration 107/1000 | Loss: 0.00001710
Iteration 108/1000 | Loss: 0.00001710
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001710
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001708
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001707
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001706
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001704
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Iteration 155/1000 | Loss: 0.00001704
Iteration 156/1000 | Loss: 0.00001704
Iteration 157/1000 | Loss: 0.00001704
Iteration 158/1000 | Loss: 0.00001704
Iteration 159/1000 | Loss: 0.00001704
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Iteration 165/1000 | Loss: 0.00001703
Iteration 166/1000 | Loss: 0.00001703
Iteration 167/1000 | Loss: 0.00001703
Iteration 168/1000 | Loss: 0.00001703
Iteration 169/1000 | Loss: 0.00001703
Iteration 170/1000 | Loss: 0.00001703
Iteration 171/1000 | Loss: 0.00001703
Iteration 172/1000 | Loss: 0.00001703
Iteration 173/1000 | Loss: 0.00001703
Iteration 174/1000 | Loss: 0.00001703
Iteration 175/1000 | Loss: 0.00001703
Iteration 176/1000 | Loss: 0.00001703
Iteration 177/1000 | Loss: 0.00001703
Iteration 178/1000 | Loss: 0.00001703
Iteration 179/1000 | Loss: 0.00001703
Iteration 180/1000 | Loss: 0.00001703
Iteration 181/1000 | Loss: 0.00001703
Iteration 182/1000 | Loss: 0.00001702
Iteration 183/1000 | Loss: 0.00001702
Iteration 184/1000 | Loss: 0.00001702
Iteration 185/1000 | Loss: 0.00001702
Iteration 186/1000 | Loss: 0.00001702
Iteration 187/1000 | Loss: 0.00001702
Iteration 188/1000 | Loss: 0.00001702
Iteration 189/1000 | Loss: 0.00001702
Iteration 190/1000 | Loss: 0.00001702
Iteration 191/1000 | Loss: 0.00001702
Iteration 192/1000 | Loss: 0.00001702
Iteration 193/1000 | Loss: 0.00001702
Iteration 194/1000 | Loss: 0.00001702
Iteration 195/1000 | Loss: 0.00001702
Iteration 196/1000 | Loss: 0.00001702
Iteration 197/1000 | Loss: 0.00001702
Iteration 198/1000 | Loss: 0.00001702
Iteration 199/1000 | Loss: 0.00001702
Iteration 200/1000 | Loss: 0.00001702
Iteration 201/1000 | Loss: 0.00001702
Iteration 202/1000 | Loss: 0.00001702
Iteration 203/1000 | Loss: 0.00001702
Iteration 204/1000 | Loss: 0.00001702
Iteration 205/1000 | Loss: 0.00001702
Iteration 206/1000 | Loss: 0.00001702
Iteration 207/1000 | Loss: 0.00001702
Iteration 208/1000 | Loss: 0.00001702
Iteration 209/1000 | Loss: 0.00001702
Iteration 210/1000 | Loss: 0.00001702
Iteration 211/1000 | Loss: 0.00001702
Iteration 212/1000 | Loss: 0.00001702
Iteration 213/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.70171661011409e-05, 1.70171661011409e-05, 1.70171661011409e-05, 1.70171661011409e-05, 1.70171661011409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.70171661011409e-05

Optimization complete. Final v2v error: 3.532557725906372 mm

Highest mean error: 4.426033973693848 mm for frame 178

Lowest mean error: 3.0259954929351807 mm for frame 79

Saving results

Total time: 109.69076299667358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0068/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0068/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121350
Iteration 2/25 | Loss: 0.00263585
Iteration 3/25 | Loss: 0.00158731
Iteration 4/25 | Loss: 0.00137991
Iteration 5/25 | Loss: 0.00122012
Iteration 6/25 | Loss: 0.00116327
Iteration 7/25 | Loss: 0.00110756
Iteration 8/25 | Loss: 0.00108475
Iteration 9/25 | Loss: 0.00108543
Iteration 10/25 | Loss: 0.00110240
Iteration 11/25 | Loss: 0.00110959
Iteration 12/25 | Loss: 0.00109668
Iteration 13/25 | Loss: 0.00108631
Iteration 14/25 | Loss: 0.00108639
Iteration 15/25 | Loss: 0.00107796
Iteration 16/25 | Loss: 0.00107981
Iteration 17/25 | Loss: 0.00107712
Iteration 18/25 | Loss: 0.00105253
Iteration 19/25 | Loss: 0.00103358
Iteration 20/25 | Loss: 0.00102977
Iteration 21/25 | Loss: 0.00103452
Iteration 22/25 | Loss: 0.00101407
Iteration 23/25 | Loss: 0.00101387
Iteration 24/25 | Loss: 0.00101661
Iteration 25/25 | Loss: 0.00101751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.66760588
Iteration 2/25 | Loss: 0.00495471
Iteration 3/25 | Loss: 0.00422915
Iteration 4/25 | Loss: 0.00422914
Iteration 5/25 | Loss: 0.00422914
Iteration 6/25 | Loss: 0.00422914
Iteration 7/25 | Loss: 0.00422914
Iteration 8/25 | Loss: 0.00422914
Iteration 9/25 | Loss: 0.00422914
Iteration 10/25 | Loss: 0.00422914
Iteration 11/25 | Loss: 0.00422914
Iteration 12/25 | Loss: 0.00422914
Iteration 13/25 | Loss: 0.00422914
Iteration 14/25 | Loss: 0.00422914
Iteration 15/25 | Loss: 0.00422914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0042291427962481976, 0.0042291427962481976, 0.0042291427962481976, 0.0042291427962481976, 0.0042291427962481976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0042291427962481976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00422914
Iteration 2/1000 | Loss: 0.00229571
Iteration 3/1000 | Loss: 0.00288859
Iteration 4/1000 | Loss: 0.00231775
Iteration 5/1000 | Loss: 0.00231310
Iteration 6/1000 | Loss: 0.00230243
Iteration 7/1000 | Loss: 0.00284938
Iteration 8/1000 | Loss: 0.00303912
Iteration 9/1000 | Loss: 0.00198302
Iteration 10/1000 | Loss: 0.00273679
Iteration 11/1000 | Loss: 0.00225315
Iteration 12/1000 | Loss: 0.00215813
Iteration 13/1000 | Loss: 0.00326394
Iteration 14/1000 | Loss: 0.00200264
Iteration 15/1000 | Loss: 0.00178627
Iteration 16/1000 | Loss: 0.00239316
Iteration 17/1000 | Loss: 0.00231122
Iteration 18/1000 | Loss: 0.00216876
Iteration 19/1000 | Loss: 0.00247283
Iteration 20/1000 | Loss: 0.00298557
Iteration 21/1000 | Loss: 0.00181812
Iteration 22/1000 | Loss: 0.00241108
Iteration 23/1000 | Loss: 0.00205708
Iteration 24/1000 | Loss: 0.00369153
Iteration 25/1000 | Loss: 0.00213190
Iteration 26/1000 | Loss: 0.00197083
Iteration 27/1000 | Loss: 0.00214105
Iteration 28/1000 | Loss: 0.00197334
Iteration 29/1000 | Loss: 0.00168538
Iteration 30/1000 | Loss: 0.00180757
Iteration 31/1000 | Loss: 0.00176577
Iteration 32/1000 | Loss: 0.00245610
Iteration 33/1000 | Loss: 0.00345763
Iteration 34/1000 | Loss: 0.00214252
Iteration 35/1000 | Loss: 0.00305500
Iteration 36/1000 | Loss: 0.00339876
Iteration 37/1000 | Loss: 0.00249279
Iteration 38/1000 | Loss: 0.00294995
Iteration 39/1000 | Loss: 0.00180956
Iteration 40/1000 | Loss: 0.00163272
Iteration 41/1000 | Loss: 0.00173043
Iteration 42/1000 | Loss: 0.00199684
Iteration 43/1000 | Loss: 0.00166234
Iteration 44/1000 | Loss: 0.00160857
Iteration 45/1000 | Loss: 0.00140747
Iteration 46/1000 | Loss: 0.00185430
Iteration 47/1000 | Loss: 0.00121619
Iteration 48/1000 | Loss: 0.00131257
Iteration 49/1000 | Loss: 0.00122487
Iteration 50/1000 | Loss: 0.00096021
Iteration 51/1000 | Loss: 0.00101097
Iteration 52/1000 | Loss: 0.00119306
Iteration 53/1000 | Loss: 0.00090523
Iteration 54/1000 | Loss: 0.00073142
Iteration 55/1000 | Loss: 0.00071751
Iteration 56/1000 | Loss: 0.00256203
Iteration 57/1000 | Loss: 0.00100694
Iteration 58/1000 | Loss: 0.00116190
Iteration 59/1000 | Loss: 0.00147399
Iteration 60/1000 | Loss: 0.00198645
Iteration 61/1000 | Loss: 0.00167331
Iteration 62/1000 | Loss: 0.00096566
Iteration 63/1000 | Loss: 0.00073262
Iteration 64/1000 | Loss: 0.00053727
Iteration 65/1000 | Loss: 0.00061706
Iteration 66/1000 | Loss: 0.00147519
Iteration 67/1000 | Loss: 0.00099049
Iteration 68/1000 | Loss: 0.00151475
Iteration 69/1000 | Loss: 0.00099840
Iteration 70/1000 | Loss: 0.00075813
Iteration 71/1000 | Loss: 0.00089653
Iteration 72/1000 | Loss: 0.00075685
Iteration 73/1000 | Loss: 0.00042915
Iteration 74/1000 | Loss: 0.00075623
Iteration 75/1000 | Loss: 0.00055790
Iteration 76/1000 | Loss: 0.00085126
Iteration 77/1000 | Loss: 0.00091295
Iteration 78/1000 | Loss: 0.00170954
Iteration 79/1000 | Loss: 0.00125482
Iteration 80/1000 | Loss: 0.00060242
Iteration 81/1000 | Loss: 0.00085870
Iteration 82/1000 | Loss: 0.00083941
Iteration 83/1000 | Loss: 0.00077995
Iteration 84/1000 | Loss: 0.00075478
Iteration 85/1000 | Loss: 0.00058109
Iteration 86/1000 | Loss: 0.00043833
Iteration 87/1000 | Loss: 0.00024089
Iteration 88/1000 | Loss: 0.00022035
Iteration 89/1000 | Loss: 0.00043206
Iteration 90/1000 | Loss: 0.00081730
Iteration 91/1000 | Loss: 0.00053609
Iteration 92/1000 | Loss: 0.00059853
Iteration 93/1000 | Loss: 0.00031579
Iteration 94/1000 | Loss: 0.00028025
Iteration 95/1000 | Loss: 0.00025714
Iteration 96/1000 | Loss: 0.00049436
Iteration 97/1000 | Loss: 0.00058799
Iteration 98/1000 | Loss: 0.00017643
Iteration 99/1000 | Loss: 0.00065764
Iteration 100/1000 | Loss: 0.00029083
Iteration 101/1000 | Loss: 0.00038354
Iteration 102/1000 | Loss: 0.00041771
Iteration 103/1000 | Loss: 0.00027999
Iteration 104/1000 | Loss: 0.00020919
Iteration 105/1000 | Loss: 0.00061232
Iteration 106/1000 | Loss: 0.00035285
Iteration 107/1000 | Loss: 0.00019815
Iteration 108/1000 | Loss: 0.00052785
Iteration 109/1000 | Loss: 0.00036789
Iteration 110/1000 | Loss: 0.00013389
Iteration 111/1000 | Loss: 0.00037264
Iteration 112/1000 | Loss: 0.00040262
Iteration 113/1000 | Loss: 0.00023785
Iteration 114/1000 | Loss: 0.00070072
Iteration 115/1000 | Loss: 0.00068078
Iteration 116/1000 | Loss: 0.00054171
Iteration 117/1000 | Loss: 0.00059448
Iteration 118/1000 | Loss: 0.00100001
Iteration 119/1000 | Loss: 0.00139008
Iteration 120/1000 | Loss: 0.00217838
Iteration 121/1000 | Loss: 0.00112081
Iteration 122/1000 | Loss: 0.00173287
Iteration 123/1000 | Loss: 0.00212682
Iteration 124/1000 | Loss: 0.00167447
Iteration 125/1000 | Loss: 0.00091252
Iteration 126/1000 | Loss: 0.00055514
Iteration 127/1000 | Loss: 0.00072719
Iteration 128/1000 | Loss: 0.00052189
Iteration 129/1000 | Loss: 0.00030862
Iteration 130/1000 | Loss: 0.00026645
Iteration 131/1000 | Loss: 0.00044040
Iteration 132/1000 | Loss: 0.00028575
Iteration 133/1000 | Loss: 0.00010388
Iteration 134/1000 | Loss: 0.00024896
Iteration 135/1000 | Loss: 0.00023106
Iteration 136/1000 | Loss: 0.00012338
Iteration 137/1000 | Loss: 0.00008914
Iteration 138/1000 | Loss: 0.00012953
Iteration 139/1000 | Loss: 0.00024237
Iteration 140/1000 | Loss: 0.00049888
Iteration 141/1000 | Loss: 0.00033700
Iteration 142/1000 | Loss: 0.00059429
Iteration 143/1000 | Loss: 0.00051957
Iteration 144/1000 | Loss: 0.00072397
Iteration 145/1000 | Loss: 0.00194528
Iteration 146/1000 | Loss: 0.00014817
Iteration 147/1000 | Loss: 0.00034491
Iteration 148/1000 | Loss: 0.00030492
Iteration 149/1000 | Loss: 0.00042700
Iteration 150/1000 | Loss: 0.00009535
Iteration 151/1000 | Loss: 0.00030896
Iteration 152/1000 | Loss: 0.00028871
Iteration 153/1000 | Loss: 0.00010013
Iteration 154/1000 | Loss: 0.00059867
Iteration 155/1000 | Loss: 0.00059441
Iteration 156/1000 | Loss: 0.00108902
Iteration 157/1000 | Loss: 0.00060205
Iteration 158/1000 | Loss: 0.00101585
Iteration 159/1000 | Loss: 0.00037755
Iteration 160/1000 | Loss: 0.00089043
Iteration 161/1000 | Loss: 0.00045769
Iteration 162/1000 | Loss: 0.00038539
Iteration 163/1000 | Loss: 0.00013977
Iteration 164/1000 | Loss: 0.00020740
Iteration 165/1000 | Loss: 0.00010922
Iteration 166/1000 | Loss: 0.00015254
Iteration 167/1000 | Loss: 0.00038480
Iteration 168/1000 | Loss: 0.00056153
Iteration 169/1000 | Loss: 0.00013050
Iteration 170/1000 | Loss: 0.00013793
Iteration 171/1000 | Loss: 0.00022183
Iteration 172/1000 | Loss: 0.00032954
Iteration 173/1000 | Loss: 0.00053810
Iteration 174/1000 | Loss: 0.00035659
Iteration 175/1000 | Loss: 0.00082342
Iteration 176/1000 | Loss: 0.00015909
Iteration 177/1000 | Loss: 0.00013560
Iteration 178/1000 | Loss: 0.00009195
Iteration 179/1000 | Loss: 0.00005102
Iteration 180/1000 | Loss: 0.00013317
Iteration 181/1000 | Loss: 0.00004002
Iteration 182/1000 | Loss: 0.00024441
Iteration 183/1000 | Loss: 0.00055156
Iteration 184/1000 | Loss: 0.00038009
Iteration 185/1000 | Loss: 0.00078658
Iteration 186/1000 | Loss: 0.00034655
Iteration 187/1000 | Loss: 0.00072203
Iteration 188/1000 | Loss: 0.00062008
Iteration 189/1000 | Loss: 0.00065811
Iteration 190/1000 | Loss: 0.00080890
Iteration 191/1000 | Loss: 0.00109970
Iteration 192/1000 | Loss: 0.00052514
Iteration 193/1000 | Loss: 0.00023874
Iteration 194/1000 | Loss: 0.00027222
Iteration 195/1000 | Loss: 0.00030157
Iteration 196/1000 | Loss: 0.00034493
Iteration 197/1000 | Loss: 0.00017175
Iteration 198/1000 | Loss: 0.00032917
Iteration 199/1000 | Loss: 0.00028008
Iteration 200/1000 | Loss: 0.00029759
Iteration 201/1000 | Loss: 0.00020356
Iteration 202/1000 | Loss: 0.00032810
Iteration 203/1000 | Loss: 0.00036311
Iteration 204/1000 | Loss: 0.00035959
Iteration 205/1000 | Loss: 0.00067463
Iteration 206/1000 | Loss: 0.00032536
Iteration 207/1000 | Loss: 0.00046434
Iteration 208/1000 | Loss: 0.00052976
Iteration 209/1000 | Loss: 0.00200994
Iteration 210/1000 | Loss: 0.00061072
Iteration 211/1000 | Loss: 0.00040423
Iteration 212/1000 | Loss: 0.00033051
Iteration 213/1000 | Loss: 0.00030876
Iteration 214/1000 | Loss: 0.00024618
Iteration 215/1000 | Loss: 0.00031457
Iteration 216/1000 | Loss: 0.00031144
Iteration 217/1000 | Loss: 0.00038550
Iteration 218/1000 | Loss: 0.00035861
Iteration 219/1000 | Loss: 0.00035680
Iteration 220/1000 | Loss: 0.00022308
Iteration 221/1000 | Loss: 0.00028851
Iteration 222/1000 | Loss: 0.00028602
Iteration 223/1000 | Loss: 0.00017372
Iteration 224/1000 | Loss: 0.00015687
Iteration 225/1000 | Loss: 0.00024632
Iteration 226/1000 | Loss: 0.00013001
Iteration 227/1000 | Loss: 0.00028902
Iteration 228/1000 | Loss: 0.00011640
Iteration 229/1000 | Loss: 0.00063715
Iteration 230/1000 | Loss: 0.00062926
Iteration 231/1000 | Loss: 0.00040323
Iteration 232/1000 | Loss: 0.00056142
Iteration 233/1000 | Loss: 0.00046422
Iteration 234/1000 | Loss: 0.00037492
Iteration 235/1000 | Loss: 0.00042878
Iteration 236/1000 | Loss: 0.00030687
Iteration 237/1000 | Loss: 0.00023113
Iteration 238/1000 | Loss: 0.00020410
Iteration 239/1000 | Loss: 0.00036962
Iteration 240/1000 | Loss: 0.00041709
Iteration 241/1000 | Loss: 0.00046787
Iteration 242/1000 | Loss: 0.00033924
Iteration 243/1000 | Loss: 0.00027024
Iteration 244/1000 | Loss: 0.00048233
Iteration 245/1000 | Loss: 0.00052868
Iteration 246/1000 | Loss: 0.00021268
Iteration 247/1000 | Loss: 0.00050966
Iteration 248/1000 | Loss: 0.00033433
Iteration 249/1000 | Loss: 0.00035568
Iteration 250/1000 | Loss: 0.00029041
Iteration 251/1000 | Loss: 0.00052905
Iteration 252/1000 | Loss: 0.00030583
Iteration 253/1000 | Loss: 0.00020038
Iteration 254/1000 | Loss: 0.00004950
Iteration 255/1000 | Loss: 0.00006282
Iteration 256/1000 | Loss: 0.00036633
Iteration 257/1000 | Loss: 0.00026886
Iteration 258/1000 | Loss: 0.00006260
Iteration 259/1000 | Loss: 0.00027327
Iteration 260/1000 | Loss: 0.00032961
Iteration 261/1000 | Loss: 0.00011506
Iteration 262/1000 | Loss: 0.00017221
Iteration 263/1000 | Loss: 0.00014439
Iteration 264/1000 | Loss: 0.00011110
Iteration 265/1000 | Loss: 0.00011118
Iteration 266/1000 | Loss: 0.00017253
Iteration 267/1000 | Loss: 0.00011623
Iteration 268/1000 | Loss: 0.00010905
Iteration 269/1000 | Loss: 0.00011343
Iteration 270/1000 | Loss: 0.00021609
Iteration 271/1000 | Loss: 0.00028176
Iteration 272/1000 | Loss: 0.00022725
Iteration 273/1000 | Loss: 0.00043203
Iteration 274/1000 | Loss: 0.00042296
Iteration 275/1000 | Loss: 0.00026976
Iteration 276/1000 | Loss: 0.00024392
Iteration 277/1000 | Loss: 0.00007375
Iteration 278/1000 | Loss: 0.00014390
Iteration 279/1000 | Loss: 0.00007575
Iteration 280/1000 | Loss: 0.00015453
Iteration 281/1000 | Loss: 0.00024697
Iteration 282/1000 | Loss: 0.00042146
Iteration 283/1000 | Loss: 0.00025183
Iteration 284/1000 | Loss: 0.00008075
Iteration 285/1000 | Loss: 0.00032509
Iteration 286/1000 | Loss: 0.00024350
Iteration 287/1000 | Loss: 0.00028264
Iteration 288/1000 | Loss: 0.00058962
Iteration 289/1000 | Loss: 0.00031135
Iteration 290/1000 | Loss: 0.00023803
Iteration 291/1000 | Loss: 0.00022360
Iteration 292/1000 | Loss: 0.00020083
Iteration 293/1000 | Loss: 0.00018132
Iteration 294/1000 | Loss: 0.00023988
Iteration 295/1000 | Loss: 0.00015750
Iteration 296/1000 | Loss: 0.00042974
Iteration 297/1000 | Loss: 0.00042849
Iteration 298/1000 | Loss: 0.00020590
Iteration 299/1000 | Loss: 0.00009051
Iteration 300/1000 | Loss: 0.00010738
Iteration 301/1000 | Loss: 0.00027682
Iteration 302/1000 | Loss: 0.00019399
Iteration 303/1000 | Loss: 0.00025642
Iteration 304/1000 | Loss: 0.00021917
Iteration 305/1000 | Loss: 0.00020351
Iteration 306/1000 | Loss: 0.00026299
Iteration 307/1000 | Loss: 0.00026262
Iteration 308/1000 | Loss: 0.00022303
Iteration 309/1000 | Loss: 0.00040560
Iteration 310/1000 | Loss: 0.00029543
Iteration 311/1000 | Loss: 0.00047494
Iteration 312/1000 | Loss: 0.00034820
Iteration 313/1000 | Loss: 0.00042164
Iteration 314/1000 | Loss: 0.00040236
Iteration 315/1000 | Loss: 0.00081354
Iteration 316/1000 | Loss: 0.00061497
Iteration 317/1000 | Loss: 0.00050589
Iteration 318/1000 | Loss: 0.00052264
Iteration 319/1000 | Loss: 0.00045467
Iteration 320/1000 | Loss: 0.00039437
Iteration 321/1000 | Loss: 0.00034646
Iteration 322/1000 | Loss: 0.00034557
Iteration 323/1000 | Loss: 0.00011097
Iteration 324/1000 | Loss: 0.00020697
Iteration 325/1000 | Loss: 0.00070695
Iteration 326/1000 | Loss: 0.00018298
Iteration 327/1000 | Loss: 0.00031688
Iteration 328/1000 | Loss: 0.00068599
Iteration 329/1000 | Loss: 0.00039281
Iteration 330/1000 | Loss: 0.00009176
Iteration 331/1000 | Loss: 0.00004184
Iteration 332/1000 | Loss: 0.00004146
Iteration 333/1000 | Loss: 0.00022771
Iteration 334/1000 | Loss: 0.00014896
Iteration 335/1000 | Loss: 0.00023205
Iteration 336/1000 | Loss: 0.00009508
Iteration 337/1000 | Loss: 0.00024509
Iteration 338/1000 | Loss: 0.00013018
Iteration 339/1000 | Loss: 0.00021622
Iteration 340/1000 | Loss: 0.00019472
Iteration 341/1000 | Loss: 0.00005924
Iteration 342/1000 | Loss: 0.00027197
Iteration 343/1000 | Loss: 0.00025360
Iteration 344/1000 | Loss: 0.00038292
Iteration 345/1000 | Loss: 0.00003811
Iteration 346/1000 | Loss: 0.00028059
Iteration 347/1000 | Loss: 0.00021616
Iteration 348/1000 | Loss: 0.00029106
Iteration 349/1000 | Loss: 0.00005161
Iteration 350/1000 | Loss: 0.00046141
Iteration 351/1000 | Loss: 0.00022119
Iteration 352/1000 | Loss: 0.00011853
Iteration 353/1000 | Loss: 0.00022550
Iteration 354/1000 | Loss: 0.00098723
Iteration 355/1000 | Loss: 0.00011804
Iteration 356/1000 | Loss: 0.00016411
Iteration 357/1000 | Loss: 0.00027580
Iteration 358/1000 | Loss: 0.00065565
Iteration 359/1000 | Loss: 0.00018038
Iteration 360/1000 | Loss: 0.00045597
Iteration 361/1000 | Loss: 0.00009625
Iteration 362/1000 | Loss: 0.00024615
Iteration 363/1000 | Loss: 0.00003091
Iteration 364/1000 | Loss: 0.00016413
Iteration 365/1000 | Loss: 0.00019352
Iteration 366/1000 | Loss: 0.00017339
Iteration 367/1000 | Loss: 0.00005689
Iteration 368/1000 | Loss: 0.00012578
Iteration 369/1000 | Loss: 0.00006496
Iteration 370/1000 | Loss: 0.00005272
Iteration 371/1000 | Loss: 0.00003666
Iteration 372/1000 | Loss: 0.00002726
Iteration 373/1000 | Loss: 0.00049872
Iteration 374/1000 | Loss: 0.00027403
Iteration 375/1000 | Loss: 0.00006516
Iteration 376/1000 | Loss: 0.00011488
Iteration 377/1000 | Loss: 0.00040815
Iteration 378/1000 | Loss: 0.00041726
Iteration 379/1000 | Loss: 0.00012867
Iteration 380/1000 | Loss: 0.00002817
Iteration 381/1000 | Loss: 0.00017745
Iteration 382/1000 | Loss: 0.00033206
Iteration 383/1000 | Loss: 0.00022648
Iteration 384/1000 | Loss: 0.00024253
Iteration 385/1000 | Loss: 0.00058529
Iteration 386/1000 | Loss: 0.00060448
Iteration 387/1000 | Loss: 0.00043990
Iteration 388/1000 | Loss: 0.00028777
Iteration 389/1000 | Loss: 0.00031276
Iteration 390/1000 | Loss: 0.00034838
Iteration 391/1000 | Loss: 0.00029886
Iteration 392/1000 | Loss: 0.00023601
Iteration 393/1000 | Loss: 0.00015138
Iteration 394/1000 | Loss: 0.00022255
Iteration 395/1000 | Loss: 0.00033266
Iteration 396/1000 | Loss: 0.00049489
Iteration 397/1000 | Loss: 0.00019826
Iteration 398/1000 | Loss: 0.00027550
Iteration 399/1000 | Loss: 0.00037468
Iteration 400/1000 | Loss: 0.00105200
Iteration 401/1000 | Loss: 0.00132627
Iteration 402/1000 | Loss: 0.00112229
Iteration 403/1000 | Loss: 0.00061684
Iteration 404/1000 | Loss: 0.00088016
Iteration 405/1000 | Loss: 0.00088570
Iteration 406/1000 | Loss: 0.00086308
Iteration 407/1000 | Loss: 0.00073096
Iteration 408/1000 | Loss: 0.00060945
Iteration 409/1000 | Loss: 0.00104911
Iteration 410/1000 | Loss: 0.00064696
Iteration 411/1000 | Loss: 0.00138550
Iteration 412/1000 | Loss: 0.00089406
Iteration 413/1000 | Loss: 0.00071034
Iteration 414/1000 | Loss: 0.00112212
Iteration 415/1000 | Loss: 0.00109286
Iteration 416/1000 | Loss: 0.00074030
Iteration 417/1000 | Loss: 0.00046068
Iteration 418/1000 | Loss: 0.00025954
Iteration 419/1000 | Loss: 0.00004079
Iteration 420/1000 | Loss: 0.00025628
Iteration 421/1000 | Loss: 0.00043209
Iteration 422/1000 | Loss: 0.00040072
Iteration 423/1000 | Loss: 0.00021996
Iteration 424/1000 | Loss: 0.00024446
Iteration 425/1000 | Loss: 0.00017442
Iteration 426/1000 | Loss: 0.00004160
Iteration 427/1000 | Loss: 0.00003300
Iteration 428/1000 | Loss: 0.00016202
Iteration 429/1000 | Loss: 0.00018712
Iteration 430/1000 | Loss: 0.00036512
Iteration 431/1000 | Loss: 0.00038604
Iteration 432/1000 | Loss: 0.00029069
Iteration 433/1000 | Loss: 0.00043465
Iteration 434/1000 | Loss: 0.00022740
Iteration 435/1000 | Loss: 0.00024796
Iteration 436/1000 | Loss: 0.00021453
Iteration 437/1000 | Loss: 0.00022733
Iteration 438/1000 | Loss: 0.00008750
Iteration 439/1000 | Loss: 0.00023419
Iteration 440/1000 | Loss: 0.00012014
Iteration 441/1000 | Loss: 0.00010245
Iteration 442/1000 | Loss: 0.00009652
Iteration 443/1000 | Loss: 0.00002627
Iteration 444/1000 | Loss: 0.00002929
Iteration 445/1000 | Loss: 0.00005826
Iteration 446/1000 | Loss: 0.00009753
Iteration 447/1000 | Loss: 0.00002818
Iteration 448/1000 | Loss: 0.00003856
Iteration 449/1000 | Loss: 0.00002447
Iteration 450/1000 | Loss: 0.00002170
Iteration 451/1000 | Loss: 0.00002058
Iteration 452/1000 | Loss: 0.00020652
Iteration 453/1000 | Loss: 0.00016448
Iteration 454/1000 | Loss: 0.00018762
Iteration 455/1000 | Loss: 0.00041614
Iteration 456/1000 | Loss: 0.00033969
Iteration 457/1000 | Loss: 0.00004944
Iteration 458/1000 | Loss: 0.00003104
Iteration 459/1000 | Loss: 0.00008354
Iteration 460/1000 | Loss: 0.00002772
Iteration 461/1000 | Loss: 0.00002230
Iteration 462/1000 | Loss: 0.00002113
Iteration 463/1000 | Loss: 0.00002019
Iteration 464/1000 | Loss: 0.00002616
Iteration 465/1000 | Loss: 0.00001928
Iteration 466/1000 | Loss: 0.00002164
Iteration 467/1000 | Loss: 0.00001918
Iteration 468/1000 | Loss: 0.00001881
Iteration 469/1000 | Loss: 0.00001877
Iteration 470/1000 | Loss: 0.00009407
Iteration 471/1000 | Loss: 0.00070968
Iteration 472/1000 | Loss: 0.00021467
Iteration 473/1000 | Loss: 0.00010359
Iteration 474/1000 | Loss: 0.00004316
Iteration 475/1000 | Loss: 0.00002344
Iteration 476/1000 | Loss: 0.00001849
Iteration 477/1000 | Loss: 0.00001847
Iteration 478/1000 | Loss: 0.00003171
Iteration 479/1000 | Loss: 0.00001842
Iteration 480/1000 | Loss: 0.00001840
Iteration 481/1000 | Loss: 0.00001834
Iteration 482/1000 | Loss: 0.00001834
Iteration 483/1000 | Loss: 0.00001833
Iteration 484/1000 | Loss: 0.00001829
Iteration 485/1000 | Loss: 0.00002120
Iteration 486/1000 | Loss: 0.00002606
Iteration 487/1000 | Loss: 0.00008170
Iteration 488/1000 | Loss: 0.00005953
Iteration 489/1000 | Loss: 0.00016289
Iteration 490/1000 | Loss: 0.00010704
Iteration 491/1000 | Loss: 0.00015909
Iteration 492/1000 | Loss: 0.00010180
Iteration 493/1000 | Loss: 0.00002090
Iteration 494/1000 | Loss: 0.00012726
Iteration 495/1000 | Loss: 0.00005513
Iteration 496/1000 | Loss: 0.00006663
Iteration 497/1000 | Loss: 0.00023151
Iteration 498/1000 | Loss: 0.00054503
Iteration 499/1000 | Loss: 0.00028545
Iteration 500/1000 | Loss: 0.00027594
Iteration 501/1000 | Loss: 0.00110870
Iteration 502/1000 | Loss: 0.00080747
Iteration 503/1000 | Loss: 0.00044531
Iteration 504/1000 | Loss: 0.00085968
Iteration 505/1000 | Loss: 0.00031964
Iteration 506/1000 | Loss: 0.00008457
Iteration 507/1000 | Loss: 0.00015849
Iteration 508/1000 | Loss: 0.00023715
Iteration 509/1000 | Loss: 0.00005952
Iteration 510/1000 | Loss: 0.00015518
Iteration 511/1000 | Loss: 0.00004625
Iteration 512/1000 | Loss: 0.00002590
Iteration 513/1000 | Loss: 0.00002302
Iteration 514/1000 | Loss: 0.00012831
Iteration 515/1000 | Loss: 0.00011834
Iteration 516/1000 | Loss: 0.00002761
Iteration 517/1000 | Loss: 0.00016860
Iteration 518/1000 | Loss: 0.00026910
Iteration 519/1000 | Loss: 0.00044392
Iteration 520/1000 | Loss: 0.00009089
Iteration 521/1000 | Loss: 0.00014717
Iteration 522/1000 | Loss: 0.00007763
Iteration 523/1000 | Loss: 0.00008965
Iteration 524/1000 | Loss: 0.00015104
Iteration 525/1000 | Loss: 0.00022332
Iteration 526/1000 | Loss: 0.00048482
Iteration 527/1000 | Loss: 0.00025785
Iteration 528/1000 | Loss: 0.00011677
Iteration 529/1000 | Loss: 0.00017732
Iteration 530/1000 | Loss: 0.00038295
Iteration 531/1000 | Loss: 0.00023449
Iteration 532/1000 | Loss: 0.00020910
Iteration 533/1000 | Loss: 0.00008715
Iteration 534/1000 | Loss: 0.00003275
Iteration 535/1000 | Loss: 0.00047801
Iteration 536/1000 | Loss: 0.00055163
Iteration 537/1000 | Loss: 0.00026903
Iteration 538/1000 | Loss: 0.00017718
Iteration 539/1000 | Loss: 0.00042493
Iteration 540/1000 | Loss: 0.00047161
Iteration 541/1000 | Loss: 0.00043824
Iteration 542/1000 | Loss: 0.00042316
Iteration 543/1000 | Loss: 0.00038335
Iteration 544/1000 | Loss: 0.00036098
Iteration 545/1000 | Loss: 0.00022342
Iteration 546/1000 | Loss: 0.00022231
Iteration 547/1000 | Loss: 0.00059561
Iteration 548/1000 | Loss: 0.00013198
Iteration 549/1000 | Loss: 0.00032486
Iteration 550/1000 | Loss: 0.00032657
Iteration 551/1000 | Loss: 0.00037944
Iteration 552/1000 | Loss: 0.00028504
Iteration 553/1000 | Loss: 0.00023421
Iteration 554/1000 | Loss: 0.00014655
Iteration 555/1000 | Loss: 0.00006556
Iteration 556/1000 | Loss: 0.00016989
Iteration 557/1000 | Loss: 0.00025357
Iteration 558/1000 | Loss: 0.00014467
Iteration 559/1000 | Loss: 0.00006974
Iteration 560/1000 | Loss: 0.00012077
Iteration 561/1000 | Loss: 0.00013156
Iteration 562/1000 | Loss: 0.00011630
Iteration 563/1000 | Loss: 0.00012797
Iteration 564/1000 | Loss: 0.00008595
Iteration 565/1000 | Loss: 0.00002732
Iteration 566/1000 | Loss: 0.00005494
Iteration 567/1000 | Loss: 0.00013740
Iteration 568/1000 | Loss: 0.00006870
Iteration 569/1000 | Loss: 0.00006612
Iteration 570/1000 | Loss: 0.00020218
Iteration 571/1000 | Loss: 0.00006892
Iteration 572/1000 | Loss: 0.00020317
Iteration 573/1000 | Loss: 0.00038004
Iteration 574/1000 | Loss: 0.00044394
Iteration 575/1000 | Loss: 0.00027255
Iteration 576/1000 | Loss: 0.00020652
Iteration 577/1000 | Loss: 0.00013664
Iteration 578/1000 | Loss: 0.00021006
Iteration 579/1000 | Loss: 0.00017361
Iteration 580/1000 | Loss: 0.00012007
Iteration 581/1000 | Loss: 0.00019954
Iteration 582/1000 | Loss: 0.00036094
Iteration 583/1000 | Loss: 0.00032494
Iteration 584/1000 | Loss: 0.00027360
Iteration 585/1000 | Loss: 0.00006920
Iteration 586/1000 | Loss: 0.00020759
Iteration 587/1000 | Loss: 0.00012645
Iteration 588/1000 | Loss: 0.00016357
Iteration 589/1000 | Loss: 0.00013440
Iteration 590/1000 | Loss: 0.00012635
Iteration 591/1000 | Loss: 0.00013844
Iteration 592/1000 | Loss: 0.00004437
Iteration 593/1000 | Loss: 0.00011359
Iteration 594/1000 | Loss: 0.00010820
Iteration 595/1000 | Loss: 0.00015448
Iteration 596/1000 | Loss: 0.00028678
Iteration 597/1000 | Loss: 0.00019669
Iteration 598/1000 | Loss: 0.00015185
Iteration 599/1000 | Loss: 0.00011725
Iteration 600/1000 | Loss: 0.00007805
Iteration 601/1000 | Loss: 0.00018946
Iteration 602/1000 | Loss: 0.00041775
Iteration 603/1000 | Loss: 0.00018830
Iteration 604/1000 | Loss: 0.00014593
Iteration 605/1000 | Loss: 0.00008859
Iteration 606/1000 | Loss: 0.00023707
Iteration 607/1000 | Loss: 0.00016520
Iteration 608/1000 | Loss: 0.00011180
Iteration 609/1000 | Loss: 0.00011567
Iteration 610/1000 | Loss: 0.00033710
Iteration 611/1000 | Loss: 0.00010055
Iteration 612/1000 | Loss: 0.00017702
Iteration 613/1000 | Loss: 0.00023303
Iteration 614/1000 | Loss: 0.00036099
Iteration 615/1000 | Loss: 0.00021165
Iteration 616/1000 | Loss: 0.00011388
Iteration 617/1000 | Loss: 0.00007789
Iteration 618/1000 | Loss: 0.00011918
Iteration 619/1000 | Loss: 0.00013073
Iteration 620/1000 | Loss: 0.00005304
Iteration 621/1000 | Loss: 0.00007189
Iteration 622/1000 | Loss: 0.00019793
Iteration 623/1000 | Loss: 0.00038359
Iteration 624/1000 | Loss: 0.00023991
Iteration 625/1000 | Loss: 0.00023886
Iteration 626/1000 | Loss: 0.00018919
Iteration 627/1000 | Loss: 0.00006529
Iteration 628/1000 | Loss: 0.00015818
Iteration 629/1000 | Loss: 0.00016004
Iteration 630/1000 | Loss: 0.00010705
Iteration 631/1000 | Loss: 0.00014093
Iteration 632/1000 | Loss: 0.00011693
Iteration 633/1000 | Loss: 0.00018435
Iteration 634/1000 | Loss: 0.00008294
Iteration 635/1000 | Loss: 0.00021323
Iteration 636/1000 | Loss: 0.00010050
Iteration 637/1000 | Loss: 0.00011530
Iteration 638/1000 | Loss: 0.00008818
Iteration 639/1000 | Loss: 0.00006852
Iteration 640/1000 | Loss: 0.00010666
Iteration 641/1000 | Loss: 0.00007290
Iteration 642/1000 | Loss: 0.00006092
Iteration 643/1000 | Loss: 0.00011701
Iteration 644/1000 | Loss: 0.00005302
Iteration 645/1000 | Loss: 0.00008449
Iteration 646/1000 | Loss: 0.00002272
Iteration 647/1000 | Loss: 0.00019768
Iteration 648/1000 | Loss: 0.00009761
Iteration 649/1000 | Loss: 0.00011175
Iteration 650/1000 | Loss: 0.00002272
Iteration 651/1000 | Loss: 0.00002151
Iteration 652/1000 | Loss: 0.00011298
Iteration 653/1000 | Loss: 0.00008236
Iteration 654/1000 | Loss: 0.00003550
Iteration 655/1000 | Loss: 0.00007003
Iteration 656/1000 | Loss: 0.00010378
Iteration 657/1000 | Loss: 0.00007549
Iteration 658/1000 | Loss: 0.00008375
Iteration 659/1000 | Loss: 0.00006747
Iteration 660/1000 | Loss: 0.00008233
Iteration 661/1000 | Loss: 0.00021596
Iteration 662/1000 | Loss: 0.00018466
Iteration 663/1000 | Loss: 0.00011951
Iteration 664/1000 | Loss: 0.00002193
Iteration 665/1000 | Loss: 0.00002080
Iteration 666/1000 | Loss: 0.00035718
Iteration 667/1000 | Loss: 0.00034669
Iteration 668/1000 | Loss: 0.00033023
Iteration 669/1000 | Loss: 0.00026150
Iteration 670/1000 | Loss: 0.00031531
Iteration 671/1000 | Loss: 0.00025774
Iteration 672/1000 | Loss: 0.00031624
Iteration 673/1000 | Loss: 0.00030286
Iteration 674/1000 | Loss: 0.00064869
Iteration 675/1000 | Loss: 0.00028612
Iteration 676/1000 | Loss: 0.00027842
Iteration 677/1000 | Loss: 0.00018152
Iteration 678/1000 | Loss: 0.00034888
Iteration 679/1000 | Loss: 0.00013255
Iteration 680/1000 | Loss: 0.00006836
Iteration 681/1000 | Loss: 0.00024507
Iteration 682/1000 | Loss: 0.00037745
Iteration 683/1000 | Loss: 0.00014404
Iteration 684/1000 | Loss: 0.00034645
Iteration 685/1000 | Loss: 0.00006261
Iteration 686/1000 | Loss: 0.00011148
Iteration 687/1000 | Loss: 0.00011082
Iteration 688/1000 | Loss: 0.00026989
Iteration 689/1000 | Loss: 0.00020563
Iteration 690/1000 | Loss: 0.00002826
Iteration 691/1000 | Loss: 0.00002548
Iteration 692/1000 | Loss: 0.00004678
Iteration 693/1000 | Loss: 0.00005148
Iteration 694/1000 | Loss: 0.00005309
Iteration 695/1000 | Loss: 0.00004791
Iteration 696/1000 | Loss: 0.00003552
Iteration 697/1000 | Loss: 0.00002299
Iteration 698/1000 | Loss: 0.00002104
Iteration 699/1000 | Loss: 0.00008550
Iteration 700/1000 | Loss: 0.00043137
Iteration 701/1000 | Loss: 0.00030930
Iteration 702/1000 | Loss: 0.00016452
Iteration 703/1000 | Loss: 0.00023788
Iteration 704/1000 | Loss: 0.00017602
Iteration 705/1000 | Loss: 0.00107468
Iteration 706/1000 | Loss: 0.00057198
Iteration 707/1000 | Loss: 0.00021821
Iteration 708/1000 | Loss: 0.00002646
Iteration 709/1000 | Loss: 0.00004248
Iteration 710/1000 | Loss: 0.00002145
Iteration 711/1000 | Loss: 0.00003120
Iteration 712/1000 | Loss: 0.00017697
Iteration 713/1000 | Loss: 0.00050332
Iteration 714/1000 | Loss: 0.00043681
Iteration 715/1000 | Loss: 0.00036710
Iteration 716/1000 | Loss: 0.00016981
Iteration 717/1000 | Loss: 0.00006270
Iteration 718/1000 | Loss: 0.00014003
Iteration 719/1000 | Loss: 0.00013425
Iteration 720/1000 | Loss: 0.00035136
Iteration 721/1000 | Loss: 0.00011768
Iteration 722/1000 | Loss: 0.00024064
Iteration 723/1000 | Loss: 0.00021672
Iteration 724/1000 | Loss: 0.00014806
Iteration 725/1000 | Loss: 0.00006152
Iteration 726/1000 | Loss: 0.00002069
Iteration 727/1000 | Loss: 0.00007570
Iteration 728/1000 | Loss: 0.00003723
Iteration 729/1000 | Loss: 0.00002000
Iteration 730/1000 | Loss: 0.00007324
Iteration 731/1000 | Loss: 0.00010911
Iteration 732/1000 | Loss: 0.00012034
Iteration 733/1000 | Loss: 0.00004267
Iteration 734/1000 | Loss: 0.00019604
Iteration 735/1000 | Loss: 0.00009735
Iteration 736/1000 | Loss: 0.00005419
Iteration 737/1000 | Loss: 0.00003671
Iteration 738/1000 | Loss: 0.00009461
Iteration 739/1000 | Loss: 0.00006419
Iteration 740/1000 | Loss: 0.00008176
Iteration 741/1000 | Loss: 0.00013223
Iteration 742/1000 | Loss: 0.00003943
Iteration 743/1000 | Loss: 0.00005568
Iteration 744/1000 | Loss: 0.00003986
Iteration 745/1000 | Loss: 0.00002287
Iteration 746/1000 | Loss: 0.00002552
Iteration 747/1000 | Loss: 0.00001985
Iteration 748/1000 | Loss: 0.00001980
Iteration 749/1000 | Loss: 0.00001980
Iteration 750/1000 | Loss: 0.00001980
Iteration 751/1000 | Loss: 0.00001980
Iteration 752/1000 | Loss: 0.00002039
Iteration 753/1000 | Loss: 0.00001969
Iteration 754/1000 | Loss: 0.00001968
Iteration 755/1000 | Loss: 0.00001967
Iteration 756/1000 | Loss: 0.00001967
Iteration 757/1000 | Loss: 0.00001967
Iteration 758/1000 | Loss: 0.00001966
Iteration 759/1000 | Loss: 0.00001966
Iteration 760/1000 | Loss: 0.00001966
Iteration 761/1000 | Loss: 0.00002050
Iteration 762/1000 | Loss: 0.00002270
Iteration 763/1000 | Loss: 0.00002269
Iteration 764/1000 | Loss: 0.00023945
Iteration 765/1000 | Loss: 0.00017414
Iteration 766/1000 | Loss: 0.00012391
Iteration 767/1000 | Loss: 0.00010518
Iteration 768/1000 | Loss: 0.00030443
Iteration 769/1000 | Loss: 0.00016200
Iteration 770/1000 | Loss: 0.00015313
Iteration 771/1000 | Loss: 0.00012531
Iteration 772/1000 | Loss: 0.00028861
Iteration 773/1000 | Loss: 0.00005267
Iteration 774/1000 | Loss: 0.00002403
Iteration 775/1000 | Loss: 0.00002093
Iteration 776/1000 | Loss: 0.00001989
Iteration 777/1000 | Loss: 0.00001966
Iteration 778/1000 | Loss: 0.00002279
Iteration 779/1000 | Loss: 0.00002064
Iteration 780/1000 | Loss: 0.00002285
Iteration 781/1000 | Loss: 0.00016013
Iteration 782/1000 | Loss: 0.00017849
Iteration 783/1000 | Loss: 0.00007538
Iteration 784/1000 | Loss: 0.00010807
Iteration 785/1000 | Loss: 0.00010162
Iteration 786/1000 | Loss: 0.00002622
Iteration 787/1000 | Loss: 0.00002244
Iteration 788/1000 | Loss: 0.00020936
Iteration 789/1000 | Loss: 0.00016392
Iteration 790/1000 | Loss: 0.00025952
Iteration 791/1000 | Loss: 0.00004932
Iteration 792/1000 | Loss: 0.00009782
Iteration 793/1000 | Loss: 0.00004936
Iteration 794/1000 | Loss: 0.00001998
Iteration 795/1000 | Loss: 0.00009173
Iteration 796/1000 | Loss: 0.00003949
Iteration 797/1000 | Loss: 0.00010270
Iteration 798/1000 | Loss: 0.00003921
Iteration 799/1000 | Loss: 0.00010078
Iteration 800/1000 | Loss: 0.00004173
Iteration 801/1000 | Loss: 0.00009860
Iteration 802/1000 | Loss: 0.00004815
Iteration 803/1000 | Loss: 0.00009727
Iteration 804/1000 | Loss: 0.00004456
Iteration 805/1000 | Loss: 0.00022133
Iteration 806/1000 | Loss: 0.00010704
Iteration 807/1000 | Loss: 0.00011102
Iteration 808/1000 | Loss: 0.00009365
Iteration 809/1000 | Loss: 0.00008597
Iteration 810/1000 | Loss: 0.00028693
Iteration 811/1000 | Loss: 0.00009334
Iteration 812/1000 | Loss: 0.00017315
Iteration 813/1000 | Loss: 0.00017262
Iteration 814/1000 | Loss: 0.00013693
Iteration 815/1000 | Loss: 0.00004788
Iteration 816/1000 | Loss: 0.00015279
Iteration 817/1000 | Loss: 0.00014916
Iteration 818/1000 | Loss: 0.00043852
Iteration 819/1000 | Loss: 0.00015762
Iteration 820/1000 | Loss: 0.00012293
Iteration 821/1000 | Loss: 0.00017513
Iteration 822/1000 | Loss: 0.00008943
Iteration 823/1000 | Loss: 0.00003938
Iteration 824/1000 | Loss: 0.00018699
Iteration 825/1000 | Loss: 0.00002935
Iteration 826/1000 | Loss: 0.00002041
Iteration 827/1000 | Loss: 0.00002145
Iteration 828/1000 | Loss: 0.00003426
Iteration 829/1000 | Loss: 0.00001823
Iteration 830/1000 | Loss: 0.00001710
Iteration 831/1000 | Loss: 0.00001678
Iteration 832/1000 | Loss: 0.00001654
Iteration 833/1000 | Loss: 0.00001998
Iteration 834/1000 | Loss: 0.00001714
Iteration 835/1000 | Loss: 0.00001945
Iteration 836/1000 | Loss: 0.00001702
Iteration 837/1000 | Loss: 0.00001984
Iteration 838/1000 | Loss: 0.00001718
Iteration 839/1000 | Loss: 0.00001957
Iteration 840/1000 | Loss: 0.00001811
Iteration 841/1000 | Loss: 0.00001952
Iteration 842/1000 | Loss: 0.00001950
Iteration 843/1000 | Loss: 0.00001846
Iteration 844/1000 | Loss: 0.00001796
Iteration 845/1000 | Loss: 0.00001934
Iteration 846/1000 | Loss: 0.00003662
Iteration 847/1000 | Loss: 0.00002211
Iteration 848/1000 | Loss: 0.00002505
Iteration 849/1000 | Loss: 0.00002544
Iteration 850/1000 | Loss: 0.00001937
Iteration 851/1000 | Loss: 0.00002147
Iteration 852/1000 | Loss: 0.00001898
Iteration 853/1000 | Loss: 0.00002200
Iteration 854/1000 | Loss: 0.00002214
Iteration 855/1000 | Loss: 0.00001922
Iteration 856/1000 | Loss: 0.00002980
Iteration 857/1000 | Loss: 0.00001934
Iteration 858/1000 | Loss: 0.00001940
Iteration 859/1000 | Loss: 0.00001879
Iteration 860/1000 | Loss: 0.00002056
Iteration 861/1000 | Loss: 0.00001893
Iteration 862/1000 | Loss: 0.00002073
Iteration 863/1000 | Loss: 0.00002089
Iteration 864/1000 | Loss: 0.00001933
Iteration 865/1000 | Loss: 0.00001923
Iteration 866/1000 | Loss: 0.00008208
Iteration 867/1000 | Loss: 0.00017633
Iteration 868/1000 | Loss: 0.00015462
Iteration 869/1000 | Loss: 0.00010158
Iteration 870/1000 | Loss: 0.00008879
Iteration 871/1000 | Loss: 0.00002248
Iteration 872/1000 | Loss: 0.00002047
Iteration 873/1000 | Loss: 0.00004850
Iteration 874/1000 | Loss: 0.00001926
Iteration 875/1000 | Loss: 0.00010247
Iteration 876/1000 | Loss: 0.00007662
Iteration 877/1000 | Loss: 0.00010694
Iteration 878/1000 | Loss: 0.00006973
Iteration 879/1000 | Loss: 0.00010716
Iteration 880/1000 | Loss: 0.00010144
Iteration 881/1000 | Loss: 0.00009361
Iteration 882/1000 | Loss: 0.00009806
Iteration 883/1000 | Loss: 0.00005910
Iteration 884/1000 | Loss: 0.00016051
Iteration 885/1000 | Loss: 0.00011011
Iteration 886/1000 | Loss: 0.00007972
Iteration 887/1000 | Loss: 0.00011792
Iteration 888/1000 | Loss: 0.00007408
Iteration 889/1000 | Loss: 0.00008549
Iteration 890/1000 | Loss: 0.00024138
Iteration 891/1000 | Loss: 0.00019732
Iteration 892/1000 | Loss: 0.00024003
Iteration 893/1000 | Loss: 0.00020199
Iteration 894/1000 | Loss: 0.00018360
Iteration 895/1000 | Loss: 0.00002734
Iteration 896/1000 | Loss: 0.00002850
Iteration 897/1000 | Loss: 0.00002334
Iteration 898/1000 | Loss: 0.00025595
Iteration 899/1000 | Loss: 0.00010566
Iteration 900/1000 | Loss: 0.00003856
Iteration 901/1000 | Loss: 0.00002247
Iteration 902/1000 | Loss: 0.00004122
Iteration 903/1000 | Loss: 0.00002606
Iteration 904/1000 | Loss: 0.00015619
Iteration 905/1000 | Loss: 0.00009445
Iteration 906/1000 | Loss: 0.00018649
Iteration 907/1000 | Loss: 0.00002620
Iteration 908/1000 | Loss: 0.00002114
Iteration 909/1000 | Loss: 0.00030151
Iteration 910/1000 | Loss: 0.00009591
Iteration 911/1000 | Loss: 0.00026771
Iteration 912/1000 | Loss: 0.00008464
Iteration 913/1000 | Loss: 0.00024004
Iteration 914/1000 | Loss: 0.00002419
Iteration 915/1000 | Loss: 0.00002105
Iteration 916/1000 | Loss: 0.00002518
Iteration 917/1000 | Loss: 0.00030405
Iteration 918/1000 | Loss: 0.00022810
Iteration 919/1000 | Loss: 0.00026269
Iteration 920/1000 | Loss: 0.00027738
Iteration 921/1000 | Loss: 0.00002813
Iteration 922/1000 | Loss: 0.00002232
Iteration 923/1000 | Loss: 0.00018209
Iteration 924/1000 | Loss: 0.00007808
Iteration 925/1000 | Loss: 0.00004516
Iteration 926/1000 | Loss: 0.00001720
Iteration 927/1000 | Loss: 0.00002809
Iteration 928/1000 | Loss: 0.00001631
Iteration 929/1000 | Loss: 0.00002536
Iteration 930/1000 | Loss: 0.00001597
Iteration 931/1000 | Loss: 0.00001589
Iteration 932/1000 | Loss: 0.00001579
Iteration 933/1000 | Loss: 0.00001563
Iteration 934/1000 | Loss: 0.00001558
Iteration 935/1000 | Loss: 0.00001554
Iteration 936/1000 | Loss: 0.00001548
Iteration 937/1000 | Loss: 0.00001548
Iteration 938/1000 | Loss: 0.00001546
Iteration 939/1000 | Loss: 0.00001545
Iteration 940/1000 | Loss: 0.00001545
Iteration 941/1000 | Loss: 0.00001545
Iteration 942/1000 | Loss: 0.00001544
Iteration 943/1000 | Loss: 0.00001544
Iteration 944/1000 | Loss: 0.00001544
Iteration 945/1000 | Loss: 0.00001543
Iteration 946/1000 | Loss: 0.00001543
Iteration 947/1000 | Loss: 0.00001543
Iteration 948/1000 | Loss: 0.00001543
Iteration 949/1000 | Loss: 0.00001543
Iteration 950/1000 | Loss: 0.00001542
Iteration 951/1000 | Loss: 0.00001542
Iteration 952/1000 | Loss: 0.00001542
Iteration 953/1000 | Loss: 0.00001542
Iteration 954/1000 | Loss: 0.00001542
Iteration 955/1000 | Loss: 0.00001541
Iteration 956/1000 | Loss: 0.00001541
Iteration 957/1000 | Loss: 0.00001541
Iteration 958/1000 | Loss: 0.00001541
Iteration 959/1000 | Loss: 0.00001541
Iteration 960/1000 | Loss: 0.00001541
Iteration 961/1000 | Loss: 0.00001540
Iteration 962/1000 | Loss: 0.00001540
Iteration 963/1000 | Loss: 0.00001540
Iteration 964/1000 | Loss: 0.00001540
Iteration 965/1000 | Loss: 0.00001540
Iteration 966/1000 | Loss: 0.00001540
Iteration 967/1000 | Loss: 0.00001540
Iteration 968/1000 | Loss: 0.00001540
Iteration 969/1000 | Loss: 0.00001540
Iteration 970/1000 | Loss: 0.00001540
Iteration 971/1000 | Loss: 0.00001540
Iteration 972/1000 | Loss: 0.00001539
Iteration 973/1000 | Loss: 0.00001539
Iteration 974/1000 | Loss: 0.00001539
Iteration 975/1000 | Loss: 0.00001539
Iteration 976/1000 | Loss: 0.00001539
Iteration 977/1000 | Loss: 0.00001539
Iteration 978/1000 | Loss: 0.00001539
Iteration 979/1000 | Loss: 0.00001539
Iteration 980/1000 | Loss: 0.00001539
Iteration 981/1000 | Loss: 0.00001539
Iteration 982/1000 | Loss: 0.00001539
Iteration 983/1000 | Loss: 0.00001539
Iteration 984/1000 | Loss: 0.00001538
Iteration 985/1000 | Loss: 0.00001538
Iteration 986/1000 | Loss: 0.00001538
Iteration 987/1000 | Loss: 0.00001538
Iteration 988/1000 | Loss: 0.00001538
Iteration 989/1000 | Loss: 0.00001538
Iteration 990/1000 | Loss: 0.00001538
Iteration 991/1000 | Loss: 0.00001538
Iteration 992/1000 | Loss: 0.00001538
Iteration 993/1000 | Loss: 0.00001538
Iteration 994/1000 | Loss: 0.00001537
Iteration 995/1000 | Loss: 0.00001537
Iteration 996/1000 | Loss: 0.00001537
Iteration 997/1000 | Loss: 0.00001537
Iteration 998/1000 | Loss: 0.00001537
Iteration 999/1000 | Loss: 0.00001537
Iteration 1000/1000 | Loss: 0.00002700

Optimization complete. Final v2v error: 3.310804605484009 mm

Highest mean error: 11.095024108886719 mm for frame 80

Lowest mean error: 2.747248649597168 mm for frame 233

Saving results

Total time: 1506.3731796741486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_6242/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_6242/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_6242/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377371
Iteration 2/25 | Loss: 0.00125587
Iteration 3/25 | Loss: 0.00114881
Iteration 4/25 | Loss: 0.00113016
Iteration 5/25 | Loss: 0.00112578
Iteration 6/25 | Loss: 0.00112445
Iteration 7/25 | Loss: 0.00112445
Iteration 8/25 | Loss: 0.00112445
Iteration 9/25 | Loss: 0.00112445
Iteration 10/25 | Loss: 0.00112445
Iteration 11/25 | Loss: 0.00112445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011244481429457664, 0.0011244481429457664, 0.0011244481429457664, 0.0011244481429457664, 0.0011244481429457664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011244481429457664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37800694
Iteration 2/25 | Loss: 0.00085905
Iteration 3/25 | Loss: 0.00085904
Iteration 4/25 | Loss: 0.00085904
Iteration 5/25 | Loss: 0.00085904
Iteration 6/25 | Loss: 0.00085904
Iteration 7/25 | Loss: 0.00085904
Iteration 8/25 | Loss: 0.00085904
Iteration 9/25 | Loss: 0.00085904
Iteration 10/25 | Loss: 0.00085904
Iteration 11/25 | Loss: 0.00085904
Iteration 12/25 | Loss: 0.00085904
Iteration 13/25 | Loss: 0.00085904
Iteration 14/25 | Loss: 0.00085904
Iteration 15/25 | Loss: 0.00085904
Iteration 16/25 | Loss: 0.00085904
Iteration 17/25 | Loss: 0.00085904
Iteration 18/25 | Loss: 0.00085904
Iteration 19/25 | Loss: 0.00085904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008590391953475773, 0.0008590391953475773, 0.0008590391953475773, 0.0008590391953475773, 0.0008590391953475773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008590391953475773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085904
Iteration 2/1000 | Loss: 0.00005110
Iteration 3/1000 | Loss: 0.00003457
Iteration 4/1000 | Loss: 0.00003002
Iteration 5/1000 | Loss: 0.00002828
Iteration 6/1000 | Loss: 0.00002694
Iteration 7/1000 | Loss: 0.00002624
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002539
Iteration 10/1000 | Loss: 0.00002513
Iteration 11/1000 | Loss: 0.00002487
Iteration 12/1000 | Loss: 0.00002471
Iteration 13/1000 | Loss: 0.00002467
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002465
Iteration 16/1000 | Loss: 0.00002465
Iteration 17/1000 | Loss: 0.00002465
Iteration 18/1000 | Loss: 0.00002465
Iteration 19/1000 | Loss: 0.00002465
Iteration 20/1000 | Loss: 0.00002465
Iteration 21/1000 | Loss: 0.00002464
Iteration 22/1000 | Loss: 0.00002464
Iteration 23/1000 | Loss: 0.00002464
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002464
Iteration 26/1000 | Loss: 0.00002464
Iteration 27/1000 | Loss: 0.00002464
Iteration 28/1000 | Loss: 0.00002463
Iteration 29/1000 | Loss: 0.00002462
Iteration 30/1000 | Loss: 0.00002461
Iteration 31/1000 | Loss: 0.00002460
Iteration 32/1000 | Loss: 0.00002460
Iteration 33/1000 | Loss: 0.00002460
Iteration 34/1000 | Loss: 0.00002459
Iteration 35/1000 | Loss: 0.00002459
Iteration 36/1000 | Loss: 0.00002459
Iteration 37/1000 | Loss: 0.00002459
Iteration 38/1000 | Loss: 0.00002459
Iteration 39/1000 | Loss: 0.00002459
Iteration 40/1000 | Loss: 0.00002459
Iteration 41/1000 | Loss: 0.00002459
Iteration 42/1000 | Loss: 0.00002459
Iteration 43/1000 | Loss: 0.00002459
Iteration 44/1000 | Loss: 0.00002459
Iteration 45/1000 | Loss: 0.00002459
Iteration 46/1000 | Loss: 0.00002458
Iteration 47/1000 | Loss: 0.00002458
Iteration 48/1000 | Loss: 0.00002458
Iteration 49/1000 | Loss: 0.00002457
Iteration 50/1000 | Loss: 0.00002457
Iteration 51/1000 | Loss: 0.00002456
Iteration 52/1000 | Loss: 0.00002456
Iteration 53/1000 | Loss: 0.00002456
Iteration 54/1000 | Loss: 0.00002455
Iteration 55/1000 | Loss: 0.00002455
Iteration 56/1000 | Loss: 0.00002455
Iteration 57/1000 | Loss: 0.00002454
Iteration 58/1000 | Loss: 0.00002454
Iteration 59/1000 | Loss: 0.00002454
Iteration 60/1000 | Loss: 0.00002454
Iteration 61/1000 | Loss: 0.00002454
Iteration 62/1000 | Loss: 0.00002454
Iteration 63/1000 | Loss: 0.00002454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [2.454455716360826e-05, 2.454455716360826e-05, 2.454455716360826e-05, 2.454455716360826e-05, 2.454455716360826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.454455716360826e-05

Optimization complete. Final v2v error: 4.278504371643066 mm

Highest mean error: 4.544276714324951 mm for frame 109

Lowest mean error: 3.9019341468811035 mm for frame 23

Saving results

Total time: 30.75310206413269
