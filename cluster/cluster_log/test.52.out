Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=52, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2912-2967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977166
Iteration 2/25 | Loss: 0.00275640
Iteration 3/25 | Loss: 0.00240803
Iteration 4/25 | Loss: 0.00233833
Iteration 5/25 | Loss: 0.00207856
Iteration 6/25 | Loss: 0.00197371
Iteration 7/25 | Loss: 0.00193491
Iteration 8/25 | Loss: 0.00193426
Iteration 9/25 | Loss: 0.00189944
Iteration 10/25 | Loss: 0.00189371
Iteration 11/25 | Loss: 0.00189176
Iteration 12/25 | Loss: 0.00188633
Iteration 13/25 | Loss: 0.00186899
Iteration 14/25 | Loss: 0.00186565
Iteration 15/25 | Loss: 0.00186465
Iteration 16/25 | Loss: 0.00186323
Iteration 17/25 | Loss: 0.00186231
Iteration 18/25 | Loss: 0.00186663
Iteration 19/25 | Loss: 0.00186163
Iteration 20/25 | Loss: 0.00186037
Iteration 21/25 | Loss: 0.00186000
Iteration 22/25 | Loss: 0.00185999
Iteration 23/25 | Loss: 0.00185998
Iteration 24/25 | Loss: 0.00185998
Iteration 25/25 | Loss: 0.00185998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35707307
Iteration 2/25 | Loss: 0.00458549
Iteration 3/25 | Loss: 0.00458548
Iteration 4/25 | Loss: 0.00458548
Iteration 5/25 | Loss: 0.00458548
Iteration 6/25 | Loss: 0.00458548
Iteration 7/25 | Loss: 0.00458548
Iteration 8/25 | Loss: 0.00458548
Iteration 9/25 | Loss: 0.00458548
Iteration 10/25 | Loss: 0.00458548
Iteration 11/25 | Loss: 0.00458548
Iteration 12/25 | Loss: 0.00458548
Iteration 13/25 | Loss: 0.00458548
Iteration 14/25 | Loss: 0.00458548
Iteration 15/25 | Loss: 0.00458548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00458547892048955, 0.00458547892048955, 0.00458547892048955, 0.00458547892048955, 0.00458547892048955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00458547892048955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00458548
Iteration 2/1000 | Loss: 0.00067306
Iteration 3/1000 | Loss: 0.00051135
Iteration 4/1000 | Loss: 0.00045192
Iteration 5/1000 | Loss: 0.00041676
Iteration 6/1000 | Loss: 0.00039349
Iteration 7/1000 | Loss: 0.00035998
Iteration 8/1000 | Loss: 0.00033933
Iteration 9/1000 | Loss: 0.00032475
Iteration 10/1000 | Loss: 0.00031291
Iteration 11/1000 | Loss: 0.00030242
Iteration 12/1000 | Loss: 0.00029637
Iteration 13/1000 | Loss: 0.00029220
Iteration 14/1000 | Loss: 0.00028946
Iteration 15/1000 | Loss: 0.00028727
Iteration 16/1000 | Loss: 0.00028558
Iteration 17/1000 | Loss: 0.00028416
Iteration 18/1000 | Loss: 0.00028317
Iteration 19/1000 | Loss: 0.00028213
Iteration 20/1000 | Loss: 0.00028130
Iteration 21/1000 | Loss: 0.00028075
Iteration 22/1000 | Loss: 0.00028034
Iteration 23/1000 | Loss: 0.00027995
Iteration 24/1000 | Loss: 0.00027975
Iteration 25/1000 | Loss: 0.00027955
Iteration 26/1000 | Loss: 0.00027940
Iteration 27/1000 | Loss: 0.00027939
Iteration 28/1000 | Loss: 0.00027932
Iteration 29/1000 | Loss: 0.00027932
Iteration 30/1000 | Loss: 0.00027925
Iteration 31/1000 | Loss: 0.00027923
Iteration 32/1000 | Loss: 0.00027923
Iteration 33/1000 | Loss: 0.00027923
Iteration 34/1000 | Loss: 0.00027922
Iteration 35/1000 | Loss: 0.00027920
Iteration 36/1000 | Loss: 0.00027920
Iteration 37/1000 | Loss: 0.00027920
Iteration 38/1000 | Loss: 0.00027920
Iteration 39/1000 | Loss: 0.00027920
Iteration 40/1000 | Loss: 0.00027920
Iteration 41/1000 | Loss: 0.00027920
Iteration 42/1000 | Loss: 0.00027920
Iteration 43/1000 | Loss: 0.00027920
Iteration 44/1000 | Loss: 0.00027920
Iteration 45/1000 | Loss: 0.00027920
Iteration 46/1000 | Loss: 0.00027920
Iteration 47/1000 | Loss: 0.00027920
Iteration 48/1000 | Loss: 0.00027920
Iteration 49/1000 | Loss: 0.00027920
Iteration 50/1000 | Loss: 0.00027920
Iteration 51/1000 | Loss: 0.00027919
Iteration 52/1000 | Loss: 0.00027919
Iteration 53/1000 | Loss: 0.00027919
Iteration 54/1000 | Loss: 0.00027919
Iteration 55/1000 | Loss: 0.00027919
Iteration 56/1000 | Loss: 0.00027919
Iteration 57/1000 | Loss: 0.00027919
Iteration 58/1000 | Loss: 0.00027919
Iteration 59/1000 | Loss: 0.00027919
Iteration 60/1000 | Loss: 0.00027918
Iteration 61/1000 | Loss: 0.00027918
Iteration 62/1000 | Loss: 0.00027918
Iteration 63/1000 | Loss: 0.00027918
Iteration 64/1000 | Loss: 0.00027918
Iteration 65/1000 | Loss: 0.00027918
Iteration 66/1000 | Loss: 0.00027918
Iteration 67/1000 | Loss: 0.00027918
Iteration 68/1000 | Loss: 0.00027918
Iteration 69/1000 | Loss: 0.00027918
Iteration 70/1000 | Loss: 0.00027918
Iteration 71/1000 | Loss: 0.00027918
Iteration 72/1000 | Loss: 0.00027918
Iteration 73/1000 | Loss: 0.00027918
Iteration 74/1000 | Loss: 0.00027918
Iteration 75/1000 | Loss: 0.00027918
Iteration 76/1000 | Loss: 0.00027918
Iteration 77/1000 | Loss: 0.00027918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [0.00027917741681449115, 0.00027917741681449115, 0.00027917741681449115, 0.00027917741681449115, 0.00027917741681449115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027917741681449115

Optimization complete. Final v2v error: 9.790348052978516 mm

Highest mean error: 10.356422424316406 mm for frame 24

Lowest mean error: 5.541741847991943 mm for frame 1

Saving results

Total time: 78.34897589683533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386649
Iteration 2/25 | Loss: 0.00126730
Iteration 3/25 | Loss: 0.00109448
Iteration 4/25 | Loss: 0.00108172
Iteration 5/25 | Loss: 0.00107813
Iteration 6/25 | Loss: 0.00107782
Iteration 7/25 | Loss: 0.00107782
Iteration 8/25 | Loss: 0.00107782
Iteration 9/25 | Loss: 0.00107782
Iteration 10/25 | Loss: 0.00107782
Iteration 11/25 | Loss: 0.00107782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010778151918202639, 0.0010778151918202639, 0.0010778151918202639, 0.0010778151918202639, 0.0010778151918202639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010778151918202639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63426614
Iteration 2/25 | Loss: 0.00076510
Iteration 3/25 | Loss: 0.00076510
Iteration 4/25 | Loss: 0.00076510
Iteration 5/25 | Loss: 0.00076509
Iteration 6/25 | Loss: 0.00076509
Iteration 7/25 | Loss: 0.00076509
Iteration 8/25 | Loss: 0.00076509
Iteration 9/25 | Loss: 0.00076509
Iteration 10/25 | Loss: 0.00076509
Iteration 11/25 | Loss: 0.00076509
Iteration 12/25 | Loss: 0.00076509
Iteration 13/25 | Loss: 0.00076509
Iteration 14/25 | Loss: 0.00076509
Iteration 15/25 | Loss: 0.00076509
Iteration 16/25 | Loss: 0.00076509
Iteration 17/25 | Loss: 0.00076509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007650930201634765, 0.0007650930201634765, 0.0007650930201634765, 0.0007650930201634765, 0.0007650930201634765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007650930201634765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076509
Iteration 2/1000 | Loss: 0.00001998
Iteration 3/1000 | Loss: 0.00001301
Iteration 4/1000 | Loss: 0.00001177
Iteration 5/1000 | Loss: 0.00001100
Iteration 6/1000 | Loss: 0.00001051
Iteration 7/1000 | Loss: 0.00001003
Iteration 8/1000 | Loss: 0.00000982
Iteration 9/1000 | Loss: 0.00000969
Iteration 10/1000 | Loss: 0.00000939
Iteration 11/1000 | Loss: 0.00000916
Iteration 12/1000 | Loss: 0.00000916
Iteration 13/1000 | Loss: 0.00000915
Iteration 14/1000 | Loss: 0.00000914
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000908
Iteration 17/1000 | Loss: 0.00000908
Iteration 18/1000 | Loss: 0.00000904
Iteration 19/1000 | Loss: 0.00000903
Iteration 20/1000 | Loss: 0.00000903
Iteration 21/1000 | Loss: 0.00000900
Iteration 22/1000 | Loss: 0.00000899
Iteration 23/1000 | Loss: 0.00000898
Iteration 24/1000 | Loss: 0.00000891
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000888
Iteration 27/1000 | Loss: 0.00000888
Iteration 28/1000 | Loss: 0.00000888
Iteration 29/1000 | Loss: 0.00000887
Iteration 30/1000 | Loss: 0.00000887
Iteration 31/1000 | Loss: 0.00000886
Iteration 32/1000 | Loss: 0.00000886
Iteration 33/1000 | Loss: 0.00000885
Iteration 34/1000 | Loss: 0.00000885
Iteration 35/1000 | Loss: 0.00000884
Iteration 36/1000 | Loss: 0.00000883
Iteration 37/1000 | Loss: 0.00000883
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000880
Iteration 45/1000 | Loss: 0.00000880
Iteration 46/1000 | Loss: 0.00000879
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000878
Iteration 50/1000 | Loss: 0.00000878
Iteration 51/1000 | Loss: 0.00000878
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000877
Iteration 57/1000 | Loss: 0.00000877
Iteration 58/1000 | Loss: 0.00000876
Iteration 59/1000 | Loss: 0.00000876
Iteration 60/1000 | Loss: 0.00000876
Iteration 61/1000 | Loss: 0.00000876
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000875
Iteration 64/1000 | Loss: 0.00000875
Iteration 65/1000 | Loss: 0.00000875
Iteration 66/1000 | Loss: 0.00000874
Iteration 67/1000 | Loss: 0.00000874
Iteration 68/1000 | Loss: 0.00000874
Iteration 69/1000 | Loss: 0.00000874
Iteration 70/1000 | Loss: 0.00000873
Iteration 71/1000 | Loss: 0.00000873
Iteration 72/1000 | Loss: 0.00000873
Iteration 73/1000 | Loss: 0.00000873
Iteration 74/1000 | Loss: 0.00000873
Iteration 75/1000 | Loss: 0.00000873
Iteration 76/1000 | Loss: 0.00000872
Iteration 77/1000 | Loss: 0.00000871
Iteration 78/1000 | Loss: 0.00000871
Iteration 79/1000 | Loss: 0.00000870
Iteration 80/1000 | Loss: 0.00000869
Iteration 81/1000 | Loss: 0.00000869
Iteration 82/1000 | Loss: 0.00000869
Iteration 83/1000 | Loss: 0.00000868
Iteration 84/1000 | Loss: 0.00000868
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000867
Iteration 87/1000 | Loss: 0.00000867
Iteration 88/1000 | Loss: 0.00000867
Iteration 89/1000 | Loss: 0.00000867
Iteration 90/1000 | Loss: 0.00000867
Iteration 91/1000 | Loss: 0.00000867
Iteration 92/1000 | Loss: 0.00000867
Iteration 93/1000 | Loss: 0.00000867
Iteration 94/1000 | Loss: 0.00000866
Iteration 95/1000 | Loss: 0.00000866
Iteration 96/1000 | Loss: 0.00000866
Iteration 97/1000 | Loss: 0.00000866
Iteration 98/1000 | Loss: 0.00000866
Iteration 99/1000 | Loss: 0.00000866
Iteration 100/1000 | Loss: 0.00000865
Iteration 101/1000 | Loss: 0.00000865
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000864
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000864
Iteration 107/1000 | Loss: 0.00000863
Iteration 108/1000 | Loss: 0.00000863
Iteration 109/1000 | Loss: 0.00000863
Iteration 110/1000 | Loss: 0.00000863
Iteration 111/1000 | Loss: 0.00000863
Iteration 112/1000 | Loss: 0.00000863
Iteration 113/1000 | Loss: 0.00000863
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000861
Iteration 125/1000 | Loss: 0.00000861
Iteration 126/1000 | Loss: 0.00000861
Iteration 127/1000 | Loss: 0.00000861
Iteration 128/1000 | Loss: 0.00000861
Iteration 129/1000 | Loss: 0.00000861
Iteration 130/1000 | Loss: 0.00000861
Iteration 131/1000 | Loss: 0.00000861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [8.607263225712813e-06, 8.607263225712813e-06, 8.607263225712813e-06, 8.607263225712813e-06, 8.607263225712813e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.607263225712813e-06

Optimization complete. Final v2v error: 2.5310075283050537 mm

Highest mean error: 2.6776466369628906 mm for frame 106

Lowest mean error: 2.3935396671295166 mm for frame 197

Saving results

Total time: 38.613691329956055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937489
Iteration 2/25 | Loss: 0.00239237
Iteration 3/25 | Loss: 0.00168386
Iteration 4/25 | Loss: 0.00162651
Iteration 5/25 | Loss: 0.00168267
Iteration 6/25 | Loss: 0.00145992
Iteration 7/25 | Loss: 0.00136179
Iteration 8/25 | Loss: 0.00129536
Iteration 9/25 | Loss: 0.00127067
Iteration 10/25 | Loss: 0.00127392
Iteration 11/25 | Loss: 0.00126710
Iteration 12/25 | Loss: 0.00127779
Iteration 13/25 | Loss: 0.00125742
Iteration 14/25 | Loss: 0.00125014
Iteration 15/25 | Loss: 0.00124880
Iteration 16/25 | Loss: 0.00124565
Iteration 17/25 | Loss: 0.00124337
Iteration 18/25 | Loss: 0.00124275
Iteration 19/25 | Loss: 0.00124247
Iteration 20/25 | Loss: 0.00124246
Iteration 21/25 | Loss: 0.00124246
Iteration 22/25 | Loss: 0.00124245
Iteration 23/25 | Loss: 0.00124245
Iteration 24/25 | Loss: 0.00124245
Iteration 25/25 | Loss: 0.00124245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33472443
Iteration 2/25 | Loss: 0.00072276
Iteration 3/25 | Loss: 0.00072276
Iteration 4/25 | Loss: 0.00072276
Iteration 5/25 | Loss: 0.00072276
Iteration 6/25 | Loss: 0.00072276
Iteration 7/25 | Loss: 0.00072276
Iteration 8/25 | Loss: 0.00072276
Iteration 9/25 | Loss: 0.00072276
Iteration 10/25 | Loss: 0.00072276
Iteration 11/25 | Loss: 0.00072276
Iteration 12/25 | Loss: 0.00072276
Iteration 13/25 | Loss: 0.00072276
Iteration 14/25 | Loss: 0.00072276
Iteration 15/25 | Loss: 0.00072276
Iteration 16/25 | Loss: 0.00072276
Iteration 17/25 | Loss: 0.00072276
Iteration 18/25 | Loss: 0.00072276
Iteration 19/25 | Loss: 0.00072276
Iteration 20/25 | Loss: 0.00072276
Iteration 21/25 | Loss: 0.00072276
Iteration 22/25 | Loss: 0.00072276
Iteration 23/25 | Loss: 0.00072276
Iteration 24/25 | Loss: 0.00072276
Iteration 25/25 | Loss: 0.00072276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072276
Iteration 2/1000 | Loss: 0.00510448
Iteration 3/1000 | Loss: 0.00248231
Iteration 4/1000 | Loss: 0.00193117
Iteration 5/1000 | Loss: 0.00049642
Iteration 6/1000 | Loss: 0.00040699
Iteration 7/1000 | Loss: 0.00064476
Iteration 8/1000 | Loss: 0.00025759
Iteration 9/1000 | Loss: 0.00034195
Iteration 10/1000 | Loss: 0.00010591
Iteration 11/1000 | Loss: 0.00009584
Iteration 12/1000 | Loss: 0.00071876
Iteration 13/1000 | Loss: 0.00338797
Iteration 14/1000 | Loss: 0.00416677
Iteration 15/1000 | Loss: 0.00574320
Iteration 16/1000 | Loss: 0.00222634
Iteration 17/1000 | Loss: 0.00126168
Iteration 18/1000 | Loss: 0.00066451
Iteration 19/1000 | Loss: 0.00013214
Iteration 20/1000 | Loss: 0.00202006
Iteration 21/1000 | Loss: 0.00040563
Iteration 22/1000 | Loss: 0.00051984
Iteration 23/1000 | Loss: 0.00051102
Iteration 24/1000 | Loss: 0.00049573
Iteration 25/1000 | Loss: 0.00042166
Iteration 26/1000 | Loss: 0.00074322
Iteration 27/1000 | Loss: 0.00046647
Iteration 28/1000 | Loss: 0.00070212
Iteration 29/1000 | Loss: 0.00019239
Iteration 30/1000 | Loss: 0.00025268
Iteration 31/1000 | Loss: 0.00025197
Iteration 32/1000 | Loss: 0.00021638
Iteration 33/1000 | Loss: 0.00016500
Iteration 34/1000 | Loss: 0.00023010
Iteration 35/1000 | Loss: 0.00025160
Iteration 36/1000 | Loss: 0.00020495
Iteration 37/1000 | Loss: 0.00014079
Iteration 38/1000 | Loss: 0.00013480
Iteration 39/1000 | Loss: 0.00014005
Iteration 40/1000 | Loss: 0.00028998
Iteration 41/1000 | Loss: 0.00020467
Iteration 42/1000 | Loss: 0.00011793
Iteration 43/1000 | Loss: 0.00023669
Iteration 44/1000 | Loss: 0.00151418
Iteration 45/1000 | Loss: 0.00012927
Iteration 46/1000 | Loss: 0.00010879
Iteration 47/1000 | Loss: 0.00010147
Iteration 48/1000 | Loss: 0.00009279
Iteration 49/1000 | Loss: 0.00008976
Iteration 50/1000 | Loss: 0.00009064
Iteration 51/1000 | Loss: 0.00031692
Iteration 52/1000 | Loss: 0.00044708
Iteration 53/1000 | Loss: 0.00011767
Iteration 54/1000 | Loss: 0.00008803
Iteration 55/1000 | Loss: 0.00008682
Iteration 56/1000 | Loss: 0.00007580
Iteration 57/1000 | Loss: 0.00052815
Iteration 58/1000 | Loss: 0.00012612
Iteration 59/1000 | Loss: 0.00009381
Iteration 60/1000 | Loss: 0.00006541
Iteration 61/1000 | Loss: 0.00005638
Iteration 62/1000 | Loss: 0.00008729
Iteration 63/1000 | Loss: 0.00022152
Iteration 64/1000 | Loss: 0.00007739
Iteration 65/1000 | Loss: 0.00006733
Iteration 66/1000 | Loss: 0.00005706
Iteration 67/1000 | Loss: 0.00005297
Iteration 68/1000 | Loss: 0.00005134
Iteration 69/1000 | Loss: 0.00004922
Iteration 70/1000 | Loss: 0.00004783
Iteration 71/1000 | Loss: 0.00004657
Iteration 72/1000 | Loss: 0.00004542
Iteration 73/1000 | Loss: 0.00004419
Iteration 74/1000 | Loss: 0.00004310
Iteration 75/1000 | Loss: 0.00006447
Iteration 76/1000 | Loss: 0.00036978
Iteration 77/1000 | Loss: 0.00006720
Iteration 78/1000 | Loss: 0.00008519
Iteration 79/1000 | Loss: 0.00006264
Iteration 80/1000 | Loss: 0.00005163
Iteration 81/1000 | Loss: 0.00004594
Iteration 82/1000 | Loss: 0.00004260
Iteration 83/1000 | Loss: 0.00004368
Iteration 84/1000 | Loss: 0.00004061
Iteration 85/1000 | Loss: 0.00003963
Iteration 86/1000 | Loss: 0.00003911
Iteration 87/1000 | Loss: 0.00003858
Iteration 88/1000 | Loss: 0.00003757
Iteration 89/1000 | Loss: 0.00003809
Iteration 90/1000 | Loss: 0.00003925
Iteration 91/1000 | Loss: 0.00003743
Iteration 92/1000 | Loss: 0.00003689
Iteration 93/1000 | Loss: 0.00004008
Iteration 94/1000 | Loss: 0.00003626
Iteration 95/1000 | Loss: 0.00003494
Iteration 96/1000 | Loss: 0.00003441
Iteration 97/1000 | Loss: 0.00003401
Iteration 98/1000 | Loss: 0.00003345
Iteration 99/1000 | Loss: 0.00035546
Iteration 100/1000 | Loss: 0.00042139
Iteration 101/1000 | Loss: 0.00020697
Iteration 102/1000 | Loss: 0.00034092
Iteration 103/1000 | Loss: 0.00036718
Iteration 104/1000 | Loss: 0.00003841
Iteration 105/1000 | Loss: 0.00005813
Iteration 106/1000 | Loss: 0.00004848
Iteration 107/1000 | Loss: 0.00005259
Iteration 108/1000 | Loss: 0.00003669
Iteration 109/1000 | Loss: 0.00003580
Iteration 110/1000 | Loss: 0.00003464
Iteration 111/1000 | Loss: 0.00003622
Iteration 112/1000 | Loss: 0.00003460
Iteration 113/1000 | Loss: 0.00038514
Iteration 114/1000 | Loss: 0.00014930
Iteration 115/1000 | Loss: 0.00003454
Iteration 116/1000 | Loss: 0.00060192
Iteration 117/1000 | Loss: 0.00010370
Iteration 118/1000 | Loss: 0.00004025
Iteration 119/1000 | Loss: 0.00003799
Iteration 120/1000 | Loss: 0.00003680
Iteration 121/1000 | Loss: 0.00003640
Iteration 122/1000 | Loss: 0.00003597
Iteration 123/1000 | Loss: 0.00003546
Iteration 124/1000 | Loss: 0.00003785
Iteration 125/1000 | Loss: 0.00063222
Iteration 126/1000 | Loss: 0.00072934
Iteration 127/1000 | Loss: 0.00007362
Iteration 128/1000 | Loss: 0.00014081
Iteration 129/1000 | Loss: 0.00011278
Iteration 130/1000 | Loss: 0.00007893
Iteration 131/1000 | Loss: 0.00018703
Iteration 132/1000 | Loss: 0.00007984
Iteration 133/1000 | Loss: 0.00004995
Iteration 134/1000 | Loss: 0.00041859
Iteration 135/1000 | Loss: 0.00006969
Iteration 136/1000 | Loss: 0.00004738
Iteration 137/1000 | Loss: 0.00005580
Iteration 138/1000 | Loss: 0.00006716
Iteration 139/1000 | Loss: 0.00003886
Iteration 140/1000 | Loss: 0.00005903
Iteration 141/1000 | Loss: 0.00005051
Iteration 142/1000 | Loss: 0.00005447
Iteration 143/1000 | Loss: 0.00003852
Iteration 144/1000 | Loss: 0.00005986
Iteration 145/1000 | Loss: 0.00005980
Iteration 146/1000 | Loss: 0.00007297
Iteration 147/1000 | Loss: 0.00005833
Iteration 148/1000 | Loss: 0.00007009
Iteration 149/1000 | Loss: 0.00005317
Iteration 150/1000 | Loss: 0.00003667
Iteration 151/1000 | Loss: 0.00006806
Iteration 152/1000 | Loss: 0.00005555
Iteration 153/1000 | Loss: 0.00005553
Iteration 154/1000 | Loss: 0.00004629
Iteration 155/1000 | Loss: 0.00003640
Iteration 156/1000 | Loss: 0.00004449
Iteration 157/1000 | Loss: 0.00003065
Iteration 158/1000 | Loss: 0.00003017
Iteration 159/1000 | Loss: 0.00003567
Iteration 160/1000 | Loss: 0.00004162
Iteration 161/1000 | Loss: 0.00003332
Iteration 162/1000 | Loss: 0.00004439
Iteration 163/1000 | Loss: 0.00005292
Iteration 164/1000 | Loss: 0.00005146
Iteration 165/1000 | Loss: 0.00004457
Iteration 166/1000 | Loss: 0.00025623
Iteration 167/1000 | Loss: 0.00005857
Iteration 168/1000 | Loss: 0.00003431
Iteration 169/1000 | Loss: 0.00006847
Iteration 170/1000 | Loss: 0.00032451
Iteration 171/1000 | Loss: 0.00018485
Iteration 172/1000 | Loss: 0.00008156
Iteration 173/1000 | Loss: 0.00005746
Iteration 174/1000 | Loss: 0.00004574
Iteration 175/1000 | Loss: 0.00004231
Iteration 176/1000 | Loss: 0.00005002
Iteration 177/1000 | Loss: 0.00006560
Iteration 178/1000 | Loss: 0.00005718
Iteration 179/1000 | Loss: 0.00005707
Iteration 180/1000 | Loss: 0.00005598
Iteration 181/1000 | Loss: 0.00005703
Iteration 182/1000 | Loss: 0.00033342
Iteration 183/1000 | Loss: 0.00005135
Iteration 184/1000 | Loss: 0.00006869
Iteration 185/1000 | Loss: 0.00006429
Iteration 186/1000 | Loss: 0.00006713
Iteration 187/1000 | Loss: 0.00003607
Iteration 188/1000 | Loss: 0.00006076
Iteration 189/1000 | Loss: 0.00005926
Iteration 190/1000 | Loss: 0.00003706
Iteration 191/1000 | Loss: 0.00003448
Iteration 192/1000 | Loss: 0.00004557
Iteration 193/1000 | Loss: 0.00005337
Iteration 194/1000 | Loss: 0.00004457
Iteration 195/1000 | Loss: 0.00004771
Iteration 196/1000 | Loss: 0.00005718
Iteration 197/1000 | Loss: 0.00005736
Iteration 198/1000 | Loss: 0.00005810
Iteration 199/1000 | Loss: 0.00005018
Iteration 200/1000 | Loss: 0.00003378
Iteration 201/1000 | Loss: 0.00004997
Iteration 202/1000 | Loss: 0.00005791
Iteration 203/1000 | Loss: 0.00003853
Iteration 204/1000 | Loss: 0.00005093
Iteration 205/1000 | Loss: 0.00006080
Iteration 206/1000 | Loss: 0.00003211
Iteration 207/1000 | Loss: 0.00003135
Iteration 208/1000 | Loss: 0.00025500
Iteration 209/1000 | Loss: 0.00006689
Iteration 210/1000 | Loss: 0.00009313
Iteration 211/1000 | Loss: 0.00009679
Iteration 212/1000 | Loss: 0.00005918
Iteration 213/1000 | Loss: 0.00005798
Iteration 214/1000 | Loss: 0.00003522
Iteration 215/1000 | Loss: 0.00005803
Iteration 216/1000 | Loss: 0.00014569
Iteration 217/1000 | Loss: 0.00031433
Iteration 218/1000 | Loss: 0.00006501
Iteration 219/1000 | Loss: 0.00003731
Iteration 220/1000 | Loss: 0.00004434
Iteration 221/1000 | Loss: 0.00004817
Iteration 222/1000 | Loss: 0.00005987
Iteration 223/1000 | Loss: 0.00004829
Iteration 224/1000 | Loss: 0.00006021
Iteration 225/1000 | Loss: 0.00006488
Iteration 226/1000 | Loss: 0.00005804
Iteration 227/1000 | Loss: 0.00006183
Iteration 228/1000 | Loss: 0.00012651
Iteration 229/1000 | Loss: 0.00005251
Iteration 230/1000 | Loss: 0.00010516
Iteration 231/1000 | Loss: 0.00019781
Iteration 232/1000 | Loss: 0.00012493
Iteration 233/1000 | Loss: 0.00016353
Iteration 234/1000 | Loss: 0.00005819
Iteration 235/1000 | Loss: 0.00005212
Iteration 236/1000 | Loss: 0.00005485
Iteration 237/1000 | Loss: 0.00005119
Iteration 238/1000 | Loss: 0.00005594
Iteration 239/1000 | Loss: 0.00004877
Iteration 240/1000 | Loss: 0.00005564
Iteration 241/1000 | Loss: 0.00028358
Iteration 242/1000 | Loss: 0.00011557
Iteration 243/1000 | Loss: 0.00005618
Iteration 244/1000 | Loss: 0.00010074
Iteration 245/1000 | Loss: 0.00011133
Iteration 246/1000 | Loss: 0.00010658
Iteration 247/1000 | Loss: 0.00006131
Iteration 248/1000 | Loss: 0.00004158
Iteration 249/1000 | Loss: 0.00003383
Iteration 250/1000 | Loss: 0.00003164
Iteration 251/1000 | Loss: 0.00003056
Iteration 252/1000 | Loss: 0.00002993
Iteration 253/1000 | Loss: 0.00002938
Iteration 254/1000 | Loss: 0.00002885
Iteration 255/1000 | Loss: 0.00002844
Iteration 256/1000 | Loss: 0.00002818
Iteration 257/1000 | Loss: 0.00002807
Iteration 258/1000 | Loss: 0.00002806
Iteration 259/1000 | Loss: 0.00002794
Iteration 260/1000 | Loss: 0.00002791
Iteration 261/1000 | Loss: 0.00002783
Iteration 262/1000 | Loss: 0.00002782
Iteration 263/1000 | Loss: 0.00002780
Iteration 264/1000 | Loss: 0.00002777
Iteration 265/1000 | Loss: 0.00002775
Iteration 266/1000 | Loss: 0.00002775
Iteration 267/1000 | Loss: 0.00002775
Iteration 268/1000 | Loss: 0.00002775
Iteration 269/1000 | Loss: 0.00002775
Iteration 270/1000 | Loss: 0.00002774
Iteration 271/1000 | Loss: 0.00002771
Iteration 272/1000 | Loss: 0.00002770
Iteration 273/1000 | Loss: 0.00002769
Iteration 274/1000 | Loss: 0.00002769
Iteration 275/1000 | Loss: 0.00002769
Iteration 276/1000 | Loss: 0.00002769
Iteration 277/1000 | Loss: 0.00002768
Iteration 278/1000 | Loss: 0.00002768
Iteration 279/1000 | Loss: 0.00002768
Iteration 280/1000 | Loss: 0.00002767
Iteration 281/1000 | Loss: 0.00002767
Iteration 282/1000 | Loss: 0.00002766
Iteration 283/1000 | Loss: 0.00002765
Iteration 284/1000 | Loss: 0.00002765
Iteration 285/1000 | Loss: 0.00002765
Iteration 286/1000 | Loss: 0.00002765
Iteration 287/1000 | Loss: 0.00002764
Iteration 288/1000 | Loss: 0.00002764
Iteration 289/1000 | Loss: 0.00002764
Iteration 290/1000 | Loss: 0.00002763
Iteration 291/1000 | Loss: 0.00002763
Iteration 292/1000 | Loss: 0.00002762
Iteration 293/1000 | Loss: 0.00002762
Iteration 294/1000 | Loss: 0.00002761
Iteration 295/1000 | Loss: 0.00002760
Iteration 296/1000 | Loss: 0.00002759
Iteration 297/1000 | Loss: 0.00038660
Iteration 298/1000 | Loss: 0.00041958
Iteration 299/1000 | Loss: 0.00003862
Iteration 300/1000 | Loss: 0.00006794
Iteration 301/1000 | Loss: 0.00003174
Iteration 302/1000 | Loss: 0.00003120
Iteration 303/1000 | Loss: 0.00003080
Iteration 304/1000 | Loss: 0.00003035
Iteration 305/1000 | Loss: 0.00002970
Iteration 306/1000 | Loss: 0.00005163
Iteration 307/1000 | Loss: 0.00004873
Iteration 308/1000 | Loss: 0.00003100
Iteration 309/1000 | Loss: 0.00003080
Iteration 310/1000 | Loss: 0.00005039
Iteration 311/1000 | Loss: 0.00004708
Iteration 312/1000 | Loss: 0.00004976
Iteration 313/1000 | Loss: 0.00004475
Iteration 314/1000 | Loss: 0.00004874
Iteration 315/1000 | Loss: 0.00003083
Iteration 316/1000 | Loss: 0.00002890
Iteration 317/1000 | Loss: 0.00002829
Iteration 318/1000 | Loss: 0.00002768
Iteration 319/1000 | Loss: 0.00002745
Iteration 320/1000 | Loss: 0.00002741
Iteration 321/1000 | Loss: 0.00002730
Iteration 322/1000 | Loss: 0.00002730
Iteration 323/1000 | Loss: 0.00002729
Iteration 324/1000 | Loss: 0.00002715
Iteration 325/1000 | Loss: 0.00002704
Iteration 326/1000 | Loss: 0.00002691
Iteration 327/1000 | Loss: 0.00002673
Iteration 328/1000 | Loss: 0.00002670
Iteration 329/1000 | Loss: 0.00002668
Iteration 330/1000 | Loss: 0.00002666
Iteration 331/1000 | Loss: 0.00002662
Iteration 332/1000 | Loss: 0.00002661
Iteration 333/1000 | Loss: 0.00002661
Iteration 334/1000 | Loss: 0.00002660
Iteration 335/1000 | Loss: 0.00002660
Iteration 336/1000 | Loss: 0.00002659
Iteration 337/1000 | Loss: 0.00002659
Iteration 338/1000 | Loss: 0.00002659
Iteration 339/1000 | Loss: 0.00002658
Iteration 340/1000 | Loss: 0.00002658
Iteration 341/1000 | Loss: 0.00002658
Iteration 342/1000 | Loss: 0.00002658
Iteration 343/1000 | Loss: 0.00002658
Iteration 344/1000 | Loss: 0.00002658
Iteration 345/1000 | Loss: 0.00002658
Iteration 346/1000 | Loss: 0.00002658
Iteration 347/1000 | Loss: 0.00002658
Iteration 348/1000 | Loss: 0.00002657
Iteration 349/1000 | Loss: 0.00002656
Iteration 350/1000 | Loss: 0.00002656
Iteration 351/1000 | Loss: 0.00002656
Iteration 352/1000 | Loss: 0.00002656
Iteration 353/1000 | Loss: 0.00002656
Iteration 354/1000 | Loss: 0.00002656
Iteration 355/1000 | Loss: 0.00002656
Iteration 356/1000 | Loss: 0.00002656
Iteration 357/1000 | Loss: 0.00002656
Iteration 358/1000 | Loss: 0.00002656
Iteration 359/1000 | Loss: 0.00002655
Iteration 360/1000 | Loss: 0.00002655
Iteration 361/1000 | Loss: 0.00002655
Iteration 362/1000 | Loss: 0.00002655
Iteration 363/1000 | Loss: 0.00002655
Iteration 364/1000 | Loss: 0.00002655
Iteration 365/1000 | Loss: 0.00002655
Iteration 366/1000 | Loss: 0.00002655
Iteration 367/1000 | Loss: 0.00002654
Iteration 368/1000 | Loss: 0.00002654
Iteration 369/1000 | Loss: 0.00002654
Iteration 370/1000 | Loss: 0.00002654
Iteration 371/1000 | Loss: 0.00002653
Iteration 372/1000 | Loss: 0.00002653
Iteration 373/1000 | Loss: 0.00002653
Iteration 374/1000 | Loss: 0.00002653
Iteration 375/1000 | Loss: 0.00002653
Iteration 376/1000 | Loss: 0.00002653
Iteration 377/1000 | Loss: 0.00002653
Iteration 378/1000 | Loss: 0.00002653
Iteration 379/1000 | Loss: 0.00002653
Iteration 380/1000 | Loss: 0.00002653
Iteration 381/1000 | Loss: 0.00002653
Iteration 382/1000 | Loss: 0.00002652
Iteration 383/1000 | Loss: 0.00002652
Iteration 384/1000 | Loss: 0.00002652
Iteration 385/1000 | Loss: 0.00002652
Iteration 386/1000 | Loss: 0.00002652
Iteration 387/1000 | Loss: 0.00002652
Iteration 388/1000 | Loss: 0.00002652
Iteration 389/1000 | Loss: 0.00002652
Iteration 390/1000 | Loss: 0.00002652
Iteration 391/1000 | Loss: 0.00002652
Iteration 392/1000 | Loss: 0.00002652
Iteration 393/1000 | Loss: 0.00002652
Iteration 394/1000 | Loss: 0.00002652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 394. Stopping optimization.
Last 5 losses: [2.6519321181694977e-05, 2.6519321181694977e-05, 2.6519321181694977e-05, 2.6519321181694977e-05, 2.6519321181694977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6519321181694977e-05

Optimization complete. Final v2v error: 4.005911827087402 mm

Highest mean error: 6.75988245010376 mm for frame 48

Lowest mean error: 3.241987943649292 mm for frame 123

Saving results

Total time: 425.8328683376312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979828
Iteration 2/25 | Loss: 0.00238796
Iteration 3/25 | Loss: 0.00208582
Iteration 4/25 | Loss: 0.00182727
Iteration 5/25 | Loss: 0.00154674
Iteration 6/25 | Loss: 0.00198398
Iteration 7/25 | Loss: 0.00154849
Iteration 8/25 | Loss: 0.00128601
Iteration 9/25 | Loss: 0.00123620
Iteration 10/25 | Loss: 0.00122384
Iteration 11/25 | Loss: 0.00121993
Iteration 12/25 | Loss: 0.00121829
Iteration 13/25 | Loss: 0.00121707
Iteration 14/25 | Loss: 0.00121654
Iteration 15/25 | Loss: 0.00122265
Iteration 16/25 | Loss: 0.00121559
Iteration 17/25 | Loss: 0.00121419
Iteration 18/25 | Loss: 0.00121388
Iteration 19/25 | Loss: 0.00121379
Iteration 20/25 | Loss: 0.00121378
Iteration 21/25 | Loss: 0.00121378
Iteration 22/25 | Loss: 0.00121378
Iteration 23/25 | Loss: 0.00121378
Iteration 24/25 | Loss: 0.00121377
Iteration 25/25 | Loss: 0.00121376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34099054
Iteration 2/25 | Loss: 0.00073358
Iteration 3/25 | Loss: 0.00073358
Iteration 4/25 | Loss: 0.00073358
Iteration 5/25 | Loss: 0.00073358
Iteration 6/25 | Loss: 0.00073358
Iteration 7/25 | Loss: 0.00073358
Iteration 8/25 | Loss: 0.00073358
Iteration 9/25 | Loss: 0.00073358
Iteration 10/25 | Loss: 0.00073358
Iteration 11/25 | Loss: 0.00073358
Iteration 12/25 | Loss: 0.00073358
Iteration 13/25 | Loss: 0.00073358
Iteration 14/25 | Loss: 0.00073358
Iteration 15/25 | Loss: 0.00073358
Iteration 16/25 | Loss: 0.00073358
Iteration 17/25 | Loss: 0.00073358
Iteration 18/25 | Loss: 0.00073358
Iteration 19/25 | Loss: 0.00073358
Iteration 20/25 | Loss: 0.00073358
Iteration 21/25 | Loss: 0.00073358
Iteration 22/25 | Loss: 0.00073358
Iteration 23/25 | Loss: 0.00073358
Iteration 24/25 | Loss: 0.00073358
Iteration 25/25 | Loss: 0.00073358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073358
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002540
Iteration 5/1000 | Loss: 0.00002448
Iteration 6/1000 | Loss: 0.00002389
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00002289
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002264
Iteration 11/1000 | Loss: 0.00002245
Iteration 12/1000 | Loss: 0.00002235
Iteration 13/1000 | Loss: 0.00002233
Iteration 14/1000 | Loss: 0.00002229
Iteration 15/1000 | Loss: 0.00002224
Iteration 16/1000 | Loss: 0.00002215
Iteration 17/1000 | Loss: 0.00002209
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002208
Iteration 20/1000 | Loss: 0.00002208
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002205
Iteration 23/1000 | Loss: 0.00002205
Iteration 24/1000 | Loss: 0.00002205
Iteration 25/1000 | Loss: 0.00002204
Iteration 26/1000 | Loss: 0.00002204
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002203
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002203
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002202
Iteration 34/1000 | Loss: 0.00002202
Iteration 35/1000 | Loss: 0.00002202
Iteration 36/1000 | Loss: 0.00002202
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002201
Iteration 41/1000 | Loss: 0.00002201
Iteration 42/1000 | Loss: 0.00002201
Iteration 43/1000 | Loss: 0.00002201
Iteration 44/1000 | Loss: 0.00002201
Iteration 45/1000 | Loss: 0.00002201
Iteration 46/1000 | Loss: 0.00002201
Iteration 47/1000 | Loss: 0.00002201
Iteration 48/1000 | Loss: 0.00002201
Iteration 49/1000 | Loss: 0.00002200
Iteration 50/1000 | Loss: 0.00002200
Iteration 51/1000 | Loss: 0.00002200
Iteration 52/1000 | Loss: 0.00002200
Iteration 53/1000 | Loss: 0.00002200
Iteration 54/1000 | Loss: 0.00002200
Iteration 55/1000 | Loss: 0.00002200
Iteration 56/1000 | Loss: 0.00002200
Iteration 57/1000 | Loss: 0.00002200
Iteration 58/1000 | Loss: 0.00002200
Iteration 59/1000 | Loss: 0.00002200
Iteration 60/1000 | Loss: 0.00002200
Iteration 61/1000 | Loss: 0.00002200
Iteration 62/1000 | Loss: 0.00002200
Iteration 63/1000 | Loss: 0.00002199
Iteration 64/1000 | Loss: 0.00002199
Iteration 65/1000 | Loss: 0.00002199
Iteration 66/1000 | Loss: 0.00002199
Iteration 67/1000 | Loss: 0.00002199
Iteration 68/1000 | Loss: 0.00002199
Iteration 69/1000 | Loss: 0.00002199
Iteration 70/1000 | Loss: 0.00002199
Iteration 71/1000 | Loss: 0.00002199
Iteration 72/1000 | Loss: 0.00002199
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002199
Iteration 75/1000 | Loss: 0.00002199
Iteration 76/1000 | Loss: 0.00002199
Iteration 77/1000 | Loss: 0.00002199
Iteration 78/1000 | Loss: 0.00002199
Iteration 79/1000 | Loss: 0.00002199
Iteration 80/1000 | Loss: 0.00002199
Iteration 81/1000 | Loss: 0.00002199
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002198
Iteration 84/1000 | Loss: 0.00002198
Iteration 85/1000 | Loss: 0.00002198
Iteration 86/1000 | Loss: 0.00002198
Iteration 87/1000 | Loss: 0.00002198
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002197
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002197
Iteration 94/1000 | Loss: 0.00002197
Iteration 95/1000 | Loss: 0.00002197
Iteration 96/1000 | Loss: 0.00002197
Iteration 97/1000 | Loss: 0.00002197
Iteration 98/1000 | Loss: 0.00002197
Iteration 99/1000 | Loss: 0.00002197
Iteration 100/1000 | Loss: 0.00002197
Iteration 101/1000 | Loss: 0.00002197
Iteration 102/1000 | Loss: 0.00002197
Iteration 103/1000 | Loss: 0.00002197
Iteration 104/1000 | Loss: 0.00002197
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002197
Iteration 108/1000 | Loss: 0.00002197
Iteration 109/1000 | Loss: 0.00002197
Iteration 110/1000 | Loss: 0.00002197
Iteration 111/1000 | Loss: 0.00002197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.1973359253024682e-05, 2.1973359253024682e-05, 2.1973359253024682e-05, 2.1973359253024682e-05, 2.1973359253024682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1973359253024682e-05

Optimization complete. Final v2v error: 4.011590003967285 mm

Highest mean error: 4.106390476226807 mm for frame 115

Lowest mean error: 3.6573808193206787 mm for frame 0

Saving results

Total time: 55.85898280143738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448695
Iteration 2/25 | Loss: 0.00120990
Iteration 3/25 | Loss: 0.00112228
Iteration 4/25 | Loss: 0.00110581
Iteration 5/25 | Loss: 0.00110085
Iteration 6/25 | Loss: 0.00110025
Iteration 7/25 | Loss: 0.00110025
Iteration 8/25 | Loss: 0.00110025
Iteration 9/25 | Loss: 0.00110025
Iteration 10/25 | Loss: 0.00110025
Iteration 11/25 | Loss: 0.00110025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001100247260183096, 0.001100247260183096, 0.001100247260183096, 0.001100247260183096, 0.001100247260183096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001100247260183096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34466410
Iteration 2/25 | Loss: 0.00075873
Iteration 3/25 | Loss: 0.00075873
Iteration 4/25 | Loss: 0.00075872
Iteration 5/25 | Loss: 0.00075872
Iteration 6/25 | Loss: 0.00075872
Iteration 7/25 | Loss: 0.00075872
Iteration 8/25 | Loss: 0.00075872
Iteration 9/25 | Loss: 0.00075872
Iteration 10/25 | Loss: 0.00075872
Iteration 11/25 | Loss: 0.00075872
Iteration 12/25 | Loss: 0.00075872
Iteration 13/25 | Loss: 0.00075872
Iteration 14/25 | Loss: 0.00075872
Iteration 15/25 | Loss: 0.00075872
Iteration 16/25 | Loss: 0.00075872
Iteration 17/25 | Loss: 0.00075872
Iteration 18/25 | Loss: 0.00075872
Iteration 19/25 | Loss: 0.00075872
Iteration 20/25 | Loss: 0.00075872
Iteration 21/25 | Loss: 0.00075872
Iteration 22/25 | Loss: 0.00075872
Iteration 23/25 | Loss: 0.00075872
Iteration 24/25 | Loss: 0.00075872
Iteration 25/25 | Loss: 0.00075872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075872
Iteration 2/1000 | Loss: 0.00002927
Iteration 3/1000 | Loss: 0.00002174
Iteration 4/1000 | Loss: 0.00001928
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00001758
Iteration 7/1000 | Loss: 0.00001708
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001621
Iteration 10/1000 | Loss: 0.00001600
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001557
Iteration 15/1000 | Loss: 0.00001547
Iteration 16/1000 | Loss: 0.00001536
Iteration 17/1000 | Loss: 0.00001533
Iteration 18/1000 | Loss: 0.00001526
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001523
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001521
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001519
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001516
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001515
Iteration 31/1000 | Loss: 0.00001514
Iteration 32/1000 | Loss: 0.00001513
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001505
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001502
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001489
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001486
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001480
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001478
Iteration 96/1000 | Loss: 0.00001478
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001477
Iteration 101/1000 | Loss: 0.00001477
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001477
Iteration 106/1000 | Loss: 0.00001477
Iteration 107/1000 | Loss: 0.00001477
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001474
Iteration 150/1000 | Loss: 0.00001474
Iteration 151/1000 | Loss: 0.00001474
Iteration 152/1000 | Loss: 0.00001474
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4739174730493687e-05, 1.4739174730493687e-05, 1.4739174730493687e-05, 1.4739174730493687e-05, 1.4739174730493687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4739174730493687e-05

Optimization complete. Final v2v error: 3.275411367416382 mm

Highest mean error: 3.413876533508301 mm for frame 117

Lowest mean error: 3.109776020050049 mm for frame 52

Saving results

Total time: 38.6962411403656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391423
Iteration 2/25 | Loss: 0.00112404
Iteration 3/25 | Loss: 0.00105314
Iteration 4/25 | Loss: 0.00104530
Iteration 5/25 | Loss: 0.00104347
Iteration 6/25 | Loss: 0.00104347
Iteration 7/25 | Loss: 0.00104347
Iteration 8/25 | Loss: 0.00104347
Iteration 9/25 | Loss: 0.00104347
Iteration 10/25 | Loss: 0.00104347
Iteration 11/25 | Loss: 0.00104347
Iteration 12/25 | Loss: 0.00104347
Iteration 13/25 | Loss: 0.00104347
Iteration 14/25 | Loss: 0.00104347
Iteration 15/25 | Loss: 0.00104347
Iteration 16/25 | Loss: 0.00104347
Iteration 17/25 | Loss: 0.00104347
Iteration 18/25 | Loss: 0.00104347
Iteration 19/25 | Loss: 0.00104347
Iteration 20/25 | Loss: 0.00104347
Iteration 21/25 | Loss: 0.00104347
Iteration 22/25 | Loss: 0.00104347
Iteration 23/25 | Loss: 0.00104347
Iteration 24/25 | Loss: 0.00104347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001043471391312778, 0.001043471391312778, 0.001043471391312778, 0.001043471391312778, 0.001043471391312778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043471391312778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.01932931
Iteration 2/25 | Loss: 0.00075418
Iteration 3/25 | Loss: 0.00075417
Iteration 4/25 | Loss: 0.00075417
Iteration 5/25 | Loss: 0.00075417
Iteration 6/25 | Loss: 0.00075417
Iteration 7/25 | Loss: 0.00075417
Iteration 8/25 | Loss: 0.00075417
Iteration 9/25 | Loss: 0.00075417
Iteration 10/25 | Loss: 0.00075417
Iteration 11/25 | Loss: 0.00075417
Iteration 12/25 | Loss: 0.00075417
Iteration 13/25 | Loss: 0.00075417
Iteration 14/25 | Loss: 0.00075417
Iteration 15/25 | Loss: 0.00075417
Iteration 16/25 | Loss: 0.00075417
Iteration 17/25 | Loss: 0.00075417
Iteration 18/25 | Loss: 0.00075417
Iteration 19/25 | Loss: 0.00075417
Iteration 20/25 | Loss: 0.00075417
Iteration 21/25 | Loss: 0.00075417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007541700615547597, 0.0007541700615547597, 0.0007541700615547597, 0.0007541700615547597, 0.0007541700615547597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007541700615547597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075417
Iteration 2/1000 | Loss: 0.00001977
Iteration 3/1000 | Loss: 0.00001282
Iteration 4/1000 | Loss: 0.00001090
Iteration 5/1000 | Loss: 0.00000992
Iteration 6/1000 | Loss: 0.00000938
Iteration 7/1000 | Loss: 0.00000904
Iteration 8/1000 | Loss: 0.00000891
Iteration 9/1000 | Loss: 0.00000888
Iteration 10/1000 | Loss: 0.00000866
Iteration 11/1000 | Loss: 0.00000866
Iteration 12/1000 | Loss: 0.00000843
Iteration 13/1000 | Loss: 0.00000837
Iteration 14/1000 | Loss: 0.00000831
Iteration 15/1000 | Loss: 0.00000830
Iteration 16/1000 | Loss: 0.00000826
Iteration 17/1000 | Loss: 0.00000825
Iteration 18/1000 | Loss: 0.00000824
Iteration 19/1000 | Loss: 0.00000823
Iteration 20/1000 | Loss: 0.00000822
Iteration 21/1000 | Loss: 0.00000821
Iteration 22/1000 | Loss: 0.00000819
Iteration 23/1000 | Loss: 0.00000818
Iteration 24/1000 | Loss: 0.00000817
Iteration 25/1000 | Loss: 0.00000817
Iteration 26/1000 | Loss: 0.00000816
Iteration 27/1000 | Loss: 0.00000816
Iteration 28/1000 | Loss: 0.00000815
Iteration 29/1000 | Loss: 0.00000813
Iteration 30/1000 | Loss: 0.00000812
Iteration 31/1000 | Loss: 0.00000811
Iteration 32/1000 | Loss: 0.00000810
Iteration 33/1000 | Loss: 0.00000810
Iteration 34/1000 | Loss: 0.00000808
Iteration 35/1000 | Loss: 0.00000808
Iteration 36/1000 | Loss: 0.00000808
Iteration 37/1000 | Loss: 0.00000807
Iteration 38/1000 | Loss: 0.00000807
Iteration 39/1000 | Loss: 0.00000805
Iteration 40/1000 | Loss: 0.00000804
Iteration 41/1000 | Loss: 0.00000803
Iteration 42/1000 | Loss: 0.00000803
Iteration 43/1000 | Loss: 0.00000802
Iteration 44/1000 | Loss: 0.00000801
Iteration 45/1000 | Loss: 0.00000801
Iteration 46/1000 | Loss: 0.00000801
Iteration 47/1000 | Loss: 0.00000800
Iteration 48/1000 | Loss: 0.00000800
Iteration 49/1000 | Loss: 0.00000799
Iteration 50/1000 | Loss: 0.00000799
Iteration 51/1000 | Loss: 0.00000798
Iteration 52/1000 | Loss: 0.00000798
Iteration 53/1000 | Loss: 0.00000797
Iteration 54/1000 | Loss: 0.00000797
Iteration 55/1000 | Loss: 0.00000797
Iteration 56/1000 | Loss: 0.00000797
Iteration 57/1000 | Loss: 0.00000797
Iteration 58/1000 | Loss: 0.00000797
Iteration 59/1000 | Loss: 0.00000797
Iteration 60/1000 | Loss: 0.00000797
Iteration 61/1000 | Loss: 0.00000797
Iteration 62/1000 | Loss: 0.00000797
Iteration 63/1000 | Loss: 0.00000796
Iteration 64/1000 | Loss: 0.00000796
Iteration 65/1000 | Loss: 0.00000796
Iteration 66/1000 | Loss: 0.00000796
Iteration 67/1000 | Loss: 0.00000796
Iteration 68/1000 | Loss: 0.00000796
Iteration 69/1000 | Loss: 0.00000796
Iteration 70/1000 | Loss: 0.00000795
Iteration 71/1000 | Loss: 0.00000795
Iteration 72/1000 | Loss: 0.00000794
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000793
Iteration 75/1000 | Loss: 0.00000793
Iteration 76/1000 | Loss: 0.00000793
Iteration 77/1000 | Loss: 0.00000793
Iteration 78/1000 | Loss: 0.00000793
Iteration 79/1000 | Loss: 0.00000792
Iteration 80/1000 | Loss: 0.00000792
Iteration 81/1000 | Loss: 0.00000792
Iteration 82/1000 | Loss: 0.00000792
Iteration 83/1000 | Loss: 0.00000791
Iteration 84/1000 | Loss: 0.00000791
Iteration 85/1000 | Loss: 0.00000790
Iteration 86/1000 | Loss: 0.00000789
Iteration 87/1000 | Loss: 0.00000789
Iteration 88/1000 | Loss: 0.00000788
Iteration 89/1000 | Loss: 0.00000788
Iteration 90/1000 | Loss: 0.00000788
Iteration 91/1000 | Loss: 0.00000788
Iteration 92/1000 | Loss: 0.00000788
Iteration 93/1000 | Loss: 0.00000788
Iteration 94/1000 | Loss: 0.00000788
Iteration 95/1000 | Loss: 0.00000788
Iteration 96/1000 | Loss: 0.00000788
Iteration 97/1000 | Loss: 0.00000787
Iteration 98/1000 | Loss: 0.00000785
Iteration 99/1000 | Loss: 0.00000785
Iteration 100/1000 | Loss: 0.00000785
Iteration 101/1000 | Loss: 0.00000785
Iteration 102/1000 | Loss: 0.00000785
Iteration 103/1000 | Loss: 0.00000785
Iteration 104/1000 | Loss: 0.00000785
Iteration 105/1000 | Loss: 0.00000785
Iteration 106/1000 | Loss: 0.00000785
Iteration 107/1000 | Loss: 0.00000785
Iteration 108/1000 | Loss: 0.00000784
Iteration 109/1000 | Loss: 0.00000784
Iteration 110/1000 | Loss: 0.00000784
Iteration 111/1000 | Loss: 0.00000784
Iteration 112/1000 | Loss: 0.00000784
Iteration 113/1000 | Loss: 0.00000783
Iteration 114/1000 | Loss: 0.00000783
Iteration 115/1000 | Loss: 0.00000783
Iteration 116/1000 | Loss: 0.00000783
Iteration 117/1000 | Loss: 0.00000782
Iteration 118/1000 | Loss: 0.00000782
Iteration 119/1000 | Loss: 0.00000782
Iteration 120/1000 | Loss: 0.00000782
Iteration 121/1000 | Loss: 0.00000782
Iteration 122/1000 | Loss: 0.00000781
Iteration 123/1000 | Loss: 0.00000781
Iteration 124/1000 | Loss: 0.00000781
Iteration 125/1000 | Loss: 0.00000781
Iteration 126/1000 | Loss: 0.00000781
Iteration 127/1000 | Loss: 0.00000781
Iteration 128/1000 | Loss: 0.00000781
Iteration 129/1000 | Loss: 0.00000780
Iteration 130/1000 | Loss: 0.00000780
Iteration 131/1000 | Loss: 0.00000779
Iteration 132/1000 | Loss: 0.00000779
Iteration 133/1000 | Loss: 0.00000778
Iteration 134/1000 | Loss: 0.00000778
Iteration 135/1000 | Loss: 0.00000778
Iteration 136/1000 | Loss: 0.00000778
Iteration 137/1000 | Loss: 0.00000777
Iteration 138/1000 | Loss: 0.00000777
Iteration 139/1000 | Loss: 0.00000777
Iteration 140/1000 | Loss: 0.00000777
Iteration 141/1000 | Loss: 0.00000777
Iteration 142/1000 | Loss: 0.00000777
Iteration 143/1000 | Loss: 0.00000777
Iteration 144/1000 | Loss: 0.00000777
Iteration 145/1000 | Loss: 0.00000776
Iteration 146/1000 | Loss: 0.00000776
Iteration 147/1000 | Loss: 0.00000776
Iteration 148/1000 | Loss: 0.00000775
Iteration 149/1000 | Loss: 0.00000775
Iteration 150/1000 | Loss: 0.00000774
Iteration 151/1000 | Loss: 0.00000774
Iteration 152/1000 | Loss: 0.00000774
Iteration 153/1000 | Loss: 0.00000774
Iteration 154/1000 | Loss: 0.00000774
Iteration 155/1000 | Loss: 0.00000774
Iteration 156/1000 | Loss: 0.00000773
Iteration 157/1000 | Loss: 0.00000773
Iteration 158/1000 | Loss: 0.00000773
Iteration 159/1000 | Loss: 0.00000773
Iteration 160/1000 | Loss: 0.00000772
Iteration 161/1000 | Loss: 0.00000772
Iteration 162/1000 | Loss: 0.00000772
Iteration 163/1000 | Loss: 0.00000772
Iteration 164/1000 | Loss: 0.00000772
Iteration 165/1000 | Loss: 0.00000772
Iteration 166/1000 | Loss: 0.00000772
Iteration 167/1000 | Loss: 0.00000772
Iteration 168/1000 | Loss: 0.00000772
Iteration 169/1000 | Loss: 0.00000772
Iteration 170/1000 | Loss: 0.00000771
Iteration 171/1000 | Loss: 0.00000771
Iteration 172/1000 | Loss: 0.00000771
Iteration 173/1000 | Loss: 0.00000771
Iteration 174/1000 | Loss: 0.00000771
Iteration 175/1000 | Loss: 0.00000771
Iteration 176/1000 | Loss: 0.00000771
Iteration 177/1000 | Loss: 0.00000771
Iteration 178/1000 | Loss: 0.00000771
Iteration 179/1000 | Loss: 0.00000771
Iteration 180/1000 | Loss: 0.00000771
Iteration 181/1000 | Loss: 0.00000771
Iteration 182/1000 | Loss: 0.00000771
Iteration 183/1000 | Loss: 0.00000770
Iteration 184/1000 | Loss: 0.00000770
Iteration 185/1000 | Loss: 0.00000770
Iteration 186/1000 | Loss: 0.00000770
Iteration 187/1000 | Loss: 0.00000770
Iteration 188/1000 | Loss: 0.00000770
Iteration 189/1000 | Loss: 0.00000770
Iteration 190/1000 | Loss: 0.00000770
Iteration 191/1000 | Loss: 0.00000770
Iteration 192/1000 | Loss: 0.00000770
Iteration 193/1000 | Loss: 0.00000770
Iteration 194/1000 | Loss: 0.00000770
Iteration 195/1000 | Loss: 0.00000769
Iteration 196/1000 | Loss: 0.00000769
Iteration 197/1000 | Loss: 0.00000769
Iteration 198/1000 | Loss: 0.00000769
Iteration 199/1000 | Loss: 0.00000769
Iteration 200/1000 | Loss: 0.00000769
Iteration 201/1000 | Loss: 0.00000769
Iteration 202/1000 | Loss: 0.00000769
Iteration 203/1000 | Loss: 0.00000769
Iteration 204/1000 | Loss: 0.00000769
Iteration 205/1000 | Loss: 0.00000768
Iteration 206/1000 | Loss: 0.00000768
Iteration 207/1000 | Loss: 0.00000768
Iteration 208/1000 | Loss: 0.00000768
Iteration 209/1000 | Loss: 0.00000768
Iteration 210/1000 | Loss: 0.00000768
Iteration 211/1000 | Loss: 0.00000768
Iteration 212/1000 | Loss: 0.00000767
Iteration 213/1000 | Loss: 0.00000767
Iteration 214/1000 | Loss: 0.00000767
Iteration 215/1000 | Loss: 0.00000767
Iteration 216/1000 | Loss: 0.00000767
Iteration 217/1000 | Loss: 0.00000767
Iteration 218/1000 | Loss: 0.00000767
Iteration 219/1000 | Loss: 0.00000767
Iteration 220/1000 | Loss: 0.00000767
Iteration 221/1000 | Loss: 0.00000767
Iteration 222/1000 | Loss: 0.00000767
Iteration 223/1000 | Loss: 0.00000767
Iteration 224/1000 | Loss: 0.00000767
Iteration 225/1000 | Loss: 0.00000766
Iteration 226/1000 | Loss: 0.00000766
Iteration 227/1000 | Loss: 0.00000766
Iteration 228/1000 | Loss: 0.00000766
Iteration 229/1000 | Loss: 0.00000766
Iteration 230/1000 | Loss: 0.00000766
Iteration 231/1000 | Loss: 0.00000766
Iteration 232/1000 | Loss: 0.00000766
Iteration 233/1000 | Loss: 0.00000766
Iteration 234/1000 | Loss: 0.00000766
Iteration 235/1000 | Loss: 0.00000766
Iteration 236/1000 | Loss: 0.00000766
Iteration 237/1000 | Loss: 0.00000766
Iteration 238/1000 | Loss: 0.00000766
Iteration 239/1000 | Loss: 0.00000766
Iteration 240/1000 | Loss: 0.00000765
Iteration 241/1000 | Loss: 0.00000765
Iteration 242/1000 | Loss: 0.00000765
Iteration 243/1000 | Loss: 0.00000765
Iteration 244/1000 | Loss: 0.00000765
Iteration 245/1000 | Loss: 0.00000765
Iteration 246/1000 | Loss: 0.00000765
Iteration 247/1000 | Loss: 0.00000765
Iteration 248/1000 | Loss: 0.00000765
Iteration 249/1000 | Loss: 0.00000765
Iteration 250/1000 | Loss: 0.00000765
Iteration 251/1000 | Loss: 0.00000765
Iteration 252/1000 | Loss: 0.00000765
Iteration 253/1000 | Loss: 0.00000765
Iteration 254/1000 | Loss: 0.00000765
Iteration 255/1000 | Loss: 0.00000765
Iteration 256/1000 | Loss: 0.00000765
Iteration 257/1000 | Loss: 0.00000764
Iteration 258/1000 | Loss: 0.00000764
Iteration 259/1000 | Loss: 0.00000764
Iteration 260/1000 | Loss: 0.00000764
Iteration 261/1000 | Loss: 0.00000764
Iteration 262/1000 | Loss: 0.00000764
Iteration 263/1000 | Loss: 0.00000764
Iteration 264/1000 | Loss: 0.00000764
Iteration 265/1000 | Loss: 0.00000764
Iteration 266/1000 | Loss: 0.00000764
Iteration 267/1000 | Loss: 0.00000764
Iteration 268/1000 | Loss: 0.00000764
Iteration 269/1000 | Loss: 0.00000764
Iteration 270/1000 | Loss: 0.00000764
Iteration 271/1000 | Loss: 0.00000764
Iteration 272/1000 | Loss: 0.00000764
Iteration 273/1000 | Loss: 0.00000763
Iteration 274/1000 | Loss: 0.00000763
Iteration 275/1000 | Loss: 0.00000763
Iteration 276/1000 | Loss: 0.00000763
Iteration 277/1000 | Loss: 0.00000763
Iteration 278/1000 | Loss: 0.00000763
Iteration 279/1000 | Loss: 0.00000763
Iteration 280/1000 | Loss: 0.00000763
Iteration 281/1000 | Loss: 0.00000763
Iteration 282/1000 | Loss: 0.00000763
Iteration 283/1000 | Loss: 0.00000763
Iteration 284/1000 | Loss: 0.00000763
Iteration 285/1000 | Loss: 0.00000763
Iteration 286/1000 | Loss: 0.00000763
Iteration 287/1000 | Loss: 0.00000763
Iteration 288/1000 | Loss: 0.00000763
Iteration 289/1000 | Loss: 0.00000763
Iteration 290/1000 | Loss: 0.00000763
Iteration 291/1000 | Loss: 0.00000763
Iteration 292/1000 | Loss: 0.00000763
Iteration 293/1000 | Loss: 0.00000763
Iteration 294/1000 | Loss: 0.00000763
Iteration 295/1000 | Loss: 0.00000763
Iteration 296/1000 | Loss: 0.00000763
Iteration 297/1000 | Loss: 0.00000763
Iteration 298/1000 | Loss: 0.00000763
Iteration 299/1000 | Loss: 0.00000763
Iteration 300/1000 | Loss: 0.00000763
Iteration 301/1000 | Loss: 0.00000763
Iteration 302/1000 | Loss: 0.00000763
Iteration 303/1000 | Loss: 0.00000763
Iteration 304/1000 | Loss: 0.00000763
Iteration 305/1000 | Loss: 0.00000763
Iteration 306/1000 | Loss: 0.00000763
Iteration 307/1000 | Loss: 0.00000763
Iteration 308/1000 | Loss: 0.00000763
Iteration 309/1000 | Loss: 0.00000763
Iteration 310/1000 | Loss: 0.00000763
Iteration 311/1000 | Loss: 0.00000763
Iteration 312/1000 | Loss: 0.00000763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 312. Stopping optimization.
Last 5 losses: [7.632812412339263e-06, 7.632812412339263e-06, 7.632812412339263e-06, 7.632812412339263e-06, 7.632812412339263e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.632812412339263e-06

Optimization complete. Final v2v error: 2.3673267364501953 mm

Highest mean error: 2.476884603500366 mm for frame 104

Lowest mean error: 2.289842367172241 mm for frame 17

Saving results

Total time: 43.610079765319824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055444
Iteration 2/25 | Loss: 0.00151063
Iteration 3/25 | Loss: 0.00125876
Iteration 4/25 | Loss: 0.00123930
Iteration 5/25 | Loss: 0.00119879
Iteration 6/25 | Loss: 0.00110603
Iteration 7/25 | Loss: 0.00111190
Iteration 8/25 | Loss: 0.00109567
Iteration 9/25 | Loss: 0.00106713
Iteration 10/25 | Loss: 0.00105151
Iteration 11/25 | Loss: 0.00104778
Iteration 12/25 | Loss: 0.00108371
Iteration 13/25 | Loss: 0.00106317
Iteration 14/25 | Loss: 0.00106618
Iteration 15/25 | Loss: 0.00103100
Iteration 16/25 | Loss: 0.00102768
Iteration 17/25 | Loss: 0.00102675
Iteration 18/25 | Loss: 0.00102666
Iteration 19/25 | Loss: 0.00102665
Iteration 20/25 | Loss: 0.00102664
Iteration 21/25 | Loss: 0.00102664
Iteration 22/25 | Loss: 0.00102664
Iteration 23/25 | Loss: 0.00102664
Iteration 24/25 | Loss: 0.00102664
Iteration 25/25 | Loss: 0.00102664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27222180
Iteration 2/25 | Loss: 0.00130137
Iteration 3/25 | Loss: 0.00106231
Iteration 4/25 | Loss: 0.00106231
Iteration 5/25 | Loss: 0.00106231
Iteration 6/25 | Loss: 0.00106231
Iteration 7/25 | Loss: 0.00106231
Iteration 8/25 | Loss: 0.00106231
Iteration 9/25 | Loss: 0.00106231
Iteration 10/25 | Loss: 0.00106231
Iteration 11/25 | Loss: 0.00106231
Iteration 12/25 | Loss: 0.00106231
Iteration 13/25 | Loss: 0.00106231
Iteration 14/25 | Loss: 0.00106231
Iteration 15/25 | Loss: 0.00106231
Iteration 16/25 | Loss: 0.00106231
Iteration 17/25 | Loss: 0.00106231
Iteration 18/25 | Loss: 0.00106231
Iteration 19/25 | Loss: 0.00106231
Iteration 20/25 | Loss: 0.00106231
Iteration 21/25 | Loss: 0.00106231
Iteration 22/25 | Loss: 0.00106231
Iteration 23/25 | Loss: 0.00106231
Iteration 24/25 | Loss: 0.00106231
Iteration 25/25 | Loss: 0.00106231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106231
Iteration 2/1000 | Loss: 0.00011342
Iteration 3/1000 | Loss: 0.00014397
Iteration 4/1000 | Loss: 0.00001954
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00003035
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001454
Iteration 12/1000 | Loss: 0.00001435
Iteration 13/1000 | Loss: 0.00001416
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001349
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001326
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001324
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001323
Iteration 45/1000 | Loss: 0.00001323
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001310
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001309
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001299
Iteration 97/1000 | Loss: 0.00001299
Iteration 98/1000 | Loss: 0.00001299
Iteration 99/1000 | Loss: 0.00001299
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001298
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001297
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001295
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001295
Iteration 128/1000 | Loss: 0.00001295
Iteration 129/1000 | Loss: 0.00001295
Iteration 130/1000 | Loss: 0.00001295
Iteration 131/1000 | Loss: 0.00001295
Iteration 132/1000 | Loss: 0.00001295
Iteration 133/1000 | Loss: 0.00001295
Iteration 134/1000 | Loss: 0.00001295
Iteration 135/1000 | Loss: 0.00001295
Iteration 136/1000 | Loss: 0.00001295
Iteration 137/1000 | Loss: 0.00001295
Iteration 138/1000 | Loss: 0.00001295
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001294
Iteration 141/1000 | Loss: 0.00001294
Iteration 142/1000 | Loss: 0.00001294
Iteration 143/1000 | Loss: 0.00001294
Iteration 144/1000 | Loss: 0.00001294
Iteration 145/1000 | Loss: 0.00001294
Iteration 146/1000 | Loss: 0.00001294
Iteration 147/1000 | Loss: 0.00001293
Iteration 148/1000 | Loss: 0.00001293
Iteration 149/1000 | Loss: 0.00001293
Iteration 150/1000 | Loss: 0.00001293
Iteration 151/1000 | Loss: 0.00001293
Iteration 152/1000 | Loss: 0.00001293
Iteration 153/1000 | Loss: 0.00001293
Iteration 154/1000 | Loss: 0.00001293
Iteration 155/1000 | Loss: 0.00001293
Iteration 156/1000 | Loss: 0.00001293
Iteration 157/1000 | Loss: 0.00001293
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001292
Iteration 163/1000 | Loss: 0.00001292
Iteration 164/1000 | Loss: 0.00001292
Iteration 165/1000 | Loss: 0.00001292
Iteration 166/1000 | Loss: 0.00001292
Iteration 167/1000 | Loss: 0.00001292
Iteration 168/1000 | Loss: 0.00001291
Iteration 169/1000 | Loss: 0.00001291
Iteration 170/1000 | Loss: 0.00001291
Iteration 171/1000 | Loss: 0.00001291
Iteration 172/1000 | Loss: 0.00001291
Iteration 173/1000 | Loss: 0.00001291
Iteration 174/1000 | Loss: 0.00001291
Iteration 175/1000 | Loss: 0.00001291
Iteration 176/1000 | Loss: 0.00001291
Iteration 177/1000 | Loss: 0.00001291
Iteration 178/1000 | Loss: 0.00001291
Iteration 179/1000 | Loss: 0.00001291
Iteration 180/1000 | Loss: 0.00001291
Iteration 181/1000 | Loss: 0.00001291
Iteration 182/1000 | Loss: 0.00001291
Iteration 183/1000 | Loss: 0.00001291
Iteration 184/1000 | Loss: 0.00001291
Iteration 185/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2913447790197097e-05, 1.2913447790197097e-05, 1.2913447790197097e-05, 1.2913447790197097e-05, 1.2913447790197097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2913447790197097e-05

Optimization complete. Final v2v error: 2.8012654781341553 mm

Highest mean error: 11.697158813476562 mm for frame 150

Lowest mean error: 2.525183916091919 mm for frame 86

Saving results

Total time: 71.21380281448364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444088
Iteration 2/25 | Loss: 0.00117254
Iteration 3/25 | Loss: 0.00108142
Iteration 4/25 | Loss: 0.00106410
Iteration 5/25 | Loss: 0.00105824
Iteration 6/25 | Loss: 0.00105650
Iteration 7/25 | Loss: 0.00105603
Iteration 8/25 | Loss: 0.00105603
Iteration 9/25 | Loss: 0.00105603
Iteration 10/25 | Loss: 0.00105603
Iteration 11/25 | Loss: 0.00105603
Iteration 12/25 | Loss: 0.00105603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010560312075540423, 0.0010560312075540423, 0.0010560312075540423, 0.0010560312075540423, 0.0010560312075540423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010560312075540423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23399842
Iteration 2/25 | Loss: 0.00080592
Iteration 3/25 | Loss: 0.00080589
Iteration 4/25 | Loss: 0.00080589
Iteration 5/25 | Loss: 0.00080589
Iteration 6/25 | Loss: 0.00080589
Iteration 7/25 | Loss: 0.00080589
Iteration 8/25 | Loss: 0.00080589
Iteration 9/25 | Loss: 0.00080589
Iteration 10/25 | Loss: 0.00080589
Iteration 11/25 | Loss: 0.00080589
Iteration 12/25 | Loss: 0.00080589
Iteration 13/25 | Loss: 0.00080589
Iteration 14/25 | Loss: 0.00080589
Iteration 15/25 | Loss: 0.00080589
Iteration 16/25 | Loss: 0.00080589
Iteration 17/25 | Loss: 0.00080589
Iteration 18/25 | Loss: 0.00080589
Iteration 19/25 | Loss: 0.00080589
Iteration 20/25 | Loss: 0.00080589
Iteration 21/25 | Loss: 0.00080589
Iteration 22/25 | Loss: 0.00080589
Iteration 23/25 | Loss: 0.00080589
Iteration 24/25 | Loss: 0.00080589
Iteration 25/25 | Loss: 0.00080589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080589
Iteration 2/1000 | Loss: 0.00003708
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001260
Iteration 8/1000 | Loss: 0.00001208
Iteration 9/1000 | Loss: 0.00001165
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001148
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001107
Iteration 27/1000 | Loss: 0.00001103
Iteration 28/1000 | Loss: 0.00001102
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001100
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001085
Iteration 47/1000 | Loss: 0.00001085
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001083
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001082
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001073
Iteration 82/1000 | Loss: 0.00001072
Iteration 83/1000 | Loss: 0.00001072
Iteration 84/1000 | Loss: 0.00001072
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001070
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001070
Iteration 106/1000 | Loss: 0.00001070
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001069
Iteration 110/1000 | Loss: 0.00001069
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001068
Iteration 113/1000 | Loss: 0.00001068
Iteration 114/1000 | Loss: 0.00001068
Iteration 115/1000 | Loss: 0.00001068
Iteration 116/1000 | Loss: 0.00001068
Iteration 117/1000 | Loss: 0.00001068
Iteration 118/1000 | Loss: 0.00001068
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001066
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001066
Iteration 126/1000 | Loss: 0.00001066
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001065
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001062
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001062
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001062
Iteration 172/1000 | Loss: 0.00001062
Iteration 173/1000 | Loss: 0.00001062
Iteration 174/1000 | Loss: 0.00001062
Iteration 175/1000 | Loss: 0.00001062
Iteration 176/1000 | Loss: 0.00001062
Iteration 177/1000 | Loss: 0.00001062
Iteration 178/1000 | Loss: 0.00001062
Iteration 179/1000 | Loss: 0.00001062
Iteration 180/1000 | Loss: 0.00001062
Iteration 181/1000 | Loss: 0.00001062
Iteration 182/1000 | Loss: 0.00001062
Iteration 183/1000 | Loss: 0.00001062
Iteration 184/1000 | Loss: 0.00001062
Iteration 185/1000 | Loss: 0.00001062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.0624368769640569e-05, 1.0624368769640569e-05, 1.0624368769640569e-05, 1.0624368769640569e-05, 1.0624368769640569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0624368769640569e-05

Optimization complete. Final v2v error: 2.8088221549987793 mm

Highest mean error: 3.299539566040039 mm for frame 57

Lowest mean error: 2.508443593978882 mm for frame 17

Saving results

Total time: 39.79096174240112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435312
Iteration 2/25 | Loss: 0.00141044
Iteration 3/25 | Loss: 0.00116261
Iteration 4/25 | Loss: 0.00112503
Iteration 5/25 | Loss: 0.00111800
Iteration 6/25 | Loss: 0.00111609
Iteration 7/25 | Loss: 0.00111609
Iteration 8/25 | Loss: 0.00111609
Iteration 9/25 | Loss: 0.00111609
Iteration 10/25 | Loss: 0.00111609
Iteration 11/25 | Loss: 0.00111609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001116091152653098, 0.001116091152653098, 0.001116091152653098, 0.001116091152653098, 0.001116091152653098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001116091152653098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34260023
Iteration 2/25 | Loss: 0.00075048
Iteration 3/25 | Loss: 0.00075048
Iteration 4/25 | Loss: 0.00075047
Iteration 5/25 | Loss: 0.00075047
Iteration 6/25 | Loss: 0.00075047
Iteration 7/25 | Loss: 0.00075047
Iteration 8/25 | Loss: 0.00075047
Iteration 9/25 | Loss: 0.00075047
Iteration 10/25 | Loss: 0.00075047
Iteration 11/25 | Loss: 0.00075047
Iteration 12/25 | Loss: 0.00075047
Iteration 13/25 | Loss: 0.00075047
Iteration 14/25 | Loss: 0.00075047
Iteration 15/25 | Loss: 0.00075047
Iteration 16/25 | Loss: 0.00075047
Iteration 17/25 | Loss: 0.00075047
Iteration 18/25 | Loss: 0.00075047
Iteration 19/25 | Loss: 0.00075047
Iteration 20/25 | Loss: 0.00075047
Iteration 21/25 | Loss: 0.00075047
Iteration 22/25 | Loss: 0.00075047
Iteration 23/25 | Loss: 0.00075047
Iteration 24/25 | Loss: 0.00075047
Iteration 25/25 | Loss: 0.00075047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075047
Iteration 2/1000 | Loss: 0.00003419
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00001895
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001678
Iteration 8/1000 | Loss: 0.00001646
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001604
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001574
Iteration 17/1000 | Loss: 0.00001573
Iteration 18/1000 | Loss: 0.00001572
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001565
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001556
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001550
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001548
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001547
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001544
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001540
Iteration 47/1000 | Loss: 0.00001537
Iteration 48/1000 | Loss: 0.00001537
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001535
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001533
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001529
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001523
Iteration 104/1000 | Loss: 0.00001523
Iteration 105/1000 | Loss: 0.00001523
Iteration 106/1000 | Loss: 0.00001523
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001523
Iteration 110/1000 | Loss: 0.00001523
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001522
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001522
Iteration 125/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.5221472494886257e-05, 1.5221472494886257e-05, 1.5221472494886257e-05, 1.5221472494886257e-05, 1.5221472494886257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5221472494886257e-05

Optimization complete. Final v2v error: 3.2868292331695557 mm

Highest mean error: 4.504461288452148 mm for frame 203

Lowest mean error: 2.8053486347198486 mm for frame 97

Saving results

Total time: 41.52385210990906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423149
Iteration 2/25 | Loss: 0.00116289
Iteration 3/25 | Loss: 0.00108850
Iteration 4/25 | Loss: 0.00107635
Iteration 5/25 | Loss: 0.00107208
Iteration 6/25 | Loss: 0.00107095
Iteration 7/25 | Loss: 0.00107095
Iteration 8/25 | Loss: 0.00107095
Iteration 9/25 | Loss: 0.00107095
Iteration 10/25 | Loss: 0.00107095
Iteration 11/25 | Loss: 0.00107095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010709509951993823, 0.0010709509951993823, 0.0010709509951993823, 0.0010709509951993823, 0.0010709509951993823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010709509951993823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79880500
Iteration 2/25 | Loss: 0.00087597
Iteration 3/25 | Loss: 0.00087595
Iteration 4/25 | Loss: 0.00087595
Iteration 5/25 | Loss: 0.00087595
Iteration 6/25 | Loss: 0.00087595
Iteration 7/25 | Loss: 0.00087595
Iteration 8/25 | Loss: 0.00087595
Iteration 9/25 | Loss: 0.00087595
Iteration 10/25 | Loss: 0.00087594
Iteration 11/25 | Loss: 0.00087594
Iteration 12/25 | Loss: 0.00087594
Iteration 13/25 | Loss: 0.00087594
Iteration 14/25 | Loss: 0.00087594
Iteration 15/25 | Loss: 0.00087594
Iteration 16/25 | Loss: 0.00087594
Iteration 17/25 | Loss: 0.00087594
Iteration 18/25 | Loss: 0.00087594
Iteration 19/25 | Loss: 0.00087594
Iteration 20/25 | Loss: 0.00087594
Iteration 21/25 | Loss: 0.00087594
Iteration 22/25 | Loss: 0.00087594
Iteration 23/25 | Loss: 0.00087594
Iteration 24/25 | Loss: 0.00087594
Iteration 25/25 | Loss: 0.00087594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087594
Iteration 2/1000 | Loss: 0.00002878
Iteration 3/1000 | Loss: 0.00001757
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001130
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001099
Iteration 17/1000 | Loss: 0.00001098
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001096
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001095
Iteration 24/1000 | Loss: 0.00001094
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001092
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001082
Iteration 47/1000 | Loss: 0.00001082
Iteration 48/1000 | Loss: 0.00001082
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001081
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001075
Iteration 63/1000 | Loss: 0.00001075
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001072
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001072
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001070
Iteration 81/1000 | Loss: 0.00001070
Iteration 82/1000 | Loss: 0.00001070
Iteration 83/1000 | Loss: 0.00001070
Iteration 84/1000 | Loss: 0.00001070
Iteration 85/1000 | Loss: 0.00001070
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001069
Iteration 90/1000 | Loss: 0.00001069
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001065
Iteration 111/1000 | Loss: 0.00001065
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001063
Iteration 119/1000 | Loss: 0.00001063
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001063
Iteration 127/1000 | Loss: 0.00001063
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001062
Iteration 130/1000 | Loss: 0.00001062
Iteration 131/1000 | Loss: 0.00001062
Iteration 132/1000 | Loss: 0.00001062
Iteration 133/1000 | Loss: 0.00001062
Iteration 134/1000 | Loss: 0.00001062
Iteration 135/1000 | Loss: 0.00001062
Iteration 136/1000 | Loss: 0.00001062
Iteration 137/1000 | Loss: 0.00001062
Iteration 138/1000 | Loss: 0.00001062
Iteration 139/1000 | Loss: 0.00001062
Iteration 140/1000 | Loss: 0.00001062
Iteration 141/1000 | Loss: 0.00001062
Iteration 142/1000 | Loss: 0.00001062
Iteration 143/1000 | Loss: 0.00001062
Iteration 144/1000 | Loss: 0.00001062
Iteration 145/1000 | Loss: 0.00001062
Iteration 146/1000 | Loss: 0.00001062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.0615369319566526e-05, 1.0615369319566526e-05, 1.0615369319566526e-05, 1.0615369319566526e-05, 1.0615369319566526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0615369319566526e-05

Optimization complete. Final v2v error: 2.7751517295837402 mm

Highest mean error: 3.1821749210357666 mm for frame 82

Lowest mean error: 2.378481149673462 mm for frame 5

Saving results

Total time: 37.505497217178345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866996
Iteration 2/25 | Loss: 0.00313981
Iteration 3/25 | Loss: 0.00220488
Iteration 4/25 | Loss: 0.00173227
Iteration 5/25 | Loss: 0.00165692
Iteration 6/25 | Loss: 0.00165107
Iteration 7/25 | Loss: 0.00165506
Iteration 8/25 | Loss: 0.00159161
Iteration 9/25 | Loss: 0.00151014
Iteration 10/25 | Loss: 0.00147916
Iteration 11/25 | Loss: 0.00144880
Iteration 12/25 | Loss: 0.00142608
Iteration 13/25 | Loss: 0.00143646
Iteration 14/25 | Loss: 0.00141073
Iteration 15/25 | Loss: 0.00139807
Iteration 16/25 | Loss: 0.00141763
Iteration 17/25 | Loss: 0.00141506
Iteration 18/25 | Loss: 0.00138317
Iteration 19/25 | Loss: 0.00139987
Iteration 20/25 | Loss: 0.00139716
Iteration 21/25 | Loss: 0.00137311
Iteration 22/25 | Loss: 0.00136800
Iteration 23/25 | Loss: 0.00136721
Iteration 24/25 | Loss: 0.00136703
Iteration 25/25 | Loss: 0.00136683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37007856
Iteration 2/25 | Loss: 0.00501918
Iteration 3/25 | Loss: 0.00484952
Iteration 4/25 | Loss: 0.00471944
Iteration 5/25 | Loss: 0.00363342
Iteration 6/25 | Loss: 0.00363286
Iteration 7/25 | Loss: 0.00363286
Iteration 8/25 | Loss: 0.00363285
Iteration 9/25 | Loss: 0.00363285
Iteration 10/25 | Loss: 0.00363285
Iteration 11/25 | Loss: 0.00363285
Iteration 12/25 | Loss: 0.00363285
Iteration 13/25 | Loss: 0.00363285
Iteration 14/25 | Loss: 0.00363285
Iteration 15/25 | Loss: 0.00363285
Iteration 16/25 | Loss: 0.00363285
Iteration 17/25 | Loss: 0.00363285
Iteration 18/25 | Loss: 0.00363285
Iteration 19/25 | Loss: 0.00363285
Iteration 20/25 | Loss: 0.00363285
Iteration 21/25 | Loss: 0.00363285
Iteration 22/25 | Loss: 0.00363285
Iteration 23/25 | Loss: 0.00363285
Iteration 24/25 | Loss: 0.00363285
Iteration 25/25 | Loss: 0.00363285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00363285
Iteration 2/1000 | Loss: 0.00173641
Iteration 3/1000 | Loss: 0.00108878
Iteration 4/1000 | Loss: 0.00388373
Iteration 5/1000 | Loss: 0.00285550
Iteration 6/1000 | Loss: 0.00100706
Iteration 7/1000 | Loss: 0.00133473
Iteration 8/1000 | Loss: 0.00304765
Iteration 9/1000 | Loss: 0.00159021
Iteration 10/1000 | Loss: 0.00063992
Iteration 11/1000 | Loss: 0.00165876
Iteration 12/1000 | Loss: 0.00165163
Iteration 13/1000 | Loss: 0.00071381
Iteration 14/1000 | Loss: 0.00079931
Iteration 15/1000 | Loss: 0.00150638
Iteration 16/1000 | Loss: 0.00043762
Iteration 17/1000 | Loss: 0.00165026
Iteration 18/1000 | Loss: 0.00142063
Iteration 19/1000 | Loss: 0.00074279
Iteration 20/1000 | Loss: 0.00039211
Iteration 21/1000 | Loss: 0.00012093
Iteration 22/1000 | Loss: 0.00066908
Iteration 23/1000 | Loss: 0.00105973
Iteration 24/1000 | Loss: 0.00032150
Iteration 25/1000 | Loss: 0.00018899
Iteration 26/1000 | Loss: 0.00129497
Iteration 27/1000 | Loss: 0.00048062
Iteration 28/1000 | Loss: 0.00376372
Iteration 29/1000 | Loss: 0.00279455
Iteration 30/1000 | Loss: 0.00304352
Iteration 31/1000 | Loss: 0.00179250
Iteration 32/1000 | Loss: 0.00278031
Iteration 33/1000 | Loss: 0.00202464
Iteration 34/1000 | Loss: 0.00201706
Iteration 35/1000 | Loss: 0.00348970
Iteration 36/1000 | Loss: 0.00212391
Iteration 37/1000 | Loss: 0.00284684
Iteration 38/1000 | Loss: 0.00204728
Iteration 39/1000 | Loss: 0.00164833
Iteration 40/1000 | Loss: 0.00099302
Iteration 41/1000 | Loss: 0.00100315
Iteration 42/1000 | Loss: 0.00050278
Iteration 43/1000 | Loss: 0.00148590
Iteration 44/1000 | Loss: 0.00103663
Iteration 45/1000 | Loss: 0.00045146
Iteration 46/1000 | Loss: 0.00028957
Iteration 47/1000 | Loss: 0.00012629
Iteration 48/1000 | Loss: 0.00006714
Iteration 49/1000 | Loss: 0.00015107
Iteration 50/1000 | Loss: 0.00011894
Iteration 51/1000 | Loss: 0.00009831
Iteration 52/1000 | Loss: 0.00006894
Iteration 53/1000 | Loss: 0.00005586
Iteration 54/1000 | Loss: 0.00005087
Iteration 55/1000 | Loss: 0.00060126
Iteration 56/1000 | Loss: 0.00016410
Iteration 57/1000 | Loss: 0.00005691
Iteration 58/1000 | Loss: 0.00005092
Iteration 59/1000 | Loss: 0.00014615
Iteration 60/1000 | Loss: 0.00012118
Iteration 61/1000 | Loss: 0.00004796
Iteration 62/1000 | Loss: 0.00004550
Iteration 63/1000 | Loss: 0.00014724
Iteration 64/1000 | Loss: 0.00054250
Iteration 65/1000 | Loss: 0.00027516
Iteration 66/1000 | Loss: 0.00008747
Iteration 67/1000 | Loss: 0.00004484
Iteration 68/1000 | Loss: 0.00004352
Iteration 69/1000 | Loss: 0.00028080
Iteration 70/1000 | Loss: 0.00182664
Iteration 71/1000 | Loss: 0.00051808
Iteration 72/1000 | Loss: 0.00056800
Iteration 73/1000 | Loss: 0.00056097
Iteration 74/1000 | Loss: 0.00006006
Iteration 75/1000 | Loss: 0.00004719
Iteration 76/1000 | Loss: 0.00004244
Iteration 77/1000 | Loss: 0.00003981
Iteration 78/1000 | Loss: 0.00043043
Iteration 79/1000 | Loss: 0.00096534
Iteration 80/1000 | Loss: 0.00063579
Iteration 81/1000 | Loss: 0.00015699
Iteration 82/1000 | Loss: 0.00027005
Iteration 83/1000 | Loss: 0.00005090
Iteration 84/1000 | Loss: 0.00059054
Iteration 85/1000 | Loss: 0.00006071
Iteration 86/1000 | Loss: 0.00003700
Iteration 87/1000 | Loss: 0.00003267
Iteration 88/1000 | Loss: 0.00003073
Iteration 89/1000 | Loss: 0.00002924
Iteration 90/1000 | Loss: 0.00002826
Iteration 91/1000 | Loss: 0.00002740
Iteration 92/1000 | Loss: 0.00002688
Iteration 93/1000 | Loss: 0.00023101
Iteration 94/1000 | Loss: 0.00002837
Iteration 95/1000 | Loss: 0.00002621
Iteration 96/1000 | Loss: 0.00002530
Iteration 97/1000 | Loss: 0.00025909
Iteration 98/1000 | Loss: 0.00048575
Iteration 99/1000 | Loss: 0.00036601
Iteration 100/1000 | Loss: 0.00002832
Iteration 101/1000 | Loss: 0.00002463
Iteration 102/1000 | Loss: 0.00002366
Iteration 103/1000 | Loss: 0.00002280
Iteration 104/1000 | Loss: 0.00002228
Iteration 105/1000 | Loss: 0.00053739
Iteration 106/1000 | Loss: 0.00045693
Iteration 107/1000 | Loss: 0.00014485
Iteration 108/1000 | Loss: 0.00002193
Iteration 109/1000 | Loss: 0.00041156
Iteration 110/1000 | Loss: 0.00003289
Iteration 111/1000 | Loss: 0.00002866
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002153
Iteration 114/1000 | Loss: 0.00016726
Iteration 115/1000 | Loss: 0.00005972
Iteration 116/1000 | Loss: 0.00016621
Iteration 117/1000 | Loss: 0.00023855
Iteration 118/1000 | Loss: 0.00003037
Iteration 119/1000 | Loss: 0.00019552
Iteration 120/1000 | Loss: 0.00019387
Iteration 121/1000 | Loss: 0.00002719
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001867
Iteration 125/1000 | Loss: 0.00013062
Iteration 126/1000 | Loss: 0.00007699
Iteration 127/1000 | Loss: 0.00017093
Iteration 128/1000 | Loss: 0.00019590
Iteration 129/1000 | Loss: 0.00007346
Iteration 130/1000 | Loss: 0.00016794
Iteration 131/1000 | Loss: 0.00017210
Iteration 132/1000 | Loss: 0.00015286
Iteration 133/1000 | Loss: 0.00014520
Iteration 134/1000 | Loss: 0.00068383
Iteration 135/1000 | Loss: 0.00027625
Iteration 136/1000 | Loss: 0.00002492
Iteration 137/1000 | Loss: 0.00018897
Iteration 138/1000 | Loss: 0.00019905
Iteration 139/1000 | Loss: 0.00061441
Iteration 140/1000 | Loss: 0.00015650
Iteration 141/1000 | Loss: 0.00018825
Iteration 142/1000 | Loss: 0.00012989
Iteration 143/1000 | Loss: 0.00003023
Iteration 144/1000 | Loss: 0.00017245
Iteration 145/1000 | Loss: 0.00022332
Iteration 146/1000 | Loss: 0.00026736
Iteration 147/1000 | Loss: 0.00022214
Iteration 148/1000 | Loss: 0.00020866
Iteration 149/1000 | Loss: 0.00051364
Iteration 150/1000 | Loss: 0.00017931
Iteration 151/1000 | Loss: 0.00021318
Iteration 152/1000 | Loss: 0.00022022
Iteration 153/1000 | Loss: 0.00021429
Iteration 154/1000 | Loss: 0.00020410
Iteration 155/1000 | Loss: 0.00022103
Iteration 156/1000 | Loss: 0.00004199
Iteration 157/1000 | Loss: 0.00003026
Iteration 158/1000 | Loss: 0.00002469
Iteration 159/1000 | Loss: 0.00002252
Iteration 160/1000 | Loss: 0.00002154
Iteration 161/1000 | Loss: 0.00002063
Iteration 162/1000 | Loss: 0.00002001
Iteration 163/1000 | Loss: 0.00001944
Iteration 164/1000 | Loss: 0.00038706
Iteration 165/1000 | Loss: 0.00034968
Iteration 166/1000 | Loss: 0.00015264
Iteration 167/1000 | Loss: 0.00045949
Iteration 168/1000 | Loss: 0.00032756
Iteration 169/1000 | Loss: 0.00007298
Iteration 170/1000 | Loss: 0.00001970
Iteration 171/1000 | Loss: 0.00001913
Iteration 172/1000 | Loss: 0.00001899
Iteration 173/1000 | Loss: 0.00001896
Iteration 174/1000 | Loss: 0.00001881
Iteration 175/1000 | Loss: 0.00001875
Iteration 176/1000 | Loss: 0.00001873
Iteration 177/1000 | Loss: 0.00001873
Iteration 178/1000 | Loss: 0.00001872
Iteration 179/1000 | Loss: 0.00001871
Iteration 180/1000 | Loss: 0.00001870
Iteration 181/1000 | Loss: 0.00001869
Iteration 182/1000 | Loss: 0.00001863
Iteration 183/1000 | Loss: 0.00001859
Iteration 184/1000 | Loss: 0.00001858
Iteration 185/1000 | Loss: 0.00001858
Iteration 186/1000 | Loss: 0.00001857
Iteration 187/1000 | Loss: 0.00001857
Iteration 188/1000 | Loss: 0.00001857
Iteration 189/1000 | Loss: 0.00001856
Iteration 190/1000 | Loss: 0.00001856
Iteration 191/1000 | Loss: 0.00001856
Iteration 192/1000 | Loss: 0.00001855
Iteration 193/1000 | Loss: 0.00001854
Iteration 194/1000 | Loss: 0.00001854
Iteration 195/1000 | Loss: 0.00001853
Iteration 196/1000 | Loss: 0.00001853
Iteration 197/1000 | Loss: 0.00001853
Iteration 198/1000 | Loss: 0.00001852
Iteration 199/1000 | Loss: 0.00001852
Iteration 200/1000 | Loss: 0.00001851
Iteration 201/1000 | Loss: 0.00001850
Iteration 202/1000 | Loss: 0.00040561
Iteration 203/1000 | Loss: 0.00023250
Iteration 204/1000 | Loss: 0.00002375
Iteration 205/1000 | Loss: 0.00007209
Iteration 206/1000 | Loss: 0.00023009
Iteration 207/1000 | Loss: 0.00032400
Iteration 208/1000 | Loss: 0.00003165
Iteration 209/1000 | Loss: 0.00002516
Iteration 210/1000 | Loss: 0.00024425
Iteration 211/1000 | Loss: 0.00005225
Iteration 212/1000 | Loss: 0.00018419
Iteration 213/1000 | Loss: 0.00002914
Iteration 214/1000 | Loss: 0.00038638
Iteration 215/1000 | Loss: 0.00002937
Iteration 216/1000 | Loss: 0.00012336
Iteration 217/1000 | Loss: 0.00002840
Iteration 218/1000 | Loss: 0.00002176
Iteration 219/1000 | Loss: 0.00002040
Iteration 220/1000 | Loss: 0.00001898
Iteration 221/1000 | Loss: 0.00001806
Iteration 222/1000 | Loss: 0.00001760
Iteration 223/1000 | Loss: 0.00001727
Iteration 224/1000 | Loss: 0.00001695
Iteration 225/1000 | Loss: 0.00001685
Iteration 226/1000 | Loss: 0.00001684
Iteration 227/1000 | Loss: 0.00001664
Iteration 228/1000 | Loss: 0.00001644
Iteration 229/1000 | Loss: 0.00001623
Iteration 230/1000 | Loss: 0.00001602
Iteration 231/1000 | Loss: 0.00001597
Iteration 232/1000 | Loss: 0.00001595
Iteration 233/1000 | Loss: 0.00001594
Iteration 234/1000 | Loss: 0.00001594
Iteration 235/1000 | Loss: 0.00001594
Iteration 236/1000 | Loss: 0.00001592
Iteration 237/1000 | Loss: 0.00001579
Iteration 238/1000 | Loss: 0.00001577
Iteration 239/1000 | Loss: 0.00001576
Iteration 240/1000 | Loss: 0.00001576
Iteration 241/1000 | Loss: 0.00025824
Iteration 242/1000 | Loss: 0.00001621
Iteration 243/1000 | Loss: 0.00001550
Iteration 244/1000 | Loss: 0.00001493
Iteration 245/1000 | Loss: 0.00001448
Iteration 246/1000 | Loss: 0.00001428
Iteration 247/1000 | Loss: 0.00001412
Iteration 248/1000 | Loss: 0.00001409
Iteration 249/1000 | Loss: 0.00001408
Iteration 250/1000 | Loss: 0.00001408
Iteration 251/1000 | Loss: 0.00001408
Iteration 252/1000 | Loss: 0.00001407
Iteration 253/1000 | Loss: 0.00001407
Iteration 254/1000 | Loss: 0.00001407
Iteration 255/1000 | Loss: 0.00001406
Iteration 256/1000 | Loss: 0.00001406
Iteration 257/1000 | Loss: 0.00001406
Iteration 258/1000 | Loss: 0.00001406
Iteration 259/1000 | Loss: 0.00001406
Iteration 260/1000 | Loss: 0.00001406
Iteration 261/1000 | Loss: 0.00001405
Iteration 262/1000 | Loss: 0.00001405
Iteration 263/1000 | Loss: 0.00001405
Iteration 264/1000 | Loss: 0.00001405
Iteration 265/1000 | Loss: 0.00001404
Iteration 266/1000 | Loss: 0.00001403
Iteration 267/1000 | Loss: 0.00001403
Iteration 268/1000 | Loss: 0.00001403
Iteration 269/1000 | Loss: 0.00001403
Iteration 270/1000 | Loss: 0.00001403
Iteration 271/1000 | Loss: 0.00001403
Iteration 272/1000 | Loss: 0.00001403
Iteration 273/1000 | Loss: 0.00001403
Iteration 274/1000 | Loss: 0.00001403
Iteration 275/1000 | Loss: 0.00001402
Iteration 276/1000 | Loss: 0.00001402
Iteration 277/1000 | Loss: 0.00001402
Iteration 278/1000 | Loss: 0.00001402
Iteration 279/1000 | Loss: 0.00001402
Iteration 280/1000 | Loss: 0.00001402
Iteration 281/1000 | Loss: 0.00001402
Iteration 282/1000 | Loss: 0.00001401
Iteration 283/1000 | Loss: 0.00001401
Iteration 284/1000 | Loss: 0.00001400
Iteration 285/1000 | Loss: 0.00001400
Iteration 286/1000 | Loss: 0.00001400
Iteration 287/1000 | Loss: 0.00001399
Iteration 288/1000 | Loss: 0.00001399
Iteration 289/1000 | Loss: 0.00001399
Iteration 290/1000 | Loss: 0.00001399
Iteration 291/1000 | Loss: 0.00001399
Iteration 292/1000 | Loss: 0.00001399
Iteration 293/1000 | Loss: 0.00001399
Iteration 294/1000 | Loss: 0.00001399
Iteration 295/1000 | Loss: 0.00001399
Iteration 296/1000 | Loss: 0.00001399
Iteration 297/1000 | Loss: 0.00001399
Iteration 298/1000 | Loss: 0.00001399
Iteration 299/1000 | Loss: 0.00001399
Iteration 300/1000 | Loss: 0.00001398
Iteration 301/1000 | Loss: 0.00001398
Iteration 302/1000 | Loss: 0.00001398
Iteration 303/1000 | Loss: 0.00001398
Iteration 304/1000 | Loss: 0.00001398
Iteration 305/1000 | Loss: 0.00001397
Iteration 306/1000 | Loss: 0.00001397
Iteration 307/1000 | Loss: 0.00001397
Iteration 308/1000 | Loss: 0.00001397
Iteration 309/1000 | Loss: 0.00001397
Iteration 310/1000 | Loss: 0.00001396
Iteration 311/1000 | Loss: 0.00001396
Iteration 312/1000 | Loss: 0.00001396
Iteration 313/1000 | Loss: 0.00001396
Iteration 314/1000 | Loss: 0.00001396
Iteration 315/1000 | Loss: 0.00001396
Iteration 316/1000 | Loss: 0.00001396
Iteration 317/1000 | Loss: 0.00001396
Iteration 318/1000 | Loss: 0.00001396
Iteration 319/1000 | Loss: 0.00001396
Iteration 320/1000 | Loss: 0.00001396
Iteration 321/1000 | Loss: 0.00001396
Iteration 322/1000 | Loss: 0.00001395
Iteration 323/1000 | Loss: 0.00001395
Iteration 324/1000 | Loss: 0.00001395
Iteration 325/1000 | Loss: 0.00001395
Iteration 326/1000 | Loss: 0.00001395
Iteration 327/1000 | Loss: 0.00001395
Iteration 328/1000 | Loss: 0.00001395
Iteration 329/1000 | Loss: 0.00001395
Iteration 330/1000 | Loss: 0.00001395
Iteration 331/1000 | Loss: 0.00001395
Iteration 332/1000 | Loss: 0.00001395
Iteration 333/1000 | Loss: 0.00001395
Iteration 334/1000 | Loss: 0.00001395
Iteration 335/1000 | Loss: 0.00001395
Iteration 336/1000 | Loss: 0.00001395
Iteration 337/1000 | Loss: 0.00001395
Iteration 338/1000 | Loss: 0.00001395
Iteration 339/1000 | Loss: 0.00001395
Iteration 340/1000 | Loss: 0.00001395
Iteration 341/1000 | Loss: 0.00001395
Iteration 342/1000 | Loss: 0.00001394
Iteration 343/1000 | Loss: 0.00001394
Iteration 344/1000 | Loss: 0.00001394
Iteration 345/1000 | Loss: 0.00001394
Iteration 346/1000 | Loss: 0.00001394
Iteration 347/1000 | Loss: 0.00001394
Iteration 348/1000 | Loss: 0.00001394
Iteration 349/1000 | Loss: 0.00001394
Iteration 350/1000 | Loss: 0.00001394
Iteration 351/1000 | Loss: 0.00001394
Iteration 352/1000 | Loss: 0.00001394
Iteration 353/1000 | Loss: 0.00001394
Iteration 354/1000 | Loss: 0.00001394
Iteration 355/1000 | Loss: 0.00001394
Iteration 356/1000 | Loss: 0.00001394
Iteration 357/1000 | Loss: 0.00001394
Iteration 358/1000 | Loss: 0.00001394
Iteration 359/1000 | Loss: 0.00001394
Iteration 360/1000 | Loss: 0.00001394
Iteration 361/1000 | Loss: 0.00001394
Iteration 362/1000 | Loss: 0.00001394
Iteration 363/1000 | Loss: 0.00001394
Iteration 364/1000 | Loss: 0.00001393
Iteration 365/1000 | Loss: 0.00001393
Iteration 366/1000 | Loss: 0.00001393
Iteration 367/1000 | Loss: 0.00001393
Iteration 368/1000 | Loss: 0.00001393
Iteration 369/1000 | Loss: 0.00001393
Iteration 370/1000 | Loss: 0.00001393
Iteration 371/1000 | Loss: 0.00001393
Iteration 372/1000 | Loss: 0.00001393
Iteration 373/1000 | Loss: 0.00001393
Iteration 374/1000 | Loss: 0.00001393
Iteration 375/1000 | Loss: 0.00001393
Iteration 376/1000 | Loss: 0.00001393
Iteration 377/1000 | Loss: 0.00001393
Iteration 378/1000 | Loss: 0.00001393
Iteration 379/1000 | Loss: 0.00001393
Iteration 380/1000 | Loss: 0.00001393
Iteration 381/1000 | Loss: 0.00001393
Iteration 382/1000 | Loss: 0.00001393
Iteration 383/1000 | Loss: 0.00001393
Iteration 384/1000 | Loss: 0.00001393
Iteration 385/1000 | Loss: 0.00001393
Iteration 386/1000 | Loss: 0.00001393
Iteration 387/1000 | Loss: 0.00001393
Iteration 388/1000 | Loss: 0.00001393
Iteration 389/1000 | Loss: 0.00001393
Iteration 390/1000 | Loss: 0.00001393
Iteration 391/1000 | Loss: 0.00001393
Iteration 392/1000 | Loss: 0.00001393
Iteration 393/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [1.3925367966294289e-05, 1.3925367966294289e-05, 1.3925367966294289e-05, 1.3925367966294289e-05, 1.3925367966294289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3925367966294289e-05

Optimization complete. Final v2v error: 2.9630374908447266 mm

Highest mean error: 10.110289573669434 mm for frame 71

Lowest mean error: 2.482253074645996 mm for frame 153

Saving results

Total time: 398.14383816719055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585939
Iteration 2/25 | Loss: 0.00122243
Iteration 3/25 | Loss: 0.00114196
Iteration 4/25 | Loss: 0.00113485
Iteration 5/25 | Loss: 0.00113393
Iteration 6/25 | Loss: 0.00113393
Iteration 7/25 | Loss: 0.00113393
Iteration 8/25 | Loss: 0.00113393
Iteration 9/25 | Loss: 0.00113393
Iteration 10/25 | Loss: 0.00113393
Iteration 11/25 | Loss: 0.00113393
Iteration 12/25 | Loss: 0.00113393
Iteration 13/25 | Loss: 0.00113393
Iteration 14/25 | Loss: 0.00113393
Iteration 15/25 | Loss: 0.00113393
Iteration 16/25 | Loss: 0.00113393
Iteration 17/25 | Loss: 0.00113393
Iteration 18/25 | Loss: 0.00113393
Iteration 19/25 | Loss: 0.00113393
Iteration 20/25 | Loss: 0.00113393
Iteration 21/25 | Loss: 0.00113393
Iteration 22/25 | Loss: 0.00113393
Iteration 23/25 | Loss: 0.00113393
Iteration 24/25 | Loss: 0.00113393
Iteration 25/25 | Loss: 0.00113393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.72871828
Iteration 2/25 | Loss: 0.00084871
Iteration 3/25 | Loss: 0.00084868
Iteration 4/25 | Loss: 0.00084868
Iteration 5/25 | Loss: 0.00084868
Iteration 6/25 | Loss: 0.00084868
Iteration 7/25 | Loss: 0.00084868
Iteration 8/25 | Loss: 0.00084868
Iteration 9/25 | Loss: 0.00084868
Iteration 10/25 | Loss: 0.00084868
Iteration 11/25 | Loss: 0.00084868
Iteration 12/25 | Loss: 0.00084867
Iteration 13/25 | Loss: 0.00084867
Iteration 14/25 | Loss: 0.00084867
Iteration 15/25 | Loss: 0.00084867
Iteration 16/25 | Loss: 0.00084867
Iteration 17/25 | Loss: 0.00084867
Iteration 18/25 | Loss: 0.00084867
Iteration 19/25 | Loss: 0.00084867
Iteration 20/25 | Loss: 0.00084867
Iteration 21/25 | Loss: 0.00084867
Iteration 22/25 | Loss: 0.00084867
Iteration 23/25 | Loss: 0.00084867
Iteration 24/25 | Loss: 0.00084867
Iteration 25/25 | Loss: 0.00084867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084867
Iteration 2/1000 | Loss: 0.00002221
Iteration 3/1000 | Loss: 0.00001679
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001368
Iteration 13/1000 | Loss: 0.00001367
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001353
Iteration 16/1000 | Loss: 0.00001352
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001339
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001329
Iteration 35/1000 | Loss: 0.00001327
Iteration 36/1000 | Loss: 0.00001327
Iteration 37/1000 | Loss: 0.00001327
Iteration 38/1000 | Loss: 0.00001327
Iteration 39/1000 | Loss: 0.00001327
Iteration 40/1000 | Loss: 0.00001327
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001326
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001326
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001321
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001312
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001311
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001310
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001306
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001305
Iteration 102/1000 | Loss: 0.00001305
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001302
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001300
Iteration 120/1000 | Loss: 0.00001300
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001299
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001297
Iteration 135/1000 | Loss: 0.00001295
Iteration 136/1000 | Loss: 0.00001295
Iteration 137/1000 | Loss: 0.00001295
Iteration 138/1000 | Loss: 0.00001295
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001294
Iteration 141/1000 | Loss: 0.00001294
Iteration 142/1000 | Loss: 0.00001294
Iteration 143/1000 | Loss: 0.00001294
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001293
Iteration 146/1000 | Loss: 0.00001293
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001291
Iteration 152/1000 | Loss: 0.00001291
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001290
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001290
Iteration 158/1000 | Loss: 0.00001290
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001287
Iteration 166/1000 | Loss: 0.00001287
Iteration 167/1000 | Loss: 0.00001287
Iteration 168/1000 | Loss: 0.00001287
Iteration 169/1000 | Loss: 0.00001287
Iteration 170/1000 | Loss: 0.00001287
Iteration 171/1000 | Loss: 0.00001287
Iteration 172/1000 | Loss: 0.00001287
Iteration 173/1000 | Loss: 0.00001287
Iteration 174/1000 | Loss: 0.00001287
Iteration 175/1000 | Loss: 0.00001287
Iteration 176/1000 | Loss: 0.00001287
Iteration 177/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.287099985347595e-05, 1.287099985347595e-05, 1.287099985347595e-05, 1.287099985347595e-05, 1.287099985347595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.287099985347595e-05

Optimization complete. Final v2v error: 3.0393264293670654 mm

Highest mean error: 3.372426986694336 mm for frame 178

Lowest mean error: 2.7698895931243896 mm for frame 201

Saving results

Total time: 40.92771601676941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461781
Iteration 2/25 | Loss: 0.00130258
Iteration 3/25 | Loss: 0.00115183
Iteration 4/25 | Loss: 0.00113764
Iteration 5/25 | Loss: 0.00113345
Iteration 6/25 | Loss: 0.00113312
Iteration 7/25 | Loss: 0.00113312
Iteration 8/25 | Loss: 0.00113312
Iteration 9/25 | Loss: 0.00113312
Iteration 10/25 | Loss: 0.00113312
Iteration 11/25 | Loss: 0.00113312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001133118406869471, 0.001133118406869471, 0.001133118406869471, 0.001133118406869471, 0.001133118406869471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001133118406869471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37001812
Iteration 2/25 | Loss: 0.00077044
Iteration 3/25 | Loss: 0.00077044
Iteration 4/25 | Loss: 0.00077044
Iteration 5/25 | Loss: 0.00077044
Iteration 6/25 | Loss: 0.00077044
Iteration 7/25 | Loss: 0.00077044
Iteration 8/25 | Loss: 0.00077044
Iteration 9/25 | Loss: 0.00077043
Iteration 10/25 | Loss: 0.00077043
Iteration 11/25 | Loss: 0.00077043
Iteration 12/25 | Loss: 0.00077043
Iteration 13/25 | Loss: 0.00077043
Iteration 14/25 | Loss: 0.00077043
Iteration 15/25 | Loss: 0.00077043
Iteration 16/25 | Loss: 0.00077043
Iteration 17/25 | Loss: 0.00077043
Iteration 18/25 | Loss: 0.00077043
Iteration 19/25 | Loss: 0.00077043
Iteration 20/25 | Loss: 0.00077043
Iteration 21/25 | Loss: 0.00077043
Iteration 22/25 | Loss: 0.00077043
Iteration 23/25 | Loss: 0.00077043
Iteration 24/25 | Loss: 0.00077043
Iteration 25/25 | Loss: 0.00077043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077043
Iteration 2/1000 | Loss: 0.00003200
Iteration 3/1000 | Loss: 0.00002213
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001915
Iteration 6/1000 | Loss: 0.00001813
Iteration 7/1000 | Loss: 0.00001761
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001637
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001605
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001593
Iteration 32/1000 | Loss: 0.00001592
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001588
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001583
Iteration 45/1000 | Loss: 0.00001582
Iteration 46/1000 | Loss: 0.00001582
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001570
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001557
Iteration 132/1000 | Loss: 0.00001557
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5571145922876894e-05, 1.5571145922876894e-05, 1.5571145922876894e-05, 1.5571145922876894e-05, 1.5571145922876894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5571145922876894e-05

Optimization complete. Final v2v error: 3.32387113571167 mm

Highest mean error: 3.705897331237793 mm for frame 15

Lowest mean error: 2.8464620113372803 mm for frame 189

Saving results

Total time: 43.3885281085968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873342
Iteration 2/25 | Loss: 0.00127950
Iteration 3/25 | Loss: 0.00117476
Iteration 4/25 | Loss: 0.00115237
Iteration 5/25 | Loss: 0.00114463
Iteration 6/25 | Loss: 0.00114265
Iteration 7/25 | Loss: 0.00114243
Iteration 8/25 | Loss: 0.00114242
Iteration 9/25 | Loss: 0.00114242
Iteration 10/25 | Loss: 0.00114242
Iteration 11/25 | Loss: 0.00114242
Iteration 12/25 | Loss: 0.00114242
Iteration 13/25 | Loss: 0.00114242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011424164986237884, 0.0011424164986237884, 0.0011424164986237884, 0.0011424164986237884, 0.0011424164986237884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011424164986237884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37202144
Iteration 2/25 | Loss: 0.00082013
Iteration 3/25 | Loss: 0.00082010
Iteration 4/25 | Loss: 0.00082010
Iteration 5/25 | Loss: 0.00082009
Iteration 6/25 | Loss: 0.00082009
Iteration 7/25 | Loss: 0.00082009
Iteration 8/25 | Loss: 0.00082009
Iteration 9/25 | Loss: 0.00082009
Iteration 10/25 | Loss: 0.00082009
Iteration 11/25 | Loss: 0.00082009
Iteration 12/25 | Loss: 0.00082009
Iteration 13/25 | Loss: 0.00082009
Iteration 14/25 | Loss: 0.00082009
Iteration 15/25 | Loss: 0.00082009
Iteration 16/25 | Loss: 0.00082009
Iteration 17/25 | Loss: 0.00082009
Iteration 18/25 | Loss: 0.00082009
Iteration 19/25 | Loss: 0.00082009
Iteration 20/25 | Loss: 0.00082009
Iteration 21/25 | Loss: 0.00082009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008200930315069854, 0.0008200930315069854, 0.0008200930315069854, 0.0008200930315069854, 0.0008200930315069854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008200930315069854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082009
Iteration 2/1000 | Loss: 0.00004821
Iteration 3/1000 | Loss: 0.00003178
Iteration 4/1000 | Loss: 0.00002528
Iteration 5/1000 | Loss: 0.00002351
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002139
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00002019
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002001
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001989
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001985
Iteration 21/1000 | Loss: 0.00001983
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001981
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001980
Iteration 33/1000 | Loss: 0.00001980
Iteration 34/1000 | Loss: 0.00001980
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001979
Iteration 38/1000 | Loss: 0.00001979
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001979
Iteration 43/1000 | Loss: 0.00001978
Iteration 44/1000 | Loss: 0.00001978
Iteration 45/1000 | Loss: 0.00001978
Iteration 46/1000 | Loss: 0.00001978
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001977
Iteration 52/1000 | Loss: 0.00001977
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001975
Iteration 64/1000 | Loss: 0.00001975
Iteration 65/1000 | Loss: 0.00001975
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001974
Iteration 69/1000 | Loss: 0.00001974
Iteration 70/1000 | Loss: 0.00001974
Iteration 71/1000 | Loss: 0.00001973
Iteration 72/1000 | Loss: 0.00001973
Iteration 73/1000 | Loss: 0.00001973
Iteration 74/1000 | Loss: 0.00001973
Iteration 75/1000 | Loss: 0.00001973
Iteration 76/1000 | Loss: 0.00001973
Iteration 77/1000 | Loss: 0.00001973
Iteration 78/1000 | Loss: 0.00001973
Iteration 79/1000 | Loss: 0.00001973
Iteration 80/1000 | Loss: 0.00001972
Iteration 81/1000 | Loss: 0.00001972
Iteration 82/1000 | Loss: 0.00001972
Iteration 83/1000 | Loss: 0.00001972
Iteration 84/1000 | Loss: 0.00001972
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001969
Iteration 101/1000 | Loss: 0.00001969
Iteration 102/1000 | Loss: 0.00001969
Iteration 103/1000 | Loss: 0.00001969
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001968
Iteration 106/1000 | Loss: 0.00001968
Iteration 107/1000 | Loss: 0.00001968
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001966
Iteration 115/1000 | Loss: 0.00001966
Iteration 116/1000 | Loss: 0.00001966
Iteration 117/1000 | Loss: 0.00001966
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001966
Iteration 122/1000 | Loss: 0.00001966
Iteration 123/1000 | Loss: 0.00001965
Iteration 124/1000 | Loss: 0.00001965
Iteration 125/1000 | Loss: 0.00001965
Iteration 126/1000 | Loss: 0.00001965
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001964
Iteration 133/1000 | Loss: 0.00001964
Iteration 134/1000 | Loss: 0.00001964
Iteration 135/1000 | Loss: 0.00001964
Iteration 136/1000 | Loss: 0.00001964
Iteration 137/1000 | Loss: 0.00001964
Iteration 138/1000 | Loss: 0.00001964
Iteration 139/1000 | Loss: 0.00001964
Iteration 140/1000 | Loss: 0.00001964
Iteration 141/1000 | Loss: 0.00001964
Iteration 142/1000 | Loss: 0.00001964
Iteration 143/1000 | Loss: 0.00001964
Iteration 144/1000 | Loss: 0.00001964
Iteration 145/1000 | Loss: 0.00001964
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001964
Iteration 149/1000 | Loss: 0.00001964
Iteration 150/1000 | Loss: 0.00001964
Iteration 151/1000 | Loss: 0.00001964
Iteration 152/1000 | Loss: 0.00001964
Iteration 153/1000 | Loss: 0.00001964
Iteration 154/1000 | Loss: 0.00001964
Iteration 155/1000 | Loss: 0.00001964
Iteration 156/1000 | Loss: 0.00001964
Iteration 157/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.9641904145828448e-05, 1.9641904145828448e-05, 1.9641904145828448e-05, 1.9641904145828448e-05, 1.9641904145828448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9641904145828448e-05

Optimization complete. Final v2v error: 3.7051453590393066 mm

Highest mean error: 5.3360915184021 mm for frame 67

Lowest mean error: 3.0979936122894287 mm for frame 97

Saving results

Total time: 38.91477632522583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505847
Iteration 2/25 | Loss: 0.00127185
Iteration 3/25 | Loss: 0.00113874
Iteration 4/25 | Loss: 0.00111802
Iteration 5/25 | Loss: 0.00111301
Iteration 6/25 | Loss: 0.00111291
Iteration 7/25 | Loss: 0.00111291
Iteration 8/25 | Loss: 0.00111291
Iteration 9/25 | Loss: 0.00111291
Iteration 10/25 | Loss: 0.00111291
Iteration 11/25 | Loss: 0.00111291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001112910220399499, 0.001112910220399499, 0.001112910220399499, 0.001112910220399499, 0.001112910220399499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001112910220399499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32749236
Iteration 2/25 | Loss: 0.00079940
Iteration 3/25 | Loss: 0.00079937
Iteration 4/25 | Loss: 0.00079937
Iteration 5/25 | Loss: 0.00079937
Iteration 6/25 | Loss: 0.00079937
Iteration 7/25 | Loss: 0.00079936
Iteration 8/25 | Loss: 0.00079936
Iteration 9/25 | Loss: 0.00079936
Iteration 10/25 | Loss: 0.00079936
Iteration 11/25 | Loss: 0.00079936
Iteration 12/25 | Loss: 0.00079936
Iteration 13/25 | Loss: 0.00079936
Iteration 14/25 | Loss: 0.00079936
Iteration 15/25 | Loss: 0.00079936
Iteration 16/25 | Loss: 0.00079936
Iteration 17/25 | Loss: 0.00079936
Iteration 18/25 | Loss: 0.00079936
Iteration 19/25 | Loss: 0.00079936
Iteration 20/25 | Loss: 0.00079936
Iteration 21/25 | Loss: 0.00079936
Iteration 22/25 | Loss: 0.00079936
Iteration 23/25 | Loss: 0.00079936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007993635954335332, 0.0007993635954335332, 0.0007993635954335332, 0.0007993635954335332, 0.0007993635954335332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007993635954335332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079936
Iteration 2/1000 | Loss: 0.00002261
Iteration 3/1000 | Loss: 0.00001530
Iteration 4/1000 | Loss: 0.00001364
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001185
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001130
Iteration 13/1000 | Loss: 0.00001129
Iteration 14/1000 | Loss: 0.00001115
Iteration 15/1000 | Loss: 0.00001114
Iteration 16/1000 | Loss: 0.00001110
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001095
Iteration 25/1000 | Loss: 0.00001095
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001094
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001091
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001086
Iteration 50/1000 | Loss: 0.00001086
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001086
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001083
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001082
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001080
Iteration 105/1000 | Loss: 0.00001080
Iteration 106/1000 | Loss: 0.00001080
Iteration 107/1000 | Loss: 0.00001080
Iteration 108/1000 | Loss: 0.00001080
Iteration 109/1000 | Loss: 0.00001080
Iteration 110/1000 | Loss: 0.00001080
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001080
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.079540561477188e-05, 1.079540561477188e-05, 1.079540561477188e-05, 1.079540561477188e-05, 1.079540561477188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.079540561477188e-05

Optimization complete. Final v2v error: 2.8176043033599854 mm

Highest mean error: 3.010456085205078 mm for frame 148

Lowest mean error: 2.660741090774536 mm for frame 211

Saving results

Total time: 34.994033098220825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418008
Iteration 2/25 | Loss: 0.00122993
Iteration 3/25 | Loss: 0.00113415
Iteration 4/25 | Loss: 0.00111910
Iteration 5/25 | Loss: 0.00111448
Iteration 6/25 | Loss: 0.00111375
Iteration 7/25 | Loss: 0.00111371
Iteration 8/25 | Loss: 0.00111371
Iteration 9/25 | Loss: 0.00111371
Iteration 10/25 | Loss: 0.00111371
Iteration 11/25 | Loss: 0.00111371
Iteration 12/25 | Loss: 0.00111371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011137065012007952, 0.0011137065012007952, 0.0011137065012007952, 0.0011137065012007952, 0.0011137065012007952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011137065012007952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40032709
Iteration 2/25 | Loss: 0.00083380
Iteration 3/25 | Loss: 0.00083380
Iteration 4/25 | Loss: 0.00083380
Iteration 5/25 | Loss: 0.00083380
Iteration 6/25 | Loss: 0.00083380
Iteration 7/25 | Loss: 0.00083380
Iteration 8/25 | Loss: 0.00083380
Iteration 9/25 | Loss: 0.00083380
Iteration 10/25 | Loss: 0.00083380
Iteration 11/25 | Loss: 0.00083380
Iteration 12/25 | Loss: 0.00083380
Iteration 13/25 | Loss: 0.00083380
Iteration 14/25 | Loss: 0.00083380
Iteration 15/25 | Loss: 0.00083380
Iteration 16/25 | Loss: 0.00083380
Iteration 17/25 | Loss: 0.00083380
Iteration 18/25 | Loss: 0.00083380
Iteration 19/25 | Loss: 0.00083380
Iteration 20/25 | Loss: 0.00083380
Iteration 21/25 | Loss: 0.00083380
Iteration 22/25 | Loss: 0.00083380
Iteration 23/25 | Loss: 0.00083380
Iteration 24/25 | Loss: 0.00083380
Iteration 25/25 | Loss: 0.00083380

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083380
Iteration 2/1000 | Loss: 0.00002712
Iteration 3/1000 | Loss: 0.00001985
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001794
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001701
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001648
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001598
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001574
Iteration 14/1000 | Loss: 0.00001573
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001546
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001539
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001524
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001519
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001516
Iteration 61/1000 | Loss: 0.00001515
Iteration 62/1000 | Loss: 0.00001515
Iteration 63/1000 | Loss: 0.00001515
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001508
Iteration 80/1000 | Loss: 0.00001508
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001504
Iteration 102/1000 | Loss: 0.00001504
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001503
Iteration 105/1000 | Loss: 0.00001503
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001502
Iteration 110/1000 | Loss: 0.00001502
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001501
Iteration 116/1000 | Loss: 0.00001501
Iteration 117/1000 | Loss: 0.00001501
Iteration 118/1000 | Loss: 0.00001501
Iteration 119/1000 | Loss: 0.00001501
Iteration 120/1000 | Loss: 0.00001501
Iteration 121/1000 | Loss: 0.00001501
Iteration 122/1000 | Loss: 0.00001501
Iteration 123/1000 | Loss: 0.00001501
Iteration 124/1000 | Loss: 0.00001501
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001500
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001499
Iteration 132/1000 | Loss: 0.00001499
Iteration 133/1000 | Loss: 0.00001499
Iteration 134/1000 | Loss: 0.00001499
Iteration 135/1000 | Loss: 0.00001499
Iteration 136/1000 | Loss: 0.00001499
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001498
Iteration 157/1000 | Loss: 0.00001498
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001497
Iteration 163/1000 | Loss: 0.00001497
Iteration 164/1000 | Loss: 0.00001497
Iteration 165/1000 | Loss: 0.00001497
Iteration 166/1000 | Loss: 0.00001497
Iteration 167/1000 | Loss: 0.00001497
Iteration 168/1000 | Loss: 0.00001497
Iteration 169/1000 | Loss: 0.00001497
Iteration 170/1000 | Loss: 0.00001497
Iteration 171/1000 | Loss: 0.00001497
Iteration 172/1000 | Loss: 0.00001496
Iteration 173/1000 | Loss: 0.00001496
Iteration 174/1000 | Loss: 0.00001496
Iteration 175/1000 | Loss: 0.00001496
Iteration 176/1000 | Loss: 0.00001496
Iteration 177/1000 | Loss: 0.00001496
Iteration 178/1000 | Loss: 0.00001496
Iteration 179/1000 | Loss: 0.00001496
Iteration 180/1000 | Loss: 0.00001496
Iteration 181/1000 | Loss: 0.00001496
Iteration 182/1000 | Loss: 0.00001496
Iteration 183/1000 | Loss: 0.00001496
Iteration 184/1000 | Loss: 0.00001496
Iteration 185/1000 | Loss: 0.00001496
Iteration 186/1000 | Loss: 0.00001496
Iteration 187/1000 | Loss: 0.00001496
Iteration 188/1000 | Loss: 0.00001496
Iteration 189/1000 | Loss: 0.00001496
Iteration 190/1000 | Loss: 0.00001496
Iteration 191/1000 | Loss: 0.00001495
Iteration 192/1000 | Loss: 0.00001495
Iteration 193/1000 | Loss: 0.00001495
Iteration 194/1000 | Loss: 0.00001495
Iteration 195/1000 | Loss: 0.00001495
Iteration 196/1000 | Loss: 0.00001495
Iteration 197/1000 | Loss: 0.00001495
Iteration 198/1000 | Loss: 0.00001495
Iteration 199/1000 | Loss: 0.00001495
Iteration 200/1000 | Loss: 0.00001495
Iteration 201/1000 | Loss: 0.00001495
Iteration 202/1000 | Loss: 0.00001495
Iteration 203/1000 | Loss: 0.00001495
Iteration 204/1000 | Loss: 0.00001495
Iteration 205/1000 | Loss: 0.00001495
Iteration 206/1000 | Loss: 0.00001495
Iteration 207/1000 | Loss: 0.00001495
Iteration 208/1000 | Loss: 0.00001495
Iteration 209/1000 | Loss: 0.00001495
Iteration 210/1000 | Loss: 0.00001494
Iteration 211/1000 | Loss: 0.00001494
Iteration 212/1000 | Loss: 0.00001494
Iteration 213/1000 | Loss: 0.00001494
Iteration 214/1000 | Loss: 0.00001494
Iteration 215/1000 | Loss: 0.00001494
Iteration 216/1000 | Loss: 0.00001494
Iteration 217/1000 | Loss: 0.00001494
Iteration 218/1000 | Loss: 0.00001494
Iteration 219/1000 | Loss: 0.00001494
Iteration 220/1000 | Loss: 0.00001494
Iteration 221/1000 | Loss: 0.00001494
Iteration 222/1000 | Loss: 0.00001494
Iteration 223/1000 | Loss: 0.00001494
Iteration 224/1000 | Loss: 0.00001494
Iteration 225/1000 | Loss: 0.00001494
Iteration 226/1000 | Loss: 0.00001494
Iteration 227/1000 | Loss: 0.00001494
Iteration 228/1000 | Loss: 0.00001494
Iteration 229/1000 | Loss: 0.00001494
Iteration 230/1000 | Loss: 0.00001494
Iteration 231/1000 | Loss: 0.00001494
Iteration 232/1000 | Loss: 0.00001494
Iteration 233/1000 | Loss: 0.00001494
Iteration 234/1000 | Loss: 0.00001494
Iteration 235/1000 | Loss: 0.00001494
Iteration 236/1000 | Loss: 0.00001494
Iteration 237/1000 | Loss: 0.00001494
Iteration 238/1000 | Loss: 0.00001494
Iteration 239/1000 | Loss: 0.00001494
Iteration 240/1000 | Loss: 0.00001494
Iteration 241/1000 | Loss: 0.00001494
Iteration 242/1000 | Loss: 0.00001494
Iteration 243/1000 | Loss: 0.00001494
Iteration 244/1000 | Loss: 0.00001494
Iteration 245/1000 | Loss: 0.00001494
Iteration 246/1000 | Loss: 0.00001494
Iteration 247/1000 | Loss: 0.00001494
Iteration 248/1000 | Loss: 0.00001494
Iteration 249/1000 | Loss: 0.00001494
Iteration 250/1000 | Loss: 0.00001494
Iteration 251/1000 | Loss: 0.00001494
Iteration 252/1000 | Loss: 0.00001494
Iteration 253/1000 | Loss: 0.00001494
Iteration 254/1000 | Loss: 0.00001494
Iteration 255/1000 | Loss: 0.00001494
Iteration 256/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.4935275430616457e-05, 1.4935275430616457e-05, 1.4935275430616457e-05, 1.4935275430616457e-05, 1.4935275430616457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4935275430616457e-05

Optimization complete. Final v2v error: 3.2926225662231445 mm

Highest mean error: 4.052526950836182 mm for frame 19

Lowest mean error: 3.0344088077545166 mm for frame 40

Saving results

Total time: 43.68964767456055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954696
Iteration 2/25 | Loss: 0.00204942
Iteration 3/25 | Loss: 0.00140455
Iteration 4/25 | Loss: 0.00138707
Iteration 5/25 | Loss: 0.00138413
Iteration 6/25 | Loss: 0.00138330
Iteration 7/25 | Loss: 0.00138330
Iteration 8/25 | Loss: 0.00138330
Iteration 9/25 | Loss: 0.00138330
Iteration 10/25 | Loss: 0.00138330
Iteration 11/25 | Loss: 0.00138330
Iteration 12/25 | Loss: 0.00138330
Iteration 13/25 | Loss: 0.00138330
Iteration 14/25 | Loss: 0.00138330
Iteration 15/25 | Loss: 0.00138330
Iteration 16/25 | Loss: 0.00138330
Iteration 17/25 | Loss: 0.00138330
Iteration 18/25 | Loss: 0.00138330
Iteration 19/25 | Loss: 0.00138330
Iteration 20/25 | Loss: 0.00138330
Iteration 21/25 | Loss: 0.00138330
Iteration 22/25 | Loss: 0.00138330
Iteration 23/25 | Loss: 0.00138330
Iteration 24/25 | Loss: 0.00138330
Iteration 25/25 | Loss: 0.00138330

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66245210
Iteration 2/25 | Loss: 0.00090734
Iteration 3/25 | Loss: 0.00090733
Iteration 4/25 | Loss: 0.00090733
Iteration 5/25 | Loss: 0.00090733
Iteration 6/25 | Loss: 0.00090733
Iteration 7/25 | Loss: 0.00090733
Iteration 8/25 | Loss: 0.00090733
Iteration 9/25 | Loss: 0.00090733
Iteration 10/25 | Loss: 0.00090733
Iteration 11/25 | Loss: 0.00090733
Iteration 12/25 | Loss: 0.00090733
Iteration 13/25 | Loss: 0.00090733
Iteration 14/25 | Loss: 0.00090733
Iteration 15/25 | Loss: 0.00090733
Iteration 16/25 | Loss: 0.00090733
Iteration 17/25 | Loss: 0.00090733
Iteration 18/25 | Loss: 0.00090733
Iteration 19/25 | Loss: 0.00090733
Iteration 20/25 | Loss: 0.00090733
Iteration 21/25 | Loss: 0.00090733
Iteration 22/25 | Loss: 0.00090733
Iteration 23/25 | Loss: 0.00090733
Iteration 24/25 | Loss: 0.00090733
Iteration 25/25 | Loss: 0.00090733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090733
Iteration 2/1000 | Loss: 0.00007478
Iteration 3/1000 | Loss: 0.00005272
Iteration 4/1000 | Loss: 0.00004446
Iteration 5/1000 | Loss: 0.00004140
Iteration 6/1000 | Loss: 0.00004004
Iteration 7/1000 | Loss: 0.00003920
Iteration 8/1000 | Loss: 0.00003823
Iteration 9/1000 | Loss: 0.00003722
Iteration 10/1000 | Loss: 0.00003654
Iteration 11/1000 | Loss: 0.00003602
Iteration 12/1000 | Loss: 0.00003549
Iteration 13/1000 | Loss: 0.00003511
Iteration 14/1000 | Loss: 0.00003477
Iteration 15/1000 | Loss: 0.00003454
Iteration 16/1000 | Loss: 0.00003432
Iteration 17/1000 | Loss: 0.00003422
Iteration 18/1000 | Loss: 0.00003405
Iteration 19/1000 | Loss: 0.00003404
Iteration 20/1000 | Loss: 0.00003404
Iteration 21/1000 | Loss: 0.00003398
Iteration 22/1000 | Loss: 0.00003385
Iteration 23/1000 | Loss: 0.00003382
Iteration 24/1000 | Loss: 0.00003362
Iteration 25/1000 | Loss: 0.00003359
Iteration 26/1000 | Loss: 0.00003359
Iteration 27/1000 | Loss: 0.00003359
Iteration 28/1000 | Loss: 0.00003359
Iteration 29/1000 | Loss: 0.00003345
Iteration 30/1000 | Loss: 0.00003333
Iteration 31/1000 | Loss: 0.00003330
Iteration 32/1000 | Loss: 0.00003327
Iteration 33/1000 | Loss: 0.00003327
Iteration 34/1000 | Loss: 0.00003327
Iteration 35/1000 | Loss: 0.00003327
Iteration 36/1000 | Loss: 0.00003327
Iteration 37/1000 | Loss: 0.00003327
Iteration 38/1000 | Loss: 0.00003326
Iteration 39/1000 | Loss: 0.00003326
Iteration 40/1000 | Loss: 0.00003326
Iteration 41/1000 | Loss: 0.00003326
Iteration 42/1000 | Loss: 0.00003325
Iteration 43/1000 | Loss: 0.00003323
Iteration 44/1000 | Loss: 0.00003323
Iteration 45/1000 | Loss: 0.00003317
Iteration 46/1000 | Loss: 0.00003316
Iteration 47/1000 | Loss: 0.00003316
Iteration 48/1000 | Loss: 0.00003316
Iteration 49/1000 | Loss: 0.00003316
Iteration 50/1000 | Loss: 0.00003316
Iteration 51/1000 | Loss: 0.00003315
Iteration 52/1000 | Loss: 0.00003314
Iteration 53/1000 | Loss: 0.00003313
Iteration 54/1000 | Loss: 0.00003313
Iteration 55/1000 | Loss: 0.00003313
Iteration 56/1000 | Loss: 0.00003309
Iteration 57/1000 | Loss: 0.00003309
Iteration 58/1000 | Loss: 0.00003308
Iteration 59/1000 | Loss: 0.00003308
Iteration 60/1000 | Loss: 0.00003308
Iteration 61/1000 | Loss: 0.00003308
Iteration 62/1000 | Loss: 0.00003308
Iteration 63/1000 | Loss: 0.00003307
Iteration 64/1000 | Loss: 0.00003307
Iteration 65/1000 | Loss: 0.00003307
Iteration 66/1000 | Loss: 0.00003306
Iteration 67/1000 | Loss: 0.00003306
Iteration 68/1000 | Loss: 0.00003306
Iteration 69/1000 | Loss: 0.00003306
Iteration 70/1000 | Loss: 0.00003306
Iteration 71/1000 | Loss: 0.00003306
Iteration 72/1000 | Loss: 0.00003306
Iteration 73/1000 | Loss: 0.00003306
Iteration 74/1000 | Loss: 0.00003306
Iteration 75/1000 | Loss: 0.00003306
Iteration 76/1000 | Loss: 0.00003305
Iteration 77/1000 | Loss: 0.00003305
Iteration 78/1000 | Loss: 0.00003305
Iteration 79/1000 | Loss: 0.00003305
Iteration 80/1000 | Loss: 0.00003304
Iteration 81/1000 | Loss: 0.00003304
Iteration 82/1000 | Loss: 0.00003304
Iteration 83/1000 | Loss: 0.00003304
Iteration 84/1000 | Loss: 0.00003304
Iteration 85/1000 | Loss: 0.00003304
Iteration 86/1000 | Loss: 0.00003304
Iteration 87/1000 | Loss: 0.00003304
Iteration 88/1000 | Loss: 0.00003304
Iteration 89/1000 | Loss: 0.00003304
Iteration 90/1000 | Loss: 0.00003304
Iteration 91/1000 | Loss: 0.00003304
Iteration 92/1000 | Loss: 0.00003304
Iteration 93/1000 | Loss: 0.00003303
Iteration 94/1000 | Loss: 0.00003303
Iteration 95/1000 | Loss: 0.00003303
Iteration 96/1000 | Loss: 0.00003302
Iteration 97/1000 | Loss: 0.00003302
Iteration 98/1000 | Loss: 0.00003302
Iteration 99/1000 | Loss: 0.00003302
Iteration 100/1000 | Loss: 0.00003302
Iteration 101/1000 | Loss: 0.00003302
Iteration 102/1000 | Loss: 0.00003302
Iteration 103/1000 | Loss: 0.00003302
Iteration 104/1000 | Loss: 0.00003302
Iteration 105/1000 | Loss: 0.00003302
Iteration 106/1000 | Loss: 0.00003302
Iteration 107/1000 | Loss: 0.00003301
Iteration 108/1000 | Loss: 0.00003301
Iteration 109/1000 | Loss: 0.00003301
Iteration 110/1000 | Loss: 0.00003301
Iteration 111/1000 | Loss: 0.00003301
Iteration 112/1000 | Loss: 0.00003301
Iteration 113/1000 | Loss: 0.00003301
Iteration 114/1000 | Loss: 0.00003301
Iteration 115/1000 | Loss: 0.00003301
Iteration 116/1000 | Loss: 0.00003301
Iteration 117/1000 | Loss: 0.00003301
Iteration 118/1000 | Loss: 0.00003300
Iteration 119/1000 | Loss: 0.00003300
Iteration 120/1000 | Loss: 0.00003300
Iteration 121/1000 | Loss: 0.00003300
Iteration 122/1000 | Loss: 0.00003300
Iteration 123/1000 | Loss: 0.00003300
Iteration 124/1000 | Loss: 0.00003300
Iteration 125/1000 | Loss: 0.00003300
Iteration 126/1000 | Loss: 0.00003300
Iteration 127/1000 | Loss: 0.00003300
Iteration 128/1000 | Loss: 0.00003300
Iteration 129/1000 | Loss: 0.00003300
Iteration 130/1000 | Loss: 0.00003300
Iteration 131/1000 | Loss: 0.00003300
Iteration 132/1000 | Loss: 0.00003300
Iteration 133/1000 | Loss: 0.00003300
Iteration 134/1000 | Loss: 0.00003299
Iteration 135/1000 | Loss: 0.00003299
Iteration 136/1000 | Loss: 0.00003299
Iteration 137/1000 | Loss: 0.00003299
Iteration 138/1000 | Loss: 0.00003299
Iteration 139/1000 | Loss: 0.00003299
Iteration 140/1000 | Loss: 0.00003299
Iteration 141/1000 | Loss: 0.00003299
Iteration 142/1000 | Loss: 0.00003299
Iteration 143/1000 | Loss: 0.00003299
Iteration 144/1000 | Loss: 0.00003299
Iteration 145/1000 | Loss: 0.00003298
Iteration 146/1000 | Loss: 0.00003298
Iteration 147/1000 | Loss: 0.00003298
Iteration 148/1000 | Loss: 0.00003298
Iteration 149/1000 | Loss: 0.00003298
Iteration 150/1000 | Loss: 0.00003298
Iteration 151/1000 | Loss: 0.00003298
Iteration 152/1000 | Loss: 0.00003298
Iteration 153/1000 | Loss: 0.00003298
Iteration 154/1000 | Loss: 0.00003297
Iteration 155/1000 | Loss: 0.00003297
Iteration 156/1000 | Loss: 0.00003297
Iteration 157/1000 | Loss: 0.00003297
Iteration 158/1000 | Loss: 0.00003297
Iteration 159/1000 | Loss: 0.00003297
Iteration 160/1000 | Loss: 0.00003297
Iteration 161/1000 | Loss: 0.00003297
Iteration 162/1000 | Loss: 0.00003297
Iteration 163/1000 | Loss: 0.00003297
Iteration 164/1000 | Loss: 0.00003296
Iteration 165/1000 | Loss: 0.00003296
Iteration 166/1000 | Loss: 0.00003296
Iteration 167/1000 | Loss: 0.00003296
Iteration 168/1000 | Loss: 0.00003296
Iteration 169/1000 | Loss: 0.00003296
Iteration 170/1000 | Loss: 0.00003296
Iteration 171/1000 | Loss: 0.00003296
Iteration 172/1000 | Loss: 0.00003296
Iteration 173/1000 | Loss: 0.00003296
Iteration 174/1000 | Loss: 0.00003295
Iteration 175/1000 | Loss: 0.00003295
Iteration 176/1000 | Loss: 0.00003295
Iteration 177/1000 | Loss: 0.00003295
Iteration 178/1000 | Loss: 0.00003295
Iteration 179/1000 | Loss: 0.00003295
Iteration 180/1000 | Loss: 0.00003295
Iteration 181/1000 | Loss: 0.00003295
Iteration 182/1000 | Loss: 0.00003295
Iteration 183/1000 | Loss: 0.00003295
Iteration 184/1000 | Loss: 0.00003295
Iteration 185/1000 | Loss: 0.00003295
Iteration 186/1000 | Loss: 0.00003295
Iteration 187/1000 | Loss: 0.00003295
Iteration 188/1000 | Loss: 0.00003295
Iteration 189/1000 | Loss: 0.00003295
Iteration 190/1000 | Loss: 0.00003295
Iteration 191/1000 | Loss: 0.00003295
Iteration 192/1000 | Loss: 0.00003295
Iteration 193/1000 | Loss: 0.00003295
Iteration 194/1000 | Loss: 0.00003295
Iteration 195/1000 | Loss: 0.00003295
Iteration 196/1000 | Loss: 0.00003295
Iteration 197/1000 | Loss: 0.00003295
Iteration 198/1000 | Loss: 0.00003295
Iteration 199/1000 | Loss: 0.00003295
Iteration 200/1000 | Loss: 0.00003295
Iteration 201/1000 | Loss: 0.00003295
Iteration 202/1000 | Loss: 0.00003295
Iteration 203/1000 | Loss: 0.00003295
Iteration 204/1000 | Loss: 0.00003295
Iteration 205/1000 | Loss: 0.00003295
Iteration 206/1000 | Loss: 0.00003295
Iteration 207/1000 | Loss: 0.00003295
Iteration 208/1000 | Loss: 0.00003295
Iteration 209/1000 | Loss: 0.00003295
Iteration 210/1000 | Loss: 0.00003295
Iteration 211/1000 | Loss: 0.00003295
Iteration 212/1000 | Loss: 0.00003295
Iteration 213/1000 | Loss: 0.00003295
Iteration 214/1000 | Loss: 0.00003295
Iteration 215/1000 | Loss: 0.00003295
Iteration 216/1000 | Loss: 0.00003295
Iteration 217/1000 | Loss: 0.00003295
Iteration 218/1000 | Loss: 0.00003295
Iteration 219/1000 | Loss: 0.00003295
Iteration 220/1000 | Loss: 0.00003295
Iteration 221/1000 | Loss: 0.00003295
Iteration 222/1000 | Loss: 0.00003295
Iteration 223/1000 | Loss: 0.00003295
Iteration 224/1000 | Loss: 0.00003295
Iteration 225/1000 | Loss: 0.00003295
Iteration 226/1000 | Loss: 0.00003295
Iteration 227/1000 | Loss: 0.00003295
Iteration 228/1000 | Loss: 0.00003295
Iteration 229/1000 | Loss: 0.00003295
Iteration 230/1000 | Loss: 0.00003295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [3.2947460567811504e-05, 3.2947460567811504e-05, 3.2947460567811504e-05, 3.2947460567811504e-05, 3.2947460567811504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2947460567811504e-05

Optimization complete. Final v2v error: 4.711472511291504 mm

Highest mean error: 5.35312557220459 mm for frame 58

Lowest mean error: 4.262090682983398 mm for frame 113

Saving results

Total time: 53.955286502838135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882552
Iteration 2/25 | Loss: 0.00166406
Iteration 3/25 | Loss: 0.00131931
Iteration 4/25 | Loss: 0.00132463
Iteration 5/25 | Loss: 0.00125846
Iteration 6/25 | Loss: 0.00125408
Iteration 7/25 | Loss: 0.00123553
Iteration 8/25 | Loss: 0.00122051
Iteration 9/25 | Loss: 0.00122211
Iteration 10/25 | Loss: 0.00122057
Iteration 11/25 | Loss: 0.00120508
Iteration 12/25 | Loss: 0.00119693
Iteration 13/25 | Loss: 0.00119425
Iteration 14/25 | Loss: 0.00119413
Iteration 15/25 | Loss: 0.00119329
Iteration 16/25 | Loss: 0.00119583
Iteration 17/25 | Loss: 0.00119355
Iteration 18/25 | Loss: 0.00119198
Iteration 19/25 | Loss: 0.00119152
Iteration 20/25 | Loss: 0.00119285
Iteration 21/25 | Loss: 0.00119058
Iteration 22/25 | Loss: 0.00119770
Iteration 23/25 | Loss: 0.00119545
Iteration 24/25 | Loss: 0.00119534
Iteration 25/25 | Loss: 0.00118666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66125917
Iteration 2/25 | Loss: 0.00147868
Iteration 3/25 | Loss: 0.00147863
Iteration 4/25 | Loss: 0.00147863
Iteration 5/25 | Loss: 0.00147863
Iteration 6/25 | Loss: 0.00147863
Iteration 7/25 | Loss: 0.00147863
Iteration 8/25 | Loss: 0.00147863
Iteration 9/25 | Loss: 0.00147863
Iteration 10/25 | Loss: 0.00147863
Iteration 11/25 | Loss: 0.00147863
Iteration 12/25 | Loss: 0.00147863
Iteration 13/25 | Loss: 0.00147863
Iteration 14/25 | Loss: 0.00147863
Iteration 15/25 | Loss: 0.00147863
Iteration 16/25 | Loss: 0.00147863
Iteration 17/25 | Loss: 0.00147863
Iteration 18/25 | Loss: 0.00147863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014786266256123781, 0.0014786266256123781, 0.0014786266256123781, 0.0014786266256123781, 0.0014786266256123781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014786266256123781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147863
Iteration 2/1000 | Loss: 0.00191576
Iteration 3/1000 | Loss: 0.00016407
Iteration 4/1000 | Loss: 0.00006438
Iteration 5/1000 | Loss: 0.00040995
Iteration 6/1000 | Loss: 0.00020127
Iteration 7/1000 | Loss: 0.00028009
Iteration 8/1000 | Loss: 0.00019628
Iteration 9/1000 | Loss: 0.00144833
Iteration 10/1000 | Loss: 0.00108372
Iteration 11/1000 | Loss: 0.00004764
Iteration 12/1000 | Loss: 0.00004261
Iteration 13/1000 | Loss: 0.00003854
Iteration 14/1000 | Loss: 0.00004512
Iteration 15/1000 | Loss: 0.00004386
Iteration 16/1000 | Loss: 0.00003640
Iteration 17/1000 | Loss: 0.00003513
Iteration 18/1000 | Loss: 0.00015540
Iteration 19/1000 | Loss: 0.00008895
Iteration 20/1000 | Loss: 0.00005959
Iteration 21/1000 | Loss: 0.00004285
Iteration 22/1000 | Loss: 0.00004418
Iteration 23/1000 | Loss: 0.00014990
Iteration 24/1000 | Loss: 0.00009848
Iteration 25/1000 | Loss: 0.00003594
Iteration 26/1000 | Loss: 0.00004232
Iteration 27/1000 | Loss: 0.00003989
Iteration 28/1000 | Loss: 0.00003410
Iteration 29/1000 | Loss: 0.00003859
Iteration 30/1000 | Loss: 0.00071587
Iteration 31/1000 | Loss: 0.00004478
Iteration 32/1000 | Loss: 0.00005033
Iteration 33/1000 | Loss: 0.00015778
Iteration 34/1000 | Loss: 0.00026855
Iteration 35/1000 | Loss: 0.00012849
Iteration 36/1000 | Loss: 0.00028293
Iteration 37/1000 | Loss: 0.00012437
Iteration 38/1000 | Loss: 0.00042974
Iteration 39/1000 | Loss: 0.00007796
Iteration 40/1000 | Loss: 0.00003343
Iteration 41/1000 | Loss: 0.00003193
Iteration 42/1000 | Loss: 0.00043779
Iteration 43/1000 | Loss: 0.00026175
Iteration 44/1000 | Loss: 0.00003128
Iteration 45/1000 | Loss: 0.00045754
Iteration 46/1000 | Loss: 0.00010248
Iteration 47/1000 | Loss: 0.00023030
Iteration 48/1000 | Loss: 0.00011097
Iteration 49/1000 | Loss: 0.00023061
Iteration 50/1000 | Loss: 0.00009109
Iteration 51/1000 | Loss: 0.00018000
Iteration 52/1000 | Loss: 0.00003094
Iteration 53/1000 | Loss: 0.00002925
Iteration 54/1000 | Loss: 0.00002880
Iteration 55/1000 | Loss: 0.00002849
Iteration 56/1000 | Loss: 0.00002813
Iteration 57/1000 | Loss: 0.00002784
Iteration 58/1000 | Loss: 0.00002755
Iteration 59/1000 | Loss: 0.00002731
Iteration 60/1000 | Loss: 0.00002720
Iteration 61/1000 | Loss: 0.00002720
Iteration 62/1000 | Loss: 0.00002714
Iteration 63/1000 | Loss: 0.00002707
Iteration 64/1000 | Loss: 0.00002706
Iteration 65/1000 | Loss: 0.00002704
Iteration 66/1000 | Loss: 0.00002703
Iteration 67/1000 | Loss: 0.00002702
Iteration 68/1000 | Loss: 0.00002697
Iteration 69/1000 | Loss: 0.00002695
Iteration 70/1000 | Loss: 0.00002695
Iteration 71/1000 | Loss: 0.00002691
Iteration 72/1000 | Loss: 0.00002680
Iteration 73/1000 | Loss: 0.00002679
Iteration 74/1000 | Loss: 0.00002678
Iteration 75/1000 | Loss: 0.00002678
Iteration 76/1000 | Loss: 0.00002677
Iteration 77/1000 | Loss: 0.00002676
Iteration 78/1000 | Loss: 0.00002673
Iteration 79/1000 | Loss: 0.00002670
Iteration 80/1000 | Loss: 0.00002669
Iteration 81/1000 | Loss: 0.00002668
Iteration 82/1000 | Loss: 0.00002667
Iteration 83/1000 | Loss: 0.00002667
Iteration 84/1000 | Loss: 0.00002667
Iteration 85/1000 | Loss: 0.00002667
Iteration 86/1000 | Loss: 0.00002666
Iteration 87/1000 | Loss: 0.00002666
Iteration 88/1000 | Loss: 0.00002666
Iteration 89/1000 | Loss: 0.00002666
Iteration 90/1000 | Loss: 0.00002665
Iteration 91/1000 | Loss: 0.00002665
Iteration 92/1000 | Loss: 0.00002665
Iteration 93/1000 | Loss: 0.00002665
Iteration 94/1000 | Loss: 0.00002664
Iteration 95/1000 | Loss: 0.00002664
Iteration 96/1000 | Loss: 0.00002664
Iteration 97/1000 | Loss: 0.00002664
Iteration 98/1000 | Loss: 0.00002664
Iteration 99/1000 | Loss: 0.00002664
Iteration 100/1000 | Loss: 0.00002664
Iteration 101/1000 | Loss: 0.00002664
Iteration 102/1000 | Loss: 0.00002664
Iteration 103/1000 | Loss: 0.00002664
Iteration 104/1000 | Loss: 0.00002663
Iteration 105/1000 | Loss: 0.00002663
Iteration 106/1000 | Loss: 0.00002663
Iteration 107/1000 | Loss: 0.00002663
Iteration 108/1000 | Loss: 0.00002663
Iteration 109/1000 | Loss: 0.00002662
Iteration 110/1000 | Loss: 0.00002662
Iteration 111/1000 | Loss: 0.00002662
Iteration 112/1000 | Loss: 0.00002661
Iteration 113/1000 | Loss: 0.00002661
Iteration 114/1000 | Loss: 0.00002661
Iteration 115/1000 | Loss: 0.00002660
Iteration 116/1000 | Loss: 0.00002660
Iteration 117/1000 | Loss: 0.00002660
Iteration 118/1000 | Loss: 0.00002660
Iteration 119/1000 | Loss: 0.00002659
Iteration 120/1000 | Loss: 0.00002659
Iteration 121/1000 | Loss: 0.00002659
Iteration 122/1000 | Loss: 0.00002659
Iteration 123/1000 | Loss: 0.00002659
Iteration 124/1000 | Loss: 0.00002659
Iteration 125/1000 | Loss: 0.00002659
Iteration 126/1000 | Loss: 0.00002659
Iteration 127/1000 | Loss: 0.00002659
Iteration 128/1000 | Loss: 0.00002659
Iteration 129/1000 | Loss: 0.00002659
Iteration 130/1000 | Loss: 0.00002658
Iteration 131/1000 | Loss: 0.00002658
Iteration 132/1000 | Loss: 0.00002658
Iteration 133/1000 | Loss: 0.00002658
Iteration 134/1000 | Loss: 0.00002658
Iteration 135/1000 | Loss: 0.00002658
Iteration 136/1000 | Loss: 0.00002658
Iteration 137/1000 | Loss: 0.00002658
Iteration 138/1000 | Loss: 0.00002658
Iteration 139/1000 | Loss: 0.00002658
Iteration 140/1000 | Loss: 0.00002658
Iteration 141/1000 | Loss: 0.00002658
Iteration 142/1000 | Loss: 0.00002658
Iteration 143/1000 | Loss: 0.00002658
Iteration 144/1000 | Loss: 0.00002658
Iteration 145/1000 | Loss: 0.00002658
Iteration 146/1000 | Loss: 0.00002658
Iteration 147/1000 | Loss: 0.00002657
Iteration 148/1000 | Loss: 0.00002657
Iteration 149/1000 | Loss: 0.00002657
Iteration 150/1000 | Loss: 0.00002657
Iteration 151/1000 | Loss: 0.00002657
Iteration 152/1000 | Loss: 0.00002657
Iteration 153/1000 | Loss: 0.00002656
Iteration 154/1000 | Loss: 0.00002656
Iteration 155/1000 | Loss: 0.00002656
Iteration 156/1000 | Loss: 0.00002656
Iteration 157/1000 | Loss: 0.00002655
Iteration 158/1000 | Loss: 0.00002655
Iteration 159/1000 | Loss: 0.00002655
Iteration 160/1000 | Loss: 0.00002655
Iteration 161/1000 | Loss: 0.00002655
Iteration 162/1000 | Loss: 0.00002655
Iteration 163/1000 | Loss: 0.00002654
Iteration 164/1000 | Loss: 0.00002654
Iteration 165/1000 | Loss: 0.00002654
Iteration 166/1000 | Loss: 0.00002654
Iteration 167/1000 | Loss: 0.00002654
Iteration 168/1000 | Loss: 0.00002654
Iteration 169/1000 | Loss: 0.00002654
Iteration 170/1000 | Loss: 0.00002654
Iteration 171/1000 | Loss: 0.00002654
Iteration 172/1000 | Loss: 0.00002653
Iteration 173/1000 | Loss: 0.00002653
Iteration 174/1000 | Loss: 0.00002653
Iteration 175/1000 | Loss: 0.00002653
Iteration 176/1000 | Loss: 0.00002653
Iteration 177/1000 | Loss: 0.00002652
Iteration 178/1000 | Loss: 0.00002652
Iteration 179/1000 | Loss: 0.00002652
Iteration 180/1000 | Loss: 0.00002652
Iteration 181/1000 | Loss: 0.00002652
Iteration 182/1000 | Loss: 0.00002651
Iteration 183/1000 | Loss: 0.00002651
Iteration 184/1000 | Loss: 0.00002651
Iteration 185/1000 | Loss: 0.00002651
Iteration 186/1000 | Loss: 0.00002651
Iteration 187/1000 | Loss: 0.00002651
Iteration 188/1000 | Loss: 0.00002651
Iteration 189/1000 | Loss: 0.00002651
Iteration 190/1000 | Loss: 0.00002651
Iteration 191/1000 | Loss: 0.00002651
Iteration 192/1000 | Loss: 0.00002651
Iteration 193/1000 | Loss: 0.00002651
Iteration 194/1000 | Loss: 0.00002651
Iteration 195/1000 | Loss: 0.00002650
Iteration 196/1000 | Loss: 0.00002650
Iteration 197/1000 | Loss: 0.00002650
Iteration 198/1000 | Loss: 0.00002650
Iteration 199/1000 | Loss: 0.00002650
Iteration 200/1000 | Loss: 0.00002650
Iteration 201/1000 | Loss: 0.00002650
Iteration 202/1000 | Loss: 0.00002650
Iteration 203/1000 | Loss: 0.00002650
Iteration 204/1000 | Loss: 0.00002650
Iteration 205/1000 | Loss: 0.00002650
Iteration 206/1000 | Loss: 0.00002650
Iteration 207/1000 | Loss: 0.00002650
Iteration 208/1000 | Loss: 0.00002650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.650104033818934e-05, 2.650104033818934e-05, 2.650104033818934e-05, 2.650104033818934e-05, 2.650104033818934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.650104033818934e-05

Optimization complete. Final v2v error: 4.19536018371582 mm

Highest mean error: 6.808029651641846 mm for frame 41

Lowest mean error: 2.8820252418518066 mm for frame 136

Saving results

Total time: 160.70100903511047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491830
Iteration 2/25 | Loss: 0.00120328
Iteration 3/25 | Loss: 0.00111899
Iteration 4/25 | Loss: 0.00110239
Iteration 5/25 | Loss: 0.00109824
Iteration 6/25 | Loss: 0.00109733
Iteration 7/25 | Loss: 0.00109733
Iteration 8/25 | Loss: 0.00109733
Iteration 9/25 | Loss: 0.00109733
Iteration 10/25 | Loss: 0.00109733
Iteration 11/25 | Loss: 0.00109733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010973275639116764, 0.0010973275639116764, 0.0010973275639116764, 0.0010973275639116764, 0.0010973275639116764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010973275639116764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97641677
Iteration 2/25 | Loss: 0.00067390
Iteration 3/25 | Loss: 0.00067384
Iteration 4/25 | Loss: 0.00067384
Iteration 5/25 | Loss: 0.00067384
Iteration 6/25 | Loss: 0.00067384
Iteration 7/25 | Loss: 0.00067384
Iteration 8/25 | Loss: 0.00067384
Iteration 9/25 | Loss: 0.00067384
Iteration 10/25 | Loss: 0.00067384
Iteration 11/25 | Loss: 0.00067384
Iteration 12/25 | Loss: 0.00067384
Iteration 13/25 | Loss: 0.00067384
Iteration 14/25 | Loss: 0.00067384
Iteration 15/25 | Loss: 0.00067384
Iteration 16/25 | Loss: 0.00067384
Iteration 17/25 | Loss: 0.00067384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00067384063731879, 0.00067384063731879, 0.00067384063731879, 0.00067384063731879, 0.00067384063731879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00067384063731879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067384
Iteration 2/1000 | Loss: 0.00004383
Iteration 3/1000 | Loss: 0.00002654
Iteration 4/1000 | Loss: 0.00002182
Iteration 5/1000 | Loss: 0.00002068
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001914
Iteration 8/1000 | Loss: 0.00001866
Iteration 9/1000 | Loss: 0.00001829
Iteration 10/1000 | Loss: 0.00001792
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001688
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001636
Iteration 21/1000 | Loss: 0.00001635
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001635
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001630
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001629
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001628
Iteration 34/1000 | Loss: 0.00001628
Iteration 35/1000 | Loss: 0.00001628
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001623
Iteration 41/1000 | Loss: 0.00001622
Iteration 42/1000 | Loss: 0.00001622
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001619
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001611
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001611
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001611
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001610
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001610
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001609
Iteration 85/1000 | Loss: 0.00001609
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001608
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001608
Iteration 92/1000 | Loss: 0.00001608
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001608
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.6081336070783436e-05, 1.6081336070783436e-05, 1.6081336070783436e-05, 1.6081336070783436e-05, 1.6081336070783436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6081336070783436e-05

Optimization complete. Final v2v error: 3.3710720539093018 mm

Highest mean error: 3.3931167125701904 mm for frame 6

Lowest mean error: 3.3483617305755615 mm for frame 55

Saving results

Total time: 37.510619163513184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548325
Iteration 2/25 | Loss: 0.00116716
Iteration 3/25 | Loss: 0.00109677
Iteration 4/25 | Loss: 0.00108684
Iteration 5/25 | Loss: 0.00108377
Iteration 6/25 | Loss: 0.00108311
Iteration 7/25 | Loss: 0.00108311
Iteration 8/25 | Loss: 0.00108311
Iteration 9/25 | Loss: 0.00108311
Iteration 10/25 | Loss: 0.00108311
Iteration 11/25 | Loss: 0.00108311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010831081308424473, 0.0010831081308424473, 0.0010831081308424473, 0.0010831081308424473, 0.0010831081308424473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010831081308424473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.92748928
Iteration 2/25 | Loss: 0.00076048
Iteration 3/25 | Loss: 0.00076048
Iteration 4/25 | Loss: 0.00076048
Iteration 5/25 | Loss: 0.00076048
Iteration 6/25 | Loss: 0.00076048
Iteration 7/25 | Loss: 0.00076048
Iteration 8/25 | Loss: 0.00076048
Iteration 9/25 | Loss: 0.00076048
Iteration 10/25 | Loss: 0.00076048
Iteration 11/25 | Loss: 0.00076048
Iteration 12/25 | Loss: 0.00076048
Iteration 13/25 | Loss: 0.00076048
Iteration 14/25 | Loss: 0.00076048
Iteration 15/25 | Loss: 0.00076048
Iteration 16/25 | Loss: 0.00076048
Iteration 17/25 | Loss: 0.00076048
Iteration 18/25 | Loss: 0.00076048
Iteration 19/25 | Loss: 0.00076048
Iteration 20/25 | Loss: 0.00076048
Iteration 21/25 | Loss: 0.00076048
Iteration 22/25 | Loss: 0.00076048
Iteration 23/25 | Loss: 0.00076048
Iteration 24/25 | Loss: 0.00076048
Iteration 25/25 | Loss: 0.00076048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007604806451126933, 0.0007604806451126933, 0.0007604806451126933, 0.0007604806451126933, 0.0007604806451126933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007604806451126933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076048
Iteration 2/1000 | Loss: 0.00002453
Iteration 3/1000 | Loss: 0.00001740
Iteration 4/1000 | Loss: 0.00001453
Iteration 5/1000 | Loss: 0.00001372
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001203
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001163
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001133
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001113
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001096
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001094
Iteration 89/1000 | Loss: 0.00001094
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001093
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001091
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001090
Iteration 109/1000 | Loss: 0.00001090
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001086
Iteration 130/1000 | Loss: 0.00001086
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001086
Iteration 142/1000 | Loss: 0.00001085
Iteration 143/1000 | Loss: 0.00001085
Iteration 144/1000 | Loss: 0.00001085
Iteration 145/1000 | Loss: 0.00001085
Iteration 146/1000 | Loss: 0.00001085
Iteration 147/1000 | Loss: 0.00001085
Iteration 148/1000 | Loss: 0.00001085
Iteration 149/1000 | Loss: 0.00001085
Iteration 150/1000 | Loss: 0.00001085
Iteration 151/1000 | Loss: 0.00001085
Iteration 152/1000 | Loss: 0.00001084
Iteration 153/1000 | Loss: 0.00001084
Iteration 154/1000 | Loss: 0.00001084
Iteration 155/1000 | Loss: 0.00001084
Iteration 156/1000 | Loss: 0.00001084
Iteration 157/1000 | Loss: 0.00001084
Iteration 158/1000 | Loss: 0.00001084
Iteration 159/1000 | Loss: 0.00001084
Iteration 160/1000 | Loss: 0.00001084
Iteration 161/1000 | Loss: 0.00001084
Iteration 162/1000 | Loss: 0.00001084
Iteration 163/1000 | Loss: 0.00001084
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001084
Iteration 166/1000 | Loss: 0.00001084
Iteration 167/1000 | Loss: 0.00001084
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001083
Iteration 170/1000 | Loss: 0.00001083
Iteration 171/1000 | Loss: 0.00001083
Iteration 172/1000 | Loss: 0.00001083
Iteration 173/1000 | Loss: 0.00001083
Iteration 174/1000 | Loss: 0.00001083
Iteration 175/1000 | Loss: 0.00001083
Iteration 176/1000 | Loss: 0.00001083
Iteration 177/1000 | Loss: 0.00001082
Iteration 178/1000 | Loss: 0.00001082
Iteration 179/1000 | Loss: 0.00001082
Iteration 180/1000 | Loss: 0.00001082
Iteration 181/1000 | Loss: 0.00001082
Iteration 182/1000 | Loss: 0.00001082
Iteration 183/1000 | Loss: 0.00001082
Iteration 184/1000 | Loss: 0.00001081
Iteration 185/1000 | Loss: 0.00001081
Iteration 186/1000 | Loss: 0.00001081
Iteration 187/1000 | Loss: 0.00001081
Iteration 188/1000 | Loss: 0.00001081
Iteration 189/1000 | Loss: 0.00001081
Iteration 190/1000 | Loss: 0.00001081
Iteration 191/1000 | Loss: 0.00001081
Iteration 192/1000 | Loss: 0.00001080
Iteration 193/1000 | Loss: 0.00001080
Iteration 194/1000 | Loss: 0.00001080
Iteration 195/1000 | Loss: 0.00001080
Iteration 196/1000 | Loss: 0.00001080
Iteration 197/1000 | Loss: 0.00001080
Iteration 198/1000 | Loss: 0.00001080
Iteration 199/1000 | Loss: 0.00001080
Iteration 200/1000 | Loss: 0.00001080
Iteration 201/1000 | Loss: 0.00001080
Iteration 202/1000 | Loss: 0.00001080
Iteration 203/1000 | Loss: 0.00001080
Iteration 204/1000 | Loss: 0.00001080
Iteration 205/1000 | Loss: 0.00001080
Iteration 206/1000 | Loss: 0.00001080
Iteration 207/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0802164069900755e-05, 1.0802164069900755e-05, 1.0802164069900755e-05, 1.0802164069900755e-05, 1.0802164069900755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0802164069900755e-05

Optimization complete. Final v2v error: 2.8083834648132324 mm

Highest mean error: 3.570141077041626 mm for frame 65

Lowest mean error: 2.426079034805298 mm for frame 128

Saving results

Total time: 41.2825984954834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467991
Iteration 2/25 | Loss: 0.00119058
Iteration 3/25 | Loss: 0.00110718
Iteration 4/25 | Loss: 0.00109805
Iteration 5/25 | Loss: 0.00109520
Iteration 6/25 | Loss: 0.00109476
Iteration 7/25 | Loss: 0.00109476
Iteration 8/25 | Loss: 0.00109476
Iteration 9/25 | Loss: 0.00109476
Iteration 10/25 | Loss: 0.00109476
Iteration 11/25 | Loss: 0.00109476
Iteration 12/25 | Loss: 0.00109476
Iteration 13/25 | Loss: 0.00109476
Iteration 14/25 | Loss: 0.00109476
Iteration 15/25 | Loss: 0.00109476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010947580449283123, 0.0010947580449283123, 0.0010947580449283123, 0.0010947580449283123, 0.0010947580449283123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010947580449283123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50612235
Iteration 2/25 | Loss: 0.00085124
Iteration 3/25 | Loss: 0.00085124
Iteration 4/25 | Loss: 0.00085124
Iteration 5/25 | Loss: 0.00085124
Iteration 6/25 | Loss: 0.00085124
Iteration 7/25 | Loss: 0.00085124
Iteration 8/25 | Loss: 0.00085124
Iteration 9/25 | Loss: 0.00085124
Iteration 10/25 | Loss: 0.00085124
Iteration 11/25 | Loss: 0.00085124
Iteration 12/25 | Loss: 0.00085124
Iteration 13/25 | Loss: 0.00085124
Iteration 14/25 | Loss: 0.00085124
Iteration 15/25 | Loss: 0.00085124
Iteration 16/25 | Loss: 0.00085124
Iteration 17/25 | Loss: 0.00085124
Iteration 18/25 | Loss: 0.00085124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008512355270795524, 0.0008512355270795524, 0.0008512355270795524, 0.0008512355270795524, 0.0008512355270795524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008512355270795524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085124
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001221
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001147
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00001137
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001127
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001090
Iteration 36/1000 | Loss: 0.00001089
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001086
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001085
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001083
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001082
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001081
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001081
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001078
Iteration 105/1000 | Loss: 0.00001078
Iteration 106/1000 | Loss: 0.00001078
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001076
Iteration 119/1000 | Loss: 0.00001076
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001074
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001071
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001069
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001068
Iteration 161/1000 | Loss: 0.00001068
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001068
Iteration 164/1000 | Loss: 0.00001068
Iteration 165/1000 | Loss: 0.00001068
Iteration 166/1000 | Loss: 0.00001067
Iteration 167/1000 | Loss: 0.00001067
Iteration 168/1000 | Loss: 0.00001067
Iteration 169/1000 | Loss: 0.00001067
Iteration 170/1000 | Loss: 0.00001067
Iteration 171/1000 | Loss: 0.00001067
Iteration 172/1000 | Loss: 0.00001067
Iteration 173/1000 | Loss: 0.00001067
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001067
Iteration 176/1000 | Loss: 0.00001067
Iteration 177/1000 | Loss: 0.00001067
Iteration 178/1000 | Loss: 0.00001067
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001067
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001066
Iteration 187/1000 | Loss: 0.00001066
Iteration 188/1000 | Loss: 0.00001066
Iteration 189/1000 | Loss: 0.00001066
Iteration 190/1000 | Loss: 0.00001066
Iteration 191/1000 | Loss: 0.00001066
Iteration 192/1000 | Loss: 0.00001065
Iteration 193/1000 | Loss: 0.00001065
Iteration 194/1000 | Loss: 0.00001065
Iteration 195/1000 | Loss: 0.00001065
Iteration 196/1000 | Loss: 0.00001065
Iteration 197/1000 | Loss: 0.00001065
Iteration 198/1000 | Loss: 0.00001065
Iteration 199/1000 | Loss: 0.00001065
Iteration 200/1000 | Loss: 0.00001065
Iteration 201/1000 | Loss: 0.00001065
Iteration 202/1000 | Loss: 0.00001065
Iteration 203/1000 | Loss: 0.00001065
Iteration 204/1000 | Loss: 0.00001065
Iteration 205/1000 | Loss: 0.00001065
Iteration 206/1000 | Loss: 0.00001065
Iteration 207/1000 | Loss: 0.00001065
Iteration 208/1000 | Loss: 0.00001065
Iteration 209/1000 | Loss: 0.00001065
Iteration 210/1000 | Loss: 0.00001065
Iteration 211/1000 | Loss: 0.00001065
Iteration 212/1000 | Loss: 0.00001065
Iteration 213/1000 | Loss: 0.00001065
Iteration 214/1000 | Loss: 0.00001065
Iteration 215/1000 | Loss: 0.00001065
Iteration 216/1000 | Loss: 0.00001065
Iteration 217/1000 | Loss: 0.00001065
Iteration 218/1000 | Loss: 0.00001065
Iteration 219/1000 | Loss: 0.00001065
Iteration 220/1000 | Loss: 0.00001065
Iteration 221/1000 | Loss: 0.00001065
Iteration 222/1000 | Loss: 0.00001065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.0646348528098315e-05, 1.0646348528098315e-05, 1.0646348528098315e-05, 1.0646348528098315e-05, 1.0646348528098315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0646348528098315e-05

Optimization complete. Final v2v error: 2.735952377319336 mm

Highest mean error: 3.3196616172790527 mm for frame 39

Lowest mean error: 2.47188138961792 mm for frame 101

Saving results

Total time: 38.518723011016846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731539
Iteration 2/25 | Loss: 0.00127759
Iteration 3/25 | Loss: 0.00121396
Iteration 4/25 | Loss: 0.00120766
Iteration 5/25 | Loss: 0.00120568
Iteration 6/25 | Loss: 0.00120568
Iteration 7/25 | Loss: 0.00120568
Iteration 8/25 | Loss: 0.00120568
Iteration 9/25 | Loss: 0.00120568
Iteration 10/25 | Loss: 0.00120568
Iteration 11/25 | Loss: 0.00120568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012056789128109813, 0.0012056789128109813, 0.0012056789128109813, 0.0012056789128109813, 0.0012056789128109813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012056789128109813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17386425
Iteration 2/25 | Loss: 0.00079640
Iteration 3/25 | Loss: 0.00079638
Iteration 4/25 | Loss: 0.00079638
Iteration 5/25 | Loss: 0.00079638
Iteration 6/25 | Loss: 0.00079638
Iteration 7/25 | Loss: 0.00079638
Iteration 8/25 | Loss: 0.00079638
Iteration 9/25 | Loss: 0.00079638
Iteration 10/25 | Loss: 0.00079638
Iteration 11/25 | Loss: 0.00079638
Iteration 12/25 | Loss: 0.00079638
Iteration 13/25 | Loss: 0.00079638
Iteration 14/25 | Loss: 0.00079638
Iteration 15/25 | Loss: 0.00079638
Iteration 16/25 | Loss: 0.00079638
Iteration 17/25 | Loss: 0.00079638
Iteration 18/25 | Loss: 0.00079638
Iteration 19/25 | Loss: 0.00079638
Iteration 20/25 | Loss: 0.00079638
Iteration 21/25 | Loss: 0.00079638
Iteration 22/25 | Loss: 0.00079638
Iteration 23/25 | Loss: 0.00079638
Iteration 24/25 | Loss: 0.00079638
Iteration 25/25 | Loss: 0.00079638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079638
Iteration 2/1000 | Loss: 0.00003235
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002113
Iteration 5/1000 | Loss: 0.00001981
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001888
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001758
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001720
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001712
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001709
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001707
Iteration 28/1000 | Loss: 0.00001707
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001703
Iteration 34/1000 | Loss: 0.00001700
Iteration 35/1000 | Loss: 0.00001700
Iteration 36/1000 | Loss: 0.00001700
Iteration 37/1000 | Loss: 0.00001700
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001700
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001696
Iteration 46/1000 | Loss: 0.00001696
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001690
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001690
Iteration 66/1000 | Loss: 0.00001690
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001689
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001688
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001685
Iteration 91/1000 | Loss: 0.00001685
Iteration 92/1000 | Loss: 0.00001685
Iteration 93/1000 | Loss: 0.00001685
Iteration 94/1000 | Loss: 0.00001685
Iteration 95/1000 | Loss: 0.00001685
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001684
Iteration 98/1000 | Loss: 0.00001684
Iteration 99/1000 | Loss: 0.00001684
Iteration 100/1000 | Loss: 0.00001684
Iteration 101/1000 | Loss: 0.00001684
Iteration 102/1000 | Loss: 0.00001684
Iteration 103/1000 | Loss: 0.00001684
Iteration 104/1000 | Loss: 0.00001684
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001683
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001682
Iteration 117/1000 | Loss: 0.00001682
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001682
Iteration 120/1000 | Loss: 0.00001682
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001681
Iteration 137/1000 | Loss: 0.00001681
Iteration 138/1000 | Loss: 0.00001681
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001681
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001681
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001680
Iteration 149/1000 | Loss: 0.00001680
Iteration 150/1000 | Loss: 0.00001680
Iteration 151/1000 | Loss: 0.00001680
Iteration 152/1000 | Loss: 0.00001680
Iteration 153/1000 | Loss: 0.00001680
Iteration 154/1000 | Loss: 0.00001680
Iteration 155/1000 | Loss: 0.00001680
Iteration 156/1000 | Loss: 0.00001680
Iteration 157/1000 | Loss: 0.00001680
Iteration 158/1000 | Loss: 0.00001680
Iteration 159/1000 | Loss: 0.00001680
Iteration 160/1000 | Loss: 0.00001680
Iteration 161/1000 | Loss: 0.00001680
Iteration 162/1000 | Loss: 0.00001680
Iteration 163/1000 | Loss: 0.00001680
Iteration 164/1000 | Loss: 0.00001679
Iteration 165/1000 | Loss: 0.00001679
Iteration 166/1000 | Loss: 0.00001679
Iteration 167/1000 | Loss: 0.00001679
Iteration 168/1000 | Loss: 0.00001679
Iteration 169/1000 | Loss: 0.00001679
Iteration 170/1000 | Loss: 0.00001679
Iteration 171/1000 | Loss: 0.00001679
Iteration 172/1000 | Loss: 0.00001679
Iteration 173/1000 | Loss: 0.00001679
Iteration 174/1000 | Loss: 0.00001679
Iteration 175/1000 | Loss: 0.00001679
Iteration 176/1000 | Loss: 0.00001679
Iteration 177/1000 | Loss: 0.00001679
Iteration 178/1000 | Loss: 0.00001679
Iteration 179/1000 | Loss: 0.00001679
Iteration 180/1000 | Loss: 0.00001679
Iteration 181/1000 | Loss: 0.00001679
Iteration 182/1000 | Loss: 0.00001679
Iteration 183/1000 | Loss: 0.00001679
Iteration 184/1000 | Loss: 0.00001679
Iteration 185/1000 | Loss: 0.00001679
Iteration 186/1000 | Loss: 0.00001679
Iteration 187/1000 | Loss: 0.00001679
Iteration 188/1000 | Loss: 0.00001679
Iteration 189/1000 | Loss: 0.00001679
Iteration 190/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.678860826359596e-05, 1.678860826359596e-05, 1.678860826359596e-05, 1.678860826359596e-05, 1.678860826359596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.678860826359596e-05

Optimization complete. Final v2v error: 3.411376476287842 mm

Highest mean error: 3.5938241481781006 mm for frame 10

Lowest mean error: 3.1397006511688232 mm for frame 132

Saving results

Total time: 39.285661935806274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978278
Iteration 2/25 | Loss: 0.00175232
Iteration 3/25 | Loss: 0.00133736
Iteration 4/25 | Loss: 0.00131485
Iteration 5/25 | Loss: 0.00130790
Iteration 6/25 | Loss: 0.00130644
Iteration 7/25 | Loss: 0.00130644
Iteration 8/25 | Loss: 0.00130644
Iteration 9/25 | Loss: 0.00130644
Iteration 10/25 | Loss: 0.00130644
Iteration 11/25 | Loss: 0.00130644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013064405648037791, 0.0013064405648037791, 0.0013064405648037791, 0.0013064405648037791, 0.0013064405648037791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013064405648037791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82068527
Iteration 2/25 | Loss: 0.00104011
Iteration 3/25 | Loss: 0.00104010
Iteration 4/25 | Loss: 0.00104010
Iteration 5/25 | Loss: 0.00104010
Iteration 6/25 | Loss: 0.00104010
Iteration 7/25 | Loss: 0.00104009
Iteration 8/25 | Loss: 0.00104009
Iteration 9/25 | Loss: 0.00104009
Iteration 10/25 | Loss: 0.00104009
Iteration 11/25 | Loss: 0.00104009
Iteration 12/25 | Loss: 0.00104009
Iteration 13/25 | Loss: 0.00104009
Iteration 14/25 | Loss: 0.00104009
Iteration 15/25 | Loss: 0.00104009
Iteration 16/25 | Loss: 0.00104009
Iteration 17/25 | Loss: 0.00104009
Iteration 18/25 | Loss: 0.00104009
Iteration 19/25 | Loss: 0.00104009
Iteration 20/25 | Loss: 0.00104009
Iteration 21/25 | Loss: 0.00104009
Iteration 22/25 | Loss: 0.00104009
Iteration 23/25 | Loss: 0.00104009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010400931350886822, 0.0010400931350886822, 0.0010400931350886822, 0.0010400931350886822, 0.0010400931350886822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010400931350886822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104009
Iteration 2/1000 | Loss: 0.00006865
Iteration 3/1000 | Loss: 0.00004751
Iteration 4/1000 | Loss: 0.00003828
Iteration 5/1000 | Loss: 0.00003521
Iteration 6/1000 | Loss: 0.00003357
Iteration 7/1000 | Loss: 0.00003280
Iteration 8/1000 | Loss: 0.00003211
Iteration 9/1000 | Loss: 0.00003152
Iteration 10/1000 | Loss: 0.00003114
Iteration 11/1000 | Loss: 0.00003076
Iteration 12/1000 | Loss: 0.00003046
Iteration 13/1000 | Loss: 0.00003020
Iteration 14/1000 | Loss: 0.00002992
Iteration 15/1000 | Loss: 0.00002970
Iteration 16/1000 | Loss: 0.00002952
Iteration 17/1000 | Loss: 0.00002932
Iteration 18/1000 | Loss: 0.00002914
Iteration 19/1000 | Loss: 0.00002904
Iteration 20/1000 | Loss: 0.00002900
Iteration 21/1000 | Loss: 0.00002897
Iteration 22/1000 | Loss: 0.00002895
Iteration 23/1000 | Loss: 0.00002893
Iteration 24/1000 | Loss: 0.00002892
Iteration 25/1000 | Loss: 0.00002889
Iteration 26/1000 | Loss: 0.00002888
Iteration 27/1000 | Loss: 0.00002888
Iteration 28/1000 | Loss: 0.00002878
Iteration 29/1000 | Loss: 0.00002877
Iteration 30/1000 | Loss: 0.00002876
Iteration 31/1000 | Loss: 0.00002875
Iteration 32/1000 | Loss: 0.00002875
Iteration 33/1000 | Loss: 0.00002874
Iteration 34/1000 | Loss: 0.00002874
Iteration 35/1000 | Loss: 0.00002874
Iteration 36/1000 | Loss: 0.00002874
Iteration 37/1000 | Loss: 0.00002873
Iteration 38/1000 | Loss: 0.00002873
Iteration 39/1000 | Loss: 0.00002873
Iteration 40/1000 | Loss: 0.00002873
Iteration 41/1000 | Loss: 0.00002872
Iteration 42/1000 | Loss: 0.00002872
Iteration 43/1000 | Loss: 0.00002872
Iteration 44/1000 | Loss: 0.00002871
Iteration 45/1000 | Loss: 0.00002871
Iteration 46/1000 | Loss: 0.00002871
Iteration 47/1000 | Loss: 0.00002871
Iteration 48/1000 | Loss: 0.00002871
Iteration 49/1000 | Loss: 0.00002870
Iteration 50/1000 | Loss: 0.00002870
Iteration 51/1000 | Loss: 0.00002870
Iteration 52/1000 | Loss: 0.00002869
Iteration 53/1000 | Loss: 0.00002869
Iteration 54/1000 | Loss: 0.00002869
Iteration 55/1000 | Loss: 0.00002869
Iteration 56/1000 | Loss: 0.00002869
Iteration 57/1000 | Loss: 0.00002869
Iteration 58/1000 | Loss: 0.00002868
Iteration 59/1000 | Loss: 0.00002868
Iteration 60/1000 | Loss: 0.00002868
Iteration 61/1000 | Loss: 0.00002868
Iteration 62/1000 | Loss: 0.00002867
Iteration 63/1000 | Loss: 0.00002867
Iteration 64/1000 | Loss: 0.00002867
Iteration 65/1000 | Loss: 0.00002867
Iteration 66/1000 | Loss: 0.00002867
Iteration 67/1000 | Loss: 0.00002866
Iteration 68/1000 | Loss: 0.00002866
Iteration 69/1000 | Loss: 0.00002866
Iteration 70/1000 | Loss: 0.00002866
Iteration 71/1000 | Loss: 0.00002866
Iteration 72/1000 | Loss: 0.00002866
Iteration 73/1000 | Loss: 0.00002866
Iteration 74/1000 | Loss: 0.00002865
Iteration 75/1000 | Loss: 0.00002865
Iteration 76/1000 | Loss: 0.00002865
Iteration 77/1000 | Loss: 0.00002865
Iteration 78/1000 | Loss: 0.00002864
Iteration 79/1000 | Loss: 0.00002864
Iteration 80/1000 | Loss: 0.00002864
Iteration 81/1000 | Loss: 0.00002864
Iteration 82/1000 | Loss: 0.00002863
Iteration 83/1000 | Loss: 0.00002863
Iteration 84/1000 | Loss: 0.00002863
Iteration 85/1000 | Loss: 0.00002863
Iteration 86/1000 | Loss: 0.00002863
Iteration 87/1000 | Loss: 0.00002862
Iteration 88/1000 | Loss: 0.00002862
Iteration 89/1000 | Loss: 0.00002862
Iteration 90/1000 | Loss: 0.00002862
Iteration 91/1000 | Loss: 0.00002862
Iteration 92/1000 | Loss: 0.00002862
Iteration 93/1000 | Loss: 0.00002862
Iteration 94/1000 | Loss: 0.00002862
Iteration 95/1000 | Loss: 0.00002862
Iteration 96/1000 | Loss: 0.00002861
Iteration 97/1000 | Loss: 0.00002861
Iteration 98/1000 | Loss: 0.00002861
Iteration 99/1000 | Loss: 0.00002861
Iteration 100/1000 | Loss: 0.00002861
Iteration 101/1000 | Loss: 0.00002861
Iteration 102/1000 | Loss: 0.00002860
Iteration 103/1000 | Loss: 0.00002860
Iteration 104/1000 | Loss: 0.00002860
Iteration 105/1000 | Loss: 0.00002860
Iteration 106/1000 | Loss: 0.00002860
Iteration 107/1000 | Loss: 0.00002860
Iteration 108/1000 | Loss: 0.00002859
Iteration 109/1000 | Loss: 0.00002859
Iteration 110/1000 | Loss: 0.00002859
Iteration 111/1000 | Loss: 0.00002859
Iteration 112/1000 | Loss: 0.00002859
Iteration 113/1000 | Loss: 0.00002859
Iteration 114/1000 | Loss: 0.00002859
Iteration 115/1000 | Loss: 0.00002859
Iteration 116/1000 | Loss: 0.00002859
Iteration 117/1000 | Loss: 0.00002859
Iteration 118/1000 | Loss: 0.00002859
Iteration 119/1000 | Loss: 0.00002859
Iteration 120/1000 | Loss: 0.00002859
Iteration 121/1000 | Loss: 0.00002859
Iteration 122/1000 | Loss: 0.00002858
Iteration 123/1000 | Loss: 0.00002858
Iteration 124/1000 | Loss: 0.00002858
Iteration 125/1000 | Loss: 0.00002858
Iteration 126/1000 | Loss: 0.00002858
Iteration 127/1000 | Loss: 0.00002857
Iteration 128/1000 | Loss: 0.00002857
Iteration 129/1000 | Loss: 0.00002857
Iteration 130/1000 | Loss: 0.00002857
Iteration 131/1000 | Loss: 0.00002857
Iteration 132/1000 | Loss: 0.00002857
Iteration 133/1000 | Loss: 0.00002857
Iteration 134/1000 | Loss: 0.00002857
Iteration 135/1000 | Loss: 0.00002857
Iteration 136/1000 | Loss: 0.00002857
Iteration 137/1000 | Loss: 0.00002857
Iteration 138/1000 | Loss: 0.00002857
Iteration 139/1000 | Loss: 0.00002856
Iteration 140/1000 | Loss: 0.00002856
Iteration 141/1000 | Loss: 0.00002856
Iteration 142/1000 | Loss: 0.00002856
Iteration 143/1000 | Loss: 0.00002856
Iteration 144/1000 | Loss: 0.00002856
Iteration 145/1000 | Loss: 0.00002856
Iteration 146/1000 | Loss: 0.00002856
Iteration 147/1000 | Loss: 0.00002856
Iteration 148/1000 | Loss: 0.00002856
Iteration 149/1000 | Loss: 0.00002856
Iteration 150/1000 | Loss: 0.00002856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.8561908038682304e-05, 2.8561908038682304e-05, 2.8561908038682304e-05, 2.8561908038682304e-05, 2.8561908038682304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8561908038682304e-05

Optimization complete. Final v2v error: 4.3551435470581055 mm

Highest mean error: 5.195920944213867 mm for frame 76

Lowest mean error: 3.3595097064971924 mm for frame 27

Saving results

Total time: 47.56163501739502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845157
Iteration 2/25 | Loss: 0.00117309
Iteration 3/25 | Loss: 0.00108457
Iteration 4/25 | Loss: 0.00107404
Iteration 5/25 | Loss: 0.00107041
Iteration 6/25 | Loss: 0.00106950
Iteration 7/25 | Loss: 0.00106950
Iteration 8/25 | Loss: 0.00106950
Iteration 9/25 | Loss: 0.00106950
Iteration 10/25 | Loss: 0.00106950
Iteration 11/25 | Loss: 0.00106950
Iteration 12/25 | Loss: 0.00106950
Iteration 13/25 | Loss: 0.00106950
Iteration 14/25 | Loss: 0.00106950
Iteration 15/25 | Loss: 0.00106950
Iteration 16/25 | Loss: 0.00106950
Iteration 17/25 | Loss: 0.00106950
Iteration 18/25 | Loss: 0.00106950
Iteration 19/25 | Loss: 0.00106950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010695045348256826, 0.0010695045348256826, 0.0010695045348256826, 0.0010695045348256826, 0.0010695045348256826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010695045348256826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66531646
Iteration 2/25 | Loss: 0.00087481
Iteration 3/25 | Loss: 0.00087481
Iteration 4/25 | Loss: 0.00087481
Iteration 5/25 | Loss: 0.00087481
Iteration 6/25 | Loss: 0.00087481
Iteration 7/25 | Loss: 0.00087481
Iteration 8/25 | Loss: 0.00087481
Iteration 9/25 | Loss: 0.00087481
Iteration 10/25 | Loss: 0.00087481
Iteration 11/25 | Loss: 0.00087481
Iteration 12/25 | Loss: 0.00087481
Iteration 13/25 | Loss: 0.00087481
Iteration 14/25 | Loss: 0.00087481
Iteration 15/25 | Loss: 0.00087481
Iteration 16/25 | Loss: 0.00087481
Iteration 17/25 | Loss: 0.00087481
Iteration 18/25 | Loss: 0.00087481
Iteration 19/25 | Loss: 0.00087481
Iteration 20/25 | Loss: 0.00087481
Iteration 21/25 | Loss: 0.00087481
Iteration 22/25 | Loss: 0.00087481
Iteration 23/25 | Loss: 0.00087481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008748102118261158, 0.0008748102118261158, 0.0008748102118261158, 0.0008748102118261158, 0.0008748102118261158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008748102118261158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087481
Iteration 2/1000 | Loss: 0.00001913
Iteration 3/1000 | Loss: 0.00001271
Iteration 4/1000 | Loss: 0.00001104
Iteration 5/1000 | Loss: 0.00001045
Iteration 6/1000 | Loss: 0.00001004
Iteration 7/1000 | Loss: 0.00000969
Iteration 8/1000 | Loss: 0.00000943
Iteration 9/1000 | Loss: 0.00000933
Iteration 10/1000 | Loss: 0.00000914
Iteration 11/1000 | Loss: 0.00000906
Iteration 12/1000 | Loss: 0.00000903
Iteration 13/1000 | Loss: 0.00000899
Iteration 14/1000 | Loss: 0.00000898
Iteration 15/1000 | Loss: 0.00000892
Iteration 16/1000 | Loss: 0.00000891
Iteration 17/1000 | Loss: 0.00000890
Iteration 18/1000 | Loss: 0.00000889
Iteration 19/1000 | Loss: 0.00000887
Iteration 20/1000 | Loss: 0.00000887
Iteration 21/1000 | Loss: 0.00000886
Iteration 22/1000 | Loss: 0.00000886
Iteration 23/1000 | Loss: 0.00000885
Iteration 24/1000 | Loss: 0.00000884
Iteration 25/1000 | Loss: 0.00000882
Iteration 26/1000 | Loss: 0.00000881
Iteration 27/1000 | Loss: 0.00000881
Iteration 28/1000 | Loss: 0.00000881
Iteration 29/1000 | Loss: 0.00000881
Iteration 30/1000 | Loss: 0.00000881
Iteration 31/1000 | Loss: 0.00000881
Iteration 32/1000 | Loss: 0.00000881
Iteration 33/1000 | Loss: 0.00000881
Iteration 34/1000 | Loss: 0.00000880
Iteration 35/1000 | Loss: 0.00000880
Iteration 36/1000 | Loss: 0.00000880
Iteration 37/1000 | Loss: 0.00000879
Iteration 38/1000 | Loss: 0.00000879
Iteration 39/1000 | Loss: 0.00000878
Iteration 40/1000 | Loss: 0.00000878
Iteration 41/1000 | Loss: 0.00000877
Iteration 42/1000 | Loss: 0.00000877
Iteration 43/1000 | Loss: 0.00000877
Iteration 44/1000 | Loss: 0.00000877
Iteration 45/1000 | Loss: 0.00000877
Iteration 46/1000 | Loss: 0.00000876
Iteration 47/1000 | Loss: 0.00000875
Iteration 48/1000 | Loss: 0.00000875
Iteration 49/1000 | Loss: 0.00000872
Iteration 50/1000 | Loss: 0.00000872
Iteration 51/1000 | Loss: 0.00000872
Iteration 52/1000 | Loss: 0.00000872
Iteration 53/1000 | Loss: 0.00000872
Iteration 54/1000 | Loss: 0.00000872
Iteration 55/1000 | Loss: 0.00000872
Iteration 56/1000 | Loss: 0.00000872
Iteration 57/1000 | Loss: 0.00000872
Iteration 58/1000 | Loss: 0.00000872
Iteration 59/1000 | Loss: 0.00000871
Iteration 60/1000 | Loss: 0.00000871
Iteration 61/1000 | Loss: 0.00000871
Iteration 62/1000 | Loss: 0.00000870
Iteration 63/1000 | Loss: 0.00000869
Iteration 64/1000 | Loss: 0.00000869
Iteration 65/1000 | Loss: 0.00000869
Iteration 66/1000 | Loss: 0.00000868
Iteration 67/1000 | Loss: 0.00000868
Iteration 68/1000 | Loss: 0.00000868
Iteration 69/1000 | Loss: 0.00000867
Iteration 70/1000 | Loss: 0.00000867
Iteration 71/1000 | Loss: 0.00000867
Iteration 72/1000 | Loss: 0.00000867
Iteration 73/1000 | Loss: 0.00000867
Iteration 74/1000 | Loss: 0.00000867
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000866
Iteration 79/1000 | Loss: 0.00000865
Iteration 80/1000 | Loss: 0.00000865
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000864
Iteration 83/1000 | Loss: 0.00000864
Iteration 84/1000 | Loss: 0.00000864
Iteration 85/1000 | Loss: 0.00000864
Iteration 86/1000 | Loss: 0.00000864
Iteration 87/1000 | Loss: 0.00000864
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000863
Iteration 95/1000 | Loss: 0.00000863
Iteration 96/1000 | Loss: 0.00000863
Iteration 97/1000 | Loss: 0.00000863
Iteration 98/1000 | Loss: 0.00000863
Iteration 99/1000 | Loss: 0.00000863
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000863
Iteration 102/1000 | Loss: 0.00000863
Iteration 103/1000 | Loss: 0.00000862
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000862
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000862
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000862
Iteration 110/1000 | Loss: 0.00000861
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000861
Iteration 113/1000 | Loss: 0.00000861
Iteration 114/1000 | Loss: 0.00000861
Iteration 115/1000 | Loss: 0.00000860
Iteration 116/1000 | Loss: 0.00000860
Iteration 117/1000 | Loss: 0.00000860
Iteration 118/1000 | Loss: 0.00000860
Iteration 119/1000 | Loss: 0.00000860
Iteration 120/1000 | Loss: 0.00000860
Iteration 121/1000 | Loss: 0.00000860
Iteration 122/1000 | Loss: 0.00000860
Iteration 123/1000 | Loss: 0.00000860
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000858
Iteration 131/1000 | Loss: 0.00000858
Iteration 132/1000 | Loss: 0.00000858
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000857
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000856
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000855
Iteration 153/1000 | Loss: 0.00000855
Iteration 154/1000 | Loss: 0.00000855
Iteration 155/1000 | Loss: 0.00000855
Iteration 156/1000 | Loss: 0.00000855
Iteration 157/1000 | Loss: 0.00000855
Iteration 158/1000 | Loss: 0.00000855
Iteration 159/1000 | Loss: 0.00000855
Iteration 160/1000 | Loss: 0.00000855
Iteration 161/1000 | Loss: 0.00000855
Iteration 162/1000 | Loss: 0.00000855
Iteration 163/1000 | Loss: 0.00000854
Iteration 164/1000 | Loss: 0.00000854
Iteration 165/1000 | Loss: 0.00000854
Iteration 166/1000 | Loss: 0.00000853
Iteration 167/1000 | Loss: 0.00000853
Iteration 168/1000 | Loss: 0.00000853
Iteration 169/1000 | Loss: 0.00000853
Iteration 170/1000 | Loss: 0.00000853
Iteration 171/1000 | Loss: 0.00000852
Iteration 172/1000 | Loss: 0.00000852
Iteration 173/1000 | Loss: 0.00000852
Iteration 174/1000 | Loss: 0.00000852
Iteration 175/1000 | Loss: 0.00000852
Iteration 176/1000 | Loss: 0.00000852
Iteration 177/1000 | Loss: 0.00000852
Iteration 178/1000 | Loss: 0.00000852
Iteration 179/1000 | Loss: 0.00000852
Iteration 180/1000 | Loss: 0.00000852
Iteration 181/1000 | Loss: 0.00000851
Iteration 182/1000 | Loss: 0.00000851
Iteration 183/1000 | Loss: 0.00000851
Iteration 184/1000 | Loss: 0.00000851
Iteration 185/1000 | Loss: 0.00000851
Iteration 186/1000 | Loss: 0.00000851
Iteration 187/1000 | Loss: 0.00000850
Iteration 188/1000 | Loss: 0.00000850
Iteration 189/1000 | Loss: 0.00000850
Iteration 190/1000 | Loss: 0.00000850
Iteration 191/1000 | Loss: 0.00000850
Iteration 192/1000 | Loss: 0.00000850
Iteration 193/1000 | Loss: 0.00000850
Iteration 194/1000 | Loss: 0.00000850
Iteration 195/1000 | Loss: 0.00000850
Iteration 196/1000 | Loss: 0.00000850
Iteration 197/1000 | Loss: 0.00000850
Iteration 198/1000 | Loss: 0.00000850
Iteration 199/1000 | Loss: 0.00000850
Iteration 200/1000 | Loss: 0.00000850
Iteration 201/1000 | Loss: 0.00000850
Iteration 202/1000 | Loss: 0.00000850
Iteration 203/1000 | Loss: 0.00000850
Iteration 204/1000 | Loss: 0.00000850
Iteration 205/1000 | Loss: 0.00000850
Iteration 206/1000 | Loss: 0.00000850
Iteration 207/1000 | Loss: 0.00000850
Iteration 208/1000 | Loss: 0.00000850
Iteration 209/1000 | Loss: 0.00000850
Iteration 210/1000 | Loss: 0.00000850
Iteration 211/1000 | Loss: 0.00000850
Iteration 212/1000 | Loss: 0.00000850
Iteration 213/1000 | Loss: 0.00000850
Iteration 214/1000 | Loss: 0.00000850
Iteration 215/1000 | Loss: 0.00000850
Iteration 216/1000 | Loss: 0.00000850
Iteration 217/1000 | Loss: 0.00000850
Iteration 218/1000 | Loss: 0.00000850
Iteration 219/1000 | Loss: 0.00000850
Iteration 220/1000 | Loss: 0.00000850
Iteration 221/1000 | Loss: 0.00000850
Iteration 222/1000 | Loss: 0.00000850
Iteration 223/1000 | Loss: 0.00000850
Iteration 224/1000 | Loss: 0.00000850
Iteration 225/1000 | Loss: 0.00000850
Iteration 226/1000 | Loss: 0.00000850
Iteration 227/1000 | Loss: 0.00000850
Iteration 228/1000 | Loss: 0.00000850
Iteration 229/1000 | Loss: 0.00000850
Iteration 230/1000 | Loss: 0.00000850
Iteration 231/1000 | Loss: 0.00000850
Iteration 232/1000 | Loss: 0.00000850
Iteration 233/1000 | Loss: 0.00000850
Iteration 234/1000 | Loss: 0.00000850
Iteration 235/1000 | Loss: 0.00000850
Iteration 236/1000 | Loss: 0.00000850
Iteration 237/1000 | Loss: 0.00000850
Iteration 238/1000 | Loss: 0.00000850
Iteration 239/1000 | Loss: 0.00000850
Iteration 240/1000 | Loss: 0.00000850
Iteration 241/1000 | Loss: 0.00000850
Iteration 242/1000 | Loss: 0.00000850
Iteration 243/1000 | Loss: 0.00000850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [8.502361197315622e-06, 8.502361197315622e-06, 8.502361197315622e-06, 8.502361197315622e-06, 8.502361197315622e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.502361197315622e-06

Optimization complete. Final v2v error: 2.502997875213623 mm

Highest mean error: 2.912458658218384 mm for frame 55

Lowest mean error: 2.2947282791137695 mm for frame 127

Saving results

Total time: 38.11960792541504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491458
Iteration 2/25 | Loss: 0.00114781
Iteration 3/25 | Loss: 0.00108188
Iteration 4/25 | Loss: 0.00107421
Iteration 5/25 | Loss: 0.00107180
Iteration 6/25 | Loss: 0.00107140
Iteration 7/25 | Loss: 0.00107140
Iteration 8/25 | Loss: 0.00107140
Iteration 9/25 | Loss: 0.00107140
Iteration 10/25 | Loss: 0.00107140
Iteration 11/25 | Loss: 0.00107140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010713988449424505, 0.0010713988449424505, 0.0010713988449424505, 0.0010713988449424505, 0.0010713988449424505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010713988449424505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.97194290
Iteration 2/25 | Loss: 0.00080374
Iteration 3/25 | Loss: 0.00080374
Iteration 4/25 | Loss: 0.00080374
Iteration 5/25 | Loss: 0.00080374
Iteration 6/25 | Loss: 0.00080374
Iteration 7/25 | Loss: 0.00080374
Iteration 8/25 | Loss: 0.00080374
Iteration 9/25 | Loss: 0.00080373
Iteration 10/25 | Loss: 0.00080373
Iteration 11/25 | Loss: 0.00080373
Iteration 12/25 | Loss: 0.00080373
Iteration 13/25 | Loss: 0.00080373
Iteration 14/25 | Loss: 0.00080373
Iteration 15/25 | Loss: 0.00080373
Iteration 16/25 | Loss: 0.00080373
Iteration 17/25 | Loss: 0.00080373
Iteration 18/25 | Loss: 0.00080373
Iteration 19/25 | Loss: 0.00080373
Iteration 20/25 | Loss: 0.00080373
Iteration 21/25 | Loss: 0.00080373
Iteration 22/25 | Loss: 0.00080373
Iteration 23/25 | Loss: 0.00080373
Iteration 24/25 | Loss: 0.00080373
Iteration 25/25 | Loss: 0.00080373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080373
Iteration 2/1000 | Loss: 0.00001987
Iteration 3/1000 | Loss: 0.00001411
Iteration 4/1000 | Loss: 0.00001297
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001167
Iteration 7/1000 | Loss: 0.00001121
Iteration 8/1000 | Loss: 0.00001091
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001048
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001038
Iteration 16/1000 | Loss: 0.00001037
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001036
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001029
Iteration 21/1000 | Loss: 0.00001028
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001024
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001012
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001011
Iteration 34/1000 | Loss: 0.00001011
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001010
Iteration 37/1000 | Loss: 0.00001009
Iteration 38/1000 | Loss: 0.00001009
Iteration 39/1000 | Loss: 0.00001008
Iteration 40/1000 | Loss: 0.00001008
Iteration 41/1000 | Loss: 0.00001008
Iteration 42/1000 | Loss: 0.00001008
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001007
Iteration 50/1000 | Loss: 0.00001007
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001004
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001004
Iteration 59/1000 | Loss: 0.00001003
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000997
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000996
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000996
Iteration 88/1000 | Loss: 0.00000996
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000993
Iteration 98/1000 | Loss: 0.00000993
Iteration 99/1000 | Loss: 0.00000993
Iteration 100/1000 | Loss: 0.00000993
Iteration 101/1000 | Loss: 0.00000993
Iteration 102/1000 | Loss: 0.00000992
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000992
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000990
Iteration 112/1000 | Loss: 0.00000990
Iteration 113/1000 | Loss: 0.00000989
Iteration 114/1000 | Loss: 0.00000989
Iteration 115/1000 | Loss: 0.00000989
Iteration 116/1000 | Loss: 0.00000989
Iteration 117/1000 | Loss: 0.00000988
Iteration 118/1000 | Loss: 0.00000988
Iteration 119/1000 | Loss: 0.00000987
Iteration 120/1000 | Loss: 0.00000987
Iteration 121/1000 | Loss: 0.00000986
Iteration 122/1000 | Loss: 0.00000986
Iteration 123/1000 | Loss: 0.00000986
Iteration 124/1000 | Loss: 0.00000986
Iteration 125/1000 | Loss: 0.00000986
Iteration 126/1000 | Loss: 0.00000986
Iteration 127/1000 | Loss: 0.00000985
Iteration 128/1000 | Loss: 0.00000985
Iteration 129/1000 | Loss: 0.00000985
Iteration 130/1000 | Loss: 0.00000985
Iteration 131/1000 | Loss: 0.00000985
Iteration 132/1000 | Loss: 0.00000985
Iteration 133/1000 | Loss: 0.00000985
Iteration 134/1000 | Loss: 0.00000985
Iteration 135/1000 | Loss: 0.00000984
Iteration 136/1000 | Loss: 0.00000984
Iteration 137/1000 | Loss: 0.00000984
Iteration 138/1000 | Loss: 0.00000983
Iteration 139/1000 | Loss: 0.00000983
Iteration 140/1000 | Loss: 0.00000983
Iteration 141/1000 | Loss: 0.00000982
Iteration 142/1000 | Loss: 0.00000982
Iteration 143/1000 | Loss: 0.00000982
Iteration 144/1000 | Loss: 0.00000982
Iteration 145/1000 | Loss: 0.00000981
Iteration 146/1000 | Loss: 0.00000981
Iteration 147/1000 | Loss: 0.00000981
Iteration 148/1000 | Loss: 0.00000981
Iteration 149/1000 | Loss: 0.00000980
Iteration 150/1000 | Loss: 0.00000980
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000979
Iteration 154/1000 | Loss: 0.00000979
Iteration 155/1000 | Loss: 0.00000979
Iteration 156/1000 | Loss: 0.00000979
Iteration 157/1000 | Loss: 0.00000979
Iteration 158/1000 | Loss: 0.00000978
Iteration 159/1000 | Loss: 0.00000978
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000978
Iteration 165/1000 | Loss: 0.00000978
Iteration 166/1000 | Loss: 0.00000978
Iteration 167/1000 | Loss: 0.00000978
Iteration 168/1000 | Loss: 0.00000978
Iteration 169/1000 | Loss: 0.00000978
Iteration 170/1000 | Loss: 0.00000978
Iteration 171/1000 | Loss: 0.00000978
Iteration 172/1000 | Loss: 0.00000977
Iteration 173/1000 | Loss: 0.00000977
Iteration 174/1000 | Loss: 0.00000977
Iteration 175/1000 | Loss: 0.00000977
Iteration 176/1000 | Loss: 0.00000977
Iteration 177/1000 | Loss: 0.00000977
Iteration 178/1000 | Loss: 0.00000977
Iteration 179/1000 | Loss: 0.00000977
Iteration 180/1000 | Loss: 0.00000977
Iteration 181/1000 | Loss: 0.00000977
Iteration 182/1000 | Loss: 0.00000977
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000975
Iteration 195/1000 | Loss: 0.00000975
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00000975
Iteration 198/1000 | Loss: 0.00000975
Iteration 199/1000 | Loss: 0.00000974
Iteration 200/1000 | Loss: 0.00000974
Iteration 201/1000 | Loss: 0.00000974
Iteration 202/1000 | Loss: 0.00000974
Iteration 203/1000 | Loss: 0.00000974
Iteration 204/1000 | Loss: 0.00000974
Iteration 205/1000 | Loss: 0.00000974
Iteration 206/1000 | Loss: 0.00000974
Iteration 207/1000 | Loss: 0.00000974
Iteration 208/1000 | Loss: 0.00000974
Iteration 209/1000 | Loss: 0.00000973
Iteration 210/1000 | Loss: 0.00000973
Iteration 211/1000 | Loss: 0.00000973
Iteration 212/1000 | Loss: 0.00000973
Iteration 213/1000 | Loss: 0.00000973
Iteration 214/1000 | Loss: 0.00000973
Iteration 215/1000 | Loss: 0.00000973
Iteration 216/1000 | Loss: 0.00000973
Iteration 217/1000 | Loss: 0.00000973
Iteration 218/1000 | Loss: 0.00000973
Iteration 219/1000 | Loss: 0.00000973
Iteration 220/1000 | Loss: 0.00000973
Iteration 221/1000 | Loss: 0.00000972
Iteration 222/1000 | Loss: 0.00000972
Iteration 223/1000 | Loss: 0.00000972
Iteration 224/1000 | Loss: 0.00000972
Iteration 225/1000 | Loss: 0.00000972
Iteration 226/1000 | Loss: 0.00000972
Iteration 227/1000 | Loss: 0.00000972
Iteration 228/1000 | Loss: 0.00000972
Iteration 229/1000 | Loss: 0.00000972
Iteration 230/1000 | Loss: 0.00000972
Iteration 231/1000 | Loss: 0.00000972
Iteration 232/1000 | Loss: 0.00000972
Iteration 233/1000 | Loss: 0.00000972
Iteration 234/1000 | Loss: 0.00000972
Iteration 235/1000 | Loss: 0.00000972
Iteration 236/1000 | Loss: 0.00000972
Iteration 237/1000 | Loss: 0.00000972
Iteration 238/1000 | Loss: 0.00000972
Iteration 239/1000 | Loss: 0.00000971
Iteration 240/1000 | Loss: 0.00000971
Iteration 241/1000 | Loss: 0.00000971
Iteration 242/1000 | Loss: 0.00000971
Iteration 243/1000 | Loss: 0.00000971
Iteration 244/1000 | Loss: 0.00000971
Iteration 245/1000 | Loss: 0.00000971
Iteration 246/1000 | Loss: 0.00000971
Iteration 247/1000 | Loss: 0.00000971
Iteration 248/1000 | Loss: 0.00000971
Iteration 249/1000 | Loss: 0.00000971
Iteration 250/1000 | Loss: 0.00000971
Iteration 251/1000 | Loss: 0.00000971
Iteration 252/1000 | Loss: 0.00000971
Iteration 253/1000 | Loss: 0.00000971
Iteration 254/1000 | Loss: 0.00000971
Iteration 255/1000 | Loss: 0.00000971
Iteration 256/1000 | Loss: 0.00000971
Iteration 257/1000 | Loss: 0.00000970
Iteration 258/1000 | Loss: 0.00000970
Iteration 259/1000 | Loss: 0.00000970
Iteration 260/1000 | Loss: 0.00000970
Iteration 261/1000 | Loss: 0.00000970
Iteration 262/1000 | Loss: 0.00000970
Iteration 263/1000 | Loss: 0.00000970
Iteration 264/1000 | Loss: 0.00000970
Iteration 265/1000 | Loss: 0.00000970
Iteration 266/1000 | Loss: 0.00000970
Iteration 267/1000 | Loss: 0.00000970
Iteration 268/1000 | Loss: 0.00000969
Iteration 269/1000 | Loss: 0.00000969
Iteration 270/1000 | Loss: 0.00000969
Iteration 271/1000 | Loss: 0.00000969
Iteration 272/1000 | Loss: 0.00000969
Iteration 273/1000 | Loss: 0.00000969
Iteration 274/1000 | Loss: 0.00000969
Iteration 275/1000 | Loss: 0.00000969
Iteration 276/1000 | Loss: 0.00000969
Iteration 277/1000 | Loss: 0.00000969
Iteration 278/1000 | Loss: 0.00000969
Iteration 279/1000 | Loss: 0.00000969
Iteration 280/1000 | Loss: 0.00000969
Iteration 281/1000 | Loss: 0.00000969
Iteration 282/1000 | Loss: 0.00000969
Iteration 283/1000 | Loss: 0.00000969
Iteration 284/1000 | Loss: 0.00000969
Iteration 285/1000 | Loss: 0.00000968
Iteration 286/1000 | Loss: 0.00000968
Iteration 287/1000 | Loss: 0.00000968
Iteration 288/1000 | Loss: 0.00000968
Iteration 289/1000 | Loss: 0.00000968
Iteration 290/1000 | Loss: 0.00000968
Iteration 291/1000 | Loss: 0.00000968
Iteration 292/1000 | Loss: 0.00000968
Iteration 293/1000 | Loss: 0.00000968
Iteration 294/1000 | Loss: 0.00000968
Iteration 295/1000 | Loss: 0.00000968
Iteration 296/1000 | Loss: 0.00000968
Iteration 297/1000 | Loss: 0.00000968
Iteration 298/1000 | Loss: 0.00000968
Iteration 299/1000 | Loss: 0.00000968
Iteration 300/1000 | Loss: 0.00000968
Iteration 301/1000 | Loss: 0.00000968
Iteration 302/1000 | Loss: 0.00000968
Iteration 303/1000 | Loss: 0.00000968
Iteration 304/1000 | Loss: 0.00000968
Iteration 305/1000 | Loss: 0.00000968
Iteration 306/1000 | Loss: 0.00000968
Iteration 307/1000 | Loss: 0.00000968
Iteration 308/1000 | Loss: 0.00000968
Iteration 309/1000 | Loss: 0.00000968
Iteration 310/1000 | Loss: 0.00000968
Iteration 311/1000 | Loss: 0.00000968
Iteration 312/1000 | Loss: 0.00000968
Iteration 313/1000 | Loss: 0.00000968
Iteration 314/1000 | Loss: 0.00000968
Iteration 315/1000 | Loss: 0.00000968
Iteration 316/1000 | Loss: 0.00000968
Iteration 317/1000 | Loss: 0.00000968
Iteration 318/1000 | Loss: 0.00000968
Iteration 319/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 319. Stopping optimization.
Last 5 losses: [9.684996257419698e-06, 9.684996257419698e-06, 9.684996257419698e-06, 9.684996257419698e-06, 9.684996257419698e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.684996257419698e-06

Optimization complete. Final v2v error: 2.670828104019165 mm

Highest mean error: 3.1286299228668213 mm for frame 68

Lowest mean error: 2.4110591411590576 mm for frame 135

Saving results

Total time: 43.95883822441101
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828888
Iteration 2/25 | Loss: 0.00161083
Iteration 3/25 | Loss: 0.00126207
Iteration 4/25 | Loss: 0.00123680
Iteration 5/25 | Loss: 0.00123169
Iteration 6/25 | Loss: 0.00122791
Iteration 7/25 | Loss: 0.00122375
Iteration 8/25 | Loss: 0.00122351
Iteration 9/25 | Loss: 0.00122342
Iteration 10/25 | Loss: 0.00122341
Iteration 11/25 | Loss: 0.00122341
Iteration 12/25 | Loss: 0.00122341
Iteration 13/25 | Loss: 0.00122341
Iteration 14/25 | Loss: 0.00122341
Iteration 15/25 | Loss: 0.00122341
Iteration 16/25 | Loss: 0.00122341
Iteration 17/25 | Loss: 0.00122341
Iteration 18/25 | Loss: 0.00122341
Iteration 19/25 | Loss: 0.00122341
Iteration 20/25 | Loss: 0.00122341
Iteration 21/25 | Loss: 0.00122341
Iteration 22/25 | Loss: 0.00122340
Iteration 23/25 | Loss: 0.00122340
Iteration 24/25 | Loss: 0.00122340
Iteration 25/25 | Loss: 0.00122340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19575763
Iteration 2/25 | Loss: 0.00089042
Iteration 3/25 | Loss: 0.00089042
Iteration 4/25 | Loss: 0.00089042
Iteration 5/25 | Loss: 0.00089042
Iteration 6/25 | Loss: 0.00089042
Iteration 7/25 | Loss: 0.00089042
Iteration 8/25 | Loss: 0.00089042
Iteration 9/25 | Loss: 0.00089042
Iteration 10/25 | Loss: 0.00089042
Iteration 11/25 | Loss: 0.00089042
Iteration 12/25 | Loss: 0.00089042
Iteration 13/25 | Loss: 0.00089042
Iteration 14/25 | Loss: 0.00089042
Iteration 15/25 | Loss: 0.00089042
Iteration 16/25 | Loss: 0.00089042
Iteration 17/25 | Loss: 0.00089042
Iteration 18/25 | Loss: 0.00089042
Iteration 19/25 | Loss: 0.00089042
Iteration 20/25 | Loss: 0.00089042
Iteration 21/25 | Loss: 0.00089042
Iteration 22/25 | Loss: 0.00089042
Iteration 23/25 | Loss: 0.00089042
Iteration 24/25 | Loss: 0.00089042
Iteration 25/25 | Loss: 0.00089042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089042
Iteration 2/1000 | Loss: 0.00003400
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001946
Iteration 5/1000 | Loss: 0.00001840
Iteration 6/1000 | Loss: 0.00001780
Iteration 7/1000 | Loss: 0.00001744
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001642
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001639
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001637
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001633
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001630
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001630
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00001625
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001623
Iteration 64/1000 | Loss: 0.00001622
Iteration 65/1000 | Loss: 0.00001622
Iteration 66/1000 | Loss: 0.00001622
Iteration 67/1000 | Loss: 0.00001621
Iteration 68/1000 | Loss: 0.00001621
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001621
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001621
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001621
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001619
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001618
Iteration 93/1000 | Loss: 0.00001618
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001618
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001617
Iteration 107/1000 | Loss: 0.00001617
Iteration 108/1000 | Loss: 0.00001617
Iteration 109/1000 | Loss: 0.00001617
Iteration 110/1000 | Loss: 0.00001617
Iteration 111/1000 | Loss: 0.00001617
Iteration 112/1000 | Loss: 0.00001617
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.6165189663297497e-05, 1.6165189663297497e-05, 1.6165189663297497e-05, 1.6165189663297497e-05, 1.6165189663297497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6165189663297497e-05

Optimization complete. Final v2v error: 3.4115049839019775 mm

Highest mean error: 4.085865497589111 mm for frame 107

Lowest mean error: 3.048302173614502 mm for frame 144

Saving results

Total time: 43.25616240501404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421927
Iteration 2/25 | Loss: 0.00117269
Iteration 3/25 | Loss: 0.00110101
Iteration 4/25 | Loss: 0.00108711
Iteration 5/25 | Loss: 0.00108246
Iteration 6/25 | Loss: 0.00108156
Iteration 7/25 | Loss: 0.00108156
Iteration 8/25 | Loss: 0.00108156
Iteration 9/25 | Loss: 0.00108156
Iteration 10/25 | Loss: 0.00108156
Iteration 11/25 | Loss: 0.00108156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010815637651830912, 0.0010815637651830912, 0.0010815637651830912, 0.0010815637651830912, 0.0010815637651830912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010815637651830912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62100434
Iteration 2/25 | Loss: 0.00082808
Iteration 3/25 | Loss: 0.00082808
Iteration 4/25 | Loss: 0.00082808
Iteration 5/25 | Loss: 0.00082808
Iteration 6/25 | Loss: 0.00082808
Iteration 7/25 | Loss: 0.00082808
Iteration 8/25 | Loss: 0.00082808
Iteration 9/25 | Loss: 0.00082808
Iteration 10/25 | Loss: 0.00082808
Iteration 11/25 | Loss: 0.00082808
Iteration 12/25 | Loss: 0.00082808
Iteration 13/25 | Loss: 0.00082808
Iteration 14/25 | Loss: 0.00082808
Iteration 15/25 | Loss: 0.00082808
Iteration 16/25 | Loss: 0.00082808
Iteration 17/25 | Loss: 0.00082808
Iteration 18/25 | Loss: 0.00082808
Iteration 19/25 | Loss: 0.00082808
Iteration 20/25 | Loss: 0.00082808
Iteration 21/25 | Loss: 0.00082808
Iteration 22/25 | Loss: 0.00082808
Iteration 23/25 | Loss: 0.00082808
Iteration 24/25 | Loss: 0.00082808
Iteration 25/25 | Loss: 0.00082808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082808
Iteration 2/1000 | Loss: 0.00002285
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001298
Iteration 6/1000 | Loss: 0.00001262
Iteration 7/1000 | Loss: 0.00001232
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001166
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001163
Iteration 15/1000 | Loss: 0.00001163
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001154
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001134
Iteration 25/1000 | Loss: 0.00001134
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001130
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001128
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001124
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001123
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001118
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001116
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001114
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001112
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001112
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001111
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001108
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001107
Iteration 96/1000 | Loss: 0.00001107
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001106
Iteration 101/1000 | Loss: 0.00001106
Iteration 102/1000 | Loss: 0.00001106
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001105
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001103
Iteration 112/1000 | Loss: 0.00001103
Iteration 113/1000 | Loss: 0.00001103
Iteration 114/1000 | Loss: 0.00001103
Iteration 115/1000 | Loss: 0.00001103
Iteration 116/1000 | Loss: 0.00001103
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001101
Iteration 122/1000 | Loss: 0.00001101
Iteration 123/1000 | Loss: 0.00001101
Iteration 124/1000 | Loss: 0.00001101
Iteration 125/1000 | Loss: 0.00001101
Iteration 126/1000 | Loss: 0.00001101
Iteration 127/1000 | Loss: 0.00001101
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001100
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001099
Iteration 140/1000 | Loss: 0.00001099
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001098
Iteration 143/1000 | Loss: 0.00001098
Iteration 144/1000 | Loss: 0.00001098
Iteration 145/1000 | Loss: 0.00001098
Iteration 146/1000 | Loss: 0.00001098
Iteration 147/1000 | Loss: 0.00001098
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001098
Iteration 155/1000 | Loss: 0.00001098
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001097
Iteration 160/1000 | Loss: 0.00001097
Iteration 161/1000 | Loss: 0.00001097
Iteration 162/1000 | Loss: 0.00001097
Iteration 163/1000 | Loss: 0.00001097
Iteration 164/1000 | Loss: 0.00001097
Iteration 165/1000 | Loss: 0.00001097
Iteration 166/1000 | Loss: 0.00001097
Iteration 167/1000 | Loss: 0.00001096
Iteration 168/1000 | Loss: 0.00001096
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001096
Iteration 172/1000 | Loss: 0.00001096
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001096
Iteration 177/1000 | Loss: 0.00001096
Iteration 178/1000 | Loss: 0.00001096
Iteration 179/1000 | Loss: 0.00001096
Iteration 180/1000 | Loss: 0.00001096
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001096
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001095
Iteration 186/1000 | Loss: 0.00001095
Iteration 187/1000 | Loss: 0.00001095
Iteration 188/1000 | Loss: 0.00001095
Iteration 189/1000 | Loss: 0.00001095
Iteration 190/1000 | Loss: 0.00001095
Iteration 191/1000 | Loss: 0.00001095
Iteration 192/1000 | Loss: 0.00001095
Iteration 193/1000 | Loss: 0.00001095
Iteration 194/1000 | Loss: 0.00001095
Iteration 195/1000 | Loss: 0.00001094
Iteration 196/1000 | Loss: 0.00001094
Iteration 197/1000 | Loss: 0.00001094
Iteration 198/1000 | Loss: 0.00001094
Iteration 199/1000 | Loss: 0.00001094
Iteration 200/1000 | Loss: 0.00001094
Iteration 201/1000 | Loss: 0.00001094
Iteration 202/1000 | Loss: 0.00001094
Iteration 203/1000 | Loss: 0.00001094
Iteration 204/1000 | Loss: 0.00001094
Iteration 205/1000 | Loss: 0.00001094
Iteration 206/1000 | Loss: 0.00001094
Iteration 207/1000 | Loss: 0.00001094
Iteration 208/1000 | Loss: 0.00001093
Iteration 209/1000 | Loss: 0.00001093
Iteration 210/1000 | Loss: 0.00001093
Iteration 211/1000 | Loss: 0.00001093
Iteration 212/1000 | Loss: 0.00001093
Iteration 213/1000 | Loss: 0.00001093
Iteration 214/1000 | Loss: 0.00001093
Iteration 215/1000 | Loss: 0.00001093
Iteration 216/1000 | Loss: 0.00001092
Iteration 217/1000 | Loss: 0.00001092
Iteration 218/1000 | Loss: 0.00001092
Iteration 219/1000 | Loss: 0.00001092
Iteration 220/1000 | Loss: 0.00001092
Iteration 221/1000 | Loss: 0.00001092
Iteration 222/1000 | Loss: 0.00001092
Iteration 223/1000 | Loss: 0.00001092
Iteration 224/1000 | Loss: 0.00001092
Iteration 225/1000 | Loss: 0.00001092
Iteration 226/1000 | Loss: 0.00001092
Iteration 227/1000 | Loss: 0.00001092
Iteration 228/1000 | Loss: 0.00001092
Iteration 229/1000 | Loss: 0.00001092
Iteration 230/1000 | Loss: 0.00001092
Iteration 231/1000 | Loss: 0.00001092
Iteration 232/1000 | Loss: 0.00001092
Iteration 233/1000 | Loss: 0.00001092
Iteration 234/1000 | Loss: 0.00001092
Iteration 235/1000 | Loss: 0.00001092
Iteration 236/1000 | Loss: 0.00001092
Iteration 237/1000 | Loss: 0.00001092
Iteration 238/1000 | Loss: 0.00001092
Iteration 239/1000 | Loss: 0.00001092
Iteration 240/1000 | Loss: 0.00001092
Iteration 241/1000 | Loss: 0.00001092
Iteration 242/1000 | Loss: 0.00001092
Iteration 243/1000 | Loss: 0.00001092
Iteration 244/1000 | Loss: 0.00001092
Iteration 245/1000 | Loss: 0.00001092
Iteration 246/1000 | Loss: 0.00001092
Iteration 247/1000 | Loss: 0.00001092
Iteration 248/1000 | Loss: 0.00001092
Iteration 249/1000 | Loss: 0.00001092
Iteration 250/1000 | Loss: 0.00001092
Iteration 251/1000 | Loss: 0.00001092
Iteration 252/1000 | Loss: 0.00001092
Iteration 253/1000 | Loss: 0.00001092
Iteration 254/1000 | Loss: 0.00001092
Iteration 255/1000 | Loss: 0.00001092
Iteration 256/1000 | Loss: 0.00001092
Iteration 257/1000 | Loss: 0.00001092
Iteration 258/1000 | Loss: 0.00001092
Iteration 259/1000 | Loss: 0.00001092
Iteration 260/1000 | Loss: 0.00001092
Iteration 261/1000 | Loss: 0.00001092
Iteration 262/1000 | Loss: 0.00001092
Iteration 263/1000 | Loss: 0.00001092
Iteration 264/1000 | Loss: 0.00001092
Iteration 265/1000 | Loss: 0.00001092
Iteration 266/1000 | Loss: 0.00001092
Iteration 267/1000 | Loss: 0.00001092
Iteration 268/1000 | Loss: 0.00001092
Iteration 269/1000 | Loss: 0.00001092
Iteration 270/1000 | Loss: 0.00001092
Iteration 271/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.0918054613284767e-05, 1.0918054613284767e-05, 1.0918054613284767e-05, 1.0918054613284767e-05, 1.0918054613284767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0918054613284767e-05

Optimization complete. Final v2v error: 2.8432247638702393 mm

Highest mean error: 3.359706163406372 mm for frame 62

Lowest mean error: 2.657665729522705 mm for frame 86

Saving results

Total time: 39.948734283447266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418421
Iteration 2/25 | Loss: 0.00151281
Iteration 3/25 | Loss: 0.00121766
Iteration 4/25 | Loss: 0.00117506
Iteration 5/25 | Loss: 0.00116605
Iteration 6/25 | Loss: 0.00116412
Iteration 7/25 | Loss: 0.00116402
Iteration 8/25 | Loss: 0.00116402
Iteration 9/25 | Loss: 0.00116402
Iteration 10/25 | Loss: 0.00116402
Iteration 11/25 | Loss: 0.00116402
Iteration 12/25 | Loss: 0.00116402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011640203883871436, 0.0011640203883871436, 0.0011640203883871436, 0.0011640203883871436, 0.0011640203883871436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011640203883871436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35879612
Iteration 2/25 | Loss: 0.00056652
Iteration 3/25 | Loss: 0.00056652
Iteration 4/25 | Loss: 0.00056652
Iteration 5/25 | Loss: 0.00056652
Iteration 6/25 | Loss: 0.00056652
Iteration 7/25 | Loss: 0.00056652
Iteration 8/25 | Loss: 0.00056652
Iteration 9/25 | Loss: 0.00056652
Iteration 10/25 | Loss: 0.00056651
Iteration 11/25 | Loss: 0.00056651
Iteration 12/25 | Loss: 0.00056651
Iteration 13/25 | Loss: 0.00056651
Iteration 14/25 | Loss: 0.00056651
Iteration 15/25 | Loss: 0.00056651
Iteration 16/25 | Loss: 0.00056651
Iteration 17/25 | Loss: 0.00056651
Iteration 18/25 | Loss: 0.00056651
Iteration 19/25 | Loss: 0.00056651
Iteration 20/25 | Loss: 0.00056651
Iteration 21/25 | Loss: 0.00056651
Iteration 22/25 | Loss: 0.00056651
Iteration 23/25 | Loss: 0.00056651
Iteration 24/25 | Loss: 0.00056651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005665146163664758, 0.0005665146163664758, 0.0005665146163664758, 0.0005665146163664758, 0.0005665146163664758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005665146163664758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056651
Iteration 2/1000 | Loss: 0.00004146
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00002267
Iteration 5/1000 | Loss: 0.00002191
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001944
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001921
Iteration 13/1000 | Loss: 0.00001918
Iteration 14/1000 | Loss: 0.00001911
Iteration 15/1000 | Loss: 0.00001910
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001900
Iteration 19/1000 | Loss: 0.00001899
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001897
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001893
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001890
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001888
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001888
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001887
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001885
Iteration 42/1000 | Loss: 0.00001885
Iteration 43/1000 | Loss: 0.00001884
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001880
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001879
Iteration 52/1000 | Loss: 0.00001879
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001878
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001878
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001877
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001877
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001877
Iteration 67/1000 | Loss: 0.00001877
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001877
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001876
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001875
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001874
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001872
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001871
Iteration 95/1000 | Loss: 0.00001870
Iteration 96/1000 | Loss: 0.00001870
Iteration 97/1000 | Loss: 0.00001870
Iteration 98/1000 | Loss: 0.00001870
Iteration 99/1000 | Loss: 0.00001870
Iteration 100/1000 | Loss: 0.00001870
Iteration 101/1000 | Loss: 0.00001870
Iteration 102/1000 | Loss: 0.00001870
Iteration 103/1000 | Loss: 0.00001870
Iteration 104/1000 | Loss: 0.00001870
Iteration 105/1000 | Loss: 0.00001869
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001869
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001867
Iteration 115/1000 | Loss: 0.00001867
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001867
Iteration 118/1000 | Loss: 0.00001867
Iteration 119/1000 | Loss: 0.00001867
Iteration 120/1000 | Loss: 0.00001867
Iteration 121/1000 | Loss: 0.00001867
Iteration 122/1000 | Loss: 0.00001867
Iteration 123/1000 | Loss: 0.00001867
Iteration 124/1000 | Loss: 0.00001867
Iteration 125/1000 | Loss: 0.00001867
Iteration 126/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.86744819075102e-05, 1.86744819075102e-05, 1.86744819075102e-05, 1.86744819075102e-05, 1.86744819075102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.86744819075102e-05

Optimization complete. Final v2v error: 3.6070964336395264 mm

Highest mean error: 3.9700679779052734 mm for frame 61

Lowest mean error: 3.1617677211761475 mm for frame 1

Saving results

Total time: 37.0323703289032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030791
Iteration 2/25 | Loss: 0.01030791
Iteration 3/25 | Loss: 0.01030791
Iteration 4/25 | Loss: 0.01030791
Iteration 5/25 | Loss: 0.01030790
Iteration 6/25 | Loss: 0.01030790
Iteration 7/25 | Loss: 0.01030790
Iteration 8/25 | Loss: 0.01030790
Iteration 9/25 | Loss: 0.01030790
Iteration 10/25 | Loss: 0.01030790
Iteration 11/25 | Loss: 0.01030790
Iteration 12/25 | Loss: 0.01030790
Iteration 13/25 | Loss: 0.01030790
Iteration 14/25 | Loss: 0.01030790
Iteration 15/25 | Loss: 0.01030789
Iteration 16/25 | Loss: 0.01030789
Iteration 17/25 | Loss: 0.01030789
Iteration 18/25 | Loss: 0.01030789
Iteration 19/25 | Loss: 0.01030789
Iteration 20/25 | Loss: 0.01030789
Iteration 21/25 | Loss: 0.01030789
Iteration 22/25 | Loss: 0.01030788
Iteration 23/25 | Loss: 0.01030788
Iteration 24/25 | Loss: 0.01030788
Iteration 25/25 | Loss: 0.01030788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83547986
Iteration 2/25 | Loss: 0.09552526
Iteration 3/25 | Loss: 0.08993586
Iteration 4/25 | Loss: 0.08831030
Iteration 5/25 | Loss: 0.08831029
Iteration 6/25 | Loss: 0.08831029
Iteration 7/25 | Loss: 0.08831029
Iteration 8/25 | Loss: 0.08831028
Iteration 9/25 | Loss: 0.08831028
Iteration 10/25 | Loss: 0.08831028
Iteration 11/25 | Loss: 0.08831028
Iteration 12/25 | Loss: 0.08831028
Iteration 13/25 | Loss: 0.08831026
Iteration 14/25 | Loss: 0.08831026
Iteration 15/25 | Loss: 0.08831026
Iteration 16/25 | Loss: 0.08831026
Iteration 17/25 | Loss: 0.08831026
Iteration 18/25 | Loss: 0.08831026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.08831026405096054, 0.08831026405096054, 0.08831026405096054, 0.08831026405096054, 0.08831026405096054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08831026405096054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08831026
Iteration 2/1000 | Loss: 0.00322255
Iteration 3/1000 | Loss: 0.00231926
Iteration 4/1000 | Loss: 0.00039035
Iteration 5/1000 | Loss: 0.00051070
Iteration 6/1000 | Loss: 0.00050610
Iteration 7/1000 | Loss: 0.00009966
Iteration 8/1000 | Loss: 0.00013035
Iteration 9/1000 | Loss: 0.00018068
Iteration 10/1000 | Loss: 0.00007693
Iteration 11/1000 | Loss: 0.00005135
Iteration 12/1000 | Loss: 0.00011937
Iteration 13/1000 | Loss: 0.00003189
Iteration 14/1000 | Loss: 0.00002938
Iteration 15/1000 | Loss: 0.00013021
Iteration 16/1000 | Loss: 0.00002647
Iteration 17/1000 | Loss: 0.00002553
Iteration 18/1000 | Loss: 0.00005045
Iteration 19/1000 | Loss: 0.00002365
Iteration 20/1000 | Loss: 0.00017084
Iteration 21/1000 | Loss: 0.00003520
Iteration 22/1000 | Loss: 0.00004199
Iteration 23/1000 | Loss: 0.00002123
Iteration 24/1000 | Loss: 0.00007187
Iteration 25/1000 | Loss: 0.00006911
Iteration 26/1000 | Loss: 0.00007791
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002308
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00002136
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00010834
Iteration 34/1000 | Loss: 0.00007463
Iteration 35/1000 | Loss: 0.00005791
Iteration 36/1000 | Loss: 0.00004991
Iteration 37/1000 | Loss: 0.00002838
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00003464
Iteration 40/1000 | Loss: 0.00003626
Iteration 41/1000 | Loss: 0.00011565
Iteration 42/1000 | Loss: 0.00003316
Iteration 43/1000 | Loss: 0.00004139
Iteration 44/1000 | Loss: 0.00004479
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002899
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00005656
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00003254
Iteration 52/1000 | Loss: 0.00001851
Iteration 53/1000 | Loss: 0.00002938
Iteration 54/1000 | Loss: 0.00009380
Iteration 55/1000 | Loss: 0.00004364
Iteration 56/1000 | Loss: 0.00005324
Iteration 57/1000 | Loss: 0.00005647
Iteration 58/1000 | Loss: 0.00004699
Iteration 59/1000 | Loss: 0.00007035
Iteration 60/1000 | Loss: 0.00004452
Iteration 61/1000 | Loss: 0.00008312
Iteration 62/1000 | Loss: 0.00008402
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00008016
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001609
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00008291
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00002171
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001592
Iteration 75/1000 | Loss: 0.00001591
Iteration 76/1000 | Loss: 0.00001591
Iteration 77/1000 | Loss: 0.00001591
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001590
Iteration 80/1000 | Loss: 0.00001590
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001590
Iteration 83/1000 | Loss: 0.00001590
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001584
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001583
Iteration 118/1000 | Loss: 0.00001583
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001582
Iteration 121/1000 | Loss: 0.00001582
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001579
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001578
Iteration 134/1000 | Loss: 0.00001578
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001577
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001575
Iteration 150/1000 | Loss: 0.00001575
Iteration 151/1000 | Loss: 0.00001575
Iteration 152/1000 | Loss: 0.00001575
Iteration 153/1000 | Loss: 0.00001575
Iteration 154/1000 | Loss: 0.00001574
Iteration 155/1000 | Loss: 0.00001574
Iteration 156/1000 | Loss: 0.00001574
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001573
Iteration 159/1000 | Loss: 0.00001573
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001572
Iteration 165/1000 | Loss: 0.00001572
Iteration 166/1000 | Loss: 0.00001572
Iteration 167/1000 | Loss: 0.00001572
Iteration 168/1000 | Loss: 0.00001572
Iteration 169/1000 | Loss: 0.00001572
Iteration 170/1000 | Loss: 0.00001572
Iteration 171/1000 | Loss: 0.00001572
Iteration 172/1000 | Loss: 0.00001572
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001572
Iteration 175/1000 | Loss: 0.00001572
Iteration 176/1000 | Loss: 0.00001572
Iteration 177/1000 | Loss: 0.00001572
Iteration 178/1000 | Loss: 0.00001571
Iteration 179/1000 | Loss: 0.00001571
Iteration 180/1000 | Loss: 0.00001571
Iteration 181/1000 | Loss: 0.00001571
Iteration 182/1000 | Loss: 0.00001570
Iteration 183/1000 | Loss: 0.00001570
Iteration 184/1000 | Loss: 0.00001570
Iteration 185/1000 | Loss: 0.00001570
Iteration 186/1000 | Loss: 0.00001570
Iteration 187/1000 | Loss: 0.00001569
Iteration 188/1000 | Loss: 0.00001569
Iteration 189/1000 | Loss: 0.00001569
Iteration 190/1000 | Loss: 0.00001569
Iteration 191/1000 | Loss: 0.00001568
Iteration 192/1000 | Loss: 0.00001568
Iteration 193/1000 | Loss: 0.00001568
Iteration 194/1000 | Loss: 0.00001567
Iteration 195/1000 | Loss: 0.00001567
Iteration 196/1000 | Loss: 0.00001567
Iteration 197/1000 | Loss: 0.00001567
Iteration 198/1000 | Loss: 0.00001567
Iteration 199/1000 | Loss: 0.00001567
Iteration 200/1000 | Loss: 0.00001567
Iteration 201/1000 | Loss: 0.00001566
Iteration 202/1000 | Loss: 0.00001566
Iteration 203/1000 | Loss: 0.00001566
Iteration 204/1000 | Loss: 0.00001566
Iteration 205/1000 | Loss: 0.00001565
Iteration 206/1000 | Loss: 0.00001565
Iteration 207/1000 | Loss: 0.00001565
Iteration 208/1000 | Loss: 0.00005939
Iteration 209/1000 | Loss: 0.00001709
Iteration 210/1000 | Loss: 0.00002207
Iteration 211/1000 | Loss: 0.00001574
Iteration 212/1000 | Loss: 0.00001592
Iteration 213/1000 | Loss: 0.00001580
Iteration 214/1000 | Loss: 0.00001579
Iteration 215/1000 | Loss: 0.00001579
Iteration 216/1000 | Loss: 0.00001579
Iteration 217/1000 | Loss: 0.00001579
Iteration 218/1000 | Loss: 0.00001579
Iteration 219/1000 | Loss: 0.00001579
Iteration 220/1000 | Loss: 0.00001579
Iteration 221/1000 | Loss: 0.00001579
Iteration 222/1000 | Loss: 0.00001578
Iteration 223/1000 | Loss: 0.00001578
Iteration 224/1000 | Loss: 0.00001578
Iteration 225/1000 | Loss: 0.00001578
Iteration 226/1000 | Loss: 0.00001578
Iteration 227/1000 | Loss: 0.00001577
Iteration 228/1000 | Loss: 0.00001577
Iteration 229/1000 | Loss: 0.00001577
Iteration 230/1000 | Loss: 0.00001577
Iteration 231/1000 | Loss: 0.00001577
Iteration 232/1000 | Loss: 0.00001569
Iteration 233/1000 | Loss: 0.00001560
Iteration 234/1000 | Loss: 0.00001582
Iteration 235/1000 | Loss: 0.00001580
Iteration 236/1000 | Loss: 0.00001580
Iteration 237/1000 | Loss: 0.00001576
Iteration 238/1000 | Loss: 0.00001576
Iteration 239/1000 | Loss: 0.00001575
Iteration 240/1000 | Loss: 0.00001564
Iteration 241/1000 | Loss: 0.00001564
Iteration 242/1000 | Loss: 0.00001560
Iteration 243/1000 | Loss: 0.00001559
Iteration 244/1000 | Loss: 0.00001559
Iteration 245/1000 | Loss: 0.00001559
Iteration 246/1000 | Loss: 0.00001559
Iteration 247/1000 | Loss: 0.00001559
Iteration 248/1000 | Loss: 0.00001558
Iteration 249/1000 | Loss: 0.00001558
Iteration 250/1000 | Loss: 0.00001558
Iteration 251/1000 | Loss: 0.00001558
Iteration 252/1000 | Loss: 0.00001558
Iteration 253/1000 | Loss: 0.00001558
Iteration 254/1000 | Loss: 0.00001558
Iteration 255/1000 | Loss: 0.00001558
Iteration 256/1000 | Loss: 0.00001558
Iteration 257/1000 | Loss: 0.00001581
Iteration 258/1000 | Loss: 0.00003229
Iteration 259/1000 | Loss: 0.00001736
Iteration 260/1000 | Loss: 0.00001862
Iteration 261/1000 | Loss: 0.00001569
Iteration 262/1000 | Loss: 0.00001568
Iteration 263/1000 | Loss: 0.00001568
Iteration 264/1000 | Loss: 0.00001568
Iteration 265/1000 | Loss: 0.00001560
Iteration 266/1000 | Loss: 0.00001560
Iteration 267/1000 | Loss: 0.00001560
Iteration 268/1000 | Loss: 0.00001560
Iteration 269/1000 | Loss: 0.00001560
Iteration 270/1000 | Loss: 0.00001560
Iteration 271/1000 | Loss: 0.00001560
Iteration 272/1000 | Loss: 0.00001559
Iteration 273/1000 | Loss: 0.00001559
Iteration 274/1000 | Loss: 0.00001559
Iteration 275/1000 | Loss: 0.00001559
Iteration 276/1000 | Loss: 0.00001559
Iteration 277/1000 | Loss: 0.00001559
Iteration 278/1000 | Loss: 0.00001559
Iteration 279/1000 | Loss: 0.00001559
Iteration 280/1000 | Loss: 0.00001559
Iteration 281/1000 | Loss: 0.00001559
Iteration 282/1000 | Loss: 0.00001559
Iteration 283/1000 | Loss: 0.00001559
Iteration 284/1000 | Loss: 0.00001559
Iteration 285/1000 | Loss: 0.00001559
Iteration 286/1000 | Loss: 0.00001559
Iteration 287/1000 | Loss: 0.00001559
Iteration 288/1000 | Loss: 0.00001559
Iteration 289/1000 | Loss: 0.00001559
Iteration 290/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.558756957820151e-05, 1.558756957820151e-05, 1.558756957820151e-05, 1.558756957820151e-05, 1.558756957820151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.558756957820151e-05

Optimization complete. Final v2v error: 3.364772081375122 mm

Highest mean error: 8.885555267333984 mm for frame 166

Lowest mean error: 2.9782183170318604 mm for frame 106

Saving results

Total time: 146.96049618721008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021718
Iteration 2/25 | Loss: 0.01021718
Iteration 3/25 | Loss: 0.01021718
Iteration 4/25 | Loss: 0.00295273
Iteration 5/25 | Loss: 0.00182994
Iteration 6/25 | Loss: 0.00147504
Iteration 7/25 | Loss: 0.00133817
Iteration 8/25 | Loss: 0.00131095
Iteration 9/25 | Loss: 0.00129374
Iteration 10/25 | Loss: 0.00121334
Iteration 11/25 | Loss: 0.00117871
Iteration 12/25 | Loss: 0.00116688
Iteration 13/25 | Loss: 0.00115885
Iteration 14/25 | Loss: 0.00115309
Iteration 15/25 | Loss: 0.00115133
Iteration 16/25 | Loss: 0.00114771
Iteration 17/25 | Loss: 0.00114883
Iteration 18/25 | Loss: 0.00114703
Iteration 19/25 | Loss: 0.00114781
Iteration 20/25 | Loss: 0.00114696
Iteration 21/25 | Loss: 0.00114608
Iteration 22/25 | Loss: 0.00114608
Iteration 23/25 | Loss: 0.00114608
Iteration 24/25 | Loss: 0.00114608
Iteration 25/25 | Loss: 0.00114608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98009229
Iteration 2/25 | Loss: 0.00107038
Iteration 3/25 | Loss: 0.00107038
Iteration 4/25 | Loss: 0.00107038
Iteration 5/25 | Loss: 0.00107038
Iteration 6/25 | Loss: 0.00107038
Iteration 7/25 | Loss: 0.00107038
Iteration 8/25 | Loss: 0.00107038
Iteration 9/25 | Loss: 0.00107038
Iteration 10/25 | Loss: 0.00107038
Iteration 11/25 | Loss: 0.00107038
Iteration 12/25 | Loss: 0.00107038
Iteration 13/25 | Loss: 0.00107038
Iteration 14/25 | Loss: 0.00107038
Iteration 15/25 | Loss: 0.00107038
Iteration 16/25 | Loss: 0.00107038
Iteration 17/25 | Loss: 0.00107038
Iteration 18/25 | Loss: 0.00107038
Iteration 19/25 | Loss: 0.00107038
Iteration 20/25 | Loss: 0.00107038
Iteration 21/25 | Loss: 0.00107038
Iteration 22/25 | Loss: 0.00107038
Iteration 23/25 | Loss: 0.00107038
Iteration 24/25 | Loss: 0.00107038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001070379395969212, 0.001070379395969212, 0.001070379395969212, 0.001070379395969212, 0.001070379395969212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001070379395969212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107038
Iteration 2/1000 | Loss: 0.00005346
Iteration 3/1000 | Loss: 0.00010944
Iteration 4/1000 | Loss: 0.00014347
Iteration 5/1000 | Loss: 0.00015077
Iteration 6/1000 | Loss: 0.00037126
Iteration 7/1000 | Loss: 0.00027092
Iteration 8/1000 | Loss: 0.00003685
Iteration 9/1000 | Loss: 0.00003285
Iteration 10/1000 | Loss: 0.00016176
Iteration 11/1000 | Loss: 0.00003404
Iteration 12/1000 | Loss: 0.00003143
Iteration 13/1000 | Loss: 0.00003004
Iteration 14/1000 | Loss: 0.00002917
Iteration 15/1000 | Loss: 0.00002821
Iteration 16/1000 | Loss: 0.00002740
Iteration 17/1000 | Loss: 0.00002659
Iteration 18/1000 | Loss: 0.00002586
Iteration 19/1000 | Loss: 0.00010143
Iteration 20/1000 | Loss: 0.00002839
Iteration 21/1000 | Loss: 0.00103651
Iteration 22/1000 | Loss: 0.00019365
Iteration 23/1000 | Loss: 0.00009157
Iteration 24/1000 | Loss: 0.00003938
Iteration 25/1000 | Loss: 0.00002780
Iteration 26/1000 | Loss: 0.00002458
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001705
Iteration 32/1000 | Loss: 0.00001649
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001496
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001492
Iteration 46/1000 | Loss: 0.00001492
Iteration 47/1000 | Loss: 0.00001492
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001485
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001483
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001476
Iteration 103/1000 | Loss: 0.00001476
Iteration 104/1000 | Loss: 0.00001476
Iteration 105/1000 | Loss: 0.00001476
Iteration 106/1000 | Loss: 0.00001476
Iteration 107/1000 | Loss: 0.00001476
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001475
Iteration 110/1000 | Loss: 0.00001475
Iteration 111/1000 | Loss: 0.00001475
Iteration 112/1000 | Loss: 0.00001475
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001475
Iteration 115/1000 | Loss: 0.00001475
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4751136404811405e-05, 1.4751136404811405e-05, 1.4751136404811405e-05, 1.4751136404811405e-05, 1.4751136404811405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4751136404811405e-05

Optimization complete. Final v2v error: 3.27620530128479 mm

Highest mean error: 4.138453483581543 mm for frame 178

Lowest mean error: 2.9212024211883545 mm for frame 128

Saving results

Total time: 104.56293058395386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787822
Iteration 2/25 | Loss: 0.00126718
Iteration 3/25 | Loss: 0.00112371
Iteration 4/25 | Loss: 0.00111708
Iteration 5/25 | Loss: 0.00111645
Iteration 6/25 | Loss: 0.00111645
Iteration 7/25 | Loss: 0.00111645
Iteration 8/25 | Loss: 0.00111645
Iteration 9/25 | Loss: 0.00111645
Iteration 10/25 | Loss: 0.00111645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011164534371346235, 0.0011164534371346235, 0.0011164534371346235, 0.0011164534371346235, 0.0011164534371346235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011164534371346235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41080523
Iteration 2/25 | Loss: 0.00076432
Iteration 3/25 | Loss: 0.00076430
Iteration 4/25 | Loss: 0.00076430
Iteration 5/25 | Loss: 0.00076430
Iteration 6/25 | Loss: 0.00076430
Iteration 7/25 | Loss: 0.00076430
Iteration 8/25 | Loss: 0.00076430
Iteration 9/25 | Loss: 0.00076430
Iteration 10/25 | Loss: 0.00076430
Iteration 11/25 | Loss: 0.00076430
Iteration 12/25 | Loss: 0.00076430
Iteration 13/25 | Loss: 0.00076430
Iteration 14/25 | Loss: 0.00076430
Iteration 15/25 | Loss: 0.00076430
Iteration 16/25 | Loss: 0.00076430
Iteration 17/25 | Loss: 0.00076430
Iteration 18/25 | Loss: 0.00076430
Iteration 19/25 | Loss: 0.00076430
Iteration 20/25 | Loss: 0.00076430
Iteration 21/25 | Loss: 0.00076430
Iteration 22/25 | Loss: 0.00076430
Iteration 23/25 | Loss: 0.00076430
Iteration 24/25 | Loss: 0.00076430
Iteration 25/25 | Loss: 0.00076430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076430
Iteration 2/1000 | Loss: 0.00001998
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001310
Iteration 5/1000 | Loss: 0.00001226
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001104
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001077
Iteration 14/1000 | Loss: 0.00001077
Iteration 15/1000 | Loss: 0.00001076
Iteration 16/1000 | Loss: 0.00001075
Iteration 17/1000 | Loss: 0.00001073
Iteration 18/1000 | Loss: 0.00001070
Iteration 19/1000 | Loss: 0.00001069
Iteration 20/1000 | Loss: 0.00001067
Iteration 21/1000 | Loss: 0.00001067
Iteration 22/1000 | Loss: 0.00001059
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001046
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001042
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00001041
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001036
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001033
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001028
Iteration 69/1000 | Loss: 0.00001028
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001026
Iteration 76/1000 | Loss: 0.00001026
Iteration 77/1000 | Loss: 0.00001025
Iteration 78/1000 | Loss: 0.00001025
Iteration 79/1000 | Loss: 0.00001025
Iteration 80/1000 | Loss: 0.00001025
Iteration 81/1000 | Loss: 0.00001025
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001025
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001023
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001022
Iteration 99/1000 | Loss: 0.00001022
Iteration 100/1000 | Loss: 0.00001022
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001022
Iteration 106/1000 | Loss: 0.00001022
Iteration 107/1000 | Loss: 0.00001022
Iteration 108/1000 | Loss: 0.00001022
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001020
Iteration 112/1000 | Loss: 0.00001020
Iteration 113/1000 | Loss: 0.00001020
Iteration 114/1000 | Loss: 0.00001020
Iteration 115/1000 | Loss: 0.00001020
Iteration 116/1000 | Loss: 0.00001020
Iteration 117/1000 | Loss: 0.00001020
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001019
Iteration 123/1000 | Loss: 0.00001019
Iteration 124/1000 | Loss: 0.00001019
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001018
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001018
Iteration 130/1000 | Loss: 0.00001018
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001017
Iteration 135/1000 | Loss: 0.00001017
Iteration 136/1000 | Loss: 0.00001017
Iteration 137/1000 | Loss: 0.00001017
Iteration 138/1000 | Loss: 0.00001017
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001016
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001016
Iteration 149/1000 | Loss: 0.00001016
Iteration 150/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.016437090584077e-05, 1.016437090584077e-05, 1.016437090584077e-05, 1.016437090584077e-05, 1.016437090584077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.016437090584077e-05

Optimization complete. Final v2v error: 2.6537280082702637 mm

Highest mean error: 3.0892226696014404 mm for frame 82

Lowest mean error: 2.2468857765197754 mm for frame 46

Saving results

Total time: 39.207627058029175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592892
Iteration 2/25 | Loss: 0.00151633
Iteration 3/25 | Loss: 0.00121400
Iteration 4/25 | Loss: 0.00117800
Iteration 5/25 | Loss: 0.00117376
Iteration 6/25 | Loss: 0.00117249
Iteration 7/25 | Loss: 0.00117228
Iteration 8/25 | Loss: 0.00117228
Iteration 9/25 | Loss: 0.00117228
Iteration 10/25 | Loss: 0.00117228
Iteration 11/25 | Loss: 0.00117228
Iteration 12/25 | Loss: 0.00117228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011722772615030408, 0.0011722772615030408, 0.0011722772615030408, 0.0011722772615030408, 0.0011722772615030408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011722772615030408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17746437
Iteration 2/25 | Loss: 0.00066691
Iteration 3/25 | Loss: 0.00066691
Iteration 4/25 | Loss: 0.00066691
Iteration 5/25 | Loss: 0.00066691
Iteration 6/25 | Loss: 0.00066691
Iteration 7/25 | Loss: 0.00066691
Iteration 8/25 | Loss: 0.00066691
Iteration 9/25 | Loss: 0.00066691
Iteration 10/25 | Loss: 0.00066691
Iteration 11/25 | Loss: 0.00066691
Iteration 12/25 | Loss: 0.00066691
Iteration 13/25 | Loss: 0.00066691
Iteration 14/25 | Loss: 0.00066691
Iteration 15/25 | Loss: 0.00066691
Iteration 16/25 | Loss: 0.00066691
Iteration 17/25 | Loss: 0.00066691
Iteration 18/25 | Loss: 0.00066691
Iteration 19/25 | Loss: 0.00066691
Iteration 20/25 | Loss: 0.00066691
Iteration 21/25 | Loss: 0.00066691
Iteration 22/25 | Loss: 0.00066691
Iteration 23/25 | Loss: 0.00066691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006669109570793808, 0.0006669109570793808, 0.0006669109570793808, 0.0006669109570793808, 0.0006669109570793808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006669109570793808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066691
Iteration 2/1000 | Loss: 0.00004903
Iteration 3/1000 | Loss: 0.00002783
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001874
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001777
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001719
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001688
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001687
Iteration 19/1000 | Loss: 0.00001687
Iteration 20/1000 | Loss: 0.00001687
Iteration 21/1000 | Loss: 0.00001687
Iteration 22/1000 | Loss: 0.00001687
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001684
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001684
Iteration 33/1000 | Loss: 0.00001684
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001684
Iteration 37/1000 | Loss: 0.00001684
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001679
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001667
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001667
Iteration 59/1000 | Loss: 0.00001666
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001665
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001665
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001663
Iteration 73/1000 | Loss: 0.00001662
Iteration 74/1000 | Loss: 0.00001662
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001661
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001655
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001652
Iteration 110/1000 | Loss: 0.00001652
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001651
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001648
Iteration 128/1000 | Loss: 0.00001648
Iteration 129/1000 | Loss: 0.00001648
Iteration 130/1000 | Loss: 0.00001648
Iteration 131/1000 | Loss: 0.00001648
Iteration 132/1000 | Loss: 0.00001648
Iteration 133/1000 | Loss: 0.00001648
Iteration 134/1000 | Loss: 0.00001648
Iteration 135/1000 | Loss: 0.00001648
Iteration 136/1000 | Loss: 0.00001648
Iteration 137/1000 | Loss: 0.00001648
Iteration 138/1000 | Loss: 0.00001648
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001647
Iteration 141/1000 | Loss: 0.00001647
Iteration 142/1000 | Loss: 0.00001647
Iteration 143/1000 | Loss: 0.00001647
Iteration 144/1000 | Loss: 0.00001647
Iteration 145/1000 | Loss: 0.00001647
Iteration 146/1000 | Loss: 0.00001647
Iteration 147/1000 | Loss: 0.00001647
Iteration 148/1000 | Loss: 0.00001647
Iteration 149/1000 | Loss: 0.00001647
Iteration 150/1000 | Loss: 0.00001647
Iteration 151/1000 | Loss: 0.00001647
Iteration 152/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.646532109589316e-05, 1.646532109589316e-05, 1.646532109589316e-05, 1.646532109589316e-05, 1.646532109589316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.646532109589316e-05

Optimization complete. Final v2v error: 3.3193399906158447 mm

Highest mean error: 5.100464344024658 mm for frame 58

Lowest mean error: 2.87106990814209 mm for frame 138

Saving results

Total time: 37.14826440811157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831993
Iteration 2/25 | Loss: 0.00116057
Iteration 3/25 | Loss: 0.00107450
Iteration 4/25 | Loss: 0.00106423
Iteration 5/25 | Loss: 0.00106072
Iteration 6/25 | Loss: 0.00105995
Iteration 7/25 | Loss: 0.00105995
Iteration 8/25 | Loss: 0.00105995
Iteration 9/25 | Loss: 0.00105995
Iteration 10/25 | Loss: 0.00105995
Iteration 11/25 | Loss: 0.00105995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010599525412544608, 0.0010599525412544608, 0.0010599525412544608, 0.0010599525412544608, 0.0010599525412544608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010599525412544608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43458188
Iteration 2/25 | Loss: 0.00084238
Iteration 3/25 | Loss: 0.00084238
Iteration 4/25 | Loss: 0.00084238
Iteration 5/25 | Loss: 0.00084238
Iteration 6/25 | Loss: 0.00084238
Iteration 7/25 | Loss: 0.00084238
Iteration 8/25 | Loss: 0.00084238
Iteration 9/25 | Loss: 0.00084238
Iteration 10/25 | Loss: 0.00084238
Iteration 11/25 | Loss: 0.00084238
Iteration 12/25 | Loss: 0.00084238
Iteration 13/25 | Loss: 0.00084238
Iteration 14/25 | Loss: 0.00084238
Iteration 15/25 | Loss: 0.00084238
Iteration 16/25 | Loss: 0.00084238
Iteration 17/25 | Loss: 0.00084238
Iteration 18/25 | Loss: 0.00084238
Iteration 19/25 | Loss: 0.00084238
Iteration 20/25 | Loss: 0.00084238
Iteration 21/25 | Loss: 0.00084238
Iteration 22/25 | Loss: 0.00084238
Iteration 23/25 | Loss: 0.00084238
Iteration 24/25 | Loss: 0.00084238
Iteration 25/25 | Loss: 0.00084238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084238
Iteration 2/1000 | Loss: 0.00001817
Iteration 3/1000 | Loss: 0.00001191
Iteration 4/1000 | Loss: 0.00001063
Iteration 5/1000 | Loss: 0.00001002
Iteration 6/1000 | Loss: 0.00000965
Iteration 7/1000 | Loss: 0.00000928
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000899
Iteration 10/1000 | Loss: 0.00000897
Iteration 11/1000 | Loss: 0.00000881
Iteration 12/1000 | Loss: 0.00000878
Iteration 13/1000 | Loss: 0.00000877
Iteration 14/1000 | Loss: 0.00000875
Iteration 15/1000 | Loss: 0.00000873
Iteration 16/1000 | Loss: 0.00000873
Iteration 17/1000 | Loss: 0.00000865
Iteration 18/1000 | Loss: 0.00000863
Iteration 19/1000 | Loss: 0.00000861
Iteration 20/1000 | Loss: 0.00000861
Iteration 21/1000 | Loss: 0.00000861
Iteration 22/1000 | Loss: 0.00000856
Iteration 23/1000 | Loss: 0.00000856
Iteration 24/1000 | Loss: 0.00000856
Iteration 25/1000 | Loss: 0.00000855
Iteration 26/1000 | Loss: 0.00000855
Iteration 27/1000 | Loss: 0.00000855
Iteration 28/1000 | Loss: 0.00000851
Iteration 29/1000 | Loss: 0.00000851
Iteration 30/1000 | Loss: 0.00000851
Iteration 31/1000 | Loss: 0.00000851
Iteration 32/1000 | Loss: 0.00000851
Iteration 33/1000 | Loss: 0.00000851
Iteration 34/1000 | Loss: 0.00000851
Iteration 35/1000 | Loss: 0.00000851
Iteration 36/1000 | Loss: 0.00000851
Iteration 37/1000 | Loss: 0.00000850
Iteration 38/1000 | Loss: 0.00000849
Iteration 39/1000 | Loss: 0.00000848
Iteration 40/1000 | Loss: 0.00000847
Iteration 41/1000 | Loss: 0.00000847
Iteration 42/1000 | Loss: 0.00000847
Iteration 43/1000 | Loss: 0.00000847
Iteration 44/1000 | Loss: 0.00000846
Iteration 45/1000 | Loss: 0.00000846
Iteration 46/1000 | Loss: 0.00000846
Iteration 47/1000 | Loss: 0.00000845
Iteration 48/1000 | Loss: 0.00000843
Iteration 49/1000 | Loss: 0.00000843
Iteration 50/1000 | Loss: 0.00000843
Iteration 51/1000 | Loss: 0.00000842
Iteration 52/1000 | Loss: 0.00000842
Iteration 53/1000 | Loss: 0.00000842
Iteration 54/1000 | Loss: 0.00000842
Iteration 55/1000 | Loss: 0.00000842
Iteration 56/1000 | Loss: 0.00000842
Iteration 57/1000 | Loss: 0.00000842
Iteration 58/1000 | Loss: 0.00000841
Iteration 59/1000 | Loss: 0.00000841
Iteration 60/1000 | Loss: 0.00000841
Iteration 61/1000 | Loss: 0.00000841
Iteration 62/1000 | Loss: 0.00000841
Iteration 63/1000 | Loss: 0.00000841
Iteration 64/1000 | Loss: 0.00000840
Iteration 65/1000 | Loss: 0.00000839
Iteration 66/1000 | Loss: 0.00000839
Iteration 67/1000 | Loss: 0.00000839
Iteration 68/1000 | Loss: 0.00000839
Iteration 69/1000 | Loss: 0.00000839
Iteration 70/1000 | Loss: 0.00000839
Iteration 71/1000 | Loss: 0.00000839
Iteration 72/1000 | Loss: 0.00000839
Iteration 73/1000 | Loss: 0.00000839
Iteration 74/1000 | Loss: 0.00000838
Iteration 75/1000 | Loss: 0.00000838
Iteration 76/1000 | Loss: 0.00000838
Iteration 77/1000 | Loss: 0.00000838
Iteration 78/1000 | Loss: 0.00000838
Iteration 79/1000 | Loss: 0.00000838
Iteration 80/1000 | Loss: 0.00000838
Iteration 81/1000 | Loss: 0.00000838
Iteration 82/1000 | Loss: 0.00000837
Iteration 83/1000 | Loss: 0.00000837
Iteration 84/1000 | Loss: 0.00000837
Iteration 85/1000 | Loss: 0.00000837
Iteration 86/1000 | Loss: 0.00000836
Iteration 87/1000 | Loss: 0.00000836
Iteration 88/1000 | Loss: 0.00000836
Iteration 89/1000 | Loss: 0.00000836
Iteration 90/1000 | Loss: 0.00000836
Iteration 91/1000 | Loss: 0.00000836
Iteration 92/1000 | Loss: 0.00000836
Iteration 93/1000 | Loss: 0.00000836
Iteration 94/1000 | Loss: 0.00000836
Iteration 95/1000 | Loss: 0.00000836
Iteration 96/1000 | Loss: 0.00000835
Iteration 97/1000 | Loss: 0.00000835
Iteration 98/1000 | Loss: 0.00000835
Iteration 99/1000 | Loss: 0.00000835
Iteration 100/1000 | Loss: 0.00000835
Iteration 101/1000 | Loss: 0.00000835
Iteration 102/1000 | Loss: 0.00000834
Iteration 103/1000 | Loss: 0.00000834
Iteration 104/1000 | Loss: 0.00000834
Iteration 105/1000 | Loss: 0.00000834
Iteration 106/1000 | Loss: 0.00000834
Iteration 107/1000 | Loss: 0.00000834
Iteration 108/1000 | Loss: 0.00000833
Iteration 109/1000 | Loss: 0.00000833
Iteration 110/1000 | Loss: 0.00000833
Iteration 111/1000 | Loss: 0.00000833
Iteration 112/1000 | Loss: 0.00000833
Iteration 113/1000 | Loss: 0.00000833
Iteration 114/1000 | Loss: 0.00000833
Iteration 115/1000 | Loss: 0.00000833
Iteration 116/1000 | Loss: 0.00000832
Iteration 117/1000 | Loss: 0.00000832
Iteration 118/1000 | Loss: 0.00000832
Iteration 119/1000 | Loss: 0.00000831
Iteration 120/1000 | Loss: 0.00000831
Iteration 121/1000 | Loss: 0.00000831
Iteration 122/1000 | Loss: 0.00000830
Iteration 123/1000 | Loss: 0.00000830
Iteration 124/1000 | Loss: 0.00000830
Iteration 125/1000 | Loss: 0.00000829
Iteration 126/1000 | Loss: 0.00000829
Iteration 127/1000 | Loss: 0.00000829
Iteration 128/1000 | Loss: 0.00000829
Iteration 129/1000 | Loss: 0.00000829
Iteration 130/1000 | Loss: 0.00000829
Iteration 131/1000 | Loss: 0.00000829
Iteration 132/1000 | Loss: 0.00000829
Iteration 133/1000 | Loss: 0.00000829
Iteration 134/1000 | Loss: 0.00000829
Iteration 135/1000 | Loss: 0.00000829
Iteration 136/1000 | Loss: 0.00000828
Iteration 137/1000 | Loss: 0.00000828
Iteration 138/1000 | Loss: 0.00000828
Iteration 139/1000 | Loss: 0.00000828
Iteration 140/1000 | Loss: 0.00000828
Iteration 141/1000 | Loss: 0.00000827
Iteration 142/1000 | Loss: 0.00000827
Iteration 143/1000 | Loss: 0.00000826
Iteration 144/1000 | Loss: 0.00000826
Iteration 145/1000 | Loss: 0.00000826
Iteration 146/1000 | Loss: 0.00000826
Iteration 147/1000 | Loss: 0.00000826
Iteration 148/1000 | Loss: 0.00000826
Iteration 149/1000 | Loss: 0.00000826
Iteration 150/1000 | Loss: 0.00000826
Iteration 151/1000 | Loss: 0.00000826
Iteration 152/1000 | Loss: 0.00000826
Iteration 153/1000 | Loss: 0.00000825
Iteration 154/1000 | Loss: 0.00000825
Iteration 155/1000 | Loss: 0.00000825
Iteration 156/1000 | Loss: 0.00000825
Iteration 157/1000 | Loss: 0.00000825
Iteration 158/1000 | Loss: 0.00000825
Iteration 159/1000 | Loss: 0.00000825
Iteration 160/1000 | Loss: 0.00000825
Iteration 161/1000 | Loss: 0.00000825
Iteration 162/1000 | Loss: 0.00000825
Iteration 163/1000 | Loss: 0.00000825
Iteration 164/1000 | Loss: 0.00000825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [8.254372914962005e-06, 8.254372914962005e-06, 8.254372914962005e-06, 8.254372914962005e-06, 8.254372914962005e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.254372914962005e-06

Optimization complete. Final v2v error: 2.4764938354492188 mm

Highest mean error: 2.9239790439605713 mm for frame 82

Lowest mean error: 2.3211419582366943 mm for frame 10

Saving results

Total time: 35.111146688461304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026938
Iteration 2/25 | Loss: 0.00213615
Iteration 3/25 | Loss: 0.00163941
Iteration 4/25 | Loss: 0.00149286
Iteration 5/25 | Loss: 0.00142299
Iteration 6/25 | Loss: 0.00140356
Iteration 7/25 | Loss: 0.00133475
Iteration 8/25 | Loss: 0.00134881
Iteration 9/25 | Loss: 0.00132269
Iteration 10/25 | Loss: 0.00131309
Iteration 11/25 | Loss: 0.00130949
Iteration 12/25 | Loss: 0.00129788
Iteration 13/25 | Loss: 0.00129527
Iteration 14/25 | Loss: 0.00127188
Iteration 15/25 | Loss: 0.00127169
Iteration 16/25 | Loss: 0.00127146
Iteration 17/25 | Loss: 0.00125368
Iteration 18/25 | Loss: 0.00124172
Iteration 19/25 | Loss: 0.00123108
Iteration 20/25 | Loss: 0.00122208
Iteration 21/25 | Loss: 0.00122331
Iteration 22/25 | Loss: 0.00122218
Iteration 23/25 | Loss: 0.00121952
Iteration 24/25 | Loss: 0.00121906
Iteration 25/25 | Loss: 0.00121393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36214280
Iteration 2/25 | Loss: 0.00159854
Iteration 3/25 | Loss: 0.00118045
Iteration 4/25 | Loss: 0.00118045
Iteration 5/25 | Loss: 0.00118045
Iteration 6/25 | Loss: 0.00118045
Iteration 7/25 | Loss: 0.00118045
Iteration 8/25 | Loss: 0.00118045
Iteration 9/25 | Loss: 0.00118045
Iteration 10/25 | Loss: 0.00118045
Iteration 11/25 | Loss: 0.00118045
Iteration 12/25 | Loss: 0.00118044
Iteration 13/25 | Loss: 0.00118044
Iteration 14/25 | Loss: 0.00118044
Iteration 15/25 | Loss: 0.00118044
Iteration 16/25 | Loss: 0.00118044
Iteration 17/25 | Loss: 0.00118044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011804449604824185, 0.0011804449604824185, 0.0011804449604824185, 0.0011804449604824185, 0.0011804449604824185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011804449604824185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118045
Iteration 2/1000 | Loss: 0.00046142
Iteration 3/1000 | Loss: 0.00040589
Iteration 4/1000 | Loss: 0.00008126
Iteration 5/1000 | Loss: 0.00016368
Iteration 6/1000 | Loss: 0.00027576
Iteration 7/1000 | Loss: 0.00018211
Iteration 8/1000 | Loss: 0.00064538
Iteration 9/1000 | Loss: 0.00025106
Iteration 10/1000 | Loss: 0.00018322
Iteration 11/1000 | Loss: 0.00022046
Iteration 12/1000 | Loss: 0.00025633
Iteration 13/1000 | Loss: 0.00021016
Iteration 14/1000 | Loss: 0.00031437
Iteration 15/1000 | Loss: 0.00019603
Iteration 16/1000 | Loss: 0.00047324
Iteration 17/1000 | Loss: 0.00012507
Iteration 18/1000 | Loss: 0.00014854
Iteration 19/1000 | Loss: 0.00005027
Iteration 20/1000 | Loss: 0.00027416
Iteration 21/1000 | Loss: 0.00038400
Iteration 22/1000 | Loss: 0.00022001
Iteration 23/1000 | Loss: 0.00019668
Iteration 24/1000 | Loss: 0.00033928
Iteration 25/1000 | Loss: 0.00021206
Iteration 26/1000 | Loss: 0.00031538
Iteration 27/1000 | Loss: 0.00018715
Iteration 28/1000 | Loss: 0.00008018
Iteration 29/1000 | Loss: 0.00011572
Iteration 30/1000 | Loss: 0.00011971
Iteration 31/1000 | Loss: 0.00010517
Iteration 32/1000 | Loss: 0.00003826
Iteration 33/1000 | Loss: 0.00010166
Iteration 34/1000 | Loss: 0.00006843
Iteration 35/1000 | Loss: 0.00017300
Iteration 36/1000 | Loss: 0.00009427
Iteration 37/1000 | Loss: 0.00080976
Iteration 38/1000 | Loss: 0.00040685
Iteration 39/1000 | Loss: 0.00005239
Iteration 40/1000 | Loss: 0.00003988
Iteration 41/1000 | Loss: 0.00004771
Iteration 42/1000 | Loss: 0.00003484
Iteration 43/1000 | Loss: 0.00003848
Iteration 44/1000 | Loss: 0.00012380
Iteration 45/1000 | Loss: 0.00005145
Iteration 46/1000 | Loss: 0.00025233
Iteration 47/1000 | Loss: 0.00024867
Iteration 48/1000 | Loss: 0.00005639
Iteration 49/1000 | Loss: 0.00004189
Iteration 50/1000 | Loss: 0.00005777
Iteration 51/1000 | Loss: 0.00004116
Iteration 52/1000 | Loss: 0.00003688
Iteration 53/1000 | Loss: 0.00019870
Iteration 54/1000 | Loss: 0.00028605
Iteration 55/1000 | Loss: 0.00005140
Iteration 56/1000 | Loss: 0.00004701
Iteration 57/1000 | Loss: 0.00004360
Iteration 58/1000 | Loss: 0.00003431
Iteration 59/1000 | Loss: 0.00004063
Iteration 60/1000 | Loss: 0.00003455
Iteration 61/1000 | Loss: 0.00017191
Iteration 62/1000 | Loss: 0.00004809
Iteration 63/1000 | Loss: 0.00004726
Iteration 64/1000 | Loss: 0.00027845
Iteration 65/1000 | Loss: 0.00017885
Iteration 66/1000 | Loss: 0.00010989
Iteration 67/1000 | Loss: 0.00030319
Iteration 68/1000 | Loss: 0.00011995
Iteration 69/1000 | Loss: 0.00078196
Iteration 70/1000 | Loss: 0.00012427
Iteration 71/1000 | Loss: 0.00031520
Iteration 72/1000 | Loss: 0.00013689
Iteration 73/1000 | Loss: 0.00027091
Iteration 74/1000 | Loss: 0.00012864
Iteration 75/1000 | Loss: 0.00003856
Iteration 76/1000 | Loss: 0.00026040
Iteration 77/1000 | Loss: 0.00012243
Iteration 78/1000 | Loss: 0.00035551
Iteration 79/1000 | Loss: 0.00011738
Iteration 80/1000 | Loss: 0.00027034
Iteration 81/1000 | Loss: 0.00004727
Iteration 82/1000 | Loss: 0.00007736
Iteration 83/1000 | Loss: 0.00004061
Iteration 84/1000 | Loss: 0.00004864
Iteration 85/1000 | Loss: 0.00003872
Iteration 86/1000 | Loss: 0.00022849
Iteration 87/1000 | Loss: 0.00004291
Iteration 88/1000 | Loss: 0.00003461
Iteration 89/1000 | Loss: 0.00003396
Iteration 90/1000 | Loss: 0.00003792
Iteration 91/1000 | Loss: 0.00003707
Iteration 92/1000 | Loss: 0.00003170
Iteration 93/1000 | Loss: 0.00003769
Iteration 94/1000 | Loss: 0.00003685
Iteration 95/1000 | Loss: 0.00003794
Iteration 96/1000 | Loss: 0.00003705
Iteration 97/1000 | Loss: 0.00003796
Iteration 98/1000 | Loss: 0.00003658
Iteration 99/1000 | Loss: 0.00003868
Iteration 100/1000 | Loss: 0.00003708
Iteration 101/1000 | Loss: 0.00003983
Iteration 102/1000 | Loss: 0.00003996
Iteration 103/1000 | Loss: 0.00004044
Iteration 104/1000 | Loss: 0.00012295
Iteration 105/1000 | Loss: 0.00005068
Iteration 106/1000 | Loss: 0.00003693
Iteration 107/1000 | Loss: 0.00005112
Iteration 108/1000 | Loss: 0.00005204
Iteration 109/1000 | Loss: 0.00003652
Iteration 110/1000 | Loss: 0.00004223
Iteration 111/1000 | Loss: 0.00003802
Iteration 112/1000 | Loss: 0.00003638
Iteration 113/1000 | Loss: 0.00003772
Iteration 114/1000 | Loss: 0.00005604
Iteration 115/1000 | Loss: 0.00003929
Iteration 116/1000 | Loss: 0.00003629
Iteration 117/1000 | Loss: 0.00003757
Iteration 118/1000 | Loss: 0.00003757
Iteration 119/1000 | Loss: 0.00003777
Iteration 120/1000 | Loss: 0.00003724
Iteration 121/1000 | Loss: 0.00013113
Iteration 122/1000 | Loss: 0.00003873
Iteration 123/1000 | Loss: 0.00005163
Iteration 124/1000 | Loss: 0.00003720
Iteration 125/1000 | Loss: 0.00004514
Iteration 126/1000 | Loss: 0.00003746
Iteration 127/1000 | Loss: 0.00004079
Iteration 128/1000 | Loss: 0.00003805
Iteration 129/1000 | Loss: 0.00003793
Iteration 130/1000 | Loss: 0.00032557
Iteration 131/1000 | Loss: 0.00007140
Iteration 132/1000 | Loss: 0.00003747
Iteration 133/1000 | Loss: 0.00003717
Iteration 134/1000 | Loss: 0.00004802
Iteration 135/1000 | Loss: 0.00004587
Iteration 136/1000 | Loss: 0.00004888
Iteration 137/1000 | Loss: 0.00004592
Iteration 138/1000 | Loss: 0.00004165
Iteration 139/1000 | Loss: 0.00005497
Iteration 140/1000 | Loss: 0.00003218
Iteration 141/1000 | Loss: 0.00002880
Iteration 142/1000 | Loss: 0.00002771
Iteration 143/1000 | Loss: 0.00002711
Iteration 144/1000 | Loss: 0.00003659
Iteration 145/1000 | Loss: 0.00002705
Iteration 146/1000 | Loss: 0.00002633
Iteration 147/1000 | Loss: 0.00026830
Iteration 148/1000 | Loss: 0.00003287
Iteration 149/1000 | Loss: 0.00002969
Iteration 150/1000 | Loss: 0.00025442
Iteration 151/1000 | Loss: 0.00002721
Iteration 152/1000 | Loss: 0.00002590
Iteration 153/1000 | Loss: 0.00005283
Iteration 154/1000 | Loss: 0.00002469
Iteration 155/1000 | Loss: 0.00007461
Iteration 156/1000 | Loss: 0.00002446
Iteration 157/1000 | Loss: 0.00002433
Iteration 158/1000 | Loss: 0.00002432
Iteration 159/1000 | Loss: 0.00002432
Iteration 160/1000 | Loss: 0.00002429
Iteration 161/1000 | Loss: 0.00002426
Iteration 162/1000 | Loss: 0.00002426
Iteration 163/1000 | Loss: 0.00002425
Iteration 164/1000 | Loss: 0.00002425
Iteration 165/1000 | Loss: 0.00002422
Iteration 166/1000 | Loss: 0.00002422
Iteration 167/1000 | Loss: 0.00002421
Iteration 168/1000 | Loss: 0.00002420
Iteration 169/1000 | Loss: 0.00002420
Iteration 170/1000 | Loss: 0.00002420
Iteration 171/1000 | Loss: 0.00002420
Iteration 172/1000 | Loss: 0.00002419
Iteration 173/1000 | Loss: 0.00002419
Iteration 174/1000 | Loss: 0.00002419
Iteration 175/1000 | Loss: 0.00002418
Iteration 176/1000 | Loss: 0.00002417
Iteration 177/1000 | Loss: 0.00002415
Iteration 178/1000 | Loss: 0.00002414
Iteration 179/1000 | Loss: 0.00002407
Iteration 180/1000 | Loss: 0.00002405
Iteration 181/1000 | Loss: 0.00002404
Iteration 182/1000 | Loss: 0.00002403
Iteration 183/1000 | Loss: 0.00002402
Iteration 184/1000 | Loss: 0.00019015
Iteration 185/1000 | Loss: 0.00005818
Iteration 186/1000 | Loss: 0.00002622
Iteration 187/1000 | Loss: 0.00002460
Iteration 188/1000 | Loss: 0.00002406
Iteration 189/1000 | Loss: 0.00002394
Iteration 190/1000 | Loss: 0.00002393
Iteration 191/1000 | Loss: 0.00002392
Iteration 192/1000 | Loss: 0.00002392
Iteration 193/1000 | Loss: 0.00002392
Iteration 194/1000 | Loss: 0.00002392
Iteration 195/1000 | Loss: 0.00002392
Iteration 196/1000 | Loss: 0.00002391
Iteration 197/1000 | Loss: 0.00002391
Iteration 198/1000 | Loss: 0.00002391
Iteration 199/1000 | Loss: 0.00002391
Iteration 200/1000 | Loss: 0.00002391
Iteration 201/1000 | Loss: 0.00002391
Iteration 202/1000 | Loss: 0.00002391
Iteration 203/1000 | Loss: 0.00002391
Iteration 204/1000 | Loss: 0.00002391
Iteration 205/1000 | Loss: 0.00002391
Iteration 206/1000 | Loss: 0.00002391
Iteration 207/1000 | Loss: 0.00002391
Iteration 208/1000 | Loss: 0.00002390
Iteration 209/1000 | Loss: 0.00002390
Iteration 210/1000 | Loss: 0.00002390
Iteration 211/1000 | Loss: 0.00002389
Iteration 212/1000 | Loss: 0.00002389
Iteration 213/1000 | Loss: 0.00002389
Iteration 214/1000 | Loss: 0.00002389
Iteration 215/1000 | Loss: 0.00013409
Iteration 216/1000 | Loss: 0.00002400
Iteration 217/1000 | Loss: 0.00002389
Iteration 218/1000 | Loss: 0.00002389
Iteration 219/1000 | Loss: 0.00002389
Iteration 220/1000 | Loss: 0.00002388
Iteration 221/1000 | Loss: 0.00002388
Iteration 222/1000 | Loss: 0.00002388
Iteration 223/1000 | Loss: 0.00002388
Iteration 224/1000 | Loss: 0.00002388
Iteration 225/1000 | Loss: 0.00002388
Iteration 226/1000 | Loss: 0.00002388
Iteration 227/1000 | Loss: 0.00002388
Iteration 228/1000 | Loss: 0.00002388
Iteration 229/1000 | Loss: 0.00002388
Iteration 230/1000 | Loss: 0.00007198
Iteration 231/1000 | Loss: 0.00002398
Iteration 232/1000 | Loss: 0.00004718
Iteration 233/1000 | Loss: 0.00002765
Iteration 234/1000 | Loss: 0.00002780
Iteration 235/1000 | Loss: 0.00004420
Iteration 236/1000 | Loss: 0.00002713
Iteration 237/1000 | Loss: 0.00002393
Iteration 238/1000 | Loss: 0.00002392
Iteration 239/1000 | Loss: 0.00002390
Iteration 240/1000 | Loss: 0.00002390
Iteration 241/1000 | Loss: 0.00002390
Iteration 242/1000 | Loss: 0.00002390
Iteration 243/1000 | Loss: 0.00002526
Iteration 244/1000 | Loss: 0.00002478
Iteration 245/1000 | Loss: 0.00002389
Iteration 246/1000 | Loss: 0.00002389
Iteration 247/1000 | Loss: 0.00002389
Iteration 248/1000 | Loss: 0.00002389
Iteration 249/1000 | Loss: 0.00002389
Iteration 250/1000 | Loss: 0.00002389
Iteration 251/1000 | Loss: 0.00002518
Iteration 252/1000 | Loss: 0.00002507
Iteration 253/1000 | Loss: 0.00003147
Iteration 254/1000 | Loss: 0.00002390
Iteration 255/1000 | Loss: 0.00002390
Iteration 256/1000 | Loss: 0.00002389
Iteration 257/1000 | Loss: 0.00002389
Iteration 258/1000 | Loss: 0.00002389
Iteration 259/1000 | Loss: 0.00002389
Iteration 260/1000 | Loss: 0.00002389
Iteration 261/1000 | Loss: 0.00002389
Iteration 262/1000 | Loss: 0.00002389
Iteration 263/1000 | Loss: 0.00002389
Iteration 264/1000 | Loss: 0.00002389
Iteration 265/1000 | Loss: 0.00002389
Iteration 266/1000 | Loss: 0.00002389
Iteration 267/1000 | Loss: 0.00002389
Iteration 268/1000 | Loss: 0.00002389
Iteration 269/1000 | Loss: 0.00002389
Iteration 270/1000 | Loss: 0.00002389
Iteration 271/1000 | Loss: 0.00002389
Iteration 272/1000 | Loss: 0.00002389
Iteration 273/1000 | Loss: 0.00002388
Iteration 274/1000 | Loss: 0.00002388
Iteration 275/1000 | Loss: 0.00002388
Iteration 276/1000 | Loss: 0.00002388
Iteration 277/1000 | Loss: 0.00002388
Iteration 278/1000 | Loss: 0.00002388
Iteration 279/1000 | Loss: 0.00002388
Iteration 280/1000 | Loss: 0.00002388
Iteration 281/1000 | Loss: 0.00002388
Iteration 282/1000 | Loss: 0.00002388
Iteration 283/1000 | Loss: 0.00002388
Iteration 284/1000 | Loss: 0.00002388
Iteration 285/1000 | Loss: 0.00002388
Iteration 286/1000 | Loss: 0.00002388
Iteration 287/1000 | Loss: 0.00002388
Iteration 288/1000 | Loss: 0.00002388
Iteration 289/1000 | Loss: 0.00002388
Iteration 290/1000 | Loss: 0.00002388
Iteration 291/1000 | Loss: 0.00002388
Iteration 292/1000 | Loss: 0.00002388
Iteration 293/1000 | Loss: 0.00002388
Iteration 294/1000 | Loss: 0.00002388
Iteration 295/1000 | Loss: 0.00002388
Iteration 296/1000 | Loss: 0.00002388
Iteration 297/1000 | Loss: 0.00002388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [2.3882457753643394e-05, 2.3882457753643394e-05, 2.3882457753643394e-05, 2.3882457753643394e-05, 2.3882457753643394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3882457753643394e-05

Optimization complete. Final v2v error: 4.019145488739014 mm

Highest mean error: 11.514150619506836 mm for frame 18

Lowest mean error: 3.4011991024017334 mm for frame 189

Saving results

Total time: 308.73700618743896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470886
Iteration 2/25 | Loss: 0.00128015
Iteration 3/25 | Loss: 0.00116538
Iteration 4/25 | Loss: 0.00114567
Iteration 5/25 | Loss: 0.00114096
Iteration 6/25 | Loss: 0.00114064
Iteration 7/25 | Loss: 0.00114064
Iteration 8/25 | Loss: 0.00114064
Iteration 9/25 | Loss: 0.00114064
Iteration 10/25 | Loss: 0.00114064
Iteration 11/25 | Loss: 0.00114064
Iteration 12/25 | Loss: 0.00114064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011406400008127093, 0.0011406400008127093, 0.0011406400008127093, 0.0011406400008127093, 0.0011406400008127093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011406400008127093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33633459
Iteration 2/25 | Loss: 0.00083134
Iteration 3/25 | Loss: 0.00083134
Iteration 4/25 | Loss: 0.00083133
Iteration 5/25 | Loss: 0.00083133
Iteration 6/25 | Loss: 0.00083133
Iteration 7/25 | Loss: 0.00083133
Iteration 8/25 | Loss: 0.00083133
Iteration 9/25 | Loss: 0.00083133
Iteration 10/25 | Loss: 0.00083133
Iteration 11/25 | Loss: 0.00083133
Iteration 12/25 | Loss: 0.00083133
Iteration 13/25 | Loss: 0.00083133
Iteration 14/25 | Loss: 0.00083133
Iteration 15/25 | Loss: 0.00083133
Iteration 16/25 | Loss: 0.00083133
Iteration 17/25 | Loss: 0.00083133
Iteration 18/25 | Loss: 0.00083133
Iteration 19/25 | Loss: 0.00083133
Iteration 20/25 | Loss: 0.00083133
Iteration 21/25 | Loss: 0.00083133
Iteration 22/25 | Loss: 0.00083133
Iteration 23/25 | Loss: 0.00083133
Iteration 24/25 | Loss: 0.00083133
Iteration 25/25 | Loss: 0.00083133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083133
Iteration 2/1000 | Loss: 0.00002149
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001580
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001444
Iteration 9/1000 | Loss: 0.00001428
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001406
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001391
Iteration 22/1000 | Loss: 0.00001389
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001383
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001379
Iteration 29/1000 | Loss: 0.00001379
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001378
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001376
Iteration 41/1000 | Loss: 0.00001376
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001375
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001374
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001372
Iteration 54/1000 | Loss: 0.00001372
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001364
Iteration 86/1000 | Loss: 0.00001363
Iteration 87/1000 | Loss: 0.00001363
Iteration 88/1000 | Loss: 0.00001363
Iteration 89/1000 | Loss: 0.00001363
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001358
Iteration 104/1000 | Loss: 0.00001358
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.3576215678767767e-05, 1.3576215678767767e-05, 1.3576215678767767e-05, 1.3576215678767767e-05, 1.3576215678767767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3576215678767767e-05

Optimization complete. Final v2v error: 3.0976855754852295 mm

Highest mean error: 3.489288568496704 mm for frame 147

Lowest mean error: 2.8523919582366943 mm for frame 41

Saving results

Total time: 36.58621263504028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00285383
Iteration 2/25 | Loss: 0.00127264
Iteration 3/25 | Loss: 0.00107345
Iteration 4/25 | Loss: 0.00105033
Iteration 5/25 | Loss: 0.00104445
Iteration 6/25 | Loss: 0.00104240
Iteration 7/25 | Loss: 0.00104213
Iteration 8/25 | Loss: 0.00104213
Iteration 9/25 | Loss: 0.00104213
Iteration 10/25 | Loss: 0.00104213
Iteration 11/25 | Loss: 0.00104213
Iteration 12/25 | Loss: 0.00104213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010421312181279063, 0.0010421312181279063, 0.0010421312181279063, 0.0010421312181279063, 0.0010421312181279063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010421312181279063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31924689
Iteration 2/25 | Loss: 0.00089269
Iteration 3/25 | Loss: 0.00089269
Iteration 4/25 | Loss: 0.00089269
Iteration 5/25 | Loss: 0.00089269
Iteration 6/25 | Loss: 0.00089269
Iteration 7/25 | Loss: 0.00089269
Iteration 8/25 | Loss: 0.00089269
Iteration 9/25 | Loss: 0.00089269
Iteration 10/25 | Loss: 0.00089269
Iteration 11/25 | Loss: 0.00089269
Iteration 12/25 | Loss: 0.00089269
Iteration 13/25 | Loss: 0.00089269
Iteration 14/25 | Loss: 0.00089269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00089268694864586, 0.00089268694864586, 0.00089268694864586, 0.00089268694864586, 0.00089268694864586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00089268694864586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089269
Iteration 2/1000 | Loss: 0.00003882
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001424
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001316
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001231
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001157
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001156
Iteration 42/1000 | Loss: 0.00001156
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001155
Iteration 45/1000 | Loss: 0.00001155
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001148
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001147
Iteration 67/1000 | Loss: 0.00001147
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001144
Iteration 75/1000 | Loss: 0.00001144
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001143
Iteration 78/1000 | Loss: 0.00001143
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001141
Iteration 86/1000 | Loss: 0.00001141
Iteration 87/1000 | Loss: 0.00001141
Iteration 88/1000 | Loss: 0.00001141
Iteration 89/1000 | Loss: 0.00001141
Iteration 90/1000 | Loss: 0.00001141
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001139
Iteration 98/1000 | Loss: 0.00001139
Iteration 99/1000 | Loss: 0.00001139
Iteration 100/1000 | Loss: 0.00001138
Iteration 101/1000 | Loss: 0.00001138
Iteration 102/1000 | Loss: 0.00001138
Iteration 103/1000 | Loss: 0.00001138
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001136
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001135
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001135
Iteration 138/1000 | Loss: 0.00001135
Iteration 139/1000 | Loss: 0.00001134
Iteration 140/1000 | Loss: 0.00001134
Iteration 141/1000 | Loss: 0.00001134
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001133
Iteration 146/1000 | Loss: 0.00001133
Iteration 147/1000 | Loss: 0.00001133
Iteration 148/1000 | Loss: 0.00001133
Iteration 149/1000 | Loss: 0.00001133
Iteration 150/1000 | Loss: 0.00001133
Iteration 151/1000 | Loss: 0.00001133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1334739610902034e-05, 1.1334739610902034e-05, 1.1334739610902034e-05, 1.1334739610902034e-05, 1.1334739610902034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1334739610902034e-05

Optimization complete. Final v2v error: 2.8764023780822754 mm

Highest mean error: 3.062856912612915 mm for frame 135

Lowest mean error: 2.6542930603027344 mm for frame 42

Saving results

Total time: 42.09666109085083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825000
Iteration 2/25 | Loss: 0.00116291
Iteration 3/25 | Loss: 0.00107737
Iteration 4/25 | Loss: 0.00107199
Iteration 5/25 | Loss: 0.00107071
Iteration 6/25 | Loss: 0.00107071
Iteration 7/25 | Loss: 0.00107071
Iteration 8/25 | Loss: 0.00107071
Iteration 9/25 | Loss: 0.00107071
Iteration 10/25 | Loss: 0.00107071
Iteration 11/25 | Loss: 0.00107071
Iteration 12/25 | Loss: 0.00107071
Iteration 13/25 | Loss: 0.00107071
Iteration 14/25 | Loss: 0.00107071
Iteration 15/25 | Loss: 0.00107071
Iteration 16/25 | Loss: 0.00107071
Iteration 17/25 | Loss: 0.00107071
Iteration 18/25 | Loss: 0.00107071
Iteration 19/25 | Loss: 0.00107071
Iteration 20/25 | Loss: 0.00107071
Iteration 21/25 | Loss: 0.00107071
Iteration 22/25 | Loss: 0.00107071
Iteration 23/25 | Loss: 0.00107071
Iteration 24/25 | Loss: 0.00107071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010707145556807518, 0.0010707145556807518, 0.0010707145556807518, 0.0010707145556807518, 0.0010707145556807518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010707145556807518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34483564
Iteration 2/25 | Loss: 0.00072638
Iteration 3/25 | Loss: 0.00072637
Iteration 4/25 | Loss: 0.00072637
Iteration 5/25 | Loss: 0.00072637
Iteration 6/25 | Loss: 0.00072637
Iteration 7/25 | Loss: 0.00072637
Iteration 8/25 | Loss: 0.00072637
Iteration 9/25 | Loss: 0.00072637
Iteration 10/25 | Loss: 0.00072637
Iteration 11/25 | Loss: 0.00072637
Iteration 12/25 | Loss: 0.00072637
Iteration 13/25 | Loss: 0.00072637
Iteration 14/25 | Loss: 0.00072637
Iteration 15/25 | Loss: 0.00072637
Iteration 16/25 | Loss: 0.00072637
Iteration 17/25 | Loss: 0.00072637
Iteration 18/25 | Loss: 0.00072637
Iteration 19/25 | Loss: 0.00072637
Iteration 20/25 | Loss: 0.00072637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007263711304403841, 0.0007263711304403841, 0.0007263711304403841, 0.0007263711304403841, 0.0007263711304403841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007263711304403841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072637
Iteration 2/1000 | Loss: 0.00001970
Iteration 3/1000 | Loss: 0.00001367
Iteration 4/1000 | Loss: 0.00001249
Iteration 5/1000 | Loss: 0.00001170
Iteration 6/1000 | Loss: 0.00001114
Iteration 7/1000 | Loss: 0.00001082
Iteration 8/1000 | Loss: 0.00001054
Iteration 9/1000 | Loss: 0.00001051
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001022
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00001005
Iteration 14/1000 | Loss: 0.00001002
Iteration 15/1000 | Loss: 0.00001001
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000975
Iteration 22/1000 | Loss: 0.00000974
Iteration 23/1000 | Loss: 0.00000973
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000972
Iteration 26/1000 | Loss: 0.00000971
Iteration 27/1000 | Loss: 0.00000969
Iteration 28/1000 | Loss: 0.00000968
Iteration 29/1000 | Loss: 0.00000968
Iteration 30/1000 | Loss: 0.00000967
Iteration 31/1000 | Loss: 0.00000967
Iteration 32/1000 | Loss: 0.00000966
Iteration 33/1000 | Loss: 0.00000964
Iteration 34/1000 | Loss: 0.00000964
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000962
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000961
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000952
Iteration 48/1000 | Loss: 0.00000952
Iteration 49/1000 | Loss: 0.00000952
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00000952
Iteration 52/1000 | Loss: 0.00000952
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000946
Iteration 55/1000 | Loss: 0.00000946
Iteration 56/1000 | Loss: 0.00000945
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000943
Iteration 59/1000 | Loss: 0.00000942
Iteration 60/1000 | Loss: 0.00000942
Iteration 61/1000 | Loss: 0.00000942
Iteration 62/1000 | Loss: 0.00000942
Iteration 63/1000 | Loss: 0.00000941
Iteration 64/1000 | Loss: 0.00000941
Iteration 65/1000 | Loss: 0.00000941
Iteration 66/1000 | Loss: 0.00000941
Iteration 67/1000 | Loss: 0.00000941
Iteration 68/1000 | Loss: 0.00000941
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000941
Iteration 72/1000 | Loss: 0.00000940
Iteration 73/1000 | Loss: 0.00000940
Iteration 74/1000 | Loss: 0.00000940
Iteration 75/1000 | Loss: 0.00000940
Iteration 76/1000 | Loss: 0.00000940
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000939
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000938
Iteration 81/1000 | Loss: 0.00000938
Iteration 82/1000 | Loss: 0.00000937
Iteration 83/1000 | Loss: 0.00000937
Iteration 84/1000 | Loss: 0.00000937
Iteration 85/1000 | Loss: 0.00000937
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000936
Iteration 88/1000 | Loss: 0.00000936
Iteration 89/1000 | Loss: 0.00000935
Iteration 90/1000 | Loss: 0.00000934
Iteration 91/1000 | Loss: 0.00000933
Iteration 92/1000 | Loss: 0.00000933
Iteration 93/1000 | Loss: 0.00000932
Iteration 94/1000 | Loss: 0.00000932
Iteration 95/1000 | Loss: 0.00000931
Iteration 96/1000 | Loss: 0.00000931
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000930
Iteration 99/1000 | Loss: 0.00000930
Iteration 100/1000 | Loss: 0.00000930
Iteration 101/1000 | Loss: 0.00000930
Iteration 102/1000 | Loss: 0.00000929
Iteration 103/1000 | Loss: 0.00000929
Iteration 104/1000 | Loss: 0.00000929
Iteration 105/1000 | Loss: 0.00000929
Iteration 106/1000 | Loss: 0.00000929
Iteration 107/1000 | Loss: 0.00000929
Iteration 108/1000 | Loss: 0.00000929
Iteration 109/1000 | Loss: 0.00000929
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000929
Iteration 112/1000 | Loss: 0.00000929
Iteration 113/1000 | Loss: 0.00000929
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000929
Iteration 116/1000 | Loss: 0.00000929
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [9.289921763411257e-06, 9.289921763411257e-06, 9.289921763411257e-06, 9.289921763411257e-06, 9.289921763411257e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.289921763411257e-06

Optimization complete. Final v2v error: 2.610834836959839 mm

Highest mean error: 2.7070114612579346 mm for frame 61

Lowest mean error: 2.5283899307250977 mm for frame 121

Saving results

Total time: 34.8861141204834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026486
Iteration 2/25 | Loss: 0.00254186
Iteration 3/25 | Loss: 0.00183851
Iteration 4/25 | Loss: 0.00170114
Iteration 5/25 | Loss: 0.00156606
Iteration 6/25 | Loss: 0.00151505
Iteration 7/25 | Loss: 0.00145729
Iteration 8/25 | Loss: 0.00143498
Iteration 9/25 | Loss: 0.00144218
Iteration 10/25 | Loss: 0.00143345
Iteration 11/25 | Loss: 0.00142040
Iteration 12/25 | Loss: 0.00139617
Iteration 13/25 | Loss: 0.00138792
Iteration 14/25 | Loss: 0.00138932
Iteration 15/25 | Loss: 0.00138288
Iteration 16/25 | Loss: 0.00138218
Iteration 17/25 | Loss: 0.00138189
Iteration 18/25 | Loss: 0.00138822
Iteration 19/25 | Loss: 0.00138134
Iteration 20/25 | Loss: 0.00137979
Iteration 21/25 | Loss: 0.00137922
Iteration 22/25 | Loss: 0.00137910
Iteration 23/25 | Loss: 0.00137899
Iteration 24/25 | Loss: 0.00137879
Iteration 25/25 | Loss: 0.00137860

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82503366
Iteration 2/25 | Loss: 0.00244555
Iteration 3/25 | Loss: 0.00244555
Iteration 4/25 | Loss: 0.00244555
Iteration 5/25 | Loss: 0.00244555
Iteration 6/25 | Loss: 0.00244554
Iteration 7/25 | Loss: 0.00244554
Iteration 8/25 | Loss: 0.00244554
Iteration 9/25 | Loss: 0.00244554
Iteration 10/25 | Loss: 0.00244554
Iteration 11/25 | Loss: 0.00244554
Iteration 12/25 | Loss: 0.00244554
Iteration 13/25 | Loss: 0.00244554
Iteration 14/25 | Loss: 0.00244554
Iteration 15/25 | Loss: 0.00244554
Iteration 16/25 | Loss: 0.00244554
Iteration 17/25 | Loss: 0.00244554
Iteration 18/25 | Loss: 0.00244554
Iteration 19/25 | Loss: 0.00244554
Iteration 20/25 | Loss: 0.00244554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002445543184876442, 0.002445543184876442, 0.002445543184876442, 0.002445543184876442, 0.002445543184876442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002445543184876442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244554
Iteration 2/1000 | Loss: 0.00051924
Iteration 3/1000 | Loss: 0.00034437
Iteration 4/1000 | Loss: 0.00055292
Iteration 5/1000 | Loss: 0.00082657
Iteration 6/1000 | Loss: 0.00038584
Iteration 7/1000 | Loss: 0.00036475
Iteration 8/1000 | Loss: 0.00028223
Iteration 9/1000 | Loss: 0.00014509
Iteration 10/1000 | Loss: 0.00012990
Iteration 11/1000 | Loss: 0.00011671
Iteration 12/1000 | Loss: 0.00010970
Iteration 13/1000 | Loss: 0.00010484
Iteration 14/1000 | Loss: 0.00035623
Iteration 15/1000 | Loss: 0.00041155
Iteration 16/1000 | Loss: 0.00012767
Iteration 17/1000 | Loss: 0.00010606
Iteration 18/1000 | Loss: 0.00009721
Iteration 19/1000 | Loss: 0.00009429
Iteration 20/1000 | Loss: 0.00009150
Iteration 21/1000 | Loss: 0.00009000
Iteration 22/1000 | Loss: 0.00008855
Iteration 23/1000 | Loss: 0.00008706
Iteration 24/1000 | Loss: 0.00008605
Iteration 25/1000 | Loss: 0.00008509
Iteration 26/1000 | Loss: 0.00008409
Iteration 27/1000 | Loss: 0.00038505
Iteration 28/1000 | Loss: 0.00009378
Iteration 29/1000 | Loss: 0.00008639
Iteration 30/1000 | Loss: 0.00008355
Iteration 31/1000 | Loss: 0.00044359
Iteration 32/1000 | Loss: 0.00017086
Iteration 33/1000 | Loss: 0.00043383
Iteration 34/1000 | Loss: 0.00020368
Iteration 35/1000 | Loss: 0.00008165
Iteration 36/1000 | Loss: 0.00007977
Iteration 37/1000 | Loss: 0.00045254
Iteration 38/1000 | Loss: 0.00061072
Iteration 39/1000 | Loss: 0.00015732
Iteration 40/1000 | Loss: 0.00011459
Iteration 41/1000 | Loss: 0.00010268
Iteration 42/1000 | Loss: 0.00011423
Iteration 43/1000 | Loss: 0.00009998
Iteration 44/1000 | Loss: 0.00010015
Iteration 45/1000 | Loss: 0.00008706
Iteration 46/1000 | Loss: 0.00008810
Iteration 47/1000 | Loss: 0.00007047
Iteration 48/1000 | Loss: 0.00006894
Iteration 49/1000 | Loss: 0.00006701
Iteration 50/1000 | Loss: 0.00006635
Iteration 51/1000 | Loss: 0.00006569
Iteration 52/1000 | Loss: 0.00006519
Iteration 53/1000 | Loss: 0.00035083
Iteration 54/1000 | Loss: 0.00006629
Iteration 55/1000 | Loss: 0.00006407
Iteration 56/1000 | Loss: 0.00006311
Iteration 57/1000 | Loss: 0.00006193
Iteration 58/1000 | Loss: 0.00006127
Iteration 59/1000 | Loss: 0.00006067
Iteration 60/1000 | Loss: 0.00006045
Iteration 61/1000 | Loss: 0.00006026
Iteration 62/1000 | Loss: 0.00006021
Iteration 63/1000 | Loss: 0.00006007
Iteration 64/1000 | Loss: 0.00006002
Iteration 65/1000 | Loss: 0.00006002
Iteration 66/1000 | Loss: 0.00006001
Iteration 67/1000 | Loss: 0.00005998
Iteration 68/1000 | Loss: 0.00005997
Iteration 69/1000 | Loss: 0.00005996
Iteration 70/1000 | Loss: 0.00005996
Iteration 71/1000 | Loss: 0.00005995
Iteration 72/1000 | Loss: 0.00005994
Iteration 73/1000 | Loss: 0.00005989
Iteration 74/1000 | Loss: 0.00005989
Iteration 75/1000 | Loss: 0.00005987
Iteration 76/1000 | Loss: 0.00005986
Iteration 77/1000 | Loss: 0.00005986
Iteration 78/1000 | Loss: 0.00005986
Iteration 79/1000 | Loss: 0.00005986
Iteration 80/1000 | Loss: 0.00005985
Iteration 81/1000 | Loss: 0.00005985
Iteration 82/1000 | Loss: 0.00005984
Iteration 83/1000 | Loss: 0.00005984
Iteration 84/1000 | Loss: 0.00005984
Iteration 85/1000 | Loss: 0.00005984
Iteration 86/1000 | Loss: 0.00005984
Iteration 87/1000 | Loss: 0.00005983
Iteration 88/1000 | Loss: 0.00005983
Iteration 89/1000 | Loss: 0.00005983
Iteration 90/1000 | Loss: 0.00005983
Iteration 91/1000 | Loss: 0.00005982
Iteration 92/1000 | Loss: 0.00005982
Iteration 93/1000 | Loss: 0.00005982
Iteration 94/1000 | Loss: 0.00005982
Iteration 95/1000 | Loss: 0.00005981
Iteration 96/1000 | Loss: 0.00005981
Iteration 97/1000 | Loss: 0.00005981
Iteration 98/1000 | Loss: 0.00005981
Iteration 99/1000 | Loss: 0.00005981
Iteration 100/1000 | Loss: 0.00005981
Iteration 101/1000 | Loss: 0.00005981
Iteration 102/1000 | Loss: 0.00005981
Iteration 103/1000 | Loss: 0.00005981
Iteration 104/1000 | Loss: 0.00005981
Iteration 105/1000 | Loss: 0.00005980
Iteration 106/1000 | Loss: 0.00005980
Iteration 107/1000 | Loss: 0.00005980
Iteration 108/1000 | Loss: 0.00005980
Iteration 109/1000 | Loss: 0.00005980
Iteration 110/1000 | Loss: 0.00005980
Iteration 111/1000 | Loss: 0.00005980
Iteration 112/1000 | Loss: 0.00005980
Iteration 113/1000 | Loss: 0.00005980
Iteration 114/1000 | Loss: 0.00005980
Iteration 115/1000 | Loss: 0.00005980
Iteration 116/1000 | Loss: 0.00005980
Iteration 117/1000 | Loss: 0.00005980
Iteration 118/1000 | Loss: 0.00005980
Iteration 119/1000 | Loss: 0.00005979
Iteration 120/1000 | Loss: 0.00005979
Iteration 121/1000 | Loss: 0.00005979
Iteration 122/1000 | Loss: 0.00005979
Iteration 123/1000 | Loss: 0.00005979
Iteration 124/1000 | Loss: 0.00005979
Iteration 125/1000 | Loss: 0.00005979
Iteration 126/1000 | Loss: 0.00005979
Iteration 127/1000 | Loss: 0.00005979
Iteration 128/1000 | Loss: 0.00005979
Iteration 129/1000 | Loss: 0.00005979
Iteration 130/1000 | Loss: 0.00005979
Iteration 131/1000 | Loss: 0.00005979
Iteration 132/1000 | Loss: 0.00005979
Iteration 133/1000 | Loss: 0.00005979
Iteration 134/1000 | Loss: 0.00005979
Iteration 135/1000 | Loss: 0.00005979
Iteration 136/1000 | Loss: 0.00005979
Iteration 137/1000 | Loss: 0.00005979
Iteration 138/1000 | Loss: 0.00005979
Iteration 139/1000 | Loss: 0.00005979
Iteration 140/1000 | Loss: 0.00005979
Iteration 141/1000 | Loss: 0.00005979
Iteration 142/1000 | Loss: 0.00005979
Iteration 143/1000 | Loss: 0.00005979
Iteration 144/1000 | Loss: 0.00005979
Iteration 145/1000 | Loss: 0.00005979
Iteration 146/1000 | Loss: 0.00005979
Iteration 147/1000 | Loss: 0.00005979
Iteration 148/1000 | Loss: 0.00005979
Iteration 149/1000 | Loss: 0.00005979
Iteration 150/1000 | Loss: 0.00005979
Iteration 151/1000 | Loss: 0.00005979
Iteration 152/1000 | Loss: 0.00005979
Iteration 153/1000 | Loss: 0.00005979
Iteration 154/1000 | Loss: 0.00005979
Iteration 155/1000 | Loss: 0.00005979
Iteration 156/1000 | Loss: 0.00005979
Iteration 157/1000 | Loss: 0.00005979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [5.9792029787786305e-05, 5.9792029787786305e-05, 5.9792029787786305e-05, 5.9792029787786305e-05, 5.9792029787786305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.9792029787786305e-05

Optimization complete. Final v2v error: 4.397194862365723 mm

Highest mean error: 11.709858894348145 mm for frame 51

Lowest mean error: 2.877551794052124 mm for frame 6

Saving results

Total time: 134.05715918540955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084718
Iteration 2/25 | Loss: 0.00388802
Iteration 3/25 | Loss: 0.00285789
Iteration 4/25 | Loss: 0.00256282
Iteration 5/25 | Loss: 0.00233527
Iteration 6/25 | Loss: 0.00206062
Iteration 7/25 | Loss: 0.00189982
Iteration 8/25 | Loss: 0.00183498
Iteration 9/25 | Loss: 0.00176791
Iteration 10/25 | Loss: 0.00163544
Iteration 11/25 | Loss: 0.00159814
Iteration 12/25 | Loss: 0.00151014
Iteration 13/25 | Loss: 0.00146404
Iteration 14/25 | Loss: 0.00148679
Iteration 15/25 | Loss: 0.00147002
Iteration 16/25 | Loss: 0.00145963
Iteration 17/25 | Loss: 0.00141570
Iteration 18/25 | Loss: 0.00141251
Iteration 19/25 | Loss: 0.00139242
Iteration 20/25 | Loss: 0.00140309
Iteration 21/25 | Loss: 0.00138894
Iteration 22/25 | Loss: 0.00138076
Iteration 23/25 | Loss: 0.00138234
Iteration 24/25 | Loss: 0.00140309
Iteration 25/25 | Loss: 0.00138991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60044813
Iteration 2/25 | Loss: 0.00591440
Iteration 3/25 | Loss: 0.00465260
Iteration 4/25 | Loss: 0.00465258
Iteration 5/25 | Loss: 0.00465258
Iteration 6/25 | Loss: 0.00465258
Iteration 7/25 | Loss: 0.00465258
Iteration 8/25 | Loss: 0.00465258
Iteration 9/25 | Loss: 0.00465258
Iteration 10/25 | Loss: 0.00465258
Iteration 11/25 | Loss: 0.00465258
Iteration 12/25 | Loss: 0.00465258
Iteration 13/25 | Loss: 0.00465258
Iteration 14/25 | Loss: 0.00465258
Iteration 15/25 | Loss: 0.00465258
Iteration 16/25 | Loss: 0.00465258
Iteration 17/25 | Loss: 0.00465258
Iteration 18/25 | Loss: 0.00465258
Iteration 19/25 | Loss: 0.00465258
Iteration 20/25 | Loss: 0.00465258
Iteration 21/25 | Loss: 0.00465258
Iteration 22/25 | Loss: 0.00465258
Iteration 23/25 | Loss: 0.00465258
Iteration 24/25 | Loss: 0.00465258
Iteration 25/25 | Loss: 0.00465258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00465258
Iteration 2/1000 | Loss: 0.00307792
Iteration 3/1000 | Loss: 0.00523821
Iteration 4/1000 | Loss: 0.00210268
Iteration 5/1000 | Loss: 0.00066645
Iteration 6/1000 | Loss: 0.00098271
Iteration 7/1000 | Loss: 0.00114027
Iteration 8/1000 | Loss: 0.00036570
Iteration 9/1000 | Loss: 0.00039228
Iteration 10/1000 | Loss: 0.00126039
Iteration 11/1000 | Loss: 0.00352246
Iteration 12/1000 | Loss: 0.00222773
Iteration 13/1000 | Loss: 0.00064765
Iteration 14/1000 | Loss: 0.00110717
Iteration 15/1000 | Loss: 0.00090397
Iteration 16/1000 | Loss: 0.00054903
Iteration 17/1000 | Loss: 0.00327667
Iteration 18/1000 | Loss: 0.00140828
Iteration 19/1000 | Loss: 0.00364594
Iteration 20/1000 | Loss: 0.00360098
Iteration 21/1000 | Loss: 0.00537407
Iteration 22/1000 | Loss: 0.00570347
Iteration 23/1000 | Loss: 0.00508675
Iteration 24/1000 | Loss: 0.00037532
Iteration 25/1000 | Loss: 0.00194767
Iteration 26/1000 | Loss: 0.00025376
Iteration 27/1000 | Loss: 0.00022713
Iteration 28/1000 | Loss: 0.00040600
Iteration 29/1000 | Loss: 0.00055746
Iteration 30/1000 | Loss: 0.00089734
Iteration 31/1000 | Loss: 0.00028141
Iteration 32/1000 | Loss: 0.00019992
Iteration 33/1000 | Loss: 0.00037039
Iteration 34/1000 | Loss: 0.00084384
Iteration 35/1000 | Loss: 0.00019282
Iteration 36/1000 | Loss: 0.00054714
Iteration 37/1000 | Loss: 0.00019009
Iteration 38/1000 | Loss: 0.00017658
Iteration 39/1000 | Loss: 0.00021069
Iteration 40/1000 | Loss: 0.00029946
Iteration 41/1000 | Loss: 0.00018553
Iteration 42/1000 | Loss: 0.00027259
Iteration 43/1000 | Loss: 0.00030538
Iteration 44/1000 | Loss: 0.00033801
Iteration 45/1000 | Loss: 0.00298043
Iteration 46/1000 | Loss: 0.00329385
Iteration 47/1000 | Loss: 0.00106853
Iteration 48/1000 | Loss: 0.00036074
Iteration 49/1000 | Loss: 0.00156162
Iteration 50/1000 | Loss: 0.00075580
Iteration 51/1000 | Loss: 0.00077041
Iteration 52/1000 | Loss: 0.00017116
Iteration 53/1000 | Loss: 0.00021925
Iteration 54/1000 | Loss: 0.00124956
Iteration 55/1000 | Loss: 0.00007584
Iteration 56/1000 | Loss: 0.00025025
Iteration 57/1000 | Loss: 0.00042918
Iteration 58/1000 | Loss: 0.00005275
Iteration 59/1000 | Loss: 0.00036415
Iteration 60/1000 | Loss: 0.00004600
Iteration 61/1000 | Loss: 0.00004192
Iteration 62/1000 | Loss: 0.00063182
Iteration 63/1000 | Loss: 0.00003769
Iteration 64/1000 | Loss: 0.00003528
Iteration 65/1000 | Loss: 0.00003353
Iteration 66/1000 | Loss: 0.00008137
Iteration 67/1000 | Loss: 0.00003551
Iteration 68/1000 | Loss: 0.00003420
Iteration 69/1000 | Loss: 0.00003060
Iteration 70/1000 | Loss: 0.00004065
Iteration 71/1000 | Loss: 0.00002965
Iteration 72/1000 | Loss: 0.00002935
Iteration 73/1000 | Loss: 0.00002912
Iteration 74/1000 | Loss: 0.00002896
Iteration 75/1000 | Loss: 0.00002894
Iteration 76/1000 | Loss: 0.00002889
Iteration 77/1000 | Loss: 0.00002884
Iteration 78/1000 | Loss: 0.00002884
Iteration 79/1000 | Loss: 0.00002882
Iteration 80/1000 | Loss: 0.00002882
Iteration 81/1000 | Loss: 0.00002881
Iteration 82/1000 | Loss: 0.00002881
Iteration 83/1000 | Loss: 0.00002881
Iteration 84/1000 | Loss: 0.00002881
Iteration 85/1000 | Loss: 0.00002880
Iteration 86/1000 | Loss: 0.00002880
Iteration 87/1000 | Loss: 0.00002880
Iteration 88/1000 | Loss: 0.00002878
Iteration 89/1000 | Loss: 0.00002878
Iteration 90/1000 | Loss: 0.00002878
Iteration 91/1000 | Loss: 0.00002877
Iteration 92/1000 | Loss: 0.00002876
Iteration 93/1000 | Loss: 0.00002875
Iteration 94/1000 | Loss: 0.00002875
Iteration 95/1000 | Loss: 0.00002874
Iteration 96/1000 | Loss: 0.00002874
Iteration 97/1000 | Loss: 0.00002873
Iteration 98/1000 | Loss: 0.00002872
Iteration 99/1000 | Loss: 0.00002871
Iteration 100/1000 | Loss: 0.00002871
Iteration 101/1000 | Loss: 0.00002871
Iteration 102/1000 | Loss: 0.00002871
Iteration 103/1000 | Loss: 0.00002870
Iteration 104/1000 | Loss: 0.00002870
Iteration 105/1000 | Loss: 0.00002870
Iteration 106/1000 | Loss: 0.00002870
Iteration 107/1000 | Loss: 0.00002870
Iteration 108/1000 | Loss: 0.00002869
Iteration 109/1000 | Loss: 0.00002869
Iteration 110/1000 | Loss: 0.00002869
Iteration 111/1000 | Loss: 0.00002867
Iteration 112/1000 | Loss: 0.00002865
Iteration 113/1000 | Loss: 0.00002864
Iteration 114/1000 | Loss: 0.00002864
Iteration 115/1000 | Loss: 0.00002864
Iteration 116/1000 | Loss: 0.00002863
Iteration 117/1000 | Loss: 0.00002863
Iteration 118/1000 | Loss: 0.00002863
Iteration 119/1000 | Loss: 0.00002862
Iteration 120/1000 | Loss: 0.00002862
Iteration 121/1000 | Loss: 0.00002862
Iteration 122/1000 | Loss: 0.00002862
Iteration 123/1000 | Loss: 0.00002862
Iteration 124/1000 | Loss: 0.00002861
Iteration 125/1000 | Loss: 0.00002861
Iteration 126/1000 | Loss: 0.00002861
Iteration 127/1000 | Loss: 0.00002860
Iteration 128/1000 | Loss: 0.00002860
Iteration 129/1000 | Loss: 0.00002860
Iteration 130/1000 | Loss: 0.00002860
Iteration 131/1000 | Loss: 0.00002860
Iteration 132/1000 | Loss: 0.00002859
Iteration 133/1000 | Loss: 0.00002858
Iteration 134/1000 | Loss: 0.00002858
Iteration 135/1000 | Loss: 0.00002858
Iteration 136/1000 | Loss: 0.00002858
Iteration 137/1000 | Loss: 0.00002858
Iteration 138/1000 | Loss: 0.00002858
Iteration 139/1000 | Loss: 0.00002858
Iteration 140/1000 | Loss: 0.00002858
Iteration 141/1000 | Loss: 0.00002858
Iteration 142/1000 | Loss: 0.00002858
Iteration 143/1000 | Loss: 0.00002858
Iteration 144/1000 | Loss: 0.00002858
Iteration 145/1000 | Loss: 0.00002858
Iteration 146/1000 | Loss: 0.00002858
Iteration 147/1000 | Loss: 0.00002857
Iteration 148/1000 | Loss: 0.00002857
Iteration 149/1000 | Loss: 0.00002857
Iteration 150/1000 | Loss: 0.00002856
Iteration 151/1000 | Loss: 0.00002856
Iteration 152/1000 | Loss: 0.00002856
Iteration 153/1000 | Loss: 0.00002856
Iteration 154/1000 | Loss: 0.00002856
Iteration 155/1000 | Loss: 0.00002855
Iteration 156/1000 | Loss: 0.00002855
Iteration 157/1000 | Loss: 0.00002855
Iteration 158/1000 | Loss: 0.00002855
Iteration 159/1000 | Loss: 0.00002855
Iteration 160/1000 | Loss: 0.00002855
Iteration 161/1000 | Loss: 0.00002855
Iteration 162/1000 | Loss: 0.00002855
Iteration 163/1000 | Loss: 0.00002855
Iteration 164/1000 | Loss: 0.00002855
Iteration 165/1000 | Loss: 0.00002855
Iteration 166/1000 | Loss: 0.00002855
Iteration 167/1000 | Loss: 0.00002855
Iteration 168/1000 | Loss: 0.00002855
Iteration 169/1000 | Loss: 0.00002855
Iteration 170/1000 | Loss: 0.00002854
Iteration 171/1000 | Loss: 0.00002854
Iteration 172/1000 | Loss: 0.00002854
Iteration 173/1000 | Loss: 0.00002854
Iteration 174/1000 | Loss: 0.00002854
Iteration 175/1000 | Loss: 0.00002854
Iteration 176/1000 | Loss: 0.00002854
Iteration 177/1000 | Loss: 0.00002854
Iteration 178/1000 | Loss: 0.00002854
Iteration 179/1000 | Loss: 0.00002853
Iteration 180/1000 | Loss: 0.00002853
Iteration 181/1000 | Loss: 0.00002853
Iteration 182/1000 | Loss: 0.00002853
Iteration 183/1000 | Loss: 0.00002853
Iteration 184/1000 | Loss: 0.00002853
Iteration 185/1000 | Loss: 0.00002852
Iteration 186/1000 | Loss: 0.00002852
Iteration 187/1000 | Loss: 0.00002852
Iteration 188/1000 | Loss: 0.00002852
Iteration 189/1000 | Loss: 0.00002852
Iteration 190/1000 | Loss: 0.00002852
Iteration 191/1000 | Loss: 0.00002852
Iteration 192/1000 | Loss: 0.00002852
Iteration 193/1000 | Loss: 0.00002852
Iteration 194/1000 | Loss: 0.00002852
Iteration 195/1000 | Loss: 0.00002852
Iteration 196/1000 | Loss: 0.00002852
Iteration 197/1000 | Loss: 0.00002851
Iteration 198/1000 | Loss: 0.00002851
Iteration 199/1000 | Loss: 0.00002851
Iteration 200/1000 | Loss: 0.00002851
Iteration 201/1000 | Loss: 0.00002851
Iteration 202/1000 | Loss: 0.00002851
Iteration 203/1000 | Loss: 0.00002851
Iteration 204/1000 | Loss: 0.00002851
Iteration 205/1000 | Loss: 0.00002851
Iteration 206/1000 | Loss: 0.00002851
Iteration 207/1000 | Loss: 0.00002851
Iteration 208/1000 | Loss: 0.00002851
Iteration 209/1000 | Loss: 0.00002851
Iteration 210/1000 | Loss: 0.00002851
Iteration 211/1000 | Loss: 0.00002851
Iteration 212/1000 | Loss: 0.00002851
Iteration 213/1000 | Loss: 0.00002851
Iteration 214/1000 | Loss: 0.00002851
Iteration 215/1000 | Loss: 0.00002851
Iteration 216/1000 | Loss: 0.00002851
Iteration 217/1000 | Loss: 0.00002851
Iteration 218/1000 | Loss: 0.00002851
Iteration 219/1000 | Loss: 0.00002851
Iteration 220/1000 | Loss: 0.00002851
Iteration 221/1000 | Loss: 0.00002851
Iteration 222/1000 | Loss: 0.00002851
Iteration 223/1000 | Loss: 0.00002851
Iteration 224/1000 | Loss: 0.00002851
Iteration 225/1000 | Loss: 0.00002851
Iteration 226/1000 | Loss: 0.00002851
Iteration 227/1000 | Loss: 0.00002851
Iteration 228/1000 | Loss: 0.00002851
Iteration 229/1000 | Loss: 0.00002851
Iteration 230/1000 | Loss: 0.00002851
Iteration 231/1000 | Loss: 0.00002851
Iteration 232/1000 | Loss: 0.00002851
Iteration 233/1000 | Loss: 0.00002851
Iteration 234/1000 | Loss: 0.00002851
Iteration 235/1000 | Loss: 0.00002851
Iteration 236/1000 | Loss: 0.00002851
Iteration 237/1000 | Loss: 0.00002851
Iteration 238/1000 | Loss: 0.00002851
Iteration 239/1000 | Loss: 0.00002851
Iteration 240/1000 | Loss: 0.00002851
Iteration 241/1000 | Loss: 0.00002851
Iteration 242/1000 | Loss: 0.00002851
Iteration 243/1000 | Loss: 0.00002851
Iteration 244/1000 | Loss: 0.00002851
Iteration 245/1000 | Loss: 0.00002851
Iteration 246/1000 | Loss: 0.00002851
Iteration 247/1000 | Loss: 0.00002851
Iteration 248/1000 | Loss: 0.00002851
Iteration 249/1000 | Loss: 0.00002851
Iteration 250/1000 | Loss: 0.00002851
Iteration 251/1000 | Loss: 0.00002851
Iteration 252/1000 | Loss: 0.00002851
Iteration 253/1000 | Loss: 0.00002851
Iteration 254/1000 | Loss: 0.00002851
Iteration 255/1000 | Loss: 0.00002851
Iteration 256/1000 | Loss: 0.00002851
Iteration 257/1000 | Loss: 0.00002851
Iteration 258/1000 | Loss: 0.00002851
Iteration 259/1000 | Loss: 0.00002851
Iteration 260/1000 | Loss: 0.00002851
Iteration 261/1000 | Loss: 0.00002851
Iteration 262/1000 | Loss: 0.00002851
Iteration 263/1000 | Loss: 0.00002851
Iteration 264/1000 | Loss: 0.00002851
Iteration 265/1000 | Loss: 0.00002851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.8513532015495002e-05, 2.8513532015495002e-05, 2.8513532015495002e-05, 2.8513532015495002e-05, 2.8513532015495002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8513532015495002e-05

Optimization complete. Final v2v error: 3.9537599086761475 mm

Highest mean error: 5.4669880867004395 mm for frame 62

Lowest mean error: 3.320847272872925 mm for frame 90

Saving results

Total time: 158.72678422927856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061797
Iteration 2/25 | Loss: 0.00152620
Iteration 3/25 | Loss: 0.00101776
Iteration 4/25 | Loss: 0.00092204
Iteration 5/25 | Loss: 0.00092306
Iteration 6/25 | Loss: 0.00101270
Iteration 7/25 | Loss: 0.00095379
Iteration 8/25 | Loss: 0.00088405
Iteration 9/25 | Loss: 0.00083554
Iteration 10/25 | Loss: 0.00084963
Iteration 11/25 | Loss: 0.00084212
Iteration 12/25 | Loss: 0.00083831
Iteration 13/25 | Loss: 0.00083748
Iteration 14/25 | Loss: 0.00083531
Iteration 15/25 | Loss: 0.00083382
Iteration 16/25 | Loss: 0.00082551
Iteration 17/25 | Loss: 0.00082047
Iteration 18/25 | Loss: 0.00081884
Iteration 19/25 | Loss: 0.00080509
Iteration 20/25 | Loss: 0.00080401
Iteration 21/25 | Loss: 0.00080395
Iteration 22/25 | Loss: 0.00080365
Iteration 23/25 | Loss: 0.00079968
Iteration 24/25 | Loss: 0.00078816
Iteration 25/25 | Loss: 0.00078889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68572271
Iteration 2/25 | Loss: 0.00167597
Iteration 3/25 | Loss: 0.00167597
Iteration 4/25 | Loss: 0.00167597
Iteration 5/25 | Loss: 0.00167596
Iteration 6/25 | Loss: 0.00167596
Iteration 7/25 | Loss: 0.00167596
Iteration 8/25 | Loss: 0.00167596
Iteration 9/25 | Loss: 0.00167596
Iteration 10/25 | Loss: 0.00167596
Iteration 11/25 | Loss: 0.00167596
Iteration 12/25 | Loss: 0.00167596
Iteration 13/25 | Loss: 0.00167596
Iteration 14/25 | Loss: 0.00167596
Iteration 15/25 | Loss: 0.00167596
Iteration 16/25 | Loss: 0.00167596
Iteration 17/25 | Loss: 0.00167596
Iteration 18/25 | Loss: 0.00167596
Iteration 19/25 | Loss: 0.00167596
Iteration 20/25 | Loss: 0.00167596
Iteration 21/25 | Loss: 0.00167596
Iteration 22/25 | Loss: 0.00167596
Iteration 23/25 | Loss: 0.00167596
Iteration 24/25 | Loss: 0.00167596
Iteration 25/25 | Loss: 0.00167596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167596
Iteration 2/1000 | Loss: 0.00030470
Iteration 3/1000 | Loss: 0.00039815
Iteration 4/1000 | Loss: 0.00045527
Iteration 5/1000 | Loss: 0.00043039
Iteration 6/1000 | Loss: 0.00066957
Iteration 7/1000 | Loss: 0.00059793
Iteration 8/1000 | Loss: 0.00038669
Iteration 9/1000 | Loss: 0.00038835
Iteration 10/1000 | Loss: 0.00078190
Iteration 11/1000 | Loss: 0.00127147
Iteration 12/1000 | Loss: 0.00071268
Iteration 13/1000 | Loss: 0.00057914
Iteration 14/1000 | Loss: 0.00063652
Iteration 15/1000 | Loss: 0.00038061
Iteration 16/1000 | Loss: 0.00057389
Iteration 17/1000 | Loss: 0.00069652
Iteration 18/1000 | Loss: 0.00085049
Iteration 19/1000 | Loss: 0.00087139
Iteration 20/1000 | Loss: 0.00084131
Iteration 21/1000 | Loss: 0.00067878
Iteration 22/1000 | Loss: 0.00057445
Iteration 23/1000 | Loss: 0.00062568
Iteration 24/1000 | Loss: 0.00052573
Iteration 25/1000 | Loss: 0.00051804
Iteration 26/1000 | Loss: 0.00073454
Iteration 27/1000 | Loss: 0.00073734
Iteration 28/1000 | Loss: 0.00110572
Iteration 29/1000 | Loss: 0.00068074
Iteration 30/1000 | Loss: 0.00028334
Iteration 31/1000 | Loss: 0.00057924
Iteration 32/1000 | Loss: 0.00071570
Iteration 33/1000 | Loss: 0.00046300
Iteration 34/1000 | Loss: 0.00038445
Iteration 35/1000 | Loss: 0.00034490
Iteration 36/1000 | Loss: 0.00045544
Iteration 37/1000 | Loss: 0.00056735
Iteration 38/1000 | Loss: 0.00041295
Iteration 39/1000 | Loss: 0.00049689
Iteration 40/1000 | Loss: 0.00025059
Iteration 41/1000 | Loss: 0.00010420
Iteration 42/1000 | Loss: 0.00024297
Iteration 43/1000 | Loss: 0.00034136
Iteration 44/1000 | Loss: 0.00131826
Iteration 45/1000 | Loss: 0.00094222
Iteration 46/1000 | Loss: 0.00006642
Iteration 47/1000 | Loss: 0.00075751
Iteration 48/1000 | Loss: 0.00138327
Iteration 49/1000 | Loss: 0.00131787
Iteration 50/1000 | Loss: 0.00007578
Iteration 51/1000 | Loss: 0.00004151
Iteration 52/1000 | Loss: 0.00003390
Iteration 53/1000 | Loss: 0.00038027
Iteration 54/1000 | Loss: 0.00039589
Iteration 55/1000 | Loss: 0.00019170
Iteration 56/1000 | Loss: 0.00060057
Iteration 57/1000 | Loss: 0.00008476
Iteration 58/1000 | Loss: 0.00006742
Iteration 59/1000 | Loss: 0.00003731
Iteration 60/1000 | Loss: 0.00005193
Iteration 61/1000 | Loss: 0.00006425
Iteration 62/1000 | Loss: 0.00021724
Iteration 63/1000 | Loss: 0.00019428
Iteration 64/1000 | Loss: 0.00015304
Iteration 65/1000 | Loss: 0.00006402
Iteration 66/1000 | Loss: 0.00006021
Iteration 67/1000 | Loss: 0.00033979
Iteration 68/1000 | Loss: 0.00016880
Iteration 69/1000 | Loss: 0.00036097
Iteration 70/1000 | Loss: 0.00012637
Iteration 71/1000 | Loss: 0.00018599
Iteration 72/1000 | Loss: 0.00013809
Iteration 73/1000 | Loss: 0.00012295
Iteration 74/1000 | Loss: 0.00014545
Iteration 75/1000 | Loss: 0.00022571
Iteration 76/1000 | Loss: 0.00035075
Iteration 77/1000 | Loss: 0.00023752
Iteration 78/1000 | Loss: 0.00010597
Iteration 79/1000 | Loss: 0.00026100
Iteration 80/1000 | Loss: 0.00016949
Iteration 81/1000 | Loss: 0.00017902
Iteration 82/1000 | Loss: 0.00012845
Iteration 83/1000 | Loss: 0.00010157
Iteration 84/1000 | Loss: 0.00010869
Iteration 85/1000 | Loss: 0.00024128
Iteration 86/1000 | Loss: 0.00026517
Iteration 87/1000 | Loss: 0.00033886
Iteration 88/1000 | Loss: 0.00039781
Iteration 89/1000 | Loss: 0.00019384
Iteration 90/1000 | Loss: 0.00003706
Iteration 91/1000 | Loss: 0.00005833
Iteration 92/1000 | Loss: 0.00018933
Iteration 93/1000 | Loss: 0.00003091
Iteration 94/1000 | Loss: 0.00009304
Iteration 95/1000 | Loss: 0.00055722
Iteration 96/1000 | Loss: 0.00064237
Iteration 97/1000 | Loss: 0.00028506
Iteration 98/1000 | Loss: 0.00032170
Iteration 99/1000 | Loss: 0.00022069
Iteration 100/1000 | Loss: 0.00028356
Iteration 101/1000 | Loss: 0.00016149
Iteration 102/1000 | Loss: 0.00008928
Iteration 103/1000 | Loss: 0.00013473
Iteration 104/1000 | Loss: 0.00009582
Iteration 105/1000 | Loss: 0.00013414
Iteration 106/1000 | Loss: 0.00008397
Iteration 107/1000 | Loss: 0.00003074
Iteration 108/1000 | Loss: 0.00002615
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00058645
Iteration 112/1000 | Loss: 0.00013851
Iteration 113/1000 | Loss: 0.00019155
Iteration 114/1000 | Loss: 0.00022273
Iteration 115/1000 | Loss: 0.00023680
Iteration 116/1000 | Loss: 0.00019995
Iteration 117/1000 | Loss: 0.00022254
Iteration 118/1000 | Loss: 0.00047875
Iteration 119/1000 | Loss: 0.00015009
Iteration 120/1000 | Loss: 0.00020240
Iteration 121/1000 | Loss: 0.00062419
Iteration 122/1000 | Loss: 0.00027974
Iteration 123/1000 | Loss: 0.00027430
Iteration 124/1000 | Loss: 0.00025261
Iteration 125/1000 | Loss: 0.00020895
Iteration 126/1000 | Loss: 0.00021785
Iteration 127/1000 | Loss: 0.00026432
Iteration 128/1000 | Loss: 0.00020665
Iteration 129/1000 | Loss: 0.00068142
Iteration 130/1000 | Loss: 0.00039769
Iteration 131/1000 | Loss: 0.00039482
Iteration 132/1000 | Loss: 0.00039515
Iteration 133/1000 | Loss: 0.00040584
Iteration 134/1000 | Loss: 0.00076670
Iteration 135/1000 | Loss: 0.00029659
Iteration 136/1000 | Loss: 0.00032679
Iteration 137/1000 | Loss: 0.00034045
Iteration 138/1000 | Loss: 0.00031673
Iteration 139/1000 | Loss: 0.00032552
Iteration 140/1000 | Loss: 0.00031564
Iteration 141/1000 | Loss: 0.00040970
Iteration 142/1000 | Loss: 0.00003710
Iteration 143/1000 | Loss: 0.00010279
Iteration 144/1000 | Loss: 0.00006411
Iteration 145/1000 | Loss: 0.00004038
Iteration 146/1000 | Loss: 0.00028777
Iteration 147/1000 | Loss: 0.00020519
Iteration 148/1000 | Loss: 0.00004349
Iteration 149/1000 | Loss: 0.00004794
Iteration 150/1000 | Loss: 0.00004633
Iteration 151/1000 | Loss: 0.00005049
Iteration 152/1000 | Loss: 0.00006734
Iteration 153/1000 | Loss: 0.00007213
Iteration 154/1000 | Loss: 0.00049208
Iteration 155/1000 | Loss: 0.00003044
Iteration 156/1000 | Loss: 0.00002285
Iteration 157/1000 | Loss: 0.00031102
Iteration 158/1000 | Loss: 0.00019704
Iteration 159/1000 | Loss: 0.00045967
Iteration 160/1000 | Loss: 0.00002721
Iteration 161/1000 | Loss: 0.00067190
Iteration 162/1000 | Loss: 0.00042596
Iteration 163/1000 | Loss: 0.00022231
Iteration 164/1000 | Loss: 0.00071051
Iteration 165/1000 | Loss: 0.00065347
Iteration 166/1000 | Loss: 0.00048444
Iteration 167/1000 | Loss: 0.00023963
Iteration 168/1000 | Loss: 0.00016712
Iteration 169/1000 | Loss: 0.00022711
Iteration 170/1000 | Loss: 0.00017442
Iteration 171/1000 | Loss: 0.00002733
Iteration 172/1000 | Loss: 0.00011709
Iteration 173/1000 | Loss: 0.00018314
Iteration 174/1000 | Loss: 0.00023344
Iteration 175/1000 | Loss: 0.00035494
Iteration 176/1000 | Loss: 0.00013066
Iteration 177/1000 | Loss: 0.00002735
Iteration 178/1000 | Loss: 0.00002266
Iteration 179/1000 | Loss: 0.00041166
Iteration 180/1000 | Loss: 0.00004061
Iteration 181/1000 | Loss: 0.00002064
Iteration 182/1000 | Loss: 0.00001858
Iteration 183/1000 | Loss: 0.00006042
Iteration 184/1000 | Loss: 0.00001872
Iteration 185/1000 | Loss: 0.00001479
Iteration 186/1000 | Loss: 0.00001376
Iteration 187/1000 | Loss: 0.00001323
Iteration 188/1000 | Loss: 0.00001465
Iteration 189/1000 | Loss: 0.00001462
Iteration 190/1000 | Loss: 0.00001345
Iteration 191/1000 | Loss: 0.00001282
Iteration 192/1000 | Loss: 0.00001280
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001270
Iteration 195/1000 | Loss: 0.00001269
Iteration 196/1000 | Loss: 0.00001266
Iteration 197/1000 | Loss: 0.00001266
Iteration 198/1000 | Loss: 0.00001262
Iteration 199/1000 | Loss: 0.00001252
Iteration 200/1000 | Loss: 0.00001248
Iteration 201/1000 | Loss: 0.00001248
Iteration 202/1000 | Loss: 0.00001247
Iteration 203/1000 | Loss: 0.00001246
Iteration 204/1000 | Loss: 0.00001245
Iteration 205/1000 | Loss: 0.00001245
Iteration 206/1000 | Loss: 0.00001245
Iteration 207/1000 | Loss: 0.00001244
Iteration 208/1000 | Loss: 0.00001243
Iteration 209/1000 | Loss: 0.00001243
Iteration 210/1000 | Loss: 0.00001242
Iteration 211/1000 | Loss: 0.00001242
Iteration 212/1000 | Loss: 0.00001242
Iteration 213/1000 | Loss: 0.00001241
Iteration 214/1000 | Loss: 0.00001241
Iteration 215/1000 | Loss: 0.00001241
Iteration 216/1000 | Loss: 0.00001241
Iteration 217/1000 | Loss: 0.00001241
Iteration 218/1000 | Loss: 0.00001241
Iteration 219/1000 | Loss: 0.00001241
Iteration 220/1000 | Loss: 0.00001241
Iteration 221/1000 | Loss: 0.00001241
Iteration 222/1000 | Loss: 0.00001241
Iteration 223/1000 | Loss: 0.00001241
Iteration 224/1000 | Loss: 0.00001241
Iteration 225/1000 | Loss: 0.00001240
Iteration 226/1000 | Loss: 0.00001240
Iteration 227/1000 | Loss: 0.00001240
Iteration 228/1000 | Loss: 0.00001240
Iteration 229/1000 | Loss: 0.00001240
Iteration 230/1000 | Loss: 0.00001239
Iteration 231/1000 | Loss: 0.00001239
Iteration 232/1000 | Loss: 0.00001238
Iteration 233/1000 | Loss: 0.00001238
Iteration 234/1000 | Loss: 0.00001238
Iteration 235/1000 | Loss: 0.00001238
Iteration 236/1000 | Loss: 0.00001238
Iteration 237/1000 | Loss: 0.00001238
Iteration 238/1000 | Loss: 0.00001237
Iteration 239/1000 | Loss: 0.00001237
Iteration 240/1000 | Loss: 0.00001237
Iteration 241/1000 | Loss: 0.00001237
Iteration 242/1000 | Loss: 0.00001236
Iteration 243/1000 | Loss: 0.00001236
Iteration 244/1000 | Loss: 0.00001236
Iteration 245/1000 | Loss: 0.00001235
Iteration 246/1000 | Loss: 0.00001235
Iteration 247/1000 | Loss: 0.00001235
Iteration 248/1000 | Loss: 0.00001235
Iteration 249/1000 | Loss: 0.00001235
Iteration 250/1000 | Loss: 0.00001234
Iteration 251/1000 | Loss: 0.00001234
Iteration 252/1000 | Loss: 0.00001234
Iteration 253/1000 | Loss: 0.00001234
Iteration 254/1000 | Loss: 0.00001234
Iteration 255/1000 | Loss: 0.00001234
Iteration 256/1000 | Loss: 0.00001233
Iteration 257/1000 | Loss: 0.00001233
Iteration 258/1000 | Loss: 0.00001233
Iteration 259/1000 | Loss: 0.00001232
Iteration 260/1000 | Loss: 0.00001232
Iteration 261/1000 | Loss: 0.00001232
Iteration 262/1000 | Loss: 0.00001232
Iteration 263/1000 | Loss: 0.00001232
Iteration 264/1000 | Loss: 0.00001232
Iteration 265/1000 | Loss: 0.00001232
Iteration 266/1000 | Loss: 0.00001231
Iteration 267/1000 | Loss: 0.00001231
Iteration 268/1000 | Loss: 0.00001231
Iteration 269/1000 | Loss: 0.00001231
Iteration 270/1000 | Loss: 0.00001231
Iteration 271/1000 | Loss: 0.00001231
Iteration 272/1000 | Loss: 0.00001231
Iteration 273/1000 | Loss: 0.00001231
Iteration 274/1000 | Loss: 0.00001231
Iteration 275/1000 | Loss: 0.00001231
Iteration 276/1000 | Loss: 0.00001230
Iteration 277/1000 | Loss: 0.00001230
Iteration 278/1000 | Loss: 0.00001230
Iteration 279/1000 | Loss: 0.00001230
Iteration 280/1000 | Loss: 0.00001230
Iteration 281/1000 | Loss: 0.00001230
Iteration 282/1000 | Loss: 0.00001230
Iteration 283/1000 | Loss: 0.00001230
Iteration 284/1000 | Loss: 0.00001230
Iteration 285/1000 | Loss: 0.00001230
Iteration 286/1000 | Loss: 0.00001230
Iteration 287/1000 | Loss: 0.00001230
Iteration 288/1000 | Loss: 0.00001230
Iteration 289/1000 | Loss: 0.00001229
Iteration 290/1000 | Loss: 0.00001229
Iteration 291/1000 | Loss: 0.00001229
Iteration 292/1000 | Loss: 0.00001229
Iteration 293/1000 | Loss: 0.00001229
Iteration 294/1000 | Loss: 0.00001229
Iteration 295/1000 | Loss: 0.00001229
Iteration 296/1000 | Loss: 0.00001229
Iteration 297/1000 | Loss: 0.00001229
Iteration 298/1000 | Loss: 0.00001229
Iteration 299/1000 | Loss: 0.00001228
Iteration 300/1000 | Loss: 0.00001228
Iteration 301/1000 | Loss: 0.00001228
Iteration 302/1000 | Loss: 0.00001228
Iteration 303/1000 | Loss: 0.00001228
Iteration 304/1000 | Loss: 0.00001227
Iteration 305/1000 | Loss: 0.00001227
Iteration 306/1000 | Loss: 0.00001227
Iteration 307/1000 | Loss: 0.00001227
Iteration 308/1000 | Loss: 0.00001226
Iteration 309/1000 | Loss: 0.00001226
Iteration 310/1000 | Loss: 0.00001226
Iteration 311/1000 | Loss: 0.00001226
Iteration 312/1000 | Loss: 0.00001225
Iteration 313/1000 | Loss: 0.00001225
Iteration 314/1000 | Loss: 0.00001225
Iteration 315/1000 | Loss: 0.00001224
Iteration 316/1000 | Loss: 0.00001224
Iteration 317/1000 | Loss: 0.00001224
Iteration 318/1000 | Loss: 0.00001224
Iteration 319/1000 | Loss: 0.00001224
Iteration 320/1000 | Loss: 0.00001224
Iteration 321/1000 | Loss: 0.00001224
Iteration 322/1000 | Loss: 0.00001223
Iteration 323/1000 | Loss: 0.00001223
Iteration 324/1000 | Loss: 0.00001223
Iteration 325/1000 | Loss: 0.00001223
Iteration 326/1000 | Loss: 0.00001223
Iteration 327/1000 | Loss: 0.00001223
Iteration 328/1000 | Loss: 0.00001222
Iteration 329/1000 | Loss: 0.00001222
Iteration 330/1000 | Loss: 0.00001222
Iteration 331/1000 | Loss: 0.00001221
Iteration 332/1000 | Loss: 0.00001221
Iteration 333/1000 | Loss: 0.00001221
Iteration 334/1000 | Loss: 0.00001220
Iteration 335/1000 | Loss: 0.00001220
Iteration 336/1000 | Loss: 0.00001220
Iteration 337/1000 | Loss: 0.00001220
Iteration 338/1000 | Loss: 0.00001220
Iteration 339/1000 | Loss: 0.00001219
Iteration 340/1000 | Loss: 0.00001219
Iteration 341/1000 | Loss: 0.00001219
Iteration 342/1000 | Loss: 0.00001219
Iteration 343/1000 | Loss: 0.00001219
Iteration 344/1000 | Loss: 0.00001219
Iteration 345/1000 | Loss: 0.00001219
Iteration 346/1000 | Loss: 0.00001219
Iteration 347/1000 | Loss: 0.00001218
Iteration 348/1000 | Loss: 0.00001218
Iteration 349/1000 | Loss: 0.00001218
Iteration 350/1000 | Loss: 0.00001218
Iteration 351/1000 | Loss: 0.00001218
Iteration 352/1000 | Loss: 0.00001218
Iteration 353/1000 | Loss: 0.00001218
Iteration 354/1000 | Loss: 0.00001218
Iteration 355/1000 | Loss: 0.00001218
Iteration 356/1000 | Loss: 0.00001218
Iteration 357/1000 | Loss: 0.00001218
Iteration 358/1000 | Loss: 0.00001218
Iteration 359/1000 | Loss: 0.00001218
Iteration 360/1000 | Loss: 0.00001218
Iteration 361/1000 | Loss: 0.00001218
Iteration 362/1000 | Loss: 0.00001218
Iteration 363/1000 | Loss: 0.00001218
Iteration 364/1000 | Loss: 0.00001218
Iteration 365/1000 | Loss: 0.00001218
Iteration 366/1000 | Loss: 0.00001217
Iteration 367/1000 | Loss: 0.00001217
Iteration 368/1000 | Loss: 0.00001217
Iteration 369/1000 | Loss: 0.00001217
Iteration 370/1000 | Loss: 0.00001217
Iteration 371/1000 | Loss: 0.00001217
Iteration 372/1000 | Loss: 0.00001217
Iteration 373/1000 | Loss: 0.00001217
Iteration 374/1000 | Loss: 0.00001217
Iteration 375/1000 | Loss: 0.00001217
Iteration 376/1000 | Loss: 0.00001217
Iteration 377/1000 | Loss: 0.00001217
Iteration 378/1000 | Loss: 0.00001217
Iteration 379/1000 | Loss: 0.00001217
Iteration 380/1000 | Loss: 0.00001217
Iteration 381/1000 | Loss: 0.00001217
Iteration 382/1000 | Loss: 0.00001217
Iteration 383/1000 | Loss: 0.00001217
Iteration 384/1000 | Loss: 0.00001217
Iteration 385/1000 | Loss: 0.00001217
Iteration 386/1000 | Loss: 0.00001217
Iteration 387/1000 | Loss: 0.00001217
Iteration 388/1000 | Loss: 0.00001217
Iteration 389/1000 | Loss: 0.00001217
Iteration 390/1000 | Loss: 0.00001217
Iteration 391/1000 | Loss: 0.00001217
Iteration 392/1000 | Loss: 0.00001217
Iteration 393/1000 | Loss: 0.00001217
Iteration 394/1000 | Loss: 0.00001217
Iteration 395/1000 | Loss: 0.00001217
Iteration 396/1000 | Loss: 0.00001217
Iteration 397/1000 | Loss: 0.00001217
Iteration 398/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [1.217296903632814e-05, 1.217296903632814e-05, 1.217296903632814e-05, 1.217296903632814e-05, 1.217296903632814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.217296903632814e-05

Optimization complete. Final v2v error: 2.8982808589935303 mm

Highest mean error: 4.0356316566467285 mm for frame 71

Lowest mean error: 2.4789929389953613 mm for frame 1

Saving results

Total time: 315.86631417274475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871706
Iteration 2/25 | Loss: 0.00109805
Iteration 3/25 | Loss: 0.00087191
Iteration 4/25 | Loss: 0.00082672
Iteration 5/25 | Loss: 0.00081338
Iteration 6/25 | Loss: 0.00081114
Iteration 7/25 | Loss: 0.00081100
Iteration 8/25 | Loss: 0.00081100
Iteration 9/25 | Loss: 0.00081100
Iteration 10/25 | Loss: 0.00081100
Iteration 11/25 | Loss: 0.00081100
Iteration 12/25 | Loss: 0.00081100
Iteration 13/25 | Loss: 0.00081100
Iteration 14/25 | Loss: 0.00081100
Iteration 15/25 | Loss: 0.00081100
Iteration 16/25 | Loss: 0.00081100
Iteration 17/25 | Loss: 0.00081100
Iteration 18/25 | Loss: 0.00081100
Iteration 19/25 | Loss: 0.00081100
Iteration 20/25 | Loss: 0.00081100
Iteration 21/25 | Loss: 0.00081100
Iteration 22/25 | Loss: 0.00081100
Iteration 23/25 | Loss: 0.00081100
Iteration 24/25 | Loss: 0.00081100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008110045455396175, 0.0008110045455396175, 0.0008110045455396175, 0.0008110045455396175, 0.0008110045455396175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008110045455396175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55115402
Iteration 2/25 | Loss: 0.00119486
Iteration 3/25 | Loss: 0.00119480
Iteration 4/25 | Loss: 0.00119480
Iteration 5/25 | Loss: 0.00119480
Iteration 6/25 | Loss: 0.00119480
Iteration 7/25 | Loss: 0.00119480
Iteration 8/25 | Loss: 0.00119480
Iteration 9/25 | Loss: 0.00119480
Iteration 10/25 | Loss: 0.00119480
Iteration 11/25 | Loss: 0.00119480
Iteration 12/25 | Loss: 0.00119480
Iteration 13/25 | Loss: 0.00119480
Iteration 14/25 | Loss: 0.00119480
Iteration 15/25 | Loss: 0.00119480
Iteration 16/25 | Loss: 0.00119480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011947955936193466, 0.0011947955936193466, 0.0011947955936193466, 0.0011947955936193466, 0.0011947955936193466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011947955936193466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119480
Iteration 2/1000 | Loss: 0.00003534
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00002038
Iteration 6/1000 | Loss: 0.00001925
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001817
Iteration 9/1000 | Loss: 0.00001786
Iteration 10/1000 | Loss: 0.00001760
Iteration 11/1000 | Loss: 0.00001749
Iteration 12/1000 | Loss: 0.00001747
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001740
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001730
Iteration 19/1000 | Loss: 0.00001730
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001726
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001718
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001710
Iteration 44/1000 | Loss: 0.00001710
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001707
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001699
Iteration 89/1000 | Loss: 0.00001699
Iteration 90/1000 | Loss: 0.00001699
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001699
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001697
Iteration 100/1000 | Loss: 0.00001697
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00001693
Iteration 121/1000 | Loss: 0.00001692
Iteration 122/1000 | Loss: 0.00001692
Iteration 123/1000 | Loss: 0.00001692
Iteration 124/1000 | Loss: 0.00001692
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001691
Iteration 127/1000 | Loss: 0.00001691
Iteration 128/1000 | Loss: 0.00001691
Iteration 129/1000 | Loss: 0.00001690
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001690
Iteration 132/1000 | Loss: 0.00001689
Iteration 133/1000 | Loss: 0.00001689
Iteration 134/1000 | Loss: 0.00001689
Iteration 135/1000 | Loss: 0.00001689
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001689
Iteration 138/1000 | Loss: 0.00001689
Iteration 139/1000 | Loss: 0.00001689
Iteration 140/1000 | Loss: 0.00001689
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001688
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001687
Iteration 153/1000 | Loss: 0.00001687
Iteration 154/1000 | Loss: 0.00001687
Iteration 155/1000 | Loss: 0.00001687
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001685
Iteration 162/1000 | Loss: 0.00001685
Iteration 163/1000 | Loss: 0.00001685
Iteration 164/1000 | Loss: 0.00001685
Iteration 165/1000 | Loss: 0.00001685
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001685
Iteration 168/1000 | Loss: 0.00001685
Iteration 169/1000 | Loss: 0.00001685
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001684
Iteration 172/1000 | Loss: 0.00001684
Iteration 173/1000 | Loss: 0.00001684
Iteration 174/1000 | Loss: 0.00001684
Iteration 175/1000 | Loss: 0.00001684
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001684
Iteration 179/1000 | Loss: 0.00001684
Iteration 180/1000 | Loss: 0.00001684
Iteration 181/1000 | Loss: 0.00001684
Iteration 182/1000 | Loss: 0.00001684
Iteration 183/1000 | Loss: 0.00001683
Iteration 184/1000 | Loss: 0.00001683
Iteration 185/1000 | Loss: 0.00001683
Iteration 186/1000 | Loss: 0.00001683
Iteration 187/1000 | Loss: 0.00001683
Iteration 188/1000 | Loss: 0.00001683
Iteration 189/1000 | Loss: 0.00001683
Iteration 190/1000 | Loss: 0.00001683
Iteration 191/1000 | Loss: 0.00001683
Iteration 192/1000 | Loss: 0.00001683
Iteration 193/1000 | Loss: 0.00001683
Iteration 194/1000 | Loss: 0.00001683
Iteration 195/1000 | Loss: 0.00001683
Iteration 196/1000 | Loss: 0.00001683
Iteration 197/1000 | Loss: 0.00001683
Iteration 198/1000 | Loss: 0.00001683
Iteration 199/1000 | Loss: 0.00001683
Iteration 200/1000 | Loss: 0.00001683
Iteration 201/1000 | Loss: 0.00001683
Iteration 202/1000 | Loss: 0.00001682
Iteration 203/1000 | Loss: 0.00001682
Iteration 204/1000 | Loss: 0.00001682
Iteration 205/1000 | Loss: 0.00001682
Iteration 206/1000 | Loss: 0.00001682
Iteration 207/1000 | Loss: 0.00001682
Iteration 208/1000 | Loss: 0.00001682
Iteration 209/1000 | Loss: 0.00001682
Iteration 210/1000 | Loss: 0.00001682
Iteration 211/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.6822354155010544e-05, 1.6822354155010544e-05, 1.6822354155010544e-05, 1.6822354155010544e-05, 1.6822354155010544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6822354155010544e-05

Optimization complete. Final v2v error: 3.5033328533172607 mm

Highest mean error: 4.372750759124756 mm for frame 160

Lowest mean error: 2.9432578086853027 mm for frame 137

Saving results

Total time: 46.7249059677124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390284
Iteration 2/25 | Loss: 0.00084478
Iteration 3/25 | Loss: 0.00075962
Iteration 4/25 | Loss: 0.00074041
Iteration 5/25 | Loss: 0.00073624
Iteration 6/25 | Loss: 0.00073557
Iteration 7/25 | Loss: 0.00073557
Iteration 8/25 | Loss: 0.00073557
Iteration 9/25 | Loss: 0.00073557
Iteration 10/25 | Loss: 0.00073557
Iteration 11/25 | Loss: 0.00073557
Iteration 12/25 | Loss: 0.00073557
Iteration 13/25 | Loss: 0.00073557
Iteration 14/25 | Loss: 0.00073557
Iteration 15/25 | Loss: 0.00073557
Iteration 16/25 | Loss: 0.00073557
Iteration 17/25 | Loss: 0.00073557
Iteration 18/25 | Loss: 0.00073557
Iteration 19/25 | Loss: 0.00073557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007355716079473495, 0.0007355716079473495, 0.0007355716079473495, 0.0007355716079473495, 0.0007355716079473495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007355716079473495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56972456
Iteration 2/25 | Loss: 0.00122555
Iteration 3/25 | Loss: 0.00122554
Iteration 4/25 | Loss: 0.00122554
Iteration 5/25 | Loss: 0.00122554
Iteration 6/25 | Loss: 0.00122554
Iteration 7/25 | Loss: 0.00122554
Iteration 8/25 | Loss: 0.00122554
Iteration 9/25 | Loss: 0.00122554
Iteration 10/25 | Loss: 0.00122554
Iteration 11/25 | Loss: 0.00122554
Iteration 12/25 | Loss: 0.00122554
Iteration 13/25 | Loss: 0.00122554
Iteration 14/25 | Loss: 0.00122554
Iteration 15/25 | Loss: 0.00122554
Iteration 16/25 | Loss: 0.00122554
Iteration 17/25 | Loss: 0.00122554
Iteration 18/25 | Loss: 0.00122554
Iteration 19/25 | Loss: 0.00122554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012255421606823802, 0.0012255421606823802, 0.0012255421606823802, 0.0012255421606823802, 0.0012255421606823802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012255421606823802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122554
Iteration 2/1000 | Loss: 0.00001851
Iteration 3/1000 | Loss: 0.00001514
Iteration 4/1000 | Loss: 0.00001414
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001339
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001289
Iteration 10/1000 | Loss: 0.00001282
Iteration 11/1000 | Loss: 0.00001282
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001281
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001272
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001264
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001255
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001251
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001249
Iteration 82/1000 | Loss: 0.00001249
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001246
Iteration 89/1000 | Loss: 0.00001246
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001245
Iteration 92/1000 | Loss: 0.00001245
Iteration 93/1000 | Loss: 0.00001245
Iteration 94/1000 | Loss: 0.00001244
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001240
Iteration 105/1000 | Loss: 0.00001240
Iteration 106/1000 | Loss: 0.00001240
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001238
Iteration 116/1000 | Loss: 0.00001238
Iteration 117/1000 | Loss: 0.00001238
Iteration 118/1000 | Loss: 0.00001238
Iteration 119/1000 | Loss: 0.00001238
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001237
Iteration 135/1000 | Loss: 0.00001237
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.2366582268441562e-05, 1.2366582268441562e-05, 1.2366582268441562e-05, 1.2366582268441562e-05, 1.2366582268441562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2366582268441562e-05

Optimization complete. Final v2v error: 2.9978604316711426 mm

Highest mean error: 3.044656753540039 mm for frame 191

Lowest mean error: 2.9725544452667236 mm for frame 41

Saving results

Total time: 34.61614680290222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886926
Iteration 2/25 | Loss: 0.00150131
Iteration 3/25 | Loss: 0.00111847
Iteration 4/25 | Loss: 0.00104129
Iteration 5/25 | Loss: 0.00102174
Iteration 6/25 | Loss: 0.00100385
Iteration 7/25 | Loss: 0.00102473
Iteration 8/25 | Loss: 0.00097837
Iteration 9/25 | Loss: 0.00093364
Iteration 10/25 | Loss: 0.00093316
Iteration 11/25 | Loss: 0.00093881
Iteration 12/25 | Loss: 0.00092593
Iteration 13/25 | Loss: 0.00089603
Iteration 14/25 | Loss: 0.00088881
Iteration 15/25 | Loss: 0.00088785
Iteration 16/25 | Loss: 0.00088751
Iteration 17/25 | Loss: 0.00088742
Iteration 18/25 | Loss: 0.00088741
Iteration 19/25 | Loss: 0.00088741
Iteration 20/25 | Loss: 0.00088741
Iteration 21/25 | Loss: 0.00088741
Iteration 22/25 | Loss: 0.00088741
Iteration 23/25 | Loss: 0.00088741
Iteration 24/25 | Loss: 0.00088741
Iteration 25/25 | Loss: 0.00088741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09778273
Iteration 2/25 | Loss: 0.00113705
Iteration 3/25 | Loss: 0.00113704
Iteration 4/25 | Loss: 0.00113704
Iteration 5/25 | Loss: 0.00113704
Iteration 6/25 | Loss: 0.00113704
Iteration 7/25 | Loss: 0.00113704
Iteration 8/25 | Loss: 0.00113704
Iteration 9/25 | Loss: 0.00113704
Iteration 10/25 | Loss: 0.00113704
Iteration 11/25 | Loss: 0.00113704
Iteration 12/25 | Loss: 0.00113704
Iteration 13/25 | Loss: 0.00113704
Iteration 14/25 | Loss: 0.00113704
Iteration 15/25 | Loss: 0.00113704
Iteration 16/25 | Loss: 0.00113704
Iteration 17/25 | Loss: 0.00113704
Iteration 18/25 | Loss: 0.00113704
Iteration 19/25 | Loss: 0.00113704
Iteration 20/25 | Loss: 0.00113704
Iteration 21/25 | Loss: 0.00113704
Iteration 22/25 | Loss: 0.00113704
Iteration 23/25 | Loss: 0.00113704
Iteration 24/25 | Loss: 0.00113704
Iteration 25/25 | Loss: 0.00113704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113704
Iteration 2/1000 | Loss: 0.00004209
Iteration 3/1000 | Loss: 0.00003183
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00002646
Iteration 6/1000 | Loss: 0.00002528
Iteration 7/1000 | Loss: 0.00002460
Iteration 8/1000 | Loss: 0.00002385
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002318
Iteration 12/1000 | Loss: 0.00002318
Iteration 13/1000 | Loss: 0.00002303
Iteration 14/1000 | Loss: 0.00002299
Iteration 15/1000 | Loss: 0.00002299
Iteration 16/1000 | Loss: 0.00002299
Iteration 17/1000 | Loss: 0.00002299
Iteration 18/1000 | Loss: 0.00002298
Iteration 19/1000 | Loss: 0.00002298
Iteration 20/1000 | Loss: 0.00002298
Iteration 21/1000 | Loss: 0.00002298
Iteration 22/1000 | Loss: 0.00002298
Iteration 23/1000 | Loss: 0.00002298
Iteration 24/1000 | Loss: 0.00002297
Iteration 25/1000 | Loss: 0.00002297
Iteration 26/1000 | Loss: 0.00002297
Iteration 27/1000 | Loss: 0.00002297
Iteration 28/1000 | Loss: 0.00002296
Iteration 29/1000 | Loss: 0.00002296
Iteration 30/1000 | Loss: 0.00002296
Iteration 31/1000 | Loss: 0.00002296
Iteration 32/1000 | Loss: 0.00002296
Iteration 33/1000 | Loss: 0.00002296
Iteration 34/1000 | Loss: 0.00002296
Iteration 35/1000 | Loss: 0.00002296
Iteration 36/1000 | Loss: 0.00002295
Iteration 37/1000 | Loss: 0.00002295
Iteration 38/1000 | Loss: 0.00002295
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00002294
Iteration 41/1000 | Loss: 0.00002294
Iteration 42/1000 | Loss: 0.00002294
Iteration 43/1000 | Loss: 0.00002293
Iteration 44/1000 | Loss: 0.00002293
Iteration 45/1000 | Loss: 0.00002293
Iteration 46/1000 | Loss: 0.00002293
Iteration 47/1000 | Loss: 0.00002293
Iteration 48/1000 | Loss: 0.00002292
Iteration 49/1000 | Loss: 0.00002292
Iteration 50/1000 | Loss: 0.00002292
Iteration 51/1000 | Loss: 0.00002292
Iteration 52/1000 | Loss: 0.00002292
Iteration 53/1000 | Loss: 0.00002291
Iteration 54/1000 | Loss: 0.00002291
Iteration 55/1000 | Loss: 0.00002290
Iteration 56/1000 | Loss: 0.00002290
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002289
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002289
Iteration 61/1000 | Loss: 0.00002289
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002288
Iteration 66/1000 | Loss: 0.00002288
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002288
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Iteration 77/1000 | Loss: 0.00002288
Iteration 78/1000 | Loss: 0.00002288
Iteration 79/1000 | Loss: 0.00002288
Iteration 80/1000 | Loss: 0.00002288
Iteration 81/1000 | Loss: 0.00002288
Iteration 82/1000 | Loss: 0.00002288
Iteration 83/1000 | Loss: 0.00002288
Iteration 84/1000 | Loss: 0.00002288
Iteration 85/1000 | Loss: 0.00002288
Iteration 86/1000 | Loss: 0.00002288
Iteration 87/1000 | Loss: 0.00002288
Iteration 88/1000 | Loss: 0.00002288
Iteration 89/1000 | Loss: 0.00002288
Iteration 90/1000 | Loss: 0.00002288
Iteration 91/1000 | Loss: 0.00002288
Iteration 92/1000 | Loss: 0.00002288
Iteration 93/1000 | Loss: 0.00002288
Iteration 94/1000 | Loss: 0.00002288
Iteration 95/1000 | Loss: 0.00002288
Iteration 96/1000 | Loss: 0.00002288
Iteration 97/1000 | Loss: 0.00002288
Iteration 98/1000 | Loss: 0.00002288
Iteration 99/1000 | Loss: 0.00002288
Iteration 100/1000 | Loss: 0.00002288
Iteration 101/1000 | Loss: 0.00002288
Iteration 102/1000 | Loss: 0.00002288
Iteration 103/1000 | Loss: 0.00002288
Iteration 104/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.2884360078023747e-05, 2.2884360078023747e-05, 2.2884360078023747e-05, 2.2884360078023747e-05, 2.2884360078023747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2884360078023747e-05

Optimization complete. Final v2v error: 4.039480686187744 mm

Highest mean error: 4.477886199951172 mm for frame 73

Lowest mean error: 3.663372039794922 mm for frame 38

Saving results

Total time: 51.54765033721924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897929
Iteration 2/25 | Loss: 0.00094855
Iteration 3/25 | Loss: 0.00081914
Iteration 4/25 | Loss: 0.00079431
Iteration 5/25 | Loss: 0.00079153
Iteration 6/25 | Loss: 0.00079080
Iteration 7/25 | Loss: 0.00079080
Iteration 8/25 | Loss: 0.00079080
Iteration 9/25 | Loss: 0.00079080
Iteration 10/25 | Loss: 0.00079080
Iteration 11/25 | Loss: 0.00079080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007907957769930363, 0.0007907957769930363, 0.0007907957769930363, 0.0007907957769930363, 0.0007907957769930363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007907957769930363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.24819708
Iteration 2/25 | Loss: 0.00112209
Iteration 3/25 | Loss: 0.00112208
Iteration 4/25 | Loss: 0.00112208
Iteration 5/25 | Loss: 0.00112208
Iteration 6/25 | Loss: 0.00112208
Iteration 7/25 | Loss: 0.00112208
Iteration 8/25 | Loss: 0.00112208
Iteration 9/25 | Loss: 0.00112208
Iteration 10/25 | Loss: 0.00112208
Iteration 11/25 | Loss: 0.00112208
Iteration 12/25 | Loss: 0.00112208
Iteration 13/25 | Loss: 0.00112208
Iteration 14/25 | Loss: 0.00112208
Iteration 15/25 | Loss: 0.00112208
Iteration 16/25 | Loss: 0.00112208
Iteration 17/25 | Loss: 0.00112208
Iteration 18/25 | Loss: 0.00112208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011220803717151284, 0.0011220803717151284, 0.0011220803717151284, 0.0011220803717151284, 0.0011220803717151284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011220803717151284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112208
Iteration 2/1000 | Loss: 0.00002996
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00002168
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001993
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00001899
Iteration 11/1000 | Loss: 0.00001895
Iteration 12/1000 | Loss: 0.00001892
Iteration 13/1000 | Loss: 0.00001885
Iteration 14/1000 | Loss: 0.00001881
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001873
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001872
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001853
Iteration 50/1000 | Loss: 0.00001853
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001843
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001841
Iteration 62/1000 | Loss: 0.00001841
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001840
Iteration 68/1000 | Loss: 0.00001840
Iteration 69/1000 | Loss: 0.00001839
Iteration 70/1000 | Loss: 0.00001839
Iteration 71/1000 | Loss: 0.00001839
Iteration 72/1000 | Loss: 0.00001839
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001838
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001838
Iteration 77/1000 | Loss: 0.00001838
Iteration 78/1000 | Loss: 0.00001837
Iteration 79/1000 | Loss: 0.00001837
Iteration 80/1000 | Loss: 0.00001837
Iteration 81/1000 | Loss: 0.00001837
Iteration 82/1000 | Loss: 0.00001836
Iteration 83/1000 | Loss: 0.00001836
Iteration 84/1000 | Loss: 0.00001836
Iteration 85/1000 | Loss: 0.00001836
Iteration 86/1000 | Loss: 0.00001836
Iteration 87/1000 | Loss: 0.00001836
Iteration 88/1000 | Loss: 0.00001836
Iteration 89/1000 | Loss: 0.00001836
Iteration 90/1000 | Loss: 0.00001836
Iteration 91/1000 | Loss: 0.00001836
Iteration 92/1000 | Loss: 0.00001836
Iteration 93/1000 | Loss: 0.00001836
Iteration 94/1000 | Loss: 0.00001836
Iteration 95/1000 | Loss: 0.00001836
Iteration 96/1000 | Loss: 0.00001836
Iteration 97/1000 | Loss: 0.00001836
Iteration 98/1000 | Loss: 0.00001836
Iteration 99/1000 | Loss: 0.00001836
Iteration 100/1000 | Loss: 0.00001836
Iteration 101/1000 | Loss: 0.00001836
Iteration 102/1000 | Loss: 0.00001836
Iteration 103/1000 | Loss: 0.00001836
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001836
Iteration 121/1000 | Loss: 0.00001836
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001836
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001836
Iteration 127/1000 | Loss: 0.00001836
Iteration 128/1000 | Loss: 0.00001836
Iteration 129/1000 | Loss: 0.00001836
Iteration 130/1000 | Loss: 0.00001836
Iteration 131/1000 | Loss: 0.00001836
Iteration 132/1000 | Loss: 0.00001836
Iteration 133/1000 | Loss: 0.00001836
Iteration 134/1000 | Loss: 0.00001836
Iteration 135/1000 | Loss: 0.00001836
Iteration 136/1000 | Loss: 0.00001836
Iteration 137/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.835712100728415e-05, 1.835712100728415e-05, 1.835712100728415e-05, 1.835712100728415e-05, 1.835712100728415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.835712100728415e-05

Optimization complete. Final v2v error: 3.6198627948760986 mm

Highest mean error: 4.089939594268799 mm for frame 199

Lowest mean error: 3.299865961074829 mm for frame 4

Saving results

Total time: 34.70272707939148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838725
Iteration 2/25 | Loss: 0.00138404
Iteration 3/25 | Loss: 0.00089274
Iteration 4/25 | Loss: 0.00082870
Iteration 5/25 | Loss: 0.00081529
Iteration 6/25 | Loss: 0.00081062
Iteration 7/25 | Loss: 0.00080909
Iteration 8/25 | Loss: 0.00080904
Iteration 9/25 | Loss: 0.00080904
Iteration 10/25 | Loss: 0.00080904
Iteration 11/25 | Loss: 0.00080904
Iteration 12/25 | Loss: 0.00080904
Iteration 13/25 | Loss: 0.00080904
Iteration 14/25 | Loss: 0.00080904
Iteration 15/25 | Loss: 0.00080904
Iteration 16/25 | Loss: 0.00080904
Iteration 17/25 | Loss: 0.00080904
Iteration 18/25 | Loss: 0.00080904
Iteration 19/25 | Loss: 0.00080904
Iteration 20/25 | Loss: 0.00080904
Iteration 21/25 | Loss: 0.00080904
Iteration 22/25 | Loss: 0.00080904
Iteration 23/25 | Loss: 0.00080904
Iteration 24/25 | Loss: 0.00080904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008090432966127992, 0.0008090432966127992, 0.0008090432966127992, 0.0008090432966127992, 0.0008090432966127992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008090432966127992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63360667
Iteration 2/25 | Loss: 0.00130890
Iteration 3/25 | Loss: 0.00130890
Iteration 4/25 | Loss: 0.00130890
Iteration 5/25 | Loss: 0.00130890
Iteration 6/25 | Loss: 0.00130890
Iteration 7/25 | Loss: 0.00130890
Iteration 8/25 | Loss: 0.00130890
Iteration 9/25 | Loss: 0.00130890
Iteration 10/25 | Loss: 0.00130890
Iteration 11/25 | Loss: 0.00130890
Iteration 12/25 | Loss: 0.00130890
Iteration 13/25 | Loss: 0.00130890
Iteration 14/25 | Loss: 0.00130890
Iteration 15/25 | Loss: 0.00130890
Iteration 16/25 | Loss: 0.00130890
Iteration 17/25 | Loss: 0.00130890
Iteration 18/25 | Loss: 0.00130890
Iteration 19/25 | Loss: 0.00130890
Iteration 20/25 | Loss: 0.00130890
Iteration 21/25 | Loss: 0.00130890
Iteration 22/25 | Loss: 0.00130890
Iteration 23/25 | Loss: 0.00130890
Iteration 24/25 | Loss: 0.00130890
Iteration 25/25 | Loss: 0.00130890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130890
Iteration 2/1000 | Loss: 0.00003343
Iteration 3/1000 | Loss: 0.00002614
Iteration 4/1000 | Loss: 0.00002376
Iteration 5/1000 | Loss: 0.00002233
Iteration 6/1000 | Loss: 0.00002129
Iteration 7/1000 | Loss: 0.00002068
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00001977
Iteration 10/1000 | Loss: 0.00001948
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001929
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001916
Iteration 16/1000 | Loss: 0.00001916
Iteration 17/1000 | Loss: 0.00001915
Iteration 18/1000 | Loss: 0.00001915
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001913
Iteration 27/1000 | Loss: 0.00001912
Iteration 28/1000 | Loss: 0.00001912
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001910
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001910
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00001909
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001906
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001904
Iteration 58/1000 | Loss: 0.00001904
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001902
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001901
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001898
Iteration 91/1000 | Loss: 0.00001898
Iteration 92/1000 | Loss: 0.00001898
Iteration 93/1000 | Loss: 0.00001898
Iteration 94/1000 | Loss: 0.00001898
Iteration 95/1000 | Loss: 0.00001898
Iteration 96/1000 | Loss: 0.00001898
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001898
Iteration 100/1000 | Loss: 0.00001898
Iteration 101/1000 | Loss: 0.00001898
Iteration 102/1000 | Loss: 0.00001898
Iteration 103/1000 | Loss: 0.00001898
Iteration 104/1000 | Loss: 0.00001898
Iteration 105/1000 | Loss: 0.00001898
Iteration 106/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.8976292267325334e-05, 1.8976292267325334e-05, 1.8976292267325334e-05, 1.8976292267325334e-05, 1.8976292267325334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8976292267325334e-05

Optimization complete. Final v2v error: 3.540827512741089 mm

Highest mean error: 4.023913860321045 mm for frame 56

Lowest mean error: 3.0066580772399902 mm for frame 0

Saving results

Total time: 39.400657653808594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058147
Iteration 2/25 | Loss: 0.00141647
Iteration 3/25 | Loss: 0.00110567
Iteration 4/25 | Loss: 0.00104274
Iteration 5/25 | Loss: 0.00102318
Iteration 6/25 | Loss: 0.00101765
Iteration 7/25 | Loss: 0.00101637
Iteration 8/25 | Loss: 0.00101635
Iteration 9/25 | Loss: 0.00101635
Iteration 10/25 | Loss: 0.00101635
Iteration 11/25 | Loss: 0.00101635
Iteration 12/25 | Loss: 0.00101635
Iteration 13/25 | Loss: 0.00101635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010163511615246534, 0.0010163511615246534, 0.0010163511615246534, 0.0010163511615246534, 0.0010163511615246534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010163511615246534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02600408
Iteration 2/25 | Loss: 0.00121716
Iteration 3/25 | Loss: 0.00121698
Iteration 4/25 | Loss: 0.00121698
Iteration 5/25 | Loss: 0.00121698
Iteration 6/25 | Loss: 0.00121698
Iteration 7/25 | Loss: 0.00121698
Iteration 8/25 | Loss: 0.00121698
Iteration 9/25 | Loss: 0.00121698
Iteration 10/25 | Loss: 0.00121698
Iteration 11/25 | Loss: 0.00121698
Iteration 12/25 | Loss: 0.00121698
Iteration 13/25 | Loss: 0.00121698
Iteration 14/25 | Loss: 0.00121698
Iteration 15/25 | Loss: 0.00121698
Iteration 16/25 | Loss: 0.00121698
Iteration 17/25 | Loss: 0.00121698
Iteration 18/25 | Loss: 0.00121698
Iteration 19/25 | Loss: 0.00121698
Iteration 20/25 | Loss: 0.00121698
Iteration 21/25 | Loss: 0.00121698
Iteration 22/25 | Loss: 0.00121698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001216975855641067, 0.001216975855641067, 0.001216975855641067, 0.001216975855641067, 0.001216975855641067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001216975855641067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121698
Iteration 2/1000 | Loss: 0.00012225
Iteration 3/1000 | Loss: 0.00008387
Iteration 4/1000 | Loss: 0.00007163
Iteration 5/1000 | Loss: 0.00006606
Iteration 6/1000 | Loss: 0.00006250
Iteration 7/1000 | Loss: 0.00005993
Iteration 8/1000 | Loss: 0.00005824
Iteration 9/1000 | Loss: 0.00005623
Iteration 10/1000 | Loss: 0.00005482
Iteration 11/1000 | Loss: 0.00005419
Iteration 12/1000 | Loss: 0.00005338
Iteration 13/1000 | Loss: 0.00005273
Iteration 14/1000 | Loss: 0.00005232
Iteration 15/1000 | Loss: 0.00005189
Iteration 16/1000 | Loss: 0.00005155
Iteration 17/1000 | Loss: 0.00005126
Iteration 18/1000 | Loss: 0.00005097
Iteration 19/1000 | Loss: 0.00005071
Iteration 20/1000 | Loss: 0.00005050
Iteration 21/1000 | Loss: 0.00005033
Iteration 22/1000 | Loss: 0.00005019
Iteration 23/1000 | Loss: 0.00005008
Iteration 24/1000 | Loss: 0.00005008
Iteration 25/1000 | Loss: 0.00005004
Iteration 26/1000 | Loss: 0.00005000
Iteration 27/1000 | Loss: 0.00005000
Iteration 28/1000 | Loss: 0.00004997
Iteration 29/1000 | Loss: 0.00004996
Iteration 30/1000 | Loss: 0.00004993
Iteration 31/1000 | Loss: 0.00004978
Iteration 32/1000 | Loss: 0.00004973
Iteration 33/1000 | Loss: 0.00004973
Iteration 34/1000 | Loss: 0.00004967
Iteration 35/1000 | Loss: 0.00004963
Iteration 36/1000 | Loss: 0.00004962
Iteration 37/1000 | Loss: 0.00004959
Iteration 38/1000 | Loss: 0.00004959
Iteration 39/1000 | Loss: 0.00004958
Iteration 40/1000 | Loss: 0.00004958
Iteration 41/1000 | Loss: 0.00004957
Iteration 42/1000 | Loss: 0.00004957
Iteration 43/1000 | Loss: 0.00004956
Iteration 44/1000 | Loss: 0.00004956
Iteration 45/1000 | Loss: 0.00004954
Iteration 46/1000 | Loss: 0.00004954
Iteration 47/1000 | Loss: 0.00004954
Iteration 48/1000 | Loss: 0.00004954
Iteration 49/1000 | Loss: 0.00004954
Iteration 50/1000 | Loss: 0.00004954
Iteration 51/1000 | Loss: 0.00004953
Iteration 52/1000 | Loss: 0.00004953
Iteration 53/1000 | Loss: 0.00004953
Iteration 54/1000 | Loss: 0.00004953
Iteration 55/1000 | Loss: 0.00004953
Iteration 56/1000 | Loss: 0.00004952
Iteration 57/1000 | Loss: 0.00004952
Iteration 58/1000 | Loss: 0.00004952
Iteration 59/1000 | Loss: 0.00004952
Iteration 60/1000 | Loss: 0.00004952
Iteration 61/1000 | Loss: 0.00004952
Iteration 62/1000 | Loss: 0.00004952
Iteration 63/1000 | Loss: 0.00004952
Iteration 64/1000 | Loss: 0.00004952
Iteration 65/1000 | Loss: 0.00004951
Iteration 66/1000 | Loss: 0.00004951
Iteration 67/1000 | Loss: 0.00004951
Iteration 68/1000 | Loss: 0.00004950
Iteration 69/1000 | Loss: 0.00004950
Iteration 70/1000 | Loss: 0.00004950
Iteration 71/1000 | Loss: 0.00004950
Iteration 72/1000 | Loss: 0.00004950
Iteration 73/1000 | Loss: 0.00004950
Iteration 74/1000 | Loss: 0.00004950
Iteration 75/1000 | Loss: 0.00004949
Iteration 76/1000 | Loss: 0.00004949
Iteration 77/1000 | Loss: 0.00004949
Iteration 78/1000 | Loss: 0.00004949
Iteration 79/1000 | Loss: 0.00004949
Iteration 80/1000 | Loss: 0.00004948
Iteration 81/1000 | Loss: 0.00004948
Iteration 82/1000 | Loss: 0.00004948
Iteration 83/1000 | Loss: 0.00004948
Iteration 84/1000 | Loss: 0.00004947
Iteration 85/1000 | Loss: 0.00004947
Iteration 86/1000 | Loss: 0.00004947
Iteration 87/1000 | Loss: 0.00004947
Iteration 88/1000 | Loss: 0.00004947
Iteration 89/1000 | Loss: 0.00004946
Iteration 90/1000 | Loss: 0.00004946
Iteration 91/1000 | Loss: 0.00004946
Iteration 92/1000 | Loss: 0.00004946
Iteration 93/1000 | Loss: 0.00004946
Iteration 94/1000 | Loss: 0.00004946
Iteration 95/1000 | Loss: 0.00004946
Iteration 96/1000 | Loss: 0.00004945
Iteration 97/1000 | Loss: 0.00004945
Iteration 98/1000 | Loss: 0.00004945
Iteration 99/1000 | Loss: 0.00004945
Iteration 100/1000 | Loss: 0.00004945
Iteration 101/1000 | Loss: 0.00004945
Iteration 102/1000 | Loss: 0.00004944
Iteration 103/1000 | Loss: 0.00004944
Iteration 104/1000 | Loss: 0.00004944
Iteration 105/1000 | Loss: 0.00004944
Iteration 106/1000 | Loss: 0.00004944
Iteration 107/1000 | Loss: 0.00004944
Iteration 108/1000 | Loss: 0.00004944
Iteration 109/1000 | Loss: 0.00004944
Iteration 110/1000 | Loss: 0.00004943
Iteration 111/1000 | Loss: 0.00004943
Iteration 112/1000 | Loss: 0.00004943
Iteration 113/1000 | Loss: 0.00004943
Iteration 114/1000 | Loss: 0.00004943
Iteration 115/1000 | Loss: 0.00004943
Iteration 116/1000 | Loss: 0.00004943
Iteration 117/1000 | Loss: 0.00004943
Iteration 118/1000 | Loss: 0.00004942
Iteration 119/1000 | Loss: 0.00004942
Iteration 120/1000 | Loss: 0.00004942
Iteration 121/1000 | Loss: 0.00004942
Iteration 122/1000 | Loss: 0.00004942
Iteration 123/1000 | Loss: 0.00004942
Iteration 124/1000 | Loss: 0.00004942
Iteration 125/1000 | Loss: 0.00004941
Iteration 126/1000 | Loss: 0.00004941
Iteration 127/1000 | Loss: 0.00004941
Iteration 128/1000 | Loss: 0.00004941
Iteration 129/1000 | Loss: 0.00004941
Iteration 130/1000 | Loss: 0.00004941
Iteration 131/1000 | Loss: 0.00004941
Iteration 132/1000 | Loss: 0.00004941
Iteration 133/1000 | Loss: 0.00004940
Iteration 134/1000 | Loss: 0.00004940
Iteration 135/1000 | Loss: 0.00004940
Iteration 136/1000 | Loss: 0.00004940
Iteration 137/1000 | Loss: 0.00004940
Iteration 138/1000 | Loss: 0.00004940
Iteration 139/1000 | Loss: 0.00004940
Iteration 140/1000 | Loss: 0.00004939
Iteration 141/1000 | Loss: 0.00004939
Iteration 142/1000 | Loss: 0.00004939
Iteration 143/1000 | Loss: 0.00004939
Iteration 144/1000 | Loss: 0.00004939
Iteration 145/1000 | Loss: 0.00004939
Iteration 146/1000 | Loss: 0.00004939
Iteration 147/1000 | Loss: 0.00004939
Iteration 148/1000 | Loss: 0.00004939
Iteration 149/1000 | Loss: 0.00004938
Iteration 150/1000 | Loss: 0.00004938
Iteration 151/1000 | Loss: 0.00004938
Iteration 152/1000 | Loss: 0.00004938
Iteration 153/1000 | Loss: 0.00004938
Iteration 154/1000 | Loss: 0.00004938
Iteration 155/1000 | Loss: 0.00004938
Iteration 156/1000 | Loss: 0.00004938
Iteration 157/1000 | Loss: 0.00004938
Iteration 158/1000 | Loss: 0.00004937
Iteration 159/1000 | Loss: 0.00004937
Iteration 160/1000 | Loss: 0.00004937
Iteration 161/1000 | Loss: 0.00004937
Iteration 162/1000 | Loss: 0.00004937
Iteration 163/1000 | Loss: 0.00004937
Iteration 164/1000 | Loss: 0.00004937
Iteration 165/1000 | Loss: 0.00004937
Iteration 166/1000 | Loss: 0.00004936
Iteration 167/1000 | Loss: 0.00004936
Iteration 168/1000 | Loss: 0.00004936
Iteration 169/1000 | Loss: 0.00004936
Iteration 170/1000 | Loss: 0.00004936
Iteration 171/1000 | Loss: 0.00004936
Iteration 172/1000 | Loss: 0.00004935
Iteration 173/1000 | Loss: 0.00004935
Iteration 174/1000 | Loss: 0.00004935
Iteration 175/1000 | Loss: 0.00004935
Iteration 176/1000 | Loss: 0.00004935
Iteration 177/1000 | Loss: 0.00004935
Iteration 178/1000 | Loss: 0.00004935
Iteration 179/1000 | Loss: 0.00004935
Iteration 180/1000 | Loss: 0.00004935
Iteration 181/1000 | Loss: 0.00004935
Iteration 182/1000 | Loss: 0.00004935
Iteration 183/1000 | Loss: 0.00004935
Iteration 184/1000 | Loss: 0.00004935
Iteration 185/1000 | Loss: 0.00004935
Iteration 186/1000 | Loss: 0.00004935
Iteration 187/1000 | Loss: 0.00004935
Iteration 188/1000 | Loss: 0.00004935
Iteration 189/1000 | Loss: 0.00004934
Iteration 190/1000 | Loss: 0.00004934
Iteration 191/1000 | Loss: 0.00004934
Iteration 192/1000 | Loss: 0.00004934
Iteration 193/1000 | Loss: 0.00004934
Iteration 194/1000 | Loss: 0.00004934
Iteration 195/1000 | Loss: 0.00004934
Iteration 196/1000 | Loss: 0.00004934
Iteration 197/1000 | Loss: 0.00004934
Iteration 198/1000 | Loss: 0.00004934
Iteration 199/1000 | Loss: 0.00004934
Iteration 200/1000 | Loss: 0.00004934
Iteration 201/1000 | Loss: 0.00004934
Iteration 202/1000 | Loss: 0.00004934
Iteration 203/1000 | Loss: 0.00004934
Iteration 204/1000 | Loss: 0.00004933
Iteration 205/1000 | Loss: 0.00004933
Iteration 206/1000 | Loss: 0.00004933
Iteration 207/1000 | Loss: 0.00004933
Iteration 208/1000 | Loss: 0.00004933
Iteration 209/1000 | Loss: 0.00004933
Iteration 210/1000 | Loss: 0.00004933
Iteration 211/1000 | Loss: 0.00004933
Iteration 212/1000 | Loss: 0.00004933
Iteration 213/1000 | Loss: 0.00004933
Iteration 214/1000 | Loss: 0.00004933
Iteration 215/1000 | Loss: 0.00004933
Iteration 216/1000 | Loss: 0.00004932
Iteration 217/1000 | Loss: 0.00004932
Iteration 218/1000 | Loss: 0.00004932
Iteration 219/1000 | Loss: 0.00004932
Iteration 220/1000 | Loss: 0.00004932
Iteration 221/1000 | Loss: 0.00004932
Iteration 222/1000 | Loss: 0.00004932
Iteration 223/1000 | Loss: 0.00004932
Iteration 224/1000 | Loss: 0.00004932
Iteration 225/1000 | Loss: 0.00004932
Iteration 226/1000 | Loss: 0.00004932
Iteration 227/1000 | Loss: 0.00004932
Iteration 228/1000 | Loss: 0.00004932
Iteration 229/1000 | Loss: 0.00004932
Iteration 230/1000 | Loss: 0.00004931
Iteration 231/1000 | Loss: 0.00004931
Iteration 232/1000 | Loss: 0.00004931
Iteration 233/1000 | Loss: 0.00004931
Iteration 234/1000 | Loss: 0.00004931
Iteration 235/1000 | Loss: 0.00004931
Iteration 236/1000 | Loss: 0.00004931
Iteration 237/1000 | Loss: 0.00004931
Iteration 238/1000 | Loss: 0.00004931
Iteration 239/1000 | Loss: 0.00004931
Iteration 240/1000 | Loss: 0.00004931
Iteration 241/1000 | Loss: 0.00004931
Iteration 242/1000 | Loss: 0.00004931
Iteration 243/1000 | Loss: 0.00004931
Iteration 244/1000 | Loss: 0.00004931
Iteration 245/1000 | Loss: 0.00004931
Iteration 246/1000 | Loss: 0.00004931
Iteration 247/1000 | Loss: 0.00004931
Iteration 248/1000 | Loss: 0.00004931
Iteration 249/1000 | Loss: 0.00004931
Iteration 250/1000 | Loss: 0.00004931
Iteration 251/1000 | Loss: 0.00004930
Iteration 252/1000 | Loss: 0.00004930
Iteration 253/1000 | Loss: 0.00004930
Iteration 254/1000 | Loss: 0.00004930
Iteration 255/1000 | Loss: 0.00004930
Iteration 256/1000 | Loss: 0.00004930
Iteration 257/1000 | Loss: 0.00004930
Iteration 258/1000 | Loss: 0.00004930
Iteration 259/1000 | Loss: 0.00004930
Iteration 260/1000 | Loss: 0.00004930
Iteration 261/1000 | Loss: 0.00004930
Iteration 262/1000 | Loss: 0.00004930
Iteration 263/1000 | Loss: 0.00004930
Iteration 264/1000 | Loss: 0.00004930
Iteration 265/1000 | Loss: 0.00004930
Iteration 266/1000 | Loss: 0.00004930
Iteration 267/1000 | Loss: 0.00004930
Iteration 268/1000 | Loss: 0.00004930
Iteration 269/1000 | Loss: 0.00004930
Iteration 270/1000 | Loss: 0.00004929
Iteration 271/1000 | Loss: 0.00004929
Iteration 272/1000 | Loss: 0.00004929
Iteration 273/1000 | Loss: 0.00004929
Iteration 274/1000 | Loss: 0.00004929
Iteration 275/1000 | Loss: 0.00004929
Iteration 276/1000 | Loss: 0.00004929
Iteration 277/1000 | Loss: 0.00004929
Iteration 278/1000 | Loss: 0.00004929
Iteration 279/1000 | Loss: 0.00004929
Iteration 280/1000 | Loss: 0.00004929
Iteration 281/1000 | Loss: 0.00004929
Iteration 282/1000 | Loss: 0.00004929
Iteration 283/1000 | Loss: 0.00004929
Iteration 284/1000 | Loss: 0.00004929
Iteration 285/1000 | Loss: 0.00004929
Iteration 286/1000 | Loss: 0.00004929
Iteration 287/1000 | Loss: 0.00004929
Iteration 288/1000 | Loss: 0.00004929
Iteration 289/1000 | Loss: 0.00004929
Iteration 290/1000 | Loss: 0.00004929
Iteration 291/1000 | Loss: 0.00004929
Iteration 292/1000 | Loss: 0.00004929
Iteration 293/1000 | Loss: 0.00004929
Iteration 294/1000 | Loss: 0.00004929
Iteration 295/1000 | Loss: 0.00004929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [4.929343776893802e-05, 4.929343776893802e-05, 4.929343776893802e-05, 4.929343776893802e-05, 4.929343776893802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.929343776893802e-05

Optimization complete. Final v2v error: 5.7746968269348145 mm

Highest mean error: 6.344963550567627 mm for frame 181

Lowest mean error: 5.111320972442627 mm for frame 151

Saving results

Total time: 73.99983167648315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838580
Iteration 2/25 | Loss: 0.00101032
Iteration 3/25 | Loss: 0.00078841
Iteration 4/25 | Loss: 0.00074483
Iteration 5/25 | Loss: 0.00073668
Iteration 6/25 | Loss: 0.00073518
Iteration 7/25 | Loss: 0.00073518
Iteration 8/25 | Loss: 0.00073518
Iteration 9/25 | Loss: 0.00073518
Iteration 10/25 | Loss: 0.00073518
Iteration 11/25 | Loss: 0.00073518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007351817330345511, 0.0007351817330345511, 0.0007351817330345511, 0.0007351817330345511, 0.0007351817330345511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007351817330345511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60334456
Iteration 2/25 | Loss: 0.00130675
Iteration 3/25 | Loss: 0.00130675
Iteration 4/25 | Loss: 0.00130675
Iteration 5/25 | Loss: 0.00130675
Iteration 6/25 | Loss: 0.00130675
Iteration 7/25 | Loss: 0.00130675
Iteration 8/25 | Loss: 0.00130675
Iteration 9/25 | Loss: 0.00130675
Iteration 10/25 | Loss: 0.00130675
Iteration 11/25 | Loss: 0.00130675
Iteration 12/25 | Loss: 0.00130675
Iteration 13/25 | Loss: 0.00130675
Iteration 14/25 | Loss: 0.00130675
Iteration 15/25 | Loss: 0.00130675
Iteration 16/25 | Loss: 0.00130675
Iteration 17/25 | Loss: 0.00130675
Iteration 18/25 | Loss: 0.00130675
Iteration 19/25 | Loss: 0.00130675
Iteration 20/25 | Loss: 0.00130675
Iteration 21/25 | Loss: 0.00130675
Iteration 22/25 | Loss: 0.00130675
Iteration 23/25 | Loss: 0.00130675
Iteration 24/25 | Loss: 0.00130675
Iteration 25/25 | Loss: 0.00130675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130675
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001472
Iteration 4/1000 | Loss: 0.00001345
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001213
Iteration 7/1000 | Loss: 0.00001183
Iteration 8/1000 | Loss: 0.00001156
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001105
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001100
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001099
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001090
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [1.0896547792071942e-05, 1.0896547792071942e-05, 1.0896547792071942e-05, 1.0896547792071942e-05, 1.0896547792071942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0896547792071942e-05

Optimization complete. Final v2v error: 2.813800573348999 mm

Highest mean error: 2.96366024017334 mm for frame 209

Lowest mean error: 2.7079246044158936 mm for frame 15

Saving results

Total time: 32.24762272834778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477752
Iteration 2/25 | Loss: 0.00103369
Iteration 3/25 | Loss: 0.00086775
Iteration 4/25 | Loss: 0.00084128
Iteration 5/25 | Loss: 0.00083078
Iteration 6/25 | Loss: 0.00082814
Iteration 7/25 | Loss: 0.00082795
Iteration 8/25 | Loss: 0.00082795
Iteration 9/25 | Loss: 0.00082795
Iteration 10/25 | Loss: 0.00082795
Iteration 11/25 | Loss: 0.00082795
Iteration 12/25 | Loss: 0.00082795
Iteration 13/25 | Loss: 0.00082795
Iteration 14/25 | Loss: 0.00082795
Iteration 15/25 | Loss: 0.00082795
Iteration 16/25 | Loss: 0.00082795
Iteration 17/25 | Loss: 0.00082795
Iteration 18/25 | Loss: 0.00082795
Iteration 19/25 | Loss: 0.00082795
Iteration 20/25 | Loss: 0.00082795
Iteration 21/25 | Loss: 0.00082795
Iteration 22/25 | Loss: 0.00082795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008279481553472579, 0.0008279481553472579, 0.0008279481553472579, 0.0008279481553472579, 0.0008279481553472579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008279481553472579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55536556
Iteration 2/25 | Loss: 0.00119734
Iteration 3/25 | Loss: 0.00119734
Iteration 4/25 | Loss: 0.00119734
Iteration 5/25 | Loss: 0.00119734
Iteration 6/25 | Loss: 0.00119734
Iteration 7/25 | Loss: 0.00119734
Iteration 8/25 | Loss: 0.00119734
Iteration 9/25 | Loss: 0.00119734
Iteration 10/25 | Loss: 0.00119734
Iteration 11/25 | Loss: 0.00119734
Iteration 12/25 | Loss: 0.00119734
Iteration 13/25 | Loss: 0.00119734
Iteration 14/25 | Loss: 0.00119734
Iteration 15/25 | Loss: 0.00119734
Iteration 16/25 | Loss: 0.00119734
Iteration 17/25 | Loss: 0.00119734
Iteration 18/25 | Loss: 0.00119734
Iteration 19/25 | Loss: 0.00119734
Iteration 20/25 | Loss: 0.00119734
Iteration 21/25 | Loss: 0.00119734
Iteration 22/25 | Loss: 0.00119734
Iteration 23/25 | Loss: 0.00119734
Iteration 24/25 | Loss: 0.00119734
Iteration 25/25 | Loss: 0.00119734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119734
Iteration 2/1000 | Loss: 0.00002729
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001949
Iteration 8/1000 | Loss: 0.00001920
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001880
Iteration 11/1000 | Loss: 0.00001880
Iteration 12/1000 | Loss: 0.00001877
Iteration 13/1000 | Loss: 0.00001876
Iteration 14/1000 | Loss: 0.00001876
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001872
Iteration 17/1000 | Loss: 0.00001866
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001860
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00001858
Iteration 23/1000 | Loss: 0.00001858
Iteration 24/1000 | Loss: 0.00001857
Iteration 25/1000 | Loss: 0.00001857
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00001856
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001855
Iteration 32/1000 | Loss: 0.00001855
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001854
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001852
Iteration 47/1000 | Loss: 0.00001852
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001851
Iteration 50/1000 | Loss: 0.00001851
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001850
Iteration 54/1000 | Loss: 0.00001849
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001843
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.841433549998328e-05, 1.841433549998328e-05, 1.841433549998328e-05, 1.841433549998328e-05, 1.841433549998328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.841433549998328e-05

Optimization complete. Final v2v error: 3.5717523097991943 mm

Highest mean error: 3.7855327129364014 mm for frame 147

Lowest mean error: 3.435401678085327 mm for frame 198

Saving results

Total time: 34.82279419898987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574866
Iteration 2/25 | Loss: 0.00111757
Iteration 3/25 | Loss: 0.00088473
Iteration 4/25 | Loss: 0.00084534
Iteration 5/25 | Loss: 0.00083728
Iteration 6/25 | Loss: 0.00083539
Iteration 7/25 | Loss: 0.00083520
Iteration 8/25 | Loss: 0.00083520
Iteration 9/25 | Loss: 0.00083520
Iteration 10/25 | Loss: 0.00083520
Iteration 11/25 | Loss: 0.00083520
Iteration 12/25 | Loss: 0.00083520
Iteration 13/25 | Loss: 0.00083520
Iteration 14/25 | Loss: 0.00083520
Iteration 15/25 | Loss: 0.00083520
Iteration 16/25 | Loss: 0.00083520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008351983851753175, 0.0008351983851753175, 0.0008351983851753175, 0.0008351983851753175, 0.0008351983851753175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008351983851753175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.90751553
Iteration 2/25 | Loss: 0.00084744
Iteration 3/25 | Loss: 0.00084734
Iteration 4/25 | Loss: 0.00084734
Iteration 5/25 | Loss: 0.00084734
Iteration 6/25 | Loss: 0.00084734
Iteration 7/25 | Loss: 0.00084733
Iteration 8/25 | Loss: 0.00084733
Iteration 9/25 | Loss: 0.00084733
Iteration 10/25 | Loss: 0.00084733
Iteration 11/25 | Loss: 0.00084733
Iteration 12/25 | Loss: 0.00084733
Iteration 13/25 | Loss: 0.00084733
Iteration 14/25 | Loss: 0.00084733
Iteration 15/25 | Loss: 0.00084733
Iteration 16/25 | Loss: 0.00084733
Iteration 17/25 | Loss: 0.00084733
Iteration 18/25 | Loss: 0.00084733
Iteration 19/25 | Loss: 0.00084733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008473340421915054, 0.0008473340421915054, 0.0008473340421915054, 0.0008473340421915054, 0.0008473340421915054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008473340421915054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084733
Iteration 2/1000 | Loss: 0.00003300
Iteration 3/1000 | Loss: 0.00002354
Iteration 4/1000 | Loss: 0.00002176
Iteration 5/1000 | Loss: 0.00002080
Iteration 6/1000 | Loss: 0.00002025
Iteration 7/1000 | Loss: 0.00001985
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001932
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001926
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001898
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001896
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001895
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001893
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001893
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001893
Iteration 45/1000 | Loss: 0.00001893
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001891
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001890
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001887
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001884
Iteration 89/1000 | Loss: 0.00001884
Iteration 90/1000 | Loss: 0.00001884
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001883
Iteration 93/1000 | Loss: 0.00001883
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001880
Iteration 96/1000 | Loss: 0.00001880
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001877
Iteration 104/1000 | Loss: 0.00001877
Iteration 105/1000 | Loss: 0.00001877
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001875
Iteration 109/1000 | Loss: 0.00001875
Iteration 110/1000 | Loss: 0.00001875
Iteration 111/1000 | Loss: 0.00001875
Iteration 112/1000 | Loss: 0.00001875
Iteration 113/1000 | Loss: 0.00001874
Iteration 114/1000 | Loss: 0.00001874
Iteration 115/1000 | Loss: 0.00001874
Iteration 116/1000 | Loss: 0.00001874
Iteration 117/1000 | Loss: 0.00001874
Iteration 118/1000 | Loss: 0.00001874
Iteration 119/1000 | Loss: 0.00001874
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001873
Iteration 122/1000 | Loss: 0.00001873
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001872
Iteration 126/1000 | Loss: 0.00001872
Iteration 127/1000 | Loss: 0.00001872
Iteration 128/1000 | Loss: 0.00001871
Iteration 129/1000 | Loss: 0.00001871
Iteration 130/1000 | Loss: 0.00001871
Iteration 131/1000 | Loss: 0.00001871
Iteration 132/1000 | Loss: 0.00001871
Iteration 133/1000 | Loss: 0.00001871
Iteration 134/1000 | Loss: 0.00001871
Iteration 135/1000 | Loss: 0.00001871
Iteration 136/1000 | Loss: 0.00001871
Iteration 137/1000 | Loss: 0.00001871
Iteration 138/1000 | Loss: 0.00001871
Iteration 139/1000 | Loss: 0.00001871
Iteration 140/1000 | Loss: 0.00001871
Iteration 141/1000 | Loss: 0.00001871
Iteration 142/1000 | Loss: 0.00001870
Iteration 143/1000 | Loss: 0.00001870
Iteration 144/1000 | Loss: 0.00001870
Iteration 145/1000 | Loss: 0.00001870
Iteration 146/1000 | Loss: 0.00001870
Iteration 147/1000 | Loss: 0.00001870
Iteration 148/1000 | Loss: 0.00001870
Iteration 149/1000 | Loss: 0.00001870
Iteration 150/1000 | Loss: 0.00001870
Iteration 151/1000 | Loss: 0.00001870
Iteration 152/1000 | Loss: 0.00001870
Iteration 153/1000 | Loss: 0.00001870
Iteration 154/1000 | Loss: 0.00001870
Iteration 155/1000 | Loss: 0.00001870
Iteration 156/1000 | Loss: 0.00001870
Iteration 157/1000 | Loss: 0.00001870
Iteration 158/1000 | Loss: 0.00001870
Iteration 159/1000 | Loss: 0.00001870
Iteration 160/1000 | Loss: 0.00001870
Iteration 161/1000 | Loss: 0.00001870
Iteration 162/1000 | Loss: 0.00001870
Iteration 163/1000 | Loss: 0.00001870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.8696364350034855e-05, 1.8696364350034855e-05, 1.8696364350034855e-05, 1.8696364350034855e-05, 1.8696364350034855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8696364350034855e-05

Optimization complete. Final v2v error: 3.6534557342529297 mm

Highest mean error: 3.952331304550171 mm for frame 228

Lowest mean error: 3.440764904022217 mm for frame 2

Saving results

Total time: 40.44479250907898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858713
Iteration 2/25 | Loss: 0.00121717
Iteration 3/25 | Loss: 0.00091555
Iteration 4/25 | Loss: 0.00085354
Iteration 5/25 | Loss: 0.00082341
Iteration 6/25 | Loss: 0.00081209
Iteration 7/25 | Loss: 0.00081279
Iteration 8/25 | Loss: 0.00080925
Iteration 9/25 | Loss: 0.00081199
Iteration 10/25 | Loss: 0.00081001
Iteration 11/25 | Loss: 0.00080756
Iteration 12/25 | Loss: 0.00080732
Iteration 13/25 | Loss: 0.00080721
Iteration 14/25 | Loss: 0.00080712
Iteration 15/25 | Loss: 0.00080680
Iteration 16/25 | Loss: 0.00081200
Iteration 17/25 | Loss: 0.00080308
Iteration 18/25 | Loss: 0.00080135
Iteration 19/25 | Loss: 0.00080074
Iteration 20/25 | Loss: 0.00080070
Iteration 21/25 | Loss: 0.00080070
Iteration 22/25 | Loss: 0.00080356
Iteration 23/25 | Loss: 0.00080069
Iteration 24/25 | Loss: 0.00080068
Iteration 25/25 | Loss: 0.00080068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16507483
Iteration 2/25 | Loss: 0.00125671
Iteration 3/25 | Loss: 0.00125671
Iteration 4/25 | Loss: 0.00125671
Iteration 5/25 | Loss: 0.00125670
Iteration 6/25 | Loss: 0.00125670
Iteration 7/25 | Loss: 0.00125670
Iteration 8/25 | Loss: 0.00125670
Iteration 9/25 | Loss: 0.00125670
Iteration 10/25 | Loss: 0.00125670
Iteration 11/25 | Loss: 0.00125670
Iteration 12/25 | Loss: 0.00125670
Iteration 13/25 | Loss: 0.00125670
Iteration 14/25 | Loss: 0.00125670
Iteration 15/25 | Loss: 0.00125670
Iteration 16/25 | Loss: 0.00125670
Iteration 17/25 | Loss: 0.00125670
Iteration 18/25 | Loss: 0.00125670
Iteration 19/25 | Loss: 0.00125670
Iteration 20/25 | Loss: 0.00125670
Iteration 21/25 | Loss: 0.00125670
Iteration 22/25 | Loss: 0.00125670
Iteration 23/25 | Loss: 0.00125670
Iteration 24/25 | Loss: 0.00125670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012567030498757958, 0.0012567030498757958, 0.0012567030498757958, 0.0012567030498757958, 0.0012567030498757958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012567030498757958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125670
Iteration 2/1000 | Loss: 0.00004177
Iteration 3/1000 | Loss: 0.00002629
Iteration 4/1000 | Loss: 0.00002167
Iteration 5/1000 | Loss: 0.00002000
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001838
Iteration 8/1000 | Loss: 0.00001792
Iteration 9/1000 | Loss: 0.00001760
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001723
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001705
Iteration 18/1000 | Loss: 0.00001705
Iteration 19/1000 | Loss: 0.00001699
Iteration 20/1000 | Loss: 0.00001696
Iteration 21/1000 | Loss: 0.00001694
Iteration 22/1000 | Loss: 0.00001692
Iteration 23/1000 | Loss: 0.00001691
Iteration 24/1000 | Loss: 0.00001691
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001689
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001684
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001676
Iteration 36/1000 | Loss: 0.00001676
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001675
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00001674
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001674
Iteration 43/1000 | Loss: 0.00001673
Iteration 44/1000 | Loss: 0.00001673
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001671
Iteration 51/1000 | Loss: 0.00001671
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001669
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001667
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00001667
Iteration 67/1000 | Loss: 0.00001666
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001666
Iteration 70/1000 | Loss: 0.00001666
Iteration 71/1000 | Loss: 0.00001666
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001665
Iteration 75/1000 | Loss: 0.00001665
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001664
Iteration 92/1000 | Loss: 0.00001664
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001663
Iteration 96/1000 | Loss: 0.00001663
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001663
Iteration 105/1000 | Loss: 0.00001663
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001662
Iteration 110/1000 | Loss: 0.00001662
Iteration 111/1000 | Loss: 0.00001662
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001662
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001662
Iteration 117/1000 | Loss: 0.00001661
Iteration 118/1000 | Loss: 0.00001661
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001661
Iteration 126/1000 | Loss: 0.00001661
Iteration 127/1000 | Loss: 0.00001661
Iteration 128/1000 | Loss: 0.00001661
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Iteration 145/1000 | Loss: 0.00001660
Iteration 146/1000 | Loss: 0.00001660
Iteration 147/1000 | Loss: 0.00001660
Iteration 148/1000 | Loss: 0.00001660
Iteration 149/1000 | Loss: 0.00001660
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.659442023083102e-05, 1.659442023083102e-05, 1.659442023083102e-05, 1.659442023083102e-05, 1.659442023083102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.659442023083102e-05

Optimization complete. Final v2v error: 3.4250261783599854 mm

Highest mean error: 4.127299785614014 mm for frame 82

Lowest mean error: 2.9360101222991943 mm for frame 125

Saving results

Total time: 62.190959453582764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615393
Iteration 2/25 | Loss: 0.00104757
Iteration 3/25 | Loss: 0.00085290
Iteration 4/25 | Loss: 0.00080607
Iteration 5/25 | Loss: 0.00079639
Iteration 6/25 | Loss: 0.00079403
Iteration 7/25 | Loss: 0.00079365
Iteration 8/25 | Loss: 0.00079365
Iteration 9/25 | Loss: 0.00079365
Iteration 10/25 | Loss: 0.00079365
Iteration 11/25 | Loss: 0.00079365
Iteration 12/25 | Loss: 0.00079365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000793652085121721, 0.000793652085121721, 0.000793652085121721, 0.000793652085121721, 0.000793652085121721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000793652085121721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00399518
Iteration 2/25 | Loss: 0.00127932
Iteration 3/25 | Loss: 0.00127932
Iteration 4/25 | Loss: 0.00127932
Iteration 5/25 | Loss: 0.00127932
Iteration 6/25 | Loss: 0.00127932
Iteration 7/25 | Loss: 0.00127932
Iteration 8/25 | Loss: 0.00127932
Iteration 9/25 | Loss: 0.00127932
Iteration 10/25 | Loss: 0.00127932
Iteration 11/25 | Loss: 0.00127932
Iteration 12/25 | Loss: 0.00127932
Iteration 13/25 | Loss: 0.00127932
Iteration 14/25 | Loss: 0.00127932
Iteration 15/25 | Loss: 0.00127932
Iteration 16/25 | Loss: 0.00127932
Iteration 17/25 | Loss: 0.00127932
Iteration 18/25 | Loss: 0.00127932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012793177738785744, 0.0012793177738785744, 0.0012793177738785744, 0.0012793177738785744, 0.0012793177738785744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012793177738785744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127932
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001635
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001591
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001586
Iteration 13/1000 | Loss: 0.00001584
Iteration 14/1000 | Loss: 0.00001584
Iteration 15/1000 | Loss: 0.00001583
Iteration 16/1000 | Loss: 0.00001579
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001574
Iteration 21/1000 | Loss: 0.00001574
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001572
Iteration 26/1000 | Loss: 0.00001571
Iteration 27/1000 | Loss: 0.00001571
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001564
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001562
Iteration 42/1000 | Loss: 0.00001562
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001561
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001553
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001551
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001550
Iteration 110/1000 | Loss: 0.00001550
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001548
Iteration 121/1000 | Loss: 0.00001548
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001546
Iteration 130/1000 | Loss: 0.00001546
Iteration 131/1000 | Loss: 0.00001546
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001545
Iteration 139/1000 | Loss: 0.00001545
Iteration 140/1000 | Loss: 0.00001545
Iteration 141/1000 | Loss: 0.00001545
Iteration 142/1000 | Loss: 0.00001545
Iteration 143/1000 | Loss: 0.00001545
Iteration 144/1000 | Loss: 0.00001545
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001544
Iteration 150/1000 | Loss: 0.00001544
Iteration 151/1000 | Loss: 0.00001544
Iteration 152/1000 | Loss: 0.00001544
Iteration 153/1000 | Loss: 0.00001544
Iteration 154/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.5444822565768845e-05, 1.5444822565768845e-05, 1.5444822565768845e-05, 1.5444822565768845e-05, 1.5444822565768845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5444822565768845e-05

Optimization complete. Final v2v error: 3.3088080883026123 mm

Highest mean error: 3.6444544792175293 mm for frame 167

Lowest mean error: 3.051837921142578 mm for frame 117

Saving results

Total time: 40.59969902038574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859586
Iteration 2/25 | Loss: 0.00091974
Iteration 3/25 | Loss: 0.00081629
Iteration 4/25 | Loss: 0.00078096
Iteration 5/25 | Loss: 0.00077198
Iteration 6/25 | Loss: 0.00076989
Iteration 7/25 | Loss: 0.00076925
Iteration 8/25 | Loss: 0.00076925
Iteration 9/25 | Loss: 0.00076925
Iteration 10/25 | Loss: 0.00076925
Iteration 11/25 | Loss: 0.00076925
Iteration 12/25 | Loss: 0.00076925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007692486396990716, 0.0007692486396990716, 0.0007692486396990716, 0.0007692486396990716, 0.0007692486396990716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007692486396990716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60175884
Iteration 2/25 | Loss: 0.00122415
Iteration 3/25 | Loss: 0.00122415
Iteration 4/25 | Loss: 0.00122415
Iteration 5/25 | Loss: 0.00122415
Iteration 6/25 | Loss: 0.00122415
Iteration 7/25 | Loss: 0.00122415
Iteration 8/25 | Loss: 0.00122415
Iteration 9/25 | Loss: 0.00122415
Iteration 10/25 | Loss: 0.00122415
Iteration 11/25 | Loss: 0.00122415
Iteration 12/25 | Loss: 0.00122415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012241497170180082, 0.0012241497170180082, 0.0012241497170180082, 0.0012241497170180082, 0.0012241497170180082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012241497170180082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122415
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00002232
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001840
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001689
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001685
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001685
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001678
Iteration 23/1000 | Loss: 0.00001676
Iteration 24/1000 | Loss: 0.00001676
Iteration 25/1000 | Loss: 0.00001676
Iteration 26/1000 | Loss: 0.00001676
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001676
Iteration 30/1000 | Loss: 0.00001676
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001667
Iteration 59/1000 | Loss: 0.00001666
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001666
Iteration 62/1000 | Loss: 0.00001666
Iteration 63/1000 | Loss: 0.00001666
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001665
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001664
Iteration 74/1000 | Loss: 0.00001664
Iteration 75/1000 | Loss: 0.00001664
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001663
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001663
Iteration 90/1000 | Loss: 0.00001663
Iteration 91/1000 | Loss: 0.00001663
Iteration 92/1000 | Loss: 0.00001663
Iteration 93/1000 | Loss: 0.00001663
Iteration 94/1000 | Loss: 0.00001663
Iteration 95/1000 | Loss: 0.00001663
Iteration 96/1000 | Loss: 0.00001663
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.662968315940816e-05, 1.662968315940816e-05, 1.662968315940816e-05, 1.662968315940816e-05, 1.662968315940816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.662968315940816e-05

Optimization complete. Final v2v error: 3.4139339923858643 mm

Highest mean error: 3.512484550476074 mm for frame 65

Lowest mean error: 3.328948974609375 mm for frame 53

Saving results

Total time: 27.717966556549072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371745
Iteration 2/25 | Loss: 0.00116121
Iteration 3/25 | Loss: 0.00086045
Iteration 4/25 | Loss: 0.00077717
Iteration 5/25 | Loss: 0.00076243
Iteration 6/25 | Loss: 0.00076072
Iteration 7/25 | Loss: 0.00076037
Iteration 8/25 | Loss: 0.00076037
Iteration 9/25 | Loss: 0.00076037
Iteration 10/25 | Loss: 0.00076037
Iteration 11/25 | Loss: 0.00076037
Iteration 12/25 | Loss: 0.00076037
Iteration 13/25 | Loss: 0.00076037
Iteration 14/25 | Loss: 0.00076037
Iteration 15/25 | Loss: 0.00076037
Iteration 16/25 | Loss: 0.00076037
Iteration 17/25 | Loss: 0.00076037
Iteration 18/25 | Loss: 0.00076037
Iteration 19/25 | Loss: 0.00076037
Iteration 20/25 | Loss: 0.00076037
Iteration 21/25 | Loss: 0.00076037
Iteration 22/25 | Loss: 0.00076037
Iteration 23/25 | Loss: 0.00076037
Iteration 24/25 | Loss: 0.00076037
Iteration 25/25 | Loss: 0.00076037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70900345
Iteration 2/25 | Loss: 0.00143045
Iteration 3/25 | Loss: 0.00143045
Iteration 4/25 | Loss: 0.00143045
Iteration 5/25 | Loss: 0.00143044
Iteration 6/25 | Loss: 0.00143044
Iteration 7/25 | Loss: 0.00143044
Iteration 8/25 | Loss: 0.00143044
Iteration 9/25 | Loss: 0.00143044
Iteration 10/25 | Loss: 0.00143044
Iteration 11/25 | Loss: 0.00143044
Iteration 12/25 | Loss: 0.00143044
Iteration 13/25 | Loss: 0.00143044
Iteration 14/25 | Loss: 0.00143044
Iteration 15/25 | Loss: 0.00143044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00143044360447675, 0.00143044360447675, 0.00143044360447675, 0.00143044360447675, 0.00143044360447675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00143044360447675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143044
Iteration 2/1000 | Loss: 0.00003728
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001343
Iteration 7/1000 | Loss: 0.00001282
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001206
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001180
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001179
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001164
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001152
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001150
Iteration 139/1000 | Loss: 0.00001150
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001149
Iteration 144/1000 | Loss: 0.00001149
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001149
Iteration 147/1000 | Loss: 0.00001149
Iteration 148/1000 | Loss: 0.00001149
Iteration 149/1000 | Loss: 0.00001149
Iteration 150/1000 | Loss: 0.00001149
Iteration 151/1000 | Loss: 0.00001149
Iteration 152/1000 | Loss: 0.00001149
Iteration 153/1000 | Loss: 0.00001149
Iteration 154/1000 | Loss: 0.00001148
Iteration 155/1000 | Loss: 0.00001148
Iteration 156/1000 | Loss: 0.00001148
Iteration 157/1000 | Loss: 0.00001148
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001147
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001146
Iteration 168/1000 | Loss: 0.00001146
Iteration 169/1000 | Loss: 0.00001146
Iteration 170/1000 | Loss: 0.00001146
Iteration 171/1000 | Loss: 0.00001146
Iteration 172/1000 | Loss: 0.00001146
Iteration 173/1000 | Loss: 0.00001146
Iteration 174/1000 | Loss: 0.00001146
Iteration 175/1000 | Loss: 0.00001146
Iteration 176/1000 | Loss: 0.00001146
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001145
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Iteration 189/1000 | Loss: 0.00001145
Iteration 190/1000 | Loss: 0.00001145
Iteration 191/1000 | Loss: 0.00001145
Iteration 192/1000 | Loss: 0.00001145
Iteration 193/1000 | Loss: 0.00001145
Iteration 194/1000 | Loss: 0.00001145
Iteration 195/1000 | Loss: 0.00001145
Iteration 196/1000 | Loss: 0.00001145
Iteration 197/1000 | Loss: 0.00001145
Iteration 198/1000 | Loss: 0.00001145
Iteration 199/1000 | Loss: 0.00001145
Iteration 200/1000 | Loss: 0.00001145
Iteration 201/1000 | Loss: 0.00001145
Iteration 202/1000 | Loss: 0.00001145
Iteration 203/1000 | Loss: 0.00001145
Iteration 204/1000 | Loss: 0.00001144
Iteration 205/1000 | Loss: 0.00001144
Iteration 206/1000 | Loss: 0.00001144
Iteration 207/1000 | Loss: 0.00001144
Iteration 208/1000 | Loss: 0.00001144
Iteration 209/1000 | Loss: 0.00001144
Iteration 210/1000 | Loss: 0.00001144
Iteration 211/1000 | Loss: 0.00001144
Iteration 212/1000 | Loss: 0.00001144
Iteration 213/1000 | Loss: 0.00001144
Iteration 214/1000 | Loss: 0.00001144
Iteration 215/1000 | Loss: 0.00001144
Iteration 216/1000 | Loss: 0.00001144
Iteration 217/1000 | Loss: 0.00001144
Iteration 218/1000 | Loss: 0.00001144
Iteration 219/1000 | Loss: 0.00001144
Iteration 220/1000 | Loss: 0.00001144
Iteration 221/1000 | Loss: 0.00001144
Iteration 222/1000 | Loss: 0.00001144
Iteration 223/1000 | Loss: 0.00001144
Iteration 224/1000 | Loss: 0.00001144
Iteration 225/1000 | Loss: 0.00001144
Iteration 226/1000 | Loss: 0.00001143
Iteration 227/1000 | Loss: 0.00001143
Iteration 228/1000 | Loss: 0.00001143
Iteration 229/1000 | Loss: 0.00001143
Iteration 230/1000 | Loss: 0.00001143
Iteration 231/1000 | Loss: 0.00001143
Iteration 232/1000 | Loss: 0.00001143
Iteration 233/1000 | Loss: 0.00001143
Iteration 234/1000 | Loss: 0.00001143
Iteration 235/1000 | Loss: 0.00001143
Iteration 236/1000 | Loss: 0.00001143
Iteration 237/1000 | Loss: 0.00001143
Iteration 238/1000 | Loss: 0.00001143
Iteration 239/1000 | Loss: 0.00001143
Iteration 240/1000 | Loss: 0.00001143
Iteration 241/1000 | Loss: 0.00001143
Iteration 242/1000 | Loss: 0.00001143
Iteration 243/1000 | Loss: 0.00001143
Iteration 244/1000 | Loss: 0.00001143
Iteration 245/1000 | Loss: 0.00001143
Iteration 246/1000 | Loss: 0.00001142
Iteration 247/1000 | Loss: 0.00001142
Iteration 248/1000 | Loss: 0.00001142
Iteration 249/1000 | Loss: 0.00001142
Iteration 250/1000 | Loss: 0.00001142
Iteration 251/1000 | Loss: 0.00001142
Iteration 252/1000 | Loss: 0.00001142
Iteration 253/1000 | Loss: 0.00001142
Iteration 254/1000 | Loss: 0.00001142
Iteration 255/1000 | Loss: 0.00001142
Iteration 256/1000 | Loss: 0.00001142
Iteration 257/1000 | Loss: 0.00001142
Iteration 258/1000 | Loss: 0.00001142
Iteration 259/1000 | Loss: 0.00001142
Iteration 260/1000 | Loss: 0.00001142
Iteration 261/1000 | Loss: 0.00001142
Iteration 262/1000 | Loss: 0.00001142
Iteration 263/1000 | Loss: 0.00001142
Iteration 264/1000 | Loss: 0.00001142
Iteration 265/1000 | Loss: 0.00001142
Iteration 266/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.141906341217691e-05, 1.141906341217691e-05, 1.141906341217691e-05, 1.141906341217691e-05, 1.141906341217691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.141906341217691e-05

Optimization complete. Final v2v error: 2.9006757736206055 mm

Highest mean error: 3.074329376220703 mm for frame 41

Lowest mean error: 2.731020212173462 mm for frame 134

Saving results

Total time: 42.214370012283325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886733
Iteration 2/25 | Loss: 0.00090933
Iteration 3/25 | Loss: 0.00076856
Iteration 4/25 | Loss: 0.00075106
Iteration 5/25 | Loss: 0.00074856
Iteration 6/25 | Loss: 0.00074777
Iteration 7/25 | Loss: 0.00074768
Iteration 8/25 | Loss: 0.00074768
Iteration 9/25 | Loss: 0.00074768
Iteration 10/25 | Loss: 0.00074768
Iteration 11/25 | Loss: 0.00074768
Iteration 12/25 | Loss: 0.00074768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007476847968064249, 0.0007476847968064249, 0.0007476847968064249, 0.0007476847968064249, 0.0007476847968064249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007476847968064249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99057829
Iteration 2/25 | Loss: 0.00122121
Iteration 3/25 | Loss: 0.00122121
Iteration 4/25 | Loss: 0.00122121
Iteration 5/25 | Loss: 0.00122121
Iteration 6/25 | Loss: 0.00122121
Iteration 7/25 | Loss: 0.00122121
Iteration 8/25 | Loss: 0.00122121
Iteration 9/25 | Loss: 0.00122121
Iteration 10/25 | Loss: 0.00122121
Iteration 11/25 | Loss: 0.00122121
Iteration 12/25 | Loss: 0.00122121
Iteration 13/25 | Loss: 0.00122121
Iteration 14/25 | Loss: 0.00122121
Iteration 15/25 | Loss: 0.00122121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012212068540975451, 0.0012212068540975451, 0.0012212068540975451, 0.0012212068540975451, 0.0012212068540975451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012212068540975451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122121
Iteration 2/1000 | Loss: 0.00002347
Iteration 3/1000 | Loss: 0.00001544
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001390
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001261
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001259
Iteration 38/1000 | Loss: 0.00001259
Iteration 39/1000 | Loss: 0.00001258
Iteration 40/1000 | Loss: 0.00001258
Iteration 41/1000 | Loss: 0.00001258
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001257
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001256
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001254
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001253
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001253
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001252
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001251
Iteration 64/1000 | Loss: 0.00001251
Iteration 65/1000 | Loss: 0.00001251
Iteration 66/1000 | Loss: 0.00001251
Iteration 67/1000 | Loss: 0.00001251
Iteration 68/1000 | Loss: 0.00001251
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001251
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001249
Iteration 75/1000 | Loss: 0.00001249
Iteration 76/1000 | Loss: 0.00001248
Iteration 77/1000 | Loss: 0.00001248
Iteration 78/1000 | Loss: 0.00001248
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001247
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001246
Iteration 84/1000 | Loss: 0.00001246
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001245
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001236
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001236
Iteration 129/1000 | Loss: 0.00001236
Iteration 130/1000 | Loss: 0.00001236
Iteration 131/1000 | Loss: 0.00001236
Iteration 132/1000 | Loss: 0.00001236
Iteration 133/1000 | Loss: 0.00001236
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001235
Iteration 140/1000 | Loss: 0.00001235
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001235
Iteration 143/1000 | Loss: 0.00001235
Iteration 144/1000 | Loss: 0.00001235
Iteration 145/1000 | Loss: 0.00001235
Iteration 146/1000 | Loss: 0.00001235
Iteration 147/1000 | Loss: 0.00001234
Iteration 148/1000 | Loss: 0.00001234
Iteration 149/1000 | Loss: 0.00001234
Iteration 150/1000 | Loss: 0.00001234
Iteration 151/1000 | Loss: 0.00001234
Iteration 152/1000 | Loss: 0.00001234
Iteration 153/1000 | Loss: 0.00001234
Iteration 154/1000 | Loss: 0.00001234
Iteration 155/1000 | Loss: 0.00001234
Iteration 156/1000 | Loss: 0.00001234
Iteration 157/1000 | Loss: 0.00001234
Iteration 158/1000 | Loss: 0.00001233
Iteration 159/1000 | Loss: 0.00001233
Iteration 160/1000 | Loss: 0.00001233
Iteration 161/1000 | Loss: 0.00001233
Iteration 162/1000 | Loss: 0.00001233
Iteration 163/1000 | Loss: 0.00001233
Iteration 164/1000 | Loss: 0.00001233
Iteration 165/1000 | Loss: 0.00001233
Iteration 166/1000 | Loss: 0.00001233
Iteration 167/1000 | Loss: 0.00001233
Iteration 168/1000 | Loss: 0.00001233
Iteration 169/1000 | Loss: 0.00001233
Iteration 170/1000 | Loss: 0.00001232
Iteration 171/1000 | Loss: 0.00001232
Iteration 172/1000 | Loss: 0.00001232
Iteration 173/1000 | Loss: 0.00001232
Iteration 174/1000 | Loss: 0.00001232
Iteration 175/1000 | Loss: 0.00001232
Iteration 176/1000 | Loss: 0.00001232
Iteration 177/1000 | Loss: 0.00001232
Iteration 178/1000 | Loss: 0.00001231
Iteration 179/1000 | Loss: 0.00001231
Iteration 180/1000 | Loss: 0.00001231
Iteration 181/1000 | Loss: 0.00001231
Iteration 182/1000 | Loss: 0.00001231
Iteration 183/1000 | Loss: 0.00001231
Iteration 184/1000 | Loss: 0.00001231
Iteration 185/1000 | Loss: 0.00001231
Iteration 186/1000 | Loss: 0.00001231
Iteration 187/1000 | Loss: 0.00001231
Iteration 188/1000 | Loss: 0.00001231
Iteration 189/1000 | Loss: 0.00001231
Iteration 190/1000 | Loss: 0.00001230
Iteration 191/1000 | Loss: 0.00001230
Iteration 192/1000 | Loss: 0.00001230
Iteration 193/1000 | Loss: 0.00001230
Iteration 194/1000 | Loss: 0.00001230
Iteration 195/1000 | Loss: 0.00001230
Iteration 196/1000 | Loss: 0.00001230
Iteration 197/1000 | Loss: 0.00001230
Iteration 198/1000 | Loss: 0.00001230
Iteration 199/1000 | Loss: 0.00001230
Iteration 200/1000 | Loss: 0.00001230
Iteration 201/1000 | Loss: 0.00001230
Iteration 202/1000 | Loss: 0.00001230
Iteration 203/1000 | Loss: 0.00001230
Iteration 204/1000 | Loss: 0.00001230
Iteration 205/1000 | Loss: 0.00001230
Iteration 206/1000 | Loss: 0.00001230
Iteration 207/1000 | Loss: 0.00001230
Iteration 208/1000 | Loss: 0.00001230
Iteration 209/1000 | Loss: 0.00001230
Iteration 210/1000 | Loss: 0.00001230
Iteration 211/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.2304963092901744e-05, 1.2304963092901744e-05, 1.2304963092901744e-05, 1.2304963092901744e-05, 1.2304963092901744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2304963092901744e-05

Optimization complete. Final v2v error: 2.9682934284210205 mm

Highest mean error: 3.4541878700256348 mm for frame 63

Lowest mean error: 2.822835683822632 mm for frame 99

Saving results

Total time: 35.79047966003418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997943
Iteration 2/25 | Loss: 0.00192329
Iteration 3/25 | Loss: 0.00107315
Iteration 4/25 | Loss: 0.00103152
Iteration 5/25 | Loss: 0.00102112
Iteration 6/25 | Loss: 0.00101781
Iteration 7/25 | Loss: 0.00101705
Iteration 8/25 | Loss: 0.00101700
Iteration 9/25 | Loss: 0.00101700
Iteration 10/25 | Loss: 0.00101700
Iteration 11/25 | Loss: 0.00101700
Iteration 12/25 | Loss: 0.00101700
Iteration 13/25 | Loss: 0.00101700
Iteration 14/25 | Loss: 0.00101700
Iteration 15/25 | Loss: 0.00101700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010169981978833675, 0.0010169981978833675, 0.0010169981978833675, 0.0010169981978833675, 0.0010169981978833675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010169981978833675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25699568
Iteration 2/25 | Loss: 0.00047487
Iteration 3/25 | Loss: 0.00047487
Iteration 4/25 | Loss: 0.00047487
Iteration 5/25 | Loss: 0.00047487
Iteration 6/25 | Loss: 0.00047487
Iteration 7/25 | Loss: 0.00047487
Iteration 8/25 | Loss: 0.00047487
Iteration 9/25 | Loss: 0.00047487
Iteration 10/25 | Loss: 0.00047487
Iteration 11/25 | Loss: 0.00047487
Iteration 12/25 | Loss: 0.00047487
Iteration 13/25 | Loss: 0.00047487
Iteration 14/25 | Loss: 0.00047487
Iteration 15/25 | Loss: 0.00047487
Iteration 16/25 | Loss: 0.00047487
Iteration 17/25 | Loss: 0.00047487
Iteration 18/25 | Loss: 0.00047487
Iteration 19/25 | Loss: 0.00047487
Iteration 20/25 | Loss: 0.00047487
Iteration 21/25 | Loss: 0.00047487
Iteration 22/25 | Loss: 0.00047487
Iteration 23/25 | Loss: 0.00047487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00047486581024713814, 0.00047486581024713814, 0.00047486581024713814, 0.00047486581024713814, 0.00047486581024713814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047486581024713814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047487
Iteration 2/1000 | Loss: 0.00006609
Iteration 3/1000 | Loss: 0.00004523
Iteration 4/1000 | Loss: 0.00003817
Iteration 5/1000 | Loss: 0.00003586
Iteration 6/1000 | Loss: 0.00003448
Iteration 7/1000 | Loss: 0.00003368
Iteration 8/1000 | Loss: 0.00003303
Iteration 9/1000 | Loss: 0.00003228
Iteration 10/1000 | Loss: 0.00003174
Iteration 11/1000 | Loss: 0.00003133
Iteration 12/1000 | Loss: 0.00003098
Iteration 13/1000 | Loss: 0.00003071
Iteration 14/1000 | Loss: 0.00003048
Iteration 15/1000 | Loss: 0.00003027
Iteration 16/1000 | Loss: 0.00003010
Iteration 17/1000 | Loss: 0.00003005
Iteration 18/1000 | Loss: 0.00002992
Iteration 19/1000 | Loss: 0.00002989
Iteration 20/1000 | Loss: 0.00002974
Iteration 21/1000 | Loss: 0.00002971
Iteration 22/1000 | Loss: 0.00002963
Iteration 23/1000 | Loss: 0.00002962
Iteration 24/1000 | Loss: 0.00002961
Iteration 25/1000 | Loss: 0.00002960
Iteration 26/1000 | Loss: 0.00002959
Iteration 27/1000 | Loss: 0.00002956
Iteration 28/1000 | Loss: 0.00002955
Iteration 29/1000 | Loss: 0.00002955
Iteration 30/1000 | Loss: 0.00002955
Iteration 31/1000 | Loss: 0.00002951
Iteration 32/1000 | Loss: 0.00002947
Iteration 33/1000 | Loss: 0.00002946
Iteration 34/1000 | Loss: 0.00002946
Iteration 35/1000 | Loss: 0.00002945
Iteration 36/1000 | Loss: 0.00002944
Iteration 37/1000 | Loss: 0.00002943
Iteration 38/1000 | Loss: 0.00002942
Iteration 39/1000 | Loss: 0.00002942
Iteration 40/1000 | Loss: 0.00002942
Iteration 41/1000 | Loss: 0.00002942
Iteration 42/1000 | Loss: 0.00002942
Iteration 43/1000 | Loss: 0.00002942
Iteration 44/1000 | Loss: 0.00002942
Iteration 45/1000 | Loss: 0.00002942
Iteration 46/1000 | Loss: 0.00002940
Iteration 47/1000 | Loss: 0.00002940
Iteration 48/1000 | Loss: 0.00002940
Iteration 49/1000 | Loss: 0.00002940
Iteration 50/1000 | Loss: 0.00002940
Iteration 51/1000 | Loss: 0.00002939
Iteration 52/1000 | Loss: 0.00002939
Iteration 53/1000 | Loss: 0.00002939
Iteration 54/1000 | Loss: 0.00002939
Iteration 55/1000 | Loss: 0.00002939
Iteration 56/1000 | Loss: 0.00002939
Iteration 57/1000 | Loss: 0.00002939
Iteration 58/1000 | Loss: 0.00002939
Iteration 59/1000 | Loss: 0.00002938
Iteration 60/1000 | Loss: 0.00002938
Iteration 61/1000 | Loss: 0.00002936
Iteration 62/1000 | Loss: 0.00002936
Iteration 63/1000 | Loss: 0.00002936
Iteration 64/1000 | Loss: 0.00002934
Iteration 65/1000 | Loss: 0.00002934
Iteration 66/1000 | Loss: 0.00002934
Iteration 67/1000 | Loss: 0.00002933
Iteration 68/1000 | Loss: 0.00002933
Iteration 69/1000 | Loss: 0.00002933
Iteration 70/1000 | Loss: 0.00002933
Iteration 71/1000 | Loss: 0.00002932
Iteration 72/1000 | Loss: 0.00002931
Iteration 73/1000 | Loss: 0.00002931
Iteration 74/1000 | Loss: 0.00002931
Iteration 75/1000 | Loss: 0.00002931
Iteration 76/1000 | Loss: 0.00002930
Iteration 77/1000 | Loss: 0.00002930
Iteration 78/1000 | Loss: 0.00002930
Iteration 79/1000 | Loss: 0.00002930
Iteration 80/1000 | Loss: 0.00002930
Iteration 81/1000 | Loss: 0.00002930
Iteration 82/1000 | Loss: 0.00002928
Iteration 83/1000 | Loss: 0.00002928
Iteration 84/1000 | Loss: 0.00002927
Iteration 85/1000 | Loss: 0.00002927
Iteration 86/1000 | Loss: 0.00002927
Iteration 87/1000 | Loss: 0.00002926
Iteration 88/1000 | Loss: 0.00002926
Iteration 89/1000 | Loss: 0.00002926
Iteration 90/1000 | Loss: 0.00002926
Iteration 91/1000 | Loss: 0.00002926
Iteration 92/1000 | Loss: 0.00002926
Iteration 93/1000 | Loss: 0.00002926
Iteration 94/1000 | Loss: 0.00002926
Iteration 95/1000 | Loss: 0.00002926
Iteration 96/1000 | Loss: 0.00002925
Iteration 97/1000 | Loss: 0.00002925
Iteration 98/1000 | Loss: 0.00002925
Iteration 99/1000 | Loss: 0.00002925
Iteration 100/1000 | Loss: 0.00002925
Iteration 101/1000 | Loss: 0.00002925
Iteration 102/1000 | Loss: 0.00002925
Iteration 103/1000 | Loss: 0.00002925
Iteration 104/1000 | Loss: 0.00002925
Iteration 105/1000 | Loss: 0.00002924
Iteration 106/1000 | Loss: 0.00002924
Iteration 107/1000 | Loss: 0.00002924
Iteration 108/1000 | Loss: 0.00002924
Iteration 109/1000 | Loss: 0.00002924
Iteration 110/1000 | Loss: 0.00002924
Iteration 111/1000 | Loss: 0.00002924
Iteration 112/1000 | Loss: 0.00002924
Iteration 113/1000 | Loss: 0.00002923
Iteration 114/1000 | Loss: 0.00002923
Iteration 115/1000 | Loss: 0.00002923
Iteration 116/1000 | Loss: 0.00002923
Iteration 117/1000 | Loss: 0.00002923
Iteration 118/1000 | Loss: 0.00002923
Iteration 119/1000 | Loss: 0.00002923
Iteration 120/1000 | Loss: 0.00002923
Iteration 121/1000 | Loss: 0.00002923
Iteration 122/1000 | Loss: 0.00002921
Iteration 123/1000 | Loss: 0.00002921
Iteration 124/1000 | Loss: 0.00002921
Iteration 125/1000 | Loss: 0.00002921
Iteration 126/1000 | Loss: 0.00002921
Iteration 127/1000 | Loss: 0.00002921
Iteration 128/1000 | Loss: 0.00002921
Iteration 129/1000 | Loss: 0.00002921
Iteration 130/1000 | Loss: 0.00002921
Iteration 131/1000 | Loss: 0.00002920
Iteration 132/1000 | Loss: 0.00002920
Iteration 133/1000 | Loss: 0.00002920
Iteration 134/1000 | Loss: 0.00002920
Iteration 135/1000 | Loss: 0.00002920
Iteration 136/1000 | Loss: 0.00002920
Iteration 137/1000 | Loss: 0.00002920
Iteration 138/1000 | Loss: 0.00002919
Iteration 139/1000 | Loss: 0.00002919
Iteration 140/1000 | Loss: 0.00002919
Iteration 141/1000 | Loss: 0.00002919
Iteration 142/1000 | Loss: 0.00002919
Iteration 143/1000 | Loss: 0.00002919
Iteration 144/1000 | Loss: 0.00002919
Iteration 145/1000 | Loss: 0.00002919
Iteration 146/1000 | Loss: 0.00002919
Iteration 147/1000 | Loss: 0.00002919
Iteration 148/1000 | Loss: 0.00002918
Iteration 149/1000 | Loss: 0.00002918
Iteration 150/1000 | Loss: 0.00002918
Iteration 151/1000 | Loss: 0.00002918
Iteration 152/1000 | Loss: 0.00002918
Iteration 153/1000 | Loss: 0.00002918
Iteration 154/1000 | Loss: 0.00002918
Iteration 155/1000 | Loss: 0.00002918
Iteration 156/1000 | Loss: 0.00002918
Iteration 157/1000 | Loss: 0.00002918
Iteration 158/1000 | Loss: 0.00002918
Iteration 159/1000 | Loss: 0.00002918
Iteration 160/1000 | Loss: 0.00002918
Iteration 161/1000 | Loss: 0.00002918
Iteration 162/1000 | Loss: 0.00002918
Iteration 163/1000 | Loss: 0.00002917
Iteration 164/1000 | Loss: 0.00002917
Iteration 165/1000 | Loss: 0.00002917
Iteration 166/1000 | Loss: 0.00002917
Iteration 167/1000 | Loss: 0.00002917
Iteration 168/1000 | Loss: 0.00002917
Iteration 169/1000 | Loss: 0.00002916
Iteration 170/1000 | Loss: 0.00002916
Iteration 171/1000 | Loss: 0.00002916
Iteration 172/1000 | Loss: 0.00002915
Iteration 173/1000 | Loss: 0.00002915
Iteration 174/1000 | Loss: 0.00002915
Iteration 175/1000 | Loss: 0.00002914
Iteration 176/1000 | Loss: 0.00002914
Iteration 177/1000 | Loss: 0.00002913
Iteration 178/1000 | Loss: 0.00002913
Iteration 179/1000 | Loss: 0.00002913
Iteration 180/1000 | Loss: 0.00002913
Iteration 181/1000 | Loss: 0.00002913
Iteration 182/1000 | Loss: 0.00002913
Iteration 183/1000 | Loss: 0.00002913
Iteration 184/1000 | Loss: 0.00002913
Iteration 185/1000 | Loss: 0.00002913
Iteration 186/1000 | Loss: 0.00002913
Iteration 187/1000 | Loss: 0.00002913
Iteration 188/1000 | Loss: 0.00002912
Iteration 189/1000 | Loss: 0.00002912
Iteration 190/1000 | Loss: 0.00002912
Iteration 191/1000 | Loss: 0.00002912
Iteration 192/1000 | Loss: 0.00002912
Iteration 193/1000 | Loss: 0.00002912
Iteration 194/1000 | Loss: 0.00002912
Iteration 195/1000 | Loss: 0.00002912
Iteration 196/1000 | Loss: 0.00002912
Iteration 197/1000 | Loss: 0.00002911
Iteration 198/1000 | Loss: 0.00002911
Iteration 199/1000 | Loss: 0.00002911
Iteration 200/1000 | Loss: 0.00002911
Iteration 201/1000 | Loss: 0.00002911
Iteration 202/1000 | Loss: 0.00002911
Iteration 203/1000 | Loss: 0.00002911
Iteration 204/1000 | Loss: 0.00002911
Iteration 205/1000 | Loss: 0.00002910
Iteration 206/1000 | Loss: 0.00002910
Iteration 207/1000 | Loss: 0.00002910
Iteration 208/1000 | Loss: 0.00002910
Iteration 209/1000 | Loss: 0.00002910
Iteration 210/1000 | Loss: 0.00002910
Iteration 211/1000 | Loss: 0.00002910
Iteration 212/1000 | Loss: 0.00002910
Iteration 213/1000 | Loss: 0.00002909
Iteration 214/1000 | Loss: 0.00002909
Iteration 215/1000 | Loss: 0.00002909
Iteration 216/1000 | Loss: 0.00002909
Iteration 217/1000 | Loss: 0.00002909
Iteration 218/1000 | Loss: 0.00002909
Iteration 219/1000 | Loss: 0.00002909
Iteration 220/1000 | Loss: 0.00002909
Iteration 221/1000 | Loss: 0.00002909
Iteration 222/1000 | Loss: 0.00002909
Iteration 223/1000 | Loss: 0.00002909
Iteration 224/1000 | Loss: 0.00002909
Iteration 225/1000 | Loss: 0.00002909
Iteration 226/1000 | Loss: 0.00002909
Iteration 227/1000 | Loss: 0.00002908
Iteration 228/1000 | Loss: 0.00002908
Iteration 229/1000 | Loss: 0.00002908
Iteration 230/1000 | Loss: 0.00002908
Iteration 231/1000 | Loss: 0.00002908
Iteration 232/1000 | Loss: 0.00002908
Iteration 233/1000 | Loss: 0.00002908
Iteration 234/1000 | Loss: 0.00002908
Iteration 235/1000 | Loss: 0.00002908
Iteration 236/1000 | Loss: 0.00002908
Iteration 237/1000 | Loss: 0.00002908
Iteration 238/1000 | Loss: 0.00002908
Iteration 239/1000 | Loss: 0.00002908
Iteration 240/1000 | Loss: 0.00002908
Iteration 241/1000 | Loss: 0.00002908
Iteration 242/1000 | Loss: 0.00002908
Iteration 243/1000 | Loss: 0.00002908
Iteration 244/1000 | Loss: 0.00002908
Iteration 245/1000 | Loss: 0.00002908
Iteration 246/1000 | Loss: 0.00002908
Iteration 247/1000 | Loss: 0.00002908
Iteration 248/1000 | Loss: 0.00002908
Iteration 249/1000 | Loss: 0.00002908
Iteration 250/1000 | Loss: 0.00002908
Iteration 251/1000 | Loss: 0.00002908
Iteration 252/1000 | Loss: 0.00002908
Iteration 253/1000 | Loss: 0.00002908
Iteration 254/1000 | Loss: 0.00002908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.9079463274683803e-05, 2.9079463274683803e-05, 2.9079463274683803e-05, 2.9079463274683803e-05, 2.9079463274683803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9079463274683803e-05

Optimization complete. Final v2v error: 4.330851078033447 mm

Highest mean error: 5.118936538696289 mm for frame 90

Lowest mean error: 3.9021174907684326 mm for frame 14

Saving results

Total time: 55.66407084465027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_009/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_009/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822824
Iteration 2/25 | Loss: 0.00121918
Iteration 3/25 | Loss: 0.00095993
Iteration 4/25 | Loss: 0.00089015
Iteration 5/25 | Loss: 0.00086687
Iteration 6/25 | Loss: 0.00085317
Iteration 7/25 | Loss: 0.00083746
Iteration 8/25 | Loss: 0.00083123
Iteration 9/25 | Loss: 0.00082765
Iteration 10/25 | Loss: 0.00083567
Iteration 11/25 | Loss: 0.00084650
Iteration 12/25 | Loss: 0.00083441
Iteration 13/25 | Loss: 0.00083707
Iteration 14/25 | Loss: 0.00082731
Iteration 15/25 | Loss: 0.00082615
Iteration 16/25 | Loss: 0.00081859
Iteration 17/25 | Loss: 0.00082015
Iteration 18/25 | Loss: 0.00081580
Iteration 19/25 | Loss: 0.00081333
Iteration 20/25 | Loss: 0.00081715
Iteration 21/25 | Loss: 0.00081572
Iteration 22/25 | Loss: 0.00081303
Iteration 23/25 | Loss: 0.00081499
Iteration 24/25 | Loss: 0.00081370
Iteration 25/25 | Loss: 0.00081062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64549553
Iteration 2/25 | Loss: 0.00138586
Iteration 3/25 | Loss: 0.00138584
Iteration 4/25 | Loss: 0.00138584
Iteration 5/25 | Loss: 0.00138584
Iteration 6/25 | Loss: 0.00138584
Iteration 7/25 | Loss: 0.00138584
Iteration 8/25 | Loss: 0.00138584
Iteration 9/25 | Loss: 0.00138584
Iteration 10/25 | Loss: 0.00138584
Iteration 11/25 | Loss: 0.00138584
Iteration 12/25 | Loss: 0.00138584
Iteration 13/25 | Loss: 0.00138584
Iteration 14/25 | Loss: 0.00138584
Iteration 15/25 | Loss: 0.00138584
Iteration 16/25 | Loss: 0.00138584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013858381425961852, 0.0013858381425961852, 0.0013858381425961852, 0.0013858381425961852, 0.0013858381425961852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013858381425961852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138584
Iteration 2/1000 | Loss: 0.00031095
Iteration 3/1000 | Loss: 0.00053957
Iteration 4/1000 | Loss: 0.00005640
Iteration 5/1000 | Loss: 0.00004034
Iteration 6/1000 | Loss: 0.00049462
Iteration 7/1000 | Loss: 0.00038810
Iteration 8/1000 | Loss: 0.00003608
Iteration 9/1000 | Loss: 0.00003105
Iteration 10/1000 | Loss: 0.00002875
Iteration 11/1000 | Loss: 0.00028402
Iteration 12/1000 | Loss: 0.00027223
Iteration 13/1000 | Loss: 0.00030046
Iteration 14/1000 | Loss: 0.00030132
Iteration 15/1000 | Loss: 0.00013705
Iteration 16/1000 | Loss: 0.00003298
Iteration 17/1000 | Loss: 0.00013928
Iteration 18/1000 | Loss: 0.00014100
Iteration 19/1000 | Loss: 0.00016563
Iteration 20/1000 | Loss: 0.00025718
Iteration 21/1000 | Loss: 0.00004836
Iteration 22/1000 | Loss: 0.00055372
Iteration 23/1000 | Loss: 0.00051277
Iteration 24/1000 | Loss: 0.00061840
Iteration 25/1000 | Loss: 0.00021307
Iteration 26/1000 | Loss: 0.00019204
Iteration 27/1000 | Loss: 0.00004438
Iteration 28/1000 | Loss: 0.00003314
Iteration 29/1000 | Loss: 0.00002959
Iteration 30/1000 | Loss: 0.00024649
Iteration 31/1000 | Loss: 0.00018119
Iteration 32/1000 | Loss: 0.00033364
Iteration 33/1000 | Loss: 0.00031818
Iteration 34/1000 | Loss: 0.00014885
Iteration 35/1000 | Loss: 0.00019270
Iteration 36/1000 | Loss: 0.00002646
Iteration 37/1000 | Loss: 0.00015246
Iteration 38/1000 | Loss: 0.00012470
Iteration 39/1000 | Loss: 0.00014416
Iteration 40/1000 | Loss: 0.00010492
Iteration 41/1000 | Loss: 0.00003508
Iteration 42/1000 | Loss: 0.00018489
Iteration 43/1000 | Loss: 0.00006481
Iteration 44/1000 | Loss: 0.00002946
Iteration 45/1000 | Loss: 0.00002703
Iteration 46/1000 | Loss: 0.00002524
Iteration 47/1000 | Loss: 0.00002441
Iteration 48/1000 | Loss: 0.00002408
Iteration 49/1000 | Loss: 0.00002401
Iteration 50/1000 | Loss: 0.00002375
Iteration 51/1000 | Loss: 0.00091415
Iteration 52/1000 | Loss: 0.00030082
Iteration 53/1000 | Loss: 0.00041632
Iteration 54/1000 | Loss: 0.00028415
Iteration 55/1000 | Loss: 0.00013408
Iteration 56/1000 | Loss: 0.00013421
Iteration 57/1000 | Loss: 0.00025022
Iteration 58/1000 | Loss: 0.00015123
Iteration 59/1000 | Loss: 0.00003259
Iteration 60/1000 | Loss: 0.00014400
Iteration 61/1000 | Loss: 0.00010870
Iteration 62/1000 | Loss: 0.00008724
Iteration 63/1000 | Loss: 0.00046399
Iteration 64/1000 | Loss: 0.00021502
Iteration 65/1000 | Loss: 0.00017536
Iteration 66/1000 | Loss: 0.00003059
Iteration 67/1000 | Loss: 0.00009332
Iteration 68/1000 | Loss: 0.00002811
Iteration 69/1000 | Loss: 0.00021628
Iteration 70/1000 | Loss: 0.00022902
Iteration 71/1000 | Loss: 0.00004580
Iteration 72/1000 | Loss: 0.00003358
Iteration 73/1000 | Loss: 0.00003093
Iteration 74/1000 | Loss: 0.00075738
Iteration 75/1000 | Loss: 0.00011342
Iteration 76/1000 | Loss: 0.00004148
Iteration 77/1000 | Loss: 0.00003017
Iteration 78/1000 | Loss: 0.00002789
Iteration 79/1000 | Loss: 0.00002678
Iteration 80/1000 | Loss: 0.00002615
Iteration 81/1000 | Loss: 0.00002578
Iteration 82/1000 | Loss: 0.00016190
Iteration 83/1000 | Loss: 0.00006507
Iteration 84/1000 | Loss: 0.00012059
Iteration 85/1000 | Loss: 0.00002738
Iteration 86/1000 | Loss: 0.00002679
Iteration 87/1000 | Loss: 0.00002430
Iteration 88/1000 | Loss: 0.00002312
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002275
Iteration 91/1000 | Loss: 0.00002123
Iteration 92/1000 | Loss: 0.00002093
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002064
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002018
Iteration 98/1000 | Loss: 0.00002012
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002010
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002007
Iteration 105/1000 | Loss: 0.00002007
Iteration 106/1000 | Loss: 0.00002007
Iteration 107/1000 | Loss: 0.00002007
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002005
Iteration 112/1000 | Loss: 0.00002005
Iteration 113/1000 | Loss: 0.00002005
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002004
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002003
Iteration 118/1000 | Loss: 0.00002003
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002001
Iteration 124/1000 | Loss: 0.00002001
Iteration 125/1000 | Loss: 0.00002001
Iteration 126/1000 | Loss: 0.00002000
Iteration 127/1000 | Loss: 0.00002000
Iteration 128/1000 | Loss: 0.00002000
Iteration 129/1000 | Loss: 0.00002000
Iteration 130/1000 | Loss: 0.00002000
Iteration 131/1000 | Loss: 0.00002000
Iteration 132/1000 | Loss: 0.00001999
Iteration 133/1000 | Loss: 0.00001999
Iteration 134/1000 | Loss: 0.00001999
Iteration 135/1000 | Loss: 0.00001999
Iteration 136/1000 | Loss: 0.00001999
Iteration 137/1000 | Loss: 0.00001999
Iteration 138/1000 | Loss: 0.00001999
Iteration 139/1000 | Loss: 0.00001999
Iteration 140/1000 | Loss: 0.00001999
Iteration 141/1000 | Loss: 0.00001999
Iteration 142/1000 | Loss: 0.00001998
Iteration 143/1000 | Loss: 0.00001998
Iteration 144/1000 | Loss: 0.00001998
Iteration 145/1000 | Loss: 0.00001998
Iteration 146/1000 | Loss: 0.00001998
Iteration 147/1000 | Loss: 0.00001998
Iteration 148/1000 | Loss: 0.00001997
Iteration 149/1000 | Loss: 0.00001997
Iteration 150/1000 | Loss: 0.00001997
Iteration 151/1000 | Loss: 0.00001997
Iteration 152/1000 | Loss: 0.00001996
Iteration 153/1000 | Loss: 0.00001996
Iteration 154/1000 | Loss: 0.00001996
Iteration 155/1000 | Loss: 0.00001996
Iteration 156/1000 | Loss: 0.00001996
Iteration 157/1000 | Loss: 0.00001996
Iteration 158/1000 | Loss: 0.00001996
Iteration 159/1000 | Loss: 0.00001996
Iteration 160/1000 | Loss: 0.00001996
Iteration 161/1000 | Loss: 0.00001996
Iteration 162/1000 | Loss: 0.00001996
Iteration 163/1000 | Loss: 0.00001996
Iteration 164/1000 | Loss: 0.00001996
Iteration 165/1000 | Loss: 0.00001996
Iteration 166/1000 | Loss: 0.00001996
Iteration 167/1000 | Loss: 0.00001996
Iteration 168/1000 | Loss: 0.00001996
Iteration 169/1000 | Loss: 0.00001996
Iteration 170/1000 | Loss: 0.00001996
Iteration 171/1000 | Loss: 0.00001996
Iteration 172/1000 | Loss: 0.00001996
Iteration 173/1000 | Loss: 0.00001996
Iteration 174/1000 | Loss: 0.00001996
Iteration 175/1000 | Loss: 0.00001996
Iteration 176/1000 | Loss: 0.00001996
Iteration 177/1000 | Loss: 0.00001996
Iteration 178/1000 | Loss: 0.00001996
Iteration 179/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.9955825337092392e-05, 1.9955825337092392e-05, 1.9955825337092392e-05, 1.9955825337092392e-05, 1.9955825337092392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9955825337092392e-05

Optimization complete. Final v2v error: 3.5988543033599854 mm

Highest mean error: 5.376535415649414 mm for frame 219

Lowest mean error: 2.9989564418792725 mm for frame 54

Saving results

Total time: 209.1417760848999
