Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=291, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16296-16351
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507096
Iteration 2/25 | Loss: 0.00138851
Iteration 3/25 | Loss: 0.00131879
Iteration 4/25 | Loss: 0.00131133
Iteration 5/25 | Loss: 0.00130941
Iteration 6/25 | Loss: 0.00130941
Iteration 7/25 | Loss: 0.00130941
Iteration 8/25 | Loss: 0.00130941
Iteration 9/25 | Loss: 0.00130941
Iteration 10/25 | Loss: 0.00130941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013094099704176188, 0.0013094099704176188, 0.0013094099704176188, 0.0013094099704176188, 0.0013094099704176188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013094099704176188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42071128
Iteration 2/25 | Loss: 0.00077298
Iteration 3/25 | Loss: 0.00077296
Iteration 4/25 | Loss: 0.00077296
Iteration 5/25 | Loss: 0.00077296
Iteration 6/25 | Loss: 0.00077296
Iteration 7/25 | Loss: 0.00077295
Iteration 8/25 | Loss: 0.00077295
Iteration 9/25 | Loss: 0.00077295
Iteration 10/25 | Loss: 0.00077295
Iteration 11/25 | Loss: 0.00077295
Iteration 12/25 | Loss: 0.00077295
Iteration 13/25 | Loss: 0.00077295
Iteration 14/25 | Loss: 0.00077295
Iteration 15/25 | Loss: 0.00077295
Iteration 16/25 | Loss: 0.00077295
Iteration 17/25 | Loss: 0.00077295
Iteration 18/25 | Loss: 0.00077295
Iteration 19/25 | Loss: 0.00077295
Iteration 20/25 | Loss: 0.00077295
Iteration 21/25 | Loss: 0.00077295
Iteration 22/25 | Loss: 0.00077295
Iteration 23/25 | Loss: 0.00077295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007729536737315357, 0.0007729536737315357, 0.0007729536737315357, 0.0007729536737315357, 0.0007729536737315357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007729536737315357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077295
Iteration 2/1000 | Loss: 0.00002718
Iteration 3/1000 | Loss: 0.00002022
Iteration 4/1000 | Loss: 0.00001852
Iteration 5/1000 | Loss: 0.00001773
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001685
Iteration 8/1000 | Loss: 0.00001652
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001615
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001612
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001603
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001597
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001590
Iteration 23/1000 | Loss: 0.00001589
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001588
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00001553
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001541
Iteration 80/1000 | Loss: 0.00001541
Iteration 81/1000 | Loss: 0.00001541
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001539
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001535
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001534
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001533
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001528
Iteration 116/1000 | Loss: 0.00001528
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001528
Iteration 119/1000 | Loss: 0.00001528
Iteration 120/1000 | Loss: 0.00001528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.5282234016922303e-05, 1.5282234016922303e-05, 1.5282234016922303e-05, 1.5282234016922303e-05, 1.5282234016922303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5282234016922303e-05

Optimization complete. Final v2v error: 3.2805678844451904 mm

Highest mean error: 3.482743263244629 mm for frame 173

Lowest mean error: 2.9940683841705322 mm for frame 222

Saving results

Total time: 41.352309703826904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073623
Iteration 2/25 | Loss: 0.01073623
Iteration 3/25 | Loss: 0.00335309
Iteration 4/25 | Loss: 0.00190321
Iteration 5/25 | Loss: 0.00181170
Iteration 6/25 | Loss: 0.00202484
Iteration 7/25 | Loss: 0.00208579
Iteration 8/25 | Loss: 0.00165367
Iteration 9/25 | Loss: 0.00159246
Iteration 10/25 | Loss: 0.00150736
Iteration 11/25 | Loss: 0.00149434
Iteration 12/25 | Loss: 0.00141678
Iteration 13/25 | Loss: 0.00141030
Iteration 14/25 | Loss: 0.00140243
Iteration 15/25 | Loss: 0.00139084
Iteration 16/25 | Loss: 0.00139848
Iteration 17/25 | Loss: 0.00139594
Iteration 18/25 | Loss: 0.00139296
Iteration 19/25 | Loss: 0.00139218
Iteration 20/25 | Loss: 0.00139243
Iteration 21/25 | Loss: 0.00139985
Iteration 22/25 | Loss: 0.00138927
Iteration 23/25 | Loss: 0.00140745
Iteration 24/25 | Loss: 0.00137716
Iteration 25/25 | Loss: 0.00137445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56933159
Iteration 2/25 | Loss: 0.00085082
Iteration 3/25 | Loss: 0.00077607
Iteration 4/25 | Loss: 0.00077606
Iteration 5/25 | Loss: 0.00077606
Iteration 6/25 | Loss: 0.00077606
Iteration 7/25 | Loss: 0.00077606
Iteration 8/25 | Loss: 0.00077606
Iteration 9/25 | Loss: 0.00077606
Iteration 10/25 | Loss: 0.00077606
Iteration 11/25 | Loss: 0.00077606
Iteration 12/25 | Loss: 0.00077606
Iteration 13/25 | Loss: 0.00077606
Iteration 14/25 | Loss: 0.00077606
Iteration 15/25 | Loss: 0.00077606
Iteration 16/25 | Loss: 0.00077606
Iteration 17/25 | Loss: 0.00077606
Iteration 18/25 | Loss: 0.00077606
Iteration 19/25 | Loss: 0.00077606
Iteration 20/25 | Loss: 0.00077606
Iteration 21/25 | Loss: 0.00077606
Iteration 22/25 | Loss: 0.00077606
Iteration 23/25 | Loss: 0.00077606
Iteration 24/25 | Loss: 0.00077606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007760612643323839, 0.0007760612643323839, 0.0007760612643323839, 0.0007760612643323839, 0.0007760612643323839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007760612643323839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077606
Iteration 2/1000 | Loss: 0.00023939
Iteration 3/1000 | Loss: 0.00009512
Iteration 4/1000 | Loss: 0.00007415
Iteration 5/1000 | Loss: 0.00007688
Iteration 6/1000 | Loss: 0.00013677
Iteration 7/1000 | Loss: 0.00004680
Iteration 8/1000 | Loss: 0.00004866
Iteration 9/1000 | Loss: 0.00002441
Iteration 10/1000 | Loss: 0.00002154
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002076
Iteration 13/1000 | Loss: 0.00002037
Iteration 14/1000 | Loss: 0.00002992
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00003030
Iteration 17/1000 | Loss: 0.00002030
Iteration 18/1000 | Loss: 0.00002107
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00002860
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00002038
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002418
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00002042
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001926
Iteration 33/1000 | Loss: 0.00001979
Iteration 34/1000 | Loss: 0.00003860
Iteration 35/1000 | Loss: 0.00004992
Iteration 36/1000 | Loss: 0.00003002
Iteration 37/1000 | Loss: 0.00002280
Iteration 38/1000 | Loss: 0.00002606
Iteration 39/1000 | Loss: 0.00002072
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001912
Iteration 44/1000 | Loss: 0.00001912
Iteration 45/1000 | Loss: 0.00001912
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00002259
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001910
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001910
Iteration 58/1000 | Loss: 0.00001910
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.9097373296972364e-05, 1.9097373296972364e-05, 1.9097373296972364e-05, 1.9097373296972364e-05, 1.9097373296972364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9097373296972364e-05

Optimization complete. Final v2v error: 3.7549147605895996 mm

Highest mean error: 3.8912546634674072 mm for frame 28

Lowest mean error: 3.61922287940979 mm for frame 199

Saving results

Total time: 115.98498511314392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038078
Iteration 2/25 | Loss: 0.00177918
Iteration 3/25 | Loss: 0.00160156
Iteration 4/25 | Loss: 0.00131320
Iteration 5/25 | Loss: 0.00128474
Iteration 6/25 | Loss: 0.00127869
Iteration 7/25 | Loss: 0.00126820
Iteration 8/25 | Loss: 0.00126600
Iteration 9/25 | Loss: 0.00126121
Iteration 10/25 | Loss: 0.00126268
Iteration 11/25 | Loss: 0.00125926
Iteration 12/25 | Loss: 0.00125779
Iteration 13/25 | Loss: 0.00125397
Iteration 14/25 | Loss: 0.00125367
Iteration 15/25 | Loss: 0.00125364
Iteration 16/25 | Loss: 0.00125364
Iteration 17/25 | Loss: 0.00125364
Iteration 18/25 | Loss: 0.00125364
Iteration 19/25 | Loss: 0.00125364
Iteration 20/25 | Loss: 0.00125364
Iteration 21/25 | Loss: 0.00125364
Iteration 22/25 | Loss: 0.00125364
Iteration 23/25 | Loss: 0.00125364
Iteration 24/25 | Loss: 0.00125364
Iteration 25/25 | Loss: 0.00125363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.63920784
Iteration 2/25 | Loss: 0.00111699
Iteration 3/25 | Loss: 0.00088736
Iteration 4/25 | Loss: 0.00088736
Iteration 5/25 | Loss: 0.00088736
Iteration 6/25 | Loss: 0.00088736
Iteration 7/25 | Loss: 0.00088736
Iteration 8/25 | Loss: 0.00088736
Iteration 9/25 | Loss: 0.00088736
Iteration 10/25 | Loss: 0.00088736
Iteration 11/25 | Loss: 0.00088736
Iteration 12/25 | Loss: 0.00088736
Iteration 13/25 | Loss: 0.00088736
Iteration 14/25 | Loss: 0.00088736
Iteration 15/25 | Loss: 0.00088736
Iteration 16/25 | Loss: 0.00088736
Iteration 17/25 | Loss: 0.00088736
Iteration 18/25 | Loss: 0.00088736
Iteration 19/25 | Loss: 0.00088736
Iteration 20/25 | Loss: 0.00088736
Iteration 21/25 | Loss: 0.00088736
Iteration 22/25 | Loss: 0.00088736
Iteration 23/25 | Loss: 0.00088736
Iteration 24/25 | Loss: 0.00088736
Iteration 25/25 | Loss: 0.00088736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088736
Iteration 2/1000 | Loss: 0.00027325
Iteration 3/1000 | Loss: 0.00012129
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00006471
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00014495
Iteration 8/1000 | Loss: 0.00003838
Iteration 9/1000 | Loss: 0.00003917
Iteration 10/1000 | Loss: 0.00004104
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00004895
Iteration 13/1000 | Loss: 0.00003144
Iteration 14/1000 | Loss: 0.00001792
Iteration 15/1000 | Loss: 0.00001569
Iteration 16/1000 | Loss: 0.00001542
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001502
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00005148
Iteration 31/1000 | Loss: 0.00009357
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00006229
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001466
Iteration 38/1000 | Loss: 0.00001466
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00004961
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00004981
Iteration 76/1000 | Loss: 0.00002964
Iteration 77/1000 | Loss: 0.00004608
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001457
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001452
Iteration 94/1000 | Loss: 0.00001452
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001451
Iteration 100/1000 | Loss: 0.00001451
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001449
Iteration 108/1000 | Loss: 0.00001449
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00002984
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00002992
Iteration 138/1000 | Loss: 0.00001445
Iteration 139/1000 | Loss: 0.00001444
Iteration 140/1000 | Loss: 0.00001444
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00003504
Iteration 147/1000 | Loss: 0.00002065
Iteration 148/1000 | Loss: 0.00002237
Iteration 149/1000 | Loss: 0.00001444
Iteration 150/1000 | Loss: 0.00001443
Iteration 151/1000 | Loss: 0.00001443
Iteration 152/1000 | Loss: 0.00001442
Iteration 153/1000 | Loss: 0.00001442
Iteration 154/1000 | Loss: 0.00001441
Iteration 155/1000 | Loss: 0.00001441
Iteration 156/1000 | Loss: 0.00001441
Iteration 157/1000 | Loss: 0.00001441
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001441
Iteration 160/1000 | Loss: 0.00001441
Iteration 161/1000 | Loss: 0.00001441
Iteration 162/1000 | Loss: 0.00001440
Iteration 163/1000 | Loss: 0.00001440
Iteration 164/1000 | Loss: 0.00001440
Iteration 165/1000 | Loss: 0.00001440
Iteration 166/1000 | Loss: 0.00001440
Iteration 167/1000 | Loss: 0.00001440
Iteration 168/1000 | Loss: 0.00001440
Iteration 169/1000 | Loss: 0.00001439
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001439
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00002336
Iteration 174/1000 | Loss: 0.00002700
Iteration 175/1000 | Loss: 0.00002355
Iteration 176/1000 | Loss: 0.00001438
Iteration 177/1000 | Loss: 0.00001438
Iteration 178/1000 | Loss: 0.00001438
Iteration 179/1000 | Loss: 0.00001438
Iteration 180/1000 | Loss: 0.00001438
Iteration 181/1000 | Loss: 0.00001438
Iteration 182/1000 | Loss: 0.00001438
Iteration 183/1000 | Loss: 0.00001438
Iteration 184/1000 | Loss: 0.00001438
Iteration 185/1000 | Loss: 0.00001438
Iteration 186/1000 | Loss: 0.00001438
Iteration 187/1000 | Loss: 0.00001438
Iteration 188/1000 | Loss: 0.00001438
Iteration 189/1000 | Loss: 0.00001438
Iteration 190/1000 | Loss: 0.00001438
Iteration 191/1000 | Loss: 0.00001438
Iteration 192/1000 | Loss: 0.00001438
Iteration 193/1000 | Loss: 0.00001438
Iteration 194/1000 | Loss: 0.00001438
Iteration 195/1000 | Loss: 0.00001438
Iteration 196/1000 | Loss: 0.00001438
Iteration 197/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.4380110769707244e-05, 1.4380110769707244e-05, 1.4380110769707244e-05, 1.4380110769707244e-05, 1.4380110769707244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4380110769707244e-05

Optimization complete. Final v2v error: 3.232759952545166 mm

Highest mean error: 3.716359853744507 mm for frame 37

Lowest mean error: 2.8966007232666016 mm for frame 65

Saving results

Total time: 91.72491145133972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476877
Iteration 2/25 | Loss: 0.00135035
Iteration 3/25 | Loss: 0.00128952
Iteration 4/25 | Loss: 0.00127994
Iteration 5/25 | Loss: 0.00127646
Iteration 6/25 | Loss: 0.00127606
Iteration 7/25 | Loss: 0.00127606
Iteration 8/25 | Loss: 0.00127606
Iteration 9/25 | Loss: 0.00127606
Iteration 10/25 | Loss: 0.00127606
Iteration 11/25 | Loss: 0.00127606
Iteration 12/25 | Loss: 0.00127606
Iteration 13/25 | Loss: 0.00127606
Iteration 14/25 | Loss: 0.00127606
Iteration 15/25 | Loss: 0.00127606
Iteration 16/25 | Loss: 0.00127606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012760576792061329, 0.0012760576792061329, 0.0012760576792061329, 0.0012760576792061329, 0.0012760576792061329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012760576792061329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57811069
Iteration 2/25 | Loss: 0.00085417
Iteration 3/25 | Loss: 0.00085417
Iteration 4/25 | Loss: 0.00085417
Iteration 5/25 | Loss: 0.00085417
Iteration 6/25 | Loss: 0.00085417
Iteration 7/25 | Loss: 0.00085417
Iteration 8/25 | Loss: 0.00085417
Iteration 9/25 | Loss: 0.00085417
Iteration 10/25 | Loss: 0.00085417
Iteration 11/25 | Loss: 0.00085417
Iteration 12/25 | Loss: 0.00085417
Iteration 13/25 | Loss: 0.00085417
Iteration 14/25 | Loss: 0.00085417
Iteration 15/25 | Loss: 0.00085417
Iteration 16/25 | Loss: 0.00085417
Iteration 17/25 | Loss: 0.00085417
Iteration 18/25 | Loss: 0.00085417
Iteration 19/25 | Loss: 0.00085417
Iteration 20/25 | Loss: 0.00085417
Iteration 21/25 | Loss: 0.00085417
Iteration 22/25 | Loss: 0.00085417
Iteration 23/25 | Loss: 0.00085417
Iteration 24/25 | Loss: 0.00085417
Iteration 25/25 | Loss: 0.00085417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085417
Iteration 2/1000 | Loss: 0.00003752
Iteration 3/1000 | Loss: 0.00002151
Iteration 4/1000 | Loss: 0.00001755
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001570
Iteration 7/1000 | Loss: 0.00001532
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001478
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001434
Iteration 17/1000 | Loss: 0.00001428
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001415
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001412
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001409
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001406
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001405
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001401
Iteration 63/1000 | Loss: 0.00001401
Iteration 64/1000 | Loss: 0.00001401
Iteration 65/1000 | Loss: 0.00001400
Iteration 66/1000 | Loss: 0.00001400
Iteration 67/1000 | Loss: 0.00001400
Iteration 68/1000 | Loss: 0.00001400
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001396
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001396
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001395
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001395
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001395
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001391
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001384
Iteration 139/1000 | Loss: 0.00001384
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001384
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001383
Iteration 147/1000 | Loss: 0.00001383
Iteration 148/1000 | Loss: 0.00001383
Iteration 149/1000 | Loss: 0.00001383
Iteration 150/1000 | Loss: 0.00001382
Iteration 151/1000 | Loss: 0.00001382
Iteration 152/1000 | Loss: 0.00001382
Iteration 153/1000 | Loss: 0.00001382
Iteration 154/1000 | Loss: 0.00001382
Iteration 155/1000 | Loss: 0.00001382
Iteration 156/1000 | Loss: 0.00001382
Iteration 157/1000 | Loss: 0.00001381
Iteration 158/1000 | Loss: 0.00001381
Iteration 159/1000 | Loss: 0.00001381
Iteration 160/1000 | Loss: 0.00001381
Iteration 161/1000 | Loss: 0.00001381
Iteration 162/1000 | Loss: 0.00001381
Iteration 163/1000 | Loss: 0.00001381
Iteration 164/1000 | Loss: 0.00001381
Iteration 165/1000 | Loss: 0.00001381
Iteration 166/1000 | Loss: 0.00001381
Iteration 167/1000 | Loss: 0.00001380
Iteration 168/1000 | Loss: 0.00001380
Iteration 169/1000 | Loss: 0.00001380
Iteration 170/1000 | Loss: 0.00001380
Iteration 171/1000 | Loss: 0.00001380
Iteration 172/1000 | Loss: 0.00001380
Iteration 173/1000 | Loss: 0.00001380
Iteration 174/1000 | Loss: 0.00001379
Iteration 175/1000 | Loss: 0.00001379
Iteration 176/1000 | Loss: 0.00001379
Iteration 177/1000 | Loss: 0.00001379
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001379
Iteration 180/1000 | Loss: 0.00001379
Iteration 181/1000 | Loss: 0.00001379
Iteration 182/1000 | Loss: 0.00001379
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001378
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001378
Iteration 192/1000 | Loss: 0.00001378
Iteration 193/1000 | Loss: 0.00001378
Iteration 194/1000 | Loss: 0.00001378
Iteration 195/1000 | Loss: 0.00001378
Iteration 196/1000 | Loss: 0.00001378
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001377
Iteration 202/1000 | Loss: 0.00001377
Iteration 203/1000 | Loss: 0.00001377
Iteration 204/1000 | Loss: 0.00001377
Iteration 205/1000 | Loss: 0.00001377
Iteration 206/1000 | Loss: 0.00001377
Iteration 207/1000 | Loss: 0.00001377
Iteration 208/1000 | Loss: 0.00001377
Iteration 209/1000 | Loss: 0.00001377
Iteration 210/1000 | Loss: 0.00001377
Iteration 211/1000 | Loss: 0.00001377
Iteration 212/1000 | Loss: 0.00001377
Iteration 213/1000 | Loss: 0.00001377
Iteration 214/1000 | Loss: 0.00001377
Iteration 215/1000 | Loss: 0.00001377
Iteration 216/1000 | Loss: 0.00001377
Iteration 217/1000 | Loss: 0.00001377
Iteration 218/1000 | Loss: 0.00001377
Iteration 219/1000 | Loss: 0.00001377
Iteration 220/1000 | Loss: 0.00001377
Iteration 221/1000 | Loss: 0.00001377
Iteration 222/1000 | Loss: 0.00001377
Iteration 223/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.376723412249703e-05, 1.376723412249703e-05, 1.376723412249703e-05, 1.376723412249703e-05, 1.376723412249703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.376723412249703e-05

Optimization complete. Final v2v error: 3.11751127243042 mm

Highest mean error: 3.6439809799194336 mm for frame 39

Lowest mean error: 2.897535800933838 mm for frame 97

Saving results

Total time: 42.52595400810242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911831
Iteration 2/25 | Loss: 0.00150427
Iteration 3/25 | Loss: 0.00140598
Iteration 4/25 | Loss: 0.00139202
Iteration 5/25 | Loss: 0.00138868
Iteration 6/25 | Loss: 0.00138807
Iteration 7/25 | Loss: 0.00138807
Iteration 8/25 | Loss: 0.00138807
Iteration 9/25 | Loss: 0.00138807
Iteration 10/25 | Loss: 0.00138807
Iteration 11/25 | Loss: 0.00138807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013880651677027345, 0.0013880651677027345, 0.0013880651677027345, 0.0013880651677027345, 0.0013880651677027345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013880651677027345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24283123
Iteration 2/25 | Loss: 0.00113947
Iteration 3/25 | Loss: 0.00113941
Iteration 4/25 | Loss: 0.00113941
Iteration 5/25 | Loss: 0.00113941
Iteration 6/25 | Loss: 0.00113941
Iteration 7/25 | Loss: 0.00113941
Iteration 8/25 | Loss: 0.00113940
Iteration 9/25 | Loss: 0.00113940
Iteration 10/25 | Loss: 0.00113940
Iteration 11/25 | Loss: 0.00113940
Iteration 12/25 | Loss: 0.00113940
Iteration 13/25 | Loss: 0.00113940
Iteration 14/25 | Loss: 0.00113940
Iteration 15/25 | Loss: 0.00113940
Iteration 16/25 | Loss: 0.00113940
Iteration 17/25 | Loss: 0.00113940
Iteration 18/25 | Loss: 0.00113940
Iteration 19/25 | Loss: 0.00113940
Iteration 20/25 | Loss: 0.00113940
Iteration 21/25 | Loss: 0.00113940
Iteration 22/25 | Loss: 0.00113940
Iteration 23/25 | Loss: 0.00113940
Iteration 24/25 | Loss: 0.00113940
Iteration 25/25 | Loss: 0.00113940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011394041357561946, 0.0011394041357561946, 0.0011394041357561946, 0.0011394041357561946, 0.0011394041357561946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011394041357561946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113940
Iteration 2/1000 | Loss: 0.00005004
Iteration 3/1000 | Loss: 0.00003113
Iteration 4/1000 | Loss: 0.00002542
Iteration 5/1000 | Loss: 0.00002265
Iteration 6/1000 | Loss: 0.00002106
Iteration 7/1000 | Loss: 0.00002029
Iteration 8/1000 | Loss: 0.00001980
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001856
Iteration 13/1000 | Loss: 0.00001836
Iteration 14/1000 | Loss: 0.00001820
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001799
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001799
Iteration 20/1000 | Loss: 0.00001799
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001795
Iteration 27/1000 | Loss: 0.00001795
Iteration 28/1000 | Loss: 0.00001795
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001794
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001793
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001792
Iteration 36/1000 | Loss: 0.00001791
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001791
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001790
Iteration 41/1000 | Loss: 0.00001789
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001789
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001788
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001787
Iteration 55/1000 | Loss: 0.00001787
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001786
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001785
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00001785
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001783
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001783
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001783
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Iteration 109/1000 | Loss: 0.00001781
Iteration 110/1000 | Loss: 0.00001781
Iteration 111/1000 | Loss: 0.00001780
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001780
Iteration 114/1000 | Loss: 0.00001780
Iteration 115/1000 | Loss: 0.00001780
Iteration 116/1000 | Loss: 0.00001780
Iteration 117/1000 | Loss: 0.00001780
Iteration 118/1000 | Loss: 0.00001780
Iteration 119/1000 | Loss: 0.00001780
Iteration 120/1000 | Loss: 0.00001780
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001779
Iteration 137/1000 | Loss: 0.00001779
Iteration 138/1000 | Loss: 0.00001779
Iteration 139/1000 | Loss: 0.00001779
Iteration 140/1000 | Loss: 0.00001779
Iteration 141/1000 | Loss: 0.00001779
Iteration 142/1000 | Loss: 0.00001779
Iteration 143/1000 | Loss: 0.00001779
Iteration 144/1000 | Loss: 0.00001779
Iteration 145/1000 | Loss: 0.00001779
Iteration 146/1000 | Loss: 0.00001779
Iteration 147/1000 | Loss: 0.00001779
Iteration 148/1000 | Loss: 0.00001779
Iteration 149/1000 | Loss: 0.00001779
Iteration 150/1000 | Loss: 0.00001779
Iteration 151/1000 | Loss: 0.00001779
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001779
Iteration 154/1000 | Loss: 0.00001779
Iteration 155/1000 | Loss: 0.00001779
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001779
Iteration 158/1000 | Loss: 0.00001778
Iteration 159/1000 | Loss: 0.00001778
Iteration 160/1000 | Loss: 0.00001778
Iteration 161/1000 | Loss: 0.00001778
Iteration 162/1000 | Loss: 0.00001778
Iteration 163/1000 | Loss: 0.00001778
Iteration 164/1000 | Loss: 0.00001778
Iteration 165/1000 | Loss: 0.00001778
Iteration 166/1000 | Loss: 0.00001778
Iteration 167/1000 | Loss: 0.00001778
Iteration 168/1000 | Loss: 0.00001778
Iteration 169/1000 | Loss: 0.00001778
Iteration 170/1000 | Loss: 0.00001778
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001778
Iteration 173/1000 | Loss: 0.00001778
Iteration 174/1000 | Loss: 0.00001778
Iteration 175/1000 | Loss: 0.00001778
Iteration 176/1000 | Loss: 0.00001778
Iteration 177/1000 | Loss: 0.00001778
Iteration 178/1000 | Loss: 0.00001778
Iteration 179/1000 | Loss: 0.00001778
Iteration 180/1000 | Loss: 0.00001778
Iteration 181/1000 | Loss: 0.00001778
Iteration 182/1000 | Loss: 0.00001778
Iteration 183/1000 | Loss: 0.00001778
Iteration 184/1000 | Loss: 0.00001778
Iteration 185/1000 | Loss: 0.00001778
Iteration 186/1000 | Loss: 0.00001778
Iteration 187/1000 | Loss: 0.00001778
Iteration 188/1000 | Loss: 0.00001778
Iteration 189/1000 | Loss: 0.00001778
Iteration 190/1000 | Loss: 0.00001778
Iteration 191/1000 | Loss: 0.00001778
Iteration 192/1000 | Loss: 0.00001778
Iteration 193/1000 | Loss: 0.00001778
Iteration 194/1000 | Loss: 0.00001778
Iteration 195/1000 | Loss: 0.00001778
Iteration 196/1000 | Loss: 0.00001778
Iteration 197/1000 | Loss: 0.00001778
Iteration 198/1000 | Loss: 0.00001778
Iteration 199/1000 | Loss: 0.00001778
Iteration 200/1000 | Loss: 0.00001778
Iteration 201/1000 | Loss: 0.00001778
Iteration 202/1000 | Loss: 0.00001778
Iteration 203/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.7775008018361405e-05, 1.7775008018361405e-05, 1.7775008018361405e-05, 1.7775008018361405e-05, 1.7775008018361405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7775008018361405e-05

Optimization complete. Final v2v error: 3.606178045272827 mm

Highest mean error: 4.102177619934082 mm for frame 123

Lowest mean error: 3.40492844581604 mm for frame 54

Saving results

Total time: 40.55704712867737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393079
Iteration 2/25 | Loss: 0.00142164
Iteration 3/25 | Loss: 0.00136367
Iteration 4/25 | Loss: 0.00135452
Iteration 5/25 | Loss: 0.00135136
Iteration 6/25 | Loss: 0.00135079
Iteration 7/25 | Loss: 0.00135079
Iteration 8/25 | Loss: 0.00135079
Iteration 9/25 | Loss: 0.00135079
Iteration 10/25 | Loss: 0.00135079
Iteration 11/25 | Loss: 0.00135079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013507901458069682, 0.0013507901458069682, 0.0013507901458069682, 0.0013507901458069682, 0.0013507901458069682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013507901458069682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33448803
Iteration 2/25 | Loss: 0.00156972
Iteration 3/25 | Loss: 0.00156972
Iteration 4/25 | Loss: 0.00156972
Iteration 5/25 | Loss: 0.00156971
Iteration 6/25 | Loss: 0.00156971
Iteration 7/25 | Loss: 0.00156971
Iteration 8/25 | Loss: 0.00156971
Iteration 9/25 | Loss: 0.00156971
Iteration 10/25 | Loss: 0.00156971
Iteration 11/25 | Loss: 0.00156971
Iteration 12/25 | Loss: 0.00156971
Iteration 13/25 | Loss: 0.00156971
Iteration 14/25 | Loss: 0.00156971
Iteration 15/25 | Loss: 0.00156971
Iteration 16/25 | Loss: 0.00156971
Iteration 17/25 | Loss: 0.00156971
Iteration 18/25 | Loss: 0.00156971
Iteration 19/25 | Loss: 0.00156971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015697128837928176, 0.0015697128837928176, 0.0015697128837928176, 0.0015697128837928176, 0.0015697128837928176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015697128837928176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156971
Iteration 2/1000 | Loss: 0.00005087
Iteration 3/1000 | Loss: 0.00002833
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00002372
Iteration 6/1000 | Loss: 0.00002308
Iteration 7/1000 | Loss: 0.00002261
Iteration 8/1000 | Loss: 0.00002232
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002202
Iteration 11/1000 | Loss: 0.00002197
Iteration 12/1000 | Loss: 0.00002190
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002185
Iteration 15/1000 | Loss: 0.00002185
Iteration 16/1000 | Loss: 0.00002181
Iteration 17/1000 | Loss: 0.00002180
Iteration 18/1000 | Loss: 0.00002180
Iteration 19/1000 | Loss: 0.00002174
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002166
Iteration 22/1000 | Loss: 0.00002166
Iteration 23/1000 | Loss: 0.00002164
Iteration 24/1000 | Loss: 0.00002164
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002162
Iteration 27/1000 | Loss: 0.00002161
Iteration 28/1000 | Loss: 0.00002160
Iteration 29/1000 | Loss: 0.00002160
Iteration 30/1000 | Loss: 0.00002156
Iteration 31/1000 | Loss: 0.00002156
Iteration 32/1000 | Loss: 0.00002154
Iteration 33/1000 | Loss: 0.00002154
Iteration 34/1000 | Loss: 0.00002153
Iteration 35/1000 | Loss: 0.00002153
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002153
Iteration 38/1000 | Loss: 0.00002153
Iteration 39/1000 | Loss: 0.00002153
Iteration 40/1000 | Loss: 0.00002152
Iteration 41/1000 | Loss: 0.00002152
Iteration 42/1000 | Loss: 0.00002152
Iteration 43/1000 | Loss: 0.00002152
Iteration 44/1000 | Loss: 0.00002152
Iteration 45/1000 | Loss: 0.00002152
Iteration 46/1000 | Loss: 0.00002152
Iteration 47/1000 | Loss: 0.00002151
Iteration 48/1000 | Loss: 0.00002151
Iteration 49/1000 | Loss: 0.00002151
Iteration 50/1000 | Loss: 0.00002151
Iteration 51/1000 | Loss: 0.00002151
Iteration 52/1000 | Loss: 0.00002151
Iteration 53/1000 | Loss: 0.00002151
Iteration 54/1000 | Loss: 0.00002151
Iteration 55/1000 | Loss: 0.00002151
Iteration 56/1000 | Loss: 0.00002151
Iteration 57/1000 | Loss: 0.00002151
Iteration 58/1000 | Loss: 0.00002151
Iteration 59/1000 | Loss: 0.00002151
Iteration 60/1000 | Loss: 0.00002151
Iteration 61/1000 | Loss: 0.00002151
Iteration 62/1000 | Loss: 0.00002151
Iteration 63/1000 | Loss: 0.00002151
Iteration 64/1000 | Loss: 0.00002151
Iteration 65/1000 | Loss: 0.00002151
Iteration 66/1000 | Loss: 0.00002151
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00002151
Iteration 74/1000 | Loss: 0.00002151
Iteration 75/1000 | Loss: 0.00002151
Iteration 76/1000 | Loss: 0.00002151
Iteration 77/1000 | Loss: 0.00002151
Iteration 78/1000 | Loss: 0.00002151
Iteration 79/1000 | Loss: 0.00002151
Iteration 80/1000 | Loss: 0.00002151
Iteration 81/1000 | Loss: 0.00002151
Iteration 82/1000 | Loss: 0.00002151
Iteration 83/1000 | Loss: 0.00002151
Iteration 84/1000 | Loss: 0.00002151
Iteration 85/1000 | Loss: 0.00002151
Iteration 86/1000 | Loss: 0.00002151
Iteration 87/1000 | Loss: 0.00002151
Iteration 88/1000 | Loss: 0.00002151
Iteration 89/1000 | Loss: 0.00002151
Iteration 90/1000 | Loss: 0.00002151
Iteration 91/1000 | Loss: 0.00002151
Iteration 92/1000 | Loss: 0.00002151
Iteration 93/1000 | Loss: 0.00002151
Iteration 94/1000 | Loss: 0.00002151
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002151
Iteration 103/1000 | Loss: 0.00002151
Iteration 104/1000 | Loss: 0.00002151
Iteration 105/1000 | Loss: 0.00002151
Iteration 106/1000 | Loss: 0.00002151
Iteration 107/1000 | Loss: 0.00002151
Iteration 108/1000 | Loss: 0.00002151
Iteration 109/1000 | Loss: 0.00002151
Iteration 110/1000 | Loss: 0.00002151
Iteration 111/1000 | Loss: 0.00002151
Iteration 112/1000 | Loss: 0.00002151
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002151
Iteration 115/1000 | Loss: 0.00002151
Iteration 116/1000 | Loss: 0.00002151
Iteration 117/1000 | Loss: 0.00002151
Iteration 118/1000 | Loss: 0.00002151
Iteration 119/1000 | Loss: 0.00002151
Iteration 120/1000 | Loss: 0.00002151
Iteration 121/1000 | Loss: 0.00002151
Iteration 122/1000 | Loss: 0.00002151
Iteration 123/1000 | Loss: 0.00002151
Iteration 124/1000 | Loss: 0.00002151
Iteration 125/1000 | Loss: 0.00002151
Iteration 126/1000 | Loss: 0.00002151
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.1509637008421123e-05, 2.1509637008421123e-05, 2.1509637008421123e-05, 2.1509637008421123e-05, 2.1509637008421123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1509637008421123e-05

Optimization complete. Final v2v error: 3.8522732257843018 mm

Highest mean error: 4.265201091766357 mm for frame 48

Lowest mean error: 3.420017719268799 mm for frame 1

Saving results

Total time: 32.46406269073486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449856
Iteration 2/25 | Loss: 0.00150972
Iteration 3/25 | Loss: 0.00135980
Iteration 4/25 | Loss: 0.00134217
Iteration 5/25 | Loss: 0.00133982
Iteration 6/25 | Loss: 0.00133969
Iteration 7/25 | Loss: 0.00133969
Iteration 8/25 | Loss: 0.00133969
Iteration 9/25 | Loss: 0.00133969
Iteration 10/25 | Loss: 0.00133969
Iteration 11/25 | Loss: 0.00133969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013396914582699537, 0.0013396914582699537, 0.0013396914582699537, 0.0013396914582699537, 0.0013396914582699537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013396914582699537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.21039820
Iteration 2/25 | Loss: 0.00086414
Iteration 3/25 | Loss: 0.00086412
Iteration 4/25 | Loss: 0.00086411
Iteration 5/25 | Loss: 0.00086411
Iteration 6/25 | Loss: 0.00086411
Iteration 7/25 | Loss: 0.00086411
Iteration 8/25 | Loss: 0.00086411
Iteration 9/25 | Loss: 0.00086411
Iteration 10/25 | Loss: 0.00086411
Iteration 11/25 | Loss: 0.00086411
Iteration 12/25 | Loss: 0.00086411
Iteration 13/25 | Loss: 0.00086411
Iteration 14/25 | Loss: 0.00086411
Iteration 15/25 | Loss: 0.00086411
Iteration 16/25 | Loss: 0.00086411
Iteration 17/25 | Loss: 0.00086411
Iteration 18/25 | Loss: 0.00086411
Iteration 19/25 | Loss: 0.00086411
Iteration 20/25 | Loss: 0.00086411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008641129243187606, 0.0008641129243187606, 0.0008641129243187606, 0.0008641129243187606, 0.0008641129243187606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008641129243187606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086411
Iteration 2/1000 | Loss: 0.00004627
Iteration 3/1000 | Loss: 0.00002914
Iteration 4/1000 | Loss: 0.00002467
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002225
Iteration 7/1000 | Loss: 0.00002170
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002098
Iteration 11/1000 | Loss: 0.00002094
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002084
Iteration 14/1000 | Loss: 0.00002081
Iteration 15/1000 | Loss: 0.00002081
Iteration 16/1000 | Loss: 0.00002081
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002080
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002078
Iteration 21/1000 | Loss: 0.00002078
Iteration 22/1000 | Loss: 0.00002074
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002074
Iteration 25/1000 | Loss: 0.00002073
Iteration 26/1000 | Loss: 0.00002073
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002072
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00002070
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002069
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002069
Iteration 39/1000 | Loss: 0.00002068
Iteration 40/1000 | Loss: 0.00002068
Iteration 41/1000 | Loss: 0.00002068
Iteration 42/1000 | Loss: 0.00002068
Iteration 43/1000 | Loss: 0.00002067
Iteration 44/1000 | Loss: 0.00002067
Iteration 45/1000 | Loss: 0.00002067
Iteration 46/1000 | Loss: 0.00002067
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00002066
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002066
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002064
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002063
Iteration 56/1000 | Loss: 0.00002063
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00002063
Iteration 59/1000 | Loss: 0.00002063
Iteration 60/1000 | Loss: 0.00002063
Iteration 61/1000 | Loss: 0.00002062
Iteration 62/1000 | Loss: 0.00002062
Iteration 63/1000 | Loss: 0.00002062
Iteration 64/1000 | Loss: 0.00002062
Iteration 65/1000 | Loss: 0.00002062
Iteration 66/1000 | Loss: 0.00002061
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002061
Iteration 69/1000 | Loss: 0.00002061
Iteration 70/1000 | Loss: 0.00002060
Iteration 71/1000 | Loss: 0.00002060
Iteration 72/1000 | Loss: 0.00002060
Iteration 73/1000 | Loss: 0.00002060
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002060
Iteration 88/1000 | Loss: 0.00002060
Iteration 89/1000 | Loss: 0.00002060
Iteration 90/1000 | Loss: 0.00002060
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002060
Iteration 104/1000 | Loss: 0.00002060
Iteration 105/1000 | Loss: 0.00002060
Iteration 106/1000 | Loss: 0.00002060
Iteration 107/1000 | Loss: 0.00002060
Iteration 108/1000 | Loss: 0.00002060
Iteration 109/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.060119732050225e-05, 2.060119732050225e-05, 2.060119732050225e-05, 2.060119732050225e-05, 2.060119732050225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.060119732050225e-05

Optimization complete. Final v2v error: 3.8478353023529053 mm

Highest mean error: 4.4820404052734375 mm for frame 24

Lowest mean error: 3.590130090713501 mm for frame 127

Saving results

Total time: 31.345080852508545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873774
Iteration 2/25 | Loss: 0.00184409
Iteration 3/25 | Loss: 0.00149922
Iteration 4/25 | Loss: 0.00147109
Iteration 5/25 | Loss: 0.00146370
Iteration 6/25 | Loss: 0.00146196
Iteration 7/25 | Loss: 0.00146196
Iteration 8/25 | Loss: 0.00146196
Iteration 9/25 | Loss: 0.00146196
Iteration 10/25 | Loss: 0.00146196
Iteration 11/25 | Loss: 0.00146196
Iteration 12/25 | Loss: 0.00146196
Iteration 13/25 | Loss: 0.00146196
Iteration 14/25 | Loss: 0.00146196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014619621215388179, 0.0014619621215388179, 0.0014619621215388179, 0.0014619621215388179, 0.0014619621215388179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014619621215388179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16089070
Iteration 2/25 | Loss: 0.00150307
Iteration 3/25 | Loss: 0.00150305
Iteration 4/25 | Loss: 0.00150305
Iteration 5/25 | Loss: 0.00150305
Iteration 6/25 | Loss: 0.00150305
Iteration 7/25 | Loss: 0.00150305
Iteration 8/25 | Loss: 0.00150305
Iteration 9/25 | Loss: 0.00150305
Iteration 10/25 | Loss: 0.00150305
Iteration 11/25 | Loss: 0.00150305
Iteration 12/25 | Loss: 0.00150305
Iteration 13/25 | Loss: 0.00150305
Iteration 14/25 | Loss: 0.00150305
Iteration 15/25 | Loss: 0.00150305
Iteration 16/25 | Loss: 0.00150305
Iteration 17/25 | Loss: 0.00150305
Iteration 18/25 | Loss: 0.00150305
Iteration 19/25 | Loss: 0.00150305
Iteration 20/25 | Loss: 0.00150305
Iteration 21/25 | Loss: 0.00150305
Iteration 22/25 | Loss: 0.00150305
Iteration 23/25 | Loss: 0.00150305
Iteration 24/25 | Loss: 0.00150305
Iteration 25/25 | Loss: 0.00150305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150305
Iteration 2/1000 | Loss: 0.00010153
Iteration 3/1000 | Loss: 0.00005934
Iteration 4/1000 | Loss: 0.00004719
Iteration 5/1000 | Loss: 0.00004488
Iteration 6/1000 | Loss: 0.00004296
Iteration 7/1000 | Loss: 0.00004124
Iteration 8/1000 | Loss: 0.00003988
Iteration 9/1000 | Loss: 0.00003888
Iteration 10/1000 | Loss: 0.00003808
Iteration 11/1000 | Loss: 0.00003747
Iteration 12/1000 | Loss: 0.00003705
Iteration 13/1000 | Loss: 0.00003661
Iteration 14/1000 | Loss: 0.00003626
Iteration 15/1000 | Loss: 0.00003598
Iteration 16/1000 | Loss: 0.00003579
Iteration 17/1000 | Loss: 0.00003573
Iteration 18/1000 | Loss: 0.00003563
Iteration 19/1000 | Loss: 0.00003549
Iteration 20/1000 | Loss: 0.00003541
Iteration 21/1000 | Loss: 0.00003535
Iteration 22/1000 | Loss: 0.00003535
Iteration 23/1000 | Loss: 0.00003531
Iteration 24/1000 | Loss: 0.00003525
Iteration 25/1000 | Loss: 0.00003517
Iteration 26/1000 | Loss: 0.00003516
Iteration 27/1000 | Loss: 0.00003512
Iteration 28/1000 | Loss: 0.00003512
Iteration 29/1000 | Loss: 0.00003511
Iteration 30/1000 | Loss: 0.00003511
Iteration 31/1000 | Loss: 0.00003508
Iteration 32/1000 | Loss: 0.00003507
Iteration 33/1000 | Loss: 0.00003505
Iteration 34/1000 | Loss: 0.00003505
Iteration 35/1000 | Loss: 0.00003504
Iteration 36/1000 | Loss: 0.00003503
Iteration 37/1000 | Loss: 0.00003503
Iteration 38/1000 | Loss: 0.00003502
Iteration 39/1000 | Loss: 0.00003502
Iteration 40/1000 | Loss: 0.00003502
Iteration 41/1000 | Loss: 0.00003501
Iteration 42/1000 | Loss: 0.00003501
Iteration 43/1000 | Loss: 0.00003501
Iteration 44/1000 | Loss: 0.00003501
Iteration 45/1000 | Loss: 0.00003501
Iteration 46/1000 | Loss: 0.00003500
Iteration 47/1000 | Loss: 0.00003500
Iteration 48/1000 | Loss: 0.00003500
Iteration 49/1000 | Loss: 0.00003500
Iteration 50/1000 | Loss: 0.00003500
Iteration 51/1000 | Loss: 0.00003500
Iteration 52/1000 | Loss: 0.00003499
Iteration 53/1000 | Loss: 0.00003499
Iteration 54/1000 | Loss: 0.00003499
Iteration 55/1000 | Loss: 0.00003498
Iteration 56/1000 | Loss: 0.00003498
Iteration 57/1000 | Loss: 0.00003498
Iteration 58/1000 | Loss: 0.00003498
Iteration 59/1000 | Loss: 0.00003498
Iteration 60/1000 | Loss: 0.00003498
Iteration 61/1000 | Loss: 0.00003497
Iteration 62/1000 | Loss: 0.00003497
Iteration 63/1000 | Loss: 0.00003497
Iteration 64/1000 | Loss: 0.00003497
Iteration 65/1000 | Loss: 0.00003497
Iteration 66/1000 | Loss: 0.00003497
Iteration 67/1000 | Loss: 0.00003496
Iteration 68/1000 | Loss: 0.00003496
Iteration 69/1000 | Loss: 0.00003496
Iteration 70/1000 | Loss: 0.00003496
Iteration 71/1000 | Loss: 0.00003496
Iteration 72/1000 | Loss: 0.00003496
Iteration 73/1000 | Loss: 0.00003496
Iteration 74/1000 | Loss: 0.00003496
Iteration 75/1000 | Loss: 0.00003495
Iteration 76/1000 | Loss: 0.00003495
Iteration 77/1000 | Loss: 0.00003495
Iteration 78/1000 | Loss: 0.00003495
Iteration 79/1000 | Loss: 0.00003495
Iteration 80/1000 | Loss: 0.00003495
Iteration 81/1000 | Loss: 0.00003495
Iteration 82/1000 | Loss: 0.00003494
Iteration 83/1000 | Loss: 0.00003494
Iteration 84/1000 | Loss: 0.00003494
Iteration 85/1000 | Loss: 0.00003493
Iteration 86/1000 | Loss: 0.00003493
Iteration 87/1000 | Loss: 0.00003493
Iteration 88/1000 | Loss: 0.00003493
Iteration 89/1000 | Loss: 0.00003493
Iteration 90/1000 | Loss: 0.00003493
Iteration 91/1000 | Loss: 0.00003492
Iteration 92/1000 | Loss: 0.00003492
Iteration 93/1000 | Loss: 0.00003492
Iteration 94/1000 | Loss: 0.00003492
Iteration 95/1000 | Loss: 0.00003491
Iteration 96/1000 | Loss: 0.00003491
Iteration 97/1000 | Loss: 0.00003491
Iteration 98/1000 | Loss: 0.00003490
Iteration 99/1000 | Loss: 0.00003489
Iteration 100/1000 | Loss: 0.00003489
Iteration 101/1000 | Loss: 0.00003489
Iteration 102/1000 | Loss: 0.00003489
Iteration 103/1000 | Loss: 0.00003489
Iteration 104/1000 | Loss: 0.00003489
Iteration 105/1000 | Loss: 0.00003489
Iteration 106/1000 | Loss: 0.00003489
Iteration 107/1000 | Loss: 0.00003487
Iteration 108/1000 | Loss: 0.00003486
Iteration 109/1000 | Loss: 0.00003486
Iteration 110/1000 | Loss: 0.00003486
Iteration 111/1000 | Loss: 0.00003486
Iteration 112/1000 | Loss: 0.00003486
Iteration 113/1000 | Loss: 0.00003486
Iteration 114/1000 | Loss: 0.00003486
Iteration 115/1000 | Loss: 0.00003486
Iteration 116/1000 | Loss: 0.00003485
Iteration 117/1000 | Loss: 0.00003485
Iteration 118/1000 | Loss: 0.00003485
Iteration 119/1000 | Loss: 0.00003485
Iteration 120/1000 | Loss: 0.00003484
Iteration 121/1000 | Loss: 0.00003484
Iteration 122/1000 | Loss: 0.00003484
Iteration 123/1000 | Loss: 0.00003484
Iteration 124/1000 | Loss: 0.00003484
Iteration 125/1000 | Loss: 0.00003483
Iteration 126/1000 | Loss: 0.00003483
Iteration 127/1000 | Loss: 0.00003483
Iteration 128/1000 | Loss: 0.00003483
Iteration 129/1000 | Loss: 0.00003483
Iteration 130/1000 | Loss: 0.00003483
Iteration 131/1000 | Loss: 0.00003483
Iteration 132/1000 | Loss: 0.00003483
Iteration 133/1000 | Loss: 0.00003483
Iteration 134/1000 | Loss: 0.00003483
Iteration 135/1000 | Loss: 0.00003483
Iteration 136/1000 | Loss: 0.00003483
Iteration 137/1000 | Loss: 0.00003483
Iteration 138/1000 | Loss: 0.00003483
Iteration 139/1000 | Loss: 0.00003482
Iteration 140/1000 | Loss: 0.00003482
Iteration 141/1000 | Loss: 0.00003482
Iteration 142/1000 | Loss: 0.00003481
Iteration 143/1000 | Loss: 0.00003481
Iteration 144/1000 | Loss: 0.00003481
Iteration 145/1000 | Loss: 0.00003481
Iteration 146/1000 | Loss: 0.00003481
Iteration 147/1000 | Loss: 0.00003481
Iteration 148/1000 | Loss: 0.00003480
Iteration 149/1000 | Loss: 0.00003480
Iteration 150/1000 | Loss: 0.00003480
Iteration 151/1000 | Loss: 0.00003480
Iteration 152/1000 | Loss: 0.00003480
Iteration 153/1000 | Loss: 0.00003480
Iteration 154/1000 | Loss: 0.00003479
Iteration 155/1000 | Loss: 0.00003479
Iteration 156/1000 | Loss: 0.00003479
Iteration 157/1000 | Loss: 0.00003479
Iteration 158/1000 | Loss: 0.00003479
Iteration 159/1000 | Loss: 0.00003479
Iteration 160/1000 | Loss: 0.00003479
Iteration 161/1000 | Loss: 0.00003479
Iteration 162/1000 | Loss: 0.00003479
Iteration 163/1000 | Loss: 0.00003479
Iteration 164/1000 | Loss: 0.00003479
Iteration 165/1000 | Loss: 0.00003478
Iteration 166/1000 | Loss: 0.00003478
Iteration 167/1000 | Loss: 0.00003478
Iteration 168/1000 | Loss: 0.00003478
Iteration 169/1000 | Loss: 0.00003478
Iteration 170/1000 | Loss: 0.00003478
Iteration 171/1000 | Loss: 0.00003477
Iteration 172/1000 | Loss: 0.00003477
Iteration 173/1000 | Loss: 0.00003477
Iteration 174/1000 | Loss: 0.00003477
Iteration 175/1000 | Loss: 0.00003476
Iteration 176/1000 | Loss: 0.00003476
Iteration 177/1000 | Loss: 0.00003476
Iteration 178/1000 | Loss: 0.00003476
Iteration 179/1000 | Loss: 0.00003475
Iteration 180/1000 | Loss: 0.00003475
Iteration 181/1000 | Loss: 0.00003475
Iteration 182/1000 | Loss: 0.00003475
Iteration 183/1000 | Loss: 0.00003475
Iteration 184/1000 | Loss: 0.00003475
Iteration 185/1000 | Loss: 0.00003475
Iteration 186/1000 | Loss: 0.00003475
Iteration 187/1000 | Loss: 0.00003474
Iteration 188/1000 | Loss: 0.00003474
Iteration 189/1000 | Loss: 0.00003474
Iteration 190/1000 | Loss: 0.00003474
Iteration 191/1000 | Loss: 0.00003474
Iteration 192/1000 | Loss: 0.00003474
Iteration 193/1000 | Loss: 0.00003474
Iteration 194/1000 | Loss: 0.00003474
Iteration 195/1000 | Loss: 0.00003474
Iteration 196/1000 | Loss: 0.00003473
Iteration 197/1000 | Loss: 0.00003473
Iteration 198/1000 | Loss: 0.00003473
Iteration 199/1000 | Loss: 0.00003473
Iteration 200/1000 | Loss: 0.00003473
Iteration 201/1000 | Loss: 0.00003473
Iteration 202/1000 | Loss: 0.00003473
Iteration 203/1000 | Loss: 0.00003473
Iteration 204/1000 | Loss: 0.00003473
Iteration 205/1000 | Loss: 0.00003472
Iteration 206/1000 | Loss: 0.00003472
Iteration 207/1000 | Loss: 0.00003472
Iteration 208/1000 | Loss: 0.00003472
Iteration 209/1000 | Loss: 0.00003472
Iteration 210/1000 | Loss: 0.00003472
Iteration 211/1000 | Loss: 0.00003472
Iteration 212/1000 | Loss: 0.00003472
Iteration 213/1000 | Loss: 0.00003471
Iteration 214/1000 | Loss: 0.00003471
Iteration 215/1000 | Loss: 0.00003471
Iteration 216/1000 | Loss: 0.00003471
Iteration 217/1000 | Loss: 0.00003471
Iteration 218/1000 | Loss: 0.00003471
Iteration 219/1000 | Loss: 0.00003471
Iteration 220/1000 | Loss: 0.00003471
Iteration 221/1000 | Loss: 0.00003471
Iteration 222/1000 | Loss: 0.00003471
Iteration 223/1000 | Loss: 0.00003470
Iteration 224/1000 | Loss: 0.00003470
Iteration 225/1000 | Loss: 0.00003470
Iteration 226/1000 | Loss: 0.00003470
Iteration 227/1000 | Loss: 0.00003470
Iteration 228/1000 | Loss: 0.00003470
Iteration 229/1000 | Loss: 0.00003469
Iteration 230/1000 | Loss: 0.00003469
Iteration 231/1000 | Loss: 0.00003469
Iteration 232/1000 | Loss: 0.00003469
Iteration 233/1000 | Loss: 0.00003469
Iteration 234/1000 | Loss: 0.00003468
Iteration 235/1000 | Loss: 0.00003468
Iteration 236/1000 | Loss: 0.00003468
Iteration 237/1000 | Loss: 0.00003468
Iteration 238/1000 | Loss: 0.00003468
Iteration 239/1000 | Loss: 0.00003468
Iteration 240/1000 | Loss: 0.00003468
Iteration 241/1000 | Loss: 0.00003468
Iteration 242/1000 | Loss: 0.00003467
Iteration 243/1000 | Loss: 0.00003467
Iteration 244/1000 | Loss: 0.00003467
Iteration 245/1000 | Loss: 0.00003467
Iteration 246/1000 | Loss: 0.00003467
Iteration 247/1000 | Loss: 0.00003467
Iteration 248/1000 | Loss: 0.00003467
Iteration 249/1000 | Loss: 0.00003467
Iteration 250/1000 | Loss: 0.00003467
Iteration 251/1000 | Loss: 0.00003466
Iteration 252/1000 | Loss: 0.00003466
Iteration 253/1000 | Loss: 0.00003466
Iteration 254/1000 | Loss: 0.00003466
Iteration 255/1000 | Loss: 0.00003466
Iteration 256/1000 | Loss: 0.00003466
Iteration 257/1000 | Loss: 0.00003466
Iteration 258/1000 | Loss: 0.00003466
Iteration 259/1000 | Loss: 0.00003466
Iteration 260/1000 | Loss: 0.00003466
Iteration 261/1000 | Loss: 0.00003466
Iteration 262/1000 | Loss: 0.00003466
Iteration 263/1000 | Loss: 0.00003466
Iteration 264/1000 | Loss: 0.00003466
Iteration 265/1000 | Loss: 0.00003466
Iteration 266/1000 | Loss: 0.00003465
Iteration 267/1000 | Loss: 0.00003465
Iteration 268/1000 | Loss: 0.00003465
Iteration 269/1000 | Loss: 0.00003465
Iteration 270/1000 | Loss: 0.00003465
Iteration 271/1000 | Loss: 0.00003465
Iteration 272/1000 | Loss: 0.00003465
Iteration 273/1000 | Loss: 0.00003465
Iteration 274/1000 | Loss: 0.00003465
Iteration 275/1000 | Loss: 0.00003465
Iteration 276/1000 | Loss: 0.00003465
Iteration 277/1000 | Loss: 0.00003465
Iteration 278/1000 | Loss: 0.00003465
Iteration 279/1000 | Loss: 0.00003465
Iteration 280/1000 | Loss: 0.00003465
Iteration 281/1000 | Loss: 0.00003465
Iteration 282/1000 | Loss: 0.00003465
Iteration 283/1000 | Loss: 0.00003465
Iteration 284/1000 | Loss: 0.00003465
Iteration 285/1000 | Loss: 0.00003465
Iteration 286/1000 | Loss: 0.00003465
Iteration 287/1000 | Loss: 0.00003465
Iteration 288/1000 | Loss: 0.00003465
Iteration 289/1000 | Loss: 0.00003465
Iteration 290/1000 | Loss: 0.00003465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [3.46490451192949e-05, 3.46490451192949e-05, 3.46490451192949e-05, 3.46490451192949e-05, 3.46490451192949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.46490451192949e-05

Optimization complete. Final v2v error: 4.701927661895752 mm

Highest mean error: 6.158444881439209 mm for frame 158

Lowest mean error: 3.7911949157714844 mm for frame 196

Saving results

Total time: 63.40721321105957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878255
Iteration 2/25 | Loss: 0.00163671
Iteration 3/25 | Loss: 0.00135891
Iteration 4/25 | Loss: 0.00133862
Iteration 5/25 | Loss: 0.00133272
Iteration 6/25 | Loss: 0.00133028
Iteration 7/25 | Loss: 0.00133025
Iteration 8/25 | Loss: 0.00133021
Iteration 9/25 | Loss: 0.00133021
Iteration 10/25 | Loss: 0.00133021
Iteration 11/25 | Loss: 0.00133021
Iteration 12/25 | Loss: 0.00133021
Iteration 13/25 | Loss: 0.00133021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001330206636339426, 0.001330206636339426, 0.001330206636339426, 0.001330206636339426, 0.001330206636339426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001330206636339426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15326941
Iteration 2/25 | Loss: 0.00145046
Iteration 3/25 | Loss: 0.00145045
Iteration 4/25 | Loss: 0.00145045
Iteration 5/25 | Loss: 0.00145045
Iteration 6/25 | Loss: 0.00145045
Iteration 7/25 | Loss: 0.00145045
Iteration 8/25 | Loss: 0.00145045
Iteration 9/25 | Loss: 0.00145045
Iteration 10/25 | Loss: 0.00145045
Iteration 11/25 | Loss: 0.00145045
Iteration 12/25 | Loss: 0.00145045
Iteration 13/25 | Loss: 0.00145045
Iteration 14/25 | Loss: 0.00145045
Iteration 15/25 | Loss: 0.00145045
Iteration 16/25 | Loss: 0.00145045
Iteration 17/25 | Loss: 0.00145045
Iteration 18/25 | Loss: 0.00145045
Iteration 19/25 | Loss: 0.00145045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001450449344702065, 0.001450449344702065, 0.001450449344702065, 0.001450449344702065, 0.001450449344702065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001450449344702065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145045
Iteration 2/1000 | Loss: 0.00007122
Iteration 3/1000 | Loss: 0.00004330
Iteration 4/1000 | Loss: 0.00003225
Iteration 5/1000 | Loss: 0.00002982
Iteration 6/1000 | Loss: 0.00002837
Iteration 7/1000 | Loss: 0.00002749
Iteration 8/1000 | Loss: 0.00002671
Iteration 9/1000 | Loss: 0.00002618
Iteration 10/1000 | Loss: 0.00002566
Iteration 11/1000 | Loss: 0.00002537
Iteration 12/1000 | Loss: 0.00002516
Iteration 13/1000 | Loss: 0.00002501
Iteration 14/1000 | Loss: 0.00002481
Iteration 15/1000 | Loss: 0.00002478
Iteration 16/1000 | Loss: 0.00002461
Iteration 17/1000 | Loss: 0.00002445
Iteration 18/1000 | Loss: 0.00002442
Iteration 19/1000 | Loss: 0.00002441
Iteration 20/1000 | Loss: 0.00002441
Iteration 21/1000 | Loss: 0.00002440
Iteration 22/1000 | Loss: 0.00002440
Iteration 23/1000 | Loss: 0.00002438
Iteration 24/1000 | Loss: 0.00002437
Iteration 25/1000 | Loss: 0.00002435
Iteration 26/1000 | Loss: 0.00002433
Iteration 27/1000 | Loss: 0.00002433
Iteration 28/1000 | Loss: 0.00002433
Iteration 29/1000 | Loss: 0.00002432
Iteration 30/1000 | Loss: 0.00002430
Iteration 31/1000 | Loss: 0.00002429
Iteration 32/1000 | Loss: 0.00002429
Iteration 33/1000 | Loss: 0.00002429
Iteration 34/1000 | Loss: 0.00002429
Iteration 35/1000 | Loss: 0.00002429
Iteration 36/1000 | Loss: 0.00002429
Iteration 37/1000 | Loss: 0.00002429
Iteration 38/1000 | Loss: 0.00002429
Iteration 39/1000 | Loss: 0.00002429
Iteration 40/1000 | Loss: 0.00002429
Iteration 41/1000 | Loss: 0.00002428
Iteration 42/1000 | Loss: 0.00002427
Iteration 43/1000 | Loss: 0.00002427
Iteration 44/1000 | Loss: 0.00002426
Iteration 45/1000 | Loss: 0.00002426
Iteration 46/1000 | Loss: 0.00002426
Iteration 47/1000 | Loss: 0.00002425
Iteration 48/1000 | Loss: 0.00002425
Iteration 49/1000 | Loss: 0.00002424
Iteration 50/1000 | Loss: 0.00002424
Iteration 51/1000 | Loss: 0.00002423
Iteration 52/1000 | Loss: 0.00002423
Iteration 53/1000 | Loss: 0.00002423
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002420
Iteration 58/1000 | Loss: 0.00002419
Iteration 59/1000 | Loss: 0.00002419
Iteration 60/1000 | Loss: 0.00002414
Iteration 61/1000 | Loss: 0.00002413
Iteration 62/1000 | Loss: 0.00002413
Iteration 63/1000 | Loss: 0.00002413
Iteration 64/1000 | Loss: 0.00002413
Iteration 65/1000 | Loss: 0.00002413
Iteration 66/1000 | Loss: 0.00002412
Iteration 67/1000 | Loss: 0.00002412
Iteration 68/1000 | Loss: 0.00002412
Iteration 69/1000 | Loss: 0.00002412
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002411
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002409
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002409
Iteration 82/1000 | Loss: 0.00002408
Iteration 83/1000 | Loss: 0.00002408
Iteration 84/1000 | Loss: 0.00002408
Iteration 85/1000 | Loss: 0.00002408
Iteration 86/1000 | Loss: 0.00002408
Iteration 87/1000 | Loss: 0.00002408
Iteration 88/1000 | Loss: 0.00002408
Iteration 89/1000 | Loss: 0.00002407
Iteration 90/1000 | Loss: 0.00002407
Iteration 91/1000 | Loss: 0.00002406
Iteration 92/1000 | Loss: 0.00002406
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002406
Iteration 95/1000 | Loss: 0.00002406
Iteration 96/1000 | Loss: 0.00002406
Iteration 97/1000 | Loss: 0.00002405
Iteration 98/1000 | Loss: 0.00002405
Iteration 99/1000 | Loss: 0.00002405
Iteration 100/1000 | Loss: 0.00002405
Iteration 101/1000 | Loss: 0.00002405
Iteration 102/1000 | Loss: 0.00002404
Iteration 103/1000 | Loss: 0.00002404
Iteration 104/1000 | Loss: 0.00002404
Iteration 105/1000 | Loss: 0.00002404
Iteration 106/1000 | Loss: 0.00002404
Iteration 107/1000 | Loss: 0.00002404
Iteration 108/1000 | Loss: 0.00002404
Iteration 109/1000 | Loss: 0.00002404
Iteration 110/1000 | Loss: 0.00002404
Iteration 111/1000 | Loss: 0.00002404
Iteration 112/1000 | Loss: 0.00002404
Iteration 113/1000 | Loss: 0.00002404
Iteration 114/1000 | Loss: 0.00002404
Iteration 115/1000 | Loss: 0.00002404
Iteration 116/1000 | Loss: 0.00002404
Iteration 117/1000 | Loss: 0.00002404
Iteration 118/1000 | Loss: 0.00002404
Iteration 119/1000 | Loss: 0.00002404
Iteration 120/1000 | Loss: 0.00002404
Iteration 121/1000 | Loss: 0.00002404
Iteration 122/1000 | Loss: 0.00002404
Iteration 123/1000 | Loss: 0.00002404
Iteration 124/1000 | Loss: 0.00002404
Iteration 125/1000 | Loss: 0.00002404
Iteration 126/1000 | Loss: 0.00002404
Iteration 127/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.4040822609094903e-05, 2.4040822609094903e-05, 2.4040822609094903e-05, 2.4040822609094903e-05, 2.4040822609094903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4040822609094903e-05

Optimization complete. Final v2v error: 3.98392653465271 mm

Highest mean error: 5.311135292053223 mm for frame 62

Lowest mean error: 3.4215734004974365 mm for frame 89

Saving results

Total time: 42.31093454360962
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642662
Iteration 2/25 | Loss: 0.00187478
Iteration 3/25 | Loss: 0.00151393
Iteration 4/25 | Loss: 0.00147692
Iteration 5/25 | Loss: 0.00147225
Iteration 6/25 | Loss: 0.00147181
Iteration 7/25 | Loss: 0.00147181
Iteration 8/25 | Loss: 0.00147181
Iteration 9/25 | Loss: 0.00147181
Iteration 10/25 | Loss: 0.00147181
Iteration 11/25 | Loss: 0.00147181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014718050369992852, 0.0014718050369992852, 0.0014718050369992852, 0.0014718050369992852, 0.0014718050369992852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014718050369992852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23853993
Iteration 2/25 | Loss: 0.00094832
Iteration 3/25 | Loss: 0.00094830
Iteration 4/25 | Loss: 0.00094829
Iteration 5/25 | Loss: 0.00094829
Iteration 6/25 | Loss: 0.00094829
Iteration 7/25 | Loss: 0.00094829
Iteration 8/25 | Loss: 0.00094829
Iteration 9/25 | Loss: 0.00094829
Iteration 10/25 | Loss: 0.00094829
Iteration 11/25 | Loss: 0.00094829
Iteration 12/25 | Loss: 0.00094829
Iteration 13/25 | Loss: 0.00094829
Iteration 14/25 | Loss: 0.00094829
Iteration 15/25 | Loss: 0.00094829
Iteration 16/25 | Loss: 0.00094829
Iteration 17/25 | Loss: 0.00094829
Iteration 18/25 | Loss: 0.00094829
Iteration 19/25 | Loss: 0.00094829
Iteration 20/25 | Loss: 0.00094829
Iteration 21/25 | Loss: 0.00094829
Iteration 22/25 | Loss: 0.00094829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009482921450398862, 0.0009482921450398862, 0.0009482921450398862, 0.0009482921450398862, 0.0009482921450398862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009482921450398862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094829
Iteration 2/1000 | Loss: 0.00006806
Iteration 3/1000 | Loss: 0.00003867
Iteration 4/1000 | Loss: 0.00002864
Iteration 5/1000 | Loss: 0.00002541
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002153
Iteration 9/1000 | Loss: 0.00002077
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00002000
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001942
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001930
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001927
Iteration 23/1000 | Loss: 0.00001927
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001923
Iteration 30/1000 | Loss: 0.00001923
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001922
Iteration 36/1000 | Loss: 0.00001922
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001921
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001919
Iteration 44/1000 | Loss: 0.00001919
Iteration 45/1000 | Loss: 0.00001918
Iteration 46/1000 | Loss: 0.00001918
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001917
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001916
Iteration 55/1000 | Loss: 0.00001916
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001912
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001911
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001910
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001909
Iteration 82/1000 | Loss: 0.00001909
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001909
Iteration 85/1000 | Loss: 0.00001908
Iteration 86/1000 | Loss: 0.00001908
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001907
Iteration 93/1000 | Loss: 0.00001907
Iteration 94/1000 | Loss: 0.00001907
Iteration 95/1000 | Loss: 0.00001907
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001902
Iteration 115/1000 | Loss: 0.00001902
Iteration 116/1000 | Loss: 0.00001902
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001902
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001901
Iteration 123/1000 | Loss: 0.00001901
Iteration 124/1000 | Loss: 0.00001901
Iteration 125/1000 | Loss: 0.00001901
Iteration 126/1000 | Loss: 0.00001900
Iteration 127/1000 | Loss: 0.00001900
Iteration 128/1000 | Loss: 0.00001900
Iteration 129/1000 | Loss: 0.00001900
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001900
Iteration 133/1000 | Loss: 0.00001900
Iteration 134/1000 | Loss: 0.00001900
Iteration 135/1000 | Loss: 0.00001900
Iteration 136/1000 | Loss: 0.00001899
Iteration 137/1000 | Loss: 0.00001899
Iteration 138/1000 | Loss: 0.00001898
Iteration 139/1000 | Loss: 0.00001898
Iteration 140/1000 | Loss: 0.00001897
Iteration 141/1000 | Loss: 0.00001897
Iteration 142/1000 | Loss: 0.00001897
Iteration 143/1000 | Loss: 0.00001896
Iteration 144/1000 | Loss: 0.00001896
Iteration 145/1000 | Loss: 0.00001896
Iteration 146/1000 | Loss: 0.00001896
Iteration 147/1000 | Loss: 0.00001896
Iteration 148/1000 | Loss: 0.00001896
Iteration 149/1000 | Loss: 0.00001896
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001895
Iteration 153/1000 | Loss: 0.00001895
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001895
Iteration 156/1000 | Loss: 0.00001894
Iteration 157/1000 | Loss: 0.00001894
Iteration 158/1000 | Loss: 0.00001894
Iteration 159/1000 | Loss: 0.00001894
Iteration 160/1000 | Loss: 0.00001894
Iteration 161/1000 | Loss: 0.00001894
Iteration 162/1000 | Loss: 0.00001893
Iteration 163/1000 | Loss: 0.00001893
Iteration 164/1000 | Loss: 0.00001893
Iteration 165/1000 | Loss: 0.00001893
Iteration 166/1000 | Loss: 0.00001893
Iteration 167/1000 | Loss: 0.00001893
Iteration 168/1000 | Loss: 0.00001893
Iteration 169/1000 | Loss: 0.00001893
Iteration 170/1000 | Loss: 0.00001892
Iteration 171/1000 | Loss: 0.00001892
Iteration 172/1000 | Loss: 0.00001891
Iteration 173/1000 | Loss: 0.00001891
Iteration 174/1000 | Loss: 0.00001891
Iteration 175/1000 | Loss: 0.00001891
Iteration 176/1000 | Loss: 0.00001891
Iteration 177/1000 | Loss: 0.00001891
Iteration 178/1000 | Loss: 0.00001891
Iteration 179/1000 | Loss: 0.00001891
Iteration 180/1000 | Loss: 0.00001891
Iteration 181/1000 | Loss: 0.00001891
Iteration 182/1000 | Loss: 0.00001890
Iteration 183/1000 | Loss: 0.00001890
Iteration 184/1000 | Loss: 0.00001890
Iteration 185/1000 | Loss: 0.00001890
Iteration 186/1000 | Loss: 0.00001890
Iteration 187/1000 | Loss: 0.00001890
Iteration 188/1000 | Loss: 0.00001890
Iteration 189/1000 | Loss: 0.00001890
Iteration 190/1000 | Loss: 0.00001889
Iteration 191/1000 | Loss: 0.00001889
Iteration 192/1000 | Loss: 0.00001889
Iteration 193/1000 | Loss: 0.00001889
Iteration 194/1000 | Loss: 0.00001889
Iteration 195/1000 | Loss: 0.00001889
Iteration 196/1000 | Loss: 0.00001889
Iteration 197/1000 | Loss: 0.00001889
Iteration 198/1000 | Loss: 0.00001889
Iteration 199/1000 | Loss: 0.00001889
Iteration 200/1000 | Loss: 0.00001889
Iteration 201/1000 | Loss: 0.00001889
Iteration 202/1000 | Loss: 0.00001888
Iteration 203/1000 | Loss: 0.00001888
Iteration 204/1000 | Loss: 0.00001888
Iteration 205/1000 | Loss: 0.00001888
Iteration 206/1000 | Loss: 0.00001888
Iteration 207/1000 | Loss: 0.00001888
Iteration 208/1000 | Loss: 0.00001888
Iteration 209/1000 | Loss: 0.00001888
Iteration 210/1000 | Loss: 0.00001888
Iteration 211/1000 | Loss: 0.00001888
Iteration 212/1000 | Loss: 0.00001888
Iteration 213/1000 | Loss: 0.00001888
Iteration 214/1000 | Loss: 0.00001888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.887929829536006e-05, 1.887929829536006e-05, 1.887929829536006e-05, 1.887929829536006e-05, 1.887929829536006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.887929829536006e-05

Optimization complete. Final v2v error: 3.7285678386688232 mm

Highest mean error: 3.939702272415161 mm for frame 36

Lowest mean error: 3.5821433067321777 mm for frame 5

Saving results

Total time: 42.58135008811951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058317
Iteration 2/25 | Loss: 0.00277386
Iteration 3/25 | Loss: 0.00173104
Iteration 4/25 | Loss: 0.00141812
Iteration 5/25 | Loss: 0.00126018
Iteration 6/25 | Loss: 0.00120620
Iteration 7/25 | Loss: 0.00118881
Iteration 8/25 | Loss: 0.00117262
Iteration 9/25 | Loss: 0.00111452
Iteration 10/25 | Loss: 0.00109525
Iteration 11/25 | Loss: 0.00109031
Iteration 12/25 | Loss: 0.00107641
Iteration 13/25 | Loss: 0.00107770
Iteration 14/25 | Loss: 0.00107241
Iteration 15/25 | Loss: 0.00107181
Iteration 16/25 | Loss: 0.00107103
Iteration 17/25 | Loss: 0.00107153
Iteration 18/25 | Loss: 0.00107082
Iteration 19/25 | Loss: 0.00107046
Iteration 20/25 | Loss: 0.00107004
Iteration 21/25 | Loss: 0.00107019
Iteration 22/25 | Loss: 0.00106938
Iteration 23/25 | Loss: 0.00107015
Iteration 24/25 | Loss: 0.00107060
Iteration 25/25 | Loss: 0.00107000

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81984496
Iteration 2/25 | Loss: 0.00185355
Iteration 3/25 | Loss: 0.00185355
Iteration 4/25 | Loss: 0.00185354
Iteration 5/25 | Loss: 0.00185354
Iteration 6/25 | Loss: 0.00185354
Iteration 7/25 | Loss: 0.00185354
Iteration 8/25 | Loss: 0.00185354
Iteration 9/25 | Loss: 0.00185354
Iteration 10/25 | Loss: 0.00185354
Iteration 11/25 | Loss: 0.00185354
Iteration 12/25 | Loss: 0.00185354
Iteration 13/25 | Loss: 0.00185354
Iteration 14/25 | Loss: 0.00185354
Iteration 15/25 | Loss: 0.00185354
Iteration 16/25 | Loss: 0.00185354
Iteration 17/25 | Loss: 0.00185354
Iteration 18/25 | Loss: 0.00185354
Iteration 19/25 | Loss: 0.00185354
Iteration 20/25 | Loss: 0.00185354
Iteration 21/25 | Loss: 0.00185354
Iteration 22/25 | Loss: 0.00185354
Iteration 23/25 | Loss: 0.00185354
Iteration 24/25 | Loss: 0.00185354
Iteration 25/25 | Loss: 0.00185354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185354
Iteration 2/1000 | Loss: 0.00005192
Iteration 3/1000 | Loss: 0.00003709
Iteration 4/1000 | Loss: 0.00003020
Iteration 5/1000 | Loss: 0.00006196
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00009425
Iteration 8/1000 | Loss: 0.00004410
Iteration 9/1000 | Loss: 0.00002474
Iteration 10/1000 | Loss: 0.00006096
Iteration 11/1000 | Loss: 0.00002713
Iteration 12/1000 | Loss: 0.00002707
Iteration 13/1000 | Loss: 0.00002742
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00003261
Iteration 16/1000 | Loss: 0.00002387
Iteration 17/1000 | Loss: 0.00002386
Iteration 18/1000 | Loss: 0.00002385
Iteration 19/1000 | Loss: 0.00002385
Iteration 20/1000 | Loss: 0.00002367
Iteration 21/1000 | Loss: 0.00002362
Iteration 22/1000 | Loss: 0.00002359
Iteration 23/1000 | Loss: 0.00002358
Iteration 24/1000 | Loss: 0.00002358
Iteration 25/1000 | Loss: 0.00002358
Iteration 26/1000 | Loss: 0.00002357
Iteration 27/1000 | Loss: 0.00002356
Iteration 28/1000 | Loss: 0.00002356
Iteration 29/1000 | Loss: 0.00002355
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002355
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002353
Iteration 36/1000 | Loss: 0.00002352
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00002351
Iteration 39/1000 | Loss: 0.00002351
Iteration 40/1000 | Loss: 0.00002351
Iteration 41/1000 | Loss: 0.00002351
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002350
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002350
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002346
Iteration 51/1000 | Loss: 0.00002346
Iteration 52/1000 | Loss: 0.00002346
Iteration 53/1000 | Loss: 0.00002346
Iteration 54/1000 | Loss: 0.00002346
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002345
Iteration 59/1000 | Loss: 0.00002344
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002343
Iteration 62/1000 | Loss: 0.00002342
Iteration 63/1000 | Loss: 0.00002341
Iteration 64/1000 | Loss: 0.00002561
Iteration 65/1000 | Loss: 0.00002342
Iteration 66/1000 | Loss: 0.00002342
Iteration 67/1000 | Loss: 0.00005749
Iteration 68/1000 | Loss: 0.00008563
Iteration 69/1000 | Loss: 0.00003139
Iteration 70/1000 | Loss: 0.00002721
Iteration 71/1000 | Loss: 0.00002445
Iteration 72/1000 | Loss: 0.00002330
Iteration 73/1000 | Loss: 0.00002330
Iteration 74/1000 | Loss: 0.00002330
Iteration 75/1000 | Loss: 0.00002330
Iteration 76/1000 | Loss: 0.00002330
Iteration 77/1000 | Loss: 0.00002330
Iteration 78/1000 | Loss: 0.00002330
Iteration 79/1000 | Loss: 0.00002329
Iteration 80/1000 | Loss: 0.00002329
Iteration 81/1000 | Loss: 0.00002329
Iteration 82/1000 | Loss: 0.00002329
Iteration 83/1000 | Loss: 0.00002329
Iteration 84/1000 | Loss: 0.00002329
Iteration 85/1000 | Loss: 0.00002352
Iteration 86/1000 | Loss: 0.00002328
Iteration 87/1000 | Loss: 0.00002328
Iteration 88/1000 | Loss: 0.00002327
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002327
Iteration 91/1000 | Loss: 0.00002327
Iteration 92/1000 | Loss: 0.00002327
Iteration 93/1000 | Loss: 0.00002327
Iteration 94/1000 | Loss: 0.00002327
Iteration 95/1000 | Loss: 0.00002327
Iteration 96/1000 | Loss: 0.00002327
Iteration 97/1000 | Loss: 0.00002327
Iteration 98/1000 | Loss: 0.00002327
Iteration 99/1000 | Loss: 0.00002327
Iteration 100/1000 | Loss: 0.00002327
Iteration 101/1000 | Loss: 0.00002327
Iteration 102/1000 | Loss: 0.00002327
Iteration 103/1000 | Loss: 0.00002327
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00002326
Iteration 113/1000 | Loss: 0.00002326
Iteration 114/1000 | Loss: 0.00002326
Iteration 115/1000 | Loss: 0.00002326
Iteration 116/1000 | Loss: 0.00002326
Iteration 117/1000 | Loss: 0.00002326
Iteration 118/1000 | Loss: 0.00002326
Iteration 119/1000 | Loss: 0.00002326
Iteration 120/1000 | Loss: 0.00002326
Iteration 121/1000 | Loss: 0.00002326
Iteration 122/1000 | Loss: 0.00002326
Iteration 123/1000 | Loss: 0.00002326
Iteration 124/1000 | Loss: 0.00002326
Iteration 125/1000 | Loss: 0.00002326
Iteration 126/1000 | Loss: 0.00002326
Iteration 127/1000 | Loss: 0.00002326
Iteration 128/1000 | Loss: 0.00002326
Iteration 129/1000 | Loss: 0.00002326
Iteration 130/1000 | Loss: 0.00002326
Iteration 131/1000 | Loss: 0.00002326
Iteration 132/1000 | Loss: 0.00002326
Iteration 133/1000 | Loss: 0.00002326
Iteration 134/1000 | Loss: 0.00002326
Iteration 135/1000 | Loss: 0.00002326
Iteration 136/1000 | Loss: 0.00002326
Iteration 137/1000 | Loss: 0.00002326
Iteration 138/1000 | Loss: 0.00002326
Iteration 139/1000 | Loss: 0.00002326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.326387766515836e-05, 2.326387766515836e-05, 2.326387766515836e-05, 2.326387766515836e-05, 2.326387766515836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.326387766515836e-05

Optimization complete. Final v2v error: 3.638544797897339 mm

Highest mean error: 8.495474815368652 mm for frame 55

Lowest mean error: 2.8820977210998535 mm for frame 27

Saving results

Total time: 83.69518065452576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826814
Iteration 2/25 | Loss: 0.00152039
Iteration 3/25 | Loss: 0.00136459
Iteration 4/25 | Loss: 0.00134485
Iteration 5/25 | Loss: 0.00133870
Iteration 6/25 | Loss: 0.00133638
Iteration 7/25 | Loss: 0.00133621
Iteration 8/25 | Loss: 0.00133621
Iteration 9/25 | Loss: 0.00133621
Iteration 10/25 | Loss: 0.00133621
Iteration 11/25 | Loss: 0.00133621
Iteration 12/25 | Loss: 0.00133621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00133620947599411, 0.00133620947599411, 0.00133620947599411, 0.00133620947599411, 0.00133620947599411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133620947599411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84763193
Iteration 2/25 | Loss: 0.00160764
Iteration 3/25 | Loss: 0.00160764
Iteration 4/25 | Loss: 0.00160764
Iteration 5/25 | Loss: 0.00160764
Iteration 6/25 | Loss: 0.00160764
Iteration 7/25 | Loss: 0.00160764
Iteration 8/25 | Loss: 0.00160764
Iteration 9/25 | Loss: 0.00160764
Iteration 10/25 | Loss: 0.00160764
Iteration 11/25 | Loss: 0.00160764
Iteration 12/25 | Loss: 0.00160764
Iteration 13/25 | Loss: 0.00160764
Iteration 14/25 | Loss: 0.00160764
Iteration 15/25 | Loss: 0.00160764
Iteration 16/25 | Loss: 0.00160764
Iteration 17/25 | Loss: 0.00160764
Iteration 18/25 | Loss: 0.00160764
Iteration 19/25 | Loss: 0.00160764
Iteration 20/25 | Loss: 0.00160764
Iteration 21/25 | Loss: 0.00160764
Iteration 22/25 | Loss: 0.00160764
Iteration 23/25 | Loss: 0.00160764
Iteration 24/25 | Loss: 0.00160764
Iteration 25/25 | Loss: 0.00160764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160764
Iteration 2/1000 | Loss: 0.00004852
Iteration 3/1000 | Loss: 0.00003032
Iteration 4/1000 | Loss: 0.00002518
Iteration 5/1000 | Loss: 0.00002325
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002155
Iteration 8/1000 | Loss: 0.00002114
Iteration 9/1000 | Loss: 0.00002087
Iteration 10/1000 | Loss: 0.00002072
Iteration 11/1000 | Loss: 0.00002059
Iteration 12/1000 | Loss: 0.00002058
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002046
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00002035
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002033
Iteration 21/1000 | Loss: 0.00002032
Iteration 22/1000 | Loss: 0.00002032
Iteration 23/1000 | Loss: 0.00002027
Iteration 24/1000 | Loss: 0.00002025
Iteration 25/1000 | Loss: 0.00002025
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002020
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00002019
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00002017
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002016
Iteration 37/1000 | Loss: 0.00002013
Iteration 38/1000 | Loss: 0.00002013
Iteration 39/1000 | Loss: 0.00002013
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002012
Iteration 42/1000 | Loss: 0.00002012
Iteration 43/1000 | Loss: 0.00002012
Iteration 44/1000 | Loss: 0.00002012
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002009
Iteration 47/1000 | Loss: 0.00002008
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002007
Iteration 51/1000 | Loss: 0.00002006
Iteration 52/1000 | Loss: 0.00002006
Iteration 53/1000 | Loss: 0.00002006
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002006
Iteration 56/1000 | Loss: 0.00002005
Iteration 57/1000 | Loss: 0.00002005
Iteration 58/1000 | Loss: 0.00002005
Iteration 59/1000 | Loss: 0.00002004
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002003
Iteration 63/1000 | Loss: 0.00002002
Iteration 64/1000 | Loss: 0.00002002
Iteration 65/1000 | Loss: 0.00002002
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002001
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002000
Iteration 72/1000 | Loss: 0.00002000
Iteration 73/1000 | Loss: 0.00002000
Iteration 74/1000 | Loss: 0.00002000
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001999
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00001999
Iteration 88/1000 | Loss: 0.00001999
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Iteration 91/1000 | Loss: 0.00001997
Iteration 92/1000 | Loss: 0.00001997
Iteration 93/1000 | Loss: 0.00001997
Iteration 94/1000 | Loss: 0.00001997
Iteration 95/1000 | Loss: 0.00001997
Iteration 96/1000 | Loss: 0.00001997
Iteration 97/1000 | Loss: 0.00001997
Iteration 98/1000 | Loss: 0.00001997
Iteration 99/1000 | Loss: 0.00001997
Iteration 100/1000 | Loss: 0.00001997
Iteration 101/1000 | Loss: 0.00001997
Iteration 102/1000 | Loss: 0.00001997
Iteration 103/1000 | Loss: 0.00001996
Iteration 104/1000 | Loss: 0.00001996
Iteration 105/1000 | Loss: 0.00001996
Iteration 106/1000 | Loss: 0.00001996
Iteration 107/1000 | Loss: 0.00001996
Iteration 108/1000 | Loss: 0.00001996
Iteration 109/1000 | Loss: 0.00001996
Iteration 110/1000 | Loss: 0.00001996
Iteration 111/1000 | Loss: 0.00001996
Iteration 112/1000 | Loss: 0.00001996
Iteration 113/1000 | Loss: 0.00001996
Iteration 114/1000 | Loss: 0.00001996
Iteration 115/1000 | Loss: 0.00001996
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001996
Iteration 118/1000 | Loss: 0.00001996
Iteration 119/1000 | Loss: 0.00001996
Iteration 120/1000 | Loss: 0.00001996
Iteration 121/1000 | Loss: 0.00001996
Iteration 122/1000 | Loss: 0.00001996
Iteration 123/1000 | Loss: 0.00001996
Iteration 124/1000 | Loss: 0.00001996
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001996
Iteration 127/1000 | Loss: 0.00001996
Iteration 128/1000 | Loss: 0.00001996
Iteration 129/1000 | Loss: 0.00001996
Iteration 130/1000 | Loss: 0.00001996
Iteration 131/1000 | Loss: 0.00001996
Iteration 132/1000 | Loss: 0.00001996
Iteration 133/1000 | Loss: 0.00001996
Iteration 134/1000 | Loss: 0.00001996
Iteration 135/1000 | Loss: 0.00001996
Iteration 136/1000 | Loss: 0.00001996
Iteration 137/1000 | Loss: 0.00001996
Iteration 138/1000 | Loss: 0.00001996
Iteration 139/1000 | Loss: 0.00001996
Iteration 140/1000 | Loss: 0.00001996
Iteration 141/1000 | Loss: 0.00001996
Iteration 142/1000 | Loss: 0.00001996
Iteration 143/1000 | Loss: 0.00001996
Iteration 144/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.996364699152764e-05, 1.996364699152764e-05, 1.996364699152764e-05, 1.996364699152764e-05, 1.996364699152764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.996364699152764e-05

Optimization complete. Final v2v error: 3.8243236541748047 mm

Highest mean error: 4.306230545043945 mm for frame 94

Lowest mean error: 3.511704683303833 mm for frame 6

Saving results

Total time: 37.66009044647217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622086
Iteration 2/25 | Loss: 0.00140378
Iteration 3/25 | Loss: 0.00132407
Iteration 4/25 | Loss: 0.00131631
Iteration 5/25 | Loss: 0.00131275
Iteration 6/25 | Loss: 0.00131188
Iteration 7/25 | Loss: 0.00131188
Iteration 8/25 | Loss: 0.00131188
Iteration 9/25 | Loss: 0.00131188
Iteration 10/25 | Loss: 0.00131188
Iteration 11/25 | Loss: 0.00131188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013118835631757975, 0.0013118835631757975, 0.0013118835631757975, 0.0013118835631757975, 0.0013118835631757975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013118835631757975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95274019
Iteration 2/25 | Loss: 0.00152160
Iteration 3/25 | Loss: 0.00152160
Iteration 4/25 | Loss: 0.00152160
Iteration 5/25 | Loss: 0.00152160
Iteration 6/25 | Loss: 0.00152160
Iteration 7/25 | Loss: 0.00152160
Iteration 8/25 | Loss: 0.00152159
Iteration 9/25 | Loss: 0.00152159
Iteration 10/25 | Loss: 0.00152159
Iteration 11/25 | Loss: 0.00152159
Iteration 12/25 | Loss: 0.00152159
Iteration 13/25 | Loss: 0.00152159
Iteration 14/25 | Loss: 0.00152159
Iteration 15/25 | Loss: 0.00152159
Iteration 16/25 | Loss: 0.00152159
Iteration 17/25 | Loss: 0.00152159
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001521594705991447, 0.001521594705991447, 0.001521594705991447, 0.001521594705991447, 0.001521594705991447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001521594705991447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152159
Iteration 2/1000 | Loss: 0.00004055
Iteration 3/1000 | Loss: 0.00002612
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00002061
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001956
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001924
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001908
Iteration 15/1000 | Loss: 0.00001897
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001893
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001891
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001889
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001886
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001885
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001879
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001878
Iteration 65/1000 | Loss: 0.00001878
Iteration 66/1000 | Loss: 0.00001878
Iteration 67/1000 | Loss: 0.00001878
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001877
Iteration 71/1000 | Loss: 0.00001877
Iteration 72/1000 | Loss: 0.00001877
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001876
Iteration 77/1000 | Loss: 0.00001876
Iteration 78/1000 | Loss: 0.00001875
Iteration 79/1000 | Loss: 0.00001875
Iteration 80/1000 | Loss: 0.00001875
Iteration 81/1000 | Loss: 0.00001875
Iteration 82/1000 | Loss: 0.00001875
Iteration 83/1000 | Loss: 0.00001875
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001874
Iteration 86/1000 | Loss: 0.00001874
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001874
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001870
Iteration 110/1000 | Loss: 0.00001869
Iteration 111/1000 | Loss: 0.00001869
Iteration 112/1000 | Loss: 0.00001869
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Iteration 123/1000 | Loss: 0.00001868
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001868
Iteration 130/1000 | Loss: 0.00001868
Iteration 131/1000 | Loss: 0.00001868
Iteration 132/1000 | Loss: 0.00001868
Iteration 133/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.8678769265534356e-05, 1.8678769265534356e-05, 1.8678769265534356e-05, 1.8678769265534356e-05, 1.8678769265534356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8678769265534356e-05

Optimization complete. Final v2v error: 3.649055242538452 mm

Highest mean error: 3.9022955894470215 mm for frame 66

Lowest mean error: 3.341796636581421 mm for frame 120

Saving results

Total time: 33.04505205154419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768772
Iteration 2/25 | Loss: 0.00183076
Iteration 3/25 | Loss: 0.00158732
Iteration 4/25 | Loss: 0.00153853
Iteration 5/25 | Loss: 0.00152303
Iteration 6/25 | Loss: 0.00151906
Iteration 7/25 | Loss: 0.00151730
Iteration 8/25 | Loss: 0.00151715
Iteration 9/25 | Loss: 0.00151715
Iteration 10/25 | Loss: 0.00151715
Iteration 11/25 | Loss: 0.00151715
Iteration 12/25 | Loss: 0.00151715
Iteration 13/25 | Loss: 0.00151715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015171475242823362, 0.0015171475242823362, 0.0015171475242823362, 0.0015171475242823362, 0.0015171475242823362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015171475242823362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19590104
Iteration 2/25 | Loss: 0.00164834
Iteration 3/25 | Loss: 0.00164827
Iteration 4/25 | Loss: 0.00164827
Iteration 5/25 | Loss: 0.00164827
Iteration 6/25 | Loss: 0.00164827
Iteration 7/25 | Loss: 0.00164827
Iteration 8/25 | Loss: 0.00164827
Iteration 9/25 | Loss: 0.00164827
Iteration 10/25 | Loss: 0.00164827
Iteration 11/25 | Loss: 0.00164827
Iteration 12/25 | Loss: 0.00164827
Iteration 13/25 | Loss: 0.00164827
Iteration 14/25 | Loss: 0.00164827
Iteration 15/25 | Loss: 0.00164827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016482708742842078, 0.0016482708742842078, 0.0016482708742842078, 0.0016482708742842078, 0.0016482708742842078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016482708742842078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164827
Iteration 2/1000 | Loss: 0.00011854
Iteration 3/1000 | Loss: 0.00007077
Iteration 4/1000 | Loss: 0.00005610
Iteration 5/1000 | Loss: 0.00005170
Iteration 6/1000 | Loss: 0.00004922
Iteration 7/1000 | Loss: 0.00004779
Iteration 8/1000 | Loss: 0.00004638
Iteration 9/1000 | Loss: 0.00004503
Iteration 10/1000 | Loss: 0.00004375
Iteration 11/1000 | Loss: 0.00004253
Iteration 12/1000 | Loss: 0.00004158
Iteration 13/1000 | Loss: 0.00004099
Iteration 14/1000 | Loss: 0.00004046
Iteration 15/1000 | Loss: 0.00004011
Iteration 16/1000 | Loss: 0.00003978
Iteration 17/1000 | Loss: 0.00003967
Iteration 18/1000 | Loss: 0.00003951
Iteration 19/1000 | Loss: 0.00003945
Iteration 20/1000 | Loss: 0.00003939
Iteration 21/1000 | Loss: 0.00003935
Iteration 22/1000 | Loss: 0.00003934
Iteration 23/1000 | Loss: 0.00003933
Iteration 24/1000 | Loss: 0.00003923
Iteration 25/1000 | Loss: 0.00003923
Iteration 26/1000 | Loss: 0.00003922
Iteration 27/1000 | Loss: 0.00003922
Iteration 28/1000 | Loss: 0.00003922
Iteration 29/1000 | Loss: 0.00003921
Iteration 30/1000 | Loss: 0.00003921
Iteration 31/1000 | Loss: 0.00003920
Iteration 32/1000 | Loss: 0.00003920
Iteration 33/1000 | Loss: 0.00003920
Iteration 34/1000 | Loss: 0.00003919
Iteration 35/1000 | Loss: 0.00003919
Iteration 36/1000 | Loss: 0.00003919
Iteration 37/1000 | Loss: 0.00003918
Iteration 38/1000 | Loss: 0.00003917
Iteration 39/1000 | Loss: 0.00003917
Iteration 40/1000 | Loss: 0.00003917
Iteration 41/1000 | Loss: 0.00003916
Iteration 42/1000 | Loss: 0.00003916
Iteration 43/1000 | Loss: 0.00003916
Iteration 44/1000 | Loss: 0.00003915
Iteration 45/1000 | Loss: 0.00003915
Iteration 46/1000 | Loss: 0.00003914
Iteration 47/1000 | Loss: 0.00003914
Iteration 48/1000 | Loss: 0.00003914
Iteration 49/1000 | Loss: 0.00003913
Iteration 50/1000 | Loss: 0.00003913
Iteration 51/1000 | Loss: 0.00003913
Iteration 52/1000 | Loss: 0.00003912
Iteration 53/1000 | Loss: 0.00003912
Iteration 54/1000 | Loss: 0.00003912
Iteration 55/1000 | Loss: 0.00003912
Iteration 56/1000 | Loss: 0.00003912
Iteration 57/1000 | Loss: 0.00003912
Iteration 58/1000 | Loss: 0.00003911
Iteration 59/1000 | Loss: 0.00003911
Iteration 60/1000 | Loss: 0.00003911
Iteration 61/1000 | Loss: 0.00003911
Iteration 62/1000 | Loss: 0.00003910
Iteration 63/1000 | Loss: 0.00003910
Iteration 64/1000 | Loss: 0.00003910
Iteration 65/1000 | Loss: 0.00003910
Iteration 66/1000 | Loss: 0.00003909
Iteration 67/1000 | Loss: 0.00003909
Iteration 68/1000 | Loss: 0.00003909
Iteration 69/1000 | Loss: 0.00003909
Iteration 70/1000 | Loss: 0.00003909
Iteration 71/1000 | Loss: 0.00003909
Iteration 72/1000 | Loss: 0.00003909
Iteration 73/1000 | Loss: 0.00003909
Iteration 74/1000 | Loss: 0.00003908
Iteration 75/1000 | Loss: 0.00003908
Iteration 76/1000 | Loss: 0.00003908
Iteration 77/1000 | Loss: 0.00003908
Iteration 78/1000 | Loss: 0.00003908
Iteration 79/1000 | Loss: 0.00003907
Iteration 80/1000 | Loss: 0.00003907
Iteration 81/1000 | Loss: 0.00003907
Iteration 82/1000 | Loss: 0.00003907
Iteration 83/1000 | Loss: 0.00003906
Iteration 84/1000 | Loss: 0.00003906
Iteration 85/1000 | Loss: 0.00003906
Iteration 86/1000 | Loss: 0.00003906
Iteration 87/1000 | Loss: 0.00003906
Iteration 88/1000 | Loss: 0.00003905
Iteration 89/1000 | Loss: 0.00003905
Iteration 90/1000 | Loss: 0.00003905
Iteration 91/1000 | Loss: 0.00003905
Iteration 92/1000 | Loss: 0.00003904
Iteration 93/1000 | Loss: 0.00003904
Iteration 94/1000 | Loss: 0.00003903
Iteration 95/1000 | Loss: 0.00003903
Iteration 96/1000 | Loss: 0.00003903
Iteration 97/1000 | Loss: 0.00003903
Iteration 98/1000 | Loss: 0.00003903
Iteration 99/1000 | Loss: 0.00003902
Iteration 100/1000 | Loss: 0.00003902
Iteration 101/1000 | Loss: 0.00003902
Iteration 102/1000 | Loss: 0.00003902
Iteration 103/1000 | Loss: 0.00003902
Iteration 104/1000 | Loss: 0.00003902
Iteration 105/1000 | Loss: 0.00003902
Iteration 106/1000 | Loss: 0.00003901
Iteration 107/1000 | Loss: 0.00003901
Iteration 108/1000 | Loss: 0.00003901
Iteration 109/1000 | Loss: 0.00003901
Iteration 110/1000 | Loss: 0.00003901
Iteration 111/1000 | Loss: 0.00003901
Iteration 112/1000 | Loss: 0.00003901
Iteration 113/1000 | Loss: 0.00003901
Iteration 114/1000 | Loss: 0.00003900
Iteration 115/1000 | Loss: 0.00003900
Iteration 116/1000 | Loss: 0.00003900
Iteration 117/1000 | Loss: 0.00003900
Iteration 118/1000 | Loss: 0.00003900
Iteration 119/1000 | Loss: 0.00003900
Iteration 120/1000 | Loss: 0.00003900
Iteration 121/1000 | Loss: 0.00003900
Iteration 122/1000 | Loss: 0.00003900
Iteration 123/1000 | Loss: 0.00003899
Iteration 124/1000 | Loss: 0.00003899
Iteration 125/1000 | Loss: 0.00003899
Iteration 126/1000 | Loss: 0.00003899
Iteration 127/1000 | Loss: 0.00003899
Iteration 128/1000 | Loss: 0.00003899
Iteration 129/1000 | Loss: 0.00003898
Iteration 130/1000 | Loss: 0.00003898
Iteration 131/1000 | Loss: 0.00003898
Iteration 132/1000 | Loss: 0.00003898
Iteration 133/1000 | Loss: 0.00003898
Iteration 134/1000 | Loss: 0.00003897
Iteration 135/1000 | Loss: 0.00003897
Iteration 136/1000 | Loss: 0.00003897
Iteration 137/1000 | Loss: 0.00003897
Iteration 138/1000 | Loss: 0.00003897
Iteration 139/1000 | Loss: 0.00003897
Iteration 140/1000 | Loss: 0.00003897
Iteration 141/1000 | Loss: 0.00003897
Iteration 142/1000 | Loss: 0.00003897
Iteration 143/1000 | Loss: 0.00003897
Iteration 144/1000 | Loss: 0.00003897
Iteration 145/1000 | Loss: 0.00003897
Iteration 146/1000 | Loss: 0.00003897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.896817361237481e-05, 3.896817361237481e-05, 3.896817361237481e-05, 3.896817361237481e-05, 3.896817361237481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.896817361237481e-05

Optimization complete. Final v2v error: 5.261369705200195 mm

Highest mean error: 6.038066864013672 mm for frame 110

Lowest mean error: 4.230869770050049 mm for frame 15

Saving results

Total time: 46.334641218185425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795138
Iteration 2/25 | Loss: 0.00197933
Iteration 3/25 | Loss: 0.00150151
Iteration 4/25 | Loss: 0.00144520
Iteration 5/25 | Loss: 0.00143604
Iteration 6/25 | Loss: 0.00143319
Iteration 7/25 | Loss: 0.00143207
Iteration 8/25 | Loss: 0.00143464
Iteration 9/25 | Loss: 0.00143506
Iteration 10/25 | Loss: 0.00143089
Iteration 11/25 | Loss: 0.00142670
Iteration 12/25 | Loss: 0.00142563
Iteration 13/25 | Loss: 0.00142524
Iteration 14/25 | Loss: 0.00142513
Iteration 15/25 | Loss: 0.00142511
Iteration 16/25 | Loss: 0.00142511
Iteration 17/25 | Loss: 0.00142511
Iteration 18/25 | Loss: 0.00142511
Iteration 19/25 | Loss: 0.00142511
Iteration 20/25 | Loss: 0.00142511
Iteration 21/25 | Loss: 0.00142511
Iteration 22/25 | Loss: 0.00142510
Iteration 23/25 | Loss: 0.00142508
Iteration 24/25 | Loss: 0.00142508
Iteration 25/25 | Loss: 0.00142508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56135547
Iteration 2/25 | Loss: 0.00107771
Iteration 3/25 | Loss: 0.00107770
Iteration 4/25 | Loss: 0.00107770
Iteration 5/25 | Loss: 0.00107770
Iteration 6/25 | Loss: 0.00107770
Iteration 7/25 | Loss: 0.00107770
Iteration 8/25 | Loss: 0.00107770
Iteration 9/25 | Loss: 0.00107770
Iteration 10/25 | Loss: 0.00107770
Iteration 11/25 | Loss: 0.00107770
Iteration 12/25 | Loss: 0.00107770
Iteration 13/25 | Loss: 0.00107770
Iteration 14/25 | Loss: 0.00107770
Iteration 15/25 | Loss: 0.00107770
Iteration 16/25 | Loss: 0.00107770
Iteration 17/25 | Loss: 0.00107770
Iteration 18/25 | Loss: 0.00107770
Iteration 19/25 | Loss: 0.00107770
Iteration 20/25 | Loss: 0.00107770
Iteration 21/25 | Loss: 0.00107770
Iteration 22/25 | Loss: 0.00107770
Iteration 23/25 | Loss: 0.00107770
Iteration 24/25 | Loss: 0.00107770
Iteration 25/25 | Loss: 0.00107770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107770
Iteration 2/1000 | Loss: 0.00010192
Iteration 3/1000 | Loss: 0.00005717
Iteration 4/1000 | Loss: 0.00004562
Iteration 5/1000 | Loss: 0.00004077
Iteration 6/1000 | Loss: 0.00003878
Iteration 7/1000 | Loss: 0.00003730
Iteration 8/1000 | Loss: 0.00003626
Iteration 9/1000 | Loss: 0.00003555
Iteration 10/1000 | Loss: 0.00003503
Iteration 11/1000 | Loss: 0.00003467
Iteration 12/1000 | Loss: 0.00003428
Iteration 13/1000 | Loss: 0.00003397
Iteration 14/1000 | Loss: 0.00003367
Iteration 15/1000 | Loss: 0.00003349
Iteration 16/1000 | Loss: 0.00003332
Iteration 17/1000 | Loss: 0.00003327
Iteration 18/1000 | Loss: 0.00003322
Iteration 19/1000 | Loss: 0.00003319
Iteration 20/1000 | Loss: 0.00003315
Iteration 21/1000 | Loss: 0.00003304
Iteration 22/1000 | Loss: 0.00003300
Iteration 23/1000 | Loss: 0.00003293
Iteration 24/1000 | Loss: 0.00003293
Iteration 25/1000 | Loss: 0.00003291
Iteration 26/1000 | Loss: 0.00003291
Iteration 27/1000 | Loss: 0.00003290
Iteration 28/1000 | Loss: 0.00003289
Iteration 29/1000 | Loss: 0.00003289
Iteration 30/1000 | Loss: 0.00003289
Iteration 31/1000 | Loss: 0.00003289
Iteration 32/1000 | Loss: 0.00003288
Iteration 33/1000 | Loss: 0.00003288
Iteration 34/1000 | Loss: 0.00003287
Iteration 35/1000 | Loss: 0.00003287
Iteration 36/1000 | Loss: 0.00003287
Iteration 37/1000 | Loss: 0.00003287
Iteration 38/1000 | Loss: 0.00003286
Iteration 39/1000 | Loss: 0.00003286
Iteration 40/1000 | Loss: 0.00003286
Iteration 41/1000 | Loss: 0.00003286
Iteration 42/1000 | Loss: 0.00003286
Iteration 43/1000 | Loss: 0.00003286
Iteration 44/1000 | Loss: 0.00003286
Iteration 45/1000 | Loss: 0.00003286
Iteration 46/1000 | Loss: 0.00003286
Iteration 47/1000 | Loss: 0.00003286
Iteration 48/1000 | Loss: 0.00003286
Iteration 49/1000 | Loss: 0.00003285
Iteration 50/1000 | Loss: 0.00003285
Iteration 51/1000 | Loss: 0.00003285
Iteration 52/1000 | Loss: 0.00003285
Iteration 53/1000 | Loss: 0.00003284
Iteration 54/1000 | Loss: 0.00003284
Iteration 55/1000 | Loss: 0.00003282
Iteration 56/1000 | Loss: 0.00003282
Iteration 57/1000 | Loss: 0.00003282
Iteration 58/1000 | Loss: 0.00003282
Iteration 59/1000 | Loss: 0.00003282
Iteration 60/1000 | Loss: 0.00003282
Iteration 61/1000 | Loss: 0.00003282
Iteration 62/1000 | Loss: 0.00003282
Iteration 63/1000 | Loss: 0.00003282
Iteration 64/1000 | Loss: 0.00003281
Iteration 65/1000 | Loss: 0.00003281
Iteration 66/1000 | Loss: 0.00003281
Iteration 67/1000 | Loss: 0.00003281
Iteration 68/1000 | Loss: 0.00003280
Iteration 69/1000 | Loss: 0.00003280
Iteration 70/1000 | Loss: 0.00003280
Iteration 71/1000 | Loss: 0.00003280
Iteration 72/1000 | Loss: 0.00003279
Iteration 73/1000 | Loss: 0.00003279
Iteration 74/1000 | Loss: 0.00003279
Iteration 75/1000 | Loss: 0.00003278
Iteration 76/1000 | Loss: 0.00003277
Iteration 77/1000 | Loss: 0.00003277
Iteration 78/1000 | Loss: 0.00003277
Iteration 79/1000 | Loss: 0.00003277
Iteration 80/1000 | Loss: 0.00003277
Iteration 81/1000 | Loss: 0.00003277
Iteration 82/1000 | Loss: 0.00003277
Iteration 83/1000 | Loss: 0.00003277
Iteration 84/1000 | Loss: 0.00003276
Iteration 85/1000 | Loss: 0.00003276
Iteration 86/1000 | Loss: 0.00003276
Iteration 87/1000 | Loss: 0.00003276
Iteration 88/1000 | Loss: 0.00003276
Iteration 89/1000 | Loss: 0.00003276
Iteration 90/1000 | Loss: 0.00003275
Iteration 91/1000 | Loss: 0.00003275
Iteration 92/1000 | Loss: 0.00003275
Iteration 93/1000 | Loss: 0.00003275
Iteration 94/1000 | Loss: 0.00003274
Iteration 95/1000 | Loss: 0.00003274
Iteration 96/1000 | Loss: 0.00003274
Iteration 97/1000 | Loss: 0.00003274
Iteration 98/1000 | Loss: 0.00003274
Iteration 99/1000 | Loss: 0.00003274
Iteration 100/1000 | Loss: 0.00003274
Iteration 101/1000 | Loss: 0.00003274
Iteration 102/1000 | Loss: 0.00003273
Iteration 103/1000 | Loss: 0.00003273
Iteration 104/1000 | Loss: 0.00003273
Iteration 105/1000 | Loss: 0.00003273
Iteration 106/1000 | Loss: 0.00003272
Iteration 107/1000 | Loss: 0.00003272
Iteration 108/1000 | Loss: 0.00003272
Iteration 109/1000 | Loss: 0.00003272
Iteration 110/1000 | Loss: 0.00003272
Iteration 111/1000 | Loss: 0.00003272
Iteration 112/1000 | Loss: 0.00003272
Iteration 113/1000 | Loss: 0.00003272
Iteration 114/1000 | Loss: 0.00003272
Iteration 115/1000 | Loss: 0.00003272
Iteration 116/1000 | Loss: 0.00003272
Iteration 117/1000 | Loss: 0.00003271
Iteration 118/1000 | Loss: 0.00003271
Iteration 119/1000 | Loss: 0.00003271
Iteration 120/1000 | Loss: 0.00003271
Iteration 121/1000 | Loss: 0.00003271
Iteration 122/1000 | Loss: 0.00003271
Iteration 123/1000 | Loss: 0.00003270
Iteration 124/1000 | Loss: 0.00003270
Iteration 125/1000 | Loss: 0.00003270
Iteration 126/1000 | Loss: 0.00003270
Iteration 127/1000 | Loss: 0.00003270
Iteration 128/1000 | Loss: 0.00003270
Iteration 129/1000 | Loss: 0.00003270
Iteration 130/1000 | Loss: 0.00003270
Iteration 131/1000 | Loss: 0.00003270
Iteration 132/1000 | Loss: 0.00003269
Iteration 133/1000 | Loss: 0.00003269
Iteration 134/1000 | Loss: 0.00003269
Iteration 135/1000 | Loss: 0.00003269
Iteration 136/1000 | Loss: 0.00003269
Iteration 137/1000 | Loss: 0.00003269
Iteration 138/1000 | Loss: 0.00003269
Iteration 139/1000 | Loss: 0.00003269
Iteration 140/1000 | Loss: 0.00003269
Iteration 141/1000 | Loss: 0.00003269
Iteration 142/1000 | Loss: 0.00003269
Iteration 143/1000 | Loss: 0.00003268
Iteration 144/1000 | Loss: 0.00003268
Iteration 145/1000 | Loss: 0.00003268
Iteration 146/1000 | Loss: 0.00003268
Iteration 147/1000 | Loss: 0.00003268
Iteration 148/1000 | Loss: 0.00003267
Iteration 149/1000 | Loss: 0.00003267
Iteration 150/1000 | Loss: 0.00003267
Iteration 151/1000 | Loss: 0.00003266
Iteration 152/1000 | Loss: 0.00003266
Iteration 153/1000 | Loss: 0.00003266
Iteration 154/1000 | Loss: 0.00003266
Iteration 155/1000 | Loss: 0.00003266
Iteration 156/1000 | Loss: 0.00003266
Iteration 157/1000 | Loss: 0.00003266
Iteration 158/1000 | Loss: 0.00003266
Iteration 159/1000 | Loss: 0.00003266
Iteration 160/1000 | Loss: 0.00003265
Iteration 161/1000 | Loss: 0.00003265
Iteration 162/1000 | Loss: 0.00003265
Iteration 163/1000 | Loss: 0.00003265
Iteration 164/1000 | Loss: 0.00003265
Iteration 165/1000 | Loss: 0.00003265
Iteration 166/1000 | Loss: 0.00003265
Iteration 167/1000 | Loss: 0.00003265
Iteration 168/1000 | Loss: 0.00003265
Iteration 169/1000 | Loss: 0.00003264
Iteration 170/1000 | Loss: 0.00003264
Iteration 171/1000 | Loss: 0.00003264
Iteration 172/1000 | Loss: 0.00003264
Iteration 173/1000 | Loss: 0.00003264
Iteration 174/1000 | Loss: 0.00003264
Iteration 175/1000 | Loss: 0.00003264
Iteration 176/1000 | Loss: 0.00003264
Iteration 177/1000 | Loss: 0.00003264
Iteration 178/1000 | Loss: 0.00003264
Iteration 179/1000 | Loss: 0.00003264
Iteration 180/1000 | Loss: 0.00003264
Iteration 181/1000 | Loss: 0.00003264
Iteration 182/1000 | Loss: 0.00003264
Iteration 183/1000 | Loss: 0.00003264
Iteration 184/1000 | Loss: 0.00003264
Iteration 185/1000 | Loss: 0.00003263
Iteration 186/1000 | Loss: 0.00003263
Iteration 187/1000 | Loss: 0.00003263
Iteration 188/1000 | Loss: 0.00003263
Iteration 189/1000 | Loss: 0.00003263
Iteration 190/1000 | Loss: 0.00003263
Iteration 191/1000 | Loss: 0.00003262
Iteration 192/1000 | Loss: 0.00003262
Iteration 193/1000 | Loss: 0.00003262
Iteration 194/1000 | Loss: 0.00003262
Iteration 195/1000 | Loss: 0.00003262
Iteration 196/1000 | Loss: 0.00003262
Iteration 197/1000 | Loss: 0.00003262
Iteration 198/1000 | Loss: 0.00003262
Iteration 199/1000 | Loss: 0.00003262
Iteration 200/1000 | Loss: 0.00003262
Iteration 201/1000 | Loss: 0.00003262
Iteration 202/1000 | Loss: 0.00003262
Iteration 203/1000 | Loss: 0.00003262
Iteration 204/1000 | Loss: 0.00003262
Iteration 205/1000 | Loss: 0.00003262
Iteration 206/1000 | Loss: 0.00003261
Iteration 207/1000 | Loss: 0.00003261
Iteration 208/1000 | Loss: 0.00003261
Iteration 209/1000 | Loss: 0.00003261
Iteration 210/1000 | Loss: 0.00003261
Iteration 211/1000 | Loss: 0.00003261
Iteration 212/1000 | Loss: 0.00003261
Iteration 213/1000 | Loss: 0.00003260
Iteration 214/1000 | Loss: 0.00003260
Iteration 215/1000 | Loss: 0.00003260
Iteration 216/1000 | Loss: 0.00003260
Iteration 217/1000 | Loss: 0.00003260
Iteration 218/1000 | Loss: 0.00003260
Iteration 219/1000 | Loss: 0.00003260
Iteration 220/1000 | Loss: 0.00003260
Iteration 221/1000 | Loss: 0.00003260
Iteration 222/1000 | Loss: 0.00003260
Iteration 223/1000 | Loss: 0.00003259
Iteration 224/1000 | Loss: 0.00003259
Iteration 225/1000 | Loss: 0.00003259
Iteration 226/1000 | Loss: 0.00003259
Iteration 227/1000 | Loss: 0.00003259
Iteration 228/1000 | Loss: 0.00003259
Iteration 229/1000 | Loss: 0.00003259
Iteration 230/1000 | Loss: 0.00003259
Iteration 231/1000 | Loss: 0.00003259
Iteration 232/1000 | Loss: 0.00003259
Iteration 233/1000 | Loss: 0.00003259
Iteration 234/1000 | Loss: 0.00003258
Iteration 235/1000 | Loss: 0.00003258
Iteration 236/1000 | Loss: 0.00003258
Iteration 237/1000 | Loss: 0.00003258
Iteration 238/1000 | Loss: 0.00003258
Iteration 239/1000 | Loss: 0.00003258
Iteration 240/1000 | Loss: 0.00003258
Iteration 241/1000 | Loss: 0.00003258
Iteration 242/1000 | Loss: 0.00003258
Iteration 243/1000 | Loss: 0.00003258
Iteration 244/1000 | Loss: 0.00003258
Iteration 245/1000 | Loss: 0.00003258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [3.258352444390766e-05, 3.258352444390766e-05, 3.258352444390766e-05, 3.258352444390766e-05, 3.258352444390766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.258352444390766e-05

Optimization complete. Final v2v error: 4.6672234535217285 mm

Highest mean error: 6.1504998207092285 mm for frame 34

Lowest mean error: 3.8197150230407715 mm for frame 2

Saving results

Total time: 66.26416730880737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926379
Iteration 2/25 | Loss: 0.00138421
Iteration 3/25 | Loss: 0.00132525
Iteration 4/25 | Loss: 0.00131882
Iteration 5/25 | Loss: 0.00131654
Iteration 6/25 | Loss: 0.00131637
Iteration 7/25 | Loss: 0.00131637
Iteration 8/25 | Loss: 0.00131637
Iteration 9/25 | Loss: 0.00131637
Iteration 10/25 | Loss: 0.00131637
Iteration 11/25 | Loss: 0.00131637
Iteration 12/25 | Loss: 0.00131637
Iteration 13/25 | Loss: 0.00131637
Iteration 14/25 | Loss: 0.00131637
Iteration 15/25 | Loss: 0.00131637
Iteration 16/25 | Loss: 0.00131637
Iteration 17/25 | Loss: 0.00131637
Iteration 18/25 | Loss: 0.00131637
Iteration 19/25 | Loss: 0.00131637
Iteration 20/25 | Loss: 0.00131637
Iteration 21/25 | Loss: 0.00131637
Iteration 22/25 | Loss: 0.00131637
Iteration 23/25 | Loss: 0.00131637
Iteration 24/25 | Loss: 0.00131637
Iteration 25/25 | Loss: 0.00131637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27695405
Iteration 2/25 | Loss: 0.00142797
Iteration 3/25 | Loss: 0.00142797
Iteration 4/25 | Loss: 0.00142797
Iteration 5/25 | Loss: 0.00142797
Iteration 6/25 | Loss: 0.00142797
Iteration 7/25 | Loss: 0.00142797
Iteration 8/25 | Loss: 0.00142797
Iteration 9/25 | Loss: 0.00142797
Iteration 10/25 | Loss: 0.00142797
Iteration 11/25 | Loss: 0.00142797
Iteration 12/25 | Loss: 0.00142797
Iteration 13/25 | Loss: 0.00142797
Iteration 14/25 | Loss: 0.00142797
Iteration 15/25 | Loss: 0.00142797
Iteration 16/25 | Loss: 0.00142797
Iteration 17/25 | Loss: 0.00142797
Iteration 18/25 | Loss: 0.00142797
Iteration 19/25 | Loss: 0.00142797
Iteration 20/25 | Loss: 0.00142797
Iteration 21/25 | Loss: 0.00142797
Iteration 22/25 | Loss: 0.00142797
Iteration 23/25 | Loss: 0.00142797
Iteration 24/25 | Loss: 0.00142797
Iteration 25/25 | Loss: 0.00142797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014279691968113184, 0.0014279691968113184, 0.0014279691968113184, 0.0014279691968113184, 0.0014279691968113184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014279691968113184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142797
Iteration 2/1000 | Loss: 0.00003377
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001745
Iteration 7/1000 | Loss: 0.00001706
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001658
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001656
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001655
Iteration 24/1000 | Loss: 0.00001655
Iteration 25/1000 | Loss: 0.00001655
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001647
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001632
Iteration 39/1000 | Loss: 0.00001631
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001624
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.6221716578002088e-05, 1.6221716578002088e-05, 1.6221716578002088e-05, 1.6221716578002088e-05, 1.6221716578002088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6221716578002088e-05

Optimization complete. Final v2v error: 3.4810097217559814 mm

Highest mean error: 3.8076794147491455 mm for frame 118

Lowest mean error: 3.3238277435302734 mm for frame 35

Saving results

Total time: 29.164475917816162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898833
Iteration 2/25 | Loss: 0.00153914
Iteration 3/25 | Loss: 0.00139514
Iteration 4/25 | Loss: 0.00137743
Iteration 5/25 | Loss: 0.00137351
Iteration 6/25 | Loss: 0.00137313
Iteration 7/25 | Loss: 0.00137313
Iteration 8/25 | Loss: 0.00137313
Iteration 9/25 | Loss: 0.00137313
Iteration 10/25 | Loss: 0.00137313
Iteration 11/25 | Loss: 0.00137313
Iteration 12/25 | Loss: 0.00137313
Iteration 13/25 | Loss: 0.00137313
Iteration 14/25 | Loss: 0.00137313
Iteration 15/25 | Loss: 0.00137313
Iteration 16/25 | Loss: 0.00137313
Iteration 17/25 | Loss: 0.00137313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013731273356825113, 0.0013731273356825113, 0.0013731273356825113, 0.0013731273356825113, 0.0013731273356825113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013731273356825113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23481619
Iteration 2/25 | Loss: 0.00121832
Iteration 3/25 | Loss: 0.00121829
Iteration 4/25 | Loss: 0.00121829
Iteration 5/25 | Loss: 0.00121829
Iteration 6/25 | Loss: 0.00121829
Iteration 7/25 | Loss: 0.00121829
Iteration 8/25 | Loss: 0.00121829
Iteration 9/25 | Loss: 0.00121829
Iteration 10/25 | Loss: 0.00121829
Iteration 11/25 | Loss: 0.00121829
Iteration 12/25 | Loss: 0.00121829
Iteration 13/25 | Loss: 0.00121829
Iteration 14/25 | Loss: 0.00121829
Iteration 15/25 | Loss: 0.00121829
Iteration 16/25 | Loss: 0.00121829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012182858772575855, 0.0012182858772575855, 0.0012182858772575855, 0.0012182858772575855, 0.0012182858772575855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012182858772575855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121829
Iteration 2/1000 | Loss: 0.00005193
Iteration 3/1000 | Loss: 0.00003397
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00002390
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002129
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00002044
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00002001
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001954
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001950
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001945
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001939
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001938
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001938
Iteration 34/1000 | Loss: 0.00001938
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001938
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001937
Iteration 43/1000 | Loss: 0.00001937
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001935
Iteration 54/1000 | Loss: 0.00001935
Iteration 55/1000 | Loss: 0.00001934
Iteration 56/1000 | Loss: 0.00001934
Iteration 57/1000 | Loss: 0.00001934
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001933
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001932
Iteration 76/1000 | Loss: 0.00001932
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001931
Iteration 81/1000 | Loss: 0.00001931
Iteration 82/1000 | Loss: 0.00001931
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00001931
Iteration 86/1000 | Loss: 0.00001931
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001930
Iteration 89/1000 | Loss: 0.00001930
Iteration 90/1000 | Loss: 0.00001930
Iteration 91/1000 | Loss: 0.00001930
Iteration 92/1000 | Loss: 0.00001930
Iteration 93/1000 | Loss: 0.00001930
Iteration 94/1000 | Loss: 0.00001930
Iteration 95/1000 | Loss: 0.00001930
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001930
Iteration 99/1000 | Loss: 0.00001930
Iteration 100/1000 | Loss: 0.00001930
Iteration 101/1000 | Loss: 0.00001930
Iteration 102/1000 | Loss: 0.00001929
Iteration 103/1000 | Loss: 0.00001929
Iteration 104/1000 | Loss: 0.00001929
Iteration 105/1000 | Loss: 0.00001929
Iteration 106/1000 | Loss: 0.00001929
Iteration 107/1000 | Loss: 0.00001929
Iteration 108/1000 | Loss: 0.00001929
Iteration 109/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.929442623804789e-05, 1.929442623804789e-05, 1.929442623804789e-05, 1.929442623804789e-05, 1.929442623804789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.929442623804789e-05

Optimization complete. Final v2v error: 3.7767159938812256 mm

Highest mean error: 4.1992998123168945 mm for frame 40

Lowest mean error: 3.443814516067505 mm for frame 7

Saving results

Total time: 34.617263078689575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553560
Iteration 2/25 | Loss: 0.00160617
Iteration 3/25 | Loss: 0.00137732
Iteration 4/25 | Loss: 0.00136443
Iteration 5/25 | Loss: 0.00136250
Iteration 6/25 | Loss: 0.00136250
Iteration 7/25 | Loss: 0.00136250
Iteration 8/25 | Loss: 0.00136250
Iteration 9/25 | Loss: 0.00136250
Iteration 10/25 | Loss: 0.00136250
Iteration 11/25 | Loss: 0.00136250
Iteration 12/25 | Loss: 0.00136250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013625012943521142, 0.0013625012943521142, 0.0013625012943521142, 0.0013625012943521142, 0.0013625012943521142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013625012943521142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74464816
Iteration 2/25 | Loss: 0.00111227
Iteration 3/25 | Loss: 0.00111227
Iteration 4/25 | Loss: 0.00111227
Iteration 5/25 | Loss: 0.00111227
Iteration 6/25 | Loss: 0.00111227
Iteration 7/25 | Loss: 0.00111227
Iteration 8/25 | Loss: 0.00111227
Iteration 9/25 | Loss: 0.00111227
Iteration 10/25 | Loss: 0.00111227
Iteration 11/25 | Loss: 0.00111227
Iteration 12/25 | Loss: 0.00111227
Iteration 13/25 | Loss: 0.00111227
Iteration 14/25 | Loss: 0.00111227
Iteration 15/25 | Loss: 0.00111227
Iteration 16/25 | Loss: 0.00111227
Iteration 17/25 | Loss: 0.00111227
Iteration 18/25 | Loss: 0.00111227
Iteration 19/25 | Loss: 0.00111227
Iteration 20/25 | Loss: 0.00111227
Iteration 21/25 | Loss: 0.00111227
Iteration 22/25 | Loss: 0.00111227
Iteration 23/25 | Loss: 0.00111227
Iteration 24/25 | Loss: 0.00111227
Iteration 25/25 | Loss: 0.00111227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111227
Iteration 2/1000 | Loss: 0.00006219
Iteration 3/1000 | Loss: 0.00004332
Iteration 4/1000 | Loss: 0.00003920
Iteration 5/1000 | Loss: 0.00003756
Iteration 6/1000 | Loss: 0.00003661
Iteration 7/1000 | Loss: 0.00003579
Iteration 8/1000 | Loss: 0.00003534
Iteration 9/1000 | Loss: 0.00003465
Iteration 10/1000 | Loss: 0.00003427
Iteration 11/1000 | Loss: 0.00003391
Iteration 12/1000 | Loss: 0.00003342
Iteration 13/1000 | Loss: 0.00003293
Iteration 14/1000 | Loss: 0.00003257
Iteration 15/1000 | Loss: 0.00003224
Iteration 16/1000 | Loss: 0.00003194
Iteration 17/1000 | Loss: 0.00003161
Iteration 18/1000 | Loss: 0.00003136
Iteration 19/1000 | Loss: 0.00003114
Iteration 20/1000 | Loss: 0.00003105
Iteration 21/1000 | Loss: 0.00003101
Iteration 22/1000 | Loss: 0.00003100
Iteration 23/1000 | Loss: 0.00003093
Iteration 24/1000 | Loss: 0.00003089
Iteration 25/1000 | Loss: 0.00003089
Iteration 26/1000 | Loss: 0.00003085
Iteration 27/1000 | Loss: 0.00003085
Iteration 28/1000 | Loss: 0.00003083
Iteration 29/1000 | Loss: 0.00003082
Iteration 30/1000 | Loss: 0.00003081
Iteration 31/1000 | Loss: 0.00003079
Iteration 32/1000 | Loss: 0.00003078
Iteration 33/1000 | Loss: 0.00003077
Iteration 34/1000 | Loss: 0.00003076
Iteration 35/1000 | Loss: 0.00003072
Iteration 36/1000 | Loss: 0.00003069
Iteration 37/1000 | Loss: 0.00003068
Iteration 38/1000 | Loss: 0.00003068
Iteration 39/1000 | Loss: 0.00003068
Iteration 40/1000 | Loss: 0.00003068
Iteration 41/1000 | Loss: 0.00003068
Iteration 42/1000 | Loss: 0.00003068
Iteration 43/1000 | Loss: 0.00003068
Iteration 44/1000 | Loss: 0.00003068
Iteration 45/1000 | Loss: 0.00003067
Iteration 46/1000 | Loss: 0.00003067
Iteration 47/1000 | Loss: 0.00003067
Iteration 48/1000 | Loss: 0.00003067
Iteration 49/1000 | Loss: 0.00003067
Iteration 50/1000 | Loss: 0.00003067
Iteration 51/1000 | Loss: 0.00003067
Iteration 52/1000 | Loss: 0.00003067
Iteration 53/1000 | Loss: 0.00003067
Iteration 54/1000 | Loss: 0.00003066
Iteration 55/1000 | Loss: 0.00003065
Iteration 56/1000 | Loss: 0.00003064
Iteration 57/1000 | Loss: 0.00003064
Iteration 58/1000 | Loss: 0.00003064
Iteration 59/1000 | Loss: 0.00003063
Iteration 60/1000 | Loss: 0.00003063
Iteration 61/1000 | Loss: 0.00003063
Iteration 62/1000 | Loss: 0.00003063
Iteration 63/1000 | Loss: 0.00003062
Iteration 64/1000 | Loss: 0.00003062
Iteration 65/1000 | Loss: 0.00003062
Iteration 66/1000 | Loss: 0.00003062
Iteration 67/1000 | Loss: 0.00003061
Iteration 68/1000 | Loss: 0.00003061
Iteration 69/1000 | Loss: 0.00003061
Iteration 70/1000 | Loss: 0.00003061
Iteration 71/1000 | Loss: 0.00003061
Iteration 72/1000 | Loss: 0.00003061
Iteration 73/1000 | Loss: 0.00003060
Iteration 74/1000 | Loss: 0.00003060
Iteration 75/1000 | Loss: 0.00003060
Iteration 76/1000 | Loss: 0.00003060
Iteration 77/1000 | Loss: 0.00003060
Iteration 78/1000 | Loss: 0.00003060
Iteration 79/1000 | Loss: 0.00003060
Iteration 80/1000 | Loss: 0.00003060
Iteration 81/1000 | Loss: 0.00003060
Iteration 82/1000 | Loss: 0.00003060
Iteration 83/1000 | Loss: 0.00003060
Iteration 84/1000 | Loss: 0.00003060
Iteration 85/1000 | Loss: 0.00003060
Iteration 86/1000 | Loss: 0.00003059
Iteration 87/1000 | Loss: 0.00003059
Iteration 88/1000 | Loss: 0.00003059
Iteration 89/1000 | Loss: 0.00003059
Iteration 90/1000 | Loss: 0.00003059
Iteration 91/1000 | Loss: 0.00003059
Iteration 92/1000 | Loss: 0.00003059
Iteration 93/1000 | Loss: 0.00003059
Iteration 94/1000 | Loss: 0.00003058
Iteration 95/1000 | Loss: 0.00003058
Iteration 96/1000 | Loss: 0.00003058
Iteration 97/1000 | Loss: 0.00003058
Iteration 98/1000 | Loss: 0.00003058
Iteration 99/1000 | Loss: 0.00003058
Iteration 100/1000 | Loss: 0.00003058
Iteration 101/1000 | Loss: 0.00003058
Iteration 102/1000 | Loss: 0.00003058
Iteration 103/1000 | Loss: 0.00003058
Iteration 104/1000 | Loss: 0.00003058
Iteration 105/1000 | Loss: 0.00003058
Iteration 106/1000 | Loss: 0.00003058
Iteration 107/1000 | Loss: 0.00003058
Iteration 108/1000 | Loss: 0.00003058
Iteration 109/1000 | Loss: 0.00003057
Iteration 110/1000 | Loss: 0.00003057
Iteration 111/1000 | Loss: 0.00003057
Iteration 112/1000 | Loss: 0.00003057
Iteration 113/1000 | Loss: 0.00003057
Iteration 114/1000 | Loss: 0.00003057
Iteration 115/1000 | Loss: 0.00003057
Iteration 116/1000 | Loss: 0.00003057
Iteration 117/1000 | Loss: 0.00003057
Iteration 118/1000 | Loss: 0.00003057
Iteration 119/1000 | Loss: 0.00003057
Iteration 120/1000 | Loss: 0.00003057
Iteration 121/1000 | Loss: 0.00003057
Iteration 122/1000 | Loss: 0.00003057
Iteration 123/1000 | Loss: 0.00003057
Iteration 124/1000 | Loss: 0.00003057
Iteration 125/1000 | Loss: 0.00003057
Iteration 126/1000 | Loss: 0.00003057
Iteration 127/1000 | Loss: 0.00003057
Iteration 128/1000 | Loss: 0.00003057
Iteration 129/1000 | Loss: 0.00003057
Iteration 130/1000 | Loss: 0.00003057
Iteration 131/1000 | Loss: 0.00003057
Iteration 132/1000 | Loss: 0.00003057
Iteration 133/1000 | Loss: 0.00003057
Iteration 134/1000 | Loss: 0.00003057
Iteration 135/1000 | Loss: 0.00003057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [3.056859350181185e-05, 3.056859350181185e-05, 3.056859350181185e-05, 3.056859350181185e-05, 3.056859350181185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.056859350181185e-05

Optimization complete. Final v2v error: 4.496404647827148 mm

Highest mean error: 5.0452446937561035 mm for frame 18

Lowest mean error: 4.065341949462891 mm for frame 0

Saving results

Total time: 45.53074502944946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422334
Iteration 2/25 | Loss: 0.00144377
Iteration 3/25 | Loss: 0.00134238
Iteration 4/25 | Loss: 0.00133202
Iteration 5/25 | Loss: 0.00132837
Iteration 6/25 | Loss: 0.00132757
Iteration 7/25 | Loss: 0.00132757
Iteration 8/25 | Loss: 0.00132757
Iteration 9/25 | Loss: 0.00132757
Iteration 10/25 | Loss: 0.00132757
Iteration 11/25 | Loss: 0.00132757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00132757390383631, 0.00132757390383631, 0.00132757390383631, 0.00132757390383631, 0.00132757390383631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00132757390383631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24967551
Iteration 2/25 | Loss: 0.00143763
Iteration 3/25 | Loss: 0.00143763
Iteration 4/25 | Loss: 0.00143763
Iteration 5/25 | Loss: 0.00143763
Iteration 6/25 | Loss: 0.00143763
Iteration 7/25 | Loss: 0.00143763
Iteration 8/25 | Loss: 0.00143763
Iteration 9/25 | Loss: 0.00143763
Iteration 10/25 | Loss: 0.00143763
Iteration 11/25 | Loss: 0.00143763
Iteration 12/25 | Loss: 0.00143763
Iteration 13/25 | Loss: 0.00143763
Iteration 14/25 | Loss: 0.00143763
Iteration 15/25 | Loss: 0.00143763
Iteration 16/25 | Loss: 0.00143763
Iteration 17/25 | Loss: 0.00143763
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014376266626641154, 0.0014376266626641154, 0.0014376266626641154, 0.0014376266626641154, 0.0014376266626641154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014376266626641154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143763
Iteration 2/1000 | Loss: 0.00003843
Iteration 3/1000 | Loss: 0.00002510
Iteration 4/1000 | Loss: 0.00002213
Iteration 5/1000 | Loss: 0.00002099
Iteration 6/1000 | Loss: 0.00002014
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001913
Iteration 9/1000 | Loss: 0.00001907
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001860
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001859
Iteration 15/1000 | Loss: 0.00001854
Iteration 16/1000 | Loss: 0.00001854
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001852
Iteration 19/1000 | Loss: 0.00001852
Iteration 20/1000 | Loss: 0.00001852
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001849
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001846
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001837
Iteration 46/1000 | Loss: 0.00001837
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001833
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001831
Iteration 89/1000 | Loss: 0.00001831
Iteration 90/1000 | Loss: 0.00001831
Iteration 91/1000 | Loss: 0.00001831
Iteration 92/1000 | Loss: 0.00001831
Iteration 93/1000 | Loss: 0.00001831
Iteration 94/1000 | Loss: 0.00001831
Iteration 95/1000 | Loss: 0.00001831
Iteration 96/1000 | Loss: 0.00001831
Iteration 97/1000 | Loss: 0.00001831
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001831
Iteration 100/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8306718629901297e-05, 1.8306718629901297e-05, 1.8306718629901297e-05, 1.8306718629901297e-05, 1.8306718629901297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8306718629901297e-05

Optimization complete. Final v2v error: 3.5802853107452393 mm

Highest mean error: 4.028779983520508 mm for frame 129

Lowest mean error: 3.145421028137207 mm for frame 14

Saving results

Total time: 31.941837787628174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103677
Iteration 2/25 | Loss: 0.00266475
Iteration 3/25 | Loss: 0.00189926
Iteration 4/25 | Loss: 0.00172785
Iteration 5/25 | Loss: 0.00183506
Iteration 6/25 | Loss: 0.00169894
Iteration 7/25 | Loss: 0.00150126
Iteration 8/25 | Loss: 0.00141720
Iteration 9/25 | Loss: 0.00139652
Iteration 10/25 | Loss: 0.00138787
Iteration 11/25 | Loss: 0.00138751
Iteration 12/25 | Loss: 0.00138396
Iteration 13/25 | Loss: 0.00137767
Iteration 14/25 | Loss: 0.00138424
Iteration 15/25 | Loss: 0.00137769
Iteration 16/25 | Loss: 0.00137725
Iteration 17/25 | Loss: 0.00137742
Iteration 18/25 | Loss: 0.00137763
Iteration 19/25 | Loss: 0.00137417
Iteration 20/25 | Loss: 0.00136717
Iteration 21/25 | Loss: 0.00136563
Iteration 22/25 | Loss: 0.00136506
Iteration 23/25 | Loss: 0.00136479
Iteration 24/25 | Loss: 0.00136751
Iteration 25/25 | Loss: 0.00137032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28986073
Iteration 2/25 | Loss: 0.00196832
Iteration 3/25 | Loss: 0.00195506
Iteration 4/25 | Loss: 0.00195506
Iteration 5/25 | Loss: 0.00195506
Iteration 6/25 | Loss: 0.00195506
Iteration 7/25 | Loss: 0.00195506
Iteration 8/25 | Loss: 0.00195506
Iteration 9/25 | Loss: 0.00195506
Iteration 10/25 | Loss: 0.00195506
Iteration 11/25 | Loss: 0.00195506
Iteration 12/25 | Loss: 0.00195506
Iteration 13/25 | Loss: 0.00195506
Iteration 14/25 | Loss: 0.00195506
Iteration 15/25 | Loss: 0.00195506
Iteration 16/25 | Loss: 0.00195506
Iteration 17/25 | Loss: 0.00195506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001955058891326189, 0.001955058891326189, 0.001955058891326189, 0.001955058891326189, 0.001955058891326189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001955058891326189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195506
Iteration 2/1000 | Loss: 0.00020407
Iteration 3/1000 | Loss: 0.00013746
Iteration 4/1000 | Loss: 0.00037506
Iteration 5/1000 | Loss: 0.00021043
Iteration 6/1000 | Loss: 0.00017642
Iteration 7/1000 | Loss: 0.00026165
Iteration 8/1000 | Loss: 0.00049197
Iteration 9/1000 | Loss: 0.00011961
Iteration 10/1000 | Loss: 0.00025699
Iteration 11/1000 | Loss: 0.00019459
Iteration 12/1000 | Loss: 0.00022481
Iteration 13/1000 | Loss: 0.00029886
Iteration 14/1000 | Loss: 0.00016260
Iteration 15/1000 | Loss: 0.00008524
Iteration 16/1000 | Loss: 0.00007048
Iteration 17/1000 | Loss: 0.00006990
Iteration 18/1000 | Loss: 0.00005958
Iteration 19/1000 | Loss: 0.00005629
Iteration 20/1000 | Loss: 0.00005434
Iteration 21/1000 | Loss: 0.00030091
Iteration 22/1000 | Loss: 0.00015555
Iteration 23/1000 | Loss: 0.00029479
Iteration 24/1000 | Loss: 0.00005646
Iteration 25/1000 | Loss: 0.00057590
Iteration 26/1000 | Loss: 0.00066972
Iteration 27/1000 | Loss: 0.00081341
Iteration 28/1000 | Loss: 0.00043060
Iteration 29/1000 | Loss: 0.00084748
Iteration 30/1000 | Loss: 0.00018832
Iteration 31/1000 | Loss: 0.00008768
Iteration 32/1000 | Loss: 0.00006544
Iteration 33/1000 | Loss: 0.00053707
Iteration 34/1000 | Loss: 0.00007776
Iteration 35/1000 | Loss: 0.00005467
Iteration 36/1000 | Loss: 0.00004473
Iteration 37/1000 | Loss: 0.00003749
Iteration 38/1000 | Loss: 0.00028644
Iteration 39/1000 | Loss: 0.00003737
Iteration 40/1000 | Loss: 0.00013288
Iteration 41/1000 | Loss: 0.00004100
Iteration 42/1000 | Loss: 0.00003500
Iteration 43/1000 | Loss: 0.00003108
Iteration 44/1000 | Loss: 0.00002928
Iteration 45/1000 | Loss: 0.00003257
Iteration 46/1000 | Loss: 0.00002703
Iteration 47/1000 | Loss: 0.00002600
Iteration 48/1000 | Loss: 0.00002509
Iteration 49/1000 | Loss: 0.00002432
Iteration 50/1000 | Loss: 0.00002364
Iteration 51/1000 | Loss: 0.00002310
Iteration 52/1000 | Loss: 0.00002273
Iteration 53/1000 | Loss: 0.00002249
Iteration 54/1000 | Loss: 0.00002235
Iteration 55/1000 | Loss: 0.00002234
Iteration 56/1000 | Loss: 0.00002233
Iteration 57/1000 | Loss: 0.00002232
Iteration 58/1000 | Loss: 0.00002230
Iteration 59/1000 | Loss: 0.00002220
Iteration 60/1000 | Loss: 0.00002218
Iteration 61/1000 | Loss: 0.00002218
Iteration 62/1000 | Loss: 0.00002217
Iteration 63/1000 | Loss: 0.00002217
Iteration 64/1000 | Loss: 0.00002216
Iteration 65/1000 | Loss: 0.00002216
Iteration 66/1000 | Loss: 0.00002215
Iteration 67/1000 | Loss: 0.00002215
Iteration 68/1000 | Loss: 0.00002215
Iteration 69/1000 | Loss: 0.00002215
Iteration 70/1000 | Loss: 0.00002215
Iteration 71/1000 | Loss: 0.00002215
Iteration 72/1000 | Loss: 0.00002215
Iteration 73/1000 | Loss: 0.00002215
Iteration 74/1000 | Loss: 0.00002215
Iteration 75/1000 | Loss: 0.00002214
Iteration 76/1000 | Loss: 0.00002214
Iteration 77/1000 | Loss: 0.00002211
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002211
Iteration 81/1000 | Loss: 0.00002210
Iteration 82/1000 | Loss: 0.00002210
Iteration 83/1000 | Loss: 0.00002210
Iteration 84/1000 | Loss: 0.00002210
Iteration 85/1000 | Loss: 0.00002210
Iteration 86/1000 | Loss: 0.00002210
Iteration 87/1000 | Loss: 0.00002209
Iteration 88/1000 | Loss: 0.00002209
Iteration 89/1000 | Loss: 0.00002209
Iteration 90/1000 | Loss: 0.00002208
Iteration 91/1000 | Loss: 0.00002208
Iteration 92/1000 | Loss: 0.00002207
Iteration 93/1000 | Loss: 0.00002207
Iteration 94/1000 | Loss: 0.00002207
Iteration 95/1000 | Loss: 0.00002207
Iteration 96/1000 | Loss: 0.00002207
Iteration 97/1000 | Loss: 0.00002207
Iteration 98/1000 | Loss: 0.00002207
Iteration 99/1000 | Loss: 0.00002207
Iteration 100/1000 | Loss: 0.00002207
Iteration 101/1000 | Loss: 0.00002206
Iteration 102/1000 | Loss: 0.00002206
Iteration 103/1000 | Loss: 0.00002206
Iteration 104/1000 | Loss: 0.00002206
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002204
Iteration 110/1000 | Loss: 0.00002203
Iteration 111/1000 | Loss: 0.00002203
Iteration 112/1000 | Loss: 0.00002203
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002201
Iteration 118/1000 | Loss: 0.00002201
Iteration 119/1000 | Loss: 0.00002201
Iteration 120/1000 | Loss: 0.00002201
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002200
Iteration 124/1000 | Loss: 0.00002200
Iteration 125/1000 | Loss: 0.00002199
Iteration 126/1000 | Loss: 0.00002199
Iteration 127/1000 | Loss: 0.00002199
Iteration 128/1000 | Loss: 0.00002199
Iteration 129/1000 | Loss: 0.00002199
Iteration 130/1000 | Loss: 0.00002199
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002199
Iteration 133/1000 | Loss: 0.00002199
Iteration 134/1000 | Loss: 0.00002199
Iteration 135/1000 | Loss: 0.00002198
Iteration 136/1000 | Loss: 0.00002198
Iteration 137/1000 | Loss: 0.00002198
Iteration 138/1000 | Loss: 0.00002198
Iteration 139/1000 | Loss: 0.00002198
Iteration 140/1000 | Loss: 0.00002198
Iteration 141/1000 | Loss: 0.00002198
Iteration 142/1000 | Loss: 0.00002198
Iteration 143/1000 | Loss: 0.00002198
Iteration 144/1000 | Loss: 0.00002198
Iteration 145/1000 | Loss: 0.00002198
Iteration 146/1000 | Loss: 0.00002198
Iteration 147/1000 | Loss: 0.00002198
Iteration 148/1000 | Loss: 0.00002198
Iteration 149/1000 | Loss: 0.00002197
Iteration 150/1000 | Loss: 0.00002197
Iteration 151/1000 | Loss: 0.00002197
Iteration 152/1000 | Loss: 0.00002197
Iteration 153/1000 | Loss: 0.00002197
Iteration 154/1000 | Loss: 0.00002197
Iteration 155/1000 | Loss: 0.00002197
Iteration 156/1000 | Loss: 0.00002197
Iteration 157/1000 | Loss: 0.00002196
Iteration 158/1000 | Loss: 0.00002196
Iteration 159/1000 | Loss: 0.00002196
Iteration 160/1000 | Loss: 0.00002196
Iteration 161/1000 | Loss: 0.00002196
Iteration 162/1000 | Loss: 0.00002196
Iteration 163/1000 | Loss: 0.00002196
Iteration 164/1000 | Loss: 0.00002196
Iteration 165/1000 | Loss: 0.00002196
Iteration 166/1000 | Loss: 0.00002196
Iteration 167/1000 | Loss: 0.00002196
Iteration 168/1000 | Loss: 0.00002196
Iteration 169/1000 | Loss: 0.00002196
Iteration 170/1000 | Loss: 0.00002196
Iteration 171/1000 | Loss: 0.00002196
Iteration 172/1000 | Loss: 0.00002196
Iteration 173/1000 | Loss: 0.00002196
Iteration 174/1000 | Loss: 0.00002196
Iteration 175/1000 | Loss: 0.00002196
Iteration 176/1000 | Loss: 0.00002196
Iteration 177/1000 | Loss: 0.00002196
Iteration 178/1000 | Loss: 0.00002196
Iteration 179/1000 | Loss: 0.00002196
Iteration 180/1000 | Loss: 0.00002196
Iteration 181/1000 | Loss: 0.00002196
Iteration 182/1000 | Loss: 0.00002196
Iteration 183/1000 | Loss: 0.00002196
Iteration 184/1000 | Loss: 0.00002196
Iteration 185/1000 | Loss: 0.00002196
Iteration 186/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.1961264792480506e-05, 2.1961264792480506e-05, 2.1961264792480506e-05, 2.1961264792480506e-05, 2.1961264792480506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1961264792480506e-05

Optimization complete. Final v2v error: 3.833202838897705 mm

Highest mean error: 9.541234016418457 mm for frame 85

Lowest mean error: 3.345651626586914 mm for frame 12

Saving results

Total time: 151.01775741577148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715686
Iteration 2/25 | Loss: 0.00154059
Iteration 3/25 | Loss: 0.00142378
Iteration 4/25 | Loss: 0.00140393
Iteration 5/25 | Loss: 0.00139747
Iteration 6/25 | Loss: 0.00139615
Iteration 7/25 | Loss: 0.00139613
Iteration 8/25 | Loss: 0.00139613
Iteration 9/25 | Loss: 0.00139613
Iteration 10/25 | Loss: 0.00139613
Iteration 11/25 | Loss: 0.00139613
Iteration 12/25 | Loss: 0.00139613
Iteration 13/25 | Loss: 0.00139613
Iteration 14/25 | Loss: 0.00139613
Iteration 15/25 | Loss: 0.00139613
Iteration 16/25 | Loss: 0.00139613
Iteration 17/25 | Loss: 0.00139613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013961329823359847, 0.0013961329823359847, 0.0013961329823359847, 0.0013961329823359847, 0.0013961329823359847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013961329823359847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25465131
Iteration 2/25 | Loss: 0.00135193
Iteration 3/25 | Loss: 0.00135193
Iteration 4/25 | Loss: 0.00135193
Iteration 5/25 | Loss: 0.00135193
Iteration 6/25 | Loss: 0.00135192
Iteration 7/25 | Loss: 0.00135192
Iteration 8/25 | Loss: 0.00135192
Iteration 9/25 | Loss: 0.00135192
Iteration 10/25 | Loss: 0.00135192
Iteration 11/25 | Loss: 0.00135192
Iteration 12/25 | Loss: 0.00135192
Iteration 13/25 | Loss: 0.00135192
Iteration 14/25 | Loss: 0.00135192
Iteration 15/25 | Loss: 0.00135192
Iteration 16/25 | Loss: 0.00135192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001351923681795597, 0.001351923681795597, 0.001351923681795597, 0.001351923681795597, 0.001351923681795597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001351923681795597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135192
Iteration 2/1000 | Loss: 0.00005932
Iteration 3/1000 | Loss: 0.00003145
Iteration 4/1000 | Loss: 0.00002725
Iteration 5/1000 | Loss: 0.00002559
Iteration 6/1000 | Loss: 0.00002468
Iteration 7/1000 | Loss: 0.00002388
Iteration 8/1000 | Loss: 0.00002307
Iteration 9/1000 | Loss: 0.00002258
Iteration 10/1000 | Loss: 0.00002224
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002175
Iteration 13/1000 | Loss: 0.00002172
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002163
Iteration 16/1000 | Loss: 0.00002162
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002153
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002150
Iteration 22/1000 | Loss: 0.00002149
Iteration 23/1000 | Loss: 0.00002149
Iteration 24/1000 | Loss: 0.00002148
Iteration 25/1000 | Loss: 0.00002147
Iteration 26/1000 | Loss: 0.00002146
Iteration 27/1000 | Loss: 0.00002145
Iteration 28/1000 | Loss: 0.00002141
Iteration 29/1000 | Loss: 0.00002141
Iteration 30/1000 | Loss: 0.00002140
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002140
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002137
Iteration 35/1000 | Loss: 0.00002136
Iteration 36/1000 | Loss: 0.00002136
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002135
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002135
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002134
Iteration 43/1000 | Loss: 0.00002134
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002131
Iteration 46/1000 | Loss: 0.00002131
Iteration 47/1000 | Loss: 0.00002130
Iteration 48/1000 | Loss: 0.00002130
Iteration 49/1000 | Loss: 0.00002129
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002126
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002125
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002124
Iteration 58/1000 | Loss: 0.00002124
Iteration 59/1000 | Loss: 0.00002123
Iteration 60/1000 | Loss: 0.00002123
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002122
Iteration 65/1000 | Loss: 0.00002122
Iteration 66/1000 | Loss: 0.00002122
Iteration 67/1000 | Loss: 0.00002122
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002122
Iteration 71/1000 | Loss: 0.00002122
Iteration 72/1000 | Loss: 0.00002122
Iteration 73/1000 | Loss: 0.00002121
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002121
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.1212079445831478e-05, 2.1212079445831478e-05, 2.1212079445831478e-05, 2.1212079445831478e-05, 2.1212079445831478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1212079445831478e-05

Optimization complete. Final v2v error: 3.9195945262908936 mm

Highest mean error: 4.316306114196777 mm for frame 50

Lowest mean error: 3.5329864025115967 mm for frame 238

Saving results

Total time: 39.007691621780396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058410
Iteration 2/25 | Loss: 0.00286707
Iteration 3/25 | Loss: 0.00199472
Iteration 4/25 | Loss: 0.00176278
Iteration 5/25 | Loss: 0.00162809
Iteration 6/25 | Loss: 0.00150277
Iteration 7/25 | Loss: 0.00145921
Iteration 8/25 | Loss: 0.00143598
Iteration 9/25 | Loss: 0.00141896
Iteration 10/25 | Loss: 0.00140409
Iteration 11/25 | Loss: 0.00139658
Iteration 12/25 | Loss: 0.00138030
Iteration 13/25 | Loss: 0.00137788
Iteration 14/25 | Loss: 0.00137050
Iteration 15/25 | Loss: 0.00137020
Iteration 16/25 | Loss: 0.00136899
Iteration 17/25 | Loss: 0.00136843
Iteration 18/25 | Loss: 0.00136835
Iteration 19/25 | Loss: 0.00136835
Iteration 20/25 | Loss: 0.00136835
Iteration 21/25 | Loss: 0.00136835
Iteration 22/25 | Loss: 0.00136835
Iteration 23/25 | Loss: 0.00136835
Iteration 24/25 | Loss: 0.00136835
Iteration 25/25 | Loss: 0.00136835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23499382
Iteration 2/25 | Loss: 0.00151534
Iteration 3/25 | Loss: 0.00141865
Iteration 4/25 | Loss: 0.00141865
Iteration 5/25 | Loss: 0.00141865
Iteration 6/25 | Loss: 0.00141865
Iteration 7/25 | Loss: 0.00141865
Iteration 8/25 | Loss: 0.00141865
Iteration 9/25 | Loss: 0.00141865
Iteration 10/25 | Loss: 0.00141865
Iteration 11/25 | Loss: 0.00141865
Iteration 12/25 | Loss: 0.00141865
Iteration 13/25 | Loss: 0.00141865
Iteration 14/25 | Loss: 0.00141865
Iteration 15/25 | Loss: 0.00141865
Iteration 16/25 | Loss: 0.00141865
Iteration 17/25 | Loss: 0.00141865
Iteration 18/25 | Loss: 0.00141865
Iteration 19/25 | Loss: 0.00141865
Iteration 20/25 | Loss: 0.00141865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001418650965206325, 0.001418650965206325, 0.001418650965206325, 0.001418650965206325, 0.001418650965206325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001418650965206325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141865
Iteration 2/1000 | Loss: 0.00023446
Iteration 3/1000 | Loss: 0.00029411
Iteration 4/1000 | Loss: 0.00009121
Iteration 5/1000 | Loss: 0.00006277
Iteration 6/1000 | Loss: 0.00005608
Iteration 7/1000 | Loss: 0.00044513
Iteration 8/1000 | Loss: 0.00043494
Iteration 9/1000 | Loss: 0.00007139
Iteration 10/1000 | Loss: 0.00004836
Iteration 11/1000 | Loss: 0.00003986
Iteration 12/1000 | Loss: 0.00022688
Iteration 13/1000 | Loss: 0.00004695
Iteration 14/1000 | Loss: 0.00003449
Iteration 15/1000 | Loss: 0.00003091
Iteration 16/1000 | Loss: 0.00002827
Iteration 17/1000 | Loss: 0.00002669
Iteration 18/1000 | Loss: 0.00002557
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002452
Iteration 21/1000 | Loss: 0.00002403
Iteration 22/1000 | Loss: 0.00002364
Iteration 23/1000 | Loss: 0.00002334
Iteration 24/1000 | Loss: 0.00002318
Iteration 25/1000 | Loss: 0.00002309
Iteration 26/1000 | Loss: 0.00002306
Iteration 27/1000 | Loss: 0.00002306
Iteration 28/1000 | Loss: 0.00002297
Iteration 29/1000 | Loss: 0.00002295
Iteration 30/1000 | Loss: 0.00002294
Iteration 31/1000 | Loss: 0.00002294
Iteration 32/1000 | Loss: 0.00002293
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002292
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002291
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00002288
Iteration 41/1000 | Loss: 0.00002285
Iteration 42/1000 | Loss: 0.00002285
Iteration 43/1000 | Loss: 0.00002285
Iteration 44/1000 | Loss: 0.00002284
Iteration 45/1000 | Loss: 0.00002284
Iteration 46/1000 | Loss: 0.00002283
Iteration 47/1000 | Loss: 0.00002283
Iteration 48/1000 | Loss: 0.00002283
Iteration 49/1000 | Loss: 0.00002283
Iteration 50/1000 | Loss: 0.00002283
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002282
Iteration 56/1000 | Loss: 0.00002282
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002282
Iteration 60/1000 | Loss: 0.00002282
Iteration 61/1000 | Loss: 0.00002282
Iteration 62/1000 | Loss: 0.00002282
Iteration 63/1000 | Loss: 0.00002281
Iteration 64/1000 | Loss: 0.00002281
Iteration 65/1000 | Loss: 0.00002281
Iteration 66/1000 | Loss: 0.00002281
Iteration 67/1000 | Loss: 0.00002281
Iteration 68/1000 | Loss: 0.00002280
Iteration 69/1000 | Loss: 0.00002280
Iteration 70/1000 | Loss: 0.00002279
Iteration 71/1000 | Loss: 0.00002279
Iteration 72/1000 | Loss: 0.00002279
Iteration 73/1000 | Loss: 0.00002279
Iteration 74/1000 | Loss: 0.00002279
Iteration 75/1000 | Loss: 0.00002279
Iteration 76/1000 | Loss: 0.00002279
Iteration 77/1000 | Loss: 0.00002279
Iteration 78/1000 | Loss: 0.00002279
Iteration 79/1000 | Loss: 0.00002278
Iteration 80/1000 | Loss: 0.00002278
Iteration 81/1000 | Loss: 0.00002278
Iteration 82/1000 | Loss: 0.00002278
Iteration 83/1000 | Loss: 0.00002278
Iteration 84/1000 | Loss: 0.00002278
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00002277
Iteration 87/1000 | Loss: 0.00002277
Iteration 88/1000 | Loss: 0.00002277
Iteration 89/1000 | Loss: 0.00002276
Iteration 90/1000 | Loss: 0.00002276
Iteration 91/1000 | Loss: 0.00002276
Iteration 92/1000 | Loss: 0.00002276
Iteration 93/1000 | Loss: 0.00002276
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002275
Iteration 97/1000 | Loss: 0.00002275
Iteration 98/1000 | Loss: 0.00002275
Iteration 99/1000 | Loss: 0.00002275
Iteration 100/1000 | Loss: 0.00002275
Iteration 101/1000 | Loss: 0.00002274
Iteration 102/1000 | Loss: 0.00002274
Iteration 103/1000 | Loss: 0.00002274
Iteration 104/1000 | Loss: 0.00002274
Iteration 105/1000 | Loss: 0.00002274
Iteration 106/1000 | Loss: 0.00002274
Iteration 107/1000 | Loss: 0.00002274
Iteration 108/1000 | Loss: 0.00002274
Iteration 109/1000 | Loss: 0.00002274
Iteration 110/1000 | Loss: 0.00002274
Iteration 111/1000 | Loss: 0.00002274
Iteration 112/1000 | Loss: 0.00002274
Iteration 113/1000 | Loss: 0.00002274
Iteration 114/1000 | Loss: 0.00002274
Iteration 115/1000 | Loss: 0.00002273
Iteration 116/1000 | Loss: 0.00002273
Iteration 117/1000 | Loss: 0.00002273
Iteration 118/1000 | Loss: 0.00002273
Iteration 119/1000 | Loss: 0.00002273
Iteration 120/1000 | Loss: 0.00002273
Iteration 121/1000 | Loss: 0.00002273
Iteration 122/1000 | Loss: 0.00002273
Iteration 123/1000 | Loss: 0.00002273
Iteration 124/1000 | Loss: 0.00002273
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002273
Iteration 127/1000 | Loss: 0.00002273
Iteration 128/1000 | Loss: 0.00002273
Iteration 129/1000 | Loss: 0.00002273
Iteration 130/1000 | Loss: 0.00002273
Iteration 131/1000 | Loss: 0.00002273
Iteration 132/1000 | Loss: 0.00002273
Iteration 133/1000 | Loss: 0.00002273
Iteration 134/1000 | Loss: 0.00002273
Iteration 135/1000 | Loss: 0.00002273
Iteration 136/1000 | Loss: 0.00002273
Iteration 137/1000 | Loss: 0.00002273
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Iteration 142/1000 | Loss: 0.00002273
Iteration 143/1000 | Loss: 0.00002273
Iteration 144/1000 | Loss: 0.00002273
Iteration 145/1000 | Loss: 0.00002273
Iteration 146/1000 | Loss: 0.00002273
Iteration 147/1000 | Loss: 0.00002273
Iteration 148/1000 | Loss: 0.00002273
Iteration 149/1000 | Loss: 0.00002273
Iteration 150/1000 | Loss: 0.00002273
Iteration 151/1000 | Loss: 0.00002273
Iteration 152/1000 | Loss: 0.00002273
Iteration 153/1000 | Loss: 0.00002273
Iteration 154/1000 | Loss: 0.00002273
Iteration 155/1000 | Loss: 0.00002273
Iteration 156/1000 | Loss: 0.00002273
Iteration 157/1000 | Loss: 0.00002273
Iteration 158/1000 | Loss: 0.00002273
Iteration 159/1000 | Loss: 0.00002273
Iteration 160/1000 | Loss: 0.00002273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.273053178214468e-05, 2.273053178214468e-05, 2.273053178214468e-05, 2.273053178214468e-05, 2.273053178214468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.273053178214468e-05

Optimization complete. Final v2v error: 3.954765796661377 mm

Highest mean error: 10.765110969543457 mm for frame 87

Lowest mean error: 3.340513229370117 mm for frame 223

Saving results

Total time: 86.95558214187622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949621
Iteration 2/25 | Loss: 0.00157698
Iteration 3/25 | Loss: 0.00143610
Iteration 4/25 | Loss: 0.00140877
Iteration 5/25 | Loss: 0.00139845
Iteration 6/25 | Loss: 0.00139583
Iteration 7/25 | Loss: 0.00139560
Iteration 8/25 | Loss: 0.00139560
Iteration 9/25 | Loss: 0.00139560
Iteration 10/25 | Loss: 0.00139560
Iteration 11/25 | Loss: 0.00139559
Iteration 12/25 | Loss: 0.00139559
Iteration 13/25 | Loss: 0.00139559
Iteration 14/25 | Loss: 0.00139559
Iteration 15/25 | Loss: 0.00139559
Iteration 16/25 | Loss: 0.00139559
Iteration 17/25 | Loss: 0.00139559
Iteration 18/25 | Loss: 0.00139559
Iteration 19/25 | Loss: 0.00139559
Iteration 20/25 | Loss: 0.00139559
Iteration 21/25 | Loss: 0.00139559
Iteration 22/25 | Loss: 0.00139559
Iteration 23/25 | Loss: 0.00139559
Iteration 24/25 | Loss: 0.00139559
Iteration 25/25 | Loss: 0.00139559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29848778
Iteration 2/25 | Loss: 0.00139740
Iteration 3/25 | Loss: 0.00139737
Iteration 4/25 | Loss: 0.00139737
Iteration 5/25 | Loss: 0.00139737
Iteration 6/25 | Loss: 0.00139737
Iteration 7/25 | Loss: 0.00139737
Iteration 8/25 | Loss: 0.00139737
Iteration 9/25 | Loss: 0.00139737
Iteration 10/25 | Loss: 0.00139737
Iteration 11/25 | Loss: 0.00139737
Iteration 12/25 | Loss: 0.00139737
Iteration 13/25 | Loss: 0.00139737
Iteration 14/25 | Loss: 0.00139737
Iteration 15/25 | Loss: 0.00139737
Iteration 16/25 | Loss: 0.00139737
Iteration 17/25 | Loss: 0.00139737
Iteration 18/25 | Loss: 0.00139737
Iteration 19/25 | Loss: 0.00139737
Iteration 20/25 | Loss: 0.00139737
Iteration 21/25 | Loss: 0.00139736
Iteration 22/25 | Loss: 0.00139737
Iteration 23/25 | Loss: 0.00139737
Iteration 24/25 | Loss: 0.00139736
Iteration 25/25 | Loss: 0.00139736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139736
Iteration 2/1000 | Loss: 0.00007032
Iteration 3/1000 | Loss: 0.00004278
Iteration 4/1000 | Loss: 0.00003580
Iteration 5/1000 | Loss: 0.00003273
Iteration 6/1000 | Loss: 0.00003122
Iteration 7/1000 | Loss: 0.00003022
Iteration 8/1000 | Loss: 0.00002931
Iteration 9/1000 | Loss: 0.00002873
Iteration 10/1000 | Loss: 0.00002830
Iteration 11/1000 | Loss: 0.00002806
Iteration 12/1000 | Loss: 0.00002787
Iteration 13/1000 | Loss: 0.00002779
Iteration 14/1000 | Loss: 0.00002765
Iteration 15/1000 | Loss: 0.00002763
Iteration 16/1000 | Loss: 0.00002757
Iteration 17/1000 | Loss: 0.00002755
Iteration 18/1000 | Loss: 0.00002755
Iteration 19/1000 | Loss: 0.00002754
Iteration 20/1000 | Loss: 0.00002754
Iteration 21/1000 | Loss: 0.00002754
Iteration 22/1000 | Loss: 0.00002753
Iteration 23/1000 | Loss: 0.00002753
Iteration 24/1000 | Loss: 0.00002753
Iteration 25/1000 | Loss: 0.00002752
Iteration 26/1000 | Loss: 0.00002751
Iteration 27/1000 | Loss: 0.00002751
Iteration 28/1000 | Loss: 0.00002751
Iteration 29/1000 | Loss: 0.00002750
Iteration 30/1000 | Loss: 0.00002750
Iteration 31/1000 | Loss: 0.00002750
Iteration 32/1000 | Loss: 0.00002749
Iteration 33/1000 | Loss: 0.00002749
Iteration 34/1000 | Loss: 0.00002749
Iteration 35/1000 | Loss: 0.00002749
Iteration 36/1000 | Loss: 0.00002748
Iteration 37/1000 | Loss: 0.00002748
Iteration 38/1000 | Loss: 0.00002748
Iteration 39/1000 | Loss: 0.00002748
Iteration 40/1000 | Loss: 0.00002748
Iteration 41/1000 | Loss: 0.00002747
Iteration 42/1000 | Loss: 0.00002747
Iteration 43/1000 | Loss: 0.00002747
Iteration 44/1000 | Loss: 0.00002747
Iteration 45/1000 | Loss: 0.00002747
Iteration 46/1000 | Loss: 0.00002746
Iteration 47/1000 | Loss: 0.00002746
Iteration 48/1000 | Loss: 0.00002746
Iteration 49/1000 | Loss: 0.00002746
Iteration 50/1000 | Loss: 0.00002746
Iteration 51/1000 | Loss: 0.00002746
Iteration 52/1000 | Loss: 0.00002746
Iteration 53/1000 | Loss: 0.00002745
Iteration 54/1000 | Loss: 0.00002745
Iteration 55/1000 | Loss: 0.00002745
Iteration 56/1000 | Loss: 0.00002745
Iteration 57/1000 | Loss: 0.00002745
Iteration 58/1000 | Loss: 0.00002745
Iteration 59/1000 | Loss: 0.00002745
Iteration 60/1000 | Loss: 0.00002745
Iteration 61/1000 | Loss: 0.00002745
Iteration 62/1000 | Loss: 0.00002745
Iteration 63/1000 | Loss: 0.00002745
Iteration 64/1000 | Loss: 0.00002745
Iteration 65/1000 | Loss: 0.00002744
Iteration 66/1000 | Loss: 0.00002744
Iteration 67/1000 | Loss: 0.00002744
Iteration 68/1000 | Loss: 0.00002744
Iteration 69/1000 | Loss: 0.00002744
Iteration 70/1000 | Loss: 0.00002744
Iteration 71/1000 | Loss: 0.00002744
Iteration 72/1000 | Loss: 0.00002744
Iteration 73/1000 | Loss: 0.00002743
Iteration 74/1000 | Loss: 0.00002743
Iteration 75/1000 | Loss: 0.00002743
Iteration 76/1000 | Loss: 0.00002743
Iteration 77/1000 | Loss: 0.00002743
Iteration 78/1000 | Loss: 0.00002743
Iteration 79/1000 | Loss: 0.00002743
Iteration 80/1000 | Loss: 0.00002743
Iteration 81/1000 | Loss: 0.00002743
Iteration 82/1000 | Loss: 0.00002743
Iteration 83/1000 | Loss: 0.00002742
Iteration 84/1000 | Loss: 0.00002742
Iteration 85/1000 | Loss: 0.00002742
Iteration 86/1000 | Loss: 0.00002742
Iteration 87/1000 | Loss: 0.00002742
Iteration 88/1000 | Loss: 0.00002742
Iteration 89/1000 | Loss: 0.00002742
Iteration 90/1000 | Loss: 0.00002742
Iteration 91/1000 | Loss: 0.00002742
Iteration 92/1000 | Loss: 0.00002742
Iteration 93/1000 | Loss: 0.00002741
Iteration 94/1000 | Loss: 0.00002741
Iteration 95/1000 | Loss: 0.00002741
Iteration 96/1000 | Loss: 0.00002741
Iteration 97/1000 | Loss: 0.00002741
Iteration 98/1000 | Loss: 0.00002741
Iteration 99/1000 | Loss: 0.00002741
Iteration 100/1000 | Loss: 0.00002741
Iteration 101/1000 | Loss: 0.00002741
Iteration 102/1000 | Loss: 0.00002740
Iteration 103/1000 | Loss: 0.00002740
Iteration 104/1000 | Loss: 0.00002740
Iteration 105/1000 | Loss: 0.00002740
Iteration 106/1000 | Loss: 0.00002740
Iteration 107/1000 | Loss: 0.00002739
Iteration 108/1000 | Loss: 0.00002739
Iteration 109/1000 | Loss: 0.00002739
Iteration 110/1000 | Loss: 0.00002739
Iteration 111/1000 | Loss: 0.00002739
Iteration 112/1000 | Loss: 0.00002739
Iteration 113/1000 | Loss: 0.00002739
Iteration 114/1000 | Loss: 0.00002738
Iteration 115/1000 | Loss: 0.00002738
Iteration 116/1000 | Loss: 0.00002738
Iteration 117/1000 | Loss: 0.00002738
Iteration 118/1000 | Loss: 0.00002738
Iteration 119/1000 | Loss: 0.00002738
Iteration 120/1000 | Loss: 0.00002738
Iteration 121/1000 | Loss: 0.00002738
Iteration 122/1000 | Loss: 0.00002738
Iteration 123/1000 | Loss: 0.00002737
Iteration 124/1000 | Loss: 0.00002737
Iteration 125/1000 | Loss: 0.00002737
Iteration 126/1000 | Loss: 0.00002737
Iteration 127/1000 | Loss: 0.00002737
Iteration 128/1000 | Loss: 0.00002736
Iteration 129/1000 | Loss: 0.00002736
Iteration 130/1000 | Loss: 0.00002736
Iteration 131/1000 | Loss: 0.00002736
Iteration 132/1000 | Loss: 0.00002736
Iteration 133/1000 | Loss: 0.00002736
Iteration 134/1000 | Loss: 0.00002736
Iteration 135/1000 | Loss: 0.00002736
Iteration 136/1000 | Loss: 0.00002736
Iteration 137/1000 | Loss: 0.00002735
Iteration 138/1000 | Loss: 0.00002735
Iteration 139/1000 | Loss: 0.00002735
Iteration 140/1000 | Loss: 0.00002735
Iteration 141/1000 | Loss: 0.00002735
Iteration 142/1000 | Loss: 0.00002734
Iteration 143/1000 | Loss: 0.00002734
Iteration 144/1000 | Loss: 0.00002734
Iteration 145/1000 | Loss: 0.00002734
Iteration 146/1000 | Loss: 0.00002734
Iteration 147/1000 | Loss: 0.00002734
Iteration 148/1000 | Loss: 0.00002734
Iteration 149/1000 | Loss: 0.00002734
Iteration 150/1000 | Loss: 0.00002734
Iteration 151/1000 | Loss: 0.00002734
Iteration 152/1000 | Loss: 0.00002734
Iteration 153/1000 | Loss: 0.00002734
Iteration 154/1000 | Loss: 0.00002734
Iteration 155/1000 | Loss: 0.00002734
Iteration 156/1000 | Loss: 0.00002734
Iteration 157/1000 | Loss: 0.00002734
Iteration 158/1000 | Loss: 0.00002733
Iteration 159/1000 | Loss: 0.00002733
Iteration 160/1000 | Loss: 0.00002733
Iteration 161/1000 | Loss: 0.00002733
Iteration 162/1000 | Loss: 0.00002733
Iteration 163/1000 | Loss: 0.00002733
Iteration 164/1000 | Loss: 0.00002733
Iteration 165/1000 | Loss: 0.00002733
Iteration 166/1000 | Loss: 0.00002733
Iteration 167/1000 | Loss: 0.00002733
Iteration 168/1000 | Loss: 0.00002733
Iteration 169/1000 | Loss: 0.00002732
Iteration 170/1000 | Loss: 0.00002732
Iteration 171/1000 | Loss: 0.00002732
Iteration 172/1000 | Loss: 0.00002732
Iteration 173/1000 | Loss: 0.00002732
Iteration 174/1000 | Loss: 0.00002732
Iteration 175/1000 | Loss: 0.00002732
Iteration 176/1000 | Loss: 0.00002731
Iteration 177/1000 | Loss: 0.00002731
Iteration 178/1000 | Loss: 0.00002731
Iteration 179/1000 | Loss: 0.00002731
Iteration 180/1000 | Loss: 0.00002731
Iteration 181/1000 | Loss: 0.00002731
Iteration 182/1000 | Loss: 0.00002731
Iteration 183/1000 | Loss: 0.00002731
Iteration 184/1000 | Loss: 0.00002731
Iteration 185/1000 | Loss: 0.00002731
Iteration 186/1000 | Loss: 0.00002731
Iteration 187/1000 | Loss: 0.00002731
Iteration 188/1000 | Loss: 0.00002731
Iteration 189/1000 | Loss: 0.00002731
Iteration 190/1000 | Loss: 0.00002731
Iteration 191/1000 | Loss: 0.00002731
Iteration 192/1000 | Loss: 0.00002731
Iteration 193/1000 | Loss: 0.00002730
Iteration 194/1000 | Loss: 0.00002730
Iteration 195/1000 | Loss: 0.00002730
Iteration 196/1000 | Loss: 0.00002730
Iteration 197/1000 | Loss: 0.00002730
Iteration 198/1000 | Loss: 0.00002730
Iteration 199/1000 | Loss: 0.00002730
Iteration 200/1000 | Loss: 0.00002730
Iteration 201/1000 | Loss: 0.00002730
Iteration 202/1000 | Loss: 0.00002730
Iteration 203/1000 | Loss: 0.00002730
Iteration 204/1000 | Loss: 0.00002730
Iteration 205/1000 | Loss: 0.00002730
Iteration 206/1000 | Loss: 0.00002730
Iteration 207/1000 | Loss: 0.00002730
Iteration 208/1000 | Loss: 0.00002730
Iteration 209/1000 | Loss: 0.00002729
Iteration 210/1000 | Loss: 0.00002729
Iteration 211/1000 | Loss: 0.00002729
Iteration 212/1000 | Loss: 0.00002729
Iteration 213/1000 | Loss: 0.00002729
Iteration 214/1000 | Loss: 0.00002729
Iteration 215/1000 | Loss: 0.00002729
Iteration 216/1000 | Loss: 0.00002729
Iteration 217/1000 | Loss: 0.00002729
Iteration 218/1000 | Loss: 0.00002729
Iteration 219/1000 | Loss: 0.00002729
Iteration 220/1000 | Loss: 0.00002729
Iteration 221/1000 | Loss: 0.00002729
Iteration 222/1000 | Loss: 0.00002729
Iteration 223/1000 | Loss: 0.00002729
Iteration 224/1000 | Loss: 0.00002729
Iteration 225/1000 | Loss: 0.00002729
Iteration 226/1000 | Loss: 0.00002729
Iteration 227/1000 | Loss: 0.00002729
Iteration 228/1000 | Loss: 0.00002729
Iteration 229/1000 | Loss: 0.00002729
Iteration 230/1000 | Loss: 0.00002729
Iteration 231/1000 | Loss: 0.00002729
Iteration 232/1000 | Loss: 0.00002729
Iteration 233/1000 | Loss: 0.00002729
Iteration 234/1000 | Loss: 0.00002729
Iteration 235/1000 | Loss: 0.00002729
Iteration 236/1000 | Loss: 0.00002729
Iteration 237/1000 | Loss: 0.00002729
Iteration 238/1000 | Loss: 0.00002729
Iteration 239/1000 | Loss: 0.00002729
Iteration 240/1000 | Loss: 0.00002729
Iteration 241/1000 | Loss: 0.00002729
Iteration 242/1000 | Loss: 0.00002729
Iteration 243/1000 | Loss: 0.00002729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.7287209377391264e-05, 2.7287209377391264e-05, 2.7287209377391264e-05, 2.7287209377391264e-05, 2.7287209377391264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7287209377391264e-05

Optimization complete. Final v2v error: 4.372942924499512 mm

Highest mean error: 5.769504070281982 mm for frame 71

Lowest mean error: 3.7964134216308594 mm for frame 0

Saving results

Total time: 43.13477110862732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864418
Iteration 2/25 | Loss: 0.00170657
Iteration 3/25 | Loss: 0.00146879
Iteration 4/25 | Loss: 0.00143763
Iteration 5/25 | Loss: 0.00143082
Iteration 6/25 | Loss: 0.00142910
Iteration 7/25 | Loss: 0.00142910
Iteration 8/25 | Loss: 0.00142910
Iteration 9/25 | Loss: 0.00142910
Iteration 10/25 | Loss: 0.00142910
Iteration 11/25 | Loss: 0.00142910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014291014522314072, 0.0014291014522314072, 0.0014291014522314072, 0.0014291014522314072, 0.0014291014522314072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014291014522314072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19694090
Iteration 2/25 | Loss: 0.00141561
Iteration 3/25 | Loss: 0.00141561
Iteration 4/25 | Loss: 0.00141560
Iteration 5/25 | Loss: 0.00141560
Iteration 6/25 | Loss: 0.00141560
Iteration 7/25 | Loss: 0.00141560
Iteration 8/25 | Loss: 0.00141560
Iteration 9/25 | Loss: 0.00141560
Iteration 10/25 | Loss: 0.00141560
Iteration 11/25 | Loss: 0.00141560
Iteration 12/25 | Loss: 0.00141560
Iteration 13/25 | Loss: 0.00141560
Iteration 14/25 | Loss: 0.00141560
Iteration 15/25 | Loss: 0.00141560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014156014658510685, 0.0014156014658510685, 0.0014156014658510685, 0.0014156014658510685, 0.0014156014658510685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014156014658510685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141560
Iteration 2/1000 | Loss: 0.00007494
Iteration 3/1000 | Loss: 0.00003728
Iteration 4/1000 | Loss: 0.00002605
Iteration 5/1000 | Loss: 0.00002373
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002144
Iteration 8/1000 | Loss: 0.00002073
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001958
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001883
Iteration 17/1000 | Loss: 0.00001880
Iteration 18/1000 | Loss: 0.00001874
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001868
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001862
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001860
Iteration 35/1000 | Loss: 0.00001860
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001859
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001857
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001852
Iteration 63/1000 | Loss: 0.00001852
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001848
Iteration 80/1000 | Loss: 0.00001848
Iteration 81/1000 | Loss: 0.00001848
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001846
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001846
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001845
Iteration 95/1000 | Loss: 0.00001845
Iteration 96/1000 | Loss: 0.00001845
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001844
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001842
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001841
Iteration 111/1000 | Loss: 0.00001841
Iteration 112/1000 | Loss: 0.00001841
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001841
Iteration 115/1000 | Loss: 0.00001841
Iteration 116/1000 | Loss: 0.00001840
Iteration 117/1000 | Loss: 0.00001840
Iteration 118/1000 | Loss: 0.00001840
Iteration 119/1000 | Loss: 0.00001840
Iteration 120/1000 | Loss: 0.00001840
Iteration 121/1000 | Loss: 0.00001840
Iteration 122/1000 | Loss: 0.00001840
Iteration 123/1000 | Loss: 0.00001840
Iteration 124/1000 | Loss: 0.00001840
Iteration 125/1000 | Loss: 0.00001840
Iteration 126/1000 | Loss: 0.00001839
Iteration 127/1000 | Loss: 0.00001839
Iteration 128/1000 | Loss: 0.00001839
Iteration 129/1000 | Loss: 0.00001839
Iteration 130/1000 | Loss: 0.00001839
Iteration 131/1000 | Loss: 0.00001839
Iteration 132/1000 | Loss: 0.00001839
Iteration 133/1000 | Loss: 0.00001839
Iteration 134/1000 | Loss: 0.00001839
Iteration 135/1000 | Loss: 0.00001839
Iteration 136/1000 | Loss: 0.00001839
Iteration 137/1000 | Loss: 0.00001839
Iteration 138/1000 | Loss: 0.00001839
Iteration 139/1000 | Loss: 0.00001839
Iteration 140/1000 | Loss: 0.00001839
Iteration 141/1000 | Loss: 0.00001838
Iteration 142/1000 | Loss: 0.00001838
Iteration 143/1000 | Loss: 0.00001838
Iteration 144/1000 | Loss: 0.00001838
Iteration 145/1000 | Loss: 0.00001838
Iteration 146/1000 | Loss: 0.00001838
Iteration 147/1000 | Loss: 0.00001838
Iteration 148/1000 | Loss: 0.00001838
Iteration 149/1000 | Loss: 0.00001838
Iteration 150/1000 | Loss: 0.00001838
Iteration 151/1000 | Loss: 0.00001838
Iteration 152/1000 | Loss: 0.00001838
Iteration 153/1000 | Loss: 0.00001837
Iteration 154/1000 | Loss: 0.00001837
Iteration 155/1000 | Loss: 0.00001837
Iteration 156/1000 | Loss: 0.00001837
Iteration 157/1000 | Loss: 0.00001837
Iteration 158/1000 | Loss: 0.00001837
Iteration 159/1000 | Loss: 0.00001837
Iteration 160/1000 | Loss: 0.00001837
Iteration 161/1000 | Loss: 0.00001837
Iteration 162/1000 | Loss: 0.00001837
Iteration 163/1000 | Loss: 0.00001837
Iteration 164/1000 | Loss: 0.00001837
Iteration 165/1000 | Loss: 0.00001837
Iteration 166/1000 | Loss: 0.00001837
Iteration 167/1000 | Loss: 0.00001837
Iteration 168/1000 | Loss: 0.00001837
Iteration 169/1000 | Loss: 0.00001837
Iteration 170/1000 | Loss: 0.00001837
Iteration 171/1000 | Loss: 0.00001837
Iteration 172/1000 | Loss: 0.00001837
Iteration 173/1000 | Loss: 0.00001837
Iteration 174/1000 | Loss: 0.00001837
Iteration 175/1000 | Loss: 0.00001837
Iteration 176/1000 | Loss: 0.00001837
Iteration 177/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.836609408201184e-05, 1.836609408201184e-05, 1.836609408201184e-05, 1.836609408201184e-05, 1.836609408201184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.836609408201184e-05

Optimization complete. Final v2v error: 3.647348642349243 mm

Highest mean error: 4.307802200317383 mm for frame 178

Lowest mean error: 3.23309063911438 mm for frame 201

Saving results

Total time: 47.11651301383972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131293
Iteration 2/25 | Loss: 0.01131292
Iteration 3/25 | Loss: 0.01131292
Iteration 4/25 | Loss: 0.01131292
Iteration 5/25 | Loss: 0.01131292
Iteration 6/25 | Loss: 0.01131292
Iteration 7/25 | Loss: 0.01131291
Iteration 8/25 | Loss: 0.01131291
Iteration 9/25 | Loss: 0.01131291
Iteration 10/25 | Loss: 0.01131291
Iteration 11/25 | Loss: 0.01131291
Iteration 12/25 | Loss: 0.01131291
Iteration 13/25 | Loss: 0.01131290
Iteration 14/25 | Loss: 0.01131290
Iteration 15/25 | Loss: 0.01131290
Iteration 16/25 | Loss: 0.01131290
Iteration 17/25 | Loss: 0.01131290
Iteration 18/25 | Loss: 0.01131290
Iteration 19/25 | Loss: 0.01131289
Iteration 20/25 | Loss: 0.01131289
Iteration 21/25 | Loss: 0.01131289
Iteration 22/25 | Loss: 0.01131289
Iteration 23/25 | Loss: 0.01131289
Iteration 24/25 | Loss: 0.01131288
Iteration 25/25 | Loss: 0.01131288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47939205
Iteration 2/25 | Loss: 0.10490569
Iteration 3/25 | Loss: 0.10471945
Iteration 4/25 | Loss: 0.10437605
Iteration 5/25 | Loss: 0.10437605
Iteration 6/25 | Loss: 0.10437605
Iteration 7/25 | Loss: 0.10437605
Iteration 8/25 | Loss: 0.10437605
Iteration 9/25 | Loss: 0.10437605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.10437604784965515, 0.10437604784965515, 0.10437604784965515, 0.10437604784965515, 0.10437604784965515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10437604784965515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10437605
Iteration 2/1000 | Loss: 0.00201249
Iteration 3/1000 | Loss: 0.00093409
Iteration 4/1000 | Loss: 0.00120419
Iteration 5/1000 | Loss: 0.00064566
Iteration 6/1000 | Loss: 0.00143151
Iteration 7/1000 | Loss: 0.00023742
Iteration 8/1000 | Loss: 0.00015255
Iteration 9/1000 | Loss: 0.00007324
Iteration 10/1000 | Loss: 0.00014077
Iteration 11/1000 | Loss: 0.00019478
Iteration 12/1000 | Loss: 0.00026544
Iteration 13/1000 | Loss: 0.00006681
Iteration 14/1000 | Loss: 0.00080492
Iteration 15/1000 | Loss: 0.00028293
Iteration 16/1000 | Loss: 0.00014268
Iteration 17/1000 | Loss: 0.00005256
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00015769
Iteration 20/1000 | Loss: 0.00026573
Iteration 21/1000 | Loss: 0.00009237
Iteration 22/1000 | Loss: 0.00003209
Iteration 23/1000 | Loss: 0.00002489
Iteration 24/1000 | Loss: 0.00002415
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00001969
Iteration 29/1000 | Loss: 0.00004710
Iteration 30/1000 | Loss: 0.00002363
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001918
Iteration 33/1000 | Loss: 0.00001897
Iteration 34/1000 | Loss: 0.00001893
Iteration 35/1000 | Loss: 0.00001876
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001864
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001841
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001840
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001825
Iteration 57/1000 | Loss: 0.00001825
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001824
Iteration 60/1000 | Loss: 0.00001824
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001821
Iteration 65/1000 | Loss: 0.00001820
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001819
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001812
Iteration 73/1000 | Loss: 0.00001812
Iteration 74/1000 | Loss: 0.00001812
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00001810
Iteration 77/1000 | Loss: 0.00001810
Iteration 78/1000 | Loss: 0.00001809
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001807
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001804
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001803
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001801
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001800
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001800
Iteration 115/1000 | Loss: 0.00001800
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001799
Iteration 118/1000 | Loss: 0.00001799
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001798
Iteration 121/1000 | Loss: 0.00001798
Iteration 122/1000 | Loss: 0.00001798
Iteration 123/1000 | Loss: 0.00001798
Iteration 124/1000 | Loss: 0.00001798
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001797
Iteration 127/1000 | Loss: 0.00001797
Iteration 128/1000 | Loss: 0.00001797
Iteration 129/1000 | Loss: 0.00001797
Iteration 130/1000 | Loss: 0.00001797
Iteration 131/1000 | Loss: 0.00001797
Iteration 132/1000 | Loss: 0.00001797
Iteration 133/1000 | Loss: 0.00001797
Iteration 134/1000 | Loss: 0.00001797
Iteration 135/1000 | Loss: 0.00001797
Iteration 136/1000 | Loss: 0.00001797
Iteration 137/1000 | Loss: 0.00001797
Iteration 138/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.7971970009966753e-05, 1.7971970009966753e-05, 1.7971970009966753e-05, 1.7971970009966753e-05, 1.7971970009966753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7971970009966753e-05

Optimization complete. Final v2v error: 3.5910964012145996 mm

Highest mean error: 4.038731575012207 mm for frame 27

Lowest mean error: 3.254411220550537 mm for frame 229

Saving results

Total time: 75.65184569358826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830090
Iteration 2/25 | Loss: 0.00171286
Iteration 3/25 | Loss: 0.00155849
Iteration 4/25 | Loss: 0.00146322
Iteration 5/25 | Loss: 0.00140133
Iteration 6/25 | Loss: 0.00139021
Iteration 7/25 | Loss: 0.00138803
Iteration 8/25 | Loss: 0.00138719
Iteration 9/25 | Loss: 0.00138695
Iteration 10/25 | Loss: 0.00138690
Iteration 11/25 | Loss: 0.00138690
Iteration 12/25 | Loss: 0.00138690
Iteration 13/25 | Loss: 0.00138690
Iteration 14/25 | Loss: 0.00138690
Iteration 15/25 | Loss: 0.00138690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013868995010852814, 0.0013868995010852814, 0.0013868995010852814, 0.0013868995010852814, 0.0013868995010852814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013868995010852814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70600200
Iteration 2/25 | Loss: 0.00137616
Iteration 3/25 | Loss: 0.00137615
Iteration 4/25 | Loss: 0.00137615
Iteration 5/25 | Loss: 0.00137615
Iteration 6/25 | Loss: 0.00137615
Iteration 7/25 | Loss: 0.00137615
Iteration 8/25 | Loss: 0.00137615
Iteration 9/25 | Loss: 0.00137615
Iteration 10/25 | Loss: 0.00137615
Iteration 11/25 | Loss: 0.00137615
Iteration 12/25 | Loss: 0.00137615
Iteration 13/25 | Loss: 0.00137615
Iteration 14/25 | Loss: 0.00137615
Iteration 15/25 | Loss: 0.00137615
Iteration 16/25 | Loss: 0.00137615
Iteration 17/25 | Loss: 0.00137615
Iteration 18/25 | Loss: 0.00137615
Iteration 19/25 | Loss: 0.00137615
Iteration 20/25 | Loss: 0.00137615
Iteration 21/25 | Loss: 0.00137615
Iteration 22/25 | Loss: 0.00137615
Iteration 23/25 | Loss: 0.00137615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013761470327153802, 0.0013761470327153802, 0.0013761470327153802, 0.0013761470327153802, 0.0013761470327153802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013761470327153802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137615
Iteration 2/1000 | Loss: 0.00007727
Iteration 3/1000 | Loss: 0.00004594
Iteration 4/1000 | Loss: 0.00003812
Iteration 5/1000 | Loss: 0.00003574
Iteration 6/1000 | Loss: 0.00003460
Iteration 7/1000 | Loss: 0.00003384
Iteration 8/1000 | Loss: 0.00003301
Iteration 9/1000 | Loss: 0.00003223
Iteration 10/1000 | Loss: 0.00003163
Iteration 11/1000 | Loss: 0.00003132
Iteration 12/1000 | Loss: 0.00003106
Iteration 13/1000 | Loss: 0.00003092
Iteration 14/1000 | Loss: 0.00003088
Iteration 15/1000 | Loss: 0.00003086
Iteration 16/1000 | Loss: 0.00003080
Iteration 17/1000 | Loss: 0.00003072
Iteration 18/1000 | Loss: 0.00003067
Iteration 19/1000 | Loss: 0.00003067
Iteration 20/1000 | Loss: 0.00003066
Iteration 21/1000 | Loss: 0.00003065
Iteration 22/1000 | Loss: 0.00003064
Iteration 23/1000 | Loss: 0.00003064
Iteration 24/1000 | Loss: 0.00003064
Iteration 25/1000 | Loss: 0.00003063
Iteration 26/1000 | Loss: 0.00003062
Iteration 27/1000 | Loss: 0.00003062
Iteration 28/1000 | Loss: 0.00003061
Iteration 29/1000 | Loss: 0.00003060
Iteration 30/1000 | Loss: 0.00003060
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003058
Iteration 33/1000 | Loss: 0.00003058
Iteration 34/1000 | Loss: 0.00003057
Iteration 35/1000 | Loss: 0.00003054
Iteration 36/1000 | Loss: 0.00003053
Iteration 37/1000 | Loss: 0.00003053
Iteration 38/1000 | Loss: 0.00003052
Iteration 39/1000 | Loss: 0.00003051
Iteration 40/1000 | Loss: 0.00003051
Iteration 41/1000 | Loss: 0.00003051
Iteration 42/1000 | Loss: 0.00003051
Iteration 43/1000 | Loss: 0.00003051
Iteration 44/1000 | Loss: 0.00003050
Iteration 45/1000 | Loss: 0.00003050
Iteration 46/1000 | Loss: 0.00003050
Iteration 47/1000 | Loss: 0.00003050
Iteration 48/1000 | Loss: 0.00003050
Iteration 49/1000 | Loss: 0.00003049
Iteration 50/1000 | Loss: 0.00003049
Iteration 51/1000 | Loss: 0.00003049
Iteration 52/1000 | Loss: 0.00003048
Iteration 53/1000 | Loss: 0.00003048
Iteration 54/1000 | Loss: 0.00003047
Iteration 55/1000 | Loss: 0.00003047
Iteration 56/1000 | Loss: 0.00003046
Iteration 57/1000 | Loss: 0.00003046
Iteration 58/1000 | Loss: 0.00003046
Iteration 59/1000 | Loss: 0.00003045
Iteration 60/1000 | Loss: 0.00003045
Iteration 61/1000 | Loss: 0.00003045
Iteration 62/1000 | Loss: 0.00003045
Iteration 63/1000 | Loss: 0.00003045
Iteration 64/1000 | Loss: 0.00003044
Iteration 65/1000 | Loss: 0.00003044
Iteration 66/1000 | Loss: 0.00003044
Iteration 67/1000 | Loss: 0.00003043
Iteration 68/1000 | Loss: 0.00003043
Iteration 69/1000 | Loss: 0.00003043
Iteration 70/1000 | Loss: 0.00003042
Iteration 71/1000 | Loss: 0.00003042
Iteration 72/1000 | Loss: 0.00003042
Iteration 73/1000 | Loss: 0.00003042
Iteration 74/1000 | Loss: 0.00003042
Iteration 75/1000 | Loss: 0.00003042
Iteration 76/1000 | Loss: 0.00003042
Iteration 77/1000 | Loss: 0.00003042
Iteration 78/1000 | Loss: 0.00003041
Iteration 79/1000 | Loss: 0.00003041
Iteration 80/1000 | Loss: 0.00003041
Iteration 81/1000 | Loss: 0.00003040
Iteration 82/1000 | Loss: 0.00003040
Iteration 83/1000 | Loss: 0.00003040
Iteration 84/1000 | Loss: 0.00003039
Iteration 85/1000 | Loss: 0.00003039
Iteration 86/1000 | Loss: 0.00003039
Iteration 87/1000 | Loss: 0.00003039
Iteration 88/1000 | Loss: 0.00003039
Iteration 89/1000 | Loss: 0.00003038
Iteration 90/1000 | Loss: 0.00003038
Iteration 91/1000 | Loss: 0.00003038
Iteration 92/1000 | Loss: 0.00003038
Iteration 93/1000 | Loss: 0.00003038
Iteration 94/1000 | Loss: 0.00003038
Iteration 95/1000 | Loss: 0.00003037
Iteration 96/1000 | Loss: 0.00003037
Iteration 97/1000 | Loss: 0.00003037
Iteration 98/1000 | Loss: 0.00003036
Iteration 99/1000 | Loss: 0.00003036
Iteration 100/1000 | Loss: 0.00003036
Iteration 101/1000 | Loss: 0.00003035
Iteration 102/1000 | Loss: 0.00003035
Iteration 103/1000 | Loss: 0.00003035
Iteration 104/1000 | Loss: 0.00003034
Iteration 105/1000 | Loss: 0.00003034
Iteration 106/1000 | Loss: 0.00003034
Iteration 107/1000 | Loss: 0.00003034
Iteration 108/1000 | Loss: 0.00003034
Iteration 109/1000 | Loss: 0.00003034
Iteration 110/1000 | Loss: 0.00003034
Iteration 111/1000 | Loss: 0.00003034
Iteration 112/1000 | Loss: 0.00003034
Iteration 113/1000 | Loss: 0.00003034
Iteration 114/1000 | Loss: 0.00003033
Iteration 115/1000 | Loss: 0.00003033
Iteration 116/1000 | Loss: 0.00003033
Iteration 117/1000 | Loss: 0.00003033
Iteration 118/1000 | Loss: 0.00003033
Iteration 119/1000 | Loss: 0.00003033
Iteration 120/1000 | Loss: 0.00003032
Iteration 121/1000 | Loss: 0.00003032
Iteration 122/1000 | Loss: 0.00003032
Iteration 123/1000 | Loss: 0.00003032
Iteration 124/1000 | Loss: 0.00003032
Iteration 125/1000 | Loss: 0.00003031
Iteration 126/1000 | Loss: 0.00003031
Iteration 127/1000 | Loss: 0.00003030
Iteration 128/1000 | Loss: 0.00003030
Iteration 129/1000 | Loss: 0.00003030
Iteration 130/1000 | Loss: 0.00003030
Iteration 131/1000 | Loss: 0.00003030
Iteration 132/1000 | Loss: 0.00003029
Iteration 133/1000 | Loss: 0.00003029
Iteration 134/1000 | Loss: 0.00003029
Iteration 135/1000 | Loss: 0.00003029
Iteration 136/1000 | Loss: 0.00003028
Iteration 137/1000 | Loss: 0.00003028
Iteration 138/1000 | Loss: 0.00003028
Iteration 139/1000 | Loss: 0.00003027
Iteration 140/1000 | Loss: 0.00003027
Iteration 141/1000 | Loss: 0.00003027
Iteration 142/1000 | Loss: 0.00003027
Iteration 143/1000 | Loss: 0.00003027
Iteration 144/1000 | Loss: 0.00003026
Iteration 145/1000 | Loss: 0.00003026
Iteration 146/1000 | Loss: 0.00003026
Iteration 147/1000 | Loss: 0.00003026
Iteration 148/1000 | Loss: 0.00003026
Iteration 149/1000 | Loss: 0.00003026
Iteration 150/1000 | Loss: 0.00003026
Iteration 151/1000 | Loss: 0.00003025
Iteration 152/1000 | Loss: 0.00003025
Iteration 153/1000 | Loss: 0.00003025
Iteration 154/1000 | Loss: 0.00003025
Iteration 155/1000 | Loss: 0.00003025
Iteration 156/1000 | Loss: 0.00003025
Iteration 157/1000 | Loss: 0.00003025
Iteration 158/1000 | Loss: 0.00003024
Iteration 159/1000 | Loss: 0.00003024
Iteration 160/1000 | Loss: 0.00003024
Iteration 161/1000 | Loss: 0.00003024
Iteration 162/1000 | Loss: 0.00003024
Iteration 163/1000 | Loss: 0.00003024
Iteration 164/1000 | Loss: 0.00003024
Iteration 165/1000 | Loss: 0.00003024
Iteration 166/1000 | Loss: 0.00003024
Iteration 167/1000 | Loss: 0.00003024
Iteration 168/1000 | Loss: 0.00003024
Iteration 169/1000 | Loss: 0.00003023
Iteration 170/1000 | Loss: 0.00003023
Iteration 171/1000 | Loss: 0.00003023
Iteration 172/1000 | Loss: 0.00003023
Iteration 173/1000 | Loss: 0.00003023
Iteration 174/1000 | Loss: 0.00003023
Iteration 175/1000 | Loss: 0.00003023
Iteration 176/1000 | Loss: 0.00003023
Iteration 177/1000 | Loss: 0.00003023
Iteration 178/1000 | Loss: 0.00003023
Iteration 179/1000 | Loss: 0.00003023
Iteration 180/1000 | Loss: 0.00003022
Iteration 181/1000 | Loss: 0.00003022
Iteration 182/1000 | Loss: 0.00003022
Iteration 183/1000 | Loss: 0.00003022
Iteration 184/1000 | Loss: 0.00003022
Iteration 185/1000 | Loss: 0.00003022
Iteration 186/1000 | Loss: 0.00003022
Iteration 187/1000 | Loss: 0.00003022
Iteration 188/1000 | Loss: 0.00003022
Iteration 189/1000 | Loss: 0.00003022
Iteration 190/1000 | Loss: 0.00003021
Iteration 191/1000 | Loss: 0.00003021
Iteration 192/1000 | Loss: 0.00003021
Iteration 193/1000 | Loss: 0.00003021
Iteration 194/1000 | Loss: 0.00003021
Iteration 195/1000 | Loss: 0.00003021
Iteration 196/1000 | Loss: 0.00003021
Iteration 197/1000 | Loss: 0.00003021
Iteration 198/1000 | Loss: 0.00003021
Iteration 199/1000 | Loss: 0.00003021
Iteration 200/1000 | Loss: 0.00003021
Iteration 201/1000 | Loss: 0.00003021
Iteration 202/1000 | Loss: 0.00003021
Iteration 203/1000 | Loss: 0.00003021
Iteration 204/1000 | Loss: 0.00003021
Iteration 205/1000 | Loss: 0.00003021
Iteration 206/1000 | Loss: 0.00003021
Iteration 207/1000 | Loss: 0.00003021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [3.0210860131774098e-05, 3.0210860131774098e-05, 3.0210860131774098e-05, 3.0210860131774098e-05, 3.0210860131774098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0210860131774098e-05

Optimization complete. Final v2v error: 4.604887008666992 mm

Highest mean error: 5.13980770111084 mm for frame 9

Lowest mean error: 3.6825196743011475 mm for frame 138

Saving results

Total time: 50.52393651008606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090848
Iteration 2/25 | Loss: 0.00242186
Iteration 3/25 | Loss: 0.00195374
Iteration 4/25 | Loss: 0.00178345
Iteration 5/25 | Loss: 0.00191634
Iteration 6/25 | Loss: 0.00184810
Iteration 7/25 | Loss: 0.00172674
Iteration 8/25 | Loss: 0.00159347
Iteration 9/25 | Loss: 0.00156971
Iteration 10/25 | Loss: 0.00156151
Iteration 11/25 | Loss: 0.00153837
Iteration 12/25 | Loss: 0.00151246
Iteration 13/25 | Loss: 0.00149487
Iteration 14/25 | Loss: 0.00147710
Iteration 15/25 | Loss: 0.00147046
Iteration 16/25 | Loss: 0.00146095
Iteration 17/25 | Loss: 0.00146412
Iteration 18/25 | Loss: 0.00146245
Iteration 19/25 | Loss: 0.00145538
Iteration 20/25 | Loss: 0.00145830
Iteration 21/25 | Loss: 0.00145635
Iteration 22/25 | Loss: 0.00145065
Iteration 23/25 | Loss: 0.00145106
Iteration 24/25 | Loss: 0.00145226
Iteration 25/25 | Loss: 0.00146256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24522245
Iteration 2/25 | Loss: 0.00327785
Iteration 3/25 | Loss: 0.00325568
Iteration 4/25 | Loss: 0.00325567
Iteration 5/25 | Loss: 0.00325567
Iteration 6/25 | Loss: 0.00325567
Iteration 7/25 | Loss: 0.00325567
Iteration 8/25 | Loss: 0.00325567
Iteration 9/25 | Loss: 0.00325567
Iteration 10/25 | Loss: 0.00325567
Iteration 11/25 | Loss: 0.00325567
Iteration 12/25 | Loss: 0.00325567
Iteration 13/25 | Loss: 0.00325567
Iteration 14/25 | Loss: 0.00325567
Iteration 15/25 | Loss: 0.00325567
Iteration 16/25 | Loss: 0.00325567
Iteration 17/25 | Loss: 0.00325567
Iteration 18/25 | Loss: 0.00325567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0032556718215346336, 0.0032556718215346336, 0.0032556718215346336, 0.0032556718215346336, 0.0032556718215346336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032556718215346336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325567
Iteration 2/1000 | Loss: 0.00060261
Iteration 3/1000 | Loss: 0.00054050
Iteration 4/1000 | Loss: 0.00174180
Iteration 5/1000 | Loss: 0.00266586
Iteration 6/1000 | Loss: 0.00267252
Iteration 7/1000 | Loss: 0.00250391
Iteration 8/1000 | Loss: 0.00458450
Iteration 9/1000 | Loss: 0.00125521
Iteration 10/1000 | Loss: 0.00132298
Iteration 11/1000 | Loss: 0.00155729
Iteration 12/1000 | Loss: 0.00139818
Iteration 13/1000 | Loss: 0.00111858
Iteration 14/1000 | Loss: 0.00157603
Iteration 15/1000 | Loss: 0.00144319
Iteration 16/1000 | Loss: 0.00114439
Iteration 17/1000 | Loss: 0.00170535
Iteration 18/1000 | Loss: 0.00100530
Iteration 19/1000 | Loss: 0.00080055
Iteration 20/1000 | Loss: 0.00143578
Iteration 21/1000 | Loss: 0.00205610
Iteration 22/1000 | Loss: 0.00116024
Iteration 23/1000 | Loss: 0.00102604
Iteration 24/1000 | Loss: 0.00160139
Iteration 25/1000 | Loss: 0.00111230
Iteration 26/1000 | Loss: 0.00114029
Iteration 27/1000 | Loss: 0.00141527
Iteration 28/1000 | Loss: 0.00079448
Iteration 29/1000 | Loss: 0.00145472
Iteration 30/1000 | Loss: 0.00076523
Iteration 31/1000 | Loss: 0.00098044
Iteration 32/1000 | Loss: 0.00083198
Iteration 33/1000 | Loss: 0.00101576
Iteration 34/1000 | Loss: 0.00094482
Iteration 35/1000 | Loss: 0.00119531
Iteration 36/1000 | Loss: 0.00247125
Iteration 37/1000 | Loss: 0.00162642
Iteration 38/1000 | Loss: 0.00108824
Iteration 39/1000 | Loss: 0.00063489
Iteration 40/1000 | Loss: 0.00069014
Iteration 41/1000 | Loss: 0.00076031
Iteration 42/1000 | Loss: 0.00074323
Iteration 43/1000 | Loss: 0.00067969
Iteration 44/1000 | Loss: 0.00087859
Iteration 45/1000 | Loss: 0.00103959
Iteration 46/1000 | Loss: 0.00116498
Iteration 47/1000 | Loss: 0.00085644
Iteration 48/1000 | Loss: 0.00076166
Iteration 49/1000 | Loss: 0.00177770
Iteration 50/1000 | Loss: 0.00120424
Iteration 51/1000 | Loss: 0.00131388
Iteration 52/1000 | Loss: 0.00140552
Iteration 53/1000 | Loss: 0.00162494
Iteration 54/1000 | Loss: 0.00111302
Iteration 55/1000 | Loss: 0.00150199
Iteration 56/1000 | Loss: 0.00053788
Iteration 57/1000 | Loss: 0.00070597
Iteration 58/1000 | Loss: 0.00043288
Iteration 59/1000 | Loss: 0.00073335
Iteration 60/1000 | Loss: 0.00093902
Iteration 61/1000 | Loss: 0.00059337
Iteration 62/1000 | Loss: 0.00054647
Iteration 63/1000 | Loss: 0.00089929
Iteration 64/1000 | Loss: 0.00086845
Iteration 65/1000 | Loss: 0.00100055
Iteration 66/1000 | Loss: 0.00138527
Iteration 67/1000 | Loss: 0.00107945
Iteration 68/1000 | Loss: 0.00154689
Iteration 69/1000 | Loss: 0.00136953
Iteration 70/1000 | Loss: 0.00109289
Iteration 71/1000 | Loss: 0.00068631
Iteration 72/1000 | Loss: 0.00084523
Iteration 73/1000 | Loss: 0.00080790
Iteration 74/1000 | Loss: 0.00167932
Iteration 75/1000 | Loss: 0.00081348
Iteration 76/1000 | Loss: 0.00124826
Iteration 77/1000 | Loss: 0.00087315
Iteration 78/1000 | Loss: 0.00069334
Iteration 79/1000 | Loss: 0.00073785
Iteration 80/1000 | Loss: 0.00063755
Iteration 81/1000 | Loss: 0.00058352
Iteration 82/1000 | Loss: 0.00056757
Iteration 83/1000 | Loss: 0.00048811
Iteration 84/1000 | Loss: 0.00066623
Iteration 85/1000 | Loss: 0.00070515
Iteration 86/1000 | Loss: 0.00064233
Iteration 87/1000 | Loss: 0.00056764
Iteration 88/1000 | Loss: 0.00048146
Iteration 89/1000 | Loss: 0.00041873
Iteration 90/1000 | Loss: 0.00150630
Iteration 91/1000 | Loss: 0.00047889
Iteration 92/1000 | Loss: 0.00027943
Iteration 93/1000 | Loss: 0.00086933
Iteration 94/1000 | Loss: 0.00029422
Iteration 95/1000 | Loss: 0.00041091
Iteration 96/1000 | Loss: 0.00061154
Iteration 97/1000 | Loss: 0.00056006
Iteration 98/1000 | Loss: 0.00093896
Iteration 99/1000 | Loss: 0.00076658
Iteration 100/1000 | Loss: 0.00058229
Iteration 101/1000 | Loss: 0.00066398
Iteration 102/1000 | Loss: 0.00067641
Iteration 103/1000 | Loss: 0.00064928
Iteration 104/1000 | Loss: 0.00079188
Iteration 105/1000 | Loss: 0.00104835
Iteration 106/1000 | Loss: 0.00080156
Iteration 107/1000 | Loss: 0.00062267
Iteration 108/1000 | Loss: 0.00076514
Iteration 109/1000 | Loss: 0.00105554
Iteration 110/1000 | Loss: 0.00061241
Iteration 111/1000 | Loss: 0.00038554
Iteration 112/1000 | Loss: 0.00074354
Iteration 113/1000 | Loss: 0.00078542
Iteration 114/1000 | Loss: 0.00055871
Iteration 115/1000 | Loss: 0.00049166
Iteration 116/1000 | Loss: 0.00036457
Iteration 117/1000 | Loss: 0.00027792
Iteration 118/1000 | Loss: 0.00034352
Iteration 119/1000 | Loss: 0.00061908
Iteration 120/1000 | Loss: 0.00013696
Iteration 121/1000 | Loss: 0.00015886
Iteration 122/1000 | Loss: 0.00011212
Iteration 123/1000 | Loss: 0.00015577
Iteration 124/1000 | Loss: 0.00013417
Iteration 125/1000 | Loss: 0.00015698
Iteration 126/1000 | Loss: 0.00010022
Iteration 127/1000 | Loss: 0.00031969
Iteration 128/1000 | Loss: 0.00012535
Iteration 129/1000 | Loss: 0.00020504
Iteration 130/1000 | Loss: 0.00016349
Iteration 131/1000 | Loss: 0.00008129
Iteration 132/1000 | Loss: 0.00020490
Iteration 133/1000 | Loss: 0.00028977
Iteration 134/1000 | Loss: 0.00019067
Iteration 135/1000 | Loss: 0.00018436
Iteration 136/1000 | Loss: 0.00014015
Iteration 137/1000 | Loss: 0.00010682
Iteration 138/1000 | Loss: 0.00050075
Iteration 139/1000 | Loss: 0.00014488
Iteration 140/1000 | Loss: 0.00013049
Iteration 141/1000 | Loss: 0.00006813
Iteration 142/1000 | Loss: 0.00022112
Iteration 143/1000 | Loss: 0.00013164
Iteration 144/1000 | Loss: 0.00012725
Iteration 145/1000 | Loss: 0.00012541
Iteration 146/1000 | Loss: 0.00018214
Iteration 147/1000 | Loss: 0.00011678
Iteration 148/1000 | Loss: 0.00039930
Iteration 149/1000 | Loss: 0.00020055
Iteration 150/1000 | Loss: 0.00028154
Iteration 151/1000 | Loss: 0.00034896
Iteration 152/1000 | Loss: 0.00025530
Iteration 153/1000 | Loss: 0.00027396
Iteration 154/1000 | Loss: 0.00007435
Iteration 155/1000 | Loss: 0.00014003
Iteration 156/1000 | Loss: 0.00019410
Iteration 157/1000 | Loss: 0.00019900
Iteration 158/1000 | Loss: 0.00012629
Iteration 159/1000 | Loss: 0.00010554
Iteration 160/1000 | Loss: 0.00015659
Iteration 161/1000 | Loss: 0.00013074
Iteration 162/1000 | Loss: 0.00009434
Iteration 163/1000 | Loss: 0.00017338
Iteration 164/1000 | Loss: 0.00011136
Iteration 165/1000 | Loss: 0.00048767
Iteration 166/1000 | Loss: 0.00019382
Iteration 167/1000 | Loss: 0.00022620
Iteration 168/1000 | Loss: 0.00019692
Iteration 169/1000 | Loss: 0.00023075
Iteration 170/1000 | Loss: 0.00022126
Iteration 171/1000 | Loss: 0.00021876
Iteration 172/1000 | Loss: 0.00017716
Iteration 173/1000 | Loss: 0.00009043
Iteration 174/1000 | Loss: 0.00008766
Iteration 175/1000 | Loss: 0.00007277
Iteration 176/1000 | Loss: 0.00008561
Iteration 177/1000 | Loss: 0.00008022
Iteration 178/1000 | Loss: 0.00007554
Iteration 179/1000 | Loss: 0.00024513
Iteration 180/1000 | Loss: 0.00049214
Iteration 181/1000 | Loss: 0.00034532
Iteration 182/1000 | Loss: 0.00042870
Iteration 183/1000 | Loss: 0.00018695
Iteration 184/1000 | Loss: 0.00007448
Iteration 185/1000 | Loss: 0.00040019
Iteration 186/1000 | Loss: 0.00018710
Iteration 187/1000 | Loss: 0.00063122
Iteration 188/1000 | Loss: 0.00039324
Iteration 189/1000 | Loss: 0.00051276
Iteration 190/1000 | Loss: 0.00009159
Iteration 191/1000 | Loss: 0.00006773
Iteration 192/1000 | Loss: 0.00007229
Iteration 193/1000 | Loss: 0.00006825
Iteration 194/1000 | Loss: 0.00007173
Iteration 195/1000 | Loss: 0.00023404
Iteration 196/1000 | Loss: 0.00008735
Iteration 197/1000 | Loss: 0.00006647
Iteration 198/1000 | Loss: 0.00022608
Iteration 199/1000 | Loss: 0.00011697
Iteration 200/1000 | Loss: 0.00007383
Iteration 201/1000 | Loss: 0.00025536
Iteration 202/1000 | Loss: 0.00029745
Iteration 203/1000 | Loss: 0.00036445
Iteration 204/1000 | Loss: 0.00022656
Iteration 205/1000 | Loss: 0.00007821
Iteration 206/1000 | Loss: 0.00006956
Iteration 207/1000 | Loss: 0.00005870
Iteration 208/1000 | Loss: 0.00006746
Iteration 209/1000 | Loss: 0.00005966
Iteration 210/1000 | Loss: 0.00005779
Iteration 211/1000 | Loss: 0.00006051
Iteration 212/1000 | Loss: 0.00005956
Iteration 213/1000 | Loss: 0.00006329
Iteration 214/1000 | Loss: 0.00005779
Iteration 215/1000 | Loss: 0.00006138
Iteration 216/1000 | Loss: 0.00006312
Iteration 217/1000 | Loss: 0.00006023
Iteration 218/1000 | Loss: 0.00006152
Iteration 219/1000 | Loss: 0.00007095
Iteration 220/1000 | Loss: 0.00006087
Iteration 221/1000 | Loss: 0.00006713
Iteration 222/1000 | Loss: 0.00006057
Iteration 223/1000 | Loss: 0.00006461
Iteration 224/1000 | Loss: 0.00006034
Iteration 225/1000 | Loss: 0.00006694
Iteration 226/1000 | Loss: 0.00006595
Iteration 227/1000 | Loss: 0.00007029
Iteration 228/1000 | Loss: 0.00006661
Iteration 229/1000 | Loss: 0.00006659
Iteration 230/1000 | Loss: 0.00007261
Iteration 231/1000 | Loss: 0.00006891
Iteration 232/1000 | Loss: 0.00006596
Iteration 233/1000 | Loss: 0.00006868
Iteration 234/1000 | Loss: 0.00006627
Iteration 235/1000 | Loss: 0.00006598
Iteration 236/1000 | Loss: 0.00006402
Iteration 237/1000 | Loss: 0.00006598
Iteration 238/1000 | Loss: 0.00006396
Iteration 239/1000 | Loss: 0.00007004
Iteration 240/1000 | Loss: 0.00006175
Iteration 241/1000 | Loss: 0.00007220
Iteration 242/1000 | Loss: 0.00006461
Iteration 243/1000 | Loss: 0.00006796
Iteration 244/1000 | Loss: 0.00006458
Iteration 245/1000 | Loss: 0.00007194
Iteration 246/1000 | Loss: 0.00006232
Iteration 247/1000 | Loss: 0.00006732
Iteration 248/1000 | Loss: 0.00006767
Iteration 249/1000 | Loss: 0.00006613
Iteration 250/1000 | Loss: 0.00006113
Iteration 251/1000 | Loss: 0.00006121
Iteration 252/1000 | Loss: 0.00005708
Iteration 253/1000 | Loss: 0.00006906
Iteration 254/1000 | Loss: 0.00005799
Iteration 255/1000 | Loss: 0.00005993
Iteration 256/1000 | Loss: 0.00006524
Iteration 257/1000 | Loss: 0.00006877
Iteration 258/1000 | Loss: 0.00006711
Iteration 259/1000 | Loss: 0.00006549
Iteration 260/1000 | Loss: 0.00006388
Iteration 261/1000 | Loss: 0.00006147
Iteration 262/1000 | Loss: 0.00006484
Iteration 263/1000 | Loss: 0.00006863
Iteration 264/1000 | Loss: 0.00007547
Iteration 265/1000 | Loss: 0.00006533
Iteration 266/1000 | Loss: 0.00006317
Iteration 267/1000 | Loss: 0.00007318
Iteration 268/1000 | Loss: 0.00006512
Iteration 269/1000 | Loss: 0.00007154
Iteration 270/1000 | Loss: 0.00006786
Iteration 271/1000 | Loss: 0.00006968
Iteration 272/1000 | Loss: 0.00006739
Iteration 273/1000 | Loss: 0.00006952
Iteration 274/1000 | Loss: 0.00006891
Iteration 275/1000 | Loss: 0.00006849
Iteration 276/1000 | Loss: 0.00006923
Iteration 277/1000 | Loss: 0.00006602
Iteration 278/1000 | Loss: 0.00006823
Iteration 279/1000 | Loss: 0.00006854
Iteration 280/1000 | Loss: 0.00007820
Iteration 281/1000 | Loss: 0.00006187
Iteration 282/1000 | Loss: 0.00007697
Iteration 283/1000 | Loss: 0.00006624
Iteration 284/1000 | Loss: 0.00006958
Iteration 285/1000 | Loss: 0.00006677
Iteration 286/1000 | Loss: 0.00006945
Iteration 287/1000 | Loss: 0.00006733
Iteration 288/1000 | Loss: 0.00006615
Iteration 289/1000 | Loss: 0.00006634
Iteration 290/1000 | Loss: 0.00006534
Iteration 291/1000 | Loss: 0.00006412
Iteration 292/1000 | Loss: 0.00006669
Iteration 293/1000 | Loss: 0.00006562
Iteration 294/1000 | Loss: 0.00007143
Iteration 295/1000 | Loss: 0.00007516
Iteration 296/1000 | Loss: 0.00006761
Iteration 297/1000 | Loss: 0.00006032
Iteration 298/1000 | Loss: 0.00006779
Iteration 299/1000 | Loss: 0.00006592
Iteration 300/1000 | Loss: 0.00006585
Iteration 301/1000 | Loss: 0.00007074
Iteration 302/1000 | Loss: 0.00007545
Iteration 303/1000 | Loss: 0.00007071
Iteration 304/1000 | Loss: 0.00006064
Iteration 305/1000 | Loss: 0.00005869
Iteration 306/1000 | Loss: 0.00005022
Iteration 307/1000 | Loss: 0.00005277
Iteration 308/1000 | Loss: 0.00005361
Iteration 309/1000 | Loss: 0.00005688
Iteration 310/1000 | Loss: 0.00005251
Iteration 311/1000 | Loss: 0.00005648
Iteration 312/1000 | Loss: 0.00005821
Iteration 313/1000 | Loss: 0.00005419
Iteration 314/1000 | Loss: 0.00005430
Iteration 315/1000 | Loss: 0.00005644
Iteration 316/1000 | Loss: 0.00005789
Iteration 317/1000 | Loss: 0.00005625
Iteration 318/1000 | Loss: 0.00005682
Iteration 319/1000 | Loss: 0.00005655
Iteration 320/1000 | Loss: 0.00005478
Iteration 321/1000 | Loss: 0.00005566
Iteration 322/1000 | Loss: 0.00005711
Iteration 323/1000 | Loss: 0.00005603
Iteration 324/1000 | Loss: 0.00005603
Iteration 325/1000 | Loss: 0.00005696
Iteration 326/1000 | Loss: 0.00005887
Iteration 327/1000 | Loss: 0.00005704
Iteration 328/1000 | Loss: 0.00006005
Iteration 329/1000 | Loss: 0.00004747
Iteration 330/1000 | Loss: 0.00005154
Iteration 331/1000 | Loss: 0.00005780
Iteration 332/1000 | Loss: 0.00005560
Iteration 333/1000 | Loss: 0.00005200
Iteration 334/1000 | Loss: 0.00005509
Iteration 335/1000 | Loss: 0.00005196
Iteration 336/1000 | Loss: 0.00005417
Iteration 337/1000 | Loss: 0.00006014
Iteration 338/1000 | Loss: 0.00005673
Iteration 339/1000 | Loss: 0.00005737
Iteration 340/1000 | Loss: 0.00005714
Iteration 341/1000 | Loss: 0.00005703
Iteration 342/1000 | Loss: 0.00005738
Iteration 343/1000 | Loss: 0.00005663
Iteration 344/1000 | Loss: 0.00005728
Iteration 345/1000 | Loss: 0.00005677
Iteration 346/1000 | Loss: 0.00005422
Iteration 347/1000 | Loss: 0.00005667
Iteration 348/1000 | Loss: 0.00005638
Iteration 349/1000 | Loss: 0.00005616
Iteration 350/1000 | Loss: 0.00005616
Iteration 351/1000 | Loss: 0.00005566
Iteration 352/1000 | Loss: 0.00005557
Iteration 353/1000 | Loss: 0.00005500
Iteration 354/1000 | Loss: 0.00005960
Iteration 355/1000 | Loss: 0.00005725
Iteration 356/1000 | Loss: 0.00005576
Iteration 357/1000 | Loss: 0.00005634
Iteration 358/1000 | Loss: 0.00005650
Iteration 359/1000 | Loss: 0.00005592
Iteration 360/1000 | Loss: 0.00005770
Iteration 361/1000 | Loss: 0.00005830
Iteration 362/1000 | Loss: 0.00005676
Iteration 363/1000 | Loss: 0.00005794
Iteration 364/1000 | Loss: 0.00005463
Iteration 365/1000 | Loss: 0.00005776
Iteration 366/1000 | Loss: 0.00005553
Iteration 367/1000 | Loss: 0.00005753
Iteration 368/1000 | Loss: 0.00005757
Iteration 369/1000 | Loss: 0.00005752
Iteration 370/1000 | Loss: 0.00005680
Iteration 371/1000 | Loss: 0.00005724
Iteration 372/1000 | Loss: 0.00006080
Iteration 373/1000 | Loss: 0.00005566
Iteration 374/1000 | Loss: 0.00005558
Iteration 375/1000 | Loss: 0.00005712
Iteration 376/1000 | Loss: 0.00005834
Iteration 377/1000 | Loss: 0.00006362
Iteration 378/1000 | Loss: 0.00005692
Iteration 379/1000 | Loss: 0.00005763
Iteration 380/1000 | Loss: 0.00005629
Iteration 381/1000 | Loss: 0.00005700
Iteration 382/1000 | Loss: 0.00005621
Iteration 383/1000 | Loss: 0.00005673
Iteration 384/1000 | Loss: 0.00005630
Iteration 385/1000 | Loss: 0.00005679
Iteration 386/1000 | Loss: 0.00005576
Iteration 387/1000 | Loss: 0.00005666
Iteration 388/1000 | Loss: 0.00005551
Iteration 389/1000 | Loss: 0.00005671
Iteration 390/1000 | Loss: 0.00005541
Iteration 391/1000 | Loss: 0.00005626
Iteration 392/1000 | Loss: 0.00005519
Iteration 393/1000 | Loss: 0.00005955
Iteration 394/1000 | Loss: 0.00005652
Iteration 395/1000 | Loss: 0.00004654
Iteration 396/1000 | Loss: 0.00006454
Iteration 397/1000 | Loss: 0.00004955
Iteration 398/1000 | Loss: 0.00004793
Iteration 399/1000 | Loss: 0.00004700
Iteration 400/1000 | Loss: 0.00004571
Iteration 401/1000 | Loss: 0.00004520
Iteration 402/1000 | Loss: 0.00004488
Iteration 403/1000 | Loss: 0.00004483
Iteration 404/1000 | Loss: 0.00004463
Iteration 405/1000 | Loss: 0.00004462
Iteration 406/1000 | Loss: 0.00004457
Iteration 407/1000 | Loss: 0.00004453
Iteration 408/1000 | Loss: 0.00004453
Iteration 409/1000 | Loss: 0.00004453
Iteration 410/1000 | Loss: 0.00004452
Iteration 411/1000 | Loss: 0.00004452
Iteration 412/1000 | Loss: 0.00004452
Iteration 413/1000 | Loss: 0.00004452
Iteration 414/1000 | Loss: 0.00004452
Iteration 415/1000 | Loss: 0.00004452
Iteration 416/1000 | Loss: 0.00004452
Iteration 417/1000 | Loss: 0.00004452
Iteration 418/1000 | Loss: 0.00004452
Iteration 419/1000 | Loss: 0.00004451
Iteration 420/1000 | Loss: 0.00004451
Iteration 421/1000 | Loss: 0.00004451
Iteration 422/1000 | Loss: 0.00004450
Iteration 423/1000 | Loss: 0.00004450
Iteration 424/1000 | Loss: 0.00004450
Iteration 425/1000 | Loss: 0.00004450
Iteration 426/1000 | Loss: 0.00004450
Iteration 427/1000 | Loss: 0.00004450
Iteration 428/1000 | Loss: 0.00004450
Iteration 429/1000 | Loss: 0.00004450
Iteration 430/1000 | Loss: 0.00004450
Iteration 431/1000 | Loss: 0.00004450
Iteration 432/1000 | Loss: 0.00004450
Iteration 433/1000 | Loss: 0.00004450
Iteration 434/1000 | Loss: 0.00004450
Iteration 435/1000 | Loss: 0.00004450
Iteration 436/1000 | Loss: 0.00004450
Iteration 437/1000 | Loss: 0.00004450
Iteration 438/1000 | Loss: 0.00004450
Iteration 439/1000 | Loss: 0.00004450
Iteration 440/1000 | Loss: 0.00004450
Iteration 441/1000 | Loss: 0.00004450
Iteration 442/1000 | Loss: 0.00004450
Iteration 443/1000 | Loss: 0.00004450
Iteration 444/1000 | Loss: 0.00004450
Iteration 445/1000 | Loss: 0.00004450
Iteration 446/1000 | Loss: 0.00004450
Iteration 447/1000 | Loss: 0.00004450
Iteration 448/1000 | Loss: 0.00004450
Iteration 449/1000 | Loss: 0.00004450
Iteration 450/1000 | Loss: 0.00004450
Iteration 451/1000 | Loss: 0.00004450
Iteration 452/1000 | Loss: 0.00004450
Iteration 453/1000 | Loss: 0.00004450
Iteration 454/1000 | Loss: 0.00004450
Iteration 455/1000 | Loss: 0.00004450
Iteration 456/1000 | Loss: 0.00004450
Iteration 457/1000 | Loss: 0.00004450
Iteration 458/1000 | Loss: 0.00004450
Iteration 459/1000 | Loss: 0.00004450
Iteration 460/1000 | Loss: 0.00004450
Iteration 461/1000 | Loss: 0.00004450
Iteration 462/1000 | Loss: 0.00004450
Iteration 463/1000 | Loss: 0.00004450
Iteration 464/1000 | Loss: 0.00004450
Iteration 465/1000 | Loss: 0.00004450
Iteration 466/1000 | Loss: 0.00004450
Iteration 467/1000 | Loss: 0.00004450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 467. Stopping optimization.
Last 5 losses: [4.449545667739585e-05, 4.449545667739585e-05, 4.449545667739585e-05, 4.449545667739585e-05, 4.449545667739585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.449545667739585e-05

Optimization complete. Final v2v error: 4.030168056488037 mm

Highest mean error: 11.733502388000488 mm for frame 31

Lowest mean error: 3.3455798625946045 mm for frame 231

Saving results

Total time: 697.9190826416016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652189
Iteration 2/25 | Loss: 0.00144543
Iteration 3/25 | Loss: 0.00135458
Iteration 4/25 | Loss: 0.00133925
Iteration 5/25 | Loss: 0.00133382
Iteration 6/25 | Loss: 0.00133329
Iteration 7/25 | Loss: 0.00133329
Iteration 8/25 | Loss: 0.00133329
Iteration 9/25 | Loss: 0.00133329
Iteration 10/25 | Loss: 0.00133329
Iteration 11/25 | Loss: 0.00133329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013332879170775414, 0.0013332879170775414, 0.0013332879170775414, 0.0013332879170775414, 0.0013332879170775414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013332879170775414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81950474
Iteration 2/25 | Loss: 0.00159847
Iteration 3/25 | Loss: 0.00159847
Iteration 4/25 | Loss: 0.00159847
Iteration 5/25 | Loss: 0.00159847
Iteration 6/25 | Loss: 0.00159846
Iteration 7/25 | Loss: 0.00159846
Iteration 8/25 | Loss: 0.00159846
Iteration 9/25 | Loss: 0.00159846
Iteration 10/25 | Loss: 0.00159846
Iteration 11/25 | Loss: 0.00159846
Iteration 12/25 | Loss: 0.00159846
Iteration 13/25 | Loss: 0.00159846
Iteration 14/25 | Loss: 0.00159846
Iteration 15/25 | Loss: 0.00159846
Iteration 16/25 | Loss: 0.00159846
Iteration 17/25 | Loss: 0.00159846
Iteration 18/25 | Loss: 0.00159846
Iteration 19/25 | Loss: 0.00159846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015984636265784502, 0.0015984636265784502, 0.0015984636265784502, 0.0015984636265784502, 0.0015984636265784502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015984636265784502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159846
Iteration 2/1000 | Loss: 0.00004040
Iteration 3/1000 | Loss: 0.00002718
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002308
Iteration 6/1000 | Loss: 0.00002215
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00002110
Iteration 9/1000 | Loss: 0.00002084
Iteration 10/1000 | Loss: 0.00002081
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002039
Iteration 14/1000 | Loss: 0.00002034
Iteration 15/1000 | Loss: 0.00002025
Iteration 16/1000 | Loss: 0.00002020
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002018
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00002015
Iteration 24/1000 | Loss: 0.00002015
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002014
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002014
Iteration 30/1000 | Loss: 0.00002014
Iteration 31/1000 | Loss: 0.00002014
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002014
Iteration 34/1000 | Loss: 0.00002013
Iteration 35/1000 | Loss: 0.00002013
Iteration 36/1000 | Loss: 0.00002013
Iteration 37/1000 | Loss: 0.00002012
Iteration 38/1000 | Loss: 0.00002012
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002007
Iteration 43/1000 | Loss: 0.00002007
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00002000
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001996
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001993
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001993
Iteration 67/1000 | Loss: 0.00001993
Iteration 68/1000 | Loss: 0.00001993
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001991
Iteration 72/1000 | Loss: 0.00001991
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001990
Iteration 76/1000 | Loss: 0.00001990
Iteration 77/1000 | Loss: 0.00001990
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001990
Iteration 80/1000 | Loss: 0.00001990
Iteration 81/1000 | Loss: 0.00001990
Iteration 82/1000 | Loss: 0.00001990
Iteration 83/1000 | Loss: 0.00001990
Iteration 84/1000 | Loss: 0.00001990
Iteration 85/1000 | Loss: 0.00001990
Iteration 86/1000 | Loss: 0.00001990
Iteration 87/1000 | Loss: 0.00001990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.9897475795005448e-05, 1.9897475795005448e-05, 1.9897475795005448e-05, 1.9897475795005448e-05, 1.9897475795005448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9897475795005448e-05

Optimization complete. Final v2v error: 3.78593111038208 mm

Highest mean error: 4.2715606689453125 mm for frame 118

Lowest mean error: 3.5076844692230225 mm for frame 103

Saving results

Total time: 33.012470960617065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1123/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1123/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619495
Iteration 2/25 | Loss: 0.00160754
Iteration 3/25 | Loss: 0.00144009
Iteration 4/25 | Loss: 0.00141940
Iteration 5/25 | Loss: 0.00141042
Iteration 6/25 | Loss: 0.00140834
Iteration 7/25 | Loss: 0.00140834
Iteration 8/25 | Loss: 0.00140834
Iteration 9/25 | Loss: 0.00140834
Iteration 10/25 | Loss: 0.00140834
Iteration 11/25 | Loss: 0.00140834
Iteration 12/25 | Loss: 0.00140834
Iteration 13/25 | Loss: 0.00140834
Iteration 14/25 | Loss: 0.00140834
Iteration 15/25 | Loss: 0.00140834
Iteration 16/25 | Loss: 0.00140834
Iteration 17/25 | Loss: 0.00140834
Iteration 18/25 | Loss: 0.00140834
Iteration 19/25 | Loss: 0.00140834
Iteration 20/25 | Loss: 0.00140834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014083399437367916, 0.0014083399437367916, 0.0014083399437367916, 0.0014083399437367916, 0.0014083399437367916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014083399437367916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25648880
Iteration 2/25 | Loss: 0.00139445
Iteration 3/25 | Loss: 0.00139439
Iteration 4/25 | Loss: 0.00139439
Iteration 5/25 | Loss: 0.00139439
Iteration 6/25 | Loss: 0.00139439
Iteration 7/25 | Loss: 0.00139439
Iteration 8/25 | Loss: 0.00139439
Iteration 9/25 | Loss: 0.00139439
Iteration 10/25 | Loss: 0.00139439
Iteration 11/25 | Loss: 0.00139439
Iteration 12/25 | Loss: 0.00139439
Iteration 13/25 | Loss: 0.00139439
Iteration 14/25 | Loss: 0.00139439
Iteration 15/25 | Loss: 0.00139439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013943904777988791, 0.0013943904777988791, 0.0013943904777988791, 0.0013943904777988791, 0.0013943904777988791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013943904777988791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139439
Iteration 2/1000 | Loss: 0.00006952
Iteration 3/1000 | Loss: 0.00003635
Iteration 4/1000 | Loss: 0.00002978
Iteration 5/1000 | Loss: 0.00002765
Iteration 6/1000 | Loss: 0.00002681
Iteration 7/1000 | Loss: 0.00002629
Iteration 8/1000 | Loss: 0.00002577
Iteration 9/1000 | Loss: 0.00002541
Iteration 10/1000 | Loss: 0.00002513
Iteration 11/1000 | Loss: 0.00002491
Iteration 12/1000 | Loss: 0.00002480
Iteration 13/1000 | Loss: 0.00002466
Iteration 14/1000 | Loss: 0.00002465
Iteration 15/1000 | Loss: 0.00002459
Iteration 16/1000 | Loss: 0.00002456
Iteration 17/1000 | Loss: 0.00002455
Iteration 18/1000 | Loss: 0.00002455
Iteration 19/1000 | Loss: 0.00002452
Iteration 20/1000 | Loss: 0.00002449
Iteration 21/1000 | Loss: 0.00002448
Iteration 22/1000 | Loss: 0.00002444
Iteration 23/1000 | Loss: 0.00002438
Iteration 24/1000 | Loss: 0.00002436
Iteration 25/1000 | Loss: 0.00002435
Iteration 26/1000 | Loss: 0.00002435
Iteration 27/1000 | Loss: 0.00002434
Iteration 28/1000 | Loss: 0.00002432
Iteration 29/1000 | Loss: 0.00002432
Iteration 30/1000 | Loss: 0.00002431
Iteration 31/1000 | Loss: 0.00002431
Iteration 32/1000 | Loss: 0.00002429
Iteration 33/1000 | Loss: 0.00002429
Iteration 34/1000 | Loss: 0.00002428
Iteration 35/1000 | Loss: 0.00002428
Iteration 36/1000 | Loss: 0.00002428
Iteration 37/1000 | Loss: 0.00002427
Iteration 38/1000 | Loss: 0.00002427
Iteration 39/1000 | Loss: 0.00002426
Iteration 40/1000 | Loss: 0.00002426
Iteration 41/1000 | Loss: 0.00002426
Iteration 42/1000 | Loss: 0.00002425
Iteration 43/1000 | Loss: 0.00002425
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002424
Iteration 46/1000 | Loss: 0.00002424
Iteration 47/1000 | Loss: 0.00002424
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002423
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002421
Iteration 55/1000 | Loss: 0.00002421
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002420
Iteration 58/1000 | Loss: 0.00002420
Iteration 59/1000 | Loss: 0.00002420
Iteration 60/1000 | Loss: 0.00002419
Iteration 61/1000 | Loss: 0.00002419
Iteration 62/1000 | Loss: 0.00002418
Iteration 63/1000 | Loss: 0.00002418
Iteration 64/1000 | Loss: 0.00002418
Iteration 65/1000 | Loss: 0.00002418
Iteration 66/1000 | Loss: 0.00002418
Iteration 67/1000 | Loss: 0.00002417
Iteration 68/1000 | Loss: 0.00002417
Iteration 69/1000 | Loss: 0.00002417
Iteration 70/1000 | Loss: 0.00002416
Iteration 71/1000 | Loss: 0.00002416
Iteration 72/1000 | Loss: 0.00002416
Iteration 73/1000 | Loss: 0.00002415
Iteration 74/1000 | Loss: 0.00002415
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002414
Iteration 78/1000 | Loss: 0.00002413
Iteration 79/1000 | Loss: 0.00002413
Iteration 80/1000 | Loss: 0.00002413
Iteration 81/1000 | Loss: 0.00002412
Iteration 82/1000 | Loss: 0.00002412
Iteration 83/1000 | Loss: 0.00002411
Iteration 84/1000 | Loss: 0.00002411
Iteration 85/1000 | Loss: 0.00002411
Iteration 86/1000 | Loss: 0.00002410
Iteration 87/1000 | Loss: 0.00002410
Iteration 88/1000 | Loss: 0.00002409
Iteration 89/1000 | Loss: 0.00002409
Iteration 90/1000 | Loss: 0.00002409
Iteration 91/1000 | Loss: 0.00002408
Iteration 92/1000 | Loss: 0.00002408
Iteration 93/1000 | Loss: 0.00002408
Iteration 94/1000 | Loss: 0.00002407
Iteration 95/1000 | Loss: 0.00002407
Iteration 96/1000 | Loss: 0.00002407
Iteration 97/1000 | Loss: 0.00002407
Iteration 98/1000 | Loss: 0.00002407
Iteration 99/1000 | Loss: 0.00002407
Iteration 100/1000 | Loss: 0.00002406
Iteration 101/1000 | Loss: 0.00002406
Iteration 102/1000 | Loss: 0.00002406
Iteration 103/1000 | Loss: 0.00002406
Iteration 104/1000 | Loss: 0.00002405
Iteration 105/1000 | Loss: 0.00002405
Iteration 106/1000 | Loss: 0.00002405
Iteration 107/1000 | Loss: 0.00002404
Iteration 108/1000 | Loss: 0.00002404
Iteration 109/1000 | Loss: 0.00002403
Iteration 110/1000 | Loss: 0.00002403
Iteration 111/1000 | Loss: 0.00002403
Iteration 112/1000 | Loss: 0.00002402
Iteration 113/1000 | Loss: 0.00002402
Iteration 114/1000 | Loss: 0.00002402
Iteration 115/1000 | Loss: 0.00002402
Iteration 116/1000 | Loss: 0.00002401
Iteration 117/1000 | Loss: 0.00002401
Iteration 118/1000 | Loss: 0.00002401
Iteration 119/1000 | Loss: 0.00002401
Iteration 120/1000 | Loss: 0.00002400
Iteration 121/1000 | Loss: 0.00002400
Iteration 122/1000 | Loss: 0.00002400
Iteration 123/1000 | Loss: 0.00002399
Iteration 124/1000 | Loss: 0.00002399
Iteration 125/1000 | Loss: 0.00002399
Iteration 126/1000 | Loss: 0.00002398
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002398
Iteration 129/1000 | Loss: 0.00002398
Iteration 130/1000 | Loss: 0.00002397
Iteration 131/1000 | Loss: 0.00002397
Iteration 132/1000 | Loss: 0.00002397
Iteration 133/1000 | Loss: 0.00002397
Iteration 134/1000 | Loss: 0.00002397
Iteration 135/1000 | Loss: 0.00002396
Iteration 136/1000 | Loss: 0.00002396
Iteration 137/1000 | Loss: 0.00002396
Iteration 138/1000 | Loss: 0.00002396
Iteration 139/1000 | Loss: 0.00002396
Iteration 140/1000 | Loss: 0.00002396
Iteration 141/1000 | Loss: 0.00002396
Iteration 142/1000 | Loss: 0.00002396
Iteration 143/1000 | Loss: 0.00002396
Iteration 144/1000 | Loss: 0.00002396
Iteration 145/1000 | Loss: 0.00002396
Iteration 146/1000 | Loss: 0.00002396
Iteration 147/1000 | Loss: 0.00002396
Iteration 148/1000 | Loss: 0.00002395
Iteration 149/1000 | Loss: 0.00002395
Iteration 150/1000 | Loss: 0.00002395
Iteration 151/1000 | Loss: 0.00002395
Iteration 152/1000 | Loss: 0.00002394
Iteration 153/1000 | Loss: 0.00002394
Iteration 154/1000 | Loss: 0.00002394
Iteration 155/1000 | Loss: 0.00002394
Iteration 156/1000 | Loss: 0.00002394
Iteration 157/1000 | Loss: 0.00002393
Iteration 158/1000 | Loss: 0.00002393
Iteration 159/1000 | Loss: 0.00002393
Iteration 160/1000 | Loss: 0.00002393
Iteration 161/1000 | Loss: 0.00002392
Iteration 162/1000 | Loss: 0.00002392
Iteration 163/1000 | Loss: 0.00002392
Iteration 164/1000 | Loss: 0.00002392
Iteration 165/1000 | Loss: 0.00002392
Iteration 166/1000 | Loss: 0.00002392
Iteration 167/1000 | Loss: 0.00002391
Iteration 168/1000 | Loss: 0.00002391
Iteration 169/1000 | Loss: 0.00002391
Iteration 170/1000 | Loss: 0.00002391
Iteration 171/1000 | Loss: 0.00002391
Iteration 172/1000 | Loss: 0.00002391
Iteration 173/1000 | Loss: 0.00002391
Iteration 174/1000 | Loss: 0.00002391
Iteration 175/1000 | Loss: 0.00002390
Iteration 176/1000 | Loss: 0.00002390
Iteration 177/1000 | Loss: 0.00002390
Iteration 178/1000 | Loss: 0.00002390
Iteration 179/1000 | Loss: 0.00002390
Iteration 180/1000 | Loss: 0.00002390
Iteration 181/1000 | Loss: 0.00002390
Iteration 182/1000 | Loss: 0.00002390
Iteration 183/1000 | Loss: 0.00002390
Iteration 184/1000 | Loss: 0.00002390
Iteration 185/1000 | Loss: 0.00002390
Iteration 186/1000 | Loss: 0.00002390
Iteration 187/1000 | Loss: 0.00002390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.3901044187368825e-05, 2.3901044187368825e-05, 2.3901044187368825e-05, 2.3901044187368825e-05, 2.3901044187368825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3901044187368825e-05

Optimization complete. Final v2v error: 4.177783489227295 mm

Highest mean error: 4.442647933959961 mm for frame 183

Lowest mean error: 3.8491830825805664 mm for frame 123

Saving results

Total time: 49.270331144332886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969636
Iteration 2/25 | Loss: 0.00456023
Iteration 3/25 | Loss: 0.00365002
Iteration 4/25 | Loss: 0.00330667
Iteration 5/25 | Loss: 0.00220597
Iteration 6/25 | Loss: 0.00196595
Iteration 7/25 | Loss: 0.00194629
Iteration 8/25 | Loss: 0.00155591
Iteration 9/25 | Loss: 0.00143590
Iteration 10/25 | Loss: 0.00141774
Iteration 11/25 | Loss: 0.00140927
Iteration 12/25 | Loss: 0.00146854
Iteration 13/25 | Loss: 0.00139681
Iteration 14/25 | Loss: 0.00129370
Iteration 15/25 | Loss: 0.00128329
Iteration 16/25 | Loss: 0.00123605
Iteration 17/25 | Loss: 0.00122818
Iteration 18/25 | Loss: 0.00124005
Iteration 19/25 | Loss: 0.00121280
Iteration 20/25 | Loss: 0.00120424
Iteration 21/25 | Loss: 0.00119451
Iteration 22/25 | Loss: 0.00120938
Iteration 23/25 | Loss: 0.00119672
Iteration 24/25 | Loss: 0.00117273
Iteration 25/25 | Loss: 0.00118369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38956690
Iteration 2/25 | Loss: 0.00134485
Iteration 3/25 | Loss: 0.00116905
Iteration 4/25 | Loss: 0.00116905
Iteration 5/25 | Loss: 0.00116904
Iteration 6/25 | Loss: 0.00116904
Iteration 7/25 | Loss: 0.00116904
Iteration 8/25 | Loss: 0.00116904
Iteration 9/25 | Loss: 0.00116904
Iteration 10/25 | Loss: 0.00116904
Iteration 11/25 | Loss: 0.00116904
Iteration 12/25 | Loss: 0.00116904
Iteration 13/25 | Loss: 0.00116904
Iteration 14/25 | Loss: 0.00116904
Iteration 15/25 | Loss: 0.00116904
Iteration 16/25 | Loss: 0.00116904
Iteration 17/25 | Loss: 0.00116904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011690431274473667, 0.0011690431274473667, 0.0011690431274473667, 0.0011690431274473667, 0.0011690431274473667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011690431274473667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116904
Iteration 2/1000 | Loss: 0.00061360
Iteration 3/1000 | Loss: 0.00128235
Iteration 4/1000 | Loss: 0.00023264
Iteration 5/1000 | Loss: 0.00048225
Iteration 6/1000 | Loss: 0.00026180
Iteration 7/1000 | Loss: 0.00132746
Iteration 8/1000 | Loss: 0.00044559
Iteration 9/1000 | Loss: 0.00422525
Iteration 10/1000 | Loss: 0.00198794
Iteration 11/1000 | Loss: 0.00038587
Iteration 12/1000 | Loss: 0.00039679
Iteration 13/1000 | Loss: 0.00026987
Iteration 14/1000 | Loss: 0.00011662
Iteration 15/1000 | Loss: 0.00014578
Iteration 16/1000 | Loss: 0.00005125
Iteration 17/1000 | Loss: 0.00007134
Iteration 18/1000 | Loss: 0.00005621
Iteration 19/1000 | Loss: 0.00003421
Iteration 20/1000 | Loss: 0.00003094
Iteration 21/1000 | Loss: 0.00002837
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00002446
Iteration 24/1000 | Loss: 0.00002291
Iteration 25/1000 | Loss: 0.00002189
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002067
Iteration 28/1000 | Loss: 0.00002027
Iteration 29/1000 | Loss: 0.00002007
Iteration 30/1000 | Loss: 0.00001983
Iteration 31/1000 | Loss: 0.00001977
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001971
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001969
Iteration 36/1000 | Loss: 0.00001968
Iteration 37/1000 | Loss: 0.00001959
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001951
Iteration 41/1000 | Loss: 0.00001949
Iteration 42/1000 | Loss: 0.00001949
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001941
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001935
Iteration 53/1000 | Loss: 0.00001935
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001931
Iteration 57/1000 | Loss: 0.00001931
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001926
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001926
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001908
Iteration 91/1000 | Loss: 0.00001908
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001906
Iteration 94/1000 | Loss: 0.00001906
Iteration 95/1000 | Loss: 0.00001906
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001904
Iteration 105/1000 | Loss: 0.00001904
Iteration 106/1000 | Loss: 0.00001904
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001902
Iteration 116/1000 | Loss: 0.00001902
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001901
Iteration 120/1000 | Loss: 0.00001901
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001901
Iteration 123/1000 | Loss: 0.00001901
Iteration 124/1000 | Loss: 0.00001901
Iteration 125/1000 | Loss: 0.00001900
Iteration 126/1000 | Loss: 0.00001900
Iteration 127/1000 | Loss: 0.00001900
Iteration 128/1000 | Loss: 0.00001900
Iteration 129/1000 | Loss: 0.00001900
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001900
Iteration 133/1000 | Loss: 0.00001900
Iteration 134/1000 | Loss: 0.00001900
Iteration 135/1000 | Loss: 0.00001900
Iteration 136/1000 | Loss: 0.00001900
Iteration 137/1000 | Loss: 0.00001900
Iteration 138/1000 | Loss: 0.00001900
Iteration 139/1000 | Loss: 0.00001900
Iteration 140/1000 | Loss: 0.00001900
Iteration 141/1000 | Loss: 0.00001900
Iteration 142/1000 | Loss: 0.00001900
Iteration 143/1000 | Loss: 0.00001900
Iteration 144/1000 | Loss: 0.00001900
Iteration 145/1000 | Loss: 0.00001900
Iteration 146/1000 | Loss: 0.00001900
Iteration 147/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.8995702703250572e-05, 1.8995702703250572e-05, 1.8995702703250572e-05, 1.8995702703250572e-05, 1.8995702703250572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8995702703250572e-05

Optimization complete. Final v2v error: 3.7362141609191895 mm

Highest mean error: 4.4731950759887695 mm for frame 160

Lowest mean error: 3.2562294006347656 mm for frame 24

Saving results

Total time: 113.95627427101135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379421
Iteration 2/25 | Loss: 0.00085684
Iteration 3/25 | Loss: 0.00077335
Iteration 4/25 | Loss: 0.00076647
Iteration 5/25 | Loss: 0.00076498
Iteration 6/25 | Loss: 0.00076498
Iteration 7/25 | Loss: 0.00076498
Iteration 8/25 | Loss: 0.00076498
Iteration 9/25 | Loss: 0.00076498
Iteration 10/25 | Loss: 0.00076498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0007649788749404252, 0.0007649788749404252, 0.0007649788749404252, 0.0007649788749404252, 0.0007649788749404252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007649788749404252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39872122
Iteration 2/25 | Loss: 0.00019260
Iteration 3/25 | Loss: 0.00019260
Iteration 4/25 | Loss: 0.00019259
Iteration 5/25 | Loss: 0.00019259
Iteration 6/25 | Loss: 0.00019259
Iteration 7/25 | Loss: 0.00019259
Iteration 8/25 | Loss: 0.00019259
Iteration 9/25 | Loss: 0.00019259
Iteration 10/25 | Loss: 0.00019259
Iteration 11/25 | Loss: 0.00019259
Iteration 12/25 | Loss: 0.00019259
Iteration 13/25 | Loss: 0.00019259
Iteration 14/25 | Loss: 0.00019259
Iteration 15/25 | Loss: 0.00019259
Iteration 16/25 | Loss: 0.00019259
Iteration 17/25 | Loss: 0.00019259
Iteration 18/25 | Loss: 0.00019259
Iteration 19/25 | Loss: 0.00019259
Iteration 20/25 | Loss: 0.00019259
Iteration 21/25 | Loss: 0.00019259
Iteration 22/25 | Loss: 0.00019259
Iteration 23/25 | Loss: 0.00019259
Iteration 24/25 | Loss: 0.00019259
Iteration 25/25 | Loss: 0.00019259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019259
Iteration 2/1000 | Loss: 0.00002058
Iteration 3/1000 | Loss: 0.00001178
Iteration 4/1000 | Loss: 0.00000979
Iteration 5/1000 | Loss: 0.00000887
Iteration 6/1000 | Loss: 0.00000832
Iteration 7/1000 | Loss: 0.00000795
Iteration 8/1000 | Loss: 0.00000776
Iteration 9/1000 | Loss: 0.00000751
Iteration 10/1000 | Loss: 0.00000736
Iteration 11/1000 | Loss: 0.00000726
Iteration 12/1000 | Loss: 0.00000722
Iteration 13/1000 | Loss: 0.00000722
Iteration 14/1000 | Loss: 0.00000722
Iteration 15/1000 | Loss: 0.00000719
Iteration 16/1000 | Loss: 0.00000719
Iteration 17/1000 | Loss: 0.00000719
Iteration 18/1000 | Loss: 0.00000719
Iteration 19/1000 | Loss: 0.00000719
Iteration 20/1000 | Loss: 0.00000719
Iteration 21/1000 | Loss: 0.00000719
Iteration 22/1000 | Loss: 0.00000719
Iteration 23/1000 | Loss: 0.00000719
Iteration 24/1000 | Loss: 0.00000719
Iteration 25/1000 | Loss: 0.00000718
Iteration 26/1000 | Loss: 0.00000718
Iteration 27/1000 | Loss: 0.00000718
Iteration 28/1000 | Loss: 0.00000718
Iteration 29/1000 | Loss: 0.00000718
Iteration 30/1000 | Loss: 0.00000717
Iteration 31/1000 | Loss: 0.00000716
Iteration 32/1000 | Loss: 0.00000716
Iteration 33/1000 | Loss: 0.00000715
Iteration 34/1000 | Loss: 0.00000715
Iteration 35/1000 | Loss: 0.00000715
Iteration 36/1000 | Loss: 0.00000715
Iteration 37/1000 | Loss: 0.00000714
Iteration 38/1000 | Loss: 0.00000714
Iteration 39/1000 | Loss: 0.00000714
Iteration 40/1000 | Loss: 0.00000712
Iteration 41/1000 | Loss: 0.00000711
Iteration 42/1000 | Loss: 0.00000710
Iteration 43/1000 | Loss: 0.00000710
Iteration 44/1000 | Loss: 0.00000710
Iteration 45/1000 | Loss: 0.00000710
Iteration 46/1000 | Loss: 0.00000710
Iteration 47/1000 | Loss: 0.00000710
Iteration 48/1000 | Loss: 0.00000710
Iteration 49/1000 | Loss: 0.00000710
Iteration 50/1000 | Loss: 0.00000710
Iteration 51/1000 | Loss: 0.00000709
Iteration 52/1000 | Loss: 0.00000708
Iteration 53/1000 | Loss: 0.00000708
Iteration 54/1000 | Loss: 0.00000708
Iteration 55/1000 | Loss: 0.00000707
Iteration 56/1000 | Loss: 0.00000707
Iteration 57/1000 | Loss: 0.00000707
Iteration 58/1000 | Loss: 0.00000707
Iteration 59/1000 | Loss: 0.00000707
Iteration 60/1000 | Loss: 0.00000707
Iteration 61/1000 | Loss: 0.00000706
Iteration 62/1000 | Loss: 0.00000706
Iteration 63/1000 | Loss: 0.00000706
Iteration 64/1000 | Loss: 0.00000706
Iteration 65/1000 | Loss: 0.00000706
Iteration 66/1000 | Loss: 0.00000705
Iteration 67/1000 | Loss: 0.00000705
Iteration 68/1000 | Loss: 0.00000704
Iteration 69/1000 | Loss: 0.00000704
Iteration 70/1000 | Loss: 0.00000704
Iteration 71/1000 | Loss: 0.00000704
Iteration 72/1000 | Loss: 0.00000704
Iteration 73/1000 | Loss: 0.00000704
Iteration 74/1000 | Loss: 0.00000703
Iteration 75/1000 | Loss: 0.00000703
Iteration 76/1000 | Loss: 0.00000703
Iteration 77/1000 | Loss: 0.00000703
Iteration 78/1000 | Loss: 0.00000703
Iteration 79/1000 | Loss: 0.00000703
Iteration 80/1000 | Loss: 0.00000703
Iteration 81/1000 | Loss: 0.00000703
Iteration 82/1000 | Loss: 0.00000702
Iteration 83/1000 | Loss: 0.00000702
Iteration 84/1000 | Loss: 0.00000702
Iteration 85/1000 | Loss: 0.00000702
Iteration 86/1000 | Loss: 0.00000702
Iteration 87/1000 | Loss: 0.00000701
Iteration 88/1000 | Loss: 0.00000701
Iteration 89/1000 | Loss: 0.00000701
Iteration 90/1000 | Loss: 0.00000701
Iteration 91/1000 | Loss: 0.00000700
Iteration 92/1000 | Loss: 0.00000699
Iteration 93/1000 | Loss: 0.00000699
Iteration 94/1000 | Loss: 0.00000699
Iteration 95/1000 | Loss: 0.00000699
Iteration 96/1000 | Loss: 0.00000699
Iteration 97/1000 | Loss: 0.00000699
Iteration 98/1000 | Loss: 0.00000699
Iteration 99/1000 | Loss: 0.00000698
Iteration 100/1000 | Loss: 0.00000698
Iteration 101/1000 | Loss: 0.00000698
Iteration 102/1000 | Loss: 0.00000697
Iteration 103/1000 | Loss: 0.00000697
Iteration 104/1000 | Loss: 0.00000696
Iteration 105/1000 | Loss: 0.00000696
Iteration 106/1000 | Loss: 0.00000695
Iteration 107/1000 | Loss: 0.00000695
Iteration 108/1000 | Loss: 0.00000695
Iteration 109/1000 | Loss: 0.00000695
Iteration 110/1000 | Loss: 0.00000695
Iteration 111/1000 | Loss: 0.00000694
Iteration 112/1000 | Loss: 0.00000694
Iteration 113/1000 | Loss: 0.00000694
Iteration 114/1000 | Loss: 0.00000694
Iteration 115/1000 | Loss: 0.00000693
Iteration 116/1000 | Loss: 0.00000693
Iteration 117/1000 | Loss: 0.00000693
Iteration 118/1000 | Loss: 0.00000693
Iteration 119/1000 | Loss: 0.00000692
Iteration 120/1000 | Loss: 0.00000692
Iteration 121/1000 | Loss: 0.00000692
Iteration 122/1000 | Loss: 0.00000692
Iteration 123/1000 | Loss: 0.00000692
Iteration 124/1000 | Loss: 0.00000692
Iteration 125/1000 | Loss: 0.00000692
Iteration 126/1000 | Loss: 0.00000692
Iteration 127/1000 | Loss: 0.00000692
Iteration 128/1000 | Loss: 0.00000692
Iteration 129/1000 | Loss: 0.00000691
Iteration 130/1000 | Loss: 0.00000691
Iteration 131/1000 | Loss: 0.00000691
Iteration 132/1000 | Loss: 0.00000690
Iteration 133/1000 | Loss: 0.00000690
Iteration 134/1000 | Loss: 0.00000690
Iteration 135/1000 | Loss: 0.00000689
Iteration 136/1000 | Loss: 0.00000689
Iteration 137/1000 | Loss: 0.00000689
Iteration 138/1000 | Loss: 0.00000688
Iteration 139/1000 | Loss: 0.00000688
Iteration 140/1000 | Loss: 0.00000688
Iteration 141/1000 | Loss: 0.00000688
Iteration 142/1000 | Loss: 0.00000688
Iteration 143/1000 | Loss: 0.00000688
Iteration 144/1000 | Loss: 0.00000688
Iteration 145/1000 | Loss: 0.00000688
Iteration 146/1000 | Loss: 0.00000688
Iteration 147/1000 | Loss: 0.00000687
Iteration 148/1000 | Loss: 0.00000687
Iteration 149/1000 | Loss: 0.00000687
Iteration 150/1000 | Loss: 0.00000687
Iteration 151/1000 | Loss: 0.00000687
Iteration 152/1000 | Loss: 0.00000687
Iteration 153/1000 | Loss: 0.00000687
Iteration 154/1000 | Loss: 0.00000687
Iteration 155/1000 | Loss: 0.00000687
Iteration 156/1000 | Loss: 0.00000687
Iteration 157/1000 | Loss: 0.00000686
Iteration 158/1000 | Loss: 0.00000686
Iteration 159/1000 | Loss: 0.00000686
Iteration 160/1000 | Loss: 0.00000686
Iteration 161/1000 | Loss: 0.00000686
Iteration 162/1000 | Loss: 0.00000686
Iteration 163/1000 | Loss: 0.00000686
Iteration 164/1000 | Loss: 0.00000686
Iteration 165/1000 | Loss: 0.00000686
Iteration 166/1000 | Loss: 0.00000685
Iteration 167/1000 | Loss: 0.00000685
Iteration 168/1000 | Loss: 0.00000685
Iteration 169/1000 | Loss: 0.00000685
Iteration 170/1000 | Loss: 0.00000685
Iteration 171/1000 | Loss: 0.00000685
Iteration 172/1000 | Loss: 0.00000685
Iteration 173/1000 | Loss: 0.00000685
Iteration 174/1000 | Loss: 0.00000685
Iteration 175/1000 | Loss: 0.00000685
Iteration 176/1000 | Loss: 0.00000685
Iteration 177/1000 | Loss: 0.00000685
Iteration 178/1000 | Loss: 0.00000685
Iteration 179/1000 | Loss: 0.00000685
Iteration 180/1000 | Loss: 0.00000685
Iteration 181/1000 | Loss: 0.00000685
Iteration 182/1000 | Loss: 0.00000685
Iteration 183/1000 | Loss: 0.00000685
Iteration 184/1000 | Loss: 0.00000685
Iteration 185/1000 | Loss: 0.00000685
Iteration 186/1000 | Loss: 0.00000685
Iteration 187/1000 | Loss: 0.00000685
Iteration 188/1000 | Loss: 0.00000685
Iteration 189/1000 | Loss: 0.00000685
Iteration 190/1000 | Loss: 0.00000685
Iteration 191/1000 | Loss: 0.00000685
Iteration 192/1000 | Loss: 0.00000685
Iteration 193/1000 | Loss: 0.00000685
Iteration 194/1000 | Loss: 0.00000685
Iteration 195/1000 | Loss: 0.00000685
Iteration 196/1000 | Loss: 0.00000685
Iteration 197/1000 | Loss: 0.00000685
Iteration 198/1000 | Loss: 0.00000685
Iteration 199/1000 | Loss: 0.00000685
Iteration 200/1000 | Loss: 0.00000685
Iteration 201/1000 | Loss: 0.00000685
Iteration 202/1000 | Loss: 0.00000685
Iteration 203/1000 | Loss: 0.00000685
Iteration 204/1000 | Loss: 0.00000685
Iteration 205/1000 | Loss: 0.00000685
Iteration 206/1000 | Loss: 0.00000685
Iteration 207/1000 | Loss: 0.00000685
Iteration 208/1000 | Loss: 0.00000685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [6.852322712802561e-06, 6.852322712802561e-06, 6.852322712802561e-06, 6.852322712802561e-06, 6.852322712802561e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.852322712802561e-06

Optimization complete. Final v2v error: 2.252263307571411 mm

Highest mean error: 2.526529312133789 mm for frame 172

Lowest mean error: 1.9694405794143677 mm for frame 76

Saving results

Total time: 40.29310584068298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981659
Iteration 2/25 | Loss: 0.00273320
Iteration 3/25 | Loss: 0.00189287
Iteration 4/25 | Loss: 0.00169331
Iteration 5/25 | Loss: 0.00150463
Iteration 6/25 | Loss: 0.00143197
Iteration 7/25 | Loss: 0.00144131
Iteration 8/25 | Loss: 0.00145761
Iteration 9/25 | Loss: 0.00144381
Iteration 10/25 | Loss: 0.00135524
Iteration 11/25 | Loss: 0.00127459
Iteration 12/25 | Loss: 0.00123591
Iteration 13/25 | Loss: 0.00120434
Iteration 14/25 | Loss: 0.00117970
Iteration 15/25 | Loss: 0.00115917
Iteration 16/25 | Loss: 0.00115174
Iteration 17/25 | Loss: 0.00115391
Iteration 18/25 | Loss: 0.00114446
Iteration 19/25 | Loss: 0.00113205
Iteration 20/25 | Loss: 0.00112686
Iteration 21/25 | Loss: 0.00112515
Iteration 22/25 | Loss: 0.00112162
Iteration 23/25 | Loss: 0.00111912
Iteration 24/25 | Loss: 0.00111778
Iteration 25/25 | Loss: 0.00111737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41293824
Iteration 2/25 | Loss: 0.00262748
Iteration 3/25 | Loss: 0.00191539
Iteration 4/25 | Loss: 0.00191535
Iteration 5/25 | Loss: 0.00191535
Iteration 6/25 | Loss: 0.00191535
Iteration 7/25 | Loss: 0.00191535
Iteration 8/25 | Loss: 0.00191535
Iteration 9/25 | Loss: 0.00191535
Iteration 10/25 | Loss: 0.00191535
Iteration 11/25 | Loss: 0.00191535
Iteration 12/25 | Loss: 0.00191535
Iteration 13/25 | Loss: 0.00191535
Iteration 14/25 | Loss: 0.00191535
Iteration 15/25 | Loss: 0.00191535
Iteration 16/25 | Loss: 0.00191535
Iteration 17/25 | Loss: 0.00191535
Iteration 18/25 | Loss: 0.00191535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019153478788211942, 0.0019153478788211942, 0.0019153478788211942, 0.0019153478788211942, 0.0019153478788211942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019153478788211942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191535
Iteration 2/1000 | Loss: 0.00269602
Iteration 3/1000 | Loss: 0.00227905
Iteration 4/1000 | Loss: 0.00208176
Iteration 5/1000 | Loss: 0.00290543
Iteration 6/1000 | Loss: 0.00161217
Iteration 7/1000 | Loss: 0.00109896
Iteration 8/1000 | Loss: 0.00346915
Iteration 9/1000 | Loss: 0.00260299
Iteration 10/1000 | Loss: 0.00040254
Iteration 11/1000 | Loss: 0.00381950
Iteration 12/1000 | Loss: 0.00180737
Iteration 13/1000 | Loss: 0.00103591
Iteration 14/1000 | Loss: 0.00029860
Iteration 15/1000 | Loss: 0.00012681
Iteration 16/1000 | Loss: 0.00010536
Iteration 17/1000 | Loss: 0.00117083
Iteration 18/1000 | Loss: 0.00136298
Iteration 19/1000 | Loss: 0.00043259
Iteration 20/1000 | Loss: 0.00049045
Iteration 21/1000 | Loss: 0.00027736
Iteration 22/1000 | Loss: 0.00034224
Iteration 23/1000 | Loss: 0.00070226
Iteration 24/1000 | Loss: 0.00038429
Iteration 25/1000 | Loss: 0.00056475
Iteration 26/1000 | Loss: 0.00119456
Iteration 27/1000 | Loss: 0.00068091
Iteration 28/1000 | Loss: 0.00062202
Iteration 29/1000 | Loss: 0.00056363
Iteration 30/1000 | Loss: 0.00088793
Iteration 31/1000 | Loss: 0.00062270
Iteration 32/1000 | Loss: 0.00022636
Iteration 33/1000 | Loss: 0.00035924
Iteration 34/1000 | Loss: 0.00025948
Iteration 35/1000 | Loss: 0.00024822
Iteration 36/1000 | Loss: 0.00009243
Iteration 37/1000 | Loss: 0.00023166
Iteration 38/1000 | Loss: 0.00024618
Iteration 39/1000 | Loss: 0.00052127
Iteration 40/1000 | Loss: 0.00010445
Iteration 41/1000 | Loss: 0.00043842
Iteration 42/1000 | Loss: 0.00008113
Iteration 43/1000 | Loss: 0.00011481
Iteration 44/1000 | Loss: 0.00017147
Iteration 45/1000 | Loss: 0.00007513
Iteration 46/1000 | Loss: 0.00006688
Iteration 47/1000 | Loss: 0.00037248
Iteration 48/1000 | Loss: 0.00006543
Iteration 49/1000 | Loss: 0.00032159
Iteration 50/1000 | Loss: 0.00052604
Iteration 51/1000 | Loss: 0.00032923
Iteration 52/1000 | Loss: 0.00008400
Iteration 53/1000 | Loss: 0.00042052
Iteration 54/1000 | Loss: 0.00031085
Iteration 55/1000 | Loss: 0.00053989
Iteration 56/1000 | Loss: 0.00014496
Iteration 57/1000 | Loss: 0.00006923
Iteration 58/1000 | Loss: 0.00005718
Iteration 59/1000 | Loss: 0.00009126
Iteration 60/1000 | Loss: 0.00004540
Iteration 61/1000 | Loss: 0.00008155
Iteration 62/1000 | Loss: 0.00004604
Iteration 63/1000 | Loss: 0.00003559
Iteration 64/1000 | Loss: 0.00030048
Iteration 65/1000 | Loss: 0.00003780
Iteration 66/1000 | Loss: 0.00003361
Iteration 67/1000 | Loss: 0.00003166
Iteration 68/1000 | Loss: 0.00003038
Iteration 69/1000 | Loss: 0.00002933
Iteration 70/1000 | Loss: 0.00009522
Iteration 71/1000 | Loss: 0.00002826
Iteration 72/1000 | Loss: 0.00008671
Iteration 73/1000 | Loss: 0.00002777
Iteration 74/1000 | Loss: 0.00002734
Iteration 75/1000 | Loss: 0.00002710
Iteration 76/1000 | Loss: 0.00002700
Iteration 77/1000 | Loss: 0.00002694
Iteration 78/1000 | Loss: 0.00002693
Iteration 79/1000 | Loss: 0.00002692
Iteration 80/1000 | Loss: 0.00002688
Iteration 81/1000 | Loss: 0.00002677
Iteration 82/1000 | Loss: 0.00002676
Iteration 83/1000 | Loss: 0.00002672
Iteration 84/1000 | Loss: 0.00002668
Iteration 85/1000 | Loss: 0.00002667
Iteration 86/1000 | Loss: 0.00002667
Iteration 87/1000 | Loss: 0.00002666
Iteration 88/1000 | Loss: 0.00002663
Iteration 89/1000 | Loss: 0.00002661
Iteration 90/1000 | Loss: 0.00002661
Iteration 91/1000 | Loss: 0.00002660
Iteration 92/1000 | Loss: 0.00002660
Iteration 93/1000 | Loss: 0.00002658
Iteration 94/1000 | Loss: 0.00002658
Iteration 95/1000 | Loss: 0.00002657
Iteration 96/1000 | Loss: 0.00002657
Iteration 97/1000 | Loss: 0.00002657
Iteration 98/1000 | Loss: 0.00002657
Iteration 99/1000 | Loss: 0.00002656
Iteration 100/1000 | Loss: 0.00002656
Iteration 101/1000 | Loss: 0.00002656
Iteration 102/1000 | Loss: 0.00002656
Iteration 103/1000 | Loss: 0.00002656
Iteration 104/1000 | Loss: 0.00002655
Iteration 105/1000 | Loss: 0.00002655
Iteration 106/1000 | Loss: 0.00002655
Iteration 107/1000 | Loss: 0.00002655
Iteration 108/1000 | Loss: 0.00002655
Iteration 109/1000 | Loss: 0.00002655
Iteration 110/1000 | Loss: 0.00002655
Iteration 111/1000 | Loss: 0.00002655
Iteration 112/1000 | Loss: 0.00002654
Iteration 113/1000 | Loss: 0.00002654
Iteration 114/1000 | Loss: 0.00002654
Iteration 115/1000 | Loss: 0.00002654
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002654
Iteration 119/1000 | Loss: 0.00002654
Iteration 120/1000 | Loss: 0.00002654
Iteration 121/1000 | Loss: 0.00002654
Iteration 122/1000 | Loss: 0.00002654
Iteration 123/1000 | Loss: 0.00002654
Iteration 124/1000 | Loss: 0.00002653
Iteration 125/1000 | Loss: 0.00002653
Iteration 126/1000 | Loss: 0.00002653
Iteration 127/1000 | Loss: 0.00034805
Iteration 128/1000 | Loss: 0.00002938
Iteration 129/1000 | Loss: 0.00002662
Iteration 130/1000 | Loss: 0.00002577
Iteration 131/1000 | Loss: 0.00002499
Iteration 132/1000 | Loss: 0.00002438
Iteration 133/1000 | Loss: 0.00002405
Iteration 134/1000 | Loss: 0.00002386
Iteration 135/1000 | Loss: 0.00002385
Iteration 136/1000 | Loss: 0.00002384
Iteration 137/1000 | Loss: 0.00002384
Iteration 138/1000 | Loss: 0.00002383
Iteration 139/1000 | Loss: 0.00002380
Iteration 140/1000 | Loss: 0.00002374
Iteration 141/1000 | Loss: 0.00002367
Iteration 142/1000 | Loss: 0.00002366
Iteration 143/1000 | Loss: 0.00002366
Iteration 144/1000 | Loss: 0.00002365
Iteration 145/1000 | Loss: 0.00002365
Iteration 146/1000 | Loss: 0.00002365
Iteration 147/1000 | Loss: 0.00002365
Iteration 148/1000 | Loss: 0.00002365
Iteration 149/1000 | Loss: 0.00002365
Iteration 150/1000 | Loss: 0.00002365
Iteration 151/1000 | Loss: 0.00002365
Iteration 152/1000 | Loss: 0.00002364
Iteration 153/1000 | Loss: 0.00002364
Iteration 154/1000 | Loss: 0.00002364
Iteration 155/1000 | Loss: 0.00002363
Iteration 156/1000 | Loss: 0.00002363
Iteration 157/1000 | Loss: 0.00002363
Iteration 158/1000 | Loss: 0.00002362
Iteration 159/1000 | Loss: 0.00002362
Iteration 160/1000 | Loss: 0.00002362
Iteration 161/1000 | Loss: 0.00002362
Iteration 162/1000 | Loss: 0.00002362
Iteration 163/1000 | Loss: 0.00002362
Iteration 164/1000 | Loss: 0.00002362
Iteration 165/1000 | Loss: 0.00002362
Iteration 166/1000 | Loss: 0.00002361
Iteration 167/1000 | Loss: 0.00002361
Iteration 168/1000 | Loss: 0.00002361
Iteration 169/1000 | Loss: 0.00002361
Iteration 170/1000 | Loss: 0.00002361
Iteration 171/1000 | Loss: 0.00002361
Iteration 172/1000 | Loss: 0.00002361
Iteration 173/1000 | Loss: 0.00002361
Iteration 174/1000 | Loss: 0.00002360
Iteration 175/1000 | Loss: 0.00002360
Iteration 176/1000 | Loss: 0.00002360
Iteration 177/1000 | Loss: 0.00002360
Iteration 178/1000 | Loss: 0.00002360
Iteration 179/1000 | Loss: 0.00002360
Iteration 180/1000 | Loss: 0.00002360
Iteration 181/1000 | Loss: 0.00002360
Iteration 182/1000 | Loss: 0.00002360
Iteration 183/1000 | Loss: 0.00002360
Iteration 184/1000 | Loss: 0.00002360
Iteration 185/1000 | Loss: 0.00002360
Iteration 186/1000 | Loss: 0.00002360
Iteration 187/1000 | Loss: 0.00002360
Iteration 188/1000 | Loss: 0.00002360
Iteration 189/1000 | Loss: 0.00002360
Iteration 190/1000 | Loss: 0.00002359
Iteration 191/1000 | Loss: 0.00002359
Iteration 192/1000 | Loss: 0.00002359
Iteration 193/1000 | Loss: 0.00002359
Iteration 194/1000 | Loss: 0.00002359
Iteration 195/1000 | Loss: 0.00002359
Iteration 196/1000 | Loss: 0.00002359
Iteration 197/1000 | Loss: 0.00002359
Iteration 198/1000 | Loss: 0.00002359
Iteration 199/1000 | Loss: 0.00002359
Iteration 200/1000 | Loss: 0.00002359
Iteration 201/1000 | Loss: 0.00002359
Iteration 202/1000 | Loss: 0.00002359
Iteration 203/1000 | Loss: 0.00002359
Iteration 204/1000 | Loss: 0.00002358
Iteration 205/1000 | Loss: 0.00002358
Iteration 206/1000 | Loss: 0.00002358
Iteration 207/1000 | Loss: 0.00002358
Iteration 208/1000 | Loss: 0.00002358
Iteration 209/1000 | Loss: 0.00002358
Iteration 210/1000 | Loss: 0.00002358
Iteration 211/1000 | Loss: 0.00002358
Iteration 212/1000 | Loss: 0.00002358
Iteration 213/1000 | Loss: 0.00002358
Iteration 214/1000 | Loss: 0.00002358
Iteration 215/1000 | Loss: 0.00002358
Iteration 216/1000 | Loss: 0.00002358
Iteration 217/1000 | Loss: 0.00002358
Iteration 218/1000 | Loss: 0.00002358
Iteration 219/1000 | Loss: 0.00002358
Iteration 220/1000 | Loss: 0.00002358
Iteration 221/1000 | Loss: 0.00002358
Iteration 222/1000 | Loss: 0.00002358
Iteration 223/1000 | Loss: 0.00002358
Iteration 224/1000 | Loss: 0.00002358
Iteration 225/1000 | Loss: 0.00002358
Iteration 226/1000 | Loss: 0.00002358
Iteration 227/1000 | Loss: 0.00002358
Iteration 228/1000 | Loss: 0.00002358
Iteration 229/1000 | Loss: 0.00002358
Iteration 230/1000 | Loss: 0.00002358
Iteration 231/1000 | Loss: 0.00002358
Iteration 232/1000 | Loss: 0.00002358
Iteration 233/1000 | Loss: 0.00002358
Iteration 234/1000 | Loss: 0.00002358
Iteration 235/1000 | Loss: 0.00002358
Iteration 236/1000 | Loss: 0.00002358
Iteration 237/1000 | Loss: 0.00002358
Iteration 238/1000 | Loss: 0.00002358
Iteration 239/1000 | Loss: 0.00002358
Iteration 240/1000 | Loss: 0.00002358
Iteration 241/1000 | Loss: 0.00002358
Iteration 242/1000 | Loss: 0.00002358
Iteration 243/1000 | Loss: 0.00002358
Iteration 244/1000 | Loss: 0.00002358
Iteration 245/1000 | Loss: 0.00002358
Iteration 246/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.358140227443073e-05, 2.358140227443073e-05, 2.358140227443073e-05, 2.358140227443073e-05, 2.358140227443073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.358140227443073e-05

Optimization complete. Final v2v error: 3.558473587036133 mm

Highest mean error: 11.136524200439453 mm for frame 126

Lowest mean error: 3.058359384536743 mm for frame 7

Saving results

Total time: 181.37596321105957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911344
Iteration 2/25 | Loss: 0.00166209
Iteration 3/25 | Loss: 0.00114319
Iteration 4/25 | Loss: 0.00107460
Iteration 5/25 | Loss: 0.00106672
Iteration 6/25 | Loss: 0.00101628
Iteration 7/25 | Loss: 0.00097713
Iteration 8/25 | Loss: 0.00093646
Iteration 9/25 | Loss: 0.00091301
Iteration 10/25 | Loss: 0.00090651
Iteration 11/25 | Loss: 0.00091462
Iteration 12/25 | Loss: 0.00090720
Iteration 13/25 | Loss: 0.00090359
Iteration 14/25 | Loss: 0.00090285
Iteration 15/25 | Loss: 0.00090260
Iteration 16/25 | Loss: 0.00090224
Iteration 17/25 | Loss: 0.00090194
Iteration 18/25 | Loss: 0.00090180
Iteration 19/25 | Loss: 0.00090167
Iteration 20/25 | Loss: 0.00090157
Iteration 21/25 | Loss: 0.00090157
Iteration 22/25 | Loss: 0.00090156
Iteration 23/25 | Loss: 0.00090156
Iteration 24/25 | Loss: 0.00090156
Iteration 25/25 | Loss: 0.00090155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44561040
Iteration 2/25 | Loss: 0.00034718
Iteration 3/25 | Loss: 0.00034718
Iteration 4/25 | Loss: 0.00034718
Iteration 5/25 | Loss: 0.00034718
Iteration 6/25 | Loss: 0.00034718
Iteration 7/25 | Loss: 0.00034718
Iteration 8/25 | Loss: 0.00034718
Iteration 9/25 | Loss: 0.00034718
Iteration 10/25 | Loss: 0.00034717
Iteration 11/25 | Loss: 0.00034717
Iteration 12/25 | Loss: 0.00034717
Iteration 13/25 | Loss: 0.00034717
Iteration 14/25 | Loss: 0.00034717
Iteration 15/25 | Loss: 0.00034717
Iteration 16/25 | Loss: 0.00034717
Iteration 17/25 | Loss: 0.00034717
Iteration 18/25 | Loss: 0.00034717
Iteration 19/25 | Loss: 0.00034717
Iteration 20/25 | Loss: 0.00034717
Iteration 21/25 | Loss: 0.00034717
Iteration 22/25 | Loss: 0.00034717
Iteration 23/25 | Loss: 0.00034717
Iteration 24/25 | Loss: 0.00034717
Iteration 25/25 | Loss: 0.00034717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034717
Iteration 2/1000 | Loss: 0.00003836
Iteration 3/1000 | Loss: 0.00008633
Iteration 4/1000 | Loss: 0.00034777
Iteration 5/1000 | Loss: 0.00021893
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001800
Iteration 8/1000 | Loss: 0.00001759
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001690
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001643
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001634
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001629
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001611
Iteration 41/1000 | Loss: 0.00001610
Iteration 42/1000 | Loss: 0.00001610
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001606
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001606
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001605
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001604
Iteration 53/1000 | Loss: 0.00001604
Iteration 54/1000 | Loss: 0.00001603
Iteration 55/1000 | Loss: 0.00001603
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001599
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001591
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001590
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001589
Iteration 88/1000 | Loss: 0.00001589
Iteration 89/1000 | Loss: 0.00001589
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001584
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001582
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001575
Iteration 106/1000 | Loss: 0.00001573
Iteration 107/1000 | Loss: 0.00001570
Iteration 108/1000 | Loss: 0.00001565
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001561
Iteration 113/1000 | Loss: 0.00002597
Iteration 114/1000 | Loss: 0.00002276
Iteration 115/1000 | Loss: 0.00002525
Iteration 116/1000 | Loss: 0.00002164
Iteration 117/1000 | Loss: 0.00001571
Iteration 118/1000 | Loss: 0.00002446
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001527
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001495
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001492
Iteration 125/1000 | Loss: 0.00001491
Iteration 126/1000 | Loss: 0.00001491
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Iteration 129/1000 | Loss: 0.00001489
Iteration 130/1000 | Loss: 0.00001489
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001472
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001466
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001465
Iteration 154/1000 | Loss: 0.00001465
Iteration 155/1000 | Loss: 0.00001465
Iteration 156/1000 | Loss: 0.00001465
Iteration 157/1000 | Loss: 0.00001465
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001464
Iteration 160/1000 | Loss: 0.00001464
Iteration 161/1000 | Loss: 0.00001464
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001463
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001462
Iteration 181/1000 | Loss: 0.00001462
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001461
Iteration 192/1000 | Loss: 0.00001461
Iteration 193/1000 | Loss: 0.00001461
Iteration 194/1000 | Loss: 0.00001461
Iteration 195/1000 | Loss: 0.00001461
Iteration 196/1000 | Loss: 0.00001461
Iteration 197/1000 | Loss: 0.00001461
Iteration 198/1000 | Loss: 0.00001461
Iteration 199/1000 | Loss: 0.00001461
Iteration 200/1000 | Loss: 0.00001461
Iteration 201/1000 | Loss: 0.00001461
Iteration 202/1000 | Loss: 0.00001461
Iteration 203/1000 | Loss: 0.00001461
Iteration 204/1000 | Loss: 0.00001461
Iteration 205/1000 | Loss: 0.00001461
Iteration 206/1000 | Loss: 0.00001461
Iteration 207/1000 | Loss: 0.00001461
Iteration 208/1000 | Loss: 0.00001461
Iteration 209/1000 | Loss: 0.00001461
Iteration 210/1000 | Loss: 0.00001461
Iteration 211/1000 | Loss: 0.00001461
Iteration 212/1000 | Loss: 0.00001461
Iteration 213/1000 | Loss: 0.00001461
Iteration 214/1000 | Loss: 0.00001461
Iteration 215/1000 | Loss: 0.00001461
Iteration 216/1000 | Loss: 0.00001461
Iteration 217/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.4608480341848917e-05, 1.4608480341848917e-05, 1.4608480341848917e-05, 1.4608480341848917e-05, 1.4608480341848917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4608480341848917e-05

Optimization complete. Final v2v error: 3.324511766433716 mm

Highest mean error: 4.237215995788574 mm for frame 85

Lowest mean error: 2.8228936195373535 mm for frame 0

Saving results

Total time: 85.3090169429779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877917
Iteration 2/25 | Loss: 0.00138479
Iteration 3/25 | Loss: 0.00107823
Iteration 4/25 | Loss: 0.00097665
Iteration 5/25 | Loss: 0.00097661
Iteration 6/25 | Loss: 0.00092867
Iteration 7/25 | Loss: 0.00090309
Iteration 8/25 | Loss: 0.00089319
Iteration 9/25 | Loss: 0.00087340
Iteration 10/25 | Loss: 0.00086088
Iteration 11/25 | Loss: 0.00086113
Iteration 12/25 | Loss: 0.00085977
Iteration 13/25 | Loss: 0.00086122
Iteration 14/25 | Loss: 0.00086103
Iteration 15/25 | Loss: 0.00086140
Iteration 16/25 | Loss: 0.00085768
Iteration 17/25 | Loss: 0.00085576
Iteration 18/25 | Loss: 0.00085471
Iteration 19/25 | Loss: 0.00085453
Iteration 20/25 | Loss: 0.00085436
Iteration 21/25 | Loss: 0.00085432
Iteration 22/25 | Loss: 0.00085431
Iteration 23/25 | Loss: 0.00085431
Iteration 24/25 | Loss: 0.00085431
Iteration 25/25 | Loss: 0.00085430

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.90833664
Iteration 2/25 | Loss: 0.00036986
Iteration 3/25 | Loss: 0.00036957
Iteration 4/25 | Loss: 0.00036956
Iteration 5/25 | Loss: 0.00036956
Iteration 6/25 | Loss: 0.00036956
Iteration 7/25 | Loss: 0.00036956
Iteration 8/25 | Loss: 0.00036956
Iteration 9/25 | Loss: 0.00036956
Iteration 10/25 | Loss: 0.00036956
Iteration 11/25 | Loss: 0.00036956
Iteration 12/25 | Loss: 0.00036956
Iteration 13/25 | Loss: 0.00036956
Iteration 14/25 | Loss: 0.00036956
Iteration 15/25 | Loss: 0.00036956
Iteration 16/25 | Loss: 0.00036956
Iteration 17/25 | Loss: 0.00036956
Iteration 18/25 | Loss: 0.00036956
Iteration 19/25 | Loss: 0.00036956
Iteration 20/25 | Loss: 0.00036956
Iteration 21/25 | Loss: 0.00036956
Iteration 22/25 | Loss: 0.00036956
Iteration 23/25 | Loss: 0.00036956
Iteration 24/25 | Loss: 0.00036956
Iteration 25/25 | Loss: 0.00036956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036956
Iteration 2/1000 | Loss: 0.00003117
Iteration 3/1000 | Loss: 0.00002353
Iteration 4/1000 | Loss: 0.00003276
Iteration 5/1000 | Loss: 0.00002096
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002752
Iteration 8/1000 | Loss: 0.00001992
Iteration 9/1000 | Loss: 0.00001983
Iteration 10/1000 | Loss: 0.00001954
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00003121
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00032683
Iteration 15/1000 | Loss: 0.00143106
Iteration 16/1000 | Loss: 0.00033234
Iteration 17/1000 | Loss: 0.00077224
Iteration 18/1000 | Loss: 0.00051353
Iteration 19/1000 | Loss: 0.00056596
Iteration 20/1000 | Loss: 0.00003996
Iteration 21/1000 | Loss: 0.00040226
Iteration 22/1000 | Loss: 0.00018227
Iteration 23/1000 | Loss: 0.00019675
Iteration 24/1000 | Loss: 0.00031508
Iteration 25/1000 | Loss: 0.00002812
Iteration 26/1000 | Loss: 0.00003489
Iteration 27/1000 | Loss: 0.00001944
Iteration 28/1000 | Loss: 0.00003027
Iteration 29/1000 | Loss: 0.00001707
Iteration 30/1000 | Loss: 0.00001622
Iteration 31/1000 | Loss: 0.00001845
Iteration 32/1000 | Loss: 0.00002638
Iteration 33/1000 | Loss: 0.00002637
Iteration 34/1000 | Loss: 0.00030262
Iteration 35/1000 | Loss: 0.00008640
Iteration 36/1000 | Loss: 0.00007219
Iteration 37/1000 | Loss: 0.00001882
Iteration 38/1000 | Loss: 0.00003481
Iteration 39/1000 | Loss: 0.00002080
Iteration 40/1000 | Loss: 0.00002607
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00003346
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00002390
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001255
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001254
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001251
Iteration 101/1000 | Loss: 0.00001251
Iteration 102/1000 | Loss: 0.00001251
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001250
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001250
Iteration 111/1000 | Loss: 0.00001250
Iteration 112/1000 | Loss: 0.00001250
Iteration 113/1000 | Loss: 0.00001250
Iteration 114/1000 | Loss: 0.00001249
Iteration 115/1000 | Loss: 0.00001249
Iteration 116/1000 | Loss: 0.00001249
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001249
Iteration 121/1000 | Loss: 0.00001249
Iteration 122/1000 | Loss: 0.00001248
Iteration 123/1000 | Loss: 0.00001248
Iteration 124/1000 | Loss: 0.00001248
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001247
Iteration 131/1000 | Loss: 0.00001247
Iteration 132/1000 | Loss: 0.00001247
Iteration 133/1000 | Loss: 0.00001247
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001247
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001247
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Iteration 141/1000 | Loss: 0.00001246
Iteration 142/1000 | Loss: 0.00001246
Iteration 143/1000 | Loss: 0.00001246
Iteration 144/1000 | Loss: 0.00001246
Iteration 145/1000 | Loss: 0.00001246
Iteration 146/1000 | Loss: 0.00001246
Iteration 147/1000 | Loss: 0.00001246
Iteration 148/1000 | Loss: 0.00001246
Iteration 149/1000 | Loss: 0.00001246
Iteration 150/1000 | Loss: 0.00001246
Iteration 151/1000 | Loss: 0.00001246
Iteration 152/1000 | Loss: 0.00001246
Iteration 153/1000 | Loss: 0.00001246
Iteration 154/1000 | Loss: 0.00001246
Iteration 155/1000 | Loss: 0.00001246
Iteration 156/1000 | Loss: 0.00001246
Iteration 157/1000 | Loss: 0.00001246
Iteration 158/1000 | Loss: 0.00001246
Iteration 159/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.2460292055038735e-05, 1.2460292055038735e-05, 1.2460292055038735e-05, 1.2460292055038735e-05, 1.2460292055038735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2460292055038735e-05

Optimization complete. Final v2v error: 2.9634077548980713 mm

Highest mean error: 5.165835380554199 mm for frame 149

Lowest mean error: 2.365255832672119 mm for frame 235

Saving results

Total time: 119.66944026947021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931644
Iteration 2/25 | Loss: 0.00171741
Iteration 3/25 | Loss: 0.00129476
Iteration 4/25 | Loss: 0.00120835
Iteration 5/25 | Loss: 0.00118193
Iteration 6/25 | Loss: 0.00112996
Iteration 7/25 | Loss: 0.00108545
Iteration 8/25 | Loss: 0.00107125
Iteration 9/25 | Loss: 0.00105175
Iteration 10/25 | Loss: 0.00105076
Iteration 11/25 | Loss: 0.00103215
Iteration 12/25 | Loss: 0.00103003
Iteration 13/25 | Loss: 0.00102970
Iteration 14/25 | Loss: 0.00102961
Iteration 15/25 | Loss: 0.00102961
Iteration 16/25 | Loss: 0.00102961
Iteration 17/25 | Loss: 0.00102960
Iteration 18/25 | Loss: 0.00102960
Iteration 19/25 | Loss: 0.00102960
Iteration 20/25 | Loss: 0.00102960
Iteration 21/25 | Loss: 0.00102960
Iteration 22/25 | Loss: 0.00102960
Iteration 23/25 | Loss: 0.00102960
Iteration 24/25 | Loss: 0.00102960
Iteration 25/25 | Loss: 0.00102960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33317351
Iteration 2/25 | Loss: 0.00061342
Iteration 3/25 | Loss: 0.00060975
Iteration 4/25 | Loss: 0.00060975
Iteration 5/25 | Loss: 0.00060975
Iteration 6/25 | Loss: 0.00060975
Iteration 7/25 | Loss: 0.00060975
Iteration 8/25 | Loss: 0.00060975
Iteration 9/25 | Loss: 0.00060975
Iteration 10/25 | Loss: 0.00060975
Iteration 11/25 | Loss: 0.00060974
Iteration 12/25 | Loss: 0.00060974
Iteration 13/25 | Loss: 0.00060974
Iteration 14/25 | Loss: 0.00060974
Iteration 15/25 | Loss: 0.00060974
Iteration 16/25 | Loss: 0.00060974
Iteration 17/25 | Loss: 0.00060974
Iteration 18/25 | Loss: 0.00060974
Iteration 19/25 | Loss: 0.00060974
Iteration 20/25 | Loss: 0.00060974
Iteration 21/25 | Loss: 0.00060974
Iteration 22/25 | Loss: 0.00060974
Iteration 23/25 | Loss: 0.00060974
Iteration 24/25 | Loss: 0.00060974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006097445148043334, 0.0006097445148043334, 0.0006097445148043334, 0.0006097445148043334, 0.0006097445148043334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006097445148043334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060974
Iteration 2/1000 | Loss: 0.00034691
Iteration 3/1000 | Loss: 0.00031160
Iteration 4/1000 | Loss: 0.00017949
Iteration 5/1000 | Loss: 0.00021458
Iteration 6/1000 | Loss: 0.00012916
Iteration 7/1000 | Loss: 0.00009544
Iteration 8/1000 | Loss: 0.00008181
Iteration 9/1000 | Loss: 0.00007283
Iteration 10/1000 | Loss: 0.00008750
Iteration 11/1000 | Loss: 0.00019145
Iteration 12/1000 | Loss: 0.00017532
Iteration 13/1000 | Loss: 0.00008883
Iteration 14/1000 | Loss: 0.00006521
Iteration 15/1000 | Loss: 0.00009243
Iteration 16/1000 | Loss: 0.00005150
Iteration 17/1000 | Loss: 0.00004516
Iteration 18/1000 | Loss: 0.00004141
Iteration 19/1000 | Loss: 0.00003805
Iteration 20/1000 | Loss: 0.00006916
Iteration 21/1000 | Loss: 0.00003992
Iteration 22/1000 | Loss: 0.00003656
Iteration 23/1000 | Loss: 0.00003387
Iteration 24/1000 | Loss: 0.00003261
Iteration 25/1000 | Loss: 0.00003169
Iteration 26/1000 | Loss: 0.00003114
Iteration 27/1000 | Loss: 0.00003045
Iteration 28/1000 | Loss: 0.00002975
Iteration 29/1000 | Loss: 0.00002932
Iteration 30/1000 | Loss: 0.00002886
Iteration 31/1000 | Loss: 0.00002848
Iteration 32/1000 | Loss: 0.00002833
Iteration 33/1000 | Loss: 0.00002830
Iteration 34/1000 | Loss: 0.00002806
Iteration 35/1000 | Loss: 0.00002765
Iteration 36/1000 | Loss: 0.00002717
Iteration 37/1000 | Loss: 0.00002681
Iteration 38/1000 | Loss: 0.00002657
Iteration 39/1000 | Loss: 0.00002642
Iteration 40/1000 | Loss: 0.00002631
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002623
Iteration 43/1000 | Loss: 0.00002621
Iteration 44/1000 | Loss: 0.00002620
Iteration 45/1000 | Loss: 0.00002620
Iteration 46/1000 | Loss: 0.00002619
Iteration 47/1000 | Loss: 0.00002616
Iteration 48/1000 | Loss: 0.00002615
Iteration 49/1000 | Loss: 0.00002615
Iteration 50/1000 | Loss: 0.00002612
Iteration 51/1000 | Loss: 0.00002605
Iteration 52/1000 | Loss: 0.00002605
Iteration 53/1000 | Loss: 0.00002601
Iteration 54/1000 | Loss: 0.00002590
Iteration 55/1000 | Loss: 0.00002576
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002572
Iteration 58/1000 | Loss: 0.00002566
Iteration 59/1000 | Loss: 0.00002564
Iteration 60/1000 | Loss: 0.00002561
Iteration 61/1000 | Loss: 0.00002559
Iteration 62/1000 | Loss: 0.00002559
Iteration 63/1000 | Loss: 0.00002558
Iteration 64/1000 | Loss: 0.00002555
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002550
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002549
Iteration 73/1000 | Loss: 0.00002549
Iteration 74/1000 | Loss: 0.00002549
Iteration 75/1000 | Loss: 0.00002549
Iteration 76/1000 | Loss: 0.00002549
Iteration 77/1000 | Loss: 0.00002549
Iteration 78/1000 | Loss: 0.00002549
Iteration 79/1000 | Loss: 0.00002549
Iteration 80/1000 | Loss: 0.00002549
Iteration 81/1000 | Loss: 0.00002548
Iteration 82/1000 | Loss: 0.00002548
Iteration 83/1000 | Loss: 0.00002548
Iteration 84/1000 | Loss: 0.00002548
Iteration 85/1000 | Loss: 0.00002548
Iteration 86/1000 | Loss: 0.00002548
Iteration 87/1000 | Loss: 0.00002547
Iteration 88/1000 | Loss: 0.00002547
Iteration 89/1000 | Loss: 0.00002547
Iteration 90/1000 | Loss: 0.00002547
Iteration 91/1000 | Loss: 0.00002547
Iteration 92/1000 | Loss: 0.00002546
Iteration 93/1000 | Loss: 0.00002546
Iteration 94/1000 | Loss: 0.00002546
Iteration 95/1000 | Loss: 0.00002546
Iteration 96/1000 | Loss: 0.00002546
Iteration 97/1000 | Loss: 0.00002546
Iteration 98/1000 | Loss: 0.00002546
Iteration 99/1000 | Loss: 0.00002546
Iteration 100/1000 | Loss: 0.00002546
Iteration 101/1000 | Loss: 0.00002546
Iteration 102/1000 | Loss: 0.00002546
Iteration 103/1000 | Loss: 0.00002546
Iteration 104/1000 | Loss: 0.00002545
Iteration 105/1000 | Loss: 0.00002545
Iteration 106/1000 | Loss: 0.00002545
Iteration 107/1000 | Loss: 0.00002545
Iteration 108/1000 | Loss: 0.00002545
Iteration 109/1000 | Loss: 0.00002545
Iteration 110/1000 | Loss: 0.00002545
Iteration 111/1000 | Loss: 0.00002545
Iteration 112/1000 | Loss: 0.00002545
Iteration 113/1000 | Loss: 0.00002545
Iteration 114/1000 | Loss: 0.00002545
Iteration 115/1000 | Loss: 0.00002545
Iteration 116/1000 | Loss: 0.00002545
Iteration 117/1000 | Loss: 0.00002545
Iteration 118/1000 | Loss: 0.00002545
Iteration 119/1000 | Loss: 0.00002545
Iteration 120/1000 | Loss: 0.00002545
Iteration 121/1000 | Loss: 0.00002545
Iteration 122/1000 | Loss: 0.00002545
Iteration 123/1000 | Loss: 0.00002545
Iteration 124/1000 | Loss: 0.00002545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.5450488465139642e-05, 2.5450488465139642e-05, 2.5450488465139642e-05, 2.5450488465139642e-05, 2.5450488465139642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5450488465139642e-05

Optimization complete. Final v2v error: 3.9815282821655273 mm

Highest mean error: 7.879891395568848 mm for frame 105

Lowest mean error: 3.1014139652252197 mm for frame 73

Saving results

Total time: 97.455322265625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458041
Iteration 2/25 | Loss: 0.00097695
Iteration 3/25 | Loss: 0.00088258
Iteration 4/25 | Loss: 0.00087555
Iteration 5/25 | Loss: 0.00087392
Iteration 6/25 | Loss: 0.00087343
Iteration 7/25 | Loss: 0.00087343
Iteration 8/25 | Loss: 0.00087343
Iteration 9/25 | Loss: 0.00087343
Iteration 10/25 | Loss: 0.00087343
Iteration 11/25 | Loss: 0.00087343
Iteration 12/25 | Loss: 0.00087343
Iteration 13/25 | Loss: 0.00087343
Iteration 14/25 | Loss: 0.00087343
Iteration 15/25 | Loss: 0.00087343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008734334842301905, 0.0008734334842301905, 0.0008734334842301905, 0.0008734334842301905, 0.0008734334842301905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008734334842301905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40580618
Iteration 2/25 | Loss: 0.00033286
Iteration 3/25 | Loss: 0.00033284
Iteration 4/25 | Loss: 0.00033284
Iteration 5/25 | Loss: 0.00033284
Iteration 6/25 | Loss: 0.00033284
Iteration 7/25 | Loss: 0.00033284
Iteration 8/25 | Loss: 0.00033284
Iteration 9/25 | Loss: 0.00033284
Iteration 10/25 | Loss: 0.00033284
Iteration 11/25 | Loss: 0.00033284
Iteration 12/25 | Loss: 0.00033284
Iteration 13/25 | Loss: 0.00033284
Iteration 14/25 | Loss: 0.00033284
Iteration 15/25 | Loss: 0.00033284
Iteration 16/25 | Loss: 0.00033284
Iteration 17/25 | Loss: 0.00033284
Iteration 18/25 | Loss: 0.00033284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00033283865195699036, 0.00033283865195699036, 0.00033283865195699036, 0.00033283865195699036, 0.00033283865195699036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033283865195699036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033284
Iteration 2/1000 | Loss: 0.00002517
Iteration 3/1000 | Loss: 0.00001478
Iteration 4/1000 | Loss: 0.00001317
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001131
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001128
Iteration 12/1000 | Loss: 0.00001127
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001123
Iteration 15/1000 | Loss: 0.00001117
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001111
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001108
Iteration 22/1000 | Loss: 0.00001108
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001104
Iteration 25/1000 | Loss: 0.00001103
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001096
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001088
Iteration 59/1000 | Loss: 0.00001088
Iteration 60/1000 | Loss: 0.00001088
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001087
Iteration 65/1000 | Loss: 0.00001087
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001086
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001085
Iteration 74/1000 | Loss: 0.00001085
Iteration 75/1000 | Loss: 0.00001085
Iteration 76/1000 | Loss: 0.00001084
Iteration 77/1000 | Loss: 0.00001084
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001084
Iteration 81/1000 | Loss: 0.00001084
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001083
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001083
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001081
Iteration 103/1000 | Loss: 0.00001081
Iteration 104/1000 | Loss: 0.00001081
Iteration 105/1000 | Loss: 0.00001081
Iteration 106/1000 | Loss: 0.00001081
Iteration 107/1000 | Loss: 0.00001080
Iteration 108/1000 | Loss: 0.00001080
Iteration 109/1000 | Loss: 0.00001080
Iteration 110/1000 | Loss: 0.00001080
Iteration 111/1000 | Loss: 0.00001079
Iteration 112/1000 | Loss: 0.00001079
Iteration 113/1000 | Loss: 0.00001079
Iteration 114/1000 | Loss: 0.00001079
Iteration 115/1000 | Loss: 0.00001079
Iteration 116/1000 | Loss: 0.00001078
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001078
Iteration 119/1000 | Loss: 0.00001078
Iteration 120/1000 | Loss: 0.00001078
Iteration 121/1000 | Loss: 0.00001078
Iteration 122/1000 | Loss: 0.00001078
Iteration 123/1000 | Loss: 0.00001078
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001078
Iteration 126/1000 | Loss: 0.00001078
Iteration 127/1000 | Loss: 0.00001078
Iteration 128/1000 | Loss: 0.00001077
Iteration 129/1000 | Loss: 0.00001077
Iteration 130/1000 | Loss: 0.00001077
Iteration 131/1000 | Loss: 0.00001077
Iteration 132/1000 | Loss: 0.00001077
Iteration 133/1000 | Loss: 0.00001077
Iteration 134/1000 | Loss: 0.00001077
Iteration 135/1000 | Loss: 0.00001077
Iteration 136/1000 | Loss: 0.00001077
Iteration 137/1000 | Loss: 0.00001077
Iteration 138/1000 | Loss: 0.00001077
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001077
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001076
Iteration 144/1000 | Loss: 0.00001076
Iteration 145/1000 | Loss: 0.00001076
Iteration 146/1000 | Loss: 0.00001076
Iteration 147/1000 | Loss: 0.00001076
Iteration 148/1000 | Loss: 0.00001076
Iteration 149/1000 | Loss: 0.00001076
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001076
Iteration 153/1000 | Loss: 0.00001076
Iteration 154/1000 | Loss: 0.00001076
Iteration 155/1000 | Loss: 0.00001076
Iteration 156/1000 | Loss: 0.00001076
Iteration 157/1000 | Loss: 0.00001076
Iteration 158/1000 | Loss: 0.00001076
Iteration 159/1000 | Loss: 0.00001076
Iteration 160/1000 | Loss: 0.00001076
Iteration 161/1000 | Loss: 0.00001075
Iteration 162/1000 | Loss: 0.00001075
Iteration 163/1000 | Loss: 0.00001075
Iteration 164/1000 | Loss: 0.00001075
Iteration 165/1000 | Loss: 0.00001075
Iteration 166/1000 | Loss: 0.00001075
Iteration 167/1000 | Loss: 0.00001075
Iteration 168/1000 | Loss: 0.00001075
Iteration 169/1000 | Loss: 0.00001075
Iteration 170/1000 | Loss: 0.00001075
Iteration 171/1000 | Loss: 0.00001075
Iteration 172/1000 | Loss: 0.00001075
Iteration 173/1000 | Loss: 0.00001075
Iteration 174/1000 | Loss: 0.00001075
Iteration 175/1000 | Loss: 0.00001074
Iteration 176/1000 | Loss: 0.00001074
Iteration 177/1000 | Loss: 0.00001074
Iteration 178/1000 | Loss: 0.00001074
Iteration 179/1000 | Loss: 0.00001074
Iteration 180/1000 | Loss: 0.00001074
Iteration 181/1000 | Loss: 0.00001074
Iteration 182/1000 | Loss: 0.00001074
Iteration 183/1000 | Loss: 0.00001074
Iteration 184/1000 | Loss: 0.00001074
Iteration 185/1000 | Loss: 0.00001074
Iteration 186/1000 | Loss: 0.00001074
Iteration 187/1000 | Loss: 0.00001074
Iteration 188/1000 | Loss: 0.00001074
Iteration 189/1000 | Loss: 0.00001074
Iteration 190/1000 | Loss: 0.00001074
Iteration 191/1000 | Loss: 0.00001074
Iteration 192/1000 | Loss: 0.00001074
Iteration 193/1000 | Loss: 0.00001074
Iteration 194/1000 | Loss: 0.00001074
Iteration 195/1000 | Loss: 0.00001074
Iteration 196/1000 | Loss: 0.00001074
Iteration 197/1000 | Loss: 0.00001074
Iteration 198/1000 | Loss: 0.00001074
Iteration 199/1000 | Loss: 0.00001074
Iteration 200/1000 | Loss: 0.00001074
Iteration 201/1000 | Loss: 0.00001074
Iteration 202/1000 | Loss: 0.00001074
Iteration 203/1000 | Loss: 0.00001074
Iteration 204/1000 | Loss: 0.00001074
Iteration 205/1000 | Loss: 0.00001074
Iteration 206/1000 | Loss: 0.00001074
Iteration 207/1000 | Loss: 0.00001074
Iteration 208/1000 | Loss: 0.00001074
Iteration 209/1000 | Loss: 0.00001074
Iteration 210/1000 | Loss: 0.00001074
Iteration 211/1000 | Loss: 0.00001074
Iteration 212/1000 | Loss: 0.00001074
Iteration 213/1000 | Loss: 0.00001074
Iteration 214/1000 | Loss: 0.00001074
Iteration 215/1000 | Loss: 0.00001074
Iteration 216/1000 | Loss: 0.00001074
Iteration 217/1000 | Loss: 0.00001074
Iteration 218/1000 | Loss: 0.00001074
Iteration 219/1000 | Loss: 0.00001074
Iteration 220/1000 | Loss: 0.00001074
Iteration 221/1000 | Loss: 0.00001074
Iteration 222/1000 | Loss: 0.00001074
Iteration 223/1000 | Loss: 0.00001074
Iteration 224/1000 | Loss: 0.00001074
Iteration 225/1000 | Loss: 0.00001074
Iteration 226/1000 | Loss: 0.00001074
Iteration 227/1000 | Loss: 0.00001074
Iteration 228/1000 | Loss: 0.00001074
Iteration 229/1000 | Loss: 0.00001074
Iteration 230/1000 | Loss: 0.00001074
Iteration 231/1000 | Loss: 0.00001074
Iteration 232/1000 | Loss: 0.00001074
Iteration 233/1000 | Loss: 0.00001074
Iteration 234/1000 | Loss: 0.00001074
Iteration 235/1000 | Loss: 0.00001074
Iteration 236/1000 | Loss: 0.00001074
Iteration 237/1000 | Loss: 0.00001074
Iteration 238/1000 | Loss: 0.00001074
Iteration 239/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.0739273420767859e-05, 1.0739273420767859e-05, 1.0739273420767859e-05, 1.0739273420767859e-05, 1.0739273420767859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0739273420767859e-05

Optimization complete. Final v2v error: 2.762742757797241 mm

Highest mean error: 3.3760461807250977 mm for frame 75

Lowest mean error: 2.288037061691284 mm for frame 0

Saving results

Total time: 34.90946316719055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846750
Iteration 2/25 | Loss: 0.00093629
Iteration 3/25 | Loss: 0.00080917
Iteration 4/25 | Loss: 0.00079633
Iteration 5/25 | Loss: 0.00079351
Iteration 6/25 | Loss: 0.00079280
Iteration 7/25 | Loss: 0.00079280
Iteration 8/25 | Loss: 0.00079280
Iteration 9/25 | Loss: 0.00079280
Iteration 10/25 | Loss: 0.00079280
Iteration 11/25 | Loss: 0.00079280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007928009727038443, 0.0007928009727038443, 0.0007928009727038443, 0.0007928009727038443, 0.0007928009727038443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007928009727038443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40324509
Iteration 2/25 | Loss: 0.00023495
Iteration 3/25 | Loss: 0.00023493
Iteration 4/25 | Loss: 0.00023493
Iteration 5/25 | Loss: 0.00023493
Iteration 6/25 | Loss: 0.00023493
Iteration 7/25 | Loss: 0.00023493
Iteration 8/25 | Loss: 0.00023493
Iteration 9/25 | Loss: 0.00023493
Iteration 10/25 | Loss: 0.00023493
Iteration 11/25 | Loss: 0.00023493
Iteration 12/25 | Loss: 0.00023493
Iteration 13/25 | Loss: 0.00023493
Iteration 14/25 | Loss: 0.00023493
Iteration 15/25 | Loss: 0.00023493
Iteration 16/25 | Loss: 0.00023493
Iteration 17/25 | Loss: 0.00023493
Iteration 18/25 | Loss: 0.00023493
Iteration 19/25 | Loss: 0.00023493
Iteration 20/25 | Loss: 0.00023493
Iteration 21/25 | Loss: 0.00023493
Iteration 22/25 | Loss: 0.00023493
Iteration 23/25 | Loss: 0.00023493
Iteration 24/25 | Loss: 0.00023493
Iteration 25/25 | Loss: 0.00023493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023493
Iteration 2/1000 | Loss: 0.00001900
Iteration 3/1000 | Loss: 0.00001221
Iteration 4/1000 | Loss: 0.00001051
Iteration 5/1000 | Loss: 0.00000978
Iteration 6/1000 | Loss: 0.00000916
Iteration 7/1000 | Loss: 0.00000879
Iteration 8/1000 | Loss: 0.00000860
Iteration 9/1000 | Loss: 0.00000859
Iteration 10/1000 | Loss: 0.00000859
Iteration 11/1000 | Loss: 0.00000859
Iteration 12/1000 | Loss: 0.00000859
Iteration 13/1000 | Loss: 0.00000859
Iteration 14/1000 | Loss: 0.00000859
Iteration 15/1000 | Loss: 0.00000859
Iteration 16/1000 | Loss: 0.00000858
Iteration 17/1000 | Loss: 0.00000858
Iteration 18/1000 | Loss: 0.00000848
Iteration 19/1000 | Loss: 0.00000838
Iteration 20/1000 | Loss: 0.00000835
Iteration 21/1000 | Loss: 0.00000831
Iteration 22/1000 | Loss: 0.00000830
Iteration 23/1000 | Loss: 0.00000828
Iteration 24/1000 | Loss: 0.00000828
Iteration 25/1000 | Loss: 0.00000828
Iteration 26/1000 | Loss: 0.00000827
Iteration 27/1000 | Loss: 0.00000826
Iteration 28/1000 | Loss: 0.00000826
Iteration 29/1000 | Loss: 0.00000825
Iteration 30/1000 | Loss: 0.00000824
Iteration 31/1000 | Loss: 0.00000823
Iteration 32/1000 | Loss: 0.00000823
Iteration 33/1000 | Loss: 0.00000823
Iteration 34/1000 | Loss: 0.00000823
Iteration 35/1000 | Loss: 0.00000823
Iteration 36/1000 | Loss: 0.00000823
Iteration 37/1000 | Loss: 0.00000823
Iteration 38/1000 | Loss: 0.00000823
Iteration 39/1000 | Loss: 0.00000822
Iteration 40/1000 | Loss: 0.00000822
Iteration 41/1000 | Loss: 0.00000822
Iteration 42/1000 | Loss: 0.00000821
Iteration 43/1000 | Loss: 0.00000821
Iteration 44/1000 | Loss: 0.00000821
Iteration 45/1000 | Loss: 0.00000820
Iteration 46/1000 | Loss: 0.00000820
Iteration 47/1000 | Loss: 0.00000820
Iteration 48/1000 | Loss: 0.00000820
Iteration 49/1000 | Loss: 0.00000820
Iteration 50/1000 | Loss: 0.00000820
Iteration 51/1000 | Loss: 0.00000819
Iteration 52/1000 | Loss: 0.00000819
Iteration 53/1000 | Loss: 0.00000819
Iteration 54/1000 | Loss: 0.00000819
Iteration 55/1000 | Loss: 0.00000819
Iteration 56/1000 | Loss: 0.00000819
Iteration 57/1000 | Loss: 0.00000819
Iteration 58/1000 | Loss: 0.00000818
Iteration 59/1000 | Loss: 0.00000818
Iteration 60/1000 | Loss: 0.00000818
Iteration 61/1000 | Loss: 0.00000818
Iteration 62/1000 | Loss: 0.00000818
Iteration 63/1000 | Loss: 0.00000818
Iteration 64/1000 | Loss: 0.00000818
Iteration 65/1000 | Loss: 0.00000818
Iteration 66/1000 | Loss: 0.00000817
Iteration 67/1000 | Loss: 0.00000817
Iteration 68/1000 | Loss: 0.00000817
Iteration 69/1000 | Loss: 0.00000817
Iteration 70/1000 | Loss: 0.00000817
Iteration 71/1000 | Loss: 0.00000817
Iteration 72/1000 | Loss: 0.00000817
Iteration 73/1000 | Loss: 0.00000817
Iteration 74/1000 | Loss: 0.00000817
Iteration 75/1000 | Loss: 0.00000816
Iteration 76/1000 | Loss: 0.00000816
Iteration 77/1000 | Loss: 0.00000816
Iteration 78/1000 | Loss: 0.00000816
Iteration 79/1000 | Loss: 0.00000816
Iteration 80/1000 | Loss: 0.00000816
Iteration 81/1000 | Loss: 0.00000815
Iteration 82/1000 | Loss: 0.00000815
Iteration 83/1000 | Loss: 0.00000815
Iteration 84/1000 | Loss: 0.00000815
Iteration 85/1000 | Loss: 0.00000815
Iteration 86/1000 | Loss: 0.00000815
Iteration 87/1000 | Loss: 0.00000815
Iteration 88/1000 | Loss: 0.00000815
Iteration 89/1000 | Loss: 0.00000815
Iteration 90/1000 | Loss: 0.00000815
Iteration 91/1000 | Loss: 0.00000815
Iteration 92/1000 | Loss: 0.00000815
Iteration 93/1000 | Loss: 0.00000815
Iteration 94/1000 | Loss: 0.00000815
Iteration 95/1000 | Loss: 0.00000814
Iteration 96/1000 | Loss: 0.00000814
Iteration 97/1000 | Loss: 0.00000814
Iteration 98/1000 | Loss: 0.00000814
Iteration 99/1000 | Loss: 0.00000814
Iteration 100/1000 | Loss: 0.00000814
Iteration 101/1000 | Loss: 0.00000814
Iteration 102/1000 | Loss: 0.00000813
Iteration 103/1000 | Loss: 0.00000813
Iteration 104/1000 | Loss: 0.00000813
Iteration 105/1000 | Loss: 0.00000813
Iteration 106/1000 | Loss: 0.00000813
Iteration 107/1000 | Loss: 0.00000813
Iteration 108/1000 | Loss: 0.00000813
Iteration 109/1000 | Loss: 0.00000813
Iteration 110/1000 | Loss: 0.00000813
Iteration 111/1000 | Loss: 0.00000813
Iteration 112/1000 | Loss: 0.00000813
Iteration 113/1000 | Loss: 0.00000813
Iteration 114/1000 | Loss: 0.00000813
Iteration 115/1000 | Loss: 0.00000813
Iteration 116/1000 | Loss: 0.00000813
Iteration 117/1000 | Loss: 0.00000813
Iteration 118/1000 | Loss: 0.00000813
Iteration 119/1000 | Loss: 0.00000813
Iteration 120/1000 | Loss: 0.00000813
Iteration 121/1000 | Loss: 0.00000813
Iteration 122/1000 | Loss: 0.00000813
Iteration 123/1000 | Loss: 0.00000813
Iteration 124/1000 | Loss: 0.00000813
Iteration 125/1000 | Loss: 0.00000813
Iteration 126/1000 | Loss: 0.00000813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [8.125470230879728e-06, 8.125470230879728e-06, 8.125470230879728e-06, 8.125470230879728e-06, 8.125470230879728e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.125470230879728e-06

Optimization complete. Final v2v error: 2.474186897277832 mm

Highest mean error: 2.6499056816101074 mm for frame 122

Lowest mean error: 2.114187240600586 mm for frame 141

Saving results

Total time: 30.80401349067688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896583
Iteration 2/25 | Loss: 0.00203706
Iteration 3/25 | Loss: 0.00111783
Iteration 4/25 | Loss: 0.00108370
Iteration 5/25 | Loss: 0.00107442
Iteration 6/25 | Loss: 0.00107136
Iteration 7/25 | Loss: 0.00107092
Iteration 8/25 | Loss: 0.00107092
Iteration 9/25 | Loss: 0.00107092
Iteration 10/25 | Loss: 0.00107092
Iteration 11/25 | Loss: 0.00107092
Iteration 12/25 | Loss: 0.00107092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001070916885510087, 0.001070916885510087, 0.001070916885510087, 0.001070916885510087, 0.001070916885510087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001070916885510087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92651969
Iteration 2/25 | Loss: 0.00031462
Iteration 3/25 | Loss: 0.00031462
Iteration 4/25 | Loss: 0.00031462
Iteration 5/25 | Loss: 0.00031462
Iteration 6/25 | Loss: 0.00031462
Iteration 7/25 | Loss: 0.00031462
Iteration 8/25 | Loss: 0.00031462
Iteration 9/25 | Loss: 0.00031462
Iteration 10/25 | Loss: 0.00031462
Iteration 11/25 | Loss: 0.00031462
Iteration 12/25 | Loss: 0.00031462
Iteration 13/25 | Loss: 0.00031462
Iteration 14/25 | Loss: 0.00031462
Iteration 15/25 | Loss: 0.00031462
Iteration 16/25 | Loss: 0.00031462
Iteration 17/25 | Loss: 0.00031462
Iteration 18/25 | Loss: 0.00031462
Iteration 19/25 | Loss: 0.00031462
Iteration 20/25 | Loss: 0.00031462
Iteration 21/25 | Loss: 0.00031462
Iteration 22/25 | Loss: 0.00031462
Iteration 23/25 | Loss: 0.00031462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00031461770413443446, 0.00031461770413443446, 0.00031461770413443446, 0.00031461770413443446, 0.00031461770413443446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031461770413443446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031462
Iteration 2/1000 | Loss: 0.00007521
Iteration 3/1000 | Loss: 0.00005166
Iteration 4/1000 | Loss: 0.00004551
Iteration 5/1000 | Loss: 0.00004362
Iteration 6/1000 | Loss: 0.00004240
Iteration 7/1000 | Loss: 0.00004150
Iteration 8/1000 | Loss: 0.00004069
Iteration 9/1000 | Loss: 0.00003976
Iteration 10/1000 | Loss: 0.00003906
Iteration 11/1000 | Loss: 0.00003856
Iteration 12/1000 | Loss: 0.00003808
Iteration 13/1000 | Loss: 0.00003772
Iteration 14/1000 | Loss: 0.00003720
Iteration 15/1000 | Loss: 0.00003689
Iteration 16/1000 | Loss: 0.00003663
Iteration 17/1000 | Loss: 0.00003635
Iteration 18/1000 | Loss: 0.00003607
Iteration 19/1000 | Loss: 0.00003580
Iteration 20/1000 | Loss: 0.00003558
Iteration 21/1000 | Loss: 0.00003539
Iteration 22/1000 | Loss: 0.00003519
Iteration 23/1000 | Loss: 0.00003504
Iteration 24/1000 | Loss: 0.00003503
Iteration 25/1000 | Loss: 0.00003493
Iteration 26/1000 | Loss: 0.00003488
Iteration 27/1000 | Loss: 0.00003484
Iteration 28/1000 | Loss: 0.00003479
Iteration 29/1000 | Loss: 0.00003479
Iteration 30/1000 | Loss: 0.00003473
Iteration 31/1000 | Loss: 0.00003471
Iteration 32/1000 | Loss: 0.00003471
Iteration 33/1000 | Loss: 0.00003471
Iteration 34/1000 | Loss: 0.00003471
Iteration 35/1000 | Loss: 0.00003469
Iteration 36/1000 | Loss: 0.00003469
Iteration 37/1000 | Loss: 0.00003469
Iteration 38/1000 | Loss: 0.00003469
Iteration 39/1000 | Loss: 0.00003469
Iteration 40/1000 | Loss: 0.00003469
Iteration 41/1000 | Loss: 0.00003468
Iteration 42/1000 | Loss: 0.00003468
Iteration 43/1000 | Loss: 0.00003468
Iteration 44/1000 | Loss: 0.00003468
Iteration 45/1000 | Loss: 0.00003468
Iteration 46/1000 | Loss: 0.00003468
Iteration 47/1000 | Loss: 0.00003468
Iteration 48/1000 | Loss: 0.00003466
Iteration 49/1000 | Loss: 0.00003465
Iteration 50/1000 | Loss: 0.00003465
Iteration 51/1000 | Loss: 0.00003463
Iteration 52/1000 | Loss: 0.00003458
Iteration 53/1000 | Loss: 0.00003457
Iteration 54/1000 | Loss: 0.00003456
Iteration 55/1000 | Loss: 0.00003456
Iteration 56/1000 | Loss: 0.00003456
Iteration 57/1000 | Loss: 0.00003456
Iteration 58/1000 | Loss: 0.00003456
Iteration 59/1000 | Loss: 0.00003456
Iteration 60/1000 | Loss: 0.00003456
Iteration 61/1000 | Loss: 0.00003456
Iteration 62/1000 | Loss: 0.00003455
Iteration 63/1000 | Loss: 0.00003454
Iteration 64/1000 | Loss: 0.00003454
Iteration 65/1000 | Loss: 0.00003453
Iteration 66/1000 | Loss: 0.00003453
Iteration 67/1000 | Loss: 0.00003452
Iteration 68/1000 | Loss: 0.00003451
Iteration 69/1000 | Loss: 0.00003451
Iteration 70/1000 | Loss: 0.00003450
Iteration 71/1000 | Loss: 0.00003448
Iteration 72/1000 | Loss: 0.00003448
Iteration 73/1000 | Loss: 0.00003448
Iteration 74/1000 | Loss: 0.00003448
Iteration 75/1000 | Loss: 0.00003448
Iteration 76/1000 | Loss: 0.00003447
Iteration 77/1000 | Loss: 0.00003447
Iteration 78/1000 | Loss: 0.00003447
Iteration 79/1000 | Loss: 0.00003447
Iteration 80/1000 | Loss: 0.00003447
Iteration 81/1000 | Loss: 0.00003447
Iteration 82/1000 | Loss: 0.00003447
Iteration 83/1000 | Loss: 0.00003446
Iteration 84/1000 | Loss: 0.00003446
Iteration 85/1000 | Loss: 0.00003446
Iteration 86/1000 | Loss: 0.00003446
Iteration 87/1000 | Loss: 0.00003446
Iteration 88/1000 | Loss: 0.00003446
Iteration 89/1000 | Loss: 0.00003446
Iteration 90/1000 | Loss: 0.00003446
Iteration 91/1000 | Loss: 0.00003446
Iteration 92/1000 | Loss: 0.00003446
Iteration 93/1000 | Loss: 0.00003446
Iteration 94/1000 | Loss: 0.00003446
Iteration 95/1000 | Loss: 0.00003446
Iteration 96/1000 | Loss: 0.00003446
Iteration 97/1000 | Loss: 0.00003446
Iteration 98/1000 | Loss: 0.00003446
Iteration 99/1000 | Loss: 0.00003446
Iteration 100/1000 | Loss: 0.00003445
Iteration 101/1000 | Loss: 0.00003445
Iteration 102/1000 | Loss: 0.00003445
Iteration 103/1000 | Loss: 0.00003445
Iteration 104/1000 | Loss: 0.00003445
Iteration 105/1000 | Loss: 0.00003444
Iteration 106/1000 | Loss: 0.00003444
Iteration 107/1000 | Loss: 0.00003444
Iteration 108/1000 | Loss: 0.00003444
Iteration 109/1000 | Loss: 0.00003444
Iteration 110/1000 | Loss: 0.00003444
Iteration 111/1000 | Loss: 0.00003444
Iteration 112/1000 | Loss: 0.00003444
Iteration 113/1000 | Loss: 0.00003444
Iteration 114/1000 | Loss: 0.00003444
Iteration 115/1000 | Loss: 0.00003444
Iteration 116/1000 | Loss: 0.00003443
Iteration 117/1000 | Loss: 0.00003443
Iteration 118/1000 | Loss: 0.00003443
Iteration 119/1000 | Loss: 0.00003443
Iteration 120/1000 | Loss: 0.00003443
Iteration 121/1000 | Loss: 0.00003442
Iteration 122/1000 | Loss: 0.00003442
Iteration 123/1000 | Loss: 0.00003442
Iteration 124/1000 | Loss: 0.00003442
Iteration 125/1000 | Loss: 0.00003442
Iteration 126/1000 | Loss: 0.00003442
Iteration 127/1000 | Loss: 0.00003442
Iteration 128/1000 | Loss: 0.00003441
Iteration 129/1000 | Loss: 0.00003441
Iteration 130/1000 | Loss: 0.00003441
Iteration 131/1000 | Loss: 0.00003441
Iteration 132/1000 | Loss: 0.00003441
Iteration 133/1000 | Loss: 0.00003441
Iteration 134/1000 | Loss: 0.00003441
Iteration 135/1000 | Loss: 0.00003441
Iteration 136/1000 | Loss: 0.00003441
Iteration 137/1000 | Loss: 0.00003441
Iteration 138/1000 | Loss: 0.00003441
Iteration 139/1000 | Loss: 0.00003441
Iteration 140/1000 | Loss: 0.00003441
Iteration 141/1000 | Loss: 0.00003441
Iteration 142/1000 | Loss: 0.00003441
Iteration 143/1000 | Loss: 0.00003440
Iteration 144/1000 | Loss: 0.00003440
Iteration 145/1000 | Loss: 0.00003440
Iteration 146/1000 | Loss: 0.00003440
Iteration 147/1000 | Loss: 0.00003440
Iteration 148/1000 | Loss: 0.00003440
Iteration 149/1000 | Loss: 0.00003440
Iteration 150/1000 | Loss: 0.00003440
Iteration 151/1000 | Loss: 0.00003440
Iteration 152/1000 | Loss: 0.00003440
Iteration 153/1000 | Loss: 0.00003440
Iteration 154/1000 | Loss: 0.00003440
Iteration 155/1000 | Loss: 0.00003440
Iteration 156/1000 | Loss: 0.00003440
Iteration 157/1000 | Loss: 0.00003440
Iteration 158/1000 | Loss: 0.00003439
Iteration 159/1000 | Loss: 0.00003439
Iteration 160/1000 | Loss: 0.00003439
Iteration 161/1000 | Loss: 0.00003439
Iteration 162/1000 | Loss: 0.00003439
Iteration 163/1000 | Loss: 0.00003439
Iteration 164/1000 | Loss: 0.00003439
Iteration 165/1000 | Loss: 0.00003439
Iteration 166/1000 | Loss: 0.00003439
Iteration 167/1000 | Loss: 0.00003439
Iteration 168/1000 | Loss: 0.00003439
Iteration 169/1000 | Loss: 0.00003439
Iteration 170/1000 | Loss: 0.00003438
Iteration 171/1000 | Loss: 0.00003438
Iteration 172/1000 | Loss: 0.00003438
Iteration 173/1000 | Loss: 0.00003438
Iteration 174/1000 | Loss: 0.00003438
Iteration 175/1000 | Loss: 0.00003438
Iteration 176/1000 | Loss: 0.00003438
Iteration 177/1000 | Loss: 0.00003438
Iteration 178/1000 | Loss: 0.00003438
Iteration 179/1000 | Loss: 0.00003438
Iteration 180/1000 | Loss: 0.00003437
Iteration 181/1000 | Loss: 0.00003437
Iteration 182/1000 | Loss: 0.00003437
Iteration 183/1000 | Loss: 0.00003437
Iteration 184/1000 | Loss: 0.00003437
Iteration 185/1000 | Loss: 0.00003437
Iteration 186/1000 | Loss: 0.00003437
Iteration 187/1000 | Loss: 0.00003437
Iteration 188/1000 | Loss: 0.00003437
Iteration 189/1000 | Loss: 0.00003437
Iteration 190/1000 | Loss: 0.00003437
Iteration 191/1000 | Loss: 0.00003437
Iteration 192/1000 | Loss: 0.00003437
Iteration 193/1000 | Loss: 0.00003437
Iteration 194/1000 | Loss: 0.00003437
Iteration 195/1000 | Loss: 0.00003437
Iteration 196/1000 | Loss: 0.00003436
Iteration 197/1000 | Loss: 0.00003436
Iteration 198/1000 | Loss: 0.00003436
Iteration 199/1000 | Loss: 0.00003436
Iteration 200/1000 | Loss: 0.00003436
Iteration 201/1000 | Loss: 0.00003436
Iteration 202/1000 | Loss: 0.00003436
Iteration 203/1000 | Loss: 0.00003436
Iteration 204/1000 | Loss: 0.00003436
Iteration 205/1000 | Loss: 0.00003436
Iteration 206/1000 | Loss: 0.00003436
Iteration 207/1000 | Loss: 0.00003436
Iteration 208/1000 | Loss: 0.00003436
Iteration 209/1000 | Loss: 0.00003436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [3.4364016755716875e-05, 3.4364016755716875e-05, 3.4364016755716875e-05, 3.4364016755716875e-05, 3.4364016755716875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4364016755716875e-05

Optimization complete. Final v2v error: 4.746464252471924 mm

Highest mean error: 5.427511692047119 mm for frame 29

Lowest mean error: 4.079381942749023 mm for frame 237

Saving results

Total time: 69.79681420326233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951059
Iteration 2/25 | Loss: 0.00154058
Iteration 3/25 | Loss: 0.00122253
Iteration 4/25 | Loss: 0.00117650
Iteration 5/25 | Loss: 0.00116917
Iteration 6/25 | Loss: 0.00116772
Iteration 7/25 | Loss: 0.00116772
Iteration 8/25 | Loss: 0.00116772
Iteration 9/25 | Loss: 0.00116772
Iteration 10/25 | Loss: 0.00116772
Iteration 11/25 | Loss: 0.00116772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001167722512036562, 0.001167722512036562, 0.001167722512036562, 0.001167722512036562, 0.001167722512036562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001167722512036562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78379321
Iteration 2/25 | Loss: 0.00043139
Iteration 3/25 | Loss: 0.00043138
Iteration 4/25 | Loss: 0.00043138
Iteration 5/25 | Loss: 0.00043138
Iteration 6/25 | Loss: 0.00043138
Iteration 7/25 | Loss: 0.00043138
Iteration 8/25 | Loss: 0.00043138
Iteration 9/25 | Loss: 0.00043138
Iteration 10/25 | Loss: 0.00043138
Iteration 11/25 | Loss: 0.00043138
Iteration 12/25 | Loss: 0.00043138
Iteration 13/25 | Loss: 0.00043138
Iteration 14/25 | Loss: 0.00043138
Iteration 15/25 | Loss: 0.00043138
Iteration 16/25 | Loss: 0.00043138
Iteration 17/25 | Loss: 0.00043138
Iteration 18/25 | Loss: 0.00043138
Iteration 19/25 | Loss: 0.00043138
Iteration 20/25 | Loss: 0.00043138
Iteration 21/25 | Loss: 0.00043138
Iteration 22/25 | Loss: 0.00043138
Iteration 23/25 | Loss: 0.00043138
Iteration 24/25 | Loss: 0.00043138
Iteration 25/25 | Loss: 0.00043138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043138
Iteration 2/1000 | Loss: 0.00006698
Iteration 3/1000 | Loss: 0.00004427
Iteration 4/1000 | Loss: 0.00003931
Iteration 5/1000 | Loss: 0.00003752
Iteration 6/1000 | Loss: 0.00003647
Iteration 7/1000 | Loss: 0.00003586
Iteration 8/1000 | Loss: 0.00003545
Iteration 9/1000 | Loss: 0.00003527
Iteration 10/1000 | Loss: 0.00003503
Iteration 11/1000 | Loss: 0.00003493
Iteration 12/1000 | Loss: 0.00003478
Iteration 13/1000 | Loss: 0.00003476
Iteration 14/1000 | Loss: 0.00003464
Iteration 15/1000 | Loss: 0.00003451
Iteration 16/1000 | Loss: 0.00003443
Iteration 17/1000 | Loss: 0.00003433
Iteration 18/1000 | Loss: 0.00003433
Iteration 19/1000 | Loss: 0.00003433
Iteration 20/1000 | Loss: 0.00003433
Iteration 21/1000 | Loss: 0.00003432
Iteration 22/1000 | Loss: 0.00003432
Iteration 23/1000 | Loss: 0.00003432
Iteration 24/1000 | Loss: 0.00003432
Iteration 25/1000 | Loss: 0.00003432
Iteration 26/1000 | Loss: 0.00003432
Iteration 27/1000 | Loss: 0.00003432
Iteration 28/1000 | Loss: 0.00003432
Iteration 29/1000 | Loss: 0.00003432
Iteration 30/1000 | Loss: 0.00003432
Iteration 31/1000 | Loss: 0.00003431
Iteration 32/1000 | Loss: 0.00003430
Iteration 33/1000 | Loss: 0.00003430
Iteration 34/1000 | Loss: 0.00003429
Iteration 35/1000 | Loss: 0.00003425
Iteration 36/1000 | Loss: 0.00003416
Iteration 37/1000 | Loss: 0.00003411
Iteration 38/1000 | Loss: 0.00003410
Iteration 39/1000 | Loss: 0.00003405
Iteration 40/1000 | Loss: 0.00003404
Iteration 41/1000 | Loss: 0.00003403
Iteration 42/1000 | Loss: 0.00003403
Iteration 43/1000 | Loss: 0.00003403
Iteration 44/1000 | Loss: 0.00003402
Iteration 45/1000 | Loss: 0.00003402
Iteration 46/1000 | Loss: 0.00003402
Iteration 47/1000 | Loss: 0.00003401
Iteration 48/1000 | Loss: 0.00003401
Iteration 49/1000 | Loss: 0.00003400
Iteration 50/1000 | Loss: 0.00003400
Iteration 51/1000 | Loss: 0.00003400
Iteration 52/1000 | Loss: 0.00003400
Iteration 53/1000 | Loss: 0.00003400
Iteration 54/1000 | Loss: 0.00003400
Iteration 55/1000 | Loss: 0.00003400
Iteration 56/1000 | Loss: 0.00003400
Iteration 57/1000 | Loss: 0.00003400
Iteration 58/1000 | Loss: 0.00003400
Iteration 59/1000 | Loss: 0.00003400
Iteration 60/1000 | Loss: 0.00003399
Iteration 61/1000 | Loss: 0.00003399
Iteration 62/1000 | Loss: 0.00003399
Iteration 63/1000 | Loss: 0.00003399
Iteration 64/1000 | Loss: 0.00003399
Iteration 65/1000 | Loss: 0.00003399
Iteration 66/1000 | Loss: 0.00003399
Iteration 67/1000 | Loss: 0.00003399
Iteration 68/1000 | Loss: 0.00003399
Iteration 69/1000 | Loss: 0.00003399
Iteration 70/1000 | Loss: 0.00003399
Iteration 71/1000 | Loss: 0.00003399
Iteration 72/1000 | Loss: 0.00003398
Iteration 73/1000 | Loss: 0.00003398
Iteration 74/1000 | Loss: 0.00003397
Iteration 75/1000 | Loss: 0.00003397
Iteration 76/1000 | Loss: 0.00003397
Iteration 77/1000 | Loss: 0.00003396
Iteration 78/1000 | Loss: 0.00003396
Iteration 79/1000 | Loss: 0.00003396
Iteration 80/1000 | Loss: 0.00003395
Iteration 81/1000 | Loss: 0.00003395
Iteration 82/1000 | Loss: 0.00003394
Iteration 83/1000 | Loss: 0.00003393
Iteration 84/1000 | Loss: 0.00003393
Iteration 85/1000 | Loss: 0.00003393
Iteration 86/1000 | Loss: 0.00003392
Iteration 87/1000 | Loss: 0.00003392
Iteration 88/1000 | Loss: 0.00003392
Iteration 89/1000 | Loss: 0.00003392
Iteration 90/1000 | Loss: 0.00003392
Iteration 91/1000 | Loss: 0.00003390
Iteration 92/1000 | Loss: 0.00003390
Iteration 93/1000 | Loss: 0.00003390
Iteration 94/1000 | Loss: 0.00003390
Iteration 95/1000 | Loss: 0.00003390
Iteration 96/1000 | Loss: 0.00003390
Iteration 97/1000 | Loss: 0.00003390
Iteration 98/1000 | Loss: 0.00003390
Iteration 99/1000 | Loss: 0.00003389
Iteration 100/1000 | Loss: 0.00003389
Iteration 101/1000 | Loss: 0.00003389
Iteration 102/1000 | Loss: 0.00003389
Iteration 103/1000 | Loss: 0.00003389
Iteration 104/1000 | Loss: 0.00003388
Iteration 105/1000 | Loss: 0.00003388
Iteration 106/1000 | Loss: 0.00003388
Iteration 107/1000 | Loss: 0.00003388
Iteration 108/1000 | Loss: 0.00003388
Iteration 109/1000 | Loss: 0.00003388
Iteration 110/1000 | Loss: 0.00003388
Iteration 111/1000 | Loss: 0.00003387
Iteration 112/1000 | Loss: 0.00003387
Iteration 113/1000 | Loss: 0.00003387
Iteration 114/1000 | Loss: 0.00003386
Iteration 115/1000 | Loss: 0.00003386
Iteration 116/1000 | Loss: 0.00003386
Iteration 117/1000 | Loss: 0.00003386
Iteration 118/1000 | Loss: 0.00003386
Iteration 119/1000 | Loss: 0.00003385
Iteration 120/1000 | Loss: 0.00003385
Iteration 121/1000 | Loss: 0.00003384
Iteration 122/1000 | Loss: 0.00003384
Iteration 123/1000 | Loss: 0.00003384
Iteration 124/1000 | Loss: 0.00003384
Iteration 125/1000 | Loss: 0.00003384
Iteration 126/1000 | Loss: 0.00003384
Iteration 127/1000 | Loss: 0.00003384
Iteration 128/1000 | Loss: 0.00003384
Iteration 129/1000 | Loss: 0.00003383
Iteration 130/1000 | Loss: 0.00003383
Iteration 131/1000 | Loss: 0.00003383
Iteration 132/1000 | Loss: 0.00003383
Iteration 133/1000 | Loss: 0.00003383
Iteration 134/1000 | Loss: 0.00003383
Iteration 135/1000 | Loss: 0.00003383
Iteration 136/1000 | Loss: 0.00003383
Iteration 137/1000 | Loss: 0.00003383
Iteration 138/1000 | Loss: 0.00003383
Iteration 139/1000 | Loss: 0.00003383
Iteration 140/1000 | Loss: 0.00003382
Iteration 141/1000 | Loss: 0.00003382
Iteration 142/1000 | Loss: 0.00003382
Iteration 143/1000 | Loss: 0.00003382
Iteration 144/1000 | Loss: 0.00003382
Iteration 145/1000 | Loss: 0.00003382
Iteration 146/1000 | Loss: 0.00003382
Iteration 147/1000 | Loss: 0.00003382
Iteration 148/1000 | Loss: 0.00003382
Iteration 149/1000 | Loss: 0.00003382
Iteration 150/1000 | Loss: 0.00003382
Iteration 151/1000 | Loss: 0.00003382
Iteration 152/1000 | Loss: 0.00003382
Iteration 153/1000 | Loss: 0.00003382
Iteration 154/1000 | Loss: 0.00003382
Iteration 155/1000 | Loss: 0.00003382
Iteration 156/1000 | Loss: 0.00003382
Iteration 157/1000 | Loss: 0.00003382
Iteration 158/1000 | Loss: 0.00003382
Iteration 159/1000 | Loss: 0.00003382
Iteration 160/1000 | Loss: 0.00003381
Iteration 161/1000 | Loss: 0.00003381
Iteration 162/1000 | Loss: 0.00003381
Iteration 163/1000 | Loss: 0.00003381
Iteration 164/1000 | Loss: 0.00003381
Iteration 165/1000 | Loss: 0.00003381
Iteration 166/1000 | Loss: 0.00003381
Iteration 167/1000 | Loss: 0.00003381
Iteration 168/1000 | Loss: 0.00003381
Iteration 169/1000 | Loss: 0.00003381
Iteration 170/1000 | Loss: 0.00003380
Iteration 171/1000 | Loss: 0.00003380
Iteration 172/1000 | Loss: 0.00003380
Iteration 173/1000 | Loss: 0.00003380
Iteration 174/1000 | Loss: 0.00003380
Iteration 175/1000 | Loss: 0.00003380
Iteration 176/1000 | Loss: 0.00003380
Iteration 177/1000 | Loss: 0.00003380
Iteration 178/1000 | Loss: 0.00003380
Iteration 179/1000 | Loss: 0.00003380
Iteration 180/1000 | Loss: 0.00003380
Iteration 181/1000 | Loss: 0.00003380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [3.380037014721893e-05, 3.380037014721893e-05, 3.380037014721893e-05, 3.380037014721893e-05, 3.380037014721893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.380037014721893e-05

Optimization complete. Final v2v error: 4.812791347503662 mm

Highest mean error: 5.100287914276123 mm for frame 34

Lowest mean error: 4.178719520568848 mm for frame 0

Saving results

Total time: 45.796955585479736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00638065
Iteration 2/25 | Loss: 0.00149066
Iteration 3/25 | Loss: 0.00098909
Iteration 4/25 | Loss: 0.00091827
Iteration 5/25 | Loss: 0.00091062
Iteration 6/25 | Loss: 0.00090913
Iteration 7/25 | Loss: 0.00090890
Iteration 8/25 | Loss: 0.00090890
Iteration 9/25 | Loss: 0.00090890
Iteration 10/25 | Loss: 0.00090890
Iteration 11/25 | Loss: 0.00090890
Iteration 12/25 | Loss: 0.00090890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009089049999602139, 0.0009089049999602139, 0.0009089049999602139, 0.0009089049999602139, 0.0009089049999602139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009089049999602139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21631229
Iteration 2/25 | Loss: 0.00021493
Iteration 3/25 | Loss: 0.00021491
Iteration 4/25 | Loss: 0.00021490
Iteration 5/25 | Loss: 0.00021490
Iteration 6/25 | Loss: 0.00021490
Iteration 7/25 | Loss: 0.00021490
Iteration 8/25 | Loss: 0.00021490
Iteration 9/25 | Loss: 0.00021490
Iteration 10/25 | Loss: 0.00021490
Iteration 11/25 | Loss: 0.00021490
Iteration 12/25 | Loss: 0.00021490
Iteration 13/25 | Loss: 0.00021490
Iteration 14/25 | Loss: 0.00021490
Iteration 15/25 | Loss: 0.00021490
Iteration 16/25 | Loss: 0.00021490
Iteration 17/25 | Loss: 0.00021490
Iteration 18/25 | Loss: 0.00021490
Iteration 19/25 | Loss: 0.00021490
Iteration 20/25 | Loss: 0.00021490
Iteration 21/25 | Loss: 0.00021490
Iteration 22/25 | Loss: 0.00021490
Iteration 23/25 | Loss: 0.00021490
Iteration 24/25 | Loss: 0.00021490
Iteration 25/25 | Loss: 0.00021490

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021490
Iteration 2/1000 | Loss: 0.00003696
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001510
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001386
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001355
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001352
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001341
Iteration 59/1000 | Loss: 0.00001341
Iteration 60/1000 | Loss: 0.00001341
Iteration 61/1000 | Loss: 0.00001340
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001339
Iteration 64/1000 | Loss: 0.00001339
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001338
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001336
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001336
Iteration 76/1000 | Loss: 0.00001336
Iteration 77/1000 | Loss: 0.00001336
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001335
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001334
Iteration 90/1000 | Loss: 0.00001334
Iteration 91/1000 | Loss: 0.00001334
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001331
Iteration 109/1000 | Loss: 0.00001331
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001330
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3291021787154023e-05, 1.3291021787154023e-05, 1.3291021787154023e-05, 1.3291021787154023e-05, 1.3291021787154023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3291021787154023e-05

Optimization complete. Final v2v error: 3.0314974784851074 mm

Highest mean error: 3.8233466148376465 mm for frame 51

Lowest mean error: 2.7514641284942627 mm for frame 12

Saving results

Total time: 33.83673667907715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519312
Iteration 2/25 | Loss: 0.00133195
Iteration 3/25 | Loss: 0.00089655
Iteration 4/25 | Loss: 0.00085941
Iteration 5/25 | Loss: 0.00085650
Iteration 6/25 | Loss: 0.00085600
Iteration 7/25 | Loss: 0.00085600
Iteration 8/25 | Loss: 0.00085600
Iteration 9/25 | Loss: 0.00085600
Iteration 10/25 | Loss: 0.00085600
Iteration 11/25 | Loss: 0.00085600
Iteration 12/25 | Loss: 0.00085600
Iteration 13/25 | Loss: 0.00085600
Iteration 14/25 | Loss: 0.00085600
Iteration 15/25 | Loss: 0.00085600
Iteration 16/25 | Loss: 0.00085600
Iteration 17/25 | Loss: 0.00085600
Iteration 18/25 | Loss: 0.00085600
Iteration 19/25 | Loss: 0.00085600
Iteration 20/25 | Loss: 0.00085600
Iteration 21/25 | Loss: 0.00085600
Iteration 22/25 | Loss: 0.00085600
Iteration 23/25 | Loss: 0.00085600
Iteration 24/25 | Loss: 0.00085600
Iteration 25/25 | Loss: 0.00085600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.16088867
Iteration 2/25 | Loss: 0.00017385
Iteration 3/25 | Loss: 0.00017385
Iteration 4/25 | Loss: 0.00017384
Iteration 5/25 | Loss: 0.00017384
Iteration 6/25 | Loss: 0.00017384
Iteration 7/25 | Loss: 0.00017384
Iteration 8/25 | Loss: 0.00017384
Iteration 9/25 | Loss: 0.00017384
Iteration 10/25 | Loss: 0.00017384
Iteration 11/25 | Loss: 0.00017384
Iteration 12/25 | Loss: 0.00017384
Iteration 13/25 | Loss: 0.00017384
Iteration 14/25 | Loss: 0.00017384
Iteration 15/25 | Loss: 0.00017384
Iteration 16/25 | Loss: 0.00017384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0001738432765705511, 0.0001738432765705511, 0.0001738432765705511, 0.0001738432765705511, 0.0001738432765705511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001738432765705511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017384
Iteration 2/1000 | Loss: 0.00002847
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001464
Iteration 9/1000 | Loss: 0.00001460
Iteration 10/1000 | Loss: 0.00001460
Iteration 11/1000 | Loss: 0.00001459
Iteration 12/1000 | Loss: 0.00001459
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001458
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001457
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001454
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001444
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001443
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001443
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001442
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001431
Iteration 138/1000 | Loss: 0.00001431
Iteration 139/1000 | Loss: 0.00001431
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001430
Iteration 143/1000 | Loss: 0.00001430
Iteration 144/1000 | Loss: 0.00001430
Iteration 145/1000 | Loss: 0.00001430
Iteration 146/1000 | Loss: 0.00001430
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4301219380286057e-05, 1.4301219380286057e-05, 1.4301219380286057e-05, 1.4301219380286057e-05, 1.4301219380286057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4301219380286057e-05

Optimization complete. Final v2v error: 3.1045289039611816 mm

Highest mean error: 3.5709164142608643 mm for frame 108

Lowest mean error: 2.771589517593384 mm for frame 77

Saving results

Total time: 29.048839569091797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501127
Iteration 2/25 | Loss: 0.00089609
Iteration 3/25 | Loss: 0.00078398
Iteration 4/25 | Loss: 0.00076328
Iteration 5/25 | Loss: 0.00075768
Iteration 6/25 | Loss: 0.00075598
Iteration 7/25 | Loss: 0.00075591
Iteration 8/25 | Loss: 0.00075591
Iteration 9/25 | Loss: 0.00075591
Iteration 10/25 | Loss: 0.00075591
Iteration 11/25 | Loss: 0.00075591
Iteration 12/25 | Loss: 0.00075591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007559120422229171, 0.0007559120422229171, 0.0007559120422229171, 0.0007559120422229171, 0.0007559120422229171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007559120422229171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54421425
Iteration 2/25 | Loss: 0.00025309
Iteration 3/25 | Loss: 0.00025309
Iteration 4/25 | Loss: 0.00025309
Iteration 5/25 | Loss: 0.00025309
Iteration 6/25 | Loss: 0.00025308
Iteration 7/25 | Loss: 0.00025308
Iteration 8/25 | Loss: 0.00025308
Iteration 9/25 | Loss: 0.00025308
Iteration 10/25 | Loss: 0.00025308
Iteration 11/25 | Loss: 0.00025308
Iteration 12/25 | Loss: 0.00025308
Iteration 13/25 | Loss: 0.00025308
Iteration 14/25 | Loss: 0.00025308
Iteration 15/25 | Loss: 0.00025308
Iteration 16/25 | Loss: 0.00025308
Iteration 17/25 | Loss: 0.00025308
Iteration 18/25 | Loss: 0.00025308
Iteration 19/25 | Loss: 0.00025308
Iteration 20/25 | Loss: 0.00025308
Iteration 21/25 | Loss: 0.00025308
Iteration 22/25 | Loss: 0.00025308
Iteration 23/25 | Loss: 0.00025308
Iteration 24/25 | Loss: 0.00025308
Iteration 25/25 | Loss: 0.00025308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025308
Iteration 2/1000 | Loss: 0.00001715
Iteration 3/1000 | Loss: 0.00000987
Iteration 4/1000 | Loss: 0.00000874
Iteration 5/1000 | Loss: 0.00000796
Iteration 6/1000 | Loss: 0.00000765
Iteration 7/1000 | Loss: 0.00000731
Iteration 8/1000 | Loss: 0.00000724
Iteration 9/1000 | Loss: 0.00000716
Iteration 10/1000 | Loss: 0.00000716
Iteration 11/1000 | Loss: 0.00000715
Iteration 12/1000 | Loss: 0.00000715
Iteration 13/1000 | Loss: 0.00000715
Iteration 14/1000 | Loss: 0.00000714
Iteration 15/1000 | Loss: 0.00000711
Iteration 16/1000 | Loss: 0.00000711
Iteration 17/1000 | Loss: 0.00000711
Iteration 18/1000 | Loss: 0.00000711
Iteration 19/1000 | Loss: 0.00000711
Iteration 20/1000 | Loss: 0.00000708
Iteration 21/1000 | Loss: 0.00000700
Iteration 22/1000 | Loss: 0.00000698
Iteration 23/1000 | Loss: 0.00000697
Iteration 24/1000 | Loss: 0.00000696
Iteration 25/1000 | Loss: 0.00000696
Iteration 26/1000 | Loss: 0.00000695
Iteration 27/1000 | Loss: 0.00000691
Iteration 28/1000 | Loss: 0.00000691
Iteration 29/1000 | Loss: 0.00000691
Iteration 30/1000 | Loss: 0.00000691
Iteration 31/1000 | Loss: 0.00000691
Iteration 32/1000 | Loss: 0.00000691
Iteration 33/1000 | Loss: 0.00000691
Iteration 34/1000 | Loss: 0.00000691
Iteration 35/1000 | Loss: 0.00000691
Iteration 36/1000 | Loss: 0.00000691
Iteration 37/1000 | Loss: 0.00000691
Iteration 38/1000 | Loss: 0.00000691
Iteration 39/1000 | Loss: 0.00000691
Iteration 40/1000 | Loss: 0.00000691
Iteration 41/1000 | Loss: 0.00000691
Iteration 42/1000 | Loss: 0.00000691
Iteration 43/1000 | Loss: 0.00000691
Iteration 44/1000 | Loss: 0.00000691
Iteration 45/1000 | Loss: 0.00000691
Iteration 46/1000 | Loss: 0.00000691
Iteration 47/1000 | Loss: 0.00000691
Iteration 48/1000 | Loss: 0.00000691
Iteration 49/1000 | Loss: 0.00000691
Iteration 50/1000 | Loss: 0.00000691
Iteration 51/1000 | Loss: 0.00000691
Iteration 52/1000 | Loss: 0.00000691
Iteration 53/1000 | Loss: 0.00000691
Iteration 54/1000 | Loss: 0.00000691
Iteration 55/1000 | Loss: 0.00000691
Iteration 56/1000 | Loss: 0.00000691
Iteration 57/1000 | Loss: 0.00000691
Iteration 58/1000 | Loss: 0.00000691
Iteration 59/1000 | Loss: 0.00000691
Iteration 60/1000 | Loss: 0.00000691
Iteration 61/1000 | Loss: 0.00000691
Iteration 62/1000 | Loss: 0.00000691
Iteration 63/1000 | Loss: 0.00000691
Iteration 64/1000 | Loss: 0.00000691
Iteration 65/1000 | Loss: 0.00000691
Iteration 66/1000 | Loss: 0.00000691
Iteration 67/1000 | Loss: 0.00000691
Iteration 68/1000 | Loss: 0.00000691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [6.90572960593272e-06, 6.90572960593272e-06, 6.90572960593272e-06, 6.90572960593272e-06, 6.90572960593272e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.90572960593272e-06

Optimization complete. Final v2v error: 2.2593045234680176 mm

Highest mean error: 2.559098720550537 mm for frame 72

Lowest mean error: 1.9306625127792358 mm for frame 0

Saving results

Total time: 25.072094202041626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383028
Iteration 2/25 | Loss: 0.00091941
Iteration 3/25 | Loss: 0.00084115
Iteration 4/25 | Loss: 0.00083268
Iteration 5/25 | Loss: 0.00082974
Iteration 6/25 | Loss: 0.00082920
Iteration 7/25 | Loss: 0.00082920
Iteration 8/25 | Loss: 0.00082920
Iteration 9/25 | Loss: 0.00082920
Iteration 10/25 | Loss: 0.00082920
Iteration 11/25 | Loss: 0.00082920
Iteration 12/25 | Loss: 0.00082920
Iteration 13/25 | Loss: 0.00082920
Iteration 14/25 | Loss: 0.00082920
Iteration 15/25 | Loss: 0.00082920
Iteration 16/25 | Loss: 0.00082920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008292046841233969, 0.0008292046841233969, 0.0008292046841233969, 0.0008292046841233969, 0.0008292046841233969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008292046841233969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40067685
Iteration 2/25 | Loss: 0.00027909
Iteration 3/25 | Loss: 0.00027909
Iteration 4/25 | Loss: 0.00027909
Iteration 5/25 | Loss: 0.00027909
Iteration 6/25 | Loss: 0.00027909
Iteration 7/25 | Loss: 0.00027909
Iteration 8/25 | Loss: 0.00027909
Iteration 9/25 | Loss: 0.00027909
Iteration 10/25 | Loss: 0.00027909
Iteration 11/25 | Loss: 0.00027909
Iteration 12/25 | Loss: 0.00027909
Iteration 13/25 | Loss: 0.00027909
Iteration 14/25 | Loss: 0.00027909
Iteration 15/25 | Loss: 0.00027909
Iteration 16/25 | Loss: 0.00027909
Iteration 17/25 | Loss: 0.00027909
Iteration 18/25 | Loss: 0.00027909
Iteration 19/25 | Loss: 0.00027909
Iteration 20/25 | Loss: 0.00027909
Iteration 21/25 | Loss: 0.00027909
Iteration 22/25 | Loss: 0.00027909
Iteration 23/25 | Loss: 0.00027909
Iteration 24/25 | Loss: 0.00027909
Iteration 25/25 | Loss: 0.00027909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027909
Iteration 2/1000 | Loss: 0.00002933
Iteration 3/1000 | Loss: 0.00001846
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001238
Iteration 7/1000 | Loss: 0.00001196
Iteration 8/1000 | Loss: 0.00001173
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001110
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001105
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001093
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001092
Iteration 53/1000 | Loss: 0.00001092
Iteration 54/1000 | Loss: 0.00001092
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001091
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001087
Iteration 69/1000 | Loss: 0.00001087
Iteration 70/1000 | Loss: 0.00001087
Iteration 71/1000 | Loss: 0.00001087
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001087
Iteration 74/1000 | Loss: 0.00001087
Iteration 75/1000 | Loss: 0.00001087
Iteration 76/1000 | Loss: 0.00001087
Iteration 77/1000 | Loss: 0.00001087
Iteration 78/1000 | Loss: 0.00001087
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001086
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001086
Iteration 85/1000 | Loss: 0.00001086
Iteration 86/1000 | Loss: 0.00001086
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001086
Iteration 89/1000 | Loss: 0.00001086
Iteration 90/1000 | Loss: 0.00001086
Iteration 91/1000 | Loss: 0.00001086
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001085
Iteration 94/1000 | Loss: 0.00001085
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001085
Iteration 99/1000 | Loss: 0.00001084
Iteration 100/1000 | Loss: 0.00001084
Iteration 101/1000 | Loss: 0.00001084
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001084
Iteration 106/1000 | Loss: 0.00001083
Iteration 107/1000 | Loss: 0.00001083
Iteration 108/1000 | Loss: 0.00001082
Iteration 109/1000 | Loss: 0.00001082
Iteration 110/1000 | Loss: 0.00001082
Iteration 111/1000 | Loss: 0.00001082
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001081
Iteration 114/1000 | Loss: 0.00001081
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001079
Iteration 121/1000 | Loss: 0.00001079
Iteration 122/1000 | Loss: 0.00001079
Iteration 123/1000 | Loss: 0.00001079
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001077
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001076
Iteration 133/1000 | Loss: 0.00001076
Iteration 134/1000 | Loss: 0.00001076
Iteration 135/1000 | Loss: 0.00001076
Iteration 136/1000 | Loss: 0.00001076
Iteration 137/1000 | Loss: 0.00001076
Iteration 138/1000 | Loss: 0.00001076
Iteration 139/1000 | Loss: 0.00001076
Iteration 140/1000 | Loss: 0.00001076
Iteration 141/1000 | Loss: 0.00001076
Iteration 142/1000 | Loss: 0.00001076
Iteration 143/1000 | Loss: 0.00001076
Iteration 144/1000 | Loss: 0.00001076
Iteration 145/1000 | Loss: 0.00001076
Iteration 146/1000 | Loss: 0.00001076
Iteration 147/1000 | Loss: 0.00001076
Iteration 148/1000 | Loss: 0.00001076
Iteration 149/1000 | Loss: 0.00001076
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001076
Iteration 153/1000 | Loss: 0.00001076
Iteration 154/1000 | Loss: 0.00001076
Iteration 155/1000 | Loss: 0.00001076
Iteration 156/1000 | Loss: 0.00001076
Iteration 157/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.0756882147688884e-05, 1.0756882147688884e-05, 1.0756882147688884e-05, 1.0756882147688884e-05, 1.0756882147688884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0756882147688884e-05

Optimization complete. Final v2v error: 2.7909278869628906 mm

Highest mean error: 3.1053385734558105 mm for frame 128

Lowest mean error: 2.6211366653442383 mm for frame 63

Saving results

Total time: 33.948747634887695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104683
Iteration 2/25 | Loss: 0.00168862
Iteration 3/25 | Loss: 0.00128151
Iteration 4/25 | Loss: 0.00125208
Iteration 5/25 | Loss: 0.00107308
Iteration 6/25 | Loss: 0.00101316
Iteration 7/25 | Loss: 0.00098806
Iteration 8/25 | Loss: 0.00099376
Iteration 9/25 | Loss: 0.00094902
Iteration 10/25 | Loss: 0.00093406
Iteration 11/25 | Loss: 0.00093129
Iteration 12/25 | Loss: 0.00093160
Iteration 13/25 | Loss: 0.00092982
Iteration 14/25 | Loss: 0.00093034
Iteration 15/25 | Loss: 0.00092936
Iteration 16/25 | Loss: 0.00093018
Iteration 17/25 | Loss: 0.00092884
Iteration 18/25 | Loss: 0.00092908
Iteration 19/25 | Loss: 0.00092899
Iteration 20/25 | Loss: 0.00092866
Iteration 21/25 | Loss: 0.00092890
Iteration 22/25 | Loss: 0.00092890
Iteration 23/25 | Loss: 0.00092726
Iteration 24/25 | Loss: 0.00093152
Iteration 25/25 | Loss: 0.00092802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41926563
Iteration 2/25 | Loss: 0.00046411
Iteration 3/25 | Loss: 0.00046411
Iteration 4/25 | Loss: 0.00046411
Iteration 5/25 | Loss: 0.00046410
Iteration 6/25 | Loss: 0.00046410
Iteration 7/25 | Loss: 0.00046410
Iteration 8/25 | Loss: 0.00046410
Iteration 9/25 | Loss: 0.00046410
Iteration 10/25 | Loss: 0.00046410
Iteration 11/25 | Loss: 0.00046410
Iteration 12/25 | Loss: 0.00046410
Iteration 13/25 | Loss: 0.00046410
Iteration 14/25 | Loss: 0.00046410
Iteration 15/25 | Loss: 0.00046410
Iteration 16/25 | Loss: 0.00046410
Iteration 17/25 | Loss: 0.00046410
Iteration 18/25 | Loss: 0.00046410
Iteration 19/25 | Loss: 0.00046410
Iteration 20/25 | Loss: 0.00046410
Iteration 21/25 | Loss: 0.00046410
Iteration 22/25 | Loss: 0.00046410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004641036211978644, 0.0004641036211978644, 0.0004641036211978644, 0.0004641036211978644, 0.0004641036211978644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004641036211978644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046410
Iteration 2/1000 | Loss: 0.00008295
Iteration 3/1000 | Loss: 0.00008703
Iteration 4/1000 | Loss: 0.00008495
Iteration 5/1000 | Loss: 0.00009334
Iteration 6/1000 | Loss: 0.00007756
Iteration 7/1000 | Loss: 0.00008128
Iteration 8/1000 | Loss: 0.00009346
Iteration 9/1000 | Loss: 0.00091123
Iteration 10/1000 | Loss: 0.00029386
Iteration 11/1000 | Loss: 0.00013648
Iteration 12/1000 | Loss: 0.00006756
Iteration 13/1000 | Loss: 0.00007612
Iteration 14/1000 | Loss: 0.00008624
Iteration 15/1000 | Loss: 0.00033804
Iteration 16/1000 | Loss: 0.00008811
Iteration 17/1000 | Loss: 0.00006570
Iteration 18/1000 | Loss: 0.00007551
Iteration 19/1000 | Loss: 0.00008433
Iteration 20/1000 | Loss: 0.00009119
Iteration 21/1000 | Loss: 0.00011023
Iteration 22/1000 | Loss: 0.00015781
Iteration 23/1000 | Loss: 0.00012830
Iteration 24/1000 | Loss: 0.00008763
Iteration 25/1000 | Loss: 0.00008894
Iteration 26/1000 | Loss: 0.00007661
Iteration 27/1000 | Loss: 0.00009538
Iteration 28/1000 | Loss: 0.00009088
Iteration 29/1000 | Loss: 0.00008822
Iteration 30/1000 | Loss: 0.00009966
Iteration 31/1000 | Loss: 0.00048697
Iteration 32/1000 | Loss: 0.00107104
Iteration 33/1000 | Loss: 0.00043445
Iteration 34/1000 | Loss: 0.00010290
Iteration 35/1000 | Loss: 0.00003026
Iteration 36/1000 | Loss: 0.00002313
Iteration 37/1000 | Loss: 0.00002133
Iteration 38/1000 | Loss: 0.00001989
Iteration 39/1000 | Loss: 0.00001904
Iteration 40/1000 | Loss: 0.00001860
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001767
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001741
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001723
Iteration 79/1000 | Loss: 0.00001723
Iteration 80/1000 | Loss: 0.00001723
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001722
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001721
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001721
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001721
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001720
Iteration 105/1000 | Loss: 0.00001720
Iteration 106/1000 | Loss: 0.00001720
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001720
Iteration 109/1000 | Loss: 0.00001720
Iteration 110/1000 | Loss: 0.00001720
Iteration 111/1000 | Loss: 0.00001720
Iteration 112/1000 | Loss: 0.00001720
Iteration 113/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.719759893603623e-05, 1.719759893603623e-05, 1.719759893603623e-05, 1.719759893603623e-05, 1.719759893603623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.719759893603623e-05

Optimization complete. Final v2v error: 3.5831642150878906 mm

Highest mean error: 4.81095552444458 mm for frame 66

Lowest mean error: 3.1609315872192383 mm for frame 7

Saving results

Total time: 116.05651545524597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033990
Iteration 2/25 | Loss: 0.00211741
Iteration 3/25 | Loss: 0.00154449
Iteration 4/25 | Loss: 0.00136356
Iteration 5/25 | Loss: 0.00141278
Iteration 6/25 | Loss: 0.00131899
Iteration 7/25 | Loss: 0.00106884
Iteration 8/25 | Loss: 0.00101027
Iteration 9/25 | Loss: 0.00097357
Iteration 10/25 | Loss: 0.00096244
Iteration 11/25 | Loss: 0.00094438
Iteration 12/25 | Loss: 0.00092353
Iteration 13/25 | Loss: 0.00091491
Iteration 14/25 | Loss: 0.00091305
Iteration 15/25 | Loss: 0.00091393
Iteration 16/25 | Loss: 0.00093049
Iteration 17/25 | Loss: 0.00093285
Iteration 18/25 | Loss: 0.00091445
Iteration 19/25 | Loss: 0.00090806
Iteration 20/25 | Loss: 0.00090235
Iteration 21/25 | Loss: 0.00089812
Iteration 22/25 | Loss: 0.00089451
Iteration 23/25 | Loss: 0.00089055
Iteration 24/25 | Loss: 0.00088908
Iteration 25/25 | Loss: 0.00088782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43623638
Iteration 2/25 | Loss: 0.00037206
Iteration 3/25 | Loss: 0.00037206
Iteration 4/25 | Loss: 0.00037206
Iteration 5/25 | Loss: 0.00037206
Iteration 6/25 | Loss: 0.00037206
Iteration 7/25 | Loss: 0.00037206
Iteration 8/25 | Loss: 0.00037206
Iteration 9/25 | Loss: 0.00037206
Iteration 10/25 | Loss: 0.00037206
Iteration 11/25 | Loss: 0.00037206
Iteration 12/25 | Loss: 0.00037206
Iteration 13/25 | Loss: 0.00037206
Iteration 14/25 | Loss: 0.00037206
Iteration 15/25 | Loss: 0.00037206
Iteration 16/25 | Loss: 0.00037206
Iteration 17/25 | Loss: 0.00037206
Iteration 18/25 | Loss: 0.00037206
Iteration 19/25 | Loss: 0.00037206
Iteration 20/25 | Loss: 0.00037206
Iteration 21/25 | Loss: 0.00037206
Iteration 22/25 | Loss: 0.00037206
Iteration 23/25 | Loss: 0.00037206
Iteration 24/25 | Loss: 0.00037206
Iteration 25/25 | Loss: 0.00037206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037206
Iteration 2/1000 | Loss: 0.00232320
Iteration 3/1000 | Loss: 0.00025943
Iteration 4/1000 | Loss: 0.00004592
Iteration 5/1000 | Loss: 0.00083294
Iteration 6/1000 | Loss: 0.00002803
Iteration 7/1000 | Loss: 0.00310516
Iteration 8/1000 | Loss: 0.00323216
Iteration 9/1000 | Loss: 0.00142753
Iteration 10/1000 | Loss: 0.00237702
Iteration 11/1000 | Loss: 0.00091444
Iteration 12/1000 | Loss: 0.00002971
Iteration 13/1000 | Loss: 0.00002572
Iteration 14/1000 | Loss: 0.00002283
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00328144
Iteration 18/1000 | Loss: 0.00251829
Iteration 19/1000 | Loss: 0.00195051
Iteration 20/1000 | Loss: 0.00033722
Iteration 21/1000 | Loss: 0.00011285
Iteration 22/1000 | Loss: 0.00007138
Iteration 23/1000 | Loss: 0.00034721
Iteration 24/1000 | Loss: 0.00012512
Iteration 25/1000 | Loss: 0.00006178
Iteration 26/1000 | Loss: 0.00006109
Iteration 27/1000 | Loss: 0.00002314
Iteration 28/1000 | Loss: 0.00003713
Iteration 29/1000 | Loss: 0.00003180
Iteration 30/1000 | Loss: 0.00005095
Iteration 31/1000 | Loss: 0.00005610
Iteration 32/1000 | Loss: 0.00003215
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00004149
Iteration 35/1000 | Loss: 0.00004872
Iteration 36/1000 | Loss: 0.00004334
Iteration 37/1000 | Loss: 0.00004801
Iteration 38/1000 | Loss: 0.00015703
Iteration 39/1000 | Loss: 0.00015941
Iteration 40/1000 | Loss: 0.00003415
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00052902
Iteration 45/1000 | Loss: 0.00025079
Iteration 46/1000 | Loss: 0.00026514
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001542
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001174
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001171
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Iteration 94/1000 | Loss: 0.00001152
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Iteration 139/1000 | Loss: 0.00001144
Iteration 140/1000 | Loss: 0.00001144
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.143827466876246e-05, 1.143827466876246e-05, 1.143827466876246e-05, 1.143827466876246e-05, 1.143827466876246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.143827466876246e-05

Optimization complete. Final v2v error: 2.847477912902832 mm

Highest mean error: 3.6011338233947754 mm for frame 71

Lowest mean error: 2.2952589988708496 mm for frame 108

Saving results

Total time: 132.04479598999023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808226
Iteration 2/25 | Loss: 0.00136409
Iteration 3/25 | Loss: 0.00101381
Iteration 4/25 | Loss: 0.00097005
Iteration 5/25 | Loss: 0.00095994
Iteration 6/25 | Loss: 0.00095729
Iteration 7/25 | Loss: 0.00095681
Iteration 8/25 | Loss: 0.00095681
Iteration 9/25 | Loss: 0.00095681
Iteration 10/25 | Loss: 0.00095681
Iteration 11/25 | Loss: 0.00095681
Iteration 12/25 | Loss: 0.00095681
Iteration 13/25 | Loss: 0.00095681
Iteration 14/25 | Loss: 0.00095681
Iteration 15/25 | Loss: 0.00095681
Iteration 16/25 | Loss: 0.00095681
Iteration 17/25 | Loss: 0.00095681
Iteration 18/25 | Loss: 0.00095681
Iteration 19/25 | Loss: 0.00095681
Iteration 20/25 | Loss: 0.00095681
Iteration 21/25 | Loss: 0.00095681
Iteration 22/25 | Loss: 0.00095681
Iteration 23/25 | Loss: 0.00095681
Iteration 24/25 | Loss: 0.00095681
Iteration 25/25 | Loss: 0.00095681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42286313
Iteration 2/25 | Loss: 0.00039237
Iteration 3/25 | Loss: 0.00039236
Iteration 4/25 | Loss: 0.00039236
Iteration 5/25 | Loss: 0.00039236
Iteration 6/25 | Loss: 0.00039236
Iteration 7/25 | Loss: 0.00039236
Iteration 8/25 | Loss: 0.00039236
Iteration 9/25 | Loss: 0.00039236
Iteration 10/25 | Loss: 0.00039236
Iteration 11/25 | Loss: 0.00039236
Iteration 12/25 | Loss: 0.00039236
Iteration 13/25 | Loss: 0.00039236
Iteration 14/25 | Loss: 0.00039236
Iteration 15/25 | Loss: 0.00039236
Iteration 16/25 | Loss: 0.00039236
Iteration 17/25 | Loss: 0.00039236
Iteration 18/25 | Loss: 0.00039236
Iteration 19/25 | Loss: 0.00039236
Iteration 20/25 | Loss: 0.00039236
Iteration 21/25 | Loss: 0.00039236
Iteration 22/25 | Loss: 0.00039236
Iteration 23/25 | Loss: 0.00039236
Iteration 24/25 | Loss: 0.00039236
Iteration 25/25 | Loss: 0.00039236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039236
Iteration 2/1000 | Loss: 0.00004673
Iteration 3/1000 | Loss: 0.00002869
Iteration 4/1000 | Loss: 0.00002474
Iteration 5/1000 | Loss: 0.00002296
Iteration 6/1000 | Loss: 0.00002175
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002056
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002004
Iteration 11/1000 | Loss: 0.00001979
Iteration 12/1000 | Loss: 0.00001965
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001953
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001937
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001930
Iteration 22/1000 | Loss: 0.00001929
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001927
Iteration 28/1000 | Loss: 0.00001926
Iteration 29/1000 | Loss: 0.00001926
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001918
Iteration 46/1000 | Loss: 0.00001918
Iteration 47/1000 | Loss: 0.00001918
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001914
Iteration 60/1000 | Loss: 0.00001914
Iteration 61/1000 | Loss: 0.00001913
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001912
Iteration 65/1000 | Loss: 0.00001912
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001911
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001910
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001909
Iteration 85/1000 | Loss: 0.00001909
Iteration 86/1000 | Loss: 0.00001909
Iteration 87/1000 | Loss: 0.00001909
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001908
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001908
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001907
Iteration 110/1000 | Loss: 0.00001907
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001905
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001905
Iteration 123/1000 | Loss: 0.00001905
Iteration 124/1000 | Loss: 0.00001905
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001905
Iteration 128/1000 | Loss: 0.00001905
Iteration 129/1000 | Loss: 0.00001905
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001904
Iteration 135/1000 | Loss: 0.00001904
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001904
Iteration 138/1000 | Loss: 0.00001904
Iteration 139/1000 | Loss: 0.00001904
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001902
Iteration 149/1000 | Loss: 0.00001902
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001901
Iteration 159/1000 | Loss: 0.00001901
Iteration 160/1000 | Loss: 0.00001901
Iteration 161/1000 | Loss: 0.00001901
Iteration 162/1000 | Loss: 0.00001901
Iteration 163/1000 | Loss: 0.00001901
Iteration 164/1000 | Loss: 0.00001900
Iteration 165/1000 | Loss: 0.00001900
Iteration 166/1000 | Loss: 0.00001900
Iteration 167/1000 | Loss: 0.00001900
Iteration 168/1000 | Loss: 0.00001900
Iteration 169/1000 | Loss: 0.00001900
Iteration 170/1000 | Loss: 0.00001900
Iteration 171/1000 | Loss: 0.00001900
Iteration 172/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.900286770251114e-05, 1.900286770251114e-05, 1.900286770251114e-05, 1.900286770251114e-05, 1.900286770251114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.900286770251114e-05

Optimization complete. Final v2v error: 3.643068790435791 mm

Highest mean error: 4.375792980194092 mm for frame 150

Lowest mean error: 2.8894472122192383 mm for frame 199

Saving results

Total time: 47.30766177177429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358312
Iteration 2/25 | Loss: 0.00089666
Iteration 3/25 | Loss: 0.00082556
Iteration 4/25 | Loss: 0.00081879
Iteration 5/25 | Loss: 0.00081650
Iteration 6/25 | Loss: 0.00081575
Iteration 7/25 | Loss: 0.00081573
Iteration 8/25 | Loss: 0.00081573
Iteration 9/25 | Loss: 0.00081573
Iteration 10/25 | Loss: 0.00081573
Iteration 11/25 | Loss: 0.00081573
Iteration 12/25 | Loss: 0.00081573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008157343254424632, 0.0008157343254424632, 0.0008157343254424632, 0.0008157343254424632, 0.0008157343254424632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008157343254424632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79544532
Iteration 2/25 | Loss: 0.00033044
Iteration 3/25 | Loss: 0.00033043
Iteration 4/25 | Loss: 0.00033043
Iteration 5/25 | Loss: 0.00033043
Iteration 6/25 | Loss: 0.00033043
Iteration 7/25 | Loss: 0.00033043
Iteration 8/25 | Loss: 0.00033043
Iteration 9/25 | Loss: 0.00033043
Iteration 10/25 | Loss: 0.00033043
Iteration 11/25 | Loss: 0.00033043
Iteration 12/25 | Loss: 0.00033043
Iteration 13/25 | Loss: 0.00033043
Iteration 14/25 | Loss: 0.00033043
Iteration 15/25 | Loss: 0.00033043
Iteration 16/25 | Loss: 0.00033043
Iteration 17/25 | Loss: 0.00033043
Iteration 18/25 | Loss: 0.00033043
Iteration 19/25 | Loss: 0.00033042
Iteration 20/25 | Loss: 0.00033042
Iteration 21/25 | Loss: 0.00033042
Iteration 22/25 | Loss: 0.00033042
Iteration 23/25 | Loss: 0.00033042
Iteration 24/25 | Loss: 0.00033042
Iteration 25/25 | Loss: 0.00033042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033042
Iteration 2/1000 | Loss: 0.00002428
Iteration 3/1000 | Loss: 0.00001221
Iteration 4/1000 | Loss: 0.00000979
Iteration 5/1000 | Loss: 0.00000895
Iteration 6/1000 | Loss: 0.00000855
Iteration 7/1000 | Loss: 0.00000822
Iteration 8/1000 | Loss: 0.00000806
Iteration 9/1000 | Loss: 0.00000806
Iteration 10/1000 | Loss: 0.00000804
Iteration 11/1000 | Loss: 0.00000803
Iteration 12/1000 | Loss: 0.00000802
Iteration 13/1000 | Loss: 0.00000802
Iteration 14/1000 | Loss: 0.00000801
Iteration 15/1000 | Loss: 0.00000801
Iteration 16/1000 | Loss: 0.00000800
Iteration 17/1000 | Loss: 0.00000800
Iteration 18/1000 | Loss: 0.00000799
Iteration 19/1000 | Loss: 0.00000799
Iteration 20/1000 | Loss: 0.00000798
Iteration 21/1000 | Loss: 0.00000798
Iteration 22/1000 | Loss: 0.00000796
Iteration 23/1000 | Loss: 0.00000792
Iteration 24/1000 | Loss: 0.00000792
Iteration 25/1000 | Loss: 0.00000789
Iteration 26/1000 | Loss: 0.00000785
Iteration 27/1000 | Loss: 0.00000783
Iteration 28/1000 | Loss: 0.00000781
Iteration 29/1000 | Loss: 0.00000780
Iteration 30/1000 | Loss: 0.00000780
Iteration 31/1000 | Loss: 0.00000779
Iteration 32/1000 | Loss: 0.00000778
Iteration 33/1000 | Loss: 0.00000777
Iteration 34/1000 | Loss: 0.00000776
Iteration 35/1000 | Loss: 0.00000776
Iteration 36/1000 | Loss: 0.00000775
Iteration 37/1000 | Loss: 0.00000775
Iteration 38/1000 | Loss: 0.00000775
Iteration 39/1000 | Loss: 0.00000775
Iteration 40/1000 | Loss: 0.00000774
Iteration 41/1000 | Loss: 0.00000774
Iteration 42/1000 | Loss: 0.00000774
Iteration 43/1000 | Loss: 0.00000774
Iteration 44/1000 | Loss: 0.00000774
Iteration 45/1000 | Loss: 0.00000774
Iteration 46/1000 | Loss: 0.00000774
Iteration 47/1000 | Loss: 0.00000774
Iteration 48/1000 | Loss: 0.00000774
Iteration 49/1000 | Loss: 0.00000774
Iteration 50/1000 | Loss: 0.00000773
Iteration 51/1000 | Loss: 0.00000773
Iteration 52/1000 | Loss: 0.00000772
Iteration 53/1000 | Loss: 0.00000772
Iteration 54/1000 | Loss: 0.00000771
Iteration 55/1000 | Loss: 0.00000771
Iteration 56/1000 | Loss: 0.00000771
Iteration 57/1000 | Loss: 0.00000770
Iteration 58/1000 | Loss: 0.00000770
Iteration 59/1000 | Loss: 0.00000770
Iteration 60/1000 | Loss: 0.00000769
Iteration 61/1000 | Loss: 0.00000769
Iteration 62/1000 | Loss: 0.00000769
Iteration 63/1000 | Loss: 0.00000768
Iteration 64/1000 | Loss: 0.00000768
Iteration 65/1000 | Loss: 0.00000768
Iteration 66/1000 | Loss: 0.00000768
Iteration 67/1000 | Loss: 0.00000768
Iteration 68/1000 | Loss: 0.00000767
Iteration 69/1000 | Loss: 0.00000767
Iteration 70/1000 | Loss: 0.00000767
Iteration 71/1000 | Loss: 0.00000767
Iteration 72/1000 | Loss: 0.00000767
Iteration 73/1000 | Loss: 0.00000767
Iteration 74/1000 | Loss: 0.00000766
Iteration 75/1000 | Loss: 0.00000766
Iteration 76/1000 | Loss: 0.00000766
Iteration 77/1000 | Loss: 0.00000765
Iteration 78/1000 | Loss: 0.00000765
Iteration 79/1000 | Loss: 0.00000765
Iteration 80/1000 | Loss: 0.00000765
Iteration 81/1000 | Loss: 0.00000764
Iteration 82/1000 | Loss: 0.00000764
Iteration 83/1000 | Loss: 0.00000764
Iteration 84/1000 | Loss: 0.00000764
Iteration 85/1000 | Loss: 0.00000763
Iteration 86/1000 | Loss: 0.00000763
Iteration 87/1000 | Loss: 0.00000763
Iteration 88/1000 | Loss: 0.00000763
Iteration 89/1000 | Loss: 0.00000763
Iteration 90/1000 | Loss: 0.00000762
Iteration 91/1000 | Loss: 0.00000762
Iteration 92/1000 | Loss: 0.00000762
Iteration 93/1000 | Loss: 0.00000762
Iteration 94/1000 | Loss: 0.00000762
Iteration 95/1000 | Loss: 0.00000761
Iteration 96/1000 | Loss: 0.00000761
Iteration 97/1000 | Loss: 0.00000761
Iteration 98/1000 | Loss: 0.00000761
Iteration 99/1000 | Loss: 0.00000761
Iteration 100/1000 | Loss: 0.00000761
Iteration 101/1000 | Loss: 0.00000760
Iteration 102/1000 | Loss: 0.00000760
Iteration 103/1000 | Loss: 0.00000760
Iteration 104/1000 | Loss: 0.00000760
Iteration 105/1000 | Loss: 0.00000760
Iteration 106/1000 | Loss: 0.00000760
Iteration 107/1000 | Loss: 0.00000760
Iteration 108/1000 | Loss: 0.00000760
Iteration 109/1000 | Loss: 0.00000760
Iteration 110/1000 | Loss: 0.00000760
Iteration 111/1000 | Loss: 0.00000760
Iteration 112/1000 | Loss: 0.00000760
Iteration 113/1000 | Loss: 0.00000759
Iteration 114/1000 | Loss: 0.00000759
Iteration 115/1000 | Loss: 0.00000759
Iteration 116/1000 | Loss: 0.00000759
Iteration 117/1000 | Loss: 0.00000759
Iteration 118/1000 | Loss: 0.00000759
Iteration 119/1000 | Loss: 0.00000759
Iteration 120/1000 | Loss: 0.00000758
Iteration 121/1000 | Loss: 0.00000758
Iteration 122/1000 | Loss: 0.00000758
Iteration 123/1000 | Loss: 0.00000758
Iteration 124/1000 | Loss: 0.00000758
Iteration 125/1000 | Loss: 0.00000758
Iteration 126/1000 | Loss: 0.00000758
Iteration 127/1000 | Loss: 0.00000758
Iteration 128/1000 | Loss: 0.00000758
Iteration 129/1000 | Loss: 0.00000758
Iteration 130/1000 | Loss: 0.00000758
Iteration 131/1000 | Loss: 0.00000757
Iteration 132/1000 | Loss: 0.00000757
Iteration 133/1000 | Loss: 0.00000757
Iteration 134/1000 | Loss: 0.00000757
Iteration 135/1000 | Loss: 0.00000757
Iteration 136/1000 | Loss: 0.00000757
Iteration 137/1000 | Loss: 0.00000757
Iteration 138/1000 | Loss: 0.00000757
Iteration 139/1000 | Loss: 0.00000757
Iteration 140/1000 | Loss: 0.00000757
Iteration 141/1000 | Loss: 0.00000757
Iteration 142/1000 | Loss: 0.00000756
Iteration 143/1000 | Loss: 0.00000756
Iteration 144/1000 | Loss: 0.00000756
Iteration 145/1000 | Loss: 0.00000756
Iteration 146/1000 | Loss: 0.00000756
Iteration 147/1000 | Loss: 0.00000756
Iteration 148/1000 | Loss: 0.00000756
Iteration 149/1000 | Loss: 0.00000756
Iteration 150/1000 | Loss: 0.00000755
Iteration 151/1000 | Loss: 0.00000755
Iteration 152/1000 | Loss: 0.00000755
Iteration 153/1000 | Loss: 0.00000755
Iteration 154/1000 | Loss: 0.00000755
Iteration 155/1000 | Loss: 0.00000755
Iteration 156/1000 | Loss: 0.00000755
Iteration 157/1000 | Loss: 0.00000755
Iteration 158/1000 | Loss: 0.00000755
Iteration 159/1000 | Loss: 0.00000754
Iteration 160/1000 | Loss: 0.00000754
Iteration 161/1000 | Loss: 0.00000754
Iteration 162/1000 | Loss: 0.00000754
Iteration 163/1000 | Loss: 0.00000753
Iteration 164/1000 | Loss: 0.00000753
Iteration 165/1000 | Loss: 0.00000753
Iteration 166/1000 | Loss: 0.00000753
Iteration 167/1000 | Loss: 0.00000753
Iteration 168/1000 | Loss: 0.00000753
Iteration 169/1000 | Loss: 0.00000753
Iteration 170/1000 | Loss: 0.00000753
Iteration 171/1000 | Loss: 0.00000753
Iteration 172/1000 | Loss: 0.00000753
Iteration 173/1000 | Loss: 0.00000752
Iteration 174/1000 | Loss: 0.00000752
Iteration 175/1000 | Loss: 0.00000752
Iteration 176/1000 | Loss: 0.00000752
Iteration 177/1000 | Loss: 0.00000752
Iteration 178/1000 | Loss: 0.00000752
Iteration 179/1000 | Loss: 0.00000752
Iteration 180/1000 | Loss: 0.00000752
Iteration 181/1000 | Loss: 0.00000752
Iteration 182/1000 | Loss: 0.00000752
Iteration 183/1000 | Loss: 0.00000752
Iteration 184/1000 | Loss: 0.00000752
Iteration 185/1000 | Loss: 0.00000752
Iteration 186/1000 | Loss: 0.00000752
Iteration 187/1000 | Loss: 0.00000752
Iteration 188/1000 | Loss: 0.00000751
Iteration 189/1000 | Loss: 0.00000751
Iteration 190/1000 | Loss: 0.00000751
Iteration 191/1000 | Loss: 0.00000751
Iteration 192/1000 | Loss: 0.00000751
Iteration 193/1000 | Loss: 0.00000751
Iteration 194/1000 | Loss: 0.00000751
Iteration 195/1000 | Loss: 0.00000751
Iteration 196/1000 | Loss: 0.00000751
Iteration 197/1000 | Loss: 0.00000751
Iteration 198/1000 | Loss: 0.00000751
Iteration 199/1000 | Loss: 0.00000751
Iteration 200/1000 | Loss: 0.00000751
Iteration 201/1000 | Loss: 0.00000750
Iteration 202/1000 | Loss: 0.00000750
Iteration 203/1000 | Loss: 0.00000750
Iteration 204/1000 | Loss: 0.00000750
Iteration 205/1000 | Loss: 0.00000750
Iteration 206/1000 | Loss: 0.00000750
Iteration 207/1000 | Loss: 0.00000750
Iteration 208/1000 | Loss: 0.00000750
Iteration 209/1000 | Loss: 0.00000750
Iteration 210/1000 | Loss: 0.00000750
Iteration 211/1000 | Loss: 0.00000750
Iteration 212/1000 | Loss: 0.00000750
Iteration 213/1000 | Loss: 0.00000750
Iteration 214/1000 | Loss: 0.00000750
Iteration 215/1000 | Loss: 0.00000750
Iteration 216/1000 | Loss: 0.00000750
Iteration 217/1000 | Loss: 0.00000750
Iteration 218/1000 | Loss: 0.00000750
Iteration 219/1000 | Loss: 0.00000750
Iteration 220/1000 | Loss: 0.00000750
Iteration 221/1000 | Loss: 0.00000750
Iteration 222/1000 | Loss: 0.00000750
Iteration 223/1000 | Loss: 0.00000750
Iteration 224/1000 | Loss: 0.00000749
Iteration 225/1000 | Loss: 0.00000749
Iteration 226/1000 | Loss: 0.00000749
Iteration 227/1000 | Loss: 0.00000749
Iteration 228/1000 | Loss: 0.00000749
Iteration 229/1000 | Loss: 0.00000749
Iteration 230/1000 | Loss: 0.00000749
Iteration 231/1000 | Loss: 0.00000749
Iteration 232/1000 | Loss: 0.00000749
Iteration 233/1000 | Loss: 0.00000749
Iteration 234/1000 | Loss: 0.00000749
Iteration 235/1000 | Loss: 0.00000749
Iteration 236/1000 | Loss: 0.00000749
Iteration 237/1000 | Loss: 0.00000749
Iteration 238/1000 | Loss: 0.00000749
Iteration 239/1000 | Loss: 0.00000749
Iteration 240/1000 | Loss: 0.00000749
Iteration 241/1000 | Loss: 0.00000749
Iteration 242/1000 | Loss: 0.00000749
Iteration 243/1000 | Loss: 0.00000749
Iteration 244/1000 | Loss: 0.00000749
Iteration 245/1000 | Loss: 0.00000749
Iteration 246/1000 | Loss: 0.00000749
Iteration 247/1000 | Loss: 0.00000749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [7.488852133974433e-06, 7.488852133974433e-06, 7.488852133974433e-06, 7.488852133974433e-06, 7.488852133974433e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.488852133974433e-06

Optimization complete. Final v2v error: 2.364244222640991 mm

Highest mean error: 2.893583297729492 mm for frame 77

Lowest mean error: 2.0945541858673096 mm for frame 140

Saving results

Total time: 37.93875527381897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039987
Iteration 2/25 | Loss: 0.00471982
Iteration 3/25 | Loss: 0.00326896
Iteration 4/25 | Loss: 0.00299081
Iteration 5/25 | Loss: 0.00221174
Iteration 6/25 | Loss: 0.00201975
Iteration 7/25 | Loss: 0.00186610
Iteration 8/25 | Loss: 0.00178446
Iteration 9/25 | Loss: 0.00174415
Iteration 10/25 | Loss: 0.00172141
Iteration 11/25 | Loss: 0.00171584
Iteration 12/25 | Loss: 0.00170939
Iteration 13/25 | Loss: 0.00169869
Iteration 14/25 | Loss: 0.00169524
Iteration 15/25 | Loss: 0.00169430
Iteration 16/25 | Loss: 0.00169326
Iteration 17/25 | Loss: 0.00169415
Iteration 18/25 | Loss: 0.00168965
Iteration 19/25 | Loss: 0.00168804
Iteration 20/25 | Loss: 0.00168754
Iteration 21/25 | Loss: 0.00168743
Iteration 22/25 | Loss: 0.00168743
Iteration 23/25 | Loss: 0.00168743
Iteration 24/25 | Loss: 0.00168743
Iteration 25/25 | Loss: 0.00168742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32213867
Iteration 2/25 | Loss: 0.00680594
Iteration 3/25 | Loss: 0.00664583
Iteration 4/25 | Loss: 0.00664583
Iteration 5/25 | Loss: 0.00664583
Iteration 6/25 | Loss: 0.00664583
Iteration 7/25 | Loss: 0.00664583
Iteration 8/25 | Loss: 0.00664583
Iteration 9/25 | Loss: 0.00664583
Iteration 10/25 | Loss: 0.00664583
Iteration 11/25 | Loss: 0.00664583
Iteration 12/25 | Loss: 0.00664583
Iteration 13/25 | Loss: 0.00664583
Iteration 14/25 | Loss: 0.00664583
Iteration 15/25 | Loss: 0.00664583
Iteration 16/25 | Loss: 0.00664583
Iteration 17/25 | Loss: 0.00664583
Iteration 18/25 | Loss: 0.00664583
Iteration 19/25 | Loss: 0.00664583
Iteration 20/25 | Loss: 0.00664583
Iteration 21/25 | Loss: 0.00664583
Iteration 22/25 | Loss: 0.00664583
Iteration 23/25 | Loss: 0.00664583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.006645828951150179, 0.006645828951150179, 0.006645828951150179, 0.006645828951150179, 0.006645828951150179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006645828951150179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00664583
Iteration 2/1000 | Loss: 0.00195410
Iteration 3/1000 | Loss: 0.00079439
Iteration 4/1000 | Loss: 0.00065548
Iteration 5/1000 | Loss: 0.00057167
Iteration 6/1000 | Loss: 0.00051704
Iteration 7/1000 | Loss: 0.00046452
Iteration 8/1000 | Loss: 0.00043023
Iteration 9/1000 | Loss: 0.00054086
Iteration 10/1000 | Loss: 0.00043029
Iteration 11/1000 | Loss: 0.00049109
Iteration 12/1000 | Loss: 0.00466783
Iteration 13/1000 | Loss: 0.02887577
Iteration 14/1000 | Loss: 0.00168107
Iteration 15/1000 | Loss: 0.00057610
Iteration 16/1000 | Loss: 0.00044549
Iteration 17/1000 | Loss: 0.00138619
Iteration 18/1000 | Loss: 0.00110787
Iteration 19/1000 | Loss: 0.00034112
Iteration 20/1000 | Loss: 0.00032228
Iteration 21/1000 | Loss: 0.00016341
Iteration 22/1000 | Loss: 0.00035757
Iteration 23/1000 | Loss: 0.00031424
Iteration 24/1000 | Loss: 0.00012134
Iteration 25/1000 | Loss: 0.00006324
Iteration 26/1000 | Loss: 0.00040679
Iteration 27/1000 | Loss: 0.00005052
Iteration 28/1000 | Loss: 0.00004229
Iteration 29/1000 | Loss: 0.00014840
Iteration 30/1000 | Loss: 0.00003470
Iteration 31/1000 | Loss: 0.00003132
Iteration 32/1000 | Loss: 0.00002879
Iteration 33/1000 | Loss: 0.00002693
Iteration 34/1000 | Loss: 0.00002522
Iteration 35/1000 | Loss: 0.00002377
Iteration 36/1000 | Loss: 0.00002302
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00002193
Iteration 39/1000 | Loss: 0.00002157
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002108
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002096
Iteration 44/1000 | Loss: 0.00002078
Iteration 45/1000 | Loss: 0.00002062
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00002018
Iteration 50/1000 | Loss: 0.00002004
Iteration 51/1000 | Loss: 0.00002000
Iteration 52/1000 | Loss: 0.00002000
Iteration 53/1000 | Loss: 0.00001998
Iteration 54/1000 | Loss: 0.00001997
Iteration 55/1000 | Loss: 0.00001997
Iteration 56/1000 | Loss: 0.00001991
Iteration 57/1000 | Loss: 0.00001991
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001990
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00001988
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001986
Iteration 66/1000 | Loss: 0.00001986
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001985
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001979
Iteration 81/1000 | Loss: 0.00001979
Iteration 82/1000 | Loss: 0.00001978
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001978
Iteration 89/1000 | Loss: 0.00001978
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001977
Iteration 93/1000 | Loss: 0.00001977
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00001977
Iteration 96/1000 | Loss: 0.00001977
Iteration 97/1000 | Loss: 0.00001976
Iteration 98/1000 | Loss: 0.00001976
Iteration 99/1000 | Loss: 0.00001976
Iteration 100/1000 | Loss: 0.00001975
Iteration 101/1000 | Loss: 0.00001975
Iteration 102/1000 | Loss: 0.00001975
Iteration 103/1000 | Loss: 0.00001975
Iteration 104/1000 | Loss: 0.00001975
Iteration 105/1000 | Loss: 0.00001975
Iteration 106/1000 | Loss: 0.00001975
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001974
Iteration 109/1000 | Loss: 0.00001974
Iteration 110/1000 | Loss: 0.00001974
Iteration 111/1000 | Loss: 0.00001974
Iteration 112/1000 | Loss: 0.00001974
Iteration 113/1000 | Loss: 0.00001974
Iteration 114/1000 | Loss: 0.00001974
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001973
Iteration 118/1000 | Loss: 0.00001973
Iteration 119/1000 | Loss: 0.00001973
Iteration 120/1000 | Loss: 0.00001973
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00001973
Iteration 123/1000 | Loss: 0.00001973
Iteration 124/1000 | Loss: 0.00001973
Iteration 125/1000 | Loss: 0.00001973
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001972
Iteration 131/1000 | Loss: 0.00001972
Iteration 132/1000 | Loss: 0.00001972
Iteration 133/1000 | Loss: 0.00001972
Iteration 134/1000 | Loss: 0.00001972
Iteration 135/1000 | Loss: 0.00001972
Iteration 136/1000 | Loss: 0.00001972
Iteration 137/1000 | Loss: 0.00001972
Iteration 138/1000 | Loss: 0.00001972
Iteration 139/1000 | Loss: 0.00001972
Iteration 140/1000 | Loss: 0.00001972
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001972
Iteration 149/1000 | Loss: 0.00001972
Iteration 150/1000 | Loss: 0.00001972
Iteration 151/1000 | Loss: 0.00001972
Iteration 152/1000 | Loss: 0.00001972
Iteration 153/1000 | Loss: 0.00001972
Iteration 154/1000 | Loss: 0.00001972
Iteration 155/1000 | Loss: 0.00001972
Iteration 156/1000 | Loss: 0.00001972
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00001972
Iteration 159/1000 | Loss: 0.00001972
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001972
Iteration 162/1000 | Loss: 0.00001972
Iteration 163/1000 | Loss: 0.00001972
Iteration 164/1000 | Loss: 0.00001972
Iteration 165/1000 | Loss: 0.00001972
Iteration 166/1000 | Loss: 0.00001972
Iteration 167/1000 | Loss: 0.00001972
Iteration 168/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.971665733435657e-05, 1.971665733435657e-05, 1.971665733435657e-05, 1.971665733435657e-05, 1.971665733435657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.971665733435657e-05

Optimization complete. Final v2v error: 3.4536280632019043 mm

Highest mean error: 12.023844718933105 mm for frame 167

Lowest mean error: 2.9683597087860107 mm for frame 35

Saving results

Total time: 128.4446370601654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00653372
Iteration 2/25 | Loss: 0.00103354
Iteration 3/25 | Loss: 0.00089400
Iteration 4/25 | Loss: 0.00088319
Iteration 5/25 | Loss: 0.00088127
Iteration 6/25 | Loss: 0.00088118
Iteration 7/25 | Loss: 0.00088118
Iteration 8/25 | Loss: 0.00088118
Iteration 9/25 | Loss: 0.00088118
Iteration 10/25 | Loss: 0.00088118
Iteration 11/25 | Loss: 0.00088118
Iteration 12/25 | Loss: 0.00088118
Iteration 13/25 | Loss: 0.00088118
Iteration 14/25 | Loss: 0.00088118
Iteration 15/25 | Loss: 0.00088118
Iteration 16/25 | Loss: 0.00088118
Iteration 17/25 | Loss: 0.00088118
Iteration 18/25 | Loss: 0.00088118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008811815059743822, 0.0008811815059743822, 0.0008811815059743822, 0.0008811815059743822, 0.0008811815059743822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008811815059743822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12007761
Iteration 2/25 | Loss: 0.00031765
Iteration 3/25 | Loss: 0.00031759
Iteration 4/25 | Loss: 0.00031759
Iteration 5/25 | Loss: 0.00031759
Iteration 6/25 | Loss: 0.00031759
Iteration 7/25 | Loss: 0.00031759
Iteration 8/25 | Loss: 0.00031759
Iteration 9/25 | Loss: 0.00031759
Iteration 10/25 | Loss: 0.00031759
Iteration 11/25 | Loss: 0.00031759
Iteration 12/25 | Loss: 0.00031759
Iteration 13/25 | Loss: 0.00031759
Iteration 14/25 | Loss: 0.00031759
Iteration 15/25 | Loss: 0.00031759
Iteration 16/25 | Loss: 0.00031759
Iteration 17/25 | Loss: 0.00031759
Iteration 18/25 | Loss: 0.00031759
Iteration 19/25 | Loss: 0.00031759
Iteration 20/25 | Loss: 0.00031759
Iteration 21/25 | Loss: 0.00031759
Iteration 22/25 | Loss: 0.00031759
Iteration 23/25 | Loss: 0.00031759
Iteration 24/25 | Loss: 0.00031759
Iteration 25/25 | Loss: 0.00031759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031759
Iteration 2/1000 | Loss: 0.00002263
Iteration 3/1000 | Loss: 0.00001548
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001276
Iteration 9/1000 | Loss: 0.00001260
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001247
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001221
Iteration 62/1000 | Loss: 0.00001221
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001217
Iteration 78/1000 | Loss: 0.00001217
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001216
Iteration 82/1000 | Loss: 0.00001216
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001216
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001212
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001204
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001204
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001204
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001203
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Iteration 151/1000 | Loss: 0.00001203
Iteration 152/1000 | Loss: 0.00001203
Iteration 153/1000 | Loss: 0.00001203
Iteration 154/1000 | Loss: 0.00001203
Iteration 155/1000 | Loss: 0.00001203
Iteration 156/1000 | Loss: 0.00001203
Iteration 157/1000 | Loss: 0.00001203
Iteration 158/1000 | Loss: 0.00001203
Iteration 159/1000 | Loss: 0.00001203
Iteration 160/1000 | Loss: 0.00001203
Iteration 161/1000 | Loss: 0.00001203
Iteration 162/1000 | Loss: 0.00001202
Iteration 163/1000 | Loss: 0.00001202
Iteration 164/1000 | Loss: 0.00001202
Iteration 165/1000 | Loss: 0.00001202
Iteration 166/1000 | Loss: 0.00001202
Iteration 167/1000 | Loss: 0.00001202
Iteration 168/1000 | Loss: 0.00001202
Iteration 169/1000 | Loss: 0.00001202
Iteration 170/1000 | Loss: 0.00001202
Iteration 171/1000 | Loss: 0.00001202
Iteration 172/1000 | Loss: 0.00001202
Iteration 173/1000 | Loss: 0.00001202
Iteration 174/1000 | Loss: 0.00001202
Iteration 175/1000 | Loss: 0.00001202
Iteration 176/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.2017504559480585e-05, 1.2017504559480585e-05, 1.2017504559480585e-05, 1.2017504559480585e-05, 1.2017504559480585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2017504559480585e-05

Optimization complete. Final v2v error: 2.940775156021118 mm

Highest mean error: 3.3263416290283203 mm for frame 122

Lowest mean error: 2.6650776863098145 mm for frame 19

Saving results

Total time: 36.02130627632141
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039729
Iteration 2/25 | Loss: 0.00217122
Iteration 3/25 | Loss: 0.00149862
Iteration 4/25 | Loss: 0.00105135
Iteration 5/25 | Loss: 0.00093499
Iteration 6/25 | Loss: 0.00092917
Iteration 7/25 | Loss: 0.00092555
Iteration 8/25 | Loss: 0.00092422
Iteration 9/25 | Loss: 0.00092378
Iteration 10/25 | Loss: 0.00092363
Iteration 11/25 | Loss: 0.00092350
Iteration 12/25 | Loss: 0.00092341
Iteration 13/25 | Loss: 0.00092405
Iteration 14/25 | Loss: 0.00092288
Iteration 15/25 | Loss: 0.00092153
Iteration 16/25 | Loss: 0.00092071
Iteration 17/25 | Loss: 0.00092046
Iteration 18/25 | Loss: 0.00092288
Iteration 19/25 | Loss: 0.00092082
Iteration 20/25 | Loss: 0.00091950
Iteration 21/25 | Loss: 0.00092318
Iteration 22/25 | Loss: 0.00092085
Iteration 23/25 | Loss: 0.00092274
Iteration 24/25 | Loss: 0.00092140
Iteration 25/25 | Loss: 0.00091817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52909374
Iteration 2/25 | Loss: 0.00044303
Iteration 3/25 | Loss: 0.00044303
Iteration 4/25 | Loss: 0.00044303
Iteration 5/25 | Loss: 0.00044303
Iteration 6/25 | Loss: 0.00044303
Iteration 7/25 | Loss: 0.00044303
Iteration 8/25 | Loss: 0.00044303
Iteration 9/25 | Loss: 0.00044303
Iteration 10/25 | Loss: 0.00044303
Iteration 11/25 | Loss: 0.00044303
Iteration 12/25 | Loss: 0.00044303
Iteration 13/25 | Loss: 0.00044303
Iteration 14/25 | Loss: 0.00044303
Iteration 15/25 | Loss: 0.00044303
Iteration 16/25 | Loss: 0.00044303
Iteration 17/25 | Loss: 0.00044303
Iteration 18/25 | Loss: 0.00044303
Iteration 19/25 | Loss: 0.00044303
Iteration 20/25 | Loss: 0.00044303
Iteration 21/25 | Loss: 0.00044303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004430256667546928, 0.0004430256667546928, 0.0004430256667546928, 0.0004430256667546928, 0.0004430256667546928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004430256667546928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044303
Iteration 2/1000 | Loss: 0.00017898
Iteration 3/1000 | Loss: 0.00003878
Iteration 4/1000 | Loss: 0.00007428
Iteration 5/1000 | Loss: 0.00016282
Iteration 6/1000 | Loss: 0.00002383
Iteration 7/1000 | Loss: 0.00016999
Iteration 8/1000 | Loss: 0.00013741
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00001890
Iteration 11/1000 | Loss: 0.00014375
Iteration 12/1000 | Loss: 0.00006415
Iteration 13/1000 | Loss: 0.00002114
Iteration 14/1000 | Loss: 0.00001942
Iteration 15/1000 | Loss: 0.00001832
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001584
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001548
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001544
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001543
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001541
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001540
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001538
Iteration 44/1000 | Loss: 0.00001538
Iteration 45/1000 | Loss: 0.00001538
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001537
Iteration 49/1000 | Loss: 0.00001537
Iteration 50/1000 | Loss: 0.00001537
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001535
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001535
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001535
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001530
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001524
Iteration 99/1000 | Loss: 0.00001524
Iteration 100/1000 | Loss: 0.00001524
Iteration 101/1000 | Loss: 0.00001524
Iteration 102/1000 | Loss: 0.00001524
Iteration 103/1000 | Loss: 0.00001524
Iteration 104/1000 | Loss: 0.00001523
Iteration 105/1000 | Loss: 0.00001523
Iteration 106/1000 | Loss: 0.00001523
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001522
Iteration 111/1000 | Loss: 0.00001522
Iteration 112/1000 | Loss: 0.00001522
Iteration 113/1000 | Loss: 0.00001522
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001521
Iteration 117/1000 | Loss: 0.00001521
Iteration 118/1000 | Loss: 0.00001521
Iteration 119/1000 | Loss: 0.00001521
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001520
Iteration 122/1000 | Loss: 0.00001520
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001520
Iteration 126/1000 | Loss: 0.00001520
Iteration 127/1000 | Loss: 0.00001520
Iteration 128/1000 | Loss: 0.00001520
Iteration 129/1000 | Loss: 0.00001520
Iteration 130/1000 | Loss: 0.00001520
Iteration 131/1000 | Loss: 0.00001520
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001519
Iteration 138/1000 | Loss: 0.00001519
Iteration 139/1000 | Loss: 0.00001519
Iteration 140/1000 | Loss: 0.00001519
Iteration 141/1000 | Loss: 0.00001519
Iteration 142/1000 | Loss: 0.00001519
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001518
Iteration 147/1000 | Loss: 0.00001518
Iteration 148/1000 | Loss: 0.00001518
Iteration 149/1000 | Loss: 0.00001518
Iteration 150/1000 | Loss: 0.00001518
Iteration 151/1000 | Loss: 0.00001518
Iteration 152/1000 | Loss: 0.00001518
Iteration 153/1000 | Loss: 0.00001518
Iteration 154/1000 | Loss: 0.00001518
Iteration 155/1000 | Loss: 0.00001518
Iteration 156/1000 | Loss: 0.00001518
Iteration 157/1000 | Loss: 0.00001518
Iteration 158/1000 | Loss: 0.00001518
Iteration 159/1000 | Loss: 0.00001518
Iteration 160/1000 | Loss: 0.00001518
Iteration 161/1000 | Loss: 0.00001518
Iteration 162/1000 | Loss: 0.00001518
Iteration 163/1000 | Loss: 0.00001518
Iteration 164/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.518182489235187e-05, 1.518182489235187e-05, 1.518182489235187e-05, 1.518182489235187e-05, 1.518182489235187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.518182489235187e-05

Optimization complete. Final v2v error: 3.362673759460449 mm

Highest mean error: 4.371413230895996 mm for frame 185

Lowest mean error: 2.7482993602752686 mm for frame 208

Saving results

Total time: 86.8065881729126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894471
Iteration 2/25 | Loss: 0.00195537
Iteration 3/25 | Loss: 0.00113328
Iteration 4/25 | Loss: 0.00101949
Iteration 5/25 | Loss: 0.00096369
Iteration 6/25 | Loss: 0.00098127
Iteration 7/25 | Loss: 0.00095189
Iteration 8/25 | Loss: 0.00095350
Iteration 9/25 | Loss: 0.00091571
Iteration 10/25 | Loss: 0.00090060
Iteration 11/25 | Loss: 0.00089521
Iteration 12/25 | Loss: 0.00089306
Iteration 13/25 | Loss: 0.00089265
Iteration 14/25 | Loss: 0.00089263
Iteration 15/25 | Loss: 0.00089263
Iteration 16/25 | Loss: 0.00089263
Iteration 17/25 | Loss: 0.00089263
Iteration 18/25 | Loss: 0.00089263
Iteration 19/25 | Loss: 0.00089263
Iteration 20/25 | Loss: 0.00089263
Iteration 21/25 | Loss: 0.00089263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000892627751454711, 0.000892627751454711, 0.000892627751454711, 0.000892627751454711, 0.000892627751454711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000892627751454711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26699114
Iteration 2/25 | Loss: 0.00032710
Iteration 3/25 | Loss: 0.00032710
Iteration 4/25 | Loss: 0.00032710
Iteration 5/25 | Loss: 0.00032710
Iteration 6/25 | Loss: 0.00032710
Iteration 7/25 | Loss: 0.00032710
Iteration 8/25 | Loss: 0.00032710
Iteration 9/25 | Loss: 0.00032709
Iteration 10/25 | Loss: 0.00032709
Iteration 11/25 | Loss: 0.00032709
Iteration 12/25 | Loss: 0.00032709
Iteration 13/25 | Loss: 0.00032709
Iteration 14/25 | Loss: 0.00032709
Iteration 15/25 | Loss: 0.00032709
Iteration 16/25 | Loss: 0.00032709
Iteration 17/25 | Loss: 0.00032709
Iteration 18/25 | Loss: 0.00032709
Iteration 19/25 | Loss: 0.00032709
Iteration 20/25 | Loss: 0.00032709
Iteration 21/25 | Loss: 0.00032709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00032709442893974483, 0.00032709442893974483, 0.00032709442893974483, 0.00032709442893974483, 0.00032709442893974483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032709442893974483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032709
Iteration 2/1000 | Loss: 0.00003256
Iteration 3/1000 | Loss: 0.00002255
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001975
Iteration 7/1000 | Loss: 0.00001939
Iteration 8/1000 | Loss: 0.00001909
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001858
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001849
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001847
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001843
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001839
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001832
Iteration 37/1000 | Loss: 0.00001832
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001832
Iteration 40/1000 | Loss: 0.00001832
Iteration 41/1000 | Loss: 0.00001832
Iteration 42/1000 | Loss: 0.00001831
Iteration 43/1000 | Loss: 0.00001831
Iteration 44/1000 | Loss: 0.00001830
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001828
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001826
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001826
Iteration 59/1000 | Loss: 0.00001825
Iteration 60/1000 | Loss: 0.00001825
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001825
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001824
Iteration 65/1000 | Loss: 0.00001824
Iteration 66/1000 | Loss: 0.00001824
Iteration 67/1000 | Loss: 0.00001824
Iteration 68/1000 | Loss: 0.00001824
Iteration 69/1000 | Loss: 0.00001824
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001823
Iteration 73/1000 | Loss: 0.00001823
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001823
Iteration 76/1000 | Loss: 0.00001823
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001822
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001822
Iteration 81/1000 | Loss: 0.00001822
Iteration 82/1000 | Loss: 0.00001822
Iteration 83/1000 | Loss: 0.00001822
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001821
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001820
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001819
Iteration 95/1000 | Loss: 0.00001819
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001819
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00001818
Iteration 100/1000 | Loss: 0.00001818
Iteration 101/1000 | Loss: 0.00001818
Iteration 102/1000 | Loss: 0.00001817
Iteration 103/1000 | Loss: 0.00001817
Iteration 104/1000 | Loss: 0.00001816
Iteration 105/1000 | Loss: 0.00001816
Iteration 106/1000 | Loss: 0.00001816
Iteration 107/1000 | Loss: 0.00001816
Iteration 108/1000 | Loss: 0.00001816
Iteration 109/1000 | Loss: 0.00001816
Iteration 110/1000 | Loss: 0.00001816
Iteration 111/1000 | Loss: 0.00001815
Iteration 112/1000 | Loss: 0.00001815
Iteration 113/1000 | Loss: 0.00001815
Iteration 114/1000 | Loss: 0.00001815
Iteration 115/1000 | Loss: 0.00001815
Iteration 116/1000 | Loss: 0.00001815
Iteration 117/1000 | Loss: 0.00001814
Iteration 118/1000 | Loss: 0.00001814
Iteration 119/1000 | Loss: 0.00001814
Iteration 120/1000 | Loss: 0.00001814
Iteration 121/1000 | Loss: 0.00001813
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Iteration 124/1000 | Loss: 0.00001813
Iteration 125/1000 | Loss: 0.00001812
Iteration 126/1000 | Loss: 0.00001812
Iteration 127/1000 | Loss: 0.00001812
Iteration 128/1000 | Loss: 0.00001811
Iteration 129/1000 | Loss: 0.00001811
Iteration 130/1000 | Loss: 0.00001811
Iteration 131/1000 | Loss: 0.00001811
Iteration 132/1000 | Loss: 0.00001811
Iteration 133/1000 | Loss: 0.00001811
Iteration 134/1000 | Loss: 0.00001810
Iteration 135/1000 | Loss: 0.00001810
Iteration 136/1000 | Loss: 0.00001810
Iteration 137/1000 | Loss: 0.00001810
Iteration 138/1000 | Loss: 0.00001810
Iteration 139/1000 | Loss: 0.00001810
Iteration 140/1000 | Loss: 0.00001810
Iteration 141/1000 | Loss: 0.00001810
Iteration 142/1000 | Loss: 0.00001810
Iteration 143/1000 | Loss: 0.00001809
Iteration 144/1000 | Loss: 0.00001809
Iteration 145/1000 | Loss: 0.00001809
Iteration 146/1000 | Loss: 0.00001809
Iteration 147/1000 | Loss: 0.00001809
Iteration 148/1000 | Loss: 0.00001809
Iteration 149/1000 | Loss: 0.00001809
Iteration 150/1000 | Loss: 0.00001809
Iteration 151/1000 | Loss: 0.00001809
Iteration 152/1000 | Loss: 0.00001809
Iteration 153/1000 | Loss: 0.00001809
Iteration 154/1000 | Loss: 0.00001809
Iteration 155/1000 | Loss: 0.00001809
Iteration 156/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.8091886886395514e-05, 1.8091886886395514e-05, 1.8091886886395514e-05, 1.8091886886395514e-05, 1.8091886886395514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8091886886395514e-05

Optimization complete. Final v2v error: 3.537409782409668 mm

Highest mean error: 4.153261661529541 mm for frame 94

Lowest mean error: 2.4487476348876953 mm for frame 15

Saving results

Total time: 57.566506147384644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038523
Iteration 2/25 | Loss: 0.00243695
Iteration 3/25 | Loss: 0.00181189
Iteration 4/25 | Loss: 0.00167393
Iteration 5/25 | Loss: 0.00145057
Iteration 6/25 | Loss: 0.00145902
Iteration 7/25 | Loss: 0.00136383
Iteration 8/25 | Loss: 0.00126212
Iteration 9/25 | Loss: 0.00119553
Iteration 10/25 | Loss: 0.00115134
Iteration 11/25 | Loss: 0.00112256
Iteration 12/25 | Loss: 0.00111925
Iteration 13/25 | Loss: 0.00111232
Iteration 14/25 | Loss: 0.00110804
Iteration 15/25 | Loss: 0.00110600
Iteration 16/25 | Loss: 0.00109858
Iteration 17/25 | Loss: 0.00109557
Iteration 18/25 | Loss: 0.00109193
Iteration 19/25 | Loss: 0.00108890
Iteration 20/25 | Loss: 0.00108902
Iteration 21/25 | Loss: 0.00108857
Iteration 22/25 | Loss: 0.00108387
Iteration 23/25 | Loss: 0.00108403
Iteration 24/25 | Loss: 0.00108225
Iteration 25/25 | Loss: 0.00108489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44124138
Iteration 2/25 | Loss: 0.00186747
Iteration 3/25 | Loss: 0.00186747
Iteration 4/25 | Loss: 0.00186747
Iteration 5/25 | Loss: 0.00186747
Iteration 6/25 | Loss: 0.00186747
Iteration 7/25 | Loss: 0.00186746
Iteration 8/25 | Loss: 0.00186746
Iteration 9/25 | Loss: 0.00186746
Iteration 10/25 | Loss: 0.00186746
Iteration 11/25 | Loss: 0.00186746
Iteration 12/25 | Loss: 0.00186746
Iteration 13/25 | Loss: 0.00186746
Iteration 14/25 | Loss: 0.00186746
Iteration 15/25 | Loss: 0.00186746
Iteration 16/25 | Loss: 0.00186746
Iteration 17/25 | Loss: 0.00186746
Iteration 18/25 | Loss: 0.00186746
Iteration 19/25 | Loss: 0.00186746
Iteration 20/25 | Loss: 0.00186746
Iteration 21/25 | Loss: 0.00186746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001867463462986052, 0.001867463462986052, 0.001867463462986052, 0.001867463462986052, 0.001867463462986052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001867463462986052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186746
Iteration 2/1000 | Loss: 0.00028024
Iteration 3/1000 | Loss: 0.00042939
Iteration 4/1000 | Loss: 0.00035892
Iteration 5/1000 | Loss: 0.00030957
Iteration 6/1000 | Loss: 0.00019248
Iteration 7/1000 | Loss: 0.00017496
Iteration 8/1000 | Loss: 0.00022592
Iteration 9/1000 | Loss: 0.00022068
Iteration 10/1000 | Loss: 0.00019290
Iteration 11/1000 | Loss: 0.00015004
Iteration 12/1000 | Loss: 0.00056734
Iteration 13/1000 | Loss: 0.00017097
Iteration 14/1000 | Loss: 0.00023603
Iteration 15/1000 | Loss: 0.00014330
Iteration 16/1000 | Loss: 0.00048499
Iteration 17/1000 | Loss: 0.00135334
Iteration 18/1000 | Loss: 0.00082266
Iteration 19/1000 | Loss: 0.00239933
Iteration 20/1000 | Loss: 0.00180416
Iteration 21/1000 | Loss: 0.00202531
Iteration 22/1000 | Loss: 0.00028576
Iteration 23/1000 | Loss: 0.00028372
Iteration 24/1000 | Loss: 0.00027324
Iteration 25/1000 | Loss: 0.00013132
Iteration 26/1000 | Loss: 0.00011367
Iteration 27/1000 | Loss: 0.00010081
Iteration 28/1000 | Loss: 0.00010621
Iteration 29/1000 | Loss: 0.00009230
Iteration 30/1000 | Loss: 0.00019398
Iteration 31/1000 | Loss: 0.00014963
Iteration 32/1000 | Loss: 0.00015234
Iteration 33/1000 | Loss: 0.00008607
Iteration 34/1000 | Loss: 0.00008339
Iteration 35/1000 | Loss: 0.00020653
Iteration 36/1000 | Loss: 0.00008519
Iteration 37/1000 | Loss: 0.00008056
Iteration 38/1000 | Loss: 0.00007857
Iteration 39/1000 | Loss: 0.00007641
Iteration 40/1000 | Loss: 0.00007521
Iteration 41/1000 | Loss: 0.00007993
Iteration 42/1000 | Loss: 0.00007759
Iteration 43/1000 | Loss: 0.00007585
Iteration 44/1000 | Loss: 0.00007494
Iteration 45/1000 | Loss: 0.00007423
Iteration 46/1000 | Loss: 0.00040083
Iteration 47/1000 | Loss: 0.00096975
Iteration 48/1000 | Loss: 0.00037807
Iteration 49/1000 | Loss: 0.00009444
Iteration 50/1000 | Loss: 0.00007807
Iteration 51/1000 | Loss: 0.00018777
Iteration 52/1000 | Loss: 0.00007311
Iteration 53/1000 | Loss: 0.00039498
Iteration 54/1000 | Loss: 0.00018342
Iteration 55/1000 | Loss: 0.00008840
Iteration 56/1000 | Loss: 0.00007466
Iteration 57/1000 | Loss: 0.00007915
Iteration 58/1000 | Loss: 0.00007786
Iteration 59/1000 | Loss: 0.00007255
Iteration 60/1000 | Loss: 0.00032612
Iteration 61/1000 | Loss: 0.00008708
Iteration 62/1000 | Loss: 0.00020646
Iteration 63/1000 | Loss: 0.00008082
Iteration 64/1000 | Loss: 0.00018683
Iteration 65/1000 | Loss: 0.00007853
Iteration 66/1000 | Loss: 0.00016236
Iteration 67/1000 | Loss: 0.00016069
Iteration 68/1000 | Loss: 0.00016316
Iteration 69/1000 | Loss: 0.00016268
Iteration 70/1000 | Loss: 0.00011377
Iteration 71/1000 | Loss: 0.00015935
Iteration 72/1000 | Loss: 0.00006769
Iteration 73/1000 | Loss: 0.00006606
Iteration 74/1000 | Loss: 0.00018427
Iteration 75/1000 | Loss: 0.00065111
Iteration 76/1000 | Loss: 0.00074227
Iteration 77/1000 | Loss: 0.00022867
Iteration 78/1000 | Loss: 0.00021078
Iteration 79/1000 | Loss: 0.00007438
Iteration 80/1000 | Loss: 0.00006935
Iteration 81/1000 | Loss: 0.00027157
Iteration 82/1000 | Loss: 0.00007339
Iteration 83/1000 | Loss: 0.00006648
Iteration 84/1000 | Loss: 0.00021734
Iteration 85/1000 | Loss: 0.00008625
Iteration 86/1000 | Loss: 0.00006425
Iteration 87/1000 | Loss: 0.00019208
Iteration 88/1000 | Loss: 0.00006226
Iteration 89/1000 | Loss: 0.00006064
Iteration 90/1000 | Loss: 0.00038074
Iteration 91/1000 | Loss: 0.00030988
Iteration 92/1000 | Loss: 0.00011395
Iteration 93/1000 | Loss: 0.00024344
Iteration 94/1000 | Loss: 0.00011885
Iteration 95/1000 | Loss: 0.00008703
Iteration 96/1000 | Loss: 0.00022937
Iteration 97/1000 | Loss: 0.00017114
Iteration 98/1000 | Loss: 0.00007365
Iteration 99/1000 | Loss: 0.00007571
Iteration 100/1000 | Loss: 0.00006999
Iteration 101/1000 | Loss: 0.00007108
Iteration 102/1000 | Loss: 0.00006703
Iteration 103/1000 | Loss: 0.00020994
Iteration 104/1000 | Loss: 0.00023403
Iteration 105/1000 | Loss: 0.00007317
Iteration 106/1000 | Loss: 0.00015020
Iteration 107/1000 | Loss: 0.00029119
Iteration 108/1000 | Loss: 0.00018658
Iteration 109/1000 | Loss: 0.00025646
Iteration 110/1000 | Loss: 0.00017289
Iteration 111/1000 | Loss: 0.00021360
Iteration 112/1000 | Loss: 0.00014030
Iteration 113/1000 | Loss: 0.00024736
Iteration 114/1000 | Loss: 0.00006440
Iteration 115/1000 | Loss: 0.00015082
Iteration 116/1000 | Loss: 0.00006363
Iteration 117/1000 | Loss: 0.00029501
Iteration 118/1000 | Loss: 0.00021827
Iteration 119/1000 | Loss: 0.00008612
Iteration 120/1000 | Loss: 0.00008782
Iteration 121/1000 | Loss: 0.00009881
Iteration 122/1000 | Loss: 0.00010094
Iteration 123/1000 | Loss: 0.00012733
Iteration 124/1000 | Loss: 0.00006171
Iteration 125/1000 | Loss: 0.00015126
Iteration 126/1000 | Loss: 0.00010464
Iteration 127/1000 | Loss: 0.00005494
Iteration 128/1000 | Loss: 0.00005331
Iteration 129/1000 | Loss: 0.00005239
Iteration 130/1000 | Loss: 0.00005173
Iteration 131/1000 | Loss: 0.00005104
Iteration 132/1000 | Loss: 0.00005021
Iteration 133/1000 | Loss: 0.00004971
Iteration 134/1000 | Loss: 0.00004933
Iteration 135/1000 | Loss: 0.00004916
Iteration 136/1000 | Loss: 0.00004908
Iteration 137/1000 | Loss: 0.00004908
Iteration 138/1000 | Loss: 0.00004907
Iteration 139/1000 | Loss: 0.00004907
Iteration 140/1000 | Loss: 0.00004906
Iteration 141/1000 | Loss: 0.00004905
Iteration 142/1000 | Loss: 0.00004903
Iteration 143/1000 | Loss: 0.00004903
Iteration 144/1000 | Loss: 0.00004902
Iteration 145/1000 | Loss: 0.00004902
Iteration 146/1000 | Loss: 0.00004901
Iteration 147/1000 | Loss: 0.00004895
Iteration 148/1000 | Loss: 0.00004894
Iteration 149/1000 | Loss: 0.00004887
Iteration 150/1000 | Loss: 0.00004881
Iteration 151/1000 | Loss: 0.00004876
Iteration 152/1000 | Loss: 0.00004872
Iteration 153/1000 | Loss: 0.00004864
Iteration 154/1000 | Loss: 0.00004864
Iteration 155/1000 | Loss: 0.00004862
Iteration 156/1000 | Loss: 0.00004862
Iteration 157/1000 | Loss: 0.00004862
Iteration 158/1000 | Loss: 0.00004861
Iteration 159/1000 | Loss: 0.00004859
Iteration 160/1000 | Loss: 0.00004858
Iteration 161/1000 | Loss: 0.00004857
Iteration 162/1000 | Loss: 0.00004857
Iteration 163/1000 | Loss: 0.00004855
Iteration 164/1000 | Loss: 0.00004855
Iteration 165/1000 | Loss: 0.00004855
Iteration 166/1000 | Loss: 0.00004855
Iteration 167/1000 | Loss: 0.00004855
Iteration 168/1000 | Loss: 0.00004854
Iteration 169/1000 | Loss: 0.00004854
Iteration 170/1000 | Loss: 0.00004854
Iteration 171/1000 | Loss: 0.00004854
Iteration 172/1000 | Loss: 0.00004854
Iteration 173/1000 | Loss: 0.00004854
Iteration 174/1000 | Loss: 0.00004854
Iteration 175/1000 | Loss: 0.00004854
Iteration 176/1000 | Loss: 0.00004852
Iteration 177/1000 | Loss: 0.00004851
Iteration 178/1000 | Loss: 0.00004851
Iteration 179/1000 | Loss: 0.00004851
Iteration 180/1000 | Loss: 0.00004850
Iteration 181/1000 | Loss: 0.00004850
Iteration 182/1000 | Loss: 0.00004849
Iteration 183/1000 | Loss: 0.00004846
Iteration 184/1000 | Loss: 0.00004846
Iteration 185/1000 | Loss: 0.00004846
Iteration 186/1000 | Loss: 0.00004846
Iteration 187/1000 | Loss: 0.00004846
Iteration 188/1000 | Loss: 0.00004843
Iteration 189/1000 | Loss: 0.00004842
Iteration 190/1000 | Loss: 0.00004842
Iteration 191/1000 | Loss: 0.00004842
Iteration 192/1000 | Loss: 0.00004841
Iteration 193/1000 | Loss: 0.00004841
Iteration 194/1000 | Loss: 0.00004840
Iteration 195/1000 | Loss: 0.00004840
Iteration 196/1000 | Loss: 0.00004840
Iteration 197/1000 | Loss: 0.00004840
Iteration 198/1000 | Loss: 0.00004839
Iteration 199/1000 | Loss: 0.00004838
Iteration 200/1000 | Loss: 0.00004837
Iteration 201/1000 | Loss: 0.00004837
Iteration 202/1000 | Loss: 0.00004831
Iteration 203/1000 | Loss: 0.00004831
Iteration 204/1000 | Loss: 0.00004824
Iteration 205/1000 | Loss: 0.00004824
Iteration 206/1000 | Loss: 0.00004821
Iteration 207/1000 | Loss: 0.00004821
Iteration 208/1000 | Loss: 0.00004820
Iteration 209/1000 | Loss: 0.00004820
Iteration 210/1000 | Loss: 0.00004820
Iteration 211/1000 | Loss: 0.00004819
Iteration 212/1000 | Loss: 0.00004818
Iteration 213/1000 | Loss: 0.00004815
Iteration 214/1000 | Loss: 0.00004815
Iteration 215/1000 | Loss: 0.00004815
Iteration 216/1000 | Loss: 0.00004815
Iteration 217/1000 | Loss: 0.00004815
Iteration 218/1000 | Loss: 0.00004815
Iteration 219/1000 | Loss: 0.00004815
Iteration 220/1000 | Loss: 0.00004815
Iteration 221/1000 | Loss: 0.00004814
Iteration 222/1000 | Loss: 0.00004814
Iteration 223/1000 | Loss: 0.00004814
Iteration 224/1000 | Loss: 0.00004814
Iteration 225/1000 | Loss: 0.00004814
Iteration 226/1000 | Loss: 0.00004813
Iteration 227/1000 | Loss: 0.00004812
Iteration 228/1000 | Loss: 0.00004812
Iteration 229/1000 | Loss: 0.00004812
Iteration 230/1000 | Loss: 0.00004812
Iteration 231/1000 | Loss: 0.00004812
Iteration 232/1000 | Loss: 0.00004811
Iteration 233/1000 | Loss: 0.00004811
Iteration 234/1000 | Loss: 0.00004811
Iteration 235/1000 | Loss: 0.00004811
Iteration 236/1000 | Loss: 0.00004811
Iteration 237/1000 | Loss: 0.00004811
Iteration 238/1000 | Loss: 0.00004811
Iteration 239/1000 | Loss: 0.00004810
Iteration 240/1000 | Loss: 0.00004807
Iteration 241/1000 | Loss: 0.00004803
Iteration 242/1000 | Loss: 0.00004800
Iteration 243/1000 | Loss: 0.00004800
Iteration 244/1000 | Loss: 0.00004800
Iteration 245/1000 | Loss: 0.00004798
Iteration 246/1000 | Loss: 0.00004798
Iteration 247/1000 | Loss: 0.00004797
Iteration 248/1000 | Loss: 0.00004797
Iteration 249/1000 | Loss: 0.00004796
Iteration 250/1000 | Loss: 0.00004796
Iteration 251/1000 | Loss: 0.00004796
Iteration 252/1000 | Loss: 0.00004795
Iteration 253/1000 | Loss: 0.00004792
Iteration 254/1000 | Loss: 0.00004792
Iteration 255/1000 | Loss: 0.00004791
Iteration 256/1000 | Loss: 0.00004791
Iteration 257/1000 | Loss: 0.00004791
Iteration 258/1000 | Loss: 0.00004790
Iteration 259/1000 | Loss: 0.00004790
Iteration 260/1000 | Loss: 0.00004790
Iteration 261/1000 | Loss: 0.00004788
Iteration 262/1000 | Loss: 0.00004788
Iteration 263/1000 | Loss: 0.00004788
Iteration 264/1000 | Loss: 0.00004787
Iteration 265/1000 | Loss: 0.00004787
Iteration 266/1000 | Loss: 0.00004787
Iteration 267/1000 | Loss: 0.00004787
Iteration 268/1000 | Loss: 0.00004786
Iteration 269/1000 | Loss: 0.00004783
Iteration 270/1000 | Loss: 0.00004783
Iteration 271/1000 | Loss: 0.00004782
Iteration 272/1000 | Loss: 0.00004782
Iteration 273/1000 | Loss: 0.00004782
Iteration 274/1000 | Loss: 0.00004780
Iteration 275/1000 | Loss: 0.00004776
Iteration 276/1000 | Loss: 0.00004775
Iteration 277/1000 | Loss: 0.00004772
Iteration 278/1000 | Loss: 0.00004769
Iteration 279/1000 | Loss: 0.00004766
Iteration 280/1000 | Loss: 0.00004766
Iteration 281/1000 | Loss: 0.00004765
Iteration 282/1000 | Loss: 0.00004765
Iteration 283/1000 | Loss: 0.00004765
Iteration 284/1000 | Loss: 0.00004765
Iteration 285/1000 | Loss: 0.00004765
Iteration 286/1000 | Loss: 0.00004765
Iteration 287/1000 | Loss: 0.00004765
Iteration 288/1000 | Loss: 0.00004764
Iteration 289/1000 | Loss: 0.00004764
Iteration 290/1000 | Loss: 0.00004763
Iteration 291/1000 | Loss: 0.00004763
Iteration 292/1000 | Loss: 0.00006600
Iteration 293/1000 | Loss: 0.00016308
Iteration 294/1000 | Loss: 0.00005423
Iteration 295/1000 | Loss: 0.00005180
Iteration 296/1000 | Loss: 0.00006508
Iteration 297/1000 | Loss: 0.00004973
Iteration 298/1000 | Loss: 0.00004825
Iteration 299/1000 | Loss: 0.00004892
Iteration 300/1000 | Loss: 0.00004680
Iteration 301/1000 | Loss: 0.00004652
Iteration 302/1000 | Loss: 0.00004615
Iteration 303/1000 | Loss: 0.00004599
Iteration 304/1000 | Loss: 0.00004592
Iteration 305/1000 | Loss: 0.00004590
Iteration 306/1000 | Loss: 0.00004585
Iteration 307/1000 | Loss: 0.00004580
Iteration 308/1000 | Loss: 0.00004578
Iteration 309/1000 | Loss: 0.00004576
Iteration 310/1000 | Loss: 0.00004575
Iteration 311/1000 | Loss: 0.00004574
Iteration 312/1000 | Loss: 0.00004574
Iteration 313/1000 | Loss: 0.00004573
Iteration 314/1000 | Loss: 0.00004571
Iteration 315/1000 | Loss: 0.00004570
Iteration 316/1000 | Loss: 0.00004570
Iteration 317/1000 | Loss: 0.00004565
Iteration 318/1000 | Loss: 0.00004565
Iteration 319/1000 | Loss: 0.00004565
Iteration 320/1000 | Loss: 0.00004565
Iteration 321/1000 | Loss: 0.00004565
Iteration 322/1000 | Loss: 0.00004564
Iteration 323/1000 | Loss: 0.00004564
Iteration 324/1000 | Loss: 0.00004564
Iteration 325/1000 | Loss: 0.00004564
Iteration 326/1000 | Loss: 0.00004563
Iteration 327/1000 | Loss: 0.00004563
Iteration 328/1000 | Loss: 0.00004563
Iteration 329/1000 | Loss: 0.00004563
Iteration 330/1000 | Loss: 0.00004563
Iteration 331/1000 | Loss: 0.00004563
Iteration 332/1000 | Loss: 0.00004563
Iteration 333/1000 | Loss: 0.00004563
Iteration 334/1000 | Loss: 0.00004562
Iteration 335/1000 | Loss: 0.00004562
Iteration 336/1000 | Loss: 0.00004561
Iteration 337/1000 | Loss: 0.00004561
Iteration 338/1000 | Loss: 0.00004561
Iteration 339/1000 | Loss: 0.00004561
Iteration 340/1000 | Loss: 0.00004560
Iteration 341/1000 | Loss: 0.00004560
Iteration 342/1000 | Loss: 0.00004560
Iteration 343/1000 | Loss: 0.00004560
Iteration 344/1000 | Loss: 0.00004560
Iteration 345/1000 | Loss: 0.00004560
Iteration 346/1000 | Loss: 0.00004560
Iteration 347/1000 | Loss: 0.00004560
Iteration 348/1000 | Loss: 0.00004559
Iteration 349/1000 | Loss: 0.00004559
Iteration 350/1000 | Loss: 0.00004559
Iteration 351/1000 | Loss: 0.00004559
Iteration 352/1000 | Loss: 0.00004559
Iteration 353/1000 | Loss: 0.00004559
Iteration 354/1000 | Loss: 0.00004559
Iteration 355/1000 | Loss: 0.00004559
Iteration 356/1000 | Loss: 0.00004559
Iteration 357/1000 | Loss: 0.00004559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [4.559077206067741e-05, 4.559077206067741e-05, 4.559077206067741e-05, 4.559077206067741e-05, 4.559077206067741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.559077206067741e-05

Optimization complete. Final v2v error: 3.788806676864624 mm

Highest mean error: 11.496665000915527 mm for frame 18

Lowest mean error: 2.504544734954834 mm for frame 4

Saving results

Total time: 324.1366374492645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369871
Iteration 2/25 | Loss: 0.00089691
Iteration 3/25 | Loss: 0.00077507
Iteration 4/25 | Loss: 0.00075879
Iteration 5/25 | Loss: 0.00075386
Iteration 6/25 | Loss: 0.00075238
Iteration 7/25 | Loss: 0.00075234
Iteration 8/25 | Loss: 0.00075234
Iteration 9/25 | Loss: 0.00075234
Iteration 10/25 | Loss: 0.00075234
Iteration 11/25 | Loss: 0.00075234
Iteration 12/25 | Loss: 0.00075234
Iteration 13/25 | Loss: 0.00075234
Iteration 14/25 | Loss: 0.00075234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007523364620283246, 0.0007523364620283246, 0.0007523364620283246, 0.0007523364620283246, 0.0007523364620283246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007523364620283246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41137040
Iteration 2/25 | Loss: 0.00031973
Iteration 3/25 | Loss: 0.00031972
Iteration 4/25 | Loss: 0.00031972
Iteration 5/25 | Loss: 0.00031972
Iteration 6/25 | Loss: 0.00031972
Iteration 7/25 | Loss: 0.00031972
Iteration 8/25 | Loss: 0.00031972
Iteration 9/25 | Loss: 0.00031972
Iteration 10/25 | Loss: 0.00031972
Iteration 11/25 | Loss: 0.00031972
Iteration 12/25 | Loss: 0.00031972
Iteration 13/25 | Loss: 0.00031972
Iteration 14/25 | Loss: 0.00031972
Iteration 15/25 | Loss: 0.00031972
Iteration 16/25 | Loss: 0.00031972
Iteration 17/25 | Loss: 0.00031972
Iteration 18/25 | Loss: 0.00031972
Iteration 19/25 | Loss: 0.00031972
Iteration 20/25 | Loss: 0.00031972
Iteration 21/25 | Loss: 0.00031972
Iteration 22/25 | Loss: 0.00031972
Iteration 23/25 | Loss: 0.00031972
Iteration 24/25 | Loss: 0.00031972
Iteration 25/25 | Loss: 0.00031972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031972
Iteration 2/1000 | Loss: 0.00001391
Iteration 3/1000 | Loss: 0.00000712
Iteration 4/1000 | Loss: 0.00000619
Iteration 5/1000 | Loss: 0.00000582
Iteration 6/1000 | Loss: 0.00000562
Iteration 7/1000 | Loss: 0.00000558
Iteration 8/1000 | Loss: 0.00000552
Iteration 9/1000 | Loss: 0.00000551
Iteration 10/1000 | Loss: 0.00000551
Iteration 11/1000 | Loss: 0.00000549
Iteration 12/1000 | Loss: 0.00000547
Iteration 13/1000 | Loss: 0.00000546
Iteration 14/1000 | Loss: 0.00000546
Iteration 15/1000 | Loss: 0.00000545
Iteration 16/1000 | Loss: 0.00000545
Iteration 17/1000 | Loss: 0.00000545
Iteration 18/1000 | Loss: 0.00000545
Iteration 19/1000 | Loss: 0.00000544
Iteration 20/1000 | Loss: 0.00000544
Iteration 21/1000 | Loss: 0.00000544
Iteration 22/1000 | Loss: 0.00000544
Iteration 23/1000 | Loss: 0.00000544
Iteration 24/1000 | Loss: 0.00000544
Iteration 25/1000 | Loss: 0.00000544
Iteration 26/1000 | Loss: 0.00000543
Iteration 27/1000 | Loss: 0.00000543
Iteration 28/1000 | Loss: 0.00000543
Iteration 29/1000 | Loss: 0.00000543
Iteration 30/1000 | Loss: 0.00000543
Iteration 31/1000 | Loss: 0.00000543
Iteration 32/1000 | Loss: 0.00000543
Iteration 33/1000 | Loss: 0.00000543
Iteration 34/1000 | Loss: 0.00000543
Iteration 35/1000 | Loss: 0.00000543
Iteration 36/1000 | Loss: 0.00000543
Iteration 37/1000 | Loss: 0.00000543
Iteration 38/1000 | Loss: 0.00000542
Iteration 39/1000 | Loss: 0.00000542
Iteration 40/1000 | Loss: 0.00000542
Iteration 41/1000 | Loss: 0.00000542
Iteration 42/1000 | Loss: 0.00000542
Iteration 43/1000 | Loss: 0.00000542
Iteration 44/1000 | Loss: 0.00000542
Iteration 45/1000 | Loss: 0.00000542
Iteration 46/1000 | Loss: 0.00000542
Iteration 47/1000 | Loss: 0.00000541
Iteration 48/1000 | Loss: 0.00000541
Iteration 49/1000 | Loss: 0.00000541
Iteration 50/1000 | Loss: 0.00000540
Iteration 51/1000 | Loss: 0.00000540
Iteration 52/1000 | Loss: 0.00000540
Iteration 53/1000 | Loss: 0.00000540
Iteration 54/1000 | Loss: 0.00000539
Iteration 55/1000 | Loss: 0.00000539
Iteration 56/1000 | Loss: 0.00000539
Iteration 57/1000 | Loss: 0.00000539
Iteration 58/1000 | Loss: 0.00000539
Iteration 59/1000 | Loss: 0.00000539
Iteration 60/1000 | Loss: 0.00000539
Iteration 61/1000 | Loss: 0.00000538
Iteration 62/1000 | Loss: 0.00000537
Iteration 63/1000 | Loss: 0.00000536
Iteration 64/1000 | Loss: 0.00000536
Iteration 65/1000 | Loss: 0.00000536
Iteration 66/1000 | Loss: 0.00000536
Iteration 67/1000 | Loss: 0.00000535
Iteration 68/1000 | Loss: 0.00000535
Iteration 69/1000 | Loss: 0.00000535
Iteration 70/1000 | Loss: 0.00000534
Iteration 71/1000 | Loss: 0.00000534
Iteration 72/1000 | Loss: 0.00000534
Iteration 73/1000 | Loss: 0.00000533
Iteration 74/1000 | Loss: 0.00000533
Iteration 75/1000 | Loss: 0.00000533
Iteration 76/1000 | Loss: 0.00000533
Iteration 77/1000 | Loss: 0.00000533
Iteration 78/1000 | Loss: 0.00000533
Iteration 79/1000 | Loss: 0.00000532
Iteration 80/1000 | Loss: 0.00000532
Iteration 81/1000 | Loss: 0.00000532
Iteration 82/1000 | Loss: 0.00000531
Iteration 83/1000 | Loss: 0.00000531
Iteration 84/1000 | Loss: 0.00000530
Iteration 85/1000 | Loss: 0.00000530
Iteration 86/1000 | Loss: 0.00000530
Iteration 87/1000 | Loss: 0.00000530
Iteration 88/1000 | Loss: 0.00000530
Iteration 89/1000 | Loss: 0.00000530
Iteration 90/1000 | Loss: 0.00000530
Iteration 91/1000 | Loss: 0.00000530
Iteration 92/1000 | Loss: 0.00000530
Iteration 93/1000 | Loss: 0.00000529
Iteration 94/1000 | Loss: 0.00000529
Iteration 95/1000 | Loss: 0.00000529
Iteration 96/1000 | Loss: 0.00000529
Iteration 97/1000 | Loss: 0.00000529
Iteration 98/1000 | Loss: 0.00000529
Iteration 99/1000 | Loss: 0.00000529
Iteration 100/1000 | Loss: 0.00000528
Iteration 101/1000 | Loss: 0.00000528
Iteration 102/1000 | Loss: 0.00000528
Iteration 103/1000 | Loss: 0.00000528
Iteration 104/1000 | Loss: 0.00000528
Iteration 105/1000 | Loss: 0.00000528
Iteration 106/1000 | Loss: 0.00000528
Iteration 107/1000 | Loss: 0.00000528
Iteration 108/1000 | Loss: 0.00000528
Iteration 109/1000 | Loss: 0.00000528
Iteration 110/1000 | Loss: 0.00000528
Iteration 111/1000 | Loss: 0.00000528
Iteration 112/1000 | Loss: 0.00000528
Iteration 113/1000 | Loss: 0.00000528
Iteration 114/1000 | Loss: 0.00000527
Iteration 115/1000 | Loss: 0.00000527
Iteration 116/1000 | Loss: 0.00000527
Iteration 117/1000 | Loss: 0.00000527
Iteration 118/1000 | Loss: 0.00000527
Iteration 119/1000 | Loss: 0.00000527
Iteration 120/1000 | Loss: 0.00000527
Iteration 121/1000 | Loss: 0.00000527
Iteration 122/1000 | Loss: 0.00000527
Iteration 123/1000 | Loss: 0.00000527
Iteration 124/1000 | Loss: 0.00000527
Iteration 125/1000 | Loss: 0.00000527
Iteration 126/1000 | Loss: 0.00000527
Iteration 127/1000 | Loss: 0.00000527
Iteration 128/1000 | Loss: 0.00000527
Iteration 129/1000 | Loss: 0.00000527
Iteration 130/1000 | Loss: 0.00000526
Iteration 131/1000 | Loss: 0.00000526
Iteration 132/1000 | Loss: 0.00000526
Iteration 133/1000 | Loss: 0.00000526
Iteration 134/1000 | Loss: 0.00000526
Iteration 135/1000 | Loss: 0.00000526
Iteration 136/1000 | Loss: 0.00000526
Iteration 137/1000 | Loss: 0.00000526
Iteration 138/1000 | Loss: 0.00000526
Iteration 139/1000 | Loss: 0.00000526
Iteration 140/1000 | Loss: 0.00000526
Iteration 141/1000 | Loss: 0.00000526
Iteration 142/1000 | Loss: 0.00000526
Iteration 143/1000 | Loss: 0.00000526
Iteration 144/1000 | Loss: 0.00000526
Iteration 145/1000 | Loss: 0.00000526
Iteration 146/1000 | Loss: 0.00000526
Iteration 147/1000 | Loss: 0.00000526
Iteration 148/1000 | Loss: 0.00000526
Iteration 149/1000 | Loss: 0.00000526
Iteration 150/1000 | Loss: 0.00000526
Iteration 151/1000 | Loss: 0.00000526
Iteration 152/1000 | Loss: 0.00000526
Iteration 153/1000 | Loss: 0.00000526
Iteration 154/1000 | Loss: 0.00000526
Iteration 155/1000 | Loss: 0.00000526
Iteration 156/1000 | Loss: 0.00000526
Iteration 157/1000 | Loss: 0.00000526
Iteration 158/1000 | Loss: 0.00000526
Iteration 159/1000 | Loss: 0.00000526
Iteration 160/1000 | Loss: 0.00000526
Iteration 161/1000 | Loss: 0.00000526
Iteration 162/1000 | Loss: 0.00000526
Iteration 163/1000 | Loss: 0.00000526
Iteration 164/1000 | Loss: 0.00000526
Iteration 165/1000 | Loss: 0.00000526
Iteration 166/1000 | Loss: 0.00000526
Iteration 167/1000 | Loss: 0.00000526
Iteration 168/1000 | Loss: 0.00000526
Iteration 169/1000 | Loss: 0.00000526
Iteration 170/1000 | Loss: 0.00000526
Iteration 171/1000 | Loss: 0.00000526
Iteration 172/1000 | Loss: 0.00000526
Iteration 173/1000 | Loss: 0.00000526
Iteration 174/1000 | Loss: 0.00000526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [5.263962520984933e-06, 5.263962520984933e-06, 5.263962520984933e-06, 5.263962520984933e-06, 5.263962520984933e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.263962520984933e-06

Optimization complete. Final v2v error: 1.9526796340942383 mm

Highest mean error: 2.4188475608825684 mm for frame 88

Lowest mean error: 1.716212511062622 mm for frame 6

Saving results

Total time: 28.55558490753174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4002/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4002/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030348
Iteration 2/25 | Loss: 0.00164372
Iteration 3/25 | Loss: 0.00104072
Iteration 4/25 | Loss: 0.00100147
Iteration 5/25 | Loss: 0.00099587
Iteration 6/25 | Loss: 0.00099420
Iteration 7/25 | Loss: 0.00099420
Iteration 8/25 | Loss: 0.00099420
Iteration 9/25 | Loss: 0.00099420
Iteration 10/25 | Loss: 0.00099420
Iteration 11/25 | Loss: 0.00099420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000994198489934206, 0.000994198489934206, 0.000994198489934206, 0.000994198489934206, 0.000994198489934206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000994198489934206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48879471
Iteration 2/25 | Loss: 0.00034663
Iteration 3/25 | Loss: 0.00034662
Iteration 4/25 | Loss: 0.00034662
Iteration 5/25 | Loss: 0.00034662
Iteration 6/25 | Loss: 0.00034662
Iteration 7/25 | Loss: 0.00034662
Iteration 8/25 | Loss: 0.00034662
Iteration 9/25 | Loss: 0.00034662
Iteration 10/25 | Loss: 0.00034662
Iteration 11/25 | Loss: 0.00034662
Iteration 12/25 | Loss: 0.00034662
Iteration 13/25 | Loss: 0.00034662
Iteration 14/25 | Loss: 0.00034662
Iteration 15/25 | Loss: 0.00034662
Iteration 16/25 | Loss: 0.00034662
Iteration 17/25 | Loss: 0.00034662
Iteration 18/25 | Loss: 0.00034662
Iteration 19/25 | Loss: 0.00034662
Iteration 20/25 | Loss: 0.00034662
Iteration 21/25 | Loss: 0.00034662
Iteration 22/25 | Loss: 0.00034662
Iteration 23/25 | Loss: 0.00034662
Iteration 24/25 | Loss: 0.00034662
Iteration 25/25 | Loss: 0.00034662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034662
Iteration 2/1000 | Loss: 0.00003975
Iteration 3/1000 | Loss: 0.00002641
Iteration 4/1000 | Loss: 0.00002322
Iteration 5/1000 | Loss: 0.00002168
Iteration 6/1000 | Loss: 0.00002094
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001994
Iteration 9/1000 | Loss: 0.00001963
Iteration 10/1000 | Loss: 0.00001944
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001909
Iteration 15/1000 | Loss: 0.00001906
Iteration 16/1000 | Loss: 0.00001905
Iteration 17/1000 | Loss: 0.00001905
Iteration 18/1000 | Loss: 0.00001902
Iteration 19/1000 | Loss: 0.00001902
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001900
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001899
Iteration 26/1000 | Loss: 0.00001899
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001898
Iteration 29/1000 | Loss: 0.00001898
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001897
Iteration 34/1000 | Loss: 0.00001897
Iteration 35/1000 | Loss: 0.00001896
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001896
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001894
Iteration 43/1000 | Loss: 0.00001894
Iteration 44/1000 | Loss: 0.00001894
Iteration 45/1000 | Loss: 0.00001894
Iteration 46/1000 | Loss: 0.00001893
Iteration 47/1000 | Loss: 0.00001893
Iteration 48/1000 | Loss: 0.00001893
Iteration 49/1000 | Loss: 0.00001893
Iteration 50/1000 | Loss: 0.00001892
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001892
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001891
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001890
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001889
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001886
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001884
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001882
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001882
Iteration 123/1000 | Loss: 0.00001882
Iteration 124/1000 | Loss: 0.00001882
Iteration 125/1000 | Loss: 0.00001882
Iteration 126/1000 | Loss: 0.00001882
Iteration 127/1000 | Loss: 0.00001882
Iteration 128/1000 | Loss: 0.00001882
Iteration 129/1000 | Loss: 0.00001882
Iteration 130/1000 | Loss: 0.00001882
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.8820810510078445e-05, 1.8820810510078445e-05, 1.8820810510078445e-05, 1.8820810510078445e-05, 1.8820810510078445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8820810510078445e-05

Optimization complete. Final v2v error: 3.490576982498169 mm

Highest mean error: 4.51609468460083 mm for frame 30

Lowest mean error: 2.849456310272217 mm for frame 207

Saving results

Total time: 39.78531074523926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029019
Iteration 2/25 | Loss: 0.01029019
Iteration 3/25 | Loss: 0.01029018
Iteration 4/25 | Loss: 0.00245381
Iteration 5/25 | Loss: 0.00175234
Iteration 6/25 | Loss: 0.00162074
Iteration 7/25 | Loss: 0.00158612
Iteration 8/25 | Loss: 0.00154957
Iteration 9/25 | Loss: 0.00150045
Iteration 10/25 | Loss: 0.00147722
Iteration 11/25 | Loss: 0.00145829
Iteration 12/25 | Loss: 0.00145191
Iteration 13/25 | Loss: 0.00145230
Iteration 14/25 | Loss: 0.00144228
Iteration 15/25 | Loss: 0.00143920
Iteration 16/25 | Loss: 0.00143951
Iteration 17/25 | Loss: 0.00143808
Iteration 18/25 | Loss: 0.00143687
Iteration 19/25 | Loss: 0.00143733
Iteration 20/25 | Loss: 0.00143660
Iteration 21/25 | Loss: 0.00143937
Iteration 22/25 | Loss: 0.00143573
Iteration 23/25 | Loss: 0.00143583
Iteration 24/25 | Loss: 0.00143501
Iteration 25/25 | Loss: 0.00143477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17425597
Iteration 2/25 | Loss: 0.00471363
Iteration 3/25 | Loss: 0.00471363
Iteration 4/25 | Loss: 0.00471363
Iteration 5/25 | Loss: 0.00471363
Iteration 6/25 | Loss: 0.00471362
Iteration 7/25 | Loss: 0.00471362
Iteration 8/25 | Loss: 0.00471362
Iteration 9/25 | Loss: 0.00471362
Iteration 10/25 | Loss: 0.00471362
Iteration 11/25 | Loss: 0.00471362
Iteration 12/25 | Loss: 0.00471362
Iteration 13/25 | Loss: 0.00471362
Iteration 14/25 | Loss: 0.00471362
Iteration 15/25 | Loss: 0.00471362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0047136228531599045, 0.0047136228531599045, 0.0047136228531599045, 0.0047136228531599045, 0.0047136228531599045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047136228531599045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00471362
Iteration 2/1000 | Loss: 0.00050646
Iteration 3/1000 | Loss: 0.00040117
Iteration 4/1000 | Loss: 0.00033012
Iteration 5/1000 | Loss: 0.00034032
Iteration 6/1000 | Loss: 0.00033132
Iteration 7/1000 | Loss: 0.00029559
Iteration 8/1000 | Loss: 0.00024368
Iteration 9/1000 | Loss: 0.00023199
Iteration 10/1000 | Loss: 0.00022024
Iteration 11/1000 | Loss: 0.00021263
Iteration 12/1000 | Loss: 0.00020743
Iteration 13/1000 | Loss: 0.00019959
Iteration 14/1000 | Loss: 0.00099185
Iteration 15/1000 | Loss: 0.00322203
Iteration 16/1000 | Loss: 0.00265988
Iteration 17/1000 | Loss: 0.00549995
Iteration 18/1000 | Loss: 0.00863692
Iteration 19/1000 | Loss: 0.00899734
Iteration 20/1000 | Loss: 0.00994623
Iteration 21/1000 | Loss: 0.00491249
Iteration 22/1000 | Loss: 0.00675343
Iteration 23/1000 | Loss: 0.00270236
Iteration 24/1000 | Loss: 0.00889990
Iteration 25/1000 | Loss: 0.00557102
Iteration 26/1000 | Loss: 0.00634721
Iteration 27/1000 | Loss: 0.00592892
Iteration 28/1000 | Loss: 0.00299999
Iteration 29/1000 | Loss: 0.00327090
Iteration 30/1000 | Loss: 0.00182013
Iteration 31/1000 | Loss: 0.00273271
Iteration 32/1000 | Loss: 0.00243026
Iteration 33/1000 | Loss: 0.00235290
Iteration 34/1000 | Loss: 0.00199563
Iteration 35/1000 | Loss: 0.00222539
Iteration 36/1000 | Loss: 0.00227921
Iteration 37/1000 | Loss: 0.00413146
Iteration 38/1000 | Loss: 0.00443702
Iteration 39/1000 | Loss: 0.00266633
Iteration 40/1000 | Loss: 0.00430137
Iteration 41/1000 | Loss: 0.00451953
Iteration 42/1000 | Loss: 0.00421116
Iteration 43/1000 | Loss: 0.00149869
Iteration 44/1000 | Loss: 0.00151632
Iteration 45/1000 | Loss: 0.00143105
Iteration 46/1000 | Loss: 0.00142913
Iteration 47/1000 | Loss: 0.00156105
Iteration 48/1000 | Loss: 0.00161216
Iteration 49/1000 | Loss: 0.00154071
Iteration 50/1000 | Loss: 0.00170256
Iteration 51/1000 | Loss: 0.00141561
Iteration 52/1000 | Loss: 0.00143795
Iteration 53/1000 | Loss: 0.00156937
Iteration 54/1000 | Loss: 0.00180403
Iteration 55/1000 | Loss: 0.00152629
Iteration 56/1000 | Loss: 0.00103353
Iteration 57/1000 | Loss: 0.00111831
Iteration 58/1000 | Loss: 0.00162368
Iteration 59/1000 | Loss: 0.00131913
Iteration 60/1000 | Loss: 0.00093316
Iteration 61/1000 | Loss: 0.00137403
Iteration 62/1000 | Loss: 0.00111457
Iteration 63/1000 | Loss: 0.00110525
Iteration 64/1000 | Loss: 0.00085729
Iteration 65/1000 | Loss: 0.00062677
Iteration 66/1000 | Loss: 0.00139591
Iteration 67/1000 | Loss: 0.00087699
Iteration 68/1000 | Loss: 0.00120651
Iteration 69/1000 | Loss: 0.00091362
Iteration 70/1000 | Loss: 0.00111237
Iteration 71/1000 | Loss: 0.00088144
Iteration 72/1000 | Loss: 0.00092109
Iteration 73/1000 | Loss: 0.00132943
Iteration 74/1000 | Loss: 0.00096306
Iteration 75/1000 | Loss: 0.00166472
Iteration 76/1000 | Loss: 0.00107078
Iteration 77/1000 | Loss: 0.00144124
Iteration 78/1000 | Loss: 0.00096210
Iteration 79/1000 | Loss: 0.00223639
Iteration 80/1000 | Loss: 0.00111652
Iteration 81/1000 | Loss: 0.00296903
Iteration 82/1000 | Loss: 0.00111440
Iteration 83/1000 | Loss: 0.00200889
Iteration 84/1000 | Loss: 0.00094770
Iteration 85/1000 | Loss: 0.00118010
Iteration 86/1000 | Loss: 0.00138327
Iteration 87/1000 | Loss: 0.00155184
Iteration 88/1000 | Loss: 0.00111512
Iteration 89/1000 | Loss: 0.00110041
Iteration 90/1000 | Loss: 0.00137760
Iteration 91/1000 | Loss: 0.00116152
Iteration 92/1000 | Loss: 0.00128019
Iteration 93/1000 | Loss: 0.00117039
Iteration 94/1000 | Loss: 0.00213373
Iteration 95/1000 | Loss: 0.00126059
Iteration 96/1000 | Loss: 0.00134333
Iteration 97/1000 | Loss: 0.00077836
Iteration 98/1000 | Loss: 0.00084403
Iteration 99/1000 | Loss: 0.00078517
Iteration 100/1000 | Loss: 0.00108156
Iteration 101/1000 | Loss: 0.00056641
Iteration 102/1000 | Loss: 0.00057708
Iteration 103/1000 | Loss: 0.00049804
Iteration 104/1000 | Loss: 0.00069196
Iteration 105/1000 | Loss: 0.00060360
Iteration 106/1000 | Loss: 0.00068107
Iteration 107/1000 | Loss: 0.00073580
Iteration 108/1000 | Loss: 0.00057548
Iteration 109/1000 | Loss: 0.00075045
Iteration 110/1000 | Loss: 0.00099625
Iteration 111/1000 | Loss: 0.00077063
Iteration 112/1000 | Loss: 0.00072870
Iteration 113/1000 | Loss: 0.00073627
Iteration 114/1000 | Loss: 0.00059499
Iteration 115/1000 | Loss: 0.00062943
Iteration 116/1000 | Loss: 0.00144068
Iteration 117/1000 | Loss: 0.00090217
Iteration 118/1000 | Loss: 0.00107454
Iteration 119/1000 | Loss: 0.00159908
Iteration 120/1000 | Loss: 0.00046870
Iteration 121/1000 | Loss: 0.00079140
Iteration 122/1000 | Loss: 0.00073844
Iteration 123/1000 | Loss: 0.00074159
Iteration 124/1000 | Loss: 0.00087503
Iteration 125/1000 | Loss: 0.00048859
Iteration 126/1000 | Loss: 0.00063962
Iteration 127/1000 | Loss: 0.00064889
Iteration 128/1000 | Loss: 0.00106776
Iteration 129/1000 | Loss: 0.00094648
Iteration 130/1000 | Loss: 0.00060164
Iteration 131/1000 | Loss: 0.00065809
Iteration 132/1000 | Loss: 0.00063382
Iteration 133/1000 | Loss: 0.00121160
Iteration 134/1000 | Loss: 0.00045496
Iteration 135/1000 | Loss: 0.00106833
Iteration 136/1000 | Loss: 0.00075982
Iteration 137/1000 | Loss: 0.00056473
Iteration 138/1000 | Loss: 0.00041026
Iteration 139/1000 | Loss: 0.00038836
Iteration 140/1000 | Loss: 0.00093333
Iteration 141/1000 | Loss: 0.00042887
Iteration 142/1000 | Loss: 0.00019080
Iteration 143/1000 | Loss: 0.00036917
Iteration 144/1000 | Loss: 0.00115133
Iteration 145/1000 | Loss: 0.00081293
Iteration 146/1000 | Loss: 0.00033114
Iteration 147/1000 | Loss: 0.00020891
Iteration 148/1000 | Loss: 0.00044358
Iteration 149/1000 | Loss: 0.00108435
Iteration 150/1000 | Loss: 0.00048027
Iteration 151/1000 | Loss: 0.00035249
Iteration 152/1000 | Loss: 0.00052533
Iteration 153/1000 | Loss: 0.00055635
Iteration 154/1000 | Loss: 0.00059417
Iteration 155/1000 | Loss: 0.00036518
Iteration 156/1000 | Loss: 0.00023026
Iteration 157/1000 | Loss: 0.00042902
Iteration 158/1000 | Loss: 0.00018313
Iteration 159/1000 | Loss: 0.00012942
Iteration 160/1000 | Loss: 0.00010126
Iteration 161/1000 | Loss: 0.00009082
Iteration 162/1000 | Loss: 0.00008595
Iteration 163/1000 | Loss: 0.00008025
Iteration 164/1000 | Loss: 0.00007633
Iteration 165/1000 | Loss: 0.00014064
Iteration 166/1000 | Loss: 0.00018946
Iteration 167/1000 | Loss: 0.00012242
Iteration 168/1000 | Loss: 0.00007171
Iteration 169/1000 | Loss: 0.00024132
Iteration 170/1000 | Loss: 0.00024130
Iteration 171/1000 | Loss: 0.00020300
Iteration 172/1000 | Loss: 0.00012512
Iteration 173/1000 | Loss: 0.00015026
Iteration 174/1000 | Loss: 0.00011158
Iteration 175/1000 | Loss: 0.00029271
Iteration 176/1000 | Loss: 0.00013900
Iteration 177/1000 | Loss: 0.00007717
Iteration 178/1000 | Loss: 0.00030814
Iteration 179/1000 | Loss: 0.00019903
Iteration 180/1000 | Loss: 0.00033261
Iteration 181/1000 | Loss: 0.00022031
Iteration 182/1000 | Loss: 0.00007184
Iteration 183/1000 | Loss: 0.00019287
Iteration 184/1000 | Loss: 0.00015570
Iteration 185/1000 | Loss: 0.00024676
Iteration 186/1000 | Loss: 0.00010801
Iteration 187/1000 | Loss: 0.00007223
Iteration 188/1000 | Loss: 0.00006498
Iteration 189/1000 | Loss: 0.00006161
Iteration 190/1000 | Loss: 0.00020709
Iteration 191/1000 | Loss: 0.00006032
Iteration 192/1000 | Loss: 0.00005863
Iteration 193/1000 | Loss: 0.00021639
Iteration 194/1000 | Loss: 0.00037052
Iteration 195/1000 | Loss: 0.00026022
Iteration 196/1000 | Loss: 0.00005705
Iteration 197/1000 | Loss: 0.00016636
Iteration 198/1000 | Loss: 0.00028706
Iteration 199/1000 | Loss: 0.00051523
Iteration 200/1000 | Loss: 0.00016736
Iteration 201/1000 | Loss: 0.00007296
Iteration 202/1000 | Loss: 0.00006177
Iteration 203/1000 | Loss: 0.00029683
Iteration 204/1000 | Loss: 0.00021829
Iteration 205/1000 | Loss: 0.00005662
Iteration 206/1000 | Loss: 0.00024285
Iteration 207/1000 | Loss: 0.00017573
Iteration 208/1000 | Loss: 0.00018074
Iteration 209/1000 | Loss: 0.00012472
Iteration 210/1000 | Loss: 0.00016226
Iteration 211/1000 | Loss: 0.00018051
Iteration 212/1000 | Loss: 0.00022319
Iteration 213/1000 | Loss: 0.00013484
Iteration 214/1000 | Loss: 0.00007323
Iteration 215/1000 | Loss: 0.00011128
Iteration 216/1000 | Loss: 0.00012187
Iteration 217/1000 | Loss: 0.00010494
Iteration 218/1000 | Loss: 0.00008852
Iteration 219/1000 | Loss: 0.00011463
Iteration 220/1000 | Loss: 0.00007933
Iteration 221/1000 | Loss: 0.00005182
Iteration 222/1000 | Loss: 0.00011165
Iteration 223/1000 | Loss: 0.00023357
Iteration 224/1000 | Loss: 0.00022103
Iteration 225/1000 | Loss: 0.00009818
Iteration 226/1000 | Loss: 0.00013514
Iteration 227/1000 | Loss: 0.00010294
Iteration 228/1000 | Loss: 0.00022459
Iteration 229/1000 | Loss: 0.00005337
Iteration 230/1000 | Loss: 0.00005147
Iteration 231/1000 | Loss: 0.00013617
Iteration 232/1000 | Loss: 0.00017471
Iteration 233/1000 | Loss: 0.00016763
Iteration 234/1000 | Loss: 0.00014735
Iteration 235/1000 | Loss: 0.00008822
Iteration 236/1000 | Loss: 0.00010237
Iteration 237/1000 | Loss: 0.00009253
Iteration 238/1000 | Loss: 0.00009547
Iteration 239/1000 | Loss: 0.00019602
Iteration 240/1000 | Loss: 0.00021054
Iteration 241/1000 | Loss: 0.00017691
Iteration 242/1000 | Loss: 0.00014776
Iteration 243/1000 | Loss: 0.00034997
Iteration 244/1000 | Loss: 0.00017136
Iteration 245/1000 | Loss: 0.00025710
Iteration 246/1000 | Loss: 0.00028595
Iteration 247/1000 | Loss: 0.00020218
Iteration 248/1000 | Loss: 0.00013421
Iteration 249/1000 | Loss: 0.00012670
Iteration 250/1000 | Loss: 0.00022462
Iteration 251/1000 | Loss: 0.00017746
Iteration 252/1000 | Loss: 0.00024091
Iteration 253/1000 | Loss: 0.00030295
Iteration 254/1000 | Loss: 0.00010805
Iteration 255/1000 | Loss: 0.00015217
Iteration 256/1000 | Loss: 0.00020957
Iteration 257/1000 | Loss: 0.00017413
Iteration 258/1000 | Loss: 0.00013106
Iteration 259/1000 | Loss: 0.00043442
Iteration 260/1000 | Loss: 0.00010105
Iteration 261/1000 | Loss: 0.00013696
Iteration 262/1000 | Loss: 0.00010839
Iteration 263/1000 | Loss: 0.00009408
Iteration 264/1000 | Loss: 0.00009301
Iteration 265/1000 | Loss: 0.00007443
Iteration 266/1000 | Loss: 0.00007321
Iteration 267/1000 | Loss: 0.00006198
Iteration 268/1000 | Loss: 0.00005475
Iteration 269/1000 | Loss: 0.00005095
Iteration 270/1000 | Loss: 0.00008523
Iteration 271/1000 | Loss: 0.00014754
Iteration 272/1000 | Loss: 0.00025787
Iteration 273/1000 | Loss: 0.00019356
Iteration 274/1000 | Loss: 0.00042064
Iteration 275/1000 | Loss: 0.00028525
Iteration 276/1000 | Loss: 0.00036856
Iteration 277/1000 | Loss: 0.00034166
Iteration 278/1000 | Loss: 0.00043739
Iteration 279/1000 | Loss: 0.00033045
Iteration 280/1000 | Loss: 0.00011819
Iteration 281/1000 | Loss: 0.00010929
Iteration 282/1000 | Loss: 0.00012945
Iteration 283/1000 | Loss: 0.00006789
Iteration 284/1000 | Loss: 0.00008161
Iteration 285/1000 | Loss: 0.00006394
Iteration 286/1000 | Loss: 0.00013691
Iteration 287/1000 | Loss: 0.00033755
Iteration 288/1000 | Loss: 0.00019751
Iteration 289/1000 | Loss: 0.00014486
Iteration 290/1000 | Loss: 0.00005360
Iteration 291/1000 | Loss: 0.00004808
Iteration 292/1000 | Loss: 0.00004617
Iteration 293/1000 | Loss: 0.00016382
Iteration 294/1000 | Loss: 0.00023453
Iteration 295/1000 | Loss: 0.00019059
Iteration 296/1000 | Loss: 0.00011349
Iteration 297/1000 | Loss: 0.00016930
Iteration 298/1000 | Loss: 0.00018068
Iteration 299/1000 | Loss: 0.00014631
Iteration 300/1000 | Loss: 0.00014486
Iteration 301/1000 | Loss: 0.00014916
Iteration 302/1000 | Loss: 0.00017588
Iteration 303/1000 | Loss: 0.00009144
Iteration 304/1000 | Loss: 0.00022911
Iteration 305/1000 | Loss: 0.00014863
Iteration 306/1000 | Loss: 0.00013398
Iteration 307/1000 | Loss: 0.00013191
Iteration 308/1000 | Loss: 0.00022226
Iteration 309/1000 | Loss: 0.00016601
Iteration 310/1000 | Loss: 0.00018431
Iteration 311/1000 | Loss: 0.00006300
Iteration 312/1000 | Loss: 0.00012738
Iteration 313/1000 | Loss: 0.00007282
Iteration 314/1000 | Loss: 0.00009574
Iteration 315/1000 | Loss: 0.00011823
Iteration 316/1000 | Loss: 0.00019465
Iteration 317/1000 | Loss: 0.00022619
Iteration 318/1000 | Loss: 0.00042917
Iteration 319/1000 | Loss: 0.00018097
Iteration 320/1000 | Loss: 0.00017096
Iteration 321/1000 | Loss: 0.00020426
Iteration 322/1000 | Loss: 0.00010286
Iteration 323/1000 | Loss: 0.00023279
Iteration 324/1000 | Loss: 0.00017348
Iteration 325/1000 | Loss: 0.00033218
Iteration 326/1000 | Loss: 0.00014629
Iteration 327/1000 | Loss: 0.00025686
Iteration 328/1000 | Loss: 0.00013915
Iteration 329/1000 | Loss: 0.00017089
Iteration 330/1000 | Loss: 0.00005858
Iteration 331/1000 | Loss: 0.00007967
Iteration 332/1000 | Loss: 0.00014848
Iteration 333/1000 | Loss: 0.00014714
Iteration 334/1000 | Loss: 0.00022501
Iteration 335/1000 | Loss: 0.00020491
Iteration 336/1000 | Loss: 0.00005912
Iteration 337/1000 | Loss: 0.00010672
Iteration 338/1000 | Loss: 0.00004355
Iteration 339/1000 | Loss: 0.00004228
Iteration 340/1000 | Loss: 0.00004041
Iteration 341/1000 | Loss: 0.00015473
Iteration 342/1000 | Loss: 0.00026800
Iteration 343/1000 | Loss: 0.00026563
Iteration 344/1000 | Loss: 0.00004952
Iteration 345/1000 | Loss: 0.00004282
Iteration 346/1000 | Loss: 0.00004066
Iteration 347/1000 | Loss: 0.00003975
Iteration 348/1000 | Loss: 0.00024075
Iteration 349/1000 | Loss: 0.00019247
Iteration 350/1000 | Loss: 0.00028825
Iteration 351/1000 | Loss: 0.00039520
Iteration 352/1000 | Loss: 0.00029557
Iteration 353/1000 | Loss: 0.00040321
Iteration 354/1000 | Loss: 0.00014309
Iteration 355/1000 | Loss: 0.00034257
Iteration 356/1000 | Loss: 0.00029846
Iteration 357/1000 | Loss: 0.00024249
Iteration 358/1000 | Loss: 0.00027544
Iteration 359/1000 | Loss: 0.00024118
Iteration 360/1000 | Loss: 0.00033930
Iteration 361/1000 | Loss: 0.00015050
Iteration 362/1000 | Loss: 0.00030348
Iteration 363/1000 | Loss: 0.00022083
Iteration 364/1000 | Loss: 0.00023508
Iteration 365/1000 | Loss: 0.00020550
Iteration 366/1000 | Loss: 0.00021884
Iteration 367/1000 | Loss: 0.00020743
Iteration 368/1000 | Loss: 0.00023952
Iteration 369/1000 | Loss: 0.00012275
Iteration 370/1000 | Loss: 0.00005988
Iteration 371/1000 | Loss: 0.00013535
Iteration 372/1000 | Loss: 0.00010999
Iteration 373/1000 | Loss: 0.00011513
Iteration 374/1000 | Loss: 0.00006077
Iteration 375/1000 | Loss: 0.00006718
Iteration 376/1000 | Loss: 0.00007892
Iteration 377/1000 | Loss: 0.00015615
Iteration 378/1000 | Loss: 0.00008082
Iteration 379/1000 | Loss: 0.00005784
Iteration 380/1000 | Loss: 0.00005952
Iteration 381/1000 | Loss: 0.00009377
Iteration 382/1000 | Loss: 0.00014426
Iteration 383/1000 | Loss: 0.00020475
Iteration 384/1000 | Loss: 0.00017497
Iteration 385/1000 | Loss: 0.00016455
Iteration 386/1000 | Loss: 0.00010135
Iteration 387/1000 | Loss: 0.00025291
Iteration 388/1000 | Loss: 0.00006970
Iteration 389/1000 | Loss: 0.00008848
Iteration 390/1000 | Loss: 0.00026520
Iteration 391/1000 | Loss: 0.00014228
Iteration 392/1000 | Loss: 0.00021259
Iteration 393/1000 | Loss: 0.00023661
Iteration 394/1000 | Loss: 0.00011757
Iteration 395/1000 | Loss: 0.00023522
Iteration 396/1000 | Loss: 0.00009003
Iteration 397/1000 | Loss: 0.00009462
Iteration 398/1000 | Loss: 0.00004274
Iteration 399/1000 | Loss: 0.00007286
Iteration 400/1000 | Loss: 0.00004312
Iteration 401/1000 | Loss: 0.00006741
Iteration 402/1000 | Loss: 0.00003609
Iteration 403/1000 | Loss: 0.00003520
Iteration 404/1000 | Loss: 0.00003453
Iteration 405/1000 | Loss: 0.00003419
Iteration 406/1000 | Loss: 0.00007947
Iteration 407/1000 | Loss: 0.00005039
Iteration 408/1000 | Loss: 0.00003857
Iteration 409/1000 | Loss: 0.00003447
Iteration 410/1000 | Loss: 0.00003323
Iteration 411/1000 | Loss: 0.00003259
Iteration 412/1000 | Loss: 0.00003259
Iteration 413/1000 | Loss: 0.00003181
Iteration 414/1000 | Loss: 0.00003170
Iteration 415/1000 | Loss: 0.00003149
Iteration 416/1000 | Loss: 0.00003137
Iteration 417/1000 | Loss: 0.00003121
Iteration 418/1000 | Loss: 0.00003153
Iteration 419/1000 | Loss: 0.00003131
Iteration 420/1000 | Loss: 0.00003130
Iteration 421/1000 | Loss: 0.00003113
Iteration 422/1000 | Loss: 0.00003112
Iteration 423/1000 | Loss: 0.00003112
Iteration 424/1000 | Loss: 0.00003111
Iteration 425/1000 | Loss: 0.00003111
Iteration 426/1000 | Loss: 0.00003111
Iteration 427/1000 | Loss: 0.00003111
Iteration 428/1000 | Loss: 0.00003111
Iteration 429/1000 | Loss: 0.00003111
Iteration 430/1000 | Loss: 0.00003110
Iteration 431/1000 | Loss: 0.00003109
Iteration 432/1000 | Loss: 0.00003109
Iteration 433/1000 | Loss: 0.00003109
Iteration 434/1000 | Loss: 0.00003108
Iteration 435/1000 | Loss: 0.00003108
Iteration 436/1000 | Loss: 0.00003161
Iteration 437/1000 | Loss: 0.00003120
Iteration 438/1000 | Loss: 0.00003105
Iteration 439/1000 | Loss: 0.00003105
Iteration 440/1000 | Loss: 0.00003105
Iteration 441/1000 | Loss: 0.00003105
Iteration 442/1000 | Loss: 0.00003104
Iteration 443/1000 | Loss: 0.00003104
Iteration 444/1000 | Loss: 0.00003104
Iteration 445/1000 | Loss: 0.00003104
Iteration 446/1000 | Loss: 0.00003104
Iteration 447/1000 | Loss: 0.00003104
Iteration 448/1000 | Loss: 0.00003104
Iteration 449/1000 | Loss: 0.00003104
Iteration 450/1000 | Loss: 0.00003104
Iteration 451/1000 | Loss: 0.00003103
Iteration 452/1000 | Loss: 0.00003158
Iteration 453/1000 | Loss: 0.00003117
Iteration 454/1000 | Loss: 0.00003117
Iteration 455/1000 | Loss: 0.00003117
Iteration 456/1000 | Loss: 0.00003117
Iteration 457/1000 | Loss: 0.00003116
Iteration 458/1000 | Loss: 0.00003115
Iteration 459/1000 | Loss: 0.00003115
Iteration 460/1000 | Loss: 0.00003115
Iteration 461/1000 | Loss: 0.00003114
Iteration 462/1000 | Loss: 0.00003114
Iteration 463/1000 | Loss: 0.00003113
Iteration 464/1000 | Loss: 0.00003113
Iteration 465/1000 | Loss: 0.00003111
Iteration 466/1000 | Loss: 0.00003110
Iteration 467/1000 | Loss: 0.00003110
Iteration 468/1000 | Loss: 0.00003109
Iteration 469/1000 | Loss: 0.00003109
Iteration 470/1000 | Loss: 0.00003109
Iteration 471/1000 | Loss: 0.00003109
Iteration 472/1000 | Loss: 0.00003109
Iteration 473/1000 | Loss: 0.00003108
Iteration 474/1000 | Loss: 0.00003108
Iteration 475/1000 | Loss: 0.00003108
Iteration 476/1000 | Loss: 0.00003108
Iteration 477/1000 | Loss: 0.00003107
Iteration 478/1000 | Loss: 0.00003107
Iteration 479/1000 | Loss: 0.00003107
Iteration 480/1000 | Loss: 0.00003107
Iteration 481/1000 | Loss: 0.00003106
Iteration 482/1000 | Loss: 0.00003106
Iteration 483/1000 | Loss: 0.00003106
Iteration 484/1000 | Loss: 0.00003106
Iteration 485/1000 | Loss: 0.00003106
Iteration 486/1000 | Loss: 0.00003110
Iteration 487/1000 | Loss: 0.00003130
Iteration 488/1000 | Loss: 0.00003105
Iteration 489/1000 | Loss: 0.00003141
Iteration 490/1000 | Loss: 0.00003116
Iteration 491/1000 | Loss: 0.00003116
Iteration 492/1000 | Loss: 0.00003120
Iteration 493/1000 | Loss: 0.00003108
Iteration 494/1000 | Loss: 0.00003153
Iteration 495/1000 | Loss: 0.00003111
Iteration 496/1000 | Loss: 0.00003109
Iteration 497/1000 | Loss: 0.00003143
Iteration 498/1000 | Loss: 0.00003110
Iteration 499/1000 | Loss: 0.00003129
Iteration 500/1000 | Loss: 0.00003103
Iteration 501/1000 | Loss: 0.00003102
Iteration 502/1000 | Loss: 0.00003102
Iteration 503/1000 | Loss: 0.00003102
Iteration 504/1000 | Loss: 0.00003101
Iteration 505/1000 | Loss: 0.00003112
Iteration 506/1000 | Loss: 0.00003133
Iteration 507/1000 | Loss: 0.00003101
Iteration 508/1000 | Loss: 0.00003112
Iteration 509/1000 | Loss: 0.00003147
Iteration 510/1000 | Loss: 0.00003111
Iteration 511/1000 | Loss: 0.00003111
Iteration 512/1000 | Loss: 0.00003111
Iteration 513/1000 | Loss: 0.00003110
Iteration 514/1000 | Loss: 0.00010939
Iteration 515/1000 | Loss: 0.00012904
Iteration 516/1000 | Loss: 0.00004744
Iteration 517/1000 | Loss: 0.00003753
Iteration 518/1000 | Loss: 0.00003619
Iteration 519/1000 | Loss: 0.00003424
Iteration 520/1000 | Loss: 0.00003210
Iteration 521/1000 | Loss: 0.00025745
Iteration 522/1000 | Loss: 0.00005406
Iteration 523/1000 | Loss: 0.00003755
Iteration 524/1000 | Loss: 0.00003424
Iteration 525/1000 | Loss: 0.00003219
Iteration 526/1000 | Loss: 0.00003317
Iteration 527/1000 | Loss: 0.00015744
Iteration 528/1000 | Loss: 0.00014349
Iteration 529/1000 | Loss: 0.00003896
Iteration 530/1000 | Loss: 0.00003714
Iteration 531/1000 | Loss: 0.00003433
Iteration 532/1000 | Loss: 0.00003597
Iteration 533/1000 | Loss: 0.00003356
Iteration 534/1000 | Loss: 0.00003315
Iteration 535/1000 | Loss: 0.00008320
Iteration 536/1000 | Loss: 0.00003462
Iteration 537/1000 | Loss: 0.00004015
Iteration 538/1000 | Loss: 0.00004074
Iteration 539/1000 | Loss: 0.00003877
Iteration 540/1000 | Loss: 0.00004044
Iteration 541/1000 | Loss: 0.00003223
Iteration 542/1000 | Loss: 0.00003073
Iteration 543/1000 | Loss: 0.00003000
Iteration 544/1000 | Loss: 0.00002934
Iteration 545/1000 | Loss: 0.00002907
Iteration 546/1000 | Loss: 0.00002899
Iteration 547/1000 | Loss: 0.00002879
Iteration 548/1000 | Loss: 0.00002878
Iteration 549/1000 | Loss: 0.00002878
Iteration 550/1000 | Loss: 0.00002878
Iteration 551/1000 | Loss: 0.00002878
Iteration 552/1000 | Loss: 0.00002878
Iteration 553/1000 | Loss: 0.00002877
Iteration 554/1000 | Loss: 0.00002877
Iteration 555/1000 | Loss: 0.00002879
Iteration 556/1000 | Loss: 0.00002865
Iteration 557/1000 | Loss: 0.00002864
Iteration 558/1000 | Loss: 0.00002884
Iteration 559/1000 | Loss: 0.00002871
Iteration 560/1000 | Loss: 0.00002863
Iteration 561/1000 | Loss: 0.00002863
Iteration 562/1000 | Loss: 0.00002862
Iteration 563/1000 | Loss: 0.00002862
Iteration 564/1000 | Loss: 0.00002862
Iteration 565/1000 | Loss: 0.00002862
Iteration 566/1000 | Loss: 0.00002862
Iteration 567/1000 | Loss: 0.00002862
Iteration 568/1000 | Loss: 0.00002862
Iteration 569/1000 | Loss: 0.00002861
Iteration 570/1000 | Loss: 0.00002861
Iteration 571/1000 | Loss: 0.00002860
Iteration 572/1000 | Loss: 0.00002860
Iteration 573/1000 | Loss: 0.00002860
Iteration 574/1000 | Loss: 0.00002860
Iteration 575/1000 | Loss: 0.00002860
Iteration 576/1000 | Loss: 0.00002860
Iteration 577/1000 | Loss: 0.00002860
Iteration 578/1000 | Loss: 0.00002859
Iteration 579/1000 | Loss: 0.00002859
Iteration 580/1000 | Loss: 0.00002859
Iteration 581/1000 | Loss: 0.00002859
Iteration 582/1000 | Loss: 0.00002894
Iteration 583/1000 | Loss: 0.00002871
Iteration 584/1000 | Loss: 0.00002857
Iteration 585/1000 | Loss: 0.00002857
Iteration 586/1000 | Loss: 0.00002857
Iteration 587/1000 | Loss: 0.00002857
Iteration 588/1000 | Loss: 0.00002857
Iteration 589/1000 | Loss: 0.00002857
Iteration 590/1000 | Loss: 0.00002857
Iteration 591/1000 | Loss: 0.00002857
Iteration 592/1000 | Loss: 0.00002857
Iteration 593/1000 | Loss: 0.00002857
Iteration 594/1000 | Loss: 0.00002857
Iteration 595/1000 | Loss: 0.00002857
Iteration 596/1000 | Loss: 0.00002857
Iteration 597/1000 | Loss: 0.00002857
Iteration 598/1000 | Loss: 0.00002857
Iteration 599/1000 | Loss: 0.00002857
Iteration 600/1000 | Loss: 0.00002857
Iteration 601/1000 | Loss: 0.00002857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 601. Stopping optimization.
Last 5 losses: [2.8570841095643118e-05, 2.8570841095643118e-05, 2.8570841095643118e-05, 2.8570841095643118e-05, 2.8570841095643118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8570841095643118e-05

Optimization complete. Final v2v error: 3.113058090209961 mm

Highest mean error: 12.626005172729492 mm for frame 136

Lowest mean error: 2.4278876781463623 mm for frame 35

Saving results

Total time: 800.9217472076416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938381
Iteration 2/25 | Loss: 0.00178621
Iteration 3/25 | Loss: 0.00136000
Iteration 4/25 | Loss: 0.00128703
Iteration 5/25 | Loss: 0.00126225
Iteration 6/25 | Loss: 0.00120051
Iteration 7/25 | Loss: 0.00116681
Iteration 8/25 | Loss: 0.00114550
Iteration 9/25 | Loss: 0.00113990
Iteration 10/25 | Loss: 0.00112617
Iteration 11/25 | Loss: 0.00113249
Iteration 12/25 | Loss: 0.00112025
Iteration 13/25 | Loss: 0.00112075
Iteration 14/25 | Loss: 0.00110940
Iteration 15/25 | Loss: 0.00110770
Iteration 16/25 | Loss: 0.00110755
Iteration 17/25 | Loss: 0.00110754
Iteration 18/25 | Loss: 0.00110754
Iteration 19/25 | Loss: 0.00110753
Iteration 20/25 | Loss: 0.00110753
Iteration 21/25 | Loss: 0.00110753
Iteration 22/25 | Loss: 0.00110753
Iteration 23/25 | Loss: 0.00110753
Iteration 24/25 | Loss: 0.00110753
Iteration 25/25 | Loss: 0.00110753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34366345
Iteration 2/25 | Loss: 0.00152469
Iteration 3/25 | Loss: 0.00152463
Iteration 4/25 | Loss: 0.00152463
Iteration 5/25 | Loss: 0.00152463
Iteration 6/25 | Loss: 0.00152463
Iteration 7/25 | Loss: 0.00152463
Iteration 8/25 | Loss: 0.00152463
Iteration 9/25 | Loss: 0.00152463
Iteration 10/25 | Loss: 0.00152463
Iteration 11/25 | Loss: 0.00152463
Iteration 12/25 | Loss: 0.00152463
Iteration 13/25 | Loss: 0.00152463
Iteration 14/25 | Loss: 0.00152463
Iteration 15/25 | Loss: 0.00152463
Iteration 16/25 | Loss: 0.00152463
Iteration 17/25 | Loss: 0.00152463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001524626393802464, 0.001524626393802464, 0.001524626393802464, 0.001524626393802464, 0.001524626393802464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001524626393802464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152463
Iteration 2/1000 | Loss: 0.00026715
Iteration 3/1000 | Loss: 0.00024830
Iteration 4/1000 | Loss: 0.00014721
Iteration 5/1000 | Loss: 0.00019299
Iteration 6/1000 | Loss: 0.00013751
Iteration 7/1000 | Loss: 0.00011442
Iteration 8/1000 | Loss: 0.00009585
Iteration 9/1000 | Loss: 0.00007961
Iteration 10/1000 | Loss: 0.00017851
Iteration 11/1000 | Loss: 0.00018431
Iteration 12/1000 | Loss: 0.00010774
Iteration 13/1000 | Loss: 0.00011370
Iteration 14/1000 | Loss: 0.00010716
Iteration 15/1000 | Loss: 0.00006795
Iteration 16/1000 | Loss: 0.00005341
Iteration 17/1000 | Loss: 0.00004766
Iteration 18/1000 | Loss: 0.00004333
Iteration 19/1000 | Loss: 0.00006595
Iteration 20/1000 | Loss: 0.00004364
Iteration 21/1000 | Loss: 0.00004037
Iteration 22/1000 | Loss: 0.00003690
Iteration 23/1000 | Loss: 0.00003511
Iteration 24/1000 | Loss: 0.00003393
Iteration 25/1000 | Loss: 0.00003292
Iteration 26/1000 | Loss: 0.00003167
Iteration 27/1000 | Loss: 0.00003034
Iteration 28/1000 | Loss: 0.00002944
Iteration 29/1000 | Loss: 0.00002873
Iteration 30/1000 | Loss: 0.00002797
Iteration 31/1000 | Loss: 0.00002676
Iteration 32/1000 | Loss: 0.00002578
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002479
Iteration 35/1000 | Loss: 0.00002449
Iteration 36/1000 | Loss: 0.00002429
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002356
Iteration 39/1000 | Loss: 0.00002336
Iteration 40/1000 | Loss: 0.00002334
Iteration 41/1000 | Loss: 0.00002329
Iteration 42/1000 | Loss: 0.00002327
Iteration 43/1000 | Loss: 0.00002327
Iteration 44/1000 | Loss: 0.00002326
Iteration 45/1000 | Loss: 0.00002326
Iteration 46/1000 | Loss: 0.00002326
Iteration 47/1000 | Loss: 0.00002326
Iteration 48/1000 | Loss: 0.00002325
Iteration 49/1000 | Loss: 0.00002325
Iteration 50/1000 | Loss: 0.00002325
Iteration 51/1000 | Loss: 0.00002324
Iteration 52/1000 | Loss: 0.00002324
Iteration 53/1000 | Loss: 0.00002324
Iteration 54/1000 | Loss: 0.00002324
Iteration 55/1000 | Loss: 0.00002324
Iteration 56/1000 | Loss: 0.00002324
Iteration 57/1000 | Loss: 0.00002323
Iteration 58/1000 | Loss: 0.00002323
Iteration 59/1000 | Loss: 0.00002323
Iteration 60/1000 | Loss: 0.00002323
Iteration 61/1000 | Loss: 0.00002323
Iteration 62/1000 | Loss: 0.00002322
Iteration 63/1000 | Loss: 0.00002322
Iteration 64/1000 | Loss: 0.00002322
Iteration 65/1000 | Loss: 0.00002322
Iteration 66/1000 | Loss: 0.00002322
Iteration 67/1000 | Loss: 0.00002321
Iteration 68/1000 | Loss: 0.00002321
Iteration 69/1000 | Loss: 0.00002321
Iteration 70/1000 | Loss: 0.00002321
Iteration 71/1000 | Loss: 0.00002321
Iteration 72/1000 | Loss: 0.00002321
Iteration 73/1000 | Loss: 0.00002320
Iteration 74/1000 | Loss: 0.00002320
Iteration 75/1000 | Loss: 0.00002320
Iteration 76/1000 | Loss: 0.00002320
Iteration 77/1000 | Loss: 0.00002320
Iteration 78/1000 | Loss: 0.00002320
Iteration 79/1000 | Loss: 0.00002320
Iteration 80/1000 | Loss: 0.00002320
Iteration 81/1000 | Loss: 0.00002320
Iteration 82/1000 | Loss: 0.00002320
Iteration 83/1000 | Loss: 0.00002320
Iteration 84/1000 | Loss: 0.00002320
Iteration 85/1000 | Loss: 0.00002320
Iteration 86/1000 | Loss: 0.00002319
Iteration 87/1000 | Loss: 0.00002319
Iteration 88/1000 | Loss: 0.00002319
Iteration 89/1000 | Loss: 0.00002319
Iteration 90/1000 | Loss: 0.00002319
Iteration 91/1000 | Loss: 0.00002319
Iteration 92/1000 | Loss: 0.00002319
Iteration 93/1000 | Loss: 0.00002319
Iteration 94/1000 | Loss: 0.00002319
Iteration 95/1000 | Loss: 0.00002319
Iteration 96/1000 | Loss: 0.00002319
Iteration 97/1000 | Loss: 0.00002318
Iteration 98/1000 | Loss: 0.00002318
Iteration 99/1000 | Loss: 0.00002318
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002318
Iteration 103/1000 | Loss: 0.00002318
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002318
Iteration 107/1000 | Loss: 0.00002318
Iteration 108/1000 | Loss: 0.00002318
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002316
Iteration 118/1000 | Loss: 0.00002316
Iteration 119/1000 | Loss: 0.00002315
Iteration 120/1000 | Loss: 0.00002315
Iteration 121/1000 | Loss: 0.00002315
Iteration 122/1000 | Loss: 0.00002315
Iteration 123/1000 | Loss: 0.00002315
Iteration 124/1000 | Loss: 0.00002315
Iteration 125/1000 | Loss: 0.00002315
Iteration 126/1000 | Loss: 0.00002315
Iteration 127/1000 | Loss: 0.00002315
Iteration 128/1000 | Loss: 0.00002314
Iteration 129/1000 | Loss: 0.00002314
Iteration 130/1000 | Loss: 0.00002314
Iteration 131/1000 | Loss: 0.00002314
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002314
Iteration 135/1000 | Loss: 0.00002314
Iteration 136/1000 | Loss: 0.00002314
Iteration 137/1000 | Loss: 0.00002314
Iteration 138/1000 | Loss: 0.00002314
Iteration 139/1000 | Loss: 0.00002314
Iteration 140/1000 | Loss: 0.00002314
Iteration 141/1000 | Loss: 0.00002314
Iteration 142/1000 | Loss: 0.00002313
Iteration 143/1000 | Loss: 0.00002313
Iteration 144/1000 | Loss: 0.00002313
Iteration 145/1000 | Loss: 0.00002313
Iteration 146/1000 | Loss: 0.00002313
Iteration 147/1000 | Loss: 0.00002313
Iteration 148/1000 | Loss: 0.00002313
Iteration 149/1000 | Loss: 0.00002313
Iteration 150/1000 | Loss: 0.00002313
Iteration 151/1000 | Loss: 0.00002313
Iteration 152/1000 | Loss: 0.00002312
Iteration 153/1000 | Loss: 0.00002312
Iteration 154/1000 | Loss: 0.00002312
Iteration 155/1000 | Loss: 0.00002311
Iteration 156/1000 | Loss: 0.00002311
Iteration 157/1000 | Loss: 0.00002311
Iteration 158/1000 | Loss: 0.00002311
Iteration 159/1000 | Loss: 0.00002311
Iteration 160/1000 | Loss: 0.00002311
Iteration 161/1000 | Loss: 0.00002310
Iteration 162/1000 | Loss: 0.00002310
Iteration 163/1000 | Loss: 0.00002310
Iteration 164/1000 | Loss: 0.00002310
Iteration 165/1000 | Loss: 0.00002310
Iteration 166/1000 | Loss: 0.00002310
Iteration 167/1000 | Loss: 0.00002310
Iteration 168/1000 | Loss: 0.00002309
Iteration 169/1000 | Loss: 0.00002309
Iteration 170/1000 | Loss: 0.00002309
Iteration 171/1000 | Loss: 0.00002309
Iteration 172/1000 | Loss: 0.00002309
Iteration 173/1000 | Loss: 0.00002309
Iteration 174/1000 | Loss: 0.00002309
Iteration 175/1000 | Loss: 0.00002309
Iteration 176/1000 | Loss: 0.00002309
Iteration 177/1000 | Loss: 0.00002308
Iteration 178/1000 | Loss: 0.00002308
Iteration 179/1000 | Loss: 0.00002308
Iteration 180/1000 | Loss: 0.00002308
Iteration 181/1000 | Loss: 0.00002308
Iteration 182/1000 | Loss: 0.00002308
Iteration 183/1000 | Loss: 0.00002308
Iteration 184/1000 | Loss: 0.00002308
Iteration 185/1000 | Loss: 0.00002307
Iteration 186/1000 | Loss: 0.00002307
Iteration 187/1000 | Loss: 0.00002307
Iteration 188/1000 | Loss: 0.00002307
Iteration 189/1000 | Loss: 0.00002307
Iteration 190/1000 | Loss: 0.00002307
Iteration 191/1000 | Loss: 0.00002307
Iteration 192/1000 | Loss: 0.00002307
Iteration 193/1000 | Loss: 0.00002307
Iteration 194/1000 | Loss: 0.00002307
Iteration 195/1000 | Loss: 0.00002307
Iteration 196/1000 | Loss: 0.00002307
Iteration 197/1000 | Loss: 0.00002306
Iteration 198/1000 | Loss: 0.00002306
Iteration 199/1000 | Loss: 0.00002306
Iteration 200/1000 | Loss: 0.00002306
Iteration 201/1000 | Loss: 0.00002306
Iteration 202/1000 | Loss: 0.00002306
Iteration 203/1000 | Loss: 0.00002306
Iteration 204/1000 | Loss: 0.00002306
Iteration 205/1000 | Loss: 0.00002306
Iteration 206/1000 | Loss: 0.00002306
Iteration 207/1000 | Loss: 0.00002306
Iteration 208/1000 | Loss: 0.00002305
Iteration 209/1000 | Loss: 0.00002305
Iteration 210/1000 | Loss: 0.00002305
Iteration 211/1000 | Loss: 0.00002305
Iteration 212/1000 | Loss: 0.00002305
Iteration 213/1000 | Loss: 0.00002305
Iteration 214/1000 | Loss: 0.00002305
Iteration 215/1000 | Loss: 0.00002305
Iteration 216/1000 | Loss: 0.00002305
Iteration 217/1000 | Loss: 0.00002305
Iteration 218/1000 | Loss: 0.00002305
Iteration 219/1000 | Loss: 0.00002304
Iteration 220/1000 | Loss: 0.00002304
Iteration 221/1000 | Loss: 0.00002304
Iteration 222/1000 | Loss: 0.00002304
Iteration 223/1000 | Loss: 0.00002304
Iteration 224/1000 | Loss: 0.00002304
Iteration 225/1000 | Loss: 0.00002304
Iteration 226/1000 | Loss: 0.00002303
Iteration 227/1000 | Loss: 0.00002303
Iteration 228/1000 | Loss: 0.00002303
Iteration 229/1000 | Loss: 0.00002303
Iteration 230/1000 | Loss: 0.00002303
Iteration 231/1000 | Loss: 0.00002303
Iteration 232/1000 | Loss: 0.00002303
Iteration 233/1000 | Loss: 0.00002303
Iteration 234/1000 | Loss: 0.00002303
Iteration 235/1000 | Loss: 0.00002303
Iteration 236/1000 | Loss: 0.00002303
Iteration 237/1000 | Loss: 0.00002303
Iteration 238/1000 | Loss: 0.00002303
Iteration 239/1000 | Loss: 0.00002303
Iteration 240/1000 | Loss: 0.00002302
Iteration 241/1000 | Loss: 0.00002302
Iteration 242/1000 | Loss: 0.00002302
Iteration 243/1000 | Loss: 0.00002302
Iteration 244/1000 | Loss: 0.00002302
Iteration 245/1000 | Loss: 0.00002302
Iteration 246/1000 | Loss: 0.00002302
Iteration 247/1000 | Loss: 0.00002302
Iteration 248/1000 | Loss: 0.00002302
Iteration 249/1000 | Loss: 0.00002302
Iteration 250/1000 | Loss: 0.00002302
Iteration 251/1000 | Loss: 0.00002302
Iteration 252/1000 | Loss: 0.00002302
Iteration 253/1000 | Loss: 0.00002302
Iteration 254/1000 | Loss: 0.00002302
Iteration 255/1000 | Loss: 0.00002302
Iteration 256/1000 | Loss: 0.00002302
Iteration 257/1000 | Loss: 0.00002302
Iteration 258/1000 | Loss: 0.00002302
Iteration 259/1000 | Loss: 0.00002302
Iteration 260/1000 | Loss: 0.00002301
Iteration 261/1000 | Loss: 0.00002301
Iteration 262/1000 | Loss: 0.00002301
Iteration 263/1000 | Loss: 0.00002301
Iteration 264/1000 | Loss: 0.00002301
Iteration 265/1000 | Loss: 0.00002301
Iteration 266/1000 | Loss: 0.00002301
Iteration 267/1000 | Loss: 0.00002301
Iteration 268/1000 | Loss: 0.00002301
Iteration 269/1000 | Loss: 0.00002301
Iteration 270/1000 | Loss: 0.00002301
Iteration 271/1000 | Loss: 0.00002301
Iteration 272/1000 | Loss: 0.00002301
Iteration 273/1000 | Loss: 0.00002301
Iteration 274/1000 | Loss: 0.00002301
Iteration 275/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.301392305525951e-05, 2.301392305525951e-05, 2.301392305525951e-05, 2.301392305525951e-05, 2.301392305525951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.301392305525951e-05

Optimization complete. Final v2v error: 3.658547878265381 mm

Highest mean error: 6.180165767669678 mm for frame 111

Lowest mean error: 2.6637825965881348 mm for frame 72

Saving results

Total time: 103.98179936408997
