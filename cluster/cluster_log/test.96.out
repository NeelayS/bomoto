Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=96, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5376-5431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046369
Iteration 2/25 | Loss: 0.00170785
Iteration 3/25 | Loss: 0.00148616
Iteration 4/25 | Loss: 0.00141663
Iteration 5/25 | Loss: 0.00140872
Iteration 6/25 | Loss: 0.00139175
Iteration 7/25 | Loss: 0.00137594
Iteration 8/25 | Loss: 0.00137263
Iteration 9/25 | Loss: 0.00137212
Iteration 10/25 | Loss: 0.00137143
Iteration 11/25 | Loss: 0.00137040
Iteration 12/25 | Loss: 0.00136958
Iteration 13/25 | Loss: 0.00136946
Iteration 14/25 | Loss: 0.00136946
Iteration 15/25 | Loss: 0.00136945
Iteration 16/25 | Loss: 0.00136945
Iteration 17/25 | Loss: 0.00136945
Iteration 18/25 | Loss: 0.00136944
Iteration 19/25 | Loss: 0.00136944
Iteration 20/25 | Loss: 0.00136944
Iteration 21/25 | Loss: 0.00136944
Iteration 22/25 | Loss: 0.00136944
Iteration 23/25 | Loss: 0.00136944
Iteration 24/25 | Loss: 0.00136944
Iteration 25/25 | Loss: 0.00136944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.02221107
Iteration 2/25 | Loss: 0.00199648
Iteration 3/25 | Loss: 0.00199134
Iteration 4/25 | Loss: 0.00199134
Iteration 5/25 | Loss: 0.00199134
Iteration 6/25 | Loss: 0.00199134
Iteration 7/25 | Loss: 0.00199134
Iteration 8/25 | Loss: 0.00199134
Iteration 9/25 | Loss: 0.00199134
Iteration 10/25 | Loss: 0.00199134
Iteration 11/25 | Loss: 0.00199134
Iteration 12/25 | Loss: 0.00199134
Iteration 13/25 | Loss: 0.00199134
Iteration 14/25 | Loss: 0.00199134
Iteration 15/25 | Loss: 0.00199134
Iteration 16/25 | Loss: 0.00199134
Iteration 17/25 | Loss: 0.00199134
Iteration 18/25 | Loss: 0.00199134
Iteration 19/25 | Loss: 0.00199134
Iteration 20/25 | Loss: 0.00199134
Iteration 21/25 | Loss: 0.00199134
Iteration 22/25 | Loss: 0.00199134
Iteration 23/25 | Loss: 0.00199134
Iteration 24/25 | Loss: 0.00199134
Iteration 25/25 | Loss: 0.00199134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199134
Iteration 2/1000 | Loss: 0.00009894
Iteration 3/1000 | Loss: 0.00006745
Iteration 4/1000 | Loss: 0.00010989
Iteration 5/1000 | Loss: 0.00004681
Iteration 6/1000 | Loss: 0.00004423
Iteration 7/1000 | Loss: 0.00008710
Iteration 8/1000 | Loss: 0.00005906
Iteration 9/1000 | Loss: 0.00004099
Iteration 10/1000 | Loss: 0.00004012
Iteration 11/1000 | Loss: 0.00004228
Iteration 12/1000 | Loss: 0.00003892
Iteration 13/1000 | Loss: 0.00003841
Iteration 14/1000 | Loss: 0.00003805
Iteration 15/1000 | Loss: 0.00003774
Iteration 16/1000 | Loss: 0.00003744
Iteration 17/1000 | Loss: 0.00003723
Iteration 18/1000 | Loss: 0.00003706
Iteration 19/1000 | Loss: 0.00003699
Iteration 20/1000 | Loss: 0.00003686
Iteration 21/1000 | Loss: 0.00003678
Iteration 22/1000 | Loss: 0.00003663
Iteration 23/1000 | Loss: 0.00003658
Iteration 24/1000 | Loss: 0.00003657
Iteration 25/1000 | Loss: 0.00003656
Iteration 26/1000 | Loss: 0.00003655
Iteration 27/1000 | Loss: 0.00003655
Iteration 28/1000 | Loss: 0.00003653
Iteration 29/1000 | Loss: 0.00003651
Iteration 30/1000 | Loss: 0.00003647
Iteration 31/1000 | Loss: 0.00003639
Iteration 32/1000 | Loss: 0.00003632
Iteration 33/1000 | Loss: 0.00003632
Iteration 34/1000 | Loss: 0.00003632
Iteration 35/1000 | Loss: 0.00011901
Iteration 36/1000 | Loss: 0.00004932
Iteration 37/1000 | Loss: 0.00003635
Iteration 38/1000 | Loss: 0.00005359
Iteration 39/1000 | Loss: 0.00007194
Iteration 40/1000 | Loss: 0.00004727
Iteration 41/1000 | Loss: 0.00003874
Iteration 42/1000 | Loss: 0.00003664
Iteration 43/1000 | Loss: 0.00003623
Iteration 44/1000 | Loss: 0.00003622
Iteration 45/1000 | Loss: 0.00003622
Iteration 46/1000 | Loss: 0.00003622
Iteration 47/1000 | Loss: 0.00003622
Iteration 48/1000 | Loss: 0.00003621
Iteration 49/1000 | Loss: 0.00003621
Iteration 50/1000 | Loss: 0.00003621
Iteration 51/1000 | Loss: 0.00003621
Iteration 52/1000 | Loss: 0.00003614
Iteration 53/1000 | Loss: 0.00003614
Iteration 54/1000 | Loss: 0.00005642
Iteration 55/1000 | Loss: 0.00003662
Iteration 56/1000 | Loss: 0.00003621
Iteration 57/1000 | Loss: 0.00003621
Iteration 58/1000 | Loss: 0.00003621
Iteration 59/1000 | Loss: 0.00003621
Iteration 60/1000 | Loss: 0.00003621
Iteration 61/1000 | Loss: 0.00003620
Iteration 62/1000 | Loss: 0.00003620
Iteration 63/1000 | Loss: 0.00003620
Iteration 64/1000 | Loss: 0.00003619
Iteration 65/1000 | Loss: 0.00003619
Iteration 66/1000 | Loss: 0.00003619
Iteration 67/1000 | Loss: 0.00003619
Iteration 68/1000 | Loss: 0.00003619
Iteration 69/1000 | Loss: 0.00003619
Iteration 70/1000 | Loss: 0.00003618
Iteration 71/1000 | Loss: 0.00003618
Iteration 72/1000 | Loss: 0.00003618
Iteration 73/1000 | Loss: 0.00003618
Iteration 74/1000 | Loss: 0.00003618
Iteration 75/1000 | Loss: 0.00003618
Iteration 76/1000 | Loss: 0.00003618
Iteration 77/1000 | Loss: 0.00003618
Iteration 78/1000 | Loss: 0.00003618
Iteration 79/1000 | Loss: 0.00003618
Iteration 80/1000 | Loss: 0.00003616
Iteration 81/1000 | Loss: 0.00003616
Iteration 82/1000 | Loss: 0.00003615
Iteration 83/1000 | Loss: 0.00003614
Iteration 84/1000 | Loss: 0.00003612
Iteration 85/1000 | Loss: 0.00004216
Iteration 86/1000 | Loss: 0.00003644
Iteration 87/1000 | Loss: 0.00003614
Iteration 88/1000 | Loss: 0.00003652
Iteration 89/1000 | Loss: 0.00003610
Iteration 90/1000 | Loss: 0.00003610
Iteration 91/1000 | Loss: 0.00003610
Iteration 92/1000 | Loss: 0.00003610
Iteration 93/1000 | Loss: 0.00003610
Iteration 94/1000 | Loss: 0.00003610
Iteration 95/1000 | Loss: 0.00003610
Iteration 96/1000 | Loss: 0.00003609
Iteration 97/1000 | Loss: 0.00003609
Iteration 98/1000 | Loss: 0.00003609
Iteration 99/1000 | Loss: 0.00003609
Iteration 100/1000 | Loss: 0.00003609
Iteration 101/1000 | Loss: 0.00003609
Iteration 102/1000 | Loss: 0.00003608
Iteration 103/1000 | Loss: 0.00003608
Iteration 104/1000 | Loss: 0.00003608
Iteration 105/1000 | Loss: 0.00003607
Iteration 106/1000 | Loss: 0.00003607
Iteration 107/1000 | Loss: 0.00003607
Iteration 108/1000 | Loss: 0.00003607
Iteration 109/1000 | Loss: 0.00003607
Iteration 110/1000 | Loss: 0.00003606
Iteration 111/1000 | Loss: 0.00003606
Iteration 112/1000 | Loss: 0.00003606
Iteration 113/1000 | Loss: 0.00003605
Iteration 114/1000 | Loss: 0.00003605
Iteration 115/1000 | Loss: 0.00003605
Iteration 116/1000 | Loss: 0.00003605
Iteration 117/1000 | Loss: 0.00003605
Iteration 118/1000 | Loss: 0.00003605
Iteration 119/1000 | Loss: 0.00003604
Iteration 120/1000 | Loss: 0.00003604
Iteration 121/1000 | Loss: 0.00003604
Iteration 122/1000 | Loss: 0.00003603
Iteration 123/1000 | Loss: 0.00003603
Iteration 124/1000 | Loss: 0.00003603
Iteration 125/1000 | Loss: 0.00003603
Iteration 126/1000 | Loss: 0.00003603
Iteration 127/1000 | Loss: 0.00003602
Iteration 128/1000 | Loss: 0.00003602
Iteration 129/1000 | Loss: 0.00003602
Iteration 130/1000 | Loss: 0.00003602
Iteration 131/1000 | Loss: 0.00003601
Iteration 132/1000 | Loss: 0.00003601
Iteration 133/1000 | Loss: 0.00003601
Iteration 134/1000 | Loss: 0.00003600
Iteration 135/1000 | Loss: 0.00003600
Iteration 136/1000 | Loss: 0.00003600
Iteration 137/1000 | Loss: 0.00003600
Iteration 138/1000 | Loss: 0.00003599
Iteration 139/1000 | Loss: 0.00003599
Iteration 140/1000 | Loss: 0.00003599
Iteration 141/1000 | Loss: 0.00003599
Iteration 142/1000 | Loss: 0.00003598
Iteration 143/1000 | Loss: 0.00003598
Iteration 144/1000 | Loss: 0.00003598
Iteration 145/1000 | Loss: 0.00003598
Iteration 146/1000 | Loss: 0.00003598
Iteration 147/1000 | Loss: 0.00003598
Iteration 148/1000 | Loss: 0.00003598
Iteration 149/1000 | Loss: 0.00003598
Iteration 150/1000 | Loss: 0.00003597
Iteration 151/1000 | Loss: 0.00003597
Iteration 152/1000 | Loss: 0.00003597
Iteration 153/1000 | Loss: 0.00003597
Iteration 154/1000 | Loss: 0.00003597
Iteration 155/1000 | Loss: 0.00003597
Iteration 156/1000 | Loss: 0.00003596
Iteration 157/1000 | Loss: 0.00003596
Iteration 158/1000 | Loss: 0.00003596
Iteration 159/1000 | Loss: 0.00003595
Iteration 160/1000 | Loss: 0.00003595
Iteration 161/1000 | Loss: 0.00003595
Iteration 162/1000 | Loss: 0.00003595
Iteration 163/1000 | Loss: 0.00003594
Iteration 164/1000 | Loss: 0.00003594
Iteration 165/1000 | Loss: 0.00003594
Iteration 166/1000 | Loss: 0.00003594
Iteration 167/1000 | Loss: 0.00003594
Iteration 168/1000 | Loss: 0.00003594
Iteration 169/1000 | Loss: 0.00003594
Iteration 170/1000 | Loss: 0.00003594
Iteration 171/1000 | Loss: 0.00003594
Iteration 172/1000 | Loss: 0.00003594
Iteration 173/1000 | Loss: 0.00003594
Iteration 174/1000 | Loss: 0.00003594
Iteration 175/1000 | Loss: 0.00003593
Iteration 176/1000 | Loss: 0.00003593
Iteration 177/1000 | Loss: 0.00003593
Iteration 178/1000 | Loss: 0.00003593
Iteration 179/1000 | Loss: 0.00003593
Iteration 180/1000 | Loss: 0.00003593
Iteration 181/1000 | Loss: 0.00003593
Iteration 182/1000 | Loss: 0.00003593
Iteration 183/1000 | Loss: 0.00003593
Iteration 184/1000 | Loss: 0.00003593
Iteration 185/1000 | Loss: 0.00003593
Iteration 186/1000 | Loss: 0.00003593
Iteration 187/1000 | Loss: 0.00003593
Iteration 188/1000 | Loss: 0.00003593
Iteration 189/1000 | Loss: 0.00003593
Iteration 190/1000 | Loss: 0.00003593
Iteration 191/1000 | Loss: 0.00003593
Iteration 192/1000 | Loss: 0.00003593
Iteration 193/1000 | Loss: 0.00003593
Iteration 194/1000 | Loss: 0.00003592
Iteration 195/1000 | Loss: 0.00003592
Iteration 196/1000 | Loss: 0.00003592
Iteration 197/1000 | Loss: 0.00003592
Iteration 198/1000 | Loss: 0.00003592
Iteration 199/1000 | Loss: 0.00003592
Iteration 200/1000 | Loss: 0.00003592
Iteration 201/1000 | Loss: 0.00003592
Iteration 202/1000 | Loss: 0.00003592
Iteration 203/1000 | Loss: 0.00003592
Iteration 204/1000 | Loss: 0.00003592
Iteration 205/1000 | Loss: 0.00003592
Iteration 206/1000 | Loss: 0.00003592
Iteration 207/1000 | Loss: 0.00003592
Iteration 208/1000 | Loss: 0.00003592
Iteration 209/1000 | Loss: 0.00003592
Iteration 210/1000 | Loss: 0.00003592
Iteration 211/1000 | Loss: 0.00003592
Iteration 212/1000 | Loss: 0.00003592
Iteration 213/1000 | Loss: 0.00003592
Iteration 214/1000 | Loss: 0.00003592
Iteration 215/1000 | Loss: 0.00003592
Iteration 216/1000 | Loss: 0.00003592
Iteration 217/1000 | Loss: 0.00003592
Iteration 218/1000 | Loss: 0.00003592
Iteration 219/1000 | Loss: 0.00003592
Iteration 220/1000 | Loss: 0.00003592
Iteration 221/1000 | Loss: 0.00003592
Iteration 222/1000 | Loss: 0.00003592
Iteration 223/1000 | Loss: 0.00003592
Iteration 224/1000 | Loss: 0.00003592
Iteration 225/1000 | Loss: 0.00003592
Iteration 226/1000 | Loss: 0.00003592
Iteration 227/1000 | Loss: 0.00003592
Iteration 228/1000 | Loss: 0.00003592
Iteration 229/1000 | Loss: 0.00003592
Iteration 230/1000 | Loss: 0.00003592
Iteration 231/1000 | Loss: 0.00003592
Iteration 232/1000 | Loss: 0.00003592
Iteration 233/1000 | Loss: 0.00003592
Iteration 234/1000 | Loss: 0.00003592
Iteration 235/1000 | Loss: 0.00003592
Iteration 236/1000 | Loss: 0.00003592
Iteration 237/1000 | Loss: 0.00003592
Iteration 238/1000 | Loss: 0.00003592
Iteration 239/1000 | Loss: 0.00003592
Iteration 240/1000 | Loss: 0.00003592
Iteration 241/1000 | Loss: 0.00003592
Iteration 242/1000 | Loss: 0.00003592
Iteration 243/1000 | Loss: 0.00003592
Iteration 244/1000 | Loss: 0.00003592
Iteration 245/1000 | Loss: 0.00003592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [3.5917233617510647e-05, 3.5917233617510647e-05, 3.5917233617510647e-05, 3.5917233617510647e-05, 3.5917233617510647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5917233617510647e-05

Optimization complete. Final v2v error: 4.811419486999512 mm

Highest mean error: 7.343645095825195 mm for frame 99

Lowest mean error: 3.275104522705078 mm for frame 140

Saving results

Total time: 89.15784859657288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476529
Iteration 2/25 | Loss: 0.00137277
Iteration 3/25 | Loss: 0.00129798
Iteration 4/25 | Loss: 0.00128920
Iteration 5/25 | Loss: 0.00128790
Iteration 6/25 | Loss: 0.00128790
Iteration 7/25 | Loss: 0.00128790
Iteration 8/25 | Loss: 0.00128790
Iteration 9/25 | Loss: 0.00128790
Iteration 10/25 | Loss: 0.00128790
Iteration 11/25 | Loss: 0.00128790
Iteration 12/25 | Loss: 0.00128790
Iteration 13/25 | Loss: 0.00128790
Iteration 14/25 | Loss: 0.00128790
Iteration 15/25 | Loss: 0.00128790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00128790398593992, 0.00128790398593992, 0.00128790398593992, 0.00128790398593992, 0.00128790398593992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00128790398593992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.96904039
Iteration 2/25 | Loss: 0.00128090
Iteration 3/25 | Loss: 0.00128090
Iteration 4/25 | Loss: 0.00128089
Iteration 5/25 | Loss: 0.00128089
Iteration 6/25 | Loss: 0.00128089
Iteration 7/25 | Loss: 0.00128089
Iteration 8/25 | Loss: 0.00128089
Iteration 9/25 | Loss: 0.00128089
Iteration 10/25 | Loss: 0.00128089
Iteration 11/25 | Loss: 0.00128089
Iteration 12/25 | Loss: 0.00128089
Iteration 13/25 | Loss: 0.00128089
Iteration 14/25 | Loss: 0.00128089
Iteration 15/25 | Loss: 0.00128089
Iteration 16/25 | Loss: 0.00128089
Iteration 17/25 | Loss: 0.00128089
Iteration 18/25 | Loss: 0.00128089
Iteration 19/25 | Loss: 0.00128089
Iteration 20/25 | Loss: 0.00128089
Iteration 21/25 | Loss: 0.00128089
Iteration 22/25 | Loss: 0.00128089
Iteration 23/25 | Loss: 0.00128089
Iteration 24/25 | Loss: 0.00128089
Iteration 25/25 | Loss: 0.00128089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128089
Iteration 2/1000 | Loss: 0.00002589
Iteration 3/1000 | Loss: 0.00001915
Iteration 4/1000 | Loss: 0.00001735
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001488
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001361
Iteration 16/1000 | Loss: 0.00001354
Iteration 17/1000 | Loss: 0.00001336
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001312
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001308
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001297
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001294
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001294
Iteration 60/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.2942307876073755e-05, 1.2942307876073755e-05, 1.2942307876073755e-05, 1.2942307876073755e-05, 1.2942307876073755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2942307876073755e-05

Optimization complete. Final v2v error: 3.0988051891326904 mm

Highest mean error: 3.3516428470611572 mm for frame 228

Lowest mean error: 2.931605100631714 mm for frame 122

Saving results

Total time: 39.33909320831299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973947
Iteration 2/25 | Loss: 0.00397566
Iteration 3/25 | Loss: 0.00279565
Iteration 4/25 | Loss: 0.00245604
Iteration 5/25 | Loss: 0.00217392
Iteration 6/25 | Loss: 0.00211106
Iteration 7/25 | Loss: 0.00203803
Iteration 8/25 | Loss: 0.00199249
Iteration 9/25 | Loss: 0.00198290
Iteration 10/25 | Loss: 0.00191349
Iteration 11/25 | Loss: 0.00187600
Iteration 12/25 | Loss: 0.00183289
Iteration 13/25 | Loss: 0.00180219
Iteration 14/25 | Loss: 0.00180022
Iteration 15/25 | Loss: 0.00177913
Iteration 16/25 | Loss: 0.00177538
Iteration 17/25 | Loss: 0.00177029
Iteration 18/25 | Loss: 0.00176622
Iteration 19/25 | Loss: 0.00176195
Iteration 20/25 | Loss: 0.00175694
Iteration 21/25 | Loss: 0.00175425
Iteration 22/25 | Loss: 0.00175917
Iteration 23/25 | Loss: 0.00175262
Iteration 24/25 | Loss: 0.00175584
Iteration 25/25 | Loss: 0.00175646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26734149
Iteration 2/25 | Loss: 0.00763017
Iteration 3/25 | Loss: 0.00523071
Iteration 4/25 | Loss: 0.00522786
Iteration 5/25 | Loss: 0.00522786
Iteration 6/25 | Loss: 0.00522786
Iteration 7/25 | Loss: 0.00522786
Iteration 8/25 | Loss: 0.00522786
Iteration 9/25 | Loss: 0.00522786
Iteration 10/25 | Loss: 0.00522786
Iteration 11/25 | Loss: 0.00522786
Iteration 12/25 | Loss: 0.00522786
Iteration 13/25 | Loss: 0.00522786
Iteration 14/25 | Loss: 0.00522786
Iteration 15/25 | Loss: 0.00522786
Iteration 16/25 | Loss: 0.00522786
Iteration 17/25 | Loss: 0.00522786
Iteration 18/25 | Loss: 0.00522786
Iteration 19/25 | Loss: 0.00522786
Iteration 20/25 | Loss: 0.00522786
Iteration 21/25 | Loss: 0.00522786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.005227858666330576, 0.005227858666330576, 0.005227858666330576, 0.005227858666330576, 0.005227858666330576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005227858666330576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00522786
Iteration 2/1000 | Loss: 0.00355840
Iteration 3/1000 | Loss: 0.00099019
Iteration 4/1000 | Loss: 0.00161382
Iteration 5/1000 | Loss: 0.00102157
Iteration 6/1000 | Loss: 0.00106227
Iteration 7/1000 | Loss: 0.00034311
Iteration 8/1000 | Loss: 0.00067305
Iteration 9/1000 | Loss: 0.00102852
Iteration 10/1000 | Loss: 0.00147648
Iteration 11/1000 | Loss: 0.00065484
Iteration 12/1000 | Loss: 0.00104282
Iteration 13/1000 | Loss: 0.00160817
Iteration 14/1000 | Loss: 0.00213033
Iteration 15/1000 | Loss: 0.00072140
Iteration 16/1000 | Loss: 0.00071829
Iteration 17/1000 | Loss: 0.00056778
Iteration 18/1000 | Loss: 0.00033319
Iteration 19/1000 | Loss: 0.00039347
Iteration 20/1000 | Loss: 0.00021549
Iteration 21/1000 | Loss: 0.00034214
Iteration 22/1000 | Loss: 0.00508202
Iteration 23/1000 | Loss: 0.00059604
Iteration 24/1000 | Loss: 0.00074341
Iteration 25/1000 | Loss: 0.00117645
Iteration 26/1000 | Loss: 0.00180475
Iteration 27/1000 | Loss: 0.00151586
Iteration 28/1000 | Loss: 0.00067493
Iteration 29/1000 | Loss: 0.00070765
Iteration 30/1000 | Loss: 0.00069037
Iteration 31/1000 | Loss: 0.00100307
Iteration 32/1000 | Loss: 0.00043482
Iteration 33/1000 | Loss: 0.00081733
Iteration 34/1000 | Loss: 0.00160328
Iteration 35/1000 | Loss: 0.00098708
Iteration 36/1000 | Loss: 0.00050729
Iteration 37/1000 | Loss: 0.00094766
Iteration 38/1000 | Loss: 0.00024406
Iteration 39/1000 | Loss: 0.00038691
Iteration 40/1000 | Loss: 0.00040654
Iteration 41/1000 | Loss: 0.00045294
Iteration 42/1000 | Loss: 0.00060440
Iteration 43/1000 | Loss: 0.00083043
Iteration 44/1000 | Loss: 0.00114640
Iteration 45/1000 | Loss: 0.00053231
Iteration 46/1000 | Loss: 0.00027360
Iteration 47/1000 | Loss: 0.00026010
Iteration 48/1000 | Loss: 0.00035816
Iteration 49/1000 | Loss: 0.00019918
Iteration 50/1000 | Loss: 0.00098379
Iteration 51/1000 | Loss: 0.00061999
Iteration 52/1000 | Loss: 0.00240143
Iteration 53/1000 | Loss: 0.00218832
Iteration 54/1000 | Loss: 0.00071541
Iteration 55/1000 | Loss: 0.00061468
Iteration 56/1000 | Loss: 0.00014475
Iteration 57/1000 | Loss: 0.00011807
Iteration 58/1000 | Loss: 0.00013066
Iteration 59/1000 | Loss: 0.00010923
Iteration 60/1000 | Loss: 0.00021562
Iteration 61/1000 | Loss: 0.00010446
Iteration 62/1000 | Loss: 0.00056611
Iteration 63/1000 | Loss: 0.00020112
Iteration 64/1000 | Loss: 0.00016660
Iteration 65/1000 | Loss: 0.00010276
Iteration 66/1000 | Loss: 0.00009991
Iteration 67/1000 | Loss: 0.00096079
Iteration 68/1000 | Loss: 0.00042355
Iteration 69/1000 | Loss: 0.00035541
Iteration 70/1000 | Loss: 0.00015309
Iteration 71/1000 | Loss: 0.00012468
Iteration 72/1000 | Loss: 0.00012469
Iteration 73/1000 | Loss: 0.00034499
Iteration 74/1000 | Loss: 0.00088198
Iteration 75/1000 | Loss: 0.00032972
Iteration 76/1000 | Loss: 0.00036453
Iteration 77/1000 | Loss: 0.00045030
Iteration 78/1000 | Loss: 0.00012495
Iteration 79/1000 | Loss: 0.00018753
Iteration 80/1000 | Loss: 0.00018958
Iteration 81/1000 | Loss: 0.00036070
Iteration 82/1000 | Loss: 0.00021456
Iteration 83/1000 | Loss: 0.00018052
Iteration 84/1000 | Loss: 0.00026363
Iteration 85/1000 | Loss: 0.00012167
Iteration 86/1000 | Loss: 0.00009303
Iteration 87/1000 | Loss: 0.00009020
Iteration 88/1000 | Loss: 0.00008668
Iteration 89/1000 | Loss: 0.00009110
Iteration 90/1000 | Loss: 0.00033593
Iteration 91/1000 | Loss: 0.00087403
Iteration 92/1000 | Loss: 0.00117379
Iteration 93/1000 | Loss: 0.00247448
Iteration 94/1000 | Loss: 0.00046245
Iteration 95/1000 | Loss: 0.00102786
Iteration 96/1000 | Loss: 0.00034915
Iteration 97/1000 | Loss: 0.00026445
Iteration 98/1000 | Loss: 0.00010040
Iteration 99/1000 | Loss: 0.00251056
Iteration 100/1000 | Loss: 0.00678054
Iteration 101/1000 | Loss: 0.00154016
Iteration 102/1000 | Loss: 0.00202743
Iteration 103/1000 | Loss: 0.00016802
Iteration 104/1000 | Loss: 0.00011337
Iteration 105/1000 | Loss: 0.00013399
Iteration 106/1000 | Loss: 0.00006957
Iteration 107/1000 | Loss: 0.00043147
Iteration 108/1000 | Loss: 0.00031420
Iteration 109/1000 | Loss: 0.00042086
Iteration 110/1000 | Loss: 0.00055280
Iteration 111/1000 | Loss: 0.00057155
Iteration 112/1000 | Loss: 0.00022968
Iteration 113/1000 | Loss: 0.00004955
Iteration 114/1000 | Loss: 0.00008369
Iteration 115/1000 | Loss: 0.00011944
Iteration 116/1000 | Loss: 0.00004215
Iteration 117/1000 | Loss: 0.00021917
Iteration 118/1000 | Loss: 0.00009926
Iteration 119/1000 | Loss: 0.00003733
Iteration 120/1000 | Loss: 0.00007435
Iteration 121/1000 | Loss: 0.00019028
Iteration 122/1000 | Loss: 0.00003456
Iteration 123/1000 | Loss: 0.00003353
Iteration 124/1000 | Loss: 0.00027389
Iteration 125/1000 | Loss: 0.00032260
Iteration 126/1000 | Loss: 0.00049657
Iteration 127/1000 | Loss: 0.00075511
Iteration 128/1000 | Loss: 0.00007442
Iteration 129/1000 | Loss: 0.00041883
Iteration 130/1000 | Loss: 0.00011045
Iteration 131/1000 | Loss: 0.00052155
Iteration 132/1000 | Loss: 0.00003378
Iteration 133/1000 | Loss: 0.00003244
Iteration 134/1000 | Loss: 0.00003162
Iteration 135/1000 | Loss: 0.00006321
Iteration 136/1000 | Loss: 0.00003115
Iteration 137/1000 | Loss: 0.00003075
Iteration 138/1000 | Loss: 0.00015077
Iteration 139/1000 | Loss: 0.00003013
Iteration 140/1000 | Loss: 0.00002972
Iteration 141/1000 | Loss: 0.00002932
Iteration 142/1000 | Loss: 0.00017755
Iteration 143/1000 | Loss: 0.00014445
Iteration 144/1000 | Loss: 0.00009373
Iteration 145/1000 | Loss: 0.00002927
Iteration 146/1000 | Loss: 0.00002859
Iteration 147/1000 | Loss: 0.00002844
Iteration 148/1000 | Loss: 0.00002839
Iteration 149/1000 | Loss: 0.00008965
Iteration 150/1000 | Loss: 0.00002824
Iteration 151/1000 | Loss: 0.00002815
Iteration 152/1000 | Loss: 0.00011569
Iteration 153/1000 | Loss: 0.00011053
Iteration 154/1000 | Loss: 0.00013454
Iteration 155/1000 | Loss: 0.00008626
Iteration 156/1000 | Loss: 0.00002880
Iteration 157/1000 | Loss: 0.00004510
Iteration 158/1000 | Loss: 0.00002819
Iteration 159/1000 | Loss: 0.00004660
Iteration 160/1000 | Loss: 0.00012853
Iteration 161/1000 | Loss: 0.00008702
Iteration 162/1000 | Loss: 0.00004222
Iteration 163/1000 | Loss: 0.00003775
Iteration 164/1000 | Loss: 0.00002947
Iteration 165/1000 | Loss: 0.00002879
Iteration 166/1000 | Loss: 0.00002826
Iteration 167/1000 | Loss: 0.00002804
Iteration 168/1000 | Loss: 0.00002790
Iteration 169/1000 | Loss: 0.00002786
Iteration 170/1000 | Loss: 0.00002770
Iteration 171/1000 | Loss: 0.00002769
Iteration 172/1000 | Loss: 0.00002755
Iteration 173/1000 | Loss: 0.00002755
Iteration 174/1000 | Loss: 0.00002755
Iteration 175/1000 | Loss: 0.00002755
Iteration 176/1000 | Loss: 0.00002755
Iteration 177/1000 | Loss: 0.00002755
Iteration 178/1000 | Loss: 0.00002754
Iteration 179/1000 | Loss: 0.00002753
Iteration 180/1000 | Loss: 0.00002752
Iteration 181/1000 | Loss: 0.00002752
Iteration 182/1000 | Loss: 0.00002752
Iteration 183/1000 | Loss: 0.00002752
Iteration 184/1000 | Loss: 0.00002752
Iteration 185/1000 | Loss: 0.00002752
Iteration 186/1000 | Loss: 0.00002752
Iteration 187/1000 | Loss: 0.00002751
Iteration 188/1000 | Loss: 0.00002751
Iteration 189/1000 | Loss: 0.00002751
Iteration 190/1000 | Loss: 0.00002751
Iteration 191/1000 | Loss: 0.00002751
Iteration 192/1000 | Loss: 0.00002751
Iteration 193/1000 | Loss: 0.00002751
Iteration 194/1000 | Loss: 0.00002751
Iteration 195/1000 | Loss: 0.00002750
Iteration 196/1000 | Loss: 0.00002750
Iteration 197/1000 | Loss: 0.00002750
Iteration 198/1000 | Loss: 0.00002748
Iteration 199/1000 | Loss: 0.00002747
Iteration 200/1000 | Loss: 0.00002747
Iteration 201/1000 | Loss: 0.00002746
Iteration 202/1000 | Loss: 0.00002746
Iteration 203/1000 | Loss: 0.00002746
Iteration 204/1000 | Loss: 0.00002746
Iteration 205/1000 | Loss: 0.00002746
Iteration 206/1000 | Loss: 0.00002746
Iteration 207/1000 | Loss: 0.00002746
Iteration 208/1000 | Loss: 0.00002745
Iteration 209/1000 | Loss: 0.00002745
Iteration 210/1000 | Loss: 0.00002744
Iteration 211/1000 | Loss: 0.00002744
Iteration 212/1000 | Loss: 0.00002744
Iteration 213/1000 | Loss: 0.00002744
Iteration 214/1000 | Loss: 0.00002744
Iteration 215/1000 | Loss: 0.00002743
Iteration 216/1000 | Loss: 0.00002743
Iteration 217/1000 | Loss: 0.00002742
Iteration 218/1000 | Loss: 0.00002742
Iteration 219/1000 | Loss: 0.00002742
Iteration 220/1000 | Loss: 0.00002742
Iteration 221/1000 | Loss: 0.00002742
Iteration 222/1000 | Loss: 0.00002742
Iteration 223/1000 | Loss: 0.00002742
Iteration 224/1000 | Loss: 0.00002742
Iteration 225/1000 | Loss: 0.00002742
Iteration 226/1000 | Loss: 0.00002742
Iteration 227/1000 | Loss: 0.00002741
Iteration 228/1000 | Loss: 0.00002741
Iteration 229/1000 | Loss: 0.00002741
Iteration 230/1000 | Loss: 0.00002741
Iteration 231/1000 | Loss: 0.00002741
Iteration 232/1000 | Loss: 0.00002741
Iteration 233/1000 | Loss: 0.00002741
Iteration 234/1000 | Loss: 0.00002740
Iteration 235/1000 | Loss: 0.00002740
Iteration 236/1000 | Loss: 0.00002740
Iteration 237/1000 | Loss: 0.00002740
Iteration 238/1000 | Loss: 0.00002739
Iteration 239/1000 | Loss: 0.00002739
Iteration 240/1000 | Loss: 0.00002739
Iteration 241/1000 | Loss: 0.00002739
Iteration 242/1000 | Loss: 0.00002739
Iteration 243/1000 | Loss: 0.00002739
Iteration 244/1000 | Loss: 0.00002738
Iteration 245/1000 | Loss: 0.00002738
Iteration 246/1000 | Loss: 0.00002738
Iteration 247/1000 | Loss: 0.00002738
Iteration 248/1000 | Loss: 0.00002738
Iteration 249/1000 | Loss: 0.00002738
Iteration 250/1000 | Loss: 0.00002738
Iteration 251/1000 | Loss: 0.00002738
Iteration 252/1000 | Loss: 0.00002738
Iteration 253/1000 | Loss: 0.00002738
Iteration 254/1000 | Loss: 0.00002738
Iteration 255/1000 | Loss: 0.00002738
Iteration 256/1000 | Loss: 0.00002738
Iteration 257/1000 | Loss: 0.00002738
Iteration 258/1000 | Loss: 0.00002738
Iteration 259/1000 | Loss: 0.00002738
Iteration 260/1000 | Loss: 0.00002738
Iteration 261/1000 | Loss: 0.00002738
Iteration 262/1000 | Loss: 0.00002738
Iteration 263/1000 | Loss: 0.00002738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.7380918254493736e-05, 2.7380918254493736e-05, 2.7380918254493736e-05, 2.7380918254493736e-05, 2.7380918254493736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7380918254493736e-05

Optimization complete. Final v2v error: 3.796302080154419 mm

Highest mean error: 11.497032165527344 mm for frame 54

Lowest mean error: 3.276907444000244 mm for frame 69

Saving results

Total time: 286.5150203704834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953675
Iteration 2/25 | Loss: 0.00305739
Iteration 3/25 | Loss: 0.00226953
Iteration 4/25 | Loss: 0.00209918
Iteration 5/25 | Loss: 0.00200205
Iteration 6/25 | Loss: 0.00192930
Iteration 7/25 | Loss: 0.00190100
Iteration 8/25 | Loss: 0.00187648
Iteration 9/25 | Loss: 0.00174849
Iteration 10/25 | Loss: 0.00171495
Iteration 11/25 | Loss: 0.00165466
Iteration 12/25 | Loss: 0.00162833
Iteration 13/25 | Loss: 0.00162594
Iteration 14/25 | Loss: 0.00160503
Iteration 15/25 | Loss: 0.00159234
Iteration 16/25 | Loss: 0.00159088
Iteration 17/25 | Loss: 0.00157080
Iteration 18/25 | Loss: 0.00154951
Iteration 19/25 | Loss: 0.00154213
Iteration 20/25 | Loss: 0.00154602
Iteration 21/25 | Loss: 0.00154545
Iteration 22/25 | Loss: 0.00154594
Iteration 23/25 | Loss: 0.00154093
Iteration 24/25 | Loss: 0.00152484
Iteration 25/25 | Loss: 0.00153465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24858439
Iteration 2/25 | Loss: 0.00437982
Iteration 3/25 | Loss: 0.00437982
Iteration 4/25 | Loss: 0.00437982
Iteration 5/25 | Loss: 0.00437981
Iteration 6/25 | Loss: 0.00414120
Iteration 7/25 | Loss: 0.00414127
Iteration 8/25 | Loss: 0.00407245
Iteration 9/25 | Loss: 0.00407245
Iteration 10/25 | Loss: 0.00407244
Iteration 11/25 | Loss: 0.00407244
Iteration 12/25 | Loss: 0.00407244
Iteration 13/25 | Loss: 0.00407244
Iteration 14/25 | Loss: 0.00407244
Iteration 15/25 | Loss: 0.00407244
Iteration 16/25 | Loss: 0.00407244
Iteration 17/25 | Loss: 0.00407244
Iteration 18/25 | Loss: 0.00407244
Iteration 19/25 | Loss: 0.00407244
Iteration 20/25 | Loss: 0.00407244
Iteration 21/25 | Loss: 0.00407244
Iteration 22/25 | Loss: 0.00407244
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004072442650794983, 0.004072442650794983, 0.004072442650794983, 0.004072442650794983, 0.004072442650794983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004072442650794983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00407244
Iteration 2/1000 | Loss: 0.00058118
Iteration 3/1000 | Loss: 0.00091543
Iteration 4/1000 | Loss: 0.00021304
Iteration 5/1000 | Loss: 0.00015670
Iteration 6/1000 | Loss: 0.00018799
Iteration 7/1000 | Loss: 0.00012772
Iteration 8/1000 | Loss: 0.00044427
Iteration 9/1000 | Loss: 0.00013745
Iteration 10/1000 | Loss: 0.00020343
Iteration 11/1000 | Loss: 0.00011171
Iteration 12/1000 | Loss: 0.00010575
Iteration 13/1000 | Loss: 0.00010180
Iteration 14/1000 | Loss: 0.00009938
Iteration 15/1000 | Loss: 0.00009798
Iteration 16/1000 | Loss: 0.00009673
Iteration 17/1000 | Loss: 0.00009567
Iteration 18/1000 | Loss: 0.00010250
Iteration 19/1000 | Loss: 0.00009621
Iteration 20/1000 | Loss: 0.00009688
Iteration 21/1000 | Loss: 0.00027581
Iteration 22/1000 | Loss: 0.00024544
Iteration 23/1000 | Loss: 0.00024755
Iteration 24/1000 | Loss: 0.00009720
Iteration 25/1000 | Loss: 0.00009247
Iteration 26/1000 | Loss: 0.00009126
Iteration 27/1000 | Loss: 0.00012170
Iteration 28/1000 | Loss: 0.00009040
Iteration 29/1000 | Loss: 0.00014535
Iteration 30/1000 | Loss: 0.00011630
Iteration 31/1000 | Loss: 0.00014753
Iteration 32/1000 | Loss: 0.00012677
Iteration 33/1000 | Loss: 0.00038439
Iteration 34/1000 | Loss: 0.00037688
Iteration 35/1000 | Loss: 0.00040789
Iteration 36/1000 | Loss: 0.00030747
Iteration 37/1000 | Loss: 0.00012011
Iteration 38/1000 | Loss: 0.00035893
Iteration 39/1000 | Loss: 0.00009367
Iteration 40/1000 | Loss: 0.00009099
Iteration 41/1000 | Loss: 0.00011268
Iteration 42/1000 | Loss: 0.00008859
Iteration 43/1000 | Loss: 0.00008770
Iteration 44/1000 | Loss: 0.00008699
Iteration 45/1000 | Loss: 0.00008652
Iteration 46/1000 | Loss: 0.00015661
Iteration 47/1000 | Loss: 0.00009034
Iteration 48/1000 | Loss: 0.00008736
Iteration 49/1000 | Loss: 0.00026885
Iteration 50/1000 | Loss: 0.00018618
Iteration 51/1000 | Loss: 0.00043541
Iteration 52/1000 | Loss: 0.00025679
Iteration 53/1000 | Loss: 0.00017877
Iteration 54/1000 | Loss: 0.00008688
Iteration 55/1000 | Loss: 0.00014011
Iteration 56/1000 | Loss: 0.00008362
Iteration 57/1000 | Loss: 0.00008203
Iteration 58/1000 | Loss: 0.00008105
Iteration 59/1000 | Loss: 0.00008043
Iteration 60/1000 | Loss: 0.00008003
Iteration 61/1000 | Loss: 0.00007962
Iteration 62/1000 | Loss: 0.00007932
Iteration 63/1000 | Loss: 0.00007901
Iteration 64/1000 | Loss: 0.00007879
Iteration 65/1000 | Loss: 0.00026455
Iteration 66/1000 | Loss: 0.00018526
Iteration 67/1000 | Loss: 0.00008019
Iteration 68/1000 | Loss: 0.00025263
Iteration 69/1000 | Loss: 0.00020465
Iteration 70/1000 | Loss: 0.00007902
Iteration 71/1000 | Loss: 0.00007867
Iteration 72/1000 | Loss: 0.00027632
Iteration 73/1000 | Loss: 0.00019221
Iteration 74/1000 | Loss: 0.00007907
Iteration 75/1000 | Loss: 0.00027650
Iteration 76/1000 | Loss: 0.00009695
Iteration 77/1000 | Loss: 0.00008279
Iteration 78/1000 | Loss: 0.00008047
Iteration 79/1000 | Loss: 0.00007997
Iteration 80/1000 | Loss: 0.00007971
Iteration 81/1000 | Loss: 0.00007951
Iteration 82/1000 | Loss: 0.00007950
Iteration 83/1000 | Loss: 0.00007950
Iteration 84/1000 | Loss: 0.00007950
Iteration 85/1000 | Loss: 0.00007949
Iteration 86/1000 | Loss: 0.00007948
Iteration 87/1000 | Loss: 0.00007948
Iteration 88/1000 | Loss: 0.00007945
Iteration 89/1000 | Loss: 0.00007943
Iteration 90/1000 | Loss: 0.00007942
Iteration 91/1000 | Loss: 0.00007941
Iteration 92/1000 | Loss: 0.00007940
Iteration 93/1000 | Loss: 0.00007939
Iteration 94/1000 | Loss: 0.00007938
Iteration 95/1000 | Loss: 0.00007938
Iteration 96/1000 | Loss: 0.00007937
Iteration 97/1000 | Loss: 0.00007937
Iteration 98/1000 | Loss: 0.00007937
Iteration 99/1000 | Loss: 0.00007936
Iteration 100/1000 | Loss: 0.00007936
Iteration 101/1000 | Loss: 0.00007936
Iteration 102/1000 | Loss: 0.00007936
Iteration 103/1000 | Loss: 0.00007935
Iteration 104/1000 | Loss: 0.00007935
Iteration 105/1000 | Loss: 0.00007935
Iteration 106/1000 | Loss: 0.00007934
Iteration 107/1000 | Loss: 0.00007934
Iteration 108/1000 | Loss: 0.00007934
Iteration 109/1000 | Loss: 0.00007934
Iteration 110/1000 | Loss: 0.00007934
Iteration 111/1000 | Loss: 0.00007934
Iteration 112/1000 | Loss: 0.00007934
Iteration 113/1000 | Loss: 0.00007933
Iteration 114/1000 | Loss: 0.00007932
Iteration 115/1000 | Loss: 0.00007932
Iteration 116/1000 | Loss: 0.00007932
Iteration 117/1000 | Loss: 0.00007932
Iteration 118/1000 | Loss: 0.00007932
Iteration 119/1000 | Loss: 0.00007932
Iteration 120/1000 | Loss: 0.00007931
Iteration 121/1000 | Loss: 0.00007931
Iteration 122/1000 | Loss: 0.00007931
Iteration 123/1000 | Loss: 0.00007928
Iteration 124/1000 | Loss: 0.00007928
Iteration 125/1000 | Loss: 0.00007926
Iteration 126/1000 | Loss: 0.00007926
Iteration 127/1000 | Loss: 0.00007925
Iteration 128/1000 | Loss: 0.00007925
Iteration 129/1000 | Loss: 0.00007924
Iteration 130/1000 | Loss: 0.00028199
Iteration 131/1000 | Loss: 0.00009812
Iteration 132/1000 | Loss: 0.00008189
Iteration 133/1000 | Loss: 0.00007950
Iteration 134/1000 | Loss: 0.00007851
Iteration 135/1000 | Loss: 0.00007798
Iteration 136/1000 | Loss: 0.00007765
Iteration 137/1000 | Loss: 0.00007744
Iteration 138/1000 | Loss: 0.00007742
Iteration 139/1000 | Loss: 0.00007730
Iteration 140/1000 | Loss: 0.00007720
Iteration 141/1000 | Loss: 0.00007695
Iteration 142/1000 | Loss: 0.00007670
Iteration 143/1000 | Loss: 0.00007664
Iteration 144/1000 | Loss: 0.00007658
Iteration 145/1000 | Loss: 0.00007650
Iteration 146/1000 | Loss: 0.00007631
Iteration 147/1000 | Loss: 0.00007606
Iteration 148/1000 | Loss: 0.00007579
Iteration 149/1000 | Loss: 0.00007537
Iteration 150/1000 | Loss: 0.00007480
Iteration 151/1000 | Loss: 0.00023489
Iteration 152/1000 | Loss: 0.00020340
Iteration 153/1000 | Loss: 0.00056663
Iteration 154/1000 | Loss: 0.00043682
Iteration 155/1000 | Loss: 0.00022740
Iteration 156/1000 | Loss: 0.00017078
Iteration 157/1000 | Loss: 0.00021384
Iteration 158/1000 | Loss: 0.00015265
Iteration 159/1000 | Loss: 0.00007569
Iteration 160/1000 | Loss: 0.00007421
Iteration 161/1000 | Loss: 0.00043844
Iteration 162/1000 | Loss: 0.00016187
Iteration 163/1000 | Loss: 0.00007690
Iteration 164/1000 | Loss: 0.00007410
Iteration 165/1000 | Loss: 0.00046992
Iteration 166/1000 | Loss: 0.00024150
Iteration 167/1000 | Loss: 0.00026156
Iteration 168/1000 | Loss: 0.00008504
Iteration 169/1000 | Loss: 0.00007609
Iteration 170/1000 | Loss: 0.00007341
Iteration 171/1000 | Loss: 0.00007141
Iteration 172/1000 | Loss: 0.00006982
Iteration 173/1000 | Loss: 0.00024171
Iteration 174/1000 | Loss: 0.00016001
Iteration 175/1000 | Loss: 0.00021855
Iteration 176/1000 | Loss: 0.00013780
Iteration 177/1000 | Loss: 0.00020130
Iteration 178/1000 | Loss: 0.00012403
Iteration 179/1000 | Loss: 0.00041210
Iteration 180/1000 | Loss: 0.00027819
Iteration 181/1000 | Loss: 0.00007874
Iteration 182/1000 | Loss: 0.00036439
Iteration 183/1000 | Loss: 0.00029958
Iteration 184/1000 | Loss: 0.00025966
Iteration 185/1000 | Loss: 0.00028361
Iteration 186/1000 | Loss: 0.00011844
Iteration 187/1000 | Loss: 0.00007709
Iteration 188/1000 | Loss: 0.00013179
Iteration 189/1000 | Loss: 0.00014031
Iteration 190/1000 | Loss: 0.00015328
Iteration 191/1000 | Loss: 0.00014194
Iteration 192/1000 | Loss: 0.00018069
Iteration 193/1000 | Loss: 0.00015923
Iteration 194/1000 | Loss: 0.00015321
Iteration 195/1000 | Loss: 0.00023213
Iteration 196/1000 | Loss: 0.00016957
Iteration 197/1000 | Loss: 0.00006632
Iteration 198/1000 | Loss: 0.00006464
Iteration 199/1000 | Loss: 0.00024603
Iteration 200/1000 | Loss: 0.00006889
Iteration 201/1000 | Loss: 0.00006439
Iteration 202/1000 | Loss: 0.00024754
Iteration 203/1000 | Loss: 0.00006860
Iteration 204/1000 | Loss: 0.00044685
Iteration 205/1000 | Loss: 0.00038151
Iteration 206/1000 | Loss: 0.00007337
Iteration 207/1000 | Loss: 0.00014112
Iteration 208/1000 | Loss: 0.00007049
Iteration 209/1000 | Loss: 0.00026302
Iteration 210/1000 | Loss: 0.00064299
Iteration 211/1000 | Loss: 0.00059015
Iteration 212/1000 | Loss: 0.00059720
Iteration 213/1000 | Loss: 0.00035963
Iteration 214/1000 | Loss: 0.00059313
Iteration 215/1000 | Loss: 0.00047592
Iteration 216/1000 | Loss: 0.00044619
Iteration 217/1000 | Loss: 0.00041965
Iteration 218/1000 | Loss: 0.00034201
Iteration 219/1000 | Loss: 0.00055025
Iteration 220/1000 | Loss: 0.00043207
Iteration 221/1000 | Loss: 0.00041947
Iteration 222/1000 | Loss: 0.00062320
Iteration 223/1000 | Loss: 0.00058483
Iteration 224/1000 | Loss: 0.00119925
Iteration 225/1000 | Loss: 0.00040751
Iteration 226/1000 | Loss: 0.00049294
Iteration 227/1000 | Loss: 0.00038095
Iteration 228/1000 | Loss: 0.00023639
Iteration 229/1000 | Loss: 0.00032339
Iteration 230/1000 | Loss: 0.00008001
Iteration 231/1000 | Loss: 0.00007310
Iteration 232/1000 | Loss: 0.00009043
Iteration 233/1000 | Loss: 0.00043539
Iteration 234/1000 | Loss: 0.00011065
Iteration 235/1000 | Loss: 0.00008536
Iteration 236/1000 | Loss: 0.00007788
Iteration 237/1000 | Loss: 0.00033634
Iteration 238/1000 | Loss: 0.00018759
Iteration 239/1000 | Loss: 0.00043673
Iteration 240/1000 | Loss: 0.00033912
Iteration 241/1000 | Loss: 0.00022614
Iteration 242/1000 | Loss: 0.00008941
Iteration 243/1000 | Loss: 0.00028226
Iteration 244/1000 | Loss: 0.00035277
Iteration 245/1000 | Loss: 0.00032529
Iteration 246/1000 | Loss: 0.00011066
Iteration 247/1000 | Loss: 0.00007100
Iteration 248/1000 | Loss: 0.00010964
Iteration 249/1000 | Loss: 0.00006306
Iteration 250/1000 | Loss: 0.00024698
Iteration 251/1000 | Loss: 0.00027335
Iteration 252/1000 | Loss: 0.00008681
Iteration 253/1000 | Loss: 0.00007230
Iteration 254/1000 | Loss: 0.00025748
Iteration 255/1000 | Loss: 0.00038859
Iteration 256/1000 | Loss: 0.00028030
Iteration 257/1000 | Loss: 0.00076747
Iteration 258/1000 | Loss: 0.00025738
Iteration 259/1000 | Loss: 0.00007888
Iteration 260/1000 | Loss: 0.00006145
Iteration 261/1000 | Loss: 0.00027289
Iteration 262/1000 | Loss: 0.00033448
Iteration 263/1000 | Loss: 0.00007705
Iteration 264/1000 | Loss: 0.00007113
Iteration 265/1000 | Loss: 0.00005819
Iteration 266/1000 | Loss: 0.00006509
Iteration 267/1000 | Loss: 0.00005563
Iteration 268/1000 | Loss: 0.00005418
Iteration 269/1000 | Loss: 0.00044234
Iteration 270/1000 | Loss: 0.00023103
Iteration 271/1000 | Loss: 0.00043501
Iteration 272/1000 | Loss: 0.00043879
Iteration 273/1000 | Loss: 0.00006614
Iteration 274/1000 | Loss: 0.00005930
Iteration 275/1000 | Loss: 0.00005558
Iteration 276/1000 | Loss: 0.00024345
Iteration 277/1000 | Loss: 0.00046419
Iteration 278/1000 | Loss: 0.00021735
Iteration 279/1000 | Loss: 0.00018534
Iteration 280/1000 | Loss: 0.00009099
Iteration 281/1000 | Loss: 0.00005798
Iteration 282/1000 | Loss: 0.00024318
Iteration 283/1000 | Loss: 0.00020686
Iteration 284/1000 | Loss: 0.00025037
Iteration 285/1000 | Loss: 0.00006414
Iteration 286/1000 | Loss: 0.00005524
Iteration 287/1000 | Loss: 0.00005414
Iteration 288/1000 | Loss: 0.00028339
Iteration 289/1000 | Loss: 0.00008263
Iteration 290/1000 | Loss: 0.00005898
Iteration 291/1000 | Loss: 0.00005376
Iteration 292/1000 | Loss: 0.00005093
Iteration 293/1000 | Loss: 0.00004879
Iteration 294/1000 | Loss: 0.00024062
Iteration 295/1000 | Loss: 0.00005343
Iteration 296/1000 | Loss: 0.00004878
Iteration 297/1000 | Loss: 0.00023141
Iteration 298/1000 | Loss: 0.00005160
Iteration 299/1000 | Loss: 0.00023144
Iteration 300/1000 | Loss: 0.00023755
Iteration 301/1000 | Loss: 0.00019897
Iteration 302/1000 | Loss: 0.00023686
Iteration 303/1000 | Loss: 0.00009361
Iteration 304/1000 | Loss: 0.00005136
Iteration 305/1000 | Loss: 0.00040812
Iteration 306/1000 | Loss: 0.00021200
Iteration 307/1000 | Loss: 0.00037523
Iteration 308/1000 | Loss: 0.00008525
Iteration 309/1000 | Loss: 0.00005493
Iteration 310/1000 | Loss: 0.00005311
Iteration 311/1000 | Loss: 0.00005161
Iteration 312/1000 | Loss: 0.00005052
Iteration 313/1000 | Loss: 0.00004947
Iteration 314/1000 | Loss: 0.00024560
Iteration 315/1000 | Loss: 0.00005568
Iteration 316/1000 | Loss: 0.00005090
Iteration 317/1000 | Loss: 0.00004905
Iteration 318/1000 | Loss: 0.00023889
Iteration 319/1000 | Loss: 0.00005443
Iteration 320/1000 | Loss: 0.00004942
Iteration 321/1000 | Loss: 0.00022744
Iteration 322/1000 | Loss: 0.00018759
Iteration 323/1000 | Loss: 0.00021285
Iteration 324/1000 | Loss: 0.00026144
Iteration 325/1000 | Loss: 0.00019155
Iteration 326/1000 | Loss: 0.00006705
Iteration 327/1000 | Loss: 0.00004885
Iteration 328/1000 | Loss: 0.00022656
Iteration 329/1000 | Loss: 0.00016360
Iteration 330/1000 | Loss: 0.00005342
Iteration 331/1000 | Loss: 0.00004859
Iteration 332/1000 | Loss: 0.00004683
Iteration 333/1000 | Loss: 0.00019295
Iteration 334/1000 | Loss: 0.00004510
Iteration 335/1000 | Loss: 0.00004420
Iteration 336/1000 | Loss: 0.00042609
Iteration 337/1000 | Loss: 0.00019549
Iteration 338/1000 | Loss: 0.00004953
Iteration 339/1000 | Loss: 0.00004530
Iteration 340/1000 | Loss: 0.00041093
Iteration 341/1000 | Loss: 0.00045395
Iteration 342/1000 | Loss: 0.00006926
Iteration 343/1000 | Loss: 0.00005352
Iteration 344/1000 | Loss: 0.00004846
Iteration 345/1000 | Loss: 0.00004647
Iteration 346/1000 | Loss: 0.00004517
Iteration 347/1000 | Loss: 0.00004382
Iteration 348/1000 | Loss: 0.00022342
Iteration 349/1000 | Loss: 0.00021159
Iteration 350/1000 | Loss: 0.00021719
Iteration 351/1000 | Loss: 0.00004659
Iteration 352/1000 | Loss: 0.00042451
Iteration 353/1000 | Loss: 0.00033922
Iteration 354/1000 | Loss: 0.00089268
Iteration 355/1000 | Loss: 0.00033324
Iteration 356/1000 | Loss: 0.00049809
Iteration 357/1000 | Loss: 0.00016372
Iteration 358/1000 | Loss: 0.00005172
Iteration 359/1000 | Loss: 0.00004995
Iteration 360/1000 | Loss: 0.00004845
Iteration 361/1000 | Loss: 0.00004734
Iteration 362/1000 | Loss: 0.00042290
Iteration 363/1000 | Loss: 0.00035783
Iteration 364/1000 | Loss: 0.00041348
Iteration 365/1000 | Loss: 0.00030312
Iteration 366/1000 | Loss: 0.00047043
Iteration 367/1000 | Loss: 0.00029805
Iteration 368/1000 | Loss: 0.00037266
Iteration 369/1000 | Loss: 0.00007709
Iteration 370/1000 | Loss: 0.00005679
Iteration 371/1000 | Loss: 0.00005028
Iteration 372/1000 | Loss: 0.00004734
Iteration 373/1000 | Loss: 0.00004601
Iteration 374/1000 | Loss: 0.00004502
Iteration 375/1000 | Loss: 0.00004420
Iteration 376/1000 | Loss: 0.00004346
Iteration 377/1000 | Loss: 0.00004292
Iteration 378/1000 | Loss: 0.00022742
Iteration 379/1000 | Loss: 0.00018673
Iteration 380/1000 | Loss: 0.00004236
Iteration 381/1000 | Loss: 0.00043517
Iteration 382/1000 | Loss: 0.00017935
Iteration 383/1000 | Loss: 0.00020491
Iteration 384/1000 | Loss: 0.00010421
Iteration 385/1000 | Loss: 0.00004639
Iteration 386/1000 | Loss: 0.00004506
Iteration 387/1000 | Loss: 0.00023695
Iteration 388/1000 | Loss: 0.00004995
Iteration 389/1000 | Loss: 0.00004589
Iteration 390/1000 | Loss: 0.00004475
Iteration 391/1000 | Loss: 0.00004358
Iteration 392/1000 | Loss: 0.00004277
Iteration 393/1000 | Loss: 0.00004122
Iteration 394/1000 | Loss: 0.00004015
Iteration 395/1000 | Loss: 0.00078776
Iteration 396/1000 | Loss: 0.00056572
Iteration 397/1000 | Loss: 0.00074986
Iteration 398/1000 | Loss: 0.00044758
Iteration 399/1000 | Loss: 0.00010054
Iteration 400/1000 | Loss: 0.00006904
Iteration 401/1000 | Loss: 0.00043859
Iteration 402/1000 | Loss: 0.00047546
Iteration 403/1000 | Loss: 0.00033086
Iteration 404/1000 | Loss: 0.00032610
Iteration 405/1000 | Loss: 0.00005825
Iteration 406/1000 | Loss: 0.00012735
Iteration 407/1000 | Loss: 0.00004398
Iteration 408/1000 | Loss: 0.00004162
Iteration 409/1000 | Loss: 0.00003951
Iteration 410/1000 | Loss: 0.00003851
Iteration 411/1000 | Loss: 0.00003769
Iteration 412/1000 | Loss: 0.00003708
Iteration 413/1000 | Loss: 0.00003626
Iteration 414/1000 | Loss: 0.00003555
Iteration 415/1000 | Loss: 0.00003520
Iteration 416/1000 | Loss: 0.00021973
Iteration 417/1000 | Loss: 0.00018262
Iteration 418/1000 | Loss: 0.00020582
Iteration 419/1000 | Loss: 0.00014588
Iteration 420/1000 | Loss: 0.00003558
Iteration 421/1000 | Loss: 0.00020963
Iteration 422/1000 | Loss: 0.00005265
Iteration 423/1000 | Loss: 0.00020872
Iteration 424/1000 | Loss: 0.00004421
Iteration 425/1000 | Loss: 0.00003852
Iteration 426/1000 | Loss: 0.00003619
Iteration 427/1000 | Loss: 0.00003506
Iteration 428/1000 | Loss: 0.00003424
Iteration 429/1000 | Loss: 0.00003361
Iteration 430/1000 | Loss: 0.00021937
Iteration 431/1000 | Loss: 0.00003949
Iteration 432/1000 | Loss: 0.00003601
Iteration 433/1000 | Loss: 0.00003473
Iteration 434/1000 | Loss: 0.00003402
Iteration 435/1000 | Loss: 0.00003332
Iteration 436/1000 | Loss: 0.00003253
Iteration 437/1000 | Loss: 0.00003209
Iteration 438/1000 | Loss: 0.00073465
Iteration 439/1000 | Loss: 0.00054735
Iteration 440/1000 | Loss: 0.00097333
Iteration 441/1000 | Loss: 0.00046119
Iteration 442/1000 | Loss: 0.00084503
Iteration 443/1000 | Loss: 0.00042919
Iteration 444/1000 | Loss: 0.00087266
Iteration 445/1000 | Loss: 0.00033332
Iteration 446/1000 | Loss: 0.00075424
Iteration 447/1000 | Loss: 0.00035687
Iteration 448/1000 | Loss: 0.00005107
Iteration 449/1000 | Loss: 0.00061377
Iteration 450/1000 | Loss: 0.00027976
Iteration 451/1000 | Loss: 0.00013450
Iteration 452/1000 | Loss: 0.00055856
Iteration 453/1000 | Loss: 0.00026468
Iteration 454/1000 | Loss: 0.00003537
Iteration 455/1000 | Loss: 0.00003387
Iteration 456/1000 | Loss: 0.00048791
Iteration 457/1000 | Loss: 0.00020105
Iteration 458/1000 | Loss: 0.00025355
Iteration 459/1000 | Loss: 0.00011713
Iteration 460/1000 | Loss: 0.00003886
Iteration 461/1000 | Loss: 0.00003740
Iteration 462/1000 | Loss: 0.00003628
Iteration 463/1000 | Loss: 0.00003553
Iteration 464/1000 | Loss: 0.00038088
Iteration 465/1000 | Loss: 0.00028585
Iteration 466/1000 | Loss: 0.00035081
Iteration 467/1000 | Loss: 0.00022791
Iteration 468/1000 | Loss: 0.00036673
Iteration 469/1000 | Loss: 0.00021399
Iteration 470/1000 | Loss: 0.00031962
Iteration 471/1000 | Loss: 0.00020704
Iteration 472/1000 | Loss: 0.00014534
Iteration 473/1000 | Loss: 0.00012378
Iteration 474/1000 | Loss: 0.00031531
Iteration 475/1000 | Loss: 0.00019077
Iteration 476/1000 | Loss: 0.00015077
Iteration 477/1000 | Loss: 0.00004256
Iteration 478/1000 | Loss: 0.00003972
Iteration 479/1000 | Loss: 0.00003859
Iteration 480/1000 | Loss: 0.00003763
Iteration 481/1000 | Loss: 0.00025306
Iteration 482/1000 | Loss: 0.00014585
Iteration 483/1000 | Loss: 0.00004466
Iteration 484/1000 | Loss: 0.00003699
Iteration 485/1000 | Loss: 0.00018813
Iteration 486/1000 | Loss: 0.00018529
Iteration 487/1000 | Loss: 0.00018135
Iteration 488/1000 | Loss: 0.00034955
Iteration 489/1000 | Loss: 0.00018997
Iteration 490/1000 | Loss: 0.00004387
Iteration 491/1000 | Loss: 0.00004079
Iteration 492/1000 | Loss: 0.00003957
Iteration 493/1000 | Loss: 0.00021083
Iteration 494/1000 | Loss: 0.00004960
Iteration 495/1000 | Loss: 0.00004008
Iteration 496/1000 | Loss: 0.00003689
Iteration 497/1000 | Loss: 0.00003606
Iteration 498/1000 | Loss: 0.00003543
Iteration 499/1000 | Loss: 0.00003501
Iteration 500/1000 | Loss: 0.00025600
Iteration 501/1000 | Loss: 0.00004616
Iteration 502/1000 | Loss: 0.00003849
Iteration 503/1000 | Loss: 0.00035407
Iteration 504/1000 | Loss: 0.00007697
Iteration 505/1000 | Loss: 0.00004175
Iteration 506/1000 | Loss: 0.00003769
Iteration 507/1000 | Loss: 0.00004694
Iteration 508/1000 | Loss: 0.00004663
Iteration 509/1000 | Loss: 0.00003870
Iteration 510/1000 | Loss: 0.00003540
Iteration 511/1000 | Loss: 0.00003836
Iteration 512/1000 | Loss: 0.00006660
Iteration 513/1000 | Loss: 0.00003714
Iteration 514/1000 | Loss: 0.00003602
Iteration 515/1000 | Loss: 0.00003662
Iteration 516/1000 | Loss: 0.00003344
Iteration 517/1000 | Loss: 0.00041662
Iteration 518/1000 | Loss: 0.00031390
Iteration 519/1000 | Loss: 0.00003532
Iteration 520/1000 | Loss: 0.00041015
Iteration 521/1000 | Loss: 0.00004381
Iteration 522/1000 | Loss: 0.00004555
Iteration 523/1000 | Loss: 0.00003670
Iteration 524/1000 | Loss: 0.00003536
Iteration 525/1000 | Loss: 0.00003432
Iteration 526/1000 | Loss: 0.00003352
Iteration 527/1000 | Loss: 0.00003298
Iteration 528/1000 | Loss: 0.00003240
Iteration 529/1000 | Loss: 0.00023078
Iteration 530/1000 | Loss: 0.00003796
Iteration 531/1000 | Loss: 0.00003465
Iteration 532/1000 | Loss: 0.00003328
Iteration 533/1000 | Loss: 0.00003248
Iteration 534/1000 | Loss: 0.00021111
Iteration 535/1000 | Loss: 0.00017545
Iteration 536/1000 | Loss: 0.00060843
Iteration 537/1000 | Loss: 0.00053549
Iteration 538/1000 | Loss: 0.00003289
Iteration 539/1000 | Loss: 0.00037634
Iteration 540/1000 | Loss: 0.00017445
Iteration 541/1000 | Loss: 0.00019577
Iteration 542/1000 | Loss: 0.00004487
Iteration 543/1000 | Loss: 0.00003557
Iteration 544/1000 | Loss: 0.00021645
Iteration 545/1000 | Loss: 0.00004216
Iteration 546/1000 | Loss: 0.00003712
Iteration 547/1000 | Loss: 0.00003594
Iteration 548/1000 | Loss: 0.00003525
Iteration 549/1000 | Loss: 0.00003477
Iteration 550/1000 | Loss: 0.00003425
Iteration 551/1000 | Loss: 0.00003385
Iteration 552/1000 | Loss: 0.00003336
Iteration 553/1000 | Loss: 0.00003299
Iteration 554/1000 | Loss: 0.00003254
Iteration 555/1000 | Loss: 0.00003216
Iteration 556/1000 | Loss: 0.00003186
Iteration 557/1000 | Loss: 0.00003139
Iteration 558/1000 | Loss: 0.00041228
Iteration 559/1000 | Loss: 0.00027485
Iteration 560/1000 | Loss: 0.00003192
Iteration 561/1000 | Loss: 0.00040740
Iteration 562/1000 | Loss: 0.00025798
Iteration 563/1000 | Loss: 0.00003229
Iteration 564/1000 | Loss: 0.00003103
Iteration 565/1000 | Loss: 0.00044618
Iteration 566/1000 | Loss: 0.00041207
Iteration 567/1000 | Loss: 0.00023437
Iteration 568/1000 | Loss: 0.00036551
Iteration 569/1000 | Loss: 0.00017648
Iteration 570/1000 | Loss: 0.00014625
Iteration 571/1000 | Loss: 0.00016867
Iteration 572/1000 | Loss: 0.00003863
Iteration 573/1000 | Loss: 0.00003435
Iteration 574/1000 | Loss: 0.00003253
Iteration 575/1000 | Loss: 0.00003114
Iteration 576/1000 | Loss: 0.00003023
Iteration 577/1000 | Loss: 0.00002955
Iteration 578/1000 | Loss: 0.00002890
Iteration 579/1000 | Loss: 0.00002849
Iteration 580/1000 | Loss: 0.00002818
Iteration 581/1000 | Loss: 0.00002790
Iteration 582/1000 | Loss: 0.00002766
Iteration 583/1000 | Loss: 0.00002752
Iteration 584/1000 | Loss: 0.00002749
Iteration 585/1000 | Loss: 0.00002745
Iteration 586/1000 | Loss: 0.00002745
Iteration 587/1000 | Loss: 0.00002744
Iteration 588/1000 | Loss: 0.00002731
Iteration 589/1000 | Loss: 0.00003434
Iteration 590/1000 | Loss: 0.00002788
Iteration 591/1000 | Loss: 0.00002754
Iteration 592/1000 | Loss: 0.00002721
Iteration 593/1000 | Loss: 0.00002697
Iteration 594/1000 | Loss: 0.00002682
Iteration 595/1000 | Loss: 0.00002666
Iteration 596/1000 | Loss: 0.00002647
Iteration 597/1000 | Loss: 0.00002614
Iteration 598/1000 | Loss: 0.00002578
Iteration 599/1000 | Loss: 0.00002554
Iteration 600/1000 | Loss: 0.00040562
Iteration 601/1000 | Loss: 0.00017636
Iteration 602/1000 | Loss: 0.00002670
Iteration 603/1000 | Loss: 0.00002578
Iteration 604/1000 | Loss: 0.00040416
Iteration 605/1000 | Loss: 0.00014791
Iteration 606/1000 | Loss: 0.00002618
Iteration 607/1000 | Loss: 0.00002555
Iteration 608/1000 | Loss: 0.00039260
Iteration 609/1000 | Loss: 0.00014327
Iteration 610/1000 | Loss: 0.00002762
Iteration 611/1000 | Loss: 0.00002610
Iteration 612/1000 | Loss: 0.00002547
Iteration 613/1000 | Loss: 0.00038589
Iteration 614/1000 | Loss: 0.00008658
Iteration 615/1000 | Loss: 0.00002601
Iteration 616/1000 | Loss: 0.00002528
Iteration 617/1000 | Loss: 0.00044256
Iteration 618/1000 | Loss: 0.00005360
Iteration 619/1000 | Loss: 0.00004166
Iteration 620/1000 | Loss: 0.00003515
Iteration 621/1000 | Loss: 0.00003047
Iteration 622/1000 | Loss: 0.00002954
Iteration 623/1000 | Loss: 0.00002879
Iteration 624/1000 | Loss: 0.00002820
Iteration 625/1000 | Loss: 0.00002794
Iteration 626/1000 | Loss: 0.00002774
Iteration 627/1000 | Loss: 0.00002772
Iteration 628/1000 | Loss: 0.00002753
Iteration 629/1000 | Loss: 0.00002733
Iteration 630/1000 | Loss: 0.00002731
Iteration 631/1000 | Loss: 0.00002709
Iteration 632/1000 | Loss: 0.00002685
Iteration 633/1000 | Loss: 0.00002681
Iteration 634/1000 | Loss: 0.00002679
Iteration 635/1000 | Loss: 0.00002678
Iteration 636/1000 | Loss: 0.00002678
Iteration 637/1000 | Loss: 0.00002677
Iteration 638/1000 | Loss: 0.00002676
Iteration 639/1000 | Loss: 0.00002675
Iteration 640/1000 | Loss: 0.00002675
Iteration 641/1000 | Loss: 0.00002675
Iteration 642/1000 | Loss: 0.00002673
Iteration 643/1000 | Loss: 0.00002672
Iteration 644/1000 | Loss: 0.00002672
Iteration 645/1000 | Loss: 0.00002672
Iteration 646/1000 | Loss: 0.00002672
Iteration 647/1000 | Loss: 0.00002672
Iteration 648/1000 | Loss: 0.00002672
Iteration 649/1000 | Loss: 0.00002672
Iteration 650/1000 | Loss: 0.00002672
Iteration 651/1000 | Loss: 0.00002672
Iteration 652/1000 | Loss: 0.00002672
Iteration 653/1000 | Loss: 0.00002672
Iteration 654/1000 | Loss: 0.00002671
Iteration 655/1000 | Loss: 0.00002671
Iteration 656/1000 | Loss: 0.00002670
Iteration 657/1000 | Loss: 0.00002670
Iteration 658/1000 | Loss: 0.00002669
Iteration 659/1000 | Loss: 0.00002669
Iteration 660/1000 | Loss: 0.00002669
Iteration 661/1000 | Loss: 0.00002668
Iteration 662/1000 | Loss: 0.00002668
Iteration 663/1000 | Loss: 0.00002668
Iteration 664/1000 | Loss: 0.00002667
Iteration 665/1000 | Loss: 0.00002667
Iteration 666/1000 | Loss: 0.00002666
Iteration 667/1000 | Loss: 0.00002665
Iteration 668/1000 | Loss: 0.00002665
Iteration 669/1000 | Loss: 0.00002665
Iteration 670/1000 | Loss: 0.00002665
Iteration 671/1000 | Loss: 0.00002665
Iteration 672/1000 | Loss: 0.00002665
Iteration 673/1000 | Loss: 0.00002665
Iteration 674/1000 | Loss: 0.00002665
Iteration 675/1000 | Loss: 0.00002664
Iteration 676/1000 | Loss: 0.00002664
Iteration 677/1000 | Loss: 0.00002664
Iteration 678/1000 | Loss: 0.00002664
Iteration 679/1000 | Loss: 0.00002664
Iteration 680/1000 | Loss: 0.00002664
Iteration 681/1000 | Loss: 0.00002664
Iteration 682/1000 | Loss: 0.00002664
Iteration 683/1000 | Loss: 0.00002663
Iteration 684/1000 | Loss: 0.00002642
Iteration 685/1000 | Loss: 0.00002615
Iteration 686/1000 | Loss: 0.00002580
Iteration 687/1000 | Loss: 0.00002557
Iteration 688/1000 | Loss: 0.00019509
Iteration 689/1000 | Loss: 0.00014028
Iteration 690/1000 | Loss: 0.00002558
Iteration 691/1000 | Loss: 0.00038425
Iteration 692/1000 | Loss: 0.00012512
Iteration 693/1000 | Loss: 0.00034968
Iteration 694/1000 | Loss: 0.00029351
Iteration 695/1000 | Loss: 0.00003890
Iteration 696/1000 | Loss: 0.00003080
Iteration 697/1000 | Loss: 0.00002782
Iteration 698/1000 | Loss: 0.00002627
Iteration 699/1000 | Loss: 0.00002564
Iteration 700/1000 | Loss: 0.00002543
Iteration 701/1000 | Loss: 0.00002543
Iteration 702/1000 | Loss: 0.00002528
Iteration 703/1000 | Loss: 0.00002523
Iteration 704/1000 | Loss: 0.00044173
Iteration 705/1000 | Loss: 0.00005316
Iteration 706/1000 | Loss: 0.00002818
Iteration 707/1000 | Loss: 0.00002549
Iteration 708/1000 | Loss: 0.00002455
Iteration 709/1000 | Loss: 0.00002410
Iteration 710/1000 | Loss: 0.00002403
Iteration 711/1000 | Loss: 0.00002390
Iteration 712/1000 | Loss: 0.00002388
Iteration 713/1000 | Loss: 0.00002387
Iteration 714/1000 | Loss: 0.00002387
Iteration 715/1000 | Loss: 0.00002387
Iteration 716/1000 | Loss: 0.00002387
Iteration 717/1000 | Loss: 0.00002386
Iteration 718/1000 | Loss: 0.00002386
Iteration 719/1000 | Loss: 0.00002386
Iteration 720/1000 | Loss: 0.00002386
Iteration 721/1000 | Loss: 0.00002386
Iteration 722/1000 | Loss: 0.00002386
Iteration 723/1000 | Loss: 0.00002386
Iteration 724/1000 | Loss: 0.00002386
Iteration 725/1000 | Loss: 0.00002385
Iteration 726/1000 | Loss: 0.00002384
Iteration 727/1000 | Loss: 0.00002384
Iteration 728/1000 | Loss: 0.00002383
Iteration 729/1000 | Loss: 0.00002383
Iteration 730/1000 | Loss: 0.00002382
Iteration 731/1000 | Loss: 0.00002381
Iteration 732/1000 | Loss: 0.00002381
Iteration 733/1000 | Loss: 0.00002380
Iteration 734/1000 | Loss: 0.00002380
Iteration 735/1000 | Loss: 0.00002379
Iteration 736/1000 | Loss: 0.00002379
Iteration 737/1000 | Loss: 0.00002379
Iteration 738/1000 | Loss: 0.00002379
Iteration 739/1000 | Loss: 0.00002378
Iteration 740/1000 | Loss: 0.00002378
Iteration 741/1000 | Loss: 0.00002378
Iteration 742/1000 | Loss: 0.00002378
Iteration 743/1000 | Loss: 0.00002377
Iteration 744/1000 | Loss: 0.00002376
Iteration 745/1000 | Loss: 0.00002376
Iteration 746/1000 | Loss: 0.00002375
Iteration 747/1000 | Loss: 0.00002375
Iteration 748/1000 | Loss: 0.00002375
Iteration 749/1000 | Loss: 0.00002375
Iteration 750/1000 | Loss: 0.00002374
Iteration 751/1000 | Loss: 0.00002374
Iteration 752/1000 | Loss: 0.00002373
Iteration 753/1000 | Loss: 0.00002373
Iteration 754/1000 | Loss: 0.00002373
Iteration 755/1000 | Loss: 0.00002373
Iteration 756/1000 | Loss: 0.00002373
Iteration 757/1000 | Loss: 0.00002373
Iteration 758/1000 | Loss: 0.00002373
Iteration 759/1000 | Loss: 0.00002373
Iteration 760/1000 | Loss: 0.00002373
Iteration 761/1000 | Loss: 0.00002373
Iteration 762/1000 | Loss: 0.00002373
Iteration 763/1000 | Loss: 0.00002373
Iteration 764/1000 | Loss: 0.00002373
Iteration 765/1000 | Loss: 0.00002373
Iteration 766/1000 | Loss: 0.00002373
Iteration 767/1000 | Loss: 0.00002373
Iteration 768/1000 | Loss: 0.00002373
Iteration 769/1000 | Loss: 0.00002373
Iteration 770/1000 | Loss: 0.00002373
Iteration 771/1000 | Loss: 0.00002373
Iteration 772/1000 | Loss: 0.00002373
Iteration 773/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 773. Stopping optimization.
Last 5 losses: [2.372718336118851e-05, 2.372718336118851e-05, 2.372718336118851e-05, 2.372718336118851e-05, 2.372718336118851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.372718336118851e-05

Optimization complete. Final v2v error: 3.7544469833374023 mm

Highest mean error: 11.971261024475098 mm for frame 149

Lowest mean error: 3.2105939388275146 mm for frame 98

Saving results

Total time: 1045.0959651470184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392915
Iteration 2/25 | Loss: 0.00138561
Iteration 3/25 | Loss: 0.00131492
Iteration 4/25 | Loss: 0.00130696
Iteration 5/25 | Loss: 0.00130505
Iteration 6/25 | Loss: 0.00130505
Iteration 7/25 | Loss: 0.00130505
Iteration 8/25 | Loss: 0.00130505
Iteration 9/25 | Loss: 0.00130505
Iteration 10/25 | Loss: 0.00130505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013050460256636143, 0.0013050460256636143, 0.0013050460256636143, 0.0013050460256636143, 0.0013050460256636143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013050460256636143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30499899
Iteration 2/25 | Loss: 0.00158777
Iteration 3/25 | Loss: 0.00158777
Iteration 4/25 | Loss: 0.00158777
Iteration 5/25 | Loss: 0.00158777
Iteration 6/25 | Loss: 0.00158777
Iteration 7/25 | Loss: 0.00158777
Iteration 8/25 | Loss: 0.00158777
Iteration 9/25 | Loss: 0.00158777
Iteration 10/25 | Loss: 0.00158777
Iteration 11/25 | Loss: 0.00158777
Iteration 12/25 | Loss: 0.00158777
Iteration 13/25 | Loss: 0.00158777
Iteration 14/25 | Loss: 0.00158777
Iteration 15/25 | Loss: 0.00158777
Iteration 16/25 | Loss: 0.00158777
Iteration 17/25 | Loss: 0.00158777
Iteration 18/25 | Loss: 0.00158777
Iteration 19/25 | Loss: 0.00158776
Iteration 20/25 | Loss: 0.00158776
Iteration 21/25 | Loss: 0.00158776
Iteration 22/25 | Loss: 0.00158776
Iteration 23/25 | Loss: 0.00158776
Iteration 24/25 | Loss: 0.00158776
Iteration 25/25 | Loss: 0.00158776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015877648256719112, 0.0015877648256719112, 0.0015877648256719112, 0.0015877648256719112, 0.0015877648256719112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015877648256719112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158776
Iteration 2/1000 | Loss: 0.00003184
Iteration 3/1000 | Loss: 0.00002097
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001599
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001470
Iteration 15/1000 | Loss: 0.00001465
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001457
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001456
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001443
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001441
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001421
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001419
Iteration 82/1000 | Loss: 0.00001419
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001418
Iteration 85/1000 | Loss: 0.00001418
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001418
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001417
Iteration 94/1000 | Loss: 0.00001417
Iteration 95/1000 | Loss: 0.00001417
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001416
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001414
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001412
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001410
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001410
Iteration 136/1000 | Loss: 0.00001410
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001409
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001409
Iteration 146/1000 | Loss: 0.00001409
Iteration 147/1000 | Loss: 0.00001409
Iteration 148/1000 | Loss: 0.00001408
Iteration 149/1000 | Loss: 0.00001408
Iteration 150/1000 | Loss: 0.00001408
Iteration 151/1000 | Loss: 0.00001408
Iteration 152/1000 | Loss: 0.00001408
Iteration 153/1000 | Loss: 0.00001408
Iteration 154/1000 | Loss: 0.00001408
Iteration 155/1000 | Loss: 0.00001407
Iteration 156/1000 | Loss: 0.00001407
Iteration 157/1000 | Loss: 0.00001407
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Iteration 160/1000 | Loss: 0.00001407
Iteration 161/1000 | Loss: 0.00001407
Iteration 162/1000 | Loss: 0.00001407
Iteration 163/1000 | Loss: 0.00001407
Iteration 164/1000 | Loss: 0.00001407
Iteration 165/1000 | Loss: 0.00001407
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Iteration 168/1000 | Loss: 0.00001406
Iteration 169/1000 | Loss: 0.00001406
Iteration 170/1000 | Loss: 0.00001406
Iteration 171/1000 | Loss: 0.00001406
Iteration 172/1000 | Loss: 0.00001406
Iteration 173/1000 | Loss: 0.00001405
Iteration 174/1000 | Loss: 0.00001405
Iteration 175/1000 | Loss: 0.00001405
Iteration 176/1000 | Loss: 0.00001405
Iteration 177/1000 | Loss: 0.00001405
Iteration 178/1000 | Loss: 0.00001404
Iteration 179/1000 | Loss: 0.00001404
Iteration 180/1000 | Loss: 0.00001404
Iteration 181/1000 | Loss: 0.00001404
Iteration 182/1000 | Loss: 0.00001404
Iteration 183/1000 | Loss: 0.00001404
Iteration 184/1000 | Loss: 0.00001404
Iteration 185/1000 | Loss: 0.00001404
Iteration 186/1000 | Loss: 0.00001404
Iteration 187/1000 | Loss: 0.00001404
Iteration 188/1000 | Loss: 0.00001404
Iteration 189/1000 | Loss: 0.00001404
Iteration 190/1000 | Loss: 0.00001404
Iteration 191/1000 | Loss: 0.00001404
Iteration 192/1000 | Loss: 0.00001404
Iteration 193/1000 | Loss: 0.00001403
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001403
Iteration 199/1000 | Loss: 0.00001403
Iteration 200/1000 | Loss: 0.00001403
Iteration 201/1000 | Loss: 0.00001403
Iteration 202/1000 | Loss: 0.00001402
Iteration 203/1000 | Loss: 0.00001402
Iteration 204/1000 | Loss: 0.00001402
Iteration 205/1000 | Loss: 0.00001402
Iteration 206/1000 | Loss: 0.00001402
Iteration 207/1000 | Loss: 0.00001402
Iteration 208/1000 | Loss: 0.00001402
Iteration 209/1000 | Loss: 0.00001402
Iteration 210/1000 | Loss: 0.00001402
Iteration 211/1000 | Loss: 0.00001402
Iteration 212/1000 | Loss: 0.00001402
Iteration 213/1000 | Loss: 0.00001402
Iteration 214/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.4019515219843015e-05, 1.4019515219843015e-05, 1.4019515219843015e-05, 1.4019515219843015e-05, 1.4019515219843015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4019515219843015e-05

Optimization complete. Final v2v error: 3.1169803142547607 mm

Highest mean error: 3.595386028289795 mm for frame 88

Lowest mean error: 2.5860579013824463 mm for frame 16

Saving results

Total time: 42.497074365615845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034918
Iteration 2/25 | Loss: 0.01034918
Iteration 3/25 | Loss: 0.01034918
Iteration 4/25 | Loss: 0.01034917
Iteration 5/25 | Loss: 0.01034917
Iteration 6/25 | Loss: 0.01034917
Iteration 7/25 | Loss: 0.01034917
Iteration 8/25 | Loss: 0.01034917
Iteration 9/25 | Loss: 0.01034917
Iteration 10/25 | Loss: 0.01034917
Iteration 11/25 | Loss: 0.01034917
Iteration 12/25 | Loss: 0.01034917
Iteration 13/25 | Loss: 0.01034917
Iteration 14/25 | Loss: 0.01034917
Iteration 15/25 | Loss: 0.01034917
Iteration 16/25 | Loss: 0.01034917
Iteration 17/25 | Loss: 0.01034917
Iteration 18/25 | Loss: 0.01034917
Iteration 19/25 | Loss: 0.01034917
Iteration 20/25 | Loss: 0.01034917
Iteration 21/25 | Loss: 0.01034917
Iteration 22/25 | Loss: 0.01034917
Iteration 23/25 | Loss: 0.01034916
Iteration 24/25 | Loss: 0.01034916
Iteration 25/25 | Loss: 0.01034916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41016352
Iteration 2/25 | Loss: 0.12879555
Iteration 3/25 | Loss: 0.12874614
Iteration 4/25 | Loss: 0.12876542
Iteration 5/25 | Loss: 0.12874603
Iteration 6/25 | Loss: 0.12874600
Iteration 7/25 | Loss: 0.12874599
Iteration 8/25 | Loss: 0.12874596
Iteration 9/25 | Loss: 0.12872878
Iteration 10/25 | Loss: 0.12885354
Iteration 11/25 | Loss: 0.12856084
Iteration 12/25 | Loss: 0.12856083
Iteration 13/25 | Loss: 0.12856083
Iteration 14/25 | Loss: 0.12856081
Iteration 15/25 | Loss: 0.12856081
Iteration 16/25 | Loss: 0.12856081
Iteration 17/25 | Loss: 0.12856081
Iteration 18/25 | Loss: 0.12856081
Iteration 19/25 | Loss: 0.12856081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.12856081128120422, 0.12856081128120422, 0.12856081128120422, 0.12856081128120422, 0.12856081128120422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12856081128120422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12856081
Iteration 2/1000 | Loss: 0.00084971
Iteration 3/1000 | Loss: 0.00029851
Iteration 4/1000 | Loss: 0.00020272
Iteration 5/1000 | Loss: 0.00016659
Iteration 6/1000 | Loss: 0.00003466
Iteration 7/1000 | Loss: 0.00007252
Iteration 8/1000 | Loss: 0.00004805
Iteration 9/1000 | Loss: 0.00002105
Iteration 10/1000 | Loss: 0.00012692
Iteration 11/1000 | Loss: 0.00002220
Iteration 12/1000 | Loss: 0.00003058
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00003548
Iteration 15/1000 | Loss: 0.00009333
Iteration 16/1000 | Loss: 0.00015478
Iteration 17/1000 | Loss: 0.00003569
Iteration 18/1000 | Loss: 0.00002027
Iteration 19/1000 | Loss: 0.00001493
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001916
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00002604
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00003908
Iteration 27/1000 | Loss: 0.00001272
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001666
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00007575
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00002095
Iteration 38/1000 | Loss: 0.00002094
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001792
Iteration 48/1000 | Loss: 0.00014271
Iteration 49/1000 | Loss: 0.00002907
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002511
Iteration 52/1000 | Loss: 0.00005051
Iteration 53/1000 | Loss: 0.00004006
Iteration 54/1000 | Loss: 0.00003179
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001010
Iteration 58/1000 | Loss: 0.00000994
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000994
Iteration 62/1000 | Loss: 0.00000994
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000993
Iteration 66/1000 | Loss: 0.00000993
Iteration 67/1000 | Loss: 0.00000993
Iteration 68/1000 | Loss: 0.00000993
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000990
Iteration 73/1000 | Loss: 0.00000990
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000990
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00003530
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000985
Iteration 84/1000 | Loss: 0.00001787
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000987
Iteration 102/1000 | Loss: 0.00000982
Iteration 103/1000 | Loss: 0.00000982
Iteration 104/1000 | Loss: 0.00000982
Iteration 105/1000 | Loss: 0.00000982
Iteration 106/1000 | Loss: 0.00000982
Iteration 107/1000 | Loss: 0.00000982
Iteration 108/1000 | Loss: 0.00000981
Iteration 109/1000 | Loss: 0.00000981
Iteration 110/1000 | Loss: 0.00000981
Iteration 111/1000 | Loss: 0.00000981
Iteration 112/1000 | Loss: 0.00000981
Iteration 113/1000 | Loss: 0.00000981
Iteration 114/1000 | Loss: 0.00000981
Iteration 115/1000 | Loss: 0.00001043
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000980
Iteration 121/1000 | Loss: 0.00000980
Iteration 122/1000 | Loss: 0.00000980
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000979
Iteration 128/1000 | Loss: 0.00000979
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000979
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000978
Iteration 133/1000 | Loss: 0.00000978
Iteration 134/1000 | Loss: 0.00000978
Iteration 135/1000 | Loss: 0.00000978
Iteration 136/1000 | Loss: 0.00000978
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000978
Iteration 143/1000 | Loss: 0.00000978
Iteration 144/1000 | Loss: 0.00000978
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000977
Iteration 148/1000 | Loss: 0.00000977
Iteration 149/1000 | Loss: 0.00000977
Iteration 150/1000 | Loss: 0.00000977
Iteration 151/1000 | Loss: 0.00000977
Iteration 152/1000 | Loss: 0.00000977
Iteration 153/1000 | Loss: 0.00000977
Iteration 154/1000 | Loss: 0.00000977
Iteration 155/1000 | Loss: 0.00000977
Iteration 156/1000 | Loss: 0.00000977
Iteration 157/1000 | Loss: 0.00000977
Iteration 158/1000 | Loss: 0.00000977
Iteration 159/1000 | Loss: 0.00000977
Iteration 160/1000 | Loss: 0.00000977
Iteration 161/1000 | Loss: 0.00000977
Iteration 162/1000 | Loss: 0.00000977
Iteration 163/1000 | Loss: 0.00000977
Iteration 164/1000 | Loss: 0.00000977
Iteration 165/1000 | Loss: 0.00000977
Iteration 166/1000 | Loss: 0.00000977
Iteration 167/1000 | Loss: 0.00000977
Iteration 168/1000 | Loss: 0.00000977
Iteration 169/1000 | Loss: 0.00000977
Iteration 170/1000 | Loss: 0.00000977
Iteration 171/1000 | Loss: 0.00000977
Iteration 172/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [9.774038517207373e-06, 9.774038517207373e-06, 9.774038517207373e-06, 9.774038517207373e-06, 9.774038517207373e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.774038517207373e-06

Optimization complete. Final v2v error: 2.7067129611968994 mm

Highest mean error: 2.9911251068115234 mm for frame 188

Lowest mean error: 2.4682633876800537 mm for frame 131

Saving results

Total time: 107.89596176147461
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929221
Iteration 2/25 | Loss: 0.00208494
Iteration 3/25 | Loss: 0.00160631
Iteration 4/25 | Loss: 0.00150803
Iteration 5/25 | Loss: 0.00148339
Iteration 6/25 | Loss: 0.00145567
Iteration 7/25 | Loss: 0.00144096
Iteration 8/25 | Loss: 0.00143262
Iteration 9/25 | Loss: 0.00141535
Iteration 10/25 | Loss: 0.00140946
Iteration 11/25 | Loss: 0.00139795
Iteration 12/25 | Loss: 0.00139344
Iteration 13/25 | Loss: 0.00139267
Iteration 14/25 | Loss: 0.00139236
Iteration 15/25 | Loss: 0.00139226
Iteration 16/25 | Loss: 0.00139224
Iteration 17/25 | Loss: 0.00139223
Iteration 18/25 | Loss: 0.00139223
Iteration 19/25 | Loss: 0.00139223
Iteration 20/25 | Loss: 0.00139223
Iteration 21/25 | Loss: 0.00139223
Iteration 22/25 | Loss: 0.00139223
Iteration 23/25 | Loss: 0.00139223
Iteration 24/25 | Loss: 0.00139223
Iteration 25/25 | Loss: 0.00139223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34344018
Iteration 2/25 | Loss: 0.00138026
Iteration 3/25 | Loss: 0.00138026
Iteration 4/25 | Loss: 0.00138026
Iteration 5/25 | Loss: 0.00138026
Iteration 6/25 | Loss: 0.00138026
Iteration 7/25 | Loss: 0.00138026
Iteration 8/25 | Loss: 0.00138026
Iteration 9/25 | Loss: 0.00138026
Iteration 10/25 | Loss: 0.00138026
Iteration 11/25 | Loss: 0.00138026
Iteration 12/25 | Loss: 0.00138026
Iteration 13/25 | Loss: 0.00138026
Iteration 14/25 | Loss: 0.00138026
Iteration 15/25 | Loss: 0.00138026
Iteration 16/25 | Loss: 0.00138026
Iteration 17/25 | Loss: 0.00138026
Iteration 18/25 | Loss: 0.00138026
Iteration 19/25 | Loss: 0.00138026
Iteration 20/25 | Loss: 0.00138026
Iteration 21/25 | Loss: 0.00138026
Iteration 22/25 | Loss: 0.00138026
Iteration 23/25 | Loss: 0.00138026
Iteration 24/25 | Loss: 0.00138026
Iteration 25/25 | Loss: 0.00138026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138026
Iteration 2/1000 | Loss: 0.00020076
Iteration 3/1000 | Loss: 0.00005840
Iteration 4/1000 | Loss: 0.00006668
Iteration 5/1000 | Loss: 0.00029122
Iteration 6/1000 | Loss: 0.00045316
Iteration 7/1000 | Loss: 0.00015627
Iteration 8/1000 | Loss: 0.00014272
Iteration 9/1000 | Loss: 0.00005276
Iteration 10/1000 | Loss: 0.00024158
Iteration 11/1000 | Loss: 0.00032136
Iteration 12/1000 | Loss: 0.00015166
Iteration 13/1000 | Loss: 0.00013404
Iteration 14/1000 | Loss: 0.00030549
Iteration 15/1000 | Loss: 0.00024577
Iteration 16/1000 | Loss: 0.00005621
Iteration 17/1000 | Loss: 0.00004921
Iteration 18/1000 | Loss: 0.00027039
Iteration 19/1000 | Loss: 0.00043887
Iteration 20/1000 | Loss: 0.00032884
Iteration 21/1000 | Loss: 0.00014960
Iteration 22/1000 | Loss: 0.00022784
Iteration 23/1000 | Loss: 0.00033477
Iteration 24/1000 | Loss: 0.00007126
Iteration 25/1000 | Loss: 0.00029413
Iteration 26/1000 | Loss: 0.00052604
Iteration 27/1000 | Loss: 0.00078791
Iteration 28/1000 | Loss: 0.00056152
Iteration 29/1000 | Loss: 0.00040813
Iteration 30/1000 | Loss: 0.00054276
Iteration 31/1000 | Loss: 0.00037651
Iteration 32/1000 | Loss: 0.00022138
Iteration 33/1000 | Loss: 0.00021933
Iteration 34/1000 | Loss: 0.00009764
Iteration 35/1000 | Loss: 0.00016968
Iteration 36/1000 | Loss: 0.00009199
Iteration 37/1000 | Loss: 0.00012163
Iteration 38/1000 | Loss: 0.00053728
Iteration 39/1000 | Loss: 0.00047935
Iteration 40/1000 | Loss: 0.00023695
Iteration 41/1000 | Loss: 0.00043565
Iteration 42/1000 | Loss: 0.00043332
Iteration 43/1000 | Loss: 0.00030107
Iteration 44/1000 | Loss: 0.00040587
Iteration 45/1000 | Loss: 0.00039548
Iteration 46/1000 | Loss: 0.00049429
Iteration 47/1000 | Loss: 0.00016175
Iteration 48/1000 | Loss: 0.00014964
Iteration 49/1000 | Loss: 0.00006649
Iteration 50/1000 | Loss: 0.00049436
Iteration 51/1000 | Loss: 0.00061956
Iteration 52/1000 | Loss: 0.00018414
Iteration 53/1000 | Loss: 0.00005155
Iteration 54/1000 | Loss: 0.00004715
Iteration 55/1000 | Loss: 0.00023877
Iteration 56/1000 | Loss: 0.00023735
Iteration 57/1000 | Loss: 0.00013140
Iteration 58/1000 | Loss: 0.00012010
Iteration 59/1000 | Loss: 0.00033960
Iteration 60/1000 | Loss: 0.00032931
Iteration 61/1000 | Loss: 0.00006259
Iteration 62/1000 | Loss: 0.00004693
Iteration 63/1000 | Loss: 0.00016598
Iteration 64/1000 | Loss: 0.00100181
Iteration 65/1000 | Loss: 0.00105685
Iteration 66/1000 | Loss: 0.00062828
Iteration 67/1000 | Loss: 0.00065105
Iteration 68/1000 | Loss: 0.00026148
Iteration 69/1000 | Loss: 0.00030136
Iteration 70/1000 | Loss: 0.00005210
Iteration 71/1000 | Loss: 0.00023854
Iteration 72/1000 | Loss: 0.00012635
Iteration 73/1000 | Loss: 0.00004113
Iteration 74/1000 | Loss: 0.00003633
Iteration 75/1000 | Loss: 0.00003304
Iteration 76/1000 | Loss: 0.00016049
Iteration 77/1000 | Loss: 0.00012340
Iteration 78/1000 | Loss: 0.00013830
Iteration 79/1000 | Loss: 0.00016432
Iteration 80/1000 | Loss: 0.00005780
Iteration 81/1000 | Loss: 0.00009733
Iteration 82/1000 | Loss: 0.00028722
Iteration 83/1000 | Loss: 0.00022515
Iteration 84/1000 | Loss: 0.00007238
Iteration 85/1000 | Loss: 0.00042416
Iteration 86/1000 | Loss: 0.00003894
Iteration 87/1000 | Loss: 0.00005716
Iteration 88/1000 | Loss: 0.00059185
Iteration 89/1000 | Loss: 0.00062832
Iteration 90/1000 | Loss: 0.00041897
Iteration 91/1000 | Loss: 0.00020742
Iteration 92/1000 | Loss: 0.00041474
Iteration 93/1000 | Loss: 0.00006173
Iteration 94/1000 | Loss: 0.00078047
Iteration 95/1000 | Loss: 0.00100187
Iteration 96/1000 | Loss: 0.00061921
Iteration 97/1000 | Loss: 0.00031352
Iteration 98/1000 | Loss: 0.00065337
Iteration 99/1000 | Loss: 0.00042303
Iteration 100/1000 | Loss: 0.00049429
Iteration 101/1000 | Loss: 0.00047715
Iteration 102/1000 | Loss: 0.00044963
Iteration 103/1000 | Loss: 0.00042890
Iteration 104/1000 | Loss: 0.00043334
Iteration 105/1000 | Loss: 0.00031493
Iteration 106/1000 | Loss: 0.00082636
Iteration 107/1000 | Loss: 0.00061843
Iteration 108/1000 | Loss: 0.00067070
Iteration 109/1000 | Loss: 0.00050177
Iteration 110/1000 | Loss: 0.00078505
Iteration 111/1000 | Loss: 0.00057042
Iteration 112/1000 | Loss: 0.00073111
Iteration 113/1000 | Loss: 0.00056193
Iteration 114/1000 | Loss: 0.00031338
Iteration 115/1000 | Loss: 0.00063422
Iteration 116/1000 | Loss: 0.00087191
Iteration 117/1000 | Loss: 0.00032258
Iteration 118/1000 | Loss: 0.00051764
Iteration 119/1000 | Loss: 0.00031369
Iteration 120/1000 | Loss: 0.00004324
Iteration 121/1000 | Loss: 0.00038293
Iteration 122/1000 | Loss: 0.00020300
Iteration 123/1000 | Loss: 0.00015529
Iteration 124/1000 | Loss: 0.00015495
Iteration 125/1000 | Loss: 0.00013282
Iteration 126/1000 | Loss: 0.00012477
Iteration 127/1000 | Loss: 0.00014205
Iteration 128/1000 | Loss: 0.00011329
Iteration 129/1000 | Loss: 0.00003697
Iteration 130/1000 | Loss: 0.00041136
Iteration 131/1000 | Loss: 0.00025487
Iteration 132/1000 | Loss: 0.00036414
Iteration 133/1000 | Loss: 0.00019196
Iteration 134/1000 | Loss: 0.00004144
Iteration 135/1000 | Loss: 0.00003992
Iteration 136/1000 | Loss: 0.00003465
Iteration 137/1000 | Loss: 0.00003186
Iteration 138/1000 | Loss: 0.00015586
Iteration 139/1000 | Loss: 0.00010207
Iteration 140/1000 | Loss: 0.00003028
Iteration 141/1000 | Loss: 0.00002957
Iteration 142/1000 | Loss: 0.00015833
Iteration 143/1000 | Loss: 0.00048492
Iteration 144/1000 | Loss: 0.00013392
Iteration 145/1000 | Loss: 0.00012441
Iteration 146/1000 | Loss: 0.00004394
Iteration 147/1000 | Loss: 0.00020045
Iteration 148/1000 | Loss: 0.00085041
Iteration 149/1000 | Loss: 0.00005636
Iteration 150/1000 | Loss: 0.00005344
Iteration 151/1000 | Loss: 0.00025071
Iteration 152/1000 | Loss: 0.00014725
Iteration 153/1000 | Loss: 0.00008518
Iteration 154/1000 | Loss: 0.00023072
Iteration 155/1000 | Loss: 0.00026499
Iteration 156/1000 | Loss: 0.00031616
Iteration 157/1000 | Loss: 0.00019728
Iteration 158/1000 | Loss: 0.00026295
Iteration 159/1000 | Loss: 0.00013378
Iteration 160/1000 | Loss: 0.00014858
Iteration 161/1000 | Loss: 0.00013653
Iteration 162/1000 | Loss: 0.00015092
Iteration 163/1000 | Loss: 0.00005798
Iteration 164/1000 | Loss: 0.00014122
Iteration 165/1000 | Loss: 0.00015492
Iteration 166/1000 | Loss: 0.00012283
Iteration 167/1000 | Loss: 0.00016007
Iteration 168/1000 | Loss: 0.00019305
Iteration 169/1000 | Loss: 0.00018161
Iteration 170/1000 | Loss: 0.00003762
Iteration 171/1000 | Loss: 0.00004152
Iteration 172/1000 | Loss: 0.00003328
Iteration 173/1000 | Loss: 0.00003874
Iteration 174/1000 | Loss: 0.00012933
Iteration 175/1000 | Loss: 0.00004547
Iteration 176/1000 | Loss: 0.00018062
Iteration 177/1000 | Loss: 0.00004986
Iteration 178/1000 | Loss: 0.00004371
Iteration 179/1000 | Loss: 0.00003364
Iteration 180/1000 | Loss: 0.00004050
Iteration 181/1000 | Loss: 0.00003759
Iteration 182/1000 | Loss: 0.00021780
Iteration 183/1000 | Loss: 0.00010206
Iteration 184/1000 | Loss: 0.00019908
Iteration 185/1000 | Loss: 0.00005001
Iteration 186/1000 | Loss: 0.00003498
Iteration 187/1000 | Loss: 0.00003146
Iteration 188/1000 | Loss: 0.00002928
Iteration 189/1000 | Loss: 0.00002811
Iteration 190/1000 | Loss: 0.00002704
Iteration 191/1000 | Loss: 0.00002636
Iteration 192/1000 | Loss: 0.00044476
Iteration 193/1000 | Loss: 0.00036392
Iteration 194/1000 | Loss: 0.00009496
Iteration 195/1000 | Loss: 0.00005650
Iteration 196/1000 | Loss: 0.00036058
Iteration 197/1000 | Loss: 0.00003000
Iteration 198/1000 | Loss: 0.00002453
Iteration 199/1000 | Loss: 0.00002325
Iteration 200/1000 | Loss: 0.00002221
Iteration 201/1000 | Loss: 0.00022166
Iteration 202/1000 | Loss: 0.00002963
Iteration 203/1000 | Loss: 0.00002416
Iteration 204/1000 | Loss: 0.00002227
Iteration 205/1000 | Loss: 0.00019376
Iteration 206/1000 | Loss: 0.00002747
Iteration 207/1000 | Loss: 0.00002683
Iteration 208/1000 | Loss: 0.00002600
Iteration 209/1000 | Loss: 0.00002726
Iteration 210/1000 | Loss: 0.00017911
Iteration 211/1000 | Loss: 0.00011834
Iteration 212/1000 | Loss: 0.00012819
Iteration 213/1000 | Loss: 0.00013339
Iteration 214/1000 | Loss: 0.00006564
Iteration 215/1000 | Loss: 0.00013907
Iteration 216/1000 | Loss: 0.00008143
Iteration 217/1000 | Loss: 0.00017478
Iteration 218/1000 | Loss: 0.00016359
Iteration 219/1000 | Loss: 0.00012734
Iteration 220/1000 | Loss: 0.00003342
Iteration 221/1000 | Loss: 0.00002527
Iteration 222/1000 | Loss: 0.00018340
Iteration 223/1000 | Loss: 0.00007990
Iteration 224/1000 | Loss: 0.00002920
Iteration 225/1000 | Loss: 0.00002448
Iteration 226/1000 | Loss: 0.00002300
Iteration 227/1000 | Loss: 0.00002224
Iteration 228/1000 | Loss: 0.00002145
Iteration 229/1000 | Loss: 0.00002076
Iteration 230/1000 | Loss: 0.00002033
Iteration 231/1000 | Loss: 0.00001975
Iteration 232/1000 | Loss: 0.00001929
Iteration 233/1000 | Loss: 0.00001903
Iteration 234/1000 | Loss: 0.00001879
Iteration 235/1000 | Loss: 0.00002556
Iteration 236/1000 | Loss: 0.00001906
Iteration 237/1000 | Loss: 0.00001867
Iteration 238/1000 | Loss: 0.00001861
Iteration 239/1000 | Loss: 0.00001841
Iteration 240/1000 | Loss: 0.00001835
Iteration 241/1000 | Loss: 0.00001835
Iteration 242/1000 | Loss: 0.00001835
Iteration 243/1000 | Loss: 0.00001834
Iteration 244/1000 | Loss: 0.00001834
Iteration 245/1000 | Loss: 0.00001834
Iteration 246/1000 | Loss: 0.00001834
Iteration 247/1000 | Loss: 0.00001833
Iteration 248/1000 | Loss: 0.00001833
Iteration 249/1000 | Loss: 0.00001833
Iteration 250/1000 | Loss: 0.00001833
Iteration 251/1000 | Loss: 0.00001833
Iteration 252/1000 | Loss: 0.00001833
Iteration 253/1000 | Loss: 0.00001833
Iteration 254/1000 | Loss: 0.00001833
Iteration 255/1000 | Loss: 0.00001832
Iteration 256/1000 | Loss: 0.00001832
Iteration 257/1000 | Loss: 0.00001832
Iteration 258/1000 | Loss: 0.00001832
Iteration 259/1000 | Loss: 0.00001832
Iteration 260/1000 | Loss: 0.00001831
Iteration 261/1000 | Loss: 0.00001831
Iteration 262/1000 | Loss: 0.00001831
Iteration 263/1000 | Loss: 0.00001830
Iteration 264/1000 | Loss: 0.00001830
Iteration 265/1000 | Loss: 0.00001830
Iteration 266/1000 | Loss: 0.00001829
Iteration 267/1000 | Loss: 0.00001829
Iteration 268/1000 | Loss: 0.00001828
Iteration 269/1000 | Loss: 0.00001828
Iteration 270/1000 | Loss: 0.00001828
Iteration 271/1000 | Loss: 0.00001827
Iteration 272/1000 | Loss: 0.00001827
Iteration 273/1000 | Loss: 0.00001826
Iteration 274/1000 | Loss: 0.00001826
Iteration 275/1000 | Loss: 0.00001826
Iteration 276/1000 | Loss: 0.00001826
Iteration 277/1000 | Loss: 0.00001825
Iteration 278/1000 | Loss: 0.00001825
Iteration 279/1000 | Loss: 0.00001825
Iteration 280/1000 | Loss: 0.00001824
Iteration 281/1000 | Loss: 0.00001823
Iteration 282/1000 | Loss: 0.00001822
Iteration 283/1000 | Loss: 0.00001822
Iteration 284/1000 | Loss: 0.00001822
Iteration 285/1000 | Loss: 0.00001822
Iteration 286/1000 | Loss: 0.00001822
Iteration 287/1000 | Loss: 0.00001821
Iteration 288/1000 | Loss: 0.00001821
Iteration 289/1000 | Loss: 0.00001821
Iteration 290/1000 | Loss: 0.00001821
Iteration 291/1000 | Loss: 0.00001821
Iteration 292/1000 | Loss: 0.00001821
Iteration 293/1000 | Loss: 0.00001821
Iteration 294/1000 | Loss: 0.00001821
Iteration 295/1000 | Loss: 0.00001821
Iteration 296/1000 | Loss: 0.00001821
Iteration 297/1000 | Loss: 0.00001821
Iteration 298/1000 | Loss: 0.00001820
Iteration 299/1000 | Loss: 0.00001820
Iteration 300/1000 | Loss: 0.00001820
Iteration 301/1000 | Loss: 0.00001819
Iteration 302/1000 | Loss: 0.00001819
Iteration 303/1000 | Loss: 0.00001819
Iteration 304/1000 | Loss: 0.00001818
Iteration 305/1000 | Loss: 0.00001818
Iteration 306/1000 | Loss: 0.00001818
Iteration 307/1000 | Loss: 0.00001818
Iteration 308/1000 | Loss: 0.00001818
Iteration 309/1000 | Loss: 0.00001818
Iteration 310/1000 | Loss: 0.00001818
Iteration 311/1000 | Loss: 0.00001818
Iteration 312/1000 | Loss: 0.00001818
Iteration 313/1000 | Loss: 0.00001818
Iteration 314/1000 | Loss: 0.00001818
Iteration 315/1000 | Loss: 0.00001818
Iteration 316/1000 | Loss: 0.00001818
Iteration 317/1000 | Loss: 0.00001817
Iteration 318/1000 | Loss: 0.00001817
Iteration 319/1000 | Loss: 0.00001817
Iteration 320/1000 | Loss: 0.00001817
Iteration 321/1000 | Loss: 0.00001817
Iteration 322/1000 | Loss: 0.00001817
Iteration 323/1000 | Loss: 0.00001817
Iteration 324/1000 | Loss: 0.00001817
Iteration 325/1000 | Loss: 0.00001817
Iteration 326/1000 | Loss: 0.00001817
Iteration 327/1000 | Loss: 0.00001817
Iteration 328/1000 | Loss: 0.00001817
Iteration 329/1000 | Loss: 0.00001817
Iteration 330/1000 | Loss: 0.00001817
Iteration 331/1000 | Loss: 0.00001817
Iteration 332/1000 | Loss: 0.00001816
Iteration 333/1000 | Loss: 0.00001816
Iteration 334/1000 | Loss: 0.00001816
Iteration 335/1000 | Loss: 0.00001816
Iteration 336/1000 | Loss: 0.00001816
Iteration 337/1000 | Loss: 0.00001816
Iteration 338/1000 | Loss: 0.00001816
Iteration 339/1000 | Loss: 0.00001816
Iteration 340/1000 | Loss: 0.00001816
Iteration 341/1000 | Loss: 0.00001816
Iteration 342/1000 | Loss: 0.00001816
Iteration 343/1000 | Loss: 0.00001816
Iteration 344/1000 | Loss: 0.00001816
Iteration 345/1000 | Loss: 0.00001816
Iteration 346/1000 | Loss: 0.00001816
Iteration 347/1000 | Loss: 0.00001816
Iteration 348/1000 | Loss: 0.00001816
Iteration 349/1000 | Loss: 0.00001816
Iteration 350/1000 | Loss: 0.00001816
Iteration 351/1000 | Loss: 0.00001816
Iteration 352/1000 | Loss: 0.00001816
Iteration 353/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.8155318684875965e-05, 1.8155318684875965e-05, 1.8155318684875965e-05, 1.8155318684875965e-05, 1.8155318684875965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8155318684875965e-05

Optimization complete. Final v2v error: 3.4040722846984863 mm

Highest mean error: 5.332406044006348 mm for frame 156

Lowest mean error: 2.8208634853363037 mm for frame 40

Saving results

Total time: 422.44774889945984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093660
Iteration 2/25 | Loss: 0.00360100
Iteration 3/25 | Loss: 0.00252069
Iteration 4/25 | Loss: 0.00239666
Iteration 5/25 | Loss: 0.00159950
Iteration 6/25 | Loss: 0.00154939
Iteration 7/25 | Loss: 0.00152139
Iteration 8/25 | Loss: 0.00151112
Iteration 9/25 | Loss: 0.00150008
Iteration 10/25 | Loss: 0.00150380
Iteration 11/25 | Loss: 0.00150168
Iteration 12/25 | Loss: 0.00149770
Iteration 13/25 | Loss: 0.00149357
Iteration 14/25 | Loss: 0.00149318
Iteration 15/25 | Loss: 0.00149261
Iteration 16/25 | Loss: 0.00149231
Iteration 17/25 | Loss: 0.00149221
Iteration 18/25 | Loss: 0.00149220
Iteration 19/25 | Loss: 0.00149220
Iteration 20/25 | Loss: 0.00149220
Iteration 21/25 | Loss: 0.00149220
Iteration 22/25 | Loss: 0.00149220
Iteration 23/25 | Loss: 0.00149220
Iteration 24/25 | Loss: 0.00149219
Iteration 25/25 | Loss: 0.00149219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61776507
Iteration 2/25 | Loss: 0.00322703
Iteration 3/25 | Loss: 0.00203120
Iteration 4/25 | Loss: 0.00151152
Iteration 5/25 | Loss: 0.00151152
Iteration 6/25 | Loss: 0.00151151
Iteration 7/25 | Loss: 0.00151151
Iteration 8/25 | Loss: 0.00151151
Iteration 9/25 | Loss: 0.00151151
Iteration 10/25 | Loss: 0.00151151
Iteration 11/25 | Loss: 0.00151151
Iteration 12/25 | Loss: 0.00151151
Iteration 13/25 | Loss: 0.00151151
Iteration 14/25 | Loss: 0.00151151
Iteration 15/25 | Loss: 0.00151151
Iteration 16/25 | Loss: 0.00151151
Iteration 17/25 | Loss: 0.00151151
Iteration 18/25 | Loss: 0.00151151
Iteration 19/25 | Loss: 0.00151151
Iteration 20/25 | Loss: 0.00151151
Iteration 21/25 | Loss: 0.00151151
Iteration 22/25 | Loss: 0.00151151
Iteration 23/25 | Loss: 0.00151151
Iteration 24/25 | Loss: 0.00151151
Iteration 25/25 | Loss: 0.00151151
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015115119749680161, 0.0015115119749680161, 0.0015115119749680161, 0.0015115119749680161, 0.0015115119749680161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015115119749680161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151151
Iteration 2/1000 | Loss: 0.00163802
Iteration 3/1000 | Loss: 0.00004127
Iteration 4/1000 | Loss: 0.00002877
Iteration 5/1000 | Loss: 0.00002603
Iteration 6/1000 | Loss: 0.00002531
Iteration 7/1000 | Loss: 0.00002477
Iteration 8/1000 | Loss: 0.00002431
Iteration 9/1000 | Loss: 0.00002398
Iteration 10/1000 | Loss: 0.00002371
Iteration 11/1000 | Loss: 0.00002340
Iteration 12/1000 | Loss: 0.00002315
Iteration 13/1000 | Loss: 0.00002289
Iteration 14/1000 | Loss: 0.00002270
Iteration 15/1000 | Loss: 0.00002247
Iteration 16/1000 | Loss: 0.00002229
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002202
Iteration 21/1000 | Loss: 0.00002199
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002198
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00002196
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002192
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002188
Iteration 32/1000 | Loss: 0.00002186
Iteration 33/1000 | Loss: 0.00002186
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00002185
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00002182
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002180
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002179
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002177
Iteration 64/1000 | Loss: 0.00002177
Iteration 65/1000 | Loss: 0.00002177
Iteration 66/1000 | Loss: 0.00002177
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002177
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002176
Iteration 72/1000 | Loss: 0.00002176
Iteration 73/1000 | Loss: 0.00002176
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.1763409677078016e-05, 2.1763409677078016e-05, 2.1763409677078016e-05, 2.1763409677078016e-05, 2.1763409677078016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1763409677078016e-05

Optimization complete. Final v2v error: 3.7030529975891113 mm

Highest mean error: 9.722468376159668 mm for frame 80

Lowest mean error: 3.2938039302825928 mm for frame 4

Saving results

Total time: 61.59005570411682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793712
Iteration 2/25 | Loss: 0.00168080
Iteration 3/25 | Loss: 0.00147714
Iteration 4/25 | Loss: 0.00146159
Iteration 5/25 | Loss: 0.00145664
Iteration 6/25 | Loss: 0.00145604
Iteration 7/25 | Loss: 0.00145604
Iteration 8/25 | Loss: 0.00145604
Iteration 9/25 | Loss: 0.00145604
Iteration 10/25 | Loss: 0.00145604
Iteration 11/25 | Loss: 0.00145604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014560429845005274, 0.0014560429845005274, 0.0014560429845005274, 0.0014560429845005274, 0.0014560429845005274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014560429845005274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.24787290
Iteration 2/25 | Loss: 0.00168532
Iteration 3/25 | Loss: 0.00168532
Iteration 4/25 | Loss: 0.00168532
Iteration 5/25 | Loss: 0.00168532
Iteration 6/25 | Loss: 0.00168532
Iteration 7/25 | Loss: 0.00168532
Iteration 8/25 | Loss: 0.00168532
Iteration 9/25 | Loss: 0.00168532
Iteration 10/25 | Loss: 0.00168532
Iteration 11/25 | Loss: 0.00168532
Iteration 12/25 | Loss: 0.00168532
Iteration 13/25 | Loss: 0.00168532
Iteration 14/25 | Loss: 0.00168532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016853208653628826, 0.0016853208653628826, 0.0016853208653628826, 0.0016853208653628826, 0.0016853208653628826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016853208653628826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168532
Iteration 2/1000 | Loss: 0.00008402
Iteration 3/1000 | Loss: 0.00005435
Iteration 4/1000 | Loss: 0.00004273
Iteration 5/1000 | Loss: 0.00003878
Iteration 6/1000 | Loss: 0.00003696
Iteration 7/1000 | Loss: 0.00003573
Iteration 8/1000 | Loss: 0.00003451
Iteration 9/1000 | Loss: 0.00003362
Iteration 10/1000 | Loss: 0.00003308
Iteration 11/1000 | Loss: 0.00003257
Iteration 12/1000 | Loss: 0.00003216
Iteration 13/1000 | Loss: 0.00003185
Iteration 14/1000 | Loss: 0.00003151
Iteration 15/1000 | Loss: 0.00003121
Iteration 16/1000 | Loss: 0.00003098
Iteration 17/1000 | Loss: 0.00003083
Iteration 18/1000 | Loss: 0.00003066
Iteration 19/1000 | Loss: 0.00003058
Iteration 20/1000 | Loss: 0.00003045
Iteration 21/1000 | Loss: 0.00003040
Iteration 22/1000 | Loss: 0.00003033
Iteration 23/1000 | Loss: 0.00003029
Iteration 24/1000 | Loss: 0.00003027
Iteration 25/1000 | Loss: 0.00003026
Iteration 26/1000 | Loss: 0.00003022
Iteration 27/1000 | Loss: 0.00003021
Iteration 28/1000 | Loss: 0.00003020
Iteration 29/1000 | Loss: 0.00003018
Iteration 30/1000 | Loss: 0.00003018
Iteration 31/1000 | Loss: 0.00003018
Iteration 32/1000 | Loss: 0.00003018
Iteration 33/1000 | Loss: 0.00003018
Iteration 34/1000 | Loss: 0.00003017
Iteration 35/1000 | Loss: 0.00003017
Iteration 36/1000 | Loss: 0.00003017
Iteration 37/1000 | Loss: 0.00003017
Iteration 38/1000 | Loss: 0.00003016
Iteration 39/1000 | Loss: 0.00003016
Iteration 40/1000 | Loss: 0.00003016
Iteration 41/1000 | Loss: 0.00003016
Iteration 42/1000 | Loss: 0.00003015
Iteration 43/1000 | Loss: 0.00003015
Iteration 44/1000 | Loss: 0.00003015
Iteration 45/1000 | Loss: 0.00003015
Iteration 46/1000 | Loss: 0.00003015
Iteration 47/1000 | Loss: 0.00003015
Iteration 48/1000 | Loss: 0.00003015
Iteration 49/1000 | Loss: 0.00003015
Iteration 50/1000 | Loss: 0.00003015
Iteration 51/1000 | Loss: 0.00003015
Iteration 52/1000 | Loss: 0.00003014
Iteration 53/1000 | Loss: 0.00003012
Iteration 54/1000 | Loss: 0.00003012
Iteration 55/1000 | Loss: 0.00003012
Iteration 56/1000 | Loss: 0.00003012
Iteration 57/1000 | Loss: 0.00003012
Iteration 58/1000 | Loss: 0.00003012
Iteration 59/1000 | Loss: 0.00003012
Iteration 60/1000 | Loss: 0.00003012
Iteration 61/1000 | Loss: 0.00003012
Iteration 62/1000 | Loss: 0.00003012
Iteration 63/1000 | Loss: 0.00003011
Iteration 64/1000 | Loss: 0.00003011
Iteration 65/1000 | Loss: 0.00003011
Iteration 66/1000 | Loss: 0.00003011
Iteration 67/1000 | Loss: 0.00003011
Iteration 68/1000 | Loss: 0.00003010
Iteration 69/1000 | Loss: 0.00003010
Iteration 70/1000 | Loss: 0.00003009
Iteration 71/1000 | Loss: 0.00003009
Iteration 72/1000 | Loss: 0.00003009
Iteration 73/1000 | Loss: 0.00003008
Iteration 74/1000 | Loss: 0.00003008
Iteration 75/1000 | Loss: 0.00003008
Iteration 76/1000 | Loss: 0.00003008
Iteration 77/1000 | Loss: 0.00003008
Iteration 78/1000 | Loss: 0.00003008
Iteration 79/1000 | Loss: 0.00003008
Iteration 80/1000 | Loss: 0.00003008
Iteration 81/1000 | Loss: 0.00003008
Iteration 82/1000 | Loss: 0.00003007
Iteration 83/1000 | Loss: 0.00003007
Iteration 84/1000 | Loss: 0.00003007
Iteration 85/1000 | Loss: 0.00003007
Iteration 86/1000 | Loss: 0.00003007
Iteration 87/1000 | Loss: 0.00003007
Iteration 88/1000 | Loss: 0.00003006
Iteration 89/1000 | Loss: 0.00003006
Iteration 90/1000 | Loss: 0.00003006
Iteration 91/1000 | Loss: 0.00003006
Iteration 92/1000 | Loss: 0.00003005
Iteration 93/1000 | Loss: 0.00003005
Iteration 94/1000 | Loss: 0.00003005
Iteration 95/1000 | Loss: 0.00003005
Iteration 96/1000 | Loss: 0.00003004
Iteration 97/1000 | Loss: 0.00003004
Iteration 98/1000 | Loss: 0.00003004
Iteration 99/1000 | Loss: 0.00003004
Iteration 100/1000 | Loss: 0.00003003
Iteration 101/1000 | Loss: 0.00003003
Iteration 102/1000 | Loss: 0.00003003
Iteration 103/1000 | Loss: 0.00003003
Iteration 104/1000 | Loss: 0.00003003
Iteration 105/1000 | Loss: 0.00003002
Iteration 106/1000 | Loss: 0.00003002
Iteration 107/1000 | Loss: 0.00003002
Iteration 108/1000 | Loss: 0.00003002
Iteration 109/1000 | Loss: 0.00003002
Iteration 110/1000 | Loss: 0.00003002
Iteration 111/1000 | Loss: 0.00003002
Iteration 112/1000 | Loss: 0.00003002
Iteration 113/1000 | Loss: 0.00003001
Iteration 114/1000 | Loss: 0.00003001
Iteration 115/1000 | Loss: 0.00003001
Iteration 116/1000 | Loss: 0.00003001
Iteration 117/1000 | Loss: 0.00003001
Iteration 118/1000 | Loss: 0.00003001
Iteration 119/1000 | Loss: 0.00003000
Iteration 120/1000 | Loss: 0.00003000
Iteration 121/1000 | Loss: 0.00003000
Iteration 122/1000 | Loss: 0.00003000
Iteration 123/1000 | Loss: 0.00003000
Iteration 124/1000 | Loss: 0.00002999
Iteration 125/1000 | Loss: 0.00002999
Iteration 126/1000 | Loss: 0.00002999
Iteration 127/1000 | Loss: 0.00002999
Iteration 128/1000 | Loss: 0.00002999
Iteration 129/1000 | Loss: 0.00002999
Iteration 130/1000 | Loss: 0.00002999
Iteration 131/1000 | Loss: 0.00002999
Iteration 132/1000 | Loss: 0.00002999
Iteration 133/1000 | Loss: 0.00002999
Iteration 134/1000 | Loss: 0.00002999
Iteration 135/1000 | Loss: 0.00002999
Iteration 136/1000 | Loss: 0.00002999
Iteration 137/1000 | Loss: 0.00002999
Iteration 138/1000 | Loss: 0.00002999
Iteration 139/1000 | Loss: 0.00002999
Iteration 140/1000 | Loss: 0.00002999
Iteration 141/1000 | Loss: 0.00002999
Iteration 142/1000 | Loss: 0.00002999
Iteration 143/1000 | Loss: 0.00002999
Iteration 144/1000 | Loss: 0.00002999
Iteration 145/1000 | Loss: 0.00002999
Iteration 146/1000 | Loss: 0.00002999
Iteration 147/1000 | Loss: 0.00002999
Iteration 148/1000 | Loss: 0.00002999
Iteration 149/1000 | Loss: 0.00002999
Iteration 150/1000 | Loss: 0.00002999
Iteration 151/1000 | Loss: 0.00002999
Iteration 152/1000 | Loss: 0.00002999
Iteration 153/1000 | Loss: 0.00002999
Iteration 154/1000 | Loss: 0.00002999
Iteration 155/1000 | Loss: 0.00002999
Iteration 156/1000 | Loss: 0.00002999
Iteration 157/1000 | Loss: 0.00002999
Iteration 158/1000 | Loss: 0.00002999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.9993361749802716e-05, 2.9993361749802716e-05, 2.9993361749802716e-05, 2.9993361749802716e-05, 2.9993361749802716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9993361749802716e-05

Optimization complete. Final v2v error: 4.442440509796143 mm

Highest mean error: 5.589050769805908 mm for frame 152

Lowest mean error: 3.4775655269622803 mm for frame 16

Saving results

Total time: 47.15961742401123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358366
Iteration 2/25 | Loss: 0.00136953
Iteration 3/25 | Loss: 0.00126605
Iteration 4/25 | Loss: 0.00125815
Iteration 5/25 | Loss: 0.00125576
Iteration 6/25 | Loss: 0.00125515
Iteration 7/25 | Loss: 0.00125515
Iteration 8/25 | Loss: 0.00125515
Iteration 9/25 | Loss: 0.00125515
Iteration 10/25 | Loss: 0.00125515
Iteration 11/25 | Loss: 0.00125515
Iteration 12/25 | Loss: 0.00125515
Iteration 13/25 | Loss: 0.00125515
Iteration 14/25 | Loss: 0.00125515
Iteration 15/25 | Loss: 0.00125515
Iteration 16/25 | Loss: 0.00125515
Iteration 17/25 | Loss: 0.00125515
Iteration 18/25 | Loss: 0.00125515
Iteration 19/25 | Loss: 0.00125515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012551482068374753, 0.0012551482068374753, 0.0012551482068374753, 0.0012551482068374753, 0.0012551482068374753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012551482068374753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24701667
Iteration 2/25 | Loss: 0.00165235
Iteration 3/25 | Loss: 0.00165235
Iteration 4/25 | Loss: 0.00165234
Iteration 5/25 | Loss: 0.00165234
Iteration 6/25 | Loss: 0.00165234
Iteration 7/25 | Loss: 0.00165234
Iteration 8/25 | Loss: 0.00165234
Iteration 9/25 | Loss: 0.00165234
Iteration 10/25 | Loss: 0.00165234
Iteration 11/25 | Loss: 0.00165234
Iteration 12/25 | Loss: 0.00165234
Iteration 13/25 | Loss: 0.00165234
Iteration 14/25 | Loss: 0.00165234
Iteration 15/25 | Loss: 0.00165234
Iteration 16/25 | Loss: 0.00165234
Iteration 17/25 | Loss: 0.00165234
Iteration 18/25 | Loss: 0.00165234
Iteration 19/25 | Loss: 0.00165234
Iteration 20/25 | Loss: 0.00165234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016523420345038176, 0.0016523420345038176, 0.0016523420345038176, 0.0016523420345038176, 0.0016523420345038176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016523420345038176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165234
Iteration 2/1000 | Loss: 0.00004059
Iteration 3/1000 | Loss: 0.00002678
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001484
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001463
Iteration 13/1000 | Loss: 0.00001443
Iteration 14/1000 | Loss: 0.00001443
Iteration 15/1000 | Loss: 0.00001435
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001430
Iteration 18/1000 | Loss: 0.00001423
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001388
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001374
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001359
Iteration 67/1000 | Loss: 0.00001359
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001359
Iteration 87/1000 | Loss: 0.00001359
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001359
Iteration 96/1000 | Loss: 0.00001359
Iteration 97/1000 | Loss: 0.00001359
Iteration 98/1000 | Loss: 0.00001359
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.3586167369794566e-05, 1.3586167369794566e-05, 1.3586167369794566e-05, 1.3586167369794566e-05, 1.3586167369794566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3586167369794566e-05

Optimization complete. Final v2v error: 3.19276762008667 mm

Highest mean error: 3.653959274291992 mm for frame 73

Lowest mean error: 2.8517768383026123 mm for frame 157

Saving results

Total time: 35.934749364852905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813464
Iteration 2/25 | Loss: 0.00156003
Iteration 3/25 | Loss: 0.00132888
Iteration 4/25 | Loss: 0.00130966
Iteration 5/25 | Loss: 0.00130770
Iteration 6/25 | Loss: 0.00130770
Iteration 7/25 | Loss: 0.00130770
Iteration 8/25 | Loss: 0.00130770
Iteration 9/25 | Loss: 0.00130770
Iteration 10/25 | Loss: 0.00130770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013077033217996359, 0.0013077033217996359, 0.0013077033217996359, 0.0013077033217996359, 0.0013077033217996359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013077033217996359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92580897
Iteration 2/25 | Loss: 0.00082060
Iteration 3/25 | Loss: 0.00082059
Iteration 4/25 | Loss: 0.00082059
Iteration 5/25 | Loss: 0.00082059
Iteration 6/25 | Loss: 0.00082059
Iteration 7/25 | Loss: 0.00082058
Iteration 8/25 | Loss: 0.00082058
Iteration 9/25 | Loss: 0.00082058
Iteration 10/25 | Loss: 0.00082058
Iteration 11/25 | Loss: 0.00082058
Iteration 12/25 | Loss: 0.00082058
Iteration 13/25 | Loss: 0.00082058
Iteration 14/25 | Loss: 0.00082058
Iteration 15/25 | Loss: 0.00082058
Iteration 16/25 | Loss: 0.00082058
Iteration 17/25 | Loss: 0.00082058
Iteration 18/25 | Loss: 0.00082058
Iteration 19/25 | Loss: 0.00082058
Iteration 20/25 | Loss: 0.00082058
Iteration 21/25 | Loss: 0.00082058
Iteration 22/25 | Loss: 0.00082058
Iteration 23/25 | Loss: 0.00082058
Iteration 24/25 | Loss: 0.00082058
Iteration 25/25 | Loss: 0.00082058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082058
Iteration 2/1000 | Loss: 0.00003046
Iteration 3/1000 | Loss: 0.00002371
Iteration 4/1000 | Loss: 0.00002122
Iteration 5/1000 | Loss: 0.00002025
Iteration 6/1000 | Loss: 0.00001966
Iteration 7/1000 | Loss: 0.00001906
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001836
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001786
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001770
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001749
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001741
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001733
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001732
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00001730
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001713
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001706
Iteration 92/1000 | Loss: 0.00001706
Iteration 93/1000 | Loss: 0.00001706
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001704
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001704
Iteration 121/1000 | Loss: 0.00001704
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.7038384612533264e-05, 1.7038384612533264e-05, 1.7038384612533264e-05, 1.7038384612533264e-05, 1.7038384612533264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7038384612533264e-05

Optimization complete. Final v2v error: 3.486170530319214 mm

Highest mean error: 3.8335325717926025 mm for frame 26

Lowest mean error: 3.2999632358551025 mm for frame 129

Saving results

Total time: 36.605844497680664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442468
Iteration 2/25 | Loss: 0.00148215
Iteration 3/25 | Loss: 0.00132401
Iteration 4/25 | Loss: 0.00130348
Iteration 5/25 | Loss: 0.00129897
Iteration 6/25 | Loss: 0.00129805
Iteration 7/25 | Loss: 0.00129805
Iteration 8/25 | Loss: 0.00129805
Iteration 9/25 | Loss: 0.00129805
Iteration 10/25 | Loss: 0.00129805
Iteration 11/25 | Loss: 0.00129805
Iteration 12/25 | Loss: 0.00129805
Iteration 13/25 | Loss: 0.00129805
Iteration 14/25 | Loss: 0.00129805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012980500468984246, 0.0012980500468984246, 0.0012980500468984246, 0.0012980500468984246, 0.0012980500468984246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012980500468984246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30084193
Iteration 2/25 | Loss: 0.00144493
Iteration 3/25 | Loss: 0.00144492
Iteration 4/25 | Loss: 0.00144492
Iteration 5/25 | Loss: 0.00144492
Iteration 6/25 | Loss: 0.00144492
Iteration 7/25 | Loss: 0.00144492
Iteration 8/25 | Loss: 0.00144492
Iteration 9/25 | Loss: 0.00144492
Iteration 10/25 | Loss: 0.00144492
Iteration 11/25 | Loss: 0.00144492
Iteration 12/25 | Loss: 0.00144492
Iteration 13/25 | Loss: 0.00144492
Iteration 14/25 | Loss: 0.00144492
Iteration 15/25 | Loss: 0.00144492
Iteration 16/25 | Loss: 0.00144492
Iteration 17/25 | Loss: 0.00144492
Iteration 18/25 | Loss: 0.00144492
Iteration 19/25 | Loss: 0.00144492
Iteration 20/25 | Loss: 0.00144492
Iteration 21/25 | Loss: 0.00144492
Iteration 22/25 | Loss: 0.00144492
Iteration 23/25 | Loss: 0.00144492
Iteration 24/25 | Loss: 0.00144492
Iteration 25/25 | Loss: 0.00144492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144492
Iteration 2/1000 | Loss: 0.00002926
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001617
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001537
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001435
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001425
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001417
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001400
Iteration 39/1000 | Loss: 0.00001398
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001397
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001391
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001381
Iteration 77/1000 | Loss: 0.00001381
Iteration 78/1000 | Loss: 0.00001380
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001376
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001375
Iteration 97/1000 | Loss: 0.00001375
Iteration 98/1000 | Loss: 0.00001375
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001374
Iteration 104/1000 | Loss: 0.00001374
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001368
Iteration 134/1000 | Loss: 0.00001368
Iteration 135/1000 | Loss: 0.00001368
Iteration 136/1000 | Loss: 0.00001368
Iteration 137/1000 | Loss: 0.00001368
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001367
Iteration 140/1000 | Loss: 0.00001367
Iteration 141/1000 | Loss: 0.00001367
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001366
Iteration 144/1000 | Loss: 0.00001366
Iteration 145/1000 | Loss: 0.00001366
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001364
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001364
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001363
Iteration 161/1000 | Loss: 0.00001363
Iteration 162/1000 | Loss: 0.00001363
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Iteration 167/1000 | Loss: 0.00001363
Iteration 168/1000 | Loss: 0.00001362
Iteration 169/1000 | Loss: 0.00001362
Iteration 170/1000 | Loss: 0.00001362
Iteration 171/1000 | Loss: 0.00001362
Iteration 172/1000 | Loss: 0.00001362
Iteration 173/1000 | Loss: 0.00001361
Iteration 174/1000 | Loss: 0.00001361
Iteration 175/1000 | Loss: 0.00001361
Iteration 176/1000 | Loss: 0.00001361
Iteration 177/1000 | Loss: 0.00001361
Iteration 178/1000 | Loss: 0.00001361
Iteration 179/1000 | Loss: 0.00001361
Iteration 180/1000 | Loss: 0.00001361
Iteration 181/1000 | Loss: 0.00001361
Iteration 182/1000 | Loss: 0.00001361
Iteration 183/1000 | Loss: 0.00001361
Iteration 184/1000 | Loss: 0.00001361
Iteration 185/1000 | Loss: 0.00001361
Iteration 186/1000 | Loss: 0.00001361
Iteration 187/1000 | Loss: 0.00001361
Iteration 188/1000 | Loss: 0.00001361
Iteration 189/1000 | Loss: 0.00001360
Iteration 190/1000 | Loss: 0.00001360
Iteration 191/1000 | Loss: 0.00001360
Iteration 192/1000 | Loss: 0.00001360
Iteration 193/1000 | Loss: 0.00001360
Iteration 194/1000 | Loss: 0.00001360
Iteration 195/1000 | Loss: 0.00001360
Iteration 196/1000 | Loss: 0.00001360
Iteration 197/1000 | Loss: 0.00001359
Iteration 198/1000 | Loss: 0.00001359
Iteration 199/1000 | Loss: 0.00001359
Iteration 200/1000 | Loss: 0.00001358
Iteration 201/1000 | Loss: 0.00001358
Iteration 202/1000 | Loss: 0.00001358
Iteration 203/1000 | Loss: 0.00001358
Iteration 204/1000 | Loss: 0.00001358
Iteration 205/1000 | Loss: 0.00001358
Iteration 206/1000 | Loss: 0.00001358
Iteration 207/1000 | Loss: 0.00001358
Iteration 208/1000 | Loss: 0.00001357
Iteration 209/1000 | Loss: 0.00001357
Iteration 210/1000 | Loss: 0.00001357
Iteration 211/1000 | Loss: 0.00001357
Iteration 212/1000 | Loss: 0.00001357
Iteration 213/1000 | Loss: 0.00001357
Iteration 214/1000 | Loss: 0.00001357
Iteration 215/1000 | Loss: 0.00001357
Iteration 216/1000 | Loss: 0.00001357
Iteration 217/1000 | Loss: 0.00001357
Iteration 218/1000 | Loss: 0.00001357
Iteration 219/1000 | Loss: 0.00001356
Iteration 220/1000 | Loss: 0.00001356
Iteration 221/1000 | Loss: 0.00001356
Iteration 222/1000 | Loss: 0.00001356
Iteration 223/1000 | Loss: 0.00001356
Iteration 224/1000 | Loss: 0.00001356
Iteration 225/1000 | Loss: 0.00001356
Iteration 226/1000 | Loss: 0.00001356
Iteration 227/1000 | Loss: 0.00001355
Iteration 228/1000 | Loss: 0.00001355
Iteration 229/1000 | Loss: 0.00001355
Iteration 230/1000 | Loss: 0.00001355
Iteration 231/1000 | Loss: 0.00001355
Iteration 232/1000 | Loss: 0.00001355
Iteration 233/1000 | Loss: 0.00001355
Iteration 234/1000 | Loss: 0.00001354
Iteration 235/1000 | Loss: 0.00001354
Iteration 236/1000 | Loss: 0.00001354
Iteration 237/1000 | Loss: 0.00001354
Iteration 238/1000 | Loss: 0.00001354
Iteration 239/1000 | Loss: 0.00001354
Iteration 240/1000 | Loss: 0.00001354
Iteration 241/1000 | Loss: 0.00001354
Iteration 242/1000 | Loss: 0.00001354
Iteration 243/1000 | Loss: 0.00001354
Iteration 244/1000 | Loss: 0.00001354
Iteration 245/1000 | Loss: 0.00001354
Iteration 246/1000 | Loss: 0.00001353
Iteration 247/1000 | Loss: 0.00001353
Iteration 248/1000 | Loss: 0.00001353
Iteration 249/1000 | Loss: 0.00001353
Iteration 250/1000 | Loss: 0.00001353
Iteration 251/1000 | Loss: 0.00001353
Iteration 252/1000 | Loss: 0.00001353
Iteration 253/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.3534717254515272e-05, 1.3534717254515272e-05, 1.3534717254515272e-05, 1.3534717254515272e-05, 1.3534717254515272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3534717254515272e-05

Optimization complete. Final v2v error: 3.1596391201019287 mm

Highest mean error: 4.154357433319092 mm for frame 79

Lowest mean error: 2.741938829421997 mm for frame 65

Saving results

Total time: 52.61826539039612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519575
Iteration 2/25 | Loss: 0.00169657
Iteration 3/25 | Loss: 0.00138928
Iteration 4/25 | Loss: 0.00136983
Iteration 5/25 | Loss: 0.00136440
Iteration 6/25 | Loss: 0.00136267
Iteration 7/25 | Loss: 0.00136213
Iteration 8/25 | Loss: 0.00136213
Iteration 9/25 | Loss: 0.00136213
Iteration 10/25 | Loss: 0.00136213
Iteration 11/25 | Loss: 0.00136213
Iteration 12/25 | Loss: 0.00136213
Iteration 13/25 | Loss: 0.00136213
Iteration 14/25 | Loss: 0.00136213
Iteration 15/25 | Loss: 0.00136213
Iteration 16/25 | Loss: 0.00136213
Iteration 17/25 | Loss: 0.00136213
Iteration 18/25 | Loss: 0.00136213
Iteration 19/25 | Loss: 0.00136213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013621277175843716, 0.0013621277175843716, 0.0013621277175843716, 0.0013621277175843716, 0.0013621277175843716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013621277175843716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24619174
Iteration 2/25 | Loss: 0.00137439
Iteration 3/25 | Loss: 0.00137438
Iteration 4/25 | Loss: 0.00137437
Iteration 5/25 | Loss: 0.00137437
Iteration 6/25 | Loss: 0.00137437
Iteration 7/25 | Loss: 0.00137437
Iteration 8/25 | Loss: 0.00137437
Iteration 9/25 | Loss: 0.00137437
Iteration 10/25 | Loss: 0.00137437
Iteration 11/25 | Loss: 0.00137437
Iteration 12/25 | Loss: 0.00137437
Iteration 13/25 | Loss: 0.00137437
Iteration 14/25 | Loss: 0.00137437
Iteration 15/25 | Loss: 0.00137437
Iteration 16/25 | Loss: 0.00137437
Iteration 17/25 | Loss: 0.00137437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013743718154728413, 0.0013743718154728413, 0.0013743718154728413, 0.0013743718154728413, 0.0013743718154728413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013743718154728413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137437
Iteration 2/1000 | Loss: 0.00005984
Iteration 3/1000 | Loss: 0.00003825
Iteration 4/1000 | Loss: 0.00002838
Iteration 5/1000 | Loss: 0.00002649
Iteration 6/1000 | Loss: 0.00002513
Iteration 7/1000 | Loss: 0.00002410
Iteration 8/1000 | Loss: 0.00002346
Iteration 9/1000 | Loss: 0.00002292
Iteration 10/1000 | Loss: 0.00002257
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002183
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002115
Iteration 18/1000 | Loss: 0.00002114
Iteration 19/1000 | Loss: 0.00002108
Iteration 20/1000 | Loss: 0.00002101
Iteration 21/1000 | Loss: 0.00002100
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002097
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002090
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00002088
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002085
Iteration 34/1000 | Loss: 0.00002085
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002084
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002081
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002074
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002073
Iteration 50/1000 | Loss: 0.00002073
Iteration 51/1000 | Loss: 0.00002073
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002072
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00002071
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002070
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002069
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002069
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002067
Iteration 77/1000 | Loss: 0.00002067
Iteration 78/1000 | Loss: 0.00002067
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002067
Iteration 82/1000 | Loss: 0.00002067
Iteration 83/1000 | Loss: 0.00002067
Iteration 84/1000 | Loss: 0.00002067
Iteration 85/1000 | Loss: 0.00002067
Iteration 86/1000 | Loss: 0.00002067
Iteration 87/1000 | Loss: 0.00002067
Iteration 88/1000 | Loss: 0.00002067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.0668914658017457e-05, 2.0668914658017457e-05, 2.0668914658017457e-05, 2.0668914658017457e-05, 2.0668914658017457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0668914658017457e-05

Optimization complete. Final v2v error: 3.7204463481903076 mm

Highest mean error: 5.657654285430908 mm for frame 58

Lowest mean error: 2.8383734226226807 mm for frame 1

Saving results

Total time: 42.908008337020874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049073
Iteration 2/25 | Loss: 0.00161613
Iteration 3/25 | Loss: 0.00143476
Iteration 4/25 | Loss: 0.00133367
Iteration 5/25 | Loss: 0.00132453
Iteration 6/25 | Loss: 0.00130962
Iteration 7/25 | Loss: 0.00131366
Iteration 8/25 | Loss: 0.00130987
Iteration 9/25 | Loss: 0.00131300
Iteration 10/25 | Loss: 0.00131037
Iteration 11/25 | Loss: 0.00130725
Iteration 12/25 | Loss: 0.00130692
Iteration 13/25 | Loss: 0.00130684
Iteration 14/25 | Loss: 0.00130684
Iteration 15/25 | Loss: 0.00130684
Iteration 16/25 | Loss: 0.00130684
Iteration 17/25 | Loss: 0.00130684
Iteration 18/25 | Loss: 0.00130684
Iteration 19/25 | Loss: 0.00130684
Iteration 20/25 | Loss: 0.00130684
Iteration 21/25 | Loss: 0.00130684
Iteration 22/25 | Loss: 0.00130684
Iteration 23/25 | Loss: 0.00130684
Iteration 24/25 | Loss: 0.00130684
Iteration 25/25 | Loss: 0.00130684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013068360276520252, 0.0013068360276520252, 0.0013068360276520252, 0.0013068360276520252, 0.0013068360276520252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013068360276520252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.39155197
Iteration 2/25 | Loss: 0.00151727
Iteration 3/25 | Loss: 0.00151727
Iteration 4/25 | Loss: 0.00151727
Iteration 5/25 | Loss: 0.00151727
Iteration 6/25 | Loss: 0.00151726
Iteration 7/25 | Loss: 0.00151726
Iteration 8/25 | Loss: 0.00151726
Iteration 9/25 | Loss: 0.00151726
Iteration 10/25 | Loss: 0.00151726
Iteration 11/25 | Loss: 0.00151726
Iteration 12/25 | Loss: 0.00151726
Iteration 13/25 | Loss: 0.00151726
Iteration 14/25 | Loss: 0.00151726
Iteration 15/25 | Loss: 0.00151726
Iteration 16/25 | Loss: 0.00151726
Iteration 17/25 | Loss: 0.00151726
Iteration 18/25 | Loss: 0.00151726
Iteration 19/25 | Loss: 0.00151726
Iteration 20/25 | Loss: 0.00151726
Iteration 21/25 | Loss: 0.00151726
Iteration 22/25 | Loss: 0.00151726
Iteration 23/25 | Loss: 0.00151726
Iteration 24/25 | Loss: 0.00151726
Iteration 25/25 | Loss: 0.00151726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151726
Iteration 2/1000 | Loss: 0.00002400
Iteration 3/1000 | Loss: 0.00007288
Iteration 4/1000 | Loss: 0.00013746
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00001378
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00009119
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00006854
Iteration 13/1000 | Loss: 0.00004149
Iteration 14/1000 | Loss: 0.00004303
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00016664
Iteration 18/1000 | Loss: 0.00002063
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00005224
Iteration 25/1000 | Loss: 0.00003095
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00003319
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00002743
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00002510
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00004383
Iteration 34/1000 | Loss: 0.00003399
Iteration 35/1000 | Loss: 0.00003501
Iteration 36/1000 | Loss: 0.00002216
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001168
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.1678868759190664e-05, 1.1678868759190664e-05, 1.1678868759190664e-05, 1.1678868759190664e-05, 1.1678868759190664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1678868759190664e-05

Optimization complete. Final v2v error: 2.9099457263946533 mm

Highest mean error: 3.3458211421966553 mm for frame 177

Lowest mean error: 2.7063333988189697 mm for frame 150

Saving results

Total time: 78.68645906448364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414643
Iteration 2/25 | Loss: 0.00146337
Iteration 3/25 | Loss: 0.00129487
Iteration 4/25 | Loss: 0.00127609
Iteration 5/25 | Loss: 0.00127028
Iteration 6/25 | Loss: 0.00126933
Iteration 7/25 | Loss: 0.00126933
Iteration 8/25 | Loss: 0.00126933
Iteration 9/25 | Loss: 0.00126933
Iteration 10/25 | Loss: 0.00126933
Iteration 11/25 | Loss: 0.00126933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012693274766206741, 0.0012693274766206741, 0.0012693274766206741, 0.0012693274766206741, 0.0012693274766206741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012693274766206741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36845326
Iteration 2/25 | Loss: 0.00165720
Iteration 3/25 | Loss: 0.00165720
Iteration 4/25 | Loss: 0.00165720
Iteration 5/25 | Loss: 0.00165720
Iteration 6/25 | Loss: 0.00165720
Iteration 7/25 | Loss: 0.00165720
Iteration 8/25 | Loss: 0.00165720
Iteration 9/25 | Loss: 0.00165720
Iteration 10/25 | Loss: 0.00165720
Iteration 11/25 | Loss: 0.00165720
Iteration 12/25 | Loss: 0.00165720
Iteration 13/25 | Loss: 0.00165720
Iteration 14/25 | Loss: 0.00165720
Iteration 15/25 | Loss: 0.00165720
Iteration 16/25 | Loss: 0.00165720
Iteration 17/25 | Loss: 0.00165720
Iteration 18/25 | Loss: 0.00165720
Iteration 19/25 | Loss: 0.00165720
Iteration 20/25 | Loss: 0.00165720
Iteration 21/25 | Loss: 0.00165720
Iteration 22/25 | Loss: 0.00165720
Iteration 23/25 | Loss: 0.00165720
Iteration 24/25 | Loss: 0.00165720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001657198998145759, 0.001657198998145759, 0.001657198998145759, 0.001657198998145759, 0.001657198998145759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001657198998145759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165720
Iteration 2/1000 | Loss: 0.00003260
Iteration 3/1000 | Loss: 0.00002159
Iteration 4/1000 | Loss: 0.00001966
Iteration 5/1000 | Loss: 0.00001831
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001648
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001607
Iteration 12/1000 | Loss: 0.00001602
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00001590
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001590
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001579
Iteration 22/1000 | Loss: 0.00001572
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001546
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001543
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001533
Iteration 45/1000 | Loss: 0.00001533
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001530
Iteration 53/1000 | Loss: 0.00001530
Iteration 54/1000 | Loss: 0.00001529
Iteration 55/1000 | Loss: 0.00001529
Iteration 56/1000 | Loss: 0.00001529
Iteration 57/1000 | Loss: 0.00001528
Iteration 58/1000 | Loss: 0.00001527
Iteration 59/1000 | Loss: 0.00001527
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001525
Iteration 62/1000 | Loss: 0.00001525
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001524
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001516
Iteration 94/1000 | Loss: 0.00001516
Iteration 95/1000 | Loss: 0.00001516
Iteration 96/1000 | Loss: 0.00001516
Iteration 97/1000 | Loss: 0.00001516
Iteration 98/1000 | Loss: 0.00001516
Iteration 99/1000 | Loss: 0.00001516
Iteration 100/1000 | Loss: 0.00001515
Iteration 101/1000 | Loss: 0.00001515
Iteration 102/1000 | Loss: 0.00001515
Iteration 103/1000 | Loss: 0.00001515
Iteration 104/1000 | Loss: 0.00001515
Iteration 105/1000 | Loss: 0.00001515
Iteration 106/1000 | Loss: 0.00001515
Iteration 107/1000 | Loss: 0.00001514
Iteration 108/1000 | Loss: 0.00001514
Iteration 109/1000 | Loss: 0.00001514
Iteration 110/1000 | Loss: 0.00001514
Iteration 111/1000 | Loss: 0.00001514
Iteration 112/1000 | Loss: 0.00001514
Iteration 113/1000 | Loss: 0.00001514
Iteration 114/1000 | Loss: 0.00001514
Iteration 115/1000 | Loss: 0.00001514
Iteration 116/1000 | Loss: 0.00001514
Iteration 117/1000 | Loss: 0.00001513
Iteration 118/1000 | Loss: 0.00001513
Iteration 119/1000 | Loss: 0.00001513
Iteration 120/1000 | Loss: 0.00001513
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001512
Iteration 123/1000 | Loss: 0.00001512
Iteration 124/1000 | Loss: 0.00001512
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001512
Iteration 128/1000 | Loss: 0.00001512
Iteration 129/1000 | Loss: 0.00001512
Iteration 130/1000 | Loss: 0.00001511
Iteration 131/1000 | Loss: 0.00001511
Iteration 132/1000 | Loss: 0.00001511
Iteration 133/1000 | Loss: 0.00001511
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001511
Iteration 145/1000 | Loss: 0.00001511
Iteration 146/1000 | Loss: 0.00001510
Iteration 147/1000 | Loss: 0.00001510
Iteration 148/1000 | Loss: 0.00001510
Iteration 149/1000 | Loss: 0.00001510
Iteration 150/1000 | Loss: 0.00001510
Iteration 151/1000 | Loss: 0.00001510
Iteration 152/1000 | Loss: 0.00001510
Iteration 153/1000 | Loss: 0.00001509
Iteration 154/1000 | Loss: 0.00001509
Iteration 155/1000 | Loss: 0.00001509
Iteration 156/1000 | Loss: 0.00001509
Iteration 157/1000 | Loss: 0.00001509
Iteration 158/1000 | Loss: 0.00001509
Iteration 159/1000 | Loss: 0.00001509
Iteration 160/1000 | Loss: 0.00001509
Iteration 161/1000 | Loss: 0.00001509
Iteration 162/1000 | Loss: 0.00001508
Iteration 163/1000 | Loss: 0.00001508
Iteration 164/1000 | Loss: 0.00001508
Iteration 165/1000 | Loss: 0.00001508
Iteration 166/1000 | Loss: 0.00001508
Iteration 167/1000 | Loss: 0.00001508
Iteration 168/1000 | Loss: 0.00001507
Iteration 169/1000 | Loss: 0.00001507
Iteration 170/1000 | Loss: 0.00001507
Iteration 171/1000 | Loss: 0.00001507
Iteration 172/1000 | Loss: 0.00001507
Iteration 173/1000 | Loss: 0.00001507
Iteration 174/1000 | Loss: 0.00001507
Iteration 175/1000 | Loss: 0.00001506
Iteration 176/1000 | Loss: 0.00001506
Iteration 177/1000 | Loss: 0.00001506
Iteration 178/1000 | Loss: 0.00001506
Iteration 179/1000 | Loss: 0.00001506
Iteration 180/1000 | Loss: 0.00001505
Iteration 181/1000 | Loss: 0.00001505
Iteration 182/1000 | Loss: 0.00001505
Iteration 183/1000 | Loss: 0.00001505
Iteration 184/1000 | Loss: 0.00001505
Iteration 185/1000 | Loss: 0.00001505
Iteration 186/1000 | Loss: 0.00001505
Iteration 187/1000 | Loss: 0.00001505
Iteration 188/1000 | Loss: 0.00001505
Iteration 189/1000 | Loss: 0.00001505
Iteration 190/1000 | Loss: 0.00001505
Iteration 191/1000 | Loss: 0.00001505
Iteration 192/1000 | Loss: 0.00001505
Iteration 193/1000 | Loss: 0.00001505
Iteration 194/1000 | Loss: 0.00001505
Iteration 195/1000 | Loss: 0.00001505
Iteration 196/1000 | Loss: 0.00001505
Iteration 197/1000 | Loss: 0.00001504
Iteration 198/1000 | Loss: 0.00001504
Iteration 199/1000 | Loss: 0.00001504
Iteration 200/1000 | Loss: 0.00001504
Iteration 201/1000 | Loss: 0.00001504
Iteration 202/1000 | Loss: 0.00001504
Iteration 203/1000 | Loss: 0.00001504
Iteration 204/1000 | Loss: 0.00001504
Iteration 205/1000 | Loss: 0.00001504
Iteration 206/1000 | Loss: 0.00001504
Iteration 207/1000 | Loss: 0.00001504
Iteration 208/1000 | Loss: 0.00001504
Iteration 209/1000 | Loss: 0.00001504
Iteration 210/1000 | Loss: 0.00001504
Iteration 211/1000 | Loss: 0.00001504
Iteration 212/1000 | Loss: 0.00001504
Iteration 213/1000 | Loss: 0.00001504
Iteration 214/1000 | Loss: 0.00001504
Iteration 215/1000 | Loss: 0.00001504
Iteration 216/1000 | Loss: 0.00001503
Iteration 217/1000 | Loss: 0.00001503
Iteration 218/1000 | Loss: 0.00001503
Iteration 219/1000 | Loss: 0.00001503
Iteration 220/1000 | Loss: 0.00001503
Iteration 221/1000 | Loss: 0.00001503
Iteration 222/1000 | Loss: 0.00001503
Iteration 223/1000 | Loss: 0.00001503
Iteration 224/1000 | Loss: 0.00001503
Iteration 225/1000 | Loss: 0.00001503
Iteration 226/1000 | Loss: 0.00001503
Iteration 227/1000 | Loss: 0.00001503
Iteration 228/1000 | Loss: 0.00001503
Iteration 229/1000 | Loss: 0.00001503
Iteration 230/1000 | Loss: 0.00001503
Iteration 231/1000 | Loss: 0.00001503
Iteration 232/1000 | Loss: 0.00001503
Iteration 233/1000 | Loss: 0.00001503
Iteration 234/1000 | Loss: 0.00001503
Iteration 235/1000 | Loss: 0.00001503
Iteration 236/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.5031711882329546e-05, 1.5031711882329546e-05, 1.5031711882329546e-05, 1.5031711882329546e-05, 1.5031711882329546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5031711882329546e-05

Optimization complete. Final v2v error: 3.2627627849578857 mm

Highest mean error: 3.774754285812378 mm for frame 93

Lowest mean error: 2.6844422817230225 mm for frame 0

Saving results

Total time: 47.82977914810181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492560
Iteration 2/25 | Loss: 0.00135592
Iteration 3/25 | Loss: 0.00127962
Iteration 4/25 | Loss: 0.00127024
Iteration 5/25 | Loss: 0.00126752
Iteration 6/25 | Loss: 0.00126708
Iteration 7/25 | Loss: 0.00126708
Iteration 8/25 | Loss: 0.00126708
Iteration 9/25 | Loss: 0.00126708
Iteration 10/25 | Loss: 0.00126708
Iteration 11/25 | Loss: 0.00126708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012670843861997128, 0.0012670843861997128, 0.0012670843861997128, 0.0012670843861997128, 0.0012670843861997128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012670843861997128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.92423296
Iteration 2/25 | Loss: 0.00142538
Iteration 3/25 | Loss: 0.00142538
Iteration 4/25 | Loss: 0.00142537
Iteration 5/25 | Loss: 0.00142537
Iteration 6/25 | Loss: 0.00142537
Iteration 7/25 | Loss: 0.00142537
Iteration 8/25 | Loss: 0.00142537
Iteration 9/25 | Loss: 0.00142537
Iteration 10/25 | Loss: 0.00142537
Iteration 11/25 | Loss: 0.00142537
Iteration 12/25 | Loss: 0.00142537
Iteration 13/25 | Loss: 0.00142537
Iteration 14/25 | Loss: 0.00142537
Iteration 15/25 | Loss: 0.00142537
Iteration 16/25 | Loss: 0.00142537
Iteration 17/25 | Loss: 0.00142537
Iteration 18/25 | Loss: 0.00142537
Iteration 19/25 | Loss: 0.00142537
Iteration 20/25 | Loss: 0.00142537
Iteration 21/25 | Loss: 0.00142537
Iteration 22/25 | Loss: 0.00142537
Iteration 23/25 | Loss: 0.00142537
Iteration 24/25 | Loss: 0.00142537
Iteration 25/25 | Loss: 0.00142537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142537
Iteration 2/1000 | Loss: 0.00002600
Iteration 3/1000 | Loss: 0.00001927
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001314
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001247
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001161
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001158
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001147
Iteration 77/1000 | Loss: 0.00001147
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001146
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001145
Iteration 88/1000 | Loss: 0.00001145
Iteration 89/1000 | Loss: 0.00001144
Iteration 90/1000 | Loss: 0.00001144
Iteration 91/1000 | Loss: 0.00001144
Iteration 92/1000 | Loss: 0.00001144
Iteration 93/1000 | Loss: 0.00001144
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001143
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001143
Iteration 106/1000 | Loss: 0.00001143
Iteration 107/1000 | Loss: 0.00001143
Iteration 108/1000 | Loss: 0.00001143
Iteration 109/1000 | Loss: 0.00001143
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001143
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001143
Iteration 119/1000 | Loss: 0.00001143
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001143
Iteration 141/1000 | Loss: 0.00001143
Iteration 142/1000 | Loss: 0.00001143
Iteration 143/1000 | Loss: 0.00001143
Iteration 144/1000 | Loss: 0.00001143
Iteration 145/1000 | Loss: 0.00001143
Iteration 146/1000 | Loss: 0.00001143
Iteration 147/1000 | Loss: 0.00001143
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001143
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1431936400185805e-05, 1.1431936400185805e-05, 1.1431936400185805e-05, 1.1431936400185805e-05, 1.1431936400185805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1431936400185805e-05

Optimization complete. Final v2v error: 2.915905714035034 mm

Highest mean error: 3.3165221214294434 mm for frame 68

Lowest mean error: 2.6756346225738525 mm for frame 132

Saving results

Total time: 38.83488202095032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452499
Iteration 2/25 | Loss: 0.00136012
Iteration 3/25 | Loss: 0.00130205
Iteration 4/25 | Loss: 0.00129255
Iteration 5/25 | Loss: 0.00128943
Iteration 6/25 | Loss: 0.00128927
Iteration 7/25 | Loss: 0.00128927
Iteration 8/25 | Loss: 0.00128927
Iteration 9/25 | Loss: 0.00128927
Iteration 10/25 | Loss: 0.00128927
Iteration 11/25 | Loss: 0.00128927
Iteration 12/25 | Loss: 0.00128927
Iteration 13/25 | Loss: 0.00128927
Iteration 14/25 | Loss: 0.00128927
Iteration 15/25 | Loss: 0.00128927
Iteration 16/25 | Loss: 0.00128927
Iteration 17/25 | Loss: 0.00128927
Iteration 18/25 | Loss: 0.00128927
Iteration 19/25 | Loss: 0.00128927
Iteration 20/25 | Loss: 0.00128927
Iteration 21/25 | Loss: 0.00128927
Iteration 22/25 | Loss: 0.00128927
Iteration 23/25 | Loss: 0.00128927
Iteration 24/25 | Loss: 0.00128927
Iteration 25/25 | Loss: 0.00128927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29524910
Iteration 2/25 | Loss: 0.00139060
Iteration 3/25 | Loss: 0.00139059
Iteration 4/25 | Loss: 0.00139059
Iteration 5/25 | Loss: 0.00139059
Iteration 6/25 | Loss: 0.00139059
Iteration 7/25 | Loss: 0.00139059
Iteration 8/25 | Loss: 0.00139059
Iteration 9/25 | Loss: 0.00139059
Iteration 10/25 | Loss: 0.00139059
Iteration 11/25 | Loss: 0.00139059
Iteration 12/25 | Loss: 0.00139059
Iteration 13/25 | Loss: 0.00139059
Iteration 14/25 | Loss: 0.00139059
Iteration 15/25 | Loss: 0.00139059
Iteration 16/25 | Loss: 0.00139059
Iteration 17/25 | Loss: 0.00139059
Iteration 18/25 | Loss: 0.00139059
Iteration 19/25 | Loss: 0.00139059
Iteration 20/25 | Loss: 0.00139059
Iteration 21/25 | Loss: 0.00139059
Iteration 22/25 | Loss: 0.00139059
Iteration 23/25 | Loss: 0.00139059
Iteration 24/25 | Loss: 0.00139059
Iteration 25/25 | Loss: 0.00139059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139059
Iteration 2/1000 | Loss: 0.00002805
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00002030
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001900
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001795
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001708
Iteration 13/1000 | Loss: 0.00001707
Iteration 14/1000 | Loss: 0.00001705
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001667
Iteration 19/1000 | Loss: 0.00001666
Iteration 20/1000 | Loss: 0.00001666
Iteration 21/1000 | Loss: 0.00001665
Iteration 22/1000 | Loss: 0.00001664
Iteration 23/1000 | Loss: 0.00001664
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001661
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001649
Iteration 31/1000 | Loss: 0.00001642
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00001636
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001628
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001617
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001609
Iteration 92/1000 | Loss: 0.00001609
Iteration 93/1000 | Loss: 0.00001609
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001608
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001607
Iteration 101/1000 | Loss: 0.00001607
Iteration 102/1000 | Loss: 0.00001607
Iteration 103/1000 | Loss: 0.00001607
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001605
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001605
Iteration 116/1000 | Loss: 0.00001605
Iteration 117/1000 | Loss: 0.00001605
Iteration 118/1000 | Loss: 0.00001605
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001604
Iteration 123/1000 | Loss: 0.00001604
Iteration 124/1000 | Loss: 0.00001604
Iteration 125/1000 | Loss: 0.00001604
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001602
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001602
Iteration 144/1000 | Loss: 0.00001602
Iteration 145/1000 | Loss: 0.00001602
Iteration 146/1000 | Loss: 0.00001602
Iteration 147/1000 | Loss: 0.00001602
Iteration 148/1000 | Loss: 0.00001602
Iteration 149/1000 | Loss: 0.00001602
Iteration 150/1000 | Loss: 0.00001602
Iteration 151/1000 | Loss: 0.00001601
Iteration 152/1000 | Loss: 0.00001601
Iteration 153/1000 | Loss: 0.00001601
Iteration 154/1000 | Loss: 0.00001601
Iteration 155/1000 | Loss: 0.00001601
Iteration 156/1000 | Loss: 0.00001601
Iteration 157/1000 | Loss: 0.00001601
Iteration 158/1000 | Loss: 0.00001601
Iteration 159/1000 | Loss: 0.00001601
Iteration 160/1000 | Loss: 0.00001601
Iteration 161/1000 | Loss: 0.00001601
Iteration 162/1000 | Loss: 0.00001601
Iteration 163/1000 | Loss: 0.00001601
Iteration 164/1000 | Loss: 0.00001601
Iteration 165/1000 | Loss: 0.00001601
Iteration 166/1000 | Loss: 0.00001601
Iteration 167/1000 | Loss: 0.00001601
Iteration 168/1000 | Loss: 0.00001601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.6008520105970092e-05, 1.6008520105970092e-05, 1.6008520105970092e-05, 1.6008520105970092e-05, 1.6008520105970092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6008520105970092e-05

Optimization complete. Final v2v error: 3.438347101211548 mm

Highest mean error: 3.581683397293091 mm for frame 117

Lowest mean error: 3.2920796871185303 mm for frame 53

Saving results

Total time: 39.95715260505676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986684
Iteration 2/25 | Loss: 0.00342995
Iteration 3/25 | Loss: 0.00247367
Iteration 4/25 | Loss: 0.00219839
Iteration 5/25 | Loss: 0.00212617
Iteration 6/25 | Loss: 0.00214156
Iteration 7/25 | Loss: 0.00214461
Iteration 8/25 | Loss: 0.00206165
Iteration 9/25 | Loss: 0.00198318
Iteration 10/25 | Loss: 0.00194594
Iteration 11/25 | Loss: 0.00194640
Iteration 12/25 | Loss: 0.00194081
Iteration 13/25 | Loss: 0.00193597
Iteration 14/25 | Loss: 0.00191738
Iteration 15/25 | Loss: 0.00191280
Iteration 16/25 | Loss: 0.00191048
Iteration 17/25 | Loss: 0.00190647
Iteration 18/25 | Loss: 0.00189965
Iteration 19/25 | Loss: 0.00189277
Iteration 20/25 | Loss: 0.00190721
Iteration 21/25 | Loss: 0.00188112
Iteration 22/25 | Loss: 0.00188339
Iteration 23/25 | Loss: 0.00187607
Iteration 24/25 | Loss: 0.00187634
Iteration 25/25 | Loss: 0.00187984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24899912
Iteration 2/25 | Loss: 0.00777239
Iteration 3/25 | Loss: 0.00592019
Iteration 4/25 | Loss: 0.00592019
Iteration 5/25 | Loss: 0.00592019
Iteration 6/25 | Loss: 0.00592019
Iteration 7/25 | Loss: 0.00592019
Iteration 8/25 | Loss: 0.00592019
Iteration 9/25 | Loss: 0.00592019
Iteration 10/25 | Loss: 0.00592019
Iteration 11/25 | Loss: 0.00592018
Iteration 12/25 | Loss: 0.00592018
Iteration 13/25 | Loss: 0.00592018
Iteration 14/25 | Loss: 0.00592018
Iteration 15/25 | Loss: 0.00592018
Iteration 16/25 | Loss: 0.00592018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005920184310525656, 0.005920184310525656, 0.005920184310525656, 0.005920184310525656, 0.005920184310525656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005920184310525656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00592018
Iteration 2/1000 | Loss: 0.00940179
Iteration 3/1000 | Loss: 0.00104262
Iteration 4/1000 | Loss: 0.00068386
Iteration 5/1000 | Loss: 0.00091066
Iteration 6/1000 | Loss: 0.00054403
Iteration 7/1000 | Loss: 0.00044410
Iteration 8/1000 | Loss: 0.00052690
Iteration 9/1000 | Loss: 0.00051396
Iteration 10/1000 | Loss: 0.00044666
Iteration 11/1000 | Loss: 0.00041048
Iteration 12/1000 | Loss: 0.00034523
Iteration 13/1000 | Loss: 0.00033335
Iteration 14/1000 | Loss: 0.00033367
Iteration 15/1000 | Loss: 0.00035134
Iteration 16/1000 | Loss: 0.00036083
Iteration 17/1000 | Loss: 0.00030502
Iteration 18/1000 | Loss: 0.00033044
Iteration 19/1000 | Loss: 0.00032123
Iteration 20/1000 | Loss: 0.00029369
Iteration 21/1000 | Loss: 0.00028244
Iteration 22/1000 | Loss: 0.00030571
Iteration 23/1000 | Loss: 0.00085267
Iteration 24/1000 | Loss: 0.00108308
Iteration 25/1000 | Loss: 0.00198949
Iteration 26/1000 | Loss: 0.01360908
Iteration 27/1000 | Loss: 0.01427039
Iteration 28/1000 | Loss: 0.00252495
Iteration 29/1000 | Loss: 0.00115499
Iteration 30/1000 | Loss: 0.00077135
Iteration 31/1000 | Loss: 0.00048799
Iteration 32/1000 | Loss: 0.00076080
Iteration 33/1000 | Loss: 0.00036812
Iteration 34/1000 | Loss: 0.00109099
Iteration 35/1000 | Loss: 0.00018549
Iteration 36/1000 | Loss: 0.00091389
Iteration 37/1000 | Loss: 0.00094475
Iteration 38/1000 | Loss: 0.00031609
Iteration 39/1000 | Loss: 0.00018200
Iteration 40/1000 | Loss: 0.00034303
Iteration 41/1000 | Loss: 0.00007325
Iteration 42/1000 | Loss: 0.00009920
Iteration 43/1000 | Loss: 0.00007297
Iteration 44/1000 | Loss: 0.00007643
Iteration 45/1000 | Loss: 0.00008170
Iteration 46/1000 | Loss: 0.00004145
Iteration 47/1000 | Loss: 0.00004218
Iteration 48/1000 | Loss: 0.00007730
Iteration 49/1000 | Loss: 0.00013697
Iteration 50/1000 | Loss: 0.00009147
Iteration 51/1000 | Loss: 0.00004101
Iteration 52/1000 | Loss: 0.00007412
Iteration 53/1000 | Loss: 0.00005960
Iteration 54/1000 | Loss: 0.00005530
Iteration 55/1000 | Loss: 0.00007145
Iteration 56/1000 | Loss: 0.00003349
Iteration 57/1000 | Loss: 0.00003245
Iteration 58/1000 | Loss: 0.00004004
Iteration 59/1000 | Loss: 0.00017574
Iteration 60/1000 | Loss: 0.00006618
Iteration 61/1000 | Loss: 0.00003396
Iteration 62/1000 | Loss: 0.00004359
Iteration 63/1000 | Loss: 0.00023769
Iteration 64/1000 | Loss: 0.00013411
Iteration 65/1000 | Loss: 0.00008250
Iteration 66/1000 | Loss: 0.00003530
Iteration 67/1000 | Loss: 0.00002965
Iteration 68/1000 | Loss: 0.00002962
Iteration 69/1000 | Loss: 0.00003136
Iteration 70/1000 | Loss: 0.00003885
Iteration 71/1000 | Loss: 0.00004120
Iteration 72/1000 | Loss: 0.00003068
Iteration 73/1000 | Loss: 0.00006425
Iteration 74/1000 | Loss: 0.00003827
Iteration 75/1000 | Loss: 0.00004176
Iteration 76/1000 | Loss: 0.00006879
Iteration 77/1000 | Loss: 0.00003094
Iteration 78/1000 | Loss: 0.00004947
Iteration 79/1000 | Loss: 0.00007310
Iteration 80/1000 | Loss: 0.00003263
Iteration 81/1000 | Loss: 0.00005949
Iteration 82/1000 | Loss: 0.00012643
Iteration 83/1000 | Loss: 0.00007319
Iteration 84/1000 | Loss: 0.00002581
Iteration 85/1000 | Loss: 0.00003809
Iteration 86/1000 | Loss: 0.00019581
Iteration 87/1000 | Loss: 0.00004410
Iteration 88/1000 | Loss: 0.00007374
Iteration 89/1000 | Loss: 0.00002253
Iteration 90/1000 | Loss: 0.00003719
Iteration 91/1000 | Loss: 0.00003910
Iteration 92/1000 | Loss: 0.00002485
Iteration 93/1000 | Loss: 0.00003489
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00002064
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00002932
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00003050
Iteration 101/1000 | Loss: 0.00002272
Iteration 102/1000 | Loss: 0.00001935
Iteration 103/1000 | Loss: 0.00002284
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002509
Iteration 107/1000 | Loss: 0.00002509
Iteration 108/1000 | Loss: 0.00002292
Iteration 109/1000 | Loss: 0.00001919
Iteration 110/1000 | Loss: 0.00001916
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001915
Iteration 114/1000 | Loss: 0.00001915
Iteration 115/1000 | Loss: 0.00001915
Iteration 116/1000 | Loss: 0.00001915
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001915
Iteration 120/1000 | Loss: 0.00001915
Iteration 121/1000 | Loss: 0.00001920
Iteration 122/1000 | Loss: 0.00001920
Iteration 123/1000 | Loss: 0.00001914
Iteration 124/1000 | Loss: 0.00001914
Iteration 125/1000 | Loss: 0.00001914
Iteration 126/1000 | Loss: 0.00001914
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001913
Iteration 129/1000 | Loss: 0.00001913
Iteration 130/1000 | Loss: 0.00001913
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001913
Iteration 134/1000 | Loss: 0.00001913
Iteration 135/1000 | Loss: 0.00001913
Iteration 136/1000 | Loss: 0.00001913
Iteration 137/1000 | Loss: 0.00001913
Iteration 138/1000 | Loss: 0.00001913
Iteration 139/1000 | Loss: 0.00001913
Iteration 140/1000 | Loss: 0.00001913
Iteration 141/1000 | Loss: 0.00001913
Iteration 142/1000 | Loss: 0.00001913
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001913
Iteration 145/1000 | Loss: 0.00001913
Iteration 146/1000 | Loss: 0.00001913
Iteration 147/1000 | Loss: 0.00001913
Iteration 148/1000 | Loss: 0.00001913
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001913
Iteration 152/1000 | Loss: 0.00001913
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001913
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001913
Iteration 159/1000 | Loss: 0.00001913
Iteration 160/1000 | Loss: 0.00001913
Iteration 161/1000 | Loss: 0.00001913
Iteration 162/1000 | Loss: 0.00001913
Iteration 163/1000 | Loss: 0.00001913
Iteration 164/1000 | Loss: 0.00001913
Iteration 165/1000 | Loss: 0.00001913
Iteration 166/1000 | Loss: 0.00001913
Iteration 167/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.9133107343805023e-05, 1.9133107343805023e-05, 1.9133107343805023e-05, 1.9133107343805023e-05, 1.9133107343805023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9133107343805023e-05

Optimization complete. Final v2v error: 3.6284940242767334 mm

Highest mean error: 5.4323601722717285 mm for frame 13

Lowest mean error: 3.119913339614868 mm for frame 37

Saving results

Total time: 221.7865571975708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843700
Iteration 2/25 | Loss: 0.00169866
Iteration 3/25 | Loss: 0.00144734
Iteration 4/25 | Loss: 0.00136145
Iteration 5/25 | Loss: 0.00134629
Iteration 6/25 | Loss: 0.00132505
Iteration 7/25 | Loss: 0.00132496
Iteration 8/25 | Loss: 0.00132437
Iteration 9/25 | Loss: 0.00130330
Iteration 10/25 | Loss: 0.00130074
Iteration 11/25 | Loss: 0.00129588
Iteration 12/25 | Loss: 0.00129311
Iteration 13/25 | Loss: 0.00129338
Iteration 14/25 | Loss: 0.00129036
Iteration 15/25 | Loss: 0.00129562
Iteration 16/25 | Loss: 0.00128913
Iteration 17/25 | Loss: 0.00128846
Iteration 18/25 | Loss: 0.00129096
Iteration 19/25 | Loss: 0.00128895
Iteration 20/25 | Loss: 0.00128549
Iteration 21/25 | Loss: 0.00128412
Iteration 22/25 | Loss: 0.00128380
Iteration 23/25 | Loss: 0.00128372
Iteration 24/25 | Loss: 0.00128372
Iteration 25/25 | Loss: 0.00128372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.27386761
Iteration 2/25 | Loss: 0.00192414
Iteration 3/25 | Loss: 0.00185280
Iteration 4/25 | Loss: 0.00185279
Iteration 5/25 | Loss: 0.00185279
Iteration 6/25 | Loss: 0.00185279
Iteration 7/25 | Loss: 0.00185279
Iteration 8/25 | Loss: 0.00185279
Iteration 9/25 | Loss: 0.00185279
Iteration 10/25 | Loss: 0.00185279
Iteration 11/25 | Loss: 0.00185279
Iteration 12/25 | Loss: 0.00185279
Iteration 13/25 | Loss: 0.00185279
Iteration 14/25 | Loss: 0.00185279
Iteration 15/25 | Loss: 0.00185279
Iteration 16/25 | Loss: 0.00185279
Iteration 17/25 | Loss: 0.00185279
Iteration 18/25 | Loss: 0.00185279
Iteration 19/25 | Loss: 0.00185279
Iteration 20/25 | Loss: 0.00185279
Iteration 21/25 | Loss: 0.00185279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018527915235608816, 0.0018527915235608816, 0.0018527915235608816, 0.0018527915235608816, 0.0018527915235608816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018527915235608816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185279
Iteration 2/1000 | Loss: 0.00004042
Iteration 3/1000 | Loss: 0.00016910
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00006307
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001707
Iteration 16/1000 | Loss: 0.00001704
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001683
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001677
Iteration 36/1000 | Loss: 0.00001676
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00019343
Iteration 42/1000 | Loss: 0.00003071
Iteration 43/1000 | Loss: 0.00006606
Iteration 44/1000 | Loss: 0.00002361
Iteration 45/1000 | Loss: 0.00002228
Iteration 46/1000 | Loss: 0.00022135
Iteration 47/1000 | Loss: 0.00018159
Iteration 48/1000 | Loss: 0.00020835
Iteration 49/1000 | Loss: 0.00003532
Iteration 50/1000 | Loss: 0.00002429
Iteration 51/1000 | Loss: 0.00002923
Iteration 52/1000 | Loss: 0.00019982
Iteration 53/1000 | Loss: 0.00005146
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00009346
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001658
Iteration 66/1000 | Loss: 0.00018890
Iteration 67/1000 | Loss: 0.00024491
Iteration 68/1000 | Loss: 0.00005945
Iteration 69/1000 | Loss: 0.00009687
Iteration 70/1000 | Loss: 0.00021586
Iteration 71/1000 | Loss: 0.00005076
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00019842
Iteration 74/1000 | Loss: 0.00002192
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001690
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00023820
Iteration 80/1000 | Loss: 0.00002339
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00010914
Iteration 84/1000 | Loss: 0.00010199
Iteration 85/1000 | Loss: 0.00015967
Iteration 86/1000 | Loss: 0.00035138
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00016886
Iteration 95/1000 | Loss: 0.00022866
Iteration 96/1000 | Loss: 0.00017982
Iteration 97/1000 | Loss: 0.00002541
Iteration 98/1000 | Loss: 0.00021357
Iteration 99/1000 | Loss: 0.00012143
Iteration 100/1000 | Loss: 0.00032997
Iteration 101/1000 | Loss: 0.00031375
Iteration 102/1000 | Loss: 0.00056781
Iteration 103/1000 | Loss: 0.00022509
Iteration 104/1000 | Loss: 0.00016052
Iteration 105/1000 | Loss: 0.00042324
Iteration 106/1000 | Loss: 0.00039899
Iteration 107/1000 | Loss: 0.00008001
Iteration 108/1000 | Loss: 0.00002319
Iteration 109/1000 | Loss: 0.00015391
Iteration 110/1000 | Loss: 0.00013052
Iteration 111/1000 | Loss: 0.00012788
Iteration 112/1000 | Loss: 0.00008492
Iteration 113/1000 | Loss: 0.00011869
Iteration 114/1000 | Loss: 0.00008501
Iteration 115/1000 | Loss: 0.00004339
Iteration 116/1000 | Loss: 0.00036317
Iteration 117/1000 | Loss: 0.00004094
Iteration 118/1000 | Loss: 0.00003322
Iteration 119/1000 | Loss: 0.00008063
Iteration 120/1000 | Loss: 0.00004360
Iteration 121/1000 | Loss: 0.00002602
Iteration 122/1000 | Loss: 0.00016502
Iteration 123/1000 | Loss: 0.00026770
Iteration 124/1000 | Loss: 0.00019493
Iteration 125/1000 | Loss: 0.00002118
Iteration 126/1000 | Loss: 0.00001931
Iteration 127/1000 | Loss: 0.00003543
Iteration 128/1000 | Loss: 0.00005983
Iteration 129/1000 | Loss: 0.00037506
Iteration 130/1000 | Loss: 0.00015467
Iteration 131/1000 | Loss: 0.00016421
Iteration 132/1000 | Loss: 0.00012712
Iteration 133/1000 | Loss: 0.00015549
Iteration 134/1000 | Loss: 0.00006781
Iteration 135/1000 | Loss: 0.00008111
Iteration 136/1000 | Loss: 0.00006229
Iteration 137/1000 | Loss: 0.00005659
Iteration 138/1000 | Loss: 0.00006050
Iteration 139/1000 | Loss: 0.00004858
Iteration 140/1000 | Loss: 0.00006111
Iteration 141/1000 | Loss: 0.00006951
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001670
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001654
Iteration 146/1000 | Loss: 0.00001653
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001651
Iteration 150/1000 | Loss: 0.00001647
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001645
Iteration 153/1000 | Loss: 0.00001645
Iteration 154/1000 | Loss: 0.00001644
Iteration 155/1000 | Loss: 0.00001643
Iteration 156/1000 | Loss: 0.00001643
Iteration 157/1000 | Loss: 0.00001642
Iteration 158/1000 | Loss: 0.00001642
Iteration 159/1000 | Loss: 0.00001642
Iteration 160/1000 | Loss: 0.00001641
Iteration 161/1000 | Loss: 0.00001641
Iteration 162/1000 | Loss: 0.00001641
Iteration 163/1000 | Loss: 0.00001641
Iteration 164/1000 | Loss: 0.00001641
Iteration 165/1000 | Loss: 0.00001641
Iteration 166/1000 | Loss: 0.00001641
Iteration 167/1000 | Loss: 0.00001640
Iteration 168/1000 | Loss: 0.00001640
Iteration 169/1000 | Loss: 0.00001640
Iteration 170/1000 | Loss: 0.00001640
Iteration 171/1000 | Loss: 0.00001640
Iteration 172/1000 | Loss: 0.00001640
Iteration 173/1000 | Loss: 0.00001640
Iteration 174/1000 | Loss: 0.00001640
Iteration 175/1000 | Loss: 0.00001640
Iteration 176/1000 | Loss: 0.00001639
Iteration 177/1000 | Loss: 0.00001639
Iteration 178/1000 | Loss: 0.00001639
Iteration 179/1000 | Loss: 0.00001639
Iteration 180/1000 | Loss: 0.00001639
Iteration 181/1000 | Loss: 0.00001639
Iteration 182/1000 | Loss: 0.00001639
Iteration 183/1000 | Loss: 0.00001639
Iteration 184/1000 | Loss: 0.00001639
Iteration 185/1000 | Loss: 0.00001639
Iteration 186/1000 | Loss: 0.00001639
Iteration 187/1000 | Loss: 0.00001639
Iteration 188/1000 | Loss: 0.00001639
Iteration 189/1000 | Loss: 0.00001639
Iteration 190/1000 | Loss: 0.00001639
Iteration 191/1000 | Loss: 0.00001639
Iteration 192/1000 | Loss: 0.00001639
Iteration 193/1000 | Loss: 0.00001639
Iteration 194/1000 | Loss: 0.00001638
Iteration 195/1000 | Loss: 0.00001638
Iteration 196/1000 | Loss: 0.00001638
Iteration 197/1000 | Loss: 0.00001638
Iteration 198/1000 | Loss: 0.00001638
Iteration 199/1000 | Loss: 0.00001638
Iteration 200/1000 | Loss: 0.00001638
Iteration 201/1000 | Loss: 0.00001638
Iteration 202/1000 | Loss: 0.00001638
Iteration 203/1000 | Loss: 0.00001638
Iteration 204/1000 | Loss: 0.00001638
Iteration 205/1000 | Loss: 0.00001638
Iteration 206/1000 | Loss: 0.00001638
Iteration 207/1000 | Loss: 0.00001638
Iteration 208/1000 | Loss: 0.00001638
Iteration 209/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.6381331079173833e-05, 1.6381331079173833e-05, 1.6381331079173833e-05, 1.6381331079173833e-05, 1.6381331079173833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6381331079173833e-05

Optimization complete. Final v2v error: 3.3759958744049072 mm

Highest mean error: 7.184396743774414 mm for frame 134

Lowest mean error: 2.7534866333007812 mm for frame 2

Saving results

Total time: 212.51819968223572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852211
Iteration 2/25 | Loss: 0.00140029
Iteration 3/25 | Loss: 0.00132785
Iteration 4/25 | Loss: 0.00132138
Iteration 5/25 | Loss: 0.00132138
Iteration 6/25 | Loss: 0.00132138
Iteration 7/25 | Loss: 0.00132138
Iteration 8/25 | Loss: 0.00132138
Iteration 9/25 | Loss: 0.00132138
Iteration 10/25 | Loss: 0.00132138
Iteration 11/25 | Loss: 0.00132138
Iteration 12/25 | Loss: 0.00132138
Iteration 13/25 | Loss: 0.00132138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013213814236223698, 0.0013213814236223698, 0.0013213814236223698, 0.0013213814236223698, 0.0013213814236223698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013213814236223698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23619378
Iteration 2/25 | Loss: 0.00123570
Iteration 3/25 | Loss: 0.00123561
Iteration 4/25 | Loss: 0.00123561
Iteration 5/25 | Loss: 0.00123561
Iteration 6/25 | Loss: 0.00123561
Iteration 7/25 | Loss: 0.00123561
Iteration 8/25 | Loss: 0.00123561
Iteration 9/25 | Loss: 0.00123561
Iteration 10/25 | Loss: 0.00123561
Iteration 11/25 | Loss: 0.00123561
Iteration 12/25 | Loss: 0.00123561
Iteration 13/25 | Loss: 0.00123561
Iteration 14/25 | Loss: 0.00123561
Iteration 15/25 | Loss: 0.00123561
Iteration 16/25 | Loss: 0.00123561
Iteration 17/25 | Loss: 0.00123561
Iteration 18/25 | Loss: 0.00123561
Iteration 19/25 | Loss: 0.00123561
Iteration 20/25 | Loss: 0.00123561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012356069637462497, 0.0012356069637462497, 0.0012356069637462497, 0.0012356069637462497, 0.0012356069637462497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012356069637462497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123561
Iteration 2/1000 | Loss: 0.00003159
Iteration 3/1000 | Loss: 0.00002157
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001728
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001613
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001492
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001417
Iteration 14/1000 | Loss: 0.00001404
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001354
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001353
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001351
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001347
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001346
Iteration 61/1000 | Loss: 0.00001346
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001337
Iteration 75/1000 | Loss: 0.00001336
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001335
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001335
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001328
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001326
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001323
Iteration 142/1000 | Loss: 0.00001323
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3234136531536933e-05, 1.3234136531536933e-05, 1.3234136531536933e-05, 1.3234136531536933e-05, 1.3234136531536933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3234136531536933e-05

Optimization complete. Final v2v error: 3.1352055072784424 mm

Highest mean error: 3.4437062740325928 mm for frame 218

Lowest mean error: 2.967759132385254 mm for frame 77

Saving results

Total time: 46.73178434371948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799261
Iteration 2/25 | Loss: 0.00138841
Iteration 3/25 | Loss: 0.00130579
Iteration 4/25 | Loss: 0.00128952
Iteration 5/25 | Loss: 0.00128362
Iteration 6/25 | Loss: 0.00128204
Iteration 7/25 | Loss: 0.00128204
Iteration 8/25 | Loss: 0.00128204
Iteration 9/25 | Loss: 0.00128204
Iteration 10/25 | Loss: 0.00128204
Iteration 11/25 | Loss: 0.00128204
Iteration 12/25 | Loss: 0.00128204
Iteration 13/25 | Loss: 0.00128204
Iteration 14/25 | Loss: 0.00128204
Iteration 15/25 | Loss: 0.00128204
Iteration 16/25 | Loss: 0.00128204
Iteration 17/25 | Loss: 0.00128204
Iteration 18/25 | Loss: 0.00128204
Iteration 19/25 | Loss: 0.00128204
Iteration 20/25 | Loss: 0.00128204
Iteration 21/25 | Loss: 0.00128204
Iteration 22/25 | Loss: 0.00128204
Iteration 23/25 | Loss: 0.00128204
Iteration 24/25 | Loss: 0.00128204
Iteration 25/25 | Loss: 0.00128204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29178691
Iteration 2/25 | Loss: 0.00218125
Iteration 3/25 | Loss: 0.00218125
Iteration 4/25 | Loss: 0.00218125
Iteration 5/25 | Loss: 0.00218125
Iteration 6/25 | Loss: 0.00218125
Iteration 7/25 | Loss: 0.00218125
Iteration 8/25 | Loss: 0.00218125
Iteration 9/25 | Loss: 0.00218125
Iteration 10/25 | Loss: 0.00218124
Iteration 11/25 | Loss: 0.00218124
Iteration 12/25 | Loss: 0.00218124
Iteration 13/25 | Loss: 0.00218124
Iteration 14/25 | Loss: 0.00218124
Iteration 15/25 | Loss: 0.00218124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021812445484101772, 0.0021812445484101772, 0.0021812445484101772, 0.0021812445484101772, 0.0021812445484101772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021812445484101772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218124
Iteration 2/1000 | Loss: 0.00004800
Iteration 3/1000 | Loss: 0.00003380
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002450
Iteration 6/1000 | Loss: 0.00002345
Iteration 7/1000 | Loss: 0.00002265
Iteration 8/1000 | Loss: 0.00002209
Iteration 9/1000 | Loss: 0.00002155
Iteration 10/1000 | Loss: 0.00002119
Iteration 11/1000 | Loss: 0.00002090
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002049
Iteration 15/1000 | Loss: 0.00002046
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00002042
Iteration 18/1000 | Loss: 0.00002025
Iteration 19/1000 | Loss: 0.00002022
Iteration 20/1000 | Loss: 0.00002011
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002010
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00002000
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001997
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001995
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001992
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001992
Iteration 42/1000 | Loss: 0.00001992
Iteration 43/1000 | Loss: 0.00001992
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001991
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001990
Iteration 50/1000 | Loss: 0.00001990
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001989
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001986
Iteration 66/1000 | Loss: 0.00001986
Iteration 67/1000 | Loss: 0.00001986
Iteration 68/1000 | Loss: 0.00001986
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001985
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001983
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001983
Iteration 79/1000 | Loss: 0.00001983
Iteration 80/1000 | Loss: 0.00001983
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001981
Iteration 85/1000 | Loss: 0.00001981
Iteration 86/1000 | Loss: 0.00001981
Iteration 87/1000 | Loss: 0.00001981
Iteration 88/1000 | Loss: 0.00001980
Iteration 89/1000 | Loss: 0.00001980
Iteration 90/1000 | Loss: 0.00001980
Iteration 91/1000 | Loss: 0.00001980
Iteration 92/1000 | Loss: 0.00001980
Iteration 93/1000 | Loss: 0.00001979
Iteration 94/1000 | Loss: 0.00001979
Iteration 95/1000 | Loss: 0.00001979
Iteration 96/1000 | Loss: 0.00001979
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001978
Iteration 99/1000 | Loss: 0.00001978
Iteration 100/1000 | Loss: 0.00001978
Iteration 101/1000 | Loss: 0.00001978
Iteration 102/1000 | Loss: 0.00001977
Iteration 103/1000 | Loss: 0.00001977
Iteration 104/1000 | Loss: 0.00001977
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001977
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001977
Iteration 110/1000 | Loss: 0.00001977
Iteration 111/1000 | Loss: 0.00001977
Iteration 112/1000 | Loss: 0.00001977
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001975
Iteration 118/1000 | Loss: 0.00001975
Iteration 119/1000 | Loss: 0.00001975
Iteration 120/1000 | Loss: 0.00001975
Iteration 121/1000 | Loss: 0.00001974
Iteration 122/1000 | Loss: 0.00001974
Iteration 123/1000 | Loss: 0.00001974
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001974
Iteration 129/1000 | Loss: 0.00001974
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001974
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00001973
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001973
Iteration 138/1000 | Loss: 0.00001972
Iteration 139/1000 | Loss: 0.00001972
Iteration 140/1000 | Loss: 0.00001972
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001972
Iteration 149/1000 | Loss: 0.00001972
Iteration 150/1000 | Loss: 0.00001972
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Iteration 154/1000 | Loss: 0.00001971
Iteration 155/1000 | Loss: 0.00001971
Iteration 156/1000 | Loss: 0.00001970
Iteration 157/1000 | Loss: 0.00001970
Iteration 158/1000 | Loss: 0.00001970
Iteration 159/1000 | Loss: 0.00001970
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001969
Iteration 162/1000 | Loss: 0.00001969
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Iteration 167/1000 | Loss: 0.00001969
Iteration 168/1000 | Loss: 0.00001968
Iteration 169/1000 | Loss: 0.00001968
Iteration 170/1000 | Loss: 0.00001968
Iteration 171/1000 | Loss: 0.00001968
Iteration 172/1000 | Loss: 0.00001968
Iteration 173/1000 | Loss: 0.00001967
Iteration 174/1000 | Loss: 0.00001967
Iteration 175/1000 | Loss: 0.00001967
Iteration 176/1000 | Loss: 0.00001967
Iteration 177/1000 | Loss: 0.00001967
Iteration 178/1000 | Loss: 0.00001967
Iteration 179/1000 | Loss: 0.00001967
Iteration 180/1000 | Loss: 0.00001967
Iteration 181/1000 | Loss: 0.00001967
Iteration 182/1000 | Loss: 0.00001967
Iteration 183/1000 | Loss: 0.00001967
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001966
Iteration 186/1000 | Loss: 0.00001966
Iteration 187/1000 | Loss: 0.00001966
Iteration 188/1000 | Loss: 0.00001966
Iteration 189/1000 | Loss: 0.00001966
Iteration 190/1000 | Loss: 0.00001966
Iteration 191/1000 | Loss: 0.00001966
Iteration 192/1000 | Loss: 0.00001966
Iteration 193/1000 | Loss: 0.00001966
Iteration 194/1000 | Loss: 0.00001966
Iteration 195/1000 | Loss: 0.00001966
Iteration 196/1000 | Loss: 0.00001966
Iteration 197/1000 | Loss: 0.00001966
Iteration 198/1000 | Loss: 0.00001966
Iteration 199/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.9662949853227474e-05, 1.9662949853227474e-05, 1.9662949853227474e-05, 1.9662949853227474e-05, 1.9662949853227474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9662949853227474e-05

Optimization complete. Final v2v error: 3.757322072982788 mm

Highest mean error: 4.571511745452881 mm for frame 60

Lowest mean error: 3.49094557762146 mm for frame 35

Saving results

Total time: 43.448034048080444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00280700
Iteration 2/25 | Loss: 0.00141933
Iteration 3/25 | Loss: 0.00128430
Iteration 4/25 | Loss: 0.00126856
Iteration 5/25 | Loss: 0.00126500
Iteration 6/25 | Loss: 0.00126344
Iteration 7/25 | Loss: 0.00126287
Iteration 8/25 | Loss: 0.00126287
Iteration 9/25 | Loss: 0.00126287
Iteration 10/25 | Loss: 0.00126287
Iteration 11/25 | Loss: 0.00126287
Iteration 12/25 | Loss: 0.00126287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012628702679648995, 0.0012628702679648995, 0.0012628702679648995, 0.0012628702679648995, 0.0012628702679648995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012628702679648995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25950336
Iteration 2/25 | Loss: 0.00193722
Iteration 3/25 | Loss: 0.00193722
Iteration 4/25 | Loss: 0.00193722
Iteration 5/25 | Loss: 0.00193722
Iteration 6/25 | Loss: 0.00193722
Iteration 7/25 | Loss: 0.00193722
Iteration 8/25 | Loss: 0.00193722
Iteration 9/25 | Loss: 0.00193722
Iteration 10/25 | Loss: 0.00193722
Iteration 11/25 | Loss: 0.00193722
Iteration 12/25 | Loss: 0.00193722
Iteration 13/25 | Loss: 0.00193722
Iteration 14/25 | Loss: 0.00193722
Iteration 15/25 | Loss: 0.00193722
Iteration 16/25 | Loss: 0.00193722
Iteration 17/25 | Loss: 0.00193722
Iteration 18/25 | Loss: 0.00193722
Iteration 19/25 | Loss: 0.00193722
Iteration 20/25 | Loss: 0.00193722
Iteration 21/25 | Loss: 0.00193722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001937216380611062, 0.001937216380611062, 0.001937216380611062, 0.001937216380611062, 0.001937216380611062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001937216380611062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193722
Iteration 2/1000 | Loss: 0.00003873
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00001896
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001363
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001362
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001362
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001361
Iteration 72/1000 | Loss: 0.00001361
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001360
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001360
Iteration 78/1000 | Loss: 0.00001360
Iteration 79/1000 | Loss: 0.00001360
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001355
Iteration 107/1000 | Loss: 0.00001355
Iteration 108/1000 | Loss: 0.00001355
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001354
Iteration 111/1000 | Loss: 0.00001354
Iteration 112/1000 | Loss: 0.00001354
Iteration 113/1000 | Loss: 0.00001354
Iteration 114/1000 | Loss: 0.00001354
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001353
Iteration 118/1000 | Loss: 0.00001353
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001353
Iteration 121/1000 | Loss: 0.00001353
Iteration 122/1000 | Loss: 0.00001353
Iteration 123/1000 | Loss: 0.00001353
Iteration 124/1000 | Loss: 0.00001353
Iteration 125/1000 | Loss: 0.00001353
Iteration 126/1000 | Loss: 0.00001353
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001351
Iteration 138/1000 | Loss: 0.00001351
Iteration 139/1000 | Loss: 0.00001351
Iteration 140/1000 | Loss: 0.00001351
Iteration 141/1000 | Loss: 0.00001351
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001351
Iteration 147/1000 | Loss: 0.00001351
Iteration 148/1000 | Loss: 0.00001351
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001350
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001349
Iteration 165/1000 | Loss: 0.00001349
Iteration 166/1000 | Loss: 0.00001349
Iteration 167/1000 | Loss: 0.00001349
Iteration 168/1000 | Loss: 0.00001349
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00001349
Iteration 171/1000 | Loss: 0.00001349
Iteration 172/1000 | Loss: 0.00001349
Iteration 173/1000 | Loss: 0.00001349
Iteration 174/1000 | Loss: 0.00001349
Iteration 175/1000 | Loss: 0.00001349
Iteration 176/1000 | Loss: 0.00001348
Iteration 177/1000 | Loss: 0.00001348
Iteration 178/1000 | Loss: 0.00001348
Iteration 179/1000 | Loss: 0.00001348
Iteration 180/1000 | Loss: 0.00001348
Iteration 181/1000 | Loss: 0.00001348
Iteration 182/1000 | Loss: 0.00001348
Iteration 183/1000 | Loss: 0.00001348
Iteration 184/1000 | Loss: 0.00001348
Iteration 185/1000 | Loss: 0.00001348
Iteration 186/1000 | Loss: 0.00001348
Iteration 187/1000 | Loss: 0.00001348
Iteration 188/1000 | Loss: 0.00001348
Iteration 189/1000 | Loss: 0.00001348
Iteration 190/1000 | Loss: 0.00001348
Iteration 191/1000 | Loss: 0.00001348
Iteration 192/1000 | Loss: 0.00001347
Iteration 193/1000 | Loss: 0.00001347
Iteration 194/1000 | Loss: 0.00001347
Iteration 195/1000 | Loss: 0.00001347
Iteration 196/1000 | Loss: 0.00001347
Iteration 197/1000 | Loss: 0.00001347
Iteration 198/1000 | Loss: 0.00001347
Iteration 199/1000 | Loss: 0.00001347
Iteration 200/1000 | Loss: 0.00001347
Iteration 201/1000 | Loss: 0.00001347
Iteration 202/1000 | Loss: 0.00001347
Iteration 203/1000 | Loss: 0.00001347
Iteration 204/1000 | Loss: 0.00001347
Iteration 205/1000 | Loss: 0.00001346
Iteration 206/1000 | Loss: 0.00001346
Iteration 207/1000 | Loss: 0.00001346
Iteration 208/1000 | Loss: 0.00001346
Iteration 209/1000 | Loss: 0.00001346
Iteration 210/1000 | Loss: 0.00001346
Iteration 211/1000 | Loss: 0.00001346
Iteration 212/1000 | Loss: 0.00001346
Iteration 213/1000 | Loss: 0.00001346
Iteration 214/1000 | Loss: 0.00001346
Iteration 215/1000 | Loss: 0.00001346
Iteration 216/1000 | Loss: 0.00001346
Iteration 217/1000 | Loss: 0.00001346
Iteration 218/1000 | Loss: 0.00001346
Iteration 219/1000 | Loss: 0.00001346
Iteration 220/1000 | Loss: 0.00001346
Iteration 221/1000 | Loss: 0.00001345
Iteration 222/1000 | Loss: 0.00001345
Iteration 223/1000 | Loss: 0.00001345
Iteration 224/1000 | Loss: 0.00001345
Iteration 225/1000 | Loss: 0.00001345
Iteration 226/1000 | Loss: 0.00001345
Iteration 227/1000 | Loss: 0.00001345
Iteration 228/1000 | Loss: 0.00001345
Iteration 229/1000 | Loss: 0.00001345
Iteration 230/1000 | Loss: 0.00001345
Iteration 231/1000 | Loss: 0.00001345
Iteration 232/1000 | Loss: 0.00001345
Iteration 233/1000 | Loss: 0.00001345
Iteration 234/1000 | Loss: 0.00001345
Iteration 235/1000 | Loss: 0.00001345
Iteration 236/1000 | Loss: 0.00001345
Iteration 237/1000 | Loss: 0.00001345
Iteration 238/1000 | Loss: 0.00001345
Iteration 239/1000 | Loss: 0.00001345
Iteration 240/1000 | Loss: 0.00001345
Iteration 241/1000 | Loss: 0.00001345
Iteration 242/1000 | Loss: 0.00001345
Iteration 243/1000 | Loss: 0.00001345
Iteration 244/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.3450263395498041e-05, 1.3450263395498041e-05, 1.3450263395498041e-05, 1.3450263395498041e-05, 1.3450263395498041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3450263395498041e-05

Optimization complete. Final v2v error: 3.1560425758361816 mm

Highest mean error: 3.505662202835083 mm for frame 83

Lowest mean error: 2.802327871322632 mm for frame 5

Saving results

Total time: 44.571603298187256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817661
Iteration 2/25 | Loss: 0.00135474
Iteration 3/25 | Loss: 0.00126759
Iteration 4/25 | Loss: 0.00125521
Iteration 5/25 | Loss: 0.00125401
Iteration 6/25 | Loss: 0.00125401
Iteration 7/25 | Loss: 0.00125401
Iteration 8/25 | Loss: 0.00125401
Iteration 9/25 | Loss: 0.00125401
Iteration 10/25 | Loss: 0.00125401
Iteration 11/25 | Loss: 0.00125401
Iteration 12/25 | Loss: 0.00125401
Iteration 13/25 | Loss: 0.00125401
Iteration 14/25 | Loss: 0.00125401
Iteration 15/25 | Loss: 0.00125401
Iteration 16/25 | Loss: 0.00125401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001254011644050479, 0.001254011644050479, 0.001254011644050479, 0.001254011644050479, 0.001254011644050479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001254011644050479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29742754
Iteration 2/25 | Loss: 0.00139856
Iteration 3/25 | Loss: 0.00139856
Iteration 4/25 | Loss: 0.00139856
Iteration 5/25 | Loss: 0.00139856
Iteration 6/25 | Loss: 0.00139856
Iteration 7/25 | Loss: 0.00139856
Iteration 8/25 | Loss: 0.00139856
Iteration 9/25 | Loss: 0.00139856
Iteration 10/25 | Loss: 0.00139856
Iteration 11/25 | Loss: 0.00139856
Iteration 12/25 | Loss: 0.00139856
Iteration 13/25 | Loss: 0.00139856
Iteration 14/25 | Loss: 0.00139856
Iteration 15/25 | Loss: 0.00139856
Iteration 16/25 | Loss: 0.00139856
Iteration 17/25 | Loss: 0.00139856
Iteration 18/25 | Loss: 0.00139856
Iteration 19/25 | Loss: 0.00139856
Iteration 20/25 | Loss: 0.00139856
Iteration 21/25 | Loss: 0.00139856
Iteration 22/25 | Loss: 0.00139856
Iteration 23/25 | Loss: 0.00139856
Iteration 24/25 | Loss: 0.00139856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001398555003106594, 0.001398555003106594, 0.001398555003106594, 0.001398555003106594, 0.001398555003106594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001398555003106594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139856
Iteration 2/1000 | Loss: 0.00001965
Iteration 3/1000 | Loss: 0.00001503
Iteration 4/1000 | Loss: 0.00001384
Iteration 5/1000 | Loss: 0.00001292
Iteration 6/1000 | Loss: 0.00001230
Iteration 7/1000 | Loss: 0.00001191
Iteration 8/1000 | Loss: 0.00001160
Iteration 9/1000 | Loss: 0.00001132
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001103
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001098
Iteration 21/1000 | Loss: 0.00001098
Iteration 22/1000 | Loss: 0.00001096
Iteration 23/1000 | Loss: 0.00001095
Iteration 24/1000 | Loss: 0.00001094
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001092
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001085
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001082
Iteration 34/1000 | Loss: 0.00001078
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001064
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001060
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001060
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001056
Iteration 45/1000 | Loss: 0.00001054
Iteration 46/1000 | Loss: 0.00001054
Iteration 47/1000 | Loss: 0.00001054
Iteration 48/1000 | Loss: 0.00001053
Iteration 49/1000 | Loss: 0.00001053
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001049
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001045
Iteration 63/1000 | Loss: 0.00001044
Iteration 64/1000 | Loss: 0.00001044
Iteration 65/1000 | Loss: 0.00001044
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001040
Iteration 73/1000 | Loss: 0.00001039
Iteration 74/1000 | Loss: 0.00001039
Iteration 75/1000 | Loss: 0.00001039
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00001038
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001036
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001032
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001031
Iteration 91/1000 | Loss: 0.00001031
Iteration 92/1000 | Loss: 0.00001031
Iteration 93/1000 | Loss: 0.00001031
Iteration 94/1000 | Loss: 0.00001031
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001030
Iteration 98/1000 | Loss: 0.00001030
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001026
Iteration 103/1000 | Loss: 0.00001026
Iteration 104/1000 | Loss: 0.00001026
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001024
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001023
Iteration 110/1000 | Loss: 0.00001023
Iteration 111/1000 | Loss: 0.00001023
Iteration 112/1000 | Loss: 0.00001022
Iteration 113/1000 | Loss: 0.00001022
Iteration 114/1000 | Loss: 0.00001022
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001020
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001016
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001015
Iteration 144/1000 | Loss: 0.00001015
Iteration 145/1000 | Loss: 0.00001015
Iteration 146/1000 | Loss: 0.00001015
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001014
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001014
Iteration 154/1000 | Loss: 0.00001014
Iteration 155/1000 | Loss: 0.00001014
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001013
Iteration 160/1000 | Loss: 0.00001013
Iteration 161/1000 | Loss: 0.00001013
Iteration 162/1000 | Loss: 0.00001013
Iteration 163/1000 | Loss: 0.00001013
Iteration 164/1000 | Loss: 0.00001013
Iteration 165/1000 | Loss: 0.00001013
Iteration 166/1000 | Loss: 0.00001013
Iteration 167/1000 | Loss: 0.00001013
Iteration 168/1000 | Loss: 0.00001012
Iteration 169/1000 | Loss: 0.00001012
Iteration 170/1000 | Loss: 0.00001012
Iteration 171/1000 | Loss: 0.00001012
Iteration 172/1000 | Loss: 0.00001012
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001012
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001012
Iteration 179/1000 | Loss: 0.00001012
Iteration 180/1000 | Loss: 0.00001012
Iteration 181/1000 | Loss: 0.00001012
Iteration 182/1000 | Loss: 0.00001012
Iteration 183/1000 | Loss: 0.00001012
Iteration 184/1000 | Loss: 0.00001012
Iteration 185/1000 | Loss: 0.00001012
Iteration 186/1000 | Loss: 0.00001012
Iteration 187/1000 | Loss: 0.00001012
Iteration 188/1000 | Loss: 0.00001012
Iteration 189/1000 | Loss: 0.00001012
Iteration 190/1000 | Loss: 0.00001012
Iteration 191/1000 | Loss: 0.00001012
Iteration 192/1000 | Loss: 0.00001012
Iteration 193/1000 | Loss: 0.00001012
Iteration 194/1000 | Loss: 0.00001012
Iteration 195/1000 | Loss: 0.00001012
Iteration 196/1000 | Loss: 0.00001012
Iteration 197/1000 | Loss: 0.00001012
Iteration 198/1000 | Loss: 0.00001012
Iteration 199/1000 | Loss: 0.00001012
Iteration 200/1000 | Loss: 0.00001012
Iteration 201/1000 | Loss: 0.00001012
Iteration 202/1000 | Loss: 0.00001012
Iteration 203/1000 | Loss: 0.00001012
Iteration 204/1000 | Loss: 0.00001012
Iteration 205/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.0116134035342839e-05, 1.0116134035342839e-05, 1.0116134035342839e-05, 1.0116134035342839e-05, 1.0116134035342839e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0116134035342839e-05

Optimization complete. Final v2v error: 2.727440595626831 mm

Highest mean error: 2.853259325027466 mm for frame 144

Lowest mean error: 2.549328088760376 mm for frame 256

Saving results

Total time: 48.065006732940674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935573
Iteration 2/25 | Loss: 0.00232765
Iteration 3/25 | Loss: 0.00185073
Iteration 4/25 | Loss: 0.00169420
Iteration 5/25 | Loss: 0.00157126
Iteration 6/25 | Loss: 0.00155246
Iteration 7/25 | Loss: 0.00155664
Iteration 8/25 | Loss: 0.00158196
Iteration 9/25 | Loss: 0.00150937
Iteration 10/25 | Loss: 0.00147485
Iteration 11/25 | Loss: 0.00143007
Iteration 12/25 | Loss: 0.00142156
Iteration 13/25 | Loss: 0.00142560
Iteration 14/25 | Loss: 0.00142025
Iteration 15/25 | Loss: 0.00140763
Iteration 16/25 | Loss: 0.00139365
Iteration 17/25 | Loss: 0.00138701
Iteration 18/25 | Loss: 0.00138512
Iteration 19/25 | Loss: 0.00138421
Iteration 20/25 | Loss: 0.00138609
Iteration 21/25 | Loss: 0.00138358
Iteration 22/25 | Loss: 0.00138178
Iteration 23/25 | Loss: 0.00138136
Iteration 24/25 | Loss: 0.00138105
Iteration 25/25 | Loss: 0.00139179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27895582
Iteration 2/25 | Loss: 0.00186842
Iteration 3/25 | Loss: 0.00186842
Iteration 4/25 | Loss: 0.00186842
Iteration 5/25 | Loss: 0.00186842
Iteration 6/25 | Loss: 0.00186842
Iteration 7/25 | Loss: 0.00186842
Iteration 8/25 | Loss: 0.00186842
Iteration 9/25 | Loss: 0.00186842
Iteration 10/25 | Loss: 0.00186842
Iteration 11/25 | Loss: 0.00186841
Iteration 12/25 | Loss: 0.00186841
Iteration 13/25 | Loss: 0.00186841
Iteration 14/25 | Loss: 0.00186841
Iteration 15/25 | Loss: 0.00186841
Iteration 16/25 | Loss: 0.00186841
Iteration 17/25 | Loss: 0.00186841
Iteration 18/25 | Loss: 0.00186841
Iteration 19/25 | Loss: 0.00186841
Iteration 20/25 | Loss: 0.00186841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018684143433347344, 0.0018684143433347344, 0.0018684143433347344, 0.0018684143433347344, 0.0018684143433347344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018684143433347344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186841
Iteration 2/1000 | Loss: 0.00030348
Iteration 3/1000 | Loss: 0.00008284
Iteration 4/1000 | Loss: 0.00013206
Iteration 5/1000 | Loss: 0.00006999
Iteration 6/1000 | Loss: 0.00015918
Iteration 7/1000 | Loss: 0.00029267
Iteration 8/1000 | Loss: 0.00006470
Iteration 9/1000 | Loss: 0.00024410
Iteration 10/1000 | Loss: 0.00064241
Iteration 11/1000 | Loss: 0.00031699
Iteration 12/1000 | Loss: 0.00056936
Iteration 13/1000 | Loss: 0.00024000
Iteration 14/1000 | Loss: 0.00009611
Iteration 15/1000 | Loss: 0.00004577
Iteration 16/1000 | Loss: 0.00004399
Iteration 17/1000 | Loss: 0.00009082
Iteration 18/1000 | Loss: 0.00004173
Iteration 19/1000 | Loss: 0.00039635
Iteration 20/1000 | Loss: 0.00026812
Iteration 21/1000 | Loss: 0.00040499
Iteration 22/1000 | Loss: 0.00028173
Iteration 23/1000 | Loss: 0.00020012
Iteration 24/1000 | Loss: 0.00004611
Iteration 25/1000 | Loss: 0.00006794
Iteration 26/1000 | Loss: 0.00004443
Iteration 27/1000 | Loss: 0.00004158
Iteration 28/1000 | Loss: 0.00003980
Iteration 29/1000 | Loss: 0.00017468
Iteration 30/1000 | Loss: 0.00004126
Iteration 31/1000 | Loss: 0.00023571
Iteration 32/1000 | Loss: 0.00017577
Iteration 33/1000 | Loss: 0.00018624
Iteration 34/1000 | Loss: 0.00009229
Iteration 35/1000 | Loss: 0.00003820
Iteration 36/1000 | Loss: 0.00003508
Iteration 37/1000 | Loss: 0.00003424
Iteration 38/1000 | Loss: 0.00003351
Iteration 39/1000 | Loss: 0.00024265
Iteration 40/1000 | Loss: 0.00004840
Iteration 41/1000 | Loss: 0.00003793
Iteration 42/1000 | Loss: 0.00003606
Iteration 43/1000 | Loss: 0.00003416
Iteration 44/1000 | Loss: 0.00028794
Iteration 45/1000 | Loss: 0.00024331
Iteration 46/1000 | Loss: 0.00003311
Iteration 47/1000 | Loss: 0.00003224
Iteration 48/1000 | Loss: 0.00003150
Iteration 49/1000 | Loss: 0.00043737
Iteration 50/1000 | Loss: 0.00017709
Iteration 51/1000 | Loss: 0.00026282
Iteration 52/1000 | Loss: 0.00004770
Iteration 53/1000 | Loss: 0.00030015
Iteration 54/1000 | Loss: 0.00022477
Iteration 55/1000 | Loss: 0.00029332
Iteration 56/1000 | Loss: 0.00013093
Iteration 57/1000 | Loss: 0.00027478
Iteration 58/1000 | Loss: 0.00012770
Iteration 59/1000 | Loss: 0.00003711
Iteration 60/1000 | Loss: 0.00027647
Iteration 61/1000 | Loss: 0.00011604
Iteration 62/1000 | Loss: 0.00017100
Iteration 63/1000 | Loss: 0.00028791
Iteration 64/1000 | Loss: 0.00005284
Iteration 65/1000 | Loss: 0.00022782
Iteration 66/1000 | Loss: 0.00004764
Iteration 67/1000 | Loss: 0.00004492
Iteration 68/1000 | Loss: 0.00004293
Iteration 69/1000 | Loss: 0.00028084
Iteration 70/1000 | Loss: 0.00005577
Iteration 71/1000 | Loss: 0.00050740
Iteration 72/1000 | Loss: 0.00030423
Iteration 73/1000 | Loss: 0.00035766
Iteration 74/1000 | Loss: 0.00004950
Iteration 75/1000 | Loss: 0.00004095
Iteration 76/1000 | Loss: 0.00003671
Iteration 77/1000 | Loss: 0.00003521
Iteration 78/1000 | Loss: 0.00003763
Iteration 79/1000 | Loss: 0.00003356
Iteration 80/1000 | Loss: 0.00040663
Iteration 81/1000 | Loss: 0.00003683
Iteration 82/1000 | Loss: 0.00003283
Iteration 83/1000 | Loss: 0.00003105
Iteration 84/1000 | Loss: 0.00002980
Iteration 85/1000 | Loss: 0.00003968
Iteration 86/1000 | Loss: 0.00002850
Iteration 87/1000 | Loss: 0.00002815
Iteration 88/1000 | Loss: 0.00002785
Iteration 89/1000 | Loss: 0.00002779
Iteration 90/1000 | Loss: 0.00002768
Iteration 91/1000 | Loss: 0.00002764
Iteration 92/1000 | Loss: 0.00002759
Iteration 93/1000 | Loss: 0.00002757
Iteration 94/1000 | Loss: 0.00002755
Iteration 95/1000 | Loss: 0.00002751
Iteration 96/1000 | Loss: 0.00002749
Iteration 97/1000 | Loss: 0.00002746
Iteration 98/1000 | Loss: 0.00002746
Iteration 99/1000 | Loss: 0.00002746
Iteration 100/1000 | Loss: 0.00002746
Iteration 101/1000 | Loss: 0.00002746
Iteration 102/1000 | Loss: 0.00002746
Iteration 103/1000 | Loss: 0.00002746
Iteration 104/1000 | Loss: 0.00002746
Iteration 105/1000 | Loss: 0.00002745
Iteration 106/1000 | Loss: 0.00002745
Iteration 107/1000 | Loss: 0.00002744
Iteration 108/1000 | Loss: 0.00002743
Iteration 109/1000 | Loss: 0.00002742
Iteration 110/1000 | Loss: 0.00002742
Iteration 111/1000 | Loss: 0.00002741
Iteration 112/1000 | Loss: 0.00002741
Iteration 113/1000 | Loss: 0.00002740
Iteration 114/1000 | Loss: 0.00002740
Iteration 115/1000 | Loss: 0.00002739
Iteration 116/1000 | Loss: 0.00002736
Iteration 117/1000 | Loss: 0.00002736
Iteration 118/1000 | Loss: 0.00002736
Iteration 119/1000 | Loss: 0.00002736
Iteration 120/1000 | Loss: 0.00002735
Iteration 121/1000 | Loss: 0.00002735
Iteration 122/1000 | Loss: 0.00002735
Iteration 123/1000 | Loss: 0.00002735
Iteration 124/1000 | Loss: 0.00002735
Iteration 125/1000 | Loss: 0.00002735
Iteration 126/1000 | Loss: 0.00002735
Iteration 127/1000 | Loss: 0.00002735
Iteration 128/1000 | Loss: 0.00002734
Iteration 129/1000 | Loss: 0.00002734
Iteration 130/1000 | Loss: 0.00002734
Iteration 131/1000 | Loss: 0.00002733
Iteration 132/1000 | Loss: 0.00002733
Iteration 133/1000 | Loss: 0.00002733
Iteration 134/1000 | Loss: 0.00002733
Iteration 135/1000 | Loss: 0.00002733
Iteration 136/1000 | Loss: 0.00002733
Iteration 137/1000 | Loss: 0.00003528
Iteration 138/1000 | Loss: 0.00003528
Iteration 139/1000 | Loss: 0.00003528
Iteration 140/1000 | Loss: 0.00003528
Iteration 141/1000 | Loss: 0.00003527
Iteration 142/1000 | Loss: 0.00003527
Iteration 143/1000 | Loss: 0.00003397
Iteration 144/1000 | Loss: 0.00002924
Iteration 145/1000 | Loss: 0.00002729
Iteration 146/1000 | Loss: 0.00002728
Iteration 147/1000 | Loss: 0.00002726
Iteration 148/1000 | Loss: 0.00002726
Iteration 149/1000 | Loss: 0.00002726
Iteration 150/1000 | Loss: 0.00002726
Iteration 151/1000 | Loss: 0.00002726
Iteration 152/1000 | Loss: 0.00002726
Iteration 153/1000 | Loss: 0.00002726
Iteration 154/1000 | Loss: 0.00002726
Iteration 155/1000 | Loss: 0.00002725
Iteration 156/1000 | Loss: 0.00002725
Iteration 157/1000 | Loss: 0.00002725
Iteration 158/1000 | Loss: 0.00002725
Iteration 159/1000 | Loss: 0.00002725
Iteration 160/1000 | Loss: 0.00002725
Iteration 161/1000 | Loss: 0.00002725
Iteration 162/1000 | Loss: 0.00002725
Iteration 163/1000 | Loss: 0.00002725
Iteration 164/1000 | Loss: 0.00003279
Iteration 165/1000 | Loss: 0.00003279
Iteration 166/1000 | Loss: 0.00002735
Iteration 167/1000 | Loss: 0.00002723
Iteration 168/1000 | Loss: 0.00002723
Iteration 169/1000 | Loss: 0.00002722
Iteration 170/1000 | Loss: 0.00002721
Iteration 171/1000 | Loss: 0.00002721
Iteration 172/1000 | Loss: 0.00002721
Iteration 173/1000 | Loss: 0.00002721
Iteration 174/1000 | Loss: 0.00002838
Iteration 175/1000 | Loss: 0.00002838
Iteration 176/1000 | Loss: 0.00002725
Iteration 177/1000 | Loss: 0.00002724
Iteration 178/1000 | Loss: 0.00002724
Iteration 179/1000 | Loss: 0.00002719
Iteration 180/1000 | Loss: 0.00002719
Iteration 181/1000 | Loss: 0.00002719
Iteration 182/1000 | Loss: 0.00002718
Iteration 183/1000 | Loss: 0.00002718
Iteration 184/1000 | Loss: 0.00002718
Iteration 185/1000 | Loss: 0.00002718
Iteration 186/1000 | Loss: 0.00002718
Iteration 187/1000 | Loss: 0.00002718
Iteration 188/1000 | Loss: 0.00002718
Iteration 189/1000 | Loss: 0.00002746
Iteration 190/1000 | Loss: 0.00002718
Iteration 191/1000 | Loss: 0.00002718
Iteration 192/1000 | Loss: 0.00002718
Iteration 193/1000 | Loss: 0.00002718
Iteration 194/1000 | Loss: 0.00002718
Iteration 195/1000 | Loss: 0.00002718
Iteration 196/1000 | Loss: 0.00002718
Iteration 197/1000 | Loss: 0.00002718
Iteration 198/1000 | Loss: 0.00002718
Iteration 199/1000 | Loss: 0.00002718
Iteration 200/1000 | Loss: 0.00002718
Iteration 201/1000 | Loss: 0.00002718
Iteration 202/1000 | Loss: 0.00002718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.7177031370229088e-05, 2.7177031370229088e-05, 2.7177031370229088e-05, 2.7177031370229088e-05, 2.7177031370229088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7177031370229088e-05

Optimization complete. Final v2v error: 3.9311912059783936 mm

Highest mean error: 10.406085968017578 mm for frame 20

Lowest mean error: 2.6508867740631104 mm for frame 0

Saving results

Total time: 184.9045090675354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841279
Iteration 2/25 | Loss: 0.00170764
Iteration 3/25 | Loss: 0.00146943
Iteration 4/25 | Loss: 0.00141959
Iteration 5/25 | Loss: 0.00140626
Iteration 6/25 | Loss: 0.00140746
Iteration 7/25 | Loss: 0.00140602
Iteration 8/25 | Loss: 0.00140338
Iteration 9/25 | Loss: 0.00140241
Iteration 10/25 | Loss: 0.00140152
Iteration 11/25 | Loss: 0.00140104
Iteration 12/25 | Loss: 0.00140092
Iteration 13/25 | Loss: 0.00140084
Iteration 14/25 | Loss: 0.00140082
Iteration 15/25 | Loss: 0.00140082
Iteration 16/25 | Loss: 0.00140082
Iteration 17/25 | Loss: 0.00140082
Iteration 18/25 | Loss: 0.00140081
Iteration 19/25 | Loss: 0.00140081
Iteration 20/25 | Loss: 0.00140081
Iteration 21/25 | Loss: 0.00140081
Iteration 22/25 | Loss: 0.00140081
Iteration 23/25 | Loss: 0.00140081
Iteration 24/25 | Loss: 0.00140081
Iteration 25/25 | Loss: 0.00140081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22003675
Iteration 2/25 | Loss: 0.00197381
Iteration 3/25 | Loss: 0.00197380
Iteration 4/25 | Loss: 0.00197380
Iteration 5/25 | Loss: 0.00197380
Iteration 6/25 | Loss: 0.00197379
Iteration 7/25 | Loss: 0.00197379
Iteration 8/25 | Loss: 0.00197379
Iteration 9/25 | Loss: 0.00197379
Iteration 10/25 | Loss: 0.00197379
Iteration 11/25 | Loss: 0.00197379
Iteration 12/25 | Loss: 0.00197379
Iteration 13/25 | Loss: 0.00197379
Iteration 14/25 | Loss: 0.00197379
Iteration 15/25 | Loss: 0.00197379
Iteration 16/25 | Loss: 0.00197379
Iteration 17/25 | Loss: 0.00197379
Iteration 18/25 | Loss: 0.00197379
Iteration 19/25 | Loss: 0.00197379
Iteration 20/25 | Loss: 0.00197379
Iteration 21/25 | Loss: 0.00197379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019737922120839357, 0.0019737922120839357, 0.0019737922120839357, 0.0019737922120839357, 0.0019737922120839357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019737922120839357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197379
Iteration 2/1000 | Loss: 0.00018817
Iteration 3/1000 | Loss: 0.00012170
Iteration 4/1000 | Loss: 0.00010448
Iteration 5/1000 | Loss: 0.00009278
Iteration 6/1000 | Loss: 0.00008673
Iteration 7/1000 | Loss: 0.00013099
Iteration 8/1000 | Loss: 0.00007983
Iteration 9/1000 | Loss: 0.00036094
Iteration 10/1000 | Loss: 0.00012185
Iteration 11/1000 | Loss: 0.00013049
Iteration 12/1000 | Loss: 0.00007048
Iteration 13/1000 | Loss: 0.00006471
Iteration 14/1000 | Loss: 0.00006181
Iteration 15/1000 | Loss: 0.00009092
Iteration 16/1000 | Loss: 0.00005820
Iteration 17/1000 | Loss: 0.00005509
Iteration 18/1000 | Loss: 0.00005342
Iteration 19/1000 | Loss: 0.00005197
Iteration 20/1000 | Loss: 0.00050408
Iteration 21/1000 | Loss: 0.00006301
Iteration 22/1000 | Loss: 0.00005050
Iteration 23/1000 | Loss: 0.00004789
Iteration 24/1000 | Loss: 0.00004652
Iteration 25/1000 | Loss: 0.00004530
Iteration 26/1000 | Loss: 0.00004443
Iteration 27/1000 | Loss: 0.00004363
Iteration 28/1000 | Loss: 0.00004289
Iteration 29/1000 | Loss: 0.00004236
Iteration 30/1000 | Loss: 0.00004202
Iteration 31/1000 | Loss: 0.00004160
Iteration 32/1000 | Loss: 0.00004122
Iteration 33/1000 | Loss: 0.00004094
Iteration 34/1000 | Loss: 0.00004073
Iteration 35/1000 | Loss: 0.00004055
Iteration 36/1000 | Loss: 0.00004046
Iteration 37/1000 | Loss: 0.00004046
Iteration 38/1000 | Loss: 0.00004044
Iteration 39/1000 | Loss: 0.00004031
Iteration 40/1000 | Loss: 0.00004028
Iteration 41/1000 | Loss: 0.00004019
Iteration 42/1000 | Loss: 0.00004018
Iteration 43/1000 | Loss: 0.00004018
Iteration 44/1000 | Loss: 0.00004017
Iteration 45/1000 | Loss: 0.00004015
Iteration 46/1000 | Loss: 0.00004009
Iteration 47/1000 | Loss: 0.00004005
Iteration 48/1000 | Loss: 0.00004001
Iteration 49/1000 | Loss: 0.00004001
Iteration 50/1000 | Loss: 0.00003999
Iteration 51/1000 | Loss: 0.00003996
Iteration 52/1000 | Loss: 0.00003993
Iteration 53/1000 | Loss: 0.00003993
Iteration 54/1000 | Loss: 0.00003993
Iteration 55/1000 | Loss: 0.00003993
Iteration 56/1000 | Loss: 0.00003993
Iteration 57/1000 | Loss: 0.00003993
Iteration 58/1000 | Loss: 0.00003993
Iteration 59/1000 | Loss: 0.00003993
Iteration 60/1000 | Loss: 0.00003993
Iteration 61/1000 | Loss: 0.00003993
Iteration 62/1000 | Loss: 0.00003993
Iteration 63/1000 | Loss: 0.00003992
Iteration 64/1000 | Loss: 0.00003992
Iteration 65/1000 | Loss: 0.00003992
Iteration 66/1000 | Loss: 0.00003991
Iteration 67/1000 | Loss: 0.00003990
Iteration 68/1000 | Loss: 0.00003990
Iteration 69/1000 | Loss: 0.00003990
Iteration 70/1000 | Loss: 0.00003989
Iteration 71/1000 | Loss: 0.00003989
Iteration 72/1000 | Loss: 0.00003989
Iteration 73/1000 | Loss: 0.00003989
Iteration 74/1000 | Loss: 0.00003989
Iteration 75/1000 | Loss: 0.00003989
Iteration 76/1000 | Loss: 0.00003989
Iteration 77/1000 | Loss: 0.00003989
Iteration 78/1000 | Loss: 0.00003989
Iteration 79/1000 | Loss: 0.00003989
Iteration 80/1000 | Loss: 0.00003989
Iteration 81/1000 | Loss: 0.00003988
Iteration 82/1000 | Loss: 0.00003988
Iteration 83/1000 | Loss: 0.00003988
Iteration 84/1000 | Loss: 0.00003988
Iteration 85/1000 | Loss: 0.00003988
Iteration 86/1000 | Loss: 0.00003988
Iteration 87/1000 | Loss: 0.00003988
Iteration 88/1000 | Loss: 0.00003987
Iteration 89/1000 | Loss: 0.00003987
Iteration 90/1000 | Loss: 0.00003986
Iteration 91/1000 | Loss: 0.00003986
Iteration 92/1000 | Loss: 0.00003986
Iteration 93/1000 | Loss: 0.00003986
Iteration 94/1000 | Loss: 0.00003986
Iteration 95/1000 | Loss: 0.00003985
Iteration 96/1000 | Loss: 0.00003985
Iteration 97/1000 | Loss: 0.00003985
Iteration 98/1000 | Loss: 0.00003984
Iteration 99/1000 | Loss: 0.00003984
Iteration 100/1000 | Loss: 0.00003983
Iteration 101/1000 | Loss: 0.00003979
Iteration 102/1000 | Loss: 0.00003979
Iteration 103/1000 | Loss: 0.00003977
Iteration 104/1000 | Loss: 0.00003977
Iteration 105/1000 | Loss: 0.00003977
Iteration 106/1000 | Loss: 0.00003976
Iteration 107/1000 | Loss: 0.00003976
Iteration 108/1000 | Loss: 0.00003976
Iteration 109/1000 | Loss: 0.00003976
Iteration 110/1000 | Loss: 0.00003976
Iteration 111/1000 | Loss: 0.00003975
Iteration 112/1000 | Loss: 0.00003975
Iteration 113/1000 | Loss: 0.00003975
Iteration 114/1000 | Loss: 0.00003975
Iteration 115/1000 | Loss: 0.00003975
Iteration 116/1000 | Loss: 0.00003975
Iteration 117/1000 | Loss: 0.00003974
Iteration 118/1000 | Loss: 0.00003974
Iteration 119/1000 | Loss: 0.00003974
Iteration 120/1000 | Loss: 0.00003974
Iteration 121/1000 | Loss: 0.00003974
Iteration 122/1000 | Loss: 0.00003974
Iteration 123/1000 | Loss: 0.00003973
Iteration 124/1000 | Loss: 0.00003973
Iteration 125/1000 | Loss: 0.00003973
Iteration 126/1000 | Loss: 0.00003973
Iteration 127/1000 | Loss: 0.00003973
Iteration 128/1000 | Loss: 0.00003973
Iteration 129/1000 | Loss: 0.00003973
Iteration 130/1000 | Loss: 0.00003973
Iteration 131/1000 | Loss: 0.00003973
Iteration 132/1000 | Loss: 0.00003973
Iteration 133/1000 | Loss: 0.00003973
Iteration 134/1000 | Loss: 0.00003972
Iteration 135/1000 | Loss: 0.00003972
Iteration 136/1000 | Loss: 0.00003972
Iteration 137/1000 | Loss: 0.00003972
Iteration 138/1000 | Loss: 0.00003972
Iteration 139/1000 | Loss: 0.00003972
Iteration 140/1000 | Loss: 0.00003972
Iteration 141/1000 | Loss: 0.00003972
Iteration 142/1000 | Loss: 0.00003972
Iteration 143/1000 | Loss: 0.00003970
Iteration 144/1000 | Loss: 0.00003970
Iteration 145/1000 | Loss: 0.00003969
Iteration 146/1000 | Loss: 0.00003969
Iteration 147/1000 | Loss: 0.00003969
Iteration 148/1000 | Loss: 0.00003969
Iteration 149/1000 | Loss: 0.00003968
Iteration 150/1000 | Loss: 0.00003968
Iteration 151/1000 | Loss: 0.00003968
Iteration 152/1000 | Loss: 0.00003967
Iteration 153/1000 | Loss: 0.00003967
Iteration 154/1000 | Loss: 0.00003966
Iteration 155/1000 | Loss: 0.00003966
Iteration 156/1000 | Loss: 0.00003966
Iteration 157/1000 | Loss: 0.00003965
Iteration 158/1000 | Loss: 0.00003965
Iteration 159/1000 | Loss: 0.00003965
Iteration 160/1000 | Loss: 0.00003964
Iteration 161/1000 | Loss: 0.00003964
Iteration 162/1000 | Loss: 0.00003963
Iteration 163/1000 | Loss: 0.00003962
Iteration 164/1000 | Loss: 0.00003962
Iteration 165/1000 | Loss: 0.00003962
Iteration 166/1000 | Loss: 0.00003961
Iteration 167/1000 | Loss: 0.00003961
Iteration 168/1000 | Loss: 0.00003961
Iteration 169/1000 | Loss: 0.00003961
Iteration 170/1000 | Loss: 0.00003961
Iteration 171/1000 | Loss: 0.00003961
Iteration 172/1000 | Loss: 0.00003961
Iteration 173/1000 | Loss: 0.00003961
Iteration 174/1000 | Loss: 0.00003961
Iteration 175/1000 | Loss: 0.00003961
Iteration 176/1000 | Loss: 0.00003961
Iteration 177/1000 | Loss: 0.00003961
Iteration 178/1000 | Loss: 0.00003960
Iteration 179/1000 | Loss: 0.00003960
Iteration 180/1000 | Loss: 0.00003959
Iteration 181/1000 | Loss: 0.00003958
Iteration 182/1000 | Loss: 0.00060925
Iteration 183/1000 | Loss: 0.00213831
Iteration 184/1000 | Loss: 0.00161484
Iteration 185/1000 | Loss: 0.00212735
Iteration 186/1000 | Loss: 0.00007391
Iteration 187/1000 | Loss: 0.00004556
Iteration 188/1000 | Loss: 0.00003924
Iteration 189/1000 | Loss: 0.00003669
Iteration 190/1000 | Loss: 0.00003524
Iteration 191/1000 | Loss: 0.00003389
Iteration 192/1000 | Loss: 0.00003306
Iteration 193/1000 | Loss: 0.00003248
Iteration 194/1000 | Loss: 0.00003187
Iteration 195/1000 | Loss: 0.00003151
Iteration 196/1000 | Loss: 0.00003135
Iteration 197/1000 | Loss: 0.00003131
Iteration 198/1000 | Loss: 0.00003123
Iteration 199/1000 | Loss: 0.00003118
Iteration 200/1000 | Loss: 0.00003118
Iteration 201/1000 | Loss: 0.00003113
Iteration 202/1000 | Loss: 0.00003113
Iteration 203/1000 | Loss: 0.00003112
Iteration 204/1000 | Loss: 0.00003112
Iteration 205/1000 | Loss: 0.00003111
Iteration 206/1000 | Loss: 0.00003111
Iteration 207/1000 | Loss: 0.00003110
Iteration 208/1000 | Loss: 0.00003110
Iteration 209/1000 | Loss: 0.00003110
Iteration 210/1000 | Loss: 0.00003110
Iteration 211/1000 | Loss: 0.00003110
Iteration 212/1000 | Loss: 0.00003109
Iteration 213/1000 | Loss: 0.00003109
Iteration 214/1000 | Loss: 0.00003109
Iteration 215/1000 | Loss: 0.00003108
Iteration 216/1000 | Loss: 0.00003108
Iteration 217/1000 | Loss: 0.00003108
Iteration 218/1000 | Loss: 0.00003107
Iteration 219/1000 | Loss: 0.00003106
Iteration 220/1000 | Loss: 0.00003106
Iteration 221/1000 | Loss: 0.00003106
Iteration 222/1000 | Loss: 0.00003106
Iteration 223/1000 | Loss: 0.00003105
Iteration 224/1000 | Loss: 0.00003105
Iteration 225/1000 | Loss: 0.00003104
Iteration 226/1000 | Loss: 0.00003104
Iteration 227/1000 | Loss: 0.00003103
Iteration 228/1000 | Loss: 0.00003103
Iteration 229/1000 | Loss: 0.00003103
Iteration 230/1000 | Loss: 0.00003102
Iteration 231/1000 | Loss: 0.00003102
Iteration 232/1000 | Loss: 0.00003102
Iteration 233/1000 | Loss: 0.00003102
Iteration 234/1000 | Loss: 0.00003102
Iteration 235/1000 | Loss: 0.00003102
Iteration 236/1000 | Loss: 0.00003101
Iteration 237/1000 | Loss: 0.00003101
Iteration 238/1000 | Loss: 0.00003101
Iteration 239/1000 | Loss: 0.00003101
Iteration 240/1000 | Loss: 0.00003101
Iteration 241/1000 | Loss: 0.00003101
Iteration 242/1000 | Loss: 0.00003100
Iteration 243/1000 | Loss: 0.00003100
Iteration 244/1000 | Loss: 0.00003100
Iteration 245/1000 | Loss: 0.00003100
Iteration 246/1000 | Loss: 0.00003100
Iteration 247/1000 | Loss: 0.00003099
Iteration 248/1000 | Loss: 0.00003099
Iteration 249/1000 | Loss: 0.00003099
Iteration 250/1000 | Loss: 0.00003098
Iteration 251/1000 | Loss: 0.00003098
Iteration 252/1000 | Loss: 0.00003098
Iteration 253/1000 | Loss: 0.00003098
Iteration 254/1000 | Loss: 0.00003097
Iteration 255/1000 | Loss: 0.00003097
Iteration 256/1000 | Loss: 0.00003097
Iteration 257/1000 | Loss: 0.00003097
Iteration 258/1000 | Loss: 0.00003097
Iteration 259/1000 | Loss: 0.00003097
Iteration 260/1000 | Loss: 0.00003096
Iteration 261/1000 | Loss: 0.00003096
Iteration 262/1000 | Loss: 0.00003096
Iteration 263/1000 | Loss: 0.00003096
Iteration 264/1000 | Loss: 0.00003096
Iteration 265/1000 | Loss: 0.00003096
Iteration 266/1000 | Loss: 0.00003096
Iteration 267/1000 | Loss: 0.00003095
Iteration 268/1000 | Loss: 0.00003095
Iteration 269/1000 | Loss: 0.00003095
Iteration 270/1000 | Loss: 0.00003095
Iteration 271/1000 | Loss: 0.00003094
Iteration 272/1000 | Loss: 0.00003094
Iteration 273/1000 | Loss: 0.00003094
Iteration 274/1000 | Loss: 0.00003094
Iteration 275/1000 | Loss: 0.00003093
Iteration 276/1000 | Loss: 0.00003093
Iteration 277/1000 | Loss: 0.00003092
Iteration 278/1000 | Loss: 0.00003092
Iteration 279/1000 | Loss: 0.00003092
Iteration 280/1000 | Loss: 0.00003092
Iteration 281/1000 | Loss: 0.00003092
Iteration 282/1000 | Loss: 0.00003091
Iteration 283/1000 | Loss: 0.00003091
Iteration 284/1000 | Loss: 0.00003091
Iteration 285/1000 | Loss: 0.00003090
Iteration 286/1000 | Loss: 0.00003090
Iteration 287/1000 | Loss: 0.00003090
Iteration 288/1000 | Loss: 0.00003089
Iteration 289/1000 | Loss: 0.00003088
Iteration 290/1000 | Loss: 0.00003088
Iteration 291/1000 | Loss: 0.00003088
Iteration 292/1000 | Loss: 0.00003088
Iteration 293/1000 | Loss: 0.00003088
Iteration 294/1000 | Loss: 0.00003088
Iteration 295/1000 | Loss: 0.00003088
Iteration 296/1000 | Loss: 0.00003087
Iteration 297/1000 | Loss: 0.00003086
Iteration 298/1000 | Loss: 0.00003086
Iteration 299/1000 | Loss: 0.00003086
Iteration 300/1000 | Loss: 0.00003085
Iteration 301/1000 | Loss: 0.00003085
Iteration 302/1000 | Loss: 0.00003083
Iteration 303/1000 | Loss: 0.00003083
Iteration 304/1000 | Loss: 0.00003083
Iteration 305/1000 | Loss: 0.00003083
Iteration 306/1000 | Loss: 0.00003083
Iteration 307/1000 | Loss: 0.00003083
Iteration 308/1000 | Loss: 0.00003083
Iteration 309/1000 | Loss: 0.00003082
Iteration 310/1000 | Loss: 0.00003082
Iteration 311/1000 | Loss: 0.00003081
Iteration 312/1000 | Loss: 0.00003081
Iteration 313/1000 | Loss: 0.00003080
Iteration 314/1000 | Loss: 0.00003080
Iteration 315/1000 | Loss: 0.00003080
Iteration 316/1000 | Loss: 0.00003080
Iteration 317/1000 | Loss: 0.00003080
Iteration 318/1000 | Loss: 0.00003079
Iteration 319/1000 | Loss: 0.00003079
Iteration 320/1000 | Loss: 0.00003078
Iteration 321/1000 | Loss: 0.00003078
Iteration 322/1000 | Loss: 0.00003077
Iteration 323/1000 | Loss: 0.00003077
Iteration 324/1000 | Loss: 0.00003077
Iteration 325/1000 | Loss: 0.00003077
Iteration 326/1000 | Loss: 0.00003076
Iteration 327/1000 | Loss: 0.00003076
Iteration 328/1000 | Loss: 0.00003076
Iteration 329/1000 | Loss: 0.00003075
Iteration 330/1000 | Loss: 0.00003075
Iteration 331/1000 | Loss: 0.00003075
Iteration 332/1000 | Loss: 0.00003074
Iteration 333/1000 | Loss: 0.00003074
Iteration 334/1000 | Loss: 0.00003074
Iteration 335/1000 | Loss: 0.00003074
Iteration 336/1000 | Loss: 0.00003073
Iteration 337/1000 | Loss: 0.00003073
Iteration 338/1000 | Loss: 0.00003073
Iteration 339/1000 | Loss: 0.00003073
Iteration 340/1000 | Loss: 0.00003072
Iteration 341/1000 | Loss: 0.00003072
Iteration 342/1000 | Loss: 0.00003072
Iteration 343/1000 | Loss: 0.00003072
Iteration 344/1000 | Loss: 0.00003072
Iteration 345/1000 | Loss: 0.00003071
Iteration 346/1000 | Loss: 0.00003071
Iteration 347/1000 | Loss: 0.00003071
Iteration 348/1000 | Loss: 0.00003071
Iteration 349/1000 | Loss: 0.00003071
Iteration 350/1000 | Loss: 0.00003070
Iteration 351/1000 | Loss: 0.00003070
Iteration 352/1000 | Loss: 0.00003070
Iteration 353/1000 | Loss: 0.00003070
Iteration 354/1000 | Loss: 0.00003070
Iteration 355/1000 | Loss: 0.00003070
Iteration 356/1000 | Loss: 0.00003070
Iteration 357/1000 | Loss: 0.00003069
Iteration 358/1000 | Loss: 0.00003069
Iteration 359/1000 | Loss: 0.00003069
Iteration 360/1000 | Loss: 0.00003069
Iteration 361/1000 | Loss: 0.00003068
Iteration 362/1000 | Loss: 0.00003068
Iteration 363/1000 | Loss: 0.00003068
Iteration 364/1000 | Loss: 0.00003068
Iteration 365/1000 | Loss: 0.00003068
Iteration 366/1000 | Loss: 0.00003068
Iteration 367/1000 | Loss: 0.00003068
Iteration 368/1000 | Loss: 0.00003067
Iteration 369/1000 | Loss: 0.00003067
Iteration 370/1000 | Loss: 0.00003067
Iteration 371/1000 | Loss: 0.00003067
Iteration 372/1000 | Loss: 0.00003067
Iteration 373/1000 | Loss: 0.00003067
Iteration 374/1000 | Loss: 0.00003067
Iteration 375/1000 | Loss: 0.00003067
Iteration 376/1000 | Loss: 0.00003067
Iteration 377/1000 | Loss: 0.00003067
Iteration 378/1000 | Loss: 0.00003067
Iteration 379/1000 | Loss: 0.00003066
Iteration 380/1000 | Loss: 0.00003066
Iteration 381/1000 | Loss: 0.00003066
Iteration 382/1000 | Loss: 0.00003065
Iteration 383/1000 | Loss: 0.00003065
Iteration 384/1000 | Loss: 0.00003065
Iteration 385/1000 | Loss: 0.00003064
Iteration 386/1000 | Loss: 0.00003064
Iteration 387/1000 | Loss: 0.00003064
Iteration 388/1000 | Loss: 0.00003063
Iteration 389/1000 | Loss: 0.00003063
Iteration 390/1000 | Loss: 0.00003063
Iteration 391/1000 | Loss: 0.00003063
Iteration 392/1000 | Loss: 0.00003063
Iteration 393/1000 | Loss: 0.00003063
Iteration 394/1000 | Loss: 0.00003063
Iteration 395/1000 | Loss: 0.00003063
Iteration 396/1000 | Loss: 0.00003062
Iteration 397/1000 | Loss: 0.00003062
Iteration 398/1000 | Loss: 0.00003062
Iteration 399/1000 | Loss: 0.00003062
Iteration 400/1000 | Loss: 0.00003062
Iteration 401/1000 | Loss: 0.00003062
Iteration 402/1000 | Loss: 0.00003062
Iteration 403/1000 | Loss: 0.00003062
Iteration 404/1000 | Loss: 0.00003062
Iteration 405/1000 | Loss: 0.00003062
Iteration 406/1000 | Loss: 0.00003062
Iteration 407/1000 | Loss: 0.00003062
Iteration 408/1000 | Loss: 0.00003061
Iteration 409/1000 | Loss: 0.00003061
Iteration 410/1000 | Loss: 0.00003061
Iteration 411/1000 | Loss: 0.00003061
Iteration 412/1000 | Loss: 0.00003061
Iteration 413/1000 | Loss: 0.00003061
Iteration 414/1000 | Loss: 0.00003061
Iteration 415/1000 | Loss: 0.00003061
Iteration 416/1000 | Loss: 0.00003061
Iteration 417/1000 | Loss: 0.00003061
Iteration 418/1000 | Loss: 0.00003061
Iteration 419/1000 | Loss: 0.00003061
Iteration 420/1000 | Loss: 0.00003061
Iteration 421/1000 | Loss: 0.00003061
Iteration 422/1000 | Loss: 0.00003061
Iteration 423/1000 | Loss: 0.00003061
Iteration 424/1000 | Loss: 0.00003061
Iteration 425/1000 | Loss: 0.00003061
Iteration 426/1000 | Loss: 0.00003061
Iteration 427/1000 | Loss: 0.00003061
Iteration 428/1000 | Loss: 0.00003061
Iteration 429/1000 | Loss: 0.00003061
Iteration 430/1000 | Loss: 0.00003061
Iteration 431/1000 | Loss: 0.00003061
Iteration 432/1000 | Loss: 0.00003061
Iteration 433/1000 | Loss: 0.00003061
Iteration 434/1000 | Loss: 0.00003061
Iteration 435/1000 | Loss: 0.00003061
Iteration 436/1000 | Loss: 0.00003061
Iteration 437/1000 | Loss: 0.00003061
Iteration 438/1000 | Loss: 0.00003061
Iteration 439/1000 | Loss: 0.00003061
Iteration 440/1000 | Loss: 0.00003061
Iteration 441/1000 | Loss: 0.00003061
Iteration 442/1000 | Loss: 0.00003061
Iteration 443/1000 | Loss: 0.00003061
Iteration 444/1000 | Loss: 0.00003061
Iteration 445/1000 | Loss: 0.00003061
Iteration 446/1000 | Loss: 0.00003061
Iteration 447/1000 | Loss: 0.00003061
Iteration 448/1000 | Loss: 0.00003061
Iteration 449/1000 | Loss: 0.00003061
Iteration 450/1000 | Loss: 0.00003061
Iteration 451/1000 | Loss: 0.00003061
Iteration 452/1000 | Loss: 0.00003061
Iteration 453/1000 | Loss: 0.00003061
Iteration 454/1000 | Loss: 0.00003061
Iteration 455/1000 | Loss: 0.00003061
Iteration 456/1000 | Loss: 0.00003061
Iteration 457/1000 | Loss: 0.00003061
Iteration 458/1000 | Loss: 0.00003061
Iteration 459/1000 | Loss: 0.00003061
Iteration 460/1000 | Loss: 0.00003061
Iteration 461/1000 | Loss: 0.00003061
Iteration 462/1000 | Loss: 0.00003061
Iteration 463/1000 | Loss: 0.00003061
Iteration 464/1000 | Loss: 0.00003061
Iteration 465/1000 | Loss: 0.00003061
Iteration 466/1000 | Loss: 0.00003061
Iteration 467/1000 | Loss: 0.00003061
Iteration 468/1000 | Loss: 0.00003061
Iteration 469/1000 | Loss: 0.00003061
Iteration 470/1000 | Loss: 0.00003061
Iteration 471/1000 | Loss: 0.00003061
Iteration 472/1000 | Loss: 0.00003061
Iteration 473/1000 | Loss: 0.00003061
Iteration 474/1000 | Loss: 0.00003061
Iteration 475/1000 | Loss: 0.00003061
Iteration 476/1000 | Loss: 0.00003061
Iteration 477/1000 | Loss: 0.00003061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 477. Stopping optimization.
Last 5 losses: [3.060854214709252e-05, 3.060854214709252e-05, 3.060854214709252e-05, 3.060854214709252e-05, 3.060854214709252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060854214709252e-05

Optimization complete. Final v2v error: 4.470489978790283 mm

Highest mean error: 6.80576753616333 mm for frame 58

Lowest mean error: 3.9084982872009277 mm for frame 114

Saving results

Total time: 149.65553545951843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022599
Iteration 2/25 | Loss: 0.00151829
Iteration 3/25 | Loss: 0.00132545
Iteration 4/25 | Loss: 0.00133301
Iteration 5/25 | Loss: 0.00132178
Iteration 6/25 | Loss: 0.00131174
Iteration 7/25 | Loss: 0.00131353
Iteration 8/25 | Loss: 0.00131858
Iteration 9/25 | Loss: 0.00131438
Iteration 10/25 | Loss: 0.00130375
Iteration 11/25 | Loss: 0.00130049
Iteration 12/25 | Loss: 0.00129778
Iteration 13/25 | Loss: 0.00129745
Iteration 14/25 | Loss: 0.00128936
Iteration 15/25 | Loss: 0.00128814
Iteration 16/25 | Loss: 0.00128773
Iteration 17/25 | Loss: 0.00128828
Iteration 18/25 | Loss: 0.00128772
Iteration 19/25 | Loss: 0.00128771
Iteration 20/25 | Loss: 0.00128490
Iteration 21/25 | Loss: 0.00128419
Iteration 22/25 | Loss: 0.00128395
Iteration 23/25 | Loss: 0.00128385
Iteration 24/25 | Loss: 0.00128376
Iteration 25/25 | Loss: 0.00128441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.33487678
Iteration 2/25 | Loss: 0.00154768
Iteration 3/25 | Loss: 0.00154768
Iteration 4/25 | Loss: 0.00154768
Iteration 5/25 | Loss: 0.00154768
Iteration 6/25 | Loss: 0.00154768
Iteration 7/25 | Loss: 0.00154768
Iteration 8/25 | Loss: 0.00154768
Iteration 9/25 | Loss: 0.00154768
Iteration 10/25 | Loss: 0.00154768
Iteration 11/25 | Loss: 0.00154768
Iteration 12/25 | Loss: 0.00154768
Iteration 13/25 | Loss: 0.00154768
Iteration 14/25 | Loss: 0.00154768
Iteration 15/25 | Loss: 0.00154768
Iteration 16/25 | Loss: 0.00154768
Iteration 17/25 | Loss: 0.00154768
Iteration 18/25 | Loss: 0.00154768
Iteration 19/25 | Loss: 0.00154768
Iteration 20/25 | Loss: 0.00154768
Iteration 21/25 | Loss: 0.00154768
Iteration 22/25 | Loss: 0.00154768
Iteration 23/25 | Loss: 0.00154768
Iteration 24/25 | Loss: 0.00154768
Iteration 25/25 | Loss: 0.00154768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154768
Iteration 2/1000 | Loss: 0.00002640
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001542
Iteration 6/1000 | Loss: 0.00001472
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001355
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001267
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001266
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001265
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001264
Iteration 44/1000 | Loss: 0.00001264
Iteration 45/1000 | Loss: 0.00001264
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00001261
Iteration 54/1000 | Loss: 0.00001261
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001260
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001256
Iteration 75/1000 | Loss: 0.00001256
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001249
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001246
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001243
Iteration 122/1000 | Loss: 0.00001243
Iteration 123/1000 | Loss: 0.00001243
Iteration 124/1000 | Loss: 0.00001243
Iteration 125/1000 | Loss: 0.00001242
Iteration 126/1000 | Loss: 0.00001242
Iteration 127/1000 | Loss: 0.00001242
Iteration 128/1000 | Loss: 0.00001242
Iteration 129/1000 | Loss: 0.00001242
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001241
Iteration 132/1000 | Loss: 0.00001241
Iteration 133/1000 | Loss: 0.00001241
Iteration 134/1000 | Loss: 0.00001241
Iteration 135/1000 | Loss: 0.00001241
Iteration 136/1000 | Loss: 0.00001240
Iteration 137/1000 | Loss: 0.00001240
Iteration 138/1000 | Loss: 0.00001240
Iteration 139/1000 | Loss: 0.00001240
Iteration 140/1000 | Loss: 0.00001240
Iteration 141/1000 | Loss: 0.00001240
Iteration 142/1000 | Loss: 0.00001240
Iteration 143/1000 | Loss: 0.00001240
Iteration 144/1000 | Loss: 0.00020485
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001488
Iteration 147/1000 | Loss: 0.00001393
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001286
Iteration 155/1000 | Loss: 0.00021538
Iteration 156/1000 | Loss: 0.00001591
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001311
Iteration 159/1000 | Loss: 0.00001252
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001214
Iteration 164/1000 | Loss: 0.00001211
Iteration 165/1000 | Loss: 0.00001211
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001209
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001208
Iteration 172/1000 | Loss: 0.00001208
Iteration 173/1000 | Loss: 0.00001208
Iteration 174/1000 | Loss: 0.00001208
Iteration 175/1000 | Loss: 0.00001207
Iteration 176/1000 | Loss: 0.00001207
Iteration 177/1000 | Loss: 0.00001207
Iteration 178/1000 | Loss: 0.00001207
Iteration 179/1000 | Loss: 0.00001207
Iteration 180/1000 | Loss: 0.00001207
Iteration 181/1000 | Loss: 0.00001206
Iteration 182/1000 | Loss: 0.00001206
Iteration 183/1000 | Loss: 0.00001206
Iteration 184/1000 | Loss: 0.00001206
Iteration 185/1000 | Loss: 0.00001205
Iteration 186/1000 | Loss: 0.00001205
Iteration 187/1000 | Loss: 0.00001205
Iteration 188/1000 | Loss: 0.00001205
Iteration 189/1000 | Loss: 0.00001204
Iteration 190/1000 | Loss: 0.00001204
Iteration 191/1000 | Loss: 0.00001204
Iteration 192/1000 | Loss: 0.00001204
Iteration 193/1000 | Loss: 0.00001204
Iteration 194/1000 | Loss: 0.00001203
Iteration 195/1000 | Loss: 0.00001203
Iteration 196/1000 | Loss: 0.00001203
Iteration 197/1000 | Loss: 0.00001203
Iteration 198/1000 | Loss: 0.00001203
Iteration 199/1000 | Loss: 0.00001203
Iteration 200/1000 | Loss: 0.00001202
Iteration 201/1000 | Loss: 0.00001202
Iteration 202/1000 | Loss: 0.00001202
Iteration 203/1000 | Loss: 0.00001201
Iteration 204/1000 | Loss: 0.00001201
Iteration 205/1000 | Loss: 0.00001201
Iteration 206/1000 | Loss: 0.00001201
Iteration 207/1000 | Loss: 0.00001201
Iteration 208/1000 | Loss: 0.00001201
Iteration 209/1000 | Loss: 0.00001201
Iteration 210/1000 | Loss: 0.00001201
Iteration 211/1000 | Loss: 0.00001201
Iteration 212/1000 | Loss: 0.00001200
Iteration 213/1000 | Loss: 0.00001200
Iteration 214/1000 | Loss: 0.00001200
Iteration 215/1000 | Loss: 0.00001200
Iteration 216/1000 | Loss: 0.00001200
Iteration 217/1000 | Loss: 0.00001199
Iteration 218/1000 | Loss: 0.00001199
Iteration 219/1000 | Loss: 0.00001199
Iteration 220/1000 | Loss: 0.00001199
Iteration 221/1000 | Loss: 0.00001199
Iteration 222/1000 | Loss: 0.00001199
Iteration 223/1000 | Loss: 0.00001199
Iteration 224/1000 | Loss: 0.00001199
Iteration 225/1000 | Loss: 0.00001199
Iteration 226/1000 | Loss: 0.00001199
Iteration 227/1000 | Loss: 0.00001199
Iteration 228/1000 | Loss: 0.00001198
Iteration 229/1000 | Loss: 0.00001198
Iteration 230/1000 | Loss: 0.00001198
Iteration 231/1000 | Loss: 0.00001198
Iteration 232/1000 | Loss: 0.00001198
Iteration 233/1000 | Loss: 0.00001198
Iteration 234/1000 | Loss: 0.00001198
Iteration 235/1000 | Loss: 0.00001198
Iteration 236/1000 | Loss: 0.00001198
Iteration 237/1000 | Loss: 0.00001198
Iteration 238/1000 | Loss: 0.00001198
Iteration 239/1000 | Loss: 0.00001198
Iteration 240/1000 | Loss: 0.00001198
Iteration 241/1000 | Loss: 0.00001198
Iteration 242/1000 | Loss: 0.00001198
Iteration 243/1000 | Loss: 0.00001198
Iteration 244/1000 | Loss: 0.00001198
Iteration 245/1000 | Loss: 0.00001197
Iteration 246/1000 | Loss: 0.00001197
Iteration 247/1000 | Loss: 0.00001197
Iteration 248/1000 | Loss: 0.00001197
Iteration 249/1000 | Loss: 0.00001197
Iteration 250/1000 | Loss: 0.00001197
Iteration 251/1000 | Loss: 0.00001197
Iteration 252/1000 | Loss: 0.00001197
Iteration 253/1000 | Loss: 0.00001197
Iteration 254/1000 | Loss: 0.00001197
Iteration 255/1000 | Loss: 0.00001197
Iteration 256/1000 | Loss: 0.00001197
Iteration 257/1000 | Loss: 0.00001196
Iteration 258/1000 | Loss: 0.00001196
Iteration 259/1000 | Loss: 0.00001196
Iteration 260/1000 | Loss: 0.00001196
Iteration 261/1000 | Loss: 0.00001196
Iteration 262/1000 | Loss: 0.00001195
Iteration 263/1000 | Loss: 0.00001195
Iteration 264/1000 | Loss: 0.00001195
Iteration 265/1000 | Loss: 0.00001195
Iteration 266/1000 | Loss: 0.00001195
Iteration 267/1000 | Loss: 0.00001195
Iteration 268/1000 | Loss: 0.00001195
Iteration 269/1000 | Loss: 0.00001195
Iteration 270/1000 | Loss: 0.00001195
Iteration 271/1000 | Loss: 0.00001195
Iteration 272/1000 | Loss: 0.00001195
Iteration 273/1000 | Loss: 0.00001195
Iteration 274/1000 | Loss: 0.00001195
Iteration 275/1000 | Loss: 0.00001195
Iteration 276/1000 | Loss: 0.00001195
Iteration 277/1000 | Loss: 0.00001195
Iteration 278/1000 | Loss: 0.00001195
Iteration 279/1000 | Loss: 0.00001195
Iteration 280/1000 | Loss: 0.00001195
Iteration 281/1000 | Loss: 0.00001195
Iteration 282/1000 | Loss: 0.00001195
Iteration 283/1000 | Loss: 0.00001195
Iteration 284/1000 | Loss: 0.00001195
Iteration 285/1000 | Loss: 0.00001195
Iteration 286/1000 | Loss: 0.00001195
Iteration 287/1000 | Loss: 0.00001195
Iteration 288/1000 | Loss: 0.00001194
Iteration 289/1000 | Loss: 0.00001194
Iteration 290/1000 | Loss: 0.00001194
Iteration 291/1000 | Loss: 0.00001194
Iteration 292/1000 | Loss: 0.00001194
Iteration 293/1000 | Loss: 0.00001194
Iteration 294/1000 | Loss: 0.00001194
Iteration 295/1000 | Loss: 0.00001194
Iteration 296/1000 | Loss: 0.00001194
Iteration 297/1000 | Loss: 0.00001194
Iteration 298/1000 | Loss: 0.00001194
Iteration 299/1000 | Loss: 0.00001194
Iteration 300/1000 | Loss: 0.00001194
Iteration 301/1000 | Loss: 0.00001194
Iteration 302/1000 | Loss: 0.00001194
Iteration 303/1000 | Loss: 0.00001194
Iteration 304/1000 | Loss: 0.00001194
Iteration 305/1000 | Loss: 0.00001194
Iteration 306/1000 | Loss: 0.00001194
Iteration 307/1000 | Loss: 0.00001194
Iteration 308/1000 | Loss: 0.00001194
Iteration 309/1000 | Loss: 0.00001194
Iteration 310/1000 | Loss: 0.00001194
Iteration 311/1000 | Loss: 0.00001194
Iteration 312/1000 | Loss: 0.00001194
Iteration 313/1000 | Loss: 0.00001194
Iteration 314/1000 | Loss: 0.00001194
Iteration 315/1000 | Loss: 0.00001194
Iteration 316/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [1.1944179277634248e-05, 1.1944179277634248e-05, 1.1944179277634248e-05, 1.1944179277634248e-05, 1.1944179277634248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1944179277634248e-05

Optimization complete. Final v2v error: 2.951326608657837 mm

Highest mean error: 4.018624782562256 mm for frame 5

Lowest mean error: 2.733860492706299 mm for frame 208

Saving results

Total time: 124.0201461315155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952675
Iteration 2/25 | Loss: 0.00177697
Iteration 3/25 | Loss: 0.00145184
Iteration 4/25 | Loss: 0.00143176
Iteration 5/25 | Loss: 0.00142548
Iteration 6/25 | Loss: 0.00142438
Iteration 7/25 | Loss: 0.00142438
Iteration 8/25 | Loss: 0.00142438
Iteration 9/25 | Loss: 0.00142438
Iteration 10/25 | Loss: 0.00142438
Iteration 11/25 | Loss: 0.00142435
Iteration 12/25 | Loss: 0.00142435
Iteration 13/25 | Loss: 0.00142435
Iteration 14/25 | Loss: 0.00142435
Iteration 15/25 | Loss: 0.00142435
Iteration 16/25 | Loss: 0.00142435
Iteration 17/25 | Loss: 0.00142435
Iteration 18/25 | Loss: 0.00142435
Iteration 19/25 | Loss: 0.00142435
Iteration 20/25 | Loss: 0.00142435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014243506593629718, 0.0014243506593629718, 0.0014243506593629718, 0.0014243506593629718, 0.0014243506593629718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014243506593629718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75255197
Iteration 2/25 | Loss: 0.00153894
Iteration 3/25 | Loss: 0.00153893
Iteration 4/25 | Loss: 0.00153893
Iteration 5/25 | Loss: 0.00153893
Iteration 6/25 | Loss: 0.00153893
Iteration 7/25 | Loss: 0.00153893
Iteration 8/25 | Loss: 0.00153893
Iteration 9/25 | Loss: 0.00153893
Iteration 10/25 | Loss: 0.00153893
Iteration 11/25 | Loss: 0.00153893
Iteration 12/25 | Loss: 0.00153893
Iteration 13/25 | Loss: 0.00153893
Iteration 14/25 | Loss: 0.00153893
Iteration 15/25 | Loss: 0.00153893
Iteration 16/25 | Loss: 0.00153893
Iteration 17/25 | Loss: 0.00153893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015389283653348684, 0.0015389283653348684, 0.0015389283653348684, 0.0015389283653348684, 0.0015389283653348684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015389283653348684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153893
Iteration 2/1000 | Loss: 0.00005240
Iteration 3/1000 | Loss: 0.00004018
Iteration 4/1000 | Loss: 0.00003559
Iteration 5/1000 | Loss: 0.00003395
Iteration 6/1000 | Loss: 0.00003254
Iteration 7/1000 | Loss: 0.00003172
Iteration 8/1000 | Loss: 0.00003087
Iteration 9/1000 | Loss: 0.00003042
Iteration 10/1000 | Loss: 0.00003002
Iteration 11/1000 | Loss: 0.00002966
Iteration 12/1000 | Loss: 0.00002934
Iteration 13/1000 | Loss: 0.00002907
Iteration 14/1000 | Loss: 0.00002884
Iteration 15/1000 | Loss: 0.00002853
Iteration 16/1000 | Loss: 0.00002828
Iteration 17/1000 | Loss: 0.00002803
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002772
Iteration 20/1000 | Loss: 0.00002765
Iteration 21/1000 | Loss: 0.00002758
Iteration 22/1000 | Loss: 0.00002746
Iteration 23/1000 | Loss: 0.00002746
Iteration 24/1000 | Loss: 0.00002745
Iteration 25/1000 | Loss: 0.00002741
Iteration 26/1000 | Loss: 0.00002740
Iteration 27/1000 | Loss: 0.00002734
Iteration 28/1000 | Loss: 0.00002732
Iteration 29/1000 | Loss: 0.00002729
Iteration 30/1000 | Loss: 0.00002729
Iteration 31/1000 | Loss: 0.00002728
Iteration 32/1000 | Loss: 0.00002728
Iteration 33/1000 | Loss: 0.00002728
Iteration 34/1000 | Loss: 0.00002727
Iteration 35/1000 | Loss: 0.00002726
Iteration 36/1000 | Loss: 0.00002726
Iteration 37/1000 | Loss: 0.00002726
Iteration 38/1000 | Loss: 0.00002726
Iteration 39/1000 | Loss: 0.00002726
Iteration 40/1000 | Loss: 0.00002726
Iteration 41/1000 | Loss: 0.00002725
Iteration 42/1000 | Loss: 0.00002725
Iteration 43/1000 | Loss: 0.00002725
Iteration 44/1000 | Loss: 0.00002725
Iteration 45/1000 | Loss: 0.00002725
Iteration 46/1000 | Loss: 0.00002725
Iteration 47/1000 | Loss: 0.00002725
Iteration 48/1000 | Loss: 0.00002725
Iteration 49/1000 | Loss: 0.00002724
Iteration 50/1000 | Loss: 0.00002724
Iteration 51/1000 | Loss: 0.00002724
Iteration 52/1000 | Loss: 0.00002724
Iteration 53/1000 | Loss: 0.00002724
Iteration 54/1000 | Loss: 0.00002724
Iteration 55/1000 | Loss: 0.00002724
Iteration 56/1000 | Loss: 0.00002723
Iteration 57/1000 | Loss: 0.00002723
Iteration 58/1000 | Loss: 0.00002723
Iteration 59/1000 | Loss: 0.00002723
Iteration 60/1000 | Loss: 0.00002723
Iteration 61/1000 | Loss: 0.00002723
Iteration 62/1000 | Loss: 0.00002722
Iteration 63/1000 | Loss: 0.00002722
Iteration 64/1000 | Loss: 0.00002722
Iteration 65/1000 | Loss: 0.00002722
Iteration 66/1000 | Loss: 0.00002722
Iteration 67/1000 | Loss: 0.00002722
Iteration 68/1000 | Loss: 0.00002721
Iteration 69/1000 | Loss: 0.00002721
Iteration 70/1000 | Loss: 0.00002721
Iteration 71/1000 | Loss: 0.00002721
Iteration 72/1000 | Loss: 0.00002721
Iteration 73/1000 | Loss: 0.00002721
Iteration 74/1000 | Loss: 0.00002721
Iteration 75/1000 | Loss: 0.00002721
Iteration 76/1000 | Loss: 0.00002721
Iteration 77/1000 | Loss: 0.00002721
Iteration 78/1000 | Loss: 0.00002721
Iteration 79/1000 | Loss: 0.00002720
Iteration 80/1000 | Loss: 0.00002720
Iteration 81/1000 | Loss: 0.00002720
Iteration 82/1000 | Loss: 0.00002720
Iteration 83/1000 | Loss: 0.00002720
Iteration 84/1000 | Loss: 0.00002720
Iteration 85/1000 | Loss: 0.00002720
Iteration 86/1000 | Loss: 0.00002720
Iteration 87/1000 | Loss: 0.00002720
Iteration 88/1000 | Loss: 0.00002720
Iteration 89/1000 | Loss: 0.00002720
Iteration 90/1000 | Loss: 0.00002720
Iteration 91/1000 | Loss: 0.00002719
Iteration 92/1000 | Loss: 0.00002719
Iteration 93/1000 | Loss: 0.00002719
Iteration 94/1000 | Loss: 0.00002719
Iteration 95/1000 | Loss: 0.00002719
Iteration 96/1000 | Loss: 0.00002719
Iteration 97/1000 | Loss: 0.00002719
Iteration 98/1000 | Loss: 0.00002719
Iteration 99/1000 | Loss: 0.00002719
Iteration 100/1000 | Loss: 0.00002718
Iteration 101/1000 | Loss: 0.00002718
Iteration 102/1000 | Loss: 0.00002718
Iteration 103/1000 | Loss: 0.00002718
Iteration 104/1000 | Loss: 0.00002718
Iteration 105/1000 | Loss: 0.00002718
Iteration 106/1000 | Loss: 0.00002718
Iteration 107/1000 | Loss: 0.00002718
Iteration 108/1000 | Loss: 0.00002718
Iteration 109/1000 | Loss: 0.00002718
Iteration 110/1000 | Loss: 0.00002718
Iteration 111/1000 | Loss: 0.00002718
Iteration 112/1000 | Loss: 0.00002717
Iteration 113/1000 | Loss: 0.00002717
Iteration 114/1000 | Loss: 0.00002717
Iteration 115/1000 | Loss: 0.00002717
Iteration 116/1000 | Loss: 0.00002717
Iteration 117/1000 | Loss: 0.00002717
Iteration 118/1000 | Loss: 0.00002717
Iteration 119/1000 | Loss: 0.00002716
Iteration 120/1000 | Loss: 0.00002716
Iteration 121/1000 | Loss: 0.00002716
Iteration 122/1000 | Loss: 0.00002716
Iteration 123/1000 | Loss: 0.00002716
Iteration 124/1000 | Loss: 0.00002716
Iteration 125/1000 | Loss: 0.00002716
Iteration 126/1000 | Loss: 0.00002716
Iteration 127/1000 | Loss: 0.00002716
Iteration 128/1000 | Loss: 0.00002716
Iteration 129/1000 | Loss: 0.00002716
Iteration 130/1000 | Loss: 0.00002716
Iteration 131/1000 | Loss: 0.00002716
Iteration 132/1000 | Loss: 0.00002716
Iteration 133/1000 | Loss: 0.00002715
Iteration 134/1000 | Loss: 0.00002715
Iteration 135/1000 | Loss: 0.00002715
Iteration 136/1000 | Loss: 0.00002715
Iteration 137/1000 | Loss: 0.00002715
Iteration 138/1000 | Loss: 0.00002715
Iteration 139/1000 | Loss: 0.00002715
Iteration 140/1000 | Loss: 0.00002715
Iteration 141/1000 | Loss: 0.00002714
Iteration 142/1000 | Loss: 0.00002714
Iteration 143/1000 | Loss: 0.00002714
Iteration 144/1000 | Loss: 0.00002714
Iteration 145/1000 | Loss: 0.00002714
Iteration 146/1000 | Loss: 0.00002714
Iteration 147/1000 | Loss: 0.00002714
Iteration 148/1000 | Loss: 0.00002714
Iteration 149/1000 | Loss: 0.00002714
Iteration 150/1000 | Loss: 0.00002714
Iteration 151/1000 | Loss: 0.00002714
Iteration 152/1000 | Loss: 0.00002714
Iteration 153/1000 | Loss: 0.00002714
Iteration 154/1000 | Loss: 0.00002714
Iteration 155/1000 | Loss: 0.00002714
Iteration 156/1000 | Loss: 0.00002714
Iteration 157/1000 | Loss: 0.00002714
Iteration 158/1000 | Loss: 0.00002714
Iteration 159/1000 | Loss: 0.00002714
Iteration 160/1000 | Loss: 0.00002714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.7139074518345296e-05, 2.7139074518345296e-05, 2.7139074518345296e-05, 2.7139074518345296e-05, 2.7139074518345296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7139074518345296e-05

Optimization complete. Final v2v error: 4.4327874183654785 mm

Highest mean error: 4.934288501739502 mm for frame 189

Lowest mean error: 3.9266910552978516 mm for frame 51

Saving results

Total time: 51.62666034698486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009925
Iteration 2/25 | Loss: 0.00244683
Iteration 3/25 | Loss: 0.00197300
Iteration 4/25 | Loss: 0.00183471
Iteration 5/25 | Loss: 0.00178241
Iteration 6/25 | Loss: 0.00149170
Iteration 7/25 | Loss: 0.00144074
Iteration 8/25 | Loss: 0.00144199
Iteration 9/25 | Loss: 0.00144943
Iteration 10/25 | Loss: 0.00143111
Iteration 11/25 | Loss: 0.00142318
Iteration 12/25 | Loss: 0.00142118
Iteration 13/25 | Loss: 0.00142071
Iteration 14/25 | Loss: 0.00142061
Iteration 15/25 | Loss: 0.00142061
Iteration 16/25 | Loss: 0.00142061
Iteration 17/25 | Loss: 0.00142061
Iteration 18/25 | Loss: 0.00142061
Iteration 19/25 | Loss: 0.00142061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014206136111170053, 0.0014206136111170053, 0.0014206136111170053, 0.0014206136111170053, 0.0014206136111170053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014206136111170053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29018092
Iteration 2/25 | Loss: 0.00124727
Iteration 3/25 | Loss: 0.00124727
Iteration 4/25 | Loss: 0.00124727
Iteration 5/25 | Loss: 0.00124727
Iteration 6/25 | Loss: 0.00124727
Iteration 7/25 | Loss: 0.00124727
Iteration 8/25 | Loss: 0.00124727
Iteration 9/25 | Loss: 0.00124727
Iteration 10/25 | Loss: 0.00124727
Iteration 11/25 | Loss: 0.00124727
Iteration 12/25 | Loss: 0.00124727
Iteration 13/25 | Loss: 0.00124727
Iteration 14/25 | Loss: 0.00124727
Iteration 15/25 | Loss: 0.00124727
Iteration 16/25 | Loss: 0.00124727
Iteration 17/25 | Loss: 0.00124727
Iteration 18/25 | Loss: 0.00124727
Iteration 19/25 | Loss: 0.00124727
Iteration 20/25 | Loss: 0.00124727
Iteration 21/25 | Loss: 0.00124727
Iteration 22/25 | Loss: 0.00124727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012472668895497918, 0.0012472668895497918, 0.0012472668895497918, 0.0012472668895497918, 0.0012472668895497918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012472668895497918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124727
Iteration 2/1000 | Loss: 0.00004794
Iteration 3/1000 | Loss: 0.00003273
Iteration 4/1000 | Loss: 0.00002912
Iteration 5/1000 | Loss: 0.00002791
Iteration 6/1000 | Loss: 0.00002722
Iteration 7/1000 | Loss: 0.00002675
Iteration 8/1000 | Loss: 0.00002651
Iteration 9/1000 | Loss: 0.00002631
Iteration 10/1000 | Loss: 0.00002608
Iteration 11/1000 | Loss: 0.00002600
Iteration 12/1000 | Loss: 0.00002597
Iteration 13/1000 | Loss: 0.00002587
Iteration 14/1000 | Loss: 0.00002585
Iteration 15/1000 | Loss: 0.00002583
Iteration 16/1000 | Loss: 0.00002581
Iteration 17/1000 | Loss: 0.00002580
Iteration 18/1000 | Loss: 0.00002577
Iteration 19/1000 | Loss: 0.00002576
Iteration 20/1000 | Loss: 0.00002574
Iteration 21/1000 | Loss: 0.00002574
Iteration 22/1000 | Loss: 0.00002572
Iteration 23/1000 | Loss: 0.00002572
Iteration 24/1000 | Loss: 0.00002571
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002570
Iteration 27/1000 | Loss: 0.00002570
Iteration 28/1000 | Loss: 0.00002559
Iteration 29/1000 | Loss: 0.00002558
Iteration 30/1000 | Loss: 0.00002558
Iteration 31/1000 | Loss: 0.00002557
Iteration 32/1000 | Loss: 0.00002556
Iteration 33/1000 | Loss: 0.00002554
Iteration 34/1000 | Loss: 0.00002554
Iteration 35/1000 | Loss: 0.00002554
Iteration 36/1000 | Loss: 0.00002554
Iteration 37/1000 | Loss: 0.00002554
Iteration 38/1000 | Loss: 0.00002554
Iteration 39/1000 | Loss: 0.00002554
Iteration 40/1000 | Loss: 0.00002554
Iteration 41/1000 | Loss: 0.00002554
Iteration 42/1000 | Loss: 0.00002553
Iteration 43/1000 | Loss: 0.00002553
Iteration 44/1000 | Loss: 0.00002553
Iteration 45/1000 | Loss: 0.00002553
Iteration 46/1000 | Loss: 0.00002553
Iteration 47/1000 | Loss: 0.00002552
Iteration 48/1000 | Loss: 0.00002552
Iteration 49/1000 | Loss: 0.00002551
Iteration 50/1000 | Loss: 0.00002551
Iteration 51/1000 | Loss: 0.00002550
Iteration 52/1000 | Loss: 0.00002550
Iteration 53/1000 | Loss: 0.00002549
Iteration 54/1000 | Loss: 0.00002547
Iteration 55/1000 | Loss: 0.00002547
Iteration 56/1000 | Loss: 0.00002547
Iteration 57/1000 | Loss: 0.00002547
Iteration 58/1000 | Loss: 0.00002546
Iteration 59/1000 | Loss: 0.00002546
Iteration 60/1000 | Loss: 0.00002546
Iteration 61/1000 | Loss: 0.00002546
Iteration 62/1000 | Loss: 0.00002546
Iteration 63/1000 | Loss: 0.00002546
Iteration 64/1000 | Loss: 0.00002546
Iteration 65/1000 | Loss: 0.00002546
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002546
Iteration 68/1000 | Loss: 0.00002546
Iteration 69/1000 | Loss: 0.00002546
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002545
Iteration 72/1000 | Loss: 0.00002545
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002545
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002545
Iteration 77/1000 | Loss: 0.00002545
Iteration 78/1000 | Loss: 0.00002545
Iteration 79/1000 | Loss: 0.00002545
Iteration 80/1000 | Loss: 0.00002545
Iteration 81/1000 | Loss: 0.00002545
Iteration 82/1000 | Loss: 0.00002545
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002544
Iteration 85/1000 | Loss: 0.00002544
Iteration 86/1000 | Loss: 0.00002543
Iteration 87/1000 | Loss: 0.00002543
Iteration 88/1000 | Loss: 0.00002543
Iteration 89/1000 | Loss: 0.00002542
Iteration 90/1000 | Loss: 0.00002542
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00002541
Iteration 93/1000 | Loss: 0.00002541
Iteration 94/1000 | Loss: 0.00002541
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002541
Iteration 97/1000 | Loss: 0.00002541
Iteration 98/1000 | Loss: 0.00002540
Iteration 99/1000 | Loss: 0.00002540
Iteration 100/1000 | Loss: 0.00002540
Iteration 101/1000 | Loss: 0.00002540
Iteration 102/1000 | Loss: 0.00002540
Iteration 103/1000 | Loss: 0.00002540
Iteration 104/1000 | Loss: 0.00002540
Iteration 105/1000 | Loss: 0.00002540
Iteration 106/1000 | Loss: 0.00002540
Iteration 107/1000 | Loss: 0.00002540
Iteration 108/1000 | Loss: 0.00002540
Iteration 109/1000 | Loss: 0.00002540
Iteration 110/1000 | Loss: 0.00002540
Iteration 111/1000 | Loss: 0.00002540
Iteration 112/1000 | Loss: 0.00002540
Iteration 113/1000 | Loss: 0.00002540
Iteration 114/1000 | Loss: 0.00002540
Iteration 115/1000 | Loss: 0.00002540
Iteration 116/1000 | Loss: 0.00002540
Iteration 117/1000 | Loss: 0.00002540
Iteration 118/1000 | Loss: 0.00002540
Iteration 119/1000 | Loss: 0.00002540
Iteration 120/1000 | Loss: 0.00002540
Iteration 121/1000 | Loss: 0.00002540
Iteration 122/1000 | Loss: 0.00002540
Iteration 123/1000 | Loss: 0.00002540
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002540
Iteration 133/1000 | Loss: 0.00002540
Iteration 134/1000 | Loss: 0.00002540
Iteration 135/1000 | Loss: 0.00002540
Iteration 136/1000 | Loss: 0.00002540
Iteration 137/1000 | Loss: 0.00002540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.5396600904059596e-05, 2.5396600904059596e-05, 2.5396600904059596e-05, 2.5396600904059596e-05, 2.5396600904059596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5396600904059596e-05

Optimization complete. Final v2v error: 4.203411102294922 mm

Highest mean error: 4.726527690887451 mm for frame 112

Lowest mean error: 3.7863948345184326 mm for frame 0

Saving results

Total time: 54.67835712432861
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767199
Iteration 2/25 | Loss: 0.00164968
Iteration 3/25 | Loss: 0.00144135
Iteration 4/25 | Loss: 0.00143084
Iteration 5/25 | Loss: 0.00142925
Iteration 6/25 | Loss: 0.00142925
Iteration 7/25 | Loss: 0.00142925
Iteration 8/25 | Loss: 0.00142925
Iteration 9/25 | Loss: 0.00142925
Iteration 10/25 | Loss: 0.00142925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014292510459199548, 0.0014292510459199548, 0.0014292510459199548, 0.0014292510459199548, 0.0014292510459199548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014292510459199548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27945042
Iteration 2/25 | Loss: 0.00139542
Iteration 3/25 | Loss: 0.00139539
Iteration 4/25 | Loss: 0.00139539
Iteration 5/25 | Loss: 0.00139539
Iteration 6/25 | Loss: 0.00139539
Iteration 7/25 | Loss: 0.00139539
Iteration 8/25 | Loss: 0.00139539
Iteration 9/25 | Loss: 0.00139539
Iteration 10/25 | Loss: 0.00139539
Iteration 11/25 | Loss: 0.00139539
Iteration 12/25 | Loss: 0.00139539
Iteration 13/25 | Loss: 0.00139538
Iteration 14/25 | Loss: 0.00139538
Iteration 15/25 | Loss: 0.00139539
Iteration 16/25 | Loss: 0.00139538
Iteration 17/25 | Loss: 0.00139539
Iteration 18/25 | Loss: 0.00139539
Iteration 19/25 | Loss: 0.00139538
Iteration 20/25 | Loss: 0.00139538
Iteration 21/25 | Loss: 0.00139538
Iteration 22/25 | Loss: 0.00139538
Iteration 23/25 | Loss: 0.00139538
Iteration 24/25 | Loss: 0.00139538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013953846646472812, 0.0013953846646472812, 0.0013953846646472812, 0.0013953846646472812, 0.0013953846646472812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013953846646472812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139538
Iteration 2/1000 | Loss: 0.00004302
Iteration 3/1000 | Loss: 0.00003188
Iteration 4/1000 | Loss: 0.00002882
Iteration 5/1000 | Loss: 0.00002757
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002645
Iteration 8/1000 | Loss: 0.00002589
Iteration 9/1000 | Loss: 0.00002551
Iteration 10/1000 | Loss: 0.00002521
Iteration 11/1000 | Loss: 0.00002484
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002401
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002383
Iteration 17/1000 | Loss: 0.00002372
Iteration 18/1000 | Loss: 0.00002372
Iteration 19/1000 | Loss: 0.00002372
Iteration 20/1000 | Loss: 0.00002372
Iteration 21/1000 | Loss: 0.00002372
Iteration 22/1000 | Loss: 0.00002372
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002372
Iteration 25/1000 | Loss: 0.00002371
Iteration 26/1000 | Loss: 0.00002371
Iteration 27/1000 | Loss: 0.00002371
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002371
Iteration 30/1000 | Loss: 0.00002371
Iteration 31/1000 | Loss: 0.00002370
Iteration 32/1000 | Loss: 0.00002370
Iteration 33/1000 | Loss: 0.00002368
Iteration 34/1000 | Loss: 0.00002368
Iteration 35/1000 | Loss: 0.00002367
Iteration 36/1000 | Loss: 0.00002366
Iteration 37/1000 | Loss: 0.00002366
Iteration 38/1000 | Loss: 0.00002366
Iteration 39/1000 | Loss: 0.00002366
Iteration 40/1000 | Loss: 0.00002366
Iteration 41/1000 | Loss: 0.00002366
Iteration 42/1000 | Loss: 0.00002366
Iteration 43/1000 | Loss: 0.00002366
Iteration 44/1000 | Loss: 0.00002365
Iteration 45/1000 | Loss: 0.00002365
Iteration 46/1000 | Loss: 0.00002365
Iteration 47/1000 | Loss: 0.00002365
Iteration 48/1000 | Loss: 0.00002364
Iteration 49/1000 | Loss: 0.00002364
Iteration 50/1000 | Loss: 0.00002364
Iteration 51/1000 | Loss: 0.00002362
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002358
Iteration 54/1000 | Loss: 0.00002357
Iteration 55/1000 | Loss: 0.00002356
Iteration 56/1000 | Loss: 0.00002355
Iteration 57/1000 | Loss: 0.00002355
Iteration 58/1000 | Loss: 0.00002355
Iteration 59/1000 | Loss: 0.00002355
Iteration 60/1000 | Loss: 0.00002354
Iteration 61/1000 | Loss: 0.00002354
Iteration 62/1000 | Loss: 0.00002354
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002354
Iteration 66/1000 | Loss: 0.00002354
Iteration 67/1000 | Loss: 0.00002354
Iteration 68/1000 | Loss: 0.00002353
Iteration 69/1000 | Loss: 0.00002353
Iteration 70/1000 | Loss: 0.00002353
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002352
Iteration 73/1000 | Loss: 0.00002352
Iteration 74/1000 | Loss: 0.00002352
Iteration 75/1000 | Loss: 0.00002352
Iteration 76/1000 | Loss: 0.00002351
Iteration 77/1000 | Loss: 0.00002351
Iteration 78/1000 | Loss: 0.00002350
Iteration 79/1000 | Loss: 0.00002350
Iteration 80/1000 | Loss: 0.00002349
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002348
Iteration 84/1000 | Loss: 0.00002347
Iteration 85/1000 | Loss: 0.00002347
Iteration 86/1000 | Loss: 0.00002347
Iteration 87/1000 | Loss: 0.00002346
Iteration 88/1000 | Loss: 0.00002346
Iteration 89/1000 | Loss: 0.00002346
Iteration 90/1000 | Loss: 0.00002346
Iteration 91/1000 | Loss: 0.00002346
Iteration 92/1000 | Loss: 0.00002346
Iteration 93/1000 | Loss: 0.00002345
Iteration 94/1000 | Loss: 0.00002345
Iteration 95/1000 | Loss: 0.00002345
Iteration 96/1000 | Loss: 0.00002345
Iteration 97/1000 | Loss: 0.00002345
Iteration 98/1000 | Loss: 0.00002345
Iteration 99/1000 | Loss: 0.00002345
Iteration 100/1000 | Loss: 0.00002345
Iteration 101/1000 | Loss: 0.00002345
Iteration 102/1000 | Loss: 0.00002345
Iteration 103/1000 | Loss: 0.00002345
Iteration 104/1000 | Loss: 0.00002345
Iteration 105/1000 | Loss: 0.00002345
Iteration 106/1000 | Loss: 0.00002345
Iteration 107/1000 | Loss: 0.00002345
Iteration 108/1000 | Loss: 0.00002345
Iteration 109/1000 | Loss: 0.00002345
Iteration 110/1000 | Loss: 0.00002345
Iteration 111/1000 | Loss: 0.00002344
Iteration 112/1000 | Loss: 0.00002344
Iteration 113/1000 | Loss: 0.00002344
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002344
Iteration 116/1000 | Loss: 0.00002344
Iteration 117/1000 | Loss: 0.00002344
Iteration 118/1000 | Loss: 0.00002344
Iteration 119/1000 | Loss: 0.00002344
Iteration 120/1000 | Loss: 0.00002343
Iteration 121/1000 | Loss: 0.00002343
Iteration 122/1000 | Loss: 0.00002343
Iteration 123/1000 | Loss: 0.00002343
Iteration 124/1000 | Loss: 0.00002343
Iteration 125/1000 | Loss: 0.00002343
Iteration 126/1000 | Loss: 0.00002343
Iteration 127/1000 | Loss: 0.00002343
Iteration 128/1000 | Loss: 0.00002343
Iteration 129/1000 | Loss: 0.00002343
Iteration 130/1000 | Loss: 0.00002343
Iteration 131/1000 | Loss: 0.00002343
Iteration 132/1000 | Loss: 0.00002343
Iteration 133/1000 | Loss: 0.00002343
Iteration 134/1000 | Loss: 0.00002343
Iteration 135/1000 | Loss: 0.00002343
Iteration 136/1000 | Loss: 0.00002343
Iteration 137/1000 | Loss: 0.00002343
Iteration 138/1000 | Loss: 0.00002343
Iteration 139/1000 | Loss: 0.00002343
Iteration 140/1000 | Loss: 0.00002343
Iteration 141/1000 | Loss: 0.00002343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3433483875123784e-05, 2.3433483875123784e-05, 2.3433483875123784e-05, 2.3433483875123784e-05, 2.3433483875123784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3433483875123784e-05

Optimization complete. Final v2v error: 4.037881851196289 mm

Highest mean error: 4.274815559387207 mm for frame 77

Lowest mean error: 3.9012668132781982 mm for frame 3

Saving results

Total time: 37.63580060005188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437023
Iteration 2/25 | Loss: 0.00136533
Iteration 3/25 | Loss: 0.00128420
Iteration 4/25 | Loss: 0.00127147
Iteration 5/25 | Loss: 0.00126721
Iteration 6/25 | Loss: 0.00126668
Iteration 7/25 | Loss: 0.00126668
Iteration 8/25 | Loss: 0.00126668
Iteration 9/25 | Loss: 0.00126668
Iteration 10/25 | Loss: 0.00126668
Iteration 11/25 | Loss: 0.00126668
Iteration 12/25 | Loss: 0.00126668
Iteration 13/25 | Loss: 0.00126668
Iteration 14/25 | Loss: 0.00126668
Iteration 15/25 | Loss: 0.00126668
Iteration 16/25 | Loss: 0.00126668
Iteration 17/25 | Loss: 0.00126668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012666836846619844, 0.0012666836846619844, 0.0012666836846619844, 0.0012666836846619844, 0.0012666836846619844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012666836846619844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.53333902
Iteration 2/25 | Loss: 0.00147450
Iteration 3/25 | Loss: 0.00147449
Iteration 4/25 | Loss: 0.00147449
Iteration 5/25 | Loss: 0.00147448
Iteration 6/25 | Loss: 0.00147448
Iteration 7/25 | Loss: 0.00147448
Iteration 8/25 | Loss: 0.00147448
Iteration 9/25 | Loss: 0.00147448
Iteration 10/25 | Loss: 0.00147448
Iteration 11/25 | Loss: 0.00147448
Iteration 12/25 | Loss: 0.00147448
Iteration 13/25 | Loss: 0.00147448
Iteration 14/25 | Loss: 0.00147448
Iteration 15/25 | Loss: 0.00147448
Iteration 16/25 | Loss: 0.00147448
Iteration 17/25 | Loss: 0.00147448
Iteration 18/25 | Loss: 0.00147448
Iteration 19/25 | Loss: 0.00147448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001474482472985983, 0.001474482472985983, 0.001474482472985983, 0.001474482472985983, 0.001474482472985983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001474482472985983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147448
Iteration 2/1000 | Loss: 0.00002303
Iteration 3/1000 | Loss: 0.00001786
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001192
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001191
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001191
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001171
Iteration 123/1000 | Loss: 0.00001171
Iteration 124/1000 | Loss: 0.00001171
Iteration 125/1000 | Loss: 0.00001171
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00001171
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Iteration 143/1000 | Loss: 0.00001168
Iteration 144/1000 | Loss: 0.00001168
Iteration 145/1000 | Loss: 0.00001168
Iteration 146/1000 | Loss: 0.00001168
Iteration 147/1000 | Loss: 0.00001168
Iteration 148/1000 | Loss: 0.00001168
Iteration 149/1000 | Loss: 0.00001168
Iteration 150/1000 | Loss: 0.00001168
Iteration 151/1000 | Loss: 0.00001168
Iteration 152/1000 | Loss: 0.00001168
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001167
Iteration 156/1000 | Loss: 0.00001167
Iteration 157/1000 | Loss: 0.00001167
Iteration 158/1000 | Loss: 0.00001167
Iteration 159/1000 | Loss: 0.00001167
Iteration 160/1000 | Loss: 0.00001167
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001165
Iteration 174/1000 | Loss: 0.00001165
Iteration 175/1000 | Loss: 0.00001165
Iteration 176/1000 | Loss: 0.00001165
Iteration 177/1000 | Loss: 0.00001165
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001165
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001165
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001164
Iteration 190/1000 | Loss: 0.00001164
Iteration 191/1000 | Loss: 0.00001164
Iteration 192/1000 | Loss: 0.00001164
Iteration 193/1000 | Loss: 0.00001164
Iteration 194/1000 | Loss: 0.00001164
Iteration 195/1000 | Loss: 0.00001164
Iteration 196/1000 | Loss: 0.00001164
Iteration 197/1000 | Loss: 0.00001164
Iteration 198/1000 | Loss: 0.00001164
Iteration 199/1000 | Loss: 0.00001164
Iteration 200/1000 | Loss: 0.00001164
Iteration 201/1000 | Loss: 0.00001164
Iteration 202/1000 | Loss: 0.00001163
Iteration 203/1000 | Loss: 0.00001163
Iteration 204/1000 | Loss: 0.00001163
Iteration 205/1000 | Loss: 0.00001163
Iteration 206/1000 | Loss: 0.00001163
Iteration 207/1000 | Loss: 0.00001163
Iteration 208/1000 | Loss: 0.00001163
Iteration 209/1000 | Loss: 0.00001163
Iteration 210/1000 | Loss: 0.00001163
Iteration 211/1000 | Loss: 0.00001163
Iteration 212/1000 | Loss: 0.00001163
Iteration 213/1000 | Loss: 0.00001163
Iteration 214/1000 | Loss: 0.00001163
Iteration 215/1000 | Loss: 0.00001163
Iteration 216/1000 | Loss: 0.00001163
Iteration 217/1000 | Loss: 0.00001163
Iteration 218/1000 | Loss: 0.00001163
Iteration 219/1000 | Loss: 0.00001163
Iteration 220/1000 | Loss: 0.00001163
Iteration 221/1000 | Loss: 0.00001163
Iteration 222/1000 | Loss: 0.00001163
Iteration 223/1000 | Loss: 0.00001163
Iteration 224/1000 | Loss: 0.00001163
Iteration 225/1000 | Loss: 0.00001163
Iteration 226/1000 | Loss: 0.00001163
Iteration 227/1000 | Loss: 0.00001163
Iteration 228/1000 | Loss: 0.00001163
Iteration 229/1000 | Loss: 0.00001163
Iteration 230/1000 | Loss: 0.00001163
Iteration 231/1000 | Loss: 0.00001163
Iteration 232/1000 | Loss: 0.00001163
Iteration 233/1000 | Loss: 0.00001163
Iteration 234/1000 | Loss: 0.00001163
Iteration 235/1000 | Loss: 0.00001163
Iteration 236/1000 | Loss: 0.00001163
Iteration 237/1000 | Loss: 0.00001163
Iteration 238/1000 | Loss: 0.00001163
Iteration 239/1000 | Loss: 0.00001163
Iteration 240/1000 | Loss: 0.00001163
Iteration 241/1000 | Loss: 0.00001163
Iteration 242/1000 | Loss: 0.00001163
Iteration 243/1000 | Loss: 0.00001163
Iteration 244/1000 | Loss: 0.00001163
Iteration 245/1000 | Loss: 0.00001163
Iteration 246/1000 | Loss: 0.00001163
Iteration 247/1000 | Loss: 0.00001163
Iteration 248/1000 | Loss: 0.00001163
Iteration 249/1000 | Loss: 0.00001163
Iteration 250/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.1627451385720633e-05, 1.1627451385720633e-05, 1.1627451385720633e-05, 1.1627451385720633e-05, 1.1627451385720633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1627451385720633e-05

Optimization complete. Final v2v error: 2.944807529449463 mm

Highest mean error: 3.5679256916046143 mm for frame 92

Lowest mean error: 2.6658334732055664 mm for frame 204

Saving results

Total time: 51.7927725315094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006014
Iteration 2/25 | Loss: 0.00214002
Iteration 3/25 | Loss: 0.00157336
Iteration 4/25 | Loss: 0.00148697
Iteration 5/25 | Loss: 0.00148657
Iteration 6/25 | Loss: 0.00154471
Iteration 7/25 | Loss: 0.00146799
Iteration 8/25 | Loss: 0.00143953
Iteration 9/25 | Loss: 0.00135889
Iteration 10/25 | Loss: 0.00135005
Iteration 11/25 | Loss: 0.00133005
Iteration 12/25 | Loss: 0.00133214
Iteration 13/25 | Loss: 0.00132678
Iteration 14/25 | Loss: 0.00132860
Iteration 15/25 | Loss: 0.00133262
Iteration 16/25 | Loss: 0.00132586
Iteration 17/25 | Loss: 0.00132678
Iteration 18/25 | Loss: 0.00133448
Iteration 19/25 | Loss: 0.00132737
Iteration 20/25 | Loss: 0.00132476
Iteration 21/25 | Loss: 0.00132269
Iteration 22/25 | Loss: 0.00131493
Iteration 23/25 | Loss: 0.00131825
Iteration 24/25 | Loss: 0.00132071
Iteration 25/25 | Loss: 0.00131882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36718512
Iteration 2/25 | Loss: 0.00190739
Iteration 3/25 | Loss: 0.00190739
Iteration 4/25 | Loss: 0.00190738
Iteration 5/25 | Loss: 0.00190738
Iteration 6/25 | Loss: 0.00190738
Iteration 7/25 | Loss: 0.00190738
Iteration 8/25 | Loss: 0.00190738
Iteration 9/25 | Loss: 0.00190738
Iteration 10/25 | Loss: 0.00190738
Iteration 11/25 | Loss: 0.00190738
Iteration 12/25 | Loss: 0.00190738
Iteration 13/25 | Loss: 0.00190738
Iteration 14/25 | Loss: 0.00190738
Iteration 15/25 | Loss: 0.00190738
Iteration 16/25 | Loss: 0.00190738
Iteration 17/25 | Loss: 0.00190738
Iteration 18/25 | Loss: 0.00190738
Iteration 19/25 | Loss: 0.00190738
Iteration 20/25 | Loss: 0.00190738
Iteration 21/25 | Loss: 0.00190738
Iteration 22/25 | Loss: 0.00190738
Iteration 23/25 | Loss: 0.00190738
Iteration 24/25 | Loss: 0.00190738
Iteration 25/25 | Loss: 0.00190738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190738
Iteration 2/1000 | Loss: 0.00027651
Iteration 3/1000 | Loss: 0.00015490
Iteration 4/1000 | Loss: 0.00024847
Iteration 5/1000 | Loss: 0.00023708
Iteration 6/1000 | Loss: 0.00018054
Iteration 7/1000 | Loss: 0.00015918
Iteration 8/1000 | Loss: 0.00014688
Iteration 9/1000 | Loss: 0.00011275
Iteration 10/1000 | Loss: 0.00019616
Iteration 11/1000 | Loss: 0.00011963
Iteration 12/1000 | Loss: 0.00008681
Iteration 13/1000 | Loss: 0.00012379
Iteration 14/1000 | Loss: 0.00005792
Iteration 15/1000 | Loss: 0.00023275
Iteration 16/1000 | Loss: 0.00029763
Iteration 17/1000 | Loss: 0.00045651
Iteration 18/1000 | Loss: 0.00035767
Iteration 19/1000 | Loss: 0.00009446
Iteration 20/1000 | Loss: 0.00010344
Iteration 21/1000 | Loss: 0.00003316
Iteration 22/1000 | Loss: 0.00004190
Iteration 23/1000 | Loss: 0.00005654
Iteration 24/1000 | Loss: 0.00004777
Iteration 25/1000 | Loss: 0.00002704
Iteration 26/1000 | Loss: 0.00005763
Iteration 27/1000 | Loss: 0.00004683
Iteration 28/1000 | Loss: 0.00005431
Iteration 29/1000 | Loss: 0.00004955
Iteration 30/1000 | Loss: 0.00004775
Iteration 31/1000 | Loss: 0.00020391
Iteration 32/1000 | Loss: 0.00025768
Iteration 33/1000 | Loss: 0.00027124
Iteration 34/1000 | Loss: 0.00319357
Iteration 35/1000 | Loss: 0.00387441
Iteration 36/1000 | Loss: 0.00068494
Iteration 37/1000 | Loss: 0.00057825
Iteration 38/1000 | Loss: 0.00277478
Iteration 39/1000 | Loss: 0.00042183
Iteration 40/1000 | Loss: 0.00024986
Iteration 41/1000 | Loss: 0.00004733
Iteration 42/1000 | Loss: 0.00027199
Iteration 43/1000 | Loss: 0.00021891
Iteration 44/1000 | Loss: 0.00039694
Iteration 45/1000 | Loss: 0.00030209
Iteration 46/1000 | Loss: 0.00018011
Iteration 47/1000 | Loss: 0.00005589
Iteration 48/1000 | Loss: 0.00006075
Iteration 49/1000 | Loss: 0.00003339
Iteration 50/1000 | Loss: 0.00012090
Iteration 51/1000 | Loss: 0.00018644
Iteration 52/1000 | Loss: 0.00043228
Iteration 53/1000 | Loss: 0.00014310
Iteration 54/1000 | Loss: 0.00002706
Iteration 55/1000 | Loss: 0.00002210
Iteration 56/1000 | Loss: 0.00022687
Iteration 57/1000 | Loss: 0.00017686
Iteration 58/1000 | Loss: 0.00056480
Iteration 59/1000 | Loss: 0.00029833
Iteration 60/1000 | Loss: 0.00032073
Iteration 61/1000 | Loss: 0.00024200
Iteration 62/1000 | Loss: 0.00054339
Iteration 63/1000 | Loss: 0.00003113
Iteration 64/1000 | Loss: 0.00002255
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00004883
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00028732
Iteration 70/1000 | Loss: 0.00038795
Iteration 71/1000 | Loss: 0.00004841
Iteration 72/1000 | Loss: 0.00002969
Iteration 73/1000 | Loss: 0.00002136
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00003447
Iteration 78/1000 | Loss: 0.00007914
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001569
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001283
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001266
Iteration 90/1000 | Loss: 0.00001266
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001263
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001262
Iteration 101/1000 | Loss: 0.00001262
Iteration 102/1000 | Loss: 0.00001261
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001255
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001249
Iteration 138/1000 | Loss: 0.00001249
Iteration 139/1000 | Loss: 0.00001249
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00006809
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001247
Iteration 146/1000 | Loss: 0.00001247
Iteration 147/1000 | Loss: 0.00001247
Iteration 148/1000 | Loss: 0.00001247
Iteration 149/1000 | Loss: 0.00001247
Iteration 150/1000 | Loss: 0.00001247
Iteration 151/1000 | Loss: 0.00001247
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001246
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001244
Iteration 156/1000 | Loss: 0.00001244
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001244
Iteration 159/1000 | Loss: 0.00001244
Iteration 160/1000 | Loss: 0.00001244
Iteration 161/1000 | Loss: 0.00001244
Iteration 162/1000 | Loss: 0.00001244
Iteration 163/1000 | Loss: 0.00001244
Iteration 164/1000 | Loss: 0.00001244
Iteration 165/1000 | Loss: 0.00001243
Iteration 166/1000 | Loss: 0.00001243
Iteration 167/1000 | Loss: 0.00001243
Iteration 168/1000 | Loss: 0.00001243
Iteration 169/1000 | Loss: 0.00001243
Iteration 170/1000 | Loss: 0.00001243
Iteration 171/1000 | Loss: 0.00001243
Iteration 172/1000 | Loss: 0.00001243
Iteration 173/1000 | Loss: 0.00001242
Iteration 174/1000 | Loss: 0.00001242
Iteration 175/1000 | Loss: 0.00001242
Iteration 176/1000 | Loss: 0.00001242
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001240
Iteration 181/1000 | Loss: 0.00001240
Iteration 182/1000 | Loss: 0.00001240
Iteration 183/1000 | Loss: 0.00001240
Iteration 184/1000 | Loss: 0.00001240
Iteration 185/1000 | Loss: 0.00002886
Iteration 186/1000 | Loss: 0.00001245
Iteration 187/1000 | Loss: 0.00001242
Iteration 188/1000 | Loss: 0.00001242
Iteration 189/1000 | Loss: 0.00001242
Iteration 190/1000 | Loss: 0.00001241
Iteration 191/1000 | Loss: 0.00001241
Iteration 192/1000 | Loss: 0.00001241
Iteration 193/1000 | Loss: 0.00001240
Iteration 194/1000 | Loss: 0.00001240
Iteration 195/1000 | Loss: 0.00001240
Iteration 196/1000 | Loss: 0.00001239
Iteration 197/1000 | Loss: 0.00001239
Iteration 198/1000 | Loss: 0.00001239
Iteration 199/1000 | Loss: 0.00001239
Iteration 200/1000 | Loss: 0.00001239
Iteration 201/1000 | Loss: 0.00001239
Iteration 202/1000 | Loss: 0.00001238
Iteration 203/1000 | Loss: 0.00001238
Iteration 204/1000 | Loss: 0.00001238
Iteration 205/1000 | Loss: 0.00001238
Iteration 206/1000 | Loss: 0.00001238
Iteration 207/1000 | Loss: 0.00001238
Iteration 208/1000 | Loss: 0.00001238
Iteration 209/1000 | Loss: 0.00001237
Iteration 210/1000 | Loss: 0.00001237
Iteration 211/1000 | Loss: 0.00001237
Iteration 212/1000 | Loss: 0.00001237
Iteration 213/1000 | Loss: 0.00001237
Iteration 214/1000 | Loss: 0.00001237
Iteration 215/1000 | Loss: 0.00001237
Iteration 216/1000 | Loss: 0.00001237
Iteration 217/1000 | Loss: 0.00001237
Iteration 218/1000 | Loss: 0.00001237
Iteration 219/1000 | Loss: 0.00001237
Iteration 220/1000 | Loss: 0.00001237
Iteration 221/1000 | Loss: 0.00001237
Iteration 222/1000 | Loss: 0.00001237
Iteration 223/1000 | Loss: 0.00001237
Iteration 224/1000 | Loss: 0.00001237
Iteration 225/1000 | Loss: 0.00001237
Iteration 226/1000 | Loss: 0.00001237
Iteration 227/1000 | Loss: 0.00001237
Iteration 228/1000 | Loss: 0.00001237
Iteration 229/1000 | Loss: 0.00001237
Iteration 230/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.2371935554256197e-05, 1.2371935554256197e-05, 1.2371935554256197e-05, 1.2371935554256197e-05, 1.2371935554256197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2371935554256197e-05

Optimization complete. Final v2v error: 2.9671733379364014 mm

Highest mean error: 4.141814708709717 mm for frame 67

Lowest mean error: 2.552995204925537 mm for frame 133

Saving results

Total time: 185.55397820472717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810872
Iteration 2/25 | Loss: 0.00150055
Iteration 3/25 | Loss: 0.00132498
Iteration 4/25 | Loss: 0.00131984
Iteration 5/25 | Loss: 0.00131920
Iteration 6/25 | Loss: 0.00131920
Iteration 7/25 | Loss: 0.00131920
Iteration 8/25 | Loss: 0.00131920
Iteration 9/25 | Loss: 0.00131920
Iteration 10/25 | Loss: 0.00131920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013191967736929655, 0.0013191967736929655, 0.0013191967736929655, 0.0013191967736929655, 0.0013191967736929655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013191967736929655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93309152
Iteration 2/25 | Loss: 0.00083532
Iteration 3/25 | Loss: 0.00083531
Iteration 4/25 | Loss: 0.00083531
Iteration 5/25 | Loss: 0.00083531
Iteration 6/25 | Loss: 0.00083531
Iteration 7/25 | Loss: 0.00083531
Iteration 8/25 | Loss: 0.00083531
Iteration 9/25 | Loss: 0.00083531
Iteration 10/25 | Loss: 0.00083530
Iteration 11/25 | Loss: 0.00083530
Iteration 12/25 | Loss: 0.00083530
Iteration 13/25 | Loss: 0.00083530
Iteration 14/25 | Loss: 0.00083530
Iteration 15/25 | Loss: 0.00083530
Iteration 16/25 | Loss: 0.00083530
Iteration 17/25 | Loss: 0.00083530
Iteration 18/25 | Loss: 0.00083530
Iteration 19/25 | Loss: 0.00083530
Iteration 20/25 | Loss: 0.00083530
Iteration 21/25 | Loss: 0.00083530
Iteration 22/25 | Loss: 0.00083530
Iteration 23/25 | Loss: 0.00083530
Iteration 24/25 | Loss: 0.00083530
Iteration 25/25 | Loss: 0.00083530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083530
Iteration 2/1000 | Loss: 0.00003349
Iteration 3/1000 | Loss: 0.00002625
Iteration 4/1000 | Loss: 0.00002372
Iteration 5/1000 | Loss: 0.00002265
Iteration 6/1000 | Loss: 0.00002191
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002086
Iteration 9/1000 | Loss: 0.00002060
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00002024
Iteration 13/1000 | Loss: 0.00002023
Iteration 14/1000 | Loss: 0.00002021
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001996
Iteration 19/1000 | Loss: 0.00001996
Iteration 20/1000 | Loss: 0.00001995
Iteration 21/1000 | Loss: 0.00001989
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001985
Iteration 25/1000 | Loss: 0.00001984
Iteration 26/1000 | Loss: 0.00001981
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001976
Iteration 32/1000 | Loss: 0.00001975
Iteration 33/1000 | Loss: 0.00001975
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001974
Iteration 37/1000 | Loss: 0.00001973
Iteration 38/1000 | Loss: 0.00001973
Iteration 39/1000 | Loss: 0.00001973
Iteration 40/1000 | Loss: 0.00001973
Iteration 41/1000 | Loss: 0.00001973
Iteration 42/1000 | Loss: 0.00001973
Iteration 43/1000 | Loss: 0.00001973
Iteration 44/1000 | Loss: 0.00001972
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001972
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001972
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001963
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001961
Iteration 56/1000 | Loss: 0.00001961
Iteration 57/1000 | Loss: 0.00001961
Iteration 58/1000 | Loss: 0.00001961
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001960
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00001951
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001950
Iteration 76/1000 | Loss: 0.00001950
Iteration 77/1000 | Loss: 0.00001950
Iteration 78/1000 | Loss: 0.00001950
Iteration 79/1000 | Loss: 0.00001950
Iteration 80/1000 | Loss: 0.00001950
Iteration 81/1000 | Loss: 0.00001950
Iteration 82/1000 | Loss: 0.00001950
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001949
Iteration 87/1000 | Loss: 0.00001949
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001948
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001947
Iteration 99/1000 | Loss: 0.00001947
Iteration 100/1000 | Loss: 0.00001947
Iteration 101/1000 | Loss: 0.00001947
Iteration 102/1000 | Loss: 0.00001947
Iteration 103/1000 | Loss: 0.00001946
Iteration 104/1000 | Loss: 0.00001946
Iteration 105/1000 | Loss: 0.00001946
Iteration 106/1000 | Loss: 0.00001946
Iteration 107/1000 | Loss: 0.00001946
Iteration 108/1000 | Loss: 0.00001946
Iteration 109/1000 | Loss: 0.00001946
Iteration 110/1000 | Loss: 0.00001945
Iteration 111/1000 | Loss: 0.00001945
Iteration 112/1000 | Loss: 0.00001945
Iteration 113/1000 | Loss: 0.00001945
Iteration 114/1000 | Loss: 0.00001945
Iteration 115/1000 | Loss: 0.00001945
Iteration 116/1000 | Loss: 0.00001945
Iteration 117/1000 | Loss: 0.00001945
Iteration 118/1000 | Loss: 0.00001945
Iteration 119/1000 | Loss: 0.00001945
Iteration 120/1000 | Loss: 0.00001945
Iteration 121/1000 | Loss: 0.00001945
Iteration 122/1000 | Loss: 0.00001945
Iteration 123/1000 | Loss: 0.00001945
Iteration 124/1000 | Loss: 0.00001945
Iteration 125/1000 | Loss: 0.00001945
Iteration 126/1000 | Loss: 0.00001945
Iteration 127/1000 | Loss: 0.00001945
Iteration 128/1000 | Loss: 0.00001945
Iteration 129/1000 | Loss: 0.00001945
Iteration 130/1000 | Loss: 0.00001945
Iteration 131/1000 | Loss: 0.00001945
Iteration 132/1000 | Loss: 0.00001945
Iteration 133/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.945348958543036e-05, 1.945348958543036e-05, 1.945348958543036e-05, 1.945348958543036e-05, 1.945348958543036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.945348958543036e-05

Optimization complete. Final v2v error: 3.6726059913635254 mm

Highest mean error: 3.7596874237060547 mm for frame 106

Lowest mean error: 3.5769686698913574 mm for frame 36

Saving results

Total time: 35.549463748931885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001618
Iteration 2/25 | Loss: 0.01001618
Iteration 3/25 | Loss: 0.00364541
Iteration 4/25 | Loss: 0.00204572
Iteration 5/25 | Loss: 0.00175596
Iteration 6/25 | Loss: 0.00165006
Iteration 7/25 | Loss: 0.00156636
Iteration 8/25 | Loss: 0.00161622
Iteration 9/25 | Loss: 0.00160090
Iteration 10/25 | Loss: 0.00148912
Iteration 11/25 | Loss: 0.00142733
Iteration 12/25 | Loss: 0.00139459
Iteration 13/25 | Loss: 0.00139018
Iteration 14/25 | Loss: 0.00139883
Iteration 15/25 | Loss: 0.00139831
Iteration 16/25 | Loss: 0.00138586
Iteration 17/25 | Loss: 0.00138118
Iteration 18/25 | Loss: 0.00138530
Iteration 19/25 | Loss: 0.00138675
Iteration 20/25 | Loss: 0.00138119
Iteration 21/25 | Loss: 0.00137716
Iteration 22/25 | Loss: 0.00137714
Iteration 23/25 | Loss: 0.00137372
Iteration 24/25 | Loss: 0.00137280
Iteration 25/25 | Loss: 0.00136989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31731176
Iteration 2/25 | Loss: 0.00162818
Iteration 3/25 | Loss: 0.00162818
Iteration 4/25 | Loss: 0.00162818
Iteration 5/25 | Loss: 0.00162716
Iteration 6/25 | Loss: 0.00162716
Iteration 7/25 | Loss: 0.00162716
Iteration 8/25 | Loss: 0.00162715
Iteration 9/25 | Loss: 0.00162715
Iteration 10/25 | Loss: 0.00162715
Iteration 11/25 | Loss: 0.00162715
Iteration 12/25 | Loss: 0.00162715
Iteration 13/25 | Loss: 0.00162715
Iteration 14/25 | Loss: 0.00162715
Iteration 15/25 | Loss: 0.00162715
Iteration 16/25 | Loss: 0.00162715
Iteration 17/25 | Loss: 0.00162715
Iteration 18/25 | Loss: 0.00162715
Iteration 19/25 | Loss: 0.00162715
Iteration 20/25 | Loss: 0.00162715
Iteration 21/25 | Loss: 0.00162715
Iteration 22/25 | Loss: 0.00162715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016271532513201237, 0.0016271532513201237, 0.0016271532513201237, 0.0016271532513201237, 0.0016271532513201237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016271532513201237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162715
Iteration 2/1000 | Loss: 0.00010347
Iteration 3/1000 | Loss: 0.00005834
Iteration 4/1000 | Loss: 0.00020512
Iteration 5/1000 | Loss: 0.00010366
Iteration 6/1000 | Loss: 0.00008785
Iteration 7/1000 | Loss: 0.00006416
Iteration 8/1000 | Loss: 0.00006382
Iteration 9/1000 | Loss: 0.00005092
Iteration 10/1000 | Loss: 0.00005695
Iteration 11/1000 | Loss: 0.00005702
Iteration 12/1000 | Loss: 0.00006612
Iteration 13/1000 | Loss: 0.00004740
Iteration 14/1000 | Loss: 0.00004341
Iteration 15/1000 | Loss: 0.00004687
Iteration 16/1000 | Loss: 0.00004743
Iteration 17/1000 | Loss: 0.00006256
Iteration 18/1000 | Loss: 0.00005258
Iteration 19/1000 | Loss: 0.00005305
Iteration 20/1000 | Loss: 0.00004842
Iteration 21/1000 | Loss: 0.00004700
Iteration 22/1000 | Loss: 0.00004576
Iteration 23/1000 | Loss: 0.00006138
Iteration 24/1000 | Loss: 0.00004891
Iteration 25/1000 | Loss: 0.00006350
Iteration 26/1000 | Loss: 0.00006067
Iteration 27/1000 | Loss: 0.00005137
Iteration 28/1000 | Loss: 0.00005074
Iteration 29/1000 | Loss: 0.00005178
Iteration 30/1000 | Loss: 0.00004983
Iteration 31/1000 | Loss: 0.00004901
Iteration 32/1000 | Loss: 0.00004504
Iteration 33/1000 | Loss: 0.00003782
Iteration 34/1000 | Loss: 0.00004140
Iteration 35/1000 | Loss: 0.00003287
Iteration 36/1000 | Loss: 0.00002725
Iteration 37/1000 | Loss: 0.00002981
Iteration 38/1000 | Loss: 0.00003603
Iteration 39/1000 | Loss: 0.00003311
Iteration 40/1000 | Loss: 0.00003336
Iteration 41/1000 | Loss: 0.00003422
Iteration 42/1000 | Loss: 0.00011814
Iteration 43/1000 | Loss: 0.00004175
Iteration 44/1000 | Loss: 0.00003587
Iteration 45/1000 | Loss: 0.00003625
Iteration 46/1000 | Loss: 0.00003222
Iteration 47/1000 | Loss: 0.00003224
Iteration 48/1000 | Loss: 0.00002628
Iteration 49/1000 | Loss: 0.00002788
Iteration 50/1000 | Loss: 0.00003140
Iteration 51/1000 | Loss: 0.00002374
Iteration 52/1000 | Loss: 0.00003145
Iteration 53/1000 | Loss: 0.00004174
Iteration 54/1000 | Loss: 0.00002217
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00002103
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002068
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002053
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002051
Iteration 65/1000 | Loss: 0.00002050
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00002047
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002044
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002043
Iteration 75/1000 | Loss: 0.00002041
Iteration 76/1000 | Loss: 0.00002040
Iteration 77/1000 | Loss: 0.00002040
Iteration 78/1000 | Loss: 0.00002039
Iteration 79/1000 | Loss: 0.00002039
Iteration 80/1000 | Loss: 0.00002038
Iteration 81/1000 | Loss: 0.00002038
Iteration 82/1000 | Loss: 0.00002038
Iteration 83/1000 | Loss: 0.00002038
Iteration 84/1000 | Loss: 0.00002038
Iteration 85/1000 | Loss: 0.00002038
Iteration 86/1000 | Loss: 0.00002038
Iteration 87/1000 | Loss: 0.00002038
Iteration 88/1000 | Loss: 0.00002037
Iteration 89/1000 | Loss: 0.00002037
Iteration 90/1000 | Loss: 0.00002037
Iteration 91/1000 | Loss: 0.00002036
Iteration 92/1000 | Loss: 0.00002036
Iteration 93/1000 | Loss: 0.00002036
Iteration 94/1000 | Loss: 0.00002036
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002035
Iteration 99/1000 | Loss: 0.00002035
Iteration 100/1000 | Loss: 0.00002034
Iteration 101/1000 | Loss: 0.00002034
Iteration 102/1000 | Loss: 0.00002034
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002032
Iteration 109/1000 | Loss: 0.00002032
Iteration 110/1000 | Loss: 0.00002031
Iteration 111/1000 | Loss: 0.00002031
Iteration 112/1000 | Loss: 0.00002031
Iteration 113/1000 | Loss: 0.00002031
Iteration 114/1000 | Loss: 0.00002030
Iteration 115/1000 | Loss: 0.00002030
Iteration 116/1000 | Loss: 0.00002030
Iteration 117/1000 | Loss: 0.00002030
Iteration 118/1000 | Loss: 0.00002030
Iteration 119/1000 | Loss: 0.00002030
Iteration 120/1000 | Loss: 0.00002030
Iteration 121/1000 | Loss: 0.00002030
Iteration 122/1000 | Loss: 0.00002029
Iteration 123/1000 | Loss: 0.00002029
Iteration 124/1000 | Loss: 0.00002029
Iteration 125/1000 | Loss: 0.00002029
Iteration 126/1000 | Loss: 0.00002029
Iteration 127/1000 | Loss: 0.00002028
Iteration 128/1000 | Loss: 0.00002028
Iteration 129/1000 | Loss: 0.00002028
Iteration 130/1000 | Loss: 0.00002028
Iteration 131/1000 | Loss: 0.00002027
Iteration 132/1000 | Loss: 0.00002027
Iteration 133/1000 | Loss: 0.00002027
Iteration 134/1000 | Loss: 0.00002027
Iteration 135/1000 | Loss: 0.00002027
Iteration 136/1000 | Loss: 0.00002026
Iteration 137/1000 | Loss: 0.00002026
Iteration 138/1000 | Loss: 0.00002026
Iteration 139/1000 | Loss: 0.00002026
Iteration 140/1000 | Loss: 0.00002026
Iteration 141/1000 | Loss: 0.00002025
Iteration 142/1000 | Loss: 0.00002025
Iteration 143/1000 | Loss: 0.00002025
Iteration 144/1000 | Loss: 0.00002024
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Iteration 148/1000 | Loss: 0.00002023
Iteration 149/1000 | Loss: 0.00002023
Iteration 150/1000 | Loss: 0.00002023
Iteration 151/1000 | Loss: 0.00002023
Iteration 152/1000 | Loss: 0.00002023
Iteration 153/1000 | Loss: 0.00002022
Iteration 154/1000 | Loss: 0.00002022
Iteration 155/1000 | Loss: 0.00002022
Iteration 156/1000 | Loss: 0.00002022
Iteration 157/1000 | Loss: 0.00002022
Iteration 158/1000 | Loss: 0.00002021
Iteration 159/1000 | Loss: 0.00002021
Iteration 160/1000 | Loss: 0.00002021
Iteration 161/1000 | Loss: 0.00002021
Iteration 162/1000 | Loss: 0.00002021
Iteration 163/1000 | Loss: 0.00002021
Iteration 164/1000 | Loss: 0.00002021
Iteration 165/1000 | Loss: 0.00002020
Iteration 166/1000 | Loss: 0.00002020
Iteration 167/1000 | Loss: 0.00002020
Iteration 168/1000 | Loss: 0.00002020
Iteration 169/1000 | Loss: 0.00002020
Iteration 170/1000 | Loss: 0.00002020
Iteration 171/1000 | Loss: 0.00002020
Iteration 172/1000 | Loss: 0.00002020
Iteration 173/1000 | Loss: 0.00002020
Iteration 174/1000 | Loss: 0.00002020
Iteration 175/1000 | Loss: 0.00002020
Iteration 176/1000 | Loss: 0.00002020
Iteration 177/1000 | Loss: 0.00002020
Iteration 178/1000 | Loss: 0.00002020
Iteration 179/1000 | Loss: 0.00002020
Iteration 180/1000 | Loss: 0.00002020
Iteration 181/1000 | Loss: 0.00002020
Iteration 182/1000 | Loss: 0.00002019
Iteration 183/1000 | Loss: 0.00002019
Iteration 184/1000 | Loss: 0.00002019
Iteration 185/1000 | Loss: 0.00002019
Iteration 186/1000 | Loss: 0.00002019
Iteration 187/1000 | Loss: 0.00002019
Iteration 188/1000 | Loss: 0.00002019
Iteration 189/1000 | Loss: 0.00002019
Iteration 190/1000 | Loss: 0.00002019
Iteration 191/1000 | Loss: 0.00002019
Iteration 192/1000 | Loss: 0.00002019
Iteration 193/1000 | Loss: 0.00002019
Iteration 194/1000 | Loss: 0.00002019
Iteration 195/1000 | Loss: 0.00002019
Iteration 196/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.0191180738038383e-05, 2.0191180738038383e-05, 2.0191180738038383e-05, 2.0191180738038383e-05, 2.0191180738038383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0191180738038383e-05

Optimization complete. Final v2v error: 3.821037530899048 mm

Highest mean error: 4.629557132720947 mm for frame 210

Lowest mean error: 3.294999361038208 mm for frame 139

Saving results

Total time: 155.8704059123993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038169
Iteration 2/25 | Loss: 0.01038169
Iteration 3/25 | Loss: 0.01038169
Iteration 4/25 | Loss: 0.01038169
Iteration 5/25 | Loss: 0.01038169
Iteration 6/25 | Loss: 0.00236415
Iteration 7/25 | Loss: 0.00205758
Iteration 8/25 | Loss: 0.00201854
Iteration 9/25 | Loss: 0.00152998
Iteration 10/25 | Loss: 0.00142181
Iteration 11/25 | Loss: 0.00134289
Iteration 12/25 | Loss: 0.00129108
Iteration 13/25 | Loss: 0.00128055
Iteration 14/25 | Loss: 0.00127766
Iteration 15/25 | Loss: 0.00127731
Iteration 16/25 | Loss: 0.00127731
Iteration 17/25 | Loss: 0.00127731
Iteration 18/25 | Loss: 0.00127731
Iteration 19/25 | Loss: 0.00127731
Iteration 20/25 | Loss: 0.00127731
Iteration 21/25 | Loss: 0.00127731
Iteration 22/25 | Loss: 0.00127731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00127731217071414, 0.00127731217071414, 0.00127731217071414, 0.00127731217071414, 0.00127731217071414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127731217071414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27368605
Iteration 2/25 | Loss: 0.00155750
Iteration 3/25 | Loss: 0.00155750
Iteration 4/25 | Loss: 0.00155750
Iteration 5/25 | Loss: 0.00155750
Iteration 6/25 | Loss: 0.00155750
Iteration 7/25 | Loss: 0.00155749
Iteration 8/25 | Loss: 0.00155749
Iteration 9/25 | Loss: 0.00155749
Iteration 10/25 | Loss: 0.00155749
Iteration 11/25 | Loss: 0.00155749
Iteration 12/25 | Loss: 0.00155749
Iteration 13/25 | Loss: 0.00155749
Iteration 14/25 | Loss: 0.00155749
Iteration 15/25 | Loss: 0.00155749
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015574941644445062, 0.0015574941644445062, 0.0015574941644445062, 0.0015574941644445062, 0.0015574941644445062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015574941644445062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155749
Iteration 2/1000 | Loss: 0.00003207
Iteration 3/1000 | Loss: 0.00002502
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00001982
Iteration 8/1000 | Loss: 0.00001929
Iteration 9/1000 | Loss: 0.00001882
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00097506
Iteration 12/1000 | Loss: 0.00002126
Iteration 13/1000 | Loss: 0.00001902
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001340
Iteration 24/1000 | Loss: 0.00001340
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001305
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001304
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001304
Iteration 63/1000 | Loss: 0.00001304
Iteration 64/1000 | Loss: 0.00001304
Iteration 65/1000 | Loss: 0.00001304
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.3040880730841309e-05, 1.3040880730841309e-05, 1.3040880730841309e-05, 1.3040880730841309e-05, 1.3040880730841309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3040880730841309e-05

Optimization complete. Final v2v error: 3.1108925342559814 mm

Highest mean error: 4.096911430358887 mm for frame 51

Lowest mean error: 2.9922406673431396 mm for frame 84

Saving results

Total time: 52.704299211502075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382400
Iteration 2/25 | Loss: 0.00142222
Iteration 3/25 | Loss: 0.00129713
Iteration 4/25 | Loss: 0.00128447
Iteration 5/25 | Loss: 0.00128114
Iteration 6/25 | Loss: 0.00128114
Iteration 7/25 | Loss: 0.00128114
Iteration 8/25 | Loss: 0.00128114
Iteration 9/25 | Loss: 0.00128114
Iteration 10/25 | Loss: 0.00128114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012811380438506603, 0.0012811380438506603, 0.0012811380438506603, 0.0012811380438506603, 0.0012811380438506603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012811380438506603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26837289
Iteration 2/25 | Loss: 0.00165031
Iteration 3/25 | Loss: 0.00165030
Iteration 4/25 | Loss: 0.00165030
Iteration 5/25 | Loss: 0.00165030
Iteration 6/25 | Loss: 0.00165030
Iteration 7/25 | Loss: 0.00165030
Iteration 8/25 | Loss: 0.00165030
Iteration 9/25 | Loss: 0.00165030
Iteration 10/25 | Loss: 0.00165030
Iteration 11/25 | Loss: 0.00165030
Iteration 12/25 | Loss: 0.00165030
Iteration 13/25 | Loss: 0.00165030
Iteration 14/25 | Loss: 0.00165030
Iteration 15/25 | Loss: 0.00165030
Iteration 16/25 | Loss: 0.00165030
Iteration 17/25 | Loss: 0.00165030
Iteration 18/25 | Loss: 0.00165030
Iteration 19/25 | Loss: 0.00165030
Iteration 20/25 | Loss: 0.00165030
Iteration 21/25 | Loss: 0.00165030
Iteration 22/25 | Loss: 0.00165030
Iteration 23/25 | Loss: 0.00165030
Iteration 24/25 | Loss: 0.00165030
Iteration 25/25 | Loss: 0.00165030

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165030
Iteration 2/1000 | Loss: 0.00003772
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002138
Iteration 5/1000 | Loss: 0.00001983
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001657
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001536
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001523
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001501
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001495
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001490
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001488
Iteration 46/1000 | Loss: 0.00001488
Iteration 47/1000 | Loss: 0.00001488
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001487
Iteration 54/1000 | Loss: 0.00001487
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001485
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001484
Iteration 62/1000 | Loss: 0.00001484
Iteration 63/1000 | Loss: 0.00001483
Iteration 64/1000 | Loss: 0.00001483
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001473
Iteration 91/1000 | Loss: 0.00001473
Iteration 92/1000 | Loss: 0.00001473
Iteration 93/1000 | Loss: 0.00001473
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00001466
Iteration 112/1000 | Loss: 0.00001466
Iteration 113/1000 | Loss: 0.00001466
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.4618599379900843e-05, 1.4618599379900843e-05, 1.4618599379900843e-05, 1.4618599379900843e-05, 1.4618599379900843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4618599379900843e-05

Optimization complete. Final v2v error: 3.2265586853027344 mm

Highest mean error: 3.7239184379577637 mm for frame 196

Lowest mean error: 2.7979488372802734 mm for frame 132

Saving results

Total time: 45.42357635498047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030479
Iteration 2/25 | Loss: 0.01030479
Iteration 3/25 | Loss: 0.01030479
Iteration 4/25 | Loss: 0.01030479
Iteration 5/25 | Loss: 0.01030478
Iteration 6/25 | Loss: 0.00335550
Iteration 7/25 | Loss: 0.00208875
Iteration 8/25 | Loss: 0.00209386
Iteration 9/25 | Loss: 0.00200973
Iteration 10/25 | Loss: 0.00193617
Iteration 11/25 | Loss: 0.00186880
Iteration 12/25 | Loss: 0.00179431
Iteration 13/25 | Loss: 0.00168843
Iteration 14/25 | Loss: 0.00168123
Iteration 15/25 | Loss: 0.00164277
Iteration 16/25 | Loss: 0.00160823
Iteration 17/25 | Loss: 0.00160694
Iteration 18/25 | Loss: 0.00159900
Iteration 19/25 | Loss: 0.00159272
Iteration 20/25 | Loss: 0.00158729
Iteration 21/25 | Loss: 0.00158645
Iteration 22/25 | Loss: 0.00158335
Iteration 23/25 | Loss: 0.00158081
Iteration 24/25 | Loss: 0.00157948
Iteration 25/25 | Loss: 0.00157904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27153695
Iteration 2/25 | Loss: 0.00410501
Iteration 3/25 | Loss: 0.00390323
Iteration 4/25 | Loss: 0.00390311
Iteration 5/25 | Loss: 0.00390311
Iteration 6/25 | Loss: 0.00390311
Iteration 7/25 | Loss: 0.00390311
Iteration 8/25 | Loss: 0.00390311
Iteration 9/25 | Loss: 0.00390311
Iteration 10/25 | Loss: 0.00390311
Iteration 11/25 | Loss: 0.00390311
Iteration 12/25 | Loss: 0.00390311
Iteration 13/25 | Loss: 0.00390311
Iteration 14/25 | Loss: 0.00390311
Iteration 15/25 | Loss: 0.00390311
Iteration 16/25 | Loss: 0.00390310
Iteration 17/25 | Loss: 0.00390311
Iteration 18/25 | Loss: 0.00390311
Iteration 19/25 | Loss: 0.00390311
Iteration 20/25 | Loss: 0.00390310
Iteration 21/25 | Loss: 0.00390310
Iteration 22/25 | Loss: 0.00390310
Iteration 23/25 | Loss: 0.00390310
Iteration 24/25 | Loss: 0.00390310
Iteration 25/25 | Loss: 0.00390310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0039031049236655235, 0.0039031049236655235, 0.0039031049236655235, 0.0039031049236655235, 0.0039031049236655235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0039031049236655235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390311
Iteration 2/1000 | Loss: 0.00087184
Iteration 3/1000 | Loss: 0.00076674
Iteration 4/1000 | Loss: 0.00196569
Iteration 5/1000 | Loss: 0.00235642
Iteration 6/1000 | Loss: 0.00224737
Iteration 7/1000 | Loss: 0.00077581
Iteration 8/1000 | Loss: 0.00045670
Iteration 9/1000 | Loss: 0.00204990
Iteration 10/1000 | Loss: 0.00202892
Iteration 11/1000 | Loss: 0.00256881
Iteration 12/1000 | Loss: 0.00067495
Iteration 13/1000 | Loss: 0.00253093
Iteration 14/1000 | Loss: 0.00040180
Iteration 15/1000 | Loss: 0.00224172
Iteration 16/1000 | Loss: 0.00292274
Iteration 17/1000 | Loss: 0.00038237
Iteration 18/1000 | Loss: 0.00136136
Iteration 19/1000 | Loss: 0.00255359
Iteration 20/1000 | Loss: 0.00045180
Iteration 21/1000 | Loss: 0.00125436
Iteration 22/1000 | Loss: 0.00111543
Iteration 23/1000 | Loss: 0.00031262
Iteration 24/1000 | Loss: 0.00021040
Iteration 25/1000 | Loss: 0.00133125
Iteration 26/1000 | Loss: 0.00036149
Iteration 27/1000 | Loss: 0.00018726
Iteration 28/1000 | Loss: 0.00027994
Iteration 29/1000 | Loss: 0.00332012
Iteration 30/1000 | Loss: 0.00100240
Iteration 31/1000 | Loss: 0.00126058
Iteration 32/1000 | Loss: 0.00282817
Iteration 33/1000 | Loss: 0.00117591
Iteration 34/1000 | Loss: 0.00178404
Iteration 35/1000 | Loss: 0.00707729
Iteration 36/1000 | Loss: 0.00526649
Iteration 37/1000 | Loss: 0.00571619
Iteration 38/1000 | Loss: 0.01355779
Iteration 39/1000 | Loss: 0.00473770
Iteration 40/1000 | Loss: 0.00366141
Iteration 41/1000 | Loss: 0.00198754
Iteration 42/1000 | Loss: 0.00154078
Iteration 43/1000 | Loss: 0.00166369
Iteration 44/1000 | Loss: 0.00077497
Iteration 45/1000 | Loss: 0.00185966
Iteration 46/1000 | Loss: 0.00106200
Iteration 47/1000 | Loss: 0.00097763
Iteration 48/1000 | Loss: 0.00096923
Iteration 49/1000 | Loss: 0.00160591
Iteration 50/1000 | Loss: 0.00180175
Iteration 51/1000 | Loss: 0.00147834
Iteration 52/1000 | Loss: 0.00050975
Iteration 53/1000 | Loss: 0.00067629
Iteration 54/1000 | Loss: 0.00027224
Iteration 55/1000 | Loss: 0.00021089
Iteration 56/1000 | Loss: 0.00010213
Iteration 57/1000 | Loss: 0.00016912
Iteration 58/1000 | Loss: 0.00056457
Iteration 59/1000 | Loss: 0.00036953
Iteration 60/1000 | Loss: 0.00056150
Iteration 61/1000 | Loss: 0.00056802
Iteration 62/1000 | Loss: 0.00015879
Iteration 63/1000 | Loss: 0.00117565
Iteration 64/1000 | Loss: 0.00057626
Iteration 65/1000 | Loss: 0.00011165
Iteration 66/1000 | Loss: 0.00051879
Iteration 67/1000 | Loss: 0.00023517
Iteration 68/1000 | Loss: 0.00008156
Iteration 69/1000 | Loss: 0.00025290
Iteration 70/1000 | Loss: 0.00052611
Iteration 71/1000 | Loss: 0.00032801
Iteration 72/1000 | Loss: 0.00017085
Iteration 73/1000 | Loss: 0.00023577
Iteration 74/1000 | Loss: 0.00056440
Iteration 75/1000 | Loss: 0.00068630
Iteration 76/1000 | Loss: 0.00015687
Iteration 77/1000 | Loss: 0.00024046
Iteration 78/1000 | Loss: 0.00044682
Iteration 79/1000 | Loss: 0.00047522
Iteration 80/1000 | Loss: 0.00053844
Iteration 81/1000 | Loss: 0.00036167
Iteration 82/1000 | Loss: 0.00051607
Iteration 83/1000 | Loss: 0.00031542
Iteration 84/1000 | Loss: 0.00031726
Iteration 85/1000 | Loss: 0.00006951
Iteration 86/1000 | Loss: 0.00019018
Iteration 87/1000 | Loss: 0.00014832
Iteration 88/1000 | Loss: 0.00009358
Iteration 89/1000 | Loss: 0.00015286
Iteration 90/1000 | Loss: 0.00025000
Iteration 91/1000 | Loss: 0.00022723
Iteration 92/1000 | Loss: 0.00026138
Iteration 93/1000 | Loss: 0.00022929
Iteration 94/1000 | Loss: 0.00089593
Iteration 95/1000 | Loss: 0.00019002
Iteration 96/1000 | Loss: 0.00006517
Iteration 97/1000 | Loss: 0.00014652
Iteration 98/1000 | Loss: 0.00020829
Iteration 99/1000 | Loss: 0.00016517
Iteration 100/1000 | Loss: 0.00020674
Iteration 101/1000 | Loss: 0.00018353
Iteration 102/1000 | Loss: 0.00015346
Iteration 103/1000 | Loss: 0.00016458
Iteration 104/1000 | Loss: 0.00015624
Iteration 105/1000 | Loss: 0.00016361
Iteration 106/1000 | Loss: 0.00016161
Iteration 107/1000 | Loss: 0.00006642
Iteration 108/1000 | Loss: 0.00005135
Iteration 109/1000 | Loss: 0.00010126
Iteration 110/1000 | Loss: 0.00012946
Iteration 111/1000 | Loss: 0.00015463
Iteration 112/1000 | Loss: 0.00006182
Iteration 113/1000 | Loss: 0.00006582
Iteration 114/1000 | Loss: 0.00008916
Iteration 115/1000 | Loss: 0.00046490
Iteration 116/1000 | Loss: 0.00140193
Iteration 117/1000 | Loss: 0.00015496
Iteration 118/1000 | Loss: 0.00051300
Iteration 119/1000 | Loss: 0.00015156
Iteration 120/1000 | Loss: 0.00015730
Iteration 121/1000 | Loss: 0.00058125
Iteration 122/1000 | Loss: 0.00006924
Iteration 123/1000 | Loss: 0.00004802
Iteration 124/1000 | Loss: 0.00004167
Iteration 125/1000 | Loss: 0.00006100
Iteration 126/1000 | Loss: 0.00011709
Iteration 127/1000 | Loss: 0.00007065
Iteration 128/1000 | Loss: 0.00007131
Iteration 129/1000 | Loss: 0.00009207
Iteration 130/1000 | Loss: 0.00005905
Iteration 131/1000 | Loss: 0.00020908
Iteration 132/1000 | Loss: 0.00009180
Iteration 133/1000 | Loss: 0.00006347
Iteration 134/1000 | Loss: 0.00006713
Iteration 135/1000 | Loss: 0.00007777
Iteration 136/1000 | Loss: 0.00009715
Iteration 137/1000 | Loss: 0.00006265
Iteration 138/1000 | Loss: 0.00006822
Iteration 139/1000 | Loss: 0.00007239
Iteration 140/1000 | Loss: 0.00013852
Iteration 141/1000 | Loss: 0.00016241
Iteration 142/1000 | Loss: 0.00008947
Iteration 143/1000 | Loss: 0.00016139
Iteration 144/1000 | Loss: 0.00007724
Iteration 145/1000 | Loss: 0.00011129
Iteration 146/1000 | Loss: 0.00005925
Iteration 147/1000 | Loss: 0.00006972
Iteration 148/1000 | Loss: 0.00007555
Iteration 149/1000 | Loss: 0.00010799
Iteration 150/1000 | Loss: 0.00007482
Iteration 151/1000 | Loss: 0.00026580
Iteration 152/1000 | Loss: 0.00019855
Iteration 153/1000 | Loss: 0.00023837
Iteration 154/1000 | Loss: 0.00017751
Iteration 155/1000 | Loss: 0.00014931
Iteration 156/1000 | Loss: 0.00008596
Iteration 157/1000 | Loss: 0.00029541
Iteration 158/1000 | Loss: 0.00019338
Iteration 159/1000 | Loss: 0.00036680
Iteration 160/1000 | Loss: 0.00020670
Iteration 161/1000 | Loss: 0.00038036
Iteration 162/1000 | Loss: 0.00005742
Iteration 163/1000 | Loss: 0.00005858
Iteration 164/1000 | Loss: 0.00004591
Iteration 165/1000 | Loss: 0.00004473
Iteration 166/1000 | Loss: 0.00031993
Iteration 167/1000 | Loss: 0.00015212
Iteration 168/1000 | Loss: 0.00006999
Iteration 169/1000 | Loss: 0.00004559
Iteration 170/1000 | Loss: 0.00012282
Iteration 171/1000 | Loss: 0.00033626
Iteration 172/1000 | Loss: 0.00027583
Iteration 173/1000 | Loss: 0.00027212
Iteration 174/1000 | Loss: 0.00053627
Iteration 175/1000 | Loss: 0.00066121
Iteration 176/1000 | Loss: 0.00046839
Iteration 177/1000 | Loss: 0.00007850
Iteration 178/1000 | Loss: 0.00007428
Iteration 179/1000 | Loss: 0.00007226
Iteration 180/1000 | Loss: 0.00006443
Iteration 181/1000 | Loss: 0.00006437
Iteration 182/1000 | Loss: 0.00005913
Iteration 183/1000 | Loss: 0.00007860
Iteration 184/1000 | Loss: 0.00008606
Iteration 185/1000 | Loss: 0.00005562
Iteration 186/1000 | Loss: 0.00007379
Iteration 187/1000 | Loss: 0.00005132
Iteration 188/1000 | Loss: 0.00006490
Iteration 189/1000 | Loss: 0.00005790
Iteration 190/1000 | Loss: 0.00030434
Iteration 191/1000 | Loss: 0.00021834
Iteration 192/1000 | Loss: 0.00018193
Iteration 193/1000 | Loss: 0.00008860
Iteration 194/1000 | Loss: 0.00004678
Iteration 195/1000 | Loss: 0.00006530
Iteration 196/1000 | Loss: 0.00005994
Iteration 197/1000 | Loss: 0.00007218
Iteration 198/1000 | Loss: 0.00005828
Iteration 199/1000 | Loss: 0.00005318
Iteration 200/1000 | Loss: 0.00006960
Iteration 201/1000 | Loss: 0.00005007
Iteration 202/1000 | Loss: 0.00003257
Iteration 203/1000 | Loss: 0.00004249
Iteration 204/1000 | Loss: 0.00002487
Iteration 205/1000 | Loss: 0.00007377
Iteration 206/1000 | Loss: 0.00004107
Iteration 207/1000 | Loss: 0.00003393
Iteration 208/1000 | Loss: 0.00006561
Iteration 209/1000 | Loss: 0.00009299
Iteration 210/1000 | Loss: 0.00006703
Iteration 211/1000 | Loss: 0.00009958
Iteration 212/1000 | Loss: 0.00004752
Iteration 213/1000 | Loss: 0.00004232
Iteration 214/1000 | Loss: 0.00003989
Iteration 215/1000 | Loss: 0.00004154
Iteration 216/1000 | Loss: 0.00003170
Iteration 217/1000 | Loss: 0.00008443
Iteration 218/1000 | Loss: 0.00003678
Iteration 219/1000 | Loss: 0.00003205
Iteration 220/1000 | Loss: 0.00002903
Iteration 221/1000 | Loss: 0.00003956
Iteration 222/1000 | Loss: 0.00004246
Iteration 223/1000 | Loss: 0.00003687
Iteration 224/1000 | Loss: 0.00003446
Iteration 225/1000 | Loss: 0.00004123
Iteration 226/1000 | Loss: 0.00004893
Iteration 227/1000 | Loss: 0.00007217
Iteration 228/1000 | Loss: 0.00004311
Iteration 229/1000 | Loss: 0.00002434
Iteration 230/1000 | Loss: 0.00004217
Iteration 231/1000 | Loss: 0.00002293
Iteration 232/1000 | Loss: 0.00002331
Iteration 233/1000 | Loss: 0.00006368
Iteration 234/1000 | Loss: 0.00004272
Iteration 235/1000 | Loss: 0.00004375
Iteration 236/1000 | Loss: 0.00003346
Iteration 237/1000 | Loss: 0.00003795
Iteration 238/1000 | Loss: 0.00003025
Iteration 239/1000 | Loss: 0.00008076
Iteration 240/1000 | Loss: 0.00002928
Iteration 241/1000 | Loss: 0.00002794
Iteration 242/1000 | Loss: 0.00002910
Iteration 243/1000 | Loss: 0.00002980
Iteration 244/1000 | Loss: 0.00002907
Iteration 245/1000 | Loss: 0.00003263
Iteration 246/1000 | Loss: 0.00002861
Iteration 247/1000 | Loss: 0.00003187
Iteration 248/1000 | Loss: 0.00003648
Iteration 249/1000 | Loss: 0.00003806
Iteration 250/1000 | Loss: 0.00004381
Iteration 251/1000 | Loss: 0.00003783
Iteration 252/1000 | Loss: 0.00004123
Iteration 253/1000 | Loss: 0.00004142
Iteration 254/1000 | Loss: 0.00004793
Iteration 255/1000 | Loss: 0.00004482
Iteration 256/1000 | Loss: 0.00005108
Iteration 257/1000 | Loss: 0.00005162
Iteration 258/1000 | Loss: 0.00004301
Iteration 259/1000 | Loss: 0.00003990
Iteration 260/1000 | Loss: 0.00004247
Iteration 261/1000 | Loss: 0.00003935
Iteration 262/1000 | Loss: 0.00005103
Iteration 263/1000 | Loss: 0.00005436
Iteration 264/1000 | Loss: 0.00003253
Iteration 265/1000 | Loss: 0.00003615
Iteration 266/1000 | Loss: 0.00005247
Iteration 267/1000 | Loss: 0.00003916
Iteration 268/1000 | Loss: 0.00005163
Iteration 269/1000 | Loss: 0.00003340
Iteration 270/1000 | Loss: 0.00003462
Iteration 271/1000 | Loss: 0.00003864
Iteration 272/1000 | Loss: 0.00006120
Iteration 273/1000 | Loss: 0.00004005
Iteration 274/1000 | Loss: 0.00003557
Iteration 275/1000 | Loss: 0.00005532
Iteration 276/1000 | Loss: 0.00003690
Iteration 277/1000 | Loss: 0.00003360
Iteration 278/1000 | Loss: 0.00004871
Iteration 279/1000 | Loss: 0.00004300
Iteration 280/1000 | Loss: 0.00004871
Iteration 281/1000 | Loss: 0.00004960
Iteration 282/1000 | Loss: 0.00004033
Iteration 283/1000 | Loss: 0.00004278
Iteration 284/1000 | Loss: 0.00004309
Iteration 285/1000 | Loss: 0.00004246
Iteration 286/1000 | Loss: 0.00005087
Iteration 287/1000 | Loss: 0.00004117
Iteration 288/1000 | Loss: 0.00008465
Iteration 289/1000 | Loss: 0.00004135
Iteration 290/1000 | Loss: 0.00005159
Iteration 291/1000 | Loss: 0.00004021
Iteration 292/1000 | Loss: 0.00005586
Iteration 293/1000 | Loss: 0.00003967
Iteration 294/1000 | Loss: 0.00003717
Iteration 295/1000 | Loss: 0.00002945
Iteration 296/1000 | Loss: 0.00004745
Iteration 297/1000 | Loss: 0.00003927
Iteration 298/1000 | Loss: 0.00003527
Iteration 299/1000 | Loss: 0.00004038
Iteration 300/1000 | Loss: 0.00004109
Iteration 301/1000 | Loss: 0.00004053
Iteration 302/1000 | Loss: 0.00004059
Iteration 303/1000 | Loss: 0.00006408
Iteration 304/1000 | Loss: 0.00004414
Iteration 305/1000 | Loss: 0.00003465
Iteration 306/1000 | Loss: 0.00003659
Iteration 307/1000 | Loss: 0.00003963
Iteration 308/1000 | Loss: 0.00003399
Iteration 309/1000 | Loss: 0.00003414
Iteration 310/1000 | Loss: 0.00003632
Iteration 311/1000 | Loss: 0.00003329
Iteration 312/1000 | Loss: 0.00004604
Iteration 313/1000 | Loss: 0.00003987
Iteration 314/1000 | Loss: 0.00003337
Iteration 315/1000 | Loss: 0.00004715
Iteration 316/1000 | Loss: 0.00003683
Iteration 317/1000 | Loss: 0.00003250
Iteration 318/1000 | Loss: 0.00003479
Iteration 319/1000 | Loss: 0.00004448
Iteration 320/1000 | Loss: 0.00004735
Iteration 321/1000 | Loss: 0.00004284
Iteration 322/1000 | Loss: 0.00006012
Iteration 323/1000 | Loss: 0.00003632
Iteration 324/1000 | Loss: 0.00003957
Iteration 325/1000 | Loss: 0.00006078
Iteration 326/1000 | Loss: 0.00004268
Iteration 327/1000 | Loss: 0.00006144
Iteration 328/1000 | Loss: 0.00005155
Iteration 329/1000 | Loss: 0.00004569
Iteration 330/1000 | Loss: 0.00004267
Iteration 331/1000 | Loss: 0.00003440
Iteration 332/1000 | Loss: 0.00005070
Iteration 333/1000 | Loss: 0.00004785
Iteration 334/1000 | Loss: 0.00004815
Iteration 335/1000 | Loss: 0.00004194
Iteration 336/1000 | Loss: 0.00004031
Iteration 337/1000 | Loss: 0.00003178
Iteration 338/1000 | Loss: 0.00003940
Iteration 339/1000 | Loss: 0.00003715
Iteration 340/1000 | Loss: 0.00003678
Iteration 341/1000 | Loss: 0.00003181
Iteration 342/1000 | Loss: 0.00003616
Iteration 343/1000 | Loss: 0.00003895
Iteration 344/1000 | Loss: 0.00006914
Iteration 345/1000 | Loss: 0.00004291
Iteration 346/1000 | Loss: 0.00004627
Iteration 347/1000 | Loss: 0.00007007
Iteration 348/1000 | Loss: 0.00002960
Iteration 349/1000 | Loss: 0.00002985
Iteration 350/1000 | Loss: 0.00006902
Iteration 351/1000 | Loss: 0.00006575
Iteration 352/1000 | Loss: 0.00005060
Iteration 353/1000 | Loss: 0.00004734
Iteration 354/1000 | Loss: 0.00004478
Iteration 355/1000 | Loss: 0.00004531
Iteration 356/1000 | Loss: 0.00003253
Iteration 357/1000 | Loss: 0.00003672
Iteration 358/1000 | Loss: 0.00002713
Iteration 359/1000 | Loss: 0.00004128
Iteration 360/1000 | Loss: 0.00005637
Iteration 361/1000 | Loss: 0.00004071
Iteration 362/1000 | Loss: 0.00003624
Iteration 363/1000 | Loss: 0.00006592
Iteration 364/1000 | Loss: 0.00003386
Iteration 365/1000 | Loss: 0.00004270
Iteration 366/1000 | Loss: 0.00003000
Iteration 367/1000 | Loss: 0.00005927
Iteration 368/1000 | Loss: 0.00003402
Iteration 369/1000 | Loss: 0.00004039
Iteration 370/1000 | Loss: 0.00004320
Iteration 371/1000 | Loss: 0.00004006
Iteration 372/1000 | Loss: 0.00004437
Iteration 373/1000 | Loss: 0.00004181
Iteration 374/1000 | Loss: 0.00006428
Iteration 375/1000 | Loss: 0.00003459
Iteration 376/1000 | Loss: 0.00003604
Iteration 377/1000 | Loss: 0.00004412
Iteration 378/1000 | Loss: 0.00004277
Iteration 379/1000 | Loss: 0.00003465
Iteration 380/1000 | Loss: 0.00004585
Iteration 381/1000 | Loss: 0.00003937
Iteration 382/1000 | Loss: 0.00002661
Iteration 383/1000 | Loss: 0.00003170
Iteration 384/1000 | Loss: 0.00004154
Iteration 385/1000 | Loss: 0.00005380
Iteration 386/1000 | Loss: 0.00003892
Iteration 387/1000 | Loss: 0.00004476
Iteration 388/1000 | Loss: 0.00005209
Iteration 389/1000 | Loss: 0.00002676
Iteration 390/1000 | Loss: 0.00004312
Iteration 391/1000 | Loss: 0.00004443
Iteration 392/1000 | Loss: 0.00004509
Iteration 393/1000 | Loss: 0.00005305
Iteration 394/1000 | Loss: 0.00008053
Iteration 395/1000 | Loss: 0.00005027
Iteration 396/1000 | Loss: 0.00006084
Iteration 397/1000 | Loss: 0.00005695
Iteration 398/1000 | Loss: 0.00005425
Iteration 399/1000 | Loss: 0.00004698
Iteration 400/1000 | Loss: 0.00009022
Iteration 401/1000 | Loss: 0.00008556
Iteration 402/1000 | Loss: 0.00004304
Iteration 403/1000 | Loss: 0.00007888
Iteration 404/1000 | Loss: 0.00006660
Iteration 405/1000 | Loss: 0.00005357
Iteration 406/1000 | Loss: 0.00005420
Iteration 407/1000 | Loss: 0.00004007
Iteration 408/1000 | Loss: 0.00005000
Iteration 409/1000 | Loss: 0.00002617
Iteration 410/1000 | Loss: 0.00002116
Iteration 411/1000 | Loss: 0.00004156
Iteration 412/1000 | Loss: 0.00003948
Iteration 413/1000 | Loss: 0.00006207
Iteration 414/1000 | Loss: 0.00005178
Iteration 415/1000 | Loss: 0.00006113
Iteration 416/1000 | Loss: 0.00007132
Iteration 417/1000 | Loss: 0.00004064
Iteration 418/1000 | Loss: 0.00004085
Iteration 419/1000 | Loss: 0.00003659
Iteration 420/1000 | Loss: 0.00006530
Iteration 421/1000 | Loss: 0.00004839
Iteration 422/1000 | Loss: 0.00004352
Iteration 423/1000 | Loss: 0.00003873
Iteration 424/1000 | Loss: 0.00008185
Iteration 425/1000 | Loss: 0.00004094
Iteration 426/1000 | Loss: 0.00003239
Iteration 427/1000 | Loss: 0.00003982
Iteration 428/1000 | Loss: 0.00003636
Iteration 429/1000 | Loss: 0.00002705
Iteration 430/1000 | Loss: 0.00002882
Iteration 431/1000 | Loss: 0.00005710
Iteration 432/1000 | Loss: 0.00003083
Iteration 433/1000 | Loss: 0.00005366
Iteration 434/1000 | Loss: 0.00003782
Iteration 435/1000 | Loss: 0.00004257
Iteration 436/1000 | Loss: 0.00003097
Iteration 437/1000 | Loss: 0.00004202
Iteration 438/1000 | Loss: 0.00003111
Iteration 439/1000 | Loss: 0.00004130
Iteration 440/1000 | Loss: 0.00003754
Iteration 441/1000 | Loss: 0.00004566
Iteration 442/1000 | Loss: 0.00003436
Iteration 443/1000 | Loss: 0.00004515
Iteration 444/1000 | Loss: 0.00003353
Iteration 445/1000 | Loss: 0.00002355
Iteration 446/1000 | Loss: 0.00002837
Iteration 447/1000 | Loss: 0.00001781
Iteration 448/1000 | Loss: 0.00003367
Iteration 449/1000 | Loss: 0.00001802
Iteration 450/1000 | Loss: 0.00001694
Iteration 451/1000 | Loss: 0.00001624
Iteration 452/1000 | Loss: 0.00001791
Iteration 453/1000 | Loss: 0.00001736
Iteration 454/1000 | Loss: 0.00001720
Iteration 455/1000 | Loss: 0.00001597
Iteration 456/1000 | Loss: 0.00001578
Iteration 457/1000 | Loss: 0.00001578
Iteration 458/1000 | Loss: 0.00001578
Iteration 459/1000 | Loss: 0.00001577
Iteration 460/1000 | Loss: 0.00001577
Iteration 461/1000 | Loss: 0.00001577
Iteration 462/1000 | Loss: 0.00001577
Iteration 463/1000 | Loss: 0.00001577
Iteration 464/1000 | Loss: 0.00001577
Iteration 465/1000 | Loss: 0.00001577
Iteration 466/1000 | Loss: 0.00001577
Iteration 467/1000 | Loss: 0.00001577
Iteration 468/1000 | Loss: 0.00001576
Iteration 469/1000 | Loss: 0.00001576
Iteration 470/1000 | Loss: 0.00001576
Iteration 471/1000 | Loss: 0.00001576
Iteration 472/1000 | Loss: 0.00001576
Iteration 473/1000 | Loss: 0.00001576
Iteration 474/1000 | Loss: 0.00001576
Iteration 475/1000 | Loss: 0.00001575
Iteration 476/1000 | Loss: 0.00001575
Iteration 477/1000 | Loss: 0.00001575
Iteration 478/1000 | Loss: 0.00001575
Iteration 479/1000 | Loss: 0.00001575
Iteration 480/1000 | Loss: 0.00001575
Iteration 481/1000 | Loss: 0.00001575
Iteration 482/1000 | Loss: 0.00002105
Iteration 483/1000 | Loss: 0.00001575
Iteration 484/1000 | Loss: 0.00001574
Iteration 485/1000 | Loss: 0.00001572
Iteration 486/1000 | Loss: 0.00001572
Iteration 487/1000 | Loss: 0.00001572
Iteration 488/1000 | Loss: 0.00001572
Iteration 489/1000 | Loss: 0.00001572
Iteration 490/1000 | Loss: 0.00001572
Iteration 491/1000 | Loss: 0.00001572
Iteration 492/1000 | Loss: 0.00001572
Iteration 493/1000 | Loss: 0.00001571
Iteration 494/1000 | Loss: 0.00001571
Iteration 495/1000 | Loss: 0.00001571
Iteration 496/1000 | Loss: 0.00001571
Iteration 497/1000 | Loss: 0.00001570
Iteration 498/1000 | Loss: 0.00001570
Iteration 499/1000 | Loss: 0.00001570
Iteration 500/1000 | Loss: 0.00001570
Iteration 501/1000 | Loss: 0.00001570
Iteration 502/1000 | Loss: 0.00001569
Iteration 503/1000 | Loss: 0.00001569
Iteration 504/1000 | Loss: 0.00001569
Iteration 505/1000 | Loss: 0.00001569
Iteration 506/1000 | Loss: 0.00001569
Iteration 507/1000 | Loss: 0.00001568
Iteration 508/1000 | Loss: 0.00001568
Iteration 509/1000 | Loss: 0.00001568
Iteration 510/1000 | Loss: 0.00001568
Iteration 511/1000 | Loss: 0.00001568
Iteration 512/1000 | Loss: 0.00001567
Iteration 513/1000 | Loss: 0.00001567
Iteration 514/1000 | Loss: 0.00001567
Iteration 515/1000 | Loss: 0.00001567
Iteration 516/1000 | Loss: 0.00001567
Iteration 517/1000 | Loss: 0.00001567
Iteration 518/1000 | Loss: 0.00001567
Iteration 519/1000 | Loss: 0.00001567
Iteration 520/1000 | Loss: 0.00001566
Iteration 521/1000 | Loss: 0.00001566
Iteration 522/1000 | Loss: 0.00001566
Iteration 523/1000 | Loss: 0.00001566
Iteration 524/1000 | Loss: 0.00001566
Iteration 525/1000 | Loss: 0.00001566
Iteration 526/1000 | Loss: 0.00001566
Iteration 527/1000 | Loss: 0.00001566
Iteration 528/1000 | Loss: 0.00001566
Iteration 529/1000 | Loss: 0.00001566
Iteration 530/1000 | Loss: 0.00001566
Iteration 531/1000 | Loss: 0.00001566
Iteration 532/1000 | Loss: 0.00001566
Iteration 533/1000 | Loss: 0.00001566
Iteration 534/1000 | Loss: 0.00001566
Iteration 535/1000 | Loss: 0.00001566
Iteration 536/1000 | Loss: 0.00001566
Iteration 537/1000 | Loss: 0.00001566
Iteration 538/1000 | Loss: 0.00001566
Iteration 539/1000 | Loss: 0.00001566
Iteration 540/1000 | Loss: 0.00001566
Iteration 541/1000 | Loss: 0.00001566
Iteration 542/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 542. Stopping optimization.
Last 5 losses: [1.5656276445952244e-05, 1.5656276445952244e-05, 1.5656276445952244e-05, 1.5656276445952244e-05, 1.5656276445952244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5656276445952244e-05

Optimization complete. Final v2v error: 3.2586567401885986 mm

Highest mean error: 5.555137634277344 mm for frame 65

Lowest mean error: 2.623952865600586 mm for frame 116

Saving results

Total time: 774.9759624004364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625891
Iteration 2/25 | Loss: 0.00155310
Iteration 3/25 | Loss: 0.00138839
Iteration 4/25 | Loss: 0.00135785
Iteration 5/25 | Loss: 0.00134302
Iteration 6/25 | Loss: 0.00134724
Iteration 7/25 | Loss: 0.00134251
Iteration 8/25 | Loss: 0.00133767
Iteration 9/25 | Loss: 0.00133704
Iteration 10/25 | Loss: 0.00133649
Iteration 11/25 | Loss: 0.00133633
Iteration 12/25 | Loss: 0.00133791
Iteration 13/25 | Loss: 0.00133771
Iteration 14/25 | Loss: 0.00133176
Iteration 15/25 | Loss: 0.00132897
Iteration 16/25 | Loss: 0.00132846
Iteration 17/25 | Loss: 0.00132825
Iteration 18/25 | Loss: 0.00132820
Iteration 19/25 | Loss: 0.00132819
Iteration 20/25 | Loss: 0.00132819
Iteration 21/25 | Loss: 0.00132819
Iteration 22/25 | Loss: 0.00132819
Iteration 23/25 | Loss: 0.00132818
Iteration 24/25 | Loss: 0.00132818
Iteration 25/25 | Loss: 0.00132818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59661949
Iteration 2/25 | Loss: 0.00227116
Iteration 3/25 | Loss: 0.00227116
Iteration 4/25 | Loss: 0.00227116
Iteration 5/25 | Loss: 0.00227116
Iteration 6/25 | Loss: 0.00227116
Iteration 7/25 | Loss: 0.00227116
Iteration 8/25 | Loss: 0.00227116
Iteration 9/25 | Loss: 0.00227116
Iteration 10/25 | Loss: 0.00227116
Iteration 11/25 | Loss: 0.00227116
Iteration 12/25 | Loss: 0.00227116
Iteration 13/25 | Loss: 0.00227116
Iteration 14/25 | Loss: 0.00227116
Iteration 15/25 | Loss: 0.00227116
Iteration 16/25 | Loss: 0.00227116
Iteration 17/25 | Loss: 0.00227116
Iteration 18/25 | Loss: 0.00227116
Iteration 19/25 | Loss: 0.00227116
Iteration 20/25 | Loss: 0.00227116
Iteration 21/25 | Loss: 0.00227116
Iteration 22/25 | Loss: 0.00227116
Iteration 23/25 | Loss: 0.00227116
Iteration 24/25 | Loss: 0.00227116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0022711569909006357, 0.0022711569909006357, 0.0022711569909006357, 0.0022711569909006357, 0.0022711569909006357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022711569909006357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227116
Iteration 2/1000 | Loss: 0.00006270
Iteration 3/1000 | Loss: 0.00004476
Iteration 4/1000 | Loss: 0.00003377
Iteration 5/1000 | Loss: 0.00003094
Iteration 6/1000 | Loss: 0.00002941
Iteration 7/1000 | Loss: 0.00002777
Iteration 8/1000 | Loss: 0.00020127
Iteration 9/1000 | Loss: 0.00002659
Iteration 10/1000 | Loss: 0.00002614
Iteration 11/1000 | Loss: 0.00002561
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002499
Iteration 14/1000 | Loss: 0.00002472
Iteration 15/1000 | Loss: 0.00002449
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002409
Iteration 18/1000 | Loss: 0.00002401
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002377
Iteration 21/1000 | Loss: 0.00002371
Iteration 22/1000 | Loss: 0.00002371
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00002370
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002369
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002360
Iteration 29/1000 | Loss: 0.00002359
Iteration 30/1000 | Loss: 0.00002357
Iteration 31/1000 | Loss: 0.00002354
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002351
Iteration 34/1000 | Loss: 0.00002350
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002347
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002345
Iteration 42/1000 | Loss: 0.00002345
Iteration 43/1000 | Loss: 0.00002345
Iteration 44/1000 | Loss: 0.00002345
Iteration 45/1000 | Loss: 0.00002345
Iteration 46/1000 | Loss: 0.00002345
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002344
Iteration 49/1000 | Loss: 0.00002344
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002342
Iteration 58/1000 | Loss: 0.00002342
Iteration 59/1000 | Loss: 0.00002342
Iteration 60/1000 | Loss: 0.00002341
Iteration 61/1000 | Loss: 0.00002341
Iteration 62/1000 | Loss: 0.00002341
Iteration 63/1000 | Loss: 0.00002341
Iteration 64/1000 | Loss: 0.00002341
Iteration 65/1000 | Loss: 0.00002341
Iteration 66/1000 | Loss: 0.00002340
Iteration 67/1000 | Loss: 0.00002340
Iteration 68/1000 | Loss: 0.00002340
Iteration 69/1000 | Loss: 0.00002340
Iteration 70/1000 | Loss: 0.00002339
Iteration 71/1000 | Loss: 0.00002339
Iteration 72/1000 | Loss: 0.00002339
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002338
Iteration 75/1000 | Loss: 0.00002338
Iteration 76/1000 | Loss: 0.00002337
Iteration 77/1000 | Loss: 0.00002336
Iteration 78/1000 | Loss: 0.00002336
Iteration 79/1000 | Loss: 0.00002334
Iteration 80/1000 | Loss: 0.00002334
Iteration 81/1000 | Loss: 0.00002333
Iteration 82/1000 | Loss: 0.00002333
Iteration 83/1000 | Loss: 0.00002332
Iteration 84/1000 | Loss: 0.00002332
Iteration 85/1000 | Loss: 0.00002331
Iteration 86/1000 | Loss: 0.00002331
Iteration 87/1000 | Loss: 0.00002330
Iteration 88/1000 | Loss: 0.00002330
Iteration 89/1000 | Loss: 0.00002330
Iteration 90/1000 | Loss: 0.00002330
Iteration 91/1000 | Loss: 0.00002330
Iteration 92/1000 | Loss: 0.00002330
Iteration 93/1000 | Loss: 0.00002330
Iteration 94/1000 | Loss: 0.00002330
Iteration 95/1000 | Loss: 0.00002330
Iteration 96/1000 | Loss: 0.00002330
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002329
Iteration 100/1000 | Loss: 0.00002329
Iteration 101/1000 | Loss: 0.00002329
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002328
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00002326
Iteration 113/1000 | Loss: 0.00002326
Iteration 114/1000 | Loss: 0.00002326
Iteration 115/1000 | Loss: 0.00002326
Iteration 116/1000 | Loss: 0.00002326
Iteration 117/1000 | Loss: 0.00002326
Iteration 118/1000 | Loss: 0.00002325
Iteration 119/1000 | Loss: 0.00002325
Iteration 120/1000 | Loss: 0.00002325
Iteration 121/1000 | Loss: 0.00002325
Iteration 122/1000 | Loss: 0.00002325
Iteration 123/1000 | Loss: 0.00002325
Iteration 124/1000 | Loss: 0.00002325
Iteration 125/1000 | Loss: 0.00002325
Iteration 126/1000 | Loss: 0.00002324
Iteration 127/1000 | Loss: 0.00002324
Iteration 128/1000 | Loss: 0.00002324
Iteration 129/1000 | Loss: 0.00002323
Iteration 130/1000 | Loss: 0.00002323
Iteration 131/1000 | Loss: 0.00002323
Iteration 132/1000 | Loss: 0.00002322
Iteration 133/1000 | Loss: 0.00002322
Iteration 134/1000 | Loss: 0.00002322
Iteration 135/1000 | Loss: 0.00002322
Iteration 136/1000 | Loss: 0.00002321
Iteration 137/1000 | Loss: 0.00002321
Iteration 138/1000 | Loss: 0.00002321
Iteration 139/1000 | Loss: 0.00002321
Iteration 140/1000 | Loss: 0.00002321
Iteration 141/1000 | Loss: 0.00002321
Iteration 142/1000 | Loss: 0.00002321
Iteration 143/1000 | Loss: 0.00002321
Iteration 144/1000 | Loss: 0.00002321
Iteration 145/1000 | Loss: 0.00002320
Iteration 146/1000 | Loss: 0.00002320
Iteration 147/1000 | Loss: 0.00002320
Iteration 148/1000 | Loss: 0.00002319
Iteration 149/1000 | Loss: 0.00002319
Iteration 150/1000 | Loss: 0.00002319
Iteration 151/1000 | Loss: 0.00002319
Iteration 152/1000 | Loss: 0.00002319
Iteration 153/1000 | Loss: 0.00002319
Iteration 154/1000 | Loss: 0.00002318
Iteration 155/1000 | Loss: 0.00002318
Iteration 156/1000 | Loss: 0.00002318
Iteration 157/1000 | Loss: 0.00002318
Iteration 158/1000 | Loss: 0.00002318
Iteration 159/1000 | Loss: 0.00002318
Iteration 160/1000 | Loss: 0.00002318
Iteration 161/1000 | Loss: 0.00002318
Iteration 162/1000 | Loss: 0.00002318
Iteration 163/1000 | Loss: 0.00002317
Iteration 164/1000 | Loss: 0.00002317
Iteration 165/1000 | Loss: 0.00002317
Iteration 166/1000 | Loss: 0.00002317
Iteration 167/1000 | Loss: 0.00002317
Iteration 168/1000 | Loss: 0.00002316
Iteration 169/1000 | Loss: 0.00002316
Iteration 170/1000 | Loss: 0.00002316
Iteration 171/1000 | Loss: 0.00002316
Iteration 172/1000 | Loss: 0.00002316
Iteration 173/1000 | Loss: 0.00002316
Iteration 174/1000 | Loss: 0.00002315
Iteration 175/1000 | Loss: 0.00002315
Iteration 176/1000 | Loss: 0.00002315
Iteration 177/1000 | Loss: 0.00002315
Iteration 178/1000 | Loss: 0.00002315
Iteration 179/1000 | Loss: 0.00002315
Iteration 180/1000 | Loss: 0.00002315
Iteration 181/1000 | Loss: 0.00002315
Iteration 182/1000 | Loss: 0.00002315
Iteration 183/1000 | Loss: 0.00002315
Iteration 184/1000 | Loss: 0.00002315
Iteration 185/1000 | Loss: 0.00002315
Iteration 186/1000 | Loss: 0.00002315
Iteration 187/1000 | Loss: 0.00002314
Iteration 188/1000 | Loss: 0.00002314
Iteration 189/1000 | Loss: 0.00002314
Iteration 190/1000 | Loss: 0.00002314
Iteration 191/1000 | Loss: 0.00002314
Iteration 192/1000 | Loss: 0.00002314
Iteration 193/1000 | Loss: 0.00002314
Iteration 194/1000 | Loss: 0.00002314
Iteration 195/1000 | Loss: 0.00002314
Iteration 196/1000 | Loss: 0.00002314
Iteration 197/1000 | Loss: 0.00002313
Iteration 198/1000 | Loss: 0.00002313
Iteration 199/1000 | Loss: 0.00002313
Iteration 200/1000 | Loss: 0.00002313
Iteration 201/1000 | Loss: 0.00002313
Iteration 202/1000 | Loss: 0.00002313
Iteration 203/1000 | Loss: 0.00002313
Iteration 204/1000 | Loss: 0.00002313
Iteration 205/1000 | Loss: 0.00002313
Iteration 206/1000 | Loss: 0.00002313
Iteration 207/1000 | Loss: 0.00002313
Iteration 208/1000 | Loss: 0.00002313
Iteration 209/1000 | Loss: 0.00002313
Iteration 210/1000 | Loss: 0.00002313
Iteration 211/1000 | Loss: 0.00002313
Iteration 212/1000 | Loss: 0.00002313
Iteration 213/1000 | Loss: 0.00002313
Iteration 214/1000 | Loss: 0.00002313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.3130574845708907e-05, 2.3130574845708907e-05, 2.3130574845708907e-05, 2.3130574845708907e-05, 2.3130574845708907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3130574845708907e-05

Optimization complete. Final v2v error: 4.021793365478516 mm

Highest mean error: 4.930408000946045 mm for frame 59

Lowest mean error: 3.3303518295288086 mm for frame 81

Saving results

Total time: 73.27424693107605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826590
Iteration 2/25 | Loss: 0.00138195
Iteration 3/25 | Loss: 0.00131331
Iteration 4/25 | Loss: 0.00130492
Iteration 5/25 | Loss: 0.00130274
Iteration 6/25 | Loss: 0.00130262
Iteration 7/25 | Loss: 0.00130262
Iteration 8/25 | Loss: 0.00130262
Iteration 9/25 | Loss: 0.00130262
Iteration 10/25 | Loss: 0.00130262
Iteration 11/25 | Loss: 0.00130262
Iteration 12/25 | Loss: 0.00130262
Iteration 13/25 | Loss: 0.00130262
Iteration 14/25 | Loss: 0.00130262
Iteration 15/25 | Loss: 0.00130262
Iteration 16/25 | Loss: 0.00130262
Iteration 17/25 | Loss: 0.00130262
Iteration 18/25 | Loss: 0.00130262
Iteration 19/25 | Loss: 0.00130262
Iteration 20/25 | Loss: 0.00130262
Iteration 21/25 | Loss: 0.00130262
Iteration 22/25 | Loss: 0.00130262
Iteration 23/25 | Loss: 0.00130262
Iteration 24/25 | Loss: 0.00130262
Iteration 25/25 | Loss: 0.00130262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.14868259
Iteration 2/25 | Loss: 0.00148125
Iteration 3/25 | Loss: 0.00148124
Iteration 4/25 | Loss: 0.00148124
Iteration 5/25 | Loss: 0.00148124
Iteration 6/25 | Loss: 0.00148124
Iteration 7/25 | Loss: 0.00148124
Iteration 8/25 | Loss: 0.00148124
Iteration 9/25 | Loss: 0.00148124
Iteration 10/25 | Loss: 0.00148124
Iteration 11/25 | Loss: 0.00148124
Iteration 12/25 | Loss: 0.00148124
Iteration 13/25 | Loss: 0.00148124
Iteration 14/25 | Loss: 0.00148124
Iteration 15/25 | Loss: 0.00148124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014812380541116, 0.0014812380541116, 0.0014812380541116, 0.0014812380541116, 0.0014812380541116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014812380541116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148124
Iteration 2/1000 | Loss: 0.00002397
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001657
Iteration 6/1000 | Loss: 0.00001591
Iteration 7/1000 | Loss: 0.00001542
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001470
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001432
Iteration 12/1000 | Loss: 0.00001415
Iteration 13/1000 | Loss: 0.00001398
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001393
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001363
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001354
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001344
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.3332155504031107e-05, 1.3332155504031107e-05, 1.3332155504031107e-05, 1.3332155504031107e-05, 1.3332155504031107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3332155504031107e-05

Optimization complete. Final v2v error: 3.1083295345306396 mm

Highest mean error: 3.5679714679718018 mm for frame 76

Lowest mean error: 2.7649054527282715 mm for frame 140

Saving results

Total time: 35.05464792251587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043744
Iteration 2/25 | Loss: 0.00196416
Iteration 3/25 | Loss: 0.00150593
Iteration 4/25 | Loss: 0.00147018
Iteration 5/25 | Loss: 0.00144960
Iteration 6/25 | Loss: 0.00143287
Iteration 7/25 | Loss: 0.00139891
Iteration 8/25 | Loss: 0.00137016
Iteration 9/25 | Loss: 0.00135878
Iteration 10/25 | Loss: 0.00135491
Iteration 11/25 | Loss: 0.00135467
Iteration 12/25 | Loss: 0.00135750
Iteration 13/25 | Loss: 0.00134855
Iteration 14/25 | Loss: 0.00133642
Iteration 15/25 | Loss: 0.00133575
Iteration 16/25 | Loss: 0.00133258
Iteration 17/25 | Loss: 0.00133179
Iteration 18/25 | Loss: 0.00133489
Iteration 19/25 | Loss: 0.00133261
Iteration 20/25 | Loss: 0.00133330
Iteration 21/25 | Loss: 0.00133475
Iteration 22/25 | Loss: 0.00133030
Iteration 23/25 | Loss: 0.00133462
Iteration 24/25 | Loss: 0.00133394
Iteration 25/25 | Loss: 0.00133569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.00921535
Iteration 2/25 | Loss: 0.00202443
Iteration 3/25 | Loss: 0.00209571
Iteration 4/25 | Loss: 0.00209571
Iteration 5/25 | Loss: 0.00209571
Iteration 6/25 | Loss: 0.00209571
Iteration 7/25 | Loss: 0.00209571
Iteration 8/25 | Loss: 0.00209571
Iteration 9/25 | Loss: 0.00209570
Iteration 10/25 | Loss: 0.00209570
Iteration 11/25 | Loss: 0.00209570
Iteration 12/25 | Loss: 0.00209570
Iteration 13/25 | Loss: 0.00209570
Iteration 14/25 | Loss: 0.00209570
Iteration 15/25 | Loss: 0.00209570
Iteration 16/25 | Loss: 0.00209570
Iteration 17/25 | Loss: 0.00209570
Iteration 18/25 | Loss: 0.00209570
Iteration 19/25 | Loss: 0.00209570
Iteration 20/25 | Loss: 0.00209570
Iteration 21/25 | Loss: 0.00209570
Iteration 22/25 | Loss: 0.00209570
Iteration 23/25 | Loss: 0.00209570
Iteration 24/25 | Loss: 0.00209570
Iteration 25/25 | Loss: 0.00209570

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209570
Iteration 2/1000 | Loss: 0.00008924
Iteration 3/1000 | Loss: 0.00012094
Iteration 4/1000 | Loss: 0.00016858
Iteration 5/1000 | Loss: 0.00007568
Iteration 6/1000 | Loss: 0.00004809
Iteration 7/1000 | Loss: 0.00015087
Iteration 8/1000 | Loss: 0.00016606
Iteration 9/1000 | Loss: 0.00011102
Iteration 10/1000 | Loss: 0.00005367
Iteration 11/1000 | Loss: 0.00016343
Iteration 12/1000 | Loss: 0.00006038
Iteration 13/1000 | Loss: 0.00010183
Iteration 14/1000 | Loss: 0.00019211
Iteration 15/1000 | Loss: 0.00016177
Iteration 16/1000 | Loss: 0.00014547
Iteration 17/1000 | Loss: 0.00012275
Iteration 18/1000 | Loss: 0.00007612
Iteration 19/1000 | Loss: 0.00005991
Iteration 20/1000 | Loss: 0.00012150
Iteration 21/1000 | Loss: 0.00014683
Iteration 22/1000 | Loss: 0.00009856
Iteration 23/1000 | Loss: 0.00009770
Iteration 24/1000 | Loss: 0.00009143
Iteration 25/1000 | Loss: 0.00008696
Iteration 26/1000 | Loss: 0.00009107
Iteration 27/1000 | Loss: 0.00017982
Iteration 28/1000 | Loss: 0.00013688
Iteration 29/1000 | Loss: 0.00033042
Iteration 30/1000 | Loss: 0.00015883
Iteration 31/1000 | Loss: 0.00020898
Iteration 32/1000 | Loss: 0.00010083
Iteration 33/1000 | Loss: 0.00011188
Iteration 34/1000 | Loss: 0.00008964
Iteration 35/1000 | Loss: 0.00010659
Iteration 36/1000 | Loss: 0.00007193
Iteration 37/1000 | Loss: 0.00008564
Iteration 38/1000 | Loss: 0.00009448
Iteration 39/1000 | Loss: 0.00007406
Iteration 40/1000 | Loss: 0.00007891
Iteration 41/1000 | Loss: 0.00006986
Iteration 42/1000 | Loss: 0.00011786
Iteration 43/1000 | Loss: 0.00004167
Iteration 44/1000 | Loss: 0.00012903
Iteration 45/1000 | Loss: 0.00014273
Iteration 46/1000 | Loss: 0.00005593
Iteration 47/1000 | Loss: 0.00013685
Iteration 48/1000 | Loss: 0.00016193
Iteration 49/1000 | Loss: 0.00017366
Iteration 50/1000 | Loss: 0.00009188
Iteration 51/1000 | Loss: 0.00005791
Iteration 52/1000 | Loss: 0.00004079
Iteration 53/1000 | Loss: 0.00017205
Iteration 54/1000 | Loss: 0.00007724
Iteration 55/1000 | Loss: 0.00015783
Iteration 56/1000 | Loss: 0.00016025
Iteration 57/1000 | Loss: 0.00010007
Iteration 58/1000 | Loss: 0.00011741
Iteration 59/1000 | Loss: 0.00013315
Iteration 60/1000 | Loss: 0.00004583
Iteration 61/1000 | Loss: 0.00009897
Iteration 62/1000 | Loss: 0.00015171
Iteration 63/1000 | Loss: 0.00019037
Iteration 64/1000 | Loss: 0.00016370
Iteration 65/1000 | Loss: 0.00014735
Iteration 66/1000 | Loss: 0.00007317
Iteration 67/1000 | Loss: 0.00004911
Iteration 68/1000 | Loss: 0.00024723
Iteration 69/1000 | Loss: 0.00014240
Iteration 70/1000 | Loss: 0.00025843
Iteration 71/1000 | Loss: 0.00014650
Iteration 72/1000 | Loss: 0.00019010
Iteration 73/1000 | Loss: 0.00016892
Iteration 74/1000 | Loss: 0.00006966
Iteration 75/1000 | Loss: 0.00010474
Iteration 76/1000 | Loss: 0.00008451
Iteration 77/1000 | Loss: 0.00008596
Iteration 78/1000 | Loss: 0.00007442
Iteration 79/1000 | Loss: 0.00009108
Iteration 80/1000 | Loss: 0.00009396
Iteration 81/1000 | Loss: 0.00006826
Iteration 82/1000 | Loss: 0.00008802
Iteration 83/1000 | Loss: 0.00007121
Iteration 84/1000 | Loss: 0.00004396
Iteration 85/1000 | Loss: 0.00058541
Iteration 86/1000 | Loss: 0.00047295
Iteration 87/1000 | Loss: 0.00004987
Iteration 88/1000 | Loss: 0.00003685
Iteration 89/1000 | Loss: 0.00003145
Iteration 90/1000 | Loss: 0.00002993
Iteration 91/1000 | Loss: 0.00002901
Iteration 92/1000 | Loss: 0.00002842
Iteration 93/1000 | Loss: 0.00002803
Iteration 94/1000 | Loss: 0.00002771
Iteration 95/1000 | Loss: 0.00002759
Iteration 96/1000 | Loss: 0.00002751
Iteration 97/1000 | Loss: 0.00002749
Iteration 98/1000 | Loss: 0.00003980
Iteration 99/1000 | Loss: 0.00003443
Iteration 100/1000 | Loss: 0.00003129
Iteration 101/1000 | Loss: 0.00003384
Iteration 102/1000 | Loss: 0.00003058
Iteration 103/1000 | Loss: 0.00003303
Iteration 104/1000 | Loss: 0.00002807
Iteration 105/1000 | Loss: 0.00002702
Iteration 106/1000 | Loss: 0.00002662
Iteration 107/1000 | Loss: 0.00002644
Iteration 108/1000 | Loss: 0.00002643
Iteration 109/1000 | Loss: 0.00002642
Iteration 110/1000 | Loss: 0.00002639
Iteration 111/1000 | Loss: 0.00003737
Iteration 112/1000 | Loss: 0.00002998
Iteration 113/1000 | Loss: 0.00002885
Iteration 114/1000 | Loss: 0.00002828
Iteration 115/1000 | Loss: 0.00002785
Iteration 116/1000 | Loss: 0.00002739
Iteration 117/1000 | Loss: 0.00002709
Iteration 118/1000 | Loss: 0.00002706
Iteration 119/1000 | Loss: 0.00002672
Iteration 120/1000 | Loss: 0.00003395
Iteration 121/1000 | Loss: 0.00002891
Iteration 122/1000 | Loss: 0.00002752
Iteration 123/1000 | Loss: 0.00002701
Iteration 124/1000 | Loss: 0.00002693
Iteration 125/1000 | Loss: 0.00002679
Iteration 126/1000 | Loss: 0.00002678
Iteration 127/1000 | Loss: 0.00002672
Iteration 128/1000 | Loss: 0.00002671
Iteration 129/1000 | Loss: 0.00002670
Iteration 130/1000 | Loss: 0.00002669
Iteration 131/1000 | Loss: 0.00002669
Iteration 132/1000 | Loss: 0.00002669
Iteration 133/1000 | Loss: 0.00002668
Iteration 134/1000 | Loss: 0.00002665
Iteration 135/1000 | Loss: 0.00002664
Iteration 136/1000 | Loss: 0.00002664
Iteration 137/1000 | Loss: 0.00002664
Iteration 138/1000 | Loss: 0.00002663
Iteration 139/1000 | Loss: 0.00002663
Iteration 140/1000 | Loss: 0.00002662
Iteration 141/1000 | Loss: 0.00002662
Iteration 142/1000 | Loss: 0.00002661
Iteration 143/1000 | Loss: 0.00002660
Iteration 144/1000 | Loss: 0.00002660
Iteration 145/1000 | Loss: 0.00002659
Iteration 146/1000 | Loss: 0.00002659
Iteration 147/1000 | Loss: 0.00002658
Iteration 148/1000 | Loss: 0.00002658
Iteration 149/1000 | Loss: 0.00002658
Iteration 150/1000 | Loss: 0.00002658
Iteration 151/1000 | Loss: 0.00002657
Iteration 152/1000 | Loss: 0.00002657
Iteration 153/1000 | Loss: 0.00002656
Iteration 154/1000 | Loss: 0.00002656
Iteration 155/1000 | Loss: 0.00002655
Iteration 156/1000 | Loss: 0.00002655
Iteration 157/1000 | Loss: 0.00002654
Iteration 158/1000 | Loss: 0.00002653
Iteration 159/1000 | Loss: 0.00002653
Iteration 160/1000 | Loss: 0.00002653
Iteration 161/1000 | Loss: 0.00002652
Iteration 162/1000 | Loss: 0.00002652
Iteration 163/1000 | Loss: 0.00002651
Iteration 164/1000 | Loss: 0.00002651
Iteration 165/1000 | Loss: 0.00002651
Iteration 166/1000 | Loss: 0.00002651
Iteration 167/1000 | Loss: 0.00002650
Iteration 168/1000 | Loss: 0.00002650
Iteration 169/1000 | Loss: 0.00002650
Iteration 170/1000 | Loss: 0.00002649
Iteration 171/1000 | Loss: 0.00002649
Iteration 172/1000 | Loss: 0.00002649
Iteration 173/1000 | Loss: 0.00002649
Iteration 174/1000 | Loss: 0.00002648
Iteration 175/1000 | Loss: 0.00002648
Iteration 176/1000 | Loss: 0.00002648
Iteration 177/1000 | Loss: 0.00002647
Iteration 178/1000 | Loss: 0.00002647
Iteration 179/1000 | Loss: 0.00002647
Iteration 180/1000 | Loss: 0.00002646
Iteration 181/1000 | Loss: 0.00002646
Iteration 182/1000 | Loss: 0.00002646
Iteration 183/1000 | Loss: 0.00002646
Iteration 184/1000 | Loss: 0.00002645
Iteration 185/1000 | Loss: 0.00002645
Iteration 186/1000 | Loss: 0.00002644
Iteration 187/1000 | Loss: 0.00002644
Iteration 188/1000 | Loss: 0.00002644
Iteration 189/1000 | Loss: 0.00002643
Iteration 190/1000 | Loss: 0.00003221
Iteration 191/1000 | Loss: 0.00003520
Iteration 192/1000 | Loss: 0.00003860
Iteration 193/1000 | Loss: 0.00003033
Iteration 194/1000 | Loss: 0.00002796
Iteration 195/1000 | Loss: 0.00002706
Iteration 196/1000 | Loss: 0.00002654
Iteration 197/1000 | Loss: 0.00002577
Iteration 198/1000 | Loss: 0.00002546
Iteration 199/1000 | Loss: 0.00002540
Iteration 200/1000 | Loss: 0.00002533
Iteration 201/1000 | Loss: 0.00002532
Iteration 202/1000 | Loss: 0.00002525
Iteration 203/1000 | Loss: 0.00002525
Iteration 204/1000 | Loss: 0.00002525
Iteration 205/1000 | Loss: 0.00002524
Iteration 206/1000 | Loss: 0.00002523
Iteration 207/1000 | Loss: 0.00002523
Iteration 208/1000 | Loss: 0.00002522
Iteration 209/1000 | Loss: 0.00002522
Iteration 210/1000 | Loss: 0.00002521
Iteration 211/1000 | Loss: 0.00002521
Iteration 212/1000 | Loss: 0.00002521
Iteration 213/1000 | Loss: 0.00002520
Iteration 214/1000 | Loss: 0.00002520
Iteration 215/1000 | Loss: 0.00002520
Iteration 216/1000 | Loss: 0.00002519
Iteration 217/1000 | Loss: 0.00002519
Iteration 218/1000 | Loss: 0.00002518
Iteration 219/1000 | Loss: 0.00002518
Iteration 220/1000 | Loss: 0.00002518
Iteration 221/1000 | Loss: 0.00002518
Iteration 222/1000 | Loss: 0.00002518
Iteration 223/1000 | Loss: 0.00002518
Iteration 224/1000 | Loss: 0.00002518
Iteration 225/1000 | Loss: 0.00002517
Iteration 226/1000 | Loss: 0.00002517
Iteration 227/1000 | Loss: 0.00002516
Iteration 228/1000 | Loss: 0.00002516
Iteration 229/1000 | Loss: 0.00002516
Iteration 230/1000 | Loss: 0.00002515
Iteration 231/1000 | Loss: 0.00002515
Iteration 232/1000 | Loss: 0.00002515
Iteration 233/1000 | Loss: 0.00002515
Iteration 234/1000 | Loss: 0.00002515
Iteration 235/1000 | Loss: 0.00002515
Iteration 236/1000 | Loss: 0.00002514
Iteration 237/1000 | Loss: 0.00002514
Iteration 238/1000 | Loss: 0.00002514
Iteration 239/1000 | Loss: 0.00002514
Iteration 240/1000 | Loss: 0.00002514
Iteration 241/1000 | Loss: 0.00002514
Iteration 242/1000 | Loss: 0.00002514
Iteration 243/1000 | Loss: 0.00002514
Iteration 244/1000 | Loss: 0.00002514
Iteration 245/1000 | Loss: 0.00002514
Iteration 246/1000 | Loss: 0.00002513
Iteration 247/1000 | Loss: 0.00002513
Iteration 248/1000 | Loss: 0.00002513
Iteration 249/1000 | Loss: 0.00002513
Iteration 250/1000 | Loss: 0.00002513
Iteration 251/1000 | Loss: 0.00002513
Iteration 252/1000 | Loss: 0.00002513
Iteration 253/1000 | Loss: 0.00002512
Iteration 254/1000 | Loss: 0.00002512
Iteration 255/1000 | Loss: 0.00002512
Iteration 256/1000 | Loss: 0.00002512
Iteration 257/1000 | Loss: 0.00002512
Iteration 258/1000 | Loss: 0.00002512
Iteration 259/1000 | Loss: 0.00002512
Iteration 260/1000 | Loss: 0.00002511
Iteration 261/1000 | Loss: 0.00002511
Iteration 262/1000 | Loss: 0.00002511
Iteration 263/1000 | Loss: 0.00002511
Iteration 264/1000 | Loss: 0.00002511
Iteration 265/1000 | Loss: 0.00002511
Iteration 266/1000 | Loss: 0.00002510
Iteration 267/1000 | Loss: 0.00002510
Iteration 268/1000 | Loss: 0.00002510
Iteration 269/1000 | Loss: 0.00002510
Iteration 270/1000 | Loss: 0.00002510
Iteration 271/1000 | Loss: 0.00002509
Iteration 272/1000 | Loss: 0.00002509
Iteration 273/1000 | Loss: 0.00002509
Iteration 274/1000 | Loss: 0.00002509
Iteration 275/1000 | Loss: 0.00002509
Iteration 276/1000 | Loss: 0.00002509
Iteration 277/1000 | Loss: 0.00002509
Iteration 278/1000 | Loss: 0.00002509
Iteration 279/1000 | Loss: 0.00002509
Iteration 280/1000 | Loss: 0.00002508
Iteration 281/1000 | Loss: 0.00002508
Iteration 282/1000 | Loss: 0.00002508
Iteration 283/1000 | Loss: 0.00002508
Iteration 284/1000 | Loss: 0.00002508
Iteration 285/1000 | Loss: 0.00002508
Iteration 286/1000 | Loss: 0.00002508
Iteration 287/1000 | Loss: 0.00002508
Iteration 288/1000 | Loss: 0.00002508
Iteration 289/1000 | Loss: 0.00002507
Iteration 290/1000 | Loss: 0.00002507
Iteration 291/1000 | Loss: 0.00002507
Iteration 292/1000 | Loss: 0.00002507
Iteration 293/1000 | Loss: 0.00002506
Iteration 294/1000 | Loss: 0.00002506
Iteration 295/1000 | Loss: 0.00002506
Iteration 296/1000 | Loss: 0.00002505
Iteration 297/1000 | Loss: 0.00002505
Iteration 298/1000 | Loss: 0.00002505
Iteration 299/1000 | Loss: 0.00002505
Iteration 300/1000 | Loss: 0.00002505
Iteration 301/1000 | Loss: 0.00002505
Iteration 302/1000 | Loss: 0.00002505
Iteration 303/1000 | Loss: 0.00002505
Iteration 304/1000 | Loss: 0.00002505
Iteration 305/1000 | Loss: 0.00002505
Iteration 306/1000 | Loss: 0.00002504
Iteration 307/1000 | Loss: 0.00002504
Iteration 308/1000 | Loss: 0.00002504
Iteration 309/1000 | Loss: 0.00002504
Iteration 310/1000 | Loss: 0.00002504
Iteration 311/1000 | Loss: 0.00002504
Iteration 312/1000 | Loss: 0.00002504
Iteration 313/1000 | Loss: 0.00002504
Iteration 314/1000 | Loss: 0.00002504
Iteration 315/1000 | Loss: 0.00002504
Iteration 316/1000 | Loss: 0.00002504
Iteration 317/1000 | Loss: 0.00002504
Iteration 318/1000 | Loss: 0.00002504
Iteration 319/1000 | Loss: 0.00002504
Iteration 320/1000 | Loss: 0.00002504
Iteration 321/1000 | Loss: 0.00002504
Iteration 322/1000 | Loss: 0.00002504
Iteration 323/1000 | Loss: 0.00002504
Iteration 324/1000 | Loss: 0.00002504
Iteration 325/1000 | Loss: 0.00002504
Iteration 326/1000 | Loss: 0.00002504
Iteration 327/1000 | Loss: 0.00002504
Iteration 328/1000 | Loss: 0.00002504
Iteration 329/1000 | Loss: 0.00002504
Iteration 330/1000 | Loss: 0.00002504
Iteration 331/1000 | Loss: 0.00002504
Iteration 332/1000 | Loss: 0.00002504
Iteration 333/1000 | Loss: 0.00002504
Iteration 334/1000 | Loss: 0.00002504
Iteration 335/1000 | Loss: 0.00002504
Iteration 336/1000 | Loss: 0.00002504
Iteration 337/1000 | Loss: 0.00002504
Iteration 338/1000 | Loss: 0.00002504
Iteration 339/1000 | Loss: 0.00002504
Iteration 340/1000 | Loss: 0.00002504
Iteration 341/1000 | Loss: 0.00002504
Iteration 342/1000 | Loss: 0.00002504
Iteration 343/1000 | Loss: 0.00002504
Iteration 344/1000 | Loss: 0.00002504
Iteration 345/1000 | Loss: 0.00002504
Iteration 346/1000 | Loss: 0.00002504
Iteration 347/1000 | Loss: 0.00002504
Iteration 348/1000 | Loss: 0.00002504
Iteration 349/1000 | Loss: 0.00002504
Iteration 350/1000 | Loss: 0.00002504
Iteration 351/1000 | Loss: 0.00002504
Iteration 352/1000 | Loss: 0.00002504
Iteration 353/1000 | Loss: 0.00002504
Iteration 354/1000 | Loss: 0.00002504
Iteration 355/1000 | Loss: 0.00002504
Iteration 356/1000 | Loss: 0.00002504
Iteration 357/1000 | Loss: 0.00002504
Iteration 358/1000 | Loss: 0.00002504
Iteration 359/1000 | Loss: 0.00002504
Iteration 360/1000 | Loss: 0.00002504
Iteration 361/1000 | Loss: 0.00002504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 361. Stopping optimization.
Last 5 losses: [2.5042247216333635e-05, 2.5042247216333635e-05, 2.5042247216333635e-05, 2.5042247216333635e-05, 2.5042247216333635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5042247216333635e-05

Optimization complete. Final v2v error: 4.156332015991211 mm

Highest mean error: 5.671008586883545 mm for frame 36

Lowest mean error: 2.8664681911468506 mm for frame 71

Saving results

Total time: 245.90594005584717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005404
Iteration 2/25 | Loss: 0.01005404
Iteration 3/25 | Loss: 0.00294134
Iteration 4/25 | Loss: 0.00220007
Iteration 5/25 | Loss: 0.00194803
Iteration 6/25 | Loss: 0.00162408
Iteration 7/25 | Loss: 0.00161193
Iteration 8/25 | Loss: 0.00146536
Iteration 9/25 | Loss: 0.00140461
Iteration 10/25 | Loss: 0.00138989
Iteration 11/25 | Loss: 0.00136434
Iteration 12/25 | Loss: 0.00134853
Iteration 13/25 | Loss: 0.00134919
Iteration 14/25 | Loss: 0.00134175
Iteration 15/25 | Loss: 0.00134137
Iteration 16/25 | Loss: 0.00134123
Iteration 17/25 | Loss: 0.00134410
Iteration 18/25 | Loss: 0.00134288
Iteration 19/25 | Loss: 0.00134115
Iteration 20/25 | Loss: 0.00134115
Iteration 21/25 | Loss: 0.00134114
Iteration 22/25 | Loss: 0.00134114
Iteration 23/25 | Loss: 0.00134114
Iteration 24/25 | Loss: 0.00134114
Iteration 25/25 | Loss: 0.00134114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36983597
Iteration 2/25 | Loss: 0.00153479
Iteration 3/25 | Loss: 0.00153478
Iteration 4/25 | Loss: 0.00153478
Iteration 5/25 | Loss: 0.00153478
Iteration 6/25 | Loss: 0.00153478
Iteration 7/25 | Loss: 0.00153477
Iteration 8/25 | Loss: 0.00153477
Iteration 9/25 | Loss: 0.00153477
Iteration 10/25 | Loss: 0.00153477
Iteration 11/25 | Loss: 0.00153477
Iteration 12/25 | Loss: 0.00153477
Iteration 13/25 | Loss: 0.00153477
Iteration 14/25 | Loss: 0.00153477
Iteration 15/25 | Loss: 0.00153477
Iteration 16/25 | Loss: 0.00153477
Iteration 17/25 | Loss: 0.00153477
Iteration 18/25 | Loss: 0.00153477
Iteration 19/25 | Loss: 0.00153477
Iteration 20/25 | Loss: 0.00153477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015347726875916123, 0.0015347726875916123, 0.0015347726875916123, 0.0015347726875916123, 0.0015347726875916123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015347726875916123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153477
Iteration 2/1000 | Loss: 0.00005480
Iteration 3/1000 | Loss: 0.00004281
Iteration 4/1000 | Loss: 0.00007393
Iteration 5/1000 | Loss: 0.00011451
Iteration 6/1000 | Loss: 0.00003619
Iteration 7/1000 | Loss: 0.00003518
Iteration 8/1000 | Loss: 0.00003420
Iteration 9/1000 | Loss: 0.00007673
Iteration 10/1000 | Loss: 0.00003282
Iteration 11/1000 | Loss: 0.00003223
Iteration 12/1000 | Loss: 0.00079950
Iteration 13/1000 | Loss: 0.00086124
Iteration 14/1000 | Loss: 0.00020258
Iteration 15/1000 | Loss: 0.00003621
Iteration 16/1000 | Loss: 0.00007186
Iteration 17/1000 | Loss: 0.00005230
Iteration 18/1000 | Loss: 0.00004753
Iteration 19/1000 | Loss: 0.00004177
Iteration 20/1000 | Loss: 0.00002214
Iteration 21/1000 | Loss: 0.00004980
Iteration 22/1000 | Loss: 0.00013530
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00004369
Iteration 25/1000 | Loss: 0.00003468
Iteration 26/1000 | Loss: 0.00003805
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00012746
Iteration 29/1000 | Loss: 0.00009499
Iteration 30/1000 | Loss: 0.00004942
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00005759
Iteration 33/1000 | Loss: 0.00002099
Iteration 34/1000 | Loss: 0.00002194
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00003068
Iteration 38/1000 | Loss: 0.00001690
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00003532
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00007308
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001666
Iteration 66/1000 | Loss: 0.00001666
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001665
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001665
Iteration 71/1000 | Loss: 0.00001665
Iteration 72/1000 | Loss: 0.00001665
Iteration 73/1000 | Loss: 0.00001665
Iteration 74/1000 | Loss: 0.00001664
Iteration 75/1000 | Loss: 0.00001664
Iteration 76/1000 | Loss: 0.00001664
Iteration 77/1000 | Loss: 0.00001664
Iteration 78/1000 | Loss: 0.00001664
Iteration 79/1000 | Loss: 0.00001664
Iteration 80/1000 | Loss: 0.00001664
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001663
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001662
Iteration 93/1000 | Loss: 0.00001662
Iteration 94/1000 | Loss: 0.00001662
Iteration 95/1000 | Loss: 0.00001662
Iteration 96/1000 | Loss: 0.00001662
Iteration 97/1000 | Loss: 0.00001661
Iteration 98/1000 | Loss: 0.00001661
Iteration 99/1000 | Loss: 0.00001661
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001658
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6569199942750856e-05, 1.6569199942750856e-05, 1.6569199942750856e-05, 1.6569199942750856e-05, 1.6569199942750856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6569199942750856e-05

Optimization complete. Final v2v error: 3.4830684661865234 mm

Highest mean error: 4.052122116088867 mm for frame 234

Lowest mean error: 3.1785831451416016 mm for frame 129

Saving results

Total time: 107.6332643032074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766493
Iteration 2/25 | Loss: 0.00159509
Iteration 3/25 | Loss: 0.00131794
Iteration 4/25 | Loss: 0.00129606
Iteration 5/25 | Loss: 0.00129401
Iteration 6/25 | Loss: 0.00129401
Iteration 7/25 | Loss: 0.00129401
Iteration 8/25 | Loss: 0.00129401
Iteration 9/25 | Loss: 0.00129401
Iteration 10/25 | Loss: 0.00129401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012940072920173407, 0.0012940072920173407, 0.0012940072920173407, 0.0012940072920173407, 0.0012940072920173407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012940072920173407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27164912
Iteration 2/25 | Loss: 0.00113541
Iteration 3/25 | Loss: 0.00113541
Iteration 4/25 | Loss: 0.00113541
Iteration 5/25 | Loss: 0.00113541
Iteration 6/25 | Loss: 0.00113541
Iteration 7/25 | Loss: 0.00113541
Iteration 8/25 | Loss: 0.00113541
Iteration 9/25 | Loss: 0.00113541
Iteration 10/25 | Loss: 0.00113540
Iteration 11/25 | Loss: 0.00113540
Iteration 12/25 | Loss: 0.00113540
Iteration 13/25 | Loss: 0.00113540
Iteration 14/25 | Loss: 0.00113540
Iteration 15/25 | Loss: 0.00113540
Iteration 16/25 | Loss: 0.00113540
Iteration 17/25 | Loss: 0.00113540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011354045709595084, 0.0011354045709595084, 0.0011354045709595084, 0.0011354045709595084, 0.0011354045709595084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011354045709595084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113540
Iteration 2/1000 | Loss: 0.00002888
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001646
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001411
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001396
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001386
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001383
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.3313129784364719e-05, 1.3313129784364719e-05, 1.3313129784364719e-05, 1.3313129784364719e-05, 1.3313129784364719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3313129784364719e-05

Optimization complete. Final v2v error: 3.097663164138794 mm

Highest mean error: 3.441318988800049 mm for frame 8

Lowest mean error: 2.99658465385437 mm for frame 141

Saving results

Total time: 43.5113205909729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773272
Iteration 2/25 | Loss: 0.00182722
Iteration 3/25 | Loss: 0.00150874
Iteration 4/25 | Loss: 0.00145998
Iteration 5/25 | Loss: 0.00144151
Iteration 6/25 | Loss: 0.00144941
Iteration 7/25 | Loss: 0.00143173
Iteration 8/25 | Loss: 0.00143349
Iteration 9/25 | Loss: 0.00143501
Iteration 10/25 | Loss: 0.00143806
Iteration 11/25 | Loss: 0.00143890
Iteration 12/25 | Loss: 0.00141078
Iteration 13/25 | Loss: 0.00142391
Iteration 14/25 | Loss: 0.00141717
Iteration 15/25 | Loss: 0.00141350
Iteration 16/25 | Loss: 0.00141179
Iteration 17/25 | Loss: 0.00139691
Iteration 18/25 | Loss: 0.00138294
Iteration 19/25 | Loss: 0.00138273
Iteration 20/25 | Loss: 0.00137596
Iteration 21/25 | Loss: 0.00137478
Iteration 22/25 | Loss: 0.00138048
Iteration 23/25 | Loss: 0.00137950
Iteration 24/25 | Loss: 0.00138287
Iteration 25/25 | Loss: 0.00137845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.79455709
Iteration 2/25 | Loss: 0.00188318
Iteration 3/25 | Loss: 0.00188299
Iteration 4/25 | Loss: 0.00188298
Iteration 5/25 | Loss: 0.00188298
Iteration 6/25 | Loss: 0.00188298
Iteration 7/25 | Loss: 0.00188298
Iteration 8/25 | Loss: 0.00188298
Iteration 9/25 | Loss: 0.00188298
Iteration 10/25 | Loss: 0.00188298
Iteration 11/25 | Loss: 0.00188298
Iteration 12/25 | Loss: 0.00188298
Iteration 13/25 | Loss: 0.00188298
Iteration 14/25 | Loss: 0.00188298
Iteration 15/25 | Loss: 0.00188298
Iteration 16/25 | Loss: 0.00188298
Iteration 17/25 | Loss: 0.00188298
Iteration 18/25 | Loss: 0.00188298
Iteration 19/25 | Loss: 0.00188298
Iteration 20/25 | Loss: 0.00188298
Iteration 21/25 | Loss: 0.00188298
Iteration 22/25 | Loss: 0.00188298
Iteration 23/25 | Loss: 0.00188298
Iteration 24/25 | Loss: 0.00188298
Iteration 25/25 | Loss: 0.00188298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188298
Iteration 2/1000 | Loss: 0.00021787
Iteration 3/1000 | Loss: 0.00011478
Iteration 4/1000 | Loss: 0.00896775
Iteration 5/1000 | Loss: 0.00945542
Iteration 6/1000 | Loss: 0.00051177
Iteration 7/1000 | Loss: 0.01108054
Iteration 8/1000 | Loss: 0.00238931
Iteration 9/1000 | Loss: 0.00192533
Iteration 10/1000 | Loss: 0.00017372
Iteration 11/1000 | Loss: 0.00020236
Iteration 12/1000 | Loss: 0.00037162
Iteration 13/1000 | Loss: 0.00012583
Iteration 14/1000 | Loss: 0.00007109
Iteration 15/1000 | Loss: 0.00105069
Iteration 16/1000 | Loss: 0.00096242
Iteration 17/1000 | Loss: 0.00057510
Iteration 18/1000 | Loss: 0.00126087
Iteration 19/1000 | Loss: 0.00118586
Iteration 20/1000 | Loss: 0.00121446
Iteration 21/1000 | Loss: 0.00100382
Iteration 22/1000 | Loss: 0.00105098
Iteration 23/1000 | Loss: 0.00092686
Iteration 24/1000 | Loss: 0.00097909
Iteration 25/1000 | Loss: 0.00051515
Iteration 26/1000 | Loss: 0.00091153
Iteration 27/1000 | Loss: 0.00063216
Iteration 28/1000 | Loss: 0.00063297
Iteration 29/1000 | Loss: 0.00084134
Iteration 30/1000 | Loss: 0.00040155
Iteration 31/1000 | Loss: 0.00051640
Iteration 32/1000 | Loss: 0.00049300
Iteration 33/1000 | Loss: 0.00071089
Iteration 34/1000 | Loss: 0.00057925
Iteration 35/1000 | Loss: 0.00027226
Iteration 36/1000 | Loss: 0.00043016
Iteration 37/1000 | Loss: 0.00034002
Iteration 38/1000 | Loss: 0.00018991
Iteration 39/1000 | Loss: 0.00029226
Iteration 40/1000 | Loss: 0.00040139
Iteration 41/1000 | Loss: 0.00055585
Iteration 42/1000 | Loss: 0.00062272
Iteration 43/1000 | Loss: 0.00054736
Iteration 44/1000 | Loss: 0.00051825
Iteration 45/1000 | Loss: 0.00035638
Iteration 46/1000 | Loss: 0.00040291
Iteration 47/1000 | Loss: 0.00052677
Iteration 48/1000 | Loss: 0.00053483
Iteration 49/1000 | Loss: 0.00053012
Iteration 50/1000 | Loss: 0.00046774
Iteration 51/1000 | Loss: 0.00051348
Iteration 52/1000 | Loss: 0.00034118
Iteration 53/1000 | Loss: 0.00026760
Iteration 54/1000 | Loss: 0.00017565
Iteration 55/1000 | Loss: 0.00007503
Iteration 56/1000 | Loss: 0.00030143
Iteration 57/1000 | Loss: 0.00041537
Iteration 58/1000 | Loss: 0.00033148
Iteration 59/1000 | Loss: 0.00037077
Iteration 60/1000 | Loss: 0.00033442
Iteration 61/1000 | Loss: 0.00064095
Iteration 62/1000 | Loss: 0.00050677
Iteration 63/1000 | Loss: 0.00050038
Iteration 64/1000 | Loss: 0.00033831
Iteration 65/1000 | Loss: 0.00044298
Iteration 66/1000 | Loss: 0.00014536
Iteration 67/1000 | Loss: 0.00027281
Iteration 68/1000 | Loss: 0.00017613
Iteration 69/1000 | Loss: 0.00038051
Iteration 70/1000 | Loss: 0.00037643
Iteration 71/1000 | Loss: 0.00033012
Iteration 72/1000 | Loss: 0.00044347
Iteration 73/1000 | Loss: 0.00031560
Iteration 74/1000 | Loss: 0.00022414
Iteration 75/1000 | Loss: 0.00031447
Iteration 76/1000 | Loss: 0.00020640
Iteration 77/1000 | Loss: 0.00025874
Iteration 78/1000 | Loss: 0.00044948
Iteration 79/1000 | Loss: 0.00014542
Iteration 80/1000 | Loss: 0.00038299
Iteration 81/1000 | Loss: 0.00017615
Iteration 82/1000 | Loss: 0.00017466
Iteration 83/1000 | Loss: 0.00021180
Iteration 84/1000 | Loss: 0.00014425
Iteration 85/1000 | Loss: 0.00027389
Iteration 86/1000 | Loss: 0.00018676
Iteration 87/1000 | Loss: 0.00251622
Iteration 88/1000 | Loss: 0.00116534
Iteration 89/1000 | Loss: 0.00017146
Iteration 90/1000 | Loss: 0.00031357
Iteration 91/1000 | Loss: 0.00026178
Iteration 92/1000 | Loss: 0.00017747
Iteration 93/1000 | Loss: 0.00032221
Iteration 94/1000 | Loss: 0.00030546
Iteration 95/1000 | Loss: 0.00021979
Iteration 96/1000 | Loss: 0.00021410
Iteration 97/1000 | Loss: 0.00021394
Iteration 98/1000 | Loss: 0.00019822
Iteration 99/1000 | Loss: 0.00007049
Iteration 100/1000 | Loss: 0.00029869
Iteration 101/1000 | Loss: 0.00121505
Iteration 102/1000 | Loss: 0.00070285
Iteration 103/1000 | Loss: 0.00134526
Iteration 104/1000 | Loss: 0.00063840
Iteration 105/1000 | Loss: 0.00027581
Iteration 106/1000 | Loss: 0.00028267
Iteration 107/1000 | Loss: 0.00026616
Iteration 108/1000 | Loss: 0.00030005
Iteration 109/1000 | Loss: 0.00020068
Iteration 110/1000 | Loss: 0.00025342
Iteration 111/1000 | Loss: 0.00019620
Iteration 112/1000 | Loss: 0.00026778
Iteration 113/1000 | Loss: 0.00022622
Iteration 114/1000 | Loss: 0.00027704
Iteration 115/1000 | Loss: 0.00040811
Iteration 116/1000 | Loss: 0.00041921
Iteration 117/1000 | Loss: 0.00025204
Iteration 118/1000 | Loss: 0.00024238
Iteration 119/1000 | Loss: 0.00025905
Iteration 120/1000 | Loss: 0.00026896
Iteration 121/1000 | Loss: 0.00029722
Iteration 122/1000 | Loss: 0.00035422
Iteration 123/1000 | Loss: 0.00028090
Iteration 124/1000 | Loss: 0.00024222
Iteration 125/1000 | Loss: 0.00034379
Iteration 126/1000 | Loss: 0.00039105
Iteration 127/1000 | Loss: 0.00045294
Iteration 128/1000 | Loss: 0.00026982
Iteration 129/1000 | Loss: 0.00033556
Iteration 130/1000 | Loss: 0.00039825
Iteration 131/1000 | Loss: 0.00030358
Iteration 132/1000 | Loss: 0.00033793
Iteration 133/1000 | Loss: 0.00025941
Iteration 134/1000 | Loss: 0.00033176
Iteration 135/1000 | Loss: 0.00035610
Iteration 136/1000 | Loss: 0.00029774
Iteration 137/1000 | Loss: 0.00039965
Iteration 138/1000 | Loss: 0.00035187
Iteration 139/1000 | Loss: 0.00032892
Iteration 140/1000 | Loss: 0.00042574
Iteration 141/1000 | Loss: 0.00017804
Iteration 142/1000 | Loss: 0.00027986
Iteration 143/1000 | Loss: 0.00043627
Iteration 144/1000 | Loss: 0.00026375
Iteration 145/1000 | Loss: 0.00039514
Iteration 146/1000 | Loss: 0.00033557
Iteration 147/1000 | Loss: 0.00030786
Iteration 148/1000 | Loss: 0.00029855
Iteration 149/1000 | Loss: 0.00025674
Iteration 150/1000 | Loss: 0.00022753
Iteration 151/1000 | Loss: 0.00025898
Iteration 152/1000 | Loss: 0.00030258
Iteration 153/1000 | Loss: 0.00033500
Iteration 154/1000 | Loss: 0.00034896
Iteration 155/1000 | Loss: 0.00041115
Iteration 156/1000 | Loss: 0.00030341
Iteration 157/1000 | Loss: 0.00033941
Iteration 158/1000 | Loss: 0.00018663
Iteration 159/1000 | Loss: 0.00010894
Iteration 160/1000 | Loss: 0.00021946
Iteration 161/1000 | Loss: 0.00021423
Iteration 162/1000 | Loss: 0.00031039
Iteration 163/1000 | Loss: 0.00024933
Iteration 164/1000 | Loss: 0.00029800
Iteration 165/1000 | Loss: 0.00030379
Iteration 166/1000 | Loss: 0.00031065
Iteration 167/1000 | Loss: 0.00023169
Iteration 168/1000 | Loss: 0.00037177
Iteration 169/1000 | Loss: 0.00032827
Iteration 170/1000 | Loss: 0.00037747
Iteration 171/1000 | Loss: 0.00033411
Iteration 172/1000 | Loss: 0.00038940
Iteration 173/1000 | Loss: 0.00037473
Iteration 174/1000 | Loss: 0.00010183
Iteration 175/1000 | Loss: 0.00025018
Iteration 176/1000 | Loss: 0.00031480
Iteration 177/1000 | Loss: 0.00014513
Iteration 178/1000 | Loss: 0.00026422
Iteration 179/1000 | Loss: 0.00051538
Iteration 180/1000 | Loss: 0.00025726
Iteration 181/1000 | Loss: 0.00022826
Iteration 182/1000 | Loss: 0.00016126
Iteration 183/1000 | Loss: 0.00023021
Iteration 184/1000 | Loss: 0.00022986
Iteration 185/1000 | Loss: 0.00245691
Iteration 186/1000 | Loss: 0.00113092
Iteration 187/1000 | Loss: 0.00015329
Iteration 188/1000 | Loss: 0.00015645
Iteration 189/1000 | Loss: 0.00010777
Iteration 190/1000 | Loss: 0.00011762
Iteration 191/1000 | Loss: 0.00015612
Iteration 192/1000 | Loss: 0.00014914
Iteration 193/1000 | Loss: 0.00015873
Iteration 194/1000 | Loss: 0.00014573
Iteration 195/1000 | Loss: 0.00015906
Iteration 196/1000 | Loss: 0.00014552
Iteration 197/1000 | Loss: 0.00021730
Iteration 198/1000 | Loss: 0.00014956
Iteration 199/1000 | Loss: 0.00017090
Iteration 200/1000 | Loss: 0.00013506
Iteration 201/1000 | Loss: 0.00013333
Iteration 202/1000 | Loss: 0.00023697
Iteration 203/1000 | Loss: 0.00018982
Iteration 204/1000 | Loss: 0.00014965
Iteration 205/1000 | Loss: 0.00017715
Iteration 206/1000 | Loss: 0.00015922
Iteration 207/1000 | Loss: 0.00017740
Iteration 208/1000 | Loss: 0.00015482
Iteration 209/1000 | Loss: 0.00017914
Iteration 210/1000 | Loss: 0.00027603
Iteration 211/1000 | Loss: 0.00023990
Iteration 212/1000 | Loss: 0.00018279
Iteration 213/1000 | Loss: 0.00017704
Iteration 214/1000 | Loss: 0.00016340
Iteration 215/1000 | Loss: 0.00019122
Iteration 216/1000 | Loss: 0.00033515
Iteration 217/1000 | Loss: 0.00021995
Iteration 218/1000 | Loss: 0.00018179
Iteration 219/1000 | Loss: 0.00013441
Iteration 220/1000 | Loss: 0.00020559
Iteration 221/1000 | Loss: 0.00018027
Iteration 222/1000 | Loss: 0.00017397
Iteration 223/1000 | Loss: 0.00102749
Iteration 224/1000 | Loss: 0.00051573
Iteration 225/1000 | Loss: 0.00029768
Iteration 226/1000 | Loss: 0.00004838
Iteration 227/1000 | Loss: 0.00003981
Iteration 228/1000 | Loss: 0.00014435
Iteration 229/1000 | Loss: 0.00003693
Iteration 230/1000 | Loss: 0.00003583
Iteration 231/1000 | Loss: 0.00012958
Iteration 232/1000 | Loss: 0.00008998
Iteration 233/1000 | Loss: 0.00010367
Iteration 234/1000 | Loss: 0.00008089
Iteration 235/1000 | Loss: 0.00004469
Iteration 236/1000 | Loss: 0.00004151
Iteration 237/1000 | Loss: 0.00003575
Iteration 238/1000 | Loss: 0.00015474
Iteration 239/1000 | Loss: 0.00003675
Iteration 240/1000 | Loss: 0.00027703
Iteration 241/1000 | Loss: 0.00234467
Iteration 242/1000 | Loss: 0.00102558
Iteration 243/1000 | Loss: 0.00013881
Iteration 244/1000 | Loss: 0.00028739
Iteration 245/1000 | Loss: 0.00027400
Iteration 246/1000 | Loss: 0.00006839
Iteration 247/1000 | Loss: 0.00026991
Iteration 248/1000 | Loss: 0.00018806
Iteration 249/1000 | Loss: 0.00015478
Iteration 250/1000 | Loss: 0.00018523
Iteration 251/1000 | Loss: 0.00003796
Iteration 252/1000 | Loss: 0.00003429
Iteration 253/1000 | Loss: 0.00013111
Iteration 254/1000 | Loss: 0.00004043
Iteration 255/1000 | Loss: 0.00003552
Iteration 256/1000 | Loss: 0.00003296
Iteration 257/1000 | Loss: 0.00004631
Iteration 258/1000 | Loss: 0.00003292
Iteration 259/1000 | Loss: 0.00003939
Iteration 260/1000 | Loss: 0.00004076
Iteration 261/1000 | Loss: 0.00003143
Iteration 262/1000 | Loss: 0.00003271
Iteration 263/1000 | Loss: 0.00003151
Iteration 264/1000 | Loss: 0.00003201
Iteration 265/1000 | Loss: 0.00003155
Iteration 266/1000 | Loss: 0.00003155
Iteration 267/1000 | Loss: 0.00003155
Iteration 268/1000 | Loss: 0.00003155
Iteration 269/1000 | Loss: 0.00003155
Iteration 270/1000 | Loss: 0.00003155
Iteration 271/1000 | Loss: 0.00003154
Iteration 272/1000 | Loss: 0.00003154
Iteration 273/1000 | Loss: 0.00003154
Iteration 274/1000 | Loss: 0.00003153
Iteration 275/1000 | Loss: 0.00003153
Iteration 276/1000 | Loss: 0.00003135
Iteration 277/1000 | Loss: 0.00003116
Iteration 278/1000 | Loss: 0.00003098
Iteration 279/1000 | Loss: 0.00228047
Iteration 280/1000 | Loss: 0.00111326
Iteration 281/1000 | Loss: 0.00323881
Iteration 282/1000 | Loss: 0.00361800
Iteration 283/1000 | Loss: 0.00285167
Iteration 284/1000 | Loss: 0.00310493
Iteration 285/1000 | Loss: 0.00147367
Iteration 286/1000 | Loss: 0.00340674
Iteration 287/1000 | Loss: 0.00162045
Iteration 288/1000 | Loss: 0.00290375
Iteration 289/1000 | Loss: 0.00186573
Iteration 290/1000 | Loss: 0.00270272
Iteration 291/1000 | Loss: 0.00129001
Iteration 292/1000 | Loss: 0.00420885
Iteration 293/1000 | Loss: 0.00240699
Iteration 294/1000 | Loss: 0.00199132
Iteration 295/1000 | Loss: 0.00071526
Iteration 296/1000 | Loss: 0.00162527
Iteration 297/1000 | Loss: 0.00155210
Iteration 298/1000 | Loss: 0.00344627
Iteration 299/1000 | Loss: 0.00039009
Iteration 300/1000 | Loss: 0.00259929
Iteration 301/1000 | Loss: 0.00307215
Iteration 302/1000 | Loss: 0.00226557
Iteration 303/1000 | Loss: 0.00015161
Iteration 304/1000 | Loss: 0.00132648
Iteration 305/1000 | Loss: 0.00412301
Iteration 306/1000 | Loss: 0.00341611
Iteration 307/1000 | Loss: 0.00220683
Iteration 308/1000 | Loss: 0.00069594
Iteration 309/1000 | Loss: 0.00161911
Iteration 310/1000 | Loss: 0.00011661
Iteration 311/1000 | Loss: 0.00006126
Iteration 312/1000 | Loss: 0.00096792
Iteration 313/1000 | Loss: 0.00096019
Iteration 314/1000 | Loss: 0.00020108
Iteration 315/1000 | Loss: 0.00005145
Iteration 316/1000 | Loss: 0.00003673
Iteration 317/1000 | Loss: 0.00003357
Iteration 318/1000 | Loss: 0.00003210
Iteration 319/1000 | Loss: 0.00003111
Iteration 320/1000 | Loss: 0.00003052
Iteration 321/1000 | Loss: 0.00003000
Iteration 322/1000 | Loss: 0.00002968
Iteration 323/1000 | Loss: 0.00002936
Iteration 324/1000 | Loss: 0.00002900
Iteration 325/1000 | Loss: 0.00286238
Iteration 326/1000 | Loss: 0.00012417
Iteration 327/1000 | Loss: 0.00006310
Iteration 328/1000 | Loss: 0.00003758
Iteration 329/1000 | Loss: 0.00002887
Iteration 330/1000 | Loss: 0.00002588
Iteration 331/1000 | Loss: 0.00002427
Iteration 332/1000 | Loss: 0.00002363
Iteration 333/1000 | Loss: 0.00002326
Iteration 334/1000 | Loss: 0.00002293
Iteration 335/1000 | Loss: 0.00002272
Iteration 336/1000 | Loss: 0.00002271
Iteration 337/1000 | Loss: 0.00002270
Iteration 338/1000 | Loss: 0.00002269
Iteration 339/1000 | Loss: 0.00002258
Iteration 340/1000 | Loss: 0.00002254
Iteration 341/1000 | Loss: 0.00002254
Iteration 342/1000 | Loss: 0.00002253
Iteration 343/1000 | Loss: 0.00002252
Iteration 344/1000 | Loss: 0.00002252
Iteration 345/1000 | Loss: 0.00002248
Iteration 346/1000 | Loss: 0.00002247
Iteration 347/1000 | Loss: 0.00002245
Iteration 348/1000 | Loss: 0.00002244
Iteration 349/1000 | Loss: 0.00002244
Iteration 350/1000 | Loss: 0.00002243
Iteration 351/1000 | Loss: 0.00002243
Iteration 352/1000 | Loss: 0.00002243
Iteration 353/1000 | Loss: 0.00002242
Iteration 354/1000 | Loss: 0.00002242
Iteration 355/1000 | Loss: 0.00002242
Iteration 356/1000 | Loss: 0.00002241
Iteration 357/1000 | Loss: 0.00002241
Iteration 358/1000 | Loss: 0.00002240
Iteration 359/1000 | Loss: 0.00002237
Iteration 360/1000 | Loss: 0.00002236
Iteration 361/1000 | Loss: 0.00002236
Iteration 362/1000 | Loss: 0.00002235
Iteration 363/1000 | Loss: 0.00002235
Iteration 364/1000 | Loss: 0.00002234
Iteration 365/1000 | Loss: 0.00002234
Iteration 366/1000 | Loss: 0.00002234
Iteration 367/1000 | Loss: 0.00002234
Iteration 368/1000 | Loss: 0.00002234
Iteration 369/1000 | Loss: 0.00002234
Iteration 370/1000 | Loss: 0.00002234
Iteration 371/1000 | Loss: 0.00002233
Iteration 372/1000 | Loss: 0.00002233
Iteration 373/1000 | Loss: 0.00002233
Iteration 374/1000 | Loss: 0.00002232
Iteration 375/1000 | Loss: 0.00002232
Iteration 376/1000 | Loss: 0.00002231
Iteration 377/1000 | Loss: 0.00002231
Iteration 378/1000 | Loss: 0.00002231
Iteration 379/1000 | Loss: 0.00002230
Iteration 380/1000 | Loss: 0.00002230
Iteration 381/1000 | Loss: 0.00002230
Iteration 382/1000 | Loss: 0.00002229
Iteration 383/1000 | Loss: 0.00002229
Iteration 384/1000 | Loss: 0.00002229
Iteration 385/1000 | Loss: 0.00002229
Iteration 386/1000 | Loss: 0.00002229
Iteration 387/1000 | Loss: 0.00002229
Iteration 388/1000 | Loss: 0.00002228
Iteration 389/1000 | Loss: 0.00002228
Iteration 390/1000 | Loss: 0.00002228
Iteration 391/1000 | Loss: 0.00002228
Iteration 392/1000 | Loss: 0.00002228
Iteration 393/1000 | Loss: 0.00002228
Iteration 394/1000 | Loss: 0.00002228
Iteration 395/1000 | Loss: 0.00002228
Iteration 396/1000 | Loss: 0.00002228
Iteration 397/1000 | Loss: 0.00002228
Iteration 398/1000 | Loss: 0.00002228
Iteration 399/1000 | Loss: 0.00002228
Iteration 400/1000 | Loss: 0.00002228
Iteration 401/1000 | Loss: 0.00002228
Iteration 402/1000 | Loss: 0.00002228
Iteration 403/1000 | Loss: 0.00002227
Iteration 404/1000 | Loss: 0.00002227
Iteration 405/1000 | Loss: 0.00002227
Iteration 406/1000 | Loss: 0.00002226
Iteration 407/1000 | Loss: 0.00002226
Iteration 408/1000 | Loss: 0.00002226
Iteration 409/1000 | Loss: 0.00002226
Iteration 410/1000 | Loss: 0.00002225
Iteration 411/1000 | Loss: 0.00002225
Iteration 412/1000 | Loss: 0.00002224
Iteration 413/1000 | Loss: 0.00002224
Iteration 414/1000 | Loss: 0.00002224
Iteration 415/1000 | Loss: 0.00002223
Iteration 416/1000 | Loss: 0.00002223
Iteration 417/1000 | Loss: 0.00002223
Iteration 418/1000 | Loss: 0.00002222
Iteration 419/1000 | Loss: 0.00002222
Iteration 420/1000 | Loss: 0.00002222
Iteration 421/1000 | Loss: 0.00002222
Iteration 422/1000 | Loss: 0.00002222
Iteration 423/1000 | Loss: 0.00002222
Iteration 424/1000 | Loss: 0.00002222
Iteration 425/1000 | Loss: 0.00002222
Iteration 426/1000 | Loss: 0.00002221
Iteration 427/1000 | Loss: 0.00002221
Iteration 428/1000 | Loss: 0.00002221
Iteration 429/1000 | Loss: 0.00002221
Iteration 430/1000 | Loss: 0.00002221
Iteration 431/1000 | Loss: 0.00002220
Iteration 432/1000 | Loss: 0.00002220
Iteration 433/1000 | Loss: 0.00002220
Iteration 434/1000 | Loss: 0.00002220
Iteration 435/1000 | Loss: 0.00002220
Iteration 436/1000 | Loss: 0.00002220
Iteration 437/1000 | Loss: 0.00002219
Iteration 438/1000 | Loss: 0.00002219
Iteration 439/1000 | Loss: 0.00002219
Iteration 440/1000 | Loss: 0.00002219
Iteration 441/1000 | Loss: 0.00002219
Iteration 442/1000 | Loss: 0.00002219
Iteration 443/1000 | Loss: 0.00002219
Iteration 444/1000 | Loss: 0.00002219
Iteration 445/1000 | Loss: 0.00002219
Iteration 446/1000 | Loss: 0.00002218
Iteration 447/1000 | Loss: 0.00002218
Iteration 448/1000 | Loss: 0.00002218
Iteration 449/1000 | Loss: 0.00002218
Iteration 450/1000 | Loss: 0.00002218
Iteration 451/1000 | Loss: 0.00002217
Iteration 452/1000 | Loss: 0.00002217
Iteration 453/1000 | Loss: 0.00002217
Iteration 454/1000 | Loss: 0.00002217
Iteration 455/1000 | Loss: 0.00002217
Iteration 456/1000 | Loss: 0.00002217
Iteration 457/1000 | Loss: 0.00002217
Iteration 458/1000 | Loss: 0.00002216
Iteration 459/1000 | Loss: 0.00002216
Iteration 460/1000 | Loss: 0.00002216
Iteration 461/1000 | Loss: 0.00002216
Iteration 462/1000 | Loss: 0.00002216
Iteration 463/1000 | Loss: 0.00002216
Iteration 464/1000 | Loss: 0.00002216
Iteration 465/1000 | Loss: 0.00002216
Iteration 466/1000 | Loss: 0.00002215
Iteration 467/1000 | Loss: 0.00002215
Iteration 468/1000 | Loss: 0.00002215
Iteration 469/1000 | Loss: 0.00002215
Iteration 470/1000 | Loss: 0.00002215
Iteration 471/1000 | Loss: 0.00002215
Iteration 472/1000 | Loss: 0.00002215
Iteration 473/1000 | Loss: 0.00002215
Iteration 474/1000 | Loss: 0.00002215
Iteration 475/1000 | Loss: 0.00002215
Iteration 476/1000 | Loss: 0.00002215
Iteration 477/1000 | Loss: 0.00002215
Iteration 478/1000 | Loss: 0.00002215
Iteration 479/1000 | Loss: 0.00002214
Iteration 480/1000 | Loss: 0.00002214
Iteration 481/1000 | Loss: 0.00002214
Iteration 482/1000 | Loss: 0.00002214
Iteration 483/1000 | Loss: 0.00002214
Iteration 484/1000 | Loss: 0.00002214
Iteration 485/1000 | Loss: 0.00002214
Iteration 486/1000 | Loss: 0.00002214
Iteration 487/1000 | Loss: 0.00002214
Iteration 488/1000 | Loss: 0.00002214
Iteration 489/1000 | Loss: 0.00002214
Iteration 490/1000 | Loss: 0.00002214
Iteration 491/1000 | Loss: 0.00002214
Iteration 492/1000 | Loss: 0.00002214
Iteration 493/1000 | Loss: 0.00002214
Iteration 494/1000 | Loss: 0.00002213
Iteration 495/1000 | Loss: 0.00002213
Iteration 496/1000 | Loss: 0.00002213
Iteration 497/1000 | Loss: 0.00002213
Iteration 498/1000 | Loss: 0.00002213
Iteration 499/1000 | Loss: 0.00002213
Iteration 500/1000 | Loss: 0.00002213
Iteration 501/1000 | Loss: 0.00002213
Iteration 502/1000 | Loss: 0.00002213
Iteration 503/1000 | Loss: 0.00002213
Iteration 504/1000 | Loss: 0.00002212
Iteration 505/1000 | Loss: 0.00002212
Iteration 506/1000 | Loss: 0.00002212
Iteration 507/1000 | Loss: 0.00002212
Iteration 508/1000 | Loss: 0.00002212
Iteration 509/1000 | Loss: 0.00002212
Iteration 510/1000 | Loss: 0.00002212
Iteration 511/1000 | Loss: 0.00002211
Iteration 512/1000 | Loss: 0.00002211
Iteration 513/1000 | Loss: 0.00002211
Iteration 514/1000 | Loss: 0.00002211
Iteration 515/1000 | Loss: 0.00002211
Iteration 516/1000 | Loss: 0.00002211
Iteration 517/1000 | Loss: 0.00002211
Iteration 518/1000 | Loss: 0.00002211
Iteration 519/1000 | Loss: 0.00002211
Iteration 520/1000 | Loss: 0.00002211
Iteration 521/1000 | Loss: 0.00002211
Iteration 522/1000 | Loss: 0.00002210
Iteration 523/1000 | Loss: 0.00002210
Iteration 524/1000 | Loss: 0.00002210
Iteration 525/1000 | Loss: 0.00002210
Iteration 526/1000 | Loss: 0.00002210
Iteration 527/1000 | Loss: 0.00002209
Iteration 528/1000 | Loss: 0.00002209
Iteration 529/1000 | Loss: 0.00002209
Iteration 530/1000 | Loss: 0.00002209
Iteration 531/1000 | Loss: 0.00002209
Iteration 532/1000 | Loss: 0.00002209
Iteration 533/1000 | Loss: 0.00002209
Iteration 534/1000 | Loss: 0.00002209
Iteration 535/1000 | Loss: 0.00002209
Iteration 536/1000 | Loss: 0.00002209
Iteration 537/1000 | Loss: 0.00002209
Iteration 538/1000 | Loss: 0.00002209
Iteration 539/1000 | Loss: 0.00002209
Iteration 540/1000 | Loss: 0.00002208
Iteration 541/1000 | Loss: 0.00002208
Iteration 542/1000 | Loss: 0.00002208
Iteration 543/1000 | Loss: 0.00002208
Iteration 544/1000 | Loss: 0.00002208
Iteration 545/1000 | Loss: 0.00002208
Iteration 546/1000 | Loss: 0.00002208
Iteration 547/1000 | Loss: 0.00002208
Iteration 548/1000 | Loss: 0.00002208
Iteration 549/1000 | Loss: 0.00002207
Iteration 550/1000 | Loss: 0.00002207
Iteration 551/1000 | Loss: 0.00002207
Iteration 552/1000 | Loss: 0.00002206
Iteration 553/1000 | Loss: 0.00002206
Iteration 554/1000 | Loss: 0.00002206
Iteration 555/1000 | Loss: 0.00002206
Iteration 556/1000 | Loss: 0.00002205
Iteration 557/1000 | Loss: 0.00002205
Iteration 558/1000 | Loss: 0.00002205
Iteration 559/1000 | Loss: 0.00002205
Iteration 560/1000 | Loss: 0.00002205
Iteration 561/1000 | Loss: 0.00002205
Iteration 562/1000 | Loss: 0.00002205
Iteration 563/1000 | Loss: 0.00002204
Iteration 564/1000 | Loss: 0.00002204
Iteration 565/1000 | Loss: 0.00002204
Iteration 566/1000 | Loss: 0.00002204
Iteration 567/1000 | Loss: 0.00002204
Iteration 568/1000 | Loss: 0.00002204
Iteration 569/1000 | Loss: 0.00002204
Iteration 570/1000 | Loss: 0.00002204
Iteration 571/1000 | Loss: 0.00002204
Iteration 572/1000 | Loss: 0.00002204
Iteration 573/1000 | Loss: 0.00002204
Iteration 574/1000 | Loss: 0.00002204
Iteration 575/1000 | Loss: 0.00002204
Iteration 576/1000 | Loss: 0.00002203
Iteration 577/1000 | Loss: 0.00002203
Iteration 578/1000 | Loss: 0.00002203
Iteration 579/1000 | Loss: 0.00002203
Iteration 580/1000 | Loss: 0.00002203
Iteration 581/1000 | Loss: 0.00002203
Iteration 582/1000 | Loss: 0.00002203
Iteration 583/1000 | Loss: 0.00002203
Iteration 584/1000 | Loss: 0.00002203
Iteration 585/1000 | Loss: 0.00002203
Iteration 586/1000 | Loss: 0.00002203
Iteration 587/1000 | Loss: 0.00002203
Iteration 588/1000 | Loss: 0.00002203
Iteration 589/1000 | Loss: 0.00002203
Iteration 590/1000 | Loss: 0.00002203
Iteration 591/1000 | Loss: 0.00002203
Iteration 592/1000 | Loss: 0.00002203
Iteration 593/1000 | Loss: 0.00002203
Iteration 594/1000 | Loss: 0.00002203
Iteration 595/1000 | Loss: 0.00002203
Iteration 596/1000 | Loss: 0.00002203
Iteration 597/1000 | Loss: 0.00002203
Iteration 598/1000 | Loss: 0.00002203
Iteration 599/1000 | Loss: 0.00002203
Iteration 600/1000 | Loss: 0.00002203
Iteration 601/1000 | Loss: 0.00002203
Iteration 602/1000 | Loss: 0.00002203
Iteration 603/1000 | Loss: 0.00002203
Iteration 604/1000 | Loss: 0.00002203
Iteration 605/1000 | Loss: 0.00002203
Iteration 606/1000 | Loss: 0.00002203
Iteration 607/1000 | Loss: 0.00002203
Iteration 608/1000 | Loss: 0.00002203
Iteration 609/1000 | Loss: 0.00002203
Iteration 610/1000 | Loss: 0.00002203
Iteration 611/1000 | Loss: 0.00002203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 611. Stopping optimization.
Last 5 losses: [2.2027392333257012e-05, 2.2027392333257012e-05, 2.2027392333257012e-05, 2.2027392333257012e-05, 2.2027392333257012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2027392333257012e-05

Optimization complete. Final v2v error: 3.810438632965088 mm

Highest mean error: 6.0610761642456055 mm for frame 34

Lowest mean error: 2.7995107173919678 mm for frame 0

Saving results

Total time: 517.5028171539307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00682710
Iteration 2/25 | Loss: 0.00172156
Iteration 3/25 | Loss: 0.00147530
Iteration 4/25 | Loss: 0.00146452
Iteration 5/25 | Loss: 0.00146238
Iteration 6/25 | Loss: 0.00146238
Iteration 7/25 | Loss: 0.00146238
Iteration 8/25 | Loss: 0.00146238
Iteration 9/25 | Loss: 0.00146238
Iteration 10/25 | Loss: 0.00146238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014623793540522456, 0.0014623793540522456, 0.0014623793540522456, 0.0014623793540522456, 0.0014623793540522456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014623793540522456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80661583
Iteration 2/25 | Loss: 0.00141943
Iteration 3/25 | Loss: 0.00141943
Iteration 4/25 | Loss: 0.00141943
Iteration 5/25 | Loss: 0.00141943
Iteration 6/25 | Loss: 0.00141943
Iteration 7/25 | Loss: 0.00141943
Iteration 8/25 | Loss: 0.00141943
Iteration 9/25 | Loss: 0.00141943
Iteration 10/25 | Loss: 0.00141943
Iteration 11/25 | Loss: 0.00141943
Iteration 12/25 | Loss: 0.00141943
Iteration 13/25 | Loss: 0.00141943
Iteration 14/25 | Loss: 0.00141943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014194302493706346, 0.0014194302493706346, 0.0014194302493706346, 0.0014194302493706346, 0.0014194302493706346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014194302493706346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141943
Iteration 2/1000 | Loss: 0.00005812
Iteration 3/1000 | Loss: 0.00003355
Iteration 4/1000 | Loss: 0.00002988
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002646
Iteration 7/1000 | Loss: 0.00002570
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002496
Iteration 10/1000 | Loss: 0.00002463
Iteration 11/1000 | Loss: 0.00002447
Iteration 12/1000 | Loss: 0.00002426
Iteration 13/1000 | Loss: 0.00002415
Iteration 14/1000 | Loss: 0.00002410
Iteration 15/1000 | Loss: 0.00002394
Iteration 16/1000 | Loss: 0.00002384
Iteration 17/1000 | Loss: 0.00002383
Iteration 18/1000 | Loss: 0.00002382
Iteration 19/1000 | Loss: 0.00002377
Iteration 20/1000 | Loss: 0.00002376
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002376
Iteration 23/1000 | Loss: 0.00002375
Iteration 24/1000 | Loss: 0.00002375
Iteration 25/1000 | Loss: 0.00002374
Iteration 26/1000 | Loss: 0.00002374
Iteration 27/1000 | Loss: 0.00002373
Iteration 28/1000 | Loss: 0.00002372
Iteration 29/1000 | Loss: 0.00002372
Iteration 30/1000 | Loss: 0.00002372
Iteration 31/1000 | Loss: 0.00002371
Iteration 32/1000 | Loss: 0.00002371
Iteration 33/1000 | Loss: 0.00002371
Iteration 34/1000 | Loss: 0.00002370
Iteration 35/1000 | Loss: 0.00002370
Iteration 36/1000 | Loss: 0.00002369
Iteration 37/1000 | Loss: 0.00002368
Iteration 38/1000 | Loss: 0.00002368
Iteration 39/1000 | Loss: 0.00002368
Iteration 40/1000 | Loss: 0.00002368
Iteration 41/1000 | Loss: 0.00002368
Iteration 42/1000 | Loss: 0.00002368
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00002368
Iteration 45/1000 | Loss: 0.00002367
Iteration 46/1000 | Loss: 0.00002367
Iteration 47/1000 | Loss: 0.00002366
Iteration 48/1000 | Loss: 0.00002366
Iteration 49/1000 | Loss: 0.00002365
Iteration 50/1000 | Loss: 0.00002365
Iteration 51/1000 | Loss: 0.00002364
Iteration 52/1000 | Loss: 0.00002364
Iteration 53/1000 | Loss: 0.00002364
Iteration 54/1000 | Loss: 0.00002363
Iteration 55/1000 | Loss: 0.00002363
Iteration 56/1000 | Loss: 0.00002362
Iteration 57/1000 | Loss: 0.00002362
Iteration 58/1000 | Loss: 0.00002362
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002361
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002361
Iteration 64/1000 | Loss: 0.00002361
Iteration 65/1000 | Loss: 0.00002361
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002360
Iteration 69/1000 | Loss: 0.00002360
Iteration 70/1000 | Loss: 0.00002360
Iteration 71/1000 | Loss: 0.00002360
Iteration 72/1000 | Loss: 0.00002360
Iteration 73/1000 | Loss: 0.00002360
Iteration 74/1000 | Loss: 0.00002360
Iteration 75/1000 | Loss: 0.00002360
Iteration 76/1000 | Loss: 0.00002360
Iteration 77/1000 | Loss: 0.00002359
Iteration 78/1000 | Loss: 0.00002359
Iteration 79/1000 | Loss: 0.00002359
Iteration 80/1000 | Loss: 0.00002359
Iteration 81/1000 | Loss: 0.00002358
Iteration 82/1000 | Loss: 0.00002357
Iteration 83/1000 | Loss: 0.00002357
Iteration 84/1000 | Loss: 0.00002357
Iteration 85/1000 | Loss: 0.00002357
Iteration 86/1000 | Loss: 0.00002356
Iteration 87/1000 | Loss: 0.00002356
Iteration 88/1000 | Loss: 0.00002355
Iteration 89/1000 | Loss: 0.00002355
Iteration 90/1000 | Loss: 0.00002355
Iteration 91/1000 | Loss: 0.00002354
Iteration 92/1000 | Loss: 0.00002354
Iteration 93/1000 | Loss: 0.00002354
Iteration 94/1000 | Loss: 0.00002354
Iteration 95/1000 | Loss: 0.00002353
Iteration 96/1000 | Loss: 0.00002353
Iteration 97/1000 | Loss: 0.00002353
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002353
Iteration 100/1000 | Loss: 0.00002353
Iteration 101/1000 | Loss: 0.00002353
Iteration 102/1000 | Loss: 0.00002353
Iteration 103/1000 | Loss: 0.00002353
Iteration 104/1000 | Loss: 0.00002353
Iteration 105/1000 | Loss: 0.00002353
Iteration 106/1000 | Loss: 0.00002353
Iteration 107/1000 | Loss: 0.00002353
Iteration 108/1000 | Loss: 0.00002353
Iteration 109/1000 | Loss: 0.00002353
Iteration 110/1000 | Loss: 0.00002353
Iteration 111/1000 | Loss: 0.00002353
Iteration 112/1000 | Loss: 0.00002353
Iteration 113/1000 | Loss: 0.00002353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.352887713641394e-05, 2.352887713641394e-05, 2.352887713641394e-05, 2.352887713641394e-05, 2.352887713641394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.352887713641394e-05

Optimization complete. Final v2v error: 4.120183944702148 mm

Highest mean error: 4.407707214355469 mm for frame 215

Lowest mean error: 3.8452606201171875 mm for frame 132

Saving results

Total time: 39.04412913322449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452511
Iteration 2/25 | Loss: 0.00137711
Iteration 3/25 | Loss: 0.00130493
Iteration 4/25 | Loss: 0.00129318
Iteration 5/25 | Loss: 0.00128944
Iteration 6/25 | Loss: 0.00128901
Iteration 7/25 | Loss: 0.00128901
Iteration 8/25 | Loss: 0.00128901
Iteration 9/25 | Loss: 0.00128901
Iteration 10/25 | Loss: 0.00128901
Iteration 11/25 | Loss: 0.00128901
Iteration 12/25 | Loss: 0.00128901
Iteration 13/25 | Loss: 0.00128901
Iteration 14/25 | Loss: 0.00128901
Iteration 15/25 | Loss: 0.00128901
Iteration 16/25 | Loss: 0.00128901
Iteration 17/25 | Loss: 0.00128901
Iteration 18/25 | Loss: 0.00128901
Iteration 19/25 | Loss: 0.00128901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012890137732028961, 0.0012890137732028961, 0.0012890137732028961, 0.0012890137732028961, 0.0012890137732028961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012890137732028961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29572690
Iteration 2/25 | Loss: 0.00139127
Iteration 3/25 | Loss: 0.00139127
Iteration 4/25 | Loss: 0.00139127
Iteration 5/25 | Loss: 0.00139127
Iteration 6/25 | Loss: 0.00139126
Iteration 7/25 | Loss: 0.00139126
Iteration 8/25 | Loss: 0.00139126
Iteration 9/25 | Loss: 0.00139126
Iteration 10/25 | Loss: 0.00139126
Iteration 11/25 | Loss: 0.00139126
Iteration 12/25 | Loss: 0.00139126
Iteration 13/25 | Loss: 0.00139126
Iteration 14/25 | Loss: 0.00139126
Iteration 15/25 | Loss: 0.00139126
Iteration 16/25 | Loss: 0.00139126
Iteration 17/25 | Loss: 0.00139126
Iteration 18/25 | Loss: 0.00139126
Iteration 19/25 | Loss: 0.00139126
Iteration 20/25 | Loss: 0.00139126
Iteration 21/25 | Loss: 0.00139126
Iteration 22/25 | Loss: 0.00139126
Iteration 23/25 | Loss: 0.00139126
Iteration 24/25 | Loss: 0.00139126
Iteration 25/25 | Loss: 0.00139126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013912629801779985, 0.0013912629801779985, 0.0013912629801779985, 0.0013912629801779985, 0.0013912629801779985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013912629801779985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139126
Iteration 2/1000 | Loss: 0.00002778
Iteration 3/1000 | Loss: 0.00002270
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001937
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001780
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001724
Iteration 12/1000 | Loss: 0.00001703
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001662
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001658
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001638
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001637
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001628
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001623
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001618
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001617
Iteration 48/1000 | Loss: 0.00001617
Iteration 49/1000 | Loss: 0.00001617
Iteration 50/1000 | Loss: 0.00001616
Iteration 51/1000 | Loss: 0.00001616
Iteration 52/1000 | Loss: 0.00001616
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001615
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001614
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001614
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001613
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001613
Iteration 66/1000 | Loss: 0.00001613
Iteration 67/1000 | Loss: 0.00001613
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001611
Iteration 76/1000 | Loss: 0.00001611
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001609
Iteration 85/1000 | Loss: 0.00001609
Iteration 86/1000 | Loss: 0.00001609
Iteration 87/1000 | Loss: 0.00001609
Iteration 88/1000 | Loss: 0.00001609
Iteration 89/1000 | Loss: 0.00001609
Iteration 90/1000 | Loss: 0.00001609
Iteration 91/1000 | Loss: 0.00001608
Iteration 92/1000 | Loss: 0.00001608
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001607
Iteration 96/1000 | Loss: 0.00001607
Iteration 97/1000 | Loss: 0.00001607
Iteration 98/1000 | Loss: 0.00001607
Iteration 99/1000 | Loss: 0.00001607
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001605
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001603
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.6024812794057652e-05, 1.6024812794057652e-05, 1.6024812794057652e-05, 1.6024812794057652e-05, 1.6024812794057652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6024812794057652e-05

Optimization complete. Final v2v error: 3.4404916763305664 mm

Highest mean error: 3.5674939155578613 mm for frame 123

Lowest mean error: 3.2879490852355957 mm for frame 53

Saving results

Total time: 40.3912878036499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856159
Iteration 2/25 | Loss: 0.00185381
Iteration 3/25 | Loss: 0.00144902
Iteration 4/25 | Loss: 0.00139100
Iteration 5/25 | Loss: 0.00144680
Iteration 6/25 | Loss: 0.00135775
Iteration 7/25 | Loss: 0.00132701
Iteration 8/25 | Loss: 0.00132009
Iteration 9/25 | Loss: 0.00130306
Iteration 10/25 | Loss: 0.00130104
Iteration 11/25 | Loss: 0.00130062
Iteration 12/25 | Loss: 0.00130053
Iteration 13/25 | Loss: 0.00130053
Iteration 14/25 | Loss: 0.00130053
Iteration 15/25 | Loss: 0.00130053
Iteration 16/25 | Loss: 0.00130053
Iteration 17/25 | Loss: 0.00130053
Iteration 18/25 | Loss: 0.00130053
Iteration 19/25 | Loss: 0.00130053
Iteration 20/25 | Loss: 0.00130052
Iteration 21/25 | Loss: 0.00130052
Iteration 22/25 | Loss: 0.00130052
Iteration 23/25 | Loss: 0.00130052
Iteration 24/25 | Loss: 0.00130052
Iteration 25/25 | Loss: 0.00130052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38325524
Iteration 2/25 | Loss: 0.00159833
Iteration 3/25 | Loss: 0.00155283
Iteration 4/25 | Loss: 0.00155283
Iteration 5/25 | Loss: 0.00155283
Iteration 6/25 | Loss: 0.00155283
Iteration 7/25 | Loss: 0.00155283
Iteration 8/25 | Loss: 0.00155283
Iteration 9/25 | Loss: 0.00155283
Iteration 10/25 | Loss: 0.00155283
Iteration 11/25 | Loss: 0.00155283
Iteration 12/25 | Loss: 0.00155283
Iteration 13/25 | Loss: 0.00155283
Iteration 14/25 | Loss: 0.00155283
Iteration 15/25 | Loss: 0.00155283
Iteration 16/25 | Loss: 0.00155283
Iteration 17/25 | Loss: 0.00155283
Iteration 18/25 | Loss: 0.00155283
Iteration 19/25 | Loss: 0.00155283
Iteration 20/25 | Loss: 0.00155283
Iteration 21/25 | Loss: 0.00155283
Iteration 22/25 | Loss: 0.00155283
Iteration 23/25 | Loss: 0.00155283
Iteration 24/25 | Loss: 0.00155283
Iteration 25/25 | Loss: 0.00155283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155283
Iteration 2/1000 | Loss: 0.00003740
Iteration 3/1000 | Loss: 0.00002665
Iteration 4/1000 | Loss: 0.00011098
Iteration 5/1000 | Loss: 0.00008761
Iteration 6/1000 | Loss: 0.00001966
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00007031
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001629
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00005067
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00009180
Iteration 21/1000 | Loss: 0.00005149
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001559
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001554
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001551
Iteration 30/1000 | Loss: 0.00001550
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001545
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00001544
Iteration 42/1000 | Loss: 0.00001544
Iteration 43/1000 | Loss: 0.00001544
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001542
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001538
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001537
Iteration 60/1000 | Loss: 0.00001537
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001536
Iteration 63/1000 | Loss: 0.00001536
Iteration 64/1000 | Loss: 0.00001536
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001535
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001535
Iteration 74/1000 | Loss: 0.00001535
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001531
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001530
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001528
Iteration 119/1000 | Loss: 0.00001528
Iteration 120/1000 | Loss: 0.00001528
Iteration 121/1000 | Loss: 0.00001527
Iteration 122/1000 | Loss: 0.00001527
Iteration 123/1000 | Loss: 0.00001527
Iteration 124/1000 | Loss: 0.00001527
Iteration 125/1000 | Loss: 0.00001526
Iteration 126/1000 | Loss: 0.00001526
Iteration 127/1000 | Loss: 0.00001526
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001524
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001524
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00001524
Iteration 137/1000 | Loss: 0.00001524
Iteration 138/1000 | Loss: 0.00001524
Iteration 139/1000 | Loss: 0.00001524
Iteration 140/1000 | Loss: 0.00001524
Iteration 141/1000 | Loss: 0.00001524
Iteration 142/1000 | Loss: 0.00001524
Iteration 143/1000 | Loss: 0.00001524
Iteration 144/1000 | Loss: 0.00001523
Iteration 145/1000 | Loss: 0.00001523
Iteration 146/1000 | Loss: 0.00001523
Iteration 147/1000 | Loss: 0.00001523
Iteration 148/1000 | Loss: 0.00001523
Iteration 149/1000 | Loss: 0.00001523
Iteration 150/1000 | Loss: 0.00001523
Iteration 151/1000 | Loss: 0.00001523
Iteration 152/1000 | Loss: 0.00001522
Iteration 153/1000 | Loss: 0.00001522
Iteration 154/1000 | Loss: 0.00001522
Iteration 155/1000 | Loss: 0.00001522
Iteration 156/1000 | Loss: 0.00001522
Iteration 157/1000 | Loss: 0.00001522
Iteration 158/1000 | Loss: 0.00001522
Iteration 159/1000 | Loss: 0.00001522
Iteration 160/1000 | Loss: 0.00001522
Iteration 161/1000 | Loss: 0.00001522
Iteration 162/1000 | Loss: 0.00001522
Iteration 163/1000 | Loss: 0.00001522
Iteration 164/1000 | Loss: 0.00001521
Iteration 165/1000 | Loss: 0.00001521
Iteration 166/1000 | Loss: 0.00001521
Iteration 167/1000 | Loss: 0.00001521
Iteration 168/1000 | Loss: 0.00001521
Iteration 169/1000 | Loss: 0.00001521
Iteration 170/1000 | Loss: 0.00001521
Iteration 171/1000 | Loss: 0.00001521
Iteration 172/1000 | Loss: 0.00001521
Iteration 173/1000 | Loss: 0.00001521
Iteration 174/1000 | Loss: 0.00001521
Iteration 175/1000 | Loss: 0.00001520
Iteration 176/1000 | Loss: 0.00001520
Iteration 177/1000 | Loss: 0.00001520
Iteration 178/1000 | Loss: 0.00001520
Iteration 179/1000 | Loss: 0.00001520
Iteration 180/1000 | Loss: 0.00001520
Iteration 181/1000 | Loss: 0.00001520
Iteration 182/1000 | Loss: 0.00001520
Iteration 183/1000 | Loss: 0.00001520
Iteration 184/1000 | Loss: 0.00001520
Iteration 185/1000 | Loss: 0.00001520
Iteration 186/1000 | Loss: 0.00001520
Iteration 187/1000 | Loss: 0.00001520
Iteration 188/1000 | Loss: 0.00001519
Iteration 189/1000 | Loss: 0.00001519
Iteration 190/1000 | Loss: 0.00001519
Iteration 191/1000 | Loss: 0.00001519
Iteration 192/1000 | Loss: 0.00001519
Iteration 193/1000 | Loss: 0.00001519
Iteration 194/1000 | Loss: 0.00001518
Iteration 195/1000 | Loss: 0.00001518
Iteration 196/1000 | Loss: 0.00001518
Iteration 197/1000 | Loss: 0.00001518
Iteration 198/1000 | Loss: 0.00001518
Iteration 199/1000 | Loss: 0.00001518
Iteration 200/1000 | Loss: 0.00001518
Iteration 201/1000 | Loss: 0.00001518
Iteration 202/1000 | Loss: 0.00001518
Iteration 203/1000 | Loss: 0.00001517
Iteration 204/1000 | Loss: 0.00001517
Iteration 205/1000 | Loss: 0.00001517
Iteration 206/1000 | Loss: 0.00001517
Iteration 207/1000 | Loss: 0.00001517
Iteration 208/1000 | Loss: 0.00001517
Iteration 209/1000 | Loss: 0.00001517
Iteration 210/1000 | Loss: 0.00001517
Iteration 211/1000 | Loss: 0.00001517
Iteration 212/1000 | Loss: 0.00001517
Iteration 213/1000 | Loss: 0.00001517
Iteration 214/1000 | Loss: 0.00001517
Iteration 215/1000 | Loss: 0.00001517
Iteration 216/1000 | Loss: 0.00001517
Iteration 217/1000 | Loss: 0.00001517
Iteration 218/1000 | Loss: 0.00001517
Iteration 219/1000 | Loss: 0.00001516
Iteration 220/1000 | Loss: 0.00001516
Iteration 221/1000 | Loss: 0.00001516
Iteration 222/1000 | Loss: 0.00001516
Iteration 223/1000 | Loss: 0.00001516
Iteration 224/1000 | Loss: 0.00001516
Iteration 225/1000 | Loss: 0.00001516
Iteration 226/1000 | Loss: 0.00001516
Iteration 227/1000 | Loss: 0.00001516
Iteration 228/1000 | Loss: 0.00001516
Iteration 229/1000 | Loss: 0.00001515
Iteration 230/1000 | Loss: 0.00001515
Iteration 231/1000 | Loss: 0.00001515
Iteration 232/1000 | Loss: 0.00001515
Iteration 233/1000 | Loss: 0.00001515
Iteration 234/1000 | Loss: 0.00001515
Iteration 235/1000 | Loss: 0.00001515
Iteration 236/1000 | Loss: 0.00001515
Iteration 237/1000 | Loss: 0.00001515
Iteration 238/1000 | Loss: 0.00001515
Iteration 239/1000 | Loss: 0.00001515
Iteration 240/1000 | Loss: 0.00001515
Iteration 241/1000 | Loss: 0.00001515
Iteration 242/1000 | Loss: 0.00001515
Iteration 243/1000 | Loss: 0.00001515
Iteration 244/1000 | Loss: 0.00001515
Iteration 245/1000 | Loss: 0.00001515
Iteration 246/1000 | Loss: 0.00001515
Iteration 247/1000 | Loss: 0.00001515
Iteration 248/1000 | Loss: 0.00001515
Iteration 249/1000 | Loss: 0.00001515
Iteration 250/1000 | Loss: 0.00001515
Iteration 251/1000 | Loss: 0.00001515
Iteration 252/1000 | Loss: 0.00001514
Iteration 253/1000 | Loss: 0.00001514
Iteration 254/1000 | Loss: 0.00001514
Iteration 255/1000 | Loss: 0.00001514
Iteration 256/1000 | Loss: 0.00001514
Iteration 257/1000 | Loss: 0.00001514
Iteration 258/1000 | Loss: 0.00001514
Iteration 259/1000 | Loss: 0.00001514
Iteration 260/1000 | Loss: 0.00001514
Iteration 261/1000 | Loss: 0.00001514
Iteration 262/1000 | Loss: 0.00001514
Iteration 263/1000 | Loss: 0.00001514
Iteration 264/1000 | Loss: 0.00001514
Iteration 265/1000 | Loss: 0.00001514
Iteration 266/1000 | Loss: 0.00001514
Iteration 267/1000 | Loss: 0.00001514
Iteration 268/1000 | Loss: 0.00001514
Iteration 269/1000 | Loss: 0.00001514
Iteration 270/1000 | Loss: 0.00001514
Iteration 271/1000 | Loss: 0.00001514
Iteration 272/1000 | Loss: 0.00001514
Iteration 273/1000 | Loss: 0.00001513
Iteration 274/1000 | Loss: 0.00001513
Iteration 275/1000 | Loss: 0.00001513
Iteration 276/1000 | Loss: 0.00001513
Iteration 277/1000 | Loss: 0.00001513
Iteration 278/1000 | Loss: 0.00001513
Iteration 279/1000 | Loss: 0.00001513
Iteration 280/1000 | Loss: 0.00001513
Iteration 281/1000 | Loss: 0.00001513
Iteration 282/1000 | Loss: 0.00001513
Iteration 283/1000 | Loss: 0.00001513
Iteration 284/1000 | Loss: 0.00001513
Iteration 285/1000 | Loss: 0.00001512
Iteration 286/1000 | Loss: 0.00001512
Iteration 287/1000 | Loss: 0.00001512
Iteration 288/1000 | Loss: 0.00001512
Iteration 289/1000 | Loss: 0.00001512
Iteration 290/1000 | Loss: 0.00001512
Iteration 291/1000 | Loss: 0.00001512
Iteration 292/1000 | Loss: 0.00001512
Iteration 293/1000 | Loss: 0.00001512
Iteration 294/1000 | Loss: 0.00001512
Iteration 295/1000 | Loss: 0.00001512
Iteration 296/1000 | Loss: 0.00001511
Iteration 297/1000 | Loss: 0.00001511
Iteration 298/1000 | Loss: 0.00001511
Iteration 299/1000 | Loss: 0.00001511
Iteration 300/1000 | Loss: 0.00001511
Iteration 301/1000 | Loss: 0.00001511
Iteration 302/1000 | Loss: 0.00001511
Iteration 303/1000 | Loss: 0.00001511
Iteration 304/1000 | Loss: 0.00001511
Iteration 305/1000 | Loss: 0.00001511
Iteration 306/1000 | Loss: 0.00001511
Iteration 307/1000 | Loss: 0.00001511
Iteration 308/1000 | Loss: 0.00001511
Iteration 309/1000 | Loss: 0.00001511
Iteration 310/1000 | Loss: 0.00001511
Iteration 311/1000 | Loss: 0.00001511
Iteration 312/1000 | Loss: 0.00001511
Iteration 313/1000 | Loss: 0.00001511
Iteration 314/1000 | Loss: 0.00001511
Iteration 315/1000 | Loss: 0.00001511
Iteration 316/1000 | Loss: 0.00001511
Iteration 317/1000 | Loss: 0.00006732
Iteration 318/1000 | Loss: 0.00001516
Iteration 319/1000 | Loss: 0.00001514
Iteration 320/1000 | Loss: 0.00001514
Iteration 321/1000 | Loss: 0.00001514
Iteration 322/1000 | Loss: 0.00001514
Iteration 323/1000 | Loss: 0.00001513
Iteration 324/1000 | Loss: 0.00001513
Iteration 325/1000 | Loss: 0.00001513
Iteration 326/1000 | Loss: 0.00001513
Iteration 327/1000 | Loss: 0.00001513
Iteration 328/1000 | Loss: 0.00001513
Iteration 329/1000 | Loss: 0.00001513
Iteration 330/1000 | Loss: 0.00001512
Iteration 331/1000 | Loss: 0.00001512
Iteration 332/1000 | Loss: 0.00001512
Iteration 333/1000 | Loss: 0.00001512
Iteration 334/1000 | Loss: 0.00001512
Iteration 335/1000 | Loss: 0.00001512
Iteration 336/1000 | Loss: 0.00001511
Iteration 337/1000 | Loss: 0.00001511
Iteration 338/1000 | Loss: 0.00001511
Iteration 339/1000 | Loss: 0.00001511
Iteration 340/1000 | Loss: 0.00001511
Iteration 341/1000 | Loss: 0.00001511
Iteration 342/1000 | Loss: 0.00001511
Iteration 343/1000 | Loss: 0.00001511
Iteration 344/1000 | Loss: 0.00001510
Iteration 345/1000 | Loss: 0.00001510
Iteration 346/1000 | Loss: 0.00001510
Iteration 347/1000 | Loss: 0.00001510
Iteration 348/1000 | Loss: 0.00001510
Iteration 349/1000 | Loss: 0.00001510
Iteration 350/1000 | Loss: 0.00001510
Iteration 351/1000 | Loss: 0.00001510
Iteration 352/1000 | Loss: 0.00001510
Iteration 353/1000 | Loss: 0.00001510
Iteration 354/1000 | Loss: 0.00001510
Iteration 355/1000 | Loss: 0.00001510
Iteration 356/1000 | Loss: 0.00001510
Iteration 357/1000 | Loss: 0.00001510
Iteration 358/1000 | Loss: 0.00001510
Iteration 359/1000 | Loss: 0.00001510
Iteration 360/1000 | Loss: 0.00001510
Iteration 361/1000 | Loss: 0.00001510
Iteration 362/1000 | Loss: 0.00001510
Iteration 363/1000 | Loss: 0.00001510
Iteration 364/1000 | Loss: 0.00001510
Iteration 365/1000 | Loss: 0.00001510
Iteration 366/1000 | Loss: 0.00001510
Iteration 367/1000 | Loss: 0.00001510
Iteration 368/1000 | Loss: 0.00001510
Iteration 369/1000 | Loss: 0.00001510
Iteration 370/1000 | Loss: 0.00001510
Iteration 371/1000 | Loss: 0.00001510
Iteration 372/1000 | Loss: 0.00001510
Iteration 373/1000 | Loss: 0.00001510
Iteration 374/1000 | Loss: 0.00001510
Iteration 375/1000 | Loss: 0.00001510
Iteration 376/1000 | Loss: 0.00001510
Iteration 377/1000 | Loss: 0.00001510
Iteration 378/1000 | Loss: 0.00001510
Iteration 379/1000 | Loss: 0.00001510
Iteration 380/1000 | Loss: 0.00001510
Iteration 381/1000 | Loss: 0.00001510
Iteration 382/1000 | Loss: 0.00001510
Iteration 383/1000 | Loss: 0.00001510
Iteration 384/1000 | Loss: 0.00001510
Iteration 385/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 385. Stopping optimization.
Last 5 losses: [1.509725370851811e-05, 1.509725370851811e-05, 1.509725370851811e-05, 1.509725370851811e-05, 1.509725370851811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.509725370851811e-05

Optimization complete. Final v2v error: 3.344160556793213 mm

Highest mean error: 4.140137672424316 mm for frame 73

Lowest mean error: 3.038806676864624 mm for frame 112

Saving results

Total time: 71.58240556716919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958931
Iteration 2/25 | Loss: 0.00958930
Iteration 3/25 | Loss: 0.00958930
Iteration 4/25 | Loss: 0.00958930
Iteration 5/25 | Loss: 0.00958930
Iteration 6/25 | Loss: 0.00958930
Iteration 7/25 | Loss: 0.00958930
Iteration 8/25 | Loss: 0.00958929
Iteration 9/25 | Loss: 0.00958929
Iteration 10/25 | Loss: 0.00958929
Iteration 11/25 | Loss: 0.00958929
Iteration 12/25 | Loss: 0.00958929
Iteration 13/25 | Loss: 0.00958928
Iteration 14/25 | Loss: 0.00958928
Iteration 15/25 | Loss: 0.00958928
Iteration 16/25 | Loss: 0.00958928
Iteration 17/25 | Loss: 0.00958927
Iteration 18/25 | Loss: 0.00958927
Iteration 19/25 | Loss: 0.00958927
Iteration 20/25 | Loss: 0.00958927
Iteration 21/25 | Loss: 0.00958927
Iteration 22/25 | Loss: 0.00958926
Iteration 23/25 | Loss: 0.00958926
Iteration 24/25 | Loss: 0.00958926
Iteration 25/25 | Loss: 0.00958926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45794022
Iteration 2/25 | Loss: 0.16706069
Iteration 3/25 | Loss: 0.16514510
Iteration 4/25 | Loss: 0.16492200
Iteration 5/25 | Loss: 0.16492179
Iteration 6/25 | Loss: 0.16520055
Iteration 7/25 | Loss: 0.16503163
Iteration 8/25 | Loss: 0.16503315
Iteration 9/25 | Loss: 0.16505568
Iteration 10/25 | Loss: 0.16505560
Iteration 11/25 | Loss: 0.16505560
Iteration 12/25 | Loss: 0.16505560
Iteration 13/25 | Loss: 0.16539034
Iteration 14/25 | Loss: 0.16492178
Iteration 15/25 | Loss: 0.16492176
Iteration 16/25 | Loss: 0.16492176
Iteration 17/25 | Loss: 0.16492175
Iteration 18/25 | Loss: 0.16492175
Iteration 19/25 | Loss: 0.16492175
Iteration 20/25 | Loss: 0.16492175
Iteration 21/25 | Loss: 0.16492175
Iteration 22/25 | Loss: 0.16492175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.16492174565792084, 0.16492174565792084, 0.16492174565792084, 0.16492174565792084, 0.16492174565792084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16492174565792084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16492175
Iteration 2/1000 | Loss: 0.00863941
Iteration 3/1000 | Loss: 0.01602771
Iteration 4/1000 | Loss: 0.02196736
Iteration 5/1000 | Loss: 0.00870624
Iteration 6/1000 | Loss: 0.00072179
Iteration 7/1000 | Loss: 0.00031528
Iteration 8/1000 | Loss: 0.00025769
Iteration 9/1000 | Loss: 0.00018387
Iteration 10/1000 | Loss: 0.00030278
Iteration 11/1000 | Loss: 0.00036984
Iteration 12/1000 | Loss: 0.00013331
Iteration 13/1000 | Loss: 0.00007405
Iteration 14/1000 | Loss: 0.00005642
Iteration 15/1000 | Loss: 0.00004892
Iteration 16/1000 | Loss: 0.00004208
Iteration 17/1000 | Loss: 0.00003696
Iteration 18/1000 | Loss: 0.00003347
Iteration 19/1000 | Loss: 0.00003051
Iteration 20/1000 | Loss: 0.00002770
Iteration 21/1000 | Loss: 0.00002580
Iteration 22/1000 | Loss: 0.00002467
Iteration 23/1000 | Loss: 0.00002380
Iteration 24/1000 | Loss: 0.00002296
Iteration 25/1000 | Loss: 0.00002245
Iteration 26/1000 | Loss: 0.00002203
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002130
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002077
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002036
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002013
Iteration 35/1000 | Loss: 0.00002009
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002001
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00001994
Iteration 40/1000 | Loss: 0.00001993
Iteration 41/1000 | Loss: 0.00001993
Iteration 42/1000 | Loss: 0.00001992
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001991
Iteration 45/1000 | Loss: 0.00001991
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001986
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001985
Iteration 52/1000 | Loss: 0.00001981
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00001977
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001973
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001971
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001970
Iteration 77/1000 | Loss: 0.00001970
Iteration 78/1000 | Loss: 0.00001969
Iteration 79/1000 | Loss: 0.00001968
Iteration 80/1000 | Loss: 0.00001968
Iteration 81/1000 | Loss: 0.00001968
Iteration 82/1000 | Loss: 0.00001968
Iteration 83/1000 | Loss: 0.00001967
Iteration 84/1000 | Loss: 0.00001967
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001967
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00001964
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001964
Iteration 106/1000 | Loss: 0.00001964
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001963
Iteration 113/1000 | Loss: 0.00001963
Iteration 114/1000 | Loss: 0.00001962
Iteration 115/1000 | Loss: 0.00001962
Iteration 116/1000 | Loss: 0.00001962
Iteration 117/1000 | Loss: 0.00001962
Iteration 118/1000 | Loss: 0.00001962
Iteration 119/1000 | Loss: 0.00001962
Iteration 120/1000 | Loss: 0.00001962
Iteration 121/1000 | Loss: 0.00001961
Iteration 122/1000 | Loss: 0.00001961
Iteration 123/1000 | Loss: 0.00001961
Iteration 124/1000 | Loss: 0.00001961
Iteration 125/1000 | Loss: 0.00001961
Iteration 126/1000 | Loss: 0.00001961
Iteration 127/1000 | Loss: 0.00001961
Iteration 128/1000 | Loss: 0.00001961
Iteration 129/1000 | Loss: 0.00001961
Iteration 130/1000 | Loss: 0.00001961
Iteration 131/1000 | Loss: 0.00001960
Iteration 132/1000 | Loss: 0.00001960
Iteration 133/1000 | Loss: 0.00001960
Iteration 134/1000 | Loss: 0.00001960
Iteration 135/1000 | Loss: 0.00001960
Iteration 136/1000 | Loss: 0.00001960
Iteration 137/1000 | Loss: 0.00001960
Iteration 138/1000 | Loss: 0.00001960
Iteration 139/1000 | Loss: 0.00001960
Iteration 140/1000 | Loss: 0.00001960
Iteration 141/1000 | Loss: 0.00001960
Iteration 142/1000 | Loss: 0.00001960
Iteration 143/1000 | Loss: 0.00001960
Iteration 144/1000 | Loss: 0.00001960
Iteration 145/1000 | Loss: 0.00001960
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001960
Iteration 148/1000 | Loss: 0.00001960
Iteration 149/1000 | Loss: 0.00001959
Iteration 150/1000 | Loss: 0.00001959
Iteration 151/1000 | Loss: 0.00001959
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001959
Iteration 155/1000 | Loss: 0.00001959
Iteration 156/1000 | Loss: 0.00001959
Iteration 157/1000 | Loss: 0.00001959
Iteration 158/1000 | Loss: 0.00001959
Iteration 159/1000 | Loss: 0.00001959
Iteration 160/1000 | Loss: 0.00001959
Iteration 161/1000 | Loss: 0.00001959
Iteration 162/1000 | Loss: 0.00001958
Iteration 163/1000 | Loss: 0.00001958
Iteration 164/1000 | Loss: 0.00001958
Iteration 165/1000 | Loss: 0.00001958
Iteration 166/1000 | Loss: 0.00001958
Iteration 167/1000 | Loss: 0.00001958
Iteration 168/1000 | Loss: 0.00001958
Iteration 169/1000 | Loss: 0.00001958
Iteration 170/1000 | Loss: 0.00001958
Iteration 171/1000 | Loss: 0.00001958
Iteration 172/1000 | Loss: 0.00001958
Iteration 173/1000 | Loss: 0.00001958
Iteration 174/1000 | Loss: 0.00001958
Iteration 175/1000 | Loss: 0.00001958
Iteration 176/1000 | Loss: 0.00001958
Iteration 177/1000 | Loss: 0.00001958
Iteration 178/1000 | Loss: 0.00001958
Iteration 179/1000 | Loss: 0.00001958
Iteration 180/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.9578214050852694e-05, 1.9578214050852694e-05, 1.9578214050852694e-05, 1.9578214050852694e-05, 1.9578214050852694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9578214050852694e-05

Optimization complete. Final v2v error: 3.7637534141540527 mm

Highest mean error: 4.5099687576293945 mm for frame 95

Lowest mean error: 3.4894485473632812 mm for frame 191

Saving results

Total time: 86.01477336883545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991293
Iteration 2/25 | Loss: 0.00184693
Iteration 3/25 | Loss: 0.00158176
Iteration 4/25 | Loss: 0.00155409
Iteration 5/25 | Loss: 0.00158906
Iteration 6/25 | Loss: 0.00159724
Iteration 7/25 | Loss: 0.00149145
Iteration 8/25 | Loss: 0.00146317
Iteration 9/25 | Loss: 0.00141200
Iteration 10/25 | Loss: 0.00143405
Iteration 11/25 | Loss: 0.00144160
Iteration 12/25 | Loss: 0.00139659
Iteration 13/25 | Loss: 0.00137954
Iteration 14/25 | Loss: 0.00140562
Iteration 15/25 | Loss: 0.00138522
Iteration 16/25 | Loss: 0.00137398
Iteration 17/25 | Loss: 0.00137580
Iteration 18/25 | Loss: 0.00137537
Iteration 19/25 | Loss: 0.00136209
Iteration 20/25 | Loss: 0.00135614
Iteration 21/25 | Loss: 0.00135298
Iteration 22/25 | Loss: 0.00135315
Iteration 23/25 | Loss: 0.00135033
Iteration 24/25 | Loss: 0.00135017
Iteration 25/25 | Loss: 0.00135523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43163538
Iteration 2/25 | Loss: 0.00409409
Iteration 3/25 | Loss: 0.00225045
Iteration 4/25 | Loss: 0.00225045
Iteration 5/25 | Loss: 0.00225045
Iteration 6/25 | Loss: 0.00225045
Iteration 7/25 | Loss: 0.00225045
Iteration 8/25 | Loss: 0.00225045
Iteration 9/25 | Loss: 0.00225045
Iteration 10/25 | Loss: 0.00225045
Iteration 11/25 | Loss: 0.00225045
Iteration 12/25 | Loss: 0.00225045
Iteration 13/25 | Loss: 0.00225045
Iteration 14/25 | Loss: 0.00225045
Iteration 15/25 | Loss: 0.00225045
Iteration 16/25 | Loss: 0.00225045
Iteration 17/25 | Loss: 0.00225045
Iteration 18/25 | Loss: 0.00225045
Iteration 19/25 | Loss: 0.00225045
Iteration 20/25 | Loss: 0.00225045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022504476364701986, 0.0022504476364701986, 0.0022504476364701986, 0.0022504476364701986, 0.0022504476364701986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022504476364701986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225045
Iteration 2/1000 | Loss: 0.00022312
Iteration 3/1000 | Loss: 0.00037643
Iteration 4/1000 | Loss: 0.00036255
Iteration 5/1000 | Loss: 0.00043175
Iteration 6/1000 | Loss: 0.00084656
Iteration 7/1000 | Loss: 0.00058401
Iteration 8/1000 | Loss: 0.00050593
Iteration 9/1000 | Loss: 0.00058686
Iteration 10/1000 | Loss: 0.00018201
Iteration 11/1000 | Loss: 0.00060379
Iteration 12/1000 | Loss: 0.00028130
Iteration 13/1000 | Loss: 0.00077529
Iteration 14/1000 | Loss: 0.00044706
Iteration 15/1000 | Loss: 0.00028476
Iteration 16/1000 | Loss: 0.00022320
Iteration 17/1000 | Loss: 0.00059598
Iteration 18/1000 | Loss: 0.00014711
Iteration 19/1000 | Loss: 0.00030454
Iteration 20/1000 | Loss: 0.00015222
Iteration 21/1000 | Loss: 0.00007880
Iteration 22/1000 | Loss: 0.00008233
Iteration 23/1000 | Loss: 0.00019234
Iteration 24/1000 | Loss: 0.00009380
Iteration 25/1000 | Loss: 0.00030948
Iteration 26/1000 | Loss: 0.00020893
Iteration 27/1000 | Loss: 0.00019529
Iteration 28/1000 | Loss: 0.00013689
Iteration 29/1000 | Loss: 0.00019352
Iteration 30/1000 | Loss: 0.00022965
Iteration 31/1000 | Loss: 0.00021169
Iteration 32/1000 | Loss: 0.00026210
Iteration 33/1000 | Loss: 0.00016006
Iteration 34/1000 | Loss: 0.00036228
Iteration 35/1000 | Loss: 0.00045572
Iteration 36/1000 | Loss: 0.00063897
Iteration 37/1000 | Loss: 0.00013661
Iteration 38/1000 | Loss: 0.00009798
Iteration 39/1000 | Loss: 0.00025773
Iteration 40/1000 | Loss: 0.00006846
Iteration 41/1000 | Loss: 0.00019357
Iteration 42/1000 | Loss: 0.00009326
Iteration 43/1000 | Loss: 0.00009864
Iteration 44/1000 | Loss: 0.00009557
Iteration 45/1000 | Loss: 0.00009496
Iteration 46/1000 | Loss: 0.00012696
Iteration 47/1000 | Loss: 0.00015517
Iteration 48/1000 | Loss: 0.00038578
Iteration 49/1000 | Loss: 0.00019702
Iteration 50/1000 | Loss: 0.00012749
Iteration 51/1000 | Loss: 0.00023638
Iteration 52/1000 | Loss: 0.00020473
Iteration 53/1000 | Loss: 0.00010385
Iteration 54/1000 | Loss: 0.00011787
Iteration 55/1000 | Loss: 0.00010159
Iteration 56/1000 | Loss: 0.00018537
Iteration 57/1000 | Loss: 0.00010308
Iteration 58/1000 | Loss: 0.00021109
Iteration 59/1000 | Loss: 0.00008770
Iteration 60/1000 | Loss: 0.00007141
Iteration 61/1000 | Loss: 0.00011806
Iteration 62/1000 | Loss: 0.00007907
Iteration 63/1000 | Loss: 0.00008537
Iteration 64/1000 | Loss: 0.00004877
Iteration 65/1000 | Loss: 0.00007512
Iteration 66/1000 | Loss: 0.00007837
Iteration 67/1000 | Loss: 0.00007984
Iteration 68/1000 | Loss: 0.00008437
Iteration 69/1000 | Loss: 0.00053916
Iteration 70/1000 | Loss: 0.00024931
Iteration 71/1000 | Loss: 0.00009935
Iteration 72/1000 | Loss: 0.00015803
Iteration 73/1000 | Loss: 0.00006989
Iteration 74/1000 | Loss: 0.00020725
Iteration 75/1000 | Loss: 0.00012558
Iteration 76/1000 | Loss: 0.00005370
Iteration 77/1000 | Loss: 0.00013655
Iteration 78/1000 | Loss: 0.00010792
Iteration 79/1000 | Loss: 0.00016081
Iteration 80/1000 | Loss: 0.00007699
Iteration 81/1000 | Loss: 0.00003683
Iteration 82/1000 | Loss: 0.00049127
Iteration 83/1000 | Loss: 0.00038546
Iteration 84/1000 | Loss: 0.00033388
Iteration 85/1000 | Loss: 0.00027959
Iteration 86/1000 | Loss: 0.00003234
Iteration 87/1000 | Loss: 0.00003017
Iteration 88/1000 | Loss: 0.00002864
Iteration 89/1000 | Loss: 0.00062525
Iteration 90/1000 | Loss: 0.00047380
Iteration 91/1000 | Loss: 0.00090501
Iteration 92/1000 | Loss: 0.00042387
Iteration 93/1000 | Loss: 0.00013496
Iteration 94/1000 | Loss: 0.00021652
Iteration 95/1000 | Loss: 0.00028740
Iteration 96/1000 | Loss: 0.00032090
Iteration 97/1000 | Loss: 0.00029310
Iteration 98/1000 | Loss: 0.00056042
Iteration 99/1000 | Loss: 0.00052970
Iteration 100/1000 | Loss: 0.00037557
Iteration 101/1000 | Loss: 0.00030209
Iteration 102/1000 | Loss: 0.00042664
Iteration 103/1000 | Loss: 0.00061140
Iteration 104/1000 | Loss: 0.00018691
Iteration 105/1000 | Loss: 0.00013694
Iteration 106/1000 | Loss: 0.00075302
Iteration 107/1000 | Loss: 0.00076070
Iteration 108/1000 | Loss: 0.00013170
Iteration 109/1000 | Loss: 0.00005027
Iteration 110/1000 | Loss: 0.00039720
Iteration 111/1000 | Loss: 0.00003162
Iteration 112/1000 | Loss: 0.00002972
Iteration 113/1000 | Loss: 0.00002878
Iteration 114/1000 | Loss: 0.00002778
Iteration 115/1000 | Loss: 0.00002702
Iteration 116/1000 | Loss: 0.00026765
Iteration 117/1000 | Loss: 0.00147103
Iteration 118/1000 | Loss: 0.00058373
Iteration 119/1000 | Loss: 0.00104633
Iteration 120/1000 | Loss: 0.00028294
Iteration 121/1000 | Loss: 0.00027704
Iteration 122/1000 | Loss: 0.00024235
Iteration 123/1000 | Loss: 0.00009316
Iteration 124/1000 | Loss: 0.00002861
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00002290
Iteration 128/1000 | Loss: 0.00002205
Iteration 129/1000 | Loss: 0.00002163
Iteration 130/1000 | Loss: 0.00002121
Iteration 131/1000 | Loss: 0.00002085
Iteration 132/1000 | Loss: 0.00002059
Iteration 133/1000 | Loss: 0.00061349
Iteration 134/1000 | Loss: 0.00027424
Iteration 135/1000 | Loss: 0.00052812
Iteration 136/1000 | Loss: 0.00029690
Iteration 137/1000 | Loss: 0.00038387
Iteration 138/1000 | Loss: 0.00023277
Iteration 139/1000 | Loss: 0.00046373
Iteration 140/1000 | Loss: 0.00003360
Iteration 141/1000 | Loss: 0.00002176
Iteration 142/1000 | Loss: 0.00001943
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001702
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001666
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001636
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001623
Iteration 152/1000 | Loss: 0.00001616
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001610
Iteration 157/1000 | Loss: 0.00001609
Iteration 158/1000 | Loss: 0.00001609
Iteration 159/1000 | Loss: 0.00001609
Iteration 160/1000 | Loss: 0.00001609
Iteration 161/1000 | Loss: 0.00001609
Iteration 162/1000 | Loss: 0.00001609
Iteration 163/1000 | Loss: 0.00001609
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Iteration 170/1000 | Loss: 0.00001607
Iteration 171/1000 | Loss: 0.00001607
Iteration 172/1000 | Loss: 0.00001607
Iteration 173/1000 | Loss: 0.00001607
Iteration 174/1000 | Loss: 0.00001607
Iteration 175/1000 | Loss: 0.00001607
Iteration 176/1000 | Loss: 0.00001607
Iteration 177/1000 | Loss: 0.00001607
Iteration 178/1000 | Loss: 0.00001607
Iteration 179/1000 | Loss: 0.00001606
Iteration 180/1000 | Loss: 0.00001606
Iteration 181/1000 | Loss: 0.00001606
Iteration 182/1000 | Loss: 0.00001606
Iteration 183/1000 | Loss: 0.00001606
Iteration 184/1000 | Loss: 0.00001606
Iteration 185/1000 | Loss: 0.00001606
Iteration 186/1000 | Loss: 0.00001606
Iteration 187/1000 | Loss: 0.00001606
Iteration 188/1000 | Loss: 0.00001606
Iteration 189/1000 | Loss: 0.00001606
Iteration 190/1000 | Loss: 0.00001606
Iteration 191/1000 | Loss: 0.00001606
Iteration 192/1000 | Loss: 0.00001606
Iteration 193/1000 | Loss: 0.00001606
Iteration 194/1000 | Loss: 0.00001606
Iteration 195/1000 | Loss: 0.00001606
Iteration 196/1000 | Loss: 0.00001606
Iteration 197/1000 | Loss: 0.00001606
Iteration 198/1000 | Loss: 0.00001606
Iteration 199/1000 | Loss: 0.00001606
Iteration 200/1000 | Loss: 0.00001606
Iteration 201/1000 | Loss: 0.00001606
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.6057005268521607e-05, 1.6057005268521607e-05, 1.6057005268521607e-05, 1.6057005268521607e-05, 1.6057005268521607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6057005268521607e-05

Optimization complete. Final v2v error: 3.206967353820801 mm

Highest mean error: 10.321968078613281 mm for frame 72

Lowest mean error: 2.699263095855713 mm for frame 118

Saving results

Total time: 261.7973017692566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834233
Iteration 2/25 | Loss: 0.00135848
Iteration 3/25 | Loss: 0.00128245
Iteration 4/25 | Loss: 0.00127314
Iteration 5/25 | Loss: 0.00127013
Iteration 6/25 | Loss: 0.00126964
Iteration 7/25 | Loss: 0.00126964
Iteration 8/25 | Loss: 0.00126964
Iteration 9/25 | Loss: 0.00126964
Iteration 10/25 | Loss: 0.00126964
Iteration 11/25 | Loss: 0.00126964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012696427293121815, 0.0012696427293121815, 0.0012696427293121815, 0.0012696427293121815, 0.0012696427293121815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012696427293121815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68898451
Iteration 2/25 | Loss: 0.00155108
Iteration 3/25 | Loss: 0.00155108
Iteration 4/25 | Loss: 0.00155108
Iteration 5/25 | Loss: 0.00155108
Iteration 6/25 | Loss: 0.00155108
Iteration 7/25 | Loss: 0.00155108
Iteration 8/25 | Loss: 0.00155108
Iteration 9/25 | Loss: 0.00155108
Iteration 10/25 | Loss: 0.00155108
Iteration 11/25 | Loss: 0.00155108
Iteration 12/25 | Loss: 0.00155108
Iteration 13/25 | Loss: 0.00155108
Iteration 14/25 | Loss: 0.00155108
Iteration 15/25 | Loss: 0.00155108
Iteration 16/25 | Loss: 0.00155108
Iteration 17/25 | Loss: 0.00155108
Iteration 18/25 | Loss: 0.00155108
Iteration 19/25 | Loss: 0.00155108
Iteration 20/25 | Loss: 0.00155108
Iteration 21/25 | Loss: 0.00155108
Iteration 22/25 | Loss: 0.00155108
Iteration 23/25 | Loss: 0.00155108
Iteration 24/25 | Loss: 0.00155108
Iteration 25/25 | Loss: 0.00155108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155108
Iteration 2/1000 | Loss: 0.00002073
Iteration 3/1000 | Loss: 0.00001591
Iteration 4/1000 | Loss: 0.00001465
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001184
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001145
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001135
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001124
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001120
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001114
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001111
Iteration 56/1000 | Loss: 0.00001111
Iteration 57/1000 | Loss: 0.00001110
Iteration 58/1000 | Loss: 0.00001110
Iteration 59/1000 | Loss: 0.00001110
Iteration 60/1000 | Loss: 0.00001109
Iteration 61/1000 | Loss: 0.00001109
Iteration 62/1000 | Loss: 0.00001109
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001108
Iteration 71/1000 | Loss: 0.00001107
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001107
Iteration 74/1000 | Loss: 0.00001107
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001104
Iteration 81/1000 | Loss: 0.00001104
Iteration 82/1000 | Loss: 0.00001104
Iteration 83/1000 | Loss: 0.00001104
Iteration 84/1000 | Loss: 0.00001104
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001100
Iteration 98/1000 | Loss: 0.00001100
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001097
Iteration 102/1000 | Loss: 0.00001097
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001097
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001097
Iteration 107/1000 | Loss: 0.00001097
Iteration 108/1000 | Loss: 0.00001097
Iteration 109/1000 | Loss: 0.00001097
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001094
Iteration 116/1000 | Loss: 0.00001094
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001092
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001092
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001091
Iteration 126/1000 | Loss: 0.00001090
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Iteration 130/1000 | Loss: 0.00001090
Iteration 131/1000 | Loss: 0.00001090
Iteration 132/1000 | Loss: 0.00001090
Iteration 133/1000 | Loss: 0.00001090
Iteration 134/1000 | Loss: 0.00001090
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001089
Iteration 138/1000 | Loss: 0.00001089
Iteration 139/1000 | Loss: 0.00001089
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001089
Iteration 144/1000 | Loss: 0.00001089
Iteration 145/1000 | Loss: 0.00001089
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001089
Iteration 150/1000 | Loss: 0.00001089
Iteration 151/1000 | Loss: 0.00001089
Iteration 152/1000 | Loss: 0.00001089
Iteration 153/1000 | Loss: 0.00001089
Iteration 154/1000 | Loss: 0.00001089
Iteration 155/1000 | Loss: 0.00001089
Iteration 156/1000 | Loss: 0.00001089
Iteration 157/1000 | Loss: 0.00001089
Iteration 158/1000 | Loss: 0.00001089
Iteration 159/1000 | Loss: 0.00001089
Iteration 160/1000 | Loss: 0.00001089
Iteration 161/1000 | Loss: 0.00001089
Iteration 162/1000 | Loss: 0.00001089
Iteration 163/1000 | Loss: 0.00001089
Iteration 164/1000 | Loss: 0.00001089
Iteration 165/1000 | Loss: 0.00001089
Iteration 166/1000 | Loss: 0.00001089
Iteration 167/1000 | Loss: 0.00001089
Iteration 168/1000 | Loss: 0.00001089
Iteration 169/1000 | Loss: 0.00001089
Iteration 170/1000 | Loss: 0.00001089
Iteration 171/1000 | Loss: 0.00001089
Iteration 172/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.0894794286286924e-05, 1.0894794286286924e-05, 1.0894794286286924e-05, 1.0894794286286924e-05, 1.0894794286286924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0894794286286924e-05

Optimization complete. Final v2v error: 2.839656114578247 mm

Highest mean error: 3.335642099380493 mm for frame 55

Lowest mean error: 2.654963970184326 mm for frame 11

Saving results

Total time: 36.91352725028992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923897
Iteration 2/25 | Loss: 0.00275989
Iteration 3/25 | Loss: 0.00192891
Iteration 4/25 | Loss: 0.00182178
Iteration 5/25 | Loss: 0.00178750
Iteration 6/25 | Loss: 0.00183006
Iteration 7/25 | Loss: 0.00164719
Iteration 8/25 | Loss: 0.00157235
Iteration 9/25 | Loss: 0.00154384
Iteration 10/25 | Loss: 0.00151273
Iteration 11/25 | Loss: 0.00147983
Iteration 12/25 | Loss: 0.00151125
Iteration 13/25 | Loss: 0.00150739
Iteration 14/25 | Loss: 0.00148040
Iteration 15/25 | Loss: 0.00145147
Iteration 16/25 | Loss: 0.00143623
Iteration 17/25 | Loss: 0.00142436
Iteration 18/25 | Loss: 0.00142146
Iteration 19/25 | Loss: 0.00140803
Iteration 20/25 | Loss: 0.00140321
Iteration 21/25 | Loss: 0.00139293
Iteration 22/25 | Loss: 0.00138978
Iteration 23/25 | Loss: 0.00138700
Iteration 24/25 | Loss: 0.00138549
Iteration 25/25 | Loss: 0.00138565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10824966
Iteration 2/25 | Loss: 0.00188298
Iteration 3/25 | Loss: 0.00186686
Iteration 4/25 | Loss: 0.00186683
Iteration 5/25 | Loss: 0.00186682
Iteration 6/25 | Loss: 0.00186682
Iteration 7/25 | Loss: 0.00186682
Iteration 8/25 | Loss: 0.00186682
Iteration 9/25 | Loss: 0.00186682
Iteration 10/25 | Loss: 0.00186682
Iteration 11/25 | Loss: 0.00186682
Iteration 12/25 | Loss: 0.00186682
Iteration 13/25 | Loss: 0.00186682
Iteration 14/25 | Loss: 0.00186682
Iteration 15/25 | Loss: 0.00186682
Iteration 16/25 | Loss: 0.00186682
Iteration 17/25 | Loss: 0.00186682
Iteration 18/25 | Loss: 0.00186682
Iteration 19/25 | Loss: 0.00186682
Iteration 20/25 | Loss: 0.00186682
Iteration 21/25 | Loss: 0.00186682
Iteration 22/25 | Loss: 0.00186682
Iteration 23/25 | Loss: 0.00186682
Iteration 24/25 | Loss: 0.00186682
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0018668222473934293, 0.0018668222473934293, 0.0018668222473934293, 0.0018668222473934293, 0.0018668222473934293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018668222473934293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186682
Iteration 2/1000 | Loss: 0.00030904
Iteration 3/1000 | Loss: 0.00013690
Iteration 4/1000 | Loss: 0.00008440
Iteration 5/1000 | Loss: 0.00027670
Iteration 6/1000 | Loss: 0.00061050
Iteration 7/1000 | Loss: 0.00014501
Iteration 8/1000 | Loss: 0.00009647
Iteration 9/1000 | Loss: 0.00007993
Iteration 10/1000 | Loss: 0.00040072
Iteration 11/1000 | Loss: 0.00028670
Iteration 12/1000 | Loss: 0.00030016
Iteration 13/1000 | Loss: 0.00021874
Iteration 14/1000 | Loss: 0.00007549
Iteration 15/1000 | Loss: 0.00007286
Iteration 16/1000 | Loss: 0.00041089
Iteration 17/1000 | Loss: 0.00021131
Iteration 18/1000 | Loss: 0.00032730
Iteration 19/1000 | Loss: 0.00012413
Iteration 20/1000 | Loss: 0.00006566
Iteration 21/1000 | Loss: 0.00005665
Iteration 22/1000 | Loss: 0.00005484
Iteration 23/1000 | Loss: 0.00006019
Iteration 24/1000 | Loss: 0.00004906
Iteration 25/1000 | Loss: 0.00042572
Iteration 26/1000 | Loss: 0.00007605
Iteration 27/1000 | Loss: 0.00007360
Iteration 28/1000 | Loss: 0.00005444
Iteration 29/1000 | Loss: 0.00006214
Iteration 30/1000 | Loss: 0.00006249
Iteration 31/1000 | Loss: 0.00007426
Iteration 32/1000 | Loss: 0.00006886
Iteration 33/1000 | Loss: 0.00005245
Iteration 34/1000 | Loss: 0.00005877
Iteration 35/1000 | Loss: 0.00005366
Iteration 36/1000 | Loss: 0.00006715
Iteration 37/1000 | Loss: 0.00007087
Iteration 38/1000 | Loss: 0.00007018
Iteration 39/1000 | Loss: 0.00005833
Iteration 40/1000 | Loss: 0.00007004
Iteration 41/1000 | Loss: 0.00006894
Iteration 42/1000 | Loss: 0.00006670
Iteration 43/1000 | Loss: 0.00005650
Iteration 44/1000 | Loss: 0.00005788
Iteration 45/1000 | Loss: 0.00006537
Iteration 46/1000 | Loss: 0.00006389
Iteration 47/1000 | Loss: 0.00006720
Iteration 48/1000 | Loss: 0.00006164
Iteration 49/1000 | Loss: 0.00006887
Iteration 50/1000 | Loss: 0.00005642
Iteration 51/1000 | Loss: 0.00006479
Iteration 52/1000 | Loss: 0.00005432
Iteration 53/1000 | Loss: 0.00007899
Iteration 54/1000 | Loss: 0.00005804
Iteration 55/1000 | Loss: 0.00005979
Iteration 56/1000 | Loss: 0.00006499
Iteration 57/1000 | Loss: 0.00006496
Iteration 58/1000 | Loss: 0.00006355
Iteration 59/1000 | Loss: 0.00006656
Iteration 60/1000 | Loss: 0.00007135
Iteration 61/1000 | Loss: 0.00006296
Iteration 62/1000 | Loss: 0.00006936
Iteration 63/1000 | Loss: 0.00006253
Iteration 64/1000 | Loss: 0.00005788
Iteration 65/1000 | Loss: 0.00006861
Iteration 66/1000 | Loss: 0.00006989
Iteration 67/1000 | Loss: 0.00007753
Iteration 68/1000 | Loss: 0.00007432
Iteration 69/1000 | Loss: 0.00006390
Iteration 70/1000 | Loss: 0.00006782
Iteration 71/1000 | Loss: 0.00006675
Iteration 72/1000 | Loss: 0.00007073
Iteration 73/1000 | Loss: 0.00006544
Iteration 74/1000 | Loss: 0.00004929
Iteration 75/1000 | Loss: 0.00005056
Iteration 76/1000 | Loss: 0.00007279
Iteration 77/1000 | Loss: 0.00006648
Iteration 78/1000 | Loss: 0.00006816
Iteration 79/1000 | Loss: 0.00005807
Iteration 80/1000 | Loss: 0.00006518
Iteration 81/1000 | Loss: 0.00006766
Iteration 82/1000 | Loss: 0.00006338
Iteration 83/1000 | Loss: 0.00006565
Iteration 84/1000 | Loss: 0.00006665
Iteration 85/1000 | Loss: 0.00006610
Iteration 86/1000 | Loss: 0.00006667
Iteration 87/1000 | Loss: 0.00006659
Iteration 88/1000 | Loss: 0.00007007
Iteration 89/1000 | Loss: 0.00006604
Iteration 90/1000 | Loss: 0.00005872
Iteration 91/1000 | Loss: 0.00005868
Iteration 92/1000 | Loss: 0.00006464
Iteration 93/1000 | Loss: 0.00006701
Iteration 94/1000 | Loss: 0.00006290
Iteration 95/1000 | Loss: 0.00006557
Iteration 96/1000 | Loss: 0.00007500
Iteration 97/1000 | Loss: 0.00006728
Iteration 98/1000 | Loss: 0.00005863
Iteration 99/1000 | Loss: 0.00006722
Iteration 100/1000 | Loss: 0.00006275
Iteration 101/1000 | Loss: 0.00006545
Iteration 102/1000 | Loss: 0.00006630
Iteration 103/1000 | Loss: 0.00006604
Iteration 104/1000 | Loss: 0.00007545
Iteration 105/1000 | Loss: 0.00007076
Iteration 106/1000 | Loss: 0.00007357
Iteration 107/1000 | Loss: 0.00008267
Iteration 108/1000 | Loss: 0.00007029
Iteration 109/1000 | Loss: 0.00007178
Iteration 110/1000 | Loss: 0.00007705
Iteration 111/1000 | Loss: 0.00006783
Iteration 112/1000 | Loss: 0.00006780
Iteration 113/1000 | Loss: 0.00006787
Iteration 114/1000 | Loss: 0.00006757
Iteration 115/1000 | Loss: 0.00006845
Iteration 116/1000 | Loss: 0.00006694
Iteration 117/1000 | Loss: 0.00006895
Iteration 118/1000 | Loss: 0.00006725
Iteration 119/1000 | Loss: 0.00006939
Iteration 120/1000 | Loss: 0.00006119
Iteration 121/1000 | Loss: 0.00007096
Iteration 122/1000 | Loss: 0.00005404
Iteration 123/1000 | Loss: 0.00005561
Iteration 124/1000 | Loss: 0.00006775
Iteration 125/1000 | Loss: 0.00007252
Iteration 126/1000 | Loss: 0.00005673
Iteration 127/1000 | Loss: 0.00005494
Iteration 128/1000 | Loss: 0.00005495
Iteration 129/1000 | Loss: 0.00007502
Iteration 130/1000 | Loss: 0.00005613
Iteration 131/1000 | Loss: 0.00005877
Iteration 132/1000 | Loss: 0.00005633
Iteration 133/1000 | Loss: 0.00006281
Iteration 134/1000 | Loss: 0.00007038
Iteration 135/1000 | Loss: 0.00006387
Iteration 136/1000 | Loss: 0.00006811
Iteration 137/1000 | Loss: 0.00006508
Iteration 138/1000 | Loss: 0.00006628
Iteration 139/1000 | Loss: 0.00007020
Iteration 140/1000 | Loss: 0.00007296
Iteration 141/1000 | Loss: 0.00007000
Iteration 142/1000 | Loss: 0.00007071
Iteration 143/1000 | Loss: 0.00007712
Iteration 144/1000 | Loss: 0.00006761
Iteration 145/1000 | Loss: 0.00007142
Iteration 146/1000 | Loss: 0.00006246
Iteration 147/1000 | Loss: 0.00005509
Iteration 148/1000 | Loss: 0.00006786
Iteration 149/1000 | Loss: 0.00005673
Iteration 150/1000 | Loss: 0.00007079
Iteration 151/1000 | Loss: 0.00008756
Iteration 152/1000 | Loss: 0.00007218
Iteration 153/1000 | Loss: 0.00007650
Iteration 154/1000 | Loss: 0.00007494
Iteration 155/1000 | Loss: 0.00007058
Iteration 156/1000 | Loss: 0.00008562
Iteration 157/1000 | Loss: 0.00005839
Iteration 158/1000 | Loss: 0.00006366
Iteration 159/1000 | Loss: 0.00005589
Iteration 160/1000 | Loss: 0.00007693
Iteration 161/1000 | Loss: 0.00007004
Iteration 162/1000 | Loss: 0.00009026
Iteration 163/1000 | Loss: 0.00006643
Iteration 164/1000 | Loss: 0.00004430
Iteration 165/1000 | Loss: 0.00004116
Iteration 166/1000 | Loss: 0.00003924
Iteration 167/1000 | Loss: 0.00005882
Iteration 168/1000 | Loss: 0.00004213
Iteration 169/1000 | Loss: 0.00006337
Iteration 170/1000 | Loss: 0.00006111
Iteration 171/1000 | Loss: 0.00003922
Iteration 172/1000 | Loss: 0.00004808
Iteration 173/1000 | Loss: 0.00003739
Iteration 174/1000 | Loss: 0.00003656
Iteration 175/1000 | Loss: 0.00003615
Iteration 176/1000 | Loss: 0.00003587
Iteration 177/1000 | Loss: 0.00003549
Iteration 178/1000 | Loss: 0.00003519
Iteration 179/1000 | Loss: 0.00003516
Iteration 180/1000 | Loss: 0.00003497
Iteration 181/1000 | Loss: 0.00003480
Iteration 182/1000 | Loss: 0.00069313
Iteration 183/1000 | Loss: 0.00006853
Iteration 184/1000 | Loss: 0.00004655
Iteration 185/1000 | Loss: 0.00004293
Iteration 186/1000 | Loss: 0.00003776
Iteration 187/1000 | Loss: 0.00003529
Iteration 188/1000 | Loss: 0.00003412
Iteration 189/1000 | Loss: 0.00019903
Iteration 190/1000 | Loss: 0.00015574
Iteration 191/1000 | Loss: 0.00003292
Iteration 192/1000 | Loss: 0.00003258
Iteration 193/1000 | Loss: 0.00003239
Iteration 194/1000 | Loss: 0.00003234
Iteration 195/1000 | Loss: 0.00019798
Iteration 196/1000 | Loss: 0.00016346
Iteration 197/1000 | Loss: 0.00029130
Iteration 198/1000 | Loss: 0.00017687
Iteration 199/1000 | Loss: 0.00025228
Iteration 200/1000 | Loss: 0.00023656
Iteration 201/1000 | Loss: 0.00024077
Iteration 202/1000 | Loss: 0.00021752
Iteration 203/1000 | Loss: 0.00024539
Iteration 204/1000 | Loss: 0.00012802
Iteration 205/1000 | Loss: 0.00025939
Iteration 206/1000 | Loss: 0.00012737
Iteration 207/1000 | Loss: 0.00003964
Iteration 208/1000 | Loss: 0.00003345
Iteration 209/1000 | Loss: 0.00003262
Iteration 210/1000 | Loss: 0.00003232
Iteration 211/1000 | Loss: 0.00003213
Iteration 212/1000 | Loss: 0.00003205
Iteration 213/1000 | Loss: 0.00003203
Iteration 214/1000 | Loss: 0.00003201
Iteration 215/1000 | Loss: 0.00003199
Iteration 216/1000 | Loss: 0.00003199
Iteration 217/1000 | Loss: 0.00003198
Iteration 218/1000 | Loss: 0.00003198
Iteration 219/1000 | Loss: 0.00003197
Iteration 220/1000 | Loss: 0.00003197
Iteration 221/1000 | Loss: 0.00003196
Iteration 222/1000 | Loss: 0.00003196
Iteration 223/1000 | Loss: 0.00003194
Iteration 224/1000 | Loss: 0.00003192
Iteration 225/1000 | Loss: 0.00003192
Iteration 226/1000 | Loss: 0.00003190
Iteration 227/1000 | Loss: 0.00003190
Iteration 228/1000 | Loss: 0.00003189
Iteration 229/1000 | Loss: 0.00003189
Iteration 230/1000 | Loss: 0.00003189
Iteration 231/1000 | Loss: 0.00003188
Iteration 232/1000 | Loss: 0.00003185
Iteration 233/1000 | Loss: 0.00003182
Iteration 234/1000 | Loss: 0.00003182
Iteration 235/1000 | Loss: 0.00003181
Iteration 236/1000 | Loss: 0.00003181
Iteration 237/1000 | Loss: 0.00003180
Iteration 238/1000 | Loss: 0.00020652
Iteration 239/1000 | Loss: 0.00006434
Iteration 240/1000 | Loss: 0.00003443
Iteration 241/1000 | Loss: 0.00003241
Iteration 242/1000 | Loss: 0.00003185
Iteration 243/1000 | Loss: 0.00021406
Iteration 244/1000 | Loss: 0.00021406
Iteration 245/1000 | Loss: 0.00072227
Iteration 246/1000 | Loss: 0.00105384
Iteration 247/1000 | Loss: 0.00153750
Iteration 248/1000 | Loss: 0.00068039
Iteration 249/1000 | Loss: 0.00104142
Iteration 250/1000 | Loss: 0.00143287
Iteration 251/1000 | Loss: 0.00025235
Iteration 252/1000 | Loss: 0.00025399
Iteration 253/1000 | Loss: 0.00019529
Iteration 254/1000 | Loss: 0.00009073
Iteration 255/1000 | Loss: 0.00011234
Iteration 256/1000 | Loss: 0.00004915
Iteration 257/1000 | Loss: 0.00015518
Iteration 258/1000 | Loss: 0.00004096
Iteration 259/1000 | Loss: 0.00003426
Iteration 260/1000 | Loss: 0.00006582
Iteration 261/1000 | Loss: 0.00002906
Iteration 262/1000 | Loss: 0.00002745
Iteration 263/1000 | Loss: 0.00002685
Iteration 264/1000 | Loss: 0.00002646
Iteration 265/1000 | Loss: 0.00002616
Iteration 266/1000 | Loss: 0.00002571
Iteration 267/1000 | Loss: 0.00002534
Iteration 268/1000 | Loss: 0.00002502
Iteration 269/1000 | Loss: 0.00002496
Iteration 270/1000 | Loss: 0.00002487
Iteration 271/1000 | Loss: 0.00002484
Iteration 272/1000 | Loss: 0.00002479
Iteration 273/1000 | Loss: 0.00002465
Iteration 274/1000 | Loss: 0.00002452
Iteration 275/1000 | Loss: 0.00002441
Iteration 276/1000 | Loss: 0.00002439
Iteration 277/1000 | Loss: 0.00002437
Iteration 278/1000 | Loss: 0.00002431
Iteration 279/1000 | Loss: 0.00002426
Iteration 280/1000 | Loss: 0.00002424
Iteration 281/1000 | Loss: 0.00002418
Iteration 282/1000 | Loss: 0.00002409
Iteration 283/1000 | Loss: 0.00002403
Iteration 284/1000 | Loss: 0.00014243
Iteration 285/1000 | Loss: 0.00022926
Iteration 286/1000 | Loss: 0.00016026
Iteration 287/1000 | Loss: 0.00018303
Iteration 288/1000 | Loss: 0.00014265
Iteration 289/1000 | Loss: 0.00018199
Iteration 290/1000 | Loss: 0.00011123
Iteration 291/1000 | Loss: 0.00018238
Iteration 292/1000 | Loss: 0.00011036
Iteration 293/1000 | Loss: 0.00017791
Iteration 294/1000 | Loss: 0.00004542
Iteration 295/1000 | Loss: 0.00003127
Iteration 296/1000 | Loss: 0.00002773
Iteration 297/1000 | Loss: 0.00002649
Iteration 298/1000 | Loss: 0.00002583
Iteration 299/1000 | Loss: 0.00002509
Iteration 300/1000 | Loss: 0.00002467
Iteration 301/1000 | Loss: 0.00002427
Iteration 302/1000 | Loss: 0.00002851
Iteration 303/1000 | Loss: 0.00002443
Iteration 304/1000 | Loss: 0.00002400
Iteration 305/1000 | Loss: 0.00002355
Iteration 306/1000 | Loss: 0.00002313
Iteration 307/1000 | Loss: 0.00002305
Iteration 308/1000 | Loss: 0.00002301
Iteration 309/1000 | Loss: 0.00002295
Iteration 310/1000 | Loss: 0.00002295
Iteration 311/1000 | Loss: 0.00002291
Iteration 312/1000 | Loss: 0.00002291
Iteration 313/1000 | Loss: 0.00002291
Iteration 314/1000 | Loss: 0.00002290
Iteration 315/1000 | Loss: 0.00002289
Iteration 316/1000 | Loss: 0.00002288
Iteration 317/1000 | Loss: 0.00002288
Iteration 318/1000 | Loss: 0.00002287
Iteration 319/1000 | Loss: 0.00002287
Iteration 320/1000 | Loss: 0.00002286
Iteration 321/1000 | Loss: 0.00002286
Iteration 322/1000 | Loss: 0.00002285
Iteration 323/1000 | Loss: 0.00002285
Iteration 324/1000 | Loss: 0.00002285
Iteration 325/1000 | Loss: 0.00002284
Iteration 326/1000 | Loss: 0.00002284
Iteration 327/1000 | Loss: 0.00002283
Iteration 328/1000 | Loss: 0.00002282
Iteration 329/1000 | Loss: 0.00002282
Iteration 330/1000 | Loss: 0.00002279
Iteration 331/1000 | Loss: 0.00002278
Iteration 332/1000 | Loss: 0.00002278
Iteration 333/1000 | Loss: 0.00002278
Iteration 334/1000 | Loss: 0.00002277
Iteration 335/1000 | Loss: 0.00002277
Iteration 336/1000 | Loss: 0.00002276
Iteration 337/1000 | Loss: 0.00002275
Iteration 338/1000 | Loss: 0.00002273
Iteration 339/1000 | Loss: 0.00002272
Iteration 340/1000 | Loss: 0.00002271
Iteration 341/1000 | Loss: 0.00002270
Iteration 342/1000 | Loss: 0.00002269
Iteration 343/1000 | Loss: 0.00002268
Iteration 344/1000 | Loss: 0.00002268
Iteration 345/1000 | Loss: 0.00002267
Iteration 346/1000 | Loss: 0.00002263
Iteration 347/1000 | Loss: 0.00002262
Iteration 348/1000 | Loss: 0.00002262
Iteration 349/1000 | Loss: 0.00002258
Iteration 350/1000 | Loss: 0.00002256
Iteration 351/1000 | Loss: 0.00002256
Iteration 352/1000 | Loss: 0.00002255
Iteration 353/1000 | Loss: 0.00002255
Iteration 354/1000 | Loss: 0.00002255
Iteration 355/1000 | Loss: 0.00002254
Iteration 356/1000 | Loss: 0.00002254
Iteration 357/1000 | Loss: 0.00002254
Iteration 358/1000 | Loss: 0.00002254
Iteration 359/1000 | Loss: 0.00002254
Iteration 360/1000 | Loss: 0.00002253
Iteration 361/1000 | Loss: 0.00002253
Iteration 362/1000 | Loss: 0.00002253
Iteration 363/1000 | Loss: 0.00002253
Iteration 364/1000 | Loss: 0.00002253
Iteration 365/1000 | Loss: 0.00002252
Iteration 366/1000 | Loss: 0.00002252
Iteration 367/1000 | Loss: 0.00002252
Iteration 368/1000 | Loss: 0.00002252
Iteration 369/1000 | Loss: 0.00002252
Iteration 370/1000 | Loss: 0.00002252
Iteration 371/1000 | Loss: 0.00002252
Iteration 372/1000 | Loss: 0.00002252
Iteration 373/1000 | Loss: 0.00002251
Iteration 374/1000 | Loss: 0.00002251
Iteration 375/1000 | Loss: 0.00002251
Iteration 376/1000 | Loss: 0.00002251
Iteration 377/1000 | Loss: 0.00002251
Iteration 378/1000 | Loss: 0.00002251
Iteration 379/1000 | Loss: 0.00002251
Iteration 380/1000 | Loss: 0.00002251
Iteration 381/1000 | Loss: 0.00002251
Iteration 382/1000 | Loss: 0.00002251
Iteration 383/1000 | Loss: 0.00002251
Iteration 384/1000 | Loss: 0.00002250
Iteration 385/1000 | Loss: 0.00002250
Iteration 386/1000 | Loss: 0.00002250
Iteration 387/1000 | Loss: 0.00002250
Iteration 388/1000 | Loss: 0.00002250
Iteration 389/1000 | Loss: 0.00002250
Iteration 390/1000 | Loss: 0.00002250
Iteration 391/1000 | Loss: 0.00002250
Iteration 392/1000 | Loss: 0.00002250
Iteration 393/1000 | Loss: 0.00002250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [2.2502146748593077e-05, 2.2502146748593077e-05, 2.2502146748593077e-05, 2.2502146748593077e-05, 2.2502146748593077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2502146748593077e-05

Optimization complete. Final v2v error: 3.8503243923187256 mm

Highest mean error: 5.381359577178955 mm for frame 48

Lowest mean error: 2.7808005809783936 mm for frame 92

Saving results

Total time: 445.85469698905945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003206
Iteration 2/25 | Loss: 0.00189877
Iteration 3/25 | Loss: 0.00167008
Iteration 4/25 | Loss: 0.00155910
Iteration 5/25 | Loss: 0.00147707
Iteration 6/25 | Loss: 0.00141866
Iteration 7/25 | Loss: 0.00137165
Iteration 8/25 | Loss: 0.00135843
Iteration 9/25 | Loss: 0.00136092
Iteration 10/25 | Loss: 0.00134978
Iteration 11/25 | Loss: 0.00134545
Iteration 12/25 | Loss: 0.00133406
Iteration 13/25 | Loss: 0.00132892
Iteration 14/25 | Loss: 0.00132726
Iteration 15/25 | Loss: 0.00132336
Iteration 16/25 | Loss: 0.00131790
Iteration 17/25 | Loss: 0.00131654
Iteration 18/25 | Loss: 0.00131695
Iteration 19/25 | Loss: 0.00131309
Iteration 20/25 | Loss: 0.00131229
Iteration 21/25 | Loss: 0.00131203
Iteration 22/25 | Loss: 0.00131189
Iteration 23/25 | Loss: 0.00131161
Iteration 24/25 | Loss: 0.00131740
Iteration 25/25 | Loss: 0.00131433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36634827
Iteration 2/25 | Loss: 0.00174888
Iteration 3/25 | Loss: 0.00174888
Iteration 4/25 | Loss: 0.00163714
Iteration 5/25 | Loss: 0.00163711
Iteration 6/25 | Loss: 0.00163711
Iteration 7/25 | Loss: 0.00163711
Iteration 8/25 | Loss: 0.00163711
Iteration 9/25 | Loss: 0.00163711
Iteration 10/25 | Loss: 0.00163711
Iteration 11/25 | Loss: 0.00163710
Iteration 12/25 | Loss: 0.00163710
Iteration 13/25 | Loss: 0.00163710
Iteration 14/25 | Loss: 0.00163710
Iteration 15/25 | Loss: 0.00163710
Iteration 16/25 | Loss: 0.00163710
Iteration 17/25 | Loss: 0.00163710
Iteration 18/25 | Loss: 0.00163710
Iteration 19/25 | Loss: 0.00163710
Iteration 20/25 | Loss: 0.00163710
Iteration 21/25 | Loss: 0.00163710
Iteration 22/25 | Loss: 0.00163710
Iteration 23/25 | Loss: 0.00163710
Iteration 24/25 | Loss: 0.00163710
Iteration 25/25 | Loss: 0.00163710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163710
Iteration 2/1000 | Loss: 0.00016520
Iteration 3/1000 | Loss: 0.00057270
Iteration 4/1000 | Loss: 0.00002403
Iteration 5/1000 | Loss: 0.00028002
Iteration 6/1000 | Loss: 0.00002895
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002097
Iteration 9/1000 | Loss: 0.00016558
Iteration 10/1000 | Loss: 0.00027977
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00024163
Iteration 14/1000 | Loss: 0.00014689
Iteration 15/1000 | Loss: 0.00011132
Iteration 16/1000 | Loss: 0.00001864
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00007938
Iteration 19/1000 | Loss: 0.00001592
Iteration 20/1000 | Loss: 0.00041041
Iteration 21/1000 | Loss: 0.00028494
Iteration 22/1000 | Loss: 0.00003269
Iteration 23/1000 | Loss: 0.00003034
Iteration 24/1000 | Loss: 0.00032361
Iteration 25/1000 | Loss: 0.00025358
Iteration 26/1000 | Loss: 0.00006147
Iteration 27/1000 | Loss: 0.00008398
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00002943
Iteration 31/1000 | Loss: 0.00001553
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001297
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001294
Iteration 47/1000 | Loss: 0.00001294
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001290
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001280
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001278
Iteration 98/1000 | Loss: 0.00001278
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001275
Iteration 104/1000 | Loss: 0.00001275
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001275
Iteration 107/1000 | Loss: 0.00001274
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001273
Iteration 125/1000 | Loss: 0.00001273
Iteration 126/1000 | Loss: 0.00001273
Iteration 127/1000 | Loss: 0.00001273
Iteration 128/1000 | Loss: 0.00001273
Iteration 129/1000 | Loss: 0.00001273
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001273
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001273
Iteration 141/1000 | Loss: 0.00001273
Iteration 142/1000 | Loss: 0.00001273
Iteration 143/1000 | Loss: 0.00001273
Iteration 144/1000 | Loss: 0.00001273
Iteration 145/1000 | Loss: 0.00001273
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001273
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.2729128684441093e-05, 1.2729128684441093e-05, 1.2729128684441093e-05, 1.2729128684441093e-05, 1.2729128684441093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2729128684441093e-05

Optimization complete. Final v2v error: 3.041229724884033 mm

Highest mean error: 4.153660297393799 mm for frame 31

Lowest mean error: 2.762153387069702 mm for frame 26

Saving results

Total time: 105.66295027732849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419766
Iteration 2/25 | Loss: 0.00137045
Iteration 3/25 | Loss: 0.00129203
Iteration 4/25 | Loss: 0.00128693
Iteration 5/25 | Loss: 0.00128657
Iteration 6/25 | Loss: 0.00128658
Iteration 7/25 | Loss: 0.00128658
Iteration 8/25 | Loss: 0.00128658
Iteration 9/25 | Loss: 0.00128657
Iteration 10/25 | Loss: 0.00128658
Iteration 11/25 | Loss: 0.00128658
Iteration 12/25 | Loss: 0.00128658
Iteration 13/25 | Loss: 0.00128657
Iteration 14/25 | Loss: 0.00128658
Iteration 15/25 | Loss: 0.00128657
Iteration 16/25 | Loss: 0.00128657
Iteration 17/25 | Loss: 0.00128657
Iteration 18/25 | Loss: 0.00128657
Iteration 19/25 | Loss: 0.00128657
Iteration 20/25 | Loss: 0.00128657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012865749886259437, 0.0012865749886259437, 0.0012865749886259437, 0.0012865749886259437, 0.0012865749886259437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012865749886259437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39214694
Iteration 2/25 | Loss: 0.00129367
Iteration 3/25 | Loss: 0.00129367
Iteration 4/25 | Loss: 0.00129367
Iteration 5/25 | Loss: 0.00129367
Iteration 6/25 | Loss: 0.00129367
Iteration 7/25 | Loss: 0.00129367
Iteration 8/25 | Loss: 0.00129367
Iteration 9/25 | Loss: 0.00129367
Iteration 10/25 | Loss: 0.00129367
Iteration 11/25 | Loss: 0.00129367
Iteration 12/25 | Loss: 0.00129367
Iteration 13/25 | Loss: 0.00129367
Iteration 14/25 | Loss: 0.00129367
Iteration 15/25 | Loss: 0.00129367
Iteration 16/25 | Loss: 0.00129367
Iteration 17/25 | Loss: 0.00129367
Iteration 18/25 | Loss: 0.00129367
Iteration 19/25 | Loss: 0.00129367
Iteration 20/25 | Loss: 0.00129367
Iteration 21/25 | Loss: 0.00129367
Iteration 22/25 | Loss: 0.00129367
Iteration 23/25 | Loss: 0.00129367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012936717830598354, 0.0012936717830598354, 0.0012936717830598354, 0.0012936717830598354, 0.0012936717830598354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012936717830598354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129367
Iteration 2/1000 | Loss: 0.00002637
Iteration 3/1000 | Loss: 0.00001623
Iteration 4/1000 | Loss: 0.00001450
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001263
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001110
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001094
Iteration 28/1000 | Loss: 0.00001087
Iteration 29/1000 | Loss: 0.00001087
Iteration 30/1000 | Loss: 0.00001084
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001082
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001077
Iteration 45/1000 | Loss: 0.00001077
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001073
Iteration 51/1000 | Loss: 0.00001073
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001067
Iteration 59/1000 | Loss: 0.00001067
Iteration 60/1000 | Loss: 0.00001066
Iteration 61/1000 | Loss: 0.00001066
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001059
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001058
Iteration 73/1000 | Loss: 0.00001058
Iteration 74/1000 | Loss: 0.00001058
Iteration 75/1000 | Loss: 0.00001058
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001056
Iteration 80/1000 | Loss: 0.00001055
Iteration 81/1000 | Loss: 0.00001055
Iteration 82/1000 | Loss: 0.00001055
Iteration 83/1000 | Loss: 0.00001055
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001052
Iteration 92/1000 | Loss: 0.00001052
Iteration 93/1000 | Loss: 0.00001052
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001052
Iteration 96/1000 | Loss: 0.00001052
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001052
Iteration 102/1000 | Loss: 0.00001051
Iteration 103/1000 | Loss: 0.00001051
Iteration 104/1000 | Loss: 0.00001051
Iteration 105/1000 | Loss: 0.00001050
Iteration 106/1000 | Loss: 0.00001050
Iteration 107/1000 | Loss: 0.00001050
Iteration 108/1000 | Loss: 0.00001050
Iteration 109/1000 | Loss: 0.00001050
Iteration 110/1000 | Loss: 0.00001050
Iteration 111/1000 | Loss: 0.00001050
Iteration 112/1000 | Loss: 0.00001049
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001049
Iteration 115/1000 | Loss: 0.00001049
Iteration 116/1000 | Loss: 0.00001049
Iteration 117/1000 | Loss: 0.00001049
Iteration 118/1000 | Loss: 0.00001049
Iteration 119/1000 | Loss: 0.00001049
Iteration 120/1000 | Loss: 0.00001049
Iteration 121/1000 | Loss: 0.00001049
Iteration 122/1000 | Loss: 0.00001049
Iteration 123/1000 | Loss: 0.00001049
Iteration 124/1000 | Loss: 0.00001048
Iteration 125/1000 | Loss: 0.00001048
Iteration 126/1000 | Loss: 0.00001048
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001048
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001047
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001045
Iteration 141/1000 | Loss: 0.00001045
Iteration 142/1000 | Loss: 0.00001045
Iteration 143/1000 | Loss: 0.00001045
Iteration 144/1000 | Loss: 0.00001045
Iteration 145/1000 | Loss: 0.00001045
Iteration 146/1000 | Loss: 0.00001045
Iteration 147/1000 | Loss: 0.00001045
Iteration 148/1000 | Loss: 0.00001045
Iteration 149/1000 | Loss: 0.00001044
Iteration 150/1000 | Loss: 0.00001044
Iteration 151/1000 | Loss: 0.00001044
Iteration 152/1000 | Loss: 0.00001044
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001042
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001042
Iteration 164/1000 | Loss: 0.00001042
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001041
Iteration 167/1000 | Loss: 0.00001041
Iteration 168/1000 | Loss: 0.00001041
Iteration 169/1000 | Loss: 0.00001041
Iteration 170/1000 | Loss: 0.00001040
Iteration 171/1000 | Loss: 0.00001040
Iteration 172/1000 | Loss: 0.00001040
Iteration 173/1000 | Loss: 0.00001040
Iteration 174/1000 | Loss: 0.00001040
Iteration 175/1000 | Loss: 0.00001040
Iteration 176/1000 | Loss: 0.00001040
Iteration 177/1000 | Loss: 0.00001040
Iteration 178/1000 | Loss: 0.00001040
Iteration 179/1000 | Loss: 0.00001040
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001039
Iteration 183/1000 | Loss: 0.00001039
Iteration 184/1000 | Loss: 0.00001039
Iteration 185/1000 | Loss: 0.00001039
Iteration 186/1000 | Loss: 0.00001039
Iteration 187/1000 | Loss: 0.00001039
Iteration 188/1000 | Loss: 0.00001039
Iteration 189/1000 | Loss: 0.00001039
Iteration 190/1000 | Loss: 0.00001039
Iteration 191/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.0391316209279466e-05, 1.0391316209279466e-05, 1.0391316209279466e-05, 1.0391316209279466e-05, 1.0391316209279466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0391316209279466e-05

Optimization complete. Final v2v error: 2.766707420349121 mm

Highest mean error: 2.950521230697632 mm for frame 162

Lowest mean error: 2.630082607269287 mm for frame 13

Saving results

Total time: 47.67634582519531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004045
Iteration 2/25 | Loss: 0.00168287
Iteration 3/25 | Loss: 0.00160063
Iteration 4/25 | Loss: 0.00136187
Iteration 5/25 | Loss: 0.00133534
Iteration 6/25 | Loss: 0.00132709
Iteration 7/25 | Loss: 0.00132334
Iteration 8/25 | Loss: 0.00132232
Iteration 9/25 | Loss: 0.00132195
Iteration 10/25 | Loss: 0.00132902
Iteration 11/25 | Loss: 0.00132576
Iteration 12/25 | Loss: 0.00132313
Iteration 13/25 | Loss: 0.00132276
Iteration 14/25 | Loss: 0.00132927
Iteration 15/25 | Loss: 0.00132643
Iteration 16/25 | Loss: 0.00132345
Iteration 17/25 | Loss: 0.00132279
Iteration 18/25 | Loss: 0.00132148
Iteration 19/25 | Loss: 0.00132015
Iteration 20/25 | Loss: 0.00131977
Iteration 21/25 | Loss: 0.00131946
Iteration 22/25 | Loss: 0.00131899
Iteration 23/25 | Loss: 0.00131746
Iteration 24/25 | Loss: 0.00131720
Iteration 25/25 | Loss: 0.00131709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37206042
Iteration 2/25 | Loss: 0.00151083
Iteration 3/25 | Loss: 0.00151083
Iteration 4/25 | Loss: 0.00151083
Iteration 5/25 | Loss: 0.00151083
Iteration 6/25 | Loss: 0.00151083
Iteration 7/25 | Loss: 0.00151083
Iteration 8/25 | Loss: 0.00151083
Iteration 9/25 | Loss: 0.00151083
Iteration 10/25 | Loss: 0.00151083
Iteration 11/25 | Loss: 0.00151083
Iteration 12/25 | Loss: 0.00151083
Iteration 13/25 | Loss: 0.00151083
Iteration 14/25 | Loss: 0.00151083
Iteration 15/25 | Loss: 0.00151083
Iteration 16/25 | Loss: 0.00151083
Iteration 17/25 | Loss: 0.00151083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015108276857063174, 0.0015108276857063174, 0.0015108276857063174, 0.0015108276857063174, 0.0015108276857063174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015108276857063174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151083
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00001902
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001506
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001324
Iteration 13/1000 | Loss: 0.00001318
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001311
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001291
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001285
Iteration 48/1000 | Loss: 0.00001284
Iteration 49/1000 | Loss: 0.00001283
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001278
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001276
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001273
Iteration 73/1000 | Loss: 0.00001273
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001267
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001261
Iteration 119/1000 | Loss: 0.00001261
Iteration 120/1000 | Loss: 0.00001261
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001260
Iteration 124/1000 | Loss: 0.00001260
Iteration 125/1000 | Loss: 0.00001260
Iteration 126/1000 | Loss: 0.00001260
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001260
Iteration 129/1000 | Loss: 0.00001260
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001260
Iteration 132/1000 | Loss: 0.00001260
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001260
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001259
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001259
Iteration 144/1000 | Loss: 0.00001259
Iteration 145/1000 | Loss: 0.00001259
Iteration 146/1000 | Loss: 0.00001259
Iteration 147/1000 | Loss: 0.00001259
Iteration 148/1000 | Loss: 0.00001259
Iteration 149/1000 | Loss: 0.00001259
Iteration 150/1000 | Loss: 0.00001258
Iteration 151/1000 | Loss: 0.00001258
Iteration 152/1000 | Loss: 0.00001258
Iteration 153/1000 | Loss: 0.00001258
Iteration 154/1000 | Loss: 0.00001258
Iteration 155/1000 | Loss: 0.00001258
Iteration 156/1000 | Loss: 0.00001258
Iteration 157/1000 | Loss: 0.00001258
Iteration 158/1000 | Loss: 0.00001258
Iteration 159/1000 | Loss: 0.00001258
Iteration 160/1000 | Loss: 0.00001258
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001258
Iteration 163/1000 | Loss: 0.00001258
Iteration 164/1000 | Loss: 0.00001258
Iteration 165/1000 | Loss: 0.00001257
Iteration 166/1000 | Loss: 0.00001257
Iteration 167/1000 | Loss: 0.00001257
Iteration 168/1000 | Loss: 0.00001257
Iteration 169/1000 | Loss: 0.00001257
Iteration 170/1000 | Loss: 0.00001257
Iteration 171/1000 | Loss: 0.00001257
Iteration 172/1000 | Loss: 0.00001257
Iteration 173/1000 | Loss: 0.00001257
Iteration 174/1000 | Loss: 0.00001257
Iteration 175/1000 | Loss: 0.00001257
Iteration 176/1000 | Loss: 0.00001257
Iteration 177/1000 | Loss: 0.00001257
Iteration 178/1000 | Loss: 0.00001257
Iteration 179/1000 | Loss: 0.00001257
Iteration 180/1000 | Loss: 0.00001257
Iteration 181/1000 | Loss: 0.00001257
Iteration 182/1000 | Loss: 0.00001257
Iteration 183/1000 | Loss: 0.00001257
Iteration 184/1000 | Loss: 0.00001256
Iteration 185/1000 | Loss: 0.00001256
Iteration 186/1000 | Loss: 0.00001256
Iteration 187/1000 | Loss: 0.00001256
Iteration 188/1000 | Loss: 0.00001256
Iteration 189/1000 | Loss: 0.00001256
Iteration 190/1000 | Loss: 0.00001256
Iteration 191/1000 | Loss: 0.00001256
Iteration 192/1000 | Loss: 0.00001256
Iteration 193/1000 | Loss: 0.00001256
Iteration 194/1000 | Loss: 0.00001256
Iteration 195/1000 | Loss: 0.00001256
Iteration 196/1000 | Loss: 0.00001255
Iteration 197/1000 | Loss: 0.00001255
Iteration 198/1000 | Loss: 0.00001255
Iteration 199/1000 | Loss: 0.00001255
Iteration 200/1000 | Loss: 0.00001255
Iteration 201/1000 | Loss: 0.00001255
Iteration 202/1000 | Loss: 0.00001255
Iteration 203/1000 | Loss: 0.00001255
Iteration 204/1000 | Loss: 0.00001255
Iteration 205/1000 | Loss: 0.00001255
Iteration 206/1000 | Loss: 0.00001255
Iteration 207/1000 | Loss: 0.00001255
Iteration 208/1000 | Loss: 0.00001255
Iteration 209/1000 | Loss: 0.00001255
Iteration 210/1000 | Loss: 0.00001254
Iteration 211/1000 | Loss: 0.00001254
Iteration 212/1000 | Loss: 0.00001254
Iteration 213/1000 | Loss: 0.00001254
Iteration 214/1000 | Loss: 0.00001254
Iteration 215/1000 | Loss: 0.00001254
Iteration 216/1000 | Loss: 0.00001254
Iteration 217/1000 | Loss: 0.00001254
Iteration 218/1000 | Loss: 0.00001254
Iteration 219/1000 | Loss: 0.00001253
Iteration 220/1000 | Loss: 0.00001253
Iteration 221/1000 | Loss: 0.00001253
Iteration 222/1000 | Loss: 0.00001253
Iteration 223/1000 | Loss: 0.00001253
Iteration 224/1000 | Loss: 0.00001253
Iteration 225/1000 | Loss: 0.00001253
Iteration 226/1000 | Loss: 0.00001253
Iteration 227/1000 | Loss: 0.00001253
Iteration 228/1000 | Loss: 0.00001253
Iteration 229/1000 | Loss: 0.00001253
Iteration 230/1000 | Loss: 0.00001253
Iteration 231/1000 | Loss: 0.00001253
Iteration 232/1000 | Loss: 0.00001253
Iteration 233/1000 | Loss: 0.00001253
Iteration 234/1000 | Loss: 0.00001253
Iteration 235/1000 | Loss: 0.00001253
Iteration 236/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.2531419088190887e-05, 1.2531419088190887e-05, 1.2531419088190887e-05, 1.2531419088190887e-05, 1.2531419088190887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2531419088190887e-05

Optimization complete. Final v2v error: 3.015458345413208 mm

Highest mean error: 3.511014699935913 mm for frame 72

Lowest mean error: 2.757965087890625 mm for frame 115

Saving results

Total time: 76.58798003196716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485065
Iteration 2/25 | Loss: 0.00132218
Iteration 3/25 | Loss: 0.00127636
Iteration 4/25 | Loss: 0.00126608
Iteration 5/25 | Loss: 0.00126372
Iteration 6/25 | Loss: 0.00126372
Iteration 7/25 | Loss: 0.00126372
Iteration 8/25 | Loss: 0.00126372
Iteration 9/25 | Loss: 0.00126372
Iteration 10/25 | Loss: 0.00126372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00126372289378196, 0.00126372289378196, 0.00126372289378196, 0.00126372289378196, 0.00126372289378196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00126372289378196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28060699
Iteration 2/25 | Loss: 0.00139506
Iteration 3/25 | Loss: 0.00139506
Iteration 4/25 | Loss: 0.00139506
Iteration 5/25 | Loss: 0.00139506
Iteration 6/25 | Loss: 0.00139506
Iteration 7/25 | Loss: 0.00139506
Iteration 8/25 | Loss: 0.00139506
Iteration 9/25 | Loss: 0.00139506
Iteration 10/25 | Loss: 0.00139506
Iteration 11/25 | Loss: 0.00139506
Iteration 12/25 | Loss: 0.00139506
Iteration 13/25 | Loss: 0.00139506
Iteration 14/25 | Loss: 0.00139506
Iteration 15/25 | Loss: 0.00139506
Iteration 16/25 | Loss: 0.00139506
Iteration 17/25 | Loss: 0.00139506
Iteration 18/25 | Loss: 0.00139506
Iteration 19/25 | Loss: 0.00139506
Iteration 20/25 | Loss: 0.00139506
Iteration 21/25 | Loss: 0.00139506
Iteration 22/25 | Loss: 0.00139506
Iteration 23/25 | Loss: 0.00139506
Iteration 24/25 | Loss: 0.00139506
Iteration 25/25 | Loss: 0.00139506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139506
Iteration 2/1000 | Loss: 0.00002089
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001231
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001185
Iteration 20/1000 | Loss: 0.00001184
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001155
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001155
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001154
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1526846719789319e-05, 1.1526846719789319e-05, 1.1526846719789319e-05, 1.1526846719789319e-05, 1.1526846719789319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1526846719789319e-05

Optimization complete. Final v2v error: 2.9599251747131348 mm

Highest mean error: 3.172412395477295 mm for frame 169

Lowest mean error: 2.8328161239624023 mm for frame 33

Saving results

Total time: 41.073097705841064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038782
Iteration 2/25 | Loss: 0.01038782
Iteration 3/25 | Loss: 0.01038782
Iteration 4/25 | Loss: 0.01038782
Iteration 5/25 | Loss: 0.01038782
Iteration 6/25 | Loss: 0.01038782
Iteration 7/25 | Loss: 0.01038782
Iteration 8/25 | Loss: 0.01038782
Iteration 9/25 | Loss: 0.01038781
Iteration 10/25 | Loss: 0.01038781
Iteration 11/25 | Loss: 0.01038781
Iteration 12/25 | Loss: 0.01038781
Iteration 13/25 | Loss: 0.01038781
Iteration 14/25 | Loss: 0.01038781
Iteration 15/25 | Loss: 0.01038781
Iteration 16/25 | Loss: 0.01038781
Iteration 17/25 | Loss: 0.01038781
Iteration 18/25 | Loss: 0.01038780
Iteration 19/25 | Loss: 0.01038780
Iteration 20/25 | Loss: 0.01038780
Iteration 21/25 | Loss: 0.01038780
Iteration 22/25 | Loss: 0.01038780
Iteration 23/25 | Loss: 0.01038780
Iteration 24/25 | Loss: 0.01038780
Iteration 25/25 | Loss: 0.01038780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62792349
Iteration 2/25 | Loss: 0.08680112
Iteration 3/25 | Loss: 0.08676546
Iteration 4/25 | Loss: 0.08676545
Iteration 5/25 | Loss: 0.08676545
Iteration 6/25 | Loss: 0.08676545
Iteration 7/25 | Loss: 0.08676545
Iteration 8/25 | Loss: 0.08676544
Iteration 9/25 | Loss: 0.08676544
Iteration 10/25 | Loss: 0.08676544
Iteration 11/25 | Loss: 0.08676544
Iteration 12/25 | Loss: 0.08676544
Iteration 13/25 | Loss: 0.08676544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.08676543831825256, 0.08676543831825256, 0.08676543831825256, 0.08676543831825256, 0.08676543831825256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08676543831825256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08676544
Iteration 2/1000 | Loss: 0.00178212
Iteration 3/1000 | Loss: 0.00057633
Iteration 4/1000 | Loss: 0.00048608
Iteration 5/1000 | Loss: 0.00011479
Iteration 6/1000 | Loss: 0.00008343
Iteration 7/1000 | Loss: 0.00005290
Iteration 8/1000 | Loss: 0.00004251
Iteration 9/1000 | Loss: 0.00004534
Iteration 10/1000 | Loss: 0.00003195
Iteration 11/1000 | Loss: 0.00004243
Iteration 12/1000 | Loss: 0.00002677
Iteration 13/1000 | Loss: 0.00002523
Iteration 14/1000 | Loss: 0.00004048
Iteration 15/1000 | Loss: 0.00002365
Iteration 16/1000 | Loss: 0.00004064
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002179
Iteration 19/1000 | Loss: 0.00003015
Iteration 20/1000 | Loss: 0.00002044
Iteration 21/1000 | Loss: 0.00002007
Iteration 22/1000 | Loss: 0.00003377
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00002770
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00003163
Iteration 29/1000 | Loss: 0.00006728
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001717
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.7173158994410187e-05, 1.7173158994410187e-05, 1.7173158994410187e-05, 1.7173158994410187e-05, 1.7173158994410187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7173158994410187e-05

Optimization complete. Final v2v error: 3.6030023097991943 mm

Highest mean error: 3.9787044525146484 mm for frame 8

Lowest mean error: 3.2945761680603027 mm for frame 12

Saving results

Total time: 55.03673219680786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509909
Iteration 2/25 | Loss: 0.00146294
Iteration 3/25 | Loss: 0.00134568
Iteration 4/25 | Loss: 0.00133026
Iteration 5/25 | Loss: 0.00132321
Iteration 6/25 | Loss: 0.00132210
Iteration 7/25 | Loss: 0.00132210
Iteration 8/25 | Loss: 0.00132210
Iteration 9/25 | Loss: 0.00132210
Iteration 10/25 | Loss: 0.00132210
Iteration 11/25 | Loss: 0.00132210
Iteration 12/25 | Loss: 0.00132210
Iteration 13/25 | Loss: 0.00132210
Iteration 14/25 | Loss: 0.00132210
Iteration 15/25 | Loss: 0.00132210
Iteration 16/25 | Loss: 0.00132210
Iteration 17/25 | Loss: 0.00132210
Iteration 18/25 | Loss: 0.00132210
Iteration 19/25 | Loss: 0.00132210
Iteration 20/25 | Loss: 0.00132210
Iteration 21/25 | Loss: 0.00132210
Iteration 22/25 | Loss: 0.00132210
Iteration 23/25 | Loss: 0.00132210
Iteration 24/25 | Loss: 0.00132210
Iteration 25/25 | Loss: 0.00132210

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70647687
Iteration 2/25 | Loss: 0.00151583
Iteration 3/25 | Loss: 0.00151583
Iteration 4/25 | Loss: 0.00151583
Iteration 5/25 | Loss: 0.00151583
Iteration 6/25 | Loss: 0.00151582
Iteration 7/25 | Loss: 0.00151582
Iteration 8/25 | Loss: 0.00151582
Iteration 9/25 | Loss: 0.00151582
Iteration 10/25 | Loss: 0.00151582
Iteration 11/25 | Loss: 0.00151582
Iteration 12/25 | Loss: 0.00151582
Iteration 13/25 | Loss: 0.00151582
Iteration 14/25 | Loss: 0.00151582
Iteration 15/25 | Loss: 0.00151582
Iteration 16/25 | Loss: 0.00151582
Iteration 17/25 | Loss: 0.00151582
Iteration 18/25 | Loss: 0.00151582
Iteration 19/25 | Loss: 0.00151582
Iteration 20/25 | Loss: 0.00151582
Iteration 21/25 | Loss: 0.00151582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015158241149038076, 0.0015158241149038076, 0.0015158241149038076, 0.0015158241149038076, 0.0015158241149038076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015158241149038076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151582
Iteration 2/1000 | Loss: 0.00004459
Iteration 3/1000 | Loss: 0.00002885
Iteration 4/1000 | Loss: 0.00002532
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002207
Iteration 9/1000 | Loss: 0.00002167
Iteration 10/1000 | Loss: 0.00002128
Iteration 11/1000 | Loss: 0.00002098
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002038
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002017
Iteration 20/1000 | Loss: 0.00002015
Iteration 21/1000 | Loss: 0.00002015
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002011
Iteration 24/1000 | Loss: 0.00002011
Iteration 25/1000 | Loss: 0.00002010
Iteration 26/1000 | Loss: 0.00002009
Iteration 27/1000 | Loss: 0.00002005
Iteration 28/1000 | Loss: 0.00002005
Iteration 29/1000 | Loss: 0.00002004
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001993
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001984
Iteration 43/1000 | Loss: 0.00001980
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001979
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00001978
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001978
Iteration 50/1000 | Loss: 0.00001978
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001977
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001977
Iteration 55/1000 | Loss: 0.00001977
Iteration 56/1000 | Loss: 0.00001977
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001975
Iteration 63/1000 | Loss: 0.00001975
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001974
Iteration 66/1000 | Loss: 0.00001973
Iteration 67/1000 | Loss: 0.00001973
Iteration 68/1000 | Loss: 0.00001973
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001971
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001969
Iteration 75/1000 | Loss: 0.00001968
Iteration 76/1000 | Loss: 0.00001968
Iteration 77/1000 | Loss: 0.00001968
Iteration 78/1000 | Loss: 0.00001967
Iteration 79/1000 | Loss: 0.00001967
Iteration 80/1000 | Loss: 0.00001967
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001966
Iteration 83/1000 | Loss: 0.00001966
Iteration 84/1000 | Loss: 0.00001966
Iteration 85/1000 | Loss: 0.00001966
Iteration 86/1000 | Loss: 0.00001966
Iteration 87/1000 | Loss: 0.00001966
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001964
Iteration 95/1000 | Loss: 0.00001964
Iteration 96/1000 | Loss: 0.00001964
Iteration 97/1000 | Loss: 0.00001964
Iteration 98/1000 | Loss: 0.00001964
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001963
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001962
Iteration 103/1000 | Loss: 0.00001962
Iteration 104/1000 | Loss: 0.00001962
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001962
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001961
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001958
Iteration 115/1000 | Loss: 0.00001958
Iteration 116/1000 | Loss: 0.00001957
Iteration 117/1000 | Loss: 0.00001957
Iteration 118/1000 | Loss: 0.00001957
Iteration 119/1000 | Loss: 0.00001956
Iteration 120/1000 | Loss: 0.00001956
Iteration 121/1000 | Loss: 0.00001954
Iteration 122/1000 | Loss: 0.00001954
Iteration 123/1000 | Loss: 0.00001953
Iteration 124/1000 | Loss: 0.00001953
Iteration 125/1000 | Loss: 0.00001953
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001952
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001951
Iteration 131/1000 | Loss: 0.00001951
Iteration 132/1000 | Loss: 0.00001951
Iteration 133/1000 | Loss: 0.00001950
Iteration 134/1000 | Loss: 0.00001950
Iteration 135/1000 | Loss: 0.00001950
Iteration 136/1000 | Loss: 0.00001949
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001948
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001948
Iteration 141/1000 | Loss: 0.00001947
Iteration 142/1000 | Loss: 0.00001947
Iteration 143/1000 | Loss: 0.00001947
Iteration 144/1000 | Loss: 0.00001947
Iteration 145/1000 | Loss: 0.00001947
Iteration 146/1000 | Loss: 0.00001947
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001945
Iteration 151/1000 | Loss: 0.00001945
Iteration 152/1000 | Loss: 0.00001945
Iteration 153/1000 | Loss: 0.00001945
Iteration 154/1000 | Loss: 0.00001945
Iteration 155/1000 | Loss: 0.00001944
Iteration 156/1000 | Loss: 0.00001944
Iteration 157/1000 | Loss: 0.00001944
Iteration 158/1000 | Loss: 0.00001944
Iteration 159/1000 | Loss: 0.00001944
Iteration 160/1000 | Loss: 0.00001943
Iteration 161/1000 | Loss: 0.00001943
Iteration 162/1000 | Loss: 0.00001943
Iteration 163/1000 | Loss: 0.00001943
Iteration 164/1000 | Loss: 0.00001943
Iteration 165/1000 | Loss: 0.00001943
Iteration 166/1000 | Loss: 0.00001943
Iteration 167/1000 | Loss: 0.00001943
Iteration 168/1000 | Loss: 0.00001943
Iteration 169/1000 | Loss: 0.00001943
Iteration 170/1000 | Loss: 0.00001943
Iteration 171/1000 | Loss: 0.00001943
Iteration 172/1000 | Loss: 0.00001943
Iteration 173/1000 | Loss: 0.00001942
Iteration 174/1000 | Loss: 0.00001942
Iteration 175/1000 | Loss: 0.00001942
Iteration 176/1000 | Loss: 0.00001942
Iteration 177/1000 | Loss: 0.00001942
Iteration 178/1000 | Loss: 0.00001942
Iteration 179/1000 | Loss: 0.00001942
Iteration 180/1000 | Loss: 0.00001942
Iteration 181/1000 | Loss: 0.00001942
Iteration 182/1000 | Loss: 0.00001942
Iteration 183/1000 | Loss: 0.00001942
Iteration 184/1000 | Loss: 0.00001941
Iteration 185/1000 | Loss: 0.00001941
Iteration 186/1000 | Loss: 0.00001941
Iteration 187/1000 | Loss: 0.00001941
Iteration 188/1000 | Loss: 0.00001941
Iteration 189/1000 | Loss: 0.00001941
Iteration 190/1000 | Loss: 0.00001941
Iteration 191/1000 | Loss: 0.00001941
Iteration 192/1000 | Loss: 0.00001941
Iteration 193/1000 | Loss: 0.00001941
Iteration 194/1000 | Loss: 0.00001940
Iteration 195/1000 | Loss: 0.00001940
Iteration 196/1000 | Loss: 0.00001940
Iteration 197/1000 | Loss: 0.00001940
Iteration 198/1000 | Loss: 0.00001940
Iteration 199/1000 | Loss: 0.00001940
Iteration 200/1000 | Loss: 0.00001939
Iteration 201/1000 | Loss: 0.00001939
Iteration 202/1000 | Loss: 0.00001939
Iteration 203/1000 | Loss: 0.00001939
Iteration 204/1000 | Loss: 0.00001939
Iteration 205/1000 | Loss: 0.00001939
Iteration 206/1000 | Loss: 0.00001939
Iteration 207/1000 | Loss: 0.00001939
Iteration 208/1000 | Loss: 0.00001939
Iteration 209/1000 | Loss: 0.00001939
Iteration 210/1000 | Loss: 0.00001939
Iteration 211/1000 | Loss: 0.00001939
Iteration 212/1000 | Loss: 0.00001939
Iteration 213/1000 | Loss: 0.00001939
Iteration 214/1000 | Loss: 0.00001939
Iteration 215/1000 | Loss: 0.00001938
Iteration 216/1000 | Loss: 0.00001938
Iteration 217/1000 | Loss: 0.00001938
Iteration 218/1000 | Loss: 0.00001938
Iteration 219/1000 | Loss: 0.00001938
Iteration 220/1000 | Loss: 0.00001938
Iteration 221/1000 | Loss: 0.00001938
Iteration 222/1000 | Loss: 0.00001938
Iteration 223/1000 | Loss: 0.00001938
Iteration 224/1000 | Loss: 0.00001937
Iteration 225/1000 | Loss: 0.00001937
Iteration 226/1000 | Loss: 0.00001937
Iteration 227/1000 | Loss: 0.00001937
Iteration 228/1000 | Loss: 0.00001937
Iteration 229/1000 | Loss: 0.00001937
Iteration 230/1000 | Loss: 0.00001937
Iteration 231/1000 | Loss: 0.00001937
Iteration 232/1000 | Loss: 0.00001937
Iteration 233/1000 | Loss: 0.00001937
Iteration 234/1000 | Loss: 0.00001937
Iteration 235/1000 | Loss: 0.00001937
Iteration 236/1000 | Loss: 0.00001937
Iteration 237/1000 | Loss: 0.00001937
Iteration 238/1000 | Loss: 0.00001937
Iteration 239/1000 | Loss: 0.00001937
Iteration 240/1000 | Loss: 0.00001936
Iteration 241/1000 | Loss: 0.00001936
Iteration 242/1000 | Loss: 0.00001936
Iteration 243/1000 | Loss: 0.00001936
Iteration 244/1000 | Loss: 0.00001936
Iteration 245/1000 | Loss: 0.00001936
Iteration 246/1000 | Loss: 0.00001936
Iteration 247/1000 | Loss: 0.00001936
Iteration 248/1000 | Loss: 0.00001936
Iteration 249/1000 | Loss: 0.00001936
Iteration 250/1000 | Loss: 0.00001936
Iteration 251/1000 | Loss: 0.00001936
Iteration 252/1000 | Loss: 0.00001936
Iteration 253/1000 | Loss: 0.00001936
Iteration 254/1000 | Loss: 0.00001936
Iteration 255/1000 | Loss: 0.00001936
Iteration 256/1000 | Loss: 0.00001936
Iteration 257/1000 | Loss: 0.00001936
Iteration 258/1000 | Loss: 0.00001936
Iteration 259/1000 | Loss: 0.00001936
Iteration 260/1000 | Loss: 0.00001936
Iteration 261/1000 | Loss: 0.00001936
Iteration 262/1000 | Loss: 0.00001936
Iteration 263/1000 | Loss: 0.00001936
Iteration 264/1000 | Loss: 0.00001936
Iteration 265/1000 | Loss: 0.00001936
Iteration 266/1000 | Loss: 0.00001936
Iteration 267/1000 | Loss: 0.00001936
Iteration 268/1000 | Loss: 0.00001936
Iteration 269/1000 | Loss: 0.00001936
Iteration 270/1000 | Loss: 0.00001936
Iteration 271/1000 | Loss: 0.00001936
Iteration 272/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.9356550183147192e-05, 1.9356550183147192e-05, 1.9356550183147192e-05, 1.9356550183147192e-05, 1.9356550183147192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9356550183147192e-05

Optimization complete. Final v2v error: 3.669851303100586 mm

Highest mean error: 4.197783946990967 mm for frame 197

Lowest mean error: 3.4537851810455322 mm for frame 172

Saving results

Total time: 56.90993404388428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498370
Iteration 2/25 | Loss: 0.00150211
Iteration 3/25 | Loss: 0.00132642
Iteration 4/25 | Loss: 0.00131174
Iteration 5/25 | Loss: 0.00130947
Iteration 6/25 | Loss: 0.00130896
Iteration 7/25 | Loss: 0.00130896
Iteration 8/25 | Loss: 0.00130896
Iteration 9/25 | Loss: 0.00130896
Iteration 10/25 | Loss: 0.00130896
Iteration 11/25 | Loss: 0.00130896
Iteration 12/25 | Loss: 0.00130896
Iteration 13/25 | Loss: 0.00130896
Iteration 14/25 | Loss: 0.00130896
Iteration 15/25 | Loss: 0.00130896
Iteration 16/25 | Loss: 0.00130896
Iteration 17/25 | Loss: 0.00130896
Iteration 18/25 | Loss: 0.00130896
Iteration 19/25 | Loss: 0.00130896
Iteration 20/25 | Loss: 0.00130896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013089554850012064, 0.0013089554850012064, 0.0013089554850012064, 0.0013089554850012064, 0.0013089554850012064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013089554850012064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31889844
Iteration 2/25 | Loss: 0.00131062
Iteration 3/25 | Loss: 0.00131062
Iteration 4/25 | Loss: 0.00131062
Iteration 5/25 | Loss: 0.00131062
Iteration 6/25 | Loss: 0.00131062
Iteration 7/25 | Loss: 0.00131062
Iteration 8/25 | Loss: 0.00131062
Iteration 9/25 | Loss: 0.00131062
Iteration 10/25 | Loss: 0.00131062
Iteration 11/25 | Loss: 0.00131062
Iteration 12/25 | Loss: 0.00131062
Iteration 13/25 | Loss: 0.00131062
Iteration 14/25 | Loss: 0.00131062
Iteration 15/25 | Loss: 0.00131062
Iteration 16/25 | Loss: 0.00131062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001310617895796895, 0.001310617895796895, 0.001310617895796895, 0.001310617895796895, 0.001310617895796895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001310617895796895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131062
Iteration 2/1000 | Loss: 0.00003568
Iteration 3/1000 | Loss: 0.00002445
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001793
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001707
Iteration 10/1000 | Loss: 0.00001675
Iteration 11/1000 | Loss: 0.00001648
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001628
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001612
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001593
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001585
Iteration 31/1000 | Loss: 0.00001583
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001571
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001570
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001569
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001564
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001560
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001552
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001547
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001545
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001544
Iteration 122/1000 | Loss: 0.00001544
Iteration 123/1000 | Loss: 0.00001544
Iteration 124/1000 | Loss: 0.00001544
Iteration 125/1000 | Loss: 0.00001544
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001543
Iteration 132/1000 | Loss: 0.00001543
Iteration 133/1000 | Loss: 0.00001543
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001542
Iteration 136/1000 | Loss: 0.00001542
Iteration 137/1000 | Loss: 0.00001542
Iteration 138/1000 | Loss: 0.00001542
Iteration 139/1000 | Loss: 0.00001542
Iteration 140/1000 | Loss: 0.00001542
Iteration 141/1000 | Loss: 0.00001542
Iteration 142/1000 | Loss: 0.00001541
Iteration 143/1000 | Loss: 0.00001541
Iteration 144/1000 | Loss: 0.00001541
Iteration 145/1000 | Loss: 0.00001540
Iteration 146/1000 | Loss: 0.00001540
Iteration 147/1000 | Loss: 0.00001540
Iteration 148/1000 | Loss: 0.00001540
Iteration 149/1000 | Loss: 0.00001539
Iteration 150/1000 | Loss: 0.00001539
Iteration 151/1000 | Loss: 0.00001539
Iteration 152/1000 | Loss: 0.00001539
Iteration 153/1000 | Loss: 0.00001539
Iteration 154/1000 | Loss: 0.00001539
Iteration 155/1000 | Loss: 0.00001539
Iteration 156/1000 | Loss: 0.00001539
Iteration 157/1000 | Loss: 0.00001539
Iteration 158/1000 | Loss: 0.00001539
Iteration 159/1000 | Loss: 0.00001539
Iteration 160/1000 | Loss: 0.00001539
Iteration 161/1000 | Loss: 0.00001539
Iteration 162/1000 | Loss: 0.00001539
Iteration 163/1000 | Loss: 0.00001539
Iteration 164/1000 | Loss: 0.00001539
Iteration 165/1000 | Loss: 0.00001539
Iteration 166/1000 | Loss: 0.00001539
Iteration 167/1000 | Loss: 0.00001539
Iteration 168/1000 | Loss: 0.00001539
Iteration 169/1000 | Loss: 0.00001539
Iteration 170/1000 | Loss: 0.00001539
Iteration 171/1000 | Loss: 0.00001539
Iteration 172/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.5393814464914612e-05, 1.5393814464914612e-05, 1.5393814464914612e-05, 1.5393814464914612e-05, 1.5393814464914612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5393814464914612e-05

Optimization complete. Final v2v error: 3.1841530799865723 mm

Highest mean error: 4.417428970336914 mm for frame 61

Lowest mean error: 2.76131534576416 mm for frame 14

Saving results

Total time: 42.6235077381134
