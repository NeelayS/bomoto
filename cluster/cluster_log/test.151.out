Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=151, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8456-8511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002564
Iteration 2/25 | Loss: 0.01002563
Iteration 3/25 | Loss: 0.00328068
Iteration 4/25 | Loss: 0.00192205
Iteration 5/25 | Loss: 0.00173446
Iteration 6/25 | Loss: 0.00162118
Iteration 7/25 | Loss: 0.00152905
Iteration 8/25 | Loss: 0.00156417
Iteration 9/25 | Loss: 0.00145892
Iteration 10/25 | Loss: 0.00137445
Iteration 11/25 | Loss: 0.00133712
Iteration 12/25 | Loss: 0.00132635
Iteration 13/25 | Loss: 0.00133464
Iteration 14/25 | Loss: 0.00131992
Iteration 15/25 | Loss: 0.00131660
Iteration 16/25 | Loss: 0.00130314
Iteration 17/25 | Loss: 0.00129780
Iteration 18/25 | Loss: 0.00129728
Iteration 19/25 | Loss: 0.00129859
Iteration 20/25 | Loss: 0.00129767
Iteration 21/25 | Loss: 0.00129746
Iteration 22/25 | Loss: 0.00129703
Iteration 23/25 | Loss: 0.00129703
Iteration 24/25 | Loss: 0.00129703
Iteration 25/25 | Loss: 0.00129703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38480282
Iteration 2/25 | Loss: 0.00081359
Iteration 3/25 | Loss: 0.00075963
Iteration 4/25 | Loss: 0.00075862
Iteration 5/25 | Loss: 0.00075862
Iteration 6/25 | Loss: 0.00075862
Iteration 7/25 | Loss: 0.00075862
Iteration 8/25 | Loss: 0.00075862
Iteration 9/25 | Loss: 0.00075862
Iteration 10/25 | Loss: 0.00075862
Iteration 11/25 | Loss: 0.00075862
Iteration 12/25 | Loss: 0.00075862
Iteration 13/25 | Loss: 0.00075862
Iteration 14/25 | Loss: 0.00075862
Iteration 15/25 | Loss: 0.00075862
Iteration 16/25 | Loss: 0.00075862
Iteration 17/25 | Loss: 0.00075862
Iteration 18/25 | Loss: 0.00075862
Iteration 19/25 | Loss: 0.00075862
Iteration 20/25 | Loss: 0.00075862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007586153224110603, 0.0007586153224110603, 0.0007586153224110603, 0.0007586153224110603, 0.0007586153224110603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007586153224110603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075862
Iteration 2/1000 | Loss: 0.00007842
Iteration 3/1000 | Loss: 0.00003747
Iteration 4/1000 | Loss: 0.00002258
Iteration 5/1000 | Loss: 0.00004297
Iteration 6/1000 | Loss: 0.00007484
Iteration 7/1000 | Loss: 0.00009045
Iteration 8/1000 | Loss: 0.00005009
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00003135
Iteration 12/1000 | Loss: 0.00002013
Iteration 13/1000 | Loss: 0.00002383
Iteration 14/1000 | Loss: 0.00002015
Iteration 15/1000 | Loss: 0.00004743
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00003639
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001818
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001818
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001817
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001817
Iteration 28/1000 | Loss: 0.00003115
Iteration 29/1000 | Loss: 0.00002238
Iteration 30/1000 | Loss: 0.00002428
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002690
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001799
Iteration 35/1000 | Loss: 0.00001798
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00002181
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001804
Iteration 47/1000 | Loss: 0.00001803
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00003969
Iteration 50/1000 | Loss: 0.00002574
Iteration 51/1000 | Loss: 0.00002527
Iteration 52/1000 | Loss: 0.00032665
Iteration 53/1000 | Loss: 0.00011712
Iteration 54/1000 | Loss: 0.00004923
Iteration 55/1000 | Loss: 0.00003785
Iteration 56/1000 | Loss: 0.00001788
Iteration 57/1000 | Loss: 0.00002123
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001758
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001753
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001752
Iteration 88/1000 | Loss: 0.00001752
Iteration 89/1000 | Loss: 0.00001752
Iteration 90/1000 | Loss: 0.00001752
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001750
Iteration 102/1000 | Loss: 0.00001750
Iteration 103/1000 | Loss: 0.00001750
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001750
Iteration 120/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.7500624380772933e-05, 1.7500624380772933e-05, 1.7500624380772933e-05, 1.7500624380772933e-05, 1.7500624380772933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7500624380772933e-05

Optimization complete. Final v2v error: 3.552151679992676 mm

Highest mean error: 3.7920453548431396 mm for frame 198

Lowest mean error: 3.1285204887390137 mm for frame 93

Saving results

Total time: 106.1887788772583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051594
Iteration 2/25 | Loss: 0.00243611
Iteration 3/25 | Loss: 0.00209472
Iteration 4/25 | Loss: 0.00173992
Iteration 5/25 | Loss: 0.00162064
Iteration 6/25 | Loss: 0.00165904
Iteration 7/25 | Loss: 0.00150995
Iteration 8/25 | Loss: 0.00147046
Iteration 9/25 | Loss: 0.00140101
Iteration 10/25 | Loss: 0.00139152
Iteration 11/25 | Loss: 0.00142025
Iteration 12/25 | Loss: 0.00136274
Iteration 13/25 | Loss: 0.00135948
Iteration 14/25 | Loss: 0.00135641
Iteration 15/25 | Loss: 0.00135628
Iteration 16/25 | Loss: 0.00135705
Iteration 17/25 | Loss: 0.00135622
Iteration 18/25 | Loss: 0.00135622
Iteration 19/25 | Loss: 0.00135622
Iteration 20/25 | Loss: 0.00135622
Iteration 21/25 | Loss: 0.00135621
Iteration 22/25 | Loss: 0.00135621
Iteration 23/25 | Loss: 0.00135621
Iteration 24/25 | Loss: 0.00135621
Iteration 25/25 | Loss: 0.00135621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92143440
Iteration 2/25 | Loss: 0.00119742
Iteration 3/25 | Loss: 0.00118135
Iteration 4/25 | Loss: 0.00118135
Iteration 5/25 | Loss: 0.00118135
Iteration 6/25 | Loss: 0.00118135
Iteration 7/25 | Loss: 0.00118135
Iteration 8/25 | Loss: 0.00118135
Iteration 9/25 | Loss: 0.00118135
Iteration 10/25 | Loss: 0.00118135
Iteration 11/25 | Loss: 0.00118135
Iteration 12/25 | Loss: 0.00118135
Iteration 13/25 | Loss: 0.00118135
Iteration 14/25 | Loss: 0.00118135
Iteration 15/25 | Loss: 0.00118135
Iteration 16/25 | Loss: 0.00118135
Iteration 17/25 | Loss: 0.00118135
Iteration 18/25 | Loss: 0.00118134
Iteration 19/25 | Loss: 0.00118134
Iteration 20/25 | Loss: 0.00118134
Iteration 21/25 | Loss: 0.00118134
Iteration 22/25 | Loss: 0.00118134
Iteration 23/25 | Loss: 0.00118134
Iteration 24/25 | Loss: 0.00118134
Iteration 25/25 | Loss: 0.00118134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118134
Iteration 2/1000 | Loss: 0.00009110
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00005417
Iteration 5/1000 | Loss: 0.00001939
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00005734
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001823
Iteration 13/1000 | Loss: 0.00007512
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001798
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001789
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001788
Iteration 23/1000 | Loss: 0.00003489
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001777
Iteration 29/1000 | Loss: 0.00001774
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00007016
Iteration 33/1000 | Loss: 0.00004164
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001755
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001755
Iteration 42/1000 | Loss: 0.00001755
Iteration 43/1000 | Loss: 0.00001755
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001755
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001752
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001751
Iteration 69/1000 | Loss: 0.00001751
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00002510
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001744
Iteration 96/1000 | Loss: 0.00001744
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001744
Iteration 99/1000 | Loss: 0.00001744
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001743
Iteration 103/1000 | Loss: 0.00001743
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001742
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001887
Iteration 125/1000 | Loss: 0.00001887
Iteration 126/1000 | Loss: 0.00001738
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001736
Iteration 139/1000 | Loss: 0.00001736
Iteration 140/1000 | Loss: 0.00001736
Iteration 141/1000 | Loss: 0.00001736
Iteration 142/1000 | Loss: 0.00001736
Iteration 143/1000 | Loss: 0.00001736
Iteration 144/1000 | Loss: 0.00001736
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001735
Iteration 148/1000 | Loss: 0.00001735
Iteration 149/1000 | Loss: 0.00001735
Iteration 150/1000 | Loss: 0.00001735
Iteration 151/1000 | Loss: 0.00001735
Iteration 152/1000 | Loss: 0.00001735
Iteration 153/1000 | Loss: 0.00001735
Iteration 154/1000 | Loss: 0.00001735
Iteration 155/1000 | Loss: 0.00001735
Iteration 156/1000 | Loss: 0.00001735
Iteration 157/1000 | Loss: 0.00001735
Iteration 158/1000 | Loss: 0.00001735
Iteration 159/1000 | Loss: 0.00001735
Iteration 160/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.7348647816106677e-05, 1.7348647816106677e-05, 1.7348647816106677e-05, 1.7348647816106677e-05, 1.7348647816106677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7348647816106677e-05

Optimization complete. Final v2v error: 3.4640989303588867 mm

Highest mean error: 4.107611656188965 mm for frame 34

Lowest mean error: 3.221059560775757 mm for frame 80

Saving results

Total time: 68.19811487197876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560559
Iteration 2/25 | Loss: 0.00173641
Iteration 3/25 | Loss: 0.00148028
Iteration 4/25 | Loss: 0.00146151
Iteration 5/25 | Loss: 0.00145867
Iteration 6/25 | Loss: 0.00145803
Iteration 7/25 | Loss: 0.00145803
Iteration 8/25 | Loss: 0.00145803
Iteration 9/25 | Loss: 0.00145803
Iteration 10/25 | Loss: 0.00145803
Iteration 11/25 | Loss: 0.00145803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014580284478142858, 0.0014580284478142858, 0.0014580284478142858, 0.0014580284478142858, 0.0014580284478142858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014580284478142858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00456500
Iteration 2/25 | Loss: 0.00112624
Iteration 3/25 | Loss: 0.00112622
Iteration 4/25 | Loss: 0.00112622
Iteration 5/25 | Loss: 0.00112622
Iteration 6/25 | Loss: 0.00112622
Iteration 7/25 | Loss: 0.00112622
Iteration 8/25 | Loss: 0.00112622
Iteration 9/25 | Loss: 0.00112622
Iteration 10/25 | Loss: 0.00112622
Iteration 11/25 | Loss: 0.00112622
Iteration 12/25 | Loss: 0.00112622
Iteration 13/25 | Loss: 0.00112622
Iteration 14/25 | Loss: 0.00112622
Iteration 15/25 | Loss: 0.00112622
Iteration 16/25 | Loss: 0.00112622
Iteration 17/25 | Loss: 0.00112622
Iteration 18/25 | Loss: 0.00112622
Iteration 19/25 | Loss: 0.00112622
Iteration 20/25 | Loss: 0.00112622
Iteration 21/25 | Loss: 0.00112622
Iteration 22/25 | Loss: 0.00112622
Iteration 23/25 | Loss: 0.00112622
Iteration 24/25 | Loss: 0.00112622
Iteration 25/25 | Loss: 0.00112622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112622
Iteration 2/1000 | Loss: 0.00005255
Iteration 3/1000 | Loss: 0.00003718
Iteration 4/1000 | Loss: 0.00003075
Iteration 5/1000 | Loss: 0.00002848
Iteration 6/1000 | Loss: 0.00002751
Iteration 7/1000 | Loss: 0.00002682
Iteration 8/1000 | Loss: 0.00002609
Iteration 9/1000 | Loss: 0.00002557
Iteration 10/1000 | Loss: 0.00002520
Iteration 11/1000 | Loss: 0.00002489
Iteration 12/1000 | Loss: 0.00002461
Iteration 13/1000 | Loss: 0.00002433
Iteration 14/1000 | Loss: 0.00002410
Iteration 15/1000 | Loss: 0.00002389
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00002347
Iteration 19/1000 | Loss: 0.00002341
Iteration 20/1000 | Loss: 0.00002340
Iteration 21/1000 | Loss: 0.00002336
Iteration 22/1000 | Loss: 0.00002334
Iteration 23/1000 | Loss: 0.00002333
Iteration 24/1000 | Loss: 0.00002333
Iteration 25/1000 | Loss: 0.00002332
Iteration 26/1000 | Loss: 0.00002329
Iteration 27/1000 | Loss: 0.00002326
Iteration 28/1000 | Loss: 0.00002324
Iteration 29/1000 | Loss: 0.00002324
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002323
Iteration 32/1000 | Loss: 0.00002323
Iteration 33/1000 | Loss: 0.00002323
Iteration 34/1000 | Loss: 0.00002323
Iteration 35/1000 | Loss: 0.00002323
Iteration 36/1000 | Loss: 0.00002321
Iteration 37/1000 | Loss: 0.00002321
Iteration 38/1000 | Loss: 0.00002320
Iteration 39/1000 | Loss: 0.00002320
Iteration 40/1000 | Loss: 0.00002319
Iteration 41/1000 | Loss: 0.00002319
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00002318
Iteration 44/1000 | Loss: 0.00002318
Iteration 45/1000 | Loss: 0.00002317
Iteration 46/1000 | Loss: 0.00002317
Iteration 47/1000 | Loss: 0.00002317
Iteration 48/1000 | Loss: 0.00002315
Iteration 49/1000 | Loss: 0.00002315
Iteration 50/1000 | Loss: 0.00002315
Iteration 51/1000 | Loss: 0.00002314
Iteration 52/1000 | Loss: 0.00002314
Iteration 53/1000 | Loss: 0.00002314
Iteration 54/1000 | Loss: 0.00002313
Iteration 55/1000 | Loss: 0.00002312
Iteration 56/1000 | Loss: 0.00002312
Iteration 57/1000 | Loss: 0.00002312
Iteration 58/1000 | Loss: 0.00002312
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00002311
Iteration 61/1000 | Loss: 0.00002311
Iteration 62/1000 | Loss: 0.00002311
Iteration 63/1000 | Loss: 0.00002311
Iteration 64/1000 | Loss: 0.00002311
Iteration 65/1000 | Loss: 0.00002311
Iteration 66/1000 | Loss: 0.00002311
Iteration 67/1000 | Loss: 0.00002310
Iteration 68/1000 | Loss: 0.00002310
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002310
Iteration 71/1000 | Loss: 0.00002310
Iteration 72/1000 | Loss: 0.00002310
Iteration 73/1000 | Loss: 0.00002310
Iteration 74/1000 | Loss: 0.00002310
Iteration 75/1000 | Loss: 0.00002310
Iteration 76/1000 | Loss: 0.00002310
Iteration 77/1000 | Loss: 0.00002309
Iteration 78/1000 | Loss: 0.00002309
Iteration 79/1000 | Loss: 0.00002309
Iteration 80/1000 | Loss: 0.00002309
Iteration 81/1000 | Loss: 0.00002308
Iteration 82/1000 | Loss: 0.00002308
Iteration 83/1000 | Loss: 0.00002308
Iteration 84/1000 | Loss: 0.00002308
Iteration 85/1000 | Loss: 0.00002308
Iteration 86/1000 | Loss: 0.00002308
Iteration 87/1000 | Loss: 0.00002308
Iteration 88/1000 | Loss: 0.00002308
Iteration 89/1000 | Loss: 0.00002307
Iteration 90/1000 | Loss: 0.00002307
Iteration 91/1000 | Loss: 0.00002307
Iteration 92/1000 | Loss: 0.00002307
Iteration 93/1000 | Loss: 0.00002307
Iteration 94/1000 | Loss: 0.00002307
Iteration 95/1000 | Loss: 0.00002307
Iteration 96/1000 | Loss: 0.00002306
Iteration 97/1000 | Loss: 0.00002306
Iteration 98/1000 | Loss: 0.00002306
Iteration 99/1000 | Loss: 0.00002306
Iteration 100/1000 | Loss: 0.00002306
Iteration 101/1000 | Loss: 0.00002306
Iteration 102/1000 | Loss: 0.00002306
Iteration 103/1000 | Loss: 0.00002305
Iteration 104/1000 | Loss: 0.00002305
Iteration 105/1000 | Loss: 0.00002305
Iteration 106/1000 | Loss: 0.00002305
Iteration 107/1000 | Loss: 0.00002305
Iteration 108/1000 | Loss: 0.00002305
Iteration 109/1000 | Loss: 0.00002305
Iteration 110/1000 | Loss: 0.00002305
Iteration 111/1000 | Loss: 0.00002304
Iteration 112/1000 | Loss: 0.00002304
Iteration 113/1000 | Loss: 0.00002304
Iteration 114/1000 | Loss: 0.00002304
Iteration 115/1000 | Loss: 0.00002304
Iteration 116/1000 | Loss: 0.00002304
Iteration 117/1000 | Loss: 0.00002304
Iteration 118/1000 | Loss: 0.00002304
Iteration 119/1000 | Loss: 0.00002304
Iteration 120/1000 | Loss: 0.00002303
Iteration 121/1000 | Loss: 0.00002303
Iteration 122/1000 | Loss: 0.00002303
Iteration 123/1000 | Loss: 0.00002303
Iteration 124/1000 | Loss: 0.00002303
Iteration 125/1000 | Loss: 0.00002302
Iteration 126/1000 | Loss: 0.00002302
Iteration 127/1000 | Loss: 0.00002302
Iteration 128/1000 | Loss: 0.00002302
Iteration 129/1000 | Loss: 0.00002302
Iteration 130/1000 | Loss: 0.00002302
Iteration 131/1000 | Loss: 0.00002302
Iteration 132/1000 | Loss: 0.00002302
Iteration 133/1000 | Loss: 0.00002302
Iteration 134/1000 | Loss: 0.00002302
Iteration 135/1000 | Loss: 0.00002302
Iteration 136/1000 | Loss: 0.00002302
Iteration 137/1000 | Loss: 0.00002301
Iteration 138/1000 | Loss: 0.00002301
Iteration 139/1000 | Loss: 0.00002301
Iteration 140/1000 | Loss: 0.00002301
Iteration 141/1000 | Loss: 0.00002301
Iteration 142/1000 | Loss: 0.00002301
Iteration 143/1000 | Loss: 0.00002301
Iteration 144/1000 | Loss: 0.00002301
Iteration 145/1000 | Loss: 0.00002301
Iteration 146/1000 | Loss: 0.00002301
Iteration 147/1000 | Loss: 0.00002301
Iteration 148/1000 | Loss: 0.00002301
Iteration 149/1000 | Loss: 0.00002301
Iteration 150/1000 | Loss: 0.00002301
Iteration 151/1000 | Loss: 0.00002301
Iteration 152/1000 | Loss: 0.00002301
Iteration 153/1000 | Loss: 0.00002301
Iteration 154/1000 | Loss: 0.00002301
Iteration 155/1000 | Loss: 0.00002301
Iteration 156/1000 | Loss: 0.00002301
Iteration 157/1000 | Loss: 0.00002300
Iteration 158/1000 | Loss: 0.00002300
Iteration 159/1000 | Loss: 0.00002300
Iteration 160/1000 | Loss: 0.00002300
Iteration 161/1000 | Loss: 0.00002300
Iteration 162/1000 | Loss: 0.00002300
Iteration 163/1000 | Loss: 0.00002300
Iteration 164/1000 | Loss: 0.00002300
Iteration 165/1000 | Loss: 0.00002300
Iteration 166/1000 | Loss: 0.00002300
Iteration 167/1000 | Loss: 0.00002300
Iteration 168/1000 | Loss: 0.00002300
Iteration 169/1000 | Loss: 0.00002299
Iteration 170/1000 | Loss: 0.00002299
Iteration 171/1000 | Loss: 0.00002299
Iteration 172/1000 | Loss: 0.00002299
Iteration 173/1000 | Loss: 0.00002299
Iteration 174/1000 | Loss: 0.00002299
Iteration 175/1000 | Loss: 0.00002299
Iteration 176/1000 | Loss: 0.00002299
Iteration 177/1000 | Loss: 0.00002299
Iteration 178/1000 | Loss: 0.00002299
Iteration 179/1000 | Loss: 0.00002298
Iteration 180/1000 | Loss: 0.00002298
Iteration 181/1000 | Loss: 0.00002298
Iteration 182/1000 | Loss: 0.00002298
Iteration 183/1000 | Loss: 0.00002298
Iteration 184/1000 | Loss: 0.00002298
Iteration 185/1000 | Loss: 0.00002298
Iteration 186/1000 | Loss: 0.00002298
Iteration 187/1000 | Loss: 0.00002298
Iteration 188/1000 | Loss: 0.00002298
Iteration 189/1000 | Loss: 0.00002298
Iteration 190/1000 | Loss: 0.00002298
Iteration 191/1000 | Loss: 0.00002298
Iteration 192/1000 | Loss: 0.00002298
Iteration 193/1000 | Loss: 0.00002297
Iteration 194/1000 | Loss: 0.00002297
Iteration 195/1000 | Loss: 0.00002297
Iteration 196/1000 | Loss: 0.00002297
Iteration 197/1000 | Loss: 0.00002297
Iteration 198/1000 | Loss: 0.00002297
Iteration 199/1000 | Loss: 0.00002297
Iteration 200/1000 | Loss: 0.00002297
Iteration 201/1000 | Loss: 0.00002297
Iteration 202/1000 | Loss: 0.00002297
Iteration 203/1000 | Loss: 0.00002297
Iteration 204/1000 | Loss: 0.00002297
Iteration 205/1000 | Loss: 0.00002297
Iteration 206/1000 | Loss: 0.00002297
Iteration 207/1000 | Loss: 0.00002297
Iteration 208/1000 | Loss: 0.00002297
Iteration 209/1000 | Loss: 0.00002297
Iteration 210/1000 | Loss: 0.00002297
Iteration 211/1000 | Loss: 0.00002296
Iteration 212/1000 | Loss: 0.00002296
Iteration 213/1000 | Loss: 0.00002296
Iteration 214/1000 | Loss: 0.00002296
Iteration 215/1000 | Loss: 0.00002296
Iteration 216/1000 | Loss: 0.00002296
Iteration 217/1000 | Loss: 0.00002296
Iteration 218/1000 | Loss: 0.00002296
Iteration 219/1000 | Loss: 0.00002296
Iteration 220/1000 | Loss: 0.00002296
Iteration 221/1000 | Loss: 0.00002296
Iteration 222/1000 | Loss: 0.00002296
Iteration 223/1000 | Loss: 0.00002296
Iteration 224/1000 | Loss: 0.00002296
Iteration 225/1000 | Loss: 0.00002296
Iteration 226/1000 | Loss: 0.00002295
Iteration 227/1000 | Loss: 0.00002295
Iteration 228/1000 | Loss: 0.00002295
Iteration 229/1000 | Loss: 0.00002295
Iteration 230/1000 | Loss: 0.00002295
Iteration 231/1000 | Loss: 0.00002295
Iteration 232/1000 | Loss: 0.00002295
Iteration 233/1000 | Loss: 0.00002295
Iteration 234/1000 | Loss: 0.00002295
Iteration 235/1000 | Loss: 0.00002295
Iteration 236/1000 | Loss: 0.00002295
Iteration 237/1000 | Loss: 0.00002295
Iteration 238/1000 | Loss: 0.00002295
Iteration 239/1000 | Loss: 0.00002295
Iteration 240/1000 | Loss: 0.00002295
Iteration 241/1000 | Loss: 0.00002295
Iteration 242/1000 | Loss: 0.00002295
Iteration 243/1000 | Loss: 0.00002295
Iteration 244/1000 | Loss: 0.00002295
Iteration 245/1000 | Loss: 0.00002295
Iteration 246/1000 | Loss: 0.00002295
Iteration 247/1000 | Loss: 0.00002294
Iteration 248/1000 | Loss: 0.00002294
Iteration 249/1000 | Loss: 0.00002294
Iteration 250/1000 | Loss: 0.00002294
Iteration 251/1000 | Loss: 0.00002294
Iteration 252/1000 | Loss: 0.00002294
Iteration 253/1000 | Loss: 0.00002294
Iteration 254/1000 | Loss: 0.00002294
Iteration 255/1000 | Loss: 0.00002294
Iteration 256/1000 | Loss: 0.00002294
Iteration 257/1000 | Loss: 0.00002294
Iteration 258/1000 | Loss: 0.00002294
Iteration 259/1000 | Loss: 0.00002294
Iteration 260/1000 | Loss: 0.00002294
Iteration 261/1000 | Loss: 0.00002294
Iteration 262/1000 | Loss: 0.00002294
Iteration 263/1000 | Loss: 0.00002294
Iteration 264/1000 | Loss: 0.00002294
Iteration 265/1000 | Loss: 0.00002294
Iteration 266/1000 | Loss: 0.00002294
Iteration 267/1000 | Loss: 0.00002294
Iteration 268/1000 | Loss: 0.00002294
Iteration 269/1000 | Loss: 0.00002294
Iteration 270/1000 | Loss: 0.00002294
Iteration 271/1000 | Loss: 0.00002294
Iteration 272/1000 | Loss: 0.00002294
Iteration 273/1000 | Loss: 0.00002294
Iteration 274/1000 | Loss: 0.00002294
Iteration 275/1000 | Loss: 0.00002294
Iteration 276/1000 | Loss: 0.00002294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [2.29420475079678e-05, 2.29420475079678e-05, 2.29420475079678e-05, 2.29420475079678e-05, 2.29420475079678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.29420475079678e-05

Optimization complete. Final v2v error: 3.8787171840667725 mm

Highest mean error: 4.854417324066162 mm for frame 59

Lowest mean error: 3.1195173263549805 mm for frame 136

Saving results

Total time: 53.76043343544006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970010
Iteration 2/25 | Loss: 0.00179656
Iteration 3/25 | Loss: 0.00147867
Iteration 4/25 | Loss: 0.00146044
Iteration 5/25 | Loss: 0.00145356
Iteration 6/25 | Loss: 0.00145285
Iteration 7/25 | Loss: 0.00145285
Iteration 8/25 | Loss: 0.00145285
Iteration 9/25 | Loss: 0.00145285
Iteration 10/25 | Loss: 0.00145285
Iteration 11/25 | Loss: 0.00145285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001452851458452642, 0.001452851458452642, 0.001452851458452642, 0.001452851458452642, 0.001452851458452642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001452851458452642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89146209
Iteration 2/25 | Loss: 0.00108594
Iteration 3/25 | Loss: 0.00108594
Iteration 4/25 | Loss: 0.00108593
Iteration 5/25 | Loss: 0.00108593
Iteration 6/25 | Loss: 0.00108593
Iteration 7/25 | Loss: 0.00108593
Iteration 8/25 | Loss: 0.00108593
Iteration 9/25 | Loss: 0.00108593
Iteration 10/25 | Loss: 0.00108593
Iteration 11/25 | Loss: 0.00108593
Iteration 12/25 | Loss: 0.00108593
Iteration 13/25 | Loss: 0.00108593
Iteration 14/25 | Loss: 0.00108593
Iteration 15/25 | Loss: 0.00108593
Iteration 16/25 | Loss: 0.00108593
Iteration 17/25 | Loss: 0.00108593
Iteration 18/25 | Loss: 0.00108593
Iteration 19/25 | Loss: 0.00108593
Iteration 20/25 | Loss: 0.00108593
Iteration 21/25 | Loss: 0.00108593
Iteration 22/25 | Loss: 0.00108593
Iteration 23/25 | Loss: 0.00108593
Iteration 24/25 | Loss: 0.00108593
Iteration 25/25 | Loss: 0.00108593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108593
Iteration 2/1000 | Loss: 0.00007113
Iteration 3/1000 | Loss: 0.00004234
Iteration 4/1000 | Loss: 0.00003519
Iteration 5/1000 | Loss: 0.00003296
Iteration 6/1000 | Loss: 0.00003180
Iteration 7/1000 | Loss: 0.00003068
Iteration 8/1000 | Loss: 0.00002998
Iteration 9/1000 | Loss: 0.00002957
Iteration 10/1000 | Loss: 0.00002908
Iteration 11/1000 | Loss: 0.00002881
Iteration 12/1000 | Loss: 0.00002852
Iteration 13/1000 | Loss: 0.00002825
Iteration 14/1000 | Loss: 0.00002798
Iteration 15/1000 | Loss: 0.00002776
Iteration 16/1000 | Loss: 0.00002754
Iteration 17/1000 | Loss: 0.00002748
Iteration 18/1000 | Loss: 0.00002733
Iteration 19/1000 | Loss: 0.00002720
Iteration 20/1000 | Loss: 0.00002715
Iteration 21/1000 | Loss: 0.00002714
Iteration 22/1000 | Loss: 0.00002709
Iteration 23/1000 | Loss: 0.00002709
Iteration 24/1000 | Loss: 0.00002706
Iteration 25/1000 | Loss: 0.00002703
Iteration 26/1000 | Loss: 0.00002700
Iteration 27/1000 | Loss: 0.00002699
Iteration 28/1000 | Loss: 0.00002694
Iteration 29/1000 | Loss: 0.00002686
Iteration 30/1000 | Loss: 0.00002683
Iteration 31/1000 | Loss: 0.00002683
Iteration 32/1000 | Loss: 0.00002682
Iteration 33/1000 | Loss: 0.00002682
Iteration 34/1000 | Loss: 0.00002682
Iteration 35/1000 | Loss: 0.00002681
Iteration 36/1000 | Loss: 0.00002681
Iteration 37/1000 | Loss: 0.00002680
Iteration 38/1000 | Loss: 0.00002680
Iteration 39/1000 | Loss: 0.00002680
Iteration 40/1000 | Loss: 0.00002679
Iteration 41/1000 | Loss: 0.00002679
Iteration 42/1000 | Loss: 0.00002679
Iteration 43/1000 | Loss: 0.00002679
Iteration 44/1000 | Loss: 0.00002679
Iteration 45/1000 | Loss: 0.00002679
Iteration 46/1000 | Loss: 0.00002679
Iteration 47/1000 | Loss: 0.00002679
Iteration 48/1000 | Loss: 0.00002679
Iteration 49/1000 | Loss: 0.00002679
Iteration 50/1000 | Loss: 0.00002679
Iteration 51/1000 | Loss: 0.00002679
Iteration 52/1000 | Loss: 0.00002679
Iteration 53/1000 | Loss: 0.00002679
Iteration 54/1000 | Loss: 0.00002679
Iteration 55/1000 | Loss: 0.00002679
Iteration 56/1000 | Loss: 0.00002679
Iteration 57/1000 | Loss: 0.00002679
Iteration 58/1000 | Loss: 0.00002679
Iteration 59/1000 | Loss: 0.00002679
Iteration 60/1000 | Loss: 0.00002679
Iteration 61/1000 | Loss: 0.00002679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [2.6785484806168824e-05, 2.6785484806168824e-05, 2.6785484806168824e-05, 2.6785484806168824e-05, 2.6785484806168824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6785484806168824e-05

Optimization complete. Final v2v error: 4.319747447967529 mm

Highest mean error: 5.228365421295166 mm for frame 111

Lowest mean error: 3.517392873764038 mm for frame 52

Saving results

Total time: 46.05180287361145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753056
Iteration 2/25 | Loss: 0.00151201
Iteration 3/25 | Loss: 0.00135781
Iteration 4/25 | Loss: 0.00133357
Iteration 5/25 | Loss: 0.00132693
Iteration 6/25 | Loss: 0.00132599
Iteration 7/25 | Loss: 0.00132599
Iteration 8/25 | Loss: 0.00132599
Iteration 9/25 | Loss: 0.00132599
Iteration 10/25 | Loss: 0.00132599
Iteration 11/25 | Loss: 0.00132599
Iteration 12/25 | Loss: 0.00132599
Iteration 13/25 | Loss: 0.00132599
Iteration 14/25 | Loss: 0.00132599
Iteration 15/25 | Loss: 0.00132599
Iteration 16/25 | Loss: 0.00132599
Iteration 17/25 | Loss: 0.00132599
Iteration 18/25 | Loss: 0.00132599
Iteration 19/25 | Loss: 0.00132599
Iteration 20/25 | Loss: 0.00132599
Iteration 21/25 | Loss: 0.00132599
Iteration 22/25 | Loss: 0.00132599
Iteration 23/25 | Loss: 0.00132599
Iteration 24/25 | Loss: 0.00132599
Iteration 25/25 | Loss: 0.00132599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44647455
Iteration 2/25 | Loss: 0.00103821
Iteration 3/25 | Loss: 0.00103821
Iteration 4/25 | Loss: 0.00103821
Iteration 5/25 | Loss: 0.00103821
Iteration 6/25 | Loss: 0.00103821
Iteration 7/25 | Loss: 0.00103821
Iteration 8/25 | Loss: 0.00103821
Iteration 9/25 | Loss: 0.00103821
Iteration 10/25 | Loss: 0.00103821
Iteration 11/25 | Loss: 0.00103821
Iteration 12/25 | Loss: 0.00103821
Iteration 13/25 | Loss: 0.00103821
Iteration 14/25 | Loss: 0.00103821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010382079053670168, 0.0010382079053670168, 0.0010382079053670168, 0.0010382079053670168, 0.0010382079053670168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010382079053670168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103821
Iteration 2/1000 | Loss: 0.00004116
Iteration 3/1000 | Loss: 0.00002707
Iteration 4/1000 | Loss: 0.00002295
Iteration 5/1000 | Loss: 0.00002152
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00001995
Iteration 8/1000 | Loss: 0.00001944
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001799
Iteration 14/1000 | Loss: 0.00001792
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001784
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001781
Iteration 19/1000 | Loss: 0.00001781
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001768
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001754
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001743
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001740
Iteration 65/1000 | Loss: 0.00001740
Iteration 66/1000 | Loss: 0.00001740
Iteration 67/1000 | Loss: 0.00001740
Iteration 68/1000 | Loss: 0.00001740
Iteration 69/1000 | Loss: 0.00001740
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001739
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001737
Iteration 80/1000 | Loss: 0.00001737
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001737
Iteration 83/1000 | Loss: 0.00001736
Iteration 84/1000 | Loss: 0.00001736
Iteration 85/1000 | Loss: 0.00001736
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001735
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001733
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001732
Iteration 98/1000 | Loss: 0.00001732
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001730
Iteration 105/1000 | Loss: 0.00001730
Iteration 106/1000 | Loss: 0.00001730
Iteration 107/1000 | Loss: 0.00001730
Iteration 108/1000 | Loss: 0.00001730
Iteration 109/1000 | Loss: 0.00001730
Iteration 110/1000 | Loss: 0.00001729
Iteration 111/1000 | Loss: 0.00001729
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001729
Iteration 115/1000 | Loss: 0.00001729
Iteration 116/1000 | Loss: 0.00001729
Iteration 117/1000 | Loss: 0.00001729
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001727
Iteration 122/1000 | Loss: 0.00001727
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001726
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001726
Iteration 130/1000 | Loss: 0.00001726
Iteration 131/1000 | Loss: 0.00001726
Iteration 132/1000 | Loss: 0.00001726
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001725
Iteration 136/1000 | Loss: 0.00001724
Iteration 137/1000 | Loss: 0.00001724
Iteration 138/1000 | Loss: 0.00001724
Iteration 139/1000 | Loss: 0.00001724
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001723
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001721
Iteration 160/1000 | Loss: 0.00001721
Iteration 161/1000 | Loss: 0.00001721
Iteration 162/1000 | Loss: 0.00001721
Iteration 163/1000 | Loss: 0.00001721
Iteration 164/1000 | Loss: 0.00001721
Iteration 165/1000 | Loss: 0.00001721
Iteration 166/1000 | Loss: 0.00001721
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001720
Iteration 173/1000 | Loss: 0.00001720
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001719
Iteration 180/1000 | Loss: 0.00001719
Iteration 181/1000 | Loss: 0.00001719
Iteration 182/1000 | Loss: 0.00001719
Iteration 183/1000 | Loss: 0.00001719
Iteration 184/1000 | Loss: 0.00001719
Iteration 185/1000 | Loss: 0.00001719
Iteration 186/1000 | Loss: 0.00001719
Iteration 187/1000 | Loss: 0.00001719
Iteration 188/1000 | Loss: 0.00001719
Iteration 189/1000 | Loss: 0.00001719
Iteration 190/1000 | Loss: 0.00001719
Iteration 191/1000 | Loss: 0.00001719
Iteration 192/1000 | Loss: 0.00001719
Iteration 193/1000 | Loss: 0.00001719
Iteration 194/1000 | Loss: 0.00001718
Iteration 195/1000 | Loss: 0.00001718
Iteration 196/1000 | Loss: 0.00001718
Iteration 197/1000 | Loss: 0.00001718
Iteration 198/1000 | Loss: 0.00001718
Iteration 199/1000 | Loss: 0.00001718
Iteration 200/1000 | Loss: 0.00001718
Iteration 201/1000 | Loss: 0.00001718
Iteration 202/1000 | Loss: 0.00001718
Iteration 203/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.7183085219585337e-05, 1.7183085219585337e-05, 1.7183085219585337e-05, 1.7183085219585337e-05, 1.7183085219585337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7183085219585337e-05

Optimization complete. Final v2v error: 3.5234198570251465 mm

Highest mean error: 3.8691391944885254 mm for frame 26

Lowest mean error: 2.8807365894317627 mm for frame 168

Saving results

Total time: 51.56923770904541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598822
Iteration 2/25 | Loss: 0.00151236
Iteration 3/25 | Loss: 0.00139596
Iteration 4/25 | Loss: 0.00137201
Iteration 5/25 | Loss: 0.00136807
Iteration 6/25 | Loss: 0.00136624
Iteration 7/25 | Loss: 0.00136669
Iteration 8/25 | Loss: 0.00136535
Iteration 9/25 | Loss: 0.00136636
Iteration 10/25 | Loss: 0.00136557
Iteration 11/25 | Loss: 0.00136308
Iteration 12/25 | Loss: 0.00136167
Iteration 13/25 | Loss: 0.00136346
Iteration 14/25 | Loss: 0.00136517
Iteration 15/25 | Loss: 0.00136478
Iteration 16/25 | Loss: 0.00136200
Iteration 17/25 | Loss: 0.00136161
Iteration 18/25 | Loss: 0.00136406
Iteration 19/25 | Loss: 0.00136504
Iteration 20/25 | Loss: 0.00136070
Iteration 21/25 | Loss: 0.00135994
Iteration 22/25 | Loss: 0.00135959
Iteration 23/25 | Loss: 0.00135945
Iteration 24/25 | Loss: 0.00135941
Iteration 25/25 | Loss: 0.00135941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50490713
Iteration 2/25 | Loss: 0.00107276
Iteration 3/25 | Loss: 0.00107276
Iteration 4/25 | Loss: 0.00107275
Iteration 5/25 | Loss: 0.00107275
Iteration 6/25 | Loss: 0.00107275
Iteration 7/25 | Loss: 0.00107275
Iteration 8/25 | Loss: 0.00107275
Iteration 9/25 | Loss: 0.00107275
Iteration 10/25 | Loss: 0.00107275
Iteration 11/25 | Loss: 0.00107275
Iteration 12/25 | Loss: 0.00107275
Iteration 13/25 | Loss: 0.00107275
Iteration 14/25 | Loss: 0.00107275
Iteration 15/25 | Loss: 0.00107275
Iteration 16/25 | Loss: 0.00107275
Iteration 17/25 | Loss: 0.00107275
Iteration 18/25 | Loss: 0.00107275
Iteration 19/25 | Loss: 0.00107275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010727510089054704, 0.0010727510089054704, 0.0010727510089054704, 0.0010727510089054704, 0.0010727510089054704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010727510089054704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107275
Iteration 2/1000 | Loss: 0.00006258
Iteration 3/1000 | Loss: 0.00005482
Iteration 4/1000 | Loss: 0.00013259
Iteration 5/1000 | Loss: 0.00005111
Iteration 6/1000 | Loss: 0.00003334
Iteration 7/1000 | Loss: 0.00010832
Iteration 8/1000 | Loss: 0.00008930
Iteration 9/1000 | Loss: 0.00013566
Iteration 10/1000 | Loss: 0.00013811
Iteration 11/1000 | Loss: 0.00007762
Iteration 12/1000 | Loss: 0.00009699
Iteration 13/1000 | Loss: 0.00013684
Iteration 14/1000 | Loss: 0.00013436
Iteration 15/1000 | Loss: 0.00011699
Iteration 16/1000 | Loss: 0.00013364
Iteration 17/1000 | Loss: 0.00011392
Iteration 18/1000 | Loss: 0.00007376
Iteration 19/1000 | Loss: 0.00012801
Iteration 20/1000 | Loss: 0.00006458
Iteration 21/1000 | Loss: 0.00014144
Iteration 22/1000 | Loss: 0.00010398
Iteration 23/1000 | Loss: 0.00013188
Iteration 24/1000 | Loss: 0.00003545
Iteration 25/1000 | Loss: 0.00007543
Iteration 26/1000 | Loss: 0.00007991
Iteration 27/1000 | Loss: 0.00035617
Iteration 28/1000 | Loss: 0.00032737
Iteration 29/1000 | Loss: 0.00017693
Iteration 30/1000 | Loss: 0.00006883
Iteration 31/1000 | Loss: 0.00014326
Iteration 32/1000 | Loss: 0.00005828
Iteration 33/1000 | Loss: 0.00005092
Iteration 34/1000 | Loss: 0.00012551
Iteration 35/1000 | Loss: 0.00020551
Iteration 36/1000 | Loss: 0.00041289
Iteration 37/1000 | Loss: 0.00016855
Iteration 38/1000 | Loss: 0.00037245
Iteration 39/1000 | Loss: 0.00010326
Iteration 40/1000 | Loss: 0.00009079
Iteration 41/1000 | Loss: 0.00013585
Iteration 42/1000 | Loss: 0.00010997
Iteration 43/1000 | Loss: 0.00009250
Iteration 44/1000 | Loss: 0.00008387
Iteration 45/1000 | Loss: 0.00007494
Iteration 46/1000 | Loss: 0.00013487
Iteration 47/1000 | Loss: 0.00027415
Iteration 48/1000 | Loss: 0.00016470
Iteration 49/1000 | Loss: 0.00007033
Iteration 50/1000 | Loss: 0.00009344
Iteration 51/1000 | Loss: 0.00005905
Iteration 52/1000 | Loss: 0.00015787
Iteration 53/1000 | Loss: 0.00009520
Iteration 54/1000 | Loss: 0.00011176
Iteration 55/1000 | Loss: 0.00007664
Iteration 56/1000 | Loss: 0.00010722
Iteration 57/1000 | Loss: 0.00011005
Iteration 58/1000 | Loss: 0.00009710
Iteration 59/1000 | Loss: 0.00005037
Iteration 60/1000 | Loss: 0.00007615
Iteration 61/1000 | Loss: 0.00006030
Iteration 62/1000 | Loss: 0.00006791
Iteration 63/1000 | Loss: 0.00006824
Iteration 64/1000 | Loss: 0.00003294
Iteration 65/1000 | Loss: 0.00005368
Iteration 66/1000 | Loss: 0.00010861
Iteration 67/1000 | Loss: 0.00009417
Iteration 68/1000 | Loss: 0.00005693
Iteration 69/1000 | Loss: 0.00006644
Iteration 70/1000 | Loss: 0.00005102
Iteration 71/1000 | Loss: 0.00004437
Iteration 72/1000 | Loss: 0.00012605
Iteration 73/1000 | Loss: 0.00010610
Iteration 74/1000 | Loss: 0.00010198
Iteration 75/1000 | Loss: 0.00003926
Iteration 76/1000 | Loss: 0.00007625
Iteration 77/1000 | Loss: 0.00010630
Iteration 78/1000 | Loss: 0.00003200
Iteration 79/1000 | Loss: 0.00006839
Iteration 80/1000 | Loss: 0.00009408
Iteration 81/1000 | Loss: 0.00003893
Iteration 82/1000 | Loss: 0.00006162
Iteration 83/1000 | Loss: 0.00005549
Iteration 84/1000 | Loss: 0.00013279
Iteration 85/1000 | Loss: 0.00004541
Iteration 86/1000 | Loss: 0.00010320
Iteration 87/1000 | Loss: 0.00011870
Iteration 88/1000 | Loss: 0.00007750
Iteration 89/1000 | Loss: 0.00011246
Iteration 90/1000 | Loss: 0.00012254
Iteration 91/1000 | Loss: 0.00012931
Iteration 92/1000 | Loss: 0.00007900
Iteration 93/1000 | Loss: 0.00018691
Iteration 94/1000 | Loss: 0.00011154
Iteration 95/1000 | Loss: 0.00017565
Iteration 96/1000 | Loss: 0.00009830
Iteration 97/1000 | Loss: 0.00011352
Iteration 98/1000 | Loss: 0.00016493
Iteration 99/1000 | Loss: 0.00011246
Iteration 100/1000 | Loss: 0.00006772
Iteration 101/1000 | Loss: 0.00017306
Iteration 102/1000 | Loss: 0.00005770
Iteration 103/1000 | Loss: 0.00010264
Iteration 104/1000 | Loss: 0.00004951
Iteration 105/1000 | Loss: 0.00009644
Iteration 106/1000 | Loss: 0.00004575
Iteration 107/1000 | Loss: 0.00009553
Iteration 108/1000 | Loss: 0.00005961
Iteration 109/1000 | Loss: 0.00006105
Iteration 110/1000 | Loss: 0.00011242
Iteration 111/1000 | Loss: 0.00004038
Iteration 112/1000 | Loss: 0.00005865
Iteration 113/1000 | Loss: 0.00013750
Iteration 114/1000 | Loss: 0.00009084
Iteration 115/1000 | Loss: 0.00007698
Iteration 116/1000 | Loss: 0.00006688
Iteration 117/1000 | Loss: 0.00006731
Iteration 118/1000 | Loss: 0.00012427
Iteration 119/1000 | Loss: 0.00013533
Iteration 120/1000 | Loss: 0.00011571
Iteration 121/1000 | Loss: 0.00008154
Iteration 122/1000 | Loss: 0.00002670
Iteration 123/1000 | Loss: 0.00008552
Iteration 124/1000 | Loss: 0.00009541
Iteration 125/1000 | Loss: 0.00016565
Iteration 126/1000 | Loss: 0.00013082
Iteration 127/1000 | Loss: 0.00019622
Iteration 128/1000 | Loss: 0.00010329
Iteration 129/1000 | Loss: 0.00014218
Iteration 130/1000 | Loss: 0.00003141
Iteration 131/1000 | Loss: 0.00008604
Iteration 132/1000 | Loss: 0.00012289
Iteration 133/1000 | Loss: 0.00013164
Iteration 134/1000 | Loss: 0.00015791
Iteration 135/1000 | Loss: 0.00012139
Iteration 136/1000 | Loss: 0.00012564
Iteration 137/1000 | Loss: 0.00010567
Iteration 138/1000 | Loss: 0.00011115
Iteration 139/1000 | Loss: 0.00008446
Iteration 140/1000 | Loss: 0.00022613
Iteration 141/1000 | Loss: 0.00003316
Iteration 142/1000 | Loss: 0.00002761
Iteration 143/1000 | Loss: 0.00002687
Iteration 144/1000 | Loss: 0.00002598
Iteration 145/1000 | Loss: 0.00002554
Iteration 146/1000 | Loss: 0.00002500
Iteration 147/1000 | Loss: 0.00002467
Iteration 148/1000 | Loss: 0.00009637
Iteration 149/1000 | Loss: 0.00006491
Iteration 150/1000 | Loss: 0.00012262
Iteration 151/1000 | Loss: 0.00006068
Iteration 152/1000 | Loss: 0.00011877
Iteration 153/1000 | Loss: 0.00009886
Iteration 154/1000 | Loss: 0.00011479
Iteration 155/1000 | Loss: 0.00004040
Iteration 156/1000 | Loss: 0.00003709
Iteration 157/1000 | Loss: 0.00002611
Iteration 158/1000 | Loss: 0.00002478
Iteration 159/1000 | Loss: 0.00002430
Iteration 160/1000 | Loss: 0.00002403
Iteration 161/1000 | Loss: 0.00002398
Iteration 162/1000 | Loss: 0.00002381
Iteration 163/1000 | Loss: 0.00002377
Iteration 164/1000 | Loss: 0.00002376
Iteration 165/1000 | Loss: 0.00002373
Iteration 166/1000 | Loss: 0.00002372
Iteration 167/1000 | Loss: 0.00002372
Iteration 168/1000 | Loss: 0.00002371
Iteration 169/1000 | Loss: 0.00002370
Iteration 170/1000 | Loss: 0.00002370
Iteration 171/1000 | Loss: 0.00002369
Iteration 172/1000 | Loss: 0.00002369
Iteration 173/1000 | Loss: 0.00002369
Iteration 174/1000 | Loss: 0.00002368
Iteration 175/1000 | Loss: 0.00002368
Iteration 176/1000 | Loss: 0.00002368
Iteration 177/1000 | Loss: 0.00002367
Iteration 178/1000 | Loss: 0.00002367
Iteration 179/1000 | Loss: 0.00002366
Iteration 180/1000 | Loss: 0.00002364
Iteration 181/1000 | Loss: 0.00002360
Iteration 182/1000 | Loss: 0.00002360
Iteration 183/1000 | Loss: 0.00002360
Iteration 184/1000 | Loss: 0.00002360
Iteration 185/1000 | Loss: 0.00002360
Iteration 186/1000 | Loss: 0.00002360
Iteration 187/1000 | Loss: 0.00002359
Iteration 188/1000 | Loss: 0.00002358
Iteration 189/1000 | Loss: 0.00002358
Iteration 190/1000 | Loss: 0.00002358
Iteration 191/1000 | Loss: 0.00002358
Iteration 192/1000 | Loss: 0.00002357
Iteration 193/1000 | Loss: 0.00002357
Iteration 194/1000 | Loss: 0.00002357
Iteration 195/1000 | Loss: 0.00002356
Iteration 196/1000 | Loss: 0.00002345
Iteration 197/1000 | Loss: 0.00002345
Iteration 198/1000 | Loss: 0.00002342
Iteration 199/1000 | Loss: 0.00002341
Iteration 200/1000 | Loss: 0.00002340
Iteration 201/1000 | Loss: 0.00002340
Iteration 202/1000 | Loss: 0.00002340
Iteration 203/1000 | Loss: 0.00002339
Iteration 204/1000 | Loss: 0.00002339
Iteration 205/1000 | Loss: 0.00002338
Iteration 206/1000 | Loss: 0.00002338
Iteration 207/1000 | Loss: 0.00002338
Iteration 208/1000 | Loss: 0.00002338
Iteration 209/1000 | Loss: 0.00002337
Iteration 210/1000 | Loss: 0.00002337
Iteration 211/1000 | Loss: 0.00002337
Iteration 212/1000 | Loss: 0.00002337
Iteration 213/1000 | Loss: 0.00002337
Iteration 214/1000 | Loss: 0.00002337
Iteration 215/1000 | Loss: 0.00002336
Iteration 216/1000 | Loss: 0.00002336
Iteration 217/1000 | Loss: 0.00002336
Iteration 218/1000 | Loss: 0.00002336
Iteration 219/1000 | Loss: 0.00002336
Iteration 220/1000 | Loss: 0.00002336
Iteration 221/1000 | Loss: 0.00002336
Iteration 222/1000 | Loss: 0.00002335
Iteration 223/1000 | Loss: 0.00002335
Iteration 224/1000 | Loss: 0.00002335
Iteration 225/1000 | Loss: 0.00002334
Iteration 226/1000 | Loss: 0.00002334
Iteration 227/1000 | Loss: 0.00002334
Iteration 228/1000 | Loss: 0.00002334
Iteration 229/1000 | Loss: 0.00002334
Iteration 230/1000 | Loss: 0.00002333
Iteration 231/1000 | Loss: 0.00002333
Iteration 232/1000 | Loss: 0.00002333
Iteration 233/1000 | Loss: 0.00002333
Iteration 234/1000 | Loss: 0.00002333
Iteration 235/1000 | Loss: 0.00002333
Iteration 236/1000 | Loss: 0.00002333
Iteration 237/1000 | Loss: 0.00002332
Iteration 238/1000 | Loss: 0.00002332
Iteration 239/1000 | Loss: 0.00002332
Iteration 240/1000 | Loss: 0.00002332
Iteration 241/1000 | Loss: 0.00002332
Iteration 242/1000 | Loss: 0.00002332
Iteration 243/1000 | Loss: 0.00002332
Iteration 244/1000 | Loss: 0.00002332
Iteration 245/1000 | Loss: 0.00002332
Iteration 246/1000 | Loss: 0.00002332
Iteration 247/1000 | Loss: 0.00002332
Iteration 248/1000 | Loss: 0.00002332
Iteration 249/1000 | Loss: 0.00002332
Iteration 250/1000 | Loss: 0.00002332
Iteration 251/1000 | Loss: 0.00002331
Iteration 252/1000 | Loss: 0.00002331
Iteration 253/1000 | Loss: 0.00002331
Iteration 254/1000 | Loss: 0.00002331
Iteration 255/1000 | Loss: 0.00002331
Iteration 256/1000 | Loss: 0.00002331
Iteration 257/1000 | Loss: 0.00002331
Iteration 258/1000 | Loss: 0.00002331
Iteration 259/1000 | Loss: 0.00002331
Iteration 260/1000 | Loss: 0.00002331
Iteration 261/1000 | Loss: 0.00002331
Iteration 262/1000 | Loss: 0.00002331
Iteration 263/1000 | Loss: 0.00002331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.3313317797146738e-05, 2.3313317797146738e-05, 2.3313317797146738e-05, 2.3313317797146738e-05, 2.3313317797146738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3313317797146738e-05

Optimization complete. Final v2v error: 3.9532268047332764 mm

Highest mean error: 5.3267621994018555 mm for frame 158

Lowest mean error: 3.087693452835083 mm for frame 197

Saving results

Total time: 314.67811822891235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00686025
Iteration 2/25 | Loss: 0.00162251
Iteration 3/25 | Loss: 0.00140610
Iteration 4/25 | Loss: 0.00135642
Iteration 5/25 | Loss: 0.00134944
Iteration 6/25 | Loss: 0.00132957
Iteration 7/25 | Loss: 0.00131742
Iteration 8/25 | Loss: 0.00131540
Iteration 9/25 | Loss: 0.00131389
Iteration 10/25 | Loss: 0.00131297
Iteration 11/25 | Loss: 0.00132019
Iteration 12/25 | Loss: 0.00130583
Iteration 13/25 | Loss: 0.00130462
Iteration 14/25 | Loss: 0.00130435
Iteration 15/25 | Loss: 0.00130413
Iteration 16/25 | Loss: 0.00130417
Iteration 17/25 | Loss: 0.00130138
Iteration 18/25 | Loss: 0.00130052
Iteration 19/25 | Loss: 0.00130019
Iteration 20/25 | Loss: 0.00130002
Iteration 21/25 | Loss: 0.00129991
Iteration 22/25 | Loss: 0.00130508
Iteration 23/25 | Loss: 0.00129979
Iteration 24/25 | Loss: 0.00129727
Iteration 25/25 | Loss: 0.00129551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39088356
Iteration 2/25 | Loss: 0.00119463
Iteration 3/25 | Loss: 0.00119462
Iteration 4/25 | Loss: 0.00119462
Iteration 5/25 | Loss: 0.00119462
Iteration 6/25 | Loss: 0.00119462
Iteration 7/25 | Loss: 0.00119462
Iteration 8/25 | Loss: 0.00119462
Iteration 9/25 | Loss: 0.00119462
Iteration 10/25 | Loss: 0.00119462
Iteration 11/25 | Loss: 0.00119462
Iteration 12/25 | Loss: 0.00119462
Iteration 13/25 | Loss: 0.00119462
Iteration 14/25 | Loss: 0.00119462
Iteration 15/25 | Loss: 0.00119462
Iteration 16/25 | Loss: 0.00119462
Iteration 17/25 | Loss: 0.00119462
Iteration 18/25 | Loss: 0.00119462
Iteration 19/25 | Loss: 0.00119462
Iteration 20/25 | Loss: 0.00119462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011946227168664336, 0.0011946227168664336, 0.0011946227168664336, 0.0011946227168664336, 0.0011946227168664336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011946227168664336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119462
Iteration 2/1000 | Loss: 0.00064545
Iteration 3/1000 | Loss: 0.00037072
Iteration 4/1000 | Loss: 0.00005042
Iteration 5/1000 | Loss: 0.00008632
Iteration 6/1000 | Loss: 0.00004109
Iteration 7/1000 | Loss: 0.00003023
Iteration 8/1000 | Loss: 0.00002841
Iteration 9/1000 | Loss: 0.00002664
Iteration 10/1000 | Loss: 0.00002552
Iteration 11/1000 | Loss: 0.00002456
Iteration 12/1000 | Loss: 0.00002409
Iteration 13/1000 | Loss: 0.00002356
Iteration 14/1000 | Loss: 0.00002307
Iteration 15/1000 | Loss: 0.00002266
Iteration 16/1000 | Loss: 0.00002243
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002204
Iteration 19/1000 | Loss: 0.00002202
Iteration 20/1000 | Loss: 0.00002194
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002186
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002185
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002180
Iteration 27/1000 | Loss: 0.00002177
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002171
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002164
Iteration 37/1000 | Loss: 0.00002164
Iteration 38/1000 | Loss: 0.00002163
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002163
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002162
Iteration 43/1000 | Loss: 0.00002162
Iteration 44/1000 | Loss: 0.00002161
Iteration 45/1000 | Loss: 0.00002161
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002160
Iteration 48/1000 | Loss: 0.00002160
Iteration 49/1000 | Loss: 0.00002160
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002153
Iteration 57/1000 | Loss: 0.00002153
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002152
Iteration 60/1000 | Loss: 0.00002152
Iteration 61/1000 | Loss: 0.00002151
Iteration 62/1000 | Loss: 0.00002151
Iteration 63/1000 | Loss: 0.00002150
Iteration 64/1000 | Loss: 0.00002150
Iteration 65/1000 | Loss: 0.00002149
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002147
Iteration 69/1000 | Loss: 0.00002147
Iteration 70/1000 | Loss: 0.00002147
Iteration 71/1000 | Loss: 0.00002147
Iteration 72/1000 | Loss: 0.00002147
Iteration 73/1000 | Loss: 0.00002147
Iteration 74/1000 | Loss: 0.00002147
Iteration 75/1000 | Loss: 0.00002146
Iteration 76/1000 | Loss: 0.00002146
Iteration 77/1000 | Loss: 0.00002146
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002145
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002144
Iteration 82/1000 | Loss: 0.00002144
Iteration 83/1000 | Loss: 0.00002144
Iteration 84/1000 | Loss: 0.00002144
Iteration 85/1000 | Loss: 0.00002144
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002143
Iteration 88/1000 | Loss: 0.00002143
Iteration 89/1000 | Loss: 0.00002143
Iteration 90/1000 | Loss: 0.00002142
Iteration 91/1000 | Loss: 0.00002142
Iteration 92/1000 | Loss: 0.00002142
Iteration 93/1000 | Loss: 0.00002141
Iteration 94/1000 | Loss: 0.00002141
Iteration 95/1000 | Loss: 0.00002141
Iteration 96/1000 | Loss: 0.00002141
Iteration 97/1000 | Loss: 0.00002141
Iteration 98/1000 | Loss: 0.00002141
Iteration 99/1000 | Loss: 0.00002141
Iteration 100/1000 | Loss: 0.00002140
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002140
Iteration 103/1000 | Loss: 0.00002140
Iteration 104/1000 | Loss: 0.00002140
Iteration 105/1000 | Loss: 0.00002140
Iteration 106/1000 | Loss: 0.00002139
Iteration 107/1000 | Loss: 0.00002139
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002138
Iteration 113/1000 | Loss: 0.00002138
Iteration 114/1000 | Loss: 0.00002138
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002137
Iteration 117/1000 | Loss: 0.00002137
Iteration 118/1000 | Loss: 0.00002137
Iteration 119/1000 | Loss: 0.00002137
Iteration 120/1000 | Loss: 0.00002137
Iteration 121/1000 | Loss: 0.00002137
Iteration 122/1000 | Loss: 0.00002136
Iteration 123/1000 | Loss: 0.00002136
Iteration 124/1000 | Loss: 0.00002136
Iteration 125/1000 | Loss: 0.00002136
Iteration 126/1000 | Loss: 0.00002136
Iteration 127/1000 | Loss: 0.00002136
Iteration 128/1000 | Loss: 0.00002135
Iteration 129/1000 | Loss: 0.00002135
Iteration 130/1000 | Loss: 0.00002135
Iteration 131/1000 | Loss: 0.00002135
Iteration 132/1000 | Loss: 0.00002135
Iteration 133/1000 | Loss: 0.00002135
Iteration 134/1000 | Loss: 0.00002135
Iteration 135/1000 | Loss: 0.00002134
Iteration 136/1000 | Loss: 0.00002134
Iteration 137/1000 | Loss: 0.00002134
Iteration 138/1000 | Loss: 0.00002134
Iteration 139/1000 | Loss: 0.00002134
Iteration 140/1000 | Loss: 0.00002134
Iteration 141/1000 | Loss: 0.00002134
Iteration 142/1000 | Loss: 0.00002134
Iteration 143/1000 | Loss: 0.00002134
Iteration 144/1000 | Loss: 0.00002134
Iteration 145/1000 | Loss: 0.00002134
Iteration 146/1000 | Loss: 0.00002134
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002134
Iteration 149/1000 | Loss: 0.00002134
Iteration 150/1000 | Loss: 0.00002133
Iteration 151/1000 | Loss: 0.00002133
Iteration 152/1000 | Loss: 0.00002133
Iteration 153/1000 | Loss: 0.00002133
Iteration 154/1000 | Loss: 0.00002133
Iteration 155/1000 | Loss: 0.00002133
Iteration 156/1000 | Loss: 0.00002133
Iteration 157/1000 | Loss: 0.00002133
Iteration 158/1000 | Loss: 0.00002133
Iteration 159/1000 | Loss: 0.00002132
Iteration 160/1000 | Loss: 0.00002132
Iteration 161/1000 | Loss: 0.00002132
Iteration 162/1000 | Loss: 0.00002132
Iteration 163/1000 | Loss: 0.00002132
Iteration 164/1000 | Loss: 0.00002132
Iteration 165/1000 | Loss: 0.00002132
Iteration 166/1000 | Loss: 0.00002132
Iteration 167/1000 | Loss: 0.00002131
Iteration 168/1000 | Loss: 0.00002131
Iteration 169/1000 | Loss: 0.00002131
Iteration 170/1000 | Loss: 0.00002131
Iteration 171/1000 | Loss: 0.00002131
Iteration 172/1000 | Loss: 0.00002130
Iteration 173/1000 | Loss: 0.00002130
Iteration 174/1000 | Loss: 0.00002130
Iteration 175/1000 | Loss: 0.00002130
Iteration 176/1000 | Loss: 0.00002130
Iteration 177/1000 | Loss: 0.00002130
Iteration 178/1000 | Loss: 0.00002130
Iteration 179/1000 | Loss: 0.00002130
Iteration 180/1000 | Loss: 0.00002130
Iteration 181/1000 | Loss: 0.00002130
Iteration 182/1000 | Loss: 0.00002130
Iteration 183/1000 | Loss: 0.00002130
Iteration 184/1000 | Loss: 0.00002130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.1298803403624333e-05, 2.1298803403624333e-05, 2.1298803403624333e-05, 2.1298803403624333e-05, 2.1298803403624333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1298803403624333e-05

Optimization complete. Final v2v error: 3.8870832920074463 mm

Highest mean error: 4.320664882659912 mm for frame 48

Lowest mean error: 3.3003313541412354 mm for frame 199

Saving results

Total time: 94.58213973045349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856289
Iteration 2/25 | Loss: 0.00191155
Iteration 3/25 | Loss: 0.00158652
Iteration 4/25 | Loss: 0.00152795
Iteration 5/25 | Loss: 0.00154275
Iteration 6/25 | Loss: 0.00151145
Iteration 7/25 | Loss: 0.00143235
Iteration 8/25 | Loss: 0.00142009
Iteration 9/25 | Loss: 0.00140843
Iteration 10/25 | Loss: 0.00140344
Iteration 11/25 | Loss: 0.00139192
Iteration 12/25 | Loss: 0.00138689
Iteration 13/25 | Loss: 0.00139205
Iteration 14/25 | Loss: 0.00139496
Iteration 15/25 | Loss: 0.00138046
Iteration 16/25 | Loss: 0.00137210
Iteration 17/25 | Loss: 0.00136868
Iteration 18/25 | Loss: 0.00136760
Iteration 19/25 | Loss: 0.00136749
Iteration 20/25 | Loss: 0.00136742
Iteration 21/25 | Loss: 0.00136742
Iteration 22/25 | Loss: 0.00136742
Iteration 23/25 | Loss: 0.00136742
Iteration 24/25 | Loss: 0.00136742
Iteration 25/25 | Loss: 0.00136742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.56476021
Iteration 2/25 | Loss: 0.00093927
Iteration 3/25 | Loss: 0.00093926
Iteration 4/25 | Loss: 0.00093926
Iteration 5/25 | Loss: 0.00093926
Iteration 6/25 | Loss: 0.00093926
Iteration 7/25 | Loss: 0.00093926
Iteration 8/25 | Loss: 0.00093926
Iteration 9/25 | Loss: 0.00093926
Iteration 10/25 | Loss: 0.00093926
Iteration 11/25 | Loss: 0.00093926
Iteration 12/25 | Loss: 0.00093926
Iteration 13/25 | Loss: 0.00093926
Iteration 14/25 | Loss: 0.00093926
Iteration 15/25 | Loss: 0.00093926
Iteration 16/25 | Loss: 0.00093926
Iteration 17/25 | Loss: 0.00093926
Iteration 18/25 | Loss: 0.00093926
Iteration 19/25 | Loss: 0.00093926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009392611682415009, 0.0009392611682415009, 0.0009392611682415009, 0.0009392611682415009, 0.0009392611682415009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009392611682415009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093926
Iteration 2/1000 | Loss: 0.00012443
Iteration 3/1000 | Loss: 0.00007304
Iteration 4/1000 | Loss: 0.00029889
Iteration 5/1000 | Loss: 0.00003001
Iteration 6/1000 | Loss: 0.00002447
Iteration 7/1000 | Loss: 0.00004226
Iteration 8/1000 | Loss: 0.00017686
Iteration 9/1000 | Loss: 0.00110289
Iteration 10/1000 | Loss: 0.00002738
Iteration 11/1000 | Loss: 0.00003184
Iteration 12/1000 | Loss: 0.00011066
Iteration 13/1000 | Loss: 0.00002161
Iteration 14/1000 | Loss: 0.00002123
Iteration 15/1000 | Loss: 0.00002089
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00002020
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002007
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001994
Iteration 27/1000 | Loss: 0.00001994
Iteration 28/1000 | Loss: 0.00001993
Iteration 29/1000 | Loss: 0.00001993
Iteration 30/1000 | Loss: 0.00001984
Iteration 31/1000 | Loss: 0.00001983
Iteration 32/1000 | Loss: 0.00001982
Iteration 33/1000 | Loss: 0.00001982
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001980
Iteration 40/1000 | Loss: 0.00001980
Iteration 41/1000 | Loss: 0.00001980
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001980
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001979
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001976
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001974
Iteration 66/1000 | Loss: 0.00001974
Iteration 67/1000 | Loss: 0.00001974
Iteration 68/1000 | Loss: 0.00001974
Iteration 69/1000 | Loss: 0.00001974
Iteration 70/1000 | Loss: 0.00001974
Iteration 71/1000 | Loss: 0.00001974
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.9735236492124386e-05, 1.9735236492124386e-05, 1.9735236492124386e-05, 1.9735236492124386e-05, 1.9735236492124386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9735236492124386e-05

Optimization complete. Final v2v error: 3.67585825920105 mm

Highest mean error: 5.565272331237793 mm for frame 95

Lowest mean error: 3.2455272674560547 mm for frame 52

Saving results

Total time: 69.4589946269989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404836
Iteration 2/25 | Loss: 0.00137558
Iteration 3/25 | Loss: 0.00127812
Iteration 4/25 | Loss: 0.00126506
Iteration 5/25 | Loss: 0.00126284
Iteration 6/25 | Loss: 0.00126258
Iteration 7/25 | Loss: 0.00126258
Iteration 8/25 | Loss: 0.00126258
Iteration 9/25 | Loss: 0.00126258
Iteration 10/25 | Loss: 0.00126258
Iteration 11/25 | Loss: 0.00126258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00126257527153939, 0.00126257527153939, 0.00126257527153939, 0.00126257527153939, 0.00126257527153939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00126257527153939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38930500
Iteration 2/25 | Loss: 0.00073337
Iteration 3/25 | Loss: 0.00073336
Iteration 4/25 | Loss: 0.00073336
Iteration 5/25 | Loss: 0.00073336
Iteration 6/25 | Loss: 0.00073336
Iteration 7/25 | Loss: 0.00073336
Iteration 8/25 | Loss: 0.00073336
Iteration 9/25 | Loss: 0.00073336
Iteration 10/25 | Loss: 0.00073336
Iteration 11/25 | Loss: 0.00073336
Iteration 12/25 | Loss: 0.00073336
Iteration 13/25 | Loss: 0.00073336
Iteration 14/25 | Loss: 0.00073336
Iteration 15/25 | Loss: 0.00073336
Iteration 16/25 | Loss: 0.00073336
Iteration 17/25 | Loss: 0.00073336
Iteration 18/25 | Loss: 0.00073336
Iteration 19/25 | Loss: 0.00073336
Iteration 20/25 | Loss: 0.00073336
Iteration 21/25 | Loss: 0.00073336
Iteration 22/25 | Loss: 0.00073336
Iteration 23/25 | Loss: 0.00073336
Iteration 24/25 | Loss: 0.00073336
Iteration 25/25 | Loss: 0.00073336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073336
Iteration 2/1000 | Loss: 0.00003116
Iteration 3/1000 | Loss: 0.00002235
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001473
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001377
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001373
Iteration 77/1000 | Loss: 0.00001373
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001371
Iteration 82/1000 | Loss: 0.00001371
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001370
Iteration 86/1000 | Loss: 0.00001370
Iteration 87/1000 | Loss: 0.00001370
Iteration 88/1000 | Loss: 0.00001370
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001367
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001366
Iteration 106/1000 | Loss: 0.00001366
Iteration 107/1000 | Loss: 0.00001366
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001365
Iteration 113/1000 | Loss: 0.00001365
Iteration 114/1000 | Loss: 0.00001365
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00001365
Iteration 118/1000 | Loss: 0.00001365
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001363
Iteration 129/1000 | Loss: 0.00001363
Iteration 130/1000 | Loss: 0.00001363
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001360
Iteration 141/1000 | Loss: 0.00001360
Iteration 142/1000 | Loss: 0.00001360
Iteration 143/1000 | Loss: 0.00001360
Iteration 144/1000 | Loss: 0.00001360
Iteration 145/1000 | Loss: 0.00001360
Iteration 146/1000 | Loss: 0.00001360
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001358
Iteration 155/1000 | Loss: 0.00001358
Iteration 156/1000 | Loss: 0.00001358
Iteration 157/1000 | Loss: 0.00001358
Iteration 158/1000 | Loss: 0.00001358
Iteration 159/1000 | Loss: 0.00001358
Iteration 160/1000 | Loss: 0.00001358
Iteration 161/1000 | Loss: 0.00001358
Iteration 162/1000 | Loss: 0.00001358
Iteration 163/1000 | Loss: 0.00001358
Iteration 164/1000 | Loss: 0.00001358
Iteration 165/1000 | Loss: 0.00001358
Iteration 166/1000 | Loss: 0.00001358
Iteration 167/1000 | Loss: 0.00001358
Iteration 168/1000 | Loss: 0.00001358
Iteration 169/1000 | Loss: 0.00001357
Iteration 170/1000 | Loss: 0.00001357
Iteration 171/1000 | Loss: 0.00001357
Iteration 172/1000 | Loss: 0.00001357
Iteration 173/1000 | Loss: 0.00001357
Iteration 174/1000 | Loss: 0.00001357
Iteration 175/1000 | Loss: 0.00001357
Iteration 176/1000 | Loss: 0.00001357
Iteration 177/1000 | Loss: 0.00001357
Iteration 178/1000 | Loss: 0.00001357
Iteration 179/1000 | Loss: 0.00001357
Iteration 180/1000 | Loss: 0.00001357
Iteration 181/1000 | Loss: 0.00001357
Iteration 182/1000 | Loss: 0.00001357
Iteration 183/1000 | Loss: 0.00001357
Iteration 184/1000 | Loss: 0.00001357
Iteration 185/1000 | Loss: 0.00001357
Iteration 186/1000 | Loss: 0.00001357
Iteration 187/1000 | Loss: 0.00001357
Iteration 188/1000 | Loss: 0.00001357
Iteration 189/1000 | Loss: 0.00001357
Iteration 190/1000 | Loss: 0.00001357
Iteration 191/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.3572891475632787e-05, 1.3572891475632787e-05, 1.3572891475632787e-05, 1.3572891475632787e-05, 1.3572891475632787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3572891475632787e-05

Optimization complete. Final v2v error: 3.1586077213287354 mm

Highest mean error: 3.4848685264587402 mm for frame 75

Lowest mean error: 2.955476999282837 mm for frame 22

Saving results

Total time: 43.37437915802002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439188
Iteration 2/25 | Loss: 0.00141726
Iteration 3/25 | Loss: 0.00133413
Iteration 4/25 | Loss: 0.00132010
Iteration 5/25 | Loss: 0.00131554
Iteration 6/25 | Loss: 0.00131518
Iteration 7/25 | Loss: 0.00131518
Iteration 8/25 | Loss: 0.00131518
Iteration 9/25 | Loss: 0.00131518
Iteration 10/25 | Loss: 0.00131518
Iteration 11/25 | Loss: 0.00131518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001315183355472982, 0.001315183355472982, 0.001315183355472982, 0.001315183355472982, 0.001315183355472982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001315183355472982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98223197
Iteration 2/25 | Loss: 0.00081768
Iteration 3/25 | Loss: 0.00081768
Iteration 4/25 | Loss: 0.00081768
Iteration 5/25 | Loss: 0.00081768
Iteration 6/25 | Loss: 0.00081767
Iteration 7/25 | Loss: 0.00081767
Iteration 8/25 | Loss: 0.00081767
Iteration 9/25 | Loss: 0.00081767
Iteration 10/25 | Loss: 0.00081767
Iteration 11/25 | Loss: 0.00081767
Iteration 12/25 | Loss: 0.00081767
Iteration 13/25 | Loss: 0.00081767
Iteration 14/25 | Loss: 0.00081767
Iteration 15/25 | Loss: 0.00081767
Iteration 16/25 | Loss: 0.00081767
Iteration 17/25 | Loss: 0.00081767
Iteration 18/25 | Loss: 0.00081767
Iteration 19/25 | Loss: 0.00081767
Iteration 20/25 | Loss: 0.00081767
Iteration 21/25 | Loss: 0.00081767
Iteration 22/25 | Loss: 0.00081767
Iteration 23/25 | Loss: 0.00081767
Iteration 24/25 | Loss: 0.00081767
Iteration 25/25 | Loss: 0.00081767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081767
Iteration 2/1000 | Loss: 0.00003513
Iteration 3/1000 | Loss: 0.00002815
Iteration 4/1000 | Loss: 0.00002519
Iteration 5/1000 | Loss: 0.00002415
Iteration 6/1000 | Loss: 0.00002320
Iteration 7/1000 | Loss: 0.00002257
Iteration 8/1000 | Loss: 0.00002208
Iteration 9/1000 | Loss: 0.00002177
Iteration 10/1000 | Loss: 0.00002140
Iteration 11/1000 | Loss: 0.00002119
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00002098
Iteration 14/1000 | Loss: 0.00002079
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00002073
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002066
Iteration 19/1000 | Loss: 0.00002058
Iteration 20/1000 | Loss: 0.00002051
Iteration 21/1000 | Loss: 0.00002051
Iteration 22/1000 | Loss: 0.00002050
Iteration 23/1000 | Loss: 0.00002048
Iteration 24/1000 | Loss: 0.00002047
Iteration 25/1000 | Loss: 0.00002045
Iteration 26/1000 | Loss: 0.00002045
Iteration 27/1000 | Loss: 0.00002041
Iteration 28/1000 | Loss: 0.00002033
Iteration 29/1000 | Loss: 0.00002032
Iteration 30/1000 | Loss: 0.00002030
Iteration 31/1000 | Loss: 0.00002030
Iteration 32/1000 | Loss: 0.00002029
Iteration 33/1000 | Loss: 0.00002029
Iteration 34/1000 | Loss: 0.00002027
Iteration 35/1000 | Loss: 0.00002026
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002025
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002023
Iteration 41/1000 | Loss: 0.00002023
Iteration 42/1000 | Loss: 0.00002022
Iteration 43/1000 | Loss: 0.00002022
Iteration 44/1000 | Loss: 0.00002021
Iteration 45/1000 | Loss: 0.00002021
Iteration 46/1000 | Loss: 0.00002021
Iteration 47/1000 | Loss: 0.00002020
Iteration 48/1000 | Loss: 0.00002020
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002018
Iteration 52/1000 | Loss: 0.00002018
Iteration 53/1000 | Loss: 0.00002017
Iteration 54/1000 | Loss: 0.00002017
Iteration 55/1000 | Loss: 0.00002017
Iteration 56/1000 | Loss: 0.00002017
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00002016
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00002015
Iteration 61/1000 | Loss: 0.00002015
Iteration 62/1000 | Loss: 0.00002015
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002013
Iteration 65/1000 | Loss: 0.00002013
Iteration 66/1000 | Loss: 0.00002013
Iteration 67/1000 | Loss: 0.00002012
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002012
Iteration 71/1000 | Loss: 0.00002012
Iteration 72/1000 | Loss: 0.00002012
Iteration 73/1000 | Loss: 0.00002012
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002011
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00002010
Iteration 78/1000 | Loss: 0.00002010
Iteration 79/1000 | Loss: 0.00002010
Iteration 80/1000 | Loss: 0.00002010
Iteration 81/1000 | Loss: 0.00002010
Iteration 82/1000 | Loss: 0.00002010
Iteration 83/1000 | Loss: 0.00002010
Iteration 84/1000 | Loss: 0.00002009
Iteration 85/1000 | Loss: 0.00002009
Iteration 86/1000 | Loss: 0.00002009
Iteration 87/1000 | Loss: 0.00002009
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00002009
Iteration 90/1000 | Loss: 0.00002009
Iteration 91/1000 | Loss: 0.00002008
Iteration 92/1000 | Loss: 0.00002008
Iteration 93/1000 | Loss: 0.00002008
Iteration 94/1000 | Loss: 0.00002008
Iteration 95/1000 | Loss: 0.00002008
Iteration 96/1000 | Loss: 0.00002008
Iteration 97/1000 | Loss: 0.00002008
Iteration 98/1000 | Loss: 0.00002008
Iteration 99/1000 | Loss: 0.00002008
Iteration 100/1000 | Loss: 0.00002008
Iteration 101/1000 | Loss: 0.00002008
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00002007
Iteration 104/1000 | Loss: 0.00002007
Iteration 105/1000 | Loss: 0.00002007
Iteration 106/1000 | Loss: 0.00002007
Iteration 107/1000 | Loss: 0.00002007
Iteration 108/1000 | Loss: 0.00002007
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002006
Iteration 115/1000 | Loss: 0.00002006
Iteration 116/1000 | Loss: 0.00002006
Iteration 117/1000 | Loss: 0.00002006
Iteration 118/1000 | Loss: 0.00002006
Iteration 119/1000 | Loss: 0.00002005
Iteration 120/1000 | Loss: 0.00002005
Iteration 121/1000 | Loss: 0.00002005
Iteration 122/1000 | Loss: 0.00002005
Iteration 123/1000 | Loss: 0.00002005
Iteration 124/1000 | Loss: 0.00002005
Iteration 125/1000 | Loss: 0.00002005
Iteration 126/1000 | Loss: 0.00002005
Iteration 127/1000 | Loss: 0.00002004
Iteration 128/1000 | Loss: 0.00002004
Iteration 129/1000 | Loss: 0.00002004
Iteration 130/1000 | Loss: 0.00002004
Iteration 131/1000 | Loss: 0.00002004
Iteration 132/1000 | Loss: 0.00002003
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002003
Iteration 136/1000 | Loss: 0.00002003
Iteration 137/1000 | Loss: 0.00002003
Iteration 138/1000 | Loss: 0.00002003
Iteration 139/1000 | Loss: 0.00002003
Iteration 140/1000 | Loss: 0.00002003
Iteration 141/1000 | Loss: 0.00002003
Iteration 142/1000 | Loss: 0.00002003
Iteration 143/1000 | Loss: 0.00002003
Iteration 144/1000 | Loss: 0.00002003
Iteration 145/1000 | Loss: 0.00002003
Iteration 146/1000 | Loss: 0.00002003
Iteration 147/1000 | Loss: 0.00002003
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00002003
Iteration 150/1000 | Loss: 0.00002003
Iteration 151/1000 | Loss: 0.00002003
Iteration 152/1000 | Loss: 0.00002003
Iteration 153/1000 | Loss: 0.00002003
Iteration 154/1000 | Loss: 0.00002003
Iteration 155/1000 | Loss: 0.00002003
Iteration 156/1000 | Loss: 0.00002003
Iteration 157/1000 | Loss: 0.00002003
Iteration 158/1000 | Loss: 0.00002003
Iteration 159/1000 | Loss: 0.00002003
Iteration 160/1000 | Loss: 0.00002003
Iteration 161/1000 | Loss: 0.00002003
Iteration 162/1000 | Loss: 0.00002003
Iteration 163/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.0026440324727446e-05, 2.0026440324727446e-05, 2.0026440324727446e-05, 2.0026440324727446e-05, 2.0026440324727446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0026440324727446e-05

Optimization complete. Final v2v error: 3.79730224609375 mm

Highest mean error: 4.173120021820068 mm for frame 16

Lowest mean error: 3.508265495300293 mm for frame 44

Saving results

Total time: 41.02395582199097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404919
Iteration 2/25 | Loss: 0.00136684
Iteration 3/25 | Loss: 0.00130051
Iteration 4/25 | Loss: 0.00128411
Iteration 5/25 | Loss: 0.00127891
Iteration 6/25 | Loss: 0.00127836
Iteration 7/25 | Loss: 0.00127836
Iteration 8/25 | Loss: 0.00127836
Iteration 9/25 | Loss: 0.00127836
Iteration 10/25 | Loss: 0.00127836
Iteration 11/25 | Loss: 0.00127836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012783639831468463, 0.0012783639831468463, 0.0012783639831468463, 0.0012783639831468463, 0.0012783639831468463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012783639831468463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53980839
Iteration 2/25 | Loss: 0.00082485
Iteration 3/25 | Loss: 0.00082485
Iteration 4/25 | Loss: 0.00082485
Iteration 5/25 | Loss: 0.00082484
Iteration 6/25 | Loss: 0.00082484
Iteration 7/25 | Loss: 0.00082484
Iteration 8/25 | Loss: 0.00082484
Iteration 9/25 | Loss: 0.00082484
Iteration 10/25 | Loss: 0.00082484
Iteration 11/25 | Loss: 0.00082484
Iteration 12/25 | Loss: 0.00082484
Iteration 13/25 | Loss: 0.00082484
Iteration 14/25 | Loss: 0.00082484
Iteration 15/25 | Loss: 0.00082484
Iteration 16/25 | Loss: 0.00082484
Iteration 17/25 | Loss: 0.00082484
Iteration 18/25 | Loss: 0.00082484
Iteration 19/25 | Loss: 0.00082484
Iteration 20/25 | Loss: 0.00082484
Iteration 21/25 | Loss: 0.00082484
Iteration 22/25 | Loss: 0.00082484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008248435915447772, 0.0008248435915447772, 0.0008248435915447772, 0.0008248435915447772, 0.0008248435915447772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008248435915447772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082484
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001837
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001685
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001623
Iteration 8/1000 | Loss: 0.00001581
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001511
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001498
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001497
Iteration 21/1000 | Loss: 0.00001497
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001496
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001486
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001475
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001474
Iteration 57/1000 | Loss: 0.00001474
Iteration 58/1000 | Loss: 0.00001474
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001473
Iteration 63/1000 | Loss: 0.00001473
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001471
Iteration 68/1000 | Loss: 0.00001471
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001470
Iteration 71/1000 | Loss: 0.00001470
Iteration 72/1000 | Loss: 0.00001470
Iteration 73/1000 | Loss: 0.00001469
Iteration 74/1000 | Loss: 0.00001469
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001467
Iteration 81/1000 | Loss: 0.00001467
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001466
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001465
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001463
Iteration 97/1000 | Loss: 0.00001463
Iteration 98/1000 | Loss: 0.00001462
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001461
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001460
Iteration 107/1000 | Loss: 0.00001460
Iteration 108/1000 | Loss: 0.00001459
Iteration 109/1000 | Loss: 0.00001459
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001453
Iteration 136/1000 | Loss: 0.00001453
Iteration 137/1000 | Loss: 0.00001452
Iteration 138/1000 | Loss: 0.00001452
Iteration 139/1000 | Loss: 0.00001452
Iteration 140/1000 | Loss: 0.00001452
Iteration 141/1000 | Loss: 0.00001452
Iteration 142/1000 | Loss: 0.00001452
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001452
Iteration 145/1000 | Loss: 0.00001452
Iteration 146/1000 | Loss: 0.00001452
Iteration 147/1000 | Loss: 0.00001452
Iteration 148/1000 | Loss: 0.00001452
Iteration 149/1000 | Loss: 0.00001452
Iteration 150/1000 | Loss: 0.00001452
Iteration 151/1000 | Loss: 0.00001451
Iteration 152/1000 | Loss: 0.00001451
Iteration 153/1000 | Loss: 0.00001451
Iteration 154/1000 | Loss: 0.00001451
Iteration 155/1000 | Loss: 0.00001451
Iteration 156/1000 | Loss: 0.00001451
Iteration 157/1000 | Loss: 0.00001451
Iteration 158/1000 | Loss: 0.00001451
Iteration 159/1000 | Loss: 0.00001450
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001450
Iteration 163/1000 | Loss: 0.00001450
Iteration 164/1000 | Loss: 0.00001450
Iteration 165/1000 | Loss: 0.00001450
Iteration 166/1000 | Loss: 0.00001450
Iteration 167/1000 | Loss: 0.00001450
Iteration 168/1000 | Loss: 0.00001450
Iteration 169/1000 | Loss: 0.00001450
Iteration 170/1000 | Loss: 0.00001450
Iteration 171/1000 | Loss: 0.00001450
Iteration 172/1000 | Loss: 0.00001450
Iteration 173/1000 | Loss: 0.00001450
Iteration 174/1000 | Loss: 0.00001450
Iteration 175/1000 | Loss: 0.00001450
Iteration 176/1000 | Loss: 0.00001450
Iteration 177/1000 | Loss: 0.00001450
Iteration 178/1000 | Loss: 0.00001450
Iteration 179/1000 | Loss: 0.00001450
Iteration 180/1000 | Loss: 0.00001450
Iteration 181/1000 | Loss: 0.00001450
Iteration 182/1000 | Loss: 0.00001450
Iteration 183/1000 | Loss: 0.00001450
Iteration 184/1000 | Loss: 0.00001450
Iteration 185/1000 | Loss: 0.00001450
Iteration 186/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.450061517971335e-05, 1.450061517971335e-05, 1.450061517971335e-05, 1.450061517971335e-05, 1.450061517971335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.450061517971335e-05

Optimization complete. Final v2v error: 3.268065929412842 mm

Highest mean error: 3.652034282684326 mm for frame 187

Lowest mean error: 3.1175200939178467 mm for frame 210

Saving results

Total time: 42.839385986328125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389863
Iteration 2/25 | Loss: 0.00168376
Iteration 3/25 | Loss: 0.00135343
Iteration 4/25 | Loss: 0.00130399
Iteration 5/25 | Loss: 0.00129282
Iteration 6/25 | Loss: 0.00129049
Iteration 7/25 | Loss: 0.00128998
Iteration 8/25 | Loss: 0.00128998
Iteration 9/25 | Loss: 0.00128998
Iteration 10/25 | Loss: 0.00128998
Iteration 11/25 | Loss: 0.00128998
Iteration 12/25 | Loss: 0.00128998
Iteration 13/25 | Loss: 0.00128998
Iteration 14/25 | Loss: 0.00128998
Iteration 15/25 | Loss: 0.00128998
Iteration 16/25 | Loss: 0.00128998
Iteration 17/25 | Loss: 0.00128998
Iteration 18/25 | Loss: 0.00128998
Iteration 19/25 | Loss: 0.00128998
Iteration 20/25 | Loss: 0.00128998
Iteration 21/25 | Loss: 0.00128998
Iteration 22/25 | Loss: 0.00128998
Iteration 23/25 | Loss: 0.00128998
Iteration 24/25 | Loss: 0.00128998
Iteration 25/25 | Loss: 0.00128998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36437452
Iteration 2/25 | Loss: 0.00065748
Iteration 3/25 | Loss: 0.00065748
Iteration 4/25 | Loss: 0.00065748
Iteration 5/25 | Loss: 0.00065748
Iteration 6/25 | Loss: 0.00065748
Iteration 7/25 | Loss: 0.00065748
Iteration 8/25 | Loss: 0.00065748
Iteration 9/25 | Loss: 0.00065748
Iteration 10/25 | Loss: 0.00065748
Iteration 11/25 | Loss: 0.00065748
Iteration 12/25 | Loss: 0.00065748
Iteration 13/25 | Loss: 0.00065748
Iteration 14/25 | Loss: 0.00065748
Iteration 15/25 | Loss: 0.00065748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006574755534529686, 0.0006574755534529686, 0.0006574755534529686, 0.0006574755534529686, 0.0006574755534529686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006574755534529686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065748
Iteration 2/1000 | Loss: 0.00004877
Iteration 3/1000 | Loss: 0.00002812
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00001932
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001790
Iteration 9/1000 | Loss: 0.00001745
Iteration 10/1000 | Loss: 0.00001705
Iteration 11/1000 | Loss: 0.00001680
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001638
Iteration 15/1000 | Loss: 0.00001631
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001616
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001612
Iteration 30/1000 | Loss: 0.00001612
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001607
Iteration 53/1000 | Loss: 0.00001607
Iteration 54/1000 | Loss: 0.00001607
Iteration 55/1000 | Loss: 0.00001607
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00001607
Iteration 60/1000 | Loss: 0.00001607
Iteration 61/1000 | Loss: 0.00001607
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001607
Iteration 65/1000 | Loss: 0.00001607
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001606
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001606
Iteration 70/1000 | Loss: 0.00001606
Iteration 71/1000 | Loss: 0.00001606
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001604
Iteration 92/1000 | Loss: 0.00001604
Iteration 93/1000 | Loss: 0.00001604
Iteration 94/1000 | Loss: 0.00001604
Iteration 95/1000 | Loss: 0.00001604
Iteration 96/1000 | Loss: 0.00001604
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.6043757568695582e-05, 1.6043757568695582e-05, 1.6043757568695582e-05, 1.6043757568695582e-05, 1.6043757568695582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6043757568695582e-05

Optimization complete. Final v2v error: 3.413524866104126 mm

Highest mean error: 3.634420394897461 mm for frame 103

Lowest mean error: 3.2467386722564697 mm for frame 25

Saving results

Total time: 35.38344407081604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844408
Iteration 2/25 | Loss: 0.00200959
Iteration 3/25 | Loss: 0.00158211
Iteration 4/25 | Loss: 0.00150728
Iteration 5/25 | Loss: 0.00148799
Iteration 6/25 | Loss: 0.00151436
Iteration 7/25 | Loss: 0.00148144
Iteration 8/25 | Loss: 0.00146312
Iteration 9/25 | Loss: 0.00145277
Iteration 10/25 | Loss: 0.00146017
Iteration 11/25 | Loss: 0.00146404
Iteration 12/25 | Loss: 0.00145588
Iteration 13/25 | Loss: 0.00143588
Iteration 14/25 | Loss: 0.00143309
Iteration 15/25 | Loss: 0.00143207
Iteration 16/25 | Loss: 0.00143304
Iteration 17/25 | Loss: 0.00143673
Iteration 18/25 | Loss: 0.00143324
Iteration 19/25 | Loss: 0.00143646
Iteration 20/25 | Loss: 0.00142978
Iteration 21/25 | Loss: 0.00142992
Iteration 22/25 | Loss: 0.00142925
Iteration 23/25 | Loss: 0.00142919
Iteration 24/25 | Loss: 0.00142912
Iteration 25/25 | Loss: 0.00142911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45907927
Iteration 2/25 | Loss: 0.00075985
Iteration 3/25 | Loss: 0.00075985
Iteration 4/25 | Loss: 0.00075985
Iteration 5/25 | Loss: 0.00075985
Iteration 6/25 | Loss: 0.00075985
Iteration 7/25 | Loss: 0.00075985
Iteration 8/25 | Loss: 0.00075985
Iteration 9/25 | Loss: 0.00075985
Iteration 10/25 | Loss: 0.00075985
Iteration 11/25 | Loss: 0.00075985
Iteration 12/25 | Loss: 0.00075985
Iteration 13/25 | Loss: 0.00075985
Iteration 14/25 | Loss: 0.00075985
Iteration 15/25 | Loss: 0.00075985
Iteration 16/25 | Loss: 0.00075985
Iteration 17/25 | Loss: 0.00075985
Iteration 18/25 | Loss: 0.00075985
Iteration 19/25 | Loss: 0.00075985
Iteration 20/25 | Loss: 0.00075985
Iteration 21/25 | Loss: 0.00075985
Iteration 22/25 | Loss: 0.00075985
Iteration 23/25 | Loss: 0.00075985
Iteration 24/25 | Loss: 0.00075985
Iteration 25/25 | Loss: 0.00075985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075985
Iteration 2/1000 | Loss: 0.00004153
Iteration 3/1000 | Loss: 0.00003248
Iteration 4/1000 | Loss: 0.00003040
Iteration 5/1000 | Loss: 0.00002938
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002759
Iteration 9/1000 | Loss: 0.00002726
Iteration 10/1000 | Loss: 0.00002707
Iteration 11/1000 | Loss: 0.00002680
Iteration 12/1000 | Loss: 0.00002680
Iteration 13/1000 | Loss: 0.00035224
Iteration 14/1000 | Loss: 0.00002884
Iteration 15/1000 | Loss: 0.00002688
Iteration 16/1000 | Loss: 0.00002634
Iteration 17/1000 | Loss: 0.00002567
Iteration 18/1000 | Loss: 0.00002530
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002493
Iteration 21/1000 | Loss: 0.00002489
Iteration 22/1000 | Loss: 0.00002487
Iteration 23/1000 | Loss: 0.00002487
Iteration 24/1000 | Loss: 0.00002481
Iteration 25/1000 | Loss: 0.00002479
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002464
Iteration 28/1000 | Loss: 0.00002464
Iteration 29/1000 | Loss: 0.00002459
Iteration 30/1000 | Loss: 0.00002459
Iteration 31/1000 | Loss: 0.00002458
Iteration 32/1000 | Loss: 0.00002457
Iteration 33/1000 | Loss: 0.00002457
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002452
Iteration 36/1000 | Loss: 0.00002452
Iteration 37/1000 | Loss: 0.00002451
Iteration 38/1000 | Loss: 0.00002450
Iteration 39/1000 | Loss: 0.00002450
Iteration 40/1000 | Loss: 0.00002450
Iteration 41/1000 | Loss: 0.00002449
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002449
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002448
Iteration 47/1000 | Loss: 0.00002448
Iteration 48/1000 | Loss: 0.00002448
Iteration 49/1000 | Loss: 0.00002448
Iteration 50/1000 | Loss: 0.00002448
Iteration 51/1000 | Loss: 0.00002448
Iteration 52/1000 | Loss: 0.00002448
Iteration 53/1000 | Loss: 0.00002447
Iteration 54/1000 | Loss: 0.00002447
Iteration 55/1000 | Loss: 0.00002447
Iteration 56/1000 | Loss: 0.00002447
Iteration 57/1000 | Loss: 0.00002447
Iteration 58/1000 | Loss: 0.00002447
Iteration 59/1000 | Loss: 0.00002447
Iteration 60/1000 | Loss: 0.00002447
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00002446
Iteration 63/1000 | Loss: 0.00002446
Iteration 64/1000 | Loss: 0.00002445
Iteration 65/1000 | Loss: 0.00002445
Iteration 66/1000 | Loss: 0.00002445
Iteration 67/1000 | Loss: 0.00002444
Iteration 68/1000 | Loss: 0.00002444
Iteration 69/1000 | Loss: 0.00002444
Iteration 70/1000 | Loss: 0.00002443
Iteration 71/1000 | Loss: 0.00002443
Iteration 72/1000 | Loss: 0.00002443
Iteration 73/1000 | Loss: 0.00002443
Iteration 74/1000 | Loss: 0.00002443
Iteration 75/1000 | Loss: 0.00002443
Iteration 76/1000 | Loss: 0.00002442
Iteration 77/1000 | Loss: 0.00002442
Iteration 78/1000 | Loss: 0.00002442
Iteration 79/1000 | Loss: 0.00002442
Iteration 80/1000 | Loss: 0.00002441
Iteration 81/1000 | Loss: 0.00002440
Iteration 82/1000 | Loss: 0.00002440
Iteration 83/1000 | Loss: 0.00002440
Iteration 84/1000 | Loss: 0.00003414
Iteration 85/1000 | Loss: 0.00002996
Iteration 86/1000 | Loss: 0.00003299
Iteration 87/1000 | Loss: 0.00002862
Iteration 88/1000 | Loss: 0.00003052
Iteration 89/1000 | Loss: 0.00002762
Iteration 90/1000 | Loss: 0.00002479
Iteration 91/1000 | Loss: 0.00002452
Iteration 92/1000 | Loss: 0.00002452
Iteration 93/1000 | Loss: 0.00002447
Iteration 94/1000 | Loss: 0.00002446
Iteration 95/1000 | Loss: 0.00003140
Iteration 96/1000 | Loss: 0.00003700
Iteration 97/1000 | Loss: 0.00003292
Iteration 98/1000 | Loss: 0.00002482
Iteration 99/1000 | Loss: 0.00003009
Iteration 100/1000 | Loss: 0.00002495
Iteration 101/1000 | Loss: 0.00002986
Iteration 102/1000 | Loss: 0.00002437
Iteration 103/1000 | Loss: 0.00002436
Iteration 104/1000 | Loss: 0.00002435
Iteration 105/1000 | Loss: 0.00002434
Iteration 106/1000 | Loss: 0.00002433
Iteration 107/1000 | Loss: 0.00002433
Iteration 108/1000 | Loss: 0.00002433
Iteration 109/1000 | Loss: 0.00002433
Iteration 110/1000 | Loss: 0.00002433
Iteration 111/1000 | Loss: 0.00002432
Iteration 112/1000 | Loss: 0.00002432
Iteration 113/1000 | Loss: 0.00002431
Iteration 114/1000 | Loss: 0.00002431
Iteration 115/1000 | Loss: 0.00002430
Iteration 116/1000 | Loss: 0.00002430
Iteration 117/1000 | Loss: 0.00002430
Iteration 118/1000 | Loss: 0.00002429
Iteration 119/1000 | Loss: 0.00002429
Iteration 120/1000 | Loss: 0.00002428
Iteration 121/1000 | Loss: 0.00002428
Iteration 122/1000 | Loss: 0.00002427
Iteration 123/1000 | Loss: 0.00002426
Iteration 124/1000 | Loss: 0.00002425
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002424
Iteration 138/1000 | Loss: 0.00002424
Iteration 139/1000 | Loss: 0.00002424
Iteration 140/1000 | Loss: 0.00002424
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00002424
Iteration 143/1000 | Loss: 0.00002423
Iteration 144/1000 | Loss: 0.00002423
Iteration 145/1000 | Loss: 0.00002423
Iteration 146/1000 | Loss: 0.00002423
Iteration 147/1000 | Loss: 0.00002423
Iteration 148/1000 | Loss: 0.00002423
Iteration 149/1000 | Loss: 0.00002423
Iteration 150/1000 | Loss: 0.00002423
Iteration 151/1000 | Loss: 0.00002422
Iteration 152/1000 | Loss: 0.00002422
Iteration 153/1000 | Loss: 0.00002422
Iteration 154/1000 | Loss: 0.00002422
Iteration 155/1000 | Loss: 0.00002422
Iteration 156/1000 | Loss: 0.00002422
Iteration 157/1000 | Loss: 0.00002422
Iteration 158/1000 | Loss: 0.00002422
Iteration 159/1000 | Loss: 0.00002421
Iteration 160/1000 | Loss: 0.00002421
Iteration 161/1000 | Loss: 0.00002421
Iteration 162/1000 | Loss: 0.00002421
Iteration 163/1000 | Loss: 0.00002421
Iteration 164/1000 | Loss: 0.00002421
Iteration 165/1000 | Loss: 0.00002421
Iteration 166/1000 | Loss: 0.00002421
Iteration 167/1000 | Loss: 0.00002421
Iteration 168/1000 | Loss: 0.00002421
Iteration 169/1000 | Loss: 0.00002421
Iteration 170/1000 | Loss: 0.00002420
Iteration 171/1000 | Loss: 0.00002420
Iteration 172/1000 | Loss: 0.00002420
Iteration 173/1000 | Loss: 0.00002420
Iteration 174/1000 | Loss: 0.00002420
Iteration 175/1000 | Loss: 0.00002420
Iteration 176/1000 | Loss: 0.00002420
Iteration 177/1000 | Loss: 0.00002420
Iteration 178/1000 | Loss: 0.00002420
Iteration 179/1000 | Loss: 0.00002420
Iteration 180/1000 | Loss: 0.00002420
Iteration 181/1000 | Loss: 0.00002420
Iteration 182/1000 | Loss: 0.00002419
Iteration 183/1000 | Loss: 0.00002419
Iteration 184/1000 | Loss: 0.00002419
Iteration 185/1000 | Loss: 0.00002419
Iteration 186/1000 | Loss: 0.00002419
Iteration 187/1000 | Loss: 0.00002419
Iteration 188/1000 | Loss: 0.00002419
Iteration 189/1000 | Loss: 0.00002419
Iteration 190/1000 | Loss: 0.00002419
Iteration 191/1000 | Loss: 0.00002419
Iteration 192/1000 | Loss: 0.00002419
Iteration 193/1000 | Loss: 0.00002419
Iteration 194/1000 | Loss: 0.00002419
Iteration 195/1000 | Loss: 0.00002418
Iteration 196/1000 | Loss: 0.00002418
Iteration 197/1000 | Loss: 0.00002418
Iteration 198/1000 | Loss: 0.00002418
Iteration 199/1000 | Loss: 0.00002418
Iteration 200/1000 | Loss: 0.00002418
Iteration 201/1000 | Loss: 0.00002418
Iteration 202/1000 | Loss: 0.00002418
Iteration 203/1000 | Loss: 0.00002418
Iteration 204/1000 | Loss: 0.00002418
Iteration 205/1000 | Loss: 0.00002418
Iteration 206/1000 | Loss: 0.00002418
Iteration 207/1000 | Loss: 0.00002418
Iteration 208/1000 | Loss: 0.00002418
Iteration 209/1000 | Loss: 0.00002417
Iteration 210/1000 | Loss: 0.00002417
Iteration 211/1000 | Loss: 0.00002417
Iteration 212/1000 | Loss: 0.00002417
Iteration 213/1000 | Loss: 0.00002417
Iteration 214/1000 | Loss: 0.00002417
Iteration 215/1000 | Loss: 0.00002417
Iteration 216/1000 | Loss: 0.00002417
Iteration 217/1000 | Loss: 0.00002417
Iteration 218/1000 | Loss: 0.00002417
Iteration 219/1000 | Loss: 0.00002417
Iteration 220/1000 | Loss: 0.00002417
Iteration 221/1000 | Loss: 0.00002417
Iteration 222/1000 | Loss: 0.00002417
Iteration 223/1000 | Loss: 0.00002417
Iteration 224/1000 | Loss: 0.00002417
Iteration 225/1000 | Loss: 0.00002417
Iteration 226/1000 | Loss: 0.00002417
Iteration 227/1000 | Loss: 0.00002417
Iteration 228/1000 | Loss: 0.00002417
Iteration 229/1000 | Loss: 0.00002417
Iteration 230/1000 | Loss: 0.00002417
Iteration 231/1000 | Loss: 0.00002417
Iteration 232/1000 | Loss: 0.00002417
Iteration 233/1000 | Loss: 0.00002417
Iteration 234/1000 | Loss: 0.00002417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.4173405108740553e-05, 2.4173405108740553e-05, 2.4173405108740553e-05, 2.4173405108740553e-05, 2.4173405108740553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4173405108740553e-05

Optimization complete. Final v2v error: 4.022939682006836 mm

Highest mean error: 6.2974772453308105 mm for frame 12

Lowest mean error: 3.5689122676849365 mm for frame 202

Saving results

Total time: 118.34777164459229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431386
Iteration 2/25 | Loss: 0.00141237
Iteration 3/25 | Loss: 0.00130440
Iteration 4/25 | Loss: 0.00129764
Iteration 5/25 | Loss: 0.00129684
Iteration 6/25 | Loss: 0.00129684
Iteration 7/25 | Loss: 0.00129684
Iteration 8/25 | Loss: 0.00129684
Iteration 9/25 | Loss: 0.00129684
Iteration 10/25 | Loss: 0.00129684
Iteration 11/25 | Loss: 0.00129684
Iteration 12/25 | Loss: 0.00129684
Iteration 13/25 | Loss: 0.00129684
Iteration 14/25 | Loss: 0.00129684
Iteration 15/25 | Loss: 0.00129684
Iteration 16/25 | Loss: 0.00129684
Iteration 17/25 | Loss: 0.00129684
Iteration 18/25 | Loss: 0.00129684
Iteration 19/25 | Loss: 0.00129684
Iteration 20/25 | Loss: 0.00129684
Iteration 21/25 | Loss: 0.00129684
Iteration 22/25 | Loss: 0.00129684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012968442169949412, 0.0012968442169949412, 0.0012968442169949412, 0.0012968442169949412, 0.0012968442169949412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012968442169949412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20611525
Iteration 2/25 | Loss: 0.00058345
Iteration 3/25 | Loss: 0.00058345
Iteration 4/25 | Loss: 0.00058345
Iteration 5/25 | Loss: 0.00058344
Iteration 6/25 | Loss: 0.00058344
Iteration 7/25 | Loss: 0.00058344
Iteration 8/25 | Loss: 0.00058344
Iteration 9/25 | Loss: 0.00058344
Iteration 10/25 | Loss: 0.00058344
Iteration 11/25 | Loss: 0.00058344
Iteration 12/25 | Loss: 0.00058344
Iteration 13/25 | Loss: 0.00058344
Iteration 14/25 | Loss: 0.00058344
Iteration 15/25 | Loss: 0.00058344
Iteration 16/25 | Loss: 0.00058344
Iteration 17/25 | Loss: 0.00058344
Iteration 18/25 | Loss: 0.00058344
Iteration 19/25 | Loss: 0.00058344
Iteration 20/25 | Loss: 0.00058344
Iteration 21/25 | Loss: 0.00058344
Iteration 22/25 | Loss: 0.00058344
Iteration 23/25 | Loss: 0.00058344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000583443499635905, 0.000583443499635905, 0.000583443499635905, 0.000583443499635905, 0.000583443499635905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000583443499635905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058344
Iteration 2/1000 | Loss: 0.00003402
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001440
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001400
Iteration 16/1000 | Loss: 0.00001397
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001393
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001392
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001388
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001387
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001381
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001379
Iteration 98/1000 | Loss: 0.00001379
Iteration 99/1000 | Loss: 0.00001379
Iteration 100/1000 | Loss: 0.00001379
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001378
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001378
Iteration 116/1000 | Loss: 0.00001378
Iteration 117/1000 | Loss: 0.00001378
Iteration 118/1000 | Loss: 0.00001378
Iteration 119/1000 | Loss: 0.00001378
Iteration 120/1000 | Loss: 0.00001377
Iteration 121/1000 | Loss: 0.00001377
Iteration 122/1000 | Loss: 0.00001377
Iteration 123/1000 | Loss: 0.00001377
Iteration 124/1000 | Loss: 0.00001377
Iteration 125/1000 | Loss: 0.00001376
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001375
Iteration 130/1000 | Loss: 0.00001375
Iteration 131/1000 | Loss: 0.00001375
Iteration 132/1000 | Loss: 0.00001375
Iteration 133/1000 | Loss: 0.00001375
Iteration 134/1000 | Loss: 0.00001375
Iteration 135/1000 | Loss: 0.00001375
Iteration 136/1000 | Loss: 0.00001375
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Iteration 141/1000 | Loss: 0.00001374
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Iteration 152/1000 | Loss: 0.00001373
Iteration 153/1000 | Loss: 0.00001373
Iteration 154/1000 | Loss: 0.00001373
Iteration 155/1000 | Loss: 0.00001373
Iteration 156/1000 | Loss: 0.00001373
Iteration 157/1000 | Loss: 0.00001373
Iteration 158/1000 | Loss: 0.00001373
Iteration 159/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.3729752026847564e-05, 1.3729752026847564e-05, 1.3729752026847564e-05, 1.3729752026847564e-05, 1.3729752026847564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3729752026847564e-05

Optimization complete. Final v2v error: 3.195819616317749 mm

Highest mean error: 3.222891092300415 mm for frame 109

Lowest mean error: 3.1574172973632812 mm for frame 0

Saving results

Total time: 32.298802614212036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493696
Iteration 2/25 | Loss: 0.00148306
Iteration 3/25 | Loss: 0.00137223
Iteration 4/25 | Loss: 0.00136535
Iteration 5/25 | Loss: 0.00136384
Iteration 6/25 | Loss: 0.00136384
Iteration 7/25 | Loss: 0.00136384
Iteration 8/25 | Loss: 0.00136384
Iteration 9/25 | Loss: 0.00136384
Iteration 10/25 | Loss: 0.00136384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013638412347063422, 0.0013638412347063422, 0.0013638412347063422, 0.0013638412347063422, 0.0013638412347063422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013638412347063422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.77627373
Iteration 2/25 | Loss: 0.00079107
Iteration 3/25 | Loss: 0.00079107
Iteration 4/25 | Loss: 0.00079107
Iteration 5/25 | Loss: 0.00079107
Iteration 6/25 | Loss: 0.00079106
Iteration 7/25 | Loss: 0.00079106
Iteration 8/25 | Loss: 0.00079106
Iteration 9/25 | Loss: 0.00079106
Iteration 10/25 | Loss: 0.00079106
Iteration 11/25 | Loss: 0.00079106
Iteration 12/25 | Loss: 0.00079106
Iteration 13/25 | Loss: 0.00079106
Iteration 14/25 | Loss: 0.00079106
Iteration 15/25 | Loss: 0.00079106
Iteration 16/25 | Loss: 0.00079106
Iteration 17/25 | Loss: 0.00079106
Iteration 18/25 | Loss: 0.00079106
Iteration 19/25 | Loss: 0.00079106
Iteration 20/25 | Loss: 0.00079106
Iteration 21/25 | Loss: 0.00079106
Iteration 22/25 | Loss: 0.00079106
Iteration 23/25 | Loss: 0.00079106
Iteration 24/25 | Loss: 0.00079106
Iteration 25/25 | Loss: 0.00079106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000791063706856221, 0.000791063706856221, 0.000791063706856221, 0.000791063706856221, 0.000791063706856221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000791063706856221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079106
Iteration 2/1000 | Loss: 0.00003500
Iteration 3/1000 | Loss: 0.00002468
Iteration 4/1000 | Loss: 0.00002271
Iteration 5/1000 | Loss: 0.00002198
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002093
Iteration 8/1000 | Loss: 0.00002052
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001998
Iteration 11/1000 | Loss: 0.00001977
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001941
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001913
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001910
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001907
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001906
Iteration 47/1000 | Loss: 0.00001906
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001905
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001904
Iteration 52/1000 | Loss: 0.00001904
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001903
Iteration 65/1000 | Loss: 0.00001903
Iteration 66/1000 | Loss: 0.00001903
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001902
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001902
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001901
Iteration 84/1000 | Loss: 0.00001901
Iteration 85/1000 | Loss: 0.00001901
Iteration 86/1000 | Loss: 0.00001901
Iteration 87/1000 | Loss: 0.00001901
Iteration 88/1000 | Loss: 0.00001901
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001900
Iteration 92/1000 | Loss: 0.00001900
Iteration 93/1000 | Loss: 0.00001900
Iteration 94/1000 | Loss: 0.00001900
Iteration 95/1000 | Loss: 0.00001900
Iteration 96/1000 | Loss: 0.00001900
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001899
Iteration 103/1000 | Loss: 0.00001899
Iteration 104/1000 | Loss: 0.00001899
Iteration 105/1000 | Loss: 0.00001899
Iteration 106/1000 | Loss: 0.00001899
Iteration 107/1000 | Loss: 0.00001899
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001899
Iteration 111/1000 | Loss: 0.00001899
Iteration 112/1000 | Loss: 0.00001899
Iteration 113/1000 | Loss: 0.00001899
Iteration 114/1000 | Loss: 0.00001899
Iteration 115/1000 | Loss: 0.00001898
Iteration 116/1000 | Loss: 0.00001898
Iteration 117/1000 | Loss: 0.00001898
Iteration 118/1000 | Loss: 0.00001898
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001898
Iteration 122/1000 | Loss: 0.00001898
Iteration 123/1000 | Loss: 0.00001898
Iteration 124/1000 | Loss: 0.00001898
Iteration 125/1000 | Loss: 0.00001898
Iteration 126/1000 | Loss: 0.00001898
Iteration 127/1000 | Loss: 0.00001898
Iteration 128/1000 | Loss: 0.00001898
Iteration 129/1000 | Loss: 0.00001898
Iteration 130/1000 | Loss: 0.00001898
Iteration 131/1000 | Loss: 0.00001898
Iteration 132/1000 | Loss: 0.00001898
Iteration 133/1000 | Loss: 0.00001898
Iteration 134/1000 | Loss: 0.00001898
Iteration 135/1000 | Loss: 0.00001898
Iteration 136/1000 | Loss: 0.00001898
Iteration 137/1000 | Loss: 0.00001898
Iteration 138/1000 | Loss: 0.00001898
Iteration 139/1000 | Loss: 0.00001898
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00001898
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00001898
Iteration 144/1000 | Loss: 0.00001898
Iteration 145/1000 | Loss: 0.00001898
Iteration 146/1000 | Loss: 0.00001898
Iteration 147/1000 | Loss: 0.00001898
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.898195841931738e-05, 1.898195841931738e-05, 1.898195841931738e-05, 1.898195841931738e-05, 1.898195841931738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.898195841931738e-05

Optimization complete. Final v2v error: 3.6408438682556152 mm

Highest mean error: 4.128015041351318 mm for frame 39

Lowest mean error: 3.2800848484039307 mm for frame 0

Saving results

Total time: 35.688289642333984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954026
Iteration 2/25 | Loss: 0.00182412
Iteration 3/25 | Loss: 0.00150831
Iteration 4/25 | Loss: 0.00148436
Iteration 5/25 | Loss: 0.00147622
Iteration 6/25 | Loss: 0.00147568
Iteration 7/25 | Loss: 0.00147568
Iteration 8/25 | Loss: 0.00147568
Iteration 9/25 | Loss: 0.00147568
Iteration 10/25 | Loss: 0.00147568
Iteration 11/25 | Loss: 0.00147568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014756846940144897, 0.0014756846940144897, 0.0014756846940144897, 0.0014756846940144897, 0.0014756846940144897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014756846940144897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84683961
Iteration 2/25 | Loss: 0.00104022
Iteration 3/25 | Loss: 0.00104021
Iteration 4/25 | Loss: 0.00104021
Iteration 5/25 | Loss: 0.00104021
Iteration 6/25 | Loss: 0.00104021
Iteration 7/25 | Loss: 0.00104021
Iteration 8/25 | Loss: 0.00104021
Iteration 9/25 | Loss: 0.00104021
Iteration 10/25 | Loss: 0.00104021
Iteration 11/25 | Loss: 0.00104021
Iteration 12/25 | Loss: 0.00104021
Iteration 13/25 | Loss: 0.00104021
Iteration 14/25 | Loss: 0.00104021
Iteration 15/25 | Loss: 0.00104021
Iteration 16/25 | Loss: 0.00104021
Iteration 17/25 | Loss: 0.00104021
Iteration 18/25 | Loss: 0.00104021
Iteration 19/25 | Loss: 0.00104021
Iteration 20/25 | Loss: 0.00104021
Iteration 21/25 | Loss: 0.00104021
Iteration 22/25 | Loss: 0.00104021
Iteration 23/25 | Loss: 0.00104021
Iteration 24/25 | Loss: 0.00104021
Iteration 25/25 | Loss: 0.00104021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104021
Iteration 2/1000 | Loss: 0.00006680
Iteration 3/1000 | Loss: 0.00005077
Iteration 4/1000 | Loss: 0.00004673
Iteration 5/1000 | Loss: 0.00004422
Iteration 6/1000 | Loss: 0.00004278
Iteration 7/1000 | Loss: 0.00004161
Iteration 8/1000 | Loss: 0.00004089
Iteration 9/1000 | Loss: 0.00004029
Iteration 10/1000 | Loss: 0.00003985
Iteration 11/1000 | Loss: 0.00003938
Iteration 12/1000 | Loss: 0.00003888
Iteration 13/1000 | Loss: 0.00003845
Iteration 14/1000 | Loss: 0.00003812
Iteration 15/1000 | Loss: 0.00003773
Iteration 16/1000 | Loss: 0.00003743
Iteration 17/1000 | Loss: 0.00003713
Iteration 18/1000 | Loss: 0.00003691
Iteration 19/1000 | Loss: 0.00003674
Iteration 20/1000 | Loss: 0.00003659
Iteration 21/1000 | Loss: 0.00003649
Iteration 22/1000 | Loss: 0.00003646
Iteration 23/1000 | Loss: 0.00003642
Iteration 24/1000 | Loss: 0.00003642
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003640
Iteration 27/1000 | Loss: 0.00003639
Iteration 28/1000 | Loss: 0.00003639
Iteration 29/1000 | Loss: 0.00003639
Iteration 30/1000 | Loss: 0.00003639
Iteration 31/1000 | Loss: 0.00003639
Iteration 32/1000 | Loss: 0.00003638
Iteration 33/1000 | Loss: 0.00003638
Iteration 34/1000 | Loss: 0.00003638
Iteration 35/1000 | Loss: 0.00003638
Iteration 36/1000 | Loss: 0.00003637
Iteration 37/1000 | Loss: 0.00003637
Iteration 38/1000 | Loss: 0.00003637
Iteration 39/1000 | Loss: 0.00003637
Iteration 40/1000 | Loss: 0.00003637
Iteration 41/1000 | Loss: 0.00003637
Iteration 42/1000 | Loss: 0.00003637
Iteration 43/1000 | Loss: 0.00003637
Iteration 44/1000 | Loss: 0.00003637
Iteration 45/1000 | Loss: 0.00003636
Iteration 46/1000 | Loss: 0.00003636
Iteration 47/1000 | Loss: 0.00003635
Iteration 48/1000 | Loss: 0.00003635
Iteration 49/1000 | Loss: 0.00003633
Iteration 50/1000 | Loss: 0.00003633
Iteration 51/1000 | Loss: 0.00003632
Iteration 52/1000 | Loss: 0.00003631
Iteration 53/1000 | Loss: 0.00003631
Iteration 54/1000 | Loss: 0.00003630
Iteration 55/1000 | Loss: 0.00003630
Iteration 56/1000 | Loss: 0.00003629
Iteration 57/1000 | Loss: 0.00003629
Iteration 58/1000 | Loss: 0.00003628
Iteration 59/1000 | Loss: 0.00003628
Iteration 60/1000 | Loss: 0.00003628
Iteration 61/1000 | Loss: 0.00003627
Iteration 62/1000 | Loss: 0.00003627
Iteration 63/1000 | Loss: 0.00003627
Iteration 64/1000 | Loss: 0.00003627
Iteration 65/1000 | Loss: 0.00003626
Iteration 66/1000 | Loss: 0.00003625
Iteration 67/1000 | Loss: 0.00003625
Iteration 68/1000 | Loss: 0.00003625
Iteration 69/1000 | Loss: 0.00003625
Iteration 70/1000 | Loss: 0.00003625
Iteration 71/1000 | Loss: 0.00003625
Iteration 72/1000 | Loss: 0.00003625
Iteration 73/1000 | Loss: 0.00003625
Iteration 74/1000 | Loss: 0.00003624
Iteration 75/1000 | Loss: 0.00003624
Iteration 76/1000 | Loss: 0.00003623
Iteration 77/1000 | Loss: 0.00003623
Iteration 78/1000 | Loss: 0.00003623
Iteration 79/1000 | Loss: 0.00003623
Iteration 80/1000 | Loss: 0.00003623
Iteration 81/1000 | Loss: 0.00003623
Iteration 82/1000 | Loss: 0.00003623
Iteration 83/1000 | Loss: 0.00003623
Iteration 84/1000 | Loss: 0.00003622
Iteration 85/1000 | Loss: 0.00003621
Iteration 86/1000 | Loss: 0.00003621
Iteration 87/1000 | Loss: 0.00003621
Iteration 88/1000 | Loss: 0.00003621
Iteration 89/1000 | Loss: 0.00003620
Iteration 90/1000 | Loss: 0.00003620
Iteration 91/1000 | Loss: 0.00003620
Iteration 92/1000 | Loss: 0.00003619
Iteration 93/1000 | Loss: 0.00003619
Iteration 94/1000 | Loss: 0.00003618
Iteration 95/1000 | Loss: 0.00003618
Iteration 96/1000 | Loss: 0.00003618
Iteration 97/1000 | Loss: 0.00003618
Iteration 98/1000 | Loss: 0.00003618
Iteration 99/1000 | Loss: 0.00003617
Iteration 100/1000 | Loss: 0.00003617
Iteration 101/1000 | Loss: 0.00003617
Iteration 102/1000 | Loss: 0.00003617
Iteration 103/1000 | Loss: 0.00003617
Iteration 104/1000 | Loss: 0.00003617
Iteration 105/1000 | Loss: 0.00003617
Iteration 106/1000 | Loss: 0.00003617
Iteration 107/1000 | Loss: 0.00003617
Iteration 108/1000 | Loss: 0.00003617
Iteration 109/1000 | Loss: 0.00003617
Iteration 110/1000 | Loss: 0.00003617
Iteration 111/1000 | Loss: 0.00003617
Iteration 112/1000 | Loss: 0.00003617
Iteration 113/1000 | Loss: 0.00003617
Iteration 114/1000 | Loss: 0.00003617
Iteration 115/1000 | Loss: 0.00003617
Iteration 116/1000 | Loss: 0.00003617
Iteration 117/1000 | Loss: 0.00003617
Iteration 118/1000 | Loss: 0.00003617
Iteration 119/1000 | Loss: 0.00003617
Iteration 120/1000 | Loss: 0.00003617
Iteration 121/1000 | Loss: 0.00003617
Iteration 122/1000 | Loss: 0.00003617
Iteration 123/1000 | Loss: 0.00003617
Iteration 124/1000 | Loss: 0.00003617
Iteration 125/1000 | Loss: 0.00003617
Iteration 126/1000 | Loss: 0.00003617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [3.617125184973702e-05, 3.617125184973702e-05, 3.617125184973702e-05, 3.617125184973702e-05, 3.617125184973702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.617125184973702e-05

Optimization complete. Final v2v error: 5.063619136810303 mm

Highest mean error: 5.5640645027160645 mm for frame 96

Lowest mean error: 4.212771415710449 mm for frame 48

Saving results

Total time: 52.08395266532898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440988
Iteration 2/25 | Loss: 0.00150904
Iteration 3/25 | Loss: 0.00134025
Iteration 4/25 | Loss: 0.00133023
Iteration 5/25 | Loss: 0.00132947
Iteration 6/25 | Loss: 0.00132947
Iteration 7/25 | Loss: 0.00132947
Iteration 8/25 | Loss: 0.00132947
Iteration 9/25 | Loss: 0.00132947
Iteration 10/25 | Loss: 0.00132947
Iteration 11/25 | Loss: 0.00132947
Iteration 12/25 | Loss: 0.00132947
Iteration 13/25 | Loss: 0.00132947
Iteration 14/25 | Loss: 0.00132947
Iteration 15/25 | Loss: 0.00132947
Iteration 16/25 | Loss: 0.00132947
Iteration 17/25 | Loss: 0.00132947
Iteration 18/25 | Loss: 0.00132947
Iteration 19/25 | Loss: 0.00132947
Iteration 20/25 | Loss: 0.00132947
Iteration 21/25 | Loss: 0.00132947
Iteration 22/25 | Loss: 0.00132947
Iteration 23/25 | Loss: 0.00132947
Iteration 24/25 | Loss: 0.00132947
Iteration 25/25 | Loss: 0.00132947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35112667
Iteration 2/25 | Loss: 0.00070188
Iteration 3/25 | Loss: 0.00070186
Iteration 4/25 | Loss: 0.00070186
Iteration 5/25 | Loss: 0.00070186
Iteration 6/25 | Loss: 0.00070186
Iteration 7/25 | Loss: 0.00070186
Iteration 8/25 | Loss: 0.00070186
Iteration 9/25 | Loss: 0.00070186
Iteration 10/25 | Loss: 0.00070186
Iteration 11/25 | Loss: 0.00070186
Iteration 12/25 | Loss: 0.00070186
Iteration 13/25 | Loss: 0.00070186
Iteration 14/25 | Loss: 0.00070186
Iteration 15/25 | Loss: 0.00070186
Iteration 16/25 | Loss: 0.00070186
Iteration 17/25 | Loss: 0.00070186
Iteration 18/25 | Loss: 0.00070186
Iteration 19/25 | Loss: 0.00070186
Iteration 20/25 | Loss: 0.00070186
Iteration 21/25 | Loss: 0.00070186
Iteration 22/25 | Loss: 0.00070186
Iteration 23/25 | Loss: 0.00070186
Iteration 24/25 | Loss: 0.00070186
Iteration 25/25 | Loss: 0.00070186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007018559263087809, 0.0007018559263087809, 0.0007018559263087809, 0.0007018559263087809, 0.0007018559263087809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007018559263087809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070186
Iteration 2/1000 | Loss: 0.00003237
Iteration 3/1000 | Loss: 0.00002409
Iteration 4/1000 | Loss: 0.00002209
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001848
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001818
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001802
Iteration 30/1000 | Loss: 0.00001802
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001793
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001776
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001775
Iteration 91/1000 | Loss: 0.00001775
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001773
Iteration 107/1000 | Loss: 0.00001773
Iteration 108/1000 | Loss: 0.00001773
Iteration 109/1000 | Loss: 0.00001773
Iteration 110/1000 | Loss: 0.00001773
Iteration 111/1000 | Loss: 0.00001773
Iteration 112/1000 | Loss: 0.00001773
Iteration 113/1000 | Loss: 0.00001773
Iteration 114/1000 | Loss: 0.00001773
Iteration 115/1000 | Loss: 0.00001772
Iteration 116/1000 | Loss: 0.00001772
Iteration 117/1000 | Loss: 0.00001772
Iteration 118/1000 | Loss: 0.00001772
Iteration 119/1000 | Loss: 0.00001772
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001772
Iteration 123/1000 | Loss: 0.00001772
Iteration 124/1000 | Loss: 0.00001772
Iteration 125/1000 | Loss: 0.00001772
Iteration 126/1000 | Loss: 0.00001772
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001771
Iteration 129/1000 | Loss: 0.00001771
Iteration 130/1000 | Loss: 0.00001771
Iteration 131/1000 | Loss: 0.00001771
Iteration 132/1000 | Loss: 0.00001771
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00001771
Iteration 135/1000 | Loss: 0.00001771
Iteration 136/1000 | Loss: 0.00001771
Iteration 137/1000 | Loss: 0.00001771
Iteration 138/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.7712238332023844e-05, 1.7712238332023844e-05, 1.7712238332023844e-05, 1.7712238332023844e-05, 1.7712238332023844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7712238332023844e-05

Optimization complete. Final v2v error: 3.5615196228027344 mm

Highest mean error: 3.9431378841400146 mm for frame 102

Lowest mean error: 3.276268243789673 mm for frame 130

Saving results

Total time: 38.01838278770447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499821
Iteration 2/25 | Loss: 0.00155955
Iteration 3/25 | Loss: 0.00138315
Iteration 4/25 | Loss: 0.00136389
Iteration 5/25 | Loss: 0.00135794
Iteration 6/25 | Loss: 0.00135607
Iteration 7/25 | Loss: 0.00135538
Iteration 8/25 | Loss: 0.00135514
Iteration 9/25 | Loss: 0.00135514
Iteration 10/25 | Loss: 0.00135514
Iteration 11/25 | Loss: 0.00135514
Iteration 12/25 | Loss: 0.00135514
Iteration 13/25 | Loss: 0.00135514
Iteration 14/25 | Loss: 0.00135514
Iteration 15/25 | Loss: 0.00135514
Iteration 16/25 | Loss: 0.00135514
Iteration 17/25 | Loss: 0.00135514
Iteration 18/25 | Loss: 0.00135514
Iteration 19/25 | Loss: 0.00135514
Iteration 20/25 | Loss: 0.00135514
Iteration 21/25 | Loss: 0.00135514
Iteration 22/25 | Loss: 0.00135514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013551359297707677, 0.0013551359297707677, 0.0013551359297707677, 0.0013551359297707677, 0.0013551359297707677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013551359297707677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40614271
Iteration 2/25 | Loss: 0.00106245
Iteration 3/25 | Loss: 0.00106244
Iteration 4/25 | Loss: 0.00106244
Iteration 5/25 | Loss: 0.00106244
Iteration 6/25 | Loss: 0.00106244
Iteration 7/25 | Loss: 0.00106244
Iteration 8/25 | Loss: 0.00106244
Iteration 9/25 | Loss: 0.00106244
Iteration 10/25 | Loss: 0.00106244
Iteration 11/25 | Loss: 0.00106244
Iteration 12/25 | Loss: 0.00106244
Iteration 13/25 | Loss: 0.00106244
Iteration 14/25 | Loss: 0.00106244
Iteration 15/25 | Loss: 0.00106244
Iteration 16/25 | Loss: 0.00106244
Iteration 17/25 | Loss: 0.00106244
Iteration 18/25 | Loss: 0.00106244
Iteration 19/25 | Loss: 0.00106244
Iteration 20/25 | Loss: 0.00106244
Iteration 21/25 | Loss: 0.00106244
Iteration 22/25 | Loss: 0.00106244
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010624377755448222, 0.0010624377755448222, 0.0010624377755448222, 0.0010624377755448222, 0.0010624377755448222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010624377755448222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106244
Iteration 2/1000 | Loss: 0.00007034
Iteration 3/1000 | Loss: 0.00004561
Iteration 4/1000 | Loss: 0.00003678
Iteration 5/1000 | Loss: 0.00003437
Iteration 6/1000 | Loss: 0.00003293
Iteration 7/1000 | Loss: 0.00003180
Iteration 8/1000 | Loss: 0.00003097
Iteration 9/1000 | Loss: 0.00003021
Iteration 10/1000 | Loss: 0.00002952
Iteration 11/1000 | Loss: 0.00002913
Iteration 12/1000 | Loss: 0.00002878
Iteration 13/1000 | Loss: 0.00002839
Iteration 14/1000 | Loss: 0.00002814
Iteration 15/1000 | Loss: 0.00002794
Iteration 16/1000 | Loss: 0.00002776
Iteration 17/1000 | Loss: 0.00002761
Iteration 18/1000 | Loss: 0.00002761
Iteration 19/1000 | Loss: 0.00002759
Iteration 20/1000 | Loss: 0.00002758
Iteration 21/1000 | Loss: 0.00002756
Iteration 22/1000 | Loss: 0.00002755
Iteration 23/1000 | Loss: 0.00002755
Iteration 24/1000 | Loss: 0.00002754
Iteration 25/1000 | Loss: 0.00002747
Iteration 26/1000 | Loss: 0.00002747
Iteration 27/1000 | Loss: 0.00002745
Iteration 28/1000 | Loss: 0.00002745
Iteration 29/1000 | Loss: 0.00002743
Iteration 30/1000 | Loss: 0.00002742
Iteration 31/1000 | Loss: 0.00002742
Iteration 32/1000 | Loss: 0.00002742
Iteration 33/1000 | Loss: 0.00002742
Iteration 34/1000 | Loss: 0.00002742
Iteration 35/1000 | Loss: 0.00002742
Iteration 36/1000 | Loss: 0.00002741
Iteration 37/1000 | Loss: 0.00002741
Iteration 38/1000 | Loss: 0.00002741
Iteration 39/1000 | Loss: 0.00002741
Iteration 40/1000 | Loss: 0.00002741
Iteration 41/1000 | Loss: 0.00002741
Iteration 42/1000 | Loss: 0.00002741
Iteration 43/1000 | Loss: 0.00002740
Iteration 44/1000 | Loss: 0.00002739
Iteration 45/1000 | Loss: 0.00002739
Iteration 46/1000 | Loss: 0.00002737
Iteration 47/1000 | Loss: 0.00002736
Iteration 48/1000 | Loss: 0.00002736
Iteration 49/1000 | Loss: 0.00002736
Iteration 50/1000 | Loss: 0.00002735
Iteration 51/1000 | Loss: 0.00002734
Iteration 52/1000 | Loss: 0.00002734
Iteration 53/1000 | Loss: 0.00002734
Iteration 54/1000 | Loss: 0.00002734
Iteration 55/1000 | Loss: 0.00002734
Iteration 56/1000 | Loss: 0.00002734
Iteration 57/1000 | Loss: 0.00002734
Iteration 58/1000 | Loss: 0.00002734
Iteration 59/1000 | Loss: 0.00002734
Iteration 60/1000 | Loss: 0.00002734
Iteration 61/1000 | Loss: 0.00002733
Iteration 62/1000 | Loss: 0.00002733
Iteration 63/1000 | Loss: 0.00002733
Iteration 64/1000 | Loss: 0.00002732
Iteration 65/1000 | Loss: 0.00002732
Iteration 66/1000 | Loss: 0.00002732
Iteration 67/1000 | Loss: 0.00002732
Iteration 68/1000 | Loss: 0.00002732
Iteration 69/1000 | Loss: 0.00002731
Iteration 70/1000 | Loss: 0.00002731
Iteration 71/1000 | Loss: 0.00002731
Iteration 72/1000 | Loss: 0.00002731
Iteration 73/1000 | Loss: 0.00002731
Iteration 74/1000 | Loss: 0.00002731
Iteration 75/1000 | Loss: 0.00002731
Iteration 76/1000 | Loss: 0.00002731
Iteration 77/1000 | Loss: 0.00002731
Iteration 78/1000 | Loss: 0.00002731
Iteration 79/1000 | Loss: 0.00002731
Iteration 80/1000 | Loss: 0.00002731
Iteration 81/1000 | Loss: 0.00002730
Iteration 82/1000 | Loss: 0.00002729
Iteration 83/1000 | Loss: 0.00002729
Iteration 84/1000 | Loss: 0.00002729
Iteration 85/1000 | Loss: 0.00002729
Iteration 86/1000 | Loss: 0.00002728
Iteration 87/1000 | Loss: 0.00002728
Iteration 88/1000 | Loss: 0.00002728
Iteration 89/1000 | Loss: 0.00002728
Iteration 90/1000 | Loss: 0.00002728
Iteration 91/1000 | Loss: 0.00002727
Iteration 92/1000 | Loss: 0.00002727
Iteration 93/1000 | Loss: 0.00002727
Iteration 94/1000 | Loss: 0.00002727
Iteration 95/1000 | Loss: 0.00002727
Iteration 96/1000 | Loss: 0.00002726
Iteration 97/1000 | Loss: 0.00002726
Iteration 98/1000 | Loss: 0.00002726
Iteration 99/1000 | Loss: 0.00002725
Iteration 100/1000 | Loss: 0.00002725
Iteration 101/1000 | Loss: 0.00002725
Iteration 102/1000 | Loss: 0.00002725
Iteration 103/1000 | Loss: 0.00002725
Iteration 104/1000 | Loss: 0.00002725
Iteration 105/1000 | Loss: 0.00002725
Iteration 106/1000 | Loss: 0.00002725
Iteration 107/1000 | Loss: 0.00002724
Iteration 108/1000 | Loss: 0.00002724
Iteration 109/1000 | Loss: 0.00002724
Iteration 110/1000 | Loss: 0.00002724
Iteration 111/1000 | Loss: 0.00002724
Iteration 112/1000 | Loss: 0.00002724
Iteration 113/1000 | Loss: 0.00002724
Iteration 114/1000 | Loss: 0.00002724
Iteration 115/1000 | Loss: 0.00002724
Iteration 116/1000 | Loss: 0.00002723
Iteration 117/1000 | Loss: 0.00002723
Iteration 118/1000 | Loss: 0.00002723
Iteration 119/1000 | Loss: 0.00002723
Iteration 120/1000 | Loss: 0.00002723
Iteration 121/1000 | Loss: 0.00002722
Iteration 122/1000 | Loss: 0.00002722
Iteration 123/1000 | Loss: 0.00002722
Iteration 124/1000 | Loss: 0.00002722
Iteration 125/1000 | Loss: 0.00002721
Iteration 126/1000 | Loss: 0.00002721
Iteration 127/1000 | Loss: 0.00002721
Iteration 128/1000 | Loss: 0.00002721
Iteration 129/1000 | Loss: 0.00002721
Iteration 130/1000 | Loss: 0.00002721
Iteration 131/1000 | Loss: 0.00002720
Iteration 132/1000 | Loss: 0.00002720
Iteration 133/1000 | Loss: 0.00002720
Iteration 134/1000 | Loss: 0.00002720
Iteration 135/1000 | Loss: 0.00002719
Iteration 136/1000 | Loss: 0.00002719
Iteration 137/1000 | Loss: 0.00002719
Iteration 138/1000 | Loss: 0.00002719
Iteration 139/1000 | Loss: 0.00002719
Iteration 140/1000 | Loss: 0.00002719
Iteration 141/1000 | Loss: 0.00002719
Iteration 142/1000 | Loss: 0.00002719
Iteration 143/1000 | Loss: 0.00002718
Iteration 144/1000 | Loss: 0.00002718
Iteration 145/1000 | Loss: 0.00002718
Iteration 146/1000 | Loss: 0.00002718
Iteration 147/1000 | Loss: 0.00002718
Iteration 148/1000 | Loss: 0.00002718
Iteration 149/1000 | Loss: 0.00002717
Iteration 150/1000 | Loss: 0.00002717
Iteration 151/1000 | Loss: 0.00002717
Iteration 152/1000 | Loss: 0.00002717
Iteration 153/1000 | Loss: 0.00002717
Iteration 154/1000 | Loss: 0.00002717
Iteration 155/1000 | Loss: 0.00002717
Iteration 156/1000 | Loss: 0.00002717
Iteration 157/1000 | Loss: 0.00002717
Iteration 158/1000 | Loss: 0.00002717
Iteration 159/1000 | Loss: 0.00002717
Iteration 160/1000 | Loss: 0.00002716
Iteration 161/1000 | Loss: 0.00002716
Iteration 162/1000 | Loss: 0.00002716
Iteration 163/1000 | Loss: 0.00002716
Iteration 164/1000 | Loss: 0.00002715
Iteration 165/1000 | Loss: 0.00002715
Iteration 166/1000 | Loss: 0.00002715
Iteration 167/1000 | Loss: 0.00002715
Iteration 168/1000 | Loss: 0.00002715
Iteration 169/1000 | Loss: 0.00002715
Iteration 170/1000 | Loss: 0.00002714
Iteration 171/1000 | Loss: 0.00002714
Iteration 172/1000 | Loss: 0.00002714
Iteration 173/1000 | Loss: 0.00002714
Iteration 174/1000 | Loss: 0.00002714
Iteration 175/1000 | Loss: 0.00002714
Iteration 176/1000 | Loss: 0.00002714
Iteration 177/1000 | Loss: 0.00002714
Iteration 178/1000 | Loss: 0.00002714
Iteration 179/1000 | Loss: 0.00002714
Iteration 180/1000 | Loss: 0.00002714
Iteration 181/1000 | Loss: 0.00002714
Iteration 182/1000 | Loss: 0.00002714
Iteration 183/1000 | Loss: 0.00002713
Iteration 184/1000 | Loss: 0.00002713
Iteration 185/1000 | Loss: 0.00002713
Iteration 186/1000 | Loss: 0.00002713
Iteration 187/1000 | Loss: 0.00002713
Iteration 188/1000 | Loss: 0.00002713
Iteration 189/1000 | Loss: 0.00002713
Iteration 190/1000 | Loss: 0.00002713
Iteration 191/1000 | Loss: 0.00002713
Iteration 192/1000 | Loss: 0.00002713
Iteration 193/1000 | Loss: 0.00002713
Iteration 194/1000 | Loss: 0.00002713
Iteration 195/1000 | Loss: 0.00002713
Iteration 196/1000 | Loss: 0.00002713
Iteration 197/1000 | Loss: 0.00002713
Iteration 198/1000 | Loss: 0.00002713
Iteration 199/1000 | Loss: 0.00002713
Iteration 200/1000 | Loss: 0.00002713
Iteration 201/1000 | Loss: 0.00002713
Iteration 202/1000 | Loss: 0.00002713
Iteration 203/1000 | Loss: 0.00002713
Iteration 204/1000 | Loss: 0.00002713
Iteration 205/1000 | Loss: 0.00002713
Iteration 206/1000 | Loss: 0.00002713
Iteration 207/1000 | Loss: 0.00002713
Iteration 208/1000 | Loss: 0.00002713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.713231515372172e-05, 2.713231515372172e-05, 2.713231515372172e-05, 2.713231515372172e-05, 2.713231515372172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.713231515372172e-05

Optimization complete. Final v2v error: 4.2761735916137695 mm

Highest mean error: 5.461217403411865 mm for frame 106

Lowest mean error: 3.1883981227874756 mm for frame 75

Saving results

Total time: 48.67436647415161
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441305
Iteration 2/25 | Loss: 0.00135167
Iteration 3/25 | Loss: 0.00129133
Iteration 4/25 | Loss: 0.00127875
Iteration 5/25 | Loss: 0.00127483
Iteration 6/25 | Loss: 0.00127480
Iteration 7/25 | Loss: 0.00127480
Iteration 8/25 | Loss: 0.00127480
Iteration 9/25 | Loss: 0.00127480
Iteration 10/25 | Loss: 0.00127480
Iteration 11/25 | Loss: 0.00127480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012747969012707472, 0.0012747969012707472, 0.0012747969012707472, 0.0012747969012707472, 0.0012747969012707472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012747969012707472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.26699162
Iteration 2/25 | Loss: 0.00081481
Iteration 3/25 | Loss: 0.00081481
Iteration 4/25 | Loss: 0.00081481
Iteration 5/25 | Loss: 0.00081480
Iteration 6/25 | Loss: 0.00081480
Iteration 7/25 | Loss: 0.00081480
Iteration 8/25 | Loss: 0.00081480
Iteration 9/25 | Loss: 0.00081480
Iteration 10/25 | Loss: 0.00081480
Iteration 11/25 | Loss: 0.00081480
Iteration 12/25 | Loss: 0.00081480
Iteration 13/25 | Loss: 0.00081480
Iteration 14/25 | Loss: 0.00081480
Iteration 15/25 | Loss: 0.00081480
Iteration 16/25 | Loss: 0.00081480
Iteration 17/25 | Loss: 0.00081480
Iteration 18/25 | Loss: 0.00081480
Iteration 19/25 | Loss: 0.00081480
Iteration 20/25 | Loss: 0.00081480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000814802770037204, 0.000814802770037204, 0.000814802770037204, 0.000814802770037204, 0.000814802770037204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000814802770037204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081480
Iteration 2/1000 | Loss: 0.00002490
Iteration 3/1000 | Loss: 0.00002052
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001815
Iteration 6/1000 | Loss: 0.00001751
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001603
Iteration 12/1000 | Loss: 0.00001599
Iteration 13/1000 | Loss: 0.00001592
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001550
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001543
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001541
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001533
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001526
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001513
Iteration 57/1000 | Loss: 0.00001513
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001509
Iteration 69/1000 | Loss: 0.00001509
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001509
Iteration 72/1000 | Loss: 0.00001509
Iteration 73/1000 | Loss: 0.00001509
Iteration 74/1000 | Loss: 0.00001509
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001508
Iteration 77/1000 | Loss: 0.00001508
Iteration 78/1000 | Loss: 0.00001508
Iteration 79/1000 | Loss: 0.00001508
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00001504
Iteration 122/1000 | Loss: 0.00001504
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001503
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001503
Iteration 135/1000 | Loss: 0.00001503
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.5013180927780923e-05, 1.5013180927780923e-05, 1.5013180927780923e-05, 1.5013180927780923e-05, 1.5013180927780923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5013180927780923e-05

Optimization complete. Final v2v error: 3.2834970951080322 mm

Highest mean error: 3.6603996753692627 mm for frame 175

Lowest mean error: 2.974177360534668 mm for frame 77

Saving results

Total time: 44.29023575782776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972287
Iteration 2/25 | Loss: 0.00148282
Iteration 3/25 | Loss: 0.00111881
Iteration 4/25 | Loss: 0.00109663
Iteration 5/25 | Loss: 0.00109341
Iteration 6/25 | Loss: 0.00109248
Iteration 7/25 | Loss: 0.00109248
Iteration 8/25 | Loss: 0.00109248
Iteration 9/25 | Loss: 0.00109248
Iteration 10/25 | Loss: 0.00109248
Iteration 11/25 | Loss: 0.00109248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010924757225438952, 0.0010924757225438952, 0.0010924757225438952, 0.0010924757225438952, 0.0010924757225438952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010924757225438952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93663931
Iteration 2/25 | Loss: 0.00070486
Iteration 3/25 | Loss: 0.00070486
Iteration 4/25 | Loss: 0.00070486
Iteration 5/25 | Loss: 0.00070486
Iteration 6/25 | Loss: 0.00070486
Iteration 7/25 | Loss: 0.00070486
Iteration 8/25 | Loss: 0.00070485
Iteration 9/25 | Loss: 0.00070485
Iteration 10/25 | Loss: 0.00070485
Iteration 11/25 | Loss: 0.00070485
Iteration 12/25 | Loss: 0.00070485
Iteration 13/25 | Loss: 0.00070485
Iteration 14/25 | Loss: 0.00070485
Iteration 15/25 | Loss: 0.00070485
Iteration 16/25 | Loss: 0.00070485
Iteration 17/25 | Loss: 0.00070485
Iteration 18/25 | Loss: 0.00070485
Iteration 19/25 | Loss: 0.00070485
Iteration 20/25 | Loss: 0.00070485
Iteration 21/25 | Loss: 0.00070485
Iteration 22/25 | Loss: 0.00070485
Iteration 23/25 | Loss: 0.00070485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007048541447147727, 0.0007048541447147727, 0.0007048541447147727, 0.0007048541447147727, 0.0007048541447147727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007048541447147727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070485
Iteration 2/1000 | Loss: 0.00004411
Iteration 3/1000 | Loss: 0.00002985
Iteration 4/1000 | Loss: 0.00002505
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00002259
Iteration 7/1000 | Loss: 0.00002184
Iteration 8/1000 | Loss: 0.00002132
Iteration 9/1000 | Loss: 0.00002091
Iteration 10/1000 | Loss: 0.00002068
Iteration 11/1000 | Loss: 0.00002053
Iteration 12/1000 | Loss: 0.00002035
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002030
Iteration 15/1000 | Loss: 0.00002028
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002027
Iteration 18/1000 | Loss: 0.00002025
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002007
Iteration 24/1000 | Loss: 0.00002006
Iteration 25/1000 | Loss: 0.00002005
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002003
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00001999
Iteration 32/1000 | Loss: 0.00001999
Iteration 33/1000 | Loss: 0.00001999
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001997
Iteration 37/1000 | Loss: 0.00001997
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001996
Iteration 42/1000 | Loss: 0.00001996
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001995
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001994
Iteration 52/1000 | Loss: 0.00001994
Iteration 53/1000 | Loss: 0.00001994
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001993
Iteration 57/1000 | Loss: 0.00001993
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001992
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001991
Iteration 65/1000 | Loss: 0.00001991
Iteration 66/1000 | Loss: 0.00001991
Iteration 67/1000 | Loss: 0.00001990
Iteration 68/1000 | Loss: 0.00001990
Iteration 69/1000 | Loss: 0.00001990
Iteration 70/1000 | Loss: 0.00001990
Iteration 71/1000 | Loss: 0.00001989
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001989
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001988
Iteration 80/1000 | Loss: 0.00001988
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001987
Iteration 84/1000 | Loss: 0.00001987
Iteration 85/1000 | Loss: 0.00001987
Iteration 86/1000 | Loss: 0.00001987
Iteration 87/1000 | Loss: 0.00001987
Iteration 88/1000 | Loss: 0.00001987
Iteration 89/1000 | Loss: 0.00001987
Iteration 90/1000 | Loss: 0.00001987
Iteration 91/1000 | Loss: 0.00001986
Iteration 92/1000 | Loss: 0.00001986
Iteration 93/1000 | Loss: 0.00001986
Iteration 94/1000 | Loss: 0.00001986
Iteration 95/1000 | Loss: 0.00001985
Iteration 96/1000 | Loss: 0.00001985
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001982
Iteration 113/1000 | Loss: 0.00001982
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001981
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001979
Iteration 134/1000 | Loss: 0.00001979
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001978
Iteration 137/1000 | Loss: 0.00001978
Iteration 138/1000 | Loss: 0.00001978
Iteration 139/1000 | Loss: 0.00001978
Iteration 140/1000 | Loss: 0.00001978
Iteration 141/1000 | Loss: 0.00001978
Iteration 142/1000 | Loss: 0.00001977
Iteration 143/1000 | Loss: 0.00001977
Iteration 144/1000 | Loss: 0.00001977
Iteration 145/1000 | Loss: 0.00001977
Iteration 146/1000 | Loss: 0.00001976
Iteration 147/1000 | Loss: 0.00001976
Iteration 148/1000 | Loss: 0.00001976
Iteration 149/1000 | Loss: 0.00001976
Iteration 150/1000 | Loss: 0.00001976
Iteration 151/1000 | Loss: 0.00001976
Iteration 152/1000 | Loss: 0.00001976
Iteration 153/1000 | Loss: 0.00001975
Iteration 154/1000 | Loss: 0.00001975
Iteration 155/1000 | Loss: 0.00001975
Iteration 156/1000 | Loss: 0.00001975
Iteration 157/1000 | Loss: 0.00001975
Iteration 158/1000 | Loss: 0.00001975
Iteration 159/1000 | Loss: 0.00001974
Iteration 160/1000 | Loss: 0.00001974
Iteration 161/1000 | Loss: 0.00001974
Iteration 162/1000 | Loss: 0.00001974
Iteration 163/1000 | Loss: 0.00001973
Iteration 164/1000 | Loss: 0.00001973
Iteration 165/1000 | Loss: 0.00001973
Iteration 166/1000 | Loss: 0.00001972
Iteration 167/1000 | Loss: 0.00001972
Iteration 168/1000 | Loss: 0.00001972
Iteration 169/1000 | Loss: 0.00001972
Iteration 170/1000 | Loss: 0.00001972
Iteration 171/1000 | Loss: 0.00001971
Iteration 172/1000 | Loss: 0.00001971
Iteration 173/1000 | Loss: 0.00001971
Iteration 174/1000 | Loss: 0.00001971
Iteration 175/1000 | Loss: 0.00001971
Iteration 176/1000 | Loss: 0.00001971
Iteration 177/1000 | Loss: 0.00001971
Iteration 178/1000 | Loss: 0.00001970
Iteration 179/1000 | Loss: 0.00001970
Iteration 180/1000 | Loss: 0.00001970
Iteration 181/1000 | Loss: 0.00001970
Iteration 182/1000 | Loss: 0.00001970
Iteration 183/1000 | Loss: 0.00001970
Iteration 184/1000 | Loss: 0.00001970
Iteration 185/1000 | Loss: 0.00001970
Iteration 186/1000 | Loss: 0.00001970
Iteration 187/1000 | Loss: 0.00001970
Iteration 188/1000 | Loss: 0.00001970
Iteration 189/1000 | Loss: 0.00001970
Iteration 190/1000 | Loss: 0.00001970
Iteration 191/1000 | Loss: 0.00001970
Iteration 192/1000 | Loss: 0.00001970
Iteration 193/1000 | Loss: 0.00001970
Iteration 194/1000 | Loss: 0.00001970
Iteration 195/1000 | Loss: 0.00001970
Iteration 196/1000 | Loss: 0.00001970
Iteration 197/1000 | Loss: 0.00001970
Iteration 198/1000 | Loss: 0.00001970
Iteration 199/1000 | Loss: 0.00001970
Iteration 200/1000 | Loss: 0.00001970
Iteration 201/1000 | Loss: 0.00001970
Iteration 202/1000 | Loss: 0.00001970
Iteration 203/1000 | Loss: 0.00001970
Iteration 204/1000 | Loss: 0.00001970
Iteration 205/1000 | Loss: 0.00001970
Iteration 206/1000 | Loss: 0.00001970
Iteration 207/1000 | Loss: 0.00001970
Iteration 208/1000 | Loss: 0.00001970
Iteration 209/1000 | Loss: 0.00001970
Iteration 210/1000 | Loss: 0.00001970
Iteration 211/1000 | Loss: 0.00001970
Iteration 212/1000 | Loss: 0.00001970
Iteration 213/1000 | Loss: 0.00001970
Iteration 214/1000 | Loss: 0.00001970
Iteration 215/1000 | Loss: 0.00001970
Iteration 216/1000 | Loss: 0.00001970
Iteration 217/1000 | Loss: 0.00001970
Iteration 218/1000 | Loss: 0.00001970
Iteration 219/1000 | Loss: 0.00001970
Iteration 220/1000 | Loss: 0.00001970
Iteration 221/1000 | Loss: 0.00001970
Iteration 222/1000 | Loss: 0.00001970
Iteration 223/1000 | Loss: 0.00001970
Iteration 224/1000 | Loss: 0.00001970
Iteration 225/1000 | Loss: 0.00001970
Iteration 226/1000 | Loss: 0.00001970
Iteration 227/1000 | Loss: 0.00001970
Iteration 228/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.96961536857998e-05, 1.96961536857998e-05, 1.96961536857998e-05, 1.96961536857998e-05, 1.96961536857998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.96961536857998e-05

Optimization complete. Final v2v error: 3.612583875656128 mm

Highest mean error: 3.9739887714385986 mm for frame 137

Lowest mean error: 3.04354190826416 mm for frame 29

Saving results

Total time: 41.41881823539734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490692
Iteration 2/25 | Loss: 0.00119904
Iteration 3/25 | Loss: 0.00108358
Iteration 4/25 | Loss: 0.00106509
Iteration 5/25 | Loss: 0.00105904
Iteration 6/25 | Loss: 0.00105754
Iteration 7/25 | Loss: 0.00105754
Iteration 8/25 | Loss: 0.00105754
Iteration 9/25 | Loss: 0.00105754
Iteration 10/25 | Loss: 0.00105754
Iteration 11/25 | Loss: 0.00105754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010575371561571956, 0.0010575371561571956, 0.0010575371561571956, 0.0010575371561571956, 0.0010575371561571956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010575371561571956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23640263
Iteration 2/25 | Loss: 0.00117004
Iteration 3/25 | Loss: 0.00117004
Iteration 4/25 | Loss: 0.00117004
Iteration 5/25 | Loss: 0.00117004
Iteration 6/25 | Loss: 0.00117004
Iteration 7/25 | Loss: 0.00117004
Iteration 8/25 | Loss: 0.00117004
Iteration 9/25 | Loss: 0.00117004
Iteration 10/25 | Loss: 0.00117004
Iteration 11/25 | Loss: 0.00117004
Iteration 12/25 | Loss: 0.00117004
Iteration 13/25 | Loss: 0.00117004
Iteration 14/25 | Loss: 0.00117004
Iteration 15/25 | Loss: 0.00117004
Iteration 16/25 | Loss: 0.00117004
Iteration 17/25 | Loss: 0.00117004
Iteration 18/25 | Loss: 0.00117004
Iteration 19/25 | Loss: 0.00117004
Iteration 20/25 | Loss: 0.00117004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011700368486344814, 0.0011700368486344814, 0.0011700368486344814, 0.0011700368486344814, 0.0011700368486344814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011700368486344814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117004
Iteration 2/1000 | Loss: 0.00005302
Iteration 3/1000 | Loss: 0.00003548
Iteration 4/1000 | Loss: 0.00002828
Iteration 5/1000 | Loss: 0.00002535
Iteration 6/1000 | Loss: 0.00002365
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00002164
Iteration 10/1000 | Loss: 0.00002115
Iteration 11/1000 | Loss: 0.00002077
Iteration 12/1000 | Loss: 0.00002029
Iteration 13/1000 | Loss: 0.00002000
Iteration 14/1000 | Loss: 0.00001985
Iteration 15/1000 | Loss: 0.00001965
Iteration 16/1000 | Loss: 0.00001957
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001928
Iteration 24/1000 | Loss: 0.00001924
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001923
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001920
Iteration 31/1000 | Loss: 0.00001920
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001919
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001918
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00001918
Iteration 40/1000 | Loss: 0.00001918
Iteration 41/1000 | Loss: 0.00001918
Iteration 42/1000 | Loss: 0.00001918
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001918
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001916
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001914
Iteration 60/1000 | Loss: 0.00001914
Iteration 61/1000 | Loss: 0.00001913
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.911886101879645e-05, 1.911886101879645e-05, 1.911886101879645e-05, 1.911886101879645e-05, 1.911886101879645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.911886101879645e-05

Optimization complete. Final v2v error: 3.5374269485473633 mm

Highest mean error: 3.933913469314575 mm for frame 38

Lowest mean error: 3.2860848903656006 mm for frame 160

Saving results

Total time: 37.547775983810425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020624
Iteration 2/25 | Loss: 0.00385713
Iteration 3/25 | Loss: 0.00295285
Iteration 4/25 | Loss: 0.00234135
Iteration 5/25 | Loss: 0.00240833
Iteration 6/25 | Loss: 0.00255322
Iteration 7/25 | Loss: 0.00207712
Iteration 8/25 | Loss: 0.00181193
Iteration 9/25 | Loss: 0.00173423
Iteration 10/25 | Loss: 0.00172568
Iteration 11/25 | Loss: 0.00161535
Iteration 12/25 | Loss: 0.00160559
Iteration 13/25 | Loss: 0.00154717
Iteration 14/25 | Loss: 0.00145806
Iteration 15/25 | Loss: 0.00140896
Iteration 16/25 | Loss: 0.00136705
Iteration 17/25 | Loss: 0.00134900
Iteration 18/25 | Loss: 0.00134104
Iteration 19/25 | Loss: 0.00135148
Iteration 20/25 | Loss: 0.00134612
Iteration 21/25 | Loss: 0.00134974
Iteration 22/25 | Loss: 0.00134451
Iteration 23/25 | Loss: 0.00134784
Iteration 24/25 | Loss: 0.00135012
Iteration 25/25 | Loss: 0.00136503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08125997
Iteration 2/25 | Loss: 0.02222113
Iteration 3/25 | Loss: 0.00542102
Iteration 4/25 | Loss: 0.00570956
Iteration 5/25 | Loss: 0.00438135
Iteration 6/25 | Loss: 0.00438135
Iteration 7/25 | Loss: 0.00438135
Iteration 8/25 | Loss: 0.00438134
Iteration 9/25 | Loss: 0.00438134
Iteration 10/25 | Loss: 0.00438134
Iteration 11/25 | Loss: 0.00438134
Iteration 12/25 | Loss: 0.00438134
Iteration 13/25 | Loss: 0.00438134
Iteration 14/25 | Loss: 0.00438134
Iteration 15/25 | Loss: 0.00438134
Iteration 16/25 | Loss: 0.00438134
Iteration 17/25 | Loss: 0.00438134
Iteration 18/25 | Loss: 0.00438134
Iteration 19/25 | Loss: 0.00438134
Iteration 20/25 | Loss: 0.00438134
Iteration 21/25 | Loss: 0.00438134
Iteration 22/25 | Loss: 0.00438134
Iteration 23/25 | Loss: 0.00438134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004381342325359583, 0.004381342325359583, 0.004381342325359583, 0.004381342325359583, 0.004381342325359583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004381342325359583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00438134
Iteration 2/1000 | Loss: 0.00129555
Iteration 3/1000 | Loss: 0.00081179
Iteration 4/1000 | Loss: 0.00141459
Iteration 5/1000 | Loss: 0.00215456
Iteration 6/1000 | Loss: 0.00722209
Iteration 7/1000 | Loss: 0.00729603
Iteration 8/1000 | Loss: 0.00321419
Iteration 9/1000 | Loss: 0.00118696
Iteration 10/1000 | Loss: 0.00164323
Iteration 11/1000 | Loss: 0.00073323
Iteration 12/1000 | Loss: 0.00116812
Iteration 13/1000 | Loss: 0.00108412
Iteration 14/1000 | Loss: 0.00653654
Iteration 15/1000 | Loss: 0.00486816
Iteration 16/1000 | Loss: 0.00476555
Iteration 17/1000 | Loss: 0.00457722
Iteration 18/1000 | Loss: 0.00047569
Iteration 19/1000 | Loss: 0.00144794
Iteration 20/1000 | Loss: 0.00506869
Iteration 21/1000 | Loss: 0.00048174
Iteration 22/1000 | Loss: 0.00187972
Iteration 23/1000 | Loss: 0.00259138
Iteration 24/1000 | Loss: 0.00169038
Iteration 25/1000 | Loss: 0.00121664
Iteration 26/1000 | Loss: 0.00044817
Iteration 27/1000 | Loss: 0.00089826
Iteration 28/1000 | Loss: 0.00088219
Iteration 29/1000 | Loss: 0.00067172
Iteration 30/1000 | Loss: 0.00074665
Iteration 31/1000 | Loss: 0.00061965
Iteration 32/1000 | Loss: 0.00139250
Iteration 33/1000 | Loss: 0.00051016
Iteration 34/1000 | Loss: 0.00034193
Iteration 35/1000 | Loss: 0.00042218
Iteration 36/1000 | Loss: 0.00435448
Iteration 37/1000 | Loss: 0.00057043
Iteration 38/1000 | Loss: 0.00038325
Iteration 39/1000 | Loss: 0.00236271
Iteration 40/1000 | Loss: 0.00065919
Iteration 41/1000 | Loss: 0.00030338
Iteration 42/1000 | Loss: 0.00057819
Iteration 43/1000 | Loss: 0.00072137
Iteration 44/1000 | Loss: 0.00081508
Iteration 45/1000 | Loss: 0.00235117
Iteration 46/1000 | Loss: 0.00225191
Iteration 47/1000 | Loss: 0.00044850
Iteration 48/1000 | Loss: 0.00065478
Iteration 49/1000 | Loss: 0.00142560
Iteration 50/1000 | Loss: 0.00096840
Iteration 51/1000 | Loss: 0.00133909
Iteration 52/1000 | Loss: 0.00059099
Iteration 53/1000 | Loss: 0.00068535
Iteration 54/1000 | Loss: 0.00179194
Iteration 55/1000 | Loss: 0.00040366
Iteration 56/1000 | Loss: 0.00084415
Iteration 57/1000 | Loss: 0.00179635
Iteration 58/1000 | Loss: 0.00149995
Iteration 59/1000 | Loss: 0.00019093
Iteration 60/1000 | Loss: 0.00016004
Iteration 61/1000 | Loss: 0.00014815
Iteration 62/1000 | Loss: 0.00154442
Iteration 63/1000 | Loss: 0.00030778
Iteration 64/1000 | Loss: 0.00037588
Iteration 65/1000 | Loss: 0.00013620
Iteration 66/1000 | Loss: 0.00012930
Iteration 67/1000 | Loss: 0.00019528
Iteration 68/1000 | Loss: 0.00012379
Iteration 69/1000 | Loss: 0.00011955
Iteration 70/1000 | Loss: 0.00011626
Iteration 71/1000 | Loss: 0.00011474
Iteration 72/1000 | Loss: 0.00011315
Iteration 73/1000 | Loss: 0.00058506
Iteration 74/1000 | Loss: 0.00688105
Iteration 75/1000 | Loss: 0.00694184
Iteration 76/1000 | Loss: 0.00149195
Iteration 77/1000 | Loss: 0.00041111
Iteration 78/1000 | Loss: 0.00035455
Iteration 79/1000 | Loss: 0.00014435
Iteration 80/1000 | Loss: 0.00034317
Iteration 81/1000 | Loss: 0.00141931
Iteration 82/1000 | Loss: 0.00054888
Iteration 83/1000 | Loss: 0.00086394
Iteration 84/1000 | Loss: 0.00366171
Iteration 85/1000 | Loss: 0.00543746
Iteration 86/1000 | Loss: 0.00205206
Iteration 87/1000 | Loss: 0.00102340
Iteration 88/1000 | Loss: 0.00044431
Iteration 89/1000 | Loss: 0.00018081
Iteration 90/1000 | Loss: 0.00026139
Iteration 91/1000 | Loss: 0.00033695
Iteration 92/1000 | Loss: 0.00019923
Iteration 93/1000 | Loss: 0.00023410
Iteration 94/1000 | Loss: 0.00069043
Iteration 95/1000 | Loss: 0.00023505
Iteration 96/1000 | Loss: 0.00019678
Iteration 97/1000 | Loss: 0.00017869
Iteration 98/1000 | Loss: 0.00089212
Iteration 99/1000 | Loss: 0.00067253
Iteration 100/1000 | Loss: 0.00102255
Iteration 101/1000 | Loss: 0.00039066
Iteration 102/1000 | Loss: 0.00007553
Iteration 103/1000 | Loss: 0.00022945
Iteration 104/1000 | Loss: 0.00069285
Iteration 105/1000 | Loss: 0.00056039
Iteration 106/1000 | Loss: 0.00019702
Iteration 107/1000 | Loss: 0.00062010
Iteration 108/1000 | Loss: 0.00025802
Iteration 109/1000 | Loss: 0.00019811
Iteration 110/1000 | Loss: 0.00017865
Iteration 111/1000 | Loss: 0.00011419
Iteration 112/1000 | Loss: 0.00033475
Iteration 113/1000 | Loss: 0.00035557
Iteration 114/1000 | Loss: 0.00030432
Iteration 115/1000 | Loss: 0.00023181
Iteration 116/1000 | Loss: 0.00030298
Iteration 117/1000 | Loss: 0.00026171
Iteration 118/1000 | Loss: 0.00004273
Iteration 119/1000 | Loss: 0.00003415
Iteration 120/1000 | Loss: 0.00003077
Iteration 121/1000 | Loss: 0.00002822
Iteration 122/1000 | Loss: 0.00002607
Iteration 123/1000 | Loss: 0.00015647
Iteration 124/1000 | Loss: 0.00002919
Iteration 125/1000 | Loss: 0.00015081
Iteration 126/1000 | Loss: 0.00017662
Iteration 127/1000 | Loss: 0.00025545
Iteration 128/1000 | Loss: 0.00004476
Iteration 129/1000 | Loss: 0.00013000
Iteration 130/1000 | Loss: 0.00005628
Iteration 131/1000 | Loss: 0.00002470
Iteration 132/1000 | Loss: 0.00002347
Iteration 133/1000 | Loss: 0.00002241
Iteration 134/1000 | Loss: 0.00002180
Iteration 135/1000 | Loss: 0.00002143
Iteration 136/1000 | Loss: 0.00002092
Iteration 137/1000 | Loss: 0.00002042
Iteration 138/1000 | Loss: 0.00002005
Iteration 139/1000 | Loss: 0.00001995
Iteration 140/1000 | Loss: 0.00001993
Iteration 141/1000 | Loss: 0.00001973
Iteration 142/1000 | Loss: 0.00001968
Iteration 143/1000 | Loss: 0.00001953
Iteration 144/1000 | Loss: 0.00001939
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00026719
Iteration 147/1000 | Loss: 0.00004144
Iteration 148/1000 | Loss: 0.00014786
Iteration 149/1000 | Loss: 0.00001939
Iteration 150/1000 | Loss: 0.00001918
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001914
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001913
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001912
Iteration 159/1000 | Loss: 0.00001912
Iteration 160/1000 | Loss: 0.00001912
Iteration 161/1000 | Loss: 0.00001912
Iteration 162/1000 | Loss: 0.00001912
Iteration 163/1000 | Loss: 0.00001911
Iteration 164/1000 | Loss: 0.00001911
Iteration 165/1000 | Loss: 0.00001911
Iteration 166/1000 | Loss: 0.00001911
Iteration 167/1000 | Loss: 0.00001911
Iteration 168/1000 | Loss: 0.00001910
Iteration 169/1000 | Loss: 0.00001910
Iteration 170/1000 | Loss: 0.00001910
Iteration 171/1000 | Loss: 0.00001910
Iteration 172/1000 | Loss: 0.00001910
Iteration 173/1000 | Loss: 0.00001910
Iteration 174/1000 | Loss: 0.00001910
Iteration 175/1000 | Loss: 0.00001910
Iteration 176/1000 | Loss: 0.00001910
Iteration 177/1000 | Loss: 0.00001910
Iteration 178/1000 | Loss: 0.00001910
Iteration 179/1000 | Loss: 0.00001909
Iteration 180/1000 | Loss: 0.00001909
Iteration 181/1000 | Loss: 0.00001909
Iteration 182/1000 | Loss: 0.00001909
Iteration 183/1000 | Loss: 0.00001909
Iteration 184/1000 | Loss: 0.00001909
Iteration 185/1000 | Loss: 0.00001909
Iteration 186/1000 | Loss: 0.00001908
Iteration 187/1000 | Loss: 0.00001908
Iteration 188/1000 | Loss: 0.00001908
Iteration 189/1000 | Loss: 0.00001907
Iteration 190/1000 | Loss: 0.00001907
Iteration 191/1000 | Loss: 0.00001907
Iteration 192/1000 | Loss: 0.00001906
Iteration 193/1000 | Loss: 0.00001906
Iteration 194/1000 | Loss: 0.00001906
Iteration 195/1000 | Loss: 0.00001906
Iteration 196/1000 | Loss: 0.00001906
Iteration 197/1000 | Loss: 0.00001906
Iteration 198/1000 | Loss: 0.00001906
Iteration 199/1000 | Loss: 0.00001906
Iteration 200/1000 | Loss: 0.00001906
Iteration 201/1000 | Loss: 0.00001906
Iteration 202/1000 | Loss: 0.00001906
Iteration 203/1000 | Loss: 0.00001906
Iteration 204/1000 | Loss: 0.00001906
Iteration 205/1000 | Loss: 0.00001906
Iteration 206/1000 | Loss: 0.00001906
Iteration 207/1000 | Loss: 0.00001906
Iteration 208/1000 | Loss: 0.00001906
Iteration 209/1000 | Loss: 0.00001905
Iteration 210/1000 | Loss: 0.00001905
Iteration 211/1000 | Loss: 0.00001905
Iteration 212/1000 | Loss: 0.00001905
Iteration 213/1000 | Loss: 0.00001905
Iteration 214/1000 | Loss: 0.00001905
Iteration 215/1000 | Loss: 0.00001905
Iteration 216/1000 | Loss: 0.00001905
Iteration 217/1000 | Loss: 0.00001905
Iteration 218/1000 | Loss: 0.00001905
Iteration 219/1000 | Loss: 0.00001905
Iteration 220/1000 | Loss: 0.00001905
Iteration 221/1000 | Loss: 0.00001905
Iteration 222/1000 | Loss: 0.00001905
Iteration 223/1000 | Loss: 0.00001905
Iteration 224/1000 | Loss: 0.00001905
Iteration 225/1000 | Loss: 0.00001905
Iteration 226/1000 | Loss: 0.00001905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.9052953575737774e-05, 1.9052953575737774e-05, 1.9052953575737774e-05, 1.9052953575737774e-05, 1.9052953575737774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9052953575737774e-05

Optimization complete. Final v2v error: 3.2774658203125 mm

Highest mean error: 12.129715919494629 mm for frame 40

Lowest mean error: 2.305205821990967 mm for frame 114

Saving results

Total time: 256.3917682170868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018545
Iteration 2/25 | Loss: 0.00146731
Iteration 3/25 | Loss: 0.00111638
Iteration 4/25 | Loss: 0.00107102
Iteration 5/25 | Loss: 0.00107616
Iteration 6/25 | Loss: 0.00103661
Iteration 7/25 | Loss: 0.00102028
Iteration 8/25 | Loss: 0.00102408
Iteration 9/25 | Loss: 0.00099995
Iteration 10/25 | Loss: 0.00098490
Iteration 11/25 | Loss: 0.00097265
Iteration 12/25 | Loss: 0.00097269
Iteration 13/25 | Loss: 0.00097067
Iteration 14/25 | Loss: 0.00097096
Iteration 15/25 | Loss: 0.00097035
Iteration 16/25 | Loss: 0.00097165
Iteration 17/25 | Loss: 0.00096554
Iteration 18/25 | Loss: 0.00096410
Iteration 19/25 | Loss: 0.00096246
Iteration 20/25 | Loss: 0.00096209
Iteration 21/25 | Loss: 0.00096190
Iteration 22/25 | Loss: 0.00096173
Iteration 23/25 | Loss: 0.00096154
Iteration 24/25 | Loss: 0.00096139
Iteration 25/25 | Loss: 0.00096119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37944472
Iteration 2/25 | Loss: 0.00110389
Iteration 3/25 | Loss: 0.00110389
Iteration 4/25 | Loss: 0.00110388
Iteration 5/25 | Loss: 0.00110388
Iteration 6/25 | Loss: 0.00110388
Iteration 7/25 | Loss: 0.00110388
Iteration 8/25 | Loss: 0.00110388
Iteration 9/25 | Loss: 0.00110388
Iteration 10/25 | Loss: 0.00110388
Iteration 11/25 | Loss: 0.00110388
Iteration 12/25 | Loss: 0.00110388
Iteration 13/25 | Loss: 0.00110388
Iteration 14/25 | Loss: 0.00110388
Iteration 15/25 | Loss: 0.00110388
Iteration 16/25 | Loss: 0.00110388
Iteration 17/25 | Loss: 0.00110388
Iteration 18/25 | Loss: 0.00110388
Iteration 19/25 | Loss: 0.00110388
Iteration 20/25 | Loss: 0.00110388
Iteration 21/25 | Loss: 0.00110388
Iteration 22/25 | Loss: 0.00110388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011038820957764983, 0.0011038820957764983, 0.0011038820957764983, 0.0011038820957764983, 0.0011038820957764983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011038820957764983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110388
Iteration 2/1000 | Loss: 0.00005391
Iteration 3/1000 | Loss: 0.00004864
Iteration 4/1000 | Loss: 0.00004956
Iteration 5/1000 | Loss: 0.00005156
Iteration 6/1000 | Loss: 0.00003724
Iteration 7/1000 | Loss: 0.00003337
Iteration 8/1000 | Loss: 0.00005503
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00006766
Iteration 11/1000 | Loss: 0.00002348
Iteration 12/1000 | Loss: 0.00002600
Iteration 13/1000 | Loss: 0.00002288
Iteration 14/1000 | Loss: 0.00002517
Iteration 15/1000 | Loss: 0.00002279
Iteration 16/1000 | Loss: 0.00002277
Iteration 17/1000 | Loss: 0.00003019
Iteration 18/1000 | Loss: 0.00002252
Iteration 19/1000 | Loss: 0.00002252
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002251
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00002250
Iteration 24/1000 | Loss: 0.00002250
Iteration 25/1000 | Loss: 0.00002250
Iteration 26/1000 | Loss: 0.00002249
Iteration 27/1000 | Loss: 0.00002249
Iteration 28/1000 | Loss: 0.00002249
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002242
Iteration 31/1000 | Loss: 0.00002242
Iteration 32/1000 | Loss: 0.00002241
Iteration 33/1000 | Loss: 0.00002241
Iteration 34/1000 | Loss: 0.00002240
Iteration 35/1000 | Loss: 0.00002240
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002236
Iteration 38/1000 | Loss: 0.00002233
Iteration 39/1000 | Loss: 0.00002400
Iteration 40/1000 | Loss: 0.00002267
Iteration 41/1000 | Loss: 0.00002231
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002230
Iteration 44/1000 | Loss: 0.00002230
Iteration 45/1000 | Loss: 0.00002230
Iteration 46/1000 | Loss: 0.00002230
Iteration 47/1000 | Loss: 0.00002230
Iteration 48/1000 | Loss: 0.00002230
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002230
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002229
Iteration 54/1000 | Loss: 0.00002229
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002228
Iteration 57/1000 | Loss: 0.00002228
Iteration 58/1000 | Loss: 0.00002228
Iteration 59/1000 | Loss: 0.00002228
Iteration 60/1000 | Loss: 0.00002227
Iteration 61/1000 | Loss: 0.00002227
Iteration 62/1000 | Loss: 0.00002227
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002227
Iteration 66/1000 | Loss: 0.00002227
Iteration 67/1000 | Loss: 0.00002227
Iteration 68/1000 | Loss: 0.00002226
Iteration 69/1000 | Loss: 0.00002226
Iteration 70/1000 | Loss: 0.00002226
Iteration 71/1000 | Loss: 0.00002226
Iteration 72/1000 | Loss: 0.00002225
Iteration 73/1000 | Loss: 0.00002225
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002224
Iteration 76/1000 | Loss: 0.00002224
Iteration 77/1000 | Loss: 0.00002224
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002223
Iteration 81/1000 | Loss: 0.00002223
Iteration 82/1000 | Loss: 0.00002223
Iteration 83/1000 | Loss: 0.00002223
Iteration 84/1000 | Loss: 0.00002223
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002222
Iteration 88/1000 | Loss: 0.00002222
Iteration 89/1000 | Loss: 0.00002222
Iteration 90/1000 | Loss: 0.00002222
Iteration 91/1000 | Loss: 0.00002221
Iteration 92/1000 | Loss: 0.00002221
Iteration 93/1000 | Loss: 0.00002221
Iteration 94/1000 | Loss: 0.00002220
Iteration 95/1000 | Loss: 0.00002364
Iteration 96/1000 | Loss: 0.00003086
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002298
Iteration 100/1000 | Loss: 0.00002223
Iteration 101/1000 | Loss: 0.00002352
Iteration 102/1000 | Loss: 0.00002309
Iteration 103/1000 | Loss: 0.00002352
Iteration 104/1000 | Loss: 0.00002352
Iteration 105/1000 | Loss: 0.00002285
Iteration 106/1000 | Loss: 0.00002214
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002213
Iteration 109/1000 | Loss: 0.00002213
Iteration 110/1000 | Loss: 0.00002213
Iteration 111/1000 | Loss: 0.00002213
Iteration 112/1000 | Loss: 0.00002213
Iteration 113/1000 | Loss: 0.00002213
Iteration 114/1000 | Loss: 0.00002213
Iteration 115/1000 | Loss: 0.00002213
Iteration 116/1000 | Loss: 0.00002213
Iteration 117/1000 | Loss: 0.00002213
Iteration 118/1000 | Loss: 0.00002213
Iteration 119/1000 | Loss: 0.00002213
Iteration 120/1000 | Loss: 0.00002213
Iteration 121/1000 | Loss: 0.00002213
Iteration 122/1000 | Loss: 0.00002213
Iteration 123/1000 | Loss: 0.00002213
Iteration 124/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.2129202989162877e-05, 2.2129202989162877e-05, 2.2129202989162877e-05, 2.2129202989162877e-05, 2.2129202989162877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2129202989162877e-05

Optimization complete. Final v2v error: 2.795180559158325 mm

Highest mean error: 22.504920959472656 mm for frame 144

Lowest mean error: 2.071547269821167 mm for frame 131

Saving results

Total time: 86.66684484481812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024066
Iteration 2/25 | Loss: 0.00209473
Iteration 3/25 | Loss: 0.00138681
Iteration 4/25 | Loss: 0.00127494
Iteration 5/25 | Loss: 0.00121053
Iteration 6/25 | Loss: 0.00117940
Iteration 7/25 | Loss: 0.00118535
Iteration 8/25 | Loss: 0.00114122
Iteration 9/25 | Loss: 0.00114196
Iteration 10/25 | Loss: 0.00114292
Iteration 11/25 | Loss: 0.00113847
Iteration 12/25 | Loss: 0.00114691
Iteration 13/25 | Loss: 0.00116262
Iteration 14/25 | Loss: 0.00115451
Iteration 15/25 | Loss: 0.00115535
Iteration 16/25 | Loss: 0.00116083
Iteration 17/25 | Loss: 0.00116134
Iteration 18/25 | Loss: 0.00115578
Iteration 19/25 | Loss: 0.00116487
Iteration 20/25 | Loss: 0.00116347
Iteration 21/25 | Loss: 0.00115730
Iteration 22/25 | Loss: 0.00115262
Iteration 23/25 | Loss: 0.00115931
Iteration 24/25 | Loss: 0.00114377
Iteration 25/25 | Loss: 0.00113808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28570151
Iteration 2/25 | Loss: 0.00107537
Iteration 3/25 | Loss: 0.00107537
Iteration 4/25 | Loss: 0.00107537
Iteration 5/25 | Loss: 0.00107537
Iteration 6/25 | Loss: 0.00107537
Iteration 7/25 | Loss: 0.00107537
Iteration 8/25 | Loss: 0.00107537
Iteration 9/25 | Loss: 0.00107537
Iteration 10/25 | Loss: 0.00107537
Iteration 11/25 | Loss: 0.00107537
Iteration 12/25 | Loss: 0.00107537
Iteration 13/25 | Loss: 0.00107537
Iteration 14/25 | Loss: 0.00107537
Iteration 15/25 | Loss: 0.00107537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010753689566627145, 0.0010753689566627145, 0.0010753689566627145, 0.0010753689566627145, 0.0010753689566627145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010753689566627145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107537
Iteration 2/1000 | Loss: 0.00017998
Iteration 3/1000 | Loss: 0.00022746
Iteration 4/1000 | Loss: 0.00022858
Iteration 5/1000 | Loss: 0.00023295
Iteration 6/1000 | Loss: 0.00018375
Iteration 7/1000 | Loss: 0.00024695
Iteration 8/1000 | Loss: 0.00023215
Iteration 9/1000 | Loss: 0.00017096
Iteration 10/1000 | Loss: 0.00017673
Iteration 11/1000 | Loss: 0.00016069
Iteration 12/1000 | Loss: 0.00017051
Iteration 13/1000 | Loss: 0.00017333
Iteration 14/1000 | Loss: 0.00010064
Iteration 15/1000 | Loss: 0.00012097
Iteration 16/1000 | Loss: 0.00007522
Iteration 17/1000 | Loss: 0.00009716
Iteration 18/1000 | Loss: 0.00006368
Iteration 19/1000 | Loss: 0.00008803
Iteration 20/1000 | Loss: 0.00008175
Iteration 21/1000 | Loss: 0.00013355
Iteration 22/1000 | Loss: 0.00009694
Iteration 23/1000 | Loss: 0.00011590
Iteration 24/1000 | Loss: 0.00007835
Iteration 25/1000 | Loss: 0.00019936
Iteration 26/1000 | Loss: 0.00005420
Iteration 27/1000 | Loss: 0.00022073
Iteration 28/1000 | Loss: 0.00019884
Iteration 29/1000 | Loss: 0.00010784
Iteration 30/1000 | Loss: 0.00021292
Iteration 31/1000 | Loss: 0.00021191
Iteration 32/1000 | Loss: 0.00015031
Iteration 33/1000 | Loss: 0.00019837
Iteration 34/1000 | Loss: 0.00015889
Iteration 35/1000 | Loss: 0.00013921
Iteration 36/1000 | Loss: 0.00017305
Iteration 37/1000 | Loss: 0.00012971
Iteration 38/1000 | Loss: 0.00014334
Iteration 39/1000 | Loss: 0.00008105
Iteration 40/1000 | Loss: 0.00008318
Iteration 41/1000 | Loss: 0.00013995
Iteration 42/1000 | Loss: 0.00014352
Iteration 43/1000 | Loss: 0.00018835
Iteration 44/1000 | Loss: 0.00013277
Iteration 45/1000 | Loss: 0.00018957
Iteration 46/1000 | Loss: 0.00019463
Iteration 47/1000 | Loss: 0.00022247
Iteration 48/1000 | Loss: 0.00014137
Iteration 49/1000 | Loss: 0.00010590
Iteration 50/1000 | Loss: 0.00005524
Iteration 51/1000 | Loss: 0.00003923
Iteration 52/1000 | Loss: 0.00004678
Iteration 53/1000 | Loss: 0.00004934
Iteration 54/1000 | Loss: 0.00007485
Iteration 55/1000 | Loss: 0.00013810
Iteration 56/1000 | Loss: 0.00019301
Iteration 57/1000 | Loss: 0.00018238
Iteration 58/1000 | Loss: 0.00004234
Iteration 59/1000 | Loss: 0.00003632
Iteration 60/1000 | Loss: 0.00003386
Iteration 61/1000 | Loss: 0.00013105
Iteration 62/1000 | Loss: 0.00015287
Iteration 63/1000 | Loss: 0.00014824
Iteration 64/1000 | Loss: 0.00014998
Iteration 65/1000 | Loss: 0.00011759
Iteration 66/1000 | Loss: 0.00003456
Iteration 67/1000 | Loss: 0.00003238
Iteration 68/1000 | Loss: 0.00016269
Iteration 69/1000 | Loss: 0.00028367
Iteration 70/1000 | Loss: 0.00005013
Iteration 71/1000 | Loss: 0.00020804
Iteration 72/1000 | Loss: 0.00018788
Iteration 73/1000 | Loss: 0.00010631
Iteration 74/1000 | Loss: 0.00009665
Iteration 75/1000 | Loss: 0.00008422
Iteration 76/1000 | Loss: 0.00005102
Iteration 77/1000 | Loss: 0.00015087
Iteration 78/1000 | Loss: 0.00003935
Iteration 79/1000 | Loss: 0.00011679
Iteration 80/1000 | Loss: 0.00004298
Iteration 81/1000 | Loss: 0.00002977
Iteration 82/1000 | Loss: 0.00002891
Iteration 83/1000 | Loss: 0.00002841
Iteration 84/1000 | Loss: 0.00002805
Iteration 85/1000 | Loss: 0.00002776
Iteration 86/1000 | Loss: 0.00002749
Iteration 87/1000 | Loss: 0.00013152
Iteration 88/1000 | Loss: 0.00005102
Iteration 89/1000 | Loss: 0.00003483
Iteration 90/1000 | Loss: 0.00004306
Iteration 91/1000 | Loss: 0.00003228
Iteration 92/1000 | Loss: 0.00003017
Iteration 93/1000 | Loss: 0.00002904
Iteration 94/1000 | Loss: 0.00002844
Iteration 95/1000 | Loss: 0.00002814
Iteration 96/1000 | Loss: 0.00006861
Iteration 97/1000 | Loss: 0.00035443
Iteration 98/1000 | Loss: 0.00026495
Iteration 99/1000 | Loss: 0.00024137
Iteration 100/1000 | Loss: 0.00025004
Iteration 101/1000 | Loss: 0.00008625
Iteration 102/1000 | Loss: 0.00018373
Iteration 103/1000 | Loss: 0.00005050
Iteration 104/1000 | Loss: 0.00003669
Iteration 105/1000 | Loss: 0.00003980
Iteration 106/1000 | Loss: 0.00003544
Iteration 107/1000 | Loss: 0.00009957
Iteration 108/1000 | Loss: 0.00004156
Iteration 109/1000 | Loss: 0.00012144
Iteration 110/1000 | Loss: 0.00018451
Iteration 111/1000 | Loss: 0.00023454
Iteration 112/1000 | Loss: 0.00030015
Iteration 113/1000 | Loss: 0.00011717
Iteration 114/1000 | Loss: 0.00012509
Iteration 115/1000 | Loss: 0.00005720
Iteration 116/1000 | Loss: 0.00015653
Iteration 117/1000 | Loss: 0.00014421
Iteration 118/1000 | Loss: 0.00020193
Iteration 119/1000 | Loss: 0.00021059
Iteration 120/1000 | Loss: 0.00014986
Iteration 121/1000 | Loss: 0.00016364
Iteration 122/1000 | Loss: 0.00021612
Iteration 123/1000 | Loss: 0.00009180
Iteration 124/1000 | Loss: 0.00003956
Iteration 125/1000 | Loss: 0.00003371
Iteration 126/1000 | Loss: 0.00019769
Iteration 127/1000 | Loss: 0.00030403
Iteration 128/1000 | Loss: 0.00012278
Iteration 129/1000 | Loss: 0.00015240
Iteration 130/1000 | Loss: 0.00004455
Iteration 131/1000 | Loss: 0.00007944
Iteration 132/1000 | Loss: 0.00008530
Iteration 133/1000 | Loss: 0.00003748
Iteration 134/1000 | Loss: 0.00009306
Iteration 135/1000 | Loss: 0.00012353
Iteration 136/1000 | Loss: 0.00003682
Iteration 137/1000 | Loss: 0.00003164
Iteration 138/1000 | Loss: 0.00002882
Iteration 139/1000 | Loss: 0.00004437
Iteration 140/1000 | Loss: 0.00003012
Iteration 141/1000 | Loss: 0.00004194
Iteration 142/1000 | Loss: 0.00008030
Iteration 143/1000 | Loss: 0.00006505
Iteration 144/1000 | Loss: 0.00005389
Iteration 145/1000 | Loss: 0.00006230
Iteration 146/1000 | Loss: 0.00004856
Iteration 147/1000 | Loss: 0.00005816
Iteration 148/1000 | Loss: 0.00004277
Iteration 149/1000 | Loss: 0.00005438
Iteration 150/1000 | Loss: 0.00003910
Iteration 151/1000 | Loss: 0.00003167
Iteration 152/1000 | Loss: 0.00003016
Iteration 153/1000 | Loss: 0.00002651
Iteration 154/1000 | Loss: 0.00004171
Iteration 155/1000 | Loss: 0.00002779
Iteration 156/1000 | Loss: 0.00003447
Iteration 157/1000 | Loss: 0.00004043
Iteration 158/1000 | Loss: 0.00004391
Iteration 159/1000 | Loss: 0.00002732
Iteration 160/1000 | Loss: 0.00002631
Iteration 161/1000 | Loss: 0.00002568
Iteration 162/1000 | Loss: 0.00004285
Iteration 163/1000 | Loss: 0.00003318
Iteration 164/1000 | Loss: 0.00004207
Iteration 165/1000 | Loss: 0.00003123
Iteration 166/1000 | Loss: 0.00003076
Iteration 167/1000 | Loss: 0.00004119
Iteration 168/1000 | Loss: 0.00002777
Iteration 169/1000 | Loss: 0.00002510
Iteration 170/1000 | Loss: 0.00004015
Iteration 171/1000 | Loss: 0.00002989
Iteration 172/1000 | Loss: 0.00002691
Iteration 173/1000 | Loss: 0.00002487
Iteration 174/1000 | Loss: 0.00002492
Iteration 175/1000 | Loss: 0.00002458
Iteration 176/1000 | Loss: 0.00002446
Iteration 177/1000 | Loss: 0.00002445
Iteration 178/1000 | Loss: 0.00002445
Iteration 179/1000 | Loss: 0.00002444
Iteration 180/1000 | Loss: 0.00002444
Iteration 181/1000 | Loss: 0.00002444
Iteration 182/1000 | Loss: 0.00002443
Iteration 183/1000 | Loss: 0.00002443
Iteration 184/1000 | Loss: 0.00002442
Iteration 185/1000 | Loss: 0.00002441
Iteration 186/1000 | Loss: 0.00002441
Iteration 187/1000 | Loss: 0.00002441
Iteration 188/1000 | Loss: 0.00002441
Iteration 189/1000 | Loss: 0.00002440
Iteration 190/1000 | Loss: 0.00002440
Iteration 191/1000 | Loss: 0.00002439
Iteration 192/1000 | Loss: 0.00002439
Iteration 193/1000 | Loss: 0.00002439
Iteration 194/1000 | Loss: 0.00002439
Iteration 195/1000 | Loss: 0.00002439
Iteration 196/1000 | Loss: 0.00002438
Iteration 197/1000 | Loss: 0.00002438
Iteration 198/1000 | Loss: 0.00002438
Iteration 199/1000 | Loss: 0.00002437
Iteration 200/1000 | Loss: 0.00002453
Iteration 201/1000 | Loss: 0.00002452
Iteration 202/1000 | Loss: 0.00002451
Iteration 203/1000 | Loss: 0.00002435
Iteration 204/1000 | Loss: 0.00002434
Iteration 205/1000 | Loss: 0.00002434
Iteration 206/1000 | Loss: 0.00002434
Iteration 207/1000 | Loss: 0.00002433
Iteration 208/1000 | Loss: 0.00002433
Iteration 209/1000 | Loss: 0.00002433
Iteration 210/1000 | Loss: 0.00002433
Iteration 211/1000 | Loss: 0.00002433
Iteration 212/1000 | Loss: 0.00002433
Iteration 213/1000 | Loss: 0.00002433
Iteration 214/1000 | Loss: 0.00002433
Iteration 215/1000 | Loss: 0.00002432
Iteration 216/1000 | Loss: 0.00002432
Iteration 217/1000 | Loss: 0.00002432
Iteration 218/1000 | Loss: 0.00002432
Iteration 219/1000 | Loss: 0.00002432
Iteration 220/1000 | Loss: 0.00002432
Iteration 221/1000 | Loss: 0.00002432
Iteration 222/1000 | Loss: 0.00002431
Iteration 223/1000 | Loss: 0.00002431
Iteration 224/1000 | Loss: 0.00002431
Iteration 225/1000 | Loss: 0.00002431
Iteration 226/1000 | Loss: 0.00002431
Iteration 227/1000 | Loss: 0.00002431
Iteration 228/1000 | Loss: 0.00002431
Iteration 229/1000 | Loss: 0.00002431
Iteration 230/1000 | Loss: 0.00002430
Iteration 231/1000 | Loss: 0.00002430
Iteration 232/1000 | Loss: 0.00002430
Iteration 233/1000 | Loss: 0.00002429
Iteration 234/1000 | Loss: 0.00002429
Iteration 235/1000 | Loss: 0.00002429
Iteration 236/1000 | Loss: 0.00002429
Iteration 237/1000 | Loss: 0.00002429
Iteration 238/1000 | Loss: 0.00002429
Iteration 239/1000 | Loss: 0.00002429
Iteration 240/1000 | Loss: 0.00002429
Iteration 241/1000 | Loss: 0.00002429
Iteration 242/1000 | Loss: 0.00002429
Iteration 243/1000 | Loss: 0.00002429
Iteration 244/1000 | Loss: 0.00002428
Iteration 245/1000 | Loss: 0.00002428
Iteration 246/1000 | Loss: 0.00002428
Iteration 247/1000 | Loss: 0.00002428
Iteration 248/1000 | Loss: 0.00002428
Iteration 249/1000 | Loss: 0.00002428
Iteration 250/1000 | Loss: 0.00002428
Iteration 251/1000 | Loss: 0.00002428
Iteration 252/1000 | Loss: 0.00002428
Iteration 253/1000 | Loss: 0.00002437
Iteration 254/1000 | Loss: 0.00002436
Iteration 255/1000 | Loss: 0.00002436
Iteration 256/1000 | Loss: 0.00002436
Iteration 257/1000 | Loss: 0.00002436
Iteration 258/1000 | Loss: 0.00002435
Iteration 259/1000 | Loss: 0.00002435
Iteration 260/1000 | Loss: 0.00002435
Iteration 261/1000 | Loss: 0.00002434
Iteration 262/1000 | Loss: 0.00002434
Iteration 263/1000 | Loss: 0.00002434
Iteration 264/1000 | Loss: 0.00002428
Iteration 265/1000 | Loss: 0.00002427
Iteration 266/1000 | Loss: 0.00002427
Iteration 267/1000 | Loss: 0.00002427
Iteration 268/1000 | Loss: 0.00002427
Iteration 269/1000 | Loss: 0.00002427
Iteration 270/1000 | Loss: 0.00002426
Iteration 271/1000 | Loss: 0.00002426
Iteration 272/1000 | Loss: 0.00002426
Iteration 273/1000 | Loss: 0.00002426
Iteration 274/1000 | Loss: 0.00002426
Iteration 275/1000 | Loss: 0.00002426
Iteration 276/1000 | Loss: 0.00002426
Iteration 277/1000 | Loss: 0.00002426
Iteration 278/1000 | Loss: 0.00002426
Iteration 279/1000 | Loss: 0.00002426
Iteration 280/1000 | Loss: 0.00002426
Iteration 281/1000 | Loss: 0.00002426
Iteration 282/1000 | Loss: 0.00002426
Iteration 283/1000 | Loss: 0.00002426
Iteration 284/1000 | Loss: 0.00002426
Iteration 285/1000 | Loss: 0.00002426
Iteration 286/1000 | Loss: 0.00002426
Iteration 287/1000 | Loss: 0.00002426
Iteration 288/1000 | Loss: 0.00002426
Iteration 289/1000 | Loss: 0.00002426
Iteration 290/1000 | Loss: 0.00002426
Iteration 291/1000 | Loss: 0.00002426
Iteration 292/1000 | Loss: 0.00002426
Iteration 293/1000 | Loss: 0.00002426
Iteration 294/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.4259743440779857e-05, 2.4259743440779857e-05, 2.4259743440779857e-05, 2.4259743440779857e-05, 2.4259743440779857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4259743440779857e-05

Optimization complete. Final v2v error: 3.9665768146514893 mm

Highest mean error: 10.339065551757812 mm for frame 80

Lowest mean error: 3.306624174118042 mm for frame 204

Saving results

Total time: 338.0888321399689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542174
Iteration 2/25 | Loss: 0.00129974
Iteration 3/25 | Loss: 0.00104108
Iteration 4/25 | Loss: 0.00101829
Iteration 5/25 | Loss: 0.00101301
Iteration 6/25 | Loss: 0.00101143
Iteration 7/25 | Loss: 0.00101136
Iteration 8/25 | Loss: 0.00101058
Iteration 9/25 | Loss: 0.00101513
Iteration 10/25 | Loss: 0.00101315
Iteration 11/25 | Loss: 0.00100858
Iteration 12/25 | Loss: 0.00100817
Iteration 13/25 | Loss: 0.00100789
Iteration 14/25 | Loss: 0.00100780
Iteration 15/25 | Loss: 0.00100770
Iteration 16/25 | Loss: 0.00100754
Iteration 17/25 | Loss: 0.00100732
Iteration 18/25 | Loss: 0.00100732
Iteration 19/25 | Loss: 0.00100732
Iteration 20/25 | Loss: 0.00100732
Iteration 21/25 | Loss: 0.00100732
Iteration 22/25 | Loss: 0.00100732
Iteration 23/25 | Loss: 0.00100732
Iteration 24/25 | Loss: 0.00100732
Iteration 25/25 | Loss: 0.00100732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57439041
Iteration 2/25 | Loss: 0.00099583
Iteration 3/25 | Loss: 0.00098860
Iteration 4/25 | Loss: 0.00098860
Iteration 5/25 | Loss: 0.00098859
Iteration 6/25 | Loss: 0.00098859
Iteration 7/25 | Loss: 0.00098859
Iteration 8/25 | Loss: 0.00098859
Iteration 9/25 | Loss: 0.00098859
Iteration 10/25 | Loss: 0.00098859
Iteration 11/25 | Loss: 0.00098859
Iteration 12/25 | Loss: 0.00098859
Iteration 13/25 | Loss: 0.00098859
Iteration 14/25 | Loss: 0.00098859
Iteration 15/25 | Loss: 0.00098859
Iteration 16/25 | Loss: 0.00098859
Iteration 17/25 | Loss: 0.00098859
Iteration 18/25 | Loss: 0.00098859
Iteration 19/25 | Loss: 0.00098859
Iteration 20/25 | Loss: 0.00098859
Iteration 21/25 | Loss: 0.00098859
Iteration 22/25 | Loss: 0.00098859
Iteration 23/25 | Loss: 0.00098859
Iteration 24/25 | Loss: 0.00098859
Iteration 25/25 | Loss: 0.00098859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098859
Iteration 2/1000 | Loss: 0.00003201
Iteration 3/1000 | Loss: 0.00002027
Iteration 4/1000 | Loss: 0.00001757
Iteration 5/1000 | Loss: 0.00008946
Iteration 6/1000 | Loss: 0.00006870
Iteration 7/1000 | Loss: 0.00005862
Iteration 8/1000 | Loss: 0.00024480
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00007476
Iteration 12/1000 | Loss: 0.00002032
Iteration 13/1000 | Loss: 0.00001783
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001247
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001243
Iteration 61/1000 | Loss: 0.00001243
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.242363578057848e-05, 1.242363578057848e-05, 1.242363578057848e-05, 1.242363578057848e-05, 1.242363578057848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.242363578057848e-05

Optimization complete. Final v2v error: 2.97727632522583 mm

Highest mean error: 9.724769592285156 mm for frame 130

Lowest mean error: 2.3314685821533203 mm for frame 0

Saving results

Total time: 68.65456438064575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867633
Iteration 2/25 | Loss: 0.00116704
Iteration 3/25 | Loss: 0.00108206
Iteration 4/25 | Loss: 0.00107068
Iteration 5/25 | Loss: 0.00106679
Iteration 6/25 | Loss: 0.00106611
Iteration 7/25 | Loss: 0.00106611
Iteration 8/25 | Loss: 0.00106611
Iteration 9/25 | Loss: 0.00106611
Iteration 10/25 | Loss: 0.00106611
Iteration 11/25 | Loss: 0.00106611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010661091655492783, 0.0010661091655492783, 0.0010661091655492783, 0.0010661091655492783, 0.0010661091655492783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010661091655492783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36405623
Iteration 2/25 | Loss: 0.00089419
Iteration 3/25 | Loss: 0.00089419
Iteration 4/25 | Loss: 0.00089419
Iteration 5/25 | Loss: 0.00089418
Iteration 6/25 | Loss: 0.00089418
Iteration 7/25 | Loss: 0.00089418
Iteration 8/25 | Loss: 0.00089418
Iteration 9/25 | Loss: 0.00089418
Iteration 10/25 | Loss: 0.00089418
Iteration 11/25 | Loss: 0.00089418
Iteration 12/25 | Loss: 0.00089418
Iteration 13/25 | Loss: 0.00089418
Iteration 14/25 | Loss: 0.00089418
Iteration 15/25 | Loss: 0.00089418
Iteration 16/25 | Loss: 0.00089418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008941831183619797, 0.0008941831183619797, 0.0008941831183619797, 0.0008941831183619797, 0.0008941831183619797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008941831183619797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089418
Iteration 2/1000 | Loss: 0.00002546
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001631
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001467
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001404
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001363
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001360
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001345
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001341
Iteration 59/1000 | Loss: 0.00001341
Iteration 60/1000 | Loss: 0.00001341
Iteration 61/1000 | Loss: 0.00001340
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001340
Iteration 66/1000 | Loss: 0.00001340
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001339
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001333
Iteration 110/1000 | Loss: 0.00001333
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001332
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001332
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001331
Iteration 122/1000 | Loss: 0.00001331
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001328
Iteration 148/1000 | Loss: 0.00001328
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001328
Iteration 157/1000 | Loss: 0.00001328
Iteration 158/1000 | Loss: 0.00001328
Iteration 159/1000 | Loss: 0.00001328
Iteration 160/1000 | Loss: 0.00001328
Iteration 161/1000 | Loss: 0.00001328
Iteration 162/1000 | Loss: 0.00001328
Iteration 163/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.328185862803366e-05, 1.328185862803366e-05, 1.328185862803366e-05, 1.328185862803366e-05, 1.328185862803366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.328185862803366e-05

Optimization complete. Final v2v error: 3.150840997695923 mm

Highest mean error: 3.6066513061523438 mm for frame 179

Lowest mean error: 2.6479902267456055 mm for frame 10

Saving results

Total time: 34.24949336051941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595890
Iteration 2/25 | Loss: 0.00126017
Iteration 3/25 | Loss: 0.00107013
Iteration 4/25 | Loss: 0.00105610
Iteration 5/25 | Loss: 0.00105411
Iteration 6/25 | Loss: 0.00105354
Iteration 7/25 | Loss: 0.00105354
Iteration 8/25 | Loss: 0.00105354
Iteration 9/25 | Loss: 0.00105354
Iteration 10/25 | Loss: 0.00105354
Iteration 11/25 | Loss: 0.00105354
Iteration 12/25 | Loss: 0.00105354
Iteration 13/25 | Loss: 0.00105354
Iteration 14/25 | Loss: 0.00105354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010535428300499916, 0.0010535428300499916, 0.0010535428300499916, 0.0010535428300499916, 0.0010535428300499916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010535428300499916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99637461
Iteration 2/25 | Loss: 0.00081862
Iteration 3/25 | Loss: 0.00081860
Iteration 4/25 | Loss: 0.00081860
Iteration 5/25 | Loss: 0.00081860
Iteration 6/25 | Loss: 0.00081860
Iteration 7/25 | Loss: 0.00081859
Iteration 8/25 | Loss: 0.00081859
Iteration 9/25 | Loss: 0.00081859
Iteration 10/25 | Loss: 0.00081859
Iteration 11/25 | Loss: 0.00081859
Iteration 12/25 | Loss: 0.00081859
Iteration 13/25 | Loss: 0.00081859
Iteration 14/25 | Loss: 0.00081859
Iteration 15/25 | Loss: 0.00081859
Iteration 16/25 | Loss: 0.00081859
Iteration 17/25 | Loss: 0.00081859
Iteration 18/25 | Loss: 0.00081859
Iteration 19/25 | Loss: 0.00081859
Iteration 20/25 | Loss: 0.00081859
Iteration 21/25 | Loss: 0.00081859
Iteration 22/25 | Loss: 0.00081859
Iteration 23/25 | Loss: 0.00081859
Iteration 24/25 | Loss: 0.00081859
Iteration 25/25 | Loss: 0.00081859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008185935439541936, 0.0008185935439541936, 0.0008185935439541936, 0.0008185935439541936, 0.0008185935439541936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008185935439541936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081859
Iteration 2/1000 | Loss: 0.00004618
Iteration 3/1000 | Loss: 0.00002541
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001693
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001463
Iteration 12/1000 | Loss: 0.00001446
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001430
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001428
Iteration 18/1000 | Loss: 0.00001425
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001413
Iteration 26/1000 | Loss: 0.00001412
Iteration 27/1000 | Loss: 0.00001411
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001399
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001393
Iteration 113/1000 | Loss: 0.00001393
Iteration 114/1000 | Loss: 0.00001393
Iteration 115/1000 | Loss: 0.00001393
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001392
Iteration 119/1000 | Loss: 0.00001392
Iteration 120/1000 | Loss: 0.00001392
Iteration 121/1000 | Loss: 0.00001392
Iteration 122/1000 | Loss: 0.00001392
Iteration 123/1000 | Loss: 0.00001392
Iteration 124/1000 | Loss: 0.00001392
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001391
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001390
Iteration 144/1000 | Loss: 0.00001390
Iteration 145/1000 | Loss: 0.00001390
Iteration 146/1000 | Loss: 0.00001390
Iteration 147/1000 | Loss: 0.00001390
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001389
Iteration 158/1000 | Loss: 0.00001389
Iteration 159/1000 | Loss: 0.00001389
Iteration 160/1000 | Loss: 0.00001389
Iteration 161/1000 | Loss: 0.00001389
Iteration 162/1000 | Loss: 0.00001389
Iteration 163/1000 | Loss: 0.00001389
Iteration 164/1000 | Loss: 0.00001389
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001389
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001389
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001389
Iteration 178/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3886533452023286e-05, 1.3886533452023286e-05, 1.3886533452023286e-05, 1.3886533452023286e-05, 1.3886533452023286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3886533452023286e-05

Optimization complete. Final v2v error: 3.2193984985351562 mm

Highest mean error: 3.9026713371276855 mm for frame 87

Lowest mean error: 2.5560691356658936 mm for frame 1

Saving results

Total time: 37.90222930908203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506386
Iteration 2/25 | Loss: 0.00127740
Iteration 3/25 | Loss: 0.00105175
Iteration 4/25 | Loss: 0.00103729
Iteration 5/25 | Loss: 0.00103400
Iteration 6/25 | Loss: 0.00103306
Iteration 7/25 | Loss: 0.00103306
Iteration 8/25 | Loss: 0.00103306
Iteration 9/25 | Loss: 0.00103306
Iteration 10/25 | Loss: 0.00103306
Iteration 11/25 | Loss: 0.00103306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010330559452995658, 0.0010330559452995658, 0.0010330559452995658, 0.0010330559452995658, 0.0010330559452995658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010330559452995658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26973045
Iteration 2/25 | Loss: 0.00088274
Iteration 3/25 | Loss: 0.00088274
Iteration 4/25 | Loss: 0.00088274
Iteration 5/25 | Loss: 0.00088274
Iteration 6/25 | Loss: 0.00088274
Iteration 7/25 | Loss: 0.00088274
Iteration 8/25 | Loss: 0.00088274
Iteration 9/25 | Loss: 0.00088274
Iteration 10/25 | Loss: 0.00088274
Iteration 11/25 | Loss: 0.00088274
Iteration 12/25 | Loss: 0.00088274
Iteration 13/25 | Loss: 0.00088274
Iteration 14/25 | Loss: 0.00088274
Iteration 15/25 | Loss: 0.00088274
Iteration 16/25 | Loss: 0.00088274
Iteration 17/25 | Loss: 0.00088274
Iteration 18/25 | Loss: 0.00088274
Iteration 19/25 | Loss: 0.00088274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008827359415590763, 0.0008827359415590763, 0.0008827359415590763, 0.0008827359415590763, 0.0008827359415590763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008827359415590763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088274
Iteration 2/1000 | Loss: 0.00004432
Iteration 3/1000 | Loss: 0.00003379
Iteration 4/1000 | Loss: 0.00003063
Iteration 5/1000 | Loss: 0.00002930
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002796
Iteration 8/1000 | Loss: 0.00002746
Iteration 9/1000 | Loss: 0.00002717
Iteration 10/1000 | Loss: 0.00002694
Iteration 11/1000 | Loss: 0.00002676
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002644
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00002626
Iteration 16/1000 | Loss: 0.00002613
Iteration 17/1000 | Loss: 0.00002602
Iteration 18/1000 | Loss: 0.00002599
Iteration 19/1000 | Loss: 0.00002599
Iteration 20/1000 | Loss: 0.00002599
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00002599
Iteration 23/1000 | Loss: 0.00002599
Iteration 24/1000 | Loss: 0.00002599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [2.5988832931034267e-05, 2.5988832931034267e-05, 2.5988832931034267e-05, 2.5988832931034267e-05, 2.5988832931034267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5988832931034267e-05

Optimization complete. Final v2v error: 4.031244277954102 mm

Highest mean error: 4.44383430480957 mm for frame 105

Lowest mean error: 3.637106418609619 mm for frame 80

Saving results

Total time: 36.01860761642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853428
Iteration 2/25 | Loss: 0.00126526
Iteration 3/25 | Loss: 0.00104070
Iteration 4/25 | Loss: 0.00101001
Iteration 5/25 | Loss: 0.00100707
Iteration 6/25 | Loss: 0.00100570
Iteration 7/25 | Loss: 0.00100446
Iteration 8/25 | Loss: 0.00100398
Iteration 9/25 | Loss: 0.00100466
Iteration 10/25 | Loss: 0.00100375
Iteration 11/25 | Loss: 0.00100347
Iteration 12/25 | Loss: 0.00100340
Iteration 13/25 | Loss: 0.00100340
Iteration 14/25 | Loss: 0.00100340
Iteration 15/25 | Loss: 0.00100339
Iteration 16/25 | Loss: 0.00100339
Iteration 17/25 | Loss: 0.00100339
Iteration 18/25 | Loss: 0.00100339
Iteration 19/25 | Loss: 0.00100339
Iteration 20/25 | Loss: 0.00100339
Iteration 21/25 | Loss: 0.00100339
Iteration 22/25 | Loss: 0.00100339
Iteration 23/25 | Loss: 0.00100338
Iteration 24/25 | Loss: 0.00100338
Iteration 25/25 | Loss: 0.00100338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.49351430
Iteration 2/25 | Loss: 0.00085580
Iteration 3/25 | Loss: 0.00085580
Iteration 4/25 | Loss: 0.00085580
Iteration 5/25 | Loss: 0.00085580
Iteration 6/25 | Loss: 0.00085580
Iteration 7/25 | Loss: 0.00085580
Iteration 8/25 | Loss: 0.00085580
Iteration 9/25 | Loss: 0.00085580
Iteration 10/25 | Loss: 0.00085580
Iteration 11/25 | Loss: 0.00085580
Iteration 12/25 | Loss: 0.00085580
Iteration 13/25 | Loss: 0.00085580
Iteration 14/25 | Loss: 0.00085580
Iteration 15/25 | Loss: 0.00085580
Iteration 16/25 | Loss: 0.00085580
Iteration 17/25 | Loss: 0.00085580
Iteration 18/25 | Loss: 0.00085580
Iteration 19/25 | Loss: 0.00085580
Iteration 20/25 | Loss: 0.00085580
Iteration 21/25 | Loss: 0.00085580
Iteration 22/25 | Loss: 0.00085580
Iteration 23/25 | Loss: 0.00085580
Iteration 24/25 | Loss: 0.00085580
Iteration 25/25 | Loss: 0.00085580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085580
Iteration 2/1000 | Loss: 0.00002241
Iteration 3/1000 | Loss: 0.00001744
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001423
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001247
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001237
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001214
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001213
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001213
Iteration 139/1000 | Loss: 0.00001213
Iteration 140/1000 | Loss: 0.00001213
Iteration 141/1000 | Loss: 0.00001213
Iteration 142/1000 | Loss: 0.00001213
Iteration 143/1000 | Loss: 0.00001213
Iteration 144/1000 | Loss: 0.00001213
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.2125825378461741e-05, 1.2125825378461741e-05, 1.2125825378461741e-05, 1.2125825378461741e-05, 1.2125825378461741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2125825378461741e-05

Optimization complete. Final v2v error: 2.984877347946167 mm

Highest mean error: 8.188630104064941 mm for frame 56

Lowest mean error: 2.4194321632385254 mm for frame 0

Saving results

Total time: 50.33651328086853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844357
Iteration 2/25 | Loss: 0.00125648
Iteration 3/25 | Loss: 0.00096914
Iteration 4/25 | Loss: 0.00094856
Iteration 5/25 | Loss: 0.00094437
Iteration 6/25 | Loss: 0.00094280
Iteration 7/25 | Loss: 0.00094266
Iteration 8/25 | Loss: 0.00094266
Iteration 9/25 | Loss: 0.00094266
Iteration 10/25 | Loss: 0.00094266
Iteration 11/25 | Loss: 0.00094266
Iteration 12/25 | Loss: 0.00094266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009426621836610138, 0.0009426621836610138, 0.0009426621836610138, 0.0009426621836610138, 0.0009426621836610138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009426621836610138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14266098
Iteration 2/25 | Loss: 0.00089575
Iteration 3/25 | Loss: 0.00089575
Iteration 4/25 | Loss: 0.00089575
Iteration 5/25 | Loss: 0.00089575
Iteration 6/25 | Loss: 0.00089575
Iteration 7/25 | Loss: 0.00089575
Iteration 8/25 | Loss: 0.00089575
Iteration 9/25 | Loss: 0.00089575
Iteration 10/25 | Loss: 0.00089575
Iteration 11/25 | Loss: 0.00089575
Iteration 12/25 | Loss: 0.00089575
Iteration 13/25 | Loss: 0.00089575
Iteration 14/25 | Loss: 0.00089575
Iteration 15/25 | Loss: 0.00089575
Iteration 16/25 | Loss: 0.00089575
Iteration 17/25 | Loss: 0.00089575
Iteration 18/25 | Loss: 0.00089575
Iteration 19/25 | Loss: 0.00089575
Iteration 20/25 | Loss: 0.00089575
Iteration 21/25 | Loss: 0.00089575
Iteration 22/25 | Loss: 0.00089575
Iteration 23/25 | Loss: 0.00089575
Iteration 24/25 | Loss: 0.00089575
Iteration 25/25 | Loss: 0.00089575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089575
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001890
Iteration 5/1000 | Loss: 0.00001757
Iteration 6/1000 | Loss: 0.00001648
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001482
Iteration 13/1000 | Loss: 0.00001468
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001456
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001454
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001441
Iteration 28/1000 | Loss: 0.00001441
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001437
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001432
Iteration 47/1000 | Loss: 0.00001432
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001423
Iteration 86/1000 | Loss: 0.00001423
Iteration 87/1000 | Loss: 0.00001423
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001422
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001420
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001417
Iteration 121/1000 | Loss: 0.00001417
Iteration 122/1000 | Loss: 0.00001417
Iteration 123/1000 | Loss: 0.00001417
Iteration 124/1000 | Loss: 0.00001417
Iteration 125/1000 | Loss: 0.00001417
Iteration 126/1000 | Loss: 0.00001417
Iteration 127/1000 | Loss: 0.00001417
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001417
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001415
Iteration 141/1000 | Loss: 0.00001415
Iteration 142/1000 | Loss: 0.00001415
Iteration 143/1000 | Loss: 0.00001415
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001415
Iteration 148/1000 | Loss: 0.00001415
Iteration 149/1000 | Loss: 0.00001415
Iteration 150/1000 | Loss: 0.00001415
Iteration 151/1000 | Loss: 0.00001415
Iteration 152/1000 | Loss: 0.00001415
Iteration 153/1000 | Loss: 0.00001415
Iteration 154/1000 | Loss: 0.00001415
Iteration 155/1000 | Loss: 0.00001415
Iteration 156/1000 | Loss: 0.00001415
Iteration 157/1000 | Loss: 0.00001415
Iteration 158/1000 | Loss: 0.00001415
Iteration 159/1000 | Loss: 0.00001415
Iteration 160/1000 | Loss: 0.00001415
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001415
Iteration 166/1000 | Loss: 0.00001415
Iteration 167/1000 | Loss: 0.00001415
Iteration 168/1000 | Loss: 0.00001415
Iteration 169/1000 | Loss: 0.00001415
Iteration 170/1000 | Loss: 0.00001415
Iteration 171/1000 | Loss: 0.00001415
Iteration 172/1000 | Loss: 0.00001415
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001415
Iteration 188/1000 | Loss: 0.00001415
Iteration 189/1000 | Loss: 0.00001415
Iteration 190/1000 | Loss: 0.00001415
Iteration 191/1000 | Loss: 0.00001415
Iteration 192/1000 | Loss: 0.00001415
Iteration 193/1000 | Loss: 0.00001415
Iteration 194/1000 | Loss: 0.00001415
Iteration 195/1000 | Loss: 0.00001415
Iteration 196/1000 | Loss: 0.00001415
Iteration 197/1000 | Loss: 0.00001415
Iteration 198/1000 | Loss: 0.00001415
Iteration 199/1000 | Loss: 0.00001415
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.4153466509014834e-05, 1.4153466509014834e-05, 1.4153466509014834e-05, 1.4153466509014834e-05, 1.4153466509014834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4153466509014834e-05

Optimization complete. Final v2v error: 2.936169385910034 mm

Highest mean error: 4.405454635620117 mm for frame 62

Lowest mean error: 2.2132678031921387 mm for frame 91

Saving results

Total time: 38.34102702140808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440588
Iteration 2/25 | Loss: 0.00114389
Iteration 3/25 | Loss: 0.00104234
Iteration 4/25 | Loss: 0.00102139
Iteration 5/25 | Loss: 0.00101266
Iteration 6/25 | Loss: 0.00101012
Iteration 7/25 | Loss: 0.00100986
Iteration 8/25 | Loss: 0.00100986
Iteration 9/25 | Loss: 0.00100986
Iteration 10/25 | Loss: 0.00100986
Iteration 11/25 | Loss: 0.00100986
Iteration 12/25 | Loss: 0.00100986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010098619386553764, 0.0010098619386553764, 0.0010098619386553764, 0.0010098619386553764, 0.0010098619386553764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010098619386553764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36468923
Iteration 2/25 | Loss: 0.00098092
Iteration 3/25 | Loss: 0.00098092
Iteration 4/25 | Loss: 0.00098092
Iteration 5/25 | Loss: 0.00098092
Iteration 6/25 | Loss: 0.00098091
Iteration 7/25 | Loss: 0.00098091
Iteration 8/25 | Loss: 0.00098091
Iteration 9/25 | Loss: 0.00098091
Iteration 10/25 | Loss: 0.00098091
Iteration 11/25 | Loss: 0.00098091
Iteration 12/25 | Loss: 0.00098091
Iteration 13/25 | Loss: 0.00098091
Iteration 14/25 | Loss: 0.00098091
Iteration 15/25 | Loss: 0.00098091
Iteration 16/25 | Loss: 0.00098091
Iteration 17/25 | Loss: 0.00098091
Iteration 18/25 | Loss: 0.00098091
Iteration 19/25 | Loss: 0.00098091
Iteration 20/25 | Loss: 0.00098091
Iteration 21/25 | Loss: 0.00098091
Iteration 22/25 | Loss: 0.00098091
Iteration 23/25 | Loss: 0.00098091
Iteration 24/25 | Loss: 0.00098091
Iteration 25/25 | Loss: 0.00098091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098091
Iteration 2/1000 | Loss: 0.00002319
Iteration 3/1000 | Loss: 0.00001846
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001542
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001441
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001441
Iteration 14/1000 | Loss: 0.00001440
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001439
Iteration 17/1000 | Loss: 0.00001439
Iteration 18/1000 | Loss: 0.00001438
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001436
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001435
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001433
Iteration 25/1000 | Loss: 0.00001432
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001428
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001428
Iteration 31/1000 | Loss: 0.00001428
Iteration 32/1000 | Loss: 0.00001428
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001425
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001415
Iteration 51/1000 | Loss: 0.00001415
Iteration 52/1000 | Loss: 0.00001415
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001412
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.4116160855337512e-05, 1.4116160855337512e-05, 1.4116160855337512e-05, 1.4116160855337512e-05, 1.4116160855337512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4116160855337512e-05

Optimization complete. Final v2v error: 3.211887836456299 mm

Highest mean error: 3.6356709003448486 mm for frame 79

Lowest mean error: 2.981865882873535 mm for frame 3

Saving results

Total time: 28.590439796447754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492036
Iteration 2/25 | Loss: 0.00131724
Iteration 3/25 | Loss: 0.00107062
Iteration 4/25 | Loss: 0.00106155
Iteration 5/25 | Loss: 0.00106052
Iteration 6/25 | Loss: 0.00106052
Iteration 7/25 | Loss: 0.00106052
Iteration 8/25 | Loss: 0.00106052
Iteration 9/25 | Loss: 0.00106052
Iteration 10/25 | Loss: 0.00106052
Iteration 11/25 | Loss: 0.00106052
Iteration 12/25 | Loss: 0.00106052
Iteration 13/25 | Loss: 0.00106052
Iteration 14/25 | Loss: 0.00106052
Iteration 15/25 | Loss: 0.00106052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00106052216142416, 0.00106052216142416, 0.00106052216142416, 0.00106052216142416, 0.00106052216142416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00106052216142416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28642571
Iteration 2/25 | Loss: 0.00087691
Iteration 3/25 | Loss: 0.00087691
Iteration 4/25 | Loss: 0.00087691
Iteration 5/25 | Loss: 0.00087691
Iteration 6/25 | Loss: 0.00087691
Iteration 7/25 | Loss: 0.00087691
Iteration 8/25 | Loss: 0.00087691
Iteration 9/25 | Loss: 0.00087691
Iteration 10/25 | Loss: 0.00087691
Iteration 11/25 | Loss: 0.00087691
Iteration 12/25 | Loss: 0.00087691
Iteration 13/25 | Loss: 0.00087691
Iteration 14/25 | Loss: 0.00087691
Iteration 15/25 | Loss: 0.00087691
Iteration 16/25 | Loss: 0.00087691
Iteration 17/25 | Loss: 0.00087691
Iteration 18/25 | Loss: 0.00087691
Iteration 19/25 | Loss: 0.00087691
Iteration 20/25 | Loss: 0.00087691
Iteration 21/25 | Loss: 0.00087691
Iteration 22/25 | Loss: 0.00087691
Iteration 23/25 | Loss: 0.00087691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008769058622419834, 0.0008769058622419834, 0.0008769058622419834, 0.0008769058622419834, 0.0008769058622419834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008769058622419834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087691
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00002179
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002108
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002071
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002060
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00002058
Iteration 15/1000 | Loss: 0.00002058
Iteration 16/1000 | Loss: 0.00002058
Iteration 17/1000 | Loss: 0.00002058
Iteration 18/1000 | Loss: 0.00002058
Iteration 19/1000 | Loss: 0.00002058
Iteration 20/1000 | Loss: 0.00002058
Iteration 21/1000 | Loss: 0.00002058
Iteration 22/1000 | Loss: 0.00002057
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002056
Iteration 26/1000 | Loss: 0.00002055
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002053
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002051
Iteration 34/1000 | Loss: 0.00002051
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002047
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002041
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002029
Iteration 50/1000 | Loss: 0.00002029
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002026
Iteration 54/1000 | Loss: 0.00002026
Iteration 55/1000 | Loss: 0.00002026
Iteration 56/1000 | Loss: 0.00002026
Iteration 57/1000 | Loss: 0.00002025
Iteration 58/1000 | Loss: 0.00002025
Iteration 59/1000 | Loss: 0.00002024
Iteration 60/1000 | Loss: 0.00002024
Iteration 61/1000 | Loss: 0.00002023
Iteration 62/1000 | Loss: 0.00002023
Iteration 63/1000 | Loss: 0.00002023
Iteration 64/1000 | Loss: 0.00002023
Iteration 65/1000 | Loss: 0.00002022
Iteration 66/1000 | Loss: 0.00002021
Iteration 67/1000 | Loss: 0.00002021
Iteration 68/1000 | Loss: 0.00002021
Iteration 69/1000 | Loss: 0.00002020
Iteration 70/1000 | Loss: 0.00002020
Iteration 71/1000 | Loss: 0.00002020
Iteration 72/1000 | Loss: 0.00002020
Iteration 73/1000 | Loss: 0.00002019
Iteration 74/1000 | Loss: 0.00002019
Iteration 75/1000 | Loss: 0.00002019
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002018
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002017
Iteration 84/1000 | Loss: 0.00002017
Iteration 85/1000 | Loss: 0.00002017
Iteration 86/1000 | Loss: 0.00002017
Iteration 87/1000 | Loss: 0.00002017
Iteration 88/1000 | Loss: 0.00002017
Iteration 89/1000 | Loss: 0.00002017
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002016
Iteration 92/1000 | Loss: 0.00002016
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002015
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002012
Iteration 99/1000 | Loss: 0.00002012
Iteration 100/1000 | Loss: 0.00002012
Iteration 101/1000 | Loss: 0.00002011
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002010
Iteration 104/1000 | Loss: 0.00002010
Iteration 105/1000 | Loss: 0.00002010
Iteration 106/1000 | Loss: 0.00002009
Iteration 107/1000 | Loss: 0.00002009
Iteration 108/1000 | Loss: 0.00002009
Iteration 109/1000 | Loss: 0.00002008
Iteration 110/1000 | Loss: 0.00002008
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00002007
Iteration 113/1000 | Loss: 0.00002007
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002007
Iteration 121/1000 | Loss: 0.00002007
Iteration 122/1000 | Loss: 0.00002007
Iteration 123/1000 | Loss: 0.00002007
Iteration 124/1000 | Loss: 0.00002006
Iteration 125/1000 | Loss: 0.00002006
Iteration 126/1000 | Loss: 0.00002006
Iteration 127/1000 | Loss: 0.00002006
Iteration 128/1000 | Loss: 0.00002006
Iteration 129/1000 | Loss: 0.00002006
Iteration 130/1000 | Loss: 0.00002006
Iteration 131/1000 | Loss: 0.00002006
Iteration 132/1000 | Loss: 0.00002006
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00002006
Iteration 135/1000 | Loss: 0.00002006
Iteration 136/1000 | Loss: 0.00002006
Iteration 137/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.005563146667555e-05, 2.005563146667555e-05, 2.005563146667555e-05, 2.005563146667555e-05, 2.005563146667555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.005563146667555e-05

Optimization complete. Final v2v error: 3.5111844539642334 mm

Highest mean error: 3.964585304260254 mm for frame 70

Lowest mean error: 3.184459924697876 mm for frame 200

Saving results

Total time: 37.8753228187561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476717
Iteration 2/25 | Loss: 0.00123993
Iteration 3/25 | Loss: 0.00102331
Iteration 4/25 | Loss: 0.00100744
Iteration 5/25 | Loss: 0.00100152
Iteration 6/25 | Loss: 0.00099950
Iteration 7/25 | Loss: 0.00099950
Iteration 8/25 | Loss: 0.00099950
Iteration 9/25 | Loss: 0.00099950
Iteration 10/25 | Loss: 0.00099950
Iteration 11/25 | Loss: 0.00099950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000999501789920032, 0.000999501789920032, 0.000999501789920032, 0.000999501789920032, 0.000999501789920032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000999501789920032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75210994
Iteration 2/25 | Loss: 0.00081342
Iteration 3/25 | Loss: 0.00081342
Iteration 4/25 | Loss: 0.00081342
Iteration 5/25 | Loss: 0.00081342
Iteration 6/25 | Loss: 0.00081342
Iteration 7/25 | Loss: 0.00081342
Iteration 8/25 | Loss: 0.00081342
Iteration 9/25 | Loss: 0.00081342
Iteration 10/25 | Loss: 0.00081342
Iteration 11/25 | Loss: 0.00081342
Iteration 12/25 | Loss: 0.00081342
Iteration 13/25 | Loss: 0.00081342
Iteration 14/25 | Loss: 0.00081342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008134174859151244, 0.0008134174859151244, 0.0008134174859151244, 0.0008134174859151244, 0.0008134174859151244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008134174859151244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081342
Iteration 2/1000 | Loss: 0.00003203
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002255
Iteration 6/1000 | Loss: 0.00002191
Iteration 7/1000 | Loss: 0.00002139
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002013
Iteration 12/1000 | Loss: 0.00002002
Iteration 13/1000 | Loss: 0.00002001
Iteration 14/1000 | Loss: 0.00002001
Iteration 15/1000 | Loss: 0.00001984
Iteration 16/1000 | Loss: 0.00001976
Iteration 17/1000 | Loss: 0.00001960
Iteration 18/1000 | Loss: 0.00001957
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001942
Iteration 27/1000 | Loss: 0.00001942
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001938
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001935
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001907
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001879
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001863
Iteration 53/1000 | Loss: 0.00001862
Iteration 54/1000 | Loss: 0.00001862
Iteration 55/1000 | Loss: 0.00001862
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001859
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001858
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001858
Iteration 84/1000 | Loss: 0.00001858
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001853
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001852
Iteration 95/1000 | Loss: 0.00001852
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001849
Iteration 111/1000 | Loss: 0.00001849
Iteration 112/1000 | Loss: 0.00001849
Iteration 113/1000 | Loss: 0.00001849
Iteration 114/1000 | Loss: 0.00001849
Iteration 115/1000 | Loss: 0.00001849
Iteration 116/1000 | Loss: 0.00001848
Iteration 117/1000 | Loss: 0.00001848
Iteration 118/1000 | Loss: 0.00001848
Iteration 119/1000 | Loss: 0.00001848
Iteration 120/1000 | Loss: 0.00001848
Iteration 121/1000 | Loss: 0.00001848
Iteration 122/1000 | Loss: 0.00001848
Iteration 123/1000 | Loss: 0.00001847
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001847
Iteration 126/1000 | Loss: 0.00001847
Iteration 127/1000 | Loss: 0.00001847
Iteration 128/1000 | Loss: 0.00001847
Iteration 129/1000 | Loss: 0.00001846
Iteration 130/1000 | Loss: 0.00001846
Iteration 131/1000 | Loss: 0.00001846
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001844
Iteration 135/1000 | Loss: 0.00001844
Iteration 136/1000 | Loss: 0.00001844
Iteration 137/1000 | Loss: 0.00001844
Iteration 138/1000 | Loss: 0.00001844
Iteration 139/1000 | Loss: 0.00001844
Iteration 140/1000 | Loss: 0.00001844
Iteration 141/1000 | Loss: 0.00001844
Iteration 142/1000 | Loss: 0.00001844
Iteration 143/1000 | Loss: 0.00001844
Iteration 144/1000 | Loss: 0.00001844
Iteration 145/1000 | Loss: 0.00001844
Iteration 146/1000 | Loss: 0.00001844
Iteration 147/1000 | Loss: 0.00001843
Iteration 148/1000 | Loss: 0.00001843
Iteration 149/1000 | Loss: 0.00001842
Iteration 150/1000 | Loss: 0.00001842
Iteration 151/1000 | Loss: 0.00001842
Iteration 152/1000 | Loss: 0.00001842
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Iteration 158/1000 | Loss: 0.00001841
Iteration 159/1000 | Loss: 0.00001841
Iteration 160/1000 | Loss: 0.00001841
Iteration 161/1000 | Loss: 0.00001841
Iteration 162/1000 | Loss: 0.00001841
Iteration 163/1000 | Loss: 0.00001840
Iteration 164/1000 | Loss: 0.00001840
Iteration 165/1000 | Loss: 0.00001840
Iteration 166/1000 | Loss: 0.00001840
Iteration 167/1000 | Loss: 0.00001840
Iteration 168/1000 | Loss: 0.00001840
Iteration 169/1000 | Loss: 0.00001840
Iteration 170/1000 | Loss: 0.00001840
Iteration 171/1000 | Loss: 0.00001840
Iteration 172/1000 | Loss: 0.00001840
Iteration 173/1000 | Loss: 0.00001840
Iteration 174/1000 | Loss: 0.00001840
Iteration 175/1000 | Loss: 0.00001839
Iteration 176/1000 | Loss: 0.00001839
Iteration 177/1000 | Loss: 0.00001839
Iteration 178/1000 | Loss: 0.00001839
Iteration 179/1000 | Loss: 0.00001839
Iteration 180/1000 | Loss: 0.00001838
Iteration 181/1000 | Loss: 0.00001838
Iteration 182/1000 | Loss: 0.00001838
Iteration 183/1000 | Loss: 0.00001838
Iteration 184/1000 | Loss: 0.00001838
Iteration 185/1000 | Loss: 0.00001838
Iteration 186/1000 | Loss: 0.00001838
Iteration 187/1000 | Loss: 0.00001838
Iteration 188/1000 | Loss: 0.00001838
Iteration 189/1000 | Loss: 0.00001838
Iteration 190/1000 | Loss: 0.00001838
Iteration 191/1000 | Loss: 0.00001837
Iteration 192/1000 | Loss: 0.00001837
Iteration 193/1000 | Loss: 0.00001837
Iteration 194/1000 | Loss: 0.00001837
Iteration 195/1000 | Loss: 0.00001837
Iteration 196/1000 | Loss: 0.00001837
Iteration 197/1000 | Loss: 0.00001837
Iteration 198/1000 | Loss: 0.00001837
Iteration 199/1000 | Loss: 0.00001837
Iteration 200/1000 | Loss: 0.00001837
Iteration 201/1000 | Loss: 0.00001837
Iteration 202/1000 | Loss: 0.00001837
Iteration 203/1000 | Loss: 0.00001837
Iteration 204/1000 | Loss: 0.00001836
Iteration 205/1000 | Loss: 0.00001836
Iteration 206/1000 | Loss: 0.00001836
Iteration 207/1000 | Loss: 0.00001836
Iteration 208/1000 | Loss: 0.00001836
Iteration 209/1000 | Loss: 0.00001836
Iteration 210/1000 | Loss: 0.00001836
Iteration 211/1000 | Loss: 0.00001836
Iteration 212/1000 | Loss: 0.00001836
Iteration 213/1000 | Loss: 0.00001836
Iteration 214/1000 | Loss: 0.00001836
Iteration 215/1000 | Loss: 0.00001836
Iteration 216/1000 | Loss: 0.00001836
Iteration 217/1000 | Loss: 0.00001836
Iteration 218/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.8364553397987038e-05, 1.8364553397987038e-05, 1.8364553397987038e-05, 1.8364553397987038e-05, 1.8364553397987038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8364553397987038e-05

Optimization complete. Final v2v error: 3.539306879043579 mm

Highest mean error: 3.9511191844940186 mm for frame 260

Lowest mean error: 3.292701482772827 mm for frame 28

Saving results

Total time: 59.266674280166626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388758
Iteration 2/25 | Loss: 0.00115822
Iteration 3/25 | Loss: 0.00100511
Iteration 4/25 | Loss: 0.00098757
Iteration 5/25 | Loss: 0.00098425
Iteration 6/25 | Loss: 0.00098425
Iteration 7/25 | Loss: 0.00098425
Iteration 8/25 | Loss: 0.00098425
Iteration 9/25 | Loss: 0.00098425
Iteration 10/25 | Loss: 0.00098422
Iteration 11/25 | Loss: 0.00098422
Iteration 12/25 | Loss: 0.00098422
Iteration 13/25 | Loss: 0.00098422
Iteration 14/25 | Loss: 0.00098422
Iteration 15/25 | Loss: 0.00098422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009842209983617067, 0.0009842209983617067, 0.0009842209983617067, 0.0009842209983617067, 0.0009842209983617067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009842209983617067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26184440
Iteration 2/25 | Loss: 0.00134285
Iteration 3/25 | Loss: 0.00134284
Iteration 4/25 | Loss: 0.00134284
Iteration 5/25 | Loss: 0.00134284
Iteration 6/25 | Loss: 0.00134284
Iteration 7/25 | Loss: 0.00134284
Iteration 8/25 | Loss: 0.00134284
Iteration 9/25 | Loss: 0.00134284
Iteration 10/25 | Loss: 0.00134284
Iteration 11/25 | Loss: 0.00134284
Iteration 12/25 | Loss: 0.00134284
Iteration 13/25 | Loss: 0.00134284
Iteration 14/25 | Loss: 0.00134284
Iteration 15/25 | Loss: 0.00134284
Iteration 16/25 | Loss: 0.00134284
Iteration 17/25 | Loss: 0.00134284
Iteration 18/25 | Loss: 0.00134284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013428424717858434, 0.0013428424717858434, 0.0013428424717858434, 0.0013428424717858434, 0.0013428424717858434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013428424717858434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134284
Iteration 2/1000 | Loss: 0.00003016
Iteration 3/1000 | Loss: 0.00001938
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001283
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001242
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001238
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001225
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001205
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001204
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001200
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2003943083982449e-05, 1.2003943083982449e-05, 1.2003943083982449e-05, 1.2003943083982449e-05, 1.2003943083982449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2003943083982449e-05

Optimization complete. Final v2v error: 2.8423566818237305 mm

Highest mean error: 3.6060292720794678 mm for frame 93

Lowest mean error: 2.3868067264556885 mm for frame 102

Saving results

Total time: 36.120577573776245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012400
Iteration 2/25 | Loss: 0.00214296
Iteration 3/25 | Loss: 0.00161142
Iteration 4/25 | Loss: 0.00156830
Iteration 5/25 | Loss: 0.00178855
Iteration 6/25 | Loss: 0.00124063
Iteration 7/25 | Loss: 0.00113392
Iteration 8/25 | Loss: 0.00109826
Iteration 9/25 | Loss: 0.00109846
Iteration 10/25 | Loss: 0.00111243
Iteration 11/25 | Loss: 0.00110973
Iteration 12/25 | Loss: 0.00111064
Iteration 13/25 | Loss: 0.00110574
Iteration 14/25 | Loss: 0.00108963
Iteration 15/25 | Loss: 0.00107606
Iteration 16/25 | Loss: 0.00107513
Iteration 17/25 | Loss: 0.00106649
Iteration 18/25 | Loss: 0.00106147
Iteration 19/25 | Loss: 0.00105731
Iteration 20/25 | Loss: 0.00104807
Iteration 21/25 | Loss: 0.00104191
Iteration 22/25 | Loss: 0.00104350
Iteration 23/25 | Loss: 0.00104181
Iteration 24/25 | Loss: 0.00104290
Iteration 25/25 | Loss: 0.00104413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30254006
Iteration 2/25 | Loss: 0.00093495
Iteration 3/25 | Loss: 0.00093495
Iteration 4/25 | Loss: 0.00093495
Iteration 5/25 | Loss: 0.00093495
Iteration 6/25 | Loss: 0.00093495
Iteration 7/25 | Loss: 0.00093495
Iteration 8/25 | Loss: 0.00093495
Iteration 9/25 | Loss: 0.00093495
Iteration 10/25 | Loss: 0.00093495
Iteration 11/25 | Loss: 0.00093495
Iteration 12/25 | Loss: 0.00093495
Iteration 13/25 | Loss: 0.00093495
Iteration 14/25 | Loss: 0.00093495
Iteration 15/25 | Loss: 0.00093495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000934951938688755, 0.000934951938688755, 0.000934951938688755, 0.000934951938688755, 0.000934951938688755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000934951938688755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093495
Iteration 2/1000 | Loss: 0.00006644
Iteration 3/1000 | Loss: 0.00006071
Iteration 4/1000 | Loss: 0.00032088
Iteration 5/1000 | Loss: 0.00007132
Iteration 6/1000 | Loss: 0.00004707
Iteration 7/1000 | Loss: 0.00004565
Iteration 8/1000 | Loss: 0.00061040
Iteration 9/1000 | Loss: 0.00030575
Iteration 10/1000 | Loss: 0.00006314
Iteration 11/1000 | Loss: 0.00008318
Iteration 12/1000 | Loss: 0.00055498
Iteration 13/1000 | Loss: 0.00036263
Iteration 14/1000 | Loss: 0.00032578
Iteration 15/1000 | Loss: 0.00039079
Iteration 16/1000 | Loss: 0.00044560
Iteration 17/1000 | Loss: 0.00007173
Iteration 18/1000 | Loss: 0.00007391
Iteration 19/1000 | Loss: 0.00003835
Iteration 20/1000 | Loss: 0.00004619
Iteration 21/1000 | Loss: 0.00005171
Iteration 22/1000 | Loss: 0.00004885
Iteration 23/1000 | Loss: 0.00005435
Iteration 24/1000 | Loss: 0.00004487
Iteration 25/1000 | Loss: 0.00005605
Iteration 26/1000 | Loss: 0.00005424
Iteration 27/1000 | Loss: 0.00006579
Iteration 28/1000 | Loss: 0.00005726
Iteration 29/1000 | Loss: 0.00006547
Iteration 30/1000 | Loss: 0.00007416
Iteration 31/1000 | Loss: 0.00006446
Iteration 32/1000 | Loss: 0.00005531
Iteration 33/1000 | Loss: 0.00006774
Iteration 34/1000 | Loss: 0.00006778
Iteration 35/1000 | Loss: 0.00007407
Iteration 36/1000 | Loss: 0.00007623
Iteration 37/1000 | Loss: 0.00007153
Iteration 38/1000 | Loss: 0.00007337
Iteration 39/1000 | Loss: 0.00007762
Iteration 40/1000 | Loss: 0.00009316
Iteration 41/1000 | Loss: 0.00007856
Iteration 42/1000 | Loss: 0.00007960
Iteration 43/1000 | Loss: 0.00007596
Iteration 44/1000 | Loss: 0.00006726
Iteration 45/1000 | Loss: 0.00006697
Iteration 46/1000 | Loss: 0.00008880
Iteration 47/1000 | Loss: 0.00009632
Iteration 48/1000 | Loss: 0.00006112
Iteration 49/1000 | Loss: 0.00008829
Iteration 50/1000 | Loss: 0.00006343
Iteration 51/1000 | Loss: 0.00008896
Iteration 52/1000 | Loss: 0.00007651
Iteration 53/1000 | Loss: 0.00007709
Iteration 54/1000 | Loss: 0.00005578
Iteration 55/1000 | Loss: 0.00005165
Iteration 56/1000 | Loss: 0.00009656
Iteration 57/1000 | Loss: 0.00010550
Iteration 58/1000 | Loss: 0.00007370
Iteration 59/1000 | Loss: 0.00006675
Iteration 60/1000 | Loss: 0.00009084
Iteration 61/1000 | Loss: 0.00008149
Iteration 62/1000 | Loss: 0.00009613
Iteration 63/1000 | Loss: 0.00005543
Iteration 64/1000 | Loss: 0.00004504
Iteration 65/1000 | Loss: 0.00004596
Iteration 66/1000 | Loss: 0.00003430
Iteration 67/1000 | Loss: 0.00003106
Iteration 68/1000 | Loss: 0.00004411
Iteration 69/1000 | Loss: 0.00004332
Iteration 70/1000 | Loss: 0.00004621
Iteration 71/1000 | Loss: 0.00004447
Iteration 72/1000 | Loss: 0.00004353
Iteration 73/1000 | Loss: 0.00002206
Iteration 74/1000 | Loss: 0.00004307
Iteration 75/1000 | Loss: 0.00004023
Iteration 76/1000 | Loss: 0.00003813
Iteration 77/1000 | Loss: 0.00003503
Iteration 78/1000 | Loss: 0.00002638
Iteration 79/1000 | Loss: 0.00003372
Iteration 80/1000 | Loss: 0.00003822
Iteration 81/1000 | Loss: 0.00003637
Iteration 82/1000 | Loss: 0.00003597
Iteration 83/1000 | Loss: 0.00003518
Iteration 84/1000 | Loss: 0.00002333
Iteration 85/1000 | Loss: 0.00002115
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001780
Iteration 91/1000 | Loss: 0.00001775
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001737
Iteration 99/1000 | Loss: 0.00001734
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001730
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001723
Iteration 105/1000 | Loss: 0.00001722
Iteration 106/1000 | Loss: 0.00001721
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001720
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00062290
Iteration 113/1000 | Loss: 0.00001822
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001492
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001437
Iteration 119/1000 | Loss: 0.00001436
Iteration 120/1000 | Loss: 0.00001435
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001418
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001416
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001415
Iteration 132/1000 | Loss: 0.00001415
Iteration 133/1000 | Loss: 0.00001415
Iteration 134/1000 | Loss: 0.00001415
Iteration 135/1000 | Loss: 0.00001415
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001414
Iteration 140/1000 | Loss: 0.00001414
Iteration 141/1000 | Loss: 0.00001413
Iteration 142/1000 | Loss: 0.00001413
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001412
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001412
Iteration 150/1000 | Loss: 0.00001412
Iteration 151/1000 | Loss: 0.00001411
Iteration 152/1000 | Loss: 0.00001411
Iteration 153/1000 | Loss: 0.00001411
Iteration 154/1000 | Loss: 0.00001410
Iteration 155/1000 | Loss: 0.00001410
Iteration 156/1000 | Loss: 0.00001410
Iteration 157/1000 | Loss: 0.00001410
Iteration 158/1000 | Loss: 0.00001410
Iteration 159/1000 | Loss: 0.00001410
Iteration 160/1000 | Loss: 0.00001410
Iteration 161/1000 | Loss: 0.00001410
Iteration 162/1000 | Loss: 0.00001410
Iteration 163/1000 | Loss: 0.00001410
Iteration 164/1000 | Loss: 0.00001410
Iteration 165/1000 | Loss: 0.00001410
Iteration 166/1000 | Loss: 0.00001410
Iteration 167/1000 | Loss: 0.00001409
Iteration 168/1000 | Loss: 0.00001409
Iteration 169/1000 | Loss: 0.00001409
Iteration 170/1000 | Loss: 0.00001408
Iteration 171/1000 | Loss: 0.00001408
Iteration 172/1000 | Loss: 0.00001408
Iteration 173/1000 | Loss: 0.00001408
Iteration 174/1000 | Loss: 0.00001408
Iteration 175/1000 | Loss: 0.00001408
Iteration 176/1000 | Loss: 0.00001408
Iteration 177/1000 | Loss: 0.00001408
Iteration 178/1000 | Loss: 0.00001408
Iteration 179/1000 | Loss: 0.00001408
Iteration 180/1000 | Loss: 0.00001407
Iteration 181/1000 | Loss: 0.00001407
Iteration 182/1000 | Loss: 0.00001407
Iteration 183/1000 | Loss: 0.00001407
Iteration 184/1000 | Loss: 0.00001407
Iteration 185/1000 | Loss: 0.00001407
Iteration 186/1000 | Loss: 0.00001407
Iteration 187/1000 | Loss: 0.00001407
Iteration 188/1000 | Loss: 0.00001407
Iteration 189/1000 | Loss: 0.00001406
Iteration 190/1000 | Loss: 0.00001406
Iteration 191/1000 | Loss: 0.00001406
Iteration 192/1000 | Loss: 0.00001406
Iteration 193/1000 | Loss: 0.00001405
Iteration 194/1000 | Loss: 0.00001405
Iteration 195/1000 | Loss: 0.00001405
Iteration 196/1000 | Loss: 0.00001405
Iteration 197/1000 | Loss: 0.00001405
Iteration 198/1000 | Loss: 0.00001405
Iteration 199/1000 | Loss: 0.00001405
Iteration 200/1000 | Loss: 0.00001405
Iteration 201/1000 | Loss: 0.00001405
Iteration 202/1000 | Loss: 0.00001405
Iteration 203/1000 | Loss: 0.00001405
Iteration 204/1000 | Loss: 0.00001405
Iteration 205/1000 | Loss: 0.00001404
Iteration 206/1000 | Loss: 0.00001404
Iteration 207/1000 | Loss: 0.00001404
Iteration 208/1000 | Loss: 0.00001404
Iteration 209/1000 | Loss: 0.00001404
Iteration 210/1000 | Loss: 0.00001404
Iteration 211/1000 | Loss: 0.00001404
Iteration 212/1000 | Loss: 0.00001404
Iteration 213/1000 | Loss: 0.00001403
Iteration 214/1000 | Loss: 0.00001403
Iteration 215/1000 | Loss: 0.00001403
Iteration 216/1000 | Loss: 0.00001403
Iteration 217/1000 | Loss: 0.00001403
Iteration 218/1000 | Loss: 0.00001403
Iteration 219/1000 | Loss: 0.00001403
Iteration 220/1000 | Loss: 0.00001403
Iteration 221/1000 | Loss: 0.00001403
Iteration 222/1000 | Loss: 0.00001403
Iteration 223/1000 | Loss: 0.00001403
Iteration 224/1000 | Loss: 0.00001403
Iteration 225/1000 | Loss: 0.00001403
Iteration 226/1000 | Loss: 0.00001403
Iteration 227/1000 | Loss: 0.00001403
Iteration 228/1000 | Loss: 0.00001403
Iteration 229/1000 | Loss: 0.00001403
Iteration 230/1000 | Loss: 0.00001403
Iteration 231/1000 | Loss: 0.00001402
Iteration 232/1000 | Loss: 0.00001402
Iteration 233/1000 | Loss: 0.00001402
Iteration 234/1000 | Loss: 0.00001402
Iteration 235/1000 | Loss: 0.00001402
Iteration 236/1000 | Loss: 0.00001402
Iteration 237/1000 | Loss: 0.00001402
Iteration 238/1000 | Loss: 0.00001402
Iteration 239/1000 | Loss: 0.00001402
Iteration 240/1000 | Loss: 0.00001402
Iteration 241/1000 | Loss: 0.00001402
Iteration 242/1000 | Loss: 0.00001402
Iteration 243/1000 | Loss: 0.00001402
Iteration 244/1000 | Loss: 0.00001402
Iteration 245/1000 | Loss: 0.00001402
Iteration 246/1000 | Loss: 0.00001402
Iteration 247/1000 | Loss: 0.00001401
Iteration 248/1000 | Loss: 0.00001401
Iteration 249/1000 | Loss: 0.00001401
Iteration 250/1000 | Loss: 0.00001401
Iteration 251/1000 | Loss: 0.00001401
Iteration 252/1000 | Loss: 0.00001401
Iteration 253/1000 | Loss: 0.00001401
Iteration 254/1000 | Loss: 0.00001401
Iteration 255/1000 | Loss: 0.00001401
Iteration 256/1000 | Loss: 0.00001401
Iteration 257/1000 | Loss: 0.00001401
Iteration 258/1000 | Loss: 0.00001401
Iteration 259/1000 | Loss: 0.00001401
Iteration 260/1000 | Loss: 0.00001401
Iteration 261/1000 | Loss: 0.00001401
Iteration 262/1000 | Loss: 0.00001401
Iteration 263/1000 | Loss: 0.00001401
Iteration 264/1000 | Loss: 0.00001401
Iteration 265/1000 | Loss: 0.00001401
Iteration 266/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.4009476217324845e-05, 1.4009476217324845e-05, 1.4009476217324845e-05, 1.4009476217324845e-05, 1.4009476217324845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4009476217324845e-05

Optimization complete. Final v2v error: 3.127236843109131 mm

Highest mean error: 9.694741249084473 mm for frame 66

Lowest mean error: 2.779203414916992 mm for frame 42

Saving results

Total time: 200.1040563583374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065201
Iteration 2/25 | Loss: 0.00276163
Iteration 3/25 | Loss: 0.00167120
Iteration 4/25 | Loss: 0.00149458
Iteration 5/25 | Loss: 0.00150890
Iteration 6/25 | Loss: 0.00136629
Iteration 7/25 | Loss: 0.00127703
Iteration 8/25 | Loss: 0.00130031
Iteration 9/25 | Loss: 0.00120590
Iteration 10/25 | Loss: 0.00117789
Iteration 11/25 | Loss: 0.00114620
Iteration 12/25 | Loss: 0.00113768
Iteration 13/25 | Loss: 0.00114804
Iteration 14/25 | Loss: 0.00111226
Iteration 15/25 | Loss: 0.00110020
Iteration 16/25 | Loss: 0.00108674
Iteration 17/25 | Loss: 0.00107609
Iteration 18/25 | Loss: 0.00106868
Iteration 19/25 | Loss: 0.00106081
Iteration 20/25 | Loss: 0.00105634
Iteration 21/25 | Loss: 0.00105738
Iteration 22/25 | Loss: 0.00105137
Iteration 23/25 | Loss: 0.00104919
Iteration 24/25 | Loss: 0.00105047
Iteration 25/25 | Loss: 0.00105010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52518940
Iteration 2/25 | Loss: 0.00097480
Iteration 3/25 | Loss: 0.00097480
Iteration 4/25 | Loss: 0.00097480
Iteration 5/25 | Loss: 0.00097480
Iteration 6/25 | Loss: 0.00097480
Iteration 7/25 | Loss: 0.00097480
Iteration 8/25 | Loss: 0.00097480
Iteration 9/25 | Loss: 0.00097480
Iteration 10/25 | Loss: 0.00097480
Iteration 11/25 | Loss: 0.00097480
Iteration 12/25 | Loss: 0.00097480
Iteration 13/25 | Loss: 0.00097480
Iteration 14/25 | Loss: 0.00097480
Iteration 15/25 | Loss: 0.00097480
Iteration 16/25 | Loss: 0.00097480
Iteration 17/25 | Loss: 0.00097480
Iteration 18/25 | Loss: 0.00097480
Iteration 19/25 | Loss: 0.00097480
Iteration 20/25 | Loss: 0.00097480
Iteration 21/25 | Loss: 0.00097480
Iteration 22/25 | Loss: 0.00097480
Iteration 23/25 | Loss: 0.00097480
Iteration 24/25 | Loss: 0.00097480
Iteration 25/25 | Loss: 0.00097480

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097480
Iteration 2/1000 | Loss: 0.00004074
Iteration 3/1000 | Loss: 0.00004543
Iteration 4/1000 | Loss: 0.00002750
Iteration 5/1000 | Loss: 0.00003732
Iteration 6/1000 | Loss: 0.00007023
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00005834
Iteration 9/1000 | Loss: 0.00002965
Iteration 10/1000 | Loss: 0.00003026
Iteration 11/1000 | Loss: 0.00003069
Iteration 12/1000 | Loss: 0.00003088
Iteration 13/1000 | Loss: 0.00003779
Iteration 14/1000 | Loss: 0.00003888
Iteration 15/1000 | Loss: 0.00003653
Iteration 16/1000 | Loss: 0.00004020
Iteration 17/1000 | Loss: 0.00004636
Iteration 18/1000 | Loss: 0.00006441
Iteration 19/1000 | Loss: 0.00002796
Iteration 20/1000 | Loss: 0.00002279
Iteration 21/1000 | Loss: 0.00004047
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001793
Iteration 24/1000 | Loss: 0.00001726
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001676
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001636
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001633
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001628
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00002539
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001640
Iteration 88/1000 | Loss: 0.00002202
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00002626
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001627
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00002333
Iteration 111/1000 | Loss: 0.00048371
Iteration 112/1000 | Loss: 0.00002707
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002407
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00003304
Iteration 117/1000 | Loss: 0.00001437
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00002839
Iteration 120/1000 | Loss: 0.00007689
Iteration 121/1000 | Loss: 0.00009215
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001344
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001338
Iteration 132/1000 | Loss: 0.00001338
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001356
Iteration 135/1000 | Loss: 0.00002566
Iteration 136/1000 | Loss: 0.00006452
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00002426
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001315
Iteration 150/1000 | Loss: 0.00001315
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001311
Iteration 171/1000 | Loss: 0.00001311
Iteration 172/1000 | Loss: 0.00001311
Iteration 173/1000 | Loss: 0.00001311
Iteration 174/1000 | Loss: 0.00001311
Iteration 175/1000 | Loss: 0.00001311
Iteration 176/1000 | Loss: 0.00001311
Iteration 177/1000 | Loss: 0.00001311
Iteration 178/1000 | Loss: 0.00001311
Iteration 179/1000 | Loss: 0.00001310
Iteration 180/1000 | Loss: 0.00001310
Iteration 181/1000 | Loss: 0.00001310
Iteration 182/1000 | Loss: 0.00001310
Iteration 183/1000 | Loss: 0.00001310
Iteration 184/1000 | Loss: 0.00001310
Iteration 185/1000 | Loss: 0.00001310
Iteration 186/1000 | Loss: 0.00001310
Iteration 187/1000 | Loss: 0.00001310
Iteration 188/1000 | Loss: 0.00001310
Iteration 189/1000 | Loss: 0.00001310
Iteration 190/1000 | Loss: 0.00001310
Iteration 191/1000 | Loss: 0.00001309
Iteration 192/1000 | Loss: 0.00001309
Iteration 193/1000 | Loss: 0.00001309
Iteration 194/1000 | Loss: 0.00001309
Iteration 195/1000 | Loss: 0.00001309
Iteration 196/1000 | Loss: 0.00001308
Iteration 197/1000 | Loss: 0.00001308
Iteration 198/1000 | Loss: 0.00001308
Iteration 199/1000 | Loss: 0.00001308
Iteration 200/1000 | Loss: 0.00001343
Iteration 201/1000 | Loss: 0.00001315
Iteration 202/1000 | Loss: 0.00001314
Iteration 203/1000 | Loss: 0.00001313
Iteration 204/1000 | Loss: 0.00001313
Iteration 205/1000 | Loss: 0.00001313
Iteration 206/1000 | Loss: 0.00001311
Iteration 207/1000 | Loss: 0.00001310
Iteration 208/1000 | Loss: 0.00001310
Iteration 209/1000 | Loss: 0.00001310
Iteration 210/1000 | Loss: 0.00001310
Iteration 211/1000 | Loss: 0.00001309
Iteration 212/1000 | Loss: 0.00001309
Iteration 213/1000 | Loss: 0.00001309
Iteration 214/1000 | Loss: 0.00001309
Iteration 215/1000 | Loss: 0.00001309
Iteration 216/1000 | Loss: 0.00001309
Iteration 217/1000 | Loss: 0.00001309
Iteration 218/1000 | Loss: 0.00001309
Iteration 219/1000 | Loss: 0.00001309
Iteration 220/1000 | Loss: 0.00001309
Iteration 221/1000 | Loss: 0.00001309
Iteration 222/1000 | Loss: 0.00001309
Iteration 223/1000 | Loss: 0.00001309
Iteration 224/1000 | Loss: 0.00001309
Iteration 225/1000 | Loss: 0.00001309
Iteration 226/1000 | Loss: 0.00001309
Iteration 227/1000 | Loss: 0.00001309
Iteration 228/1000 | Loss: 0.00001309
Iteration 229/1000 | Loss: 0.00001309
Iteration 230/1000 | Loss: 0.00001309
Iteration 231/1000 | Loss: 0.00001309
Iteration 232/1000 | Loss: 0.00001309
Iteration 233/1000 | Loss: 0.00001309
Iteration 234/1000 | Loss: 0.00001309
Iteration 235/1000 | Loss: 0.00001309
Iteration 236/1000 | Loss: 0.00001309
Iteration 237/1000 | Loss: 0.00001309
Iteration 238/1000 | Loss: 0.00001309
Iteration 239/1000 | Loss: 0.00001309
Iteration 240/1000 | Loss: 0.00001309
Iteration 241/1000 | Loss: 0.00001309
Iteration 242/1000 | Loss: 0.00001309
Iteration 243/1000 | Loss: 0.00001309
Iteration 244/1000 | Loss: 0.00001309
Iteration 245/1000 | Loss: 0.00001309
Iteration 246/1000 | Loss: 0.00001309
Iteration 247/1000 | Loss: 0.00001309
Iteration 248/1000 | Loss: 0.00001309
Iteration 249/1000 | Loss: 0.00001309
Iteration 250/1000 | Loss: 0.00001309
Iteration 251/1000 | Loss: 0.00001309
Iteration 252/1000 | Loss: 0.00001309
Iteration 253/1000 | Loss: 0.00001309
Iteration 254/1000 | Loss: 0.00001309
Iteration 255/1000 | Loss: 0.00001309
Iteration 256/1000 | Loss: 0.00001309
Iteration 257/1000 | Loss: 0.00001309
Iteration 258/1000 | Loss: 0.00001309
Iteration 259/1000 | Loss: 0.00001309
Iteration 260/1000 | Loss: 0.00001309
Iteration 261/1000 | Loss: 0.00001309
Iteration 262/1000 | Loss: 0.00001309
Iteration 263/1000 | Loss: 0.00001309
Iteration 264/1000 | Loss: 0.00001309
Iteration 265/1000 | Loss: 0.00001309
Iteration 266/1000 | Loss: 0.00001309
Iteration 267/1000 | Loss: 0.00001309
Iteration 268/1000 | Loss: 0.00001309
Iteration 269/1000 | Loss: 0.00001309
Iteration 270/1000 | Loss: 0.00001309
Iteration 271/1000 | Loss: 0.00001309
Iteration 272/1000 | Loss: 0.00001309
Iteration 273/1000 | Loss: 0.00001309
Iteration 274/1000 | Loss: 0.00001309
Iteration 275/1000 | Loss: 0.00001309
Iteration 276/1000 | Loss: 0.00001309
Iteration 277/1000 | Loss: 0.00001309
Iteration 278/1000 | Loss: 0.00001309
Iteration 279/1000 | Loss: 0.00001309
Iteration 280/1000 | Loss: 0.00001309
Iteration 281/1000 | Loss: 0.00001309
Iteration 282/1000 | Loss: 0.00001309
Iteration 283/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [1.3092924746160861e-05, 1.3092924746160861e-05, 1.3092924746160861e-05, 1.3092924746160861e-05, 1.3092924746160861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3092924746160861e-05

Optimization complete. Final v2v error: 3.056431770324707 mm

Highest mean error: 9.967401504516602 mm for frame 54

Lowest mean error: 2.602550983428955 mm for frame 0

Saving results

Total time: 131.96627068519592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363217
Iteration 2/25 | Loss: 0.00107036
Iteration 3/25 | Loss: 0.00098155
Iteration 4/25 | Loss: 0.00097082
Iteration 5/25 | Loss: 0.00096692
Iteration 6/25 | Loss: 0.00096613
Iteration 7/25 | Loss: 0.00096613
Iteration 8/25 | Loss: 0.00096613
Iteration 9/25 | Loss: 0.00096613
Iteration 10/25 | Loss: 0.00096613
Iteration 11/25 | Loss: 0.00096613
Iteration 12/25 | Loss: 0.00096613
Iteration 13/25 | Loss: 0.00096613
Iteration 14/25 | Loss: 0.00096613
Iteration 15/25 | Loss: 0.00096613
Iteration 16/25 | Loss: 0.00096613
Iteration 17/25 | Loss: 0.00096613
Iteration 18/25 | Loss: 0.00096613
Iteration 19/25 | Loss: 0.00096613
Iteration 20/25 | Loss: 0.00096613
Iteration 21/25 | Loss: 0.00096613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009661332005634904, 0.0009661332005634904, 0.0009661332005634904, 0.0009661332005634904, 0.0009661332005634904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009661332005634904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52362311
Iteration 2/25 | Loss: 0.00100746
Iteration 3/25 | Loss: 0.00100746
Iteration 4/25 | Loss: 0.00100746
Iteration 5/25 | Loss: 0.00100746
Iteration 6/25 | Loss: 0.00100746
Iteration 7/25 | Loss: 0.00100746
Iteration 8/25 | Loss: 0.00100746
Iteration 9/25 | Loss: 0.00100746
Iteration 10/25 | Loss: 0.00100746
Iteration 11/25 | Loss: 0.00100746
Iteration 12/25 | Loss: 0.00100746
Iteration 13/25 | Loss: 0.00100746
Iteration 14/25 | Loss: 0.00100746
Iteration 15/25 | Loss: 0.00100746
Iteration 16/25 | Loss: 0.00100746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001007460756227374, 0.001007460756227374, 0.001007460756227374, 0.001007460756227374, 0.001007460756227374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001007460756227374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100746
Iteration 2/1000 | Loss: 0.00001763
Iteration 3/1000 | Loss: 0.00001373
Iteration 4/1000 | Loss: 0.00001231
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001140
Iteration 7/1000 | Loss: 0.00001111
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001072
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001068
Iteration 15/1000 | Loss: 0.00001068
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001065
Iteration 20/1000 | Loss: 0.00001064
Iteration 21/1000 | Loss: 0.00001064
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001062
Iteration 25/1000 | Loss: 0.00001062
Iteration 26/1000 | Loss: 0.00001062
Iteration 27/1000 | Loss: 0.00001062
Iteration 28/1000 | Loss: 0.00001062
Iteration 29/1000 | Loss: 0.00001062
Iteration 30/1000 | Loss: 0.00001061
Iteration 31/1000 | Loss: 0.00001061
Iteration 32/1000 | Loss: 0.00001061
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001060
Iteration 35/1000 | Loss: 0.00001060
Iteration 36/1000 | Loss: 0.00001060
Iteration 37/1000 | Loss: 0.00001060
Iteration 38/1000 | Loss: 0.00001059
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001059
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001057
Iteration 48/1000 | Loss: 0.00001057
Iteration 49/1000 | Loss: 0.00001057
Iteration 50/1000 | Loss: 0.00001057
Iteration 51/1000 | Loss: 0.00001057
Iteration 52/1000 | Loss: 0.00001057
Iteration 53/1000 | Loss: 0.00001057
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001056
Iteration 56/1000 | Loss: 0.00001056
Iteration 57/1000 | Loss: 0.00001056
Iteration 58/1000 | Loss: 0.00001056
Iteration 59/1000 | Loss: 0.00001056
Iteration 60/1000 | Loss: 0.00001055
Iteration 61/1000 | Loss: 0.00001055
Iteration 62/1000 | Loss: 0.00001055
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001054
Iteration 66/1000 | Loss: 0.00001054
Iteration 67/1000 | Loss: 0.00001054
Iteration 68/1000 | Loss: 0.00001054
Iteration 69/1000 | Loss: 0.00001054
Iteration 70/1000 | Loss: 0.00001053
Iteration 71/1000 | Loss: 0.00001053
Iteration 72/1000 | Loss: 0.00001053
Iteration 73/1000 | Loss: 0.00001053
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001052
Iteration 79/1000 | Loss: 0.00001052
Iteration 80/1000 | Loss: 0.00001052
Iteration 81/1000 | Loss: 0.00001052
Iteration 82/1000 | Loss: 0.00001052
Iteration 83/1000 | Loss: 0.00001052
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001051
Iteration 86/1000 | Loss: 0.00001050
Iteration 87/1000 | Loss: 0.00001050
Iteration 88/1000 | Loss: 0.00001050
Iteration 89/1000 | Loss: 0.00001049
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001049
Iteration 95/1000 | Loss: 0.00001049
Iteration 96/1000 | Loss: 0.00001049
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001048
Iteration 102/1000 | Loss: 0.00001048
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001048
Iteration 108/1000 | Loss: 0.00001048
Iteration 109/1000 | Loss: 0.00001048
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001048
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001047
Iteration 116/1000 | Loss: 0.00001047
Iteration 117/1000 | Loss: 0.00001047
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001047
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001047
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001047
Iteration 127/1000 | Loss: 0.00001047
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001046
Iteration 130/1000 | Loss: 0.00001046
Iteration 131/1000 | Loss: 0.00001046
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001046
Iteration 141/1000 | Loss: 0.00001046
Iteration 142/1000 | Loss: 0.00001046
Iteration 143/1000 | Loss: 0.00001046
Iteration 144/1000 | Loss: 0.00001046
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001046
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001046
Iteration 149/1000 | Loss: 0.00001046
Iteration 150/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0458024917170405e-05, 1.0458024917170405e-05, 1.0458024917170405e-05, 1.0458024917170405e-05, 1.0458024917170405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0458024917170405e-05

Optimization complete. Final v2v error: 2.7206363677978516 mm

Highest mean error: 3.2293925285339355 mm for frame 137

Lowest mean error: 2.4377365112304688 mm for frame 62

Saving results

Total time: 33.766279220581055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423265
Iteration 2/25 | Loss: 0.00140429
Iteration 3/25 | Loss: 0.00109921
Iteration 4/25 | Loss: 0.00105459
Iteration 5/25 | Loss: 0.00104468
Iteration 6/25 | Loss: 0.00104184
Iteration 7/25 | Loss: 0.00104166
Iteration 8/25 | Loss: 0.00104166
Iteration 9/25 | Loss: 0.00104166
Iteration 10/25 | Loss: 0.00104166
Iteration 11/25 | Loss: 0.00104166
Iteration 12/25 | Loss: 0.00104166
Iteration 13/25 | Loss: 0.00104166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001041657174937427, 0.001041657174937427, 0.001041657174937427, 0.001041657174937427, 0.001041657174937427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001041657174937427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29654145
Iteration 2/25 | Loss: 0.00068939
Iteration 3/25 | Loss: 0.00068939
Iteration 4/25 | Loss: 0.00068939
Iteration 5/25 | Loss: 0.00068939
Iteration 6/25 | Loss: 0.00068939
Iteration 7/25 | Loss: 0.00068939
Iteration 8/25 | Loss: 0.00068939
Iteration 9/25 | Loss: 0.00068939
Iteration 10/25 | Loss: 0.00068939
Iteration 11/25 | Loss: 0.00068939
Iteration 12/25 | Loss: 0.00068939
Iteration 13/25 | Loss: 0.00068939
Iteration 14/25 | Loss: 0.00068939
Iteration 15/25 | Loss: 0.00068939
Iteration 16/25 | Loss: 0.00068939
Iteration 17/25 | Loss: 0.00068939
Iteration 18/25 | Loss: 0.00068939
Iteration 19/25 | Loss: 0.00068939
Iteration 20/25 | Loss: 0.00068939
Iteration 21/25 | Loss: 0.00068939
Iteration 22/25 | Loss: 0.00068939
Iteration 23/25 | Loss: 0.00068939
Iteration 24/25 | Loss: 0.00068939
Iteration 25/25 | Loss: 0.00068939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068939
Iteration 2/1000 | Loss: 0.00003766
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002483
Iteration 5/1000 | Loss: 0.00002317
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002144
Iteration 8/1000 | Loss: 0.00002096
Iteration 9/1000 | Loss: 0.00002048
Iteration 10/1000 | Loss: 0.00002019
Iteration 11/1000 | Loss: 0.00002013
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001973
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001968
Iteration 24/1000 | Loss: 0.00001966
Iteration 25/1000 | Loss: 0.00001966
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001965
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001964
Iteration 31/1000 | Loss: 0.00001964
Iteration 32/1000 | Loss: 0.00001963
Iteration 33/1000 | Loss: 0.00001963
Iteration 34/1000 | Loss: 0.00001963
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00001963
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001963
Iteration 39/1000 | Loss: 0.00001962
Iteration 40/1000 | Loss: 0.00001962
Iteration 41/1000 | Loss: 0.00001962
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001962
Iteration 44/1000 | Loss: 0.00001962
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001961
Iteration 53/1000 | Loss: 0.00001961
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001960
Iteration 57/1000 | Loss: 0.00001960
Iteration 58/1000 | Loss: 0.00001960
Iteration 59/1000 | Loss: 0.00001960
Iteration 60/1000 | Loss: 0.00001960
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001959
Iteration 71/1000 | Loss: 0.00001959
Iteration 72/1000 | Loss: 0.00001959
Iteration 73/1000 | Loss: 0.00001959
Iteration 74/1000 | Loss: 0.00001959
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001956
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001956
Iteration 101/1000 | Loss: 0.00001956
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00001955
Iteration 104/1000 | Loss: 0.00001955
Iteration 105/1000 | Loss: 0.00001955
Iteration 106/1000 | Loss: 0.00001955
Iteration 107/1000 | Loss: 0.00001955
Iteration 108/1000 | Loss: 0.00001955
Iteration 109/1000 | Loss: 0.00001955
Iteration 110/1000 | Loss: 0.00001955
Iteration 111/1000 | Loss: 0.00001955
Iteration 112/1000 | Loss: 0.00001955
Iteration 113/1000 | Loss: 0.00001955
Iteration 114/1000 | Loss: 0.00001955
Iteration 115/1000 | Loss: 0.00001955
Iteration 116/1000 | Loss: 0.00001955
Iteration 117/1000 | Loss: 0.00001955
Iteration 118/1000 | Loss: 0.00001955
Iteration 119/1000 | Loss: 0.00001955
Iteration 120/1000 | Loss: 0.00001955
Iteration 121/1000 | Loss: 0.00001955
Iteration 122/1000 | Loss: 0.00001955
Iteration 123/1000 | Loss: 0.00001955
Iteration 124/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.955036714207381e-05, 1.955036714207381e-05, 1.955036714207381e-05, 1.955036714207381e-05, 1.955036714207381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.955036714207381e-05

Optimization complete. Final v2v error: 3.663228750228882 mm

Highest mean error: 4.1150689125061035 mm for frame 64

Lowest mean error: 3.2705435752868652 mm for frame 0

Saving results

Total time: 35.294209003448486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00222712
Iteration 2/25 | Loss: 0.00112538
Iteration 3/25 | Loss: 0.00101819
Iteration 4/25 | Loss: 0.00099652
Iteration 5/25 | Loss: 0.00099120
Iteration 6/25 | Loss: 0.00099026
Iteration 7/25 | Loss: 0.00099026
Iteration 8/25 | Loss: 0.00099026
Iteration 9/25 | Loss: 0.00099026
Iteration 10/25 | Loss: 0.00099026
Iteration 11/25 | Loss: 0.00099026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009902642341330647, 0.0009902642341330647, 0.0009902642341330647, 0.0009902642341330647, 0.0009902642341330647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009902642341330647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24385345
Iteration 2/25 | Loss: 0.00190621
Iteration 3/25 | Loss: 0.00190621
Iteration 4/25 | Loss: 0.00190621
Iteration 5/25 | Loss: 0.00190621
Iteration 6/25 | Loss: 0.00190621
Iteration 7/25 | Loss: 0.00190621
Iteration 8/25 | Loss: 0.00190621
Iteration 9/25 | Loss: 0.00190621
Iteration 10/25 | Loss: 0.00190620
Iteration 11/25 | Loss: 0.00190620
Iteration 12/25 | Loss: 0.00190620
Iteration 13/25 | Loss: 0.00190620
Iteration 14/25 | Loss: 0.00190620
Iteration 15/25 | Loss: 0.00190620
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019062049686908722, 0.0019062049686908722, 0.0019062049686908722, 0.0019062049686908722, 0.0019062049686908722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019062049686908722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190620
Iteration 2/1000 | Loss: 0.00003669
Iteration 3/1000 | Loss: 0.00002033
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001323
Iteration 6/1000 | Loss: 0.00001235
Iteration 7/1000 | Loss: 0.00001207
Iteration 8/1000 | Loss: 0.00001199
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001161
Iteration 13/1000 | Loss: 0.00001158
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [1.1464921044535004e-05, 1.1464921044535004e-05, 1.1464921044535004e-05, 1.1464921044535004e-05, 1.1464921044535004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1464921044535004e-05

Optimization complete. Final v2v error: 2.8779003620147705 mm

Highest mean error: 3.292001962661743 mm for frame 197

Lowest mean error: 2.591909646987915 mm for frame 156

Saving results

Total time: 24.100815057754517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405441
Iteration 2/25 | Loss: 0.00106820
Iteration 3/25 | Loss: 0.00097677
Iteration 4/25 | Loss: 0.00097111
Iteration 5/25 | Loss: 0.00097023
Iteration 6/25 | Loss: 0.00097023
Iteration 7/25 | Loss: 0.00097023
Iteration 8/25 | Loss: 0.00097023
Iteration 9/25 | Loss: 0.00097023
Iteration 10/25 | Loss: 0.00097023
Iteration 11/25 | Loss: 0.00097023
Iteration 12/25 | Loss: 0.00097023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009702339884825051, 0.0009702339884825051, 0.0009702339884825051, 0.0009702339884825051, 0.0009702339884825051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009702339884825051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54576600
Iteration 2/25 | Loss: 0.00080385
Iteration 3/25 | Loss: 0.00080385
Iteration 4/25 | Loss: 0.00080385
Iteration 5/25 | Loss: 0.00080385
Iteration 6/25 | Loss: 0.00080385
Iteration 7/25 | Loss: 0.00080385
Iteration 8/25 | Loss: 0.00080385
Iteration 9/25 | Loss: 0.00080385
Iteration 10/25 | Loss: 0.00080385
Iteration 11/25 | Loss: 0.00080385
Iteration 12/25 | Loss: 0.00080385
Iteration 13/25 | Loss: 0.00080385
Iteration 14/25 | Loss: 0.00080385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008038483210839331, 0.0008038483210839331, 0.0008038483210839331, 0.0008038483210839331, 0.0008038483210839331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008038483210839331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080385
Iteration 2/1000 | Loss: 0.00001639
Iteration 3/1000 | Loss: 0.00001212
Iteration 4/1000 | Loss: 0.00001079
Iteration 5/1000 | Loss: 0.00000967
Iteration 6/1000 | Loss: 0.00000914
Iteration 7/1000 | Loss: 0.00000900
Iteration 8/1000 | Loss: 0.00000878
Iteration 9/1000 | Loss: 0.00000854
Iteration 10/1000 | Loss: 0.00000845
Iteration 11/1000 | Loss: 0.00000844
Iteration 12/1000 | Loss: 0.00000843
Iteration 13/1000 | Loss: 0.00000842
Iteration 14/1000 | Loss: 0.00000842
Iteration 15/1000 | Loss: 0.00000841
Iteration 16/1000 | Loss: 0.00000840
Iteration 17/1000 | Loss: 0.00000837
Iteration 18/1000 | Loss: 0.00000836
Iteration 19/1000 | Loss: 0.00000836
Iteration 20/1000 | Loss: 0.00000835
Iteration 21/1000 | Loss: 0.00000833
Iteration 22/1000 | Loss: 0.00000833
Iteration 23/1000 | Loss: 0.00000832
Iteration 24/1000 | Loss: 0.00000832
Iteration 25/1000 | Loss: 0.00000831
Iteration 26/1000 | Loss: 0.00000830
Iteration 27/1000 | Loss: 0.00000830
Iteration 28/1000 | Loss: 0.00000827
Iteration 29/1000 | Loss: 0.00000827
Iteration 30/1000 | Loss: 0.00000827
Iteration 31/1000 | Loss: 0.00000827
Iteration 32/1000 | Loss: 0.00000826
Iteration 33/1000 | Loss: 0.00000826
Iteration 34/1000 | Loss: 0.00000826
Iteration 35/1000 | Loss: 0.00000826
Iteration 36/1000 | Loss: 0.00000826
Iteration 37/1000 | Loss: 0.00000826
Iteration 38/1000 | Loss: 0.00000826
Iteration 39/1000 | Loss: 0.00000825
Iteration 40/1000 | Loss: 0.00000825
Iteration 41/1000 | Loss: 0.00000825
Iteration 42/1000 | Loss: 0.00000824
Iteration 43/1000 | Loss: 0.00000822
Iteration 44/1000 | Loss: 0.00000822
Iteration 45/1000 | Loss: 0.00000822
Iteration 46/1000 | Loss: 0.00000822
Iteration 47/1000 | Loss: 0.00000822
Iteration 48/1000 | Loss: 0.00000822
Iteration 49/1000 | Loss: 0.00000821
Iteration 50/1000 | Loss: 0.00000821
Iteration 51/1000 | Loss: 0.00000821
Iteration 52/1000 | Loss: 0.00000821
Iteration 53/1000 | Loss: 0.00000820
Iteration 54/1000 | Loss: 0.00000820
Iteration 55/1000 | Loss: 0.00000820
Iteration 56/1000 | Loss: 0.00000820
Iteration 57/1000 | Loss: 0.00000820
Iteration 58/1000 | Loss: 0.00000820
Iteration 59/1000 | Loss: 0.00000820
Iteration 60/1000 | Loss: 0.00000820
Iteration 61/1000 | Loss: 0.00000819
Iteration 62/1000 | Loss: 0.00000819
Iteration 63/1000 | Loss: 0.00000818
Iteration 64/1000 | Loss: 0.00000818
Iteration 65/1000 | Loss: 0.00000817
Iteration 66/1000 | Loss: 0.00000816
Iteration 67/1000 | Loss: 0.00000816
Iteration 68/1000 | Loss: 0.00000815
Iteration 69/1000 | Loss: 0.00000814
Iteration 70/1000 | Loss: 0.00000814
Iteration 71/1000 | Loss: 0.00000814
Iteration 72/1000 | Loss: 0.00000813
Iteration 73/1000 | Loss: 0.00000813
Iteration 74/1000 | Loss: 0.00000813
Iteration 75/1000 | Loss: 0.00000813
Iteration 76/1000 | Loss: 0.00000812
Iteration 77/1000 | Loss: 0.00000812
Iteration 78/1000 | Loss: 0.00000812
Iteration 79/1000 | Loss: 0.00000812
Iteration 80/1000 | Loss: 0.00000812
Iteration 81/1000 | Loss: 0.00000811
Iteration 82/1000 | Loss: 0.00000811
Iteration 83/1000 | Loss: 0.00000811
Iteration 84/1000 | Loss: 0.00000811
Iteration 85/1000 | Loss: 0.00000811
Iteration 86/1000 | Loss: 0.00000810
Iteration 87/1000 | Loss: 0.00000810
Iteration 88/1000 | Loss: 0.00000810
Iteration 89/1000 | Loss: 0.00000810
Iteration 90/1000 | Loss: 0.00000809
Iteration 91/1000 | Loss: 0.00000809
Iteration 92/1000 | Loss: 0.00000809
Iteration 93/1000 | Loss: 0.00000809
Iteration 94/1000 | Loss: 0.00000808
Iteration 95/1000 | Loss: 0.00000808
Iteration 96/1000 | Loss: 0.00000808
Iteration 97/1000 | Loss: 0.00000808
Iteration 98/1000 | Loss: 0.00000808
Iteration 99/1000 | Loss: 0.00000808
Iteration 100/1000 | Loss: 0.00000807
Iteration 101/1000 | Loss: 0.00000807
Iteration 102/1000 | Loss: 0.00000807
Iteration 103/1000 | Loss: 0.00000807
Iteration 104/1000 | Loss: 0.00000807
Iteration 105/1000 | Loss: 0.00000807
Iteration 106/1000 | Loss: 0.00000807
Iteration 107/1000 | Loss: 0.00000807
Iteration 108/1000 | Loss: 0.00000807
Iteration 109/1000 | Loss: 0.00000807
Iteration 110/1000 | Loss: 0.00000806
Iteration 111/1000 | Loss: 0.00000806
Iteration 112/1000 | Loss: 0.00000806
Iteration 113/1000 | Loss: 0.00000806
Iteration 114/1000 | Loss: 0.00000806
Iteration 115/1000 | Loss: 0.00000806
Iteration 116/1000 | Loss: 0.00000806
Iteration 117/1000 | Loss: 0.00000806
Iteration 118/1000 | Loss: 0.00000806
Iteration 119/1000 | Loss: 0.00000806
Iteration 120/1000 | Loss: 0.00000806
Iteration 121/1000 | Loss: 0.00000806
Iteration 122/1000 | Loss: 0.00000806
Iteration 123/1000 | Loss: 0.00000806
Iteration 124/1000 | Loss: 0.00000806
Iteration 125/1000 | Loss: 0.00000806
Iteration 126/1000 | Loss: 0.00000806
Iteration 127/1000 | Loss: 0.00000806
Iteration 128/1000 | Loss: 0.00000806
Iteration 129/1000 | Loss: 0.00000806
Iteration 130/1000 | Loss: 0.00000806
Iteration 131/1000 | Loss: 0.00000806
Iteration 132/1000 | Loss: 0.00000806
Iteration 133/1000 | Loss: 0.00000806
Iteration 134/1000 | Loss: 0.00000806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [8.057347258727532e-06, 8.057347258727532e-06, 8.057347258727532e-06, 8.057347258727532e-06, 8.057347258727532e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.057347258727532e-06

Optimization complete. Final v2v error: 2.416957139968872 mm

Highest mean error: 2.6202969551086426 mm for frame 50

Lowest mean error: 2.2672131061553955 mm for frame 243

Saving results

Total time: 32.758949756622314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389852
Iteration 2/25 | Loss: 0.00111448
Iteration 3/25 | Loss: 0.00100364
Iteration 4/25 | Loss: 0.00099437
Iteration 5/25 | Loss: 0.00099312
Iteration 6/25 | Loss: 0.00099312
Iteration 7/25 | Loss: 0.00099312
Iteration 8/25 | Loss: 0.00099312
Iteration 9/25 | Loss: 0.00099312
Iteration 10/25 | Loss: 0.00099312
Iteration 11/25 | Loss: 0.00099312
Iteration 12/25 | Loss: 0.00099312
Iteration 13/25 | Loss: 0.00099312
Iteration 14/25 | Loss: 0.00099312
Iteration 15/25 | Loss: 0.00099312
Iteration 16/25 | Loss: 0.00099312
Iteration 17/25 | Loss: 0.00099312
Iteration 18/25 | Loss: 0.00099312
Iteration 19/25 | Loss: 0.00099312
Iteration 20/25 | Loss: 0.00099312
Iteration 21/25 | Loss: 0.00099312
Iteration 22/25 | Loss: 0.00099312
Iteration 23/25 | Loss: 0.00099312
Iteration 24/25 | Loss: 0.00099312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009931224631145597, 0.0009931224631145597, 0.0009931224631145597, 0.0009931224631145597, 0.0009931224631145597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009931224631145597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53714633
Iteration 2/25 | Loss: 0.00087457
Iteration 3/25 | Loss: 0.00087457
Iteration 4/25 | Loss: 0.00087457
Iteration 5/25 | Loss: 0.00087457
Iteration 6/25 | Loss: 0.00087457
Iteration 7/25 | Loss: 0.00087456
Iteration 8/25 | Loss: 0.00087456
Iteration 9/25 | Loss: 0.00087457
Iteration 10/25 | Loss: 0.00087456
Iteration 11/25 | Loss: 0.00087456
Iteration 12/25 | Loss: 0.00087456
Iteration 13/25 | Loss: 0.00087456
Iteration 14/25 | Loss: 0.00087456
Iteration 15/25 | Loss: 0.00087456
Iteration 16/25 | Loss: 0.00087456
Iteration 17/25 | Loss: 0.00087456
Iteration 18/25 | Loss: 0.00087456
Iteration 19/25 | Loss: 0.00087456
Iteration 20/25 | Loss: 0.00087456
Iteration 21/25 | Loss: 0.00087456
Iteration 22/25 | Loss: 0.00087456
Iteration 23/25 | Loss: 0.00087456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008745627710595727, 0.0008745627710595727, 0.0008745627710595727, 0.0008745627710595727, 0.0008745627710595727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008745627710595727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087456
Iteration 2/1000 | Loss: 0.00002855
Iteration 3/1000 | Loss: 0.00001770
Iteration 4/1000 | Loss: 0.00001342
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001167
Iteration 7/1000 | Loss: 0.00001118
Iteration 8/1000 | Loss: 0.00001082
Iteration 9/1000 | Loss: 0.00001057
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001034
Iteration 14/1000 | Loss: 0.00001026
Iteration 15/1000 | Loss: 0.00001017
Iteration 16/1000 | Loss: 0.00001010
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001005
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001002
Iteration 21/1000 | Loss: 0.00001002
Iteration 22/1000 | Loss: 0.00000999
Iteration 23/1000 | Loss: 0.00000998
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000991
Iteration 27/1000 | Loss: 0.00000991
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000991
Iteration 30/1000 | Loss: 0.00000990
Iteration 31/1000 | Loss: 0.00000990
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000989
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000987
Iteration 36/1000 | Loss: 0.00000985
Iteration 37/1000 | Loss: 0.00000984
Iteration 38/1000 | Loss: 0.00000984
Iteration 39/1000 | Loss: 0.00000984
Iteration 40/1000 | Loss: 0.00000984
Iteration 41/1000 | Loss: 0.00000984
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000979
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000977
Iteration 54/1000 | Loss: 0.00000977
Iteration 55/1000 | Loss: 0.00000977
Iteration 56/1000 | Loss: 0.00000977
Iteration 57/1000 | Loss: 0.00000977
Iteration 58/1000 | Loss: 0.00000977
Iteration 59/1000 | Loss: 0.00000977
Iteration 60/1000 | Loss: 0.00000977
Iteration 61/1000 | Loss: 0.00000977
Iteration 62/1000 | Loss: 0.00000977
Iteration 63/1000 | Loss: 0.00000977
Iteration 64/1000 | Loss: 0.00000977
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000977
Iteration 68/1000 | Loss: 0.00000977
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000977
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000977
Iteration 80/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [9.765686627360992e-06, 9.765686627360992e-06, 9.765686627360992e-06, 9.765686627360992e-06, 9.765686627360992e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.765686627360992e-06

Optimization complete. Final v2v error: 2.66797137260437 mm

Highest mean error: 3.1210670471191406 mm for frame 63

Lowest mean error: 2.3137519359588623 mm for frame 194

Saving results

Total time: 33.65628528594971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856648
Iteration 2/25 | Loss: 0.00141206
Iteration 3/25 | Loss: 0.00103430
Iteration 4/25 | Loss: 0.00098769
Iteration 5/25 | Loss: 0.00098142
Iteration 6/25 | Loss: 0.00098033
Iteration 7/25 | Loss: 0.00097982
Iteration 8/25 | Loss: 0.00097948
Iteration 9/25 | Loss: 0.00097923
Iteration 10/25 | Loss: 0.00098152
Iteration 11/25 | Loss: 0.00098185
Iteration 12/25 | Loss: 0.00098167
Iteration 13/25 | Loss: 0.00098130
Iteration 14/25 | Loss: 0.00098103
Iteration 15/25 | Loss: 0.00098054
Iteration 16/25 | Loss: 0.00097951
Iteration 17/25 | Loss: 0.00097758
Iteration 18/25 | Loss: 0.00097741
Iteration 19/25 | Loss: 0.00097749
Iteration 20/25 | Loss: 0.00097775
Iteration 21/25 | Loss: 0.00097714
Iteration 22/25 | Loss: 0.00097641
Iteration 23/25 | Loss: 0.00097553
Iteration 24/25 | Loss: 0.00097515
Iteration 25/25 | Loss: 0.00097510

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29023540
Iteration 2/25 | Loss: 0.00085262
Iteration 3/25 | Loss: 0.00085262
Iteration 4/25 | Loss: 0.00085262
Iteration 5/25 | Loss: 0.00085262
Iteration 6/25 | Loss: 0.00085262
Iteration 7/25 | Loss: 0.00085262
Iteration 8/25 | Loss: 0.00085262
Iteration 9/25 | Loss: 0.00085262
Iteration 10/25 | Loss: 0.00085262
Iteration 11/25 | Loss: 0.00085262
Iteration 12/25 | Loss: 0.00085262
Iteration 13/25 | Loss: 0.00085262
Iteration 14/25 | Loss: 0.00085262
Iteration 15/25 | Loss: 0.00085262
Iteration 16/25 | Loss: 0.00085262
Iteration 17/25 | Loss: 0.00085262
Iteration 18/25 | Loss: 0.00085262
Iteration 19/25 | Loss: 0.00085262
Iteration 20/25 | Loss: 0.00085262
Iteration 21/25 | Loss: 0.00085262
Iteration 22/25 | Loss: 0.00085262
Iteration 23/25 | Loss: 0.00085262
Iteration 24/25 | Loss: 0.00085262
Iteration 25/25 | Loss: 0.00085262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085262
Iteration 2/1000 | Loss: 0.00002063
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00003235
Iteration 8/1000 | Loss: 0.00003397
Iteration 9/1000 | Loss: 0.00002506
Iteration 10/1000 | Loss: 0.00003290
Iteration 11/1000 | Loss: 0.00002703
Iteration 12/1000 | Loss: 0.00001314
Iteration 13/1000 | Loss: 0.00003241
Iteration 14/1000 | Loss: 0.00002654
Iteration 15/1000 | Loss: 0.00002907
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001419
Iteration 18/1000 | Loss: 0.00001295
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001045
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001021
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001014
Iteration 27/1000 | Loss: 0.00001012
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001008
Iteration 30/1000 | Loss: 0.00001007
Iteration 31/1000 | Loss: 0.00001007
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001004
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000995
Iteration 39/1000 | Loss: 0.00000994
Iteration 40/1000 | Loss: 0.00000994
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000992
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000991
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000990
Iteration 47/1000 | Loss: 0.00000990
Iteration 48/1000 | Loss: 0.00000990
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000986
Iteration 53/1000 | Loss: 0.00000986
Iteration 54/1000 | Loss: 0.00000986
Iteration 55/1000 | Loss: 0.00000985
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000983
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000982
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000982
Iteration 69/1000 | Loss: 0.00000981
Iteration 70/1000 | Loss: 0.00000981
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000981
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000981
Iteration 78/1000 | Loss: 0.00000981
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000979
Iteration 84/1000 | Loss: 0.00000979
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000978
Iteration 88/1000 | Loss: 0.00000978
Iteration 89/1000 | Loss: 0.00000978
Iteration 90/1000 | Loss: 0.00000977
Iteration 91/1000 | Loss: 0.00000977
Iteration 92/1000 | Loss: 0.00000977
Iteration 93/1000 | Loss: 0.00000977
Iteration 94/1000 | Loss: 0.00000977
Iteration 95/1000 | Loss: 0.00000977
Iteration 96/1000 | Loss: 0.00000976
Iteration 97/1000 | Loss: 0.00000976
Iteration 98/1000 | Loss: 0.00000976
Iteration 99/1000 | Loss: 0.00000976
Iteration 100/1000 | Loss: 0.00000976
Iteration 101/1000 | Loss: 0.00000976
Iteration 102/1000 | Loss: 0.00000976
Iteration 103/1000 | Loss: 0.00000976
Iteration 104/1000 | Loss: 0.00000976
Iteration 105/1000 | Loss: 0.00000975
Iteration 106/1000 | Loss: 0.00000975
Iteration 107/1000 | Loss: 0.00000975
Iteration 108/1000 | Loss: 0.00000975
Iteration 109/1000 | Loss: 0.00000975
Iteration 110/1000 | Loss: 0.00000974
Iteration 111/1000 | Loss: 0.00000974
Iteration 112/1000 | Loss: 0.00000974
Iteration 113/1000 | Loss: 0.00000974
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000974
Iteration 119/1000 | Loss: 0.00000974
Iteration 120/1000 | Loss: 0.00000974
Iteration 121/1000 | Loss: 0.00000974
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000972
Iteration 133/1000 | Loss: 0.00000972
Iteration 134/1000 | Loss: 0.00000972
Iteration 135/1000 | Loss: 0.00000972
Iteration 136/1000 | Loss: 0.00000972
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000971
Iteration 144/1000 | Loss: 0.00000971
Iteration 145/1000 | Loss: 0.00000971
Iteration 146/1000 | Loss: 0.00000971
Iteration 147/1000 | Loss: 0.00000971
Iteration 148/1000 | Loss: 0.00000971
Iteration 149/1000 | Loss: 0.00000971
Iteration 150/1000 | Loss: 0.00000971
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000970
Iteration 156/1000 | Loss: 0.00000970
Iteration 157/1000 | Loss: 0.00000969
Iteration 158/1000 | Loss: 0.00000969
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000969
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000968
Iteration 164/1000 | Loss: 0.00000968
Iteration 165/1000 | Loss: 0.00000968
Iteration 166/1000 | Loss: 0.00000968
Iteration 167/1000 | Loss: 0.00000968
Iteration 168/1000 | Loss: 0.00000967
Iteration 169/1000 | Loss: 0.00000967
Iteration 170/1000 | Loss: 0.00000967
Iteration 171/1000 | Loss: 0.00000967
Iteration 172/1000 | Loss: 0.00000967
Iteration 173/1000 | Loss: 0.00000967
Iteration 174/1000 | Loss: 0.00000967
Iteration 175/1000 | Loss: 0.00000967
Iteration 176/1000 | Loss: 0.00000966
Iteration 177/1000 | Loss: 0.00000966
Iteration 178/1000 | Loss: 0.00000966
Iteration 179/1000 | Loss: 0.00000965
Iteration 180/1000 | Loss: 0.00000965
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000965
Iteration 189/1000 | Loss: 0.00000965
Iteration 190/1000 | Loss: 0.00000965
Iteration 191/1000 | Loss: 0.00000965
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000964
Iteration 199/1000 | Loss: 0.00000964
Iteration 200/1000 | Loss: 0.00000964
Iteration 201/1000 | Loss: 0.00000964
Iteration 202/1000 | Loss: 0.00000964
Iteration 203/1000 | Loss: 0.00000964
Iteration 204/1000 | Loss: 0.00000964
Iteration 205/1000 | Loss: 0.00000964
Iteration 206/1000 | Loss: 0.00000964
Iteration 207/1000 | Loss: 0.00000964
Iteration 208/1000 | Loss: 0.00000964
Iteration 209/1000 | Loss: 0.00000964
Iteration 210/1000 | Loss: 0.00000964
Iteration 211/1000 | Loss: 0.00000964
Iteration 212/1000 | Loss: 0.00000964
Iteration 213/1000 | Loss: 0.00000964
Iteration 214/1000 | Loss: 0.00000964
Iteration 215/1000 | Loss: 0.00000964
Iteration 216/1000 | Loss: 0.00000964
Iteration 217/1000 | Loss: 0.00000964
Iteration 218/1000 | Loss: 0.00000964
Iteration 219/1000 | Loss: 0.00000964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [9.639062227506656e-06, 9.639062227506656e-06, 9.639062227506656e-06, 9.639062227506656e-06, 9.639062227506656e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.639062227506656e-06

Optimization complete. Final v2v error: 2.655519723892212 mm

Highest mean error: 3.9780526161193848 mm for frame 76

Lowest mean error: 2.449281930923462 mm for frame 143

Saving results

Total time: 90.32122159004211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903166
Iteration 2/25 | Loss: 0.00122152
Iteration 3/25 | Loss: 0.00111700
Iteration 4/25 | Loss: 0.00109317
Iteration 5/25 | Loss: 0.00108470
Iteration 6/25 | Loss: 0.00108305
Iteration 7/25 | Loss: 0.00108293
Iteration 8/25 | Loss: 0.00108293
Iteration 9/25 | Loss: 0.00108293
Iteration 10/25 | Loss: 0.00108293
Iteration 11/25 | Loss: 0.00108293
Iteration 12/25 | Loss: 0.00108293
Iteration 13/25 | Loss: 0.00108293
Iteration 14/25 | Loss: 0.00108293
Iteration 15/25 | Loss: 0.00108293
Iteration 16/25 | Loss: 0.00108293
Iteration 17/25 | Loss: 0.00108293
Iteration 18/25 | Loss: 0.00108293
Iteration 19/25 | Loss: 0.00108293
Iteration 20/25 | Loss: 0.00108293
Iteration 21/25 | Loss: 0.00108293
Iteration 22/25 | Loss: 0.00108293
Iteration 23/25 | Loss: 0.00108293
Iteration 24/25 | Loss: 0.00108293
Iteration 25/25 | Loss: 0.00108293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23493314
Iteration 2/25 | Loss: 0.00114549
Iteration 3/25 | Loss: 0.00114542
Iteration 4/25 | Loss: 0.00114542
Iteration 5/25 | Loss: 0.00114542
Iteration 6/25 | Loss: 0.00114542
Iteration 7/25 | Loss: 0.00114542
Iteration 8/25 | Loss: 0.00114542
Iteration 9/25 | Loss: 0.00114542
Iteration 10/25 | Loss: 0.00114542
Iteration 11/25 | Loss: 0.00114542
Iteration 12/25 | Loss: 0.00114542
Iteration 13/25 | Loss: 0.00114542
Iteration 14/25 | Loss: 0.00114542
Iteration 15/25 | Loss: 0.00114542
Iteration 16/25 | Loss: 0.00114542
Iteration 17/25 | Loss: 0.00114542
Iteration 18/25 | Loss: 0.00114542
Iteration 19/25 | Loss: 0.00114542
Iteration 20/25 | Loss: 0.00114542
Iteration 21/25 | Loss: 0.00114542
Iteration 22/25 | Loss: 0.00114542
Iteration 23/25 | Loss: 0.00114542
Iteration 24/25 | Loss: 0.00114542
Iteration 25/25 | Loss: 0.00114542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114542
Iteration 2/1000 | Loss: 0.00006383
Iteration 3/1000 | Loss: 0.00003570
Iteration 4/1000 | Loss: 0.00002926
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002662
Iteration 7/1000 | Loss: 0.00002589
Iteration 8/1000 | Loss: 0.00002517
Iteration 9/1000 | Loss: 0.00002468
Iteration 10/1000 | Loss: 0.00002432
Iteration 11/1000 | Loss: 0.00002402
Iteration 12/1000 | Loss: 0.00002382
Iteration 13/1000 | Loss: 0.00002380
Iteration 14/1000 | Loss: 0.00002376
Iteration 15/1000 | Loss: 0.00002375
Iteration 16/1000 | Loss: 0.00002374
Iteration 17/1000 | Loss: 0.00002373
Iteration 18/1000 | Loss: 0.00002369
Iteration 19/1000 | Loss: 0.00002363
Iteration 20/1000 | Loss: 0.00002349
Iteration 21/1000 | Loss: 0.00002349
Iteration 22/1000 | Loss: 0.00002348
Iteration 23/1000 | Loss: 0.00002347
Iteration 24/1000 | Loss: 0.00002343
Iteration 25/1000 | Loss: 0.00002341
Iteration 26/1000 | Loss: 0.00002338
Iteration 27/1000 | Loss: 0.00002338
Iteration 28/1000 | Loss: 0.00002333
Iteration 29/1000 | Loss: 0.00002333
Iteration 30/1000 | Loss: 0.00002331
Iteration 31/1000 | Loss: 0.00002331
Iteration 32/1000 | Loss: 0.00002331
Iteration 33/1000 | Loss: 0.00002331
Iteration 34/1000 | Loss: 0.00002331
Iteration 35/1000 | Loss: 0.00002330
Iteration 36/1000 | Loss: 0.00002330
Iteration 37/1000 | Loss: 0.00002329
Iteration 38/1000 | Loss: 0.00002329
Iteration 39/1000 | Loss: 0.00002329
Iteration 40/1000 | Loss: 0.00002328
Iteration 41/1000 | Loss: 0.00002328
Iteration 42/1000 | Loss: 0.00002328
Iteration 43/1000 | Loss: 0.00002327
Iteration 44/1000 | Loss: 0.00002327
Iteration 45/1000 | Loss: 0.00002326
Iteration 46/1000 | Loss: 0.00002326
Iteration 47/1000 | Loss: 0.00002326
Iteration 48/1000 | Loss: 0.00002326
Iteration 49/1000 | Loss: 0.00002326
Iteration 50/1000 | Loss: 0.00002325
Iteration 51/1000 | Loss: 0.00002325
Iteration 52/1000 | Loss: 0.00002325
Iteration 53/1000 | Loss: 0.00002324
Iteration 54/1000 | Loss: 0.00002324
Iteration 55/1000 | Loss: 0.00002324
Iteration 56/1000 | Loss: 0.00002324
Iteration 57/1000 | Loss: 0.00002324
Iteration 58/1000 | Loss: 0.00002323
Iteration 59/1000 | Loss: 0.00002323
Iteration 60/1000 | Loss: 0.00002323
Iteration 61/1000 | Loss: 0.00002323
Iteration 62/1000 | Loss: 0.00002322
Iteration 63/1000 | Loss: 0.00002322
Iteration 64/1000 | Loss: 0.00002321
Iteration 65/1000 | Loss: 0.00002321
Iteration 66/1000 | Loss: 0.00002321
Iteration 67/1000 | Loss: 0.00002321
Iteration 68/1000 | Loss: 0.00002321
Iteration 69/1000 | Loss: 0.00002321
Iteration 70/1000 | Loss: 0.00002321
Iteration 71/1000 | Loss: 0.00002320
Iteration 72/1000 | Loss: 0.00002320
Iteration 73/1000 | Loss: 0.00002320
Iteration 74/1000 | Loss: 0.00002320
Iteration 75/1000 | Loss: 0.00002320
Iteration 76/1000 | Loss: 0.00002320
Iteration 77/1000 | Loss: 0.00002320
Iteration 78/1000 | Loss: 0.00002320
Iteration 79/1000 | Loss: 0.00002320
Iteration 80/1000 | Loss: 0.00002319
Iteration 81/1000 | Loss: 0.00002319
Iteration 82/1000 | Loss: 0.00002319
Iteration 83/1000 | Loss: 0.00002319
Iteration 84/1000 | Loss: 0.00002319
Iteration 85/1000 | Loss: 0.00002319
Iteration 86/1000 | Loss: 0.00002319
Iteration 87/1000 | Loss: 0.00002319
Iteration 88/1000 | Loss: 0.00002319
Iteration 89/1000 | Loss: 0.00002319
Iteration 90/1000 | Loss: 0.00002319
Iteration 91/1000 | Loss: 0.00002319
Iteration 92/1000 | Loss: 0.00002318
Iteration 93/1000 | Loss: 0.00002318
Iteration 94/1000 | Loss: 0.00002318
Iteration 95/1000 | Loss: 0.00002318
Iteration 96/1000 | Loss: 0.00002318
Iteration 97/1000 | Loss: 0.00002318
Iteration 98/1000 | Loss: 0.00002318
Iteration 99/1000 | Loss: 0.00002318
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002317
Iteration 103/1000 | Loss: 0.00002317
Iteration 104/1000 | Loss: 0.00002317
Iteration 105/1000 | Loss: 0.00002317
Iteration 106/1000 | Loss: 0.00002317
Iteration 107/1000 | Loss: 0.00002317
Iteration 108/1000 | Loss: 0.00002317
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002316
Iteration 118/1000 | Loss: 0.00002316
Iteration 119/1000 | Loss: 0.00002315
Iteration 120/1000 | Loss: 0.00002315
Iteration 121/1000 | Loss: 0.00002315
Iteration 122/1000 | Loss: 0.00002315
Iteration 123/1000 | Loss: 0.00002314
Iteration 124/1000 | Loss: 0.00002314
Iteration 125/1000 | Loss: 0.00002314
Iteration 126/1000 | Loss: 0.00002314
Iteration 127/1000 | Loss: 0.00002314
Iteration 128/1000 | Loss: 0.00002314
Iteration 129/1000 | Loss: 0.00002313
Iteration 130/1000 | Loss: 0.00002313
Iteration 131/1000 | Loss: 0.00002313
Iteration 132/1000 | Loss: 0.00002313
Iteration 133/1000 | Loss: 0.00002313
Iteration 134/1000 | Loss: 0.00002313
Iteration 135/1000 | Loss: 0.00002313
Iteration 136/1000 | Loss: 0.00002313
Iteration 137/1000 | Loss: 0.00002313
Iteration 138/1000 | Loss: 0.00002313
Iteration 139/1000 | Loss: 0.00002313
Iteration 140/1000 | Loss: 0.00002313
Iteration 141/1000 | Loss: 0.00002313
Iteration 142/1000 | Loss: 0.00002313
Iteration 143/1000 | Loss: 0.00002313
Iteration 144/1000 | Loss: 0.00002313
Iteration 145/1000 | Loss: 0.00002313
Iteration 146/1000 | Loss: 0.00002313
Iteration 147/1000 | Loss: 0.00002313
Iteration 148/1000 | Loss: 0.00002313
Iteration 149/1000 | Loss: 0.00002313
Iteration 150/1000 | Loss: 0.00002313
Iteration 151/1000 | Loss: 0.00002313
Iteration 152/1000 | Loss: 0.00002313
Iteration 153/1000 | Loss: 0.00002313
Iteration 154/1000 | Loss: 0.00002313
Iteration 155/1000 | Loss: 0.00002313
Iteration 156/1000 | Loss: 0.00002313
Iteration 157/1000 | Loss: 0.00002313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.3129929104470648e-05, 2.3129929104470648e-05, 2.3129929104470648e-05, 2.3129929104470648e-05, 2.3129929104470648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3129929104470648e-05

Optimization complete. Final v2v error: 4.072517395019531 mm

Highest mean error: 4.519970417022705 mm for frame 116

Lowest mean error: 3.255021810531616 mm for frame 0

Saving results

Total time: 39.959638595581055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1130/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1130/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954011
Iteration 2/25 | Loss: 0.00108095
Iteration 3/25 | Loss: 0.00099479
Iteration 4/25 | Loss: 0.00098238
Iteration 5/25 | Loss: 0.00097799
Iteration 6/25 | Loss: 0.00097750
Iteration 7/25 | Loss: 0.00097750
Iteration 8/25 | Loss: 0.00097750
Iteration 9/25 | Loss: 0.00097750
Iteration 10/25 | Loss: 0.00097750
Iteration 11/25 | Loss: 0.00097750
Iteration 12/25 | Loss: 0.00097750
Iteration 13/25 | Loss: 0.00097750
Iteration 14/25 | Loss: 0.00097750
Iteration 15/25 | Loss: 0.00097750
Iteration 16/25 | Loss: 0.00097750
Iteration 17/25 | Loss: 0.00097750
Iteration 18/25 | Loss: 0.00097750
Iteration 19/25 | Loss: 0.00097750
Iteration 20/25 | Loss: 0.00097750
Iteration 21/25 | Loss: 0.00097750
Iteration 22/25 | Loss: 0.00097750
Iteration 23/25 | Loss: 0.00097750
Iteration 24/25 | Loss: 0.00097750
Iteration 25/25 | Loss: 0.00097750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37040746
Iteration 2/25 | Loss: 0.00096618
Iteration 3/25 | Loss: 0.00096618
Iteration 4/25 | Loss: 0.00096618
Iteration 5/25 | Loss: 0.00096618
Iteration 6/25 | Loss: 0.00096618
Iteration 7/25 | Loss: 0.00096618
Iteration 8/25 | Loss: 0.00096618
Iteration 9/25 | Loss: 0.00096618
Iteration 10/25 | Loss: 0.00096618
Iteration 11/25 | Loss: 0.00096618
Iteration 12/25 | Loss: 0.00096618
Iteration 13/25 | Loss: 0.00096618
Iteration 14/25 | Loss: 0.00096618
Iteration 15/25 | Loss: 0.00096618
Iteration 16/25 | Loss: 0.00096618
Iteration 17/25 | Loss: 0.00096618
Iteration 18/25 | Loss: 0.00096618
Iteration 19/25 | Loss: 0.00096618
Iteration 20/25 | Loss: 0.00096618
Iteration 21/25 | Loss: 0.00096618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009661782532930374, 0.0009661782532930374, 0.0009661782532930374, 0.0009661782532930374, 0.0009661782532930374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009661782532930374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096618
Iteration 2/1000 | Loss: 0.00001797
Iteration 3/1000 | Loss: 0.00001346
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001121
Iteration 6/1000 | Loss: 0.00001096
Iteration 7/1000 | Loss: 0.00001068
Iteration 8/1000 | Loss: 0.00001067
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001047
Iteration 13/1000 | Loss: 0.00001046
Iteration 14/1000 | Loss: 0.00001045
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001039
Iteration 17/1000 | Loss: 0.00001039
Iteration 18/1000 | Loss: 0.00001039
Iteration 19/1000 | Loss: 0.00001039
Iteration 20/1000 | Loss: 0.00001039
Iteration 21/1000 | Loss: 0.00001038
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001038
Iteration 24/1000 | Loss: 0.00001038
Iteration 25/1000 | Loss: 0.00001038
Iteration 26/1000 | Loss: 0.00001037
Iteration 27/1000 | Loss: 0.00001037
Iteration 28/1000 | Loss: 0.00001037
Iteration 29/1000 | Loss: 0.00001037
Iteration 30/1000 | Loss: 0.00001037
Iteration 31/1000 | Loss: 0.00001037
Iteration 32/1000 | Loss: 0.00001036
Iteration 33/1000 | Loss: 0.00001036
Iteration 34/1000 | Loss: 0.00001035
Iteration 35/1000 | Loss: 0.00001035
Iteration 36/1000 | Loss: 0.00001035
Iteration 37/1000 | Loss: 0.00001035
Iteration 38/1000 | Loss: 0.00001035
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001033
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001032
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001032
Iteration 53/1000 | Loss: 0.00001032
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001032
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001031
Iteration 59/1000 | Loss: 0.00001031
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001030
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001028
Iteration 69/1000 | Loss: 0.00001028
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001028
Iteration 73/1000 | Loss: 0.00001028
Iteration 74/1000 | Loss: 0.00001028
Iteration 75/1000 | Loss: 0.00001028
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001028
Iteration 78/1000 | Loss: 0.00001028
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001027
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001026
Iteration 86/1000 | Loss: 0.00001026
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001025
Iteration 90/1000 | Loss: 0.00001025
Iteration 91/1000 | Loss: 0.00001025
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001025
Iteration 94/1000 | Loss: 0.00001025
Iteration 95/1000 | Loss: 0.00001025
Iteration 96/1000 | Loss: 0.00001024
Iteration 97/1000 | Loss: 0.00001024
Iteration 98/1000 | Loss: 0.00001024
Iteration 99/1000 | Loss: 0.00001024
Iteration 100/1000 | Loss: 0.00001024
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.0241706149827223e-05, 1.0241706149827223e-05, 1.0241706149827223e-05, 1.0241706149827223e-05, 1.0241706149827223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0241706149827223e-05

Optimization complete. Final v2v error: 2.668297529220581 mm

Highest mean error: 3.1978867053985596 mm for frame 146

Lowest mean error: 2.3201746940612793 mm for frame 10

Saving results

Total time: 30.67244052886963
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762322
Iteration 2/25 | Loss: 0.00144742
Iteration 3/25 | Loss: 0.00132039
Iteration 4/25 | Loss: 0.00130477
Iteration 5/25 | Loss: 0.00129996
Iteration 6/25 | Loss: 0.00129922
Iteration 7/25 | Loss: 0.00129922
Iteration 8/25 | Loss: 0.00129922
Iteration 9/25 | Loss: 0.00129922
Iteration 10/25 | Loss: 0.00129922
Iteration 11/25 | Loss: 0.00129922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012992214178666472, 0.0012992214178666472, 0.0012992214178666472, 0.0012992214178666472, 0.0012992214178666472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012992214178666472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45897365
Iteration 2/25 | Loss: 0.00089335
Iteration 3/25 | Loss: 0.00089333
Iteration 4/25 | Loss: 0.00089333
Iteration 5/25 | Loss: 0.00089333
Iteration 6/25 | Loss: 0.00089333
Iteration 7/25 | Loss: 0.00089333
Iteration 8/25 | Loss: 0.00089333
Iteration 9/25 | Loss: 0.00089333
Iteration 10/25 | Loss: 0.00089333
Iteration 11/25 | Loss: 0.00089333
Iteration 12/25 | Loss: 0.00089333
Iteration 13/25 | Loss: 0.00089333
Iteration 14/25 | Loss: 0.00089333
Iteration 15/25 | Loss: 0.00089333
Iteration 16/25 | Loss: 0.00089333
Iteration 17/25 | Loss: 0.00089333
Iteration 18/25 | Loss: 0.00089333
Iteration 19/25 | Loss: 0.00089333
Iteration 20/25 | Loss: 0.00089333
Iteration 21/25 | Loss: 0.00089333
Iteration 22/25 | Loss: 0.00089333
Iteration 23/25 | Loss: 0.00089333
Iteration 24/25 | Loss: 0.00089333
Iteration 25/25 | Loss: 0.00089333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089333
Iteration 2/1000 | Loss: 0.00003974
Iteration 3/1000 | Loss: 0.00002497
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002149
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002017
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001958
Iteration 10/1000 | Loss: 0.00001932
Iteration 11/1000 | Loss: 0.00001930
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001911
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001886
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001880
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001880
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001873
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001869
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001868
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001867
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001861
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001860
Iteration 72/1000 | Loss: 0.00001860
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001855
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001845
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001844
Iteration 128/1000 | Loss: 0.00001844
Iteration 129/1000 | Loss: 0.00001844
Iteration 130/1000 | Loss: 0.00001844
Iteration 131/1000 | Loss: 0.00001844
Iteration 132/1000 | Loss: 0.00001844
Iteration 133/1000 | Loss: 0.00001844
Iteration 134/1000 | Loss: 0.00001844
Iteration 135/1000 | Loss: 0.00001844
Iteration 136/1000 | Loss: 0.00001844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.8443635781295598e-05, 1.8443635781295598e-05, 1.8443635781295598e-05, 1.8443635781295598e-05, 1.8443635781295598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8443635781295598e-05

Optimization complete. Final v2v error: 3.5941598415374756 mm

Highest mean error: 4.457366466522217 mm for frame 142

Lowest mean error: 3.1612770557403564 mm for frame 114

Saving results

Total time: 41.72245240211487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392217
Iteration 2/25 | Loss: 0.00131241
Iteration 3/25 | Loss: 0.00123107
Iteration 4/25 | Loss: 0.00121864
Iteration 5/25 | Loss: 0.00121452
Iteration 6/25 | Loss: 0.00121423
Iteration 7/25 | Loss: 0.00121423
Iteration 8/25 | Loss: 0.00121423
Iteration 9/25 | Loss: 0.00121423
Iteration 10/25 | Loss: 0.00121423
Iteration 11/25 | Loss: 0.00121423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012142312480136752, 0.0012142312480136752, 0.0012142312480136752, 0.0012142312480136752, 0.0012142312480136752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012142312480136752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43372810
Iteration 2/25 | Loss: 0.00073445
Iteration 3/25 | Loss: 0.00073445
Iteration 4/25 | Loss: 0.00073445
Iteration 5/25 | Loss: 0.00073445
Iteration 6/25 | Loss: 0.00073445
Iteration 7/25 | Loss: 0.00073445
Iteration 8/25 | Loss: 0.00073445
Iteration 9/25 | Loss: 0.00073445
Iteration 10/25 | Loss: 0.00073445
Iteration 11/25 | Loss: 0.00073445
Iteration 12/25 | Loss: 0.00073445
Iteration 13/25 | Loss: 0.00073445
Iteration 14/25 | Loss: 0.00073445
Iteration 15/25 | Loss: 0.00073445
Iteration 16/25 | Loss: 0.00073445
Iteration 17/25 | Loss: 0.00073445
Iteration 18/25 | Loss: 0.00073445
Iteration 19/25 | Loss: 0.00073445
Iteration 20/25 | Loss: 0.00073445
Iteration 21/25 | Loss: 0.00073445
Iteration 22/25 | Loss: 0.00073445
Iteration 23/25 | Loss: 0.00073445
Iteration 24/25 | Loss: 0.00073445
Iteration 25/25 | Loss: 0.00073445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073445
Iteration 2/1000 | Loss: 0.00004872
Iteration 3/1000 | Loss: 0.00003212
Iteration 4/1000 | Loss: 0.00002628
Iteration 5/1000 | Loss: 0.00002393
Iteration 6/1000 | Loss: 0.00002235
Iteration 7/1000 | Loss: 0.00002108
Iteration 8/1000 | Loss: 0.00002041
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001904
Iteration 13/1000 | Loss: 0.00001882
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001868
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001864
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001840
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001839
Iteration 30/1000 | Loss: 0.00001839
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001838
Iteration 33/1000 | Loss: 0.00001837
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001832
Iteration 37/1000 | Loss: 0.00001832
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001832
Iteration 40/1000 | Loss: 0.00001832
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001831
Iteration 43/1000 | Loss: 0.00001831
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001830
Iteration 48/1000 | Loss: 0.00001829
Iteration 49/1000 | Loss: 0.00001829
Iteration 50/1000 | Loss: 0.00001829
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001824
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001823
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001822
Iteration 66/1000 | Loss: 0.00001821
Iteration 67/1000 | Loss: 0.00001821
Iteration 68/1000 | Loss: 0.00001821
Iteration 69/1000 | Loss: 0.00001821
Iteration 70/1000 | Loss: 0.00001820
Iteration 71/1000 | Loss: 0.00001820
Iteration 72/1000 | Loss: 0.00001820
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001820
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001819
Iteration 79/1000 | Loss: 0.00001819
Iteration 80/1000 | Loss: 0.00001819
Iteration 81/1000 | Loss: 0.00001819
Iteration 82/1000 | Loss: 0.00001819
Iteration 83/1000 | Loss: 0.00001819
Iteration 84/1000 | Loss: 0.00001819
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001818
Iteration 87/1000 | Loss: 0.00001818
Iteration 88/1000 | Loss: 0.00001818
Iteration 89/1000 | Loss: 0.00001818
Iteration 90/1000 | Loss: 0.00001818
Iteration 91/1000 | Loss: 0.00001817
Iteration 92/1000 | Loss: 0.00001817
Iteration 93/1000 | Loss: 0.00001817
Iteration 94/1000 | Loss: 0.00001817
Iteration 95/1000 | Loss: 0.00001817
Iteration 96/1000 | Loss: 0.00001817
Iteration 97/1000 | Loss: 0.00001816
Iteration 98/1000 | Loss: 0.00001816
Iteration 99/1000 | Loss: 0.00001816
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001815
Iteration 102/1000 | Loss: 0.00001815
Iteration 103/1000 | Loss: 0.00001815
Iteration 104/1000 | Loss: 0.00001815
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001815
Iteration 108/1000 | Loss: 0.00001815
Iteration 109/1000 | Loss: 0.00001815
Iteration 110/1000 | Loss: 0.00001815
Iteration 111/1000 | Loss: 0.00001814
Iteration 112/1000 | Loss: 0.00001814
Iteration 113/1000 | Loss: 0.00001814
Iteration 114/1000 | Loss: 0.00001814
Iteration 115/1000 | Loss: 0.00001814
Iteration 116/1000 | Loss: 0.00001814
Iteration 117/1000 | Loss: 0.00001814
Iteration 118/1000 | Loss: 0.00001814
Iteration 119/1000 | Loss: 0.00001814
Iteration 120/1000 | Loss: 0.00001813
Iteration 121/1000 | Loss: 0.00001813
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Iteration 124/1000 | Loss: 0.00001813
Iteration 125/1000 | Loss: 0.00001813
Iteration 126/1000 | Loss: 0.00001813
Iteration 127/1000 | Loss: 0.00001813
Iteration 128/1000 | Loss: 0.00001813
Iteration 129/1000 | Loss: 0.00001812
Iteration 130/1000 | Loss: 0.00001812
Iteration 131/1000 | Loss: 0.00001812
Iteration 132/1000 | Loss: 0.00001812
Iteration 133/1000 | Loss: 0.00001812
Iteration 134/1000 | Loss: 0.00001812
Iteration 135/1000 | Loss: 0.00001811
Iteration 136/1000 | Loss: 0.00001811
Iteration 137/1000 | Loss: 0.00001811
Iteration 138/1000 | Loss: 0.00001811
Iteration 139/1000 | Loss: 0.00001811
Iteration 140/1000 | Loss: 0.00001811
Iteration 141/1000 | Loss: 0.00001811
Iteration 142/1000 | Loss: 0.00001810
Iteration 143/1000 | Loss: 0.00001810
Iteration 144/1000 | Loss: 0.00001810
Iteration 145/1000 | Loss: 0.00001810
Iteration 146/1000 | Loss: 0.00001809
Iteration 147/1000 | Loss: 0.00001809
Iteration 148/1000 | Loss: 0.00001809
Iteration 149/1000 | Loss: 0.00001809
Iteration 150/1000 | Loss: 0.00001809
Iteration 151/1000 | Loss: 0.00001809
Iteration 152/1000 | Loss: 0.00001809
Iteration 153/1000 | Loss: 0.00001809
Iteration 154/1000 | Loss: 0.00001809
Iteration 155/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.8086382624460384e-05, 1.8086382624460384e-05, 1.8086382624460384e-05, 1.8086382624460384e-05, 1.8086382624460384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8086382624460384e-05

Optimization complete. Final v2v error: 3.551154613494873 mm

Highest mean error: 4.424544334411621 mm for frame 144

Lowest mean error: 2.8600802421569824 mm for frame 2

Saving results

Total time: 45.86808395385742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877752
Iteration 2/25 | Loss: 0.00179186
Iteration 3/25 | Loss: 0.00142671
Iteration 4/25 | Loss: 0.00135958
Iteration 5/25 | Loss: 0.00134978
Iteration 6/25 | Loss: 0.00133376
Iteration 7/25 | Loss: 0.00132942
Iteration 8/25 | Loss: 0.00131757
Iteration 9/25 | Loss: 0.00131813
Iteration 10/25 | Loss: 0.00131155
Iteration 11/25 | Loss: 0.00130898
Iteration 12/25 | Loss: 0.00130859
Iteration 13/25 | Loss: 0.00131422
Iteration 14/25 | Loss: 0.00131299
Iteration 15/25 | Loss: 0.00130865
Iteration 16/25 | Loss: 0.00130698
Iteration 17/25 | Loss: 0.00130675
Iteration 18/25 | Loss: 0.00130665
Iteration 19/25 | Loss: 0.00130665
Iteration 20/25 | Loss: 0.00130665
Iteration 21/25 | Loss: 0.00130665
Iteration 22/25 | Loss: 0.00130665
Iteration 23/25 | Loss: 0.00130665
Iteration 24/25 | Loss: 0.00130665
Iteration 25/25 | Loss: 0.00130665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.78881359
Iteration 2/25 | Loss: 0.00077065
Iteration 3/25 | Loss: 0.00077065
Iteration 4/25 | Loss: 0.00077064
Iteration 5/25 | Loss: 0.00077064
Iteration 6/25 | Loss: 0.00077064
Iteration 7/25 | Loss: 0.00077064
Iteration 8/25 | Loss: 0.00077064
Iteration 9/25 | Loss: 0.00077064
Iteration 10/25 | Loss: 0.00077064
Iteration 11/25 | Loss: 0.00077064
Iteration 12/25 | Loss: 0.00077064
Iteration 13/25 | Loss: 0.00077064
Iteration 14/25 | Loss: 0.00077064
Iteration 15/25 | Loss: 0.00077064
Iteration 16/25 | Loss: 0.00077064
Iteration 17/25 | Loss: 0.00077064
Iteration 18/25 | Loss: 0.00077064
Iteration 19/25 | Loss: 0.00077064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007706434116698802, 0.0007706434116698802, 0.0007706434116698802, 0.0007706434116698802, 0.0007706434116698802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007706434116698802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077064
Iteration 2/1000 | Loss: 0.00005363
Iteration 3/1000 | Loss: 0.00027932
Iteration 4/1000 | Loss: 0.00019051
Iteration 5/1000 | Loss: 0.00007038
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00016181
Iteration 8/1000 | Loss: 0.00022805
Iteration 9/1000 | Loss: 0.00009158
Iteration 10/1000 | Loss: 0.00015075
Iteration 11/1000 | Loss: 0.00015050
Iteration 12/1000 | Loss: 0.00019531
Iteration 13/1000 | Loss: 0.00011704
Iteration 14/1000 | Loss: 0.00017550
Iteration 15/1000 | Loss: 0.00033953
Iteration 16/1000 | Loss: 0.00004467
Iteration 17/1000 | Loss: 0.00054443
Iteration 18/1000 | Loss: 0.00019248
Iteration 19/1000 | Loss: 0.00032863
Iteration 20/1000 | Loss: 0.00016224
Iteration 21/1000 | Loss: 0.00031770
Iteration 22/1000 | Loss: 0.00013697
Iteration 23/1000 | Loss: 0.00003660
Iteration 24/1000 | Loss: 0.00012846
Iteration 25/1000 | Loss: 0.00019390
Iteration 26/1000 | Loss: 0.00018349
Iteration 27/1000 | Loss: 0.00023664
Iteration 28/1000 | Loss: 0.00013688
Iteration 29/1000 | Loss: 0.00003568
Iteration 30/1000 | Loss: 0.00002992
Iteration 31/1000 | Loss: 0.00023416
Iteration 32/1000 | Loss: 0.00014972
Iteration 33/1000 | Loss: 0.00021123
Iteration 34/1000 | Loss: 0.00015259
Iteration 35/1000 | Loss: 0.00019115
Iteration 36/1000 | Loss: 0.00014793
Iteration 37/1000 | Loss: 0.00019133
Iteration 38/1000 | Loss: 0.00014773
Iteration 39/1000 | Loss: 0.00013547
Iteration 40/1000 | Loss: 0.00003150
Iteration 41/1000 | Loss: 0.00002846
Iteration 42/1000 | Loss: 0.00002674
Iteration 43/1000 | Loss: 0.00002508
Iteration 44/1000 | Loss: 0.00002411
Iteration 45/1000 | Loss: 0.00002357
Iteration 46/1000 | Loss: 0.00002276
Iteration 47/1000 | Loss: 0.00002236
Iteration 48/1000 | Loss: 0.00002203
Iteration 49/1000 | Loss: 0.00002171
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002152
Iteration 52/1000 | Loss: 0.00002151
Iteration 53/1000 | Loss: 0.00002151
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002144
Iteration 56/1000 | Loss: 0.00002143
Iteration 57/1000 | Loss: 0.00002143
Iteration 58/1000 | Loss: 0.00002142
Iteration 59/1000 | Loss: 0.00002142
Iteration 60/1000 | Loss: 0.00002141
Iteration 61/1000 | Loss: 0.00002141
Iteration 62/1000 | Loss: 0.00002141
Iteration 63/1000 | Loss: 0.00002136
Iteration 64/1000 | Loss: 0.00002131
Iteration 65/1000 | Loss: 0.00002131
Iteration 66/1000 | Loss: 0.00002127
Iteration 67/1000 | Loss: 0.00002124
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002110
Iteration 70/1000 | Loss: 0.00002100
Iteration 71/1000 | Loss: 0.00002100
Iteration 72/1000 | Loss: 0.00002099
Iteration 73/1000 | Loss: 0.00002099
Iteration 74/1000 | Loss: 0.00002098
Iteration 75/1000 | Loss: 0.00002098
Iteration 76/1000 | Loss: 0.00002097
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002097
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002095
Iteration 82/1000 | Loss: 0.00002095
Iteration 83/1000 | Loss: 0.00002095
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002094
Iteration 86/1000 | Loss: 0.00002094
Iteration 87/1000 | Loss: 0.00002094
Iteration 88/1000 | Loss: 0.00002094
Iteration 89/1000 | Loss: 0.00002094
Iteration 90/1000 | Loss: 0.00002093
Iteration 91/1000 | Loss: 0.00002093
Iteration 92/1000 | Loss: 0.00002093
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00002093
Iteration 95/1000 | Loss: 0.00002092
Iteration 96/1000 | Loss: 0.00002092
Iteration 97/1000 | Loss: 0.00002092
Iteration 98/1000 | Loss: 0.00002092
Iteration 99/1000 | Loss: 0.00002092
Iteration 100/1000 | Loss: 0.00002092
Iteration 101/1000 | Loss: 0.00002091
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002090
Iteration 104/1000 | Loss: 0.00002090
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002089
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002089
Iteration 110/1000 | Loss: 0.00002089
Iteration 111/1000 | Loss: 0.00002089
Iteration 112/1000 | Loss: 0.00002089
Iteration 113/1000 | Loss: 0.00002089
Iteration 114/1000 | Loss: 0.00002089
Iteration 115/1000 | Loss: 0.00002089
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002088
Iteration 118/1000 | Loss: 0.00002088
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002087
Iteration 122/1000 | Loss: 0.00002087
Iteration 123/1000 | Loss: 0.00002087
Iteration 124/1000 | Loss: 0.00002087
Iteration 125/1000 | Loss: 0.00002086
Iteration 126/1000 | Loss: 0.00002086
Iteration 127/1000 | Loss: 0.00002086
Iteration 128/1000 | Loss: 0.00002086
Iteration 129/1000 | Loss: 0.00002086
Iteration 130/1000 | Loss: 0.00002086
Iteration 131/1000 | Loss: 0.00002086
Iteration 132/1000 | Loss: 0.00002086
Iteration 133/1000 | Loss: 0.00002086
Iteration 134/1000 | Loss: 0.00002085
Iteration 135/1000 | Loss: 0.00002085
Iteration 136/1000 | Loss: 0.00002085
Iteration 137/1000 | Loss: 0.00002085
Iteration 138/1000 | Loss: 0.00002085
Iteration 139/1000 | Loss: 0.00002085
Iteration 140/1000 | Loss: 0.00002085
Iteration 141/1000 | Loss: 0.00002084
Iteration 142/1000 | Loss: 0.00002084
Iteration 143/1000 | Loss: 0.00002084
Iteration 144/1000 | Loss: 0.00002084
Iteration 145/1000 | Loss: 0.00002084
Iteration 146/1000 | Loss: 0.00002084
Iteration 147/1000 | Loss: 0.00002084
Iteration 148/1000 | Loss: 0.00002084
Iteration 149/1000 | Loss: 0.00002084
Iteration 150/1000 | Loss: 0.00002083
Iteration 151/1000 | Loss: 0.00002083
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002083
Iteration 154/1000 | Loss: 0.00002083
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002083
Iteration 157/1000 | Loss: 0.00002083
Iteration 158/1000 | Loss: 0.00002083
Iteration 159/1000 | Loss: 0.00002083
Iteration 160/1000 | Loss: 0.00002083
Iteration 161/1000 | Loss: 0.00002083
Iteration 162/1000 | Loss: 0.00002083
Iteration 163/1000 | Loss: 0.00002083
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Iteration 166/1000 | Loss: 0.00002083
Iteration 167/1000 | Loss: 0.00002083
Iteration 168/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.082772334688343e-05, 2.082772334688343e-05, 2.082772334688343e-05, 2.082772334688343e-05, 2.082772334688343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.082772334688343e-05

Optimization complete. Final v2v error: 3.7891018390655518 mm

Highest mean error: 5.684096813201904 mm for frame 98

Lowest mean error: 3.16763973236084 mm for frame 19

Saving results

Total time: 115.81295204162598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680576
Iteration 2/25 | Loss: 0.00174939
Iteration 3/25 | Loss: 0.00146601
Iteration 4/25 | Loss: 0.00141981
Iteration 5/25 | Loss: 0.00141004
Iteration 6/25 | Loss: 0.00140790
Iteration 7/25 | Loss: 0.00140790
Iteration 8/25 | Loss: 0.00140790
Iteration 9/25 | Loss: 0.00140790
Iteration 10/25 | Loss: 0.00140790
Iteration 11/25 | Loss: 0.00140790
Iteration 12/25 | Loss: 0.00140790
Iteration 13/25 | Loss: 0.00140790
Iteration 14/25 | Loss: 0.00140790
Iteration 15/25 | Loss: 0.00140790
Iteration 16/25 | Loss: 0.00140790
Iteration 17/25 | Loss: 0.00140790
Iteration 18/25 | Loss: 0.00140790
Iteration 19/25 | Loss: 0.00140790
Iteration 20/25 | Loss: 0.00140790
Iteration 21/25 | Loss: 0.00140790
Iteration 22/25 | Loss: 0.00140790
Iteration 23/25 | Loss: 0.00140790
Iteration 24/25 | Loss: 0.00140790
Iteration 25/25 | Loss: 0.00140790

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24354923
Iteration 2/25 | Loss: 0.00094208
Iteration 3/25 | Loss: 0.00094207
Iteration 4/25 | Loss: 0.00094207
Iteration 5/25 | Loss: 0.00094207
Iteration 6/25 | Loss: 0.00094207
Iteration 7/25 | Loss: 0.00094207
Iteration 8/25 | Loss: 0.00094206
Iteration 9/25 | Loss: 0.00094206
Iteration 10/25 | Loss: 0.00094206
Iteration 11/25 | Loss: 0.00094206
Iteration 12/25 | Loss: 0.00094206
Iteration 13/25 | Loss: 0.00094206
Iteration 14/25 | Loss: 0.00094206
Iteration 15/25 | Loss: 0.00094206
Iteration 16/25 | Loss: 0.00094206
Iteration 17/25 | Loss: 0.00094206
Iteration 18/25 | Loss: 0.00094206
Iteration 19/25 | Loss: 0.00094206
Iteration 20/25 | Loss: 0.00094206
Iteration 21/25 | Loss: 0.00094206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009420643327757716, 0.0009420643327757716, 0.0009420643327757716, 0.0009420643327757716, 0.0009420643327757716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009420643327757716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094206
Iteration 2/1000 | Loss: 0.00007172
Iteration 3/1000 | Loss: 0.00004712
Iteration 4/1000 | Loss: 0.00004191
Iteration 5/1000 | Loss: 0.00003886
Iteration 6/1000 | Loss: 0.00003705
Iteration 7/1000 | Loss: 0.00003573
Iteration 8/1000 | Loss: 0.00003452
Iteration 9/1000 | Loss: 0.00003399
Iteration 10/1000 | Loss: 0.00003343
Iteration 11/1000 | Loss: 0.00003299
Iteration 12/1000 | Loss: 0.00003265
Iteration 13/1000 | Loss: 0.00003230
Iteration 14/1000 | Loss: 0.00003207
Iteration 15/1000 | Loss: 0.00003182
Iteration 16/1000 | Loss: 0.00003181
Iteration 17/1000 | Loss: 0.00003166
Iteration 18/1000 | Loss: 0.00003166
Iteration 19/1000 | Loss: 0.00003160
Iteration 20/1000 | Loss: 0.00003155
Iteration 21/1000 | Loss: 0.00003153
Iteration 22/1000 | Loss: 0.00003153
Iteration 23/1000 | Loss: 0.00003151
Iteration 24/1000 | Loss: 0.00003150
Iteration 25/1000 | Loss: 0.00003149
Iteration 26/1000 | Loss: 0.00003148
Iteration 27/1000 | Loss: 0.00003147
Iteration 28/1000 | Loss: 0.00003147
Iteration 29/1000 | Loss: 0.00003146
Iteration 30/1000 | Loss: 0.00003145
Iteration 31/1000 | Loss: 0.00003144
Iteration 32/1000 | Loss: 0.00003141
Iteration 33/1000 | Loss: 0.00003138
Iteration 34/1000 | Loss: 0.00003137
Iteration 35/1000 | Loss: 0.00003137
Iteration 36/1000 | Loss: 0.00003136
Iteration 37/1000 | Loss: 0.00003136
Iteration 38/1000 | Loss: 0.00003136
Iteration 39/1000 | Loss: 0.00003135
Iteration 40/1000 | Loss: 0.00003135
Iteration 41/1000 | Loss: 0.00003135
Iteration 42/1000 | Loss: 0.00003134
Iteration 43/1000 | Loss: 0.00003134
Iteration 44/1000 | Loss: 0.00003133
Iteration 45/1000 | Loss: 0.00003133
Iteration 46/1000 | Loss: 0.00003132
Iteration 47/1000 | Loss: 0.00003132
Iteration 48/1000 | Loss: 0.00003131
Iteration 49/1000 | Loss: 0.00003131
Iteration 50/1000 | Loss: 0.00003131
Iteration 51/1000 | Loss: 0.00003131
Iteration 52/1000 | Loss: 0.00003131
Iteration 53/1000 | Loss: 0.00003131
Iteration 54/1000 | Loss: 0.00003130
Iteration 55/1000 | Loss: 0.00003130
Iteration 56/1000 | Loss: 0.00003130
Iteration 57/1000 | Loss: 0.00003129
Iteration 58/1000 | Loss: 0.00003129
Iteration 59/1000 | Loss: 0.00003128
Iteration 60/1000 | Loss: 0.00003127
Iteration 61/1000 | Loss: 0.00003127
Iteration 62/1000 | Loss: 0.00003127
Iteration 63/1000 | Loss: 0.00003127
Iteration 64/1000 | Loss: 0.00003126
Iteration 65/1000 | Loss: 0.00003126
Iteration 66/1000 | Loss: 0.00003126
Iteration 67/1000 | Loss: 0.00003125
Iteration 68/1000 | Loss: 0.00003124
Iteration 69/1000 | Loss: 0.00003124
Iteration 70/1000 | Loss: 0.00003124
Iteration 71/1000 | Loss: 0.00003124
Iteration 72/1000 | Loss: 0.00003124
Iteration 73/1000 | Loss: 0.00003124
Iteration 74/1000 | Loss: 0.00003124
Iteration 75/1000 | Loss: 0.00003124
Iteration 76/1000 | Loss: 0.00003124
Iteration 77/1000 | Loss: 0.00003123
Iteration 78/1000 | Loss: 0.00003123
Iteration 79/1000 | Loss: 0.00003122
Iteration 80/1000 | Loss: 0.00003121
Iteration 81/1000 | Loss: 0.00003121
Iteration 82/1000 | Loss: 0.00003121
Iteration 83/1000 | Loss: 0.00003120
Iteration 84/1000 | Loss: 0.00003120
Iteration 85/1000 | Loss: 0.00003119
Iteration 86/1000 | Loss: 0.00003119
Iteration 87/1000 | Loss: 0.00003119
Iteration 88/1000 | Loss: 0.00003119
Iteration 89/1000 | Loss: 0.00003118
Iteration 90/1000 | Loss: 0.00003118
Iteration 91/1000 | Loss: 0.00003118
Iteration 92/1000 | Loss: 0.00003118
Iteration 93/1000 | Loss: 0.00003118
Iteration 94/1000 | Loss: 0.00003118
Iteration 95/1000 | Loss: 0.00003118
Iteration 96/1000 | Loss: 0.00003118
Iteration 97/1000 | Loss: 0.00003117
Iteration 98/1000 | Loss: 0.00003117
Iteration 99/1000 | Loss: 0.00003117
Iteration 100/1000 | Loss: 0.00003117
Iteration 101/1000 | Loss: 0.00003117
Iteration 102/1000 | Loss: 0.00003117
Iteration 103/1000 | Loss: 0.00003117
Iteration 104/1000 | Loss: 0.00003117
Iteration 105/1000 | Loss: 0.00003117
Iteration 106/1000 | Loss: 0.00003117
Iteration 107/1000 | Loss: 0.00003117
Iteration 108/1000 | Loss: 0.00003117
Iteration 109/1000 | Loss: 0.00003117
Iteration 110/1000 | Loss: 0.00003117
Iteration 111/1000 | Loss: 0.00003117
Iteration 112/1000 | Loss: 0.00003117
Iteration 113/1000 | Loss: 0.00003117
Iteration 114/1000 | Loss: 0.00003117
Iteration 115/1000 | Loss: 0.00003117
Iteration 116/1000 | Loss: 0.00003117
Iteration 117/1000 | Loss: 0.00003117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [3.117434971500188e-05, 3.117434971500188e-05, 3.117434971500188e-05, 3.117434971500188e-05, 3.117434971500188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.117434971500188e-05

Optimization complete. Final v2v error: 4.671818733215332 mm

Highest mean error: 5.14492654800415 mm for frame 2

Lowest mean error: 4.2031636238098145 mm for frame 187

Saving results

Total time: 46.22314739227295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00687052
Iteration 2/25 | Loss: 0.00171407
Iteration 3/25 | Loss: 0.00152940
Iteration 4/25 | Loss: 0.00151485
Iteration 5/25 | Loss: 0.00151243
Iteration 6/25 | Loss: 0.00151241
Iteration 7/25 | Loss: 0.00151241
Iteration 8/25 | Loss: 0.00151241
Iteration 9/25 | Loss: 0.00151241
Iteration 10/25 | Loss: 0.00151241
Iteration 11/25 | Loss: 0.00151241
Iteration 12/25 | Loss: 0.00151241
Iteration 13/25 | Loss: 0.00151241
Iteration 14/25 | Loss: 0.00151241
Iteration 15/25 | Loss: 0.00151241
Iteration 16/25 | Loss: 0.00151241
Iteration 17/25 | Loss: 0.00151241
Iteration 18/25 | Loss: 0.00151241
Iteration 19/25 | Loss: 0.00151241
Iteration 20/25 | Loss: 0.00151241
Iteration 21/25 | Loss: 0.00151241
Iteration 22/25 | Loss: 0.00151241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015124119818210602, 0.0015124119818210602, 0.0015124119818210602, 0.0015124119818210602, 0.0015124119818210602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015124119818210602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66705060
Iteration 2/25 | Loss: 0.00077431
Iteration 3/25 | Loss: 0.00077431
Iteration 4/25 | Loss: 0.00077431
Iteration 5/25 | Loss: 0.00077431
Iteration 6/25 | Loss: 0.00077431
Iteration 7/25 | Loss: 0.00077431
Iteration 8/25 | Loss: 0.00077431
Iteration 9/25 | Loss: 0.00077431
Iteration 10/25 | Loss: 0.00077431
Iteration 11/25 | Loss: 0.00077431
Iteration 12/25 | Loss: 0.00077431
Iteration 13/25 | Loss: 0.00077431
Iteration 14/25 | Loss: 0.00077431
Iteration 15/25 | Loss: 0.00077431
Iteration 16/25 | Loss: 0.00077431
Iteration 17/25 | Loss: 0.00077431
Iteration 18/25 | Loss: 0.00077431
Iteration 19/25 | Loss: 0.00077431
Iteration 20/25 | Loss: 0.00077431
Iteration 21/25 | Loss: 0.00077431
Iteration 22/25 | Loss: 0.00077431
Iteration 23/25 | Loss: 0.00077431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007743051974102855, 0.0007743051974102855, 0.0007743051974102855, 0.0007743051974102855, 0.0007743051974102855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007743051974102855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077431
Iteration 2/1000 | Loss: 0.00008343
Iteration 3/1000 | Loss: 0.00005378
Iteration 4/1000 | Loss: 0.00004520
Iteration 5/1000 | Loss: 0.00004243
Iteration 6/1000 | Loss: 0.00004098
Iteration 7/1000 | Loss: 0.00004007
Iteration 8/1000 | Loss: 0.00003927
Iteration 9/1000 | Loss: 0.00003818
Iteration 10/1000 | Loss: 0.00003731
Iteration 11/1000 | Loss: 0.00003674
Iteration 12/1000 | Loss: 0.00003627
Iteration 13/1000 | Loss: 0.00003596
Iteration 14/1000 | Loss: 0.00003562
Iteration 15/1000 | Loss: 0.00003545
Iteration 16/1000 | Loss: 0.00003522
Iteration 17/1000 | Loss: 0.00003503
Iteration 18/1000 | Loss: 0.00003500
Iteration 19/1000 | Loss: 0.00003492
Iteration 20/1000 | Loss: 0.00003492
Iteration 21/1000 | Loss: 0.00003486
Iteration 22/1000 | Loss: 0.00003481
Iteration 23/1000 | Loss: 0.00003468
Iteration 24/1000 | Loss: 0.00003455
Iteration 25/1000 | Loss: 0.00003451
Iteration 26/1000 | Loss: 0.00003448
Iteration 27/1000 | Loss: 0.00003446
Iteration 28/1000 | Loss: 0.00003446
Iteration 29/1000 | Loss: 0.00003446
Iteration 30/1000 | Loss: 0.00003446
Iteration 31/1000 | Loss: 0.00003446
Iteration 32/1000 | Loss: 0.00003446
Iteration 33/1000 | Loss: 0.00003445
Iteration 34/1000 | Loss: 0.00003445
Iteration 35/1000 | Loss: 0.00003444
Iteration 36/1000 | Loss: 0.00003439
Iteration 37/1000 | Loss: 0.00003436
Iteration 38/1000 | Loss: 0.00003436
Iteration 39/1000 | Loss: 0.00003436
Iteration 40/1000 | Loss: 0.00003436
Iteration 41/1000 | Loss: 0.00003436
Iteration 42/1000 | Loss: 0.00003436
Iteration 43/1000 | Loss: 0.00003435
Iteration 44/1000 | Loss: 0.00003435
Iteration 45/1000 | Loss: 0.00003435
Iteration 46/1000 | Loss: 0.00003435
Iteration 47/1000 | Loss: 0.00003435
Iteration 48/1000 | Loss: 0.00003435
Iteration 49/1000 | Loss: 0.00003435
Iteration 50/1000 | Loss: 0.00003433
Iteration 51/1000 | Loss: 0.00003432
Iteration 52/1000 | Loss: 0.00003427
Iteration 53/1000 | Loss: 0.00003425
Iteration 54/1000 | Loss: 0.00003425
Iteration 55/1000 | Loss: 0.00003425
Iteration 56/1000 | Loss: 0.00003425
Iteration 57/1000 | Loss: 0.00003425
Iteration 58/1000 | Loss: 0.00003425
Iteration 59/1000 | Loss: 0.00003425
Iteration 60/1000 | Loss: 0.00003424
Iteration 61/1000 | Loss: 0.00003424
Iteration 62/1000 | Loss: 0.00003424
Iteration 63/1000 | Loss: 0.00003424
Iteration 64/1000 | Loss: 0.00003424
Iteration 65/1000 | Loss: 0.00003424
Iteration 66/1000 | Loss: 0.00003424
Iteration 67/1000 | Loss: 0.00003424
Iteration 68/1000 | Loss: 0.00003423
Iteration 69/1000 | Loss: 0.00003423
Iteration 70/1000 | Loss: 0.00003423
Iteration 71/1000 | Loss: 0.00003423
Iteration 72/1000 | Loss: 0.00003423
Iteration 73/1000 | Loss: 0.00003423
Iteration 74/1000 | Loss: 0.00003423
Iteration 75/1000 | Loss: 0.00003422
Iteration 76/1000 | Loss: 0.00003421
Iteration 77/1000 | Loss: 0.00003420
Iteration 78/1000 | Loss: 0.00003420
Iteration 79/1000 | Loss: 0.00003420
Iteration 80/1000 | Loss: 0.00003420
Iteration 81/1000 | Loss: 0.00003419
Iteration 82/1000 | Loss: 0.00003419
Iteration 83/1000 | Loss: 0.00003418
Iteration 84/1000 | Loss: 0.00003417
Iteration 85/1000 | Loss: 0.00003417
Iteration 86/1000 | Loss: 0.00003417
Iteration 87/1000 | Loss: 0.00003417
Iteration 88/1000 | Loss: 0.00003417
Iteration 89/1000 | Loss: 0.00003417
Iteration 90/1000 | Loss: 0.00003416
Iteration 91/1000 | Loss: 0.00003416
Iteration 92/1000 | Loss: 0.00003416
Iteration 93/1000 | Loss: 0.00003416
Iteration 94/1000 | Loss: 0.00003416
Iteration 95/1000 | Loss: 0.00003416
Iteration 96/1000 | Loss: 0.00003416
Iteration 97/1000 | Loss: 0.00003416
Iteration 98/1000 | Loss: 0.00003415
Iteration 99/1000 | Loss: 0.00003415
Iteration 100/1000 | Loss: 0.00003415
Iteration 101/1000 | Loss: 0.00003415
Iteration 102/1000 | Loss: 0.00003414
Iteration 103/1000 | Loss: 0.00003414
Iteration 104/1000 | Loss: 0.00003414
Iteration 105/1000 | Loss: 0.00003414
Iteration 106/1000 | Loss: 0.00003412
Iteration 107/1000 | Loss: 0.00003412
Iteration 108/1000 | Loss: 0.00003412
Iteration 109/1000 | Loss: 0.00003412
Iteration 110/1000 | Loss: 0.00003412
Iteration 111/1000 | Loss: 0.00003412
Iteration 112/1000 | Loss: 0.00003411
Iteration 113/1000 | Loss: 0.00003411
Iteration 114/1000 | Loss: 0.00003411
Iteration 115/1000 | Loss: 0.00003411
Iteration 116/1000 | Loss: 0.00003411
Iteration 117/1000 | Loss: 0.00003411
Iteration 118/1000 | Loss: 0.00003411
Iteration 119/1000 | Loss: 0.00003411
Iteration 120/1000 | Loss: 0.00003410
Iteration 121/1000 | Loss: 0.00003410
Iteration 122/1000 | Loss: 0.00003410
Iteration 123/1000 | Loss: 0.00003410
Iteration 124/1000 | Loss: 0.00003410
Iteration 125/1000 | Loss: 0.00003410
Iteration 126/1000 | Loss: 0.00003410
Iteration 127/1000 | Loss: 0.00003410
Iteration 128/1000 | Loss: 0.00003410
Iteration 129/1000 | Loss: 0.00003410
Iteration 130/1000 | Loss: 0.00003410
Iteration 131/1000 | Loss: 0.00003410
Iteration 132/1000 | Loss: 0.00003410
Iteration 133/1000 | Loss: 0.00003409
Iteration 134/1000 | Loss: 0.00003409
Iteration 135/1000 | Loss: 0.00003409
Iteration 136/1000 | Loss: 0.00003409
Iteration 137/1000 | Loss: 0.00003409
Iteration 138/1000 | Loss: 0.00003408
Iteration 139/1000 | Loss: 0.00003408
Iteration 140/1000 | Loss: 0.00003408
Iteration 141/1000 | Loss: 0.00003407
Iteration 142/1000 | Loss: 0.00003407
Iteration 143/1000 | Loss: 0.00003407
Iteration 144/1000 | Loss: 0.00003407
Iteration 145/1000 | Loss: 0.00003407
Iteration 146/1000 | Loss: 0.00003407
Iteration 147/1000 | Loss: 0.00003406
Iteration 148/1000 | Loss: 0.00003406
Iteration 149/1000 | Loss: 0.00003406
Iteration 150/1000 | Loss: 0.00003406
Iteration 151/1000 | Loss: 0.00003406
Iteration 152/1000 | Loss: 0.00003406
Iteration 153/1000 | Loss: 0.00003406
Iteration 154/1000 | Loss: 0.00003406
Iteration 155/1000 | Loss: 0.00003406
Iteration 156/1000 | Loss: 0.00003405
Iteration 157/1000 | Loss: 0.00003405
Iteration 158/1000 | Loss: 0.00003405
Iteration 159/1000 | Loss: 0.00003404
Iteration 160/1000 | Loss: 0.00003404
Iteration 161/1000 | Loss: 0.00003404
Iteration 162/1000 | Loss: 0.00003404
Iteration 163/1000 | Loss: 0.00003404
Iteration 164/1000 | Loss: 0.00003404
Iteration 165/1000 | Loss: 0.00003404
Iteration 166/1000 | Loss: 0.00003404
Iteration 167/1000 | Loss: 0.00003404
Iteration 168/1000 | Loss: 0.00003404
Iteration 169/1000 | Loss: 0.00003403
Iteration 170/1000 | Loss: 0.00003403
Iteration 171/1000 | Loss: 0.00003403
Iteration 172/1000 | Loss: 0.00003403
Iteration 173/1000 | Loss: 0.00003403
Iteration 174/1000 | Loss: 0.00003403
Iteration 175/1000 | Loss: 0.00003403
Iteration 176/1000 | Loss: 0.00003402
Iteration 177/1000 | Loss: 0.00003402
Iteration 178/1000 | Loss: 0.00003402
Iteration 179/1000 | Loss: 0.00003402
Iteration 180/1000 | Loss: 0.00003402
Iteration 181/1000 | Loss: 0.00003402
Iteration 182/1000 | Loss: 0.00003402
Iteration 183/1000 | Loss: 0.00003402
Iteration 184/1000 | Loss: 0.00003402
Iteration 185/1000 | Loss: 0.00003402
Iteration 186/1000 | Loss: 0.00003402
Iteration 187/1000 | Loss: 0.00003402
Iteration 188/1000 | Loss: 0.00003402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [3.402409492991865e-05, 3.402409492991865e-05, 3.402409492991865e-05, 3.402409492991865e-05, 3.402409492991865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.402409492991865e-05

Optimization complete. Final v2v error: 4.827526092529297 mm

Highest mean error: 5.247908115386963 mm for frame 95

Lowest mean error: 4.387666702270508 mm for frame 107

Saving results

Total time: 49.837244749069214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824477
Iteration 2/25 | Loss: 0.00147154
Iteration 3/25 | Loss: 0.00125158
Iteration 4/25 | Loss: 0.00122018
Iteration 5/25 | Loss: 0.00121438
Iteration 6/25 | Loss: 0.00121326
Iteration 7/25 | Loss: 0.00121326
Iteration 8/25 | Loss: 0.00121326
Iteration 9/25 | Loss: 0.00121326
Iteration 10/25 | Loss: 0.00121326
Iteration 11/25 | Loss: 0.00121326
Iteration 12/25 | Loss: 0.00121326
Iteration 13/25 | Loss: 0.00121326
Iteration 14/25 | Loss: 0.00121326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012132565025240183, 0.0012132565025240183, 0.0012132565025240183, 0.0012132565025240183, 0.0012132565025240183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012132565025240183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03023243
Iteration 2/25 | Loss: 0.00051071
Iteration 3/25 | Loss: 0.00051070
Iteration 4/25 | Loss: 0.00051070
Iteration 5/25 | Loss: 0.00051070
Iteration 6/25 | Loss: 0.00051070
Iteration 7/25 | Loss: 0.00051070
Iteration 8/25 | Loss: 0.00051070
Iteration 9/25 | Loss: 0.00051070
Iteration 10/25 | Loss: 0.00051070
Iteration 11/25 | Loss: 0.00051070
Iteration 12/25 | Loss: 0.00051070
Iteration 13/25 | Loss: 0.00051070
Iteration 14/25 | Loss: 0.00051070
Iteration 15/25 | Loss: 0.00051070
Iteration 16/25 | Loss: 0.00051070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005107001634314656, 0.0005107001634314656, 0.0005107001634314656, 0.0005107001634314656, 0.0005107001634314656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005107001634314656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051070
Iteration 2/1000 | Loss: 0.00004676
Iteration 3/1000 | Loss: 0.00003532
Iteration 4/1000 | Loss: 0.00003047
Iteration 5/1000 | Loss: 0.00002804
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002538
Iteration 8/1000 | Loss: 0.00002463
Iteration 9/1000 | Loss: 0.00002415
Iteration 10/1000 | Loss: 0.00002374
Iteration 11/1000 | Loss: 0.00002348
Iteration 12/1000 | Loss: 0.00002319
Iteration 13/1000 | Loss: 0.00002295
Iteration 14/1000 | Loss: 0.00002275
Iteration 15/1000 | Loss: 0.00002259
Iteration 16/1000 | Loss: 0.00002255
Iteration 17/1000 | Loss: 0.00002253
Iteration 18/1000 | Loss: 0.00002252
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002250
Iteration 22/1000 | Loss: 0.00002248
Iteration 23/1000 | Loss: 0.00002246
Iteration 24/1000 | Loss: 0.00002246
Iteration 25/1000 | Loss: 0.00002245
Iteration 26/1000 | Loss: 0.00002244
Iteration 27/1000 | Loss: 0.00002243
Iteration 28/1000 | Loss: 0.00002243
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00002242
Iteration 31/1000 | Loss: 0.00002242
Iteration 32/1000 | Loss: 0.00002242
Iteration 33/1000 | Loss: 0.00002242
Iteration 34/1000 | Loss: 0.00002242
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002242
Iteration 40/1000 | Loss: 0.00002240
Iteration 41/1000 | Loss: 0.00002240
Iteration 42/1000 | Loss: 0.00002240
Iteration 43/1000 | Loss: 0.00002240
Iteration 44/1000 | Loss: 0.00002240
Iteration 45/1000 | Loss: 0.00002240
Iteration 46/1000 | Loss: 0.00002240
Iteration 47/1000 | Loss: 0.00002240
Iteration 48/1000 | Loss: 0.00002240
Iteration 49/1000 | Loss: 0.00002240
Iteration 50/1000 | Loss: 0.00002240
Iteration 51/1000 | Loss: 0.00002237
Iteration 52/1000 | Loss: 0.00002237
Iteration 53/1000 | Loss: 0.00002236
Iteration 54/1000 | Loss: 0.00002236
Iteration 55/1000 | Loss: 0.00002236
Iteration 56/1000 | Loss: 0.00002235
Iteration 57/1000 | Loss: 0.00002235
Iteration 58/1000 | Loss: 0.00002235
Iteration 59/1000 | Loss: 0.00002235
Iteration 60/1000 | Loss: 0.00002234
Iteration 61/1000 | Loss: 0.00002234
Iteration 62/1000 | Loss: 0.00002234
Iteration 63/1000 | Loss: 0.00002234
Iteration 64/1000 | Loss: 0.00002234
Iteration 65/1000 | Loss: 0.00002234
Iteration 66/1000 | Loss: 0.00002234
Iteration 67/1000 | Loss: 0.00002234
Iteration 68/1000 | Loss: 0.00002234
Iteration 69/1000 | Loss: 0.00002234
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00002234
Iteration 73/1000 | Loss: 0.00002234
Iteration 74/1000 | Loss: 0.00002234
Iteration 75/1000 | Loss: 0.00002234
Iteration 76/1000 | Loss: 0.00002233
Iteration 77/1000 | Loss: 0.00002233
Iteration 78/1000 | Loss: 0.00002233
Iteration 79/1000 | Loss: 0.00002233
Iteration 80/1000 | Loss: 0.00002232
Iteration 81/1000 | Loss: 0.00002232
Iteration 82/1000 | Loss: 0.00002232
Iteration 83/1000 | Loss: 0.00002232
Iteration 84/1000 | Loss: 0.00002232
Iteration 85/1000 | Loss: 0.00002232
Iteration 86/1000 | Loss: 0.00002231
Iteration 87/1000 | Loss: 0.00002231
Iteration 88/1000 | Loss: 0.00002231
Iteration 89/1000 | Loss: 0.00002231
Iteration 90/1000 | Loss: 0.00002231
Iteration 91/1000 | Loss: 0.00002231
Iteration 92/1000 | Loss: 0.00002230
Iteration 93/1000 | Loss: 0.00002230
Iteration 94/1000 | Loss: 0.00002230
Iteration 95/1000 | Loss: 0.00002229
Iteration 96/1000 | Loss: 0.00002229
Iteration 97/1000 | Loss: 0.00002229
Iteration 98/1000 | Loss: 0.00002229
Iteration 99/1000 | Loss: 0.00002228
Iteration 100/1000 | Loss: 0.00002227
Iteration 101/1000 | Loss: 0.00002227
Iteration 102/1000 | Loss: 0.00002227
Iteration 103/1000 | Loss: 0.00002226
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002226
Iteration 106/1000 | Loss: 0.00002226
Iteration 107/1000 | Loss: 0.00002226
Iteration 108/1000 | Loss: 0.00002226
Iteration 109/1000 | Loss: 0.00002226
Iteration 110/1000 | Loss: 0.00002226
Iteration 111/1000 | Loss: 0.00002225
Iteration 112/1000 | Loss: 0.00002225
Iteration 113/1000 | Loss: 0.00002225
Iteration 114/1000 | Loss: 0.00002224
Iteration 115/1000 | Loss: 0.00002224
Iteration 116/1000 | Loss: 0.00002224
Iteration 117/1000 | Loss: 0.00002224
Iteration 118/1000 | Loss: 0.00002224
Iteration 119/1000 | Loss: 0.00002223
Iteration 120/1000 | Loss: 0.00002223
Iteration 121/1000 | Loss: 0.00002223
Iteration 122/1000 | Loss: 0.00002223
Iteration 123/1000 | Loss: 0.00002223
Iteration 124/1000 | Loss: 0.00002223
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002223
Iteration 128/1000 | Loss: 0.00002223
Iteration 129/1000 | Loss: 0.00002223
Iteration 130/1000 | Loss: 0.00002223
Iteration 131/1000 | Loss: 0.00002223
Iteration 132/1000 | Loss: 0.00002223
Iteration 133/1000 | Loss: 0.00002222
Iteration 134/1000 | Loss: 0.00002222
Iteration 135/1000 | Loss: 0.00002222
Iteration 136/1000 | Loss: 0.00002222
Iteration 137/1000 | Loss: 0.00002222
Iteration 138/1000 | Loss: 0.00002222
Iteration 139/1000 | Loss: 0.00002222
Iteration 140/1000 | Loss: 0.00002222
Iteration 141/1000 | Loss: 0.00002222
Iteration 142/1000 | Loss: 0.00002221
Iteration 143/1000 | Loss: 0.00002221
Iteration 144/1000 | Loss: 0.00002221
Iteration 145/1000 | Loss: 0.00002221
Iteration 146/1000 | Loss: 0.00002220
Iteration 147/1000 | Loss: 0.00002220
Iteration 148/1000 | Loss: 0.00002220
Iteration 149/1000 | Loss: 0.00002220
Iteration 150/1000 | Loss: 0.00002220
Iteration 151/1000 | Loss: 0.00002220
Iteration 152/1000 | Loss: 0.00002220
Iteration 153/1000 | Loss: 0.00002220
Iteration 154/1000 | Loss: 0.00002220
Iteration 155/1000 | Loss: 0.00002219
Iteration 156/1000 | Loss: 0.00002219
Iteration 157/1000 | Loss: 0.00002219
Iteration 158/1000 | Loss: 0.00002219
Iteration 159/1000 | Loss: 0.00002219
Iteration 160/1000 | Loss: 0.00002219
Iteration 161/1000 | Loss: 0.00002219
Iteration 162/1000 | Loss: 0.00002219
Iteration 163/1000 | Loss: 0.00002219
Iteration 164/1000 | Loss: 0.00002218
Iteration 165/1000 | Loss: 0.00002218
Iteration 166/1000 | Loss: 0.00002218
Iteration 167/1000 | Loss: 0.00002218
Iteration 168/1000 | Loss: 0.00002218
Iteration 169/1000 | Loss: 0.00002218
Iteration 170/1000 | Loss: 0.00002218
Iteration 171/1000 | Loss: 0.00002218
Iteration 172/1000 | Loss: 0.00002218
Iteration 173/1000 | Loss: 0.00002218
Iteration 174/1000 | Loss: 0.00002218
Iteration 175/1000 | Loss: 0.00002218
Iteration 176/1000 | Loss: 0.00002218
Iteration 177/1000 | Loss: 0.00002218
Iteration 178/1000 | Loss: 0.00002218
Iteration 179/1000 | Loss: 0.00002218
Iteration 180/1000 | Loss: 0.00002218
Iteration 181/1000 | Loss: 0.00002217
Iteration 182/1000 | Loss: 0.00002217
Iteration 183/1000 | Loss: 0.00002217
Iteration 184/1000 | Loss: 0.00002217
Iteration 185/1000 | Loss: 0.00002217
Iteration 186/1000 | Loss: 0.00002217
Iteration 187/1000 | Loss: 0.00002217
Iteration 188/1000 | Loss: 0.00002217
Iteration 189/1000 | Loss: 0.00002217
Iteration 190/1000 | Loss: 0.00002217
Iteration 191/1000 | Loss: 0.00002217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.2174330297275446e-05, 2.2174330297275446e-05, 2.2174330297275446e-05, 2.2174330297275446e-05, 2.2174330297275446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2174330297275446e-05

Optimization complete. Final v2v error: 4.062925338745117 mm

Highest mean error: 4.315588474273682 mm for frame 131

Lowest mean error: 3.8388497829437256 mm for frame 109

Saving results

Total time: 42.53460669517517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995785
Iteration 2/25 | Loss: 0.00384463
Iteration 3/25 | Loss: 0.00266612
Iteration 4/25 | Loss: 0.00221617
Iteration 5/25 | Loss: 0.00203008
Iteration 6/25 | Loss: 0.00189315
Iteration 7/25 | Loss: 0.00180779
Iteration 8/25 | Loss: 0.00176703
Iteration 9/25 | Loss: 0.00168310
Iteration 10/25 | Loss: 0.00169031
Iteration 11/25 | Loss: 0.00158035
Iteration 12/25 | Loss: 0.00154508
Iteration 13/25 | Loss: 0.00151051
Iteration 14/25 | Loss: 0.00150189
Iteration 15/25 | Loss: 0.00145036
Iteration 16/25 | Loss: 0.00143822
Iteration 17/25 | Loss: 0.00144049
Iteration 18/25 | Loss: 0.00143760
Iteration 19/25 | Loss: 0.00144084
Iteration 20/25 | Loss: 0.00143397
Iteration 21/25 | Loss: 0.00142909
Iteration 22/25 | Loss: 0.00143196
Iteration 23/25 | Loss: 0.00142610
Iteration 24/25 | Loss: 0.00142064
Iteration 25/25 | Loss: 0.00141766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56142712
Iteration 2/25 | Loss: 0.00214206
Iteration 3/25 | Loss: 0.00166732
Iteration 4/25 | Loss: 0.00166732
Iteration 5/25 | Loss: 0.00166732
Iteration 6/25 | Loss: 0.00166732
Iteration 7/25 | Loss: 0.00166732
Iteration 8/25 | Loss: 0.00166732
Iteration 9/25 | Loss: 0.00166732
Iteration 10/25 | Loss: 0.00166732
Iteration 11/25 | Loss: 0.00166732
Iteration 12/25 | Loss: 0.00166732
Iteration 13/25 | Loss: 0.00166732
Iteration 14/25 | Loss: 0.00166732
Iteration 15/25 | Loss: 0.00166732
Iteration 16/25 | Loss: 0.00166732
Iteration 17/25 | Loss: 0.00166732
Iteration 18/25 | Loss: 0.00166732
Iteration 19/25 | Loss: 0.00166732
Iteration 20/25 | Loss: 0.00166732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016673194477334619, 0.0016673194477334619, 0.0016673194477334619, 0.0016673194477334619, 0.0016673194477334619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016673194477334619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166732
Iteration 2/1000 | Loss: 0.00166062
Iteration 3/1000 | Loss: 0.00087981
Iteration 4/1000 | Loss: 0.00029027
Iteration 5/1000 | Loss: 0.00019534
Iteration 6/1000 | Loss: 0.00037987
Iteration 7/1000 | Loss: 0.00232698
Iteration 8/1000 | Loss: 0.00219890
Iteration 9/1000 | Loss: 0.00019194
Iteration 10/1000 | Loss: 0.00034964
Iteration 11/1000 | Loss: 0.00029703
Iteration 12/1000 | Loss: 0.00026056
Iteration 13/1000 | Loss: 0.00087058
Iteration 14/1000 | Loss: 0.00004628
Iteration 15/1000 | Loss: 0.00004084
Iteration 16/1000 | Loss: 0.00025137
Iteration 17/1000 | Loss: 0.00003628
Iteration 18/1000 | Loss: 0.00023801
Iteration 19/1000 | Loss: 0.00003369
Iteration 20/1000 | Loss: 0.00003262
Iteration 21/1000 | Loss: 0.00003104
Iteration 22/1000 | Loss: 0.00003036
Iteration 23/1000 | Loss: 0.00009313
Iteration 24/1000 | Loss: 0.00002973
Iteration 25/1000 | Loss: 0.00007803
Iteration 26/1000 | Loss: 0.00002924
Iteration 27/1000 | Loss: 0.00002880
Iteration 28/1000 | Loss: 0.00002878
Iteration 29/1000 | Loss: 0.00002877
Iteration 30/1000 | Loss: 0.00002877
Iteration 31/1000 | Loss: 0.00002856
Iteration 32/1000 | Loss: 0.00014335
Iteration 33/1000 | Loss: 0.00002868
Iteration 34/1000 | Loss: 0.00002837
Iteration 35/1000 | Loss: 0.00002835
Iteration 36/1000 | Loss: 0.00002829
Iteration 37/1000 | Loss: 0.00007270
Iteration 38/1000 | Loss: 0.00003305
Iteration 39/1000 | Loss: 0.00003429
Iteration 40/1000 | Loss: 0.00008738
Iteration 41/1000 | Loss: 0.00002847
Iteration 42/1000 | Loss: 0.00002810
Iteration 43/1000 | Loss: 0.00002807
Iteration 44/1000 | Loss: 0.00002803
Iteration 45/1000 | Loss: 0.00002803
Iteration 46/1000 | Loss: 0.00002803
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002798
Iteration 49/1000 | Loss: 0.00002797
Iteration 50/1000 | Loss: 0.00002797
Iteration 51/1000 | Loss: 0.00002792
Iteration 52/1000 | Loss: 0.00002791
Iteration 53/1000 | Loss: 0.00002791
Iteration 54/1000 | Loss: 0.00002791
Iteration 55/1000 | Loss: 0.00002790
Iteration 56/1000 | Loss: 0.00002788
Iteration 57/1000 | Loss: 0.00002788
Iteration 58/1000 | Loss: 0.00002788
Iteration 59/1000 | Loss: 0.00002788
Iteration 60/1000 | Loss: 0.00002788
Iteration 61/1000 | Loss: 0.00002788
Iteration 62/1000 | Loss: 0.00002788
Iteration 63/1000 | Loss: 0.00002788
Iteration 64/1000 | Loss: 0.00023822
Iteration 65/1000 | Loss: 0.00003613
Iteration 66/1000 | Loss: 0.00003121
Iteration 67/1000 | Loss: 0.00002908
Iteration 68/1000 | Loss: 0.00005721
Iteration 69/1000 | Loss: 0.00002710
Iteration 70/1000 | Loss: 0.00002635
Iteration 71/1000 | Loss: 0.00002593
Iteration 72/1000 | Loss: 0.00002586
Iteration 73/1000 | Loss: 0.00002577
Iteration 74/1000 | Loss: 0.00002558
Iteration 75/1000 | Loss: 0.00024885
Iteration 76/1000 | Loss: 0.00007146
Iteration 77/1000 | Loss: 0.00003506
Iteration 78/1000 | Loss: 0.00002575
Iteration 79/1000 | Loss: 0.00005557
Iteration 80/1000 | Loss: 0.00004382
Iteration 81/1000 | Loss: 0.00002534
Iteration 82/1000 | Loss: 0.00002530
Iteration 83/1000 | Loss: 0.00002530
Iteration 84/1000 | Loss: 0.00002530
Iteration 85/1000 | Loss: 0.00002530
Iteration 86/1000 | Loss: 0.00002530
Iteration 87/1000 | Loss: 0.00002530
Iteration 88/1000 | Loss: 0.00002530
Iteration 89/1000 | Loss: 0.00002530
Iteration 90/1000 | Loss: 0.00002530
Iteration 91/1000 | Loss: 0.00002530
Iteration 92/1000 | Loss: 0.00002529
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002524
Iteration 95/1000 | Loss: 0.00002524
Iteration 96/1000 | Loss: 0.00003780
Iteration 97/1000 | Loss: 0.00002527
Iteration 98/1000 | Loss: 0.00002522
Iteration 99/1000 | Loss: 0.00002522
Iteration 100/1000 | Loss: 0.00002520
Iteration 101/1000 | Loss: 0.00002520
Iteration 102/1000 | Loss: 0.00002520
Iteration 103/1000 | Loss: 0.00002520
Iteration 104/1000 | Loss: 0.00002520
Iteration 105/1000 | Loss: 0.00002520
Iteration 106/1000 | Loss: 0.00002520
Iteration 107/1000 | Loss: 0.00002520
Iteration 108/1000 | Loss: 0.00002520
Iteration 109/1000 | Loss: 0.00002519
Iteration 110/1000 | Loss: 0.00002519
Iteration 111/1000 | Loss: 0.00002518
Iteration 112/1000 | Loss: 0.00002518
Iteration 113/1000 | Loss: 0.00002518
Iteration 114/1000 | Loss: 0.00002518
Iteration 115/1000 | Loss: 0.00002517
Iteration 116/1000 | Loss: 0.00002517
Iteration 117/1000 | Loss: 0.00002517
Iteration 118/1000 | Loss: 0.00002517
Iteration 119/1000 | Loss: 0.00002517
Iteration 120/1000 | Loss: 0.00002517
Iteration 121/1000 | Loss: 0.00002517
Iteration 122/1000 | Loss: 0.00002517
Iteration 123/1000 | Loss: 0.00002517
Iteration 124/1000 | Loss: 0.00002517
Iteration 125/1000 | Loss: 0.00002517
Iteration 126/1000 | Loss: 0.00002516
Iteration 127/1000 | Loss: 0.00002516
Iteration 128/1000 | Loss: 0.00002516
Iteration 129/1000 | Loss: 0.00002516
Iteration 130/1000 | Loss: 0.00002516
Iteration 131/1000 | Loss: 0.00002516
Iteration 132/1000 | Loss: 0.00002516
Iteration 133/1000 | Loss: 0.00002516
Iteration 134/1000 | Loss: 0.00002516
Iteration 135/1000 | Loss: 0.00002516
Iteration 136/1000 | Loss: 0.00002516
Iteration 137/1000 | Loss: 0.00002516
Iteration 138/1000 | Loss: 0.00002516
Iteration 139/1000 | Loss: 0.00002516
Iteration 140/1000 | Loss: 0.00002516
Iteration 141/1000 | Loss: 0.00002516
Iteration 142/1000 | Loss: 0.00002516
Iteration 143/1000 | Loss: 0.00002515
Iteration 144/1000 | Loss: 0.00002515
Iteration 145/1000 | Loss: 0.00002515
Iteration 146/1000 | Loss: 0.00002515
Iteration 147/1000 | Loss: 0.00002515
Iteration 148/1000 | Loss: 0.00002515
Iteration 149/1000 | Loss: 0.00002515
Iteration 150/1000 | Loss: 0.00002515
Iteration 151/1000 | Loss: 0.00002515
Iteration 152/1000 | Loss: 0.00002515
Iteration 153/1000 | Loss: 0.00002515
Iteration 154/1000 | Loss: 0.00002514
Iteration 155/1000 | Loss: 0.00002514
Iteration 156/1000 | Loss: 0.00002514
Iteration 157/1000 | Loss: 0.00002514
Iteration 158/1000 | Loss: 0.00002514
Iteration 159/1000 | Loss: 0.00002514
Iteration 160/1000 | Loss: 0.00002514
Iteration 161/1000 | Loss: 0.00002514
Iteration 162/1000 | Loss: 0.00002514
Iteration 163/1000 | Loss: 0.00002514
Iteration 164/1000 | Loss: 0.00002514
Iteration 165/1000 | Loss: 0.00002514
Iteration 166/1000 | Loss: 0.00002514
Iteration 167/1000 | Loss: 0.00002514
Iteration 168/1000 | Loss: 0.00002513
Iteration 169/1000 | Loss: 0.00002513
Iteration 170/1000 | Loss: 0.00002513
Iteration 171/1000 | Loss: 0.00002513
Iteration 172/1000 | Loss: 0.00002513
Iteration 173/1000 | Loss: 0.00002513
Iteration 174/1000 | Loss: 0.00002513
Iteration 175/1000 | Loss: 0.00002513
Iteration 176/1000 | Loss: 0.00002513
Iteration 177/1000 | Loss: 0.00002513
Iteration 178/1000 | Loss: 0.00002513
Iteration 179/1000 | Loss: 0.00002513
Iteration 180/1000 | Loss: 0.00002513
Iteration 181/1000 | Loss: 0.00002513
Iteration 182/1000 | Loss: 0.00002512
Iteration 183/1000 | Loss: 0.00002512
Iteration 184/1000 | Loss: 0.00002512
Iteration 185/1000 | Loss: 0.00002512
Iteration 186/1000 | Loss: 0.00002512
Iteration 187/1000 | Loss: 0.00002512
Iteration 188/1000 | Loss: 0.00002512
Iteration 189/1000 | Loss: 0.00002512
Iteration 190/1000 | Loss: 0.00002512
Iteration 191/1000 | Loss: 0.00002512
Iteration 192/1000 | Loss: 0.00002512
Iteration 193/1000 | Loss: 0.00002512
Iteration 194/1000 | Loss: 0.00002512
Iteration 195/1000 | Loss: 0.00002512
Iteration 196/1000 | Loss: 0.00002512
Iteration 197/1000 | Loss: 0.00002512
Iteration 198/1000 | Loss: 0.00002512
Iteration 199/1000 | Loss: 0.00002512
Iteration 200/1000 | Loss: 0.00002512
Iteration 201/1000 | Loss: 0.00002512
Iteration 202/1000 | Loss: 0.00002512
Iteration 203/1000 | Loss: 0.00002512
Iteration 204/1000 | Loss: 0.00002512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.5123221348621882e-05, 2.5123221348621882e-05, 2.5123221348621882e-05, 2.5123221348621882e-05, 2.5123221348621882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5123221348621882e-05

Optimization complete. Final v2v error: 3.829986095428467 mm

Highest mean error: 10.930667877197266 mm for frame 60

Lowest mean error: 3.451190948486328 mm for frame 77

Saving results

Total time: 128.99219346046448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999538
Iteration 2/25 | Loss: 0.00267176
Iteration 3/25 | Loss: 0.00213619
Iteration 4/25 | Loss: 0.00215357
Iteration 5/25 | Loss: 0.00180464
Iteration 6/25 | Loss: 0.00169774
Iteration 7/25 | Loss: 0.00162840
Iteration 8/25 | Loss: 0.00159009
Iteration 9/25 | Loss: 0.00150508
Iteration 10/25 | Loss: 0.00148015
Iteration 11/25 | Loss: 0.00147143
Iteration 12/25 | Loss: 0.00144894
Iteration 13/25 | Loss: 0.00144527
Iteration 14/25 | Loss: 0.00144376
Iteration 15/25 | Loss: 0.00144815
Iteration 16/25 | Loss: 0.00145253
Iteration 17/25 | Loss: 0.00143340
Iteration 18/25 | Loss: 0.00143273
Iteration 19/25 | Loss: 0.00142882
Iteration 20/25 | Loss: 0.00142596
Iteration 21/25 | Loss: 0.00142164
Iteration 22/25 | Loss: 0.00142048
Iteration 23/25 | Loss: 0.00142019
Iteration 24/25 | Loss: 0.00142014
Iteration 25/25 | Loss: 0.00142014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63108182
Iteration 2/25 | Loss: 0.00231955
Iteration 3/25 | Loss: 0.00178141
Iteration 4/25 | Loss: 0.00178140
Iteration 5/25 | Loss: 0.00178140
Iteration 6/25 | Loss: 0.00178140
Iteration 7/25 | Loss: 0.00178140
Iteration 8/25 | Loss: 0.00178140
Iteration 9/25 | Loss: 0.00178140
Iteration 10/25 | Loss: 0.00178140
Iteration 11/25 | Loss: 0.00178140
Iteration 12/25 | Loss: 0.00178140
Iteration 13/25 | Loss: 0.00178140
Iteration 14/25 | Loss: 0.00178140
Iteration 15/25 | Loss: 0.00178140
Iteration 16/25 | Loss: 0.00178140
Iteration 17/25 | Loss: 0.00178140
Iteration 18/25 | Loss: 0.00178140
Iteration 19/25 | Loss: 0.00178140
Iteration 20/25 | Loss: 0.00178140
Iteration 21/25 | Loss: 0.00178140
Iteration 22/25 | Loss: 0.00178140
Iteration 23/25 | Loss: 0.00178140
Iteration 24/25 | Loss: 0.00178140
Iteration 25/25 | Loss: 0.00178140

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178140
Iteration 2/1000 | Loss: 0.00171133
Iteration 3/1000 | Loss: 0.00131511
Iteration 4/1000 | Loss: 0.00195516
Iteration 5/1000 | Loss: 0.00144075
Iteration 6/1000 | Loss: 0.00145742
Iteration 7/1000 | Loss: 0.00041064
Iteration 8/1000 | Loss: 0.00038871
Iteration 9/1000 | Loss: 0.00022550
Iteration 10/1000 | Loss: 0.00039180
Iteration 11/1000 | Loss: 0.00029717
Iteration 12/1000 | Loss: 0.00015624
Iteration 13/1000 | Loss: 0.00057964
Iteration 14/1000 | Loss: 0.00052258
Iteration 15/1000 | Loss: 0.00010134
Iteration 16/1000 | Loss: 0.00028209
Iteration 17/1000 | Loss: 0.00012243
Iteration 18/1000 | Loss: 0.00036766
Iteration 19/1000 | Loss: 0.00010285
Iteration 20/1000 | Loss: 0.00034616
Iteration 21/1000 | Loss: 0.00018952
Iteration 22/1000 | Loss: 0.00028802
Iteration 23/1000 | Loss: 0.00017562
Iteration 24/1000 | Loss: 0.00007527
Iteration 25/1000 | Loss: 0.00013738
Iteration 26/1000 | Loss: 0.00030993
Iteration 27/1000 | Loss: 0.00016897
Iteration 28/1000 | Loss: 0.00016803
Iteration 29/1000 | Loss: 0.00007330
Iteration 30/1000 | Loss: 0.00011386
Iteration 31/1000 | Loss: 0.00016857
Iteration 32/1000 | Loss: 0.00022016
Iteration 33/1000 | Loss: 0.00018218
Iteration 34/1000 | Loss: 0.00022639
Iteration 35/1000 | Loss: 0.00007075
Iteration 36/1000 | Loss: 0.00033328
Iteration 37/1000 | Loss: 0.00014785
Iteration 38/1000 | Loss: 0.00021639
Iteration 39/1000 | Loss: 0.00012814
Iteration 40/1000 | Loss: 0.00013879
Iteration 41/1000 | Loss: 0.00020588
Iteration 42/1000 | Loss: 0.00018782
Iteration 43/1000 | Loss: 0.00015487
Iteration 44/1000 | Loss: 0.00019239
Iteration 45/1000 | Loss: 0.00019262
Iteration 46/1000 | Loss: 0.00010100
Iteration 47/1000 | Loss: 0.00011765
Iteration 48/1000 | Loss: 0.00010878
Iteration 49/1000 | Loss: 0.00033269
Iteration 50/1000 | Loss: 0.00014730
Iteration 51/1000 | Loss: 0.00007109
Iteration 52/1000 | Loss: 0.00042629
Iteration 53/1000 | Loss: 0.00151353
Iteration 54/1000 | Loss: 0.00105429
Iteration 55/1000 | Loss: 0.00060034
Iteration 56/1000 | Loss: 0.00011211
Iteration 57/1000 | Loss: 0.00007417
Iteration 58/1000 | Loss: 0.00035659
Iteration 59/1000 | Loss: 0.00027775
Iteration 60/1000 | Loss: 0.00011456
Iteration 61/1000 | Loss: 0.00008696
Iteration 62/1000 | Loss: 0.00007816
Iteration 63/1000 | Loss: 0.00005567
Iteration 64/1000 | Loss: 0.00013985
Iteration 65/1000 | Loss: 0.00004741
Iteration 66/1000 | Loss: 0.00008807
Iteration 67/1000 | Loss: 0.00042899
Iteration 68/1000 | Loss: 0.00027381
Iteration 69/1000 | Loss: 0.00007040
Iteration 70/1000 | Loss: 0.00043287
Iteration 71/1000 | Loss: 0.00006505
Iteration 72/1000 | Loss: 0.00011845
Iteration 73/1000 | Loss: 0.00006553
Iteration 74/1000 | Loss: 0.00017043
Iteration 75/1000 | Loss: 0.00005477
Iteration 76/1000 | Loss: 0.00004540
Iteration 77/1000 | Loss: 0.00003621
Iteration 78/1000 | Loss: 0.00008583
Iteration 79/1000 | Loss: 0.00003503
Iteration 80/1000 | Loss: 0.00003438
Iteration 81/1000 | Loss: 0.00009632
Iteration 82/1000 | Loss: 0.00003382
Iteration 83/1000 | Loss: 0.00007301
Iteration 84/1000 | Loss: 0.00017507
Iteration 85/1000 | Loss: 0.00003839
Iteration 86/1000 | Loss: 0.00003907
Iteration 87/1000 | Loss: 0.00004316
Iteration 88/1000 | Loss: 0.00003274
Iteration 89/1000 | Loss: 0.00003272
Iteration 90/1000 | Loss: 0.00006650
Iteration 91/1000 | Loss: 0.00009245
Iteration 92/1000 | Loss: 0.00003234
Iteration 93/1000 | Loss: 0.00003214
Iteration 94/1000 | Loss: 0.00003202
Iteration 95/1000 | Loss: 0.00007677
Iteration 96/1000 | Loss: 0.00013018
Iteration 97/1000 | Loss: 0.00004391
Iteration 98/1000 | Loss: 0.00003598
Iteration 99/1000 | Loss: 0.00004367
Iteration 100/1000 | Loss: 0.00005271
Iteration 101/1000 | Loss: 0.00003200
Iteration 102/1000 | Loss: 0.00006737
Iteration 103/1000 | Loss: 0.00003138
Iteration 104/1000 | Loss: 0.00003128
Iteration 105/1000 | Loss: 0.00003128
Iteration 106/1000 | Loss: 0.00003112
Iteration 107/1000 | Loss: 0.00010494
Iteration 108/1000 | Loss: 0.00018434
Iteration 109/1000 | Loss: 0.00017236
Iteration 110/1000 | Loss: 0.00017962
Iteration 111/1000 | Loss: 0.00028710
Iteration 112/1000 | Loss: 0.00011580
Iteration 113/1000 | Loss: 0.00005568
Iteration 114/1000 | Loss: 0.00004090
Iteration 115/1000 | Loss: 0.00003095
Iteration 116/1000 | Loss: 0.00004903
Iteration 117/1000 | Loss: 0.00003076
Iteration 118/1000 | Loss: 0.00003064
Iteration 119/1000 | Loss: 0.00008628
Iteration 120/1000 | Loss: 0.00014715
Iteration 121/1000 | Loss: 0.00006921
Iteration 122/1000 | Loss: 0.00004482
Iteration 123/1000 | Loss: 0.00006287
Iteration 124/1000 | Loss: 0.00003208
Iteration 125/1000 | Loss: 0.00007102
Iteration 126/1000 | Loss: 0.00004295
Iteration 127/1000 | Loss: 0.00003161
Iteration 128/1000 | Loss: 0.00007814
Iteration 129/1000 | Loss: 0.00004839
Iteration 130/1000 | Loss: 0.00003317
Iteration 131/1000 | Loss: 0.00003261
Iteration 132/1000 | Loss: 0.00007019
Iteration 133/1000 | Loss: 0.00024347
Iteration 134/1000 | Loss: 0.00011482
Iteration 135/1000 | Loss: 0.00008730
Iteration 136/1000 | Loss: 0.00003821
Iteration 137/1000 | Loss: 0.00003641
Iteration 138/1000 | Loss: 0.00003133
Iteration 139/1000 | Loss: 0.00003038
Iteration 140/1000 | Loss: 0.00003038
Iteration 141/1000 | Loss: 0.00003038
Iteration 142/1000 | Loss: 0.00003038
Iteration 143/1000 | Loss: 0.00003038
Iteration 144/1000 | Loss: 0.00003037
Iteration 145/1000 | Loss: 0.00003037
Iteration 146/1000 | Loss: 0.00003037
Iteration 147/1000 | Loss: 0.00003037
Iteration 148/1000 | Loss: 0.00003037
Iteration 149/1000 | Loss: 0.00003036
Iteration 150/1000 | Loss: 0.00003036
Iteration 151/1000 | Loss: 0.00003036
Iteration 152/1000 | Loss: 0.00003036
Iteration 153/1000 | Loss: 0.00003036
Iteration 154/1000 | Loss: 0.00003035
Iteration 155/1000 | Loss: 0.00003035
Iteration 156/1000 | Loss: 0.00003035
Iteration 157/1000 | Loss: 0.00003035
Iteration 158/1000 | Loss: 0.00003034
Iteration 159/1000 | Loss: 0.00003034
Iteration 160/1000 | Loss: 0.00003034
Iteration 161/1000 | Loss: 0.00003033
Iteration 162/1000 | Loss: 0.00005246
Iteration 163/1000 | Loss: 0.00003038
Iteration 164/1000 | Loss: 0.00003027
Iteration 165/1000 | Loss: 0.00003027
Iteration 166/1000 | Loss: 0.00003027
Iteration 167/1000 | Loss: 0.00003026
Iteration 168/1000 | Loss: 0.00003025
Iteration 169/1000 | Loss: 0.00003020
Iteration 170/1000 | Loss: 0.00007515
Iteration 171/1000 | Loss: 0.00005427
Iteration 172/1000 | Loss: 0.00003111
Iteration 173/1000 | Loss: 0.00005689
Iteration 174/1000 | Loss: 0.00003016
Iteration 175/1000 | Loss: 0.00007106
Iteration 176/1000 | Loss: 0.00005626
Iteration 177/1000 | Loss: 0.00003554
Iteration 178/1000 | Loss: 0.00007945
Iteration 179/1000 | Loss: 0.00003072
Iteration 180/1000 | Loss: 0.00003442
Iteration 181/1000 | Loss: 0.00003004
Iteration 182/1000 | Loss: 0.00002986
Iteration 183/1000 | Loss: 0.00002981
Iteration 184/1000 | Loss: 0.00002981
Iteration 185/1000 | Loss: 0.00004826
Iteration 186/1000 | Loss: 0.00003035
Iteration 187/1000 | Loss: 0.00003048
Iteration 188/1000 | Loss: 0.00002978
Iteration 189/1000 | Loss: 0.00002978
Iteration 190/1000 | Loss: 0.00002978
Iteration 191/1000 | Loss: 0.00002978
Iteration 192/1000 | Loss: 0.00002978
Iteration 193/1000 | Loss: 0.00002977
Iteration 194/1000 | Loss: 0.00002977
Iteration 195/1000 | Loss: 0.00002977
Iteration 196/1000 | Loss: 0.00002977
Iteration 197/1000 | Loss: 0.00002977
Iteration 198/1000 | Loss: 0.00002976
Iteration 199/1000 | Loss: 0.00002976
Iteration 200/1000 | Loss: 0.00002976
Iteration 201/1000 | Loss: 0.00002975
Iteration 202/1000 | Loss: 0.00002975
Iteration 203/1000 | Loss: 0.00008934
Iteration 204/1000 | Loss: 0.00002976
Iteration 205/1000 | Loss: 0.00002975
Iteration 206/1000 | Loss: 0.00002971
Iteration 207/1000 | Loss: 0.00002971
Iteration 208/1000 | Loss: 0.00002970
Iteration 209/1000 | Loss: 0.00002970
Iteration 210/1000 | Loss: 0.00002970
Iteration 211/1000 | Loss: 0.00002970
Iteration 212/1000 | Loss: 0.00002970
Iteration 213/1000 | Loss: 0.00002969
Iteration 214/1000 | Loss: 0.00002969
Iteration 215/1000 | Loss: 0.00002969
Iteration 216/1000 | Loss: 0.00002969
Iteration 217/1000 | Loss: 0.00002969
Iteration 218/1000 | Loss: 0.00002969
Iteration 219/1000 | Loss: 0.00002969
Iteration 220/1000 | Loss: 0.00002969
Iteration 221/1000 | Loss: 0.00002969
Iteration 222/1000 | Loss: 0.00002969
Iteration 223/1000 | Loss: 0.00002969
Iteration 224/1000 | Loss: 0.00002969
Iteration 225/1000 | Loss: 0.00002968
Iteration 226/1000 | Loss: 0.00002968
Iteration 227/1000 | Loss: 0.00002968
Iteration 228/1000 | Loss: 0.00002968
Iteration 229/1000 | Loss: 0.00002967
Iteration 230/1000 | Loss: 0.00002967
Iteration 231/1000 | Loss: 0.00002967
Iteration 232/1000 | Loss: 0.00002966
Iteration 233/1000 | Loss: 0.00002966
Iteration 234/1000 | Loss: 0.00002966
Iteration 235/1000 | Loss: 0.00002966
Iteration 236/1000 | Loss: 0.00002966
Iteration 237/1000 | Loss: 0.00002966
Iteration 238/1000 | Loss: 0.00006182
Iteration 239/1000 | Loss: 0.00006182
Iteration 240/1000 | Loss: 0.00059599
Iteration 241/1000 | Loss: 0.00006850
Iteration 242/1000 | Loss: 0.00009097
Iteration 243/1000 | Loss: 0.00003512
Iteration 244/1000 | Loss: 0.00010082
Iteration 245/1000 | Loss: 0.00004290
Iteration 246/1000 | Loss: 0.00003424
Iteration 247/1000 | Loss: 0.00007567
Iteration 248/1000 | Loss: 0.00002986
Iteration 249/1000 | Loss: 0.00004072
Iteration 250/1000 | Loss: 0.00003867
Iteration 251/1000 | Loss: 0.00003306
Iteration 252/1000 | Loss: 0.00002970
Iteration 253/1000 | Loss: 0.00004172
Iteration 254/1000 | Loss: 0.00003111
Iteration 255/1000 | Loss: 0.00002966
Iteration 256/1000 | Loss: 0.00002966
Iteration 257/1000 | Loss: 0.00002966
Iteration 258/1000 | Loss: 0.00002965
Iteration 259/1000 | Loss: 0.00002965
Iteration 260/1000 | Loss: 0.00002965
Iteration 261/1000 | Loss: 0.00002965
Iteration 262/1000 | Loss: 0.00002965
Iteration 263/1000 | Loss: 0.00002965
Iteration 264/1000 | Loss: 0.00002965
Iteration 265/1000 | Loss: 0.00002965
Iteration 266/1000 | Loss: 0.00003001
Iteration 267/1000 | Loss: 0.00002964
Iteration 268/1000 | Loss: 0.00002964
Iteration 269/1000 | Loss: 0.00002964
Iteration 270/1000 | Loss: 0.00002964
Iteration 271/1000 | Loss: 0.00002964
Iteration 272/1000 | Loss: 0.00002964
Iteration 273/1000 | Loss: 0.00002964
Iteration 274/1000 | Loss: 0.00002964
Iteration 275/1000 | Loss: 0.00002964
Iteration 276/1000 | Loss: 0.00002964
Iteration 277/1000 | Loss: 0.00002964
Iteration 278/1000 | Loss: 0.00002963
Iteration 279/1000 | Loss: 0.00002963
Iteration 280/1000 | Loss: 0.00002963
Iteration 281/1000 | Loss: 0.00002963
Iteration 282/1000 | Loss: 0.00002963
Iteration 283/1000 | Loss: 0.00002963
Iteration 284/1000 | Loss: 0.00002963
Iteration 285/1000 | Loss: 0.00002962
Iteration 286/1000 | Loss: 0.00002962
Iteration 287/1000 | Loss: 0.00002962
Iteration 288/1000 | Loss: 0.00002962
Iteration 289/1000 | Loss: 0.00002962
Iteration 290/1000 | Loss: 0.00002962
Iteration 291/1000 | Loss: 0.00002961
Iteration 292/1000 | Loss: 0.00002961
Iteration 293/1000 | Loss: 0.00002961
Iteration 294/1000 | Loss: 0.00002961
Iteration 295/1000 | Loss: 0.00002961
Iteration 296/1000 | Loss: 0.00002961
Iteration 297/1000 | Loss: 0.00002961
Iteration 298/1000 | Loss: 0.00002961
Iteration 299/1000 | Loss: 0.00002961
Iteration 300/1000 | Loss: 0.00002961
Iteration 301/1000 | Loss: 0.00002961
Iteration 302/1000 | Loss: 0.00002961
Iteration 303/1000 | Loss: 0.00002961
Iteration 304/1000 | Loss: 0.00002961
Iteration 305/1000 | Loss: 0.00002961
Iteration 306/1000 | Loss: 0.00002961
Iteration 307/1000 | Loss: 0.00002961
Iteration 308/1000 | Loss: 0.00002961
Iteration 309/1000 | Loss: 0.00002960
Iteration 310/1000 | Loss: 0.00002960
Iteration 311/1000 | Loss: 0.00002960
Iteration 312/1000 | Loss: 0.00002960
Iteration 313/1000 | Loss: 0.00002960
Iteration 314/1000 | Loss: 0.00002960
Iteration 315/1000 | Loss: 0.00002960
Iteration 316/1000 | Loss: 0.00002960
Iteration 317/1000 | Loss: 0.00002960
Iteration 318/1000 | Loss: 0.00002960
Iteration 319/1000 | Loss: 0.00002960
Iteration 320/1000 | Loss: 0.00002959
Iteration 321/1000 | Loss: 0.00002959
Iteration 322/1000 | Loss: 0.00002959
Iteration 323/1000 | Loss: 0.00002959
Iteration 324/1000 | Loss: 0.00002959
Iteration 325/1000 | Loss: 0.00002959
Iteration 326/1000 | Loss: 0.00002959
Iteration 327/1000 | Loss: 0.00002959
Iteration 328/1000 | Loss: 0.00002959
Iteration 329/1000 | Loss: 0.00002959
Iteration 330/1000 | Loss: 0.00002959
Iteration 331/1000 | Loss: 0.00002959
Iteration 332/1000 | Loss: 0.00002959
Iteration 333/1000 | Loss: 0.00002959
Iteration 334/1000 | Loss: 0.00002959
Iteration 335/1000 | Loss: 0.00002959
Iteration 336/1000 | Loss: 0.00002959
Iteration 337/1000 | Loss: 0.00002959
Iteration 338/1000 | Loss: 0.00002959
Iteration 339/1000 | Loss: 0.00002959
Iteration 340/1000 | Loss: 0.00002959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [2.959266203106381e-05, 2.959266203106381e-05, 2.959266203106381e-05, 2.959266203106381e-05, 2.959266203106381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.959266203106381e-05

Optimization complete. Final v2v error: 3.676910638809204 mm

Highest mean error: 10.983872413635254 mm for frame 119

Lowest mean error: 2.837644577026367 mm for frame 22

Saving results

Total time: 326.2775707244873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403004
Iteration 2/25 | Loss: 0.00133069
Iteration 3/25 | Loss: 0.00122889
Iteration 4/25 | Loss: 0.00122183
Iteration 5/25 | Loss: 0.00122183
Iteration 6/25 | Loss: 0.00122183
Iteration 7/25 | Loss: 0.00122183
Iteration 8/25 | Loss: 0.00122183
Iteration 9/25 | Loss: 0.00122183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012218283955007792, 0.0012218283955007792, 0.0012218283955007792, 0.0012218283955007792, 0.0012218283955007792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012218283955007792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63008106
Iteration 2/25 | Loss: 0.00069979
Iteration 3/25 | Loss: 0.00069979
Iteration 4/25 | Loss: 0.00069979
Iteration 5/25 | Loss: 0.00069979
Iteration 6/25 | Loss: 0.00069979
Iteration 7/25 | Loss: 0.00069979
Iteration 8/25 | Loss: 0.00069979
Iteration 9/25 | Loss: 0.00069979
Iteration 10/25 | Loss: 0.00069979
Iteration 11/25 | Loss: 0.00069979
Iteration 12/25 | Loss: 0.00069979
Iteration 13/25 | Loss: 0.00069979
Iteration 14/25 | Loss: 0.00069979
Iteration 15/25 | Loss: 0.00069979
Iteration 16/25 | Loss: 0.00069979
Iteration 17/25 | Loss: 0.00069979
Iteration 18/25 | Loss: 0.00069979
Iteration 19/25 | Loss: 0.00069979
Iteration 20/25 | Loss: 0.00069979
Iteration 21/25 | Loss: 0.00069979
Iteration 22/25 | Loss: 0.00069979
Iteration 23/25 | Loss: 0.00069979
Iteration 24/25 | Loss: 0.00069979
Iteration 25/25 | Loss: 0.00069979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006997886230237782, 0.0006997886230237782, 0.0006997886230237782, 0.0006997886230237782, 0.0006997886230237782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006997886230237782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069979
Iteration 2/1000 | Loss: 0.00002989
Iteration 3/1000 | Loss: 0.00001775
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001498
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001243
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001238
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001237
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001228
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001218
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001201
Iteration 109/1000 | Loss: 0.00001201
Iteration 110/1000 | Loss: 0.00001201
Iteration 111/1000 | Loss: 0.00001201
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001196
Iteration 129/1000 | Loss: 0.00001196
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Iteration 132/1000 | Loss: 0.00001195
Iteration 133/1000 | Loss: 0.00001195
Iteration 134/1000 | Loss: 0.00001195
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001194
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001189
Iteration 159/1000 | Loss: 0.00001189
Iteration 160/1000 | Loss: 0.00001189
Iteration 161/1000 | Loss: 0.00001189
Iteration 162/1000 | Loss: 0.00001189
Iteration 163/1000 | Loss: 0.00001188
Iteration 164/1000 | Loss: 0.00001188
Iteration 165/1000 | Loss: 0.00001188
Iteration 166/1000 | Loss: 0.00001188
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001187
Iteration 169/1000 | Loss: 0.00001187
Iteration 170/1000 | Loss: 0.00001187
Iteration 171/1000 | Loss: 0.00001187
Iteration 172/1000 | Loss: 0.00001187
Iteration 173/1000 | Loss: 0.00001187
Iteration 174/1000 | Loss: 0.00001187
Iteration 175/1000 | Loss: 0.00001186
Iteration 176/1000 | Loss: 0.00001186
Iteration 177/1000 | Loss: 0.00001186
Iteration 178/1000 | Loss: 0.00001186
Iteration 179/1000 | Loss: 0.00001186
Iteration 180/1000 | Loss: 0.00001186
Iteration 181/1000 | Loss: 0.00001186
Iteration 182/1000 | Loss: 0.00001186
Iteration 183/1000 | Loss: 0.00001186
Iteration 184/1000 | Loss: 0.00001186
Iteration 185/1000 | Loss: 0.00001186
Iteration 186/1000 | Loss: 0.00001185
Iteration 187/1000 | Loss: 0.00001185
Iteration 188/1000 | Loss: 0.00001185
Iteration 189/1000 | Loss: 0.00001185
Iteration 190/1000 | Loss: 0.00001185
Iteration 191/1000 | Loss: 0.00001185
Iteration 192/1000 | Loss: 0.00001184
Iteration 193/1000 | Loss: 0.00001184
Iteration 194/1000 | Loss: 0.00001184
Iteration 195/1000 | Loss: 0.00001184
Iteration 196/1000 | Loss: 0.00001184
Iteration 197/1000 | Loss: 0.00001184
Iteration 198/1000 | Loss: 0.00001184
Iteration 199/1000 | Loss: 0.00001184
Iteration 200/1000 | Loss: 0.00001184
Iteration 201/1000 | Loss: 0.00001184
Iteration 202/1000 | Loss: 0.00001184
Iteration 203/1000 | Loss: 0.00001184
Iteration 204/1000 | Loss: 0.00001184
Iteration 205/1000 | Loss: 0.00001184
Iteration 206/1000 | Loss: 0.00001184
Iteration 207/1000 | Loss: 0.00001184
Iteration 208/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.1839055332529824e-05, 1.1839055332529824e-05, 1.1839055332529824e-05, 1.1839055332529824e-05, 1.1839055332529824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1839055332529824e-05

Optimization complete. Final v2v error: 2.9363937377929688 mm

Highest mean error: 3.096484661102295 mm for frame 93

Lowest mean error: 2.7920494079589844 mm for frame 5

Saving results

Total time: 46.18842816352844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437090
Iteration 2/25 | Loss: 0.00134237
Iteration 3/25 | Loss: 0.00122956
Iteration 4/25 | Loss: 0.00121571
Iteration 5/25 | Loss: 0.00121204
Iteration 6/25 | Loss: 0.00121116
Iteration 7/25 | Loss: 0.00121116
Iteration 8/25 | Loss: 0.00121116
Iteration 9/25 | Loss: 0.00121116
Iteration 10/25 | Loss: 0.00121116
Iteration 11/25 | Loss: 0.00121116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012111550895497203, 0.0012111550895497203, 0.0012111550895497203, 0.0012111550895497203, 0.0012111550895497203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012111550895497203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.59159040
Iteration 2/25 | Loss: 0.00076276
Iteration 3/25 | Loss: 0.00076275
Iteration 4/25 | Loss: 0.00076275
Iteration 5/25 | Loss: 0.00076275
Iteration 6/25 | Loss: 0.00076275
Iteration 7/25 | Loss: 0.00076275
Iteration 8/25 | Loss: 0.00076275
Iteration 9/25 | Loss: 0.00076275
Iteration 10/25 | Loss: 0.00076275
Iteration 11/25 | Loss: 0.00076275
Iteration 12/25 | Loss: 0.00076275
Iteration 13/25 | Loss: 0.00076275
Iteration 14/25 | Loss: 0.00076275
Iteration 15/25 | Loss: 0.00076275
Iteration 16/25 | Loss: 0.00076275
Iteration 17/25 | Loss: 0.00076275
Iteration 18/25 | Loss: 0.00076275
Iteration 19/25 | Loss: 0.00076275
Iteration 20/25 | Loss: 0.00076275
Iteration 21/25 | Loss: 0.00076275
Iteration 22/25 | Loss: 0.00076275
Iteration 23/25 | Loss: 0.00076275
Iteration 24/25 | Loss: 0.00076275
Iteration 25/25 | Loss: 0.00076275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007627456798218191, 0.0007627456798218191, 0.0007627456798218191, 0.0007627456798218191, 0.0007627456798218191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007627456798218191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076275
Iteration 2/1000 | Loss: 0.00002889
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001727
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001568
Iteration 7/1000 | Loss: 0.00001518
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00001409
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001391
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001389
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001382
Iteration 46/1000 | Loss: 0.00001382
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001378
Iteration 57/1000 | Loss: 0.00001378
Iteration 58/1000 | Loss: 0.00001378
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001375
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001371
Iteration 71/1000 | Loss: 0.00001371
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001363
Iteration 92/1000 | Loss: 0.00001363
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001357
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001356
Iteration 112/1000 | Loss: 0.00001356
Iteration 113/1000 | Loss: 0.00001356
Iteration 114/1000 | Loss: 0.00001356
Iteration 115/1000 | Loss: 0.00001356
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001354
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001353
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001352
Iteration 143/1000 | Loss: 0.00001352
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001351
Iteration 147/1000 | Loss: 0.00001351
Iteration 148/1000 | Loss: 0.00001351
Iteration 149/1000 | Loss: 0.00001351
Iteration 150/1000 | Loss: 0.00001351
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001349
Iteration 158/1000 | Loss: 0.00001349
Iteration 159/1000 | Loss: 0.00001349
Iteration 160/1000 | Loss: 0.00001349
Iteration 161/1000 | Loss: 0.00001349
Iteration 162/1000 | Loss: 0.00001349
Iteration 163/1000 | Loss: 0.00001349
Iteration 164/1000 | Loss: 0.00001348
Iteration 165/1000 | Loss: 0.00001348
Iteration 166/1000 | Loss: 0.00001348
Iteration 167/1000 | Loss: 0.00001348
Iteration 168/1000 | Loss: 0.00001348
Iteration 169/1000 | Loss: 0.00001348
Iteration 170/1000 | Loss: 0.00001348
Iteration 171/1000 | Loss: 0.00001348
Iteration 172/1000 | Loss: 0.00001348
Iteration 173/1000 | Loss: 0.00001348
Iteration 174/1000 | Loss: 0.00001347
Iteration 175/1000 | Loss: 0.00001347
Iteration 176/1000 | Loss: 0.00001347
Iteration 177/1000 | Loss: 0.00001347
Iteration 178/1000 | Loss: 0.00001347
Iteration 179/1000 | Loss: 0.00001347
Iteration 180/1000 | Loss: 0.00001347
Iteration 181/1000 | Loss: 0.00001347
Iteration 182/1000 | Loss: 0.00001347
Iteration 183/1000 | Loss: 0.00001347
Iteration 184/1000 | Loss: 0.00001347
Iteration 185/1000 | Loss: 0.00001347
Iteration 186/1000 | Loss: 0.00001347
Iteration 187/1000 | Loss: 0.00001347
Iteration 188/1000 | Loss: 0.00001346
Iteration 189/1000 | Loss: 0.00001346
Iteration 190/1000 | Loss: 0.00001346
Iteration 191/1000 | Loss: 0.00001346
Iteration 192/1000 | Loss: 0.00001346
Iteration 193/1000 | Loss: 0.00001346
Iteration 194/1000 | Loss: 0.00001346
Iteration 195/1000 | Loss: 0.00001346
Iteration 196/1000 | Loss: 0.00001346
Iteration 197/1000 | Loss: 0.00001346
Iteration 198/1000 | Loss: 0.00001346
Iteration 199/1000 | Loss: 0.00001346
Iteration 200/1000 | Loss: 0.00001346
Iteration 201/1000 | Loss: 0.00001346
Iteration 202/1000 | Loss: 0.00001346
Iteration 203/1000 | Loss: 0.00001346
Iteration 204/1000 | Loss: 0.00001346
Iteration 205/1000 | Loss: 0.00001346
Iteration 206/1000 | Loss: 0.00001346
Iteration 207/1000 | Loss: 0.00001345
Iteration 208/1000 | Loss: 0.00001345
Iteration 209/1000 | Loss: 0.00001345
Iteration 210/1000 | Loss: 0.00001345
Iteration 211/1000 | Loss: 0.00001345
Iteration 212/1000 | Loss: 0.00001345
Iteration 213/1000 | Loss: 0.00001345
Iteration 214/1000 | Loss: 0.00001345
Iteration 215/1000 | Loss: 0.00001345
Iteration 216/1000 | Loss: 0.00001345
Iteration 217/1000 | Loss: 0.00001345
Iteration 218/1000 | Loss: 0.00001345
Iteration 219/1000 | Loss: 0.00001345
Iteration 220/1000 | Loss: 0.00001345
Iteration 221/1000 | Loss: 0.00001345
Iteration 222/1000 | Loss: 0.00001345
Iteration 223/1000 | Loss: 0.00001345
Iteration 224/1000 | Loss: 0.00001345
Iteration 225/1000 | Loss: 0.00001345
Iteration 226/1000 | Loss: 0.00001345
Iteration 227/1000 | Loss: 0.00001345
Iteration 228/1000 | Loss: 0.00001345
Iteration 229/1000 | Loss: 0.00001345
Iteration 230/1000 | Loss: 0.00001345
Iteration 231/1000 | Loss: 0.00001345
Iteration 232/1000 | Loss: 0.00001345
Iteration 233/1000 | Loss: 0.00001345
Iteration 234/1000 | Loss: 0.00001345
Iteration 235/1000 | Loss: 0.00001345
Iteration 236/1000 | Loss: 0.00001345
Iteration 237/1000 | Loss: 0.00001345
Iteration 238/1000 | Loss: 0.00001345
Iteration 239/1000 | Loss: 0.00001345
Iteration 240/1000 | Loss: 0.00001345
Iteration 241/1000 | Loss: 0.00001345
Iteration 242/1000 | Loss: 0.00001345
Iteration 243/1000 | Loss: 0.00001345
Iteration 244/1000 | Loss: 0.00001345
Iteration 245/1000 | Loss: 0.00001345
Iteration 246/1000 | Loss: 0.00001345
Iteration 247/1000 | Loss: 0.00001345
Iteration 248/1000 | Loss: 0.00001345
Iteration 249/1000 | Loss: 0.00001345
Iteration 250/1000 | Loss: 0.00001345
Iteration 251/1000 | Loss: 0.00001345
Iteration 252/1000 | Loss: 0.00001345
Iteration 253/1000 | Loss: 0.00001345
Iteration 254/1000 | Loss: 0.00001345
Iteration 255/1000 | Loss: 0.00001345
Iteration 256/1000 | Loss: 0.00001345
Iteration 257/1000 | Loss: 0.00001345
Iteration 258/1000 | Loss: 0.00001345
Iteration 259/1000 | Loss: 0.00001345
Iteration 260/1000 | Loss: 0.00001345
Iteration 261/1000 | Loss: 0.00001345
Iteration 262/1000 | Loss: 0.00001345
Iteration 263/1000 | Loss: 0.00001345
Iteration 264/1000 | Loss: 0.00001345
Iteration 265/1000 | Loss: 0.00001345
Iteration 266/1000 | Loss: 0.00001345
Iteration 267/1000 | Loss: 0.00001345
Iteration 268/1000 | Loss: 0.00001345
Iteration 269/1000 | Loss: 0.00001345
Iteration 270/1000 | Loss: 0.00001345
Iteration 271/1000 | Loss: 0.00001345
Iteration 272/1000 | Loss: 0.00001345
Iteration 273/1000 | Loss: 0.00001345
Iteration 274/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [1.3445691365632229e-05, 1.3445691365632229e-05, 1.3445691365632229e-05, 1.3445691365632229e-05, 1.3445691365632229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3445691365632229e-05

Optimization complete. Final v2v error: 3.113178014755249 mm

Highest mean error: 3.835240364074707 mm for frame 89

Lowest mean error: 2.7786569595336914 mm for frame 39

Saving results

Total time: 42.88677644729614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429576
Iteration 2/25 | Loss: 0.00139340
Iteration 3/25 | Loss: 0.00127024
Iteration 4/25 | Loss: 0.00125229
Iteration 5/25 | Loss: 0.00124772
Iteration 6/25 | Loss: 0.00124658
Iteration 7/25 | Loss: 0.00124594
Iteration 8/25 | Loss: 0.00124594
Iteration 9/25 | Loss: 0.00124594
Iteration 10/25 | Loss: 0.00124594
Iteration 11/25 | Loss: 0.00124594
Iteration 12/25 | Loss: 0.00124594
Iteration 13/25 | Loss: 0.00124594
Iteration 14/25 | Loss: 0.00124594
Iteration 15/25 | Loss: 0.00124594
Iteration 16/25 | Loss: 0.00124594
Iteration 17/25 | Loss: 0.00124594
Iteration 18/25 | Loss: 0.00124594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001245938939973712, 0.001245938939973712, 0.001245938939973712, 0.001245938939973712, 0.001245938939973712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001245938939973712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44534743
Iteration 2/25 | Loss: 0.00091395
Iteration 3/25 | Loss: 0.00091395
Iteration 4/25 | Loss: 0.00091395
Iteration 5/25 | Loss: 0.00091395
Iteration 6/25 | Loss: 0.00091395
Iteration 7/25 | Loss: 0.00091395
Iteration 8/25 | Loss: 0.00091394
Iteration 9/25 | Loss: 0.00091394
Iteration 10/25 | Loss: 0.00091394
Iteration 11/25 | Loss: 0.00091394
Iteration 12/25 | Loss: 0.00091394
Iteration 13/25 | Loss: 0.00091394
Iteration 14/25 | Loss: 0.00091394
Iteration 15/25 | Loss: 0.00091394
Iteration 16/25 | Loss: 0.00091394
Iteration 17/25 | Loss: 0.00091394
Iteration 18/25 | Loss: 0.00091394
Iteration 19/25 | Loss: 0.00091394
Iteration 20/25 | Loss: 0.00091394
Iteration 21/25 | Loss: 0.00091394
Iteration 22/25 | Loss: 0.00091394
Iteration 23/25 | Loss: 0.00091394
Iteration 24/25 | Loss: 0.00091394
Iteration 25/25 | Loss: 0.00091394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091394
Iteration 2/1000 | Loss: 0.00003279
Iteration 3/1000 | Loss: 0.00002407
Iteration 4/1000 | Loss: 0.00002179
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001969
Iteration 8/1000 | Loss: 0.00001938
Iteration 9/1000 | Loss: 0.00001921
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00001853
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001850
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001846
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001845
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001844
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001842
Iteration 39/1000 | Loss: 0.00001842
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001833
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001832
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001831
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001830
Iteration 62/1000 | Loss: 0.00001830
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001828
Iteration 71/1000 | Loss: 0.00001828
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001819
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001818
Iteration 117/1000 | Loss: 0.00001818
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001817
Iteration 122/1000 | Loss: 0.00001817
Iteration 123/1000 | Loss: 0.00001817
Iteration 124/1000 | Loss: 0.00001817
Iteration 125/1000 | Loss: 0.00001817
Iteration 126/1000 | Loss: 0.00001817
Iteration 127/1000 | Loss: 0.00001817
Iteration 128/1000 | Loss: 0.00001816
Iteration 129/1000 | Loss: 0.00001816
Iteration 130/1000 | Loss: 0.00001816
Iteration 131/1000 | Loss: 0.00001816
Iteration 132/1000 | Loss: 0.00001816
Iteration 133/1000 | Loss: 0.00001816
Iteration 134/1000 | Loss: 0.00001815
Iteration 135/1000 | Loss: 0.00001815
Iteration 136/1000 | Loss: 0.00001815
Iteration 137/1000 | Loss: 0.00001815
Iteration 138/1000 | Loss: 0.00001815
Iteration 139/1000 | Loss: 0.00001815
Iteration 140/1000 | Loss: 0.00001814
Iteration 141/1000 | Loss: 0.00001814
Iteration 142/1000 | Loss: 0.00001814
Iteration 143/1000 | Loss: 0.00001814
Iteration 144/1000 | Loss: 0.00001814
Iteration 145/1000 | Loss: 0.00001814
Iteration 146/1000 | Loss: 0.00001814
Iteration 147/1000 | Loss: 0.00001814
Iteration 148/1000 | Loss: 0.00001814
Iteration 149/1000 | Loss: 0.00001814
Iteration 150/1000 | Loss: 0.00001814
Iteration 151/1000 | Loss: 0.00001814
Iteration 152/1000 | Loss: 0.00001814
Iteration 153/1000 | Loss: 0.00001814
Iteration 154/1000 | Loss: 0.00001814
Iteration 155/1000 | Loss: 0.00001814
Iteration 156/1000 | Loss: 0.00001814
Iteration 157/1000 | Loss: 0.00001814
Iteration 158/1000 | Loss: 0.00001814
Iteration 159/1000 | Loss: 0.00001814
Iteration 160/1000 | Loss: 0.00001814
Iteration 161/1000 | Loss: 0.00001814
Iteration 162/1000 | Loss: 0.00001814
Iteration 163/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.813604285416659e-05, 1.813604285416659e-05, 1.813604285416659e-05, 1.813604285416659e-05, 1.813604285416659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.813604285416659e-05

Optimization complete. Final v2v error: 3.584895372390747 mm

Highest mean error: 4.101170063018799 mm for frame 22

Lowest mean error: 3.32051420211792 mm for frame 35

Saving results

Total time: 38.19805026054382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041745
Iteration 2/25 | Loss: 0.00143514
Iteration 3/25 | Loss: 0.00125359
Iteration 4/25 | Loss: 0.00123634
Iteration 5/25 | Loss: 0.00123089
Iteration 6/25 | Loss: 0.00123566
Iteration 7/25 | Loss: 0.00122683
Iteration 8/25 | Loss: 0.00123262
Iteration 9/25 | Loss: 0.00122627
Iteration 10/25 | Loss: 0.00121999
Iteration 11/25 | Loss: 0.00121609
Iteration 12/25 | Loss: 0.00121638
Iteration 13/25 | Loss: 0.00121424
Iteration 14/25 | Loss: 0.00121358
Iteration 15/25 | Loss: 0.00121337
Iteration 16/25 | Loss: 0.00121626
Iteration 17/25 | Loss: 0.00121800
Iteration 18/25 | Loss: 0.00121697
Iteration 19/25 | Loss: 0.00122010
Iteration 20/25 | Loss: 0.00121811
Iteration 21/25 | Loss: 0.00121833
Iteration 22/25 | Loss: 0.00121596
Iteration 23/25 | Loss: 0.00121641
Iteration 24/25 | Loss: 0.00121690
Iteration 25/25 | Loss: 0.00121555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37189388
Iteration 2/25 | Loss: 0.00068361
Iteration 3/25 | Loss: 0.00068360
Iteration 4/25 | Loss: 0.00068360
Iteration 5/25 | Loss: 0.00068360
Iteration 6/25 | Loss: 0.00068360
Iteration 7/25 | Loss: 0.00068360
Iteration 8/25 | Loss: 0.00068360
Iteration 9/25 | Loss: 0.00068360
Iteration 10/25 | Loss: 0.00068360
Iteration 11/25 | Loss: 0.00068360
Iteration 12/25 | Loss: 0.00068360
Iteration 13/25 | Loss: 0.00068360
Iteration 14/25 | Loss: 0.00068360
Iteration 15/25 | Loss: 0.00068360
Iteration 16/25 | Loss: 0.00068360
Iteration 17/25 | Loss: 0.00068360
Iteration 18/25 | Loss: 0.00068360
Iteration 19/25 | Loss: 0.00068360
Iteration 20/25 | Loss: 0.00068360
Iteration 21/25 | Loss: 0.00068360
Iteration 22/25 | Loss: 0.00068360
Iteration 23/25 | Loss: 0.00068360
Iteration 24/25 | Loss: 0.00068360
Iteration 25/25 | Loss: 0.00068360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068360
Iteration 2/1000 | Loss: 0.00015748
Iteration 3/1000 | Loss: 0.00018961
Iteration 4/1000 | Loss: 0.00009901
Iteration 5/1000 | Loss: 0.00012176
Iteration 6/1000 | Loss: 0.00015352
Iteration 7/1000 | Loss: 0.00019248
Iteration 8/1000 | Loss: 0.00007087
Iteration 9/1000 | Loss: 0.00010548
Iteration 10/1000 | Loss: 0.00033176
Iteration 11/1000 | Loss: 0.00017320
Iteration 12/1000 | Loss: 0.00029208
Iteration 13/1000 | Loss: 0.00011540
Iteration 14/1000 | Loss: 0.00010642
Iteration 15/1000 | Loss: 0.00009426
Iteration 16/1000 | Loss: 0.00008955
Iteration 17/1000 | Loss: 0.00009822
Iteration 18/1000 | Loss: 0.00013130
Iteration 19/1000 | Loss: 0.00022795
Iteration 20/1000 | Loss: 0.00023033
Iteration 21/1000 | Loss: 0.00018599
Iteration 22/1000 | Loss: 0.00017910
Iteration 23/1000 | Loss: 0.00006163
Iteration 24/1000 | Loss: 0.00003647
Iteration 25/1000 | Loss: 0.00004083
Iteration 26/1000 | Loss: 0.00004028
Iteration 27/1000 | Loss: 0.00002107
Iteration 28/1000 | Loss: 0.00001986
Iteration 29/1000 | Loss: 0.00014698
Iteration 30/1000 | Loss: 0.00021459
Iteration 31/1000 | Loss: 0.00012502
Iteration 32/1000 | Loss: 0.00022972
Iteration 33/1000 | Loss: 0.00010550
Iteration 34/1000 | Loss: 0.00016578
Iteration 35/1000 | Loss: 0.00010755
Iteration 36/1000 | Loss: 0.00013782
Iteration 37/1000 | Loss: 0.00015023
Iteration 38/1000 | Loss: 0.00013764
Iteration 39/1000 | Loss: 0.00013490
Iteration 40/1000 | Loss: 0.00020601
Iteration 41/1000 | Loss: 0.00010730
Iteration 42/1000 | Loss: 0.00039863
Iteration 43/1000 | Loss: 0.00013126
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00021250
Iteration 46/1000 | Loss: 0.00012892
Iteration 47/1000 | Loss: 0.00012684
Iteration 48/1000 | Loss: 0.00022536
Iteration 49/1000 | Loss: 0.00016430
Iteration 50/1000 | Loss: 0.00020340
Iteration 51/1000 | Loss: 0.00007580
Iteration 52/1000 | Loss: 0.00006573
Iteration 53/1000 | Loss: 0.00009761
Iteration 54/1000 | Loss: 0.00027120
Iteration 55/1000 | Loss: 0.00018600
Iteration 56/1000 | Loss: 0.00033678
Iteration 57/1000 | Loss: 0.00028323
Iteration 58/1000 | Loss: 0.00021862
Iteration 59/1000 | Loss: 0.00020997
Iteration 60/1000 | Loss: 0.00006874
Iteration 61/1000 | Loss: 0.00015380
Iteration 62/1000 | Loss: 0.00014812
Iteration 63/1000 | Loss: 0.00028961
Iteration 64/1000 | Loss: 0.00026609
Iteration 65/1000 | Loss: 0.00014137
Iteration 66/1000 | Loss: 0.00023662
Iteration 67/1000 | Loss: 0.00010293
Iteration 68/1000 | Loss: 0.00009643
Iteration 69/1000 | Loss: 0.00031762
Iteration 70/1000 | Loss: 0.00016039
Iteration 71/1000 | Loss: 0.00012516
Iteration 72/1000 | Loss: 0.00015703
Iteration 73/1000 | Loss: 0.00016212
Iteration 74/1000 | Loss: 0.00008571
Iteration 75/1000 | Loss: 0.00002710
Iteration 76/1000 | Loss: 0.00002456
Iteration 77/1000 | Loss: 0.00006962
Iteration 78/1000 | Loss: 0.00002479
Iteration 79/1000 | Loss: 0.00007125
Iteration 80/1000 | Loss: 0.00007182
Iteration 81/1000 | Loss: 0.00005555
Iteration 82/1000 | Loss: 0.00004010
Iteration 83/1000 | Loss: 0.00004143
Iteration 84/1000 | Loss: 0.00003559
Iteration 85/1000 | Loss: 0.00005272
Iteration 86/1000 | Loss: 0.00005597
Iteration 87/1000 | Loss: 0.00004767
Iteration 88/1000 | Loss: 0.00026531
Iteration 89/1000 | Loss: 0.00019235
Iteration 90/1000 | Loss: 0.00018499
Iteration 91/1000 | Loss: 0.00008392
Iteration 92/1000 | Loss: 0.00003127
Iteration 93/1000 | Loss: 0.00015890
Iteration 94/1000 | Loss: 0.00005446
Iteration 95/1000 | Loss: 0.00006570
Iteration 96/1000 | Loss: 0.00024408
Iteration 97/1000 | Loss: 0.00018769
Iteration 98/1000 | Loss: 0.00017984
Iteration 99/1000 | Loss: 0.00017348
Iteration 100/1000 | Loss: 0.00028912
Iteration 101/1000 | Loss: 0.00014805
Iteration 102/1000 | Loss: 0.00001818
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00006986
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00008329
Iteration 107/1000 | Loss: 0.00009063
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00008077
Iteration 110/1000 | Loss: 0.00002226
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00004000
Iteration 113/1000 | Loss: 0.00006005
Iteration 114/1000 | Loss: 0.00008028
Iteration 115/1000 | Loss: 0.00005423
Iteration 116/1000 | Loss: 0.00001983
Iteration 117/1000 | Loss: 0.00001762
Iteration 118/1000 | Loss: 0.00003484
Iteration 119/1000 | Loss: 0.00003643
Iteration 120/1000 | Loss: 0.00019110
Iteration 121/1000 | Loss: 0.00008954
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00040528
Iteration 125/1000 | Loss: 0.00012169
Iteration 126/1000 | Loss: 0.00012099
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00004635
Iteration 129/1000 | Loss: 0.00009123
Iteration 130/1000 | Loss: 0.00019271
Iteration 131/1000 | Loss: 0.00022536
Iteration 132/1000 | Loss: 0.00009022
Iteration 133/1000 | Loss: 0.00001589
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001478
Iteration 136/1000 | Loss: 0.00015112
Iteration 137/1000 | Loss: 0.00012663
Iteration 138/1000 | Loss: 0.00023541
Iteration 139/1000 | Loss: 0.00015058
Iteration 140/1000 | Loss: 0.00016581
Iteration 141/1000 | Loss: 0.00010458
Iteration 142/1000 | Loss: 0.00015266
Iteration 143/1000 | Loss: 0.00008614
Iteration 144/1000 | Loss: 0.00015330
Iteration 145/1000 | Loss: 0.00008296
Iteration 146/1000 | Loss: 0.00010938
Iteration 147/1000 | Loss: 0.00013799
Iteration 148/1000 | Loss: 0.00001835
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00016222
Iteration 151/1000 | Loss: 0.00019177
Iteration 152/1000 | Loss: 0.00010527
Iteration 153/1000 | Loss: 0.00010213
Iteration 154/1000 | Loss: 0.00016442
Iteration 155/1000 | Loss: 0.00015017
Iteration 156/1000 | Loss: 0.00008795
Iteration 157/1000 | Loss: 0.00003114
Iteration 158/1000 | Loss: 0.00002341
Iteration 159/1000 | Loss: 0.00009575
Iteration 160/1000 | Loss: 0.00012778
Iteration 161/1000 | Loss: 0.00009457
Iteration 162/1000 | Loss: 0.00003817
Iteration 163/1000 | Loss: 0.00008232
Iteration 164/1000 | Loss: 0.00008962
Iteration 165/1000 | Loss: 0.00008260
Iteration 166/1000 | Loss: 0.00008127
Iteration 167/1000 | Loss: 0.00007324
Iteration 168/1000 | Loss: 0.00009915
Iteration 169/1000 | Loss: 0.00007603
Iteration 170/1000 | Loss: 0.00012826
Iteration 171/1000 | Loss: 0.00011896
Iteration 172/1000 | Loss: 0.00007098
Iteration 173/1000 | Loss: 0.00008473
Iteration 174/1000 | Loss: 0.00009660
Iteration 175/1000 | Loss: 0.00009773
Iteration 176/1000 | Loss: 0.00002960
Iteration 177/1000 | Loss: 0.00010991
Iteration 178/1000 | Loss: 0.00008118
Iteration 179/1000 | Loss: 0.00005996
Iteration 180/1000 | Loss: 0.00009711
Iteration 181/1000 | Loss: 0.00008464
Iteration 182/1000 | Loss: 0.00002989
Iteration 183/1000 | Loss: 0.00003897
Iteration 184/1000 | Loss: 0.00008316
Iteration 185/1000 | Loss: 0.00013340
Iteration 186/1000 | Loss: 0.00018652
Iteration 187/1000 | Loss: 0.00019932
Iteration 188/1000 | Loss: 0.00014787
Iteration 189/1000 | Loss: 0.00009873
Iteration 190/1000 | Loss: 0.00012734
Iteration 191/1000 | Loss: 0.00003660
Iteration 192/1000 | Loss: 0.00004470
Iteration 193/1000 | Loss: 0.00008286
Iteration 194/1000 | Loss: 0.00017341
Iteration 195/1000 | Loss: 0.00001656
Iteration 196/1000 | Loss: 0.00001433
Iteration 197/1000 | Loss: 0.00001302
Iteration 198/1000 | Loss: 0.00001280
Iteration 199/1000 | Loss: 0.00001258
Iteration 200/1000 | Loss: 0.00001241
Iteration 201/1000 | Loss: 0.00001236
Iteration 202/1000 | Loss: 0.00001236
Iteration 203/1000 | Loss: 0.00001236
Iteration 204/1000 | Loss: 0.00001236
Iteration 205/1000 | Loss: 0.00001236
Iteration 206/1000 | Loss: 0.00001235
Iteration 207/1000 | Loss: 0.00001235
Iteration 208/1000 | Loss: 0.00001235
Iteration 209/1000 | Loss: 0.00001235
Iteration 210/1000 | Loss: 0.00001235
Iteration 211/1000 | Loss: 0.00001235
Iteration 212/1000 | Loss: 0.00001234
Iteration 213/1000 | Loss: 0.00001234
Iteration 214/1000 | Loss: 0.00001231
Iteration 215/1000 | Loss: 0.00001231
Iteration 216/1000 | Loss: 0.00001231
Iteration 217/1000 | Loss: 0.00001230
Iteration 218/1000 | Loss: 0.00001230
Iteration 219/1000 | Loss: 0.00001230
Iteration 220/1000 | Loss: 0.00001230
Iteration 221/1000 | Loss: 0.00001229
Iteration 222/1000 | Loss: 0.00001229
Iteration 223/1000 | Loss: 0.00001229
Iteration 224/1000 | Loss: 0.00001229
Iteration 225/1000 | Loss: 0.00001228
Iteration 226/1000 | Loss: 0.00001228
Iteration 227/1000 | Loss: 0.00001228
Iteration 228/1000 | Loss: 0.00001228
Iteration 229/1000 | Loss: 0.00001227
Iteration 230/1000 | Loss: 0.00001227
Iteration 231/1000 | Loss: 0.00001227
Iteration 232/1000 | Loss: 0.00001227
Iteration 233/1000 | Loss: 0.00001227
Iteration 234/1000 | Loss: 0.00001227
Iteration 235/1000 | Loss: 0.00001226
Iteration 236/1000 | Loss: 0.00001226
Iteration 237/1000 | Loss: 0.00001226
Iteration 238/1000 | Loss: 0.00001226
Iteration 239/1000 | Loss: 0.00001226
Iteration 240/1000 | Loss: 0.00001226
Iteration 241/1000 | Loss: 0.00001226
Iteration 242/1000 | Loss: 0.00001226
Iteration 243/1000 | Loss: 0.00001226
Iteration 244/1000 | Loss: 0.00001225
Iteration 245/1000 | Loss: 0.00001225
Iteration 246/1000 | Loss: 0.00001225
Iteration 247/1000 | Loss: 0.00001225
Iteration 248/1000 | Loss: 0.00001225
Iteration 249/1000 | Loss: 0.00001225
Iteration 250/1000 | Loss: 0.00001225
Iteration 251/1000 | Loss: 0.00001225
Iteration 252/1000 | Loss: 0.00001225
Iteration 253/1000 | Loss: 0.00001224
Iteration 254/1000 | Loss: 0.00001224
Iteration 255/1000 | Loss: 0.00001224
Iteration 256/1000 | Loss: 0.00001224
Iteration 257/1000 | Loss: 0.00001224
Iteration 258/1000 | Loss: 0.00001224
Iteration 259/1000 | Loss: 0.00001223
Iteration 260/1000 | Loss: 0.00001223
Iteration 261/1000 | Loss: 0.00001223
Iteration 262/1000 | Loss: 0.00001223
Iteration 263/1000 | Loss: 0.00001223
Iteration 264/1000 | Loss: 0.00001222
Iteration 265/1000 | Loss: 0.00001222
Iteration 266/1000 | Loss: 0.00001222
Iteration 267/1000 | Loss: 0.00001222
Iteration 268/1000 | Loss: 0.00001222
Iteration 269/1000 | Loss: 0.00001222
Iteration 270/1000 | Loss: 0.00001222
Iteration 271/1000 | Loss: 0.00001222
Iteration 272/1000 | Loss: 0.00001222
Iteration 273/1000 | Loss: 0.00001222
Iteration 274/1000 | Loss: 0.00001222
Iteration 275/1000 | Loss: 0.00001222
Iteration 276/1000 | Loss: 0.00001222
Iteration 277/1000 | Loss: 0.00001222
Iteration 278/1000 | Loss: 0.00001222
Iteration 279/1000 | Loss: 0.00001222
Iteration 280/1000 | Loss: 0.00001222
Iteration 281/1000 | Loss: 0.00001221
Iteration 282/1000 | Loss: 0.00001221
Iteration 283/1000 | Loss: 0.00001221
Iteration 284/1000 | Loss: 0.00001221
Iteration 285/1000 | Loss: 0.00001221
Iteration 286/1000 | Loss: 0.00001221
Iteration 287/1000 | Loss: 0.00001221
Iteration 288/1000 | Loss: 0.00001221
Iteration 289/1000 | Loss: 0.00001221
Iteration 290/1000 | Loss: 0.00001221
Iteration 291/1000 | Loss: 0.00001221
Iteration 292/1000 | Loss: 0.00001221
Iteration 293/1000 | Loss: 0.00001221
Iteration 294/1000 | Loss: 0.00001221
Iteration 295/1000 | Loss: 0.00001221
Iteration 296/1000 | Loss: 0.00001221
Iteration 297/1000 | Loss: 0.00001221
Iteration 298/1000 | Loss: 0.00001221
Iteration 299/1000 | Loss: 0.00001221
Iteration 300/1000 | Loss: 0.00001221
Iteration 301/1000 | Loss: 0.00001221
Iteration 302/1000 | Loss: 0.00001221
Iteration 303/1000 | Loss: 0.00001221
Iteration 304/1000 | Loss: 0.00001221
Iteration 305/1000 | Loss: 0.00001221
Iteration 306/1000 | Loss: 0.00001221
Iteration 307/1000 | Loss: 0.00001221
Iteration 308/1000 | Loss: 0.00001221
Iteration 309/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.2212630281283055e-05, 1.2212630281283055e-05, 1.2212630281283055e-05, 1.2212630281283055e-05, 1.2212630281283055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2212630281283055e-05

Optimization complete. Final v2v error: 2.951968193054199 mm

Highest mean error: 5.282217502593994 mm for frame 1

Lowest mean error: 2.7759382724761963 mm for frame 252

Saving results

Total time: 379.0387136936188
