Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=137, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7672-7727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351269
Iteration 2/25 | Loss: 0.00099087
Iteration 3/25 | Loss: 0.00092933
Iteration 4/25 | Loss: 0.00092125
Iteration 5/25 | Loss: 0.00091887
Iteration 6/25 | Loss: 0.00091800
Iteration 7/25 | Loss: 0.00091800
Iteration 8/25 | Loss: 0.00091800
Iteration 9/25 | Loss: 0.00091800
Iteration 10/25 | Loss: 0.00091800
Iteration 11/25 | Loss: 0.00091800
Iteration 12/25 | Loss: 0.00091800
Iteration 13/25 | Loss: 0.00091800
Iteration 14/25 | Loss: 0.00091800
Iteration 15/25 | Loss: 0.00091800
Iteration 16/25 | Loss: 0.00091800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009179987828247249, 0.0009179987828247249, 0.0009179987828247249, 0.0009179987828247249, 0.0009179987828247249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009179987828247249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29586136
Iteration 2/25 | Loss: 0.00190808
Iteration 3/25 | Loss: 0.00190808
Iteration 4/25 | Loss: 0.00190808
Iteration 5/25 | Loss: 0.00190807
Iteration 6/25 | Loss: 0.00190807
Iteration 7/25 | Loss: 0.00190807
Iteration 8/25 | Loss: 0.00190807
Iteration 9/25 | Loss: 0.00190807
Iteration 10/25 | Loss: 0.00190807
Iteration 11/25 | Loss: 0.00190807
Iteration 12/25 | Loss: 0.00190807
Iteration 13/25 | Loss: 0.00190807
Iteration 14/25 | Loss: 0.00190807
Iteration 15/25 | Loss: 0.00190807
Iteration 16/25 | Loss: 0.00190807
Iteration 17/25 | Loss: 0.00190807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019080726196989417, 0.0019080726196989417, 0.0019080726196989417, 0.0019080726196989417, 0.0019080726196989417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019080726196989417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190807
Iteration 2/1000 | Loss: 0.00001467
Iteration 3/1000 | Loss: 0.00000850
Iteration 4/1000 | Loss: 0.00000754
Iteration 5/1000 | Loss: 0.00000694
Iteration 6/1000 | Loss: 0.00000665
Iteration 7/1000 | Loss: 0.00000628
Iteration 8/1000 | Loss: 0.00000612
Iteration 9/1000 | Loss: 0.00000602
Iteration 10/1000 | Loss: 0.00000600
Iteration 11/1000 | Loss: 0.00000596
Iteration 12/1000 | Loss: 0.00000595
Iteration 13/1000 | Loss: 0.00000594
Iteration 14/1000 | Loss: 0.00000593
Iteration 15/1000 | Loss: 0.00000593
Iteration 16/1000 | Loss: 0.00000592
Iteration 17/1000 | Loss: 0.00000585
Iteration 18/1000 | Loss: 0.00000584
Iteration 19/1000 | Loss: 0.00000582
Iteration 20/1000 | Loss: 0.00000582
Iteration 21/1000 | Loss: 0.00000582
Iteration 22/1000 | Loss: 0.00000581
Iteration 23/1000 | Loss: 0.00000580
Iteration 24/1000 | Loss: 0.00000580
Iteration 25/1000 | Loss: 0.00000579
Iteration 26/1000 | Loss: 0.00000579
Iteration 27/1000 | Loss: 0.00000578
Iteration 28/1000 | Loss: 0.00000578
Iteration 29/1000 | Loss: 0.00000577
Iteration 30/1000 | Loss: 0.00000577
Iteration 31/1000 | Loss: 0.00000576
Iteration 32/1000 | Loss: 0.00000575
Iteration 33/1000 | Loss: 0.00000574
Iteration 34/1000 | Loss: 0.00000574
Iteration 35/1000 | Loss: 0.00000573
Iteration 36/1000 | Loss: 0.00000572
Iteration 37/1000 | Loss: 0.00000572
Iteration 38/1000 | Loss: 0.00000572
Iteration 39/1000 | Loss: 0.00000571
Iteration 40/1000 | Loss: 0.00000571
Iteration 41/1000 | Loss: 0.00000570
Iteration 42/1000 | Loss: 0.00000570
Iteration 43/1000 | Loss: 0.00000570
Iteration 44/1000 | Loss: 0.00000569
Iteration 45/1000 | Loss: 0.00000569
Iteration 46/1000 | Loss: 0.00000568
Iteration 47/1000 | Loss: 0.00000567
Iteration 48/1000 | Loss: 0.00000565
Iteration 49/1000 | Loss: 0.00000564
Iteration 50/1000 | Loss: 0.00000564
Iteration 51/1000 | Loss: 0.00000563
Iteration 52/1000 | Loss: 0.00000563
Iteration 53/1000 | Loss: 0.00000562
Iteration 54/1000 | Loss: 0.00000561
Iteration 55/1000 | Loss: 0.00000561
Iteration 56/1000 | Loss: 0.00000561
Iteration 57/1000 | Loss: 0.00000561
Iteration 58/1000 | Loss: 0.00000560
Iteration 59/1000 | Loss: 0.00000560
Iteration 60/1000 | Loss: 0.00000560
Iteration 61/1000 | Loss: 0.00000560
Iteration 62/1000 | Loss: 0.00000560
Iteration 63/1000 | Loss: 0.00000559
Iteration 64/1000 | Loss: 0.00000559
Iteration 65/1000 | Loss: 0.00000558
Iteration 66/1000 | Loss: 0.00000558
Iteration 67/1000 | Loss: 0.00000558
Iteration 68/1000 | Loss: 0.00000558
Iteration 69/1000 | Loss: 0.00000558
Iteration 70/1000 | Loss: 0.00000557
Iteration 71/1000 | Loss: 0.00000557
Iteration 72/1000 | Loss: 0.00000557
Iteration 73/1000 | Loss: 0.00000556
Iteration 74/1000 | Loss: 0.00000556
Iteration 75/1000 | Loss: 0.00000556
Iteration 76/1000 | Loss: 0.00000556
Iteration 77/1000 | Loss: 0.00000556
Iteration 78/1000 | Loss: 0.00000556
Iteration 79/1000 | Loss: 0.00000556
Iteration 80/1000 | Loss: 0.00000555
Iteration 81/1000 | Loss: 0.00000555
Iteration 82/1000 | Loss: 0.00000555
Iteration 83/1000 | Loss: 0.00000555
Iteration 84/1000 | Loss: 0.00000555
Iteration 85/1000 | Loss: 0.00000555
Iteration 86/1000 | Loss: 0.00000555
Iteration 87/1000 | Loss: 0.00000554
Iteration 88/1000 | Loss: 0.00000554
Iteration 89/1000 | Loss: 0.00000554
Iteration 90/1000 | Loss: 0.00000554
Iteration 91/1000 | Loss: 0.00000554
Iteration 92/1000 | Loss: 0.00000554
Iteration 93/1000 | Loss: 0.00000554
Iteration 94/1000 | Loss: 0.00000554
Iteration 95/1000 | Loss: 0.00000554
Iteration 96/1000 | Loss: 0.00000553
Iteration 97/1000 | Loss: 0.00000552
Iteration 98/1000 | Loss: 0.00000551
Iteration 99/1000 | Loss: 0.00000551
Iteration 100/1000 | Loss: 0.00000550
Iteration 101/1000 | Loss: 0.00000550
Iteration 102/1000 | Loss: 0.00000550
Iteration 103/1000 | Loss: 0.00000550
Iteration 104/1000 | Loss: 0.00000550
Iteration 105/1000 | Loss: 0.00000550
Iteration 106/1000 | Loss: 0.00000550
Iteration 107/1000 | Loss: 0.00000550
Iteration 108/1000 | Loss: 0.00000550
Iteration 109/1000 | Loss: 0.00000550
Iteration 110/1000 | Loss: 0.00000550
Iteration 111/1000 | Loss: 0.00000549
Iteration 112/1000 | Loss: 0.00000549
Iteration 113/1000 | Loss: 0.00000549
Iteration 114/1000 | Loss: 0.00000549
Iteration 115/1000 | Loss: 0.00000549
Iteration 116/1000 | Loss: 0.00000549
Iteration 117/1000 | Loss: 0.00000549
Iteration 118/1000 | Loss: 0.00000549
Iteration 119/1000 | Loss: 0.00000549
Iteration 120/1000 | Loss: 0.00000549
Iteration 121/1000 | Loss: 0.00000549
Iteration 122/1000 | Loss: 0.00000549
Iteration 123/1000 | Loss: 0.00000549
Iteration 124/1000 | Loss: 0.00000549
Iteration 125/1000 | Loss: 0.00000549
Iteration 126/1000 | Loss: 0.00000549
Iteration 127/1000 | Loss: 0.00000549
Iteration 128/1000 | Loss: 0.00000549
Iteration 129/1000 | Loss: 0.00000549
Iteration 130/1000 | Loss: 0.00000549
Iteration 131/1000 | Loss: 0.00000549
Iteration 132/1000 | Loss: 0.00000549
Iteration 133/1000 | Loss: 0.00000549
Iteration 134/1000 | Loss: 0.00000549
Iteration 135/1000 | Loss: 0.00000549
Iteration 136/1000 | Loss: 0.00000549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [5.485708243213594e-06, 5.485708243213594e-06, 5.485708243213594e-06, 5.485708243213594e-06, 5.485708243213594e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.485708243213594e-06

Optimization complete. Final v2v error: 2.01206111907959 mm

Highest mean error: 2.3136789798736572 mm for frame 132

Lowest mean error: 1.9468209743499756 mm for frame 144

Saving results

Total time: 34.226463079452515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067463
Iteration 2/25 | Loss: 0.01067463
Iteration 3/25 | Loss: 0.01067463
Iteration 4/25 | Loss: 0.01067463
Iteration 5/25 | Loss: 0.01067462
Iteration 6/25 | Loss: 0.01067462
Iteration 7/25 | Loss: 0.01067462
Iteration 8/25 | Loss: 0.01067462
Iteration 9/25 | Loss: 0.01067462
Iteration 10/25 | Loss: 0.01067462
Iteration 11/25 | Loss: 0.01067462
Iteration 12/25 | Loss: 0.01067462
Iteration 13/25 | Loss: 0.01067461
Iteration 14/25 | Loss: 0.01067461
Iteration 15/25 | Loss: 0.01067461
Iteration 16/25 | Loss: 0.01067461
Iteration 17/25 | Loss: 0.01067461
Iteration 18/25 | Loss: 0.01067461
Iteration 19/25 | Loss: 0.01067461
Iteration 20/25 | Loss: 0.01067461
Iteration 21/25 | Loss: 0.01067461
Iteration 22/25 | Loss: 0.01067460
Iteration 23/25 | Loss: 0.01067460
Iteration 24/25 | Loss: 0.01067460
Iteration 25/25 | Loss: 0.01067460

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68911004
Iteration 2/25 | Loss: 0.09290421
Iteration 3/25 | Loss: 0.08815567
Iteration 4/25 | Loss: 0.08811776
Iteration 5/25 | Loss: 0.08811774
Iteration 6/25 | Loss: 0.08811773
Iteration 7/25 | Loss: 0.08811773
Iteration 8/25 | Loss: 0.08811773
Iteration 9/25 | Loss: 0.08811771
Iteration 10/25 | Loss: 0.08811771
Iteration 11/25 | Loss: 0.08811771
Iteration 12/25 | Loss: 0.08811770
Iteration 13/25 | Loss: 0.08811770
Iteration 14/25 | Loss: 0.08811770
Iteration 15/25 | Loss: 0.08811770
Iteration 16/25 | Loss: 0.08811770
Iteration 17/25 | Loss: 0.08811770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.08811770379543304, 0.08811770379543304, 0.08811770379543304, 0.08811770379543304, 0.08811770379543304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08811770379543304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08811770
Iteration 2/1000 | Loss: 0.00305764
Iteration 3/1000 | Loss: 0.00217267
Iteration 4/1000 | Loss: 0.00037039
Iteration 5/1000 | Loss: 0.00091045
Iteration 6/1000 | Loss: 0.00209135
Iteration 7/1000 | Loss: 0.00070751
Iteration 8/1000 | Loss: 0.00007871
Iteration 9/1000 | Loss: 0.00016424
Iteration 10/1000 | Loss: 0.00010809
Iteration 11/1000 | Loss: 0.00030410
Iteration 12/1000 | Loss: 0.00003820
Iteration 13/1000 | Loss: 0.00003245
Iteration 14/1000 | Loss: 0.00006640
Iteration 15/1000 | Loss: 0.00023963
Iteration 16/1000 | Loss: 0.00003782
Iteration 17/1000 | Loss: 0.00004696
Iteration 18/1000 | Loss: 0.00002362
Iteration 19/1000 | Loss: 0.00002246
Iteration 20/1000 | Loss: 0.00002104
Iteration 21/1000 | Loss: 0.00008918
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00007302
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00003589
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00005063
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001577
Iteration 32/1000 | Loss: 0.00005129
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00003906
Iteration 37/1000 | Loss: 0.00002699
Iteration 38/1000 | Loss: 0.00003690
Iteration 39/1000 | Loss: 0.00003708
Iteration 40/1000 | Loss: 0.00003039
Iteration 41/1000 | Loss: 0.00002054
Iteration 42/1000 | Loss: 0.00003243
Iteration 43/1000 | Loss: 0.00004992
Iteration 44/1000 | Loss: 0.00003224
Iteration 45/1000 | Loss: 0.00002599
Iteration 46/1000 | Loss: 0.00002018
Iteration 47/1000 | Loss: 0.00002553
Iteration 48/1000 | Loss: 0.00003062
Iteration 49/1000 | Loss: 0.00002867
Iteration 50/1000 | Loss: 0.00002998
Iteration 51/1000 | Loss: 0.00002906
Iteration 52/1000 | Loss: 0.00002966
Iteration 53/1000 | Loss: 0.00010676
Iteration 54/1000 | Loss: 0.00002728
Iteration 55/1000 | Loss: 0.00004992
Iteration 56/1000 | Loss: 0.00003006
Iteration 57/1000 | Loss: 0.00002829
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00003581
Iteration 60/1000 | Loss: 0.00004311
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001418
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001390
Iteration 66/1000 | Loss: 0.00001390
Iteration 67/1000 | Loss: 0.00001390
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001389
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001380
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00004398
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001369
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001369
Iteration 120/1000 | Loss: 0.00001369
Iteration 121/1000 | Loss: 0.00001369
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001368
Iteration 125/1000 | Loss: 0.00001368
Iteration 126/1000 | Loss: 0.00001368
Iteration 127/1000 | Loss: 0.00001368
Iteration 128/1000 | Loss: 0.00001368
Iteration 129/1000 | Loss: 0.00001368
Iteration 130/1000 | Loss: 0.00001368
Iteration 131/1000 | Loss: 0.00001368
Iteration 132/1000 | Loss: 0.00001368
Iteration 133/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.3683527868124656e-05, 1.3683527868124656e-05, 1.3683527868124656e-05, 1.3683527868124656e-05, 1.3683527868124656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3683527868124656e-05

Optimization complete. Final v2v error: 3.0466039180755615 mm

Highest mean error: 10.391223907470703 mm for frame 176

Lowest mean error: 2.6631217002868652 mm for frame 105

Saving results

Total time: 120.05300378799438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372957
Iteration 2/25 | Loss: 0.00116972
Iteration 3/25 | Loss: 0.00102720
Iteration 4/25 | Loss: 0.00101360
Iteration 5/25 | Loss: 0.00100992
Iteration 6/25 | Loss: 0.00100860
Iteration 7/25 | Loss: 0.00100847
Iteration 8/25 | Loss: 0.00100847
Iteration 9/25 | Loss: 0.00100847
Iteration 10/25 | Loss: 0.00100847
Iteration 11/25 | Loss: 0.00100847
Iteration 12/25 | Loss: 0.00100847
Iteration 13/25 | Loss: 0.00100847
Iteration 14/25 | Loss: 0.00100847
Iteration 15/25 | Loss: 0.00100847
Iteration 16/25 | Loss: 0.00100847
Iteration 17/25 | Loss: 0.00100847
Iteration 18/25 | Loss: 0.00100847
Iteration 19/25 | Loss: 0.00100847
Iteration 20/25 | Loss: 0.00100847
Iteration 21/25 | Loss: 0.00100847
Iteration 22/25 | Loss: 0.00100847
Iteration 23/25 | Loss: 0.00100847
Iteration 24/25 | Loss: 0.00100847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010084658861160278, 0.0010084658861160278, 0.0010084658861160278, 0.0010084658861160278, 0.0010084658861160278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010084658861160278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30449271
Iteration 2/25 | Loss: 0.00234333
Iteration 3/25 | Loss: 0.00234332
Iteration 4/25 | Loss: 0.00234332
Iteration 5/25 | Loss: 0.00234332
Iteration 6/25 | Loss: 0.00234332
Iteration 7/25 | Loss: 0.00234332
Iteration 8/25 | Loss: 0.00234332
Iteration 9/25 | Loss: 0.00234332
Iteration 10/25 | Loss: 0.00234332
Iteration 11/25 | Loss: 0.00234332
Iteration 12/25 | Loss: 0.00234332
Iteration 13/25 | Loss: 0.00234332
Iteration 14/25 | Loss: 0.00234332
Iteration 15/25 | Loss: 0.00234332
Iteration 16/25 | Loss: 0.00234332
Iteration 17/25 | Loss: 0.00234332
Iteration 18/25 | Loss: 0.00234332
Iteration 19/25 | Loss: 0.00234332
Iteration 20/25 | Loss: 0.00234332
Iteration 21/25 | Loss: 0.00234332
Iteration 22/25 | Loss: 0.00234332
Iteration 23/25 | Loss: 0.00234332
Iteration 24/25 | Loss: 0.00234332
Iteration 25/25 | Loss: 0.00234332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234332
Iteration 2/1000 | Loss: 0.00003782
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001625
Iteration 6/1000 | Loss: 0.00001542
Iteration 7/1000 | Loss: 0.00001517
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001466
Iteration 10/1000 | Loss: 0.00001440
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001420
Iteration 15/1000 | Loss: 0.00001418
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001416
Iteration 18/1000 | Loss: 0.00001412
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001412
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001404
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001402
Iteration 56/1000 | Loss: 0.00001402
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001401
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001393
Iteration 81/1000 | Loss: 0.00001393
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001393
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001387
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001386
Iteration 105/1000 | Loss: 0.00001386
Iteration 106/1000 | Loss: 0.00001386
Iteration 107/1000 | Loss: 0.00001386
Iteration 108/1000 | Loss: 0.00001386
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001384
Iteration 115/1000 | Loss: 0.00001384
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001380
Iteration 126/1000 | Loss: 0.00001380
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001376
Iteration 129/1000 | Loss: 0.00001376
Iteration 130/1000 | Loss: 0.00001376
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001376
Iteration 135/1000 | Loss: 0.00001376
Iteration 136/1000 | Loss: 0.00001376
Iteration 137/1000 | Loss: 0.00001376
Iteration 138/1000 | Loss: 0.00001376
Iteration 139/1000 | Loss: 0.00001375
Iteration 140/1000 | Loss: 0.00001375
Iteration 141/1000 | Loss: 0.00001375
Iteration 142/1000 | Loss: 0.00001374
Iteration 143/1000 | Loss: 0.00001374
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001372
Iteration 148/1000 | Loss: 0.00001372
Iteration 149/1000 | Loss: 0.00001372
Iteration 150/1000 | Loss: 0.00001371
Iteration 151/1000 | Loss: 0.00001371
Iteration 152/1000 | Loss: 0.00001371
Iteration 153/1000 | Loss: 0.00001371
Iteration 154/1000 | Loss: 0.00001371
Iteration 155/1000 | Loss: 0.00001371
Iteration 156/1000 | Loss: 0.00001370
Iteration 157/1000 | Loss: 0.00001370
Iteration 158/1000 | Loss: 0.00001369
Iteration 159/1000 | Loss: 0.00001369
Iteration 160/1000 | Loss: 0.00001369
Iteration 161/1000 | Loss: 0.00001369
Iteration 162/1000 | Loss: 0.00001369
Iteration 163/1000 | Loss: 0.00001369
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001368
Iteration 167/1000 | Loss: 0.00001368
Iteration 168/1000 | Loss: 0.00001368
Iteration 169/1000 | Loss: 0.00001368
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001367
Iteration 172/1000 | Loss: 0.00001367
Iteration 173/1000 | Loss: 0.00001367
Iteration 174/1000 | Loss: 0.00001367
Iteration 175/1000 | Loss: 0.00001367
Iteration 176/1000 | Loss: 0.00001366
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Iteration 179/1000 | Loss: 0.00001366
Iteration 180/1000 | Loss: 0.00001366
Iteration 181/1000 | Loss: 0.00001366
Iteration 182/1000 | Loss: 0.00001366
Iteration 183/1000 | Loss: 0.00001366
Iteration 184/1000 | Loss: 0.00001365
Iteration 185/1000 | Loss: 0.00001365
Iteration 186/1000 | Loss: 0.00001365
Iteration 187/1000 | Loss: 0.00001365
Iteration 188/1000 | Loss: 0.00001365
Iteration 189/1000 | Loss: 0.00001365
Iteration 190/1000 | Loss: 0.00001365
Iteration 191/1000 | Loss: 0.00001364
Iteration 192/1000 | Loss: 0.00001364
Iteration 193/1000 | Loss: 0.00001364
Iteration 194/1000 | Loss: 0.00001364
Iteration 195/1000 | Loss: 0.00001364
Iteration 196/1000 | Loss: 0.00001364
Iteration 197/1000 | Loss: 0.00001364
Iteration 198/1000 | Loss: 0.00001364
Iteration 199/1000 | Loss: 0.00001364
Iteration 200/1000 | Loss: 0.00001363
Iteration 201/1000 | Loss: 0.00001363
Iteration 202/1000 | Loss: 0.00001363
Iteration 203/1000 | Loss: 0.00001363
Iteration 204/1000 | Loss: 0.00001363
Iteration 205/1000 | Loss: 0.00001363
Iteration 206/1000 | Loss: 0.00001363
Iteration 207/1000 | Loss: 0.00001363
Iteration 208/1000 | Loss: 0.00001363
Iteration 209/1000 | Loss: 0.00001362
Iteration 210/1000 | Loss: 0.00001362
Iteration 211/1000 | Loss: 0.00001362
Iteration 212/1000 | Loss: 0.00001362
Iteration 213/1000 | Loss: 0.00001362
Iteration 214/1000 | Loss: 0.00001362
Iteration 215/1000 | Loss: 0.00001362
Iteration 216/1000 | Loss: 0.00001362
Iteration 217/1000 | Loss: 0.00001361
Iteration 218/1000 | Loss: 0.00001361
Iteration 219/1000 | Loss: 0.00001361
Iteration 220/1000 | Loss: 0.00001361
Iteration 221/1000 | Loss: 0.00001360
Iteration 222/1000 | Loss: 0.00001360
Iteration 223/1000 | Loss: 0.00001360
Iteration 224/1000 | Loss: 0.00001360
Iteration 225/1000 | Loss: 0.00001360
Iteration 226/1000 | Loss: 0.00001360
Iteration 227/1000 | Loss: 0.00001360
Iteration 228/1000 | Loss: 0.00001360
Iteration 229/1000 | Loss: 0.00001360
Iteration 230/1000 | Loss: 0.00001360
Iteration 231/1000 | Loss: 0.00001360
Iteration 232/1000 | Loss: 0.00001360
Iteration 233/1000 | Loss: 0.00001359
Iteration 234/1000 | Loss: 0.00001359
Iteration 235/1000 | Loss: 0.00001359
Iteration 236/1000 | Loss: 0.00001359
Iteration 237/1000 | Loss: 0.00001359
Iteration 238/1000 | Loss: 0.00001359
Iteration 239/1000 | Loss: 0.00001359
Iteration 240/1000 | Loss: 0.00001359
Iteration 241/1000 | Loss: 0.00001359
Iteration 242/1000 | Loss: 0.00001359
Iteration 243/1000 | Loss: 0.00001359
Iteration 244/1000 | Loss: 0.00001359
Iteration 245/1000 | Loss: 0.00001359
Iteration 246/1000 | Loss: 0.00001359
Iteration 247/1000 | Loss: 0.00001359
Iteration 248/1000 | Loss: 0.00001359
Iteration 249/1000 | Loss: 0.00001358
Iteration 250/1000 | Loss: 0.00001358
Iteration 251/1000 | Loss: 0.00001358
Iteration 252/1000 | Loss: 0.00001358
Iteration 253/1000 | Loss: 0.00001358
Iteration 254/1000 | Loss: 0.00001358
Iteration 255/1000 | Loss: 0.00001358
Iteration 256/1000 | Loss: 0.00001358
Iteration 257/1000 | Loss: 0.00001357
Iteration 258/1000 | Loss: 0.00001357
Iteration 259/1000 | Loss: 0.00001357
Iteration 260/1000 | Loss: 0.00001357
Iteration 261/1000 | Loss: 0.00001357
Iteration 262/1000 | Loss: 0.00001357
Iteration 263/1000 | Loss: 0.00001357
Iteration 264/1000 | Loss: 0.00001357
Iteration 265/1000 | Loss: 0.00001357
Iteration 266/1000 | Loss: 0.00001357
Iteration 267/1000 | Loss: 0.00001357
Iteration 268/1000 | Loss: 0.00001357
Iteration 269/1000 | Loss: 0.00001357
Iteration 270/1000 | Loss: 0.00001357
Iteration 271/1000 | Loss: 0.00001356
Iteration 272/1000 | Loss: 0.00001356
Iteration 273/1000 | Loss: 0.00001356
Iteration 274/1000 | Loss: 0.00001356
Iteration 275/1000 | Loss: 0.00001356
Iteration 276/1000 | Loss: 0.00001356
Iteration 277/1000 | Loss: 0.00001356
Iteration 278/1000 | Loss: 0.00001356
Iteration 279/1000 | Loss: 0.00001356
Iteration 280/1000 | Loss: 0.00001356
Iteration 281/1000 | Loss: 0.00001356
Iteration 282/1000 | Loss: 0.00001356
Iteration 283/1000 | Loss: 0.00001356
Iteration 284/1000 | Loss: 0.00001356
Iteration 285/1000 | Loss: 0.00001356
Iteration 286/1000 | Loss: 0.00001356
Iteration 287/1000 | Loss: 0.00001356
Iteration 288/1000 | Loss: 0.00001356
Iteration 289/1000 | Loss: 0.00001356
Iteration 290/1000 | Loss: 0.00001356
Iteration 291/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.3555798432207666e-05, 1.3555798432207666e-05, 1.3555798432207666e-05, 1.3555798432207666e-05, 1.3555798432207666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3555798432207666e-05

Optimization complete. Final v2v error: 2.9357035160064697 mm

Highest mean error: 3.3443634510040283 mm for frame 39

Lowest mean error: 2.298274278640747 mm for frame 6

Saving results

Total time: 47.644450664520264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010175
Iteration 2/25 | Loss: 0.01010175
Iteration 3/25 | Loss: 0.01010175
Iteration 4/25 | Loss: 0.01010174
Iteration 5/25 | Loss: 0.01010174
Iteration 6/25 | Loss: 0.01010174
Iteration 7/25 | Loss: 0.01010174
Iteration 8/25 | Loss: 0.01010174
Iteration 9/25 | Loss: 0.01010174
Iteration 10/25 | Loss: 0.01010173
Iteration 11/25 | Loss: 0.01010173
Iteration 12/25 | Loss: 0.01010173
Iteration 13/25 | Loss: 0.01010173
Iteration 14/25 | Loss: 0.00224174
Iteration 15/25 | Loss: 0.00192955
Iteration 16/25 | Loss: 0.00139094
Iteration 17/25 | Loss: 0.00121896
Iteration 18/25 | Loss: 0.00117057
Iteration 19/25 | Loss: 0.00115659
Iteration 20/25 | Loss: 0.00113995
Iteration 21/25 | Loss: 0.00111954
Iteration 22/25 | Loss: 0.00111455
Iteration 23/25 | Loss: 0.00111397
Iteration 24/25 | Loss: 0.00112450
Iteration 25/25 | Loss: 0.00110983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19041932
Iteration 2/25 | Loss: 0.00173393
Iteration 3/25 | Loss: 0.00173393
Iteration 4/25 | Loss: 0.00173392
Iteration 5/25 | Loss: 0.00173392
Iteration 6/25 | Loss: 0.00173392
Iteration 7/25 | Loss: 0.00173392
Iteration 8/25 | Loss: 0.00173392
Iteration 9/25 | Loss: 0.00173392
Iteration 10/25 | Loss: 0.00173392
Iteration 11/25 | Loss: 0.00173392
Iteration 12/25 | Loss: 0.00173392
Iteration 13/25 | Loss: 0.00173392
Iteration 14/25 | Loss: 0.00173392
Iteration 15/25 | Loss: 0.00173392
Iteration 16/25 | Loss: 0.00173392
Iteration 17/25 | Loss: 0.00173392
Iteration 18/25 | Loss: 0.00173392
Iteration 19/25 | Loss: 0.00173392
Iteration 20/25 | Loss: 0.00173392
Iteration 21/25 | Loss: 0.00173392
Iteration 22/25 | Loss: 0.00173392
Iteration 23/25 | Loss: 0.00173392
Iteration 24/25 | Loss: 0.00173392
Iteration 25/25 | Loss: 0.00173392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173392
Iteration 2/1000 | Loss: 0.00007512
Iteration 3/1000 | Loss: 0.00005382
Iteration 4/1000 | Loss: 0.00004649
Iteration 5/1000 | Loss: 0.00004311
Iteration 6/1000 | Loss: 0.00028785
Iteration 7/1000 | Loss: 0.00029099
Iteration 8/1000 | Loss: 0.00026573
Iteration 9/1000 | Loss: 0.00040330
Iteration 10/1000 | Loss: 0.00024728
Iteration 11/1000 | Loss: 0.00027147
Iteration 12/1000 | Loss: 0.00021856
Iteration 13/1000 | Loss: 0.00006813
Iteration 14/1000 | Loss: 0.00009815
Iteration 15/1000 | Loss: 0.00073176
Iteration 16/1000 | Loss: 0.00034296
Iteration 17/1000 | Loss: 0.00010689
Iteration 18/1000 | Loss: 0.00005256
Iteration 19/1000 | Loss: 0.00003691
Iteration 20/1000 | Loss: 0.00002662
Iteration 21/1000 | Loss: 0.00002136
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001643
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001369
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001366
Iteration 46/1000 | Loss: 0.00001366
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001365
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001364
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001363
Iteration 61/1000 | Loss: 0.00001363
Iteration 62/1000 | Loss: 0.00001363
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001362
Iteration 70/1000 | Loss: 0.00001362
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001359
Iteration 96/1000 | Loss: 0.00001359
Iteration 97/1000 | Loss: 0.00001359
Iteration 98/1000 | Loss: 0.00001359
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001358
Iteration 102/1000 | Loss: 0.00001358
Iteration 103/1000 | Loss: 0.00001358
Iteration 104/1000 | Loss: 0.00001358
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.3580385711975396e-05, 1.3580385711975396e-05, 1.3580385711975396e-05, 1.3580385711975396e-05, 1.3580385711975396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3580385711975396e-05

Optimization complete. Final v2v error: 3.1224422454833984 mm

Highest mean error: 3.561938762664795 mm for frame 0

Lowest mean error: 2.856043815612793 mm for frame 55

Saving results

Total time: 88.17586088180542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003109
Iteration 2/25 | Loss: 0.00229498
Iteration 3/25 | Loss: 0.00186016
Iteration 4/25 | Loss: 0.00176354
Iteration 5/25 | Loss: 0.00173039
Iteration 6/25 | Loss: 0.00159638
Iteration 7/25 | Loss: 0.00145050
Iteration 8/25 | Loss: 0.00140119
Iteration 9/25 | Loss: 0.00125412
Iteration 10/25 | Loss: 0.00124303
Iteration 11/25 | Loss: 0.00120584
Iteration 12/25 | Loss: 0.00120227
Iteration 13/25 | Loss: 0.00118017
Iteration 14/25 | Loss: 0.00116326
Iteration 15/25 | Loss: 0.00116004
Iteration 16/25 | Loss: 0.00115756
Iteration 17/25 | Loss: 0.00116399
Iteration 18/25 | Loss: 0.00115451
Iteration 19/25 | Loss: 0.00115324
Iteration 20/25 | Loss: 0.00115308
Iteration 21/25 | Loss: 0.00115308
Iteration 22/25 | Loss: 0.00115308
Iteration 23/25 | Loss: 0.00115308
Iteration 24/25 | Loss: 0.00115308
Iteration 25/25 | Loss: 0.00115308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19169843
Iteration 2/25 | Loss: 0.00261833
Iteration 3/25 | Loss: 0.00261833
Iteration 4/25 | Loss: 0.00261833
Iteration 5/25 | Loss: 0.00261833
Iteration 6/25 | Loss: 0.00261832
Iteration 7/25 | Loss: 0.00261832
Iteration 8/25 | Loss: 0.00261832
Iteration 9/25 | Loss: 0.00261832
Iteration 10/25 | Loss: 0.00261832
Iteration 11/25 | Loss: 0.00261832
Iteration 12/25 | Loss: 0.00261832
Iteration 13/25 | Loss: 0.00261832
Iteration 14/25 | Loss: 0.00261832
Iteration 15/25 | Loss: 0.00261832
Iteration 16/25 | Loss: 0.00261832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026183235459029675, 0.0026183235459029675, 0.0026183235459029675, 0.0026183235459029675, 0.0026183235459029675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026183235459029675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261832
Iteration 2/1000 | Loss: 0.00019546
Iteration 3/1000 | Loss: 0.00013530
Iteration 4/1000 | Loss: 0.00011227
Iteration 5/1000 | Loss: 0.00010066
Iteration 6/1000 | Loss: 0.00009420
Iteration 7/1000 | Loss: 0.00008954
Iteration 8/1000 | Loss: 0.00033893
Iteration 9/1000 | Loss: 0.00009155
Iteration 10/1000 | Loss: 0.00393374
Iteration 11/1000 | Loss: 0.00080905
Iteration 12/1000 | Loss: 0.00015100
Iteration 13/1000 | Loss: 0.00007506
Iteration 14/1000 | Loss: 0.00005227
Iteration 15/1000 | Loss: 0.00004037
Iteration 16/1000 | Loss: 0.00003005
Iteration 17/1000 | Loss: 0.00002411
Iteration 18/1000 | Loss: 0.00002014
Iteration 19/1000 | Loss: 0.00001744
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001001
Iteration 24/1000 | Loss: 0.00000947
Iteration 25/1000 | Loss: 0.00000886
Iteration 26/1000 | Loss: 0.00000847
Iteration 27/1000 | Loss: 0.00000822
Iteration 28/1000 | Loss: 0.00000803
Iteration 29/1000 | Loss: 0.00000790
Iteration 30/1000 | Loss: 0.00000787
Iteration 31/1000 | Loss: 0.00000785
Iteration 32/1000 | Loss: 0.00000785
Iteration 33/1000 | Loss: 0.00000784
Iteration 34/1000 | Loss: 0.00000783
Iteration 35/1000 | Loss: 0.00000779
Iteration 36/1000 | Loss: 0.00000778
Iteration 37/1000 | Loss: 0.00000776
Iteration 38/1000 | Loss: 0.00000775
Iteration 39/1000 | Loss: 0.00000771
Iteration 40/1000 | Loss: 0.00000771
Iteration 41/1000 | Loss: 0.00000771
Iteration 42/1000 | Loss: 0.00000771
Iteration 43/1000 | Loss: 0.00000771
Iteration 44/1000 | Loss: 0.00000770
Iteration 45/1000 | Loss: 0.00000770
Iteration 46/1000 | Loss: 0.00000770
Iteration 47/1000 | Loss: 0.00000770
Iteration 48/1000 | Loss: 0.00000769
Iteration 49/1000 | Loss: 0.00000769
Iteration 50/1000 | Loss: 0.00000768
Iteration 51/1000 | Loss: 0.00000768
Iteration 52/1000 | Loss: 0.00000767
Iteration 53/1000 | Loss: 0.00000767
Iteration 54/1000 | Loss: 0.00000766
Iteration 55/1000 | Loss: 0.00000766
Iteration 56/1000 | Loss: 0.00000765
Iteration 57/1000 | Loss: 0.00000765
Iteration 58/1000 | Loss: 0.00000765
Iteration 59/1000 | Loss: 0.00000765
Iteration 60/1000 | Loss: 0.00000764
Iteration 61/1000 | Loss: 0.00000764
Iteration 62/1000 | Loss: 0.00000763
Iteration 63/1000 | Loss: 0.00000763
Iteration 64/1000 | Loss: 0.00000762
Iteration 65/1000 | Loss: 0.00000762
Iteration 66/1000 | Loss: 0.00000761
Iteration 67/1000 | Loss: 0.00000761
Iteration 68/1000 | Loss: 0.00000761
Iteration 69/1000 | Loss: 0.00000761
Iteration 70/1000 | Loss: 0.00000761
Iteration 71/1000 | Loss: 0.00000761
Iteration 72/1000 | Loss: 0.00000760
Iteration 73/1000 | Loss: 0.00000760
Iteration 74/1000 | Loss: 0.00000760
Iteration 75/1000 | Loss: 0.00000760
Iteration 76/1000 | Loss: 0.00000760
Iteration 77/1000 | Loss: 0.00000759
Iteration 78/1000 | Loss: 0.00000759
Iteration 79/1000 | Loss: 0.00000759
Iteration 80/1000 | Loss: 0.00000759
Iteration 81/1000 | Loss: 0.00000759
Iteration 82/1000 | Loss: 0.00000759
Iteration 83/1000 | Loss: 0.00000759
Iteration 84/1000 | Loss: 0.00000759
Iteration 85/1000 | Loss: 0.00000759
Iteration 86/1000 | Loss: 0.00000759
Iteration 87/1000 | Loss: 0.00000759
Iteration 88/1000 | Loss: 0.00000758
Iteration 89/1000 | Loss: 0.00000758
Iteration 90/1000 | Loss: 0.00000758
Iteration 91/1000 | Loss: 0.00000758
Iteration 92/1000 | Loss: 0.00000758
Iteration 93/1000 | Loss: 0.00000758
Iteration 94/1000 | Loss: 0.00000757
Iteration 95/1000 | Loss: 0.00000757
Iteration 96/1000 | Loss: 0.00000757
Iteration 97/1000 | Loss: 0.00000757
Iteration 98/1000 | Loss: 0.00000757
Iteration 99/1000 | Loss: 0.00000757
Iteration 100/1000 | Loss: 0.00000757
Iteration 101/1000 | Loss: 0.00000757
Iteration 102/1000 | Loss: 0.00000757
Iteration 103/1000 | Loss: 0.00000757
Iteration 104/1000 | Loss: 0.00000757
Iteration 105/1000 | Loss: 0.00000757
Iteration 106/1000 | Loss: 0.00000757
Iteration 107/1000 | Loss: 0.00000757
Iteration 108/1000 | Loss: 0.00000756
Iteration 109/1000 | Loss: 0.00000756
Iteration 110/1000 | Loss: 0.00000756
Iteration 111/1000 | Loss: 0.00000756
Iteration 112/1000 | Loss: 0.00000756
Iteration 113/1000 | Loss: 0.00000755
Iteration 114/1000 | Loss: 0.00000755
Iteration 115/1000 | Loss: 0.00000755
Iteration 116/1000 | Loss: 0.00000755
Iteration 117/1000 | Loss: 0.00000755
Iteration 118/1000 | Loss: 0.00000755
Iteration 119/1000 | Loss: 0.00000755
Iteration 120/1000 | Loss: 0.00000755
Iteration 121/1000 | Loss: 0.00000755
Iteration 122/1000 | Loss: 0.00000755
Iteration 123/1000 | Loss: 0.00000755
Iteration 124/1000 | Loss: 0.00000754
Iteration 125/1000 | Loss: 0.00000754
Iteration 126/1000 | Loss: 0.00000754
Iteration 127/1000 | Loss: 0.00000754
Iteration 128/1000 | Loss: 0.00000754
Iteration 129/1000 | Loss: 0.00000753
Iteration 130/1000 | Loss: 0.00000753
Iteration 131/1000 | Loss: 0.00000753
Iteration 132/1000 | Loss: 0.00000753
Iteration 133/1000 | Loss: 0.00000753
Iteration 134/1000 | Loss: 0.00000753
Iteration 135/1000 | Loss: 0.00000753
Iteration 136/1000 | Loss: 0.00000753
Iteration 137/1000 | Loss: 0.00000753
Iteration 138/1000 | Loss: 0.00000753
Iteration 139/1000 | Loss: 0.00000753
Iteration 140/1000 | Loss: 0.00000753
Iteration 141/1000 | Loss: 0.00000753
Iteration 142/1000 | Loss: 0.00000753
Iteration 143/1000 | Loss: 0.00000753
Iteration 144/1000 | Loss: 0.00000753
Iteration 145/1000 | Loss: 0.00000753
Iteration 146/1000 | Loss: 0.00000753
Iteration 147/1000 | Loss: 0.00000753
Iteration 148/1000 | Loss: 0.00000752
Iteration 149/1000 | Loss: 0.00000752
Iteration 150/1000 | Loss: 0.00000752
Iteration 151/1000 | Loss: 0.00000752
Iteration 152/1000 | Loss: 0.00000752
Iteration 153/1000 | Loss: 0.00000752
Iteration 154/1000 | Loss: 0.00000752
Iteration 155/1000 | Loss: 0.00000752
Iteration 156/1000 | Loss: 0.00000752
Iteration 157/1000 | Loss: 0.00000752
Iteration 158/1000 | Loss: 0.00000752
Iteration 159/1000 | Loss: 0.00000752
Iteration 160/1000 | Loss: 0.00000752
Iteration 161/1000 | Loss: 0.00000752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [7.521775387431262e-06, 7.521775387431262e-06, 7.521775387431262e-06, 7.521775387431262e-06, 7.521775387431262e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.521775387431262e-06

Optimization complete. Final v2v error: 2.3529677391052246 mm

Highest mean error: 2.4938771724700928 mm for frame 34

Lowest mean error: 2.224677085876465 mm for frame 115

Saving results

Total time: 85.4463152885437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045127
Iteration 2/25 | Loss: 0.00155686
Iteration 3/25 | Loss: 0.00118327
Iteration 4/25 | Loss: 0.00115617
Iteration 5/25 | Loss: 0.00114794
Iteration 6/25 | Loss: 0.00114508
Iteration 7/25 | Loss: 0.00114476
Iteration 8/25 | Loss: 0.00114476
Iteration 9/25 | Loss: 0.00114476
Iteration 10/25 | Loss: 0.00114476
Iteration 11/25 | Loss: 0.00114476
Iteration 12/25 | Loss: 0.00114476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011447635479271412, 0.0011447635479271412, 0.0011447635479271412, 0.0011447635479271412, 0.0011447635479271412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011447635479271412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88818938
Iteration 2/25 | Loss: 0.00174121
Iteration 3/25 | Loss: 0.00174120
Iteration 4/25 | Loss: 0.00174120
Iteration 5/25 | Loss: 0.00174120
Iteration 6/25 | Loss: 0.00174119
Iteration 7/25 | Loss: 0.00174119
Iteration 8/25 | Loss: 0.00174119
Iteration 9/25 | Loss: 0.00174119
Iteration 10/25 | Loss: 0.00174119
Iteration 11/25 | Loss: 0.00174119
Iteration 12/25 | Loss: 0.00174119
Iteration 13/25 | Loss: 0.00174119
Iteration 14/25 | Loss: 0.00174119
Iteration 15/25 | Loss: 0.00174119
Iteration 16/25 | Loss: 0.00174119
Iteration 17/25 | Loss: 0.00174119
Iteration 18/25 | Loss: 0.00174119
Iteration 19/25 | Loss: 0.00174119
Iteration 20/25 | Loss: 0.00174119
Iteration 21/25 | Loss: 0.00174119
Iteration 22/25 | Loss: 0.00174119
Iteration 23/25 | Loss: 0.00174119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017411934677511454, 0.0017411934677511454, 0.0017411934677511454, 0.0017411934677511454, 0.0017411934677511454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017411934677511454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174119
Iteration 2/1000 | Loss: 0.00006456
Iteration 3/1000 | Loss: 0.00004222
Iteration 4/1000 | Loss: 0.00003330
Iteration 5/1000 | Loss: 0.00003106
Iteration 6/1000 | Loss: 0.00002978
Iteration 7/1000 | Loss: 0.00002914
Iteration 8/1000 | Loss: 0.00002834
Iteration 9/1000 | Loss: 0.00002778
Iteration 10/1000 | Loss: 0.00002736
Iteration 11/1000 | Loss: 0.00002700
Iteration 12/1000 | Loss: 0.00002680
Iteration 13/1000 | Loss: 0.00002663
Iteration 14/1000 | Loss: 0.00002647
Iteration 15/1000 | Loss: 0.00002630
Iteration 16/1000 | Loss: 0.00002625
Iteration 17/1000 | Loss: 0.00002610
Iteration 18/1000 | Loss: 0.00002600
Iteration 19/1000 | Loss: 0.00002595
Iteration 20/1000 | Loss: 0.00002595
Iteration 21/1000 | Loss: 0.00002595
Iteration 22/1000 | Loss: 0.00002594
Iteration 23/1000 | Loss: 0.00002594
Iteration 24/1000 | Loss: 0.00002594
Iteration 25/1000 | Loss: 0.00002593
Iteration 26/1000 | Loss: 0.00002592
Iteration 27/1000 | Loss: 0.00002590
Iteration 28/1000 | Loss: 0.00002588
Iteration 29/1000 | Loss: 0.00002583
Iteration 30/1000 | Loss: 0.00002583
Iteration 31/1000 | Loss: 0.00002583
Iteration 32/1000 | Loss: 0.00002583
Iteration 33/1000 | Loss: 0.00002583
Iteration 34/1000 | Loss: 0.00002583
Iteration 35/1000 | Loss: 0.00002582
Iteration 36/1000 | Loss: 0.00002582
Iteration 37/1000 | Loss: 0.00002582
Iteration 38/1000 | Loss: 0.00002579
Iteration 39/1000 | Loss: 0.00002578
Iteration 40/1000 | Loss: 0.00002578
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002577
Iteration 43/1000 | Loss: 0.00002577
Iteration 44/1000 | Loss: 0.00002576
Iteration 45/1000 | Loss: 0.00002575
Iteration 46/1000 | Loss: 0.00002575
Iteration 47/1000 | Loss: 0.00002575
Iteration 48/1000 | Loss: 0.00002575
Iteration 49/1000 | Loss: 0.00002575
Iteration 50/1000 | Loss: 0.00002575
Iteration 51/1000 | Loss: 0.00002575
Iteration 52/1000 | Loss: 0.00002575
Iteration 53/1000 | Loss: 0.00002575
Iteration 54/1000 | Loss: 0.00002575
Iteration 55/1000 | Loss: 0.00002574
Iteration 56/1000 | Loss: 0.00002573
Iteration 57/1000 | Loss: 0.00002573
Iteration 58/1000 | Loss: 0.00002573
Iteration 59/1000 | Loss: 0.00002573
Iteration 60/1000 | Loss: 0.00002573
Iteration 61/1000 | Loss: 0.00002572
Iteration 62/1000 | Loss: 0.00002572
Iteration 63/1000 | Loss: 0.00002572
Iteration 64/1000 | Loss: 0.00002571
Iteration 65/1000 | Loss: 0.00002571
Iteration 66/1000 | Loss: 0.00002571
Iteration 67/1000 | Loss: 0.00002571
Iteration 68/1000 | Loss: 0.00002570
Iteration 69/1000 | Loss: 0.00002570
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002570
Iteration 72/1000 | Loss: 0.00002569
Iteration 73/1000 | Loss: 0.00002569
Iteration 74/1000 | Loss: 0.00002569
Iteration 75/1000 | Loss: 0.00002569
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002568
Iteration 81/1000 | Loss: 0.00002567
Iteration 82/1000 | Loss: 0.00002567
Iteration 83/1000 | Loss: 0.00002567
Iteration 84/1000 | Loss: 0.00002567
Iteration 85/1000 | Loss: 0.00002566
Iteration 86/1000 | Loss: 0.00002566
Iteration 87/1000 | Loss: 0.00002566
Iteration 88/1000 | Loss: 0.00002566
Iteration 89/1000 | Loss: 0.00002566
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00002565
Iteration 92/1000 | Loss: 0.00002565
Iteration 93/1000 | Loss: 0.00002565
Iteration 94/1000 | Loss: 0.00002564
Iteration 95/1000 | Loss: 0.00002564
Iteration 96/1000 | Loss: 0.00002564
Iteration 97/1000 | Loss: 0.00002563
Iteration 98/1000 | Loss: 0.00002563
Iteration 99/1000 | Loss: 0.00002562
Iteration 100/1000 | Loss: 0.00002562
Iteration 101/1000 | Loss: 0.00002562
Iteration 102/1000 | Loss: 0.00002561
Iteration 103/1000 | Loss: 0.00002561
Iteration 104/1000 | Loss: 0.00002561
Iteration 105/1000 | Loss: 0.00002561
Iteration 106/1000 | Loss: 0.00002561
Iteration 107/1000 | Loss: 0.00002560
Iteration 108/1000 | Loss: 0.00002560
Iteration 109/1000 | Loss: 0.00002560
Iteration 110/1000 | Loss: 0.00002559
Iteration 111/1000 | Loss: 0.00002559
Iteration 112/1000 | Loss: 0.00002559
Iteration 113/1000 | Loss: 0.00002558
Iteration 114/1000 | Loss: 0.00002558
Iteration 115/1000 | Loss: 0.00002558
Iteration 116/1000 | Loss: 0.00002558
Iteration 117/1000 | Loss: 0.00002558
Iteration 118/1000 | Loss: 0.00002558
Iteration 119/1000 | Loss: 0.00002558
Iteration 120/1000 | Loss: 0.00002558
Iteration 121/1000 | Loss: 0.00002557
Iteration 122/1000 | Loss: 0.00002557
Iteration 123/1000 | Loss: 0.00002557
Iteration 124/1000 | Loss: 0.00002557
Iteration 125/1000 | Loss: 0.00002557
Iteration 126/1000 | Loss: 0.00002557
Iteration 127/1000 | Loss: 0.00002557
Iteration 128/1000 | Loss: 0.00002556
Iteration 129/1000 | Loss: 0.00002556
Iteration 130/1000 | Loss: 0.00002556
Iteration 131/1000 | Loss: 0.00002556
Iteration 132/1000 | Loss: 0.00002556
Iteration 133/1000 | Loss: 0.00002556
Iteration 134/1000 | Loss: 0.00002555
Iteration 135/1000 | Loss: 0.00002555
Iteration 136/1000 | Loss: 0.00002555
Iteration 137/1000 | Loss: 0.00002555
Iteration 138/1000 | Loss: 0.00002555
Iteration 139/1000 | Loss: 0.00002555
Iteration 140/1000 | Loss: 0.00002555
Iteration 141/1000 | Loss: 0.00002555
Iteration 142/1000 | Loss: 0.00002555
Iteration 143/1000 | Loss: 0.00002555
Iteration 144/1000 | Loss: 0.00002555
Iteration 145/1000 | Loss: 0.00002554
Iteration 146/1000 | Loss: 0.00002554
Iteration 147/1000 | Loss: 0.00002554
Iteration 148/1000 | Loss: 0.00002554
Iteration 149/1000 | Loss: 0.00002554
Iteration 150/1000 | Loss: 0.00002554
Iteration 151/1000 | Loss: 0.00002554
Iteration 152/1000 | Loss: 0.00002554
Iteration 153/1000 | Loss: 0.00002554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.5544555683154613e-05, 2.5544555683154613e-05, 2.5544555683154613e-05, 2.5544555683154613e-05, 2.5544555683154613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5544555683154613e-05

Optimization complete. Final v2v error: 4.027070999145508 mm

Highest mean error: 5.019155979156494 mm for frame 67

Lowest mean error: 3.1911087036132812 mm for frame 0

Saving results

Total time: 50.38402056694031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393001
Iteration 2/25 | Loss: 0.00105716
Iteration 3/25 | Loss: 0.00096537
Iteration 4/25 | Loss: 0.00094908
Iteration 5/25 | Loss: 0.00094411
Iteration 6/25 | Loss: 0.00094233
Iteration 7/25 | Loss: 0.00094233
Iteration 8/25 | Loss: 0.00094233
Iteration 9/25 | Loss: 0.00094233
Iteration 10/25 | Loss: 0.00094233
Iteration 11/25 | Loss: 0.00094233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009423266747035086, 0.0009423266747035086, 0.0009423266747035086, 0.0009423266747035086, 0.0009423266747035086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009423266747035086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.59643173
Iteration 2/25 | Loss: 0.00190251
Iteration 3/25 | Loss: 0.00190251
Iteration 4/25 | Loss: 0.00190251
Iteration 5/25 | Loss: 0.00190251
Iteration 6/25 | Loss: 0.00190251
Iteration 7/25 | Loss: 0.00190251
Iteration 8/25 | Loss: 0.00190251
Iteration 9/25 | Loss: 0.00190251
Iteration 10/25 | Loss: 0.00190251
Iteration 11/25 | Loss: 0.00190251
Iteration 12/25 | Loss: 0.00190251
Iteration 13/25 | Loss: 0.00190251
Iteration 14/25 | Loss: 0.00190251
Iteration 15/25 | Loss: 0.00190251
Iteration 16/25 | Loss: 0.00190251
Iteration 17/25 | Loss: 0.00190251
Iteration 18/25 | Loss: 0.00190251
Iteration 19/25 | Loss: 0.00190251
Iteration 20/25 | Loss: 0.00190251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019025055225938559, 0.0019025055225938559, 0.0019025055225938559, 0.0019025055225938559, 0.0019025055225938559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019025055225938559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190251
Iteration 2/1000 | Loss: 0.00001549
Iteration 3/1000 | Loss: 0.00001082
Iteration 4/1000 | Loss: 0.00000998
Iteration 5/1000 | Loss: 0.00000913
Iteration 6/1000 | Loss: 0.00000871
Iteration 7/1000 | Loss: 0.00000859
Iteration 8/1000 | Loss: 0.00000834
Iteration 9/1000 | Loss: 0.00000833
Iteration 10/1000 | Loss: 0.00000820
Iteration 11/1000 | Loss: 0.00000805
Iteration 12/1000 | Loss: 0.00000796
Iteration 13/1000 | Loss: 0.00000793
Iteration 14/1000 | Loss: 0.00000790
Iteration 15/1000 | Loss: 0.00000789
Iteration 16/1000 | Loss: 0.00000789
Iteration 17/1000 | Loss: 0.00000780
Iteration 18/1000 | Loss: 0.00000780
Iteration 19/1000 | Loss: 0.00000780
Iteration 20/1000 | Loss: 0.00000778
Iteration 21/1000 | Loss: 0.00000775
Iteration 22/1000 | Loss: 0.00000775
Iteration 23/1000 | Loss: 0.00000775
Iteration 24/1000 | Loss: 0.00000774
Iteration 25/1000 | Loss: 0.00000774
Iteration 26/1000 | Loss: 0.00000769
Iteration 27/1000 | Loss: 0.00000769
Iteration 28/1000 | Loss: 0.00000768
Iteration 29/1000 | Loss: 0.00000768
Iteration 30/1000 | Loss: 0.00000768
Iteration 31/1000 | Loss: 0.00000768
Iteration 32/1000 | Loss: 0.00000767
Iteration 33/1000 | Loss: 0.00000766
Iteration 34/1000 | Loss: 0.00000765
Iteration 35/1000 | Loss: 0.00000765
Iteration 36/1000 | Loss: 0.00000764
Iteration 37/1000 | Loss: 0.00000764
Iteration 38/1000 | Loss: 0.00000764
Iteration 39/1000 | Loss: 0.00000764
Iteration 40/1000 | Loss: 0.00000764
Iteration 41/1000 | Loss: 0.00000764
Iteration 42/1000 | Loss: 0.00000763
Iteration 43/1000 | Loss: 0.00000763
Iteration 44/1000 | Loss: 0.00000763
Iteration 45/1000 | Loss: 0.00000763
Iteration 46/1000 | Loss: 0.00000763
Iteration 47/1000 | Loss: 0.00000763
Iteration 48/1000 | Loss: 0.00000763
Iteration 49/1000 | Loss: 0.00000763
Iteration 50/1000 | Loss: 0.00000763
Iteration 51/1000 | Loss: 0.00000763
Iteration 52/1000 | Loss: 0.00000763
Iteration 53/1000 | Loss: 0.00000763
Iteration 54/1000 | Loss: 0.00000763
Iteration 55/1000 | Loss: 0.00000762
Iteration 56/1000 | Loss: 0.00000762
Iteration 57/1000 | Loss: 0.00000762
Iteration 58/1000 | Loss: 0.00000761
Iteration 59/1000 | Loss: 0.00000760
Iteration 60/1000 | Loss: 0.00000759
Iteration 61/1000 | Loss: 0.00000759
Iteration 62/1000 | Loss: 0.00000759
Iteration 63/1000 | Loss: 0.00000758
Iteration 64/1000 | Loss: 0.00000758
Iteration 65/1000 | Loss: 0.00000758
Iteration 66/1000 | Loss: 0.00000757
Iteration 67/1000 | Loss: 0.00000757
Iteration 68/1000 | Loss: 0.00000756
Iteration 69/1000 | Loss: 0.00000756
Iteration 70/1000 | Loss: 0.00000756
Iteration 71/1000 | Loss: 0.00000756
Iteration 72/1000 | Loss: 0.00000756
Iteration 73/1000 | Loss: 0.00000755
Iteration 74/1000 | Loss: 0.00000755
Iteration 75/1000 | Loss: 0.00000755
Iteration 76/1000 | Loss: 0.00000755
Iteration 77/1000 | Loss: 0.00000755
Iteration 78/1000 | Loss: 0.00000755
Iteration 79/1000 | Loss: 0.00000755
Iteration 80/1000 | Loss: 0.00000755
Iteration 81/1000 | Loss: 0.00000755
Iteration 82/1000 | Loss: 0.00000755
Iteration 83/1000 | Loss: 0.00000754
Iteration 84/1000 | Loss: 0.00000754
Iteration 85/1000 | Loss: 0.00000754
Iteration 86/1000 | Loss: 0.00000754
Iteration 87/1000 | Loss: 0.00000754
Iteration 88/1000 | Loss: 0.00000754
Iteration 89/1000 | Loss: 0.00000753
Iteration 90/1000 | Loss: 0.00000753
Iteration 91/1000 | Loss: 0.00000753
Iteration 92/1000 | Loss: 0.00000753
Iteration 93/1000 | Loss: 0.00000752
Iteration 94/1000 | Loss: 0.00000752
Iteration 95/1000 | Loss: 0.00000752
Iteration 96/1000 | Loss: 0.00000752
Iteration 97/1000 | Loss: 0.00000752
Iteration 98/1000 | Loss: 0.00000752
Iteration 99/1000 | Loss: 0.00000752
Iteration 100/1000 | Loss: 0.00000752
Iteration 101/1000 | Loss: 0.00000752
Iteration 102/1000 | Loss: 0.00000752
Iteration 103/1000 | Loss: 0.00000752
Iteration 104/1000 | Loss: 0.00000752
Iteration 105/1000 | Loss: 0.00000752
Iteration 106/1000 | Loss: 0.00000752
Iteration 107/1000 | Loss: 0.00000752
Iteration 108/1000 | Loss: 0.00000752
Iteration 109/1000 | Loss: 0.00000752
Iteration 110/1000 | Loss: 0.00000752
Iteration 111/1000 | Loss: 0.00000752
Iteration 112/1000 | Loss: 0.00000752
Iteration 113/1000 | Loss: 0.00000752
Iteration 114/1000 | Loss: 0.00000752
Iteration 115/1000 | Loss: 0.00000752
Iteration 116/1000 | Loss: 0.00000752
Iteration 117/1000 | Loss: 0.00000752
Iteration 118/1000 | Loss: 0.00000752
Iteration 119/1000 | Loss: 0.00000752
Iteration 120/1000 | Loss: 0.00000752
Iteration 121/1000 | Loss: 0.00000752
Iteration 122/1000 | Loss: 0.00000752
Iteration 123/1000 | Loss: 0.00000752
Iteration 124/1000 | Loss: 0.00000752
Iteration 125/1000 | Loss: 0.00000752
Iteration 126/1000 | Loss: 0.00000752
Iteration 127/1000 | Loss: 0.00000752
Iteration 128/1000 | Loss: 0.00000752
Iteration 129/1000 | Loss: 0.00000752
Iteration 130/1000 | Loss: 0.00000752
Iteration 131/1000 | Loss: 0.00000752
Iteration 132/1000 | Loss: 0.00000752
Iteration 133/1000 | Loss: 0.00000752
Iteration 134/1000 | Loss: 0.00000752
Iteration 135/1000 | Loss: 0.00000752
Iteration 136/1000 | Loss: 0.00000752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [7.518648089899216e-06, 7.518648089899216e-06, 7.518648089899216e-06, 7.518648089899216e-06, 7.518648089899216e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.518648089899216e-06

Optimization complete. Final v2v error: 2.356961965560913 mm

Highest mean error: 2.6039063930511475 mm for frame 134

Lowest mean error: 2.2330057621002197 mm for frame 150

Saving results

Total time: 32.879518032073975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445326
Iteration 2/25 | Loss: 0.00134550
Iteration 3/25 | Loss: 0.00107013
Iteration 4/25 | Loss: 0.00103797
Iteration 5/25 | Loss: 0.00103108
Iteration 6/25 | Loss: 0.00102912
Iteration 7/25 | Loss: 0.00102845
Iteration 8/25 | Loss: 0.00102841
Iteration 9/25 | Loss: 0.00102841
Iteration 10/25 | Loss: 0.00102841
Iteration 11/25 | Loss: 0.00102841
Iteration 12/25 | Loss: 0.00102841
Iteration 13/25 | Loss: 0.00102841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010284091113135219, 0.0010284091113135219, 0.0010284091113135219, 0.0010284091113135219, 0.0010284091113135219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010284091113135219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13460052
Iteration 2/25 | Loss: 0.00199370
Iteration 3/25 | Loss: 0.00199368
Iteration 4/25 | Loss: 0.00199368
Iteration 5/25 | Loss: 0.00199368
Iteration 6/25 | Loss: 0.00199368
Iteration 7/25 | Loss: 0.00199368
Iteration 8/25 | Loss: 0.00199368
Iteration 9/25 | Loss: 0.00199368
Iteration 10/25 | Loss: 0.00199368
Iteration 11/25 | Loss: 0.00199368
Iteration 12/25 | Loss: 0.00199368
Iteration 13/25 | Loss: 0.00199368
Iteration 14/25 | Loss: 0.00199368
Iteration 15/25 | Loss: 0.00199368
Iteration 16/25 | Loss: 0.00199368
Iteration 17/25 | Loss: 0.00199368
Iteration 18/25 | Loss: 0.00199368
Iteration 19/25 | Loss: 0.00199368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019936799071729183, 0.0019936799071729183, 0.0019936799071729183, 0.0019936799071729183, 0.0019936799071729183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019936799071729183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199368
Iteration 2/1000 | Loss: 0.00005238
Iteration 3/1000 | Loss: 0.00003292
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002523
Iteration 6/1000 | Loss: 0.00002430
Iteration 7/1000 | Loss: 0.00002324
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002209
Iteration 10/1000 | Loss: 0.00002178
Iteration 11/1000 | Loss: 0.00002157
Iteration 12/1000 | Loss: 0.00002142
Iteration 13/1000 | Loss: 0.00002132
Iteration 14/1000 | Loss: 0.00002123
Iteration 15/1000 | Loss: 0.00002122
Iteration 16/1000 | Loss: 0.00002118
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002110
Iteration 19/1000 | Loss: 0.00002109
Iteration 20/1000 | Loss: 0.00002109
Iteration 21/1000 | Loss: 0.00002108
Iteration 22/1000 | Loss: 0.00002105
Iteration 23/1000 | Loss: 0.00002105
Iteration 24/1000 | Loss: 0.00002101
Iteration 25/1000 | Loss: 0.00002101
Iteration 26/1000 | Loss: 0.00002100
Iteration 27/1000 | Loss: 0.00002099
Iteration 28/1000 | Loss: 0.00002097
Iteration 29/1000 | Loss: 0.00002097
Iteration 30/1000 | Loss: 0.00002096
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002095
Iteration 34/1000 | Loss: 0.00002094
Iteration 35/1000 | Loss: 0.00002094
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00002093
Iteration 38/1000 | Loss: 0.00002092
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002091
Iteration 41/1000 | Loss: 0.00002091
Iteration 42/1000 | Loss: 0.00002090
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002090
Iteration 45/1000 | Loss: 0.00002090
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002090
Iteration 48/1000 | Loss: 0.00002090
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002089
Iteration 51/1000 | Loss: 0.00002089
Iteration 52/1000 | Loss: 0.00002089
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002088
Iteration 56/1000 | Loss: 0.00002088
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002086
Iteration 64/1000 | Loss: 0.00002086
Iteration 65/1000 | Loss: 0.00002086
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002085
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002085
Iteration 71/1000 | Loss: 0.00002085
Iteration 72/1000 | Loss: 0.00002085
Iteration 73/1000 | Loss: 0.00002085
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002084
Iteration 76/1000 | Loss: 0.00002084
Iteration 77/1000 | Loss: 0.00002084
Iteration 78/1000 | Loss: 0.00002084
Iteration 79/1000 | Loss: 0.00002084
Iteration 80/1000 | Loss: 0.00002084
Iteration 81/1000 | Loss: 0.00002084
Iteration 82/1000 | Loss: 0.00002084
Iteration 83/1000 | Loss: 0.00002084
Iteration 84/1000 | Loss: 0.00002084
Iteration 85/1000 | Loss: 0.00002084
Iteration 86/1000 | Loss: 0.00002084
Iteration 87/1000 | Loss: 0.00002083
Iteration 88/1000 | Loss: 0.00002083
Iteration 89/1000 | Loss: 0.00002083
Iteration 90/1000 | Loss: 0.00002083
Iteration 91/1000 | Loss: 0.00002083
Iteration 92/1000 | Loss: 0.00002082
Iteration 93/1000 | Loss: 0.00002082
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00002082
Iteration 96/1000 | Loss: 0.00002082
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002080
Iteration 101/1000 | Loss: 0.00002080
Iteration 102/1000 | Loss: 0.00002080
Iteration 103/1000 | Loss: 0.00002079
Iteration 104/1000 | Loss: 0.00002079
Iteration 105/1000 | Loss: 0.00002079
Iteration 106/1000 | Loss: 0.00002079
Iteration 107/1000 | Loss: 0.00002079
Iteration 108/1000 | Loss: 0.00002079
Iteration 109/1000 | Loss: 0.00002079
Iteration 110/1000 | Loss: 0.00002078
Iteration 111/1000 | Loss: 0.00002078
Iteration 112/1000 | Loss: 0.00002078
Iteration 113/1000 | Loss: 0.00002078
Iteration 114/1000 | Loss: 0.00002078
Iteration 115/1000 | Loss: 0.00002078
Iteration 116/1000 | Loss: 0.00002077
Iteration 117/1000 | Loss: 0.00002077
Iteration 118/1000 | Loss: 0.00002077
Iteration 119/1000 | Loss: 0.00002077
Iteration 120/1000 | Loss: 0.00002077
Iteration 121/1000 | Loss: 0.00002076
Iteration 122/1000 | Loss: 0.00002076
Iteration 123/1000 | Loss: 0.00002076
Iteration 124/1000 | Loss: 0.00002076
Iteration 125/1000 | Loss: 0.00002076
Iteration 126/1000 | Loss: 0.00002075
Iteration 127/1000 | Loss: 0.00002075
Iteration 128/1000 | Loss: 0.00002075
Iteration 129/1000 | Loss: 0.00002075
Iteration 130/1000 | Loss: 0.00002075
Iteration 131/1000 | Loss: 0.00002075
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00002075
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002074
Iteration 138/1000 | Loss: 0.00002074
Iteration 139/1000 | Loss: 0.00002074
Iteration 140/1000 | Loss: 0.00002074
Iteration 141/1000 | Loss: 0.00002074
Iteration 142/1000 | Loss: 0.00002074
Iteration 143/1000 | Loss: 0.00002074
Iteration 144/1000 | Loss: 0.00002074
Iteration 145/1000 | Loss: 0.00002074
Iteration 146/1000 | Loss: 0.00002074
Iteration 147/1000 | Loss: 0.00002073
Iteration 148/1000 | Loss: 0.00002073
Iteration 149/1000 | Loss: 0.00002073
Iteration 150/1000 | Loss: 0.00002073
Iteration 151/1000 | Loss: 0.00002073
Iteration 152/1000 | Loss: 0.00002073
Iteration 153/1000 | Loss: 0.00002072
Iteration 154/1000 | Loss: 0.00002072
Iteration 155/1000 | Loss: 0.00002072
Iteration 156/1000 | Loss: 0.00002072
Iteration 157/1000 | Loss: 0.00002071
Iteration 158/1000 | Loss: 0.00002071
Iteration 159/1000 | Loss: 0.00002071
Iteration 160/1000 | Loss: 0.00002071
Iteration 161/1000 | Loss: 0.00002071
Iteration 162/1000 | Loss: 0.00002071
Iteration 163/1000 | Loss: 0.00002071
Iteration 164/1000 | Loss: 0.00002071
Iteration 165/1000 | Loss: 0.00002071
Iteration 166/1000 | Loss: 0.00002070
Iteration 167/1000 | Loss: 0.00002070
Iteration 168/1000 | Loss: 0.00002070
Iteration 169/1000 | Loss: 0.00002070
Iteration 170/1000 | Loss: 0.00002070
Iteration 171/1000 | Loss: 0.00002070
Iteration 172/1000 | Loss: 0.00002070
Iteration 173/1000 | Loss: 0.00002070
Iteration 174/1000 | Loss: 0.00002070
Iteration 175/1000 | Loss: 0.00002070
Iteration 176/1000 | Loss: 0.00002069
Iteration 177/1000 | Loss: 0.00002069
Iteration 178/1000 | Loss: 0.00002069
Iteration 179/1000 | Loss: 0.00002068
Iteration 180/1000 | Loss: 0.00002068
Iteration 181/1000 | Loss: 0.00002068
Iteration 182/1000 | Loss: 0.00002068
Iteration 183/1000 | Loss: 0.00002068
Iteration 184/1000 | Loss: 0.00002068
Iteration 185/1000 | Loss: 0.00002068
Iteration 186/1000 | Loss: 0.00002068
Iteration 187/1000 | Loss: 0.00002068
Iteration 188/1000 | Loss: 0.00002068
Iteration 189/1000 | Loss: 0.00002068
Iteration 190/1000 | Loss: 0.00002067
Iteration 191/1000 | Loss: 0.00002067
Iteration 192/1000 | Loss: 0.00002067
Iteration 193/1000 | Loss: 0.00002067
Iteration 194/1000 | Loss: 0.00002067
Iteration 195/1000 | Loss: 0.00002067
Iteration 196/1000 | Loss: 0.00002067
Iteration 197/1000 | Loss: 0.00002067
Iteration 198/1000 | Loss: 0.00002066
Iteration 199/1000 | Loss: 0.00002066
Iteration 200/1000 | Loss: 0.00002066
Iteration 201/1000 | Loss: 0.00002066
Iteration 202/1000 | Loss: 0.00002066
Iteration 203/1000 | Loss: 0.00002066
Iteration 204/1000 | Loss: 0.00002066
Iteration 205/1000 | Loss: 0.00002066
Iteration 206/1000 | Loss: 0.00002066
Iteration 207/1000 | Loss: 0.00002066
Iteration 208/1000 | Loss: 0.00002065
Iteration 209/1000 | Loss: 0.00002065
Iteration 210/1000 | Loss: 0.00002065
Iteration 211/1000 | Loss: 0.00002065
Iteration 212/1000 | Loss: 0.00002065
Iteration 213/1000 | Loss: 0.00002065
Iteration 214/1000 | Loss: 0.00002065
Iteration 215/1000 | Loss: 0.00002065
Iteration 216/1000 | Loss: 0.00002065
Iteration 217/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.064832005999051e-05, 2.064832005999051e-05, 2.064832005999051e-05, 2.064832005999051e-05, 2.064832005999051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.064832005999051e-05

Optimization complete. Final v2v error: 3.544597864151001 mm

Highest mean error: 5.417213439941406 mm for frame 82

Lowest mean error: 2.569746255874634 mm for frame 49

Saving results

Total time: 47.54542255401611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041941
Iteration 2/25 | Loss: 0.00175739
Iteration 3/25 | Loss: 0.00157849
Iteration 4/25 | Loss: 0.00132747
Iteration 5/25 | Loss: 0.00125406
Iteration 6/25 | Loss: 0.00126942
Iteration 7/25 | Loss: 0.00131609
Iteration 8/25 | Loss: 0.00133283
Iteration 9/25 | Loss: 0.00133980
Iteration 10/25 | Loss: 0.00120988
Iteration 11/25 | Loss: 0.00118660
Iteration 12/25 | Loss: 0.00117354
Iteration 13/25 | Loss: 0.00111555
Iteration 14/25 | Loss: 0.00106365
Iteration 15/25 | Loss: 0.00107160
Iteration 16/25 | Loss: 0.00110292
Iteration 17/25 | Loss: 0.00108499
Iteration 18/25 | Loss: 0.00107003
Iteration 19/25 | Loss: 0.00106354
Iteration 20/25 | Loss: 0.00105837
Iteration 21/25 | Loss: 0.00106067
Iteration 22/25 | Loss: 0.00110077
Iteration 23/25 | Loss: 0.00105898
Iteration 24/25 | Loss: 0.00105460
Iteration 25/25 | Loss: 0.00105052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22235799
Iteration 2/25 | Loss: 0.00254570
Iteration 3/25 | Loss: 0.00254569
Iteration 4/25 | Loss: 0.00254569
Iteration 5/25 | Loss: 0.00254569
Iteration 6/25 | Loss: 0.00254569
Iteration 7/25 | Loss: 0.00254569
Iteration 8/25 | Loss: 0.00254569
Iteration 9/25 | Loss: 0.00254569
Iteration 10/25 | Loss: 0.00254569
Iteration 11/25 | Loss: 0.00254569
Iteration 12/25 | Loss: 0.00254569
Iteration 13/25 | Loss: 0.00254569
Iteration 14/25 | Loss: 0.00254569
Iteration 15/25 | Loss: 0.00254569
Iteration 16/25 | Loss: 0.00254569
Iteration 17/25 | Loss: 0.00254569
Iteration 18/25 | Loss: 0.00254569
Iteration 19/25 | Loss: 0.00254569
Iteration 20/25 | Loss: 0.00254569
Iteration 21/25 | Loss: 0.00254569
Iteration 22/25 | Loss: 0.00254569
Iteration 23/25 | Loss: 0.00254569
Iteration 24/25 | Loss: 0.00254569
Iteration 25/25 | Loss: 0.00254569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254569
Iteration 2/1000 | Loss: 0.00276875
Iteration 3/1000 | Loss: 0.00033268
Iteration 4/1000 | Loss: 0.00235979
Iteration 5/1000 | Loss: 0.00197518
Iteration 6/1000 | Loss: 0.00329471
Iteration 7/1000 | Loss: 0.00169076
Iteration 8/1000 | Loss: 0.00265774
Iteration 9/1000 | Loss: 0.00235443
Iteration 10/1000 | Loss: 0.00151892
Iteration 11/1000 | Loss: 0.00113090
Iteration 12/1000 | Loss: 0.00160306
Iteration 13/1000 | Loss: 0.00155468
Iteration 14/1000 | Loss: 0.00148611
Iteration 15/1000 | Loss: 0.00165426
Iteration 16/1000 | Loss: 0.00130195
Iteration 17/1000 | Loss: 0.00140530
Iteration 18/1000 | Loss: 0.00231670
Iteration 19/1000 | Loss: 0.00127129
Iteration 20/1000 | Loss: 0.00117153
Iteration 21/1000 | Loss: 0.00072395
Iteration 22/1000 | Loss: 0.00055750
Iteration 23/1000 | Loss: 0.00046163
Iteration 24/1000 | Loss: 0.00074785
Iteration 25/1000 | Loss: 0.00057303
Iteration 26/1000 | Loss: 0.00051800
Iteration 27/1000 | Loss: 0.00043602
Iteration 28/1000 | Loss: 0.00054780
Iteration 29/1000 | Loss: 0.00074672
Iteration 30/1000 | Loss: 0.00039960
Iteration 31/1000 | Loss: 0.00036668
Iteration 32/1000 | Loss: 0.00046999
Iteration 33/1000 | Loss: 0.00032013
Iteration 34/1000 | Loss: 0.00057851
Iteration 35/1000 | Loss: 0.00048525
Iteration 36/1000 | Loss: 0.00082045
Iteration 37/1000 | Loss: 0.00066152
Iteration 38/1000 | Loss: 0.00009726
Iteration 39/1000 | Loss: 0.00057259
Iteration 40/1000 | Loss: 0.00045399
Iteration 41/1000 | Loss: 0.00044317
Iteration 42/1000 | Loss: 0.00058364
Iteration 43/1000 | Loss: 0.00011676
Iteration 44/1000 | Loss: 0.00052806
Iteration 45/1000 | Loss: 0.00054727
Iteration 46/1000 | Loss: 0.00019473
Iteration 47/1000 | Loss: 0.00055945
Iteration 48/1000 | Loss: 0.00044628
Iteration 49/1000 | Loss: 0.00043575
Iteration 50/1000 | Loss: 0.00006448
Iteration 51/1000 | Loss: 0.00018723
Iteration 52/1000 | Loss: 0.00037024
Iteration 53/1000 | Loss: 0.00042152
Iteration 54/1000 | Loss: 0.00045281
Iteration 55/1000 | Loss: 0.00058494
Iteration 56/1000 | Loss: 0.00042121
Iteration 57/1000 | Loss: 0.00088432
Iteration 58/1000 | Loss: 0.00032175
Iteration 59/1000 | Loss: 0.00031946
Iteration 60/1000 | Loss: 0.00022610
Iteration 61/1000 | Loss: 0.00051407
Iteration 62/1000 | Loss: 0.00062310
Iteration 63/1000 | Loss: 0.00032317
Iteration 64/1000 | Loss: 0.00048673
Iteration 65/1000 | Loss: 0.00028947
Iteration 66/1000 | Loss: 0.00021486
Iteration 67/1000 | Loss: 0.00011246
Iteration 68/1000 | Loss: 0.00016585
Iteration 69/1000 | Loss: 0.00043220
Iteration 70/1000 | Loss: 0.00014784
Iteration 71/1000 | Loss: 0.00019857
Iteration 72/1000 | Loss: 0.00015662
Iteration 73/1000 | Loss: 0.00031563
Iteration 74/1000 | Loss: 0.00028664
Iteration 75/1000 | Loss: 0.00004633
Iteration 76/1000 | Loss: 0.00003893
Iteration 77/1000 | Loss: 0.00009337
Iteration 78/1000 | Loss: 0.00005434
Iteration 79/1000 | Loss: 0.00005484
Iteration 80/1000 | Loss: 0.00035691
Iteration 81/1000 | Loss: 0.00024921
Iteration 82/1000 | Loss: 0.00020421
Iteration 83/1000 | Loss: 0.00004952
Iteration 84/1000 | Loss: 0.00014978
Iteration 85/1000 | Loss: 0.00021933
Iteration 86/1000 | Loss: 0.00012257
Iteration 87/1000 | Loss: 0.00003222
Iteration 88/1000 | Loss: 0.00002489
Iteration 89/1000 | Loss: 0.00002183
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00001938
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00027483
Iteration 94/1000 | Loss: 0.00014691
Iteration 95/1000 | Loss: 0.00013708
Iteration 96/1000 | Loss: 0.00017941
Iteration 97/1000 | Loss: 0.00014632
Iteration 98/1000 | Loss: 0.00015272
Iteration 99/1000 | Loss: 0.00014025
Iteration 100/1000 | Loss: 0.00014432
Iteration 101/1000 | Loss: 0.00018111
Iteration 102/1000 | Loss: 0.00011482
Iteration 103/1000 | Loss: 0.00020880
Iteration 104/1000 | Loss: 0.00003596
Iteration 105/1000 | Loss: 0.00019271
Iteration 106/1000 | Loss: 0.00002838
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002064
Iteration 109/1000 | Loss: 0.00001848
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001461
Iteration 114/1000 | Loss: 0.00028106
Iteration 115/1000 | Loss: 0.00003047
Iteration 116/1000 | Loss: 0.00001946
Iteration 117/1000 | Loss: 0.00001647
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001286
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001128
Iteration 125/1000 | Loss: 0.00001127
Iteration 126/1000 | Loss: 0.00001126
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001124
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001123
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001119
Iteration 139/1000 | Loss: 0.00001119
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001118
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001116
Iteration 146/1000 | Loss: 0.00001116
Iteration 147/1000 | Loss: 0.00001116
Iteration 148/1000 | Loss: 0.00001116
Iteration 149/1000 | Loss: 0.00001116
Iteration 150/1000 | Loss: 0.00001115
Iteration 151/1000 | Loss: 0.00001115
Iteration 152/1000 | Loss: 0.00001115
Iteration 153/1000 | Loss: 0.00001115
Iteration 154/1000 | Loss: 0.00001115
Iteration 155/1000 | Loss: 0.00001115
Iteration 156/1000 | Loss: 0.00001115
Iteration 157/1000 | Loss: 0.00001115
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001114
Iteration 160/1000 | Loss: 0.00001114
Iteration 161/1000 | Loss: 0.00001114
Iteration 162/1000 | Loss: 0.00001113
Iteration 163/1000 | Loss: 0.00001113
Iteration 164/1000 | Loss: 0.00001113
Iteration 165/1000 | Loss: 0.00001113
Iteration 166/1000 | Loss: 0.00001113
Iteration 167/1000 | Loss: 0.00001113
Iteration 168/1000 | Loss: 0.00001113
Iteration 169/1000 | Loss: 0.00001113
Iteration 170/1000 | Loss: 0.00001113
Iteration 171/1000 | Loss: 0.00001113
Iteration 172/1000 | Loss: 0.00001113
Iteration 173/1000 | Loss: 0.00001113
Iteration 174/1000 | Loss: 0.00001113
Iteration 175/1000 | Loss: 0.00001113
Iteration 176/1000 | Loss: 0.00001112
Iteration 177/1000 | Loss: 0.00001112
Iteration 178/1000 | Loss: 0.00001112
Iteration 179/1000 | Loss: 0.00001112
Iteration 180/1000 | Loss: 0.00001112
Iteration 181/1000 | Loss: 0.00001112
Iteration 182/1000 | Loss: 0.00001112
Iteration 183/1000 | Loss: 0.00001112
Iteration 184/1000 | Loss: 0.00001112
Iteration 185/1000 | Loss: 0.00001112
Iteration 186/1000 | Loss: 0.00001111
Iteration 187/1000 | Loss: 0.00001111
Iteration 188/1000 | Loss: 0.00001111
Iteration 189/1000 | Loss: 0.00001110
Iteration 190/1000 | Loss: 0.00001110
Iteration 191/1000 | Loss: 0.00001110
Iteration 192/1000 | Loss: 0.00001110
Iteration 193/1000 | Loss: 0.00001110
Iteration 194/1000 | Loss: 0.00001109
Iteration 195/1000 | Loss: 0.00001109
Iteration 196/1000 | Loss: 0.00001108
Iteration 197/1000 | Loss: 0.00001108
Iteration 198/1000 | Loss: 0.00001108
Iteration 199/1000 | Loss: 0.00001108
Iteration 200/1000 | Loss: 0.00001108
Iteration 201/1000 | Loss: 0.00001108
Iteration 202/1000 | Loss: 0.00001108
Iteration 203/1000 | Loss: 0.00001107
Iteration 204/1000 | Loss: 0.00001107
Iteration 205/1000 | Loss: 0.00001107
Iteration 206/1000 | Loss: 0.00001107
Iteration 207/1000 | Loss: 0.00001107
Iteration 208/1000 | Loss: 0.00001107
Iteration 209/1000 | Loss: 0.00001107
Iteration 210/1000 | Loss: 0.00001107
Iteration 211/1000 | Loss: 0.00001107
Iteration 212/1000 | Loss: 0.00001107
Iteration 213/1000 | Loss: 0.00001107
Iteration 214/1000 | Loss: 0.00001107
Iteration 215/1000 | Loss: 0.00001106
Iteration 216/1000 | Loss: 0.00001106
Iteration 217/1000 | Loss: 0.00001106
Iteration 218/1000 | Loss: 0.00001106
Iteration 219/1000 | Loss: 0.00001106
Iteration 220/1000 | Loss: 0.00001106
Iteration 221/1000 | Loss: 0.00001106
Iteration 222/1000 | Loss: 0.00001105
Iteration 223/1000 | Loss: 0.00001105
Iteration 224/1000 | Loss: 0.00001105
Iteration 225/1000 | Loss: 0.00001105
Iteration 226/1000 | Loss: 0.00001105
Iteration 227/1000 | Loss: 0.00001105
Iteration 228/1000 | Loss: 0.00001105
Iteration 229/1000 | Loss: 0.00001104
Iteration 230/1000 | Loss: 0.00001104
Iteration 231/1000 | Loss: 0.00001104
Iteration 232/1000 | Loss: 0.00001104
Iteration 233/1000 | Loss: 0.00001103
Iteration 234/1000 | Loss: 0.00001103
Iteration 235/1000 | Loss: 0.00001103
Iteration 236/1000 | Loss: 0.00001103
Iteration 237/1000 | Loss: 0.00001103
Iteration 238/1000 | Loss: 0.00001103
Iteration 239/1000 | Loss: 0.00001103
Iteration 240/1000 | Loss: 0.00001103
Iteration 241/1000 | Loss: 0.00001103
Iteration 242/1000 | Loss: 0.00001102
Iteration 243/1000 | Loss: 0.00001102
Iteration 244/1000 | Loss: 0.00001102
Iteration 245/1000 | Loss: 0.00001102
Iteration 246/1000 | Loss: 0.00001102
Iteration 247/1000 | Loss: 0.00001102
Iteration 248/1000 | Loss: 0.00001102
Iteration 249/1000 | Loss: 0.00001102
Iteration 250/1000 | Loss: 0.00001102
Iteration 251/1000 | Loss: 0.00001102
Iteration 252/1000 | Loss: 0.00001102
Iteration 253/1000 | Loss: 0.00001102
Iteration 254/1000 | Loss: 0.00001102
Iteration 255/1000 | Loss: 0.00001102
Iteration 256/1000 | Loss: 0.00001102
Iteration 257/1000 | Loss: 0.00001102
Iteration 258/1000 | Loss: 0.00001102
Iteration 259/1000 | Loss: 0.00001102
Iteration 260/1000 | Loss: 0.00001101
Iteration 261/1000 | Loss: 0.00001101
Iteration 262/1000 | Loss: 0.00001101
Iteration 263/1000 | Loss: 0.00001101
Iteration 264/1000 | Loss: 0.00001101
Iteration 265/1000 | Loss: 0.00001101
Iteration 266/1000 | Loss: 0.00001101
Iteration 267/1000 | Loss: 0.00001101
Iteration 268/1000 | Loss: 0.00001101
Iteration 269/1000 | Loss: 0.00001101
Iteration 270/1000 | Loss: 0.00001101
Iteration 271/1000 | Loss: 0.00001101
Iteration 272/1000 | Loss: 0.00001101
Iteration 273/1000 | Loss: 0.00001101
Iteration 274/1000 | Loss: 0.00001101
Iteration 275/1000 | Loss: 0.00001100
Iteration 276/1000 | Loss: 0.00001100
Iteration 277/1000 | Loss: 0.00001100
Iteration 278/1000 | Loss: 0.00001100
Iteration 279/1000 | Loss: 0.00001100
Iteration 280/1000 | Loss: 0.00001099
Iteration 281/1000 | Loss: 0.00001099
Iteration 282/1000 | Loss: 0.00001099
Iteration 283/1000 | Loss: 0.00001099
Iteration 284/1000 | Loss: 0.00001099
Iteration 285/1000 | Loss: 0.00001099
Iteration 286/1000 | Loss: 0.00001099
Iteration 287/1000 | Loss: 0.00001099
Iteration 288/1000 | Loss: 0.00001099
Iteration 289/1000 | Loss: 0.00001099
Iteration 290/1000 | Loss: 0.00001099
Iteration 291/1000 | Loss: 0.00001099
Iteration 292/1000 | Loss: 0.00001099
Iteration 293/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.0990737791871652e-05, 1.0990737791871652e-05, 1.0990737791871652e-05, 1.0990737791871652e-05, 1.0990737791871652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0990737791871652e-05

Optimization complete. Final v2v error: 2.751373052597046 mm

Highest mean error: 4.01734733581543 mm for frame 44

Lowest mean error: 2.216858148574829 mm for frame 22

Saving results

Total time: 237.21371293067932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993987
Iteration 2/25 | Loss: 0.00993987
Iteration 3/25 | Loss: 0.00993987
Iteration 4/25 | Loss: 0.00258102
Iteration 5/25 | Loss: 0.00206583
Iteration 6/25 | Loss: 0.00180457
Iteration 7/25 | Loss: 0.00184176
Iteration 8/25 | Loss: 0.00166656
Iteration 9/25 | Loss: 0.00152922
Iteration 10/25 | Loss: 0.00151521
Iteration 11/25 | Loss: 0.00140170
Iteration 12/25 | Loss: 0.00136977
Iteration 13/25 | Loss: 0.00133182
Iteration 14/25 | Loss: 0.00124036
Iteration 15/25 | Loss: 0.00122782
Iteration 16/25 | Loss: 0.00117741
Iteration 17/25 | Loss: 0.00117123
Iteration 18/25 | Loss: 0.00117029
Iteration 19/25 | Loss: 0.00116970
Iteration 20/25 | Loss: 0.00117205
Iteration 21/25 | Loss: 0.00116754
Iteration 22/25 | Loss: 0.00116720
Iteration 23/25 | Loss: 0.00116717
Iteration 24/25 | Loss: 0.00116717
Iteration 25/25 | Loss: 0.00116717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17683244
Iteration 2/25 | Loss: 0.00266061
Iteration 3/25 | Loss: 0.00266061
Iteration 4/25 | Loss: 0.00266061
Iteration 5/25 | Loss: 0.00266061
Iteration 6/25 | Loss: 0.00266061
Iteration 7/25 | Loss: 0.00266061
Iteration 8/25 | Loss: 0.00266061
Iteration 9/25 | Loss: 0.00266061
Iteration 10/25 | Loss: 0.00266061
Iteration 11/25 | Loss: 0.00266061
Iteration 12/25 | Loss: 0.00266061
Iteration 13/25 | Loss: 0.00266061
Iteration 14/25 | Loss: 0.00266061
Iteration 15/25 | Loss: 0.00266061
Iteration 16/25 | Loss: 0.00266061
Iteration 17/25 | Loss: 0.00266061
Iteration 18/25 | Loss: 0.00266061
Iteration 19/25 | Loss: 0.00266061
Iteration 20/25 | Loss: 0.00266061
Iteration 21/25 | Loss: 0.00266061
Iteration 22/25 | Loss: 0.00266061
Iteration 23/25 | Loss: 0.00266061
Iteration 24/25 | Loss: 0.00266061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0026606067549437284, 0.0026606067549437284, 0.0026606067549437284, 0.0026606067549437284, 0.0026606067549437284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026606067549437284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266061
Iteration 2/1000 | Loss: 0.00019270
Iteration 3/1000 | Loss: 0.00014045
Iteration 4/1000 | Loss: 0.00011395
Iteration 5/1000 | Loss: 0.00010197
Iteration 6/1000 | Loss: 0.00009680
Iteration 7/1000 | Loss: 0.00009284
Iteration 8/1000 | Loss: 0.00032391
Iteration 9/1000 | Loss: 0.00238034
Iteration 10/1000 | Loss: 0.00044579
Iteration 11/1000 | Loss: 0.00025163
Iteration 12/1000 | Loss: 0.00008787
Iteration 13/1000 | Loss: 0.00030539
Iteration 14/1000 | Loss: 0.00007433
Iteration 15/1000 | Loss: 0.00005121
Iteration 16/1000 | Loss: 0.00006922
Iteration 17/1000 | Loss: 0.00002783
Iteration 18/1000 | Loss: 0.00002983
Iteration 19/1000 | Loss: 0.00002302
Iteration 20/1000 | Loss: 0.00005455
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00007269
Iteration 23/1000 | Loss: 0.00001680
Iteration 24/1000 | Loss: 0.00003662
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001376
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001376
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001372
Iteration 45/1000 | Loss: 0.00001372
Iteration 46/1000 | Loss: 0.00001372
Iteration 47/1000 | Loss: 0.00001372
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001367
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001366
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001366
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001360
Iteration 80/1000 | Loss: 0.00001360
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001358
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001358
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001356
Iteration 104/1000 | Loss: 0.00001356
Iteration 105/1000 | Loss: 0.00001356
Iteration 106/1000 | Loss: 0.00001356
Iteration 107/1000 | Loss: 0.00001356
Iteration 108/1000 | Loss: 0.00001356
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001355
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001354
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001353
Iteration 137/1000 | Loss: 0.00001353
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001353
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001352
Iteration 143/1000 | Loss: 0.00001352
Iteration 144/1000 | Loss: 0.00001352
Iteration 145/1000 | Loss: 0.00001352
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001352
Iteration 150/1000 | Loss: 0.00001352
Iteration 151/1000 | Loss: 0.00001352
Iteration 152/1000 | Loss: 0.00001352
Iteration 153/1000 | Loss: 0.00001352
Iteration 154/1000 | Loss: 0.00001352
Iteration 155/1000 | Loss: 0.00001352
Iteration 156/1000 | Loss: 0.00001352
Iteration 157/1000 | Loss: 0.00001352
Iteration 158/1000 | Loss: 0.00001352
Iteration 159/1000 | Loss: 0.00001352
Iteration 160/1000 | Loss: 0.00001352
Iteration 161/1000 | Loss: 0.00001352
Iteration 162/1000 | Loss: 0.00001351
Iteration 163/1000 | Loss: 0.00001351
Iteration 164/1000 | Loss: 0.00001351
Iteration 165/1000 | Loss: 0.00001351
Iteration 166/1000 | Loss: 0.00001351
Iteration 167/1000 | Loss: 0.00001351
Iteration 168/1000 | Loss: 0.00001351
Iteration 169/1000 | Loss: 0.00001351
Iteration 170/1000 | Loss: 0.00001351
Iteration 171/1000 | Loss: 0.00001351
Iteration 172/1000 | Loss: 0.00001351
Iteration 173/1000 | Loss: 0.00001351
Iteration 174/1000 | Loss: 0.00001351
Iteration 175/1000 | Loss: 0.00001351
Iteration 176/1000 | Loss: 0.00001351
Iteration 177/1000 | Loss: 0.00001351
Iteration 178/1000 | Loss: 0.00001351
Iteration 179/1000 | Loss: 0.00001351
Iteration 180/1000 | Loss: 0.00001351
Iteration 181/1000 | Loss: 0.00001351
Iteration 182/1000 | Loss: 0.00001351
Iteration 183/1000 | Loss: 0.00001351
Iteration 184/1000 | Loss: 0.00001351
Iteration 185/1000 | Loss: 0.00001351
Iteration 186/1000 | Loss: 0.00001351
Iteration 187/1000 | Loss: 0.00001351
Iteration 188/1000 | Loss: 0.00001351
Iteration 189/1000 | Loss: 0.00001351
Iteration 190/1000 | Loss: 0.00001351
Iteration 191/1000 | Loss: 0.00001351
Iteration 192/1000 | Loss: 0.00001351
Iteration 193/1000 | Loss: 0.00001351
Iteration 194/1000 | Loss: 0.00001351
Iteration 195/1000 | Loss: 0.00001351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3510924873116892e-05, 1.3510924873116892e-05, 1.3510924873116892e-05, 1.3510924873116892e-05, 1.3510924873116892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3510924873116892e-05

Optimization complete. Final v2v error: 3.076233386993408 mm

Highest mean error: 10.085602760314941 mm for frame 203

Lowest mean error: 2.666443347930908 mm for frame 163

Saving results

Total time: 102.25660037994385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831139
Iteration 2/25 | Loss: 0.00112561
Iteration 3/25 | Loss: 0.00102314
Iteration 4/25 | Loss: 0.00100053
Iteration 5/25 | Loss: 0.00099302
Iteration 6/25 | Loss: 0.00099058
Iteration 7/25 | Loss: 0.00099056
Iteration 8/25 | Loss: 0.00099056
Iteration 9/25 | Loss: 0.00099056
Iteration 10/25 | Loss: 0.00099056
Iteration 11/25 | Loss: 0.00099056
Iteration 12/25 | Loss: 0.00099056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009905612096190453, 0.0009905612096190453, 0.0009905612096190453, 0.0009905612096190453, 0.0009905612096190453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009905612096190453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06232011
Iteration 2/25 | Loss: 0.00274691
Iteration 3/25 | Loss: 0.00274690
Iteration 4/25 | Loss: 0.00274690
Iteration 5/25 | Loss: 0.00274690
Iteration 6/25 | Loss: 0.00274690
Iteration 7/25 | Loss: 0.00274690
Iteration 8/25 | Loss: 0.00274690
Iteration 9/25 | Loss: 0.00274690
Iteration 10/25 | Loss: 0.00274690
Iteration 11/25 | Loss: 0.00274690
Iteration 12/25 | Loss: 0.00274690
Iteration 13/25 | Loss: 0.00274690
Iteration 14/25 | Loss: 0.00274690
Iteration 15/25 | Loss: 0.00274690
Iteration 16/25 | Loss: 0.00274690
Iteration 17/25 | Loss: 0.00274690
Iteration 18/25 | Loss: 0.00274690
Iteration 19/25 | Loss: 0.00274690
Iteration 20/25 | Loss: 0.00274690
Iteration 21/25 | Loss: 0.00274690
Iteration 22/25 | Loss: 0.00274690
Iteration 23/25 | Loss: 0.00274690
Iteration 24/25 | Loss: 0.00274690
Iteration 25/25 | Loss: 0.00274690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0027469012420624495, 0.0027469012420624495, 0.0027469012420624495, 0.0027469012420624495, 0.0027469012420624495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027469012420624495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00274690
Iteration 2/1000 | Loss: 0.00004084
Iteration 3/1000 | Loss: 0.00002366
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001543
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001422
Iteration 9/1000 | Loss: 0.00001406
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001383
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001344
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001338
Iteration 45/1000 | Loss: 0.00001338
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001322
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001320
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001319
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001318
Iteration 96/1000 | Loss: 0.00001318
Iteration 97/1000 | Loss: 0.00001318
Iteration 98/1000 | Loss: 0.00001318
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.3184582712710835e-05, 1.3184582712710835e-05, 1.3184582712710835e-05, 1.3184582712710835e-05, 1.3184582712710835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3184582712710835e-05

Optimization complete. Final v2v error: 3.1083245277404785 mm

Highest mean error: 3.3816945552825928 mm for frame 0

Lowest mean error: 2.9368035793304443 mm for frame 164

Saving results

Total time: 35.67466115951538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00271409
Iteration 2/25 | Loss: 0.00117433
Iteration 3/25 | Loss: 0.00101177
Iteration 4/25 | Loss: 0.00099354
Iteration 5/25 | Loss: 0.00098801
Iteration 6/25 | Loss: 0.00098592
Iteration 7/25 | Loss: 0.00098570
Iteration 8/25 | Loss: 0.00098570
Iteration 9/25 | Loss: 0.00098570
Iteration 10/25 | Loss: 0.00098570
Iteration 11/25 | Loss: 0.00098570
Iteration 12/25 | Loss: 0.00098570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009857008699327707, 0.0009857008699327707, 0.0009857008699327707, 0.0009857008699327707, 0.0009857008699327707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009857008699327707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16071856
Iteration 2/25 | Loss: 0.00316704
Iteration 3/25 | Loss: 0.00316704
Iteration 4/25 | Loss: 0.00316704
Iteration 5/25 | Loss: 0.00316704
Iteration 6/25 | Loss: 0.00316704
Iteration 7/25 | Loss: 0.00316704
Iteration 8/25 | Loss: 0.00316704
Iteration 9/25 | Loss: 0.00316704
Iteration 10/25 | Loss: 0.00316704
Iteration 11/25 | Loss: 0.00316704
Iteration 12/25 | Loss: 0.00316704
Iteration 13/25 | Loss: 0.00316704
Iteration 14/25 | Loss: 0.00316704
Iteration 15/25 | Loss: 0.00316704
Iteration 16/25 | Loss: 0.00316704
Iteration 17/25 | Loss: 0.00316704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003167036222293973, 0.003167036222293973, 0.003167036222293973, 0.003167036222293973, 0.003167036222293973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003167036222293973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316704
Iteration 2/1000 | Loss: 0.00003778
Iteration 3/1000 | Loss: 0.00001934
Iteration 4/1000 | Loss: 0.00001287
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001022
Iteration 7/1000 | Loss: 0.00000981
Iteration 8/1000 | Loss: 0.00000955
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000948
Iteration 11/1000 | Loss: 0.00000946
Iteration 12/1000 | Loss: 0.00000945
Iteration 13/1000 | Loss: 0.00000937
Iteration 14/1000 | Loss: 0.00000932
Iteration 15/1000 | Loss: 0.00000922
Iteration 16/1000 | Loss: 0.00000921
Iteration 17/1000 | Loss: 0.00000919
Iteration 18/1000 | Loss: 0.00000919
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000914
Iteration 22/1000 | Loss: 0.00000913
Iteration 23/1000 | Loss: 0.00000913
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000904
Iteration 26/1000 | Loss: 0.00000899
Iteration 27/1000 | Loss: 0.00000897
Iteration 28/1000 | Loss: 0.00000896
Iteration 29/1000 | Loss: 0.00000896
Iteration 30/1000 | Loss: 0.00000895
Iteration 31/1000 | Loss: 0.00000894
Iteration 32/1000 | Loss: 0.00000893
Iteration 33/1000 | Loss: 0.00000892
Iteration 34/1000 | Loss: 0.00000892
Iteration 35/1000 | Loss: 0.00000891
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000890
Iteration 38/1000 | Loss: 0.00000890
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000888
Iteration 41/1000 | Loss: 0.00000887
Iteration 42/1000 | Loss: 0.00000887
Iteration 43/1000 | Loss: 0.00000886
Iteration 44/1000 | Loss: 0.00000886
Iteration 45/1000 | Loss: 0.00000885
Iteration 46/1000 | Loss: 0.00000885
Iteration 47/1000 | Loss: 0.00000884
Iteration 48/1000 | Loss: 0.00000883
Iteration 49/1000 | Loss: 0.00000883
Iteration 50/1000 | Loss: 0.00000883
Iteration 51/1000 | Loss: 0.00000882
Iteration 52/1000 | Loss: 0.00000881
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000880
Iteration 55/1000 | Loss: 0.00000880
Iteration 56/1000 | Loss: 0.00000879
Iteration 57/1000 | Loss: 0.00000879
Iteration 58/1000 | Loss: 0.00000879
Iteration 59/1000 | Loss: 0.00000878
Iteration 60/1000 | Loss: 0.00000878
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000877
Iteration 63/1000 | Loss: 0.00000877
Iteration 64/1000 | Loss: 0.00000877
Iteration 65/1000 | Loss: 0.00000877
Iteration 66/1000 | Loss: 0.00000877
Iteration 67/1000 | Loss: 0.00000877
Iteration 68/1000 | Loss: 0.00000877
Iteration 69/1000 | Loss: 0.00000876
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000875
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000874
Iteration 77/1000 | Loss: 0.00000874
Iteration 78/1000 | Loss: 0.00000873
Iteration 79/1000 | Loss: 0.00000873
Iteration 80/1000 | Loss: 0.00000873
Iteration 81/1000 | Loss: 0.00000872
Iteration 82/1000 | Loss: 0.00000872
Iteration 83/1000 | Loss: 0.00000872
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000871
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000870
Iteration 88/1000 | Loss: 0.00000870
Iteration 89/1000 | Loss: 0.00000870
Iteration 90/1000 | Loss: 0.00000869
Iteration 91/1000 | Loss: 0.00000869
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000868
Iteration 96/1000 | Loss: 0.00000868
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000866
Iteration 101/1000 | Loss: 0.00000866
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000865
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000864
Iteration 107/1000 | Loss: 0.00000864
Iteration 108/1000 | Loss: 0.00000864
Iteration 109/1000 | Loss: 0.00000864
Iteration 110/1000 | Loss: 0.00000863
Iteration 111/1000 | Loss: 0.00000863
Iteration 112/1000 | Loss: 0.00000863
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000862
Iteration 118/1000 | Loss: 0.00000862
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000861
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000858
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000856
Iteration 145/1000 | Loss: 0.00000856
Iteration 146/1000 | Loss: 0.00000856
Iteration 147/1000 | Loss: 0.00000856
Iteration 148/1000 | Loss: 0.00000856
Iteration 149/1000 | Loss: 0.00000855
Iteration 150/1000 | Loss: 0.00000855
Iteration 151/1000 | Loss: 0.00000855
Iteration 152/1000 | Loss: 0.00000855
Iteration 153/1000 | Loss: 0.00000855
Iteration 154/1000 | Loss: 0.00000855
Iteration 155/1000 | Loss: 0.00000855
Iteration 156/1000 | Loss: 0.00000855
Iteration 157/1000 | Loss: 0.00000855
Iteration 158/1000 | Loss: 0.00000855
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000854
Iteration 161/1000 | Loss: 0.00000854
Iteration 162/1000 | Loss: 0.00000854
Iteration 163/1000 | Loss: 0.00000854
Iteration 164/1000 | Loss: 0.00000854
Iteration 165/1000 | Loss: 0.00000854
Iteration 166/1000 | Loss: 0.00000853
Iteration 167/1000 | Loss: 0.00000853
Iteration 168/1000 | Loss: 0.00000853
Iteration 169/1000 | Loss: 0.00000853
Iteration 170/1000 | Loss: 0.00000853
Iteration 171/1000 | Loss: 0.00000853
Iteration 172/1000 | Loss: 0.00000853
Iteration 173/1000 | Loss: 0.00000853
Iteration 174/1000 | Loss: 0.00000853
Iteration 175/1000 | Loss: 0.00000853
Iteration 176/1000 | Loss: 0.00000853
Iteration 177/1000 | Loss: 0.00000853
Iteration 178/1000 | Loss: 0.00000853
Iteration 179/1000 | Loss: 0.00000853
Iteration 180/1000 | Loss: 0.00000853
Iteration 181/1000 | Loss: 0.00000853
Iteration 182/1000 | Loss: 0.00000853
Iteration 183/1000 | Loss: 0.00000853
Iteration 184/1000 | Loss: 0.00000853
Iteration 185/1000 | Loss: 0.00000853
Iteration 186/1000 | Loss: 0.00000853
Iteration 187/1000 | Loss: 0.00000853
Iteration 188/1000 | Loss: 0.00000853
Iteration 189/1000 | Loss: 0.00000853
Iteration 190/1000 | Loss: 0.00000853
Iteration 191/1000 | Loss: 0.00000853
Iteration 192/1000 | Loss: 0.00000853
Iteration 193/1000 | Loss: 0.00000853
Iteration 194/1000 | Loss: 0.00000853
Iteration 195/1000 | Loss: 0.00000853
Iteration 196/1000 | Loss: 0.00000853
Iteration 197/1000 | Loss: 0.00000853
Iteration 198/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [8.534415428584907e-06, 8.534415428584907e-06, 8.534415428584907e-06, 8.534415428584907e-06, 8.534415428584907e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.534415428584907e-06

Optimization complete. Final v2v error: 2.500288248062134 mm

Highest mean error: 2.7465031147003174 mm for frame 194

Lowest mean error: 2.1025211811065674 mm for frame 1

Saving results

Total time: 45.95423769950867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377148
Iteration 2/25 | Loss: 0.00111191
Iteration 3/25 | Loss: 0.00100271
Iteration 4/25 | Loss: 0.00098406
Iteration 5/25 | Loss: 0.00097779
Iteration 6/25 | Loss: 0.00097480
Iteration 7/25 | Loss: 0.00097451
Iteration 8/25 | Loss: 0.00097451
Iteration 9/25 | Loss: 0.00097451
Iteration 10/25 | Loss: 0.00097451
Iteration 11/25 | Loss: 0.00097451
Iteration 12/25 | Loss: 0.00097451
Iteration 13/25 | Loss: 0.00097451
Iteration 14/25 | Loss: 0.00097451
Iteration 15/25 | Loss: 0.00097451
Iteration 16/25 | Loss: 0.00097451
Iteration 17/25 | Loss: 0.00097451
Iteration 18/25 | Loss: 0.00097451
Iteration 19/25 | Loss: 0.00097451
Iteration 20/25 | Loss: 0.00097451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000974514870904386, 0.000974514870904386, 0.000974514870904386, 0.000974514870904386, 0.000974514870904386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000974514870904386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27187777
Iteration 2/25 | Loss: 0.00194464
Iteration 3/25 | Loss: 0.00194462
Iteration 4/25 | Loss: 0.00194462
Iteration 5/25 | Loss: 0.00194462
Iteration 6/25 | Loss: 0.00194462
Iteration 7/25 | Loss: 0.00194462
Iteration 8/25 | Loss: 0.00194462
Iteration 9/25 | Loss: 0.00194462
Iteration 10/25 | Loss: 0.00194462
Iteration 11/25 | Loss: 0.00194462
Iteration 12/25 | Loss: 0.00194462
Iteration 13/25 | Loss: 0.00194462
Iteration 14/25 | Loss: 0.00194462
Iteration 15/25 | Loss: 0.00194462
Iteration 16/25 | Loss: 0.00194462
Iteration 17/25 | Loss: 0.00194462
Iteration 18/25 | Loss: 0.00194462
Iteration 19/25 | Loss: 0.00194462
Iteration 20/25 | Loss: 0.00194462
Iteration 21/25 | Loss: 0.00194462
Iteration 22/25 | Loss: 0.00194462
Iteration 23/25 | Loss: 0.00194462
Iteration 24/25 | Loss: 0.00194462
Iteration 25/25 | Loss: 0.00194462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0019446173682808876, 0.0019446173682808876, 0.0019446173682808876, 0.0019446173682808876, 0.0019446173682808876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019446173682808876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194462
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00001420
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001214
Iteration 6/1000 | Loss: 0.00001159
Iteration 7/1000 | Loss: 0.00001111
Iteration 8/1000 | Loss: 0.00001083
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001021
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001019
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001014
Iteration 25/1000 | Loss: 0.00001013
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001013
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001012
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001010
Iteration 42/1000 | Loss: 0.00001010
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001008
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001007
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001005
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001004
Iteration 74/1000 | Loss: 0.00001004
Iteration 75/1000 | Loss: 0.00001004
Iteration 76/1000 | Loss: 0.00001004
Iteration 77/1000 | Loss: 0.00001004
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001004
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001003
Iteration 84/1000 | Loss: 0.00001003
Iteration 85/1000 | Loss: 0.00001003
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001003
Iteration 88/1000 | Loss: 0.00001003
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001003
Iteration 92/1000 | Loss: 0.00001003
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001003
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001002
Iteration 98/1000 | Loss: 0.00001002
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001002
Iteration 102/1000 | Loss: 0.00001002
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001002
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001002
Iteration 110/1000 | Loss: 0.00001002
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001001
Iteration 113/1000 | Loss: 0.00001001
Iteration 114/1000 | Loss: 0.00001001
Iteration 115/1000 | Loss: 0.00001001
Iteration 116/1000 | Loss: 0.00001001
Iteration 117/1000 | Loss: 0.00001001
Iteration 118/1000 | Loss: 0.00001001
Iteration 119/1000 | Loss: 0.00001001
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001000
Iteration 123/1000 | Loss: 0.00001000
Iteration 124/1000 | Loss: 0.00001000
Iteration 125/1000 | Loss: 0.00001000
Iteration 126/1000 | Loss: 0.00001000
Iteration 127/1000 | Loss: 0.00001000
Iteration 128/1000 | Loss: 0.00001000
Iteration 129/1000 | Loss: 0.00001000
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [9.997204870160203e-06, 9.997204870160203e-06, 9.997204870160203e-06, 9.997204870160203e-06, 9.997204870160203e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.997204870160203e-06

Optimization complete. Final v2v error: 2.6584792137145996 mm

Highest mean error: 3.0900068283081055 mm for frame 86

Lowest mean error: 2.3245365619659424 mm for frame 54

Saving results

Total time: 36.23052096366882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028468
Iteration 2/25 | Loss: 0.00251530
Iteration 3/25 | Loss: 0.00213704
Iteration 4/25 | Loss: 0.00111464
Iteration 5/25 | Loss: 0.00108275
Iteration 6/25 | Loss: 0.00107223
Iteration 7/25 | Loss: 0.00106814
Iteration 8/25 | Loss: 0.00106566
Iteration 9/25 | Loss: 0.00106485
Iteration 10/25 | Loss: 0.00106468
Iteration 11/25 | Loss: 0.00106468
Iteration 12/25 | Loss: 0.00106468
Iteration 13/25 | Loss: 0.00106468
Iteration 14/25 | Loss: 0.00106468
Iteration 15/25 | Loss: 0.00106468
Iteration 16/25 | Loss: 0.00106468
Iteration 17/25 | Loss: 0.00106468
Iteration 18/25 | Loss: 0.00106468
Iteration 19/25 | Loss: 0.00106468
Iteration 20/25 | Loss: 0.00106468
Iteration 21/25 | Loss: 0.00106468
Iteration 22/25 | Loss: 0.00106468
Iteration 23/25 | Loss: 0.00106468
Iteration 24/25 | Loss: 0.00106468
Iteration 25/25 | Loss: 0.00106468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16666234
Iteration 2/25 | Loss: 0.00184289
Iteration 3/25 | Loss: 0.00184289
Iteration 4/25 | Loss: 0.00184289
Iteration 5/25 | Loss: 0.00184289
Iteration 6/25 | Loss: 0.00184289
Iteration 7/25 | Loss: 0.00184289
Iteration 8/25 | Loss: 0.00184289
Iteration 9/25 | Loss: 0.00184289
Iteration 10/25 | Loss: 0.00184289
Iteration 11/25 | Loss: 0.00184289
Iteration 12/25 | Loss: 0.00184289
Iteration 13/25 | Loss: 0.00184289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0018428860930725932, 0.0018428860930725932, 0.0018428860930725932, 0.0018428860930725932, 0.0018428860930725932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018428860930725932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184289
Iteration 2/1000 | Loss: 0.00003520
Iteration 3/1000 | Loss: 0.00024747
Iteration 4/1000 | Loss: 0.00002951
Iteration 5/1000 | Loss: 0.00003379
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00020533
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00004942
Iteration 16/1000 | Loss: 0.00006914
Iteration 17/1000 | Loss: 0.00003977
Iteration 18/1000 | Loss: 0.00002613
Iteration 19/1000 | Loss: 0.00016806
Iteration 20/1000 | Loss: 0.00005865
Iteration 21/1000 | Loss: 0.00004657
Iteration 22/1000 | Loss: 0.00002105
Iteration 23/1000 | Loss: 0.00003947
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00004244
Iteration 26/1000 | Loss: 0.00002235
Iteration 27/1000 | Loss: 0.00004620
Iteration 28/1000 | Loss: 0.00002540
Iteration 29/1000 | Loss: 0.00002703
Iteration 30/1000 | Loss: 0.00002369
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001686
Iteration 39/1000 | Loss: 0.00001686
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001686
Iteration 43/1000 | Loss: 0.00001686
Iteration 44/1000 | Loss: 0.00001685
Iteration 45/1000 | Loss: 0.00001685
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001682
Iteration 54/1000 | Loss: 0.00001682
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001681
Iteration 60/1000 | Loss: 0.00001681
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001681
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001681
Iteration 71/1000 | Loss: 0.00001681
Iteration 72/1000 | Loss: 0.00001681
Iteration 73/1000 | Loss: 0.00001681
Iteration 74/1000 | Loss: 0.00001681
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001680
Iteration 82/1000 | Loss: 0.00001679
Iteration 83/1000 | Loss: 0.00001679
Iteration 84/1000 | Loss: 0.00001679
Iteration 85/1000 | Loss: 0.00001679
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001678
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001677
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001677
Iteration 108/1000 | Loss: 0.00001677
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001677
Iteration 117/1000 | Loss: 0.00001677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.6766516637289897e-05, 1.6766516637289897e-05, 1.6766516637289897e-05, 1.6766516637289897e-05, 1.6766516637289897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6766516637289897e-05

Optimization complete. Final v2v error: 3.2019412517547607 mm

Highest mean error: 9.708130836486816 mm for frame 92

Lowest mean error: 2.9190738201141357 mm for frame 17

Saving results

Total time: 65.07151341438293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861806
Iteration 2/25 | Loss: 0.00109991
Iteration 3/25 | Loss: 0.00096555
Iteration 4/25 | Loss: 0.00095267
Iteration 5/25 | Loss: 0.00095149
Iteration 6/25 | Loss: 0.00095149
Iteration 7/25 | Loss: 0.00095149
Iteration 8/25 | Loss: 0.00095149
Iteration 9/25 | Loss: 0.00095149
Iteration 10/25 | Loss: 0.00095149
Iteration 11/25 | Loss: 0.00095149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009514932753518224, 0.0009514932753518224, 0.0009514932753518224, 0.0009514932753518224, 0.0009514932753518224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009514932753518224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20405865
Iteration 2/25 | Loss: 0.00193646
Iteration 3/25 | Loss: 0.00193646
Iteration 4/25 | Loss: 0.00193646
Iteration 5/25 | Loss: 0.00193646
Iteration 6/25 | Loss: 0.00193646
Iteration 7/25 | Loss: 0.00193646
Iteration 8/25 | Loss: 0.00193646
Iteration 9/25 | Loss: 0.00193646
Iteration 10/25 | Loss: 0.00193646
Iteration 11/25 | Loss: 0.00193646
Iteration 12/25 | Loss: 0.00193646
Iteration 13/25 | Loss: 0.00193646
Iteration 14/25 | Loss: 0.00193646
Iteration 15/25 | Loss: 0.00193646
Iteration 16/25 | Loss: 0.00193646
Iteration 17/25 | Loss: 0.00193646
Iteration 18/25 | Loss: 0.00193646
Iteration 19/25 | Loss: 0.00193646
Iteration 20/25 | Loss: 0.00193646
Iteration 21/25 | Loss: 0.00193646
Iteration 22/25 | Loss: 0.00193646
Iteration 23/25 | Loss: 0.00193646
Iteration 24/25 | Loss: 0.00193646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019364551408216357, 0.0019364551408216357, 0.0019364551408216357, 0.0019364551408216357, 0.0019364551408216357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019364551408216357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193646
Iteration 2/1000 | Loss: 0.00001716
Iteration 3/1000 | Loss: 0.00001046
Iteration 4/1000 | Loss: 0.00000916
Iteration 5/1000 | Loss: 0.00000841
Iteration 6/1000 | Loss: 0.00000789
Iteration 7/1000 | Loss: 0.00000762
Iteration 8/1000 | Loss: 0.00000750
Iteration 9/1000 | Loss: 0.00000740
Iteration 10/1000 | Loss: 0.00000737
Iteration 11/1000 | Loss: 0.00000723
Iteration 12/1000 | Loss: 0.00000718
Iteration 13/1000 | Loss: 0.00000717
Iteration 14/1000 | Loss: 0.00000716
Iteration 15/1000 | Loss: 0.00000713
Iteration 16/1000 | Loss: 0.00000713
Iteration 17/1000 | Loss: 0.00000712
Iteration 18/1000 | Loss: 0.00000711
Iteration 19/1000 | Loss: 0.00000710
Iteration 20/1000 | Loss: 0.00000710
Iteration 21/1000 | Loss: 0.00000710
Iteration 22/1000 | Loss: 0.00000707
Iteration 23/1000 | Loss: 0.00000704
Iteration 24/1000 | Loss: 0.00000704
Iteration 25/1000 | Loss: 0.00000703
Iteration 26/1000 | Loss: 0.00000703
Iteration 27/1000 | Loss: 0.00000702
Iteration 28/1000 | Loss: 0.00000701
Iteration 29/1000 | Loss: 0.00000700
Iteration 30/1000 | Loss: 0.00000699
Iteration 31/1000 | Loss: 0.00000697
Iteration 32/1000 | Loss: 0.00000697
Iteration 33/1000 | Loss: 0.00000697
Iteration 34/1000 | Loss: 0.00000697
Iteration 35/1000 | Loss: 0.00000696
Iteration 36/1000 | Loss: 0.00000696
Iteration 37/1000 | Loss: 0.00000696
Iteration 38/1000 | Loss: 0.00000696
Iteration 39/1000 | Loss: 0.00000696
Iteration 40/1000 | Loss: 0.00000695
Iteration 41/1000 | Loss: 0.00000694
Iteration 42/1000 | Loss: 0.00000694
Iteration 43/1000 | Loss: 0.00000693
Iteration 44/1000 | Loss: 0.00000693
Iteration 45/1000 | Loss: 0.00000692
Iteration 46/1000 | Loss: 0.00000692
Iteration 47/1000 | Loss: 0.00000692
Iteration 48/1000 | Loss: 0.00000691
Iteration 49/1000 | Loss: 0.00000691
Iteration 50/1000 | Loss: 0.00000690
Iteration 51/1000 | Loss: 0.00000690
Iteration 52/1000 | Loss: 0.00000689
Iteration 53/1000 | Loss: 0.00000689
Iteration 54/1000 | Loss: 0.00000688
Iteration 55/1000 | Loss: 0.00000688
Iteration 56/1000 | Loss: 0.00000685
Iteration 57/1000 | Loss: 0.00000685
Iteration 58/1000 | Loss: 0.00000685
Iteration 59/1000 | Loss: 0.00000685
Iteration 60/1000 | Loss: 0.00000685
Iteration 61/1000 | Loss: 0.00000685
Iteration 62/1000 | Loss: 0.00000684
Iteration 63/1000 | Loss: 0.00000683
Iteration 64/1000 | Loss: 0.00000682
Iteration 65/1000 | Loss: 0.00000681
Iteration 66/1000 | Loss: 0.00000681
Iteration 67/1000 | Loss: 0.00000681
Iteration 68/1000 | Loss: 0.00000681
Iteration 69/1000 | Loss: 0.00000681
Iteration 70/1000 | Loss: 0.00000681
Iteration 71/1000 | Loss: 0.00000681
Iteration 72/1000 | Loss: 0.00000681
Iteration 73/1000 | Loss: 0.00000680
Iteration 74/1000 | Loss: 0.00000680
Iteration 75/1000 | Loss: 0.00000680
Iteration 76/1000 | Loss: 0.00000680
Iteration 77/1000 | Loss: 0.00000679
Iteration 78/1000 | Loss: 0.00000679
Iteration 79/1000 | Loss: 0.00000678
Iteration 80/1000 | Loss: 0.00000678
Iteration 81/1000 | Loss: 0.00000678
Iteration 82/1000 | Loss: 0.00000678
Iteration 83/1000 | Loss: 0.00000678
Iteration 84/1000 | Loss: 0.00000678
Iteration 85/1000 | Loss: 0.00000678
Iteration 86/1000 | Loss: 0.00000678
Iteration 87/1000 | Loss: 0.00000678
Iteration 88/1000 | Loss: 0.00000678
Iteration 89/1000 | Loss: 0.00000678
Iteration 90/1000 | Loss: 0.00000678
Iteration 91/1000 | Loss: 0.00000678
Iteration 92/1000 | Loss: 0.00000678
Iteration 93/1000 | Loss: 0.00000678
Iteration 94/1000 | Loss: 0.00000678
Iteration 95/1000 | Loss: 0.00000678
Iteration 96/1000 | Loss: 0.00000678
Iteration 97/1000 | Loss: 0.00000678
Iteration 98/1000 | Loss: 0.00000678
Iteration 99/1000 | Loss: 0.00000678
Iteration 100/1000 | Loss: 0.00000678
Iteration 101/1000 | Loss: 0.00000678
Iteration 102/1000 | Loss: 0.00000678
Iteration 103/1000 | Loss: 0.00000678
Iteration 104/1000 | Loss: 0.00000678
Iteration 105/1000 | Loss: 0.00000678
Iteration 106/1000 | Loss: 0.00000678
Iteration 107/1000 | Loss: 0.00000678
Iteration 108/1000 | Loss: 0.00000678
Iteration 109/1000 | Loss: 0.00000678
Iteration 110/1000 | Loss: 0.00000678
Iteration 111/1000 | Loss: 0.00000678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [6.780885087209754e-06, 6.780885087209754e-06, 6.780885087209754e-06, 6.780885087209754e-06, 6.780885087209754e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.780885087209754e-06

Optimization complete. Final v2v error: 2.1871557235717773 mm

Highest mean error: 2.3924190998077393 mm for frame 30

Lowest mean error: 2.0250298976898193 mm for frame 237

Saving results

Total time: 33.05388569831848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00734987
Iteration 2/25 | Loss: 0.00149204
Iteration 3/25 | Loss: 0.00117220
Iteration 4/25 | Loss: 0.00111336
Iteration 5/25 | Loss: 0.00110018
Iteration 6/25 | Loss: 0.00109258
Iteration 7/25 | Loss: 0.00111000
Iteration 8/25 | Loss: 0.00108477
Iteration 9/25 | Loss: 0.00107552
Iteration 10/25 | Loss: 0.00107430
Iteration 11/25 | Loss: 0.00107060
Iteration 12/25 | Loss: 0.00106875
Iteration 13/25 | Loss: 0.00106705
Iteration 14/25 | Loss: 0.00106598
Iteration 15/25 | Loss: 0.00106572
Iteration 16/25 | Loss: 0.00106558
Iteration 17/25 | Loss: 0.00106543
Iteration 18/25 | Loss: 0.00106543
Iteration 19/25 | Loss: 0.00106543
Iteration 20/25 | Loss: 0.00106543
Iteration 21/25 | Loss: 0.00106543
Iteration 22/25 | Loss: 0.00106542
Iteration 23/25 | Loss: 0.00106542
Iteration 24/25 | Loss: 0.00106542
Iteration 25/25 | Loss: 0.00106542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.20064783
Iteration 2/25 | Loss: 0.00217376
Iteration 3/25 | Loss: 0.00217376
Iteration 4/25 | Loss: 0.00217375
Iteration 5/25 | Loss: 0.00217375
Iteration 6/25 | Loss: 0.00217375
Iteration 7/25 | Loss: 0.00217375
Iteration 8/25 | Loss: 0.00217375
Iteration 9/25 | Loss: 0.00217375
Iteration 10/25 | Loss: 0.00217375
Iteration 11/25 | Loss: 0.00217375
Iteration 12/25 | Loss: 0.00217375
Iteration 13/25 | Loss: 0.00217375
Iteration 14/25 | Loss: 0.00217375
Iteration 15/25 | Loss: 0.00217375
Iteration 16/25 | Loss: 0.00217375
Iteration 17/25 | Loss: 0.00217375
Iteration 18/25 | Loss: 0.00217375
Iteration 19/25 | Loss: 0.00217375
Iteration 20/25 | Loss: 0.00217375
Iteration 21/25 | Loss: 0.00217375
Iteration 22/25 | Loss: 0.00217375
Iteration 23/25 | Loss: 0.00217375
Iteration 24/25 | Loss: 0.00217375
Iteration 25/25 | Loss: 0.00217375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217375
Iteration 2/1000 | Loss: 0.00008678
Iteration 3/1000 | Loss: 0.00005230
Iteration 4/1000 | Loss: 0.00003843
Iteration 5/1000 | Loss: 0.00003415
Iteration 6/1000 | Loss: 0.00003227
Iteration 7/1000 | Loss: 0.00071201
Iteration 8/1000 | Loss: 0.00033462
Iteration 9/1000 | Loss: 0.00062213
Iteration 10/1000 | Loss: 0.00022449
Iteration 11/1000 | Loss: 0.00059701
Iteration 12/1000 | Loss: 0.00028989
Iteration 13/1000 | Loss: 0.00003936
Iteration 14/1000 | Loss: 0.00003217
Iteration 15/1000 | Loss: 0.00069273
Iteration 16/1000 | Loss: 0.00033631
Iteration 17/1000 | Loss: 0.00005608
Iteration 18/1000 | Loss: 0.00004215
Iteration 19/1000 | Loss: 0.00003181
Iteration 20/1000 | Loss: 0.00069697
Iteration 21/1000 | Loss: 0.00011910
Iteration 22/1000 | Loss: 0.00003220
Iteration 23/1000 | Loss: 0.00002827
Iteration 24/1000 | Loss: 0.00002575
Iteration 25/1000 | Loss: 0.00002384
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00002235
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002137
Iteration 31/1000 | Loss: 0.00002105
Iteration 32/1000 | Loss: 0.00002090
Iteration 33/1000 | Loss: 0.00002086
Iteration 34/1000 | Loss: 0.00002085
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002084
Iteration 37/1000 | Loss: 0.00002079
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002076
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002073
Iteration 45/1000 | Loss: 0.00002072
Iteration 46/1000 | Loss: 0.00002071
Iteration 47/1000 | Loss: 0.00002071
Iteration 48/1000 | Loss: 0.00002071
Iteration 49/1000 | Loss: 0.00002071
Iteration 50/1000 | Loss: 0.00002070
Iteration 51/1000 | Loss: 0.00002069
Iteration 52/1000 | Loss: 0.00002069
Iteration 53/1000 | Loss: 0.00002069
Iteration 54/1000 | Loss: 0.00002068
Iteration 55/1000 | Loss: 0.00002067
Iteration 56/1000 | Loss: 0.00002066
Iteration 57/1000 | Loss: 0.00002066
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002065
Iteration 60/1000 | Loss: 0.00002062
Iteration 61/1000 | Loss: 0.00002062
Iteration 62/1000 | Loss: 0.00002059
Iteration 63/1000 | Loss: 0.00002059
Iteration 64/1000 | Loss: 0.00002059
Iteration 65/1000 | Loss: 0.00002058
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002057
Iteration 69/1000 | Loss: 0.00002057
Iteration 70/1000 | Loss: 0.00002056
Iteration 71/1000 | Loss: 0.00002055
Iteration 72/1000 | Loss: 0.00002055
Iteration 73/1000 | Loss: 0.00002053
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002050
Iteration 76/1000 | Loss: 0.00002049
Iteration 77/1000 | Loss: 0.00002049
Iteration 78/1000 | Loss: 0.00002049
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002047
Iteration 85/1000 | Loss: 0.00002047
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002045
Iteration 90/1000 | Loss: 0.00002045
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002044
Iteration 93/1000 | Loss: 0.00002044
Iteration 94/1000 | Loss: 0.00002044
Iteration 95/1000 | Loss: 0.00002044
Iteration 96/1000 | Loss: 0.00002044
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002043
Iteration 99/1000 | Loss: 0.00002043
Iteration 100/1000 | Loss: 0.00002043
Iteration 101/1000 | Loss: 0.00002043
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002040
Iteration 115/1000 | Loss: 0.00002040
Iteration 116/1000 | Loss: 0.00002040
Iteration 117/1000 | Loss: 0.00002040
Iteration 118/1000 | Loss: 0.00002040
Iteration 119/1000 | Loss: 0.00002040
Iteration 120/1000 | Loss: 0.00002039
Iteration 121/1000 | Loss: 0.00002039
Iteration 122/1000 | Loss: 0.00002038
Iteration 123/1000 | Loss: 0.00002038
Iteration 124/1000 | Loss: 0.00002038
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00002037
Iteration 127/1000 | Loss: 0.00002037
Iteration 128/1000 | Loss: 0.00002037
Iteration 129/1000 | Loss: 0.00002036
Iteration 130/1000 | Loss: 0.00002036
Iteration 131/1000 | Loss: 0.00002036
Iteration 132/1000 | Loss: 0.00002036
Iteration 133/1000 | Loss: 0.00002036
Iteration 134/1000 | Loss: 0.00002035
Iteration 135/1000 | Loss: 0.00002035
Iteration 136/1000 | Loss: 0.00002035
Iteration 137/1000 | Loss: 0.00002035
Iteration 138/1000 | Loss: 0.00002035
Iteration 139/1000 | Loss: 0.00002035
Iteration 140/1000 | Loss: 0.00002035
Iteration 141/1000 | Loss: 0.00002034
Iteration 142/1000 | Loss: 0.00002034
Iteration 143/1000 | Loss: 0.00002034
Iteration 144/1000 | Loss: 0.00002034
Iteration 145/1000 | Loss: 0.00002034
Iteration 146/1000 | Loss: 0.00002034
Iteration 147/1000 | Loss: 0.00002033
Iteration 148/1000 | Loss: 0.00002033
Iteration 149/1000 | Loss: 0.00002033
Iteration 150/1000 | Loss: 0.00002033
Iteration 151/1000 | Loss: 0.00002033
Iteration 152/1000 | Loss: 0.00002033
Iteration 153/1000 | Loss: 0.00002033
Iteration 154/1000 | Loss: 0.00002033
Iteration 155/1000 | Loss: 0.00002033
Iteration 156/1000 | Loss: 0.00002033
Iteration 157/1000 | Loss: 0.00002033
Iteration 158/1000 | Loss: 0.00002033
Iteration 159/1000 | Loss: 0.00002033
Iteration 160/1000 | Loss: 0.00002033
Iteration 161/1000 | Loss: 0.00002033
Iteration 162/1000 | Loss: 0.00002033
Iteration 163/1000 | Loss: 0.00002033
Iteration 164/1000 | Loss: 0.00002033
Iteration 165/1000 | Loss: 0.00002033
Iteration 166/1000 | Loss: 0.00002033
Iteration 167/1000 | Loss: 0.00002033
Iteration 168/1000 | Loss: 0.00002033
Iteration 169/1000 | Loss: 0.00002033
Iteration 170/1000 | Loss: 0.00002033
Iteration 171/1000 | Loss: 0.00002033
Iteration 172/1000 | Loss: 0.00002033
Iteration 173/1000 | Loss: 0.00002033
Iteration 174/1000 | Loss: 0.00002033
Iteration 175/1000 | Loss: 0.00002033
Iteration 176/1000 | Loss: 0.00002033
Iteration 177/1000 | Loss: 0.00002033
Iteration 178/1000 | Loss: 0.00002033
Iteration 179/1000 | Loss: 0.00002033
Iteration 180/1000 | Loss: 0.00002033
Iteration 181/1000 | Loss: 0.00002033
Iteration 182/1000 | Loss: 0.00002033
Iteration 183/1000 | Loss: 0.00002033
Iteration 184/1000 | Loss: 0.00002033
Iteration 185/1000 | Loss: 0.00002033
Iteration 186/1000 | Loss: 0.00002033
Iteration 187/1000 | Loss: 0.00002033
Iteration 188/1000 | Loss: 0.00002033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.0329975086497143e-05, 2.0329975086497143e-05, 2.0329975086497143e-05, 2.0329975086497143e-05, 2.0329975086497143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0329975086497143e-05

Optimization complete. Final v2v error: 3.6134090423583984 mm

Highest mean error: 5.66920280456543 mm for frame 64

Lowest mean error: 2.48002552986145 mm for frame 0

Saving results

Total time: 90.62498879432678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00561331
Iteration 2/25 | Loss: 0.00114880
Iteration 3/25 | Loss: 0.00102890
Iteration 4/25 | Loss: 0.00102164
Iteration 5/25 | Loss: 0.00102067
Iteration 6/25 | Loss: 0.00102067
Iteration 7/25 | Loss: 0.00102067
Iteration 8/25 | Loss: 0.00102067
Iteration 9/25 | Loss: 0.00102067
Iteration 10/25 | Loss: 0.00102067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010206658625975251, 0.0010206658625975251, 0.0010206658625975251, 0.0010206658625975251, 0.0010206658625975251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010206658625975251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16792297
Iteration 2/25 | Loss: 0.00196263
Iteration 3/25 | Loss: 0.00196263
Iteration 4/25 | Loss: 0.00196263
Iteration 5/25 | Loss: 0.00196263
Iteration 6/25 | Loss: 0.00196263
Iteration 7/25 | Loss: 0.00196263
Iteration 8/25 | Loss: 0.00196263
Iteration 9/25 | Loss: 0.00196263
Iteration 10/25 | Loss: 0.00196263
Iteration 11/25 | Loss: 0.00196263
Iteration 12/25 | Loss: 0.00196263
Iteration 13/25 | Loss: 0.00196263
Iteration 14/25 | Loss: 0.00196263
Iteration 15/25 | Loss: 0.00196263
Iteration 16/25 | Loss: 0.00196263
Iteration 17/25 | Loss: 0.00196263
Iteration 18/25 | Loss: 0.00196263
Iteration 19/25 | Loss: 0.00196263
Iteration 20/25 | Loss: 0.00196263
Iteration 21/25 | Loss: 0.00196263
Iteration 22/25 | Loss: 0.00196263
Iteration 23/25 | Loss: 0.00196263
Iteration 24/25 | Loss: 0.00196263
Iteration 25/25 | Loss: 0.00196263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196263
Iteration 2/1000 | Loss: 0.00003403
Iteration 3/1000 | Loss: 0.00001694
Iteration 4/1000 | Loss: 0.00001369
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00001160
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001090
Iteration 9/1000 | Loss: 0.00001066
Iteration 10/1000 | Loss: 0.00001038
Iteration 11/1000 | Loss: 0.00001011
Iteration 12/1000 | Loss: 0.00000991
Iteration 13/1000 | Loss: 0.00000990
Iteration 14/1000 | Loss: 0.00000990
Iteration 15/1000 | Loss: 0.00000989
Iteration 16/1000 | Loss: 0.00000985
Iteration 17/1000 | Loss: 0.00000984
Iteration 18/1000 | Loss: 0.00000983
Iteration 19/1000 | Loss: 0.00000983
Iteration 20/1000 | Loss: 0.00000980
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000977
Iteration 24/1000 | Loss: 0.00000977
Iteration 25/1000 | Loss: 0.00000974
Iteration 26/1000 | Loss: 0.00000973
Iteration 27/1000 | Loss: 0.00000972
Iteration 28/1000 | Loss: 0.00000972
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000971
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000968
Iteration 34/1000 | Loss: 0.00000967
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000966
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000965
Iteration 40/1000 | Loss: 0.00000965
Iteration 41/1000 | Loss: 0.00000964
Iteration 42/1000 | Loss: 0.00000964
Iteration 43/1000 | Loss: 0.00000964
Iteration 44/1000 | Loss: 0.00000964
Iteration 45/1000 | Loss: 0.00000963
Iteration 46/1000 | Loss: 0.00000963
Iteration 47/1000 | Loss: 0.00000963
Iteration 48/1000 | Loss: 0.00000963
Iteration 49/1000 | Loss: 0.00000963
Iteration 50/1000 | Loss: 0.00000963
Iteration 51/1000 | Loss: 0.00000963
Iteration 52/1000 | Loss: 0.00000963
Iteration 53/1000 | Loss: 0.00000963
Iteration 54/1000 | Loss: 0.00000963
Iteration 55/1000 | Loss: 0.00000962
Iteration 56/1000 | Loss: 0.00000962
Iteration 57/1000 | Loss: 0.00000961
Iteration 58/1000 | Loss: 0.00000961
Iteration 59/1000 | Loss: 0.00000961
Iteration 60/1000 | Loss: 0.00000961
Iteration 61/1000 | Loss: 0.00000961
Iteration 62/1000 | Loss: 0.00000961
Iteration 63/1000 | Loss: 0.00000960
Iteration 64/1000 | Loss: 0.00000960
Iteration 65/1000 | Loss: 0.00000958
Iteration 66/1000 | Loss: 0.00000958
Iteration 67/1000 | Loss: 0.00000958
Iteration 68/1000 | Loss: 0.00000958
Iteration 69/1000 | Loss: 0.00000957
Iteration 70/1000 | Loss: 0.00000957
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000954
Iteration 79/1000 | Loss: 0.00000954
Iteration 80/1000 | Loss: 0.00000953
Iteration 81/1000 | Loss: 0.00000953
Iteration 82/1000 | Loss: 0.00000953
Iteration 83/1000 | Loss: 0.00000952
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000952
Iteration 86/1000 | Loss: 0.00000952
Iteration 87/1000 | Loss: 0.00000952
Iteration 88/1000 | Loss: 0.00000952
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000951
Iteration 97/1000 | Loss: 0.00000951
Iteration 98/1000 | Loss: 0.00000951
Iteration 99/1000 | Loss: 0.00000950
Iteration 100/1000 | Loss: 0.00000950
Iteration 101/1000 | Loss: 0.00000950
Iteration 102/1000 | Loss: 0.00000950
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000948
Iteration 109/1000 | Loss: 0.00000948
Iteration 110/1000 | Loss: 0.00000948
Iteration 111/1000 | Loss: 0.00000948
Iteration 112/1000 | Loss: 0.00000948
Iteration 113/1000 | Loss: 0.00000947
Iteration 114/1000 | Loss: 0.00000947
Iteration 115/1000 | Loss: 0.00000947
Iteration 116/1000 | Loss: 0.00000947
Iteration 117/1000 | Loss: 0.00000947
Iteration 118/1000 | Loss: 0.00000946
Iteration 119/1000 | Loss: 0.00000946
Iteration 120/1000 | Loss: 0.00000946
Iteration 121/1000 | Loss: 0.00000946
Iteration 122/1000 | Loss: 0.00000946
Iteration 123/1000 | Loss: 0.00000945
Iteration 124/1000 | Loss: 0.00000945
Iteration 125/1000 | Loss: 0.00000945
Iteration 126/1000 | Loss: 0.00000945
Iteration 127/1000 | Loss: 0.00000944
Iteration 128/1000 | Loss: 0.00000944
Iteration 129/1000 | Loss: 0.00000944
Iteration 130/1000 | Loss: 0.00000944
Iteration 131/1000 | Loss: 0.00000944
Iteration 132/1000 | Loss: 0.00000944
Iteration 133/1000 | Loss: 0.00000944
Iteration 134/1000 | Loss: 0.00000944
Iteration 135/1000 | Loss: 0.00000944
Iteration 136/1000 | Loss: 0.00000944
Iteration 137/1000 | Loss: 0.00000944
Iteration 138/1000 | Loss: 0.00000943
Iteration 139/1000 | Loss: 0.00000943
Iteration 140/1000 | Loss: 0.00000943
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000942
Iteration 144/1000 | Loss: 0.00000942
Iteration 145/1000 | Loss: 0.00000942
Iteration 146/1000 | Loss: 0.00000942
Iteration 147/1000 | Loss: 0.00000942
Iteration 148/1000 | Loss: 0.00000942
Iteration 149/1000 | Loss: 0.00000942
Iteration 150/1000 | Loss: 0.00000941
Iteration 151/1000 | Loss: 0.00000941
Iteration 152/1000 | Loss: 0.00000941
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000941
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000941
Iteration 158/1000 | Loss: 0.00000941
Iteration 159/1000 | Loss: 0.00000941
Iteration 160/1000 | Loss: 0.00000941
Iteration 161/1000 | Loss: 0.00000941
Iteration 162/1000 | Loss: 0.00000941
Iteration 163/1000 | Loss: 0.00000941
Iteration 164/1000 | Loss: 0.00000941
Iteration 165/1000 | Loss: 0.00000941
Iteration 166/1000 | Loss: 0.00000941
Iteration 167/1000 | Loss: 0.00000941
Iteration 168/1000 | Loss: 0.00000941
Iteration 169/1000 | Loss: 0.00000941
Iteration 170/1000 | Loss: 0.00000941
Iteration 171/1000 | Loss: 0.00000941
Iteration 172/1000 | Loss: 0.00000941
Iteration 173/1000 | Loss: 0.00000941
Iteration 174/1000 | Loss: 0.00000941
Iteration 175/1000 | Loss: 0.00000941
Iteration 176/1000 | Loss: 0.00000941
Iteration 177/1000 | Loss: 0.00000941
Iteration 178/1000 | Loss: 0.00000941
Iteration 179/1000 | Loss: 0.00000941
Iteration 180/1000 | Loss: 0.00000941
Iteration 181/1000 | Loss: 0.00000941
Iteration 182/1000 | Loss: 0.00000941
Iteration 183/1000 | Loss: 0.00000941
Iteration 184/1000 | Loss: 0.00000941
Iteration 185/1000 | Loss: 0.00000941
Iteration 186/1000 | Loss: 0.00000941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [9.406392564415e-06, 9.406392564415e-06, 9.406392564415e-06, 9.406392564415e-06, 9.406392564415e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.406392564415e-06

Optimization complete. Final v2v error: 2.564129114151001 mm

Highest mean error: 2.7147607803344727 mm for frame 160

Lowest mean error: 2.448455810546875 mm for frame 183

Saving results

Total time: 41.44365406036377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029866
Iteration 2/25 | Loss: 0.00271573
Iteration 3/25 | Loss: 0.00191984
Iteration 4/25 | Loss: 0.00148895
Iteration 5/25 | Loss: 0.00140744
Iteration 6/25 | Loss: 0.00133627
Iteration 7/25 | Loss: 0.00124239
Iteration 8/25 | Loss: 0.00121533
Iteration 9/25 | Loss: 0.00120089
Iteration 10/25 | Loss: 0.00119564
Iteration 11/25 | Loss: 0.00118693
Iteration 12/25 | Loss: 0.00118473
Iteration 13/25 | Loss: 0.00117976
Iteration 14/25 | Loss: 0.00118043
Iteration 15/25 | Loss: 0.00117590
Iteration 16/25 | Loss: 0.00117459
Iteration 17/25 | Loss: 0.00117420
Iteration 18/25 | Loss: 0.00117409
Iteration 19/25 | Loss: 0.00117408
Iteration 20/25 | Loss: 0.00117408
Iteration 21/25 | Loss: 0.00117407
Iteration 22/25 | Loss: 0.00117407
Iteration 23/25 | Loss: 0.00117407
Iteration 24/25 | Loss: 0.00117407
Iteration 25/25 | Loss: 0.00117407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15185606
Iteration 2/25 | Loss: 0.00195334
Iteration 3/25 | Loss: 0.00195334
Iteration 4/25 | Loss: 0.00195333
Iteration 5/25 | Loss: 0.00195333
Iteration 6/25 | Loss: 0.00195333
Iteration 7/25 | Loss: 0.00195333
Iteration 8/25 | Loss: 0.00195333
Iteration 9/25 | Loss: 0.00195333
Iteration 10/25 | Loss: 0.00195333
Iteration 11/25 | Loss: 0.00195333
Iteration 12/25 | Loss: 0.00195333
Iteration 13/25 | Loss: 0.00195333
Iteration 14/25 | Loss: 0.00195333
Iteration 15/25 | Loss: 0.00195333
Iteration 16/25 | Loss: 0.00195333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001953332219272852, 0.001953332219272852, 0.001953332219272852, 0.001953332219272852, 0.001953332219272852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001953332219272852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195333
Iteration 2/1000 | Loss: 0.00005128
Iteration 3/1000 | Loss: 0.00003571
Iteration 4/1000 | Loss: 0.00003279
Iteration 5/1000 | Loss: 0.00003090
Iteration 6/1000 | Loss: 0.00002957
Iteration 7/1000 | Loss: 0.00002878
Iteration 8/1000 | Loss: 0.00002799
Iteration 9/1000 | Loss: 0.00013568
Iteration 10/1000 | Loss: 0.00002913
Iteration 11/1000 | Loss: 0.00002682
Iteration 12/1000 | Loss: 0.00002594
Iteration 13/1000 | Loss: 0.00002535
Iteration 14/1000 | Loss: 0.00002498
Iteration 15/1000 | Loss: 0.00002483
Iteration 16/1000 | Loss: 0.00002463
Iteration 17/1000 | Loss: 0.00002446
Iteration 18/1000 | Loss: 0.00002442
Iteration 19/1000 | Loss: 0.00004219
Iteration 20/1000 | Loss: 0.00002582
Iteration 21/1000 | Loss: 0.00002425
Iteration 22/1000 | Loss: 0.00002422
Iteration 23/1000 | Loss: 0.00002421
Iteration 24/1000 | Loss: 0.00002421
Iteration 25/1000 | Loss: 0.00002418
Iteration 26/1000 | Loss: 0.00002413
Iteration 27/1000 | Loss: 0.00002413
Iteration 28/1000 | Loss: 0.00002411
Iteration 29/1000 | Loss: 0.00002411
Iteration 30/1000 | Loss: 0.00002410
Iteration 31/1000 | Loss: 0.00002410
Iteration 32/1000 | Loss: 0.00002410
Iteration 33/1000 | Loss: 0.00002409
Iteration 34/1000 | Loss: 0.00002409
Iteration 35/1000 | Loss: 0.00002409
Iteration 36/1000 | Loss: 0.00002409
Iteration 37/1000 | Loss: 0.00002409
Iteration 38/1000 | Loss: 0.00002409
Iteration 39/1000 | Loss: 0.00002409
Iteration 40/1000 | Loss: 0.00002408
Iteration 41/1000 | Loss: 0.00002408
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002406
Iteration 44/1000 | Loss: 0.00002405
Iteration 45/1000 | Loss: 0.00002405
Iteration 46/1000 | Loss: 0.00002405
Iteration 47/1000 | Loss: 0.00002405
Iteration 48/1000 | Loss: 0.00002405
Iteration 49/1000 | Loss: 0.00002405
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00002404
Iteration 52/1000 | Loss: 0.00002404
Iteration 53/1000 | Loss: 0.00002403
Iteration 54/1000 | Loss: 0.00002403
Iteration 55/1000 | Loss: 0.00002403
Iteration 56/1000 | Loss: 0.00002402
Iteration 57/1000 | Loss: 0.00002402
Iteration 58/1000 | Loss: 0.00002402
Iteration 59/1000 | Loss: 0.00002402
Iteration 60/1000 | Loss: 0.00002402
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002400
Iteration 64/1000 | Loss: 0.00002400
Iteration 65/1000 | Loss: 0.00002400
Iteration 66/1000 | Loss: 0.00002400
Iteration 67/1000 | Loss: 0.00002400
Iteration 68/1000 | Loss: 0.00002399
Iteration 69/1000 | Loss: 0.00004164
Iteration 70/1000 | Loss: 0.00004163
Iteration 71/1000 | Loss: 0.00002538
Iteration 72/1000 | Loss: 0.00002399
Iteration 73/1000 | Loss: 0.00002398
Iteration 74/1000 | Loss: 0.00002398
Iteration 75/1000 | Loss: 0.00002398
Iteration 76/1000 | Loss: 0.00002398
Iteration 77/1000 | Loss: 0.00002398
Iteration 78/1000 | Loss: 0.00002398
Iteration 79/1000 | Loss: 0.00002398
Iteration 80/1000 | Loss: 0.00002397
Iteration 81/1000 | Loss: 0.00002397
Iteration 82/1000 | Loss: 0.00002397
Iteration 83/1000 | Loss: 0.00002397
Iteration 84/1000 | Loss: 0.00002397
Iteration 85/1000 | Loss: 0.00002397
Iteration 86/1000 | Loss: 0.00002397
Iteration 87/1000 | Loss: 0.00002397
Iteration 88/1000 | Loss: 0.00002397
Iteration 89/1000 | Loss: 0.00002397
Iteration 90/1000 | Loss: 0.00002397
Iteration 91/1000 | Loss: 0.00002397
Iteration 92/1000 | Loss: 0.00002396
Iteration 93/1000 | Loss: 0.00002396
Iteration 94/1000 | Loss: 0.00002396
Iteration 95/1000 | Loss: 0.00002396
Iteration 96/1000 | Loss: 0.00002396
Iteration 97/1000 | Loss: 0.00002396
Iteration 98/1000 | Loss: 0.00002396
Iteration 99/1000 | Loss: 0.00002396
Iteration 100/1000 | Loss: 0.00002396
Iteration 101/1000 | Loss: 0.00002396
Iteration 102/1000 | Loss: 0.00002396
Iteration 103/1000 | Loss: 0.00002396
Iteration 104/1000 | Loss: 0.00002396
Iteration 105/1000 | Loss: 0.00002396
Iteration 106/1000 | Loss: 0.00002396
Iteration 107/1000 | Loss: 0.00002396
Iteration 108/1000 | Loss: 0.00002396
Iteration 109/1000 | Loss: 0.00002396
Iteration 110/1000 | Loss: 0.00002396
Iteration 111/1000 | Loss: 0.00002396
Iteration 112/1000 | Loss: 0.00002396
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002396
Iteration 116/1000 | Loss: 0.00002396
Iteration 117/1000 | Loss: 0.00002396
Iteration 118/1000 | Loss: 0.00002396
Iteration 119/1000 | Loss: 0.00002396
Iteration 120/1000 | Loss: 0.00002396
Iteration 121/1000 | Loss: 0.00002396
Iteration 122/1000 | Loss: 0.00002396
Iteration 123/1000 | Loss: 0.00002396
Iteration 124/1000 | Loss: 0.00002396
Iteration 125/1000 | Loss: 0.00002396
Iteration 126/1000 | Loss: 0.00002396
Iteration 127/1000 | Loss: 0.00002396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.395612136751879e-05, 2.395612136751879e-05, 2.395612136751879e-05, 2.395612136751879e-05, 2.395612136751879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.395612136751879e-05

Optimization complete. Final v2v error: 4.086189270019531 mm

Highest mean error: 10.112152099609375 mm for frame 171

Lowest mean error: 3.782769203186035 mm for frame 48

Saving results

Total time: 80.36105918884277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00727203
Iteration 2/25 | Loss: 0.00146663
Iteration 3/25 | Loss: 0.00115062
Iteration 4/25 | Loss: 0.00112289
Iteration 5/25 | Loss: 0.00111815
Iteration 6/25 | Loss: 0.00111787
Iteration 7/25 | Loss: 0.00111787
Iteration 8/25 | Loss: 0.00111787
Iteration 9/25 | Loss: 0.00111787
Iteration 10/25 | Loss: 0.00111787
Iteration 11/25 | Loss: 0.00111787
Iteration 12/25 | Loss: 0.00111787
Iteration 13/25 | Loss: 0.00111787
Iteration 14/25 | Loss: 0.00111787
Iteration 15/25 | Loss: 0.00111787
Iteration 16/25 | Loss: 0.00111787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011178741697221994, 0.0011178741697221994, 0.0011178741697221994, 0.0011178741697221994, 0.0011178741697221994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011178741697221994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30584204
Iteration 2/25 | Loss: 0.00172712
Iteration 3/25 | Loss: 0.00172712
Iteration 4/25 | Loss: 0.00172712
Iteration 5/25 | Loss: 0.00172712
Iteration 6/25 | Loss: 0.00172712
Iteration 7/25 | Loss: 0.00172712
Iteration 8/25 | Loss: 0.00172712
Iteration 9/25 | Loss: 0.00172712
Iteration 10/25 | Loss: 0.00172712
Iteration 11/25 | Loss: 0.00172712
Iteration 12/25 | Loss: 0.00172712
Iteration 13/25 | Loss: 0.00172712
Iteration 14/25 | Loss: 0.00172712
Iteration 15/25 | Loss: 0.00172712
Iteration 16/25 | Loss: 0.00172712
Iteration 17/25 | Loss: 0.00172712
Iteration 18/25 | Loss: 0.00172712
Iteration 19/25 | Loss: 0.00172712
Iteration 20/25 | Loss: 0.00172712
Iteration 21/25 | Loss: 0.00172712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017271199030801654, 0.0017271199030801654, 0.0017271199030801654, 0.0017271199030801654, 0.0017271199030801654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017271199030801654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172712
Iteration 2/1000 | Loss: 0.00005278
Iteration 3/1000 | Loss: 0.00002882
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001960
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001933
Iteration 11/1000 | Loss: 0.00001930
Iteration 12/1000 | Loss: 0.00001928
Iteration 13/1000 | Loss: 0.00001921
Iteration 14/1000 | Loss: 0.00001917
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001895
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001880
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001876
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001872
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001871
Iteration 51/1000 | Loss: 0.00001871
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001870
Iteration 54/1000 | Loss: 0.00001870
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001865
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001864
Iteration 75/1000 | Loss: 0.00001864
Iteration 76/1000 | Loss: 0.00001864
Iteration 77/1000 | Loss: 0.00001864
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001863
Iteration 80/1000 | Loss: 0.00001863
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001861
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001860
Iteration 100/1000 | Loss: 0.00001860
Iteration 101/1000 | Loss: 0.00001860
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001856
Iteration 117/1000 | Loss: 0.00001856
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001856
Iteration 120/1000 | Loss: 0.00001856
Iteration 121/1000 | Loss: 0.00001856
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001855
Iteration 125/1000 | Loss: 0.00001855
Iteration 126/1000 | Loss: 0.00001855
Iteration 127/1000 | Loss: 0.00001855
Iteration 128/1000 | Loss: 0.00001855
Iteration 129/1000 | Loss: 0.00001855
Iteration 130/1000 | Loss: 0.00001854
Iteration 131/1000 | Loss: 0.00001854
Iteration 132/1000 | Loss: 0.00001854
Iteration 133/1000 | Loss: 0.00001854
Iteration 134/1000 | Loss: 0.00001854
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001853
Iteration 141/1000 | Loss: 0.00001853
Iteration 142/1000 | Loss: 0.00001853
Iteration 143/1000 | Loss: 0.00001853
Iteration 144/1000 | Loss: 0.00001853
Iteration 145/1000 | Loss: 0.00001853
Iteration 146/1000 | Loss: 0.00001853
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Iteration 149/1000 | Loss: 0.00001852
Iteration 150/1000 | Loss: 0.00001852
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001851
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001850
Iteration 160/1000 | Loss: 0.00001850
Iteration 161/1000 | Loss: 0.00001850
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001850
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001848
Iteration 168/1000 | Loss: 0.00001848
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001847
Iteration 172/1000 | Loss: 0.00001847
Iteration 173/1000 | Loss: 0.00001847
Iteration 174/1000 | Loss: 0.00001847
Iteration 175/1000 | Loss: 0.00001847
Iteration 176/1000 | Loss: 0.00001847
Iteration 177/1000 | Loss: 0.00001846
Iteration 178/1000 | Loss: 0.00001846
Iteration 179/1000 | Loss: 0.00001846
Iteration 180/1000 | Loss: 0.00001846
Iteration 181/1000 | Loss: 0.00001846
Iteration 182/1000 | Loss: 0.00001846
Iteration 183/1000 | Loss: 0.00001846
Iteration 184/1000 | Loss: 0.00001846
Iteration 185/1000 | Loss: 0.00001845
Iteration 186/1000 | Loss: 0.00001845
Iteration 187/1000 | Loss: 0.00001845
Iteration 188/1000 | Loss: 0.00001845
Iteration 189/1000 | Loss: 0.00001845
Iteration 190/1000 | Loss: 0.00001845
Iteration 191/1000 | Loss: 0.00001845
Iteration 192/1000 | Loss: 0.00001845
Iteration 193/1000 | Loss: 0.00001845
Iteration 194/1000 | Loss: 0.00001845
Iteration 195/1000 | Loss: 0.00001845
Iteration 196/1000 | Loss: 0.00001845
Iteration 197/1000 | Loss: 0.00001844
Iteration 198/1000 | Loss: 0.00001844
Iteration 199/1000 | Loss: 0.00001844
Iteration 200/1000 | Loss: 0.00001844
Iteration 201/1000 | Loss: 0.00001844
Iteration 202/1000 | Loss: 0.00001844
Iteration 203/1000 | Loss: 0.00001844
Iteration 204/1000 | Loss: 0.00001844
Iteration 205/1000 | Loss: 0.00001844
Iteration 206/1000 | Loss: 0.00001844
Iteration 207/1000 | Loss: 0.00001843
Iteration 208/1000 | Loss: 0.00001843
Iteration 209/1000 | Loss: 0.00001843
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Iteration 213/1000 | Loss: 0.00001843
Iteration 214/1000 | Loss: 0.00001843
Iteration 215/1000 | Loss: 0.00001843
Iteration 216/1000 | Loss: 0.00001843
Iteration 217/1000 | Loss: 0.00001843
Iteration 218/1000 | Loss: 0.00001842
Iteration 219/1000 | Loss: 0.00001842
Iteration 220/1000 | Loss: 0.00001842
Iteration 221/1000 | Loss: 0.00001842
Iteration 222/1000 | Loss: 0.00001842
Iteration 223/1000 | Loss: 0.00001842
Iteration 224/1000 | Loss: 0.00001842
Iteration 225/1000 | Loss: 0.00001841
Iteration 226/1000 | Loss: 0.00001841
Iteration 227/1000 | Loss: 0.00001841
Iteration 228/1000 | Loss: 0.00001841
Iteration 229/1000 | Loss: 0.00001841
Iteration 230/1000 | Loss: 0.00001841
Iteration 231/1000 | Loss: 0.00001841
Iteration 232/1000 | Loss: 0.00001841
Iteration 233/1000 | Loss: 0.00001841
Iteration 234/1000 | Loss: 0.00001841
Iteration 235/1000 | Loss: 0.00001841
Iteration 236/1000 | Loss: 0.00001841
Iteration 237/1000 | Loss: 0.00001841
Iteration 238/1000 | Loss: 0.00001841
Iteration 239/1000 | Loss: 0.00001841
Iteration 240/1000 | Loss: 0.00001841
Iteration 241/1000 | Loss: 0.00001841
Iteration 242/1000 | Loss: 0.00001841
Iteration 243/1000 | Loss: 0.00001841
Iteration 244/1000 | Loss: 0.00001841
Iteration 245/1000 | Loss: 0.00001841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.840677941800095e-05, 1.840677941800095e-05, 1.840677941800095e-05, 1.840677941800095e-05, 1.840677941800095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.840677941800095e-05

Optimization complete. Final v2v error: 3.549759864807129 mm

Highest mean error: 4.1439361572265625 mm for frame 22

Lowest mean error: 3.1035962104797363 mm for frame 1

Saving results

Total time: 49.6294150352478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444335
Iteration 2/25 | Loss: 0.00113552
Iteration 3/25 | Loss: 0.00099523
Iteration 4/25 | Loss: 0.00097901
Iteration 5/25 | Loss: 0.00097363
Iteration 6/25 | Loss: 0.00097181
Iteration 7/25 | Loss: 0.00097181
Iteration 8/25 | Loss: 0.00097181
Iteration 9/25 | Loss: 0.00097181
Iteration 10/25 | Loss: 0.00097181
Iteration 11/25 | Loss: 0.00097181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009718076908029616, 0.0009718076908029616, 0.0009718076908029616, 0.0009718076908029616, 0.0009718076908029616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009718076908029616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27588713
Iteration 2/25 | Loss: 0.00190343
Iteration 3/25 | Loss: 0.00190342
Iteration 4/25 | Loss: 0.00190342
Iteration 5/25 | Loss: 0.00190342
Iteration 6/25 | Loss: 0.00190342
Iteration 7/25 | Loss: 0.00190342
Iteration 8/25 | Loss: 0.00190342
Iteration 9/25 | Loss: 0.00190342
Iteration 10/25 | Loss: 0.00190342
Iteration 11/25 | Loss: 0.00190342
Iteration 12/25 | Loss: 0.00190342
Iteration 13/25 | Loss: 0.00190342
Iteration 14/25 | Loss: 0.00190342
Iteration 15/25 | Loss: 0.00190342
Iteration 16/25 | Loss: 0.00190342
Iteration 17/25 | Loss: 0.00190342
Iteration 18/25 | Loss: 0.00190342
Iteration 19/25 | Loss: 0.00190342
Iteration 20/25 | Loss: 0.00190342
Iteration 21/25 | Loss: 0.00190342
Iteration 22/25 | Loss: 0.00190342
Iteration 23/25 | Loss: 0.00190342
Iteration 24/25 | Loss: 0.00190342
Iteration 25/25 | Loss: 0.00190342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190342
Iteration 2/1000 | Loss: 0.00002085
Iteration 3/1000 | Loss: 0.00001421
Iteration 4/1000 | Loss: 0.00001315
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001213
Iteration 7/1000 | Loss: 0.00001191
Iteration 8/1000 | Loss: 0.00001168
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001125
Iteration 12/1000 | Loss: 0.00001125
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001123
Iteration 15/1000 | Loss: 0.00001122
Iteration 16/1000 | Loss: 0.00001122
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001116
Iteration 28/1000 | Loss: 0.00001116
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001115
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001115
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001100
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001098
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001095
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001090
Iteration 107/1000 | Loss: 0.00001090
Iteration 108/1000 | Loss: 0.00001090
Iteration 109/1000 | Loss: 0.00001090
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001089
Iteration 130/1000 | Loss: 0.00001089
Iteration 131/1000 | Loss: 0.00001089
Iteration 132/1000 | Loss: 0.00001089
Iteration 133/1000 | Loss: 0.00001089
Iteration 134/1000 | Loss: 0.00001089
Iteration 135/1000 | Loss: 0.00001089
Iteration 136/1000 | Loss: 0.00001089
Iteration 137/1000 | Loss: 0.00001089
Iteration 138/1000 | Loss: 0.00001089
Iteration 139/1000 | Loss: 0.00001089
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.088668159354711e-05, 1.088668159354711e-05, 1.088668159354711e-05, 1.088668159354711e-05, 1.088668159354711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.088668159354711e-05

Optimization complete. Final v2v error: 2.661674976348877 mm

Highest mean error: 3.8534183502197266 mm for frame 159

Lowest mean error: 2.1499767303466797 mm for frame 13

Saving results

Total time: 38.486830949783325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007599
Iteration 2/25 | Loss: 0.00153611
Iteration 3/25 | Loss: 0.00110823
Iteration 4/25 | Loss: 0.00104900
Iteration 5/25 | Loss: 0.00103763
Iteration 6/25 | Loss: 0.00103406
Iteration 7/25 | Loss: 0.00103373
Iteration 8/25 | Loss: 0.00103373
Iteration 9/25 | Loss: 0.00103373
Iteration 10/25 | Loss: 0.00103373
Iteration 11/25 | Loss: 0.00103373
Iteration 12/25 | Loss: 0.00103373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010337262647226453, 0.0010337262647226453, 0.0010337262647226453, 0.0010337262647226453, 0.0010337262647226453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010337262647226453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81737161
Iteration 2/25 | Loss: 0.00142798
Iteration 3/25 | Loss: 0.00142798
Iteration 4/25 | Loss: 0.00142798
Iteration 5/25 | Loss: 0.00142798
Iteration 6/25 | Loss: 0.00142798
Iteration 7/25 | Loss: 0.00142798
Iteration 8/25 | Loss: 0.00142798
Iteration 9/25 | Loss: 0.00142798
Iteration 10/25 | Loss: 0.00142798
Iteration 11/25 | Loss: 0.00142797
Iteration 12/25 | Loss: 0.00142797
Iteration 13/25 | Loss: 0.00142797
Iteration 14/25 | Loss: 0.00142797
Iteration 15/25 | Loss: 0.00142797
Iteration 16/25 | Loss: 0.00142797
Iteration 17/25 | Loss: 0.00142797
Iteration 18/25 | Loss: 0.00142797
Iteration 19/25 | Loss: 0.00142797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014279744355008006, 0.0014279744355008006, 0.0014279744355008006, 0.0014279744355008006, 0.0014279744355008006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014279744355008006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142797
Iteration 2/1000 | Loss: 0.00003445
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001558
Iteration 8/1000 | Loss: 0.00001524
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001471
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001446
Iteration 18/1000 | Loss: 0.00001443
Iteration 19/1000 | Loss: 0.00001443
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001439
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001439
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001439
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001438
Iteration 37/1000 | Loss: 0.00001438
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001437
Iteration 44/1000 | Loss: 0.00001437
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001436
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001433
Iteration 61/1000 | Loss: 0.00001433
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001432
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001432
Iteration 80/1000 | Loss: 0.00001432
Iteration 81/1000 | Loss: 0.00001432
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00001432
Iteration 84/1000 | Loss: 0.00001432
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001432
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4319056390377227e-05, 1.4319056390377227e-05, 1.4319056390377227e-05, 1.4319056390377227e-05, 1.4319056390377227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4319056390377227e-05

Optimization complete. Final v2v error: 3.2536661624908447 mm

Highest mean error: 3.5581419467926025 mm for frame 31

Lowest mean error: 3.0682809352874756 mm for frame 85

Saving results

Total time: 32.89216089248657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930838
Iteration 2/25 | Loss: 0.00175395
Iteration 3/25 | Loss: 0.00132337
Iteration 4/25 | Loss: 0.00126126
Iteration 5/25 | Loss: 0.00125043
Iteration 6/25 | Loss: 0.00118258
Iteration 7/25 | Loss: 0.00114887
Iteration 8/25 | Loss: 0.00112349
Iteration 9/25 | Loss: 0.00111288
Iteration 10/25 | Loss: 0.00111067
Iteration 11/25 | Loss: 0.00110926
Iteration 12/25 | Loss: 0.00112204
Iteration 13/25 | Loss: 0.00110128
Iteration 14/25 | Loss: 0.00109071
Iteration 15/25 | Loss: 0.00108769
Iteration 16/25 | Loss: 0.00108687
Iteration 17/25 | Loss: 0.00109035
Iteration 18/25 | Loss: 0.00109341
Iteration 19/25 | Loss: 0.00108704
Iteration 20/25 | Loss: 0.00108496
Iteration 21/25 | Loss: 0.00108731
Iteration 22/25 | Loss: 0.00108699
Iteration 23/25 | Loss: 0.00108652
Iteration 24/25 | Loss: 0.00108575
Iteration 25/25 | Loss: 0.00108389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33574009
Iteration 2/25 | Loss: 0.00162606
Iteration 3/25 | Loss: 0.00162106
Iteration 4/25 | Loss: 0.00162106
Iteration 5/25 | Loss: 0.00162106
Iteration 6/25 | Loss: 0.00162106
Iteration 7/25 | Loss: 0.00162106
Iteration 8/25 | Loss: 0.00162106
Iteration 9/25 | Loss: 0.00162106
Iteration 10/25 | Loss: 0.00162105
Iteration 11/25 | Loss: 0.00162105
Iteration 12/25 | Loss: 0.00162105
Iteration 13/25 | Loss: 0.00162105
Iteration 14/25 | Loss: 0.00162105
Iteration 15/25 | Loss: 0.00162105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016210549511015415, 0.0016210549511015415, 0.0016210549511015415, 0.0016210549511015415, 0.0016210549511015415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016210549511015415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162105
Iteration 2/1000 | Loss: 0.00027437
Iteration 3/1000 | Loss: 0.00034330
Iteration 4/1000 | Loss: 0.00018254
Iteration 5/1000 | Loss: 0.00018562
Iteration 6/1000 | Loss: 0.00011994
Iteration 7/1000 | Loss: 0.00009413
Iteration 8/1000 | Loss: 0.00008284
Iteration 9/1000 | Loss: 0.00021478
Iteration 10/1000 | Loss: 0.00010742
Iteration 11/1000 | Loss: 0.00016996
Iteration 12/1000 | Loss: 0.00014311
Iteration 13/1000 | Loss: 0.00009876
Iteration 14/1000 | Loss: 0.00007789
Iteration 15/1000 | Loss: 0.00011161
Iteration 16/1000 | Loss: 0.00006312
Iteration 17/1000 | Loss: 0.00008716
Iteration 18/1000 | Loss: 0.00008983
Iteration 19/1000 | Loss: 0.00005464
Iteration 20/1000 | Loss: 0.00004706
Iteration 21/1000 | Loss: 0.00004122
Iteration 22/1000 | Loss: 0.00008966
Iteration 23/1000 | Loss: 0.00004262
Iteration 24/1000 | Loss: 0.00004025
Iteration 25/1000 | Loss: 0.00003588
Iteration 26/1000 | Loss: 0.00003338
Iteration 27/1000 | Loss: 0.00003201
Iteration 28/1000 | Loss: 0.00003035
Iteration 29/1000 | Loss: 0.00002891
Iteration 30/1000 | Loss: 0.00002790
Iteration 31/1000 | Loss: 0.00002700
Iteration 32/1000 | Loss: 0.00002623
Iteration 33/1000 | Loss: 0.00002563
Iteration 34/1000 | Loss: 0.00003081
Iteration 35/1000 | Loss: 0.00002782
Iteration 36/1000 | Loss: 0.00002653
Iteration 37/1000 | Loss: 0.00002578
Iteration 38/1000 | Loss: 0.00002489
Iteration 39/1000 | Loss: 0.00003041
Iteration 40/1000 | Loss: 0.00002711
Iteration 41/1000 | Loss: 0.00002985
Iteration 42/1000 | Loss: 0.00002689
Iteration 43/1000 | Loss: 0.00002976
Iteration 44/1000 | Loss: 0.00002701
Iteration 45/1000 | Loss: 0.00002968
Iteration 46/1000 | Loss: 0.00002712
Iteration 47/1000 | Loss: 0.00002619
Iteration 48/1000 | Loss: 0.00002518
Iteration 49/1000 | Loss: 0.00002458
Iteration 50/1000 | Loss: 0.00002414
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002289
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002196
Iteration 56/1000 | Loss: 0.00002192
Iteration 57/1000 | Loss: 0.00002189
Iteration 58/1000 | Loss: 0.00002189
Iteration 59/1000 | Loss: 0.00002189
Iteration 60/1000 | Loss: 0.00002189
Iteration 61/1000 | Loss: 0.00002188
Iteration 62/1000 | Loss: 0.00002188
Iteration 63/1000 | Loss: 0.00002187
Iteration 64/1000 | Loss: 0.00002187
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002187
Iteration 68/1000 | Loss: 0.00002187
Iteration 69/1000 | Loss: 0.00002187
Iteration 70/1000 | Loss: 0.00002187
Iteration 71/1000 | Loss: 0.00002186
Iteration 72/1000 | Loss: 0.00002186
Iteration 73/1000 | Loss: 0.00002186
Iteration 74/1000 | Loss: 0.00002186
Iteration 75/1000 | Loss: 0.00002186
Iteration 76/1000 | Loss: 0.00002186
Iteration 77/1000 | Loss: 0.00002185
Iteration 78/1000 | Loss: 0.00002185
Iteration 79/1000 | Loss: 0.00002185
Iteration 80/1000 | Loss: 0.00002185
Iteration 81/1000 | Loss: 0.00002184
Iteration 82/1000 | Loss: 0.00002184
Iteration 83/1000 | Loss: 0.00002184
Iteration 84/1000 | Loss: 0.00002183
Iteration 85/1000 | Loss: 0.00002183
Iteration 86/1000 | Loss: 0.00002182
Iteration 87/1000 | Loss: 0.00002181
Iteration 88/1000 | Loss: 0.00002170
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002169
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002167
Iteration 93/1000 | Loss: 0.00002167
Iteration 94/1000 | Loss: 0.00002166
Iteration 95/1000 | Loss: 0.00002166
Iteration 96/1000 | Loss: 0.00002165
Iteration 97/1000 | Loss: 0.00002165
Iteration 98/1000 | Loss: 0.00002165
Iteration 99/1000 | Loss: 0.00002164
Iteration 100/1000 | Loss: 0.00002164
Iteration 101/1000 | Loss: 0.00002163
Iteration 102/1000 | Loss: 0.00002162
Iteration 103/1000 | Loss: 0.00002162
Iteration 104/1000 | Loss: 0.00002162
Iteration 105/1000 | Loss: 0.00002161
Iteration 106/1000 | Loss: 0.00002161
Iteration 107/1000 | Loss: 0.00002161
Iteration 108/1000 | Loss: 0.00002161
Iteration 109/1000 | Loss: 0.00002160
Iteration 110/1000 | Loss: 0.00002160
Iteration 111/1000 | Loss: 0.00002160
Iteration 112/1000 | Loss: 0.00002160
Iteration 113/1000 | Loss: 0.00002160
Iteration 114/1000 | Loss: 0.00002159
Iteration 115/1000 | Loss: 0.00002159
Iteration 116/1000 | Loss: 0.00002159
Iteration 117/1000 | Loss: 0.00002159
Iteration 118/1000 | Loss: 0.00002159
Iteration 119/1000 | Loss: 0.00002159
Iteration 120/1000 | Loss: 0.00002159
Iteration 121/1000 | Loss: 0.00002159
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002158
Iteration 124/1000 | Loss: 0.00002158
Iteration 125/1000 | Loss: 0.00002158
Iteration 126/1000 | Loss: 0.00002158
Iteration 127/1000 | Loss: 0.00002158
Iteration 128/1000 | Loss: 0.00002158
Iteration 129/1000 | Loss: 0.00002158
Iteration 130/1000 | Loss: 0.00002158
Iteration 131/1000 | Loss: 0.00002158
Iteration 132/1000 | Loss: 0.00002157
Iteration 133/1000 | Loss: 0.00002157
Iteration 134/1000 | Loss: 0.00002157
Iteration 135/1000 | Loss: 0.00002157
Iteration 136/1000 | Loss: 0.00002156
Iteration 137/1000 | Loss: 0.00002156
Iteration 138/1000 | Loss: 0.00002156
Iteration 139/1000 | Loss: 0.00002156
Iteration 140/1000 | Loss: 0.00002156
Iteration 141/1000 | Loss: 0.00002156
Iteration 142/1000 | Loss: 0.00002156
Iteration 143/1000 | Loss: 0.00002156
Iteration 144/1000 | Loss: 0.00002156
Iteration 145/1000 | Loss: 0.00002156
Iteration 146/1000 | Loss: 0.00002156
Iteration 147/1000 | Loss: 0.00002156
Iteration 148/1000 | Loss: 0.00002155
Iteration 149/1000 | Loss: 0.00002155
Iteration 150/1000 | Loss: 0.00002155
Iteration 151/1000 | Loss: 0.00002154
Iteration 152/1000 | Loss: 0.00002154
Iteration 153/1000 | Loss: 0.00002153
Iteration 154/1000 | Loss: 0.00002153
Iteration 155/1000 | Loss: 0.00002153
Iteration 156/1000 | Loss: 0.00002152
Iteration 157/1000 | Loss: 0.00002152
Iteration 158/1000 | Loss: 0.00002152
Iteration 159/1000 | Loss: 0.00002152
Iteration 160/1000 | Loss: 0.00002152
Iteration 161/1000 | Loss: 0.00003203
Iteration 162/1000 | Loss: 0.00002709
Iteration 163/1000 | Loss: 0.00002573
Iteration 164/1000 | Loss: 0.00002527
Iteration 165/1000 | Loss: 0.00002453
Iteration 166/1000 | Loss: 0.00004473
Iteration 167/1000 | Loss: 0.00005927
Iteration 168/1000 | Loss: 0.00005619
Iteration 169/1000 | Loss: 0.00005536
Iteration 170/1000 | Loss: 0.00003206
Iteration 171/1000 | Loss: 0.00004023
Iteration 172/1000 | Loss: 0.00002444
Iteration 173/1000 | Loss: 0.00002345
Iteration 174/1000 | Loss: 0.00002291
Iteration 175/1000 | Loss: 0.00002266
Iteration 176/1000 | Loss: 0.00002235
Iteration 177/1000 | Loss: 0.00002214
Iteration 178/1000 | Loss: 0.00002209
Iteration 179/1000 | Loss: 0.00002207
Iteration 180/1000 | Loss: 0.00002205
Iteration 181/1000 | Loss: 0.00002202
Iteration 182/1000 | Loss: 0.00002201
Iteration 183/1000 | Loss: 0.00002200
Iteration 184/1000 | Loss: 0.00002200
Iteration 185/1000 | Loss: 0.00002199
Iteration 186/1000 | Loss: 0.00002195
Iteration 187/1000 | Loss: 0.00002193
Iteration 188/1000 | Loss: 0.00002192
Iteration 189/1000 | Loss: 0.00002186
Iteration 190/1000 | Loss: 0.00002183
Iteration 191/1000 | Loss: 0.00002182
Iteration 192/1000 | Loss: 0.00002181
Iteration 193/1000 | Loss: 0.00002178
Iteration 194/1000 | Loss: 0.00002176
Iteration 195/1000 | Loss: 0.00002175
Iteration 196/1000 | Loss: 0.00002175
Iteration 197/1000 | Loss: 0.00002174
Iteration 198/1000 | Loss: 0.00002173
Iteration 199/1000 | Loss: 0.00002173
Iteration 200/1000 | Loss: 0.00002172
Iteration 201/1000 | Loss: 0.00002172
Iteration 202/1000 | Loss: 0.00002172
Iteration 203/1000 | Loss: 0.00002172
Iteration 204/1000 | Loss: 0.00002171
Iteration 205/1000 | Loss: 0.00002171
Iteration 206/1000 | Loss: 0.00002170
Iteration 207/1000 | Loss: 0.00002170
Iteration 208/1000 | Loss: 0.00002169
Iteration 209/1000 | Loss: 0.00002169
Iteration 210/1000 | Loss: 0.00002169
Iteration 211/1000 | Loss: 0.00002168
Iteration 212/1000 | Loss: 0.00002164
Iteration 213/1000 | Loss: 0.00002143
Iteration 214/1000 | Loss: 0.00002119
Iteration 215/1000 | Loss: 0.00002098
Iteration 216/1000 | Loss: 0.00002091
Iteration 217/1000 | Loss: 0.00002085
Iteration 218/1000 | Loss: 0.00002080
Iteration 219/1000 | Loss: 0.00002075
Iteration 220/1000 | Loss: 0.00002075
Iteration 221/1000 | Loss: 0.00002074
Iteration 222/1000 | Loss: 0.00002073
Iteration 223/1000 | Loss: 0.00002073
Iteration 224/1000 | Loss: 0.00002073
Iteration 225/1000 | Loss: 0.00002072
Iteration 226/1000 | Loss: 0.00002072
Iteration 227/1000 | Loss: 0.00002072
Iteration 228/1000 | Loss: 0.00002072
Iteration 229/1000 | Loss: 0.00002071
Iteration 230/1000 | Loss: 0.00002071
Iteration 231/1000 | Loss: 0.00002070
Iteration 232/1000 | Loss: 0.00002070
Iteration 233/1000 | Loss: 0.00002070
Iteration 234/1000 | Loss: 0.00002070
Iteration 235/1000 | Loss: 0.00002069
Iteration 236/1000 | Loss: 0.00002069
Iteration 237/1000 | Loss: 0.00002068
Iteration 238/1000 | Loss: 0.00002068
Iteration 239/1000 | Loss: 0.00002068
Iteration 240/1000 | Loss: 0.00002068
Iteration 241/1000 | Loss: 0.00002067
Iteration 242/1000 | Loss: 0.00002067
Iteration 243/1000 | Loss: 0.00002067
Iteration 244/1000 | Loss: 0.00002067
Iteration 245/1000 | Loss: 0.00002067
Iteration 246/1000 | Loss: 0.00002067
Iteration 247/1000 | Loss: 0.00002067
Iteration 248/1000 | Loss: 0.00002067
Iteration 249/1000 | Loss: 0.00002067
Iteration 250/1000 | Loss: 0.00002066
Iteration 251/1000 | Loss: 0.00002066
Iteration 252/1000 | Loss: 0.00002066
Iteration 253/1000 | Loss: 0.00002066
Iteration 254/1000 | Loss: 0.00002066
Iteration 255/1000 | Loss: 0.00002066
Iteration 256/1000 | Loss: 0.00002066
Iteration 257/1000 | Loss: 0.00002066
Iteration 258/1000 | Loss: 0.00002066
Iteration 259/1000 | Loss: 0.00002066
Iteration 260/1000 | Loss: 0.00002065
Iteration 261/1000 | Loss: 0.00002065
Iteration 262/1000 | Loss: 0.00002065
Iteration 263/1000 | Loss: 0.00002065
Iteration 264/1000 | Loss: 0.00002065
Iteration 265/1000 | Loss: 0.00002065
Iteration 266/1000 | Loss: 0.00002065
Iteration 267/1000 | Loss: 0.00002065
Iteration 268/1000 | Loss: 0.00002065
Iteration 269/1000 | Loss: 0.00002065
Iteration 270/1000 | Loss: 0.00002065
Iteration 271/1000 | Loss: 0.00002065
Iteration 272/1000 | Loss: 0.00002065
Iteration 273/1000 | Loss: 0.00002065
Iteration 274/1000 | Loss: 0.00002065
Iteration 275/1000 | Loss: 0.00002065
Iteration 276/1000 | Loss: 0.00002065
Iteration 277/1000 | Loss: 0.00002065
Iteration 278/1000 | Loss: 0.00002065
Iteration 279/1000 | Loss: 0.00002064
Iteration 280/1000 | Loss: 0.00002064
Iteration 281/1000 | Loss: 0.00002064
Iteration 282/1000 | Loss: 0.00002064
Iteration 283/1000 | Loss: 0.00002064
Iteration 284/1000 | Loss: 0.00002064
Iteration 285/1000 | Loss: 0.00002064
Iteration 286/1000 | Loss: 0.00002064
Iteration 287/1000 | Loss: 0.00002064
Iteration 288/1000 | Loss: 0.00002064
Iteration 289/1000 | Loss: 0.00002064
Iteration 290/1000 | Loss: 0.00002064
Iteration 291/1000 | Loss: 0.00002064
Iteration 292/1000 | Loss: 0.00002064
Iteration 293/1000 | Loss: 0.00002064
Iteration 294/1000 | Loss: 0.00002064
Iteration 295/1000 | Loss: 0.00002064
Iteration 296/1000 | Loss: 0.00002064
Iteration 297/1000 | Loss: 0.00002064
Iteration 298/1000 | Loss: 0.00002064
Iteration 299/1000 | Loss: 0.00002064
Iteration 300/1000 | Loss: 0.00002063
Iteration 301/1000 | Loss: 0.00002063
Iteration 302/1000 | Loss: 0.00002063
Iteration 303/1000 | Loss: 0.00002063
Iteration 304/1000 | Loss: 0.00002063
Iteration 305/1000 | Loss: 0.00002063
Iteration 306/1000 | Loss: 0.00002063
Iteration 307/1000 | Loss: 0.00002063
Iteration 308/1000 | Loss: 0.00002063
Iteration 309/1000 | Loss: 0.00002063
Iteration 310/1000 | Loss: 0.00002063
Iteration 311/1000 | Loss: 0.00002063
Iteration 312/1000 | Loss: 0.00002063
Iteration 313/1000 | Loss: 0.00002063
Iteration 314/1000 | Loss: 0.00002063
Iteration 315/1000 | Loss: 0.00002063
Iteration 316/1000 | Loss: 0.00002063
Iteration 317/1000 | Loss: 0.00002063
Iteration 318/1000 | Loss: 0.00002063
Iteration 319/1000 | Loss: 0.00002062
Iteration 320/1000 | Loss: 0.00002062
Iteration 321/1000 | Loss: 0.00002062
Iteration 322/1000 | Loss: 0.00002062
Iteration 323/1000 | Loss: 0.00002062
Iteration 324/1000 | Loss: 0.00002062
Iteration 325/1000 | Loss: 0.00002062
Iteration 326/1000 | Loss: 0.00002062
Iteration 327/1000 | Loss: 0.00002062
Iteration 328/1000 | Loss: 0.00002062
Iteration 329/1000 | Loss: 0.00002062
Iteration 330/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [2.0623125237761997e-05, 2.0623125237761997e-05, 2.0623125237761997e-05, 2.0623125237761997e-05, 2.0623125237761997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0623125237761997e-05

Optimization complete. Final v2v error: 3.449979066848755 mm

Highest mean error: 7.545797824859619 mm for frame 115

Lowest mean error: 2.3698368072509766 mm for frame 74

Saving results

Total time: 186.16859483718872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094233
Iteration 2/25 | Loss: 0.00227851
Iteration 3/25 | Loss: 0.00168565
Iteration 4/25 | Loss: 0.00121658
Iteration 5/25 | Loss: 0.00118486
Iteration 6/25 | Loss: 0.00114769
Iteration 7/25 | Loss: 0.00111807
Iteration 8/25 | Loss: 0.00112811
Iteration 9/25 | Loss: 0.00106979
Iteration 10/25 | Loss: 0.00107055
Iteration 11/25 | Loss: 0.00102651
Iteration 12/25 | Loss: 0.00102124
Iteration 13/25 | Loss: 0.00102079
Iteration 14/25 | Loss: 0.00102074
Iteration 15/25 | Loss: 0.00102074
Iteration 16/25 | Loss: 0.00102074
Iteration 17/25 | Loss: 0.00102074
Iteration 18/25 | Loss: 0.00102074
Iteration 19/25 | Loss: 0.00102073
Iteration 20/25 | Loss: 0.00102073
Iteration 21/25 | Loss: 0.00102073
Iteration 22/25 | Loss: 0.00102073
Iteration 23/25 | Loss: 0.00102073
Iteration 24/25 | Loss: 0.00102073
Iteration 25/25 | Loss: 0.00102073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69480324
Iteration 2/25 | Loss: 0.00202704
Iteration 3/25 | Loss: 0.00202287
Iteration 4/25 | Loss: 0.00202287
Iteration 5/25 | Loss: 0.00202287
Iteration 6/25 | Loss: 0.00202287
Iteration 7/25 | Loss: 0.00202287
Iteration 8/25 | Loss: 0.00202287
Iteration 9/25 | Loss: 0.00202287
Iteration 10/25 | Loss: 0.00202286
Iteration 11/25 | Loss: 0.00202286
Iteration 12/25 | Loss: 0.00202286
Iteration 13/25 | Loss: 0.00202286
Iteration 14/25 | Loss: 0.00202286
Iteration 15/25 | Loss: 0.00202286
Iteration 16/25 | Loss: 0.00202286
Iteration 17/25 | Loss: 0.00202286
Iteration 18/25 | Loss: 0.00202286
Iteration 19/25 | Loss: 0.00202286
Iteration 20/25 | Loss: 0.00202286
Iteration 21/25 | Loss: 0.00202286
Iteration 22/25 | Loss: 0.00202286
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0020228642970323563, 0.0020228642970323563, 0.0020228642970323563, 0.0020228642970323563, 0.0020228642970323563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020228642970323563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202286
Iteration 2/1000 | Loss: 0.00003754
Iteration 3/1000 | Loss: 0.00003322
Iteration 4/1000 | Loss: 0.00002058
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00003530
Iteration 7/1000 | Loss: 0.00004065
Iteration 8/1000 | Loss: 0.00004073
Iteration 9/1000 | Loss: 0.00002306
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00002679
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001733
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00002844
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001695
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001689
Iteration 29/1000 | Loss: 0.00001689
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001688
Iteration 33/1000 | Loss: 0.00001688
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001681
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001681
Iteration 59/1000 | Loss: 0.00001681
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001680
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001673
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001672
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001671
Iteration 120/1000 | Loss: 0.00001671
Iteration 121/1000 | Loss: 0.00001671
Iteration 122/1000 | Loss: 0.00001671
Iteration 123/1000 | Loss: 0.00001671
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001670
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001669
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001669
Iteration 144/1000 | Loss: 0.00001669
Iteration 145/1000 | Loss: 0.00001669
Iteration 146/1000 | Loss: 0.00001669
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001668
Iteration 152/1000 | Loss: 0.00001668
Iteration 153/1000 | Loss: 0.00001668
Iteration 154/1000 | Loss: 0.00001668
Iteration 155/1000 | Loss: 0.00001668
Iteration 156/1000 | Loss: 0.00001668
Iteration 157/1000 | Loss: 0.00001668
Iteration 158/1000 | Loss: 0.00001668
Iteration 159/1000 | Loss: 0.00001668
Iteration 160/1000 | Loss: 0.00001668
Iteration 161/1000 | Loss: 0.00001668
Iteration 162/1000 | Loss: 0.00001668
Iteration 163/1000 | Loss: 0.00001668
Iteration 164/1000 | Loss: 0.00001668
Iteration 165/1000 | Loss: 0.00001668
Iteration 166/1000 | Loss: 0.00001668
Iteration 167/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.66843892657198e-05, 1.66843892657198e-05, 1.66843892657198e-05, 1.66843892657198e-05, 1.66843892657198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.66843892657198e-05

Optimization complete. Final v2v error: 3.4131851196289062 mm

Highest mean error: 9.032713890075684 mm for frame 146

Lowest mean error: 3.1256637573242188 mm for frame 101

Saving results

Total time: 56.36225461959839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102223
Iteration 2/25 | Loss: 0.00291721
Iteration 3/25 | Loss: 0.00209316
Iteration 4/25 | Loss: 0.00173111
Iteration 5/25 | Loss: 0.00163422
Iteration 6/25 | Loss: 0.00173625
Iteration 7/25 | Loss: 0.00158774
Iteration 8/25 | Loss: 0.00146538
Iteration 9/25 | Loss: 0.00137636
Iteration 10/25 | Loss: 0.00129090
Iteration 11/25 | Loss: 0.00128869
Iteration 12/25 | Loss: 0.00126389
Iteration 13/25 | Loss: 0.00124576
Iteration 14/25 | Loss: 0.00123195
Iteration 15/25 | Loss: 0.00123269
Iteration 16/25 | Loss: 0.00123778
Iteration 17/25 | Loss: 0.00123059
Iteration 18/25 | Loss: 0.00121981
Iteration 19/25 | Loss: 0.00121443
Iteration 20/25 | Loss: 0.00121537
Iteration 21/25 | Loss: 0.00121238
Iteration 22/25 | Loss: 0.00121422
Iteration 23/25 | Loss: 0.00121947
Iteration 24/25 | Loss: 0.00122254
Iteration 25/25 | Loss: 0.00122183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95016015
Iteration 2/25 | Loss: 0.00204198
Iteration 3/25 | Loss: 0.00190625
Iteration 4/25 | Loss: 0.00190625
Iteration 5/25 | Loss: 0.00190625
Iteration 6/25 | Loss: 0.00190625
Iteration 7/25 | Loss: 0.00190625
Iteration 8/25 | Loss: 0.00190625
Iteration 9/25 | Loss: 0.00190625
Iteration 10/25 | Loss: 0.00190625
Iteration 11/25 | Loss: 0.00190625
Iteration 12/25 | Loss: 0.00190625
Iteration 13/25 | Loss: 0.00190625
Iteration 14/25 | Loss: 0.00190625
Iteration 15/25 | Loss: 0.00190625
Iteration 16/25 | Loss: 0.00190625
Iteration 17/25 | Loss: 0.00190625
Iteration 18/25 | Loss: 0.00190625
Iteration 19/25 | Loss: 0.00190625
Iteration 20/25 | Loss: 0.00190625
Iteration 21/25 | Loss: 0.00190625
Iteration 22/25 | Loss: 0.00190625
Iteration 23/25 | Loss: 0.00190625
Iteration 24/25 | Loss: 0.00190625
Iteration 25/25 | Loss: 0.00190625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190625
Iteration 2/1000 | Loss: 0.00089063
Iteration 3/1000 | Loss: 0.00123614
Iteration 4/1000 | Loss: 0.00079099
Iteration 5/1000 | Loss: 0.00068499
Iteration 6/1000 | Loss: 0.00020624
Iteration 7/1000 | Loss: 0.00010024
Iteration 8/1000 | Loss: 0.00021024
Iteration 9/1000 | Loss: 0.00029185
Iteration 10/1000 | Loss: 0.00026081
Iteration 11/1000 | Loss: 0.00007616
Iteration 12/1000 | Loss: 0.00008214
Iteration 13/1000 | Loss: 0.00015417
Iteration 14/1000 | Loss: 0.00008311
Iteration 15/1000 | Loss: 0.00034592
Iteration 16/1000 | Loss: 0.00051554
Iteration 17/1000 | Loss: 0.00033001
Iteration 18/1000 | Loss: 0.00030571
Iteration 19/1000 | Loss: 0.00020190
Iteration 20/1000 | Loss: 0.00029877
Iteration 21/1000 | Loss: 0.00048924
Iteration 22/1000 | Loss: 0.00032517
Iteration 23/1000 | Loss: 0.00037790
Iteration 24/1000 | Loss: 0.00020708
Iteration 25/1000 | Loss: 0.00009666
Iteration 26/1000 | Loss: 0.00022232
Iteration 27/1000 | Loss: 0.00023736
Iteration 28/1000 | Loss: 0.00022370
Iteration 29/1000 | Loss: 0.00024039
Iteration 30/1000 | Loss: 0.00020046
Iteration 31/1000 | Loss: 0.00021600
Iteration 32/1000 | Loss: 0.00018159
Iteration 33/1000 | Loss: 0.00014226
Iteration 34/1000 | Loss: 0.00013539
Iteration 35/1000 | Loss: 0.00012750
Iteration 36/1000 | Loss: 0.00020277
Iteration 37/1000 | Loss: 0.00015434
Iteration 38/1000 | Loss: 0.00005331
Iteration 39/1000 | Loss: 0.00005381
Iteration 40/1000 | Loss: 0.00038401
Iteration 41/1000 | Loss: 0.00076119
Iteration 42/1000 | Loss: 0.00089541
Iteration 43/1000 | Loss: 0.00051168
Iteration 44/1000 | Loss: 0.00073949
Iteration 45/1000 | Loss: 0.00029881
Iteration 46/1000 | Loss: 0.00036445
Iteration 47/1000 | Loss: 0.00020063
Iteration 48/1000 | Loss: 0.00008161
Iteration 49/1000 | Loss: 0.00009717
Iteration 50/1000 | Loss: 0.00009169
Iteration 51/1000 | Loss: 0.00006072
Iteration 52/1000 | Loss: 0.00014655
Iteration 53/1000 | Loss: 0.00007565
Iteration 54/1000 | Loss: 0.00028919
Iteration 55/1000 | Loss: 0.00042917
Iteration 56/1000 | Loss: 0.00022996
Iteration 57/1000 | Loss: 0.00042395
Iteration 58/1000 | Loss: 0.00090701
Iteration 59/1000 | Loss: 0.00043342
Iteration 60/1000 | Loss: 0.00034022
Iteration 61/1000 | Loss: 0.00034409
Iteration 62/1000 | Loss: 0.00019671
Iteration 63/1000 | Loss: 0.00041356
Iteration 64/1000 | Loss: 0.00052457
Iteration 65/1000 | Loss: 0.00033033
Iteration 66/1000 | Loss: 0.00045720
Iteration 67/1000 | Loss: 0.00039535
Iteration 68/1000 | Loss: 0.00014372
Iteration 69/1000 | Loss: 0.00009715
Iteration 70/1000 | Loss: 0.00036580
Iteration 71/1000 | Loss: 0.00025989
Iteration 72/1000 | Loss: 0.00026807
Iteration 73/1000 | Loss: 0.00025355
Iteration 74/1000 | Loss: 0.00026592
Iteration 75/1000 | Loss: 0.00028563
Iteration 76/1000 | Loss: 0.00019450
Iteration 77/1000 | Loss: 0.00029851
Iteration 78/1000 | Loss: 0.00020065
Iteration 79/1000 | Loss: 0.00018094
Iteration 80/1000 | Loss: 0.00014142
Iteration 81/1000 | Loss: 0.00008462
Iteration 82/1000 | Loss: 0.00006927
Iteration 83/1000 | Loss: 0.00008080
Iteration 84/1000 | Loss: 0.00005888
Iteration 85/1000 | Loss: 0.00007147
Iteration 86/1000 | Loss: 0.00027883
Iteration 87/1000 | Loss: 0.00009463
Iteration 88/1000 | Loss: 0.00014794
Iteration 89/1000 | Loss: 0.00008588
Iteration 90/1000 | Loss: 0.00008149
Iteration 91/1000 | Loss: 0.00007592
Iteration 92/1000 | Loss: 0.00007853
Iteration 93/1000 | Loss: 0.00018188
Iteration 94/1000 | Loss: 0.00011571
Iteration 95/1000 | Loss: 0.00011242
Iteration 96/1000 | Loss: 0.00037555
Iteration 97/1000 | Loss: 0.00008529
Iteration 98/1000 | Loss: 0.00014183
Iteration 99/1000 | Loss: 0.00017536
Iteration 100/1000 | Loss: 0.00011950
Iteration 101/1000 | Loss: 0.00015990
Iteration 102/1000 | Loss: 0.00013061
Iteration 103/1000 | Loss: 0.00012603
Iteration 104/1000 | Loss: 0.00013491
Iteration 105/1000 | Loss: 0.00012581
Iteration 106/1000 | Loss: 0.00014923
Iteration 107/1000 | Loss: 0.00015219
Iteration 108/1000 | Loss: 0.00032338
Iteration 109/1000 | Loss: 0.00018099
Iteration 110/1000 | Loss: 0.00008331
Iteration 111/1000 | Loss: 0.00010855
Iteration 112/1000 | Loss: 0.00006349
Iteration 113/1000 | Loss: 0.00005079
Iteration 114/1000 | Loss: 0.00010121
Iteration 115/1000 | Loss: 0.00005600
Iteration 116/1000 | Loss: 0.00005955
Iteration 117/1000 | Loss: 0.00005120
Iteration 118/1000 | Loss: 0.00017116
Iteration 119/1000 | Loss: 0.00010348
Iteration 120/1000 | Loss: 0.00014140
Iteration 121/1000 | Loss: 0.00011005
Iteration 122/1000 | Loss: 0.00010443
Iteration 123/1000 | Loss: 0.00011630
Iteration 124/1000 | Loss: 0.00005268
Iteration 125/1000 | Loss: 0.00008383
Iteration 126/1000 | Loss: 0.00005821
Iteration 127/1000 | Loss: 0.00004886
Iteration 128/1000 | Loss: 0.00005343
Iteration 129/1000 | Loss: 0.00005411
Iteration 130/1000 | Loss: 0.00009976
Iteration 131/1000 | Loss: 0.00004632
Iteration 132/1000 | Loss: 0.00006244
Iteration 133/1000 | Loss: 0.00005970
Iteration 134/1000 | Loss: 0.00003943
Iteration 135/1000 | Loss: 0.00003795
Iteration 136/1000 | Loss: 0.00019137
Iteration 137/1000 | Loss: 0.00020499
Iteration 138/1000 | Loss: 0.00021895
Iteration 139/1000 | Loss: 0.00013565
Iteration 140/1000 | Loss: 0.00005198
Iteration 141/1000 | Loss: 0.00007561
Iteration 142/1000 | Loss: 0.00006292
Iteration 143/1000 | Loss: 0.00005493
Iteration 144/1000 | Loss: 0.00005144
Iteration 145/1000 | Loss: 0.00005547
Iteration 146/1000 | Loss: 0.00004419
Iteration 147/1000 | Loss: 0.00004892
Iteration 148/1000 | Loss: 0.00005484
Iteration 149/1000 | Loss: 0.00005679
Iteration 150/1000 | Loss: 0.00005020
Iteration 151/1000 | Loss: 0.00005870
Iteration 152/1000 | Loss: 0.00004767
Iteration 153/1000 | Loss: 0.00006431
Iteration 154/1000 | Loss: 0.00006108
Iteration 155/1000 | Loss: 0.00005018
Iteration 156/1000 | Loss: 0.00004919
Iteration 157/1000 | Loss: 0.00004843
Iteration 158/1000 | Loss: 0.00006533
Iteration 159/1000 | Loss: 0.00004756
Iteration 160/1000 | Loss: 0.00005757
Iteration 161/1000 | Loss: 0.00005253
Iteration 162/1000 | Loss: 0.00004778
Iteration 163/1000 | Loss: 0.00004527
Iteration 164/1000 | Loss: 0.00005221
Iteration 165/1000 | Loss: 0.00005244
Iteration 166/1000 | Loss: 0.00010289
Iteration 167/1000 | Loss: 0.00005618
Iteration 168/1000 | Loss: 0.00006103
Iteration 169/1000 | Loss: 0.00004972
Iteration 170/1000 | Loss: 0.00006650
Iteration 171/1000 | Loss: 0.00004435
Iteration 172/1000 | Loss: 0.00003750
Iteration 173/1000 | Loss: 0.00004891
Iteration 174/1000 | Loss: 0.00005378
Iteration 175/1000 | Loss: 0.00005043
Iteration 176/1000 | Loss: 0.00004355
Iteration 177/1000 | Loss: 0.00004906
Iteration 178/1000 | Loss: 0.00004594
Iteration 179/1000 | Loss: 0.00005177
Iteration 180/1000 | Loss: 0.00004954
Iteration 181/1000 | Loss: 0.00003818
Iteration 182/1000 | Loss: 0.00004353
Iteration 183/1000 | Loss: 0.00004905
Iteration 184/1000 | Loss: 0.00004209
Iteration 185/1000 | Loss: 0.00004950
Iteration 186/1000 | Loss: 0.00004080
Iteration 187/1000 | Loss: 0.00004575
Iteration 188/1000 | Loss: 0.00004878
Iteration 189/1000 | Loss: 0.00004997
Iteration 190/1000 | Loss: 0.00004774
Iteration 191/1000 | Loss: 0.00005034
Iteration 192/1000 | Loss: 0.00004759
Iteration 193/1000 | Loss: 0.00004927
Iteration 194/1000 | Loss: 0.00004116
Iteration 195/1000 | Loss: 0.00004905
Iteration 196/1000 | Loss: 0.00005354
Iteration 197/1000 | Loss: 0.00004468
Iteration 198/1000 | Loss: 0.00004612
Iteration 199/1000 | Loss: 0.00004130
Iteration 200/1000 | Loss: 0.00004089
Iteration 201/1000 | Loss: 0.00003701
Iteration 202/1000 | Loss: 0.00003667
Iteration 203/1000 | Loss: 0.00003984
Iteration 204/1000 | Loss: 0.00007821
Iteration 205/1000 | Loss: 0.00005425
Iteration 206/1000 | Loss: 0.00005179
Iteration 207/1000 | Loss: 0.00004615
Iteration 208/1000 | Loss: 0.00005257
Iteration 209/1000 | Loss: 0.00005297
Iteration 210/1000 | Loss: 0.00003948
Iteration 211/1000 | Loss: 0.00003868
Iteration 212/1000 | Loss: 0.00004222
Iteration 213/1000 | Loss: 0.00003858
Iteration 214/1000 | Loss: 0.00003765
Iteration 215/1000 | Loss: 0.00003644
Iteration 216/1000 | Loss: 0.00003642
Iteration 217/1000 | Loss: 0.00003641
Iteration 218/1000 | Loss: 0.00003641
Iteration 219/1000 | Loss: 0.00003641
Iteration 220/1000 | Loss: 0.00003641
Iteration 221/1000 | Loss: 0.00003641
Iteration 222/1000 | Loss: 0.00003640
Iteration 223/1000 | Loss: 0.00003639
Iteration 224/1000 | Loss: 0.00003634
Iteration 225/1000 | Loss: 0.00003634
Iteration 226/1000 | Loss: 0.00007333
Iteration 227/1000 | Loss: 0.00003661
Iteration 228/1000 | Loss: 0.00003618
Iteration 229/1000 | Loss: 0.00003614
Iteration 230/1000 | Loss: 0.00003614
Iteration 231/1000 | Loss: 0.00019256
Iteration 232/1000 | Loss: 0.00014556
Iteration 233/1000 | Loss: 0.00019671
Iteration 234/1000 | Loss: 0.00012280
Iteration 235/1000 | Loss: 0.00016447
Iteration 236/1000 | Loss: 0.00004718
Iteration 237/1000 | Loss: 0.00004027
Iteration 238/1000 | Loss: 0.00005132
Iteration 239/1000 | Loss: 0.00004659
Iteration 240/1000 | Loss: 0.00004998
Iteration 241/1000 | Loss: 0.00007146
Iteration 242/1000 | Loss: 0.00004523
Iteration 243/1000 | Loss: 0.00003866
Iteration 244/1000 | Loss: 0.00003523
Iteration 245/1000 | Loss: 0.00008371
Iteration 246/1000 | Loss: 0.00004114
Iteration 247/1000 | Loss: 0.00004277
Iteration 248/1000 | Loss: 0.00006936
Iteration 249/1000 | Loss: 0.00003619
Iteration 250/1000 | Loss: 0.00004124
Iteration 251/1000 | Loss: 0.00017958
Iteration 252/1000 | Loss: 0.00010993
Iteration 253/1000 | Loss: 0.00003809
Iteration 254/1000 | Loss: 0.00004949
Iteration 255/1000 | Loss: 0.00016304
Iteration 256/1000 | Loss: 0.00017409
Iteration 257/1000 | Loss: 0.00015239
Iteration 258/1000 | Loss: 0.00010174
Iteration 259/1000 | Loss: 0.00010263
Iteration 260/1000 | Loss: 0.00010332
Iteration 261/1000 | Loss: 0.00004602
Iteration 262/1000 | Loss: 0.00004186
Iteration 263/1000 | Loss: 0.00004088
Iteration 264/1000 | Loss: 0.00005742
Iteration 265/1000 | Loss: 0.00005571
Iteration 266/1000 | Loss: 0.00004727
Iteration 267/1000 | Loss: 0.00005271
Iteration 268/1000 | Loss: 0.00006716
Iteration 269/1000 | Loss: 0.00004924
Iteration 270/1000 | Loss: 0.00008329
Iteration 271/1000 | Loss: 0.00004441
Iteration 272/1000 | Loss: 0.00004283
Iteration 273/1000 | Loss: 0.00007985
Iteration 274/1000 | Loss: 0.00013282
Iteration 275/1000 | Loss: 0.00007108
Iteration 276/1000 | Loss: 0.00003886
Iteration 277/1000 | Loss: 0.00003587
Iteration 278/1000 | Loss: 0.00005453
Iteration 279/1000 | Loss: 0.00003569
Iteration 280/1000 | Loss: 0.00005928
Iteration 281/1000 | Loss: 0.00004022
Iteration 282/1000 | Loss: 0.00005176
Iteration 283/1000 | Loss: 0.00003521
Iteration 284/1000 | Loss: 0.00005281
Iteration 285/1000 | Loss: 0.00003495
Iteration 286/1000 | Loss: 0.00003490
Iteration 287/1000 | Loss: 0.00003472
Iteration 288/1000 | Loss: 0.00007483
Iteration 289/1000 | Loss: 0.00003472
Iteration 290/1000 | Loss: 0.00020790
Iteration 291/1000 | Loss: 0.00004912
Iteration 292/1000 | Loss: 0.00005038
Iteration 293/1000 | Loss: 0.00005713
Iteration 294/1000 | Loss: 0.00005460
Iteration 295/1000 | Loss: 0.00004185
Iteration 296/1000 | Loss: 0.00004035
Iteration 297/1000 | Loss: 0.00005861
Iteration 298/1000 | Loss: 0.00025472
Iteration 299/1000 | Loss: 0.00017565
Iteration 300/1000 | Loss: 0.00009067
Iteration 301/1000 | Loss: 0.00004477
Iteration 302/1000 | Loss: 0.00003854
Iteration 303/1000 | Loss: 0.00003664
Iteration 304/1000 | Loss: 0.00003561
Iteration 305/1000 | Loss: 0.00003496
Iteration 306/1000 | Loss: 0.00006301
Iteration 307/1000 | Loss: 0.00003424
Iteration 308/1000 | Loss: 0.00003406
Iteration 309/1000 | Loss: 0.00007147
Iteration 310/1000 | Loss: 0.00005172
Iteration 311/1000 | Loss: 0.00003386
Iteration 312/1000 | Loss: 0.00003386
Iteration 313/1000 | Loss: 0.00003386
Iteration 314/1000 | Loss: 0.00003386
Iteration 315/1000 | Loss: 0.00003386
Iteration 316/1000 | Loss: 0.00003386
Iteration 317/1000 | Loss: 0.00003386
Iteration 318/1000 | Loss: 0.00003386
Iteration 319/1000 | Loss: 0.00003386
Iteration 320/1000 | Loss: 0.00003386
Iteration 321/1000 | Loss: 0.00003386
Iteration 322/1000 | Loss: 0.00003386
Iteration 323/1000 | Loss: 0.00003385
Iteration 324/1000 | Loss: 0.00003385
Iteration 325/1000 | Loss: 0.00003384
Iteration 326/1000 | Loss: 0.00003383
Iteration 327/1000 | Loss: 0.00003383
Iteration 328/1000 | Loss: 0.00003382
Iteration 329/1000 | Loss: 0.00003382
Iteration 330/1000 | Loss: 0.00003382
Iteration 331/1000 | Loss: 0.00003382
Iteration 332/1000 | Loss: 0.00003382
Iteration 333/1000 | Loss: 0.00003381
Iteration 334/1000 | Loss: 0.00003380
Iteration 335/1000 | Loss: 0.00003379
Iteration 336/1000 | Loss: 0.00003379
Iteration 337/1000 | Loss: 0.00003379
Iteration 338/1000 | Loss: 0.00003379
Iteration 339/1000 | Loss: 0.00003379
Iteration 340/1000 | Loss: 0.00003379
Iteration 341/1000 | Loss: 0.00003379
Iteration 342/1000 | Loss: 0.00003379
Iteration 343/1000 | Loss: 0.00003379
Iteration 344/1000 | Loss: 0.00003379
Iteration 345/1000 | Loss: 0.00003379
Iteration 346/1000 | Loss: 0.00003379
Iteration 347/1000 | Loss: 0.00003379
Iteration 348/1000 | Loss: 0.00003379
Iteration 349/1000 | Loss: 0.00003379
Iteration 350/1000 | Loss: 0.00003379
Iteration 351/1000 | Loss: 0.00003379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [3.378972178325057e-05, 3.378972178325057e-05, 3.378972178325057e-05, 3.378972178325057e-05, 3.378972178325057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.378972178325057e-05

Optimization complete. Final v2v error: 4.414623260498047 mm

Highest mean error: 9.855932235717773 mm for frame 43

Lowest mean error: 3.3459599018096924 mm for frame 0

Saving results

Total time: 533.7566430568695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442736
Iteration 2/25 | Loss: 0.00110921
Iteration 3/25 | Loss: 0.00098674
Iteration 4/25 | Loss: 0.00097146
Iteration 5/25 | Loss: 0.00096744
Iteration 6/25 | Loss: 0.00096644
Iteration 7/25 | Loss: 0.00096644
Iteration 8/25 | Loss: 0.00096644
Iteration 9/25 | Loss: 0.00096644
Iteration 10/25 | Loss: 0.00096644
Iteration 11/25 | Loss: 0.00096644
Iteration 12/25 | Loss: 0.00096644
Iteration 13/25 | Loss: 0.00096644
Iteration 14/25 | Loss: 0.00096644
Iteration 15/25 | Loss: 0.00096644
Iteration 16/25 | Loss: 0.00096644
Iteration 17/25 | Loss: 0.00096644
Iteration 18/25 | Loss: 0.00096644
Iteration 19/25 | Loss: 0.00096644
Iteration 20/25 | Loss: 0.00096644
Iteration 21/25 | Loss: 0.00096644
Iteration 22/25 | Loss: 0.00096644
Iteration 23/25 | Loss: 0.00096644
Iteration 24/25 | Loss: 0.00096644
Iteration 25/25 | Loss: 0.00096644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09536862
Iteration 2/25 | Loss: 0.00211393
Iteration 3/25 | Loss: 0.00211391
Iteration 4/25 | Loss: 0.00211391
Iteration 5/25 | Loss: 0.00211390
Iteration 6/25 | Loss: 0.00211390
Iteration 7/25 | Loss: 0.00211390
Iteration 8/25 | Loss: 0.00211390
Iteration 9/25 | Loss: 0.00211390
Iteration 10/25 | Loss: 0.00211390
Iteration 11/25 | Loss: 0.00211390
Iteration 12/25 | Loss: 0.00211390
Iteration 13/25 | Loss: 0.00211390
Iteration 14/25 | Loss: 0.00211390
Iteration 15/25 | Loss: 0.00211390
Iteration 16/25 | Loss: 0.00211390
Iteration 17/25 | Loss: 0.00211390
Iteration 18/25 | Loss: 0.00211390
Iteration 19/25 | Loss: 0.00211390
Iteration 20/25 | Loss: 0.00211390
Iteration 21/25 | Loss: 0.00211390
Iteration 22/25 | Loss: 0.00211390
Iteration 23/25 | Loss: 0.00211390
Iteration 24/25 | Loss: 0.00211390
Iteration 25/25 | Loss: 0.00211390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211390
Iteration 2/1000 | Loss: 0.00003711
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001290
Iteration 6/1000 | Loss: 0.00001166
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001022
Iteration 10/1000 | Loss: 0.00001011
Iteration 11/1000 | Loss: 0.00000994
Iteration 12/1000 | Loss: 0.00000992
Iteration 13/1000 | Loss: 0.00000986
Iteration 14/1000 | Loss: 0.00000976
Iteration 15/1000 | Loss: 0.00000974
Iteration 16/1000 | Loss: 0.00000968
Iteration 17/1000 | Loss: 0.00000968
Iteration 18/1000 | Loss: 0.00000959
Iteration 19/1000 | Loss: 0.00000957
Iteration 20/1000 | Loss: 0.00000955
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000952
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000950
Iteration 29/1000 | Loss: 0.00000950
Iteration 30/1000 | Loss: 0.00000949
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000949
Iteration 33/1000 | Loss: 0.00000948
Iteration 34/1000 | Loss: 0.00000947
Iteration 35/1000 | Loss: 0.00000946
Iteration 36/1000 | Loss: 0.00000945
Iteration 37/1000 | Loss: 0.00000945
Iteration 38/1000 | Loss: 0.00000945
Iteration 39/1000 | Loss: 0.00000945
Iteration 40/1000 | Loss: 0.00000945
Iteration 41/1000 | Loss: 0.00000945
Iteration 42/1000 | Loss: 0.00000945
Iteration 43/1000 | Loss: 0.00000945
Iteration 44/1000 | Loss: 0.00000945
Iteration 45/1000 | Loss: 0.00000945
Iteration 46/1000 | Loss: 0.00000945
Iteration 47/1000 | Loss: 0.00000944
Iteration 48/1000 | Loss: 0.00000944
Iteration 49/1000 | Loss: 0.00000944
Iteration 50/1000 | Loss: 0.00000944
Iteration 51/1000 | Loss: 0.00000944
Iteration 52/1000 | Loss: 0.00000944
Iteration 53/1000 | Loss: 0.00000943
Iteration 54/1000 | Loss: 0.00000943
Iteration 55/1000 | Loss: 0.00000942
Iteration 56/1000 | Loss: 0.00000942
Iteration 57/1000 | Loss: 0.00000942
Iteration 58/1000 | Loss: 0.00000941
Iteration 59/1000 | Loss: 0.00000941
Iteration 60/1000 | Loss: 0.00000941
Iteration 61/1000 | Loss: 0.00000941
Iteration 62/1000 | Loss: 0.00000941
Iteration 63/1000 | Loss: 0.00000940
Iteration 64/1000 | Loss: 0.00000940
Iteration 65/1000 | Loss: 0.00000940
Iteration 66/1000 | Loss: 0.00000940
Iteration 67/1000 | Loss: 0.00000940
Iteration 68/1000 | Loss: 0.00000940
Iteration 69/1000 | Loss: 0.00000940
Iteration 70/1000 | Loss: 0.00000939
Iteration 71/1000 | Loss: 0.00000939
Iteration 72/1000 | Loss: 0.00000939
Iteration 73/1000 | Loss: 0.00000939
Iteration 74/1000 | Loss: 0.00000939
Iteration 75/1000 | Loss: 0.00000939
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000938
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000938
Iteration 81/1000 | Loss: 0.00000938
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000938
Iteration 84/1000 | Loss: 0.00000938
Iteration 85/1000 | Loss: 0.00000938
Iteration 86/1000 | Loss: 0.00000938
Iteration 87/1000 | Loss: 0.00000937
Iteration 88/1000 | Loss: 0.00000937
Iteration 89/1000 | Loss: 0.00000937
Iteration 90/1000 | Loss: 0.00000937
Iteration 91/1000 | Loss: 0.00000937
Iteration 92/1000 | Loss: 0.00000937
Iteration 93/1000 | Loss: 0.00000937
Iteration 94/1000 | Loss: 0.00000937
Iteration 95/1000 | Loss: 0.00000936
Iteration 96/1000 | Loss: 0.00000936
Iteration 97/1000 | Loss: 0.00000936
Iteration 98/1000 | Loss: 0.00000936
Iteration 99/1000 | Loss: 0.00000936
Iteration 100/1000 | Loss: 0.00000936
Iteration 101/1000 | Loss: 0.00000936
Iteration 102/1000 | Loss: 0.00000936
Iteration 103/1000 | Loss: 0.00000936
Iteration 104/1000 | Loss: 0.00000936
Iteration 105/1000 | Loss: 0.00000935
Iteration 106/1000 | Loss: 0.00000935
Iteration 107/1000 | Loss: 0.00000935
Iteration 108/1000 | Loss: 0.00000935
Iteration 109/1000 | Loss: 0.00000935
Iteration 110/1000 | Loss: 0.00000935
Iteration 111/1000 | Loss: 0.00000935
Iteration 112/1000 | Loss: 0.00000935
Iteration 113/1000 | Loss: 0.00000935
Iteration 114/1000 | Loss: 0.00000935
Iteration 115/1000 | Loss: 0.00000935
Iteration 116/1000 | Loss: 0.00000935
Iteration 117/1000 | Loss: 0.00000935
Iteration 118/1000 | Loss: 0.00000935
Iteration 119/1000 | Loss: 0.00000934
Iteration 120/1000 | Loss: 0.00000934
Iteration 121/1000 | Loss: 0.00000934
Iteration 122/1000 | Loss: 0.00000934
Iteration 123/1000 | Loss: 0.00000934
Iteration 124/1000 | Loss: 0.00000934
Iteration 125/1000 | Loss: 0.00000934
Iteration 126/1000 | Loss: 0.00000934
Iteration 127/1000 | Loss: 0.00000934
Iteration 128/1000 | Loss: 0.00000934
Iteration 129/1000 | Loss: 0.00000933
Iteration 130/1000 | Loss: 0.00000933
Iteration 131/1000 | Loss: 0.00000933
Iteration 132/1000 | Loss: 0.00000933
Iteration 133/1000 | Loss: 0.00000933
Iteration 134/1000 | Loss: 0.00000933
Iteration 135/1000 | Loss: 0.00000932
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000932
Iteration 139/1000 | Loss: 0.00000932
Iteration 140/1000 | Loss: 0.00000932
Iteration 141/1000 | Loss: 0.00000932
Iteration 142/1000 | Loss: 0.00000932
Iteration 143/1000 | Loss: 0.00000932
Iteration 144/1000 | Loss: 0.00000932
Iteration 145/1000 | Loss: 0.00000932
Iteration 146/1000 | Loss: 0.00000932
Iteration 147/1000 | Loss: 0.00000932
Iteration 148/1000 | Loss: 0.00000932
Iteration 149/1000 | Loss: 0.00000931
Iteration 150/1000 | Loss: 0.00000931
Iteration 151/1000 | Loss: 0.00000931
Iteration 152/1000 | Loss: 0.00000931
Iteration 153/1000 | Loss: 0.00000931
Iteration 154/1000 | Loss: 0.00000931
Iteration 155/1000 | Loss: 0.00000931
Iteration 156/1000 | Loss: 0.00000930
Iteration 157/1000 | Loss: 0.00000930
Iteration 158/1000 | Loss: 0.00000930
Iteration 159/1000 | Loss: 0.00000930
Iteration 160/1000 | Loss: 0.00000930
Iteration 161/1000 | Loss: 0.00000930
Iteration 162/1000 | Loss: 0.00000930
Iteration 163/1000 | Loss: 0.00000930
Iteration 164/1000 | Loss: 0.00000930
Iteration 165/1000 | Loss: 0.00000930
Iteration 166/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [9.302936632593628e-06, 9.302936632593628e-06, 9.302936632593628e-06, 9.302936632593628e-06, 9.302936632593628e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.302936632593628e-06

Optimization complete. Final v2v error: 2.61405611038208 mm

Highest mean error: 3.0369136333465576 mm for frame 57

Lowest mean error: 2.2875707149505615 mm for frame 14

Saving results

Total time: 39.45196795463562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458810
Iteration 2/25 | Loss: 0.00116409
Iteration 3/25 | Loss: 0.00099267
Iteration 4/25 | Loss: 0.00097559
Iteration 5/25 | Loss: 0.00097218
Iteration 6/25 | Loss: 0.00097141
Iteration 7/25 | Loss: 0.00097141
Iteration 8/25 | Loss: 0.00097141
Iteration 9/25 | Loss: 0.00097141
Iteration 10/25 | Loss: 0.00097141
Iteration 11/25 | Loss: 0.00097141
Iteration 12/25 | Loss: 0.00097141
Iteration 13/25 | Loss: 0.00097141
Iteration 14/25 | Loss: 0.00097141
Iteration 15/25 | Loss: 0.00097141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009714060579426587, 0.0009714060579426587, 0.0009714060579426587, 0.0009714060579426587, 0.0009714060579426587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009714060579426587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75879049
Iteration 2/25 | Loss: 0.00193910
Iteration 3/25 | Loss: 0.00193909
Iteration 4/25 | Loss: 0.00193909
Iteration 5/25 | Loss: 0.00193909
Iteration 6/25 | Loss: 0.00193909
Iteration 7/25 | Loss: 0.00193909
Iteration 8/25 | Loss: 0.00193909
Iteration 9/25 | Loss: 0.00193909
Iteration 10/25 | Loss: 0.00193909
Iteration 11/25 | Loss: 0.00193909
Iteration 12/25 | Loss: 0.00193909
Iteration 13/25 | Loss: 0.00193909
Iteration 14/25 | Loss: 0.00193909
Iteration 15/25 | Loss: 0.00193909
Iteration 16/25 | Loss: 0.00193909
Iteration 17/25 | Loss: 0.00193909
Iteration 18/25 | Loss: 0.00193909
Iteration 19/25 | Loss: 0.00193909
Iteration 20/25 | Loss: 0.00193909
Iteration 21/25 | Loss: 0.00193909
Iteration 22/25 | Loss: 0.00193909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019390886882320046, 0.0019390886882320046, 0.0019390886882320046, 0.0019390886882320046, 0.0019390886882320046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019390886882320046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193909
Iteration 2/1000 | Loss: 0.00002714
Iteration 3/1000 | Loss: 0.00001510
Iteration 4/1000 | Loss: 0.00001252
Iteration 5/1000 | Loss: 0.00001109
Iteration 6/1000 | Loss: 0.00001035
Iteration 7/1000 | Loss: 0.00000986
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000913
Iteration 10/1000 | Loss: 0.00000895
Iteration 11/1000 | Loss: 0.00000872
Iteration 12/1000 | Loss: 0.00000867
Iteration 13/1000 | Loss: 0.00000860
Iteration 14/1000 | Loss: 0.00000846
Iteration 15/1000 | Loss: 0.00000846
Iteration 16/1000 | Loss: 0.00000841
Iteration 17/1000 | Loss: 0.00000838
Iteration 18/1000 | Loss: 0.00000837
Iteration 19/1000 | Loss: 0.00000837
Iteration 20/1000 | Loss: 0.00000837
Iteration 21/1000 | Loss: 0.00000836
Iteration 22/1000 | Loss: 0.00000836
Iteration 23/1000 | Loss: 0.00000835
Iteration 24/1000 | Loss: 0.00000834
Iteration 25/1000 | Loss: 0.00000833
Iteration 26/1000 | Loss: 0.00000832
Iteration 27/1000 | Loss: 0.00000832
Iteration 28/1000 | Loss: 0.00000832
Iteration 29/1000 | Loss: 0.00000831
Iteration 30/1000 | Loss: 0.00000827
Iteration 31/1000 | Loss: 0.00000823
Iteration 32/1000 | Loss: 0.00000822
Iteration 33/1000 | Loss: 0.00000821
Iteration 34/1000 | Loss: 0.00000820
Iteration 35/1000 | Loss: 0.00000819
Iteration 36/1000 | Loss: 0.00000819
Iteration 37/1000 | Loss: 0.00000819
Iteration 38/1000 | Loss: 0.00000818
Iteration 39/1000 | Loss: 0.00000818
Iteration 40/1000 | Loss: 0.00000817
Iteration 41/1000 | Loss: 0.00000817
Iteration 42/1000 | Loss: 0.00000816
Iteration 43/1000 | Loss: 0.00000816
Iteration 44/1000 | Loss: 0.00000815
Iteration 45/1000 | Loss: 0.00000815
Iteration 46/1000 | Loss: 0.00000815
Iteration 47/1000 | Loss: 0.00000815
Iteration 48/1000 | Loss: 0.00000815
Iteration 49/1000 | Loss: 0.00000815
Iteration 50/1000 | Loss: 0.00000815
Iteration 51/1000 | Loss: 0.00000815
Iteration 52/1000 | Loss: 0.00000815
Iteration 53/1000 | Loss: 0.00000815
Iteration 54/1000 | Loss: 0.00000814
Iteration 55/1000 | Loss: 0.00000814
Iteration 56/1000 | Loss: 0.00000814
Iteration 57/1000 | Loss: 0.00000814
Iteration 58/1000 | Loss: 0.00000813
Iteration 59/1000 | Loss: 0.00000813
Iteration 60/1000 | Loss: 0.00000812
Iteration 61/1000 | Loss: 0.00000812
Iteration 62/1000 | Loss: 0.00000812
Iteration 63/1000 | Loss: 0.00000811
Iteration 64/1000 | Loss: 0.00000811
Iteration 65/1000 | Loss: 0.00000810
Iteration 66/1000 | Loss: 0.00000810
Iteration 67/1000 | Loss: 0.00000810
Iteration 68/1000 | Loss: 0.00000810
Iteration 69/1000 | Loss: 0.00000809
Iteration 70/1000 | Loss: 0.00000809
Iteration 71/1000 | Loss: 0.00000809
Iteration 72/1000 | Loss: 0.00000809
Iteration 73/1000 | Loss: 0.00000809
Iteration 74/1000 | Loss: 0.00000809
Iteration 75/1000 | Loss: 0.00000808
Iteration 76/1000 | Loss: 0.00000808
Iteration 77/1000 | Loss: 0.00000808
Iteration 78/1000 | Loss: 0.00000808
Iteration 79/1000 | Loss: 0.00000808
Iteration 80/1000 | Loss: 0.00000808
Iteration 81/1000 | Loss: 0.00000808
Iteration 82/1000 | Loss: 0.00000808
Iteration 83/1000 | Loss: 0.00000808
Iteration 84/1000 | Loss: 0.00000808
Iteration 85/1000 | Loss: 0.00000807
Iteration 86/1000 | Loss: 0.00000807
Iteration 87/1000 | Loss: 0.00000807
Iteration 88/1000 | Loss: 0.00000807
Iteration 89/1000 | Loss: 0.00000807
Iteration 90/1000 | Loss: 0.00000807
Iteration 91/1000 | Loss: 0.00000807
Iteration 92/1000 | Loss: 0.00000806
Iteration 93/1000 | Loss: 0.00000806
Iteration 94/1000 | Loss: 0.00000806
Iteration 95/1000 | Loss: 0.00000806
Iteration 96/1000 | Loss: 0.00000806
Iteration 97/1000 | Loss: 0.00000805
Iteration 98/1000 | Loss: 0.00000805
Iteration 99/1000 | Loss: 0.00000805
Iteration 100/1000 | Loss: 0.00000805
Iteration 101/1000 | Loss: 0.00000805
Iteration 102/1000 | Loss: 0.00000804
Iteration 103/1000 | Loss: 0.00000804
Iteration 104/1000 | Loss: 0.00000804
Iteration 105/1000 | Loss: 0.00000804
Iteration 106/1000 | Loss: 0.00000804
Iteration 107/1000 | Loss: 0.00000804
Iteration 108/1000 | Loss: 0.00000803
Iteration 109/1000 | Loss: 0.00000803
Iteration 110/1000 | Loss: 0.00000803
Iteration 111/1000 | Loss: 0.00000803
Iteration 112/1000 | Loss: 0.00000802
Iteration 113/1000 | Loss: 0.00000802
Iteration 114/1000 | Loss: 0.00000802
Iteration 115/1000 | Loss: 0.00000802
Iteration 116/1000 | Loss: 0.00000802
Iteration 117/1000 | Loss: 0.00000802
Iteration 118/1000 | Loss: 0.00000802
Iteration 119/1000 | Loss: 0.00000802
Iteration 120/1000 | Loss: 0.00000801
Iteration 121/1000 | Loss: 0.00000801
Iteration 122/1000 | Loss: 0.00000801
Iteration 123/1000 | Loss: 0.00000801
Iteration 124/1000 | Loss: 0.00000801
Iteration 125/1000 | Loss: 0.00000801
Iteration 126/1000 | Loss: 0.00000801
Iteration 127/1000 | Loss: 0.00000801
Iteration 128/1000 | Loss: 0.00000800
Iteration 129/1000 | Loss: 0.00000800
Iteration 130/1000 | Loss: 0.00000800
Iteration 131/1000 | Loss: 0.00000800
Iteration 132/1000 | Loss: 0.00000800
Iteration 133/1000 | Loss: 0.00000800
Iteration 134/1000 | Loss: 0.00000800
Iteration 135/1000 | Loss: 0.00000800
Iteration 136/1000 | Loss: 0.00000800
Iteration 137/1000 | Loss: 0.00000800
Iteration 138/1000 | Loss: 0.00000800
Iteration 139/1000 | Loss: 0.00000799
Iteration 140/1000 | Loss: 0.00000799
Iteration 141/1000 | Loss: 0.00000799
Iteration 142/1000 | Loss: 0.00000799
Iteration 143/1000 | Loss: 0.00000799
Iteration 144/1000 | Loss: 0.00000799
Iteration 145/1000 | Loss: 0.00000799
Iteration 146/1000 | Loss: 0.00000799
Iteration 147/1000 | Loss: 0.00000799
Iteration 148/1000 | Loss: 0.00000799
Iteration 149/1000 | Loss: 0.00000799
Iteration 150/1000 | Loss: 0.00000798
Iteration 151/1000 | Loss: 0.00000798
Iteration 152/1000 | Loss: 0.00000798
Iteration 153/1000 | Loss: 0.00000798
Iteration 154/1000 | Loss: 0.00000798
Iteration 155/1000 | Loss: 0.00000798
Iteration 156/1000 | Loss: 0.00000798
Iteration 157/1000 | Loss: 0.00000798
Iteration 158/1000 | Loss: 0.00000798
Iteration 159/1000 | Loss: 0.00000798
Iteration 160/1000 | Loss: 0.00000798
Iteration 161/1000 | Loss: 0.00000798
Iteration 162/1000 | Loss: 0.00000798
Iteration 163/1000 | Loss: 0.00000798
Iteration 164/1000 | Loss: 0.00000798
Iteration 165/1000 | Loss: 0.00000798
Iteration 166/1000 | Loss: 0.00000798
Iteration 167/1000 | Loss: 0.00000798
Iteration 168/1000 | Loss: 0.00000798
Iteration 169/1000 | Loss: 0.00000798
Iteration 170/1000 | Loss: 0.00000798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [7.976200322445948e-06, 7.976200322445948e-06, 7.976200322445948e-06, 7.976200322445948e-06, 7.976200322445948e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.976200322445948e-06

Optimization complete. Final v2v error: 2.3933308124542236 mm

Highest mean error: 3.1427690982818604 mm for frame 171

Lowest mean error: 2.1544995307922363 mm for frame 74

Saving results

Total time: 45.8423068523407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918381
Iteration 2/25 | Loss: 0.00218324
Iteration 3/25 | Loss: 0.00122968
Iteration 4/25 | Loss: 0.00114064
Iteration 5/25 | Loss: 0.00109520
Iteration 6/25 | Loss: 0.00106455
Iteration 7/25 | Loss: 0.00104147
Iteration 8/25 | Loss: 0.00100688
Iteration 9/25 | Loss: 0.00098449
Iteration 10/25 | Loss: 0.00096606
Iteration 11/25 | Loss: 0.00096069
Iteration 12/25 | Loss: 0.00095727
Iteration 13/25 | Loss: 0.00095543
Iteration 14/25 | Loss: 0.00095473
Iteration 15/25 | Loss: 0.00095448
Iteration 16/25 | Loss: 0.00095429
Iteration 17/25 | Loss: 0.00095406
Iteration 18/25 | Loss: 0.00095577
Iteration 19/25 | Loss: 0.00095175
Iteration 20/25 | Loss: 0.00095114
Iteration 21/25 | Loss: 0.00095093
Iteration 22/25 | Loss: 0.00095088
Iteration 23/25 | Loss: 0.00095088
Iteration 24/25 | Loss: 0.00095088
Iteration 25/25 | Loss: 0.00095088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83052862
Iteration 2/25 | Loss: 0.00185879
Iteration 3/25 | Loss: 0.00185879
Iteration 4/25 | Loss: 0.00185879
Iteration 5/25 | Loss: 0.00185879
Iteration 6/25 | Loss: 0.00185879
Iteration 7/25 | Loss: 0.00185879
Iteration 8/25 | Loss: 0.00185879
Iteration 9/25 | Loss: 0.00185879
Iteration 10/25 | Loss: 0.00185879
Iteration 11/25 | Loss: 0.00185879
Iteration 12/25 | Loss: 0.00185879
Iteration 13/25 | Loss: 0.00185879
Iteration 14/25 | Loss: 0.00185879
Iteration 15/25 | Loss: 0.00185879
Iteration 16/25 | Loss: 0.00185879
Iteration 17/25 | Loss: 0.00185879
Iteration 18/25 | Loss: 0.00185879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018587850499898195, 0.0018587850499898195, 0.0018587850499898195, 0.0018587850499898195, 0.0018587850499898195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018587850499898195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185879
Iteration 2/1000 | Loss: 0.00001644
Iteration 3/1000 | Loss: 0.00001162
Iteration 4/1000 | Loss: 0.00001076
Iteration 5/1000 | Loss: 0.00001003
Iteration 6/1000 | Loss: 0.00000959
Iteration 7/1000 | Loss: 0.00000930
Iteration 8/1000 | Loss: 0.00000904
Iteration 9/1000 | Loss: 0.00000903
Iteration 10/1000 | Loss: 0.00000888
Iteration 11/1000 | Loss: 0.00000881
Iteration 12/1000 | Loss: 0.00000879
Iteration 13/1000 | Loss: 0.00000878
Iteration 14/1000 | Loss: 0.00000878
Iteration 15/1000 | Loss: 0.00000877
Iteration 16/1000 | Loss: 0.00000876
Iteration 17/1000 | Loss: 0.00000875
Iteration 18/1000 | Loss: 0.00000874
Iteration 19/1000 | Loss: 0.00000873
Iteration 20/1000 | Loss: 0.00000873
Iteration 21/1000 | Loss: 0.00000871
Iteration 22/1000 | Loss: 0.00000869
Iteration 23/1000 | Loss: 0.00000868
Iteration 24/1000 | Loss: 0.00000868
Iteration 25/1000 | Loss: 0.00000867
Iteration 26/1000 | Loss: 0.00000866
Iteration 27/1000 | Loss: 0.00000865
Iteration 28/1000 | Loss: 0.00000865
Iteration 29/1000 | Loss: 0.00000864
Iteration 30/1000 | Loss: 0.00000863
Iteration 31/1000 | Loss: 0.00000863
Iteration 32/1000 | Loss: 0.00000862
Iteration 33/1000 | Loss: 0.00000862
Iteration 34/1000 | Loss: 0.00000862
Iteration 35/1000 | Loss: 0.00000861
Iteration 36/1000 | Loss: 0.00000860
Iteration 37/1000 | Loss: 0.00000860
Iteration 38/1000 | Loss: 0.00000859
Iteration 39/1000 | Loss: 0.00000859
Iteration 40/1000 | Loss: 0.00000859
Iteration 41/1000 | Loss: 0.00000859
Iteration 42/1000 | Loss: 0.00000858
Iteration 43/1000 | Loss: 0.00000858
Iteration 44/1000 | Loss: 0.00000857
Iteration 45/1000 | Loss: 0.00000856
Iteration 46/1000 | Loss: 0.00000856
Iteration 47/1000 | Loss: 0.00000856
Iteration 48/1000 | Loss: 0.00000856
Iteration 49/1000 | Loss: 0.00000855
Iteration 50/1000 | Loss: 0.00000855
Iteration 51/1000 | Loss: 0.00000854
Iteration 52/1000 | Loss: 0.00000854
Iteration 53/1000 | Loss: 0.00000854
Iteration 54/1000 | Loss: 0.00000853
Iteration 55/1000 | Loss: 0.00000852
Iteration 56/1000 | Loss: 0.00000852
Iteration 57/1000 | Loss: 0.00000852
Iteration 58/1000 | Loss: 0.00000852
Iteration 59/1000 | Loss: 0.00000851
Iteration 60/1000 | Loss: 0.00000851
Iteration 61/1000 | Loss: 0.00000851
Iteration 62/1000 | Loss: 0.00000851
Iteration 63/1000 | Loss: 0.00000851
Iteration 64/1000 | Loss: 0.00000851
Iteration 65/1000 | Loss: 0.00000850
Iteration 66/1000 | Loss: 0.00000850
Iteration 67/1000 | Loss: 0.00000850
Iteration 68/1000 | Loss: 0.00000849
Iteration 69/1000 | Loss: 0.00000848
Iteration 70/1000 | Loss: 0.00000848
Iteration 71/1000 | Loss: 0.00000847
Iteration 72/1000 | Loss: 0.00000847
Iteration 73/1000 | Loss: 0.00000847
Iteration 74/1000 | Loss: 0.00000847
Iteration 75/1000 | Loss: 0.00000847
Iteration 76/1000 | Loss: 0.00000847
Iteration 77/1000 | Loss: 0.00000847
Iteration 78/1000 | Loss: 0.00000847
Iteration 79/1000 | Loss: 0.00000847
Iteration 80/1000 | Loss: 0.00000847
Iteration 81/1000 | Loss: 0.00000846
Iteration 82/1000 | Loss: 0.00000846
Iteration 83/1000 | Loss: 0.00000846
Iteration 84/1000 | Loss: 0.00000846
Iteration 85/1000 | Loss: 0.00000846
Iteration 86/1000 | Loss: 0.00000846
Iteration 87/1000 | Loss: 0.00000846
Iteration 88/1000 | Loss: 0.00000846
Iteration 89/1000 | Loss: 0.00000845
Iteration 90/1000 | Loss: 0.00000845
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000844
Iteration 93/1000 | Loss: 0.00000844
Iteration 94/1000 | Loss: 0.00000844
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000843
Iteration 97/1000 | Loss: 0.00000843
Iteration 98/1000 | Loss: 0.00000843
Iteration 99/1000 | Loss: 0.00000843
Iteration 100/1000 | Loss: 0.00000843
Iteration 101/1000 | Loss: 0.00000843
Iteration 102/1000 | Loss: 0.00000843
Iteration 103/1000 | Loss: 0.00000842
Iteration 104/1000 | Loss: 0.00000842
Iteration 105/1000 | Loss: 0.00000842
Iteration 106/1000 | Loss: 0.00000842
Iteration 107/1000 | Loss: 0.00000842
Iteration 108/1000 | Loss: 0.00000842
Iteration 109/1000 | Loss: 0.00000842
Iteration 110/1000 | Loss: 0.00000842
Iteration 111/1000 | Loss: 0.00000842
Iteration 112/1000 | Loss: 0.00000842
Iteration 113/1000 | Loss: 0.00000841
Iteration 114/1000 | Loss: 0.00000841
Iteration 115/1000 | Loss: 0.00000841
Iteration 116/1000 | Loss: 0.00000841
Iteration 117/1000 | Loss: 0.00000841
Iteration 118/1000 | Loss: 0.00000841
Iteration 119/1000 | Loss: 0.00000841
Iteration 120/1000 | Loss: 0.00000841
Iteration 121/1000 | Loss: 0.00000841
Iteration 122/1000 | Loss: 0.00000841
Iteration 123/1000 | Loss: 0.00000841
Iteration 124/1000 | Loss: 0.00000841
Iteration 125/1000 | Loss: 0.00000841
Iteration 126/1000 | Loss: 0.00000841
Iteration 127/1000 | Loss: 0.00000841
Iteration 128/1000 | Loss: 0.00000841
Iteration 129/1000 | Loss: 0.00000841
Iteration 130/1000 | Loss: 0.00000840
Iteration 131/1000 | Loss: 0.00000840
Iteration 132/1000 | Loss: 0.00000840
Iteration 133/1000 | Loss: 0.00000840
Iteration 134/1000 | Loss: 0.00000840
Iteration 135/1000 | Loss: 0.00000840
Iteration 136/1000 | Loss: 0.00000840
Iteration 137/1000 | Loss: 0.00000840
Iteration 138/1000 | Loss: 0.00000840
Iteration 139/1000 | Loss: 0.00000840
Iteration 140/1000 | Loss: 0.00000840
Iteration 141/1000 | Loss: 0.00000840
Iteration 142/1000 | Loss: 0.00000840
Iteration 143/1000 | Loss: 0.00000840
Iteration 144/1000 | Loss: 0.00000840
Iteration 145/1000 | Loss: 0.00000840
Iteration 146/1000 | Loss: 0.00000840
Iteration 147/1000 | Loss: 0.00000840
Iteration 148/1000 | Loss: 0.00000840
Iteration 149/1000 | Loss: 0.00000840
Iteration 150/1000 | Loss: 0.00000840
Iteration 151/1000 | Loss: 0.00000840
Iteration 152/1000 | Loss: 0.00000840
Iteration 153/1000 | Loss: 0.00000840
Iteration 154/1000 | Loss: 0.00000840
Iteration 155/1000 | Loss: 0.00000840
Iteration 156/1000 | Loss: 0.00000840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [8.401907507504802e-06, 8.401907507504802e-06, 8.401907507504802e-06, 8.401907507504802e-06, 8.401907507504802e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.401907507504802e-06

Optimization complete. Final v2v error: 2.465768814086914 mm

Highest mean error: 2.7953872680664062 mm for frame 145

Lowest mean error: 2.201479196548462 mm for frame 20

Saving results

Total time: 71.56095504760742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820689
Iteration 2/25 | Loss: 0.00124503
Iteration 3/25 | Loss: 0.00107831
Iteration 4/25 | Loss: 0.00106023
Iteration 5/25 | Loss: 0.00105575
Iteration 6/25 | Loss: 0.00105402
Iteration 7/25 | Loss: 0.00105402
Iteration 8/25 | Loss: 0.00105402
Iteration 9/25 | Loss: 0.00105402
Iteration 10/25 | Loss: 0.00105402
Iteration 11/25 | Loss: 0.00105402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010540187358856201, 0.0010540187358856201, 0.0010540187358856201, 0.0010540187358856201, 0.0010540187358856201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010540187358856201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09892118
Iteration 2/25 | Loss: 0.00129808
Iteration 3/25 | Loss: 0.00129806
Iteration 4/25 | Loss: 0.00129806
Iteration 5/25 | Loss: 0.00129806
Iteration 6/25 | Loss: 0.00129806
Iteration 7/25 | Loss: 0.00129806
Iteration 8/25 | Loss: 0.00129806
Iteration 9/25 | Loss: 0.00129806
Iteration 10/25 | Loss: 0.00129806
Iteration 11/25 | Loss: 0.00129806
Iteration 12/25 | Loss: 0.00129806
Iteration 13/25 | Loss: 0.00129806
Iteration 14/25 | Loss: 0.00129806
Iteration 15/25 | Loss: 0.00129806
Iteration 16/25 | Loss: 0.00129806
Iteration 17/25 | Loss: 0.00129806
Iteration 18/25 | Loss: 0.00129806
Iteration 19/25 | Loss: 0.00129806
Iteration 20/25 | Loss: 0.00129806
Iteration 21/25 | Loss: 0.00129806
Iteration 22/25 | Loss: 0.00129806
Iteration 23/25 | Loss: 0.00129806
Iteration 24/25 | Loss: 0.00129806
Iteration 25/25 | Loss: 0.00129806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012980583123862743, 0.0012980583123862743, 0.0012980583123862743, 0.0012980583123862743, 0.0012980583123862743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012980583123862743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129806
Iteration 2/1000 | Loss: 0.00002757
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001536
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001490
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001489
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001483
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001481
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001479
Iteration 92/1000 | Loss: 0.00001479
Iteration 93/1000 | Loss: 0.00001479
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001479
Iteration 101/1000 | Loss: 0.00001479
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.4790702152822632e-05, 1.4790702152822632e-05, 1.4790702152822632e-05, 1.4790702152822632e-05, 1.4790702152822632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4790702152822632e-05

Optimization complete. Final v2v error: 3.2151012420654297 mm

Highest mean error: 4.105949401855469 mm for frame 52

Lowest mean error: 2.6644392013549805 mm for frame 219

Saving results

Total time: 36.25368094444275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846504
Iteration 2/25 | Loss: 0.00121902
Iteration 3/25 | Loss: 0.00104284
Iteration 4/25 | Loss: 0.00101547
Iteration 5/25 | Loss: 0.00100815
Iteration 6/25 | Loss: 0.00100604
Iteration 7/25 | Loss: 0.00100591
Iteration 8/25 | Loss: 0.00100591
Iteration 9/25 | Loss: 0.00100591
Iteration 10/25 | Loss: 0.00100591
Iteration 11/25 | Loss: 0.00100591
Iteration 12/25 | Loss: 0.00100591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010059095220640302, 0.0010059095220640302, 0.0010059095220640302, 0.0010059095220640302, 0.0010059095220640302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010059095220640302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62025094
Iteration 2/25 | Loss: 0.00182795
Iteration 3/25 | Loss: 0.00182793
Iteration 4/25 | Loss: 0.00182792
Iteration 5/25 | Loss: 0.00182792
Iteration 6/25 | Loss: 0.00182792
Iteration 7/25 | Loss: 0.00182792
Iteration 8/25 | Loss: 0.00182792
Iteration 9/25 | Loss: 0.00182792
Iteration 10/25 | Loss: 0.00182792
Iteration 11/25 | Loss: 0.00182792
Iteration 12/25 | Loss: 0.00182792
Iteration 13/25 | Loss: 0.00182792
Iteration 14/25 | Loss: 0.00182792
Iteration 15/25 | Loss: 0.00182792
Iteration 16/25 | Loss: 0.00182792
Iteration 17/25 | Loss: 0.00182792
Iteration 18/25 | Loss: 0.00182792
Iteration 19/25 | Loss: 0.00182792
Iteration 20/25 | Loss: 0.00182792
Iteration 21/25 | Loss: 0.00182792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018279219511896372, 0.0018279219511896372, 0.0018279219511896372, 0.0018279219511896372, 0.0018279219511896372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018279219511896372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182792
Iteration 2/1000 | Loss: 0.00003586
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00001546
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001306
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001158
Iteration 12/1000 | Loss: 0.00001143
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001130
Iteration 21/1000 | Loss: 0.00001130
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001124
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001123
Iteration 27/1000 | Loss: 0.00001122
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001121
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001121
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001118
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001116
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001114
Iteration 54/1000 | Loss: 0.00001114
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001114
Iteration 66/1000 | Loss: 0.00001114
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001112
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001112
Iteration 85/1000 | Loss: 0.00001112
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.1098529284936376e-05, 1.1098529284936376e-05, 1.1098529284936376e-05, 1.1098529284936376e-05, 1.1098529284936376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1098529284936376e-05

Optimization complete. Final v2v error: 2.7961862087249756 mm

Highest mean error: 3.018411636352539 mm for frame 107

Lowest mean error: 2.414743661880493 mm for frame 0

Saving results

Total time: 34.86268162727356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406949
Iteration 2/25 | Loss: 0.00114715
Iteration 3/25 | Loss: 0.00101720
Iteration 4/25 | Loss: 0.00099717
Iteration 5/25 | Loss: 0.00099132
Iteration 6/25 | Loss: 0.00098933
Iteration 7/25 | Loss: 0.00098888
Iteration 8/25 | Loss: 0.00098881
Iteration 9/25 | Loss: 0.00098881
Iteration 10/25 | Loss: 0.00098881
Iteration 11/25 | Loss: 0.00098881
Iteration 12/25 | Loss: 0.00098881
Iteration 13/25 | Loss: 0.00098881
Iteration 14/25 | Loss: 0.00098881
Iteration 15/25 | Loss: 0.00098881
Iteration 16/25 | Loss: 0.00098881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009888132335618138, 0.0009888132335618138, 0.0009888132335618138, 0.0009888132335618138, 0.0009888132335618138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009888132335618138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27737498
Iteration 2/25 | Loss: 0.00181808
Iteration 3/25 | Loss: 0.00181808
Iteration 4/25 | Loss: 0.00181808
Iteration 5/25 | Loss: 0.00181808
Iteration 6/25 | Loss: 0.00181808
Iteration 7/25 | Loss: 0.00181808
Iteration 8/25 | Loss: 0.00181808
Iteration 9/25 | Loss: 0.00181808
Iteration 10/25 | Loss: 0.00181808
Iteration 11/25 | Loss: 0.00181808
Iteration 12/25 | Loss: 0.00181807
Iteration 13/25 | Loss: 0.00181808
Iteration 14/25 | Loss: 0.00181807
Iteration 15/25 | Loss: 0.00181808
Iteration 16/25 | Loss: 0.00181807
Iteration 17/25 | Loss: 0.00181807
Iteration 18/25 | Loss: 0.00181807
Iteration 19/25 | Loss: 0.00181807
Iteration 20/25 | Loss: 0.00181807
Iteration 21/25 | Loss: 0.00181807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001818074961192906, 0.001818074961192906, 0.001818074961192906, 0.001818074961192906, 0.001818074961192906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001818074961192906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181807
Iteration 2/1000 | Loss: 0.00003815
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001318
Iteration 6/1000 | Loss: 0.00001230
Iteration 7/1000 | Loss: 0.00001181
Iteration 8/1000 | Loss: 0.00001147
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001097
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001081
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001080
Iteration 15/1000 | Loss: 0.00001074
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001071
Iteration 18/1000 | Loss: 0.00001070
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001059
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001055
Iteration 30/1000 | Loss: 0.00001055
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001055
Iteration 35/1000 | Loss: 0.00001055
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001053
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001050
Iteration 49/1000 | Loss: 0.00001050
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001049
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001048
Iteration 57/1000 | Loss: 0.00001048
Iteration 58/1000 | Loss: 0.00001048
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001045
Iteration 74/1000 | Loss: 0.00001045
Iteration 75/1000 | Loss: 0.00001044
Iteration 76/1000 | Loss: 0.00001044
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001044
Iteration 81/1000 | Loss: 0.00001044
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001042
Iteration 87/1000 | Loss: 0.00001042
Iteration 88/1000 | Loss: 0.00001042
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001042
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.0421425940876361e-05, 1.0421425940876361e-05, 1.0421425940876361e-05, 1.0421425940876361e-05, 1.0421425940876361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0421425940876361e-05

Optimization complete. Final v2v error: 2.6471915245056152 mm

Highest mean error: 4.032762050628662 mm for frame 45

Lowest mean error: 2.138563871383667 mm for frame 103

Saving results

Total time: 35.38007473945618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960427
Iteration 2/25 | Loss: 0.00200111
Iteration 3/25 | Loss: 0.00123410
Iteration 4/25 | Loss: 0.00110897
Iteration 5/25 | Loss: 0.00109335
Iteration 6/25 | Loss: 0.00108561
Iteration 7/25 | Loss: 0.00108628
Iteration 8/25 | Loss: 0.00108252
Iteration 9/25 | Loss: 0.00108102
Iteration 10/25 | Loss: 0.00107109
Iteration 11/25 | Loss: 0.00107208
Iteration 12/25 | Loss: 0.00106795
Iteration 13/25 | Loss: 0.00106356
Iteration 14/25 | Loss: 0.00106162
Iteration 15/25 | Loss: 0.00106214
Iteration 16/25 | Loss: 0.00106612
Iteration 17/25 | Loss: 0.00106518
Iteration 18/25 | Loss: 0.00106226
Iteration 19/25 | Loss: 0.00106305
Iteration 20/25 | Loss: 0.00106604
Iteration 21/25 | Loss: 0.00106509
Iteration 22/25 | Loss: 0.00106475
Iteration 23/25 | Loss: 0.00106202
Iteration 24/25 | Loss: 0.00105974
Iteration 25/25 | Loss: 0.00105830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11826313
Iteration 2/25 | Loss: 0.00189934
Iteration 3/25 | Loss: 0.00189934
Iteration 4/25 | Loss: 0.00189934
Iteration 5/25 | Loss: 0.00189934
Iteration 6/25 | Loss: 0.00189934
Iteration 7/25 | Loss: 0.00189934
Iteration 8/25 | Loss: 0.00189933
Iteration 9/25 | Loss: 0.00189933
Iteration 10/25 | Loss: 0.00189933
Iteration 11/25 | Loss: 0.00189933
Iteration 12/25 | Loss: 0.00189933
Iteration 13/25 | Loss: 0.00189933
Iteration 14/25 | Loss: 0.00189933
Iteration 15/25 | Loss: 0.00189933
Iteration 16/25 | Loss: 0.00189933
Iteration 17/25 | Loss: 0.00189933
Iteration 18/25 | Loss: 0.00189933
Iteration 19/25 | Loss: 0.00189933
Iteration 20/25 | Loss: 0.00189933
Iteration 21/25 | Loss: 0.00189933
Iteration 22/25 | Loss: 0.00189933
Iteration 23/25 | Loss: 0.00189933
Iteration 24/25 | Loss: 0.00189933
Iteration 25/25 | Loss: 0.00189933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0018993342528119683, 0.0018993342528119683, 0.0018993342528119683, 0.0018993342528119683, 0.0018993342528119683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018993342528119683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189933
Iteration 2/1000 | Loss: 0.00004315
Iteration 3/1000 | Loss: 0.00002885
Iteration 4/1000 | Loss: 0.00002065
Iteration 5/1000 | Loss: 0.00013222
Iteration 6/1000 | Loss: 0.00002279
Iteration 7/1000 | Loss: 0.00016740
Iteration 8/1000 | Loss: 0.00008485
Iteration 9/1000 | Loss: 0.00014168
Iteration 10/1000 | Loss: 0.00011817
Iteration 11/1000 | Loss: 0.00013773
Iteration 12/1000 | Loss: 0.00011432
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00018456
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001711
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001424
Iteration 21/1000 | Loss: 0.00001418
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001397
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001387
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001386
Iteration 31/1000 | Loss: 0.00001384
Iteration 32/1000 | Loss: 0.00001383
Iteration 33/1000 | Loss: 0.00001380
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001374
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001362
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001362
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001360
Iteration 64/1000 | Loss: 0.00001360
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001359
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001358
Iteration 69/1000 | Loss: 0.00001358
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001355
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001351
Iteration 94/1000 | Loss: 0.00001351
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Iteration 97/1000 | Loss: 0.00001350
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001346
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001344
Iteration 126/1000 | Loss: 0.00001344
Iteration 127/1000 | Loss: 0.00001344
Iteration 128/1000 | Loss: 0.00001344
Iteration 129/1000 | Loss: 0.00001344
Iteration 130/1000 | Loss: 0.00001344
Iteration 131/1000 | Loss: 0.00001344
Iteration 132/1000 | Loss: 0.00001344
Iteration 133/1000 | Loss: 0.00001344
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001343
Iteration 136/1000 | Loss: 0.00001343
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001342
Iteration 144/1000 | Loss: 0.00001342
Iteration 145/1000 | Loss: 0.00001342
Iteration 146/1000 | Loss: 0.00001342
Iteration 147/1000 | Loss: 0.00001342
Iteration 148/1000 | Loss: 0.00001342
Iteration 149/1000 | Loss: 0.00001342
Iteration 150/1000 | Loss: 0.00001342
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001341
Iteration 158/1000 | Loss: 0.00001341
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001340
Iteration 162/1000 | Loss: 0.00001340
Iteration 163/1000 | Loss: 0.00001340
Iteration 164/1000 | Loss: 0.00001340
Iteration 165/1000 | Loss: 0.00001340
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001339
Iteration 174/1000 | Loss: 0.00001339
Iteration 175/1000 | Loss: 0.00001339
Iteration 176/1000 | Loss: 0.00001339
Iteration 177/1000 | Loss: 0.00001339
Iteration 178/1000 | Loss: 0.00001339
Iteration 179/1000 | Loss: 0.00001339
Iteration 180/1000 | Loss: 0.00001338
Iteration 181/1000 | Loss: 0.00001338
Iteration 182/1000 | Loss: 0.00001338
Iteration 183/1000 | Loss: 0.00001338
Iteration 184/1000 | Loss: 0.00001337
Iteration 185/1000 | Loss: 0.00001337
Iteration 186/1000 | Loss: 0.00001337
Iteration 187/1000 | Loss: 0.00001336
Iteration 188/1000 | Loss: 0.00001336
Iteration 189/1000 | Loss: 0.00001336
Iteration 190/1000 | Loss: 0.00001335
Iteration 191/1000 | Loss: 0.00001335
Iteration 192/1000 | Loss: 0.00001335
Iteration 193/1000 | Loss: 0.00001335
Iteration 194/1000 | Loss: 0.00001335
Iteration 195/1000 | Loss: 0.00001334
Iteration 196/1000 | Loss: 0.00001334
Iteration 197/1000 | Loss: 0.00001334
Iteration 198/1000 | Loss: 0.00001333
Iteration 199/1000 | Loss: 0.00001333
Iteration 200/1000 | Loss: 0.00001333
Iteration 201/1000 | Loss: 0.00001333
Iteration 202/1000 | Loss: 0.00001332
Iteration 203/1000 | Loss: 0.00001332
Iteration 204/1000 | Loss: 0.00001332
Iteration 205/1000 | Loss: 0.00001332
Iteration 206/1000 | Loss: 0.00001332
Iteration 207/1000 | Loss: 0.00001332
Iteration 208/1000 | Loss: 0.00001332
Iteration 209/1000 | Loss: 0.00001332
Iteration 210/1000 | Loss: 0.00001332
Iteration 211/1000 | Loss: 0.00001332
Iteration 212/1000 | Loss: 0.00001332
Iteration 213/1000 | Loss: 0.00001332
Iteration 214/1000 | Loss: 0.00001332
Iteration 215/1000 | Loss: 0.00001332
Iteration 216/1000 | Loss: 0.00001331
Iteration 217/1000 | Loss: 0.00001331
Iteration 218/1000 | Loss: 0.00001331
Iteration 219/1000 | Loss: 0.00001331
Iteration 220/1000 | Loss: 0.00001331
Iteration 221/1000 | Loss: 0.00001331
Iteration 222/1000 | Loss: 0.00001331
Iteration 223/1000 | Loss: 0.00001331
Iteration 224/1000 | Loss: 0.00001331
Iteration 225/1000 | Loss: 0.00001331
Iteration 226/1000 | Loss: 0.00001331
Iteration 227/1000 | Loss: 0.00001331
Iteration 228/1000 | Loss: 0.00001331
Iteration 229/1000 | Loss: 0.00001331
Iteration 230/1000 | Loss: 0.00001331
Iteration 231/1000 | Loss: 0.00001331
Iteration 232/1000 | Loss: 0.00001331
Iteration 233/1000 | Loss: 0.00001331
Iteration 234/1000 | Loss: 0.00001331
Iteration 235/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.3309586393006612e-05, 1.3309586393006612e-05, 1.3309586393006612e-05, 1.3309586393006612e-05, 1.3309586393006612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3309586393006612e-05

Optimization complete. Final v2v error: 3.015955686569214 mm

Highest mean error: 3.9495034217834473 mm for frame 81

Lowest mean error: 2.572460412979126 mm for frame 103

Saving results

Total time: 102.35591554641724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839546
Iteration 2/25 | Loss: 0.00130354
Iteration 3/25 | Loss: 0.00104637
Iteration 4/25 | Loss: 0.00102400
Iteration 5/25 | Loss: 0.00102166
Iteration 6/25 | Loss: 0.00102140
Iteration 7/25 | Loss: 0.00102140
Iteration 8/25 | Loss: 0.00102140
Iteration 9/25 | Loss: 0.00102140
Iteration 10/25 | Loss: 0.00102140
Iteration 11/25 | Loss: 0.00102140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010213962523266673, 0.0010213962523266673, 0.0010213962523266673, 0.0010213962523266673, 0.0010213962523266673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010213962523266673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78976595
Iteration 2/25 | Loss: 0.00097185
Iteration 3/25 | Loss: 0.00097185
Iteration 4/25 | Loss: 0.00097185
Iteration 5/25 | Loss: 0.00097185
Iteration 6/25 | Loss: 0.00097185
Iteration 7/25 | Loss: 0.00097185
Iteration 8/25 | Loss: 0.00097185
Iteration 9/25 | Loss: 0.00097184
Iteration 10/25 | Loss: 0.00097184
Iteration 11/25 | Loss: 0.00097184
Iteration 12/25 | Loss: 0.00097184
Iteration 13/25 | Loss: 0.00097184
Iteration 14/25 | Loss: 0.00097184
Iteration 15/25 | Loss: 0.00097184
Iteration 16/25 | Loss: 0.00097184
Iteration 17/25 | Loss: 0.00097184
Iteration 18/25 | Loss: 0.00097184
Iteration 19/25 | Loss: 0.00097184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009718442452140152, 0.0009718442452140152, 0.0009718442452140152, 0.0009718442452140152, 0.0009718442452140152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009718442452140152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097184
Iteration 2/1000 | Loss: 0.00003158
Iteration 3/1000 | Loss: 0.00002229
Iteration 4/1000 | Loss: 0.00002034
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001858
Iteration 7/1000 | Loss: 0.00001809
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001730
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001721
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001696
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001695
Iteration 31/1000 | Loss: 0.00001695
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001692
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001692
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001692
Iteration 68/1000 | Loss: 0.00001692
Iteration 69/1000 | Loss: 0.00001692
Iteration 70/1000 | Loss: 0.00001692
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.6922002032515593e-05, 1.6922002032515593e-05, 1.6922002032515593e-05, 1.6922002032515593e-05, 1.6922002032515593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6922002032515593e-05

Optimization complete. Final v2v error: 3.448824882507324 mm

Highest mean error: 3.5513455867767334 mm for frame 68

Lowest mean error: 3.338653087615967 mm for frame 149

Saving results

Total time: 28.263065338134766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006652
Iteration 2/25 | Loss: 0.00146117
Iteration 3/25 | Loss: 0.00119885
Iteration 4/25 | Loss: 0.00119020
Iteration 5/25 | Loss: 0.00117363
Iteration 6/25 | Loss: 0.00115427
Iteration 7/25 | Loss: 0.00112983
Iteration 8/25 | Loss: 0.00111858
Iteration 9/25 | Loss: 0.00110727
Iteration 10/25 | Loss: 0.00109757
Iteration 11/25 | Loss: 0.00110476
Iteration 12/25 | Loss: 0.00110099
Iteration 13/25 | Loss: 0.00109587
Iteration 14/25 | Loss: 0.00109239
Iteration 15/25 | Loss: 0.00109211
Iteration 16/25 | Loss: 0.00109015
Iteration 17/25 | Loss: 0.00108934
Iteration 18/25 | Loss: 0.00108809
Iteration 19/25 | Loss: 0.00108825
Iteration 20/25 | Loss: 0.00108933
Iteration 21/25 | Loss: 0.00108771
Iteration 22/25 | Loss: 0.00108931
Iteration 23/25 | Loss: 0.00108886
Iteration 24/25 | Loss: 0.00108780
Iteration 25/25 | Loss: 0.00108513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17841554
Iteration 2/25 | Loss: 0.00231768
Iteration 3/25 | Loss: 0.00231766
Iteration 4/25 | Loss: 0.00231765
Iteration 5/25 | Loss: 0.00231765
Iteration 6/25 | Loss: 0.00231765
Iteration 7/25 | Loss: 0.00231765
Iteration 8/25 | Loss: 0.00231765
Iteration 9/25 | Loss: 0.00231765
Iteration 10/25 | Loss: 0.00231765
Iteration 11/25 | Loss: 0.00231765
Iteration 12/25 | Loss: 0.00231765
Iteration 13/25 | Loss: 0.00231765
Iteration 14/25 | Loss: 0.00231765
Iteration 15/25 | Loss: 0.00231765
Iteration 16/25 | Loss: 0.00231765
Iteration 17/25 | Loss: 0.00231765
Iteration 18/25 | Loss: 0.00231765
Iteration 19/25 | Loss: 0.00231765
Iteration 20/25 | Loss: 0.00231765
Iteration 21/25 | Loss: 0.00231765
Iteration 22/25 | Loss: 0.00231765
Iteration 23/25 | Loss: 0.00231765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002317649545148015, 0.002317649545148015, 0.002317649545148015, 0.002317649545148015, 0.002317649545148015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002317649545148015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231765
Iteration 2/1000 | Loss: 0.00006886
Iteration 3/1000 | Loss: 0.00015612
Iteration 4/1000 | Loss: 0.00021171
Iteration 5/1000 | Loss: 0.00019224
Iteration 6/1000 | Loss: 0.00017821
Iteration 7/1000 | Loss: 0.00016028
Iteration 8/1000 | Loss: 0.00015463
Iteration 9/1000 | Loss: 0.00015071
Iteration 10/1000 | Loss: 0.00014225
Iteration 11/1000 | Loss: 0.00009078
Iteration 12/1000 | Loss: 0.00010766
Iteration 13/1000 | Loss: 0.00007456
Iteration 14/1000 | Loss: 0.00003215
Iteration 15/1000 | Loss: 0.00006375
Iteration 16/1000 | Loss: 0.00010040
Iteration 17/1000 | Loss: 0.00011305
Iteration 18/1000 | Loss: 0.00010714
Iteration 19/1000 | Loss: 0.00009511
Iteration 20/1000 | Loss: 0.00008223
Iteration 21/1000 | Loss: 0.00006884
Iteration 22/1000 | Loss: 0.00002730
Iteration 23/1000 | Loss: 0.00013715
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002608
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002334
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002114
Iteration 30/1000 | Loss: 0.00002063
Iteration 31/1000 | Loss: 0.00002034
Iteration 32/1000 | Loss: 0.00002024
Iteration 33/1000 | Loss: 0.00010440
Iteration 34/1000 | Loss: 0.00002716
Iteration 35/1000 | Loss: 0.00002383
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002135
Iteration 39/1000 | Loss: 0.00010896
Iteration 40/1000 | Loss: 0.00009633
Iteration 41/1000 | Loss: 0.00011023
Iteration 42/1000 | Loss: 0.00003827
Iteration 43/1000 | Loss: 0.00013896
Iteration 44/1000 | Loss: 0.00003215
Iteration 45/1000 | Loss: 0.00002551
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002065
Iteration 49/1000 | Loss: 0.00002020
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001933
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001931
Iteration 57/1000 | Loss: 0.00001930
Iteration 58/1000 | Loss: 0.00001930
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001895
Iteration 64/1000 | Loss: 0.00001895
Iteration 65/1000 | Loss: 0.00001894
Iteration 66/1000 | Loss: 0.00001894
Iteration 67/1000 | Loss: 0.00001893
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001892
Iteration 70/1000 | Loss: 0.00001892
Iteration 71/1000 | Loss: 0.00001892
Iteration 72/1000 | Loss: 0.00001892
Iteration 73/1000 | Loss: 0.00001892
Iteration 74/1000 | Loss: 0.00001891
Iteration 75/1000 | Loss: 0.00001891
Iteration 76/1000 | Loss: 0.00001891
Iteration 77/1000 | Loss: 0.00001891
Iteration 78/1000 | Loss: 0.00001891
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001891
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001890
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001890
Iteration 85/1000 | Loss: 0.00001890
Iteration 86/1000 | Loss: 0.00001890
Iteration 87/1000 | Loss: 0.00001889
Iteration 88/1000 | Loss: 0.00001889
Iteration 89/1000 | Loss: 0.00001889
Iteration 90/1000 | Loss: 0.00001888
Iteration 91/1000 | Loss: 0.00001888
Iteration 92/1000 | Loss: 0.00001887
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001886
Iteration 97/1000 | Loss: 0.00001886
Iteration 98/1000 | Loss: 0.00001886
Iteration 99/1000 | Loss: 0.00001886
Iteration 100/1000 | Loss: 0.00001886
Iteration 101/1000 | Loss: 0.00001886
Iteration 102/1000 | Loss: 0.00001886
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00001886
Iteration 105/1000 | Loss: 0.00001886
Iteration 106/1000 | Loss: 0.00001886
Iteration 107/1000 | Loss: 0.00001886
Iteration 108/1000 | Loss: 0.00001886
Iteration 109/1000 | Loss: 0.00001886
Iteration 110/1000 | Loss: 0.00001886
Iteration 111/1000 | Loss: 0.00001885
Iteration 112/1000 | Loss: 0.00001885
Iteration 113/1000 | Loss: 0.00001885
Iteration 114/1000 | Loss: 0.00001885
Iteration 115/1000 | Loss: 0.00001885
Iteration 116/1000 | Loss: 0.00001885
Iteration 117/1000 | Loss: 0.00001885
Iteration 118/1000 | Loss: 0.00001885
Iteration 119/1000 | Loss: 0.00001885
Iteration 120/1000 | Loss: 0.00001885
Iteration 121/1000 | Loss: 0.00001884
Iteration 122/1000 | Loss: 0.00001884
Iteration 123/1000 | Loss: 0.00001884
Iteration 124/1000 | Loss: 0.00001884
Iteration 125/1000 | Loss: 0.00001884
Iteration 126/1000 | Loss: 0.00001884
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001883
Iteration 132/1000 | Loss: 0.00001883
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001882
Iteration 138/1000 | Loss: 0.00001882
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001880
Iteration 144/1000 | Loss: 0.00001880
Iteration 145/1000 | Loss: 0.00001880
Iteration 146/1000 | Loss: 0.00001879
Iteration 147/1000 | Loss: 0.00001879
Iteration 148/1000 | Loss: 0.00001879
Iteration 149/1000 | Loss: 0.00001879
Iteration 150/1000 | Loss: 0.00001878
Iteration 151/1000 | Loss: 0.00001878
Iteration 152/1000 | Loss: 0.00001878
Iteration 153/1000 | Loss: 0.00001877
Iteration 154/1000 | Loss: 0.00001877
Iteration 155/1000 | Loss: 0.00001877
Iteration 156/1000 | Loss: 0.00001877
Iteration 157/1000 | Loss: 0.00001876
Iteration 158/1000 | Loss: 0.00001876
Iteration 159/1000 | Loss: 0.00001876
Iteration 160/1000 | Loss: 0.00001875
Iteration 161/1000 | Loss: 0.00001875
Iteration 162/1000 | Loss: 0.00001875
Iteration 163/1000 | Loss: 0.00001875
Iteration 164/1000 | Loss: 0.00001874
Iteration 165/1000 | Loss: 0.00001874
Iteration 166/1000 | Loss: 0.00001874
Iteration 167/1000 | Loss: 0.00001873
Iteration 168/1000 | Loss: 0.00001873
Iteration 169/1000 | Loss: 0.00001872
Iteration 170/1000 | Loss: 0.00001872
Iteration 171/1000 | Loss: 0.00001872
Iteration 172/1000 | Loss: 0.00001872
Iteration 173/1000 | Loss: 0.00001871
Iteration 174/1000 | Loss: 0.00001871
Iteration 175/1000 | Loss: 0.00001871
Iteration 176/1000 | Loss: 0.00001871
Iteration 177/1000 | Loss: 0.00001871
Iteration 178/1000 | Loss: 0.00001870
Iteration 179/1000 | Loss: 0.00001870
Iteration 180/1000 | Loss: 0.00001870
Iteration 181/1000 | Loss: 0.00001870
Iteration 182/1000 | Loss: 0.00001870
Iteration 183/1000 | Loss: 0.00001869
Iteration 184/1000 | Loss: 0.00001869
Iteration 185/1000 | Loss: 0.00001869
Iteration 186/1000 | Loss: 0.00001869
Iteration 187/1000 | Loss: 0.00001868
Iteration 188/1000 | Loss: 0.00001868
Iteration 189/1000 | Loss: 0.00001868
Iteration 190/1000 | Loss: 0.00001867
Iteration 191/1000 | Loss: 0.00001867
Iteration 192/1000 | Loss: 0.00001866
Iteration 193/1000 | Loss: 0.00001866
Iteration 194/1000 | Loss: 0.00001866
Iteration 195/1000 | Loss: 0.00001866
Iteration 196/1000 | Loss: 0.00001865
Iteration 197/1000 | Loss: 0.00001865
Iteration 198/1000 | Loss: 0.00001865
Iteration 199/1000 | Loss: 0.00001865
Iteration 200/1000 | Loss: 0.00001865
Iteration 201/1000 | Loss: 0.00001864
Iteration 202/1000 | Loss: 0.00001864
Iteration 203/1000 | Loss: 0.00001864
Iteration 204/1000 | Loss: 0.00001864
Iteration 205/1000 | Loss: 0.00001864
Iteration 206/1000 | Loss: 0.00001864
Iteration 207/1000 | Loss: 0.00001863
Iteration 208/1000 | Loss: 0.00001863
Iteration 209/1000 | Loss: 0.00001863
Iteration 210/1000 | Loss: 0.00001863
Iteration 211/1000 | Loss: 0.00001863
Iteration 212/1000 | Loss: 0.00001863
Iteration 213/1000 | Loss: 0.00001863
Iteration 214/1000 | Loss: 0.00001863
Iteration 215/1000 | Loss: 0.00001863
Iteration 216/1000 | Loss: 0.00001863
Iteration 217/1000 | Loss: 0.00001863
Iteration 218/1000 | Loss: 0.00001863
Iteration 219/1000 | Loss: 0.00001863
Iteration 220/1000 | Loss: 0.00001863
Iteration 221/1000 | Loss: 0.00001863
Iteration 222/1000 | Loss: 0.00001863
Iteration 223/1000 | Loss: 0.00001862
Iteration 224/1000 | Loss: 0.00001862
Iteration 225/1000 | Loss: 0.00001862
Iteration 226/1000 | Loss: 0.00001862
Iteration 227/1000 | Loss: 0.00001862
Iteration 228/1000 | Loss: 0.00001862
Iteration 229/1000 | Loss: 0.00001862
Iteration 230/1000 | Loss: 0.00001862
Iteration 231/1000 | Loss: 0.00001862
Iteration 232/1000 | Loss: 0.00001862
Iteration 233/1000 | Loss: 0.00001861
Iteration 234/1000 | Loss: 0.00001861
Iteration 235/1000 | Loss: 0.00001861
Iteration 236/1000 | Loss: 0.00001861
Iteration 237/1000 | Loss: 0.00001861
Iteration 238/1000 | Loss: 0.00001861
Iteration 239/1000 | Loss: 0.00001861
Iteration 240/1000 | Loss: 0.00001861
Iteration 241/1000 | Loss: 0.00001861
Iteration 242/1000 | Loss: 0.00001861
Iteration 243/1000 | Loss: 0.00001861
Iteration 244/1000 | Loss: 0.00001861
Iteration 245/1000 | Loss: 0.00001861
Iteration 246/1000 | Loss: 0.00001861
Iteration 247/1000 | Loss: 0.00001861
Iteration 248/1000 | Loss: 0.00001861
Iteration 249/1000 | Loss: 0.00001861
Iteration 250/1000 | Loss: 0.00001861
Iteration 251/1000 | Loss: 0.00001861
Iteration 252/1000 | Loss: 0.00001861
Iteration 253/1000 | Loss: 0.00001861
Iteration 254/1000 | Loss: 0.00001861
Iteration 255/1000 | Loss: 0.00001861
Iteration 256/1000 | Loss: 0.00001861
Iteration 257/1000 | Loss: 0.00001861
Iteration 258/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.861035343608819e-05, 1.861035343608819e-05, 1.861035343608819e-05, 1.861035343608819e-05, 1.861035343608819e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.861035343608819e-05

Optimization complete. Final v2v error: 3.4109387397766113 mm

Highest mean error: 4.4598164558410645 mm for frame 78

Lowest mean error: 2.4456558227539062 mm for frame 108

Saving results

Total time: 150.0957407951355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423048
Iteration 2/25 | Loss: 0.00107137
Iteration 3/25 | Loss: 0.00099370
Iteration 4/25 | Loss: 0.00097894
Iteration 5/25 | Loss: 0.00097529
Iteration 6/25 | Loss: 0.00097384
Iteration 7/25 | Loss: 0.00097370
Iteration 8/25 | Loss: 0.00097370
Iteration 9/25 | Loss: 0.00097370
Iteration 10/25 | Loss: 0.00097370
Iteration 11/25 | Loss: 0.00097370
Iteration 12/25 | Loss: 0.00097370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009737019427120686, 0.0009737019427120686, 0.0009737019427120686, 0.0009737019427120686, 0.0009737019427120686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009737019427120686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20431590
Iteration 2/25 | Loss: 0.00174559
Iteration 3/25 | Loss: 0.00174559
Iteration 4/25 | Loss: 0.00174559
Iteration 5/25 | Loss: 0.00174559
Iteration 6/25 | Loss: 0.00174559
Iteration 7/25 | Loss: 0.00174559
Iteration 8/25 | Loss: 0.00174559
Iteration 9/25 | Loss: 0.00174559
Iteration 10/25 | Loss: 0.00174559
Iteration 11/25 | Loss: 0.00174559
Iteration 12/25 | Loss: 0.00174559
Iteration 13/25 | Loss: 0.00174559
Iteration 14/25 | Loss: 0.00174559
Iteration 15/25 | Loss: 0.00174559
Iteration 16/25 | Loss: 0.00174559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017455910565331578, 0.0017455910565331578, 0.0017455910565331578, 0.0017455910565331578, 0.0017455910565331578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017455910565331578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174559
Iteration 2/1000 | Loss: 0.00001964
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001180
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001148
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001143
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001131
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001120
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001100
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001099
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001097
Iteration 48/1000 | Loss: 0.00001097
Iteration 49/1000 | Loss: 0.00001097
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001095
Iteration 59/1000 | Loss: 0.00001095
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001095
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001095
Iteration 81/1000 | Loss: 0.00001095
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.0945895155600738e-05, 1.0945895155600738e-05, 1.0945895155600738e-05, 1.0945895155600738e-05, 1.0945895155600738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0945895155600738e-05

Optimization complete. Final v2v error: 2.75282621383667 mm

Highest mean error: 2.9644320011138916 mm for frame 98

Lowest mean error: 2.4696691036224365 mm for frame 0

Saving results

Total time: 31.07246470451355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391839
Iteration 2/25 | Loss: 0.00120841
Iteration 3/25 | Loss: 0.00101870
Iteration 4/25 | Loss: 0.00099433
Iteration 5/25 | Loss: 0.00098950
Iteration 6/25 | Loss: 0.00098804
Iteration 7/25 | Loss: 0.00098804
Iteration 8/25 | Loss: 0.00098804
Iteration 9/25 | Loss: 0.00098804
Iteration 10/25 | Loss: 0.00098804
Iteration 11/25 | Loss: 0.00098804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009880404686555266, 0.0009880404686555266, 0.0009880404686555266, 0.0009880404686555266, 0.0009880404686555266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009880404686555266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20077741
Iteration 2/25 | Loss: 0.00244912
Iteration 3/25 | Loss: 0.00244912
Iteration 4/25 | Loss: 0.00244911
Iteration 5/25 | Loss: 0.00244911
Iteration 6/25 | Loss: 0.00244911
Iteration 7/25 | Loss: 0.00244911
Iteration 8/25 | Loss: 0.00244911
Iteration 9/25 | Loss: 0.00244911
Iteration 10/25 | Loss: 0.00244911
Iteration 11/25 | Loss: 0.00244911
Iteration 12/25 | Loss: 0.00244911
Iteration 13/25 | Loss: 0.00244911
Iteration 14/25 | Loss: 0.00244911
Iteration 15/25 | Loss: 0.00244911
Iteration 16/25 | Loss: 0.00244911
Iteration 17/25 | Loss: 0.00244911
Iteration 18/25 | Loss: 0.00244911
Iteration 19/25 | Loss: 0.00244911
Iteration 20/25 | Loss: 0.00244911
Iteration 21/25 | Loss: 0.00244911
Iteration 22/25 | Loss: 0.00244911
Iteration 23/25 | Loss: 0.00244911
Iteration 24/25 | Loss: 0.00244911
Iteration 25/25 | Loss: 0.00244911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244911
Iteration 2/1000 | Loss: 0.00004051
Iteration 3/1000 | Loss: 0.00002566
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001528
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001314
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001176
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001160
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001134
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001129
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001128
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001126
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001125
Iteration 36/1000 | Loss: 0.00001125
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001123
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001121
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001119
Iteration 50/1000 | Loss: 0.00001119
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001111
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001111
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001109
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001109
Iteration 83/1000 | Loss: 0.00001109
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001108
Iteration 88/1000 | Loss: 0.00001108
Iteration 89/1000 | Loss: 0.00001108
Iteration 90/1000 | Loss: 0.00001108
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001107
Iteration 93/1000 | Loss: 0.00001107
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001107
Iteration 96/1000 | Loss: 0.00001107
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001106
Iteration 102/1000 | Loss: 0.00001106
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001106
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001105
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001103
Iteration 115/1000 | Loss: 0.00001103
Iteration 116/1000 | Loss: 0.00001103
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001103
Iteration 119/1000 | Loss: 0.00001103
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001103
Iteration 125/1000 | Loss: 0.00001103
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001102
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001101
Iteration 133/1000 | Loss: 0.00001101
Iteration 134/1000 | Loss: 0.00001101
Iteration 135/1000 | Loss: 0.00001101
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001100
Iteration 144/1000 | Loss: 0.00001100
Iteration 145/1000 | Loss: 0.00001100
Iteration 146/1000 | Loss: 0.00001100
Iteration 147/1000 | Loss: 0.00001100
Iteration 148/1000 | Loss: 0.00001100
Iteration 149/1000 | Loss: 0.00001100
Iteration 150/1000 | Loss: 0.00001099
Iteration 151/1000 | Loss: 0.00001099
Iteration 152/1000 | Loss: 0.00001099
Iteration 153/1000 | Loss: 0.00001099
Iteration 154/1000 | Loss: 0.00001099
Iteration 155/1000 | Loss: 0.00001098
Iteration 156/1000 | Loss: 0.00001098
Iteration 157/1000 | Loss: 0.00001098
Iteration 158/1000 | Loss: 0.00001098
Iteration 159/1000 | Loss: 0.00001098
Iteration 160/1000 | Loss: 0.00001098
Iteration 161/1000 | Loss: 0.00001098
Iteration 162/1000 | Loss: 0.00001098
Iteration 163/1000 | Loss: 0.00001098
Iteration 164/1000 | Loss: 0.00001098
Iteration 165/1000 | Loss: 0.00001098
Iteration 166/1000 | Loss: 0.00001098
Iteration 167/1000 | Loss: 0.00001098
Iteration 168/1000 | Loss: 0.00001098
Iteration 169/1000 | Loss: 0.00001097
Iteration 170/1000 | Loss: 0.00001097
Iteration 171/1000 | Loss: 0.00001097
Iteration 172/1000 | Loss: 0.00001097
Iteration 173/1000 | Loss: 0.00001097
Iteration 174/1000 | Loss: 0.00001097
Iteration 175/1000 | Loss: 0.00001097
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001097
Iteration 178/1000 | Loss: 0.00001097
Iteration 179/1000 | Loss: 0.00001097
Iteration 180/1000 | Loss: 0.00001097
Iteration 181/1000 | Loss: 0.00001097
Iteration 182/1000 | Loss: 0.00001097
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001096
Iteration 194/1000 | Loss: 0.00001096
Iteration 195/1000 | Loss: 0.00001096
Iteration 196/1000 | Loss: 0.00001096
Iteration 197/1000 | Loss: 0.00001096
Iteration 198/1000 | Loss: 0.00001096
Iteration 199/1000 | Loss: 0.00001096
Iteration 200/1000 | Loss: 0.00001095
Iteration 201/1000 | Loss: 0.00001095
Iteration 202/1000 | Loss: 0.00001095
Iteration 203/1000 | Loss: 0.00001095
Iteration 204/1000 | Loss: 0.00001095
Iteration 205/1000 | Loss: 0.00001095
Iteration 206/1000 | Loss: 0.00001095
Iteration 207/1000 | Loss: 0.00001095
Iteration 208/1000 | Loss: 0.00001095
Iteration 209/1000 | Loss: 0.00001095
Iteration 210/1000 | Loss: 0.00001095
Iteration 211/1000 | Loss: 0.00001095
Iteration 212/1000 | Loss: 0.00001095
Iteration 213/1000 | Loss: 0.00001095
Iteration 214/1000 | Loss: 0.00001095
Iteration 215/1000 | Loss: 0.00001095
Iteration 216/1000 | Loss: 0.00001095
Iteration 217/1000 | Loss: 0.00001095
Iteration 218/1000 | Loss: 0.00001095
Iteration 219/1000 | Loss: 0.00001095
Iteration 220/1000 | Loss: 0.00001095
Iteration 221/1000 | Loss: 0.00001095
Iteration 222/1000 | Loss: 0.00001095
Iteration 223/1000 | Loss: 0.00001095
Iteration 224/1000 | Loss: 0.00001095
Iteration 225/1000 | Loss: 0.00001095
Iteration 226/1000 | Loss: 0.00001095
Iteration 227/1000 | Loss: 0.00001095
Iteration 228/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.09511593109346e-05, 1.09511593109346e-05, 1.09511593109346e-05, 1.09511593109346e-05, 1.09511593109346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.09511593109346e-05

Optimization complete. Final v2v error: 2.764427900314331 mm

Highest mean error: 3.5503158569335938 mm for frame 5

Lowest mean error: 2.0846619606018066 mm for frame 178

Saving results

Total time: 51.61312532424927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391346
Iteration 2/25 | Loss: 0.00107542
Iteration 3/25 | Loss: 0.00098097
Iteration 4/25 | Loss: 0.00097431
Iteration 5/25 | Loss: 0.00097250
Iteration 6/25 | Loss: 0.00097246
Iteration 7/25 | Loss: 0.00097246
Iteration 8/25 | Loss: 0.00097246
Iteration 9/25 | Loss: 0.00097246
Iteration 10/25 | Loss: 0.00097246
Iteration 11/25 | Loss: 0.00097246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009724635747261345, 0.0009724635747261345, 0.0009724635747261345, 0.0009724635747261345, 0.0009724635747261345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009724635747261345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20613658
Iteration 2/25 | Loss: 0.00178894
Iteration 3/25 | Loss: 0.00178894
Iteration 4/25 | Loss: 0.00178894
Iteration 5/25 | Loss: 0.00178894
Iteration 6/25 | Loss: 0.00178894
Iteration 7/25 | Loss: 0.00178894
Iteration 8/25 | Loss: 0.00178894
Iteration 9/25 | Loss: 0.00178894
Iteration 10/25 | Loss: 0.00178894
Iteration 11/25 | Loss: 0.00178894
Iteration 12/25 | Loss: 0.00178894
Iteration 13/25 | Loss: 0.00178894
Iteration 14/25 | Loss: 0.00178894
Iteration 15/25 | Loss: 0.00178894
Iteration 16/25 | Loss: 0.00178894
Iteration 17/25 | Loss: 0.00178894
Iteration 18/25 | Loss: 0.00178894
Iteration 19/25 | Loss: 0.00178894
Iteration 20/25 | Loss: 0.00178894
Iteration 21/25 | Loss: 0.00178894
Iteration 22/25 | Loss: 0.00178894
Iteration 23/25 | Loss: 0.00178894
Iteration 24/25 | Loss: 0.00178894
Iteration 25/25 | Loss: 0.00178894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178894
Iteration 2/1000 | Loss: 0.00001847
Iteration 3/1000 | Loss: 0.00001177
Iteration 4/1000 | Loss: 0.00001042
Iteration 5/1000 | Loss: 0.00000974
Iteration 6/1000 | Loss: 0.00000919
Iteration 7/1000 | Loss: 0.00000896
Iteration 8/1000 | Loss: 0.00000880
Iteration 9/1000 | Loss: 0.00000853
Iteration 10/1000 | Loss: 0.00000847
Iteration 11/1000 | Loss: 0.00000828
Iteration 12/1000 | Loss: 0.00000821
Iteration 13/1000 | Loss: 0.00000820
Iteration 14/1000 | Loss: 0.00000818
Iteration 15/1000 | Loss: 0.00000817
Iteration 16/1000 | Loss: 0.00000816
Iteration 17/1000 | Loss: 0.00000816
Iteration 18/1000 | Loss: 0.00000813
Iteration 19/1000 | Loss: 0.00000811
Iteration 20/1000 | Loss: 0.00000810
Iteration 21/1000 | Loss: 0.00000809
Iteration 22/1000 | Loss: 0.00000809
Iteration 23/1000 | Loss: 0.00000805
Iteration 24/1000 | Loss: 0.00000798
Iteration 25/1000 | Loss: 0.00000795
Iteration 26/1000 | Loss: 0.00000795
Iteration 27/1000 | Loss: 0.00000794
Iteration 28/1000 | Loss: 0.00000792
Iteration 29/1000 | Loss: 0.00000792
Iteration 30/1000 | Loss: 0.00000791
Iteration 31/1000 | Loss: 0.00000791
Iteration 32/1000 | Loss: 0.00000791
Iteration 33/1000 | Loss: 0.00000790
Iteration 34/1000 | Loss: 0.00000790
Iteration 35/1000 | Loss: 0.00000789
Iteration 36/1000 | Loss: 0.00000789
Iteration 37/1000 | Loss: 0.00000788
Iteration 38/1000 | Loss: 0.00000788
Iteration 39/1000 | Loss: 0.00000788
Iteration 40/1000 | Loss: 0.00000788
Iteration 41/1000 | Loss: 0.00000788
Iteration 42/1000 | Loss: 0.00000787
Iteration 43/1000 | Loss: 0.00000787
Iteration 44/1000 | Loss: 0.00000786
Iteration 45/1000 | Loss: 0.00000786
Iteration 46/1000 | Loss: 0.00000786
Iteration 47/1000 | Loss: 0.00000786
Iteration 48/1000 | Loss: 0.00000786
Iteration 49/1000 | Loss: 0.00000786
Iteration 50/1000 | Loss: 0.00000786
Iteration 51/1000 | Loss: 0.00000786
Iteration 52/1000 | Loss: 0.00000786
Iteration 53/1000 | Loss: 0.00000786
Iteration 54/1000 | Loss: 0.00000785
Iteration 55/1000 | Loss: 0.00000785
Iteration 56/1000 | Loss: 0.00000785
Iteration 57/1000 | Loss: 0.00000785
Iteration 58/1000 | Loss: 0.00000785
Iteration 59/1000 | Loss: 0.00000785
Iteration 60/1000 | Loss: 0.00000785
Iteration 61/1000 | Loss: 0.00000785
Iteration 62/1000 | Loss: 0.00000785
Iteration 63/1000 | Loss: 0.00000785
Iteration 64/1000 | Loss: 0.00000785
Iteration 65/1000 | Loss: 0.00000785
Iteration 66/1000 | Loss: 0.00000785
Iteration 67/1000 | Loss: 0.00000785
Iteration 68/1000 | Loss: 0.00000785
Iteration 69/1000 | Loss: 0.00000785
Iteration 70/1000 | Loss: 0.00000785
Iteration 71/1000 | Loss: 0.00000785
Iteration 72/1000 | Loss: 0.00000785
Iteration 73/1000 | Loss: 0.00000785
Iteration 74/1000 | Loss: 0.00000785
Iteration 75/1000 | Loss: 0.00000785
Iteration 76/1000 | Loss: 0.00000785
Iteration 77/1000 | Loss: 0.00000785
Iteration 78/1000 | Loss: 0.00000785
Iteration 79/1000 | Loss: 0.00000785
Iteration 80/1000 | Loss: 0.00000785
Iteration 81/1000 | Loss: 0.00000785
Iteration 82/1000 | Loss: 0.00000785
Iteration 83/1000 | Loss: 0.00000785
Iteration 84/1000 | Loss: 0.00000785
Iteration 85/1000 | Loss: 0.00000785
Iteration 86/1000 | Loss: 0.00000785
Iteration 87/1000 | Loss: 0.00000785
Iteration 88/1000 | Loss: 0.00000785
Iteration 89/1000 | Loss: 0.00000785
Iteration 90/1000 | Loss: 0.00000785
Iteration 91/1000 | Loss: 0.00000785
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000785
Iteration 96/1000 | Loss: 0.00000785
Iteration 97/1000 | Loss: 0.00000785
Iteration 98/1000 | Loss: 0.00000785
Iteration 99/1000 | Loss: 0.00000785
Iteration 100/1000 | Loss: 0.00000785
Iteration 101/1000 | Loss: 0.00000785
Iteration 102/1000 | Loss: 0.00000785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [7.846615517337341e-06, 7.846615517337341e-06, 7.846615517337341e-06, 7.846615517337341e-06, 7.846615517337341e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.846615517337341e-06

Optimization complete. Final v2v error: 2.388233184814453 mm

Highest mean error: 2.723191499710083 mm for frame 47

Lowest mean error: 2.2218689918518066 mm for frame 65

Saving results

Total time: 31.81273341178894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085465
Iteration 2/25 | Loss: 0.00185304
Iteration 3/25 | Loss: 0.00129426
Iteration 4/25 | Loss: 0.00121628
Iteration 5/25 | Loss: 0.00119957
Iteration 6/25 | Loss: 0.00118518
Iteration 7/25 | Loss: 0.00117764
Iteration 8/25 | Loss: 0.00117610
Iteration 9/25 | Loss: 0.00117866
Iteration 10/25 | Loss: 0.00118005
Iteration 11/25 | Loss: 0.00117436
Iteration 12/25 | Loss: 0.00117224
Iteration 13/25 | Loss: 0.00117300
Iteration 14/25 | Loss: 0.00117213
Iteration 15/25 | Loss: 0.00117033
Iteration 16/25 | Loss: 0.00117237
Iteration 17/25 | Loss: 0.00116941
Iteration 18/25 | Loss: 0.00116885
Iteration 19/25 | Loss: 0.00116875
Iteration 20/25 | Loss: 0.00116872
Iteration 21/25 | Loss: 0.00116872
Iteration 22/25 | Loss: 0.00116871
Iteration 23/25 | Loss: 0.00116871
Iteration 24/25 | Loss: 0.00116871
Iteration 25/25 | Loss: 0.00116871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91194284
Iteration 2/25 | Loss: 0.00217641
Iteration 3/25 | Loss: 0.00217628
Iteration 4/25 | Loss: 0.00217628
Iteration 5/25 | Loss: 0.00217628
Iteration 6/25 | Loss: 0.00217627
Iteration 7/25 | Loss: 0.00217627
Iteration 8/25 | Loss: 0.00217627
Iteration 9/25 | Loss: 0.00217627
Iteration 10/25 | Loss: 0.00217627
Iteration 11/25 | Loss: 0.00217627
Iteration 12/25 | Loss: 0.00217627
Iteration 13/25 | Loss: 0.00217627
Iteration 14/25 | Loss: 0.00217627
Iteration 15/25 | Loss: 0.00217627
Iteration 16/25 | Loss: 0.00217627
Iteration 17/25 | Loss: 0.00217627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021762733813375235, 0.0021762733813375235, 0.0021762733813375235, 0.0021762733813375235, 0.0021762733813375235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021762733813375235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217627
Iteration 2/1000 | Loss: 0.00008259
Iteration 3/1000 | Loss: 0.00004947
Iteration 4/1000 | Loss: 0.00003619
Iteration 5/1000 | Loss: 0.00003293
Iteration 6/1000 | Loss: 0.00003146
Iteration 7/1000 | Loss: 0.00003066
Iteration 8/1000 | Loss: 0.00003009
Iteration 9/1000 | Loss: 0.00002956
Iteration 10/1000 | Loss: 0.00002912
Iteration 11/1000 | Loss: 0.00002880
Iteration 12/1000 | Loss: 0.00002860
Iteration 13/1000 | Loss: 0.00002838
Iteration 14/1000 | Loss: 0.00002822
Iteration 15/1000 | Loss: 0.00002817
Iteration 16/1000 | Loss: 0.00002805
Iteration 17/1000 | Loss: 0.00002805
Iteration 18/1000 | Loss: 0.00002804
Iteration 19/1000 | Loss: 0.00002804
Iteration 20/1000 | Loss: 0.00002796
Iteration 21/1000 | Loss: 0.00002796
Iteration 22/1000 | Loss: 0.00002796
Iteration 23/1000 | Loss: 0.00002792
Iteration 24/1000 | Loss: 0.00002792
Iteration 25/1000 | Loss: 0.00002789
Iteration 26/1000 | Loss: 0.00002789
Iteration 27/1000 | Loss: 0.00002789
Iteration 28/1000 | Loss: 0.00002789
Iteration 29/1000 | Loss: 0.00002789
Iteration 30/1000 | Loss: 0.00002789
Iteration 31/1000 | Loss: 0.00002789
Iteration 32/1000 | Loss: 0.00002789
Iteration 33/1000 | Loss: 0.00002789
Iteration 34/1000 | Loss: 0.00002788
Iteration 35/1000 | Loss: 0.00002788
Iteration 36/1000 | Loss: 0.00002786
Iteration 37/1000 | Loss: 0.00002786
Iteration 38/1000 | Loss: 0.00002786
Iteration 39/1000 | Loss: 0.00002785
Iteration 40/1000 | Loss: 0.00002785
Iteration 41/1000 | Loss: 0.00002785
Iteration 42/1000 | Loss: 0.00002785
Iteration 43/1000 | Loss: 0.00002785
Iteration 44/1000 | Loss: 0.00002785
Iteration 45/1000 | Loss: 0.00002785
Iteration 46/1000 | Loss: 0.00002785
Iteration 47/1000 | Loss: 0.00002785
Iteration 48/1000 | Loss: 0.00002785
Iteration 49/1000 | Loss: 0.00002785
Iteration 50/1000 | Loss: 0.00002785
Iteration 51/1000 | Loss: 0.00002784
Iteration 52/1000 | Loss: 0.00002784
Iteration 53/1000 | Loss: 0.00002784
Iteration 54/1000 | Loss: 0.00002784
Iteration 55/1000 | Loss: 0.00002784
Iteration 56/1000 | Loss: 0.00002784
Iteration 57/1000 | Loss: 0.00002784
Iteration 58/1000 | Loss: 0.00002784
Iteration 59/1000 | Loss: 0.00002784
Iteration 60/1000 | Loss: 0.00002784
Iteration 61/1000 | Loss: 0.00002783
Iteration 62/1000 | Loss: 0.00002782
Iteration 63/1000 | Loss: 0.00002780
Iteration 64/1000 | Loss: 0.00002779
Iteration 65/1000 | Loss: 0.00002779
Iteration 66/1000 | Loss: 0.00002779
Iteration 67/1000 | Loss: 0.00002779
Iteration 68/1000 | Loss: 0.00002778
Iteration 69/1000 | Loss: 0.00002778
Iteration 70/1000 | Loss: 0.00002778
Iteration 71/1000 | Loss: 0.00002778
Iteration 72/1000 | Loss: 0.00002777
Iteration 73/1000 | Loss: 0.00002777
Iteration 74/1000 | Loss: 0.00002777
Iteration 75/1000 | Loss: 0.00002777
Iteration 76/1000 | Loss: 0.00002777
Iteration 77/1000 | Loss: 0.00002777
Iteration 78/1000 | Loss: 0.00002776
Iteration 79/1000 | Loss: 0.00002776
Iteration 80/1000 | Loss: 0.00002776
Iteration 81/1000 | Loss: 0.00002776
Iteration 82/1000 | Loss: 0.00002776
Iteration 83/1000 | Loss: 0.00002776
Iteration 84/1000 | Loss: 0.00002776
Iteration 85/1000 | Loss: 0.00002775
Iteration 86/1000 | Loss: 0.00002775
Iteration 87/1000 | Loss: 0.00002775
Iteration 88/1000 | Loss: 0.00002775
Iteration 89/1000 | Loss: 0.00002774
Iteration 90/1000 | Loss: 0.00002774
Iteration 91/1000 | Loss: 0.00002774
Iteration 92/1000 | Loss: 0.00002774
Iteration 93/1000 | Loss: 0.00002774
Iteration 94/1000 | Loss: 0.00002774
Iteration 95/1000 | Loss: 0.00002774
Iteration 96/1000 | Loss: 0.00002773
Iteration 97/1000 | Loss: 0.00002773
Iteration 98/1000 | Loss: 0.00002773
Iteration 99/1000 | Loss: 0.00002773
Iteration 100/1000 | Loss: 0.00002773
Iteration 101/1000 | Loss: 0.00002773
Iteration 102/1000 | Loss: 0.00002772
Iteration 103/1000 | Loss: 0.00002772
Iteration 104/1000 | Loss: 0.00002772
Iteration 105/1000 | Loss: 0.00002772
Iteration 106/1000 | Loss: 0.00002771
Iteration 107/1000 | Loss: 0.00002771
Iteration 108/1000 | Loss: 0.00002771
Iteration 109/1000 | Loss: 0.00002770
Iteration 110/1000 | Loss: 0.00002770
Iteration 111/1000 | Loss: 0.00002770
Iteration 112/1000 | Loss: 0.00002770
Iteration 113/1000 | Loss: 0.00002770
Iteration 114/1000 | Loss: 0.00002770
Iteration 115/1000 | Loss: 0.00002770
Iteration 116/1000 | Loss: 0.00002770
Iteration 117/1000 | Loss: 0.00002770
Iteration 118/1000 | Loss: 0.00002770
Iteration 119/1000 | Loss: 0.00002769
Iteration 120/1000 | Loss: 0.00002769
Iteration 121/1000 | Loss: 0.00002769
Iteration 122/1000 | Loss: 0.00002769
Iteration 123/1000 | Loss: 0.00002769
Iteration 124/1000 | Loss: 0.00002769
Iteration 125/1000 | Loss: 0.00002769
Iteration 126/1000 | Loss: 0.00002769
Iteration 127/1000 | Loss: 0.00002769
Iteration 128/1000 | Loss: 0.00002769
Iteration 129/1000 | Loss: 0.00002769
Iteration 130/1000 | Loss: 0.00002769
Iteration 131/1000 | Loss: 0.00002768
Iteration 132/1000 | Loss: 0.00002768
Iteration 133/1000 | Loss: 0.00002768
Iteration 134/1000 | Loss: 0.00002768
Iteration 135/1000 | Loss: 0.00002768
Iteration 136/1000 | Loss: 0.00002768
Iteration 137/1000 | Loss: 0.00002768
Iteration 138/1000 | Loss: 0.00002768
Iteration 139/1000 | Loss: 0.00002768
Iteration 140/1000 | Loss: 0.00002768
Iteration 141/1000 | Loss: 0.00002768
Iteration 142/1000 | Loss: 0.00002768
Iteration 143/1000 | Loss: 0.00002768
Iteration 144/1000 | Loss: 0.00002768
Iteration 145/1000 | Loss: 0.00002768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.7680047423928045e-05, 2.7680047423928045e-05, 2.7680047423928045e-05, 2.7680047423928045e-05, 2.7680047423928045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7680047423928045e-05

Optimization complete. Final v2v error: 4.307371616363525 mm

Highest mean error: 5.197404861450195 mm for frame 58

Lowest mean error: 3.220952033996582 mm for frame 161

Saving results

Total time: 75.87012076377869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387708
Iteration 2/25 | Loss: 0.00106932
Iteration 3/25 | Loss: 0.00097763
Iteration 4/25 | Loss: 0.00095723
Iteration 5/25 | Loss: 0.00095029
Iteration 6/25 | Loss: 0.00094841
Iteration 7/25 | Loss: 0.00094841
Iteration 8/25 | Loss: 0.00094841
Iteration 9/25 | Loss: 0.00094841
Iteration 10/25 | Loss: 0.00094841
Iteration 11/25 | Loss: 0.00094841
Iteration 12/25 | Loss: 0.00094841
Iteration 13/25 | Loss: 0.00094841
Iteration 14/25 | Loss: 0.00094841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009484109468758106, 0.0009484109468758106, 0.0009484109468758106, 0.0009484109468758106, 0.0009484109468758106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009484109468758106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36970448
Iteration 2/25 | Loss: 0.00189624
Iteration 3/25 | Loss: 0.00189624
Iteration 4/25 | Loss: 0.00189624
Iteration 5/25 | Loss: 0.00189624
Iteration 6/25 | Loss: 0.00189624
Iteration 7/25 | Loss: 0.00189624
Iteration 8/25 | Loss: 0.00189624
Iteration 9/25 | Loss: 0.00189624
Iteration 10/25 | Loss: 0.00189624
Iteration 11/25 | Loss: 0.00189623
Iteration 12/25 | Loss: 0.00189623
Iteration 13/25 | Loss: 0.00189623
Iteration 14/25 | Loss: 0.00189623
Iteration 15/25 | Loss: 0.00189623
Iteration 16/25 | Loss: 0.00189623
Iteration 17/25 | Loss: 0.00189623
Iteration 18/25 | Loss: 0.00189623
Iteration 19/25 | Loss: 0.00189623
Iteration 20/25 | Loss: 0.00189623
Iteration 21/25 | Loss: 0.00189623
Iteration 22/25 | Loss: 0.00189623
Iteration 23/25 | Loss: 0.00189623
Iteration 24/25 | Loss: 0.00189623
Iteration 25/25 | Loss: 0.00189623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189623
Iteration 2/1000 | Loss: 0.00001645
Iteration 3/1000 | Loss: 0.00001239
Iteration 4/1000 | Loss: 0.00001149
Iteration 5/1000 | Loss: 0.00001076
Iteration 6/1000 | Loss: 0.00001012
Iteration 7/1000 | Loss: 0.00000984
Iteration 8/1000 | Loss: 0.00000954
Iteration 9/1000 | Loss: 0.00000937
Iteration 10/1000 | Loss: 0.00000935
Iteration 11/1000 | Loss: 0.00000921
Iteration 12/1000 | Loss: 0.00000915
Iteration 13/1000 | Loss: 0.00000914
Iteration 14/1000 | Loss: 0.00000914
Iteration 15/1000 | Loss: 0.00000913
Iteration 16/1000 | Loss: 0.00000913
Iteration 17/1000 | Loss: 0.00000912
Iteration 18/1000 | Loss: 0.00000911
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000903
Iteration 21/1000 | Loss: 0.00000898
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000897
Iteration 24/1000 | Loss: 0.00000894
Iteration 25/1000 | Loss: 0.00000894
Iteration 26/1000 | Loss: 0.00000893
Iteration 27/1000 | Loss: 0.00000893
Iteration 28/1000 | Loss: 0.00000893
Iteration 29/1000 | Loss: 0.00000893
Iteration 30/1000 | Loss: 0.00000893
Iteration 31/1000 | Loss: 0.00000893
Iteration 32/1000 | Loss: 0.00000893
Iteration 33/1000 | Loss: 0.00000892
Iteration 34/1000 | Loss: 0.00000892
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000892
Iteration 37/1000 | Loss: 0.00000891
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000890
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000890
Iteration 42/1000 | Loss: 0.00000890
Iteration 43/1000 | Loss: 0.00000890
Iteration 44/1000 | Loss: 0.00000890
Iteration 45/1000 | Loss: 0.00000890
Iteration 46/1000 | Loss: 0.00000890
Iteration 47/1000 | Loss: 0.00000889
Iteration 48/1000 | Loss: 0.00000889
Iteration 49/1000 | Loss: 0.00000889
Iteration 50/1000 | Loss: 0.00000889
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000889
Iteration 53/1000 | Loss: 0.00000889
Iteration 54/1000 | Loss: 0.00000889
Iteration 55/1000 | Loss: 0.00000889
Iteration 56/1000 | Loss: 0.00000889
Iteration 57/1000 | Loss: 0.00000889
Iteration 58/1000 | Loss: 0.00000889
Iteration 59/1000 | Loss: 0.00000889
Iteration 60/1000 | Loss: 0.00000889
Iteration 61/1000 | Loss: 0.00000889
Iteration 62/1000 | Loss: 0.00000889
Iteration 63/1000 | Loss: 0.00000889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [8.894753591448534e-06, 8.894753591448534e-06, 8.894753591448534e-06, 8.894753591448534e-06, 8.894753591448534e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.894753591448534e-06

Optimization complete. Final v2v error: 2.531080722808838 mm

Highest mean error: 2.787803888320923 mm for frame 187

Lowest mean error: 2.269871473312378 mm for frame 210

Saving results

Total time: 32.17375445365906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955342
Iteration 2/25 | Loss: 0.00143534
Iteration 3/25 | Loss: 0.00125383
Iteration 4/25 | Loss: 0.00108858
Iteration 5/25 | Loss: 0.00105256
Iteration 6/25 | Loss: 0.00102997
Iteration 7/25 | Loss: 0.00102945
Iteration 8/25 | Loss: 0.00102289
Iteration 9/25 | Loss: 0.00101471
Iteration 10/25 | Loss: 0.00101286
Iteration 11/25 | Loss: 0.00101272
Iteration 12/25 | Loss: 0.00101209
Iteration 13/25 | Loss: 0.00101163
Iteration 14/25 | Loss: 0.00101183
Iteration 15/25 | Loss: 0.00101158
Iteration 16/25 | Loss: 0.00101174
Iteration 17/25 | Loss: 0.00101155
Iteration 18/25 | Loss: 0.00101169
Iteration 19/25 | Loss: 0.00101153
Iteration 20/25 | Loss: 0.00101172
Iteration 21/25 | Loss: 0.00101151
Iteration 22/25 | Loss: 0.00101168
Iteration 23/25 | Loss: 0.00101150
Iteration 24/25 | Loss: 0.00101171
Iteration 25/25 | Loss: 0.00101149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34602606
Iteration 2/25 | Loss: 0.00182884
Iteration 3/25 | Loss: 0.00182883
Iteration 4/25 | Loss: 0.00182883
Iteration 5/25 | Loss: 0.00182882
Iteration 6/25 | Loss: 0.00182882
Iteration 7/25 | Loss: 0.00182882
Iteration 8/25 | Loss: 0.00182882
Iteration 9/25 | Loss: 0.00182882
Iteration 10/25 | Loss: 0.00182882
Iteration 11/25 | Loss: 0.00182882
Iteration 12/25 | Loss: 0.00182882
Iteration 13/25 | Loss: 0.00182882
Iteration 14/25 | Loss: 0.00182882
Iteration 15/25 | Loss: 0.00182882
Iteration 16/25 | Loss: 0.00182882
Iteration 17/25 | Loss: 0.00182882
Iteration 18/25 | Loss: 0.00182882
Iteration 19/25 | Loss: 0.00182882
Iteration 20/25 | Loss: 0.00182882
Iteration 21/25 | Loss: 0.00182882
Iteration 22/25 | Loss: 0.00182882
Iteration 23/25 | Loss: 0.00182882
Iteration 24/25 | Loss: 0.00182882
Iteration 25/25 | Loss: 0.00182882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182882
Iteration 2/1000 | Loss: 0.00004341
Iteration 3/1000 | Loss: 0.00009525
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002148
Iteration 6/1000 | Loss: 0.00002024
Iteration 7/1000 | Loss: 0.00003120
Iteration 8/1000 | Loss: 0.00001922
Iteration 9/1000 | Loss: 0.00002635
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001833
Iteration 12/1000 | Loss: 0.00001805
Iteration 13/1000 | Loss: 0.00017699
Iteration 14/1000 | Loss: 0.00008929
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00017435
Iteration 17/1000 | Loss: 0.00005812
Iteration 18/1000 | Loss: 0.00016689
Iteration 19/1000 | Loss: 0.00006402
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00001850
Iteration 22/1000 | Loss: 0.00016255
Iteration 23/1000 | Loss: 0.00005664
Iteration 24/1000 | Loss: 0.00002617
Iteration 25/1000 | Loss: 0.00001960
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001808
Iteration 28/1000 | Loss: 0.00018135
Iteration 29/1000 | Loss: 0.00005273
Iteration 30/1000 | Loss: 0.00002759
Iteration 31/1000 | Loss: 0.00002541
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00018199
Iteration 34/1000 | Loss: 0.00032981
Iteration 35/1000 | Loss: 0.00004079
Iteration 36/1000 | Loss: 0.00002439
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00018501
Iteration 43/1000 | Loss: 0.00048768
Iteration 44/1000 | Loss: 0.00005199
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00017958
Iteration 49/1000 | Loss: 0.00048924
Iteration 50/1000 | Loss: 0.00013477
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00017444
Iteration 55/1000 | Loss: 0.00013267
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00016924
Iteration 58/1000 | Loss: 0.00015689
Iteration 59/1000 | Loss: 0.00016405
Iteration 60/1000 | Loss: 0.00014717
Iteration 61/1000 | Loss: 0.00007305
Iteration 62/1000 | Loss: 0.00002168
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00001941
Iteration 65/1000 | Loss: 0.00002177
Iteration 66/1000 | Loss: 0.00001749
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00004481
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001695
Iteration 82/1000 | Loss: 0.00001695
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001694
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001692
Iteration 96/1000 | Loss: 0.00001692
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001692
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001688
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00003429
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001814
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001683
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001681
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001681
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001680
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001680
Iteration 149/1000 | Loss: 0.00001680
Iteration 150/1000 | Loss: 0.00001680
Iteration 151/1000 | Loss: 0.00001680
Iteration 152/1000 | Loss: 0.00001680
Iteration 153/1000 | Loss: 0.00001680
Iteration 154/1000 | Loss: 0.00001679
Iteration 155/1000 | Loss: 0.00001679
Iteration 156/1000 | Loss: 0.00001679
Iteration 157/1000 | Loss: 0.00001679
Iteration 158/1000 | Loss: 0.00001679
Iteration 159/1000 | Loss: 0.00001679
Iteration 160/1000 | Loss: 0.00001679
Iteration 161/1000 | Loss: 0.00001679
Iteration 162/1000 | Loss: 0.00001679
Iteration 163/1000 | Loss: 0.00001679
Iteration 164/1000 | Loss: 0.00001679
Iteration 165/1000 | Loss: 0.00001679
Iteration 166/1000 | Loss: 0.00001679
Iteration 167/1000 | Loss: 0.00001679
Iteration 168/1000 | Loss: 0.00001679
Iteration 169/1000 | Loss: 0.00001679
Iteration 170/1000 | Loss: 0.00001679
Iteration 171/1000 | Loss: 0.00001679
Iteration 172/1000 | Loss: 0.00001679
Iteration 173/1000 | Loss: 0.00001679
Iteration 174/1000 | Loss: 0.00001679
Iteration 175/1000 | Loss: 0.00001679
Iteration 176/1000 | Loss: 0.00001678
Iteration 177/1000 | Loss: 0.00001678
Iteration 178/1000 | Loss: 0.00001678
Iteration 179/1000 | Loss: 0.00001678
Iteration 180/1000 | Loss: 0.00001678
Iteration 181/1000 | Loss: 0.00001678
Iteration 182/1000 | Loss: 0.00001678
Iteration 183/1000 | Loss: 0.00001678
Iteration 184/1000 | Loss: 0.00001678
Iteration 185/1000 | Loss: 0.00001678
Iteration 186/1000 | Loss: 0.00001678
Iteration 187/1000 | Loss: 0.00001678
Iteration 188/1000 | Loss: 0.00001678
Iteration 189/1000 | Loss: 0.00001678
Iteration 190/1000 | Loss: 0.00001678
Iteration 191/1000 | Loss: 0.00001678
Iteration 192/1000 | Loss: 0.00001678
Iteration 193/1000 | Loss: 0.00001678
Iteration 194/1000 | Loss: 0.00001678
Iteration 195/1000 | Loss: 0.00001678
Iteration 196/1000 | Loss: 0.00001678
Iteration 197/1000 | Loss: 0.00001678
Iteration 198/1000 | Loss: 0.00001678
Iteration 199/1000 | Loss: 0.00001678
Iteration 200/1000 | Loss: 0.00001678
Iteration 201/1000 | Loss: 0.00001677
Iteration 202/1000 | Loss: 0.00001677
Iteration 203/1000 | Loss: 0.00001677
Iteration 204/1000 | Loss: 0.00002099
Iteration 205/1000 | Loss: 0.00001680
Iteration 206/1000 | Loss: 0.00001680
Iteration 207/1000 | Loss: 0.00001680
Iteration 208/1000 | Loss: 0.00001680
Iteration 209/1000 | Loss: 0.00001680
Iteration 210/1000 | Loss: 0.00001680
Iteration 211/1000 | Loss: 0.00001680
Iteration 212/1000 | Loss: 0.00001680
Iteration 213/1000 | Loss: 0.00001680
Iteration 214/1000 | Loss: 0.00001680
Iteration 215/1000 | Loss: 0.00001679
Iteration 216/1000 | Loss: 0.00001679
Iteration 217/1000 | Loss: 0.00001679
Iteration 218/1000 | Loss: 0.00001708
Iteration 219/1000 | Loss: 0.00001776
Iteration 220/1000 | Loss: 0.00001762
Iteration 221/1000 | Loss: 0.00001678
Iteration 222/1000 | Loss: 0.00001678
Iteration 223/1000 | Loss: 0.00001678
Iteration 224/1000 | Loss: 0.00001677
Iteration 225/1000 | Loss: 0.00001677
Iteration 226/1000 | Loss: 0.00001677
Iteration 227/1000 | Loss: 0.00001677
Iteration 228/1000 | Loss: 0.00001677
Iteration 229/1000 | Loss: 0.00001677
Iteration 230/1000 | Loss: 0.00001677
Iteration 231/1000 | Loss: 0.00001677
Iteration 232/1000 | Loss: 0.00001677
Iteration 233/1000 | Loss: 0.00001677
Iteration 234/1000 | Loss: 0.00001677
Iteration 235/1000 | Loss: 0.00001677
Iteration 236/1000 | Loss: 0.00001677
Iteration 237/1000 | Loss: 0.00001677
Iteration 238/1000 | Loss: 0.00001677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.6773143215687014e-05, 1.6773143215687014e-05, 1.6773143215687014e-05, 1.6773143215687014e-05, 1.6773143215687014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6773143215687014e-05

Optimization complete. Final v2v error: 2.944822072982788 mm

Highest mean error: 20.06828498840332 mm for frame 21

Lowest mean error: 2.2737631797790527 mm for frame 121

Saving results

Total time: 171.1442174911499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991605
Iteration 2/25 | Loss: 0.00386498
Iteration 3/25 | Loss: 0.00271055
Iteration 4/25 | Loss: 0.00215907
Iteration 5/25 | Loss: 0.00201392
Iteration 6/25 | Loss: 0.00181538
Iteration 7/25 | Loss: 0.00178533
Iteration 8/25 | Loss: 0.00173635
Iteration 9/25 | Loss: 0.00162405
Iteration 10/25 | Loss: 0.00153592
Iteration 11/25 | Loss: 0.00153012
Iteration 12/25 | Loss: 0.00147732
Iteration 13/25 | Loss: 0.00148410
Iteration 14/25 | Loss: 0.00147354
Iteration 15/25 | Loss: 0.00146844
Iteration 16/25 | Loss: 0.00146308
Iteration 17/25 | Loss: 0.00145789
Iteration 18/25 | Loss: 0.00145966
Iteration 19/25 | Loss: 0.00145801
Iteration 20/25 | Loss: 0.00146062
Iteration 21/25 | Loss: 0.00145593
Iteration 22/25 | Loss: 0.00146000
Iteration 23/25 | Loss: 0.00145607
Iteration 24/25 | Loss: 0.00144679
Iteration 25/25 | Loss: 0.00144517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87095451
Iteration 2/25 | Loss: 0.00504475
Iteration 3/25 | Loss: 0.00499736
Iteration 4/25 | Loss: 0.00499736
Iteration 5/25 | Loss: 0.00499736
Iteration 6/25 | Loss: 0.00499736
Iteration 7/25 | Loss: 0.00499736
Iteration 8/25 | Loss: 0.00499736
Iteration 9/25 | Loss: 0.00499736
Iteration 10/25 | Loss: 0.00499736
Iteration 11/25 | Loss: 0.00499736
Iteration 12/25 | Loss: 0.00499736
Iteration 13/25 | Loss: 0.00499736
Iteration 14/25 | Loss: 0.00499736
Iteration 15/25 | Loss: 0.00499736
Iteration 16/25 | Loss: 0.00499736
Iteration 17/25 | Loss: 0.00499736
Iteration 18/25 | Loss: 0.00499736
Iteration 19/25 | Loss: 0.00499736
Iteration 20/25 | Loss: 0.00499736
Iteration 21/25 | Loss: 0.00499736
Iteration 22/25 | Loss: 0.00499736
Iteration 23/25 | Loss: 0.00499736
Iteration 24/25 | Loss: 0.00499736
Iteration 25/25 | Loss: 0.00499736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00499736
Iteration 2/1000 | Loss: 0.00058610
Iteration 3/1000 | Loss: 0.00045918
Iteration 4/1000 | Loss: 0.00084511
Iteration 5/1000 | Loss: 0.00131942
Iteration 6/1000 | Loss: 0.00092071
Iteration 7/1000 | Loss: 0.00293937
Iteration 8/1000 | Loss: 0.00407542
Iteration 9/1000 | Loss: 0.00321617
Iteration 10/1000 | Loss: 0.00570487
Iteration 11/1000 | Loss: 0.00354532
Iteration 12/1000 | Loss: 0.00391529
Iteration 13/1000 | Loss: 0.00137253
Iteration 14/1000 | Loss: 0.00213420
Iteration 15/1000 | Loss: 0.00048091
Iteration 16/1000 | Loss: 0.00108253
Iteration 17/1000 | Loss: 0.00026111
Iteration 18/1000 | Loss: 0.00038688
Iteration 19/1000 | Loss: 0.00019242
Iteration 20/1000 | Loss: 0.00019214
Iteration 21/1000 | Loss: 0.00215017
Iteration 22/1000 | Loss: 0.02035341
Iteration 23/1000 | Loss: 0.01507958
Iteration 24/1000 | Loss: 0.00481854
Iteration 25/1000 | Loss: 0.00395198
Iteration 26/1000 | Loss: 0.00189551
Iteration 27/1000 | Loss: 0.00297908
Iteration 28/1000 | Loss: 0.00119771
Iteration 29/1000 | Loss: 0.00024712
Iteration 30/1000 | Loss: 0.00113095
Iteration 31/1000 | Loss: 0.00139209
Iteration 32/1000 | Loss: 0.00156323
Iteration 33/1000 | Loss: 0.00120294
Iteration 34/1000 | Loss: 0.00065245
Iteration 35/1000 | Loss: 0.00036498
Iteration 36/1000 | Loss: 0.00044118
Iteration 37/1000 | Loss: 0.00082816
Iteration 38/1000 | Loss: 0.00059187
Iteration 39/1000 | Loss: 0.00191692
Iteration 40/1000 | Loss: 0.00114629
Iteration 41/1000 | Loss: 0.00090037
Iteration 42/1000 | Loss: 0.00091701
Iteration 43/1000 | Loss: 0.00047779
Iteration 44/1000 | Loss: 0.00015465
Iteration 45/1000 | Loss: 0.00309353
Iteration 46/1000 | Loss: 0.00806832
Iteration 47/1000 | Loss: 0.00062051
Iteration 48/1000 | Loss: 0.00636039
Iteration 49/1000 | Loss: 0.00057995
Iteration 50/1000 | Loss: 0.00164254
Iteration 51/1000 | Loss: 0.00078861
Iteration 52/1000 | Loss: 0.00077748
Iteration 53/1000 | Loss: 0.00026418
Iteration 54/1000 | Loss: 0.00075044
Iteration 55/1000 | Loss: 0.00075150
Iteration 56/1000 | Loss: 0.00030441
Iteration 57/1000 | Loss: 0.00088741
Iteration 58/1000 | Loss: 0.00066390
Iteration 59/1000 | Loss: 0.00025949
Iteration 60/1000 | Loss: 0.00027571
Iteration 61/1000 | Loss: 0.00018204
Iteration 62/1000 | Loss: 0.00011366
Iteration 63/1000 | Loss: 0.00190179
Iteration 64/1000 | Loss: 0.00186935
Iteration 65/1000 | Loss: 0.00175537
Iteration 66/1000 | Loss: 0.00058018
Iteration 67/1000 | Loss: 0.00035425
Iteration 68/1000 | Loss: 0.00074046
Iteration 69/1000 | Loss: 0.00045703
Iteration 70/1000 | Loss: 0.00048075
Iteration 71/1000 | Loss: 0.00021410
Iteration 72/1000 | Loss: 0.00020084
Iteration 73/1000 | Loss: 0.00022621
Iteration 74/1000 | Loss: 0.00010787
Iteration 75/1000 | Loss: 0.00024695
Iteration 76/1000 | Loss: 0.00011937
Iteration 77/1000 | Loss: 0.00020940
Iteration 78/1000 | Loss: 0.00012437
Iteration 79/1000 | Loss: 0.00099583
Iteration 80/1000 | Loss: 0.00108961
Iteration 81/1000 | Loss: 0.00149118
Iteration 82/1000 | Loss: 0.00139788
Iteration 83/1000 | Loss: 0.00150855
Iteration 84/1000 | Loss: 0.00115928
Iteration 85/1000 | Loss: 0.00068396
Iteration 86/1000 | Loss: 0.00071417
Iteration 87/1000 | Loss: 0.00093952
Iteration 88/1000 | Loss: 0.00206175
Iteration 89/1000 | Loss: 0.00046375
Iteration 90/1000 | Loss: 0.00017403
Iteration 91/1000 | Loss: 0.00097848
Iteration 92/1000 | Loss: 0.00273253
Iteration 93/1000 | Loss: 0.00069273
Iteration 94/1000 | Loss: 0.00094904
Iteration 95/1000 | Loss: 0.00088901
Iteration 96/1000 | Loss: 0.00039748
Iteration 97/1000 | Loss: 0.00086804
Iteration 98/1000 | Loss: 0.00092798
Iteration 99/1000 | Loss: 0.00071224
Iteration 100/1000 | Loss: 0.00061372
Iteration 101/1000 | Loss: 0.00072354
Iteration 102/1000 | Loss: 0.00062644
Iteration 103/1000 | Loss: 0.00067701
Iteration 104/1000 | Loss: 0.00076948
Iteration 105/1000 | Loss: 0.00080488
Iteration 106/1000 | Loss: 0.00051969
Iteration 107/1000 | Loss: 0.00086182
Iteration 108/1000 | Loss: 0.00077847
Iteration 109/1000 | Loss: 0.00156100
Iteration 110/1000 | Loss: 0.00165543
Iteration 111/1000 | Loss: 0.00140018
Iteration 112/1000 | Loss: 0.00129837
Iteration 113/1000 | Loss: 0.00178833
Iteration 114/1000 | Loss: 0.00071212
Iteration 115/1000 | Loss: 0.00093330
Iteration 116/1000 | Loss: 0.00093493
Iteration 117/1000 | Loss: 0.00030889
Iteration 118/1000 | Loss: 0.00022904
Iteration 119/1000 | Loss: 0.00077734
Iteration 120/1000 | Loss: 0.00090746
Iteration 121/1000 | Loss: 0.00114652
Iteration 122/1000 | Loss: 0.00062030
Iteration 123/1000 | Loss: 0.00123039
Iteration 124/1000 | Loss: 0.00082663
Iteration 125/1000 | Loss: 0.00108834
Iteration 126/1000 | Loss: 0.00009267
Iteration 127/1000 | Loss: 0.00012358
Iteration 128/1000 | Loss: 0.00010113
Iteration 129/1000 | Loss: 0.00007161
Iteration 130/1000 | Loss: 0.00016297
Iteration 131/1000 | Loss: 0.00009594
Iteration 132/1000 | Loss: 0.00019906
Iteration 133/1000 | Loss: 0.00011644
Iteration 134/1000 | Loss: 0.00020206
Iteration 135/1000 | Loss: 0.00097965
Iteration 136/1000 | Loss: 0.00006728
Iteration 137/1000 | Loss: 0.00015589
Iteration 138/1000 | Loss: 0.00006736
Iteration 139/1000 | Loss: 0.00021699
Iteration 140/1000 | Loss: 0.00009522
Iteration 141/1000 | Loss: 0.00012019
Iteration 142/1000 | Loss: 0.00015505
Iteration 143/1000 | Loss: 0.00014266
Iteration 144/1000 | Loss: 0.00019788
Iteration 145/1000 | Loss: 0.00006961
Iteration 146/1000 | Loss: 0.00019800
Iteration 147/1000 | Loss: 0.00011137
Iteration 148/1000 | Loss: 0.00013346
Iteration 149/1000 | Loss: 0.00033559
Iteration 150/1000 | Loss: 0.00024776
Iteration 151/1000 | Loss: 0.00016227
Iteration 152/1000 | Loss: 0.00011819
Iteration 153/1000 | Loss: 0.00019423
Iteration 154/1000 | Loss: 0.00014281
Iteration 155/1000 | Loss: 0.00011798
Iteration 156/1000 | Loss: 0.00005533
Iteration 157/1000 | Loss: 0.00012150
Iteration 158/1000 | Loss: 0.00008655
Iteration 159/1000 | Loss: 0.00011568
Iteration 160/1000 | Loss: 0.00010535
Iteration 161/1000 | Loss: 0.00011448
Iteration 162/1000 | Loss: 0.00020503
Iteration 163/1000 | Loss: 0.00019592
Iteration 164/1000 | Loss: 0.00032273
Iteration 165/1000 | Loss: 0.00025160
Iteration 166/1000 | Loss: 0.00028161
Iteration 167/1000 | Loss: 0.00022857
Iteration 168/1000 | Loss: 0.00161501
Iteration 169/1000 | Loss: 0.00051538
Iteration 170/1000 | Loss: 0.00012016
Iteration 171/1000 | Loss: 0.00020176
Iteration 172/1000 | Loss: 0.00005471
Iteration 173/1000 | Loss: 0.00005069
Iteration 174/1000 | Loss: 0.00004963
Iteration 175/1000 | Loss: 0.00004876
Iteration 176/1000 | Loss: 0.00004809
Iteration 177/1000 | Loss: 0.00004722
Iteration 178/1000 | Loss: 0.00004656
Iteration 179/1000 | Loss: 0.00004566
Iteration 180/1000 | Loss: 0.00052586
Iteration 181/1000 | Loss: 0.00101634
Iteration 182/1000 | Loss: 0.00043930
Iteration 183/1000 | Loss: 0.00005203
Iteration 184/1000 | Loss: 0.00004714
Iteration 185/1000 | Loss: 0.00004413
Iteration 186/1000 | Loss: 0.00004192
Iteration 187/1000 | Loss: 0.00003948
Iteration 188/1000 | Loss: 0.00137477
Iteration 189/1000 | Loss: 0.00079703
Iteration 190/1000 | Loss: 0.00109069
Iteration 191/1000 | Loss: 0.00051668
Iteration 192/1000 | Loss: 0.00087042
Iteration 193/1000 | Loss: 0.00023891
Iteration 194/1000 | Loss: 0.00129879
Iteration 195/1000 | Loss: 0.00063551
Iteration 196/1000 | Loss: 0.00005442
Iteration 197/1000 | Loss: 0.00004552
Iteration 198/1000 | Loss: 0.00004179
Iteration 199/1000 | Loss: 0.00004019
Iteration 200/1000 | Loss: 0.00003890
Iteration 201/1000 | Loss: 0.00003734
Iteration 202/1000 | Loss: 0.00003635
Iteration 203/1000 | Loss: 0.00003540
Iteration 204/1000 | Loss: 0.00003504
Iteration 205/1000 | Loss: 0.00003477
Iteration 206/1000 | Loss: 0.00003511
Iteration 207/1000 | Loss: 0.00003511
Iteration 208/1000 | Loss: 0.00003467
Iteration 209/1000 | Loss: 0.00003455
Iteration 210/1000 | Loss: 0.00003443
Iteration 211/1000 | Loss: 0.00003431
Iteration 212/1000 | Loss: 0.00003430
Iteration 213/1000 | Loss: 0.00003430
Iteration 214/1000 | Loss: 0.00003430
Iteration 215/1000 | Loss: 0.00003430
Iteration 216/1000 | Loss: 0.00003430
Iteration 217/1000 | Loss: 0.00003429
Iteration 218/1000 | Loss: 0.00003429
Iteration 219/1000 | Loss: 0.00003429
Iteration 220/1000 | Loss: 0.00003429
Iteration 221/1000 | Loss: 0.00003429
Iteration 222/1000 | Loss: 0.00003429
Iteration 223/1000 | Loss: 0.00003429
Iteration 224/1000 | Loss: 0.00003429
Iteration 225/1000 | Loss: 0.00003429
Iteration 226/1000 | Loss: 0.00003428
Iteration 227/1000 | Loss: 0.00003427
Iteration 228/1000 | Loss: 0.00003427
Iteration 229/1000 | Loss: 0.00003426
Iteration 230/1000 | Loss: 0.00003426
Iteration 231/1000 | Loss: 0.00003425
Iteration 232/1000 | Loss: 0.00003425
Iteration 233/1000 | Loss: 0.00003424
Iteration 234/1000 | Loss: 0.00003424
Iteration 235/1000 | Loss: 0.00003424
Iteration 236/1000 | Loss: 0.00003423
Iteration 237/1000 | Loss: 0.00003422
Iteration 238/1000 | Loss: 0.00003422
Iteration 239/1000 | Loss: 0.00003422
Iteration 240/1000 | Loss: 0.00003422
Iteration 241/1000 | Loss: 0.00003422
Iteration 242/1000 | Loss: 0.00003422
Iteration 243/1000 | Loss: 0.00003422
Iteration 244/1000 | Loss: 0.00003421
Iteration 245/1000 | Loss: 0.00003421
Iteration 246/1000 | Loss: 0.00003421
Iteration 247/1000 | Loss: 0.00003420
Iteration 248/1000 | Loss: 0.00003419
Iteration 249/1000 | Loss: 0.00003419
Iteration 250/1000 | Loss: 0.00003419
Iteration 251/1000 | Loss: 0.00003419
Iteration 252/1000 | Loss: 0.00003419
Iteration 253/1000 | Loss: 0.00003419
Iteration 254/1000 | Loss: 0.00003419
Iteration 255/1000 | Loss: 0.00003419
Iteration 256/1000 | Loss: 0.00003419
Iteration 257/1000 | Loss: 0.00003419
Iteration 258/1000 | Loss: 0.00003419
Iteration 259/1000 | Loss: 0.00003419
Iteration 260/1000 | Loss: 0.00003418
Iteration 261/1000 | Loss: 0.00003418
Iteration 262/1000 | Loss: 0.00003418
Iteration 263/1000 | Loss: 0.00003418
Iteration 264/1000 | Loss: 0.00003417
Iteration 265/1000 | Loss: 0.00003417
Iteration 266/1000 | Loss: 0.00003416
Iteration 267/1000 | Loss: 0.00003415
Iteration 268/1000 | Loss: 0.00003415
Iteration 269/1000 | Loss: 0.00003415
Iteration 270/1000 | Loss: 0.00003415
Iteration 271/1000 | Loss: 0.00003415
Iteration 272/1000 | Loss: 0.00003415
Iteration 273/1000 | Loss: 0.00003415
Iteration 274/1000 | Loss: 0.00003414
Iteration 275/1000 | Loss: 0.00003414
Iteration 276/1000 | Loss: 0.00003413
Iteration 277/1000 | Loss: 0.00003413
Iteration 278/1000 | Loss: 0.00003413
Iteration 279/1000 | Loss: 0.00003413
Iteration 280/1000 | Loss: 0.00003413
Iteration 281/1000 | Loss: 0.00003413
Iteration 282/1000 | Loss: 0.00003412
Iteration 283/1000 | Loss: 0.00003412
Iteration 284/1000 | Loss: 0.00003412
Iteration 285/1000 | Loss: 0.00003412
Iteration 286/1000 | Loss: 0.00003411
Iteration 287/1000 | Loss: 0.00003411
Iteration 288/1000 | Loss: 0.00003411
Iteration 289/1000 | Loss: 0.00003410
Iteration 290/1000 | Loss: 0.00003410
Iteration 291/1000 | Loss: 0.00003410
Iteration 292/1000 | Loss: 0.00003410
Iteration 293/1000 | Loss: 0.00003409
Iteration 294/1000 | Loss: 0.00003409
Iteration 295/1000 | Loss: 0.00003408
Iteration 296/1000 | Loss: 0.00003408
Iteration 297/1000 | Loss: 0.00003408
Iteration 298/1000 | Loss: 0.00003408
Iteration 299/1000 | Loss: 0.00003407
Iteration 300/1000 | Loss: 0.00003407
Iteration 301/1000 | Loss: 0.00003407
Iteration 302/1000 | Loss: 0.00003406
Iteration 303/1000 | Loss: 0.00003406
Iteration 304/1000 | Loss: 0.00003406
Iteration 305/1000 | Loss: 0.00003405
Iteration 306/1000 | Loss: 0.00003405
Iteration 307/1000 | Loss: 0.00003405
Iteration 308/1000 | Loss: 0.00003405
Iteration 309/1000 | Loss: 0.00003405
Iteration 310/1000 | Loss: 0.00003405
Iteration 311/1000 | Loss: 0.00003405
Iteration 312/1000 | Loss: 0.00003404
Iteration 313/1000 | Loss: 0.00003404
Iteration 314/1000 | Loss: 0.00003404
Iteration 315/1000 | Loss: 0.00003403
Iteration 316/1000 | Loss: 0.00003403
Iteration 317/1000 | Loss: 0.00003403
Iteration 318/1000 | Loss: 0.00003402
Iteration 319/1000 | Loss: 0.00003402
Iteration 320/1000 | Loss: 0.00003401
Iteration 321/1000 | Loss: 0.00003401
Iteration 322/1000 | Loss: 0.00003400
Iteration 323/1000 | Loss: 0.00003400
Iteration 324/1000 | Loss: 0.00003399
Iteration 325/1000 | Loss: 0.00003399
Iteration 326/1000 | Loss: 0.00003398
Iteration 327/1000 | Loss: 0.00003438
Iteration 328/1000 | Loss: 0.00003406
Iteration 329/1000 | Loss: 0.00003405
Iteration 330/1000 | Loss: 0.00003395
Iteration 331/1000 | Loss: 0.00003394
Iteration 332/1000 | Loss: 0.00003394
Iteration 333/1000 | Loss: 0.00003394
Iteration 334/1000 | Loss: 0.00003394
Iteration 335/1000 | Loss: 0.00003394
Iteration 336/1000 | Loss: 0.00003394
Iteration 337/1000 | Loss: 0.00003394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [3.3940774301299825e-05, 3.3940774301299825e-05, 3.3940774301299825e-05, 3.3940774301299825e-05, 3.3940774301299825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3940774301299825e-05

Optimization complete. Final v2v error: 3.57536244392395 mm

Highest mean error: 21.66559410095215 mm for frame 21

Lowest mean error: 2.3439674377441406 mm for frame 95

Saving results

Total time: 353.08149576187134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042920
Iteration 2/25 | Loss: 0.01042920
Iteration 3/25 | Loss: 0.01042920
Iteration 4/25 | Loss: 0.00464913
Iteration 5/25 | Loss: 0.00345524
Iteration 6/25 | Loss: 0.00280797
Iteration 7/25 | Loss: 0.00226415
Iteration 8/25 | Loss: 0.00209778
Iteration 9/25 | Loss: 0.00197918
Iteration 10/25 | Loss: 0.00191035
Iteration 11/25 | Loss: 0.00188946
Iteration 12/25 | Loss: 0.00187447
Iteration 13/25 | Loss: 0.00185294
Iteration 14/25 | Loss: 0.00183625
Iteration 15/25 | Loss: 0.00182402
Iteration 16/25 | Loss: 0.00181593
Iteration 17/25 | Loss: 0.00181076
Iteration 18/25 | Loss: 0.00180971
Iteration 19/25 | Loss: 0.00181265
Iteration 20/25 | Loss: 0.00180693
Iteration 21/25 | Loss: 0.00180452
Iteration 22/25 | Loss: 0.00180366
Iteration 23/25 | Loss: 0.00180427
Iteration 24/25 | Loss: 0.00180226
Iteration 25/25 | Loss: 0.00180144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14806449
Iteration 2/25 | Loss: 0.00789497
Iteration 3/25 | Loss: 0.00789497
Iteration 4/25 | Loss: 0.00789497
Iteration 5/25 | Loss: 0.00789497
Iteration 6/25 | Loss: 0.00789497
Iteration 7/25 | Loss: 0.00789497
Iteration 8/25 | Loss: 0.00789497
Iteration 9/25 | Loss: 0.00789497
Iteration 10/25 | Loss: 0.00789497
Iteration 11/25 | Loss: 0.00789497
Iteration 12/25 | Loss: 0.00789496
Iteration 13/25 | Loss: 0.00789497
Iteration 14/25 | Loss: 0.00789496
Iteration 15/25 | Loss: 0.00789496
Iteration 16/25 | Loss: 0.00789496
Iteration 17/25 | Loss: 0.00789496
Iteration 18/25 | Loss: 0.00789496
Iteration 19/25 | Loss: 0.00789496
Iteration 20/25 | Loss: 0.00789496
Iteration 21/25 | Loss: 0.00789496
Iteration 22/25 | Loss: 0.00789496
Iteration 23/25 | Loss: 0.00789496
Iteration 24/25 | Loss: 0.00789496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.007894963957369328, 0.007894963957369328, 0.007894963957369328, 0.007894963957369328, 0.007894963957369328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007894963957369328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00789496
Iteration 2/1000 | Loss: 0.00081538
Iteration 3/1000 | Loss: 0.00071940
Iteration 4/1000 | Loss: 0.00057664
Iteration 5/1000 | Loss: 0.00053854
Iteration 6/1000 | Loss: 0.00051185
Iteration 7/1000 | Loss: 0.00044946
Iteration 8/1000 | Loss: 0.00042179
Iteration 9/1000 | Loss: 0.00039530
Iteration 10/1000 | Loss: 0.00037889
Iteration 11/1000 | Loss: 0.00273707
Iteration 12/1000 | Loss: 0.01627317
Iteration 13/1000 | Loss: 0.01994912
Iteration 14/1000 | Loss: 0.00114428
Iteration 15/1000 | Loss: 0.00096551
Iteration 16/1000 | Loss: 0.00059477
Iteration 17/1000 | Loss: 0.00037929
Iteration 18/1000 | Loss: 0.00028739
Iteration 19/1000 | Loss: 0.00013520
Iteration 20/1000 | Loss: 0.00009490
Iteration 21/1000 | Loss: 0.00007277
Iteration 22/1000 | Loss: 0.00006015
Iteration 23/1000 | Loss: 0.00005230
Iteration 24/1000 | Loss: 0.00004439
Iteration 25/1000 | Loss: 0.00003961
Iteration 26/1000 | Loss: 0.00003612
Iteration 27/1000 | Loss: 0.00003350
Iteration 28/1000 | Loss: 0.00003081
Iteration 29/1000 | Loss: 0.00002931
Iteration 30/1000 | Loss: 0.00002759
Iteration 31/1000 | Loss: 0.00002636
Iteration 32/1000 | Loss: 0.00002558
Iteration 33/1000 | Loss: 0.00002512
Iteration 34/1000 | Loss: 0.00002478
Iteration 35/1000 | Loss: 0.00002459
Iteration 36/1000 | Loss: 0.00002442
Iteration 37/1000 | Loss: 0.00002437
Iteration 38/1000 | Loss: 0.00002436
Iteration 39/1000 | Loss: 0.00002435
Iteration 40/1000 | Loss: 0.00002432
Iteration 41/1000 | Loss: 0.00002432
Iteration 42/1000 | Loss: 0.00002431
Iteration 43/1000 | Loss: 0.00002430
Iteration 44/1000 | Loss: 0.00002429
Iteration 45/1000 | Loss: 0.00002427
Iteration 46/1000 | Loss: 0.00002427
Iteration 47/1000 | Loss: 0.00002426
Iteration 48/1000 | Loss: 0.00002425
Iteration 49/1000 | Loss: 0.00002425
Iteration 50/1000 | Loss: 0.00002425
Iteration 51/1000 | Loss: 0.00002424
Iteration 52/1000 | Loss: 0.00002423
Iteration 53/1000 | Loss: 0.00002423
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002420
Iteration 59/1000 | Loss: 0.00002420
Iteration 60/1000 | Loss: 0.00002419
Iteration 61/1000 | Loss: 0.00002419
Iteration 62/1000 | Loss: 0.00002418
Iteration 63/1000 | Loss: 0.00002416
Iteration 64/1000 | Loss: 0.00002416
Iteration 65/1000 | Loss: 0.00002414
Iteration 66/1000 | Loss: 0.00002414
Iteration 67/1000 | Loss: 0.00002413
Iteration 68/1000 | Loss: 0.00002412
Iteration 69/1000 | Loss: 0.00002412
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002411
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002410
Iteration 77/1000 | Loss: 0.00002410
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002409
Iteration 82/1000 | Loss: 0.00002409
Iteration 83/1000 | Loss: 0.00002408
Iteration 84/1000 | Loss: 0.00002408
Iteration 85/1000 | Loss: 0.00002407
Iteration 86/1000 | Loss: 0.00002407
Iteration 87/1000 | Loss: 0.00002407
Iteration 88/1000 | Loss: 0.00002407
Iteration 89/1000 | Loss: 0.00002407
Iteration 90/1000 | Loss: 0.00002406
Iteration 91/1000 | Loss: 0.00002406
Iteration 92/1000 | Loss: 0.00002406
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002405
Iteration 95/1000 | Loss: 0.00002405
Iteration 96/1000 | Loss: 0.00002404
Iteration 97/1000 | Loss: 0.00002404
Iteration 98/1000 | Loss: 0.00002404
Iteration 99/1000 | Loss: 0.00002403
Iteration 100/1000 | Loss: 0.00002403
Iteration 101/1000 | Loss: 0.00002403
Iteration 102/1000 | Loss: 0.00002403
Iteration 103/1000 | Loss: 0.00002403
Iteration 104/1000 | Loss: 0.00002403
Iteration 105/1000 | Loss: 0.00002403
Iteration 106/1000 | Loss: 0.00002403
Iteration 107/1000 | Loss: 0.00002403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.402923746558372e-05, 2.402923746558372e-05, 2.402923746558372e-05, 2.402923746558372e-05, 2.402923746558372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.402923746558372e-05

Optimization complete. Final v2v error: 3.244779586791992 mm

Highest mean error: 11.51801586151123 mm for frame 169

Lowest mean error: 2.8080625534057617 mm for frame 101

Saving results

Total time: 113.69489288330078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608227
Iteration 2/25 | Loss: 0.00115728
Iteration 3/25 | Loss: 0.00103900
Iteration 4/25 | Loss: 0.00101811
Iteration 5/25 | Loss: 0.00101074
Iteration 6/25 | Loss: 0.00100927
Iteration 7/25 | Loss: 0.00100927
Iteration 8/25 | Loss: 0.00100927
Iteration 9/25 | Loss: 0.00100927
Iteration 10/25 | Loss: 0.00100927
Iteration 11/25 | Loss: 0.00100927
Iteration 12/25 | Loss: 0.00100927
Iteration 13/25 | Loss: 0.00100927
Iteration 14/25 | Loss: 0.00100927
Iteration 15/25 | Loss: 0.00100927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001009273575618863, 0.001009273575618863, 0.001009273575618863, 0.001009273575618863, 0.001009273575618863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001009273575618863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34943652
Iteration 2/25 | Loss: 0.00374570
Iteration 3/25 | Loss: 0.00374569
Iteration 4/25 | Loss: 0.00374569
Iteration 5/25 | Loss: 0.00374569
Iteration 6/25 | Loss: 0.00374569
Iteration 7/25 | Loss: 0.00374569
Iteration 8/25 | Loss: 0.00374569
Iteration 9/25 | Loss: 0.00374569
Iteration 10/25 | Loss: 0.00374569
Iteration 11/25 | Loss: 0.00374569
Iteration 12/25 | Loss: 0.00374569
Iteration 13/25 | Loss: 0.00374569
Iteration 14/25 | Loss: 0.00374569
Iteration 15/25 | Loss: 0.00374569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0037456881254911423, 0.0037456881254911423, 0.0037456881254911423, 0.0037456881254911423, 0.0037456881254911423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037456881254911423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374569
Iteration 2/1000 | Loss: 0.00004811
Iteration 3/1000 | Loss: 0.00001927
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001261
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001260
Iteration 21/1000 | Loss: 0.00001260
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001257
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001243
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001242
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001218
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001218
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001217
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001215
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001213
Iteration 143/1000 | Loss: 0.00001213
Iteration 144/1000 | Loss: 0.00001213
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Iteration 147/1000 | Loss: 0.00001213
Iteration 148/1000 | Loss: 0.00001213
Iteration 149/1000 | Loss: 0.00001213
Iteration 150/1000 | Loss: 0.00001213
Iteration 151/1000 | Loss: 0.00001213
Iteration 152/1000 | Loss: 0.00001213
Iteration 153/1000 | Loss: 0.00001213
Iteration 154/1000 | Loss: 0.00001213
Iteration 155/1000 | Loss: 0.00001213
Iteration 156/1000 | Loss: 0.00001213
Iteration 157/1000 | Loss: 0.00001212
Iteration 158/1000 | Loss: 0.00001212
Iteration 159/1000 | Loss: 0.00001212
Iteration 160/1000 | Loss: 0.00001212
Iteration 161/1000 | Loss: 0.00001212
Iteration 162/1000 | Loss: 0.00001212
Iteration 163/1000 | Loss: 0.00001212
Iteration 164/1000 | Loss: 0.00001211
Iteration 165/1000 | Loss: 0.00001211
Iteration 166/1000 | Loss: 0.00001211
Iteration 167/1000 | Loss: 0.00001211
Iteration 168/1000 | Loss: 0.00001211
Iteration 169/1000 | Loss: 0.00001210
Iteration 170/1000 | Loss: 0.00001210
Iteration 171/1000 | Loss: 0.00001210
Iteration 172/1000 | Loss: 0.00001210
Iteration 173/1000 | Loss: 0.00001210
Iteration 174/1000 | Loss: 0.00001210
Iteration 175/1000 | Loss: 0.00001210
Iteration 176/1000 | Loss: 0.00001210
Iteration 177/1000 | Loss: 0.00001210
Iteration 178/1000 | Loss: 0.00001210
Iteration 179/1000 | Loss: 0.00001209
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001208
Iteration 184/1000 | Loss: 0.00001208
Iteration 185/1000 | Loss: 0.00001208
Iteration 186/1000 | Loss: 0.00001208
Iteration 187/1000 | Loss: 0.00001208
Iteration 188/1000 | Loss: 0.00001208
Iteration 189/1000 | Loss: 0.00001208
Iteration 190/1000 | Loss: 0.00001208
Iteration 191/1000 | Loss: 0.00001208
Iteration 192/1000 | Loss: 0.00001208
Iteration 193/1000 | Loss: 0.00001207
Iteration 194/1000 | Loss: 0.00001207
Iteration 195/1000 | Loss: 0.00001207
Iteration 196/1000 | Loss: 0.00001207
Iteration 197/1000 | Loss: 0.00001207
Iteration 198/1000 | Loss: 0.00001207
Iteration 199/1000 | Loss: 0.00001207
Iteration 200/1000 | Loss: 0.00001207
Iteration 201/1000 | Loss: 0.00001206
Iteration 202/1000 | Loss: 0.00001206
Iteration 203/1000 | Loss: 0.00001206
Iteration 204/1000 | Loss: 0.00001206
Iteration 205/1000 | Loss: 0.00001206
Iteration 206/1000 | Loss: 0.00001205
Iteration 207/1000 | Loss: 0.00001205
Iteration 208/1000 | Loss: 0.00001205
Iteration 209/1000 | Loss: 0.00001205
Iteration 210/1000 | Loss: 0.00001205
Iteration 211/1000 | Loss: 0.00001205
Iteration 212/1000 | Loss: 0.00001204
Iteration 213/1000 | Loss: 0.00001203
Iteration 214/1000 | Loss: 0.00001203
Iteration 215/1000 | Loss: 0.00001203
Iteration 216/1000 | Loss: 0.00001203
Iteration 217/1000 | Loss: 0.00001203
Iteration 218/1000 | Loss: 0.00001202
Iteration 219/1000 | Loss: 0.00001202
Iteration 220/1000 | Loss: 0.00001202
Iteration 221/1000 | Loss: 0.00001202
Iteration 222/1000 | Loss: 0.00001202
Iteration 223/1000 | Loss: 0.00001202
Iteration 224/1000 | Loss: 0.00001202
Iteration 225/1000 | Loss: 0.00001201
Iteration 226/1000 | Loss: 0.00001201
Iteration 227/1000 | Loss: 0.00001201
Iteration 228/1000 | Loss: 0.00001201
Iteration 229/1000 | Loss: 0.00001201
Iteration 230/1000 | Loss: 0.00001201
Iteration 231/1000 | Loss: 0.00001201
Iteration 232/1000 | Loss: 0.00001201
Iteration 233/1000 | Loss: 0.00001201
Iteration 234/1000 | Loss: 0.00001201
Iteration 235/1000 | Loss: 0.00001201
Iteration 236/1000 | Loss: 0.00001200
Iteration 237/1000 | Loss: 0.00001200
Iteration 238/1000 | Loss: 0.00001200
Iteration 239/1000 | Loss: 0.00001200
Iteration 240/1000 | Loss: 0.00001200
Iteration 241/1000 | Loss: 0.00001200
Iteration 242/1000 | Loss: 0.00001200
Iteration 243/1000 | Loss: 0.00001200
Iteration 244/1000 | Loss: 0.00001200
Iteration 245/1000 | Loss: 0.00001200
Iteration 246/1000 | Loss: 0.00001200
Iteration 247/1000 | Loss: 0.00001199
Iteration 248/1000 | Loss: 0.00001199
Iteration 249/1000 | Loss: 0.00001199
Iteration 250/1000 | Loss: 0.00001199
Iteration 251/1000 | Loss: 0.00001199
Iteration 252/1000 | Loss: 0.00001199
Iteration 253/1000 | Loss: 0.00001199
Iteration 254/1000 | Loss: 0.00001199
Iteration 255/1000 | Loss: 0.00001199
Iteration 256/1000 | Loss: 0.00001199
Iteration 257/1000 | Loss: 0.00001199
Iteration 258/1000 | Loss: 0.00001199
Iteration 259/1000 | Loss: 0.00001198
Iteration 260/1000 | Loss: 0.00001198
Iteration 261/1000 | Loss: 0.00001198
Iteration 262/1000 | Loss: 0.00001198
Iteration 263/1000 | Loss: 0.00001198
Iteration 264/1000 | Loss: 0.00001198
Iteration 265/1000 | Loss: 0.00001198
Iteration 266/1000 | Loss: 0.00001198
Iteration 267/1000 | Loss: 0.00001198
Iteration 268/1000 | Loss: 0.00001198
Iteration 269/1000 | Loss: 0.00001198
Iteration 270/1000 | Loss: 0.00001198
Iteration 271/1000 | Loss: 0.00001198
Iteration 272/1000 | Loss: 0.00001198
Iteration 273/1000 | Loss: 0.00001198
Iteration 274/1000 | Loss: 0.00001198
Iteration 275/1000 | Loss: 0.00001198
Iteration 276/1000 | Loss: 0.00001198
Iteration 277/1000 | Loss: 0.00001198
Iteration 278/1000 | Loss: 0.00001198
Iteration 279/1000 | Loss: 0.00001198
Iteration 280/1000 | Loss: 0.00001198
Iteration 281/1000 | Loss: 0.00001198
Iteration 282/1000 | Loss: 0.00001198
Iteration 283/1000 | Loss: 0.00001198
Iteration 284/1000 | Loss: 0.00001198
Iteration 285/1000 | Loss: 0.00001198
Iteration 286/1000 | Loss: 0.00001198
Iteration 287/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.1976914720435161e-05, 1.1976914720435161e-05, 1.1976914720435161e-05, 1.1976914720435161e-05, 1.1976914720435161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1976914720435161e-05

Optimization complete. Final v2v error: 2.978790760040283 mm

Highest mean error: 3.1345534324645996 mm for frame 206

Lowest mean error: 2.7347464561462402 mm for frame 136

Saving results

Total time: 53.755913496017456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373752
Iteration 2/25 | Loss: 0.00106021
Iteration 3/25 | Loss: 0.00096371
Iteration 4/25 | Loss: 0.00094847
Iteration 5/25 | Loss: 0.00094314
Iteration 6/25 | Loss: 0.00094110
Iteration 7/25 | Loss: 0.00094110
Iteration 8/25 | Loss: 0.00094110
Iteration 9/25 | Loss: 0.00094110
Iteration 10/25 | Loss: 0.00094110
Iteration 11/25 | Loss: 0.00094110
Iteration 12/25 | Loss: 0.00094110
Iteration 13/25 | Loss: 0.00094110
Iteration 14/25 | Loss: 0.00094110
Iteration 15/25 | Loss: 0.00094110
Iteration 16/25 | Loss: 0.00094110
Iteration 17/25 | Loss: 0.00094110
Iteration 18/25 | Loss: 0.00094110
Iteration 19/25 | Loss: 0.00094110
Iteration 20/25 | Loss: 0.00094110
Iteration 21/25 | Loss: 0.00094110
Iteration 22/25 | Loss: 0.00094110
Iteration 23/25 | Loss: 0.00094110
Iteration 24/25 | Loss: 0.00094110
Iteration 25/25 | Loss: 0.00094110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29075789
Iteration 2/25 | Loss: 0.00196419
Iteration 3/25 | Loss: 0.00196419
Iteration 4/25 | Loss: 0.00196419
Iteration 5/25 | Loss: 0.00196419
Iteration 6/25 | Loss: 0.00196419
Iteration 7/25 | Loss: 0.00196419
Iteration 8/25 | Loss: 0.00196419
Iteration 9/25 | Loss: 0.00196419
Iteration 10/25 | Loss: 0.00196419
Iteration 11/25 | Loss: 0.00196419
Iteration 12/25 | Loss: 0.00196419
Iteration 13/25 | Loss: 0.00196419
Iteration 14/25 | Loss: 0.00196419
Iteration 15/25 | Loss: 0.00196419
Iteration 16/25 | Loss: 0.00196419
Iteration 17/25 | Loss: 0.00196419
Iteration 18/25 | Loss: 0.00196419
Iteration 19/25 | Loss: 0.00196419
Iteration 20/25 | Loss: 0.00196419
Iteration 21/25 | Loss: 0.00196419
Iteration 22/25 | Loss: 0.00196419
Iteration 23/25 | Loss: 0.00196419
Iteration 24/25 | Loss: 0.00196419
Iteration 25/25 | Loss: 0.00196419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196419
Iteration 2/1000 | Loss: 0.00001290
Iteration 3/1000 | Loss: 0.00000849
Iteration 4/1000 | Loss: 0.00000748
Iteration 5/1000 | Loss: 0.00000703
Iteration 6/1000 | Loss: 0.00000652
Iteration 7/1000 | Loss: 0.00000632
Iteration 8/1000 | Loss: 0.00000626
Iteration 9/1000 | Loss: 0.00000615
Iteration 10/1000 | Loss: 0.00000603
Iteration 11/1000 | Loss: 0.00000601
Iteration 12/1000 | Loss: 0.00000599
Iteration 13/1000 | Loss: 0.00000598
Iteration 14/1000 | Loss: 0.00000596
Iteration 15/1000 | Loss: 0.00000596
Iteration 16/1000 | Loss: 0.00000596
Iteration 17/1000 | Loss: 0.00000596
Iteration 18/1000 | Loss: 0.00000596
Iteration 19/1000 | Loss: 0.00000596
Iteration 20/1000 | Loss: 0.00000596
Iteration 21/1000 | Loss: 0.00000595
Iteration 22/1000 | Loss: 0.00000593
Iteration 23/1000 | Loss: 0.00000592
Iteration 24/1000 | Loss: 0.00000588
Iteration 25/1000 | Loss: 0.00000588
Iteration 26/1000 | Loss: 0.00000588
Iteration 27/1000 | Loss: 0.00000587
Iteration 28/1000 | Loss: 0.00000587
Iteration 29/1000 | Loss: 0.00000587
Iteration 30/1000 | Loss: 0.00000587
Iteration 31/1000 | Loss: 0.00000587
Iteration 32/1000 | Loss: 0.00000587
Iteration 33/1000 | Loss: 0.00000587
Iteration 34/1000 | Loss: 0.00000587
Iteration 35/1000 | Loss: 0.00000587
Iteration 36/1000 | Loss: 0.00000587
Iteration 37/1000 | Loss: 0.00000586
Iteration 38/1000 | Loss: 0.00000584
Iteration 39/1000 | Loss: 0.00000583
Iteration 40/1000 | Loss: 0.00000583
Iteration 41/1000 | Loss: 0.00000583
Iteration 42/1000 | Loss: 0.00000582
Iteration 43/1000 | Loss: 0.00000582
Iteration 44/1000 | Loss: 0.00000582
Iteration 45/1000 | Loss: 0.00000582
Iteration 46/1000 | Loss: 0.00000582
Iteration 47/1000 | Loss: 0.00000581
Iteration 48/1000 | Loss: 0.00000581
Iteration 49/1000 | Loss: 0.00000580
Iteration 50/1000 | Loss: 0.00000576
Iteration 51/1000 | Loss: 0.00000575
Iteration 52/1000 | Loss: 0.00000574
Iteration 53/1000 | Loss: 0.00000573
Iteration 54/1000 | Loss: 0.00000573
Iteration 55/1000 | Loss: 0.00000572
Iteration 56/1000 | Loss: 0.00000572
Iteration 57/1000 | Loss: 0.00000572
Iteration 58/1000 | Loss: 0.00000572
Iteration 59/1000 | Loss: 0.00000572
Iteration 60/1000 | Loss: 0.00000571
Iteration 61/1000 | Loss: 0.00000571
Iteration 62/1000 | Loss: 0.00000571
Iteration 63/1000 | Loss: 0.00000570
Iteration 64/1000 | Loss: 0.00000570
Iteration 65/1000 | Loss: 0.00000570
Iteration 66/1000 | Loss: 0.00000569
Iteration 67/1000 | Loss: 0.00000569
Iteration 68/1000 | Loss: 0.00000569
Iteration 69/1000 | Loss: 0.00000568
Iteration 70/1000 | Loss: 0.00000568
Iteration 71/1000 | Loss: 0.00000568
Iteration 72/1000 | Loss: 0.00000567
Iteration 73/1000 | Loss: 0.00000567
Iteration 74/1000 | Loss: 0.00000567
Iteration 75/1000 | Loss: 0.00000567
Iteration 76/1000 | Loss: 0.00000567
Iteration 77/1000 | Loss: 0.00000567
Iteration 78/1000 | Loss: 0.00000567
Iteration 79/1000 | Loss: 0.00000567
Iteration 80/1000 | Loss: 0.00000567
Iteration 81/1000 | Loss: 0.00000567
Iteration 82/1000 | Loss: 0.00000567
Iteration 83/1000 | Loss: 0.00000567
Iteration 84/1000 | Loss: 0.00000567
Iteration 85/1000 | Loss: 0.00000567
Iteration 86/1000 | Loss: 0.00000567
Iteration 87/1000 | Loss: 0.00000567
Iteration 88/1000 | Loss: 0.00000567
Iteration 89/1000 | Loss: 0.00000567
Iteration 90/1000 | Loss: 0.00000567
Iteration 91/1000 | Loss: 0.00000567
Iteration 92/1000 | Loss: 0.00000567
Iteration 93/1000 | Loss: 0.00000567
Iteration 94/1000 | Loss: 0.00000567
Iteration 95/1000 | Loss: 0.00000567
Iteration 96/1000 | Loss: 0.00000567
Iteration 97/1000 | Loss: 0.00000567
Iteration 98/1000 | Loss: 0.00000567
Iteration 99/1000 | Loss: 0.00000567
Iteration 100/1000 | Loss: 0.00000567
Iteration 101/1000 | Loss: 0.00000567
Iteration 102/1000 | Loss: 0.00000567
Iteration 103/1000 | Loss: 0.00000567
Iteration 104/1000 | Loss: 0.00000567
Iteration 105/1000 | Loss: 0.00000567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [5.668896847055294e-06, 5.668896847055294e-06, 5.668896847055294e-06, 5.668896847055294e-06, 5.668896847055294e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.668896847055294e-06

Optimization complete. Final v2v error: 2.050034999847412 mm

Highest mean error: 2.111332893371582 mm for frame 58

Lowest mean error: 1.9957154989242554 mm for frame 3

Saving results

Total time: 30.54779863357544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064524
Iteration 2/25 | Loss: 0.01064523
Iteration 3/25 | Loss: 0.01064523
Iteration 4/25 | Loss: 0.01064523
Iteration 5/25 | Loss: 0.01064523
Iteration 6/25 | Loss: 0.01064523
Iteration 7/25 | Loss: 0.01064523
Iteration 8/25 | Loss: 0.01064523
Iteration 9/25 | Loss: 0.01064523
Iteration 10/25 | Loss: 0.01064523
Iteration 11/25 | Loss: 0.01064522
Iteration 12/25 | Loss: 0.01064522
Iteration 13/25 | Loss: 0.01064522
Iteration 14/25 | Loss: 0.01064522
Iteration 15/25 | Loss: 0.01064522
Iteration 16/25 | Loss: 0.01064522
Iteration 17/25 | Loss: 0.01064522
Iteration 18/25 | Loss: 0.01064522
Iteration 19/25 | Loss: 0.01064522
Iteration 20/25 | Loss: 0.01064521
Iteration 21/25 | Loss: 0.01064521
Iteration 22/25 | Loss: 0.01064521
Iteration 23/25 | Loss: 0.01064521
Iteration 24/25 | Loss: 0.01064521
Iteration 25/25 | Loss: 0.01064521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55664945
Iteration 2/25 | Loss: 0.08854470
Iteration 3/25 | Loss: 0.07961043
Iteration 4/25 | Loss: 0.08035261
Iteration 5/25 | Loss: 0.07400927
Iteration 6/25 | Loss: 0.07355255
Iteration 7/25 | Loss: 0.07260180
Iteration 8/25 | Loss: 0.07260131
Iteration 9/25 | Loss: 0.07260130
Iteration 10/25 | Loss: 0.07260130
Iteration 11/25 | Loss: 0.07260130
Iteration 12/25 | Loss: 0.07260130
Iteration 13/25 | Loss: 0.07260130
Iteration 14/25 | Loss: 0.07260130
Iteration 15/25 | Loss: 0.07260130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.07260129600763321, 0.07260129600763321, 0.07260129600763321, 0.07260129600763321, 0.07260129600763321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07260129600763321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07260129
Iteration 2/1000 | Loss: 0.01490199
Iteration 3/1000 | Loss: 0.00742082
Iteration 4/1000 | Loss: 0.00318084
Iteration 5/1000 | Loss: 0.00289140
Iteration 6/1000 | Loss: 0.00214694
Iteration 7/1000 | Loss: 0.00107616
Iteration 8/1000 | Loss: 0.00077123
Iteration 9/1000 | Loss: 0.00043612
Iteration 10/1000 | Loss: 0.00057711
Iteration 11/1000 | Loss: 0.00033722
Iteration 12/1000 | Loss: 0.00072283
Iteration 13/1000 | Loss: 0.00022602
Iteration 14/1000 | Loss: 0.00033012
Iteration 15/1000 | Loss: 0.00021186
Iteration 16/1000 | Loss: 0.00040823
Iteration 17/1000 | Loss: 0.00022391
Iteration 18/1000 | Loss: 0.00026247
Iteration 19/1000 | Loss: 0.00029790
Iteration 20/1000 | Loss: 0.00045743
Iteration 21/1000 | Loss: 0.00035598
Iteration 22/1000 | Loss: 0.00019626
Iteration 23/1000 | Loss: 0.00017679
Iteration 24/1000 | Loss: 0.00026000
Iteration 25/1000 | Loss: 0.00017557
Iteration 26/1000 | Loss: 0.00013795
Iteration 27/1000 | Loss: 0.00015486
Iteration 28/1000 | Loss: 0.00051303
Iteration 29/1000 | Loss: 0.00036056
Iteration 30/1000 | Loss: 0.00013469
Iteration 31/1000 | Loss: 0.00035350
Iteration 32/1000 | Loss: 0.00018558
Iteration 33/1000 | Loss: 0.00020467
Iteration 34/1000 | Loss: 0.00040166
Iteration 35/1000 | Loss: 0.00024726
Iteration 36/1000 | Loss: 0.00014609
Iteration 37/1000 | Loss: 0.00018614
Iteration 38/1000 | Loss: 0.00015675
Iteration 39/1000 | Loss: 0.00020641
Iteration 40/1000 | Loss: 0.00043655
Iteration 41/1000 | Loss: 0.00017001
Iteration 42/1000 | Loss: 0.00010135
Iteration 43/1000 | Loss: 0.00022005
Iteration 44/1000 | Loss: 0.00010746
Iteration 45/1000 | Loss: 0.00007608
Iteration 46/1000 | Loss: 0.00009897
Iteration 47/1000 | Loss: 0.00007242
Iteration 48/1000 | Loss: 0.00010054
Iteration 49/1000 | Loss: 0.00008198
Iteration 50/1000 | Loss: 0.00016712
Iteration 51/1000 | Loss: 0.00017292
Iteration 52/1000 | Loss: 0.00024344
Iteration 53/1000 | Loss: 0.00020054
Iteration 54/1000 | Loss: 0.00021999
Iteration 55/1000 | Loss: 0.00007278
Iteration 56/1000 | Loss: 0.00009726
Iteration 57/1000 | Loss: 0.00006951
Iteration 58/1000 | Loss: 0.00013034
Iteration 59/1000 | Loss: 0.00008999
Iteration 60/1000 | Loss: 0.00009942
Iteration 61/1000 | Loss: 0.00030528
Iteration 62/1000 | Loss: 0.00018181
Iteration 63/1000 | Loss: 0.00010612
Iteration 64/1000 | Loss: 0.00006796
Iteration 65/1000 | Loss: 0.00007471
Iteration 66/1000 | Loss: 0.00005961
Iteration 67/1000 | Loss: 0.00009415
Iteration 68/1000 | Loss: 0.00005777
Iteration 69/1000 | Loss: 0.00033107
Iteration 70/1000 | Loss: 0.00038793
Iteration 71/1000 | Loss: 0.00024415
Iteration 72/1000 | Loss: 0.00038227
Iteration 73/1000 | Loss: 0.00021352
Iteration 74/1000 | Loss: 0.00018980
Iteration 75/1000 | Loss: 0.00013013
Iteration 76/1000 | Loss: 0.00034165
Iteration 77/1000 | Loss: 0.00041345
Iteration 78/1000 | Loss: 0.00026903
Iteration 79/1000 | Loss: 0.00036571
Iteration 80/1000 | Loss: 0.00030335
Iteration 81/1000 | Loss: 0.00013933
Iteration 82/1000 | Loss: 0.00008432
Iteration 83/1000 | Loss: 0.00007449
Iteration 84/1000 | Loss: 0.00007222
Iteration 85/1000 | Loss: 0.00027543
Iteration 86/1000 | Loss: 0.00022095
Iteration 87/1000 | Loss: 0.00049076
Iteration 88/1000 | Loss: 0.00025921
Iteration 89/1000 | Loss: 0.00006934
Iteration 90/1000 | Loss: 0.00007037
Iteration 91/1000 | Loss: 0.00005936
Iteration 92/1000 | Loss: 0.00006310
Iteration 93/1000 | Loss: 0.00009999
Iteration 94/1000 | Loss: 0.00005748
Iteration 95/1000 | Loss: 0.00035181
Iteration 96/1000 | Loss: 0.00038951
Iteration 97/1000 | Loss: 0.00023159
Iteration 98/1000 | Loss: 0.00022215
Iteration 99/1000 | Loss: 0.00036641
Iteration 100/1000 | Loss: 0.00019939
Iteration 101/1000 | Loss: 0.00011410
Iteration 102/1000 | Loss: 0.00008561
Iteration 103/1000 | Loss: 0.00009114
Iteration 104/1000 | Loss: 0.00005399
Iteration 105/1000 | Loss: 0.00005275
Iteration 106/1000 | Loss: 0.00011003
Iteration 107/1000 | Loss: 0.00005842
Iteration 108/1000 | Loss: 0.00008049
Iteration 109/1000 | Loss: 0.00005100
Iteration 110/1000 | Loss: 0.00006774
Iteration 111/1000 | Loss: 0.00008853
Iteration 112/1000 | Loss: 0.00019219
Iteration 113/1000 | Loss: 0.00020006
Iteration 114/1000 | Loss: 0.00008329
Iteration 115/1000 | Loss: 0.00024641
Iteration 116/1000 | Loss: 0.00014376
Iteration 117/1000 | Loss: 0.00006635
Iteration 118/1000 | Loss: 0.00006174
Iteration 119/1000 | Loss: 0.00005797
Iteration 120/1000 | Loss: 0.00007760
Iteration 121/1000 | Loss: 0.00016756
Iteration 122/1000 | Loss: 0.00016590
Iteration 123/1000 | Loss: 0.00027891
Iteration 124/1000 | Loss: 0.00012103
Iteration 125/1000 | Loss: 0.00004982
Iteration 126/1000 | Loss: 0.00004916
Iteration 127/1000 | Loss: 0.00005691
Iteration 128/1000 | Loss: 0.00006942
Iteration 129/1000 | Loss: 0.00004837
Iteration 130/1000 | Loss: 0.00009534
Iteration 131/1000 | Loss: 0.00008645
Iteration 132/1000 | Loss: 0.00006951
Iteration 133/1000 | Loss: 0.00009474
Iteration 134/1000 | Loss: 0.00006615
Iteration 135/1000 | Loss: 0.00005656
Iteration 136/1000 | Loss: 0.00017089
Iteration 137/1000 | Loss: 0.00028560
Iteration 138/1000 | Loss: 0.00009338
Iteration 139/1000 | Loss: 0.00028669
Iteration 140/1000 | Loss: 0.00011715
Iteration 141/1000 | Loss: 0.00006474
Iteration 142/1000 | Loss: 0.00005860
Iteration 143/1000 | Loss: 0.00008651
Iteration 144/1000 | Loss: 0.00006239
Iteration 145/1000 | Loss: 0.00017406
Iteration 146/1000 | Loss: 0.00006026
Iteration 147/1000 | Loss: 0.00004939
Iteration 148/1000 | Loss: 0.00006945
Iteration 149/1000 | Loss: 0.00007769
Iteration 150/1000 | Loss: 0.00011019
Iteration 151/1000 | Loss: 0.00015540
Iteration 152/1000 | Loss: 0.00007307
Iteration 153/1000 | Loss: 0.00013626
Iteration 154/1000 | Loss: 0.00005122
Iteration 155/1000 | Loss: 0.00004893
Iteration 156/1000 | Loss: 0.00004826
Iteration 157/1000 | Loss: 0.00006602
Iteration 158/1000 | Loss: 0.00004732
Iteration 159/1000 | Loss: 0.00006616
Iteration 160/1000 | Loss: 0.00006156
Iteration 161/1000 | Loss: 0.00004684
Iteration 162/1000 | Loss: 0.00004665
Iteration 163/1000 | Loss: 0.00004663
Iteration 164/1000 | Loss: 0.00004663
Iteration 165/1000 | Loss: 0.00007562
Iteration 166/1000 | Loss: 0.00005291
Iteration 167/1000 | Loss: 0.00005157
Iteration 168/1000 | Loss: 0.00005947
Iteration 169/1000 | Loss: 0.00018733
Iteration 170/1000 | Loss: 0.00007309
Iteration 171/1000 | Loss: 0.00008689
Iteration 172/1000 | Loss: 0.00004709
Iteration 173/1000 | Loss: 0.00004667
Iteration 174/1000 | Loss: 0.00005168
Iteration 175/1000 | Loss: 0.00006231
Iteration 176/1000 | Loss: 0.00011315
Iteration 177/1000 | Loss: 0.00005830
Iteration 178/1000 | Loss: 0.00006894
Iteration 179/1000 | Loss: 0.00007932
Iteration 180/1000 | Loss: 0.00007379
Iteration 181/1000 | Loss: 0.00004968
Iteration 182/1000 | Loss: 0.00004646
Iteration 183/1000 | Loss: 0.00004645
Iteration 184/1000 | Loss: 0.00004644
Iteration 185/1000 | Loss: 0.00004644
Iteration 186/1000 | Loss: 0.00004644
Iteration 187/1000 | Loss: 0.00004644
Iteration 188/1000 | Loss: 0.00004644
Iteration 189/1000 | Loss: 0.00004644
Iteration 190/1000 | Loss: 0.00004644
Iteration 191/1000 | Loss: 0.00004644
Iteration 192/1000 | Loss: 0.00004644
Iteration 193/1000 | Loss: 0.00004644
Iteration 194/1000 | Loss: 0.00006158
Iteration 195/1000 | Loss: 0.00005024
Iteration 196/1000 | Loss: 0.00005523
Iteration 197/1000 | Loss: 0.00004720
Iteration 198/1000 | Loss: 0.00004646
Iteration 199/1000 | Loss: 0.00004642
Iteration 200/1000 | Loss: 0.00004636
Iteration 201/1000 | Loss: 0.00004641
Iteration 202/1000 | Loss: 0.00004634
Iteration 203/1000 | Loss: 0.00004634
Iteration 204/1000 | Loss: 0.00004634
Iteration 205/1000 | Loss: 0.00004634
Iteration 206/1000 | Loss: 0.00004634
Iteration 207/1000 | Loss: 0.00004634
Iteration 208/1000 | Loss: 0.00004634
Iteration 209/1000 | Loss: 0.00004634
Iteration 210/1000 | Loss: 0.00004634
Iteration 211/1000 | Loss: 0.00004634
Iteration 212/1000 | Loss: 0.00004634
Iteration 213/1000 | Loss: 0.00004633
Iteration 214/1000 | Loss: 0.00004633
Iteration 215/1000 | Loss: 0.00004633
Iteration 216/1000 | Loss: 0.00004633
Iteration 217/1000 | Loss: 0.00004632
Iteration 218/1000 | Loss: 0.00004632
Iteration 219/1000 | Loss: 0.00004631
Iteration 220/1000 | Loss: 0.00004631
Iteration 221/1000 | Loss: 0.00004631
Iteration 222/1000 | Loss: 0.00004631
Iteration 223/1000 | Loss: 0.00004631
Iteration 224/1000 | Loss: 0.00004631
Iteration 225/1000 | Loss: 0.00004631
Iteration 226/1000 | Loss: 0.00004631
Iteration 227/1000 | Loss: 0.00004631
Iteration 228/1000 | Loss: 0.00004631
Iteration 229/1000 | Loss: 0.00004631
Iteration 230/1000 | Loss: 0.00004631
Iteration 231/1000 | Loss: 0.00004631
Iteration 232/1000 | Loss: 0.00004631
Iteration 233/1000 | Loss: 0.00004631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [4.6305329306051135e-05, 4.6305329306051135e-05, 4.6305329306051135e-05, 4.6305329306051135e-05, 4.6305329306051135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6305329306051135e-05

Optimization complete. Final v2v error: 3.715247631072998 mm

Highest mean error: 20.07160186767578 mm for frame 88

Lowest mean error: 2.3568050861358643 mm for frame 202

Saving results

Total time: 319.5452046394348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033303
Iteration 2/25 | Loss: 0.00190111
Iteration 3/25 | Loss: 0.00133131
Iteration 4/25 | Loss: 0.00123082
Iteration 5/25 | Loss: 0.00125967
Iteration 6/25 | Loss: 0.00123027
Iteration 7/25 | Loss: 0.00117697
Iteration 8/25 | Loss: 0.00114975
Iteration 9/25 | Loss: 0.00113207
Iteration 10/25 | Loss: 0.00112275
Iteration 11/25 | Loss: 0.00112072
Iteration 12/25 | Loss: 0.00111748
Iteration 13/25 | Loss: 0.00111846
Iteration 14/25 | Loss: 0.00111735
Iteration 15/25 | Loss: 0.00111887
Iteration 16/25 | Loss: 0.00111762
Iteration 17/25 | Loss: 0.00111879
Iteration 18/25 | Loss: 0.00111799
Iteration 19/25 | Loss: 0.00111669
Iteration 20/25 | Loss: 0.00111658
Iteration 21/25 | Loss: 0.00111655
Iteration 22/25 | Loss: 0.00111652
Iteration 23/25 | Loss: 0.00111651
Iteration 24/25 | Loss: 0.00111651
Iteration 25/25 | Loss: 0.00111651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63194358
Iteration 2/25 | Loss: 0.00193468
Iteration 3/25 | Loss: 0.00190191
Iteration 4/25 | Loss: 0.00190191
Iteration 5/25 | Loss: 0.00190191
Iteration 6/25 | Loss: 0.00190191
Iteration 7/25 | Loss: 0.00190191
Iteration 8/25 | Loss: 0.00190191
Iteration 9/25 | Loss: 0.00190190
Iteration 10/25 | Loss: 0.00190191
Iteration 11/25 | Loss: 0.00190191
Iteration 12/25 | Loss: 0.00190190
Iteration 13/25 | Loss: 0.00190190
Iteration 14/25 | Loss: 0.00190190
Iteration 15/25 | Loss: 0.00190190
Iteration 16/25 | Loss: 0.00190190
Iteration 17/25 | Loss: 0.00190190
Iteration 18/25 | Loss: 0.00190190
Iteration 19/25 | Loss: 0.00190190
Iteration 20/25 | Loss: 0.00190190
Iteration 21/25 | Loss: 0.00190190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001901904703117907, 0.001901904703117907, 0.001901904703117907, 0.001901904703117907, 0.001901904703117907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001901904703117907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190190
Iteration 2/1000 | Loss: 0.00006461
Iteration 3/1000 | Loss: 0.00006081
Iteration 4/1000 | Loss: 0.00004826
Iteration 5/1000 | Loss: 0.00004267
Iteration 6/1000 | Loss: 0.00015051
Iteration 7/1000 | Loss: 0.00004023
Iteration 8/1000 | Loss: 0.00008289
Iteration 9/1000 | Loss: 0.00015962
Iteration 10/1000 | Loss: 0.00162216
Iteration 11/1000 | Loss: 0.00081102
Iteration 12/1000 | Loss: 0.00010276
Iteration 13/1000 | Loss: 0.00003885
Iteration 14/1000 | Loss: 0.00015000
Iteration 15/1000 | Loss: 0.00116876
Iteration 16/1000 | Loss: 0.00062105
Iteration 17/1000 | Loss: 0.00022124
Iteration 18/1000 | Loss: 0.00010861
Iteration 19/1000 | Loss: 0.00007050
Iteration 20/1000 | Loss: 0.00004654
Iteration 21/1000 | Loss: 0.00048826
Iteration 22/1000 | Loss: 0.00051344
Iteration 23/1000 | Loss: 0.00046417
Iteration 24/1000 | Loss: 0.00035255
Iteration 25/1000 | Loss: 0.00004331
Iteration 26/1000 | Loss: 0.00036180
Iteration 27/1000 | Loss: 0.00087009
Iteration 28/1000 | Loss: 0.00014547
Iteration 29/1000 | Loss: 0.00076090
Iteration 30/1000 | Loss: 0.00018962
Iteration 31/1000 | Loss: 0.00011075
Iteration 32/1000 | Loss: 0.00004487
Iteration 33/1000 | Loss: 0.00004345
Iteration 34/1000 | Loss: 0.00003345
Iteration 35/1000 | Loss: 0.00016371
Iteration 36/1000 | Loss: 0.00008886
Iteration 37/1000 | Loss: 0.00004910
Iteration 38/1000 | Loss: 0.00004926
Iteration 39/1000 | Loss: 0.00003032
Iteration 40/1000 | Loss: 0.00031545
Iteration 41/1000 | Loss: 0.00072143
Iteration 42/1000 | Loss: 0.00007751
Iteration 43/1000 | Loss: 0.00036792
Iteration 44/1000 | Loss: 0.00021909
Iteration 45/1000 | Loss: 0.00014645
Iteration 46/1000 | Loss: 0.00004833
Iteration 47/1000 | Loss: 0.00038704
Iteration 48/1000 | Loss: 0.00008849
Iteration 49/1000 | Loss: 0.00028103
Iteration 50/1000 | Loss: 0.00005547
Iteration 51/1000 | Loss: 0.00015857
Iteration 52/1000 | Loss: 0.00003476
Iteration 53/1000 | Loss: 0.00003078
Iteration 54/1000 | Loss: 0.00032619
Iteration 55/1000 | Loss: 0.00006551
Iteration 56/1000 | Loss: 0.00005279
Iteration 57/1000 | Loss: 0.00024255
Iteration 58/1000 | Loss: 0.00008152
Iteration 59/1000 | Loss: 0.00060738
Iteration 60/1000 | Loss: 0.00003109
Iteration 61/1000 | Loss: 0.00021171
Iteration 62/1000 | Loss: 0.00006516
Iteration 63/1000 | Loss: 0.00013011
Iteration 64/1000 | Loss: 0.00020167
Iteration 65/1000 | Loss: 0.00005650
Iteration 66/1000 | Loss: 0.00004962
Iteration 67/1000 | Loss: 0.00018836
Iteration 68/1000 | Loss: 0.00007314
Iteration 69/1000 | Loss: 0.00003018
Iteration 70/1000 | Loss: 0.00019665
Iteration 71/1000 | Loss: 0.00109068
Iteration 72/1000 | Loss: 0.00010162
Iteration 73/1000 | Loss: 0.00003177
Iteration 74/1000 | Loss: 0.00002794
Iteration 75/1000 | Loss: 0.00002564
Iteration 76/1000 | Loss: 0.00004825
Iteration 77/1000 | Loss: 0.00003127
Iteration 78/1000 | Loss: 0.00002565
Iteration 79/1000 | Loss: 0.00002503
Iteration 80/1000 | Loss: 0.00002500
Iteration 81/1000 | Loss: 0.00002479
Iteration 82/1000 | Loss: 0.00002450
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002483
Iteration 85/1000 | Loss: 0.00002470
Iteration 86/1000 | Loss: 0.00002470
Iteration 87/1000 | Loss: 0.00002437
Iteration 88/1000 | Loss: 0.00002651
Iteration 89/1000 | Loss: 0.00002433
Iteration 90/1000 | Loss: 0.00002496
Iteration 91/1000 | Loss: 0.00002808
Iteration 92/1000 | Loss: 0.00002531
Iteration 93/1000 | Loss: 0.00002702
Iteration 94/1000 | Loss: 0.00002567
Iteration 95/1000 | Loss: 0.00002870
Iteration 96/1000 | Loss: 0.00002560
Iteration 97/1000 | Loss: 0.00002961
Iteration 98/1000 | Loss: 0.00002619
Iteration 99/1000 | Loss: 0.00002896
Iteration 100/1000 | Loss: 0.00002562
Iteration 101/1000 | Loss: 0.00002787
Iteration 102/1000 | Loss: 0.00002732
Iteration 103/1000 | Loss: 0.00002757
Iteration 104/1000 | Loss: 0.00002593
Iteration 105/1000 | Loss: 0.00002782
Iteration 106/1000 | Loss: 0.00003004
Iteration 107/1000 | Loss: 0.00002714
Iteration 108/1000 | Loss: 0.00002387
Iteration 109/1000 | Loss: 0.00002386
Iteration 110/1000 | Loss: 0.00002384
Iteration 111/1000 | Loss: 0.00002384
Iteration 112/1000 | Loss: 0.00002383
Iteration 113/1000 | Loss: 0.00002382
Iteration 114/1000 | Loss: 0.00002382
Iteration 115/1000 | Loss: 0.00002381
Iteration 116/1000 | Loss: 0.00002381
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002381
Iteration 119/1000 | Loss: 0.00002381
Iteration 120/1000 | Loss: 0.00002381
Iteration 121/1000 | Loss: 0.00002381
Iteration 122/1000 | Loss: 0.00002381
Iteration 123/1000 | Loss: 0.00002380
Iteration 124/1000 | Loss: 0.00002380
Iteration 125/1000 | Loss: 0.00002380
Iteration 126/1000 | Loss: 0.00002380
Iteration 127/1000 | Loss: 0.00002380
Iteration 128/1000 | Loss: 0.00002380
Iteration 129/1000 | Loss: 0.00002379
Iteration 130/1000 | Loss: 0.00002379
Iteration 131/1000 | Loss: 0.00002379
Iteration 132/1000 | Loss: 0.00002379
Iteration 133/1000 | Loss: 0.00002379
Iteration 134/1000 | Loss: 0.00002379
Iteration 135/1000 | Loss: 0.00002378
Iteration 136/1000 | Loss: 0.00002378
Iteration 137/1000 | Loss: 0.00002378
Iteration 138/1000 | Loss: 0.00002377
Iteration 139/1000 | Loss: 0.00002377
Iteration 140/1000 | Loss: 0.00002376
Iteration 141/1000 | Loss: 0.00002376
Iteration 142/1000 | Loss: 0.00002376
Iteration 143/1000 | Loss: 0.00002375
Iteration 144/1000 | Loss: 0.00002375
Iteration 145/1000 | Loss: 0.00002375
Iteration 146/1000 | Loss: 0.00002375
Iteration 147/1000 | Loss: 0.00002374
Iteration 148/1000 | Loss: 0.00002374
Iteration 149/1000 | Loss: 0.00002373
Iteration 150/1000 | Loss: 0.00002373
Iteration 151/1000 | Loss: 0.00002372
Iteration 152/1000 | Loss: 0.00002372
Iteration 153/1000 | Loss: 0.00002372
Iteration 154/1000 | Loss: 0.00002372
Iteration 155/1000 | Loss: 0.00002372
Iteration 156/1000 | Loss: 0.00002371
Iteration 157/1000 | Loss: 0.00002371
Iteration 158/1000 | Loss: 0.00002371
Iteration 159/1000 | Loss: 0.00002371
Iteration 160/1000 | Loss: 0.00002371
Iteration 161/1000 | Loss: 0.00002370
Iteration 162/1000 | Loss: 0.00002370
Iteration 163/1000 | Loss: 0.00002370
Iteration 164/1000 | Loss: 0.00002370
Iteration 165/1000 | Loss: 0.00002370
Iteration 166/1000 | Loss: 0.00002370
Iteration 167/1000 | Loss: 0.00002370
Iteration 168/1000 | Loss: 0.00002369
Iteration 169/1000 | Loss: 0.00002369
Iteration 170/1000 | Loss: 0.00002369
Iteration 171/1000 | Loss: 0.00002369
Iteration 172/1000 | Loss: 0.00002369
Iteration 173/1000 | Loss: 0.00002369
Iteration 174/1000 | Loss: 0.00002369
Iteration 175/1000 | Loss: 0.00002369
Iteration 176/1000 | Loss: 0.00002369
Iteration 177/1000 | Loss: 0.00002369
Iteration 178/1000 | Loss: 0.00002369
Iteration 179/1000 | Loss: 0.00002369
Iteration 180/1000 | Loss: 0.00002369
Iteration 181/1000 | Loss: 0.00002369
Iteration 182/1000 | Loss: 0.00002369
Iteration 183/1000 | Loss: 0.00002369
Iteration 184/1000 | Loss: 0.00002369
Iteration 185/1000 | Loss: 0.00002369
Iteration 186/1000 | Loss: 0.00002369
Iteration 187/1000 | Loss: 0.00002369
Iteration 188/1000 | Loss: 0.00002369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.3686912754783407e-05, 2.3686912754783407e-05, 2.3686912754783407e-05, 2.3686912754783407e-05, 2.3686912754783407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3686912754783407e-05

Optimization complete. Final v2v error: 3.8246490955352783 mm

Highest mean error: 10.035738945007324 mm for frame 111

Lowest mean error: 2.924344062805176 mm for frame 8

Saving results

Total time: 184.71514320373535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847603
Iteration 2/25 | Loss: 0.00136036
Iteration 3/25 | Loss: 0.00106058
Iteration 4/25 | Loss: 0.00102720
Iteration 5/25 | Loss: 0.00102476
Iteration 6/25 | Loss: 0.00102474
Iteration 7/25 | Loss: 0.00102474
Iteration 8/25 | Loss: 0.00102474
Iteration 9/25 | Loss: 0.00102474
Iteration 10/25 | Loss: 0.00102474
Iteration 11/25 | Loss: 0.00102474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001024738885462284, 0.001024738885462284, 0.001024738885462284, 0.001024738885462284, 0.001024738885462284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001024738885462284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85315233
Iteration 2/25 | Loss: 0.00094900
Iteration 3/25 | Loss: 0.00094900
Iteration 4/25 | Loss: 0.00094900
Iteration 5/25 | Loss: 0.00094900
Iteration 6/25 | Loss: 0.00094900
Iteration 7/25 | Loss: 0.00094900
Iteration 8/25 | Loss: 0.00094900
Iteration 9/25 | Loss: 0.00094900
Iteration 10/25 | Loss: 0.00094900
Iteration 11/25 | Loss: 0.00094900
Iteration 12/25 | Loss: 0.00094900
Iteration 13/25 | Loss: 0.00094900
Iteration 14/25 | Loss: 0.00094900
Iteration 15/25 | Loss: 0.00094900
Iteration 16/25 | Loss: 0.00094900
Iteration 17/25 | Loss: 0.00094900
Iteration 18/25 | Loss: 0.00094900
Iteration 19/25 | Loss: 0.00094900
Iteration 20/25 | Loss: 0.00094900
Iteration 21/25 | Loss: 0.00094900
Iteration 22/25 | Loss: 0.00094900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009489978547208011, 0.0009489978547208011, 0.0009489978547208011, 0.0009489978547208011, 0.0009489978547208011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009489978547208011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094900
Iteration 2/1000 | Loss: 0.00002874
Iteration 3/1000 | Loss: 0.00002175
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001818
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001743
Iteration 9/1000 | Loss: 0.00001739
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001730
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001711
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001690
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001688
Iteration 76/1000 | Loss: 0.00001688
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001688
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001687
Iteration 84/1000 | Loss: 0.00001687
Iteration 85/1000 | Loss: 0.00001687
Iteration 86/1000 | Loss: 0.00001687
Iteration 87/1000 | Loss: 0.00001687
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001686
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00001686
Iteration 96/1000 | Loss: 0.00001686
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001684
Iteration 101/1000 | Loss: 0.00001684
Iteration 102/1000 | Loss: 0.00001684
Iteration 103/1000 | Loss: 0.00001684
Iteration 104/1000 | Loss: 0.00001684
Iteration 105/1000 | Loss: 0.00001684
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001684
Iteration 108/1000 | Loss: 0.00001684
Iteration 109/1000 | Loss: 0.00001684
Iteration 110/1000 | Loss: 0.00001684
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001683
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001683
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001683
Iteration 124/1000 | Loss: 0.00001683
Iteration 125/1000 | Loss: 0.00001683
Iteration 126/1000 | Loss: 0.00001683
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001683
Iteration 129/1000 | Loss: 0.00001683
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00001683
Iteration 132/1000 | Loss: 0.00001683
Iteration 133/1000 | Loss: 0.00001683
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001683
Iteration 137/1000 | Loss: 0.00001683
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001683
Iteration 140/1000 | Loss: 0.00001683
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001683
Iteration 147/1000 | Loss: 0.00001683
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001683
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Iteration 169/1000 | Loss: 0.00001683
Iteration 170/1000 | Loss: 0.00001683
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.6826688806759194e-05, 1.6826688806759194e-05, 1.6826688806759194e-05, 1.6826688806759194e-05, 1.6826688806759194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6826688806759194e-05

Optimization complete. Final v2v error: 3.4379074573516846 mm

Highest mean error: 3.530726671218872 mm for frame 121

Lowest mean error: 3.3672399520874023 mm for frame 149

Saving results

Total time: 31.703093767166138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842885
Iteration 2/25 | Loss: 0.00131982
Iteration 3/25 | Loss: 0.00103756
Iteration 4/25 | Loss: 0.00100700
Iteration 5/25 | Loss: 0.00100377
Iteration 6/25 | Loss: 0.00100349
Iteration 7/25 | Loss: 0.00100349
Iteration 8/25 | Loss: 0.00100349
Iteration 9/25 | Loss: 0.00100349
Iteration 10/25 | Loss: 0.00100349
Iteration 11/25 | Loss: 0.00100349
Iteration 12/25 | Loss: 0.00100349
Iteration 13/25 | Loss: 0.00100349
Iteration 14/25 | Loss: 0.00100349
Iteration 15/25 | Loss: 0.00100349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010034881997853518, 0.0010034881997853518, 0.0010034881997853518, 0.0010034881997853518, 0.0010034881997853518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010034881997853518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17651653
Iteration 2/25 | Loss: 0.00189266
Iteration 3/25 | Loss: 0.00189265
Iteration 4/25 | Loss: 0.00189265
Iteration 5/25 | Loss: 0.00189265
Iteration 6/25 | Loss: 0.00189265
Iteration 7/25 | Loss: 0.00189265
Iteration 8/25 | Loss: 0.00189265
Iteration 9/25 | Loss: 0.00189265
Iteration 10/25 | Loss: 0.00189265
Iteration 11/25 | Loss: 0.00189265
Iteration 12/25 | Loss: 0.00189265
Iteration 13/25 | Loss: 0.00189265
Iteration 14/25 | Loss: 0.00189265
Iteration 15/25 | Loss: 0.00189265
Iteration 16/25 | Loss: 0.00189265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018926496850326657, 0.0018926496850326657, 0.0018926496850326657, 0.0018926496850326657, 0.0018926496850326657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018926496850326657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189265
Iteration 2/1000 | Loss: 0.00002409
Iteration 3/1000 | Loss: 0.00001579
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001292
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001182
Iteration 8/1000 | Loss: 0.00001158
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001130
Iteration 11/1000 | Loss: 0.00001110
Iteration 12/1000 | Loss: 0.00001104
Iteration 13/1000 | Loss: 0.00001080
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001061
Iteration 16/1000 | Loss: 0.00001051
Iteration 17/1000 | Loss: 0.00001050
Iteration 18/1000 | Loss: 0.00001050
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001049
Iteration 23/1000 | Loss: 0.00001049
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001048
Iteration 26/1000 | Loss: 0.00001048
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001044
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001043
Iteration 44/1000 | Loss: 0.00001043
Iteration 45/1000 | Loss: 0.00001042
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001039
Iteration 70/1000 | Loss: 0.00001039
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001035
Iteration 81/1000 | Loss: 0.00001035
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001034
Iteration 84/1000 | Loss: 0.00001033
Iteration 85/1000 | Loss: 0.00001033
Iteration 86/1000 | Loss: 0.00001033
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001032
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001032
Iteration 92/1000 | Loss: 0.00001032
Iteration 93/1000 | Loss: 0.00001032
Iteration 94/1000 | Loss: 0.00001032
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001028
Iteration 102/1000 | Loss: 0.00001028
Iteration 103/1000 | Loss: 0.00001028
Iteration 104/1000 | Loss: 0.00001028
Iteration 105/1000 | Loss: 0.00001028
Iteration 106/1000 | Loss: 0.00001028
Iteration 107/1000 | Loss: 0.00001028
Iteration 108/1000 | Loss: 0.00001028
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001027
Iteration 117/1000 | Loss: 0.00001027
Iteration 118/1000 | Loss: 0.00001027
Iteration 119/1000 | Loss: 0.00001027
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001026
Iteration 126/1000 | Loss: 0.00001026
Iteration 127/1000 | Loss: 0.00001026
Iteration 128/1000 | Loss: 0.00001026
Iteration 129/1000 | Loss: 0.00001025
Iteration 130/1000 | Loss: 0.00001025
Iteration 131/1000 | Loss: 0.00001025
Iteration 132/1000 | Loss: 0.00001025
Iteration 133/1000 | Loss: 0.00001024
Iteration 134/1000 | Loss: 0.00001024
Iteration 135/1000 | Loss: 0.00001024
Iteration 136/1000 | Loss: 0.00001024
Iteration 137/1000 | Loss: 0.00001024
Iteration 138/1000 | Loss: 0.00001024
Iteration 139/1000 | Loss: 0.00001024
Iteration 140/1000 | Loss: 0.00001024
Iteration 141/1000 | Loss: 0.00001024
Iteration 142/1000 | Loss: 0.00001024
Iteration 143/1000 | Loss: 0.00001024
Iteration 144/1000 | Loss: 0.00001024
Iteration 145/1000 | Loss: 0.00001024
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001024
Iteration 151/1000 | Loss: 0.00001023
Iteration 152/1000 | Loss: 0.00001023
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001023
Iteration 159/1000 | Loss: 0.00001023
Iteration 160/1000 | Loss: 0.00001023
Iteration 161/1000 | Loss: 0.00001023
Iteration 162/1000 | Loss: 0.00001023
Iteration 163/1000 | Loss: 0.00001023
Iteration 164/1000 | Loss: 0.00001023
Iteration 165/1000 | Loss: 0.00001023
Iteration 166/1000 | Loss: 0.00001023
Iteration 167/1000 | Loss: 0.00001023
Iteration 168/1000 | Loss: 0.00001023
Iteration 169/1000 | Loss: 0.00001023
Iteration 170/1000 | Loss: 0.00001023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.0228938663203735e-05, 1.0228938663203735e-05, 1.0228938663203735e-05, 1.0228938663203735e-05, 1.0228938663203735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0228938663203735e-05

Optimization complete. Final v2v error: 2.650387763977051 mm

Highest mean error: 2.779963493347168 mm for frame 14

Lowest mean error: 2.5719144344329834 mm for frame 104

Saving results

Total time: 36.097304344177246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384093
Iteration 2/25 | Loss: 0.00108273
Iteration 3/25 | Loss: 0.00096289
Iteration 4/25 | Loss: 0.00095362
Iteration 5/25 | Loss: 0.00095219
Iteration 6/25 | Loss: 0.00095202
Iteration 7/25 | Loss: 0.00095202
Iteration 8/25 | Loss: 0.00095202
Iteration 9/25 | Loss: 0.00095202
Iteration 10/25 | Loss: 0.00095202
Iteration 11/25 | Loss: 0.00095202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009520196472294629, 0.0009520196472294629, 0.0009520196472294629, 0.0009520196472294629, 0.0009520196472294629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009520196472294629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19816113
Iteration 2/25 | Loss: 0.00185367
Iteration 3/25 | Loss: 0.00185366
Iteration 4/25 | Loss: 0.00185366
Iteration 5/25 | Loss: 0.00185366
Iteration 6/25 | Loss: 0.00185366
Iteration 7/25 | Loss: 0.00185366
Iteration 8/25 | Loss: 0.00185366
Iteration 9/25 | Loss: 0.00185366
Iteration 10/25 | Loss: 0.00185366
Iteration 11/25 | Loss: 0.00185366
Iteration 12/25 | Loss: 0.00185366
Iteration 13/25 | Loss: 0.00185366
Iteration 14/25 | Loss: 0.00185366
Iteration 15/25 | Loss: 0.00185366
Iteration 16/25 | Loss: 0.00185366
Iteration 17/25 | Loss: 0.00185366
Iteration 18/25 | Loss: 0.00185366
Iteration 19/25 | Loss: 0.00185366
Iteration 20/25 | Loss: 0.00185366
Iteration 21/25 | Loss: 0.00185366
Iteration 22/25 | Loss: 0.00185366
Iteration 23/25 | Loss: 0.00185366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018536611460149288, 0.0018536611460149288, 0.0018536611460149288, 0.0018536611460149288, 0.0018536611460149288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018536611460149288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185366
Iteration 2/1000 | Loss: 0.00001828
Iteration 3/1000 | Loss: 0.00001163
Iteration 4/1000 | Loss: 0.00001049
Iteration 5/1000 | Loss: 0.00000950
Iteration 6/1000 | Loss: 0.00000902
Iteration 7/1000 | Loss: 0.00000854
Iteration 8/1000 | Loss: 0.00000824
Iteration 9/1000 | Loss: 0.00000822
Iteration 10/1000 | Loss: 0.00000801
Iteration 11/1000 | Loss: 0.00000799
Iteration 12/1000 | Loss: 0.00000793
Iteration 13/1000 | Loss: 0.00000793
Iteration 14/1000 | Loss: 0.00000792
Iteration 15/1000 | Loss: 0.00000792
Iteration 16/1000 | Loss: 0.00000791
Iteration 17/1000 | Loss: 0.00000790
Iteration 18/1000 | Loss: 0.00000789
Iteration 19/1000 | Loss: 0.00000788
Iteration 20/1000 | Loss: 0.00000787
Iteration 21/1000 | Loss: 0.00000787
Iteration 22/1000 | Loss: 0.00000786
Iteration 23/1000 | Loss: 0.00000783
Iteration 24/1000 | Loss: 0.00000782
Iteration 25/1000 | Loss: 0.00000782
Iteration 26/1000 | Loss: 0.00000781
Iteration 27/1000 | Loss: 0.00000781
Iteration 28/1000 | Loss: 0.00000780
Iteration 29/1000 | Loss: 0.00000776
Iteration 30/1000 | Loss: 0.00000769
Iteration 31/1000 | Loss: 0.00000769
Iteration 32/1000 | Loss: 0.00000768
Iteration 33/1000 | Loss: 0.00000764
Iteration 34/1000 | Loss: 0.00000764
Iteration 35/1000 | Loss: 0.00000763
Iteration 36/1000 | Loss: 0.00000763
Iteration 37/1000 | Loss: 0.00000762
Iteration 38/1000 | Loss: 0.00000761
Iteration 39/1000 | Loss: 0.00000761
Iteration 40/1000 | Loss: 0.00000760
Iteration 41/1000 | Loss: 0.00000759
Iteration 42/1000 | Loss: 0.00000759
Iteration 43/1000 | Loss: 0.00000758
Iteration 44/1000 | Loss: 0.00000758
Iteration 45/1000 | Loss: 0.00000758
Iteration 46/1000 | Loss: 0.00000758
Iteration 47/1000 | Loss: 0.00000758
Iteration 48/1000 | Loss: 0.00000757
Iteration 49/1000 | Loss: 0.00000757
Iteration 50/1000 | Loss: 0.00000757
Iteration 51/1000 | Loss: 0.00000756
Iteration 52/1000 | Loss: 0.00000756
Iteration 53/1000 | Loss: 0.00000755
Iteration 54/1000 | Loss: 0.00000755
Iteration 55/1000 | Loss: 0.00000755
Iteration 56/1000 | Loss: 0.00000755
Iteration 57/1000 | Loss: 0.00000755
Iteration 58/1000 | Loss: 0.00000755
Iteration 59/1000 | Loss: 0.00000755
Iteration 60/1000 | Loss: 0.00000754
Iteration 61/1000 | Loss: 0.00000754
Iteration 62/1000 | Loss: 0.00000754
Iteration 63/1000 | Loss: 0.00000754
Iteration 64/1000 | Loss: 0.00000754
Iteration 65/1000 | Loss: 0.00000754
Iteration 66/1000 | Loss: 0.00000753
Iteration 67/1000 | Loss: 0.00000753
Iteration 68/1000 | Loss: 0.00000752
Iteration 69/1000 | Loss: 0.00000752
Iteration 70/1000 | Loss: 0.00000752
Iteration 71/1000 | Loss: 0.00000752
Iteration 72/1000 | Loss: 0.00000752
Iteration 73/1000 | Loss: 0.00000751
Iteration 74/1000 | Loss: 0.00000751
Iteration 75/1000 | Loss: 0.00000751
Iteration 76/1000 | Loss: 0.00000750
Iteration 77/1000 | Loss: 0.00000750
Iteration 78/1000 | Loss: 0.00000750
Iteration 79/1000 | Loss: 0.00000750
Iteration 80/1000 | Loss: 0.00000750
Iteration 81/1000 | Loss: 0.00000749
Iteration 82/1000 | Loss: 0.00000749
Iteration 83/1000 | Loss: 0.00000749
Iteration 84/1000 | Loss: 0.00000748
Iteration 85/1000 | Loss: 0.00000748
Iteration 86/1000 | Loss: 0.00000748
Iteration 87/1000 | Loss: 0.00000748
Iteration 88/1000 | Loss: 0.00000748
Iteration 89/1000 | Loss: 0.00000747
Iteration 90/1000 | Loss: 0.00000747
Iteration 91/1000 | Loss: 0.00000747
Iteration 92/1000 | Loss: 0.00000747
Iteration 93/1000 | Loss: 0.00000746
Iteration 94/1000 | Loss: 0.00000746
Iteration 95/1000 | Loss: 0.00000746
Iteration 96/1000 | Loss: 0.00000746
Iteration 97/1000 | Loss: 0.00000746
Iteration 98/1000 | Loss: 0.00000746
Iteration 99/1000 | Loss: 0.00000746
Iteration 100/1000 | Loss: 0.00000745
Iteration 101/1000 | Loss: 0.00000745
Iteration 102/1000 | Loss: 0.00000745
Iteration 103/1000 | Loss: 0.00000745
Iteration 104/1000 | Loss: 0.00000745
Iteration 105/1000 | Loss: 0.00000745
Iteration 106/1000 | Loss: 0.00000745
Iteration 107/1000 | Loss: 0.00000745
Iteration 108/1000 | Loss: 0.00000745
Iteration 109/1000 | Loss: 0.00000745
Iteration 110/1000 | Loss: 0.00000745
Iteration 111/1000 | Loss: 0.00000745
Iteration 112/1000 | Loss: 0.00000745
Iteration 113/1000 | Loss: 0.00000744
Iteration 114/1000 | Loss: 0.00000744
Iteration 115/1000 | Loss: 0.00000744
Iteration 116/1000 | Loss: 0.00000744
Iteration 117/1000 | Loss: 0.00000744
Iteration 118/1000 | Loss: 0.00000744
Iteration 119/1000 | Loss: 0.00000744
Iteration 120/1000 | Loss: 0.00000743
Iteration 121/1000 | Loss: 0.00000743
Iteration 122/1000 | Loss: 0.00000743
Iteration 123/1000 | Loss: 0.00000743
Iteration 124/1000 | Loss: 0.00000743
Iteration 125/1000 | Loss: 0.00000742
Iteration 126/1000 | Loss: 0.00000742
Iteration 127/1000 | Loss: 0.00000742
Iteration 128/1000 | Loss: 0.00000742
Iteration 129/1000 | Loss: 0.00000742
Iteration 130/1000 | Loss: 0.00000741
Iteration 131/1000 | Loss: 0.00000741
Iteration 132/1000 | Loss: 0.00000741
Iteration 133/1000 | Loss: 0.00000741
Iteration 134/1000 | Loss: 0.00000741
Iteration 135/1000 | Loss: 0.00000741
Iteration 136/1000 | Loss: 0.00000741
Iteration 137/1000 | Loss: 0.00000741
Iteration 138/1000 | Loss: 0.00000741
Iteration 139/1000 | Loss: 0.00000741
Iteration 140/1000 | Loss: 0.00000741
Iteration 141/1000 | Loss: 0.00000740
Iteration 142/1000 | Loss: 0.00000739
Iteration 143/1000 | Loss: 0.00000739
Iteration 144/1000 | Loss: 0.00000739
Iteration 145/1000 | Loss: 0.00000739
Iteration 146/1000 | Loss: 0.00000738
Iteration 147/1000 | Loss: 0.00000738
Iteration 148/1000 | Loss: 0.00000738
Iteration 149/1000 | Loss: 0.00000737
Iteration 150/1000 | Loss: 0.00000737
Iteration 151/1000 | Loss: 0.00000736
Iteration 152/1000 | Loss: 0.00000736
Iteration 153/1000 | Loss: 0.00000736
Iteration 154/1000 | Loss: 0.00000736
Iteration 155/1000 | Loss: 0.00000735
Iteration 156/1000 | Loss: 0.00000735
Iteration 157/1000 | Loss: 0.00000735
Iteration 158/1000 | Loss: 0.00000735
Iteration 159/1000 | Loss: 0.00000735
Iteration 160/1000 | Loss: 0.00000734
Iteration 161/1000 | Loss: 0.00000734
Iteration 162/1000 | Loss: 0.00000734
Iteration 163/1000 | Loss: 0.00000734
Iteration 164/1000 | Loss: 0.00000734
Iteration 165/1000 | Loss: 0.00000733
Iteration 166/1000 | Loss: 0.00000733
Iteration 167/1000 | Loss: 0.00000733
Iteration 168/1000 | Loss: 0.00000733
Iteration 169/1000 | Loss: 0.00000733
Iteration 170/1000 | Loss: 0.00000733
Iteration 171/1000 | Loss: 0.00000733
Iteration 172/1000 | Loss: 0.00000733
Iteration 173/1000 | Loss: 0.00000733
Iteration 174/1000 | Loss: 0.00000733
Iteration 175/1000 | Loss: 0.00000733
Iteration 176/1000 | Loss: 0.00000733
Iteration 177/1000 | Loss: 0.00000733
Iteration 178/1000 | Loss: 0.00000733
Iteration 179/1000 | Loss: 0.00000733
Iteration 180/1000 | Loss: 0.00000733
Iteration 181/1000 | Loss: 0.00000733
Iteration 182/1000 | Loss: 0.00000733
Iteration 183/1000 | Loss: 0.00000733
Iteration 184/1000 | Loss: 0.00000733
Iteration 185/1000 | Loss: 0.00000733
Iteration 186/1000 | Loss: 0.00000733
Iteration 187/1000 | Loss: 0.00000733
Iteration 188/1000 | Loss: 0.00000733
Iteration 189/1000 | Loss: 0.00000733
Iteration 190/1000 | Loss: 0.00000733
Iteration 191/1000 | Loss: 0.00000733
Iteration 192/1000 | Loss: 0.00000733
Iteration 193/1000 | Loss: 0.00000733
Iteration 194/1000 | Loss: 0.00000733
Iteration 195/1000 | Loss: 0.00000733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [7.3310807238158304e-06, 7.3310807238158304e-06, 7.3310807238158304e-06, 7.3310807238158304e-06, 7.3310807238158304e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.3310807238158304e-06

Optimization complete. Final v2v error: 2.2899527549743652 mm

Highest mean error: 2.718130111694336 mm for frame 76

Lowest mean error: 2.0846810340881348 mm for frame 31

Saving results

Total time: 36.99271893501282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936248
Iteration 2/25 | Loss: 0.00132579
Iteration 3/25 | Loss: 0.00115545
Iteration 4/25 | Loss: 0.00112709
Iteration 5/25 | Loss: 0.00112421
Iteration 6/25 | Loss: 0.00110081
Iteration 7/25 | Loss: 0.00108244
Iteration 8/25 | Loss: 0.00107999
Iteration 9/25 | Loss: 0.00107944
Iteration 10/25 | Loss: 0.00107911
Iteration 11/25 | Loss: 0.00107896
Iteration 12/25 | Loss: 0.00107896
Iteration 13/25 | Loss: 0.00107896
Iteration 14/25 | Loss: 0.00107896
Iteration 15/25 | Loss: 0.00107896
Iteration 16/25 | Loss: 0.00107896
Iteration 17/25 | Loss: 0.00107896
Iteration 18/25 | Loss: 0.00107896
Iteration 19/25 | Loss: 0.00107896
Iteration 20/25 | Loss: 0.00107896
Iteration 21/25 | Loss: 0.00107896
Iteration 22/25 | Loss: 0.00107896
Iteration 23/25 | Loss: 0.00107896
Iteration 24/25 | Loss: 0.00107896
Iteration 25/25 | Loss: 0.00107896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13952982
Iteration 2/25 | Loss: 0.00180911
Iteration 3/25 | Loss: 0.00180911
Iteration 4/25 | Loss: 0.00180911
Iteration 5/25 | Loss: 0.00180911
Iteration 6/25 | Loss: 0.00180911
Iteration 7/25 | Loss: 0.00180911
Iteration 8/25 | Loss: 0.00180911
Iteration 9/25 | Loss: 0.00180911
Iteration 10/25 | Loss: 0.00180911
Iteration 11/25 | Loss: 0.00180911
Iteration 12/25 | Loss: 0.00180911
Iteration 13/25 | Loss: 0.00180911
Iteration 14/25 | Loss: 0.00180911
Iteration 15/25 | Loss: 0.00180911
Iteration 16/25 | Loss: 0.00180911
Iteration 17/25 | Loss: 0.00180911
Iteration 18/25 | Loss: 0.00180911
Iteration 19/25 | Loss: 0.00180911
Iteration 20/25 | Loss: 0.00180911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018091097008436918, 0.0018091097008436918, 0.0018091097008436918, 0.0018091097008436918, 0.0018091097008436918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018091097008436918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180911
Iteration 2/1000 | Loss: 0.00004057
Iteration 3/1000 | Loss: 0.00002928
Iteration 4/1000 | Loss: 0.00002580
Iteration 5/1000 | Loss: 0.00002434
Iteration 6/1000 | Loss: 0.00002324
Iteration 7/1000 | Loss: 0.00002270
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002192
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002125
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002122
Iteration 16/1000 | Loss: 0.00002121
Iteration 17/1000 | Loss: 0.00002115
Iteration 18/1000 | Loss: 0.00002110
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00002097
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002096
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002096
Iteration 27/1000 | Loss: 0.00002096
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002096
Iteration 30/1000 | Loss: 0.00002096
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002096
Iteration 34/1000 | Loss: 0.00002096
Iteration 35/1000 | Loss: 0.00002096
Iteration 36/1000 | Loss: 0.00002095
Iteration 37/1000 | Loss: 0.00002095
Iteration 38/1000 | Loss: 0.00002095
Iteration 39/1000 | Loss: 0.00002095
Iteration 40/1000 | Loss: 0.00002094
Iteration 41/1000 | Loss: 0.00002094
Iteration 42/1000 | Loss: 0.00002093
Iteration 43/1000 | Loss: 0.00002093
Iteration 44/1000 | Loss: 0.00002093
Iteration 45/1000 | Loss: 0.00002093
Iteration 46/1000 | Loss: 0.00002093
Iteration 47/1000 | Loss: 0.00002092
Iteration 48/1000 | Loss: 0.00002092
Iteration 49/1000 | Loss: 0.00002092
Iteration 50/1000 | Loss: 0.00002092
Iteration 51/1000 | Loss: 0.00002092
Iteration 52/1000 | Loss: 0.00002092
Iteration 53/1000 | Loss: 0.00002091
Iteration 54/1000 | Loss: 0.00002091
Iteration 55/1000 | Loss: 0.00002090
Iteration 56/1000 | Loss: 0.00002089
Iteration 57/1000 | Loss: 0.00002089
Iteration 58/1000 | Loss: 0.00002089
Iteration 59/1000 | Loss: 0.00002089
Iteration 60/1000 | Loss: 0.00002089
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002088
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002085
Iteration 71/1000 | Loss: 0.00002085
Iteration 72/1000 | Loss: 0.00002085
Iteration 73/1000 | Loss: 0.00002084
Iteration 74/1000 | Loss: 0.00002083
Iteration 75/1000 | Loss: 0.00002083
Iteration 76/1000 | Loss: 0.00002083
Iteration 77/1000 | Loss: 0.00002083
Iteration 78/1000 | Loss: 0.00002083
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00002082
Iteration 81/1000 | Loss: 0.00002082
Iteration 82/1000 | Loss: 0.00002082
Iteration 83/1000 | Loss: 0.00002082
Iteration 84/1000 | Loss: 0.00002082
Iteration 85/1000 | Loss: 0.00002082
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002081
Iteration 88/1000 | Loss: 0.00002081
Iteration 89/1000 | Loss: 0.00002081
Iteration 90/1000 | Loss: 0.00002081
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00002081
Iteration 93/1000 | Loss: 0.00002081
Iteration 94/1000 | Loss: 0.00002080
Iteration 95/1000 | Loss: 0.00002080
Iteration 96/1000 | Loss: 0.00002080
Iteration 97/1000 | Loss: 0.00002080
Iteration 98/1000 | Loss: 0.00002079
Iteration 99/1000 | Loss: 0.00002079
Iteration 100/1000 | Loss: 0.00002079
Iteration 101/1000 | Loss: 0.00002079
Iteration 102/1000 | Loss: 0.00002079
Iteration 103/1000 | Loss: 0.00002079
Iteration 104/1000 | Loss: 0.00002079
Iteration 105/1000 | Loss: 0.00002079
Iteration 106/1000 | Loss: 0.00002079
Iteration 107/1000 | Loss: 0.00002078
Iteration 108/1000 | Loss: 0.00002078
Iteration 109/1000 | Loss: 0.00002078
Iteration 110/1000 | Loss: 0.00002078
Iteration 111/1000 | Loss: 0.00002077
Iteration 112/1000 | Loss: 0.00002077
Iteration 113/1000 | Loss: 0.00002077
Iteration 114/1000 | Loss: 0.00002077
Iteration 115/1000 | Loss: 0.00002077
Iteration 116/1000 | Loss: 0.00002077
Iteration 117/1000 | Loss: 0.00002077
Iteration 118/1000 | Loss: 0.00002077
Iteration 119/1000 | Loss: 0.00002077
Iteration 120/1000 | Loss: 0.00002077
Iteration 121/1000 | Loss: 0.00002077
Iteration 122/1000 | Loss: 0.00002077
Iteration 123/1000 | Loss: 0.00002077
Iteration 124/1000 | Loss: 0.00002077
Iteration 125/1000 | Loss: 0.00002076
Iteration 126/1000 | Loss: 0.00002076
Iteration 127/1000 | Loss: 0.00002076
Iteration 128/1000 | Loss: 0.00002076
Iteration 129/1000 | Loss: 0.00002076
Iteration 130/1000 | Loss: 0.00002076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.0764977307408117e-05, 2.0764977307408117e-05, 2.0764977307408117e-05, 2.0764977307408117e-05, 2.0764977307408117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0764977307408117e-05

Optimization complete. Final v2v error: 3.7337725162506104 mm

Highest mean error: 3.9811348915100098 mm for frame 105

Lowest mean error: 3.2705729007720947 mm for frame 60

Saving results

Total time: 52.056067943573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724296
Iteration 2/25 | Loss: 0.00163339
Iteration 3/25 | Loss: 0.00121109
Iteration 4/25 | Loss: 0.00114470
Iteration 5/25 | Loss: 0.00111878
Iteration 6/25 | Loss: 0.00110904
Iteration 7/25 | Loss: 0.00110644
Iteration 8/25 | Loss: 0.00110533
Iteration 9/25 | Loss: 0.00110478
Iteration 10/25 | Loss: 0.00110454
Iteration 11/25 | Loss: 0.00110439
Iteration 12/25 | Loss: 0.00110423
Iteration 13/25 | Loss: 0.00110397
Iteration 14/25 | Loss: 0.00110346
Iteration 15/25 | Loss: 0.00110861
Iteration 16/25 | Loss: 0.00110540
Iteration 17/25 | Loss: 0.00110460
Iteration 18/25 | Loss: 0.00110690
Iteration 19/25 | Loss: 0.00110266
Iteration 20/25 | Loss: 0.00109613
Iteration 21/25 | Loss: 0.00109105
Iteration 22/25 | Loss: 0.00108577
Iteration 23/25 | Loss: 0.00107684
Iteration 24/25 | Loss: 0.00107517
Iteration 25/25 | Loss: 0.00107278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.97922516
Iteration 2/25 | Loss: 0.00271214
Iteration 3/25 | Loss: 0.00271177
Iteration 4/25 | Loss: 0.00271176
Iteration 5/25 | Loss: 0.00271176
Iteration 6/25 | Loss: 0.00271176
Iteration 7/25 | Loss: 0.00271176
Iteration 8/25 | Loss: 0.00271176
Iteration 9/25 | Loss: 0.00271176
Iteration 10/25 | Loss: 0.00271176
Iteration 11/25 | Loss: 0.00271176
Iteration 12/25 | Loss: 0.00271176
Iteration 13/25 | Loss: 0.00271176
Iteration 14/25 | Loss: 0.00271176
Iteration 15/25 | Loss: 0.00271176
Iteration 16/25 | Loss: 0.00271176
Iteration 17/25 | Loss: 0.00271176
Iteration 18/25 | Loss: 0.00271176
Iteration 19/25 | Loss: 0.00271176
Iteration 20/25 | Loss: 0.00271176
Iteration 21/25 | Loss: 0.00271176
Iteration 22/25 | Loss: 0.00271176
Iteration 23/25 | Loss: 0.00271176
Iteration 24/25 | Loss: 0.00271176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002711760113015771, 0.002711760113015771, 0.002711760113015771, 0.002711760113015771, 0.002711760113015771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002711760113015771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271176
Iteration 2/1000 | Loss: 0.00046599
Iteration 3/1000 | Loss: 0.00008713
Iteration 4/1000 | Loss: 0.00039172
Iteration 5/1000 | Loss: 0.00005321
Iteration 6/1000 | Loss: 0.00003984
Iteration 7/1000 | Loss: 0.00003382
Iteration 8/1000 | Loss: 0.00003097
Iteration 9/1000 | Loss: 0.00002905
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00002677
Iteration 12/1000 | Loss: 0.00002625
Iteration 13/1000 | Loss: 0.00002581
Iteration 14/1000 | Loss: 0.00002541
Iteration 15/1000 | Loss: 0.00002507
Iteration 16/1000 | Loss: 0.00002474
Iteration 17/1000 | Loss: 0.00002448
Iteration 18/1000 | Loss: 0.00002423
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002393
Iteration 21/1000 | Loss: 0.00002392
Iteration 22/1000 | Loss: 0.00002391
Iteration 23/1000 | Loss: 0.00002389
Iteration 24/1000 | Loss: 0.00002385
Iteration 25/1000 | Loss: 0.00002380
Iteration 26/1000 | Loss: 0.00002374
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002372
Iteration 29/1000 | Loss: 0.00002371
Iteration 30/1000 | Loss: 0.00002370
Iteration 31/1000 | Loss: 0.00002369
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002368
Iteration 34/1000 | Loss: 0.00002367
Iteration 35/1000 | Loss: 0.00002366
Iteration 36/1000 | Loss: 0.00002365
Iteration 37/1000 | Loss: 0.00002365
Iteration 38/1000 | Loss: 0.00002365
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002363
Iteration 41/1000 | Loss: 0.00002363
Iteration 42/1000 | Loss: 0.00002362
Iteration 43/1000 | Loss: 0.00002362
Iteration 44/1000 | Loss: 0.00002361
Iteration 45/1000 | Loss: 0.00002361
Iteration 46/1000 | Loss: 0.00002360
Iteration 47/1000 | Loss: 0.00002357
Iteration 48/1000 | Loss: 0.00002356
Iteration 49/1000 | Loss: 0.00054700
Iteration 50/1000 | Loss: 0.00002559
Iteration 51/1000 | Loss: 0.00002358
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002125
Iteration 55/1000 | Loss: 0.00002091
Iteration 56/1000 | Loss: 0.00002085
Iteration 57/1000 | Loss: 0.00002077
Iteration 58/1000 | Loss: 0.00002064
Iteration 59/1000 | Loss: 0.00002062
Iteration 60/1000 | Loss: 0.00002061
Iteration 61/1000 | Loss: 0.00002061
Iteration 62/1000 | Loss: 0.00002059
Iteration 63/1000 | Loss: 0.00002059
Iteration 64/1000 | Loss: 0.00002058
Iteration 65/1000 | Loss: 0.00002057
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002056
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002052
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002046
Iteration 72/1000 | Loss: 0.00002046
Iteration 73/1000 | Loss: 0.00002046
Iteration 74/1000 | Loss: 0.00002045
Iteration 75/1000 | Loss: 0.00002045
Iteration 76/1000 | Loss: 0.00002045
Iteration 77/1000 | Loss: 0.00002045
Iteration 78/1000 | Loss: 0.00002044
Iteration 79/1000 | Loss: 0.00002044
Iteration 80/1000 | Loss: 0.00002044
Iteration 81/1000 | Loss: 0.00002043
Iteration 82/1000 | Loss: 0.00002043
Iteration 83/1000 | Loss: 0.00002043
Iteration 84/1000 | Loss: 0.00002042
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002040
Iteration 89/1000 | Loss: 0.00002039
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002038
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002037
Iteration 95/1000 | Loss: 0.00002037
Iteration 96/1000 | Loss: 0.00002037
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002035
Iteration 101/1000 | Loss: 0.00002035
Iteration 102/1000 | Loss: 0.00002035
Iteration 103/1000 | Loss: 0.00002034
Iteration 104/1000 | Loss: 0.00002034
Iteration 105/1000 | Loss: 0.00002034
Iteration 106/1000 | Loss: 0.00002034
Iteration 107/1000 | Loss: 0.00002034
Iteration 108/1000 | Loss: 0.00002034
Iteration 109/1000 | Loss: 0.00002034
Iteration 110/1000 | Loss: 0.00002034
Iteration 111/1000 | Loss: 0.00002034
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002033
Iteration 114/1000 | Loss: 0.00002033
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002031
Iteration 122/1000 | Loss: 0.00002031
Iteration 123/1000 | Loss: 0.00002031
Iteration 124/1000 | Loss: 0.00002031
Iteration 125/1000 | Loss: 0.00002031
Iteration 126/1000 | Loss: 0.00002030
Iteration 127/1000 | Loss: 0.00002030
Iteration 128/1000 | Loss: 0.00002030
Iteration 129/1000 | Loss: 0.00002030
Iteration 130/1000 | Loss: 0.00002030
Iteration 131/1000 | Loss: 0.00002030
Iteration 132/1000 | Loss: 0.00002030
Iteration 133/1000 | Loss: 0.00002030
Iteration 134/1000 | Loss: 0.00002030
Iteration 135/1000 | Loss: 0.00002030
Iteration 136/1000 | Loss: 0.00002030
Iteration 137/1000 | Loss: 0.00002029
Iteration 138/1000 | Loss: 0.00002029
Iteration 139/1000 | Loss: 0.00002029
Iteration 140/1000 | Loss: 0.00002029
Iteration 141/1000 | Loss: 0.00002029
Iteration 142/1000 | Loss: 0.00002028
Iteration 143/1000 | Loss: 0.00002028
Iteration 144/1000 | Loss: 0.00002028
Iteration 145/1000 | Loss: 0.00002028
Iteration 146/1000 | Loss: 0.00002027
Iteration 147/1000 | Loss: 0.00002027
Iteration 148/1000 | Loss: 0.00002027
Iteration 149/1000 | Loss: 0.00002027
Iteration 150/1000 | Loss: 0.00002027
Iteration 151/1000 | Loss: 0.00002027
Iteration 152/1000 | Loss: 0.00002026
Iteration 153/1000 | Loss: 0.00002026
Iteration 154/1000 | Loss: 0.00002026
Iteration 155/1000 | Loss: 0.00002026
Iteration 156/1000 | Loss: 0.00002026
Iteration 157/1000 | Loss: 0.00002026
Iteration 158/1000 | Loss: 0.00002026
Iteration 159/1000 | Loss: 0.00002026
Iteration 160/1000 | Loss: 0.00002026
Iteration 161/1000 | Loss: 0.00002025
Iteration 162/1000 | Loss: 0.00002025
Iteration 163/1000 | Loss: 0.00002025
Iteration 164/1000 | Loss: 0.00002025
Iteration 165/1000 | Loss: 0.00002025
Iteration 166/1000 | Loss: 0.00002025
Iteration 167/1000 | Loss: 0.00002025
Iteration 168/1000 | Loss: 0.00002025
Iteration 169/1000 | Loss: 0.00002025
Iteration 170/1000 | Loss: 0.00002024
Iteration 171/1000 | Loss: 0.00002024
Iteration 172/1000 | Loss: 0.00002024
Iteration 173/1000 | Loss: 0.00002024
Iteration 174/1000 | Loss: 0.00002024
Iteration 175/1000 | Loss: 0.00002024
Iteration 176/1000 | Loss: 0.00002024
Iteration 177/1000 | Loss: 0.00002024
Iteration 178/1000 | Loss: 0.00002024
Iteration 179/1000 | Loss: 0.00002023
Iteration 180/1000 | Loss: 0.00002023
Iteration 181/1000 | Loss: 0.00002023
Iteration 182/1000 | Loss: 0.00002023
Iteration 183/1000 | Loss: 0.00002023
Iteration 184/1000 | Loss: 0.00002023
Iteration 185/1000 | Loss: 0.00002023
Iteration 186/1000 | Loss: 0.00002023
Iteration 187/1000 | Loss: 0.00002023
Iteration 188/1000 | Loss: 0.00002023
Iteration 189/1000 | Loss: 0.00002023
Iteration 190/1000 | Loss: 0.00002023
Iteration 191/1000 | Loss: 0.00002023
Iteration 192/1000 | Loss: 0.00002022
Iteration 193/1000 | Loss: 0.00002022
Iteration 194/1000 | Loss: 0.00002022
Iteration 195/1000 | Loss: 0.00002022
Iteration 196/1000 | Loss: 0.00002022
Iteration 197/1000 | Loss: 0.00002022
Iteration 198/1000 | Loss: 0.00002022
Iteration 199/1000 | Loss: 0.00002022
Iteration 200/1000 | Loss: 0.00002022
Iteration 201/1000 | Loss: 0.00002022
Iteration 202/1000 | Loss: 0.00002022
Iteration 203/1000 | Loss: 0.00002022
Iteration 204/1000 | Loss: 0.00002022
Iteration 205/1000 | Loss: 0.00002022
Iteration 206/1000 | Loss: 0.00002022
Iteration 207/1000 | Loss: 0.00002022
Iteration 208/1000 | Loss: 0.00002022
Iteration 209/1000 | Loss: 0.00002022
Iteration 210/1000 | Loss: 0.00002022
Iteration 211/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.021977888944093e-05, 2.021977888944093e-05, 2.021977888944093e-05, 2.021977888944093e-05, 2.021977888944093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.021977888944093e-05

Optimization complete. Final v2v error: 3.6177706718444824 mm

Highest mean error: 5.284158706665039 mm for frame 125

Lowest mean error: 2.4240922927856445 mm for frame 13

Saving results

Total time: 103.02931571006775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823229
Iteration 2/25 | Loss: 0.00139866
Iteration 3/25 | Loss: 0.00114234
Iteration 4/25 | Loss: 0.00110617
Iteration 5/25 | Loss: 0.00109773
Iteration 6/25 | Loss: 0.00109292
Iteration 7/25 | Loss: 0.00108965
Iteration 8/25 | Loss: 0.00108868
Iteration 9/25 | Loss: 0.00108831
Iteration 10/25 | Loss: 0.00108824
Iteration 11/25 | Loss: 0.00108824
Iteration 12/25 | Loss: 0.00108824
Iteration 13/25 | Loss: 0.00108824
Iteration 14/25 | Loss: 0.00108824
Iteration 15/25 | Loss: 0.00108824
Iteration 16/25 | Loss: 0.00108824
Iteration 17/25 | Loss: 0.00108824
Iteration 18/25 | Loss: 0.00108823
Iteration 19/25 | Loss: 0.00108823
Iteration 20/25 | Loss: 0.00108823
Iteration 21/25 | Loss: 0.00108823
Iteration 22/25 | Loss: 0.00108823
Iteration 23/25 | Loss: 0.00108823
Iteration 24/25 | Loss: 0.00108823
Iteration 25/25 | Loss: 0.00108823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34455705
Iteration 2/25 | Loss: 0.00250997
Iteration 3/25 | Loss: 0.00250996
Iteration 4/25 | Loss: 0.00250996
Iteration 5/25 | Loss: 0.00250996
Iteration 6/25 | Loss: 0.00250996
Iteration 7/25 | Loss: 0.00250996
Iteration 8/25 | Loss: 0.00250996
Iteration 9/25 | Loss: 0.00250996
Iteration 10/25 | Loss: 0.00250996
Iteration 11/25 | Loss: 0.00250996
Iteration 12/25 | Loss: 0.00250996
Iteration 13/25 | Loss: 0.00250996
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0025099616032093763, 0.0025099616032093763, 0.0025099616032093763, 0.0025099616032093763, 0.0025099616032093763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025099616032093763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250996
Iteration 2/1000 | Loss: 0.00005455
Iteration 3/1000 | Loss: 0.00002368
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001624
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001558
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001558
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001557
Iteration 37/1000 | Loss: 0.00001557
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001556
Iteration 46/1000 | Loss: 0.00001556
Iteration 47/1000 | Loss: 0.00001556
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001552
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001539
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001537
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001537
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001536
Iteration 129/1000 | Loss: 0.00001536
Iteration 130/1000 | Loss: 0.00001536
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001536
Iteration 133/1000 | Loss: 0.00001536
Iteration 134/1000 | Loss: 0.00001535
Iteration 135/1000 | Loss: 0.00001535
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001535
Iteration 140/1000 | Loss: 0.00001535
Iteration 141/1000 | Loss: 0.00001535
Iteration 142/1000 | Loss: 0.00001535
Iteration 143/1000 | Loss: 0.00001535
Iteration 144/1000 | Loss: 0.00001535
Iteration 145/1000 | Loss: 0.00001535
Iteration 146/1000 | Loss: 0.00001535
Iteration 147/1000 | Loss: 0.00001535
Iteration 148/1000 | Loss: 0.00001535
Iteration 149/1000 | Loss: 0.00001534
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001534
Iteration 153/1000 | Loss: 0.00001534
Iteration 154/1000 | Loss: 0.00001534
Iteration 155/1000 | Loss: 0.00001534
Iteration 156/1000 | Loss: 0.00001534
Iteration 157/1000 | Loss: 0.00001534
Iteration 158/1000 | Loss: 0.00001534
Iteration 159/1000 | Loss: 0.00001534
Iteration 160/1000 | Loss: 0.00001534
Iteration 161/1000 | Loss: 0.00001534
Iteration 162/1000 | Loss: 0.00001534
Iteration 163/1000 | Loss: 0.00001534
Iteration 164/1000 | Loss: 0.00001533
Iteration 165/1000 | Loss: 0.00001533
Iteration 166/1000 | Loss: 0.00001533
Iteration 167/1000 | Loss: 0.00001533
Iteration 168/1000 | Loss: 0.00001533
Iteration 169/1000 | Loss: 0.00001533
Iteration 170/1000 | Loss: 0.00001533
Iteration 171/1000 | Loss: 0.00001533
Iteration 172/1000 | Loss: 0.00001533
Iteration 173/1000 | Loss: 0.00001533
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001533
Iteration 178/1000 | Loss: 0.00001533
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Iteration 181/1000 | Loss: 0.00001533
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001532
Iteration 186/1000 | Loss: 0.00001532
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001532
Iteration 189/1000 | Loss: 0.00001531
Iteration 190/1000 | Loss: 0.00001531
Iteration 191/1000 | Loss: 0.00001531
Iteration 192/1000 | Loss: 0.00001531
Iteration 193/1000 | Loss: 0.00001531
Iteration 194/1000 | Loss: 0.00001531
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001531
Iteration 197/1000 | Loss: 0.00001531
Iteration 198/1000 | Loss: 0.00001531
Iteration 199/1000 | Loss: 0.00001531
Iteration 200/1000 | Loss: 0.00001530
Iteration 201/1000 | Loss: 0.00001530
Iteration 202/1000 | Loss: 0.00001530
Iteration 203/1000 | Loss: 0.00001530
Iteration 204/1000 | Loss: 0.00001530
Iteration 205/1000 | Loss: 0.00001530
Iteration 206/1000 | Loss: 0.00001530
Iteration 207/1000 | Loss: 0.00001530
Iteration 208/1000 | Loss: 0.00001530
Iteration 209/1000 | Loss: 0.00001530
Iteration 210/1000 | Loss: 0.00001530
Iteration 211/1000 | Loss: 0.00001530
Iteration 212/1000 | Loss: 0.00001530
Iteration 213/1000 | Loss: 0.00001530
Iteration 214/1000 | Loss: 0.00001530
Iteration 215/1000 | Loss: 0.00001530
Iteration 216/1000 | Loss: 0.00001530
Iteration 217/1000 | Loss: 0.00001530
Iteration 218/1000 | Loss: 0.00001530
Iteration 219/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.529951077827718e-05, 1.529951077827718e-05, 1.529951077827718e-05, 1.529951077827718e-05, 1.529951077827718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.529951077827718e-05

Optimization complete. Final v2v error: 3.307619571685791 mm

Highest mean error: 3.7723567485809326 mm for frame 33

Lowest mean error: 2.923891067504883 mm for frame 179

Saving results

Total time: 53.1073272228241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883693
Iteration 2/25 | Loss: 0.00129088
Iteration 3/25 | Loss: 0.00107897
Iteration 4/25 | Loss: 0.00105960
Iteration 5/25 | Loss: 0.00105657
Iteration 6/25 | Loss: 0.00105617
Iteration 7/25 | Loss: 0.00105617
Iteration 8/25 | Loss: 0.00105617
Iteration 9/25 | Loss: 0.00105617
Iteration 10/25 | Loss: 0.00105617
Iteration 11/25 | Loss: 0.00105617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00105616997461766, 0.00105616997461766, 0.00105616997461766, 0.00105616997461766, 0.00105616997461766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00105616997461766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81123048
Iteration 2/25 | Loss: 0.00098094
Iteration 3/25 | Loss: 0.00098094
Iteration 4/25 | Loss: 0.00098094
Iteration 5/25 | Loss: 0.00098094
Iteration 6/25 | Loss: 0.00098094
Iteration 7/25 | Loss: 0.00098094
Iteration 8/25 | Loss: 0.00098094
Iteration 9/25 | Loss: 0.00098094
Iteration 10/25 | Loss: 0.00098093
Iteration 11/25 | Loss: 0.00098093
Iteration 12/25 | Loss: 0.00098093
Iteration 13/25 | Loss: 0.00098093
Iteration 14/25 | Loss: 0.00098093
Iteration 15/25 | Loss: 0.00098093
Iteration 16/25 | Loss: 0.00098093
Iteration 17/25 | Loss: 0.00098093
Iteration 18/25 | Loss: 0.00098093
Iteration 19/25 | Loss: 0.00098093
Iteration 20/25 | Loss: 0.00098093
Iteration 21/25 | Loss: 0.00098093
Iteration 22/25 | Loss: 0.00098093
Iteration 23/25 | Loss: 0.00098093
Iteration 24/25 | Loss: 0.00098093
Iteration 25/25 | Loss: 0.00098093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000980934128165245, 0.000980934128165245, 0.000980934128165245, 0.000980934128165245, 0.000980934128165245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000980934128165245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098093
Iteration 2/1000 | Loss: 0.00003749
Iteration 3/1000 | Loss: 0.00002836
Iteration 4/1000 | Loss: 0.00002575
Iteration 5/1000 | Loss: 0.00002460
Iteration 6/1000 | Loss: 0.00002385
Iteration 7/1000 | Loss: 0.00002355
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00002271
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002220
Iteration 12/1000 | Loss: 0.00002213
Iteration 13/1000 | Loss: 0.00002201
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002191
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002185
Iteration 28/1000 | Loss: 0.00002185
Iteration 29/1000 | Loss: 0.00002184
Iteration 30/1000 | Loss: 0.00002184
Iteration 31/1000 | Loss: 0.00002184
Iteration 32/1000 | Loss: 0.00002184
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002184
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002184
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002184
Iteration 48/1000 | Loss: 0.00002184
Iteration 49/1000 | Loss: 0.00002184
Iteration 50/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [2.1836560335941613e-05, 2.1836560335941613e-05, 2.1836560335941613e-05, 2.1836560335941613e-05, 2.1836560335941613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1836560335941613e-05

Optimization complete. Final v2v error: 3.9596710205078125 mm

Highest mean error: 4.176701545715332 mm for frame 114

Lowest mean error: 3.7863006591796875 mm for frame 0

Saving results

Total time: 29.441017150878906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041235
Iteration 2/25 | Loss: 0.00169807
Iteration 3/25 | Loss: 0.00124499
Iteration 4/25 | Loss: 0.00122491
Iteration 5/25 | Loss: 0.00115491
Iteration 6/25 | Loss: 0.00114234
Iteration 7/25 | Loss: 0.00111924
Iteration 8/25 | Loss: 0.00112624
Iteration 9/25 | Loss: 0.00111207
Iteration 10/25 | Loss: 0.00110074
Iteration 11/25 | Loss: 0.00109754
Iteration 12/25 | Loss: 0.00109473
Iteration 13/25 | Loss: 0.00109364
Iteration 14/25 | Loss: 0.00109761
Iteration 15/25 | Loss: 0.00109353
Iteration 16/25 | Loss: 0.00109647
Iteration 17/25 | Loss: 0.00108710
Iteration 18/25 | Loss: 0.00108684
Iteration 19/25 | Loss: 0.00108498
Iteration 20/25 | Loss: 0.00108777
Iteration 21/25 | Loss: 0.00109024
Iteration 22/25 | Loss: 0.00108620
Iteration 23/25 | Loss: 0.00108791
Iteration 24/25 | Loss: 0.00108807
Iteration 25/25 | Loss: 0.00108538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.73735332
Iteration 2/25 | Loss: 0.00232991
Iteration 3/25 | Loss: 0.00232987
Iteration 4/25 | Loss: 0.00232987
Iteration 5/25 | Loss: 0.00232987
Iteration 6/25 | Loss: 0.00232987
Iteration 7/25 | Loss: 0.00232987
Iteration 8/25 | Loss: 0.00232987
Iteration 9/25 | Loss: 0.00232987
Iteration 10/25 | Loss: 0.00232987
Iteration 11/25 | Loss: 0.00232987
Iteration 12/25 | Loss: 0.00232987
Iteration 13/25 | Loss: 0.00232987
Iteration 14/25 | Loss: 0.00232987
Iteration 15/25 | Loss: 0.00232987
Iteration 16/25 | Loss: 0.00232986
Iteration 17/25 | Loss: 0.00232986
Iteration 18/25 | Loss: 0.00232986
Iteration 19/25 | Loss: 0.00232986
Iteration 20/25 | Loss: 0.00232986
Iteration 21/25 | Loss: 0.00232986
Iteration 22/25 | Loss: 0.00232986
Iteration 23/25 | Loss: 0.00232986
Iteration 24/25 | Loss: 0.00232986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0023298643063753843, 0.0023298643063753843, 0.0023298643063753843, 0.0023298643063753843, 0.0023298643063753843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023298643063753843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232986
Iteration 2/1000 | Loss: 0.00009462
Iteration 3/1000 | Loss: 0.00044159
Iteration 4/1000 | Loss: 0.00035184
Iteration 5/1000 | Loss: 0.00037427
Iteration 6/1000 | Loss: 0.00036304
Iteration 7/1000 | Loss: 0.00033299
Iteration 8/1000 | Loss: 0.00004409
Iteration 9/1000 | Loss: 0.00003613
Iteration 10/1000 | Loss: 0.00003314
Iteration 11/1000 | Loss: 0.00003035
Iteration 12/1000 | Loss: 0.00002764
Iteration 13/1000 | Loss: 0.00002647
Iteration 14/1000 | Loss: 0.00002570
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00002472
Iteration 19/1000 | Loss: 0.00002456
Iteration 20/1000 | Loss: 0.00002435
Iteration 21/1000 | Loss: 0.00002423
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002419
Iteration 24/1000 | Loss: 0.00002409
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002398
Iteration 27/1000 | Loss: 0.00002397
Iteration 28/1000 | Loss: 0.00002397
Iteration 29/1000 | Loss: 0.00002397
Iteration 30/1000 | Loss: 0.00002397
Iteration 31/1000 | Loss: 0.00002396
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002396
Iteration 34/1000 | Loss: 0.00002394
Iteration 35/1000 | Loss: 0.00002394
Iteration 36/1000 | Loss: 0.00002393
Iteration 37/1000 | Loss: 0.00002392
Iteration 38/1000 | Loss: 0.00002392
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002392
Iteration 41/1000 | Loss: 0.00002392
Iteration 42/1000 | Loss: 0.00002392
Iteration 43/1000 | Loss: 0.00002391
Iteration 44/1000 | Loss: 0.00002390
Iteration 45/1000 | Loss: 0.00002390
Iteration 46/1000 | Loss: 0.00002390
Iteration 47/1000 | Loss: 0.00002390
Iteration 48/1000 | Loss: 0.00002390
Iteration 49/1000 | Loss: 0.00002390
Iteration 50/1000 | Loss: 0.00002390
Iteration 51/1000 | Loss: 0.00002390
Iteration 52/1000 | Loss: 0.00002389
Iteration 53/1000 | Loss: 0.00002389
Iteration 54/1000 | Loss: 0.00002387
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00002386
Iteration 58/1000 | Loss: 0.00002385
Iteration 59/1000 | Loss: 0.00002385
Iteration 60/1000 | Loss: 0.00002385
Iteration 61/1000 | Loss: 0.00002384
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002384
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002383
Iteration 66/1000 | Loss: 0.00002382
Iteration 67/1000 | Loss: 0.00002382
Iteration 68/1000 | Loss: 0.00002382
Iteration 69/1000 | Loss: 0.00002381
Iteration 70/1000 | Loss: 0.00002381
Iteration 71/1000 | Loss: 0.00002380
Iteration 72/1000 | Loss: 0.00002379
Iteration 73/1000 | Loss: 0.00002379
Iteration 74/1000 | Loss: 0.00002378
Iteration 75/1000 | Loss: 0.00002378
Iteration 76/1000 | Loss: 0.00002378
Iteration 77/1000 | Loss: 0.00002372
Iteration 78/1000 | Loss: 0.00002370
Iteration 79/1000 | Loss: 0.00002369
Iteration 80/1000 | Loss: 0.00002368
Iteration 81/1000 | Loss: 0.00002368
Iteration 82/1000 | Loss: 0.00002368
Iteration 83/1000 | Loss: 0.00002367
Iteration 84/1000 | Loss: 0.00002367
Iteration 85/1000 | Loss: 0.00002367
Iteration 86/1000 | Loss: 0.00002366
Iteration 87/1000 | Loss: 0.00002366
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002365
Iteration 90/1000 | Loss: 0.00002365
Iteration 91/1000 | Loss: 0.00002364
Iteration 92/1000 | Loss: 0.00002364
Iteration 93/1000 | Loss: 0.00002364
Iteration 94/1000 | Loss: 0.00002363
Iteration 95/1000 | Loss: 0.00002362
Iteration 96/1000 | Loss: 0.00002362
Iteration 97/1000 | Loss: 0.00002362
Iteration 98/1000 | Loss: 0.00002361
Iteration 99/1000 | Loss: 0.00002361
Iteration 100/1000 | Loss: 0.00002361
Iteration 101/1000 | Loss: 0.00002360
Iteration 102/1000 | Loss: 0.00002360
Iteration 103/1000 | Loss: 0.00002360
Iteration 104/1000 | Loss: 0.00002360
Iteration 105/1000 | Loss: 0.00002359
Iteration 106/1000 | Loss: 0.00002359
Iteration 107/1000 | Loss: 0.00002359
Iteration 108/1000 | Loss: 0.00002358
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002358
Iteration 111/1000 | Loss: 0.00002358
Iteration 112/1000 | Loss: 0.00002358
Iteration 113/1000 | Loss: 0.00002357
Iteration 114/1000 | Loss: 0.00002357
Iteration 115/1000 | Loss: 0.00002357
Iteration 116/1000 | Loss: 0.00002356
Iteration 117/1000 | Loss: 0.00002356
Iteration 118/1000 | Loss: 0.00002356
Iteration 119/1000 | Loss: 0.00002355
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002355
Iteration 122/1000 | Loss: 0.00002355
Iteration 123/1000 | Loss: 0.00002355
Iteration 124/1000 | Loss: 0.00002355
Iteration 125/1000 | Loss: 0.00002354
Iteration 126/1000 | Loss: 0.00002354
Iteration 127/1000 | Loss: 0.00002354
Iteration 128/1000 | Loss: 0.00002354
Iteration 129/1000 | Loss: 0.00002353
Iteration 130/1000 | Loss: 0.00002353
Iteration 131/1000 | Loss: 0.00002353
Iteration 132/1000 | Loss: 0.00002353
Iteration 133/1000 | Loss: 0.00002353
Iteration 134/1000 | Loss: 0.00002353
Iteration 135/1000 | Loss: 0.00002353
Iteration 136/1000 | Loss: 0.00002353
Iteration 137/1000 | Loss: 0.00002353
Iteration 138/1000 | Loss: 0.00002353
Iteration 139/1000 | Loss: 0.00002353
Iteration 140/1000 | Loss: 0.00002353
Iteration 141/1000 | Loss: 0.00002353
Iteration 142/1000 | Loss: 0.00002352
Iteration 143/1000 | Loss: 0.00002352
Iteration 144/1000 | Loss: 0.00002352
Iteration 145/1000 | Loss: 0.00002352
Iteration 146/1000 | Loss: 0.00002351
Iteration 147/1000 | Loss: 0.00002351
Iteration 148/1000 | Loss: 0.00002351
Iteration 149/1000 | Loss: 0.00002351
Iteration 150/1000 | Loss: 0.00002351
Iteration 151/1000 | Loss: 0.00002351
Iteration 152/1000 | Loss: 0.00002350
Iteration 153/1000 | Loss: 0.00002350
Iteration 154/1000 | Loss: 0.00002350
Iteration 155/1000 | Loss: 0.00002350
Iteration 156/1000 | Loss: 0.00002350
Iteration 157/1000 | Loss: 0.00002350
Iteration 158/1000 | Loss: 0.00002350
Iteration 159/1000 | Loss: 0.00002350
Iteration 160/1000 | Loss: 0.00002350
Iteration 161/1000 | Loss: 0.00002350
Iteration 162/1000 | Loss: 0.00002350
Iteration 163/1000 | Loss: 0.00002350
Iteration 164/1000 | Loss: 0.00002350
Iteration 165/1000 | Loss: 0.00002350
Iteration 166/1000 | Loss: 0.00002349
Iteration 167/1000 | Loss: 0.00002349
Iteration 168/1000 | Loss: 0.00002349
Iteration 169/1000 | Loss: 0.00002349
Iteration 170/1000 | Loss: 0.00002349
Iteration 171/1000 | Loss: 0.00002349
Iteration 172/1000 | Loss: 0.00002349
Iteration 173/1000 | Loss: 0.00002349
Iteration 174/1000 | Loss: 0.00002349
Iteration 175/1000 | Loss: 0.00002349
Iteration 176/1000 | Loss: 0.00002349
Iteration 177/1000 | Loss: 0.00002349
Iteration 178/1000 | Loss: 0.00002349
Iteration 179/1000 | Loss: 0.00002349
Iteration 180/1000 | Loss: 0.00002349
Iteration 181/1000 | Loss: 0.00002349
Iteration 182/1000 | Loss: 0.00002349
Iteration 183/1000 | Loss: 0.00002349
Iteration 184/1000 | Loss: 0.00002349
Iteration 185/1000 | Loss: 0.00002349
Iteration 186/1000 | Loss: 0.00002349
Iteration 187/1000 | Loss: 0.00002349
Iteration 188/1000 | Loss: 0.00002349
Iteration 189/1000 | Loss: 0.00002349
Iteration 190/1000 | Loss: 0.00002349
Iteration 191/1000 | Loss: 0.00002349
Iteration 192/1000 | Loss: 0.00002349
Iteration 193/1000 | Loss: 0.00002349
Iteration 194/1000 | Loss: 0.00002349
Iteration 195/1000 | Loss: 0.00002349
Iteration 196/1000 | Loss: 0.00002349
Iteration 197/1000 | Loss: 0.00002349
Iteration 198/1000 | Loss: 0.00002349
Iteration 199/1000 | Loss: 0.00002349
Iteration 200/1000 | Loss: 0.00002349
Iteration 201/1000 | Loss: 0.00002349
Iteration 202/1000 | Loss: 0.00002349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.3490036255680025e-05, 2.3490036255680025e-05, 2.3490036255680025e-05, 2.3490036255680025e-05, 2.3490036255680025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3490036255680025e-05

Optimization complete. Final v2v error: 3.874115228652954 mm

Highest mean error: 5.968509674072266 mm for frame 126

Lowest mean error: 2.594942569732666 mm for frame 98

Saving results

Total time: 96.3312451839447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922318
Iteration 2/25 | Loss: 0.00293154
Iteration 3/25 | Loss: 0.00182825
Iteration 4/25 | Loss: 0.00176037
Iteration 5/25 | Loss: 0.00146717
Iteration 6/25 | Loss: 0.00142497
Iteration 7/25 | Loss: 0.00140453
Iteration 8/25 | Loss: 0.00139742
Iteration 9/25 | Loss: 0.00139350
Iteration 10/25 | Loss: 0.00139313
Iteration 11/25 | Loss: 0.00137619
Iteration 12/25 | Loss: 0.00137411
Iteration 13/25 | Loss: 0.00137382
Iteration 14/25 | Loss: 0.00137116
Iteration 15/25 | Loss: 0.00137019
Iteration 16/25 | Loss: 0.00136926
Iteration 17/25 | Loss: 0.00136748
Iteration 18/25 | Loss: 0.00136677
Iteration 19/25 | Loss: 0.00136712
Iteration 20/25 | Loss: 0.00136793
Iteration 21/25 | Loss: 0.00136535
Iteration 22/25 | Loss: 0.00136513
Iteration 23/25 | Loss: 0.00136587
Iteration 24/25 | Loss: 0.00136471
Iteration 25/25 | Loss: 0.00136458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.01596880
Iteration 2/25 | Loss: 0.00798845
Iteration 3/25 | Loss: 0.00377390
Iteration 4/25 | Loss: 0.00365329
Iteration 5/25 | Loss: 0.00365329
Iteration 6/25 | Loss: 0.00365329
Iteration 7/25 | Loss: 0.00365329
Iteration 8/25 | Loss: 0.00365329
Iteration 9/25 | Loss: 0.00365329
Iteration 10/25 | Loss: 0.00365329
Iteration 11/25 | Loss: 0.00365329
Iteration 12/25 | Loss: 0.00365329
Iteration 13/25 | Loss: 0.00365328
Iteration 14/25 | Loss: 0.00365328
Iteration 15/25 | Loss: 0.00365328
Iteration 16/25 | Loss: 0.00365328
Iteration 17/25 | Loss: 0.00365328
Iteration 18/25 | Loss: 0.00365328
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0036532848607748747, 0.0036532848607748747, 0.0036532848607748747, 0.0036532848607748747, 0.0036532848607748747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036532848607748747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00365328
Iteration 2/1000 | Loss: 0.00656844
Iteration 3/1000 | Loss: 0.00193810
Iteration 4/1000 | Loss: 0.00058028
Iteration 5/1000 | Loss: 0.00247348
Iteration 6/1000 | Loss: 0.00271165
Iteration 7/1000 | Loss: 0.00299066
Iteration 8/1000 | Loss: 0.00209120
Iteration 9/1000 | Loss: 0.00439697
Iteration 10/1000 | Loss: 0.00208811
Iteration 11/1000 | Loss: 0.00099522
Iteration 12/1000 | Loss: 0.00229075
Iteration 13/1000 | Loss: 0.00038239
Iteration 14/1000 | Loss: 0.00026580
Iteration 15/1000 | Loss: 0.00143653
Iteration 16/1000 | Loss: 0.00171129
Iteration 17/1000 | Loss: 0.00025462
Iteration 18/1000 | Loss: 0.00073601
Iteration 19/1000 | Loss: 0.00329091
Iteration 20/1000 | Loss: 0.00794203
Iteration 21/1000 | Loss: 0.00819244
Iteration 22/1000 | Loss: 0.00262542
Iteration 23/1000 | Loss: 0.00136181
Iteration 24/1000 | Loss: 0.00256520
Iteration 25/1000 | Loss: 0.00186783
Iteration 26/1000 | Loss: 0.00142153
Iteration 27/1000 | Loss: 0.00096382
Iteration 28/1000 | Loss: 0.00339338
Iteration 29/1000 | Loss: 0.00182650
Iteration 30/1000 | Loss: 0.00141841
Iteration 31/1000 | Loss: 0.00342019
Iteration 32/1000 | Loss: 0.00451708
Iteration 33/1000 | Loss: 0.00154017
Iteration 34/1000 | Loss: 0.00053770
Iteration 35/1000 | Loss: 0.00116641
Iteration 36/1000 | Loss: 0.00032957
Iteration 37/1000 | Loss: 0.00067290
Iteration 38/1000 | Loss: 0.00149301
Iteration 39/1000 | Loss: 0.00145150
Iteration 40/1000 | Loss: 0.00091755
Iteration 41/1000 | Loss: 0.00027065
Iteration 42/1000 | Loss: 0.00175980
Iteration 43/1000 | Loss: 0.00013786
Iteration 44/1000 | Loss: 0.00082030
Iteration 45/1000 | Loss: 0.00088899
Iteration 46/1000 | Loss: 0.00111849
Iteration 47/1000 | Loss: 0.00058056
Iteration 48/1000 | Loss: 0.00016742
Iteration 49/1000 | Loss: 0.00090429
Iteration 50/1000 | Loss: 0.00104812
Iteration 51/1000 | Loss: 0.00057651
Iteration 52/1000 | Loss: 0.00068182
Iteration 53/1000 | Loss: 0.00080046
Iteration 54/1000 | Loss: 0.00075665
Iteration 55/1000 | Loss: 0.00112086
Iteration 56/1000 | Loss: 0.00094096
Iteration 57/1000 | Loss: 0.00075620
Iteration 58/1000 | Loss: 0.00052791
Iteration 59/1000 | Loss: 0.00052356
Iteration 60/1000 | Loss: 0.00045387
Iteration 61/1000 | Loss: 0.00020550
Iteration 62/1000 | Loss: 0.00007398
Iteration 63/1000 | Loss: 0.00006435
Iteration 64/1000 | Loss: 0.00022733
Iteration 65/1000 | Loss: 0.00015760
Iteration 66/1000 | Loss: 0.00020281
Iteration 67/1000 | Loss: 0.00045422
Iteration 68/1000 | Loss: 0.00024187
Iteration 69/1000 | Loss: 0.00077251
Iteration 70/1000 | Loss: 0.00077298
Iteration 71/1000 | Loss: 0.00060412
Iteration 72/1000 | Loss: 0.00031293
Iteration 73/1000 | Loss: 0.00038559
Iteration 74/1000 | Loss: 0.00056350
Iteration 75/1000 | Loss: 0.00027398
Iteration 76/1000 | Loss: 0.00044460
Iteration 77/1000 | Loss: 0.00066827
Iteration 78/1000 | Loss: 0.00005932
Iteration 79/1000 | Loss: 0.00039238
Iteration 80/1000 | Loss: 0.00017329
Iteration 81/1000 | Loss: 0.00031818
Iteration 82/1000 | Loss: 0.00026783
Iteration 83/1000 | Loss: 0.00040183
Iteration 84/1000 | Loss: 0.00043595
Iteration 85/1000 | Loss: 0.00016511
Iteration 86/1000 | Loss: 0.00014457
Iteration 87/1000 | Loss: 0.00014742
Iteration 88/1000 | Loss: 0.00071284
Iteration 89/1000 | Loss: 0.00006698
Iteration 90/1000 | Loss: 0.00017186
Iteration 91/1000 | Loss: 0.00015677
Iteration 92/1000 | Loss: 0.00009265
Iteration 93/1000 | Loss: 0.00105390
Iteration 94/1000 | Loss: 0.00022503
Iteration 95/1000 | Loss: 0.00019530
Iteration 96/1000 | Loss: 0.00004304
Iteration 97/1000 | Loss: 0.00056904
Iteration 98/1000 | Loss: 0.00056222
Iteration 99/1000 | Loss: 0.00031326
Iteration 100/1000 | Loss: 0.00009649
Iteration 101/1000 | Loss: 0.00004185
Iteration 102/1000 | Loss: 0.00003509
Iteration 103/1000 | Loss: 0.00003257
Iteration 104/1000 | Loss: 0.00050771
Iteration 105/1000 | Loss: 0.00057490
Iteration 106/1000 | Loss: 0.00043052
Iteration 107/1000 | Loss: 0.00004409
Iteration 108/1000 | Loss: 0.00057456
Iteration 109/1000 | Loss: 0.00037516
Iteration 110/1000 | Loss: 0.00013083
Iteration 111/1000 | Loss: 0.00004177
Iteration 112/1000 | Loss: 0.00019454
Iteration 113/1000 | Loss: 0.00006238
Iteration 114/1000 | Loss: 0.00003082
Iteration 115/1000 | Loss: 0.00024195
Iteration 116/1000 | Loss: 0.00002776
Iteration 117/1000 | Loss: 0.00002640
Iteration 118/1000 | Loss: 0.00002568
Iteration 119/1000 | Loss: 0.00002520
Iteration 120/1000 | Loss: 0.00002450
Iteration 121/1000 | Loss: 0.00002400
Iteration 122/1000 | Loss: 0.00002371
Iteration 123/1000 | Loss: 0.00002352
Iteration 124/1000 | Loss: 0.00002329
Iteration 125/1000 | Loss: 0.00002314
Iteration 126/1000 | Loss: 0.00002302
Iteration 127/1000 | Loss: 0.00002297
Iteration 128/1000 | Loss: 0.00002295
Iteration 129/1000 | Loss: 0.00002294
Iteration 130/1000 | Loss: 0.00002284
Iteration 131/1000 | Loss: 0.00002281
Iteration 132/1000 | Loss: 0.00002281
Iteration 133/1000 | Loss: 0.00002280
Iteration 134/1000 | Loss: 0.00002280
Iteration 135/1000 | Loss: 0.00002280
Iteration 136/1000 | Loss: 0.00002280
Iteration 137/1000 | Loss: 0.00002280
Iteration 138/1000 | Loss: 0.00002279
Iteration 139/1000 | Loss: 0.00002279
Iteration 140/1000 | Loss: 0.00002279
Iteration 141/1000 | Loss: 0.00002278
Iteration 142/1000 | Loss: 0.00002278
Iteration 143/1000 | Loss: 0.00002277
Iteration 144/1000 | Loss: 0.00002277
Iteration 145/1000 | Loss: 0.00002277
Iteration 146/1000 | Loss: 0.00002276
Iteration 147/1000 | Loss: 0.00002276
Iteration 148/1000 | Loss: 0.00002276
Iteration 149/1000 | Loss: 0.00002275
Iteration 150/1000 | Loss: 0.00002275
Iteration 151/1000 | Loss: 0.00002275
Iteration 152/1000 | Loss: 0.00002275
Iteration 153/1000 | Loss: 0.00002275
Iteration 154/1000 | Loss: 0.00002274
Iteration 155/1000 | Loss: 0.00002274
Iteration 156/1000 | Loss: 0.00002274
Iteration 157/1000 | Loss: 0.00002273
Iteration 158/1000 | Loss: 0.00002273
Iteration 159/1000 | Loss: 0.00002273
Iteration 160/1000 | Loss: 0.00002273
Iteration 161/1000 | Loss: 0.00002273
Iteration 162/1000 | Loss: 0.00002273
Iteration 163/1000 | Loss: 0.00002273
Iteration 164/1000 | Loss: 0.00002272
Iteration 165/1000 | Loss: 0.00002272
Iteration 166/1000 | Loss: 0.00002272
Iteration 167/1000 | Loss: 0.00002271
Iteration 168/1000 | Loss: 0.00002271
Iteration 169/1000 | Loss: 0.00002271
Iteration 170/1000 | Loss: 0.00002270
Iteration 171/1000 | Loss: 0.00002270
Iteration 172/1000 | Loss: 0.00002270
Iteration 173/1000 | Loss: 0.00002270
Iteration 174/1000 | Loss: 0.00002270
Iteration 175/1000 | Loss: 0.00002269
Iteration 176/1000 | Loss: 0.00002269
Iteration 177/1000 | Loss: 0.00002269
Iteration 178/1000 | Loss: 0.00002269
Iteration 179/1000 | Loss: 0.00002269
Iteration 180/1000 | Loss: 0.00002269
Iteration 181/1000 | Loss: 0.00002269
Iteration 182/1000 | Loss: 0.00002269
Iteration 183/1000 | Loss: 0.00002269
Iteration 184/1000 | Loss: 0.00002268
Iteration 185/1000 | Loss: 0.00002268
Iteration 186/1000 | Loss: 0.00002268
Iteration 187/1000 | Loss: 0.00002268
Iteration 188/1000 | Loss: 0.00002268
Iteration 189/1000 | Loss: 0.00002268
Iteration 190/1000 | Loss: 0.00002268
Iteration 191/1000 | Loss: 0.00002268
Iteration 192/1000 | Loss: 0.00002268
Iteration 193/1000 | Loss: 0.00002268
Iteration 194/1000 | Loss: 0.00002268
Iteration 195/1000 | Loss: 0.00002267
Iteration 196/1000 | Loss: 0.00002267
Iteration 197/1000 | Loss: 0.00002267
Iteration 198/1000 | Loss: 0.00002267
Iteration 199/1000 | Loss: 0.00002267
Iteration 200/1000 | Loss: 0.00002267
Iteration 201/1000 | Loss: 0.00002267
Iteration 202/1000 | Loss: 0.00002267
Iteration 203/1000 | Loss: 0.00002267
Iteration 204/1000 | Loss: 0.00002267
Iteration 205/1000 | Loss: 0.00002267
Iteration 206/1000 | Loss: 0.00002267
Iteration 207/1000 | Loss: 0.00002267
Iteration 208/1000 | Loss: 0.00002266
Iteration 209/1000 | Loss: 0.00002266
Iteration 210/1000 | Loss: 0.00002266
Iteration 211/1000 | Loss: 0.00002266
Iteration 212/1000 | Loss: 0.00002266
Iteration 213/1000 | Loss: 0.00002266
Iteration 214/1000 | Loss: 0.00002266
Iteration 215/1000 | Loss: 0.00002265
Iteration 216/1000 | Loss: 0.00002265
Iteration 217/1000 | Loss: 0.00002265
Iteration 218/1000 | Loss: 0.00002265
Iteration 219/1000 | Loss: 0.00002265
Iteration 220/1000 | Loss: 0.00002265
Iteration 221/1000 | Loss: 0.00002265
Iteration 222/1000 | Loss: 0.00002265
Iteration 223/1000 | Loss: 0.00002265
Iteration 224/1000 | Loss: 0.00002265
Iteration 225/1000 | Loss: 0.00002265
Iteration 226/1000 | Loss: 0.00002265
Iteration 227/1000 | Loss: 0.00002264
Iteration 228/1000 | Loss: 0.00002264
Iteration 229/1000 | Loss: 0.00002264
Iteration 230/1000 | Loss: 0.00002264
Iteration 231/1000 | Loss: 0.00002264
Iteration 232/1000 | Loss: 0.00002264
Iteration 233/1000 | Loss: 0.00002264
Iteration 234/1000 | Loss: 0.00002264
Iteration 235/1000 | Loss: 0.00002264
Iteration 236/1000 | Loss: 0.00002264
Iteration 237/1000 | Loss: 0.00002264
Iteration 238/1000 | Loss: 0.00002264
Iteration 239/1000 | Loss: 0.00002264
Iteration 240/1000 | Loss: 0.00002263
Iteration 241/1000 | Loss: 0.00002263
Iteration 242/1000 | Loss: 0.00002263
Iteration 243/1000 | Loss: 0.00002263
Iteration 244/1000 | Loss: 0.00002263
Iteration 245/1000 | Loss: 0.00002263
Iteration 246/1000 | Loss: 0.00002263
Iteration 247/1000 | Loss: 0.00002263
Iteration 248/1000 | Loss: 0.00002263
Iteration 249/1000 | Loss: 0.00002263
Iteration 250/1000 | Loss: 0.00002263
Iteration 251/1000 | Loss: 0.00002263
Iteration 252/1000 | Loss: 0.00002263
Iteration 253/1000 | Loss: 0.00002263
Iteration 254/1000 | Loss: 0.00002262
Iteration 255/1000 | Loss: 0.00002262
Iteration 256/1000 | Loss: 0.00002262
Iteration 257/1000 | Loss: 0.00002262
Iteration 258/1000 | Loss: 0.00002262
Iteration 259/1000 | Loss: 0.00002262
Iteration 260/1000 | Loss: 0.00002262
Iteration 261/1000 | Loss: 0.00002262
Iteration 262/1000 | Loss: 0.00002262
Iteration 263/1000 | Loss: 0.00002262
Iteration 264/1000 | Loss: 0.00002262
Iteration 265/1000 | Loss: 0.00002262
Iteration 266/1000 | Loss: 0.00002262
Iteration 267/1000 | Loss: 0.00002262
Iteration 268/1000 | Loss: 0.00002261
Iteration 269/1000 | Loss: 0.00002261
Iteration 270/1000 | Loss: 0.00002261
Iteration 271/1000 | Loss: 0.00002261
Iteration 272/1000 | Loss: 0.00002261
Iteration 273/1000 | Loss: 0.00002261
Iteration 274/1000 | Loss: 0.00002261
Iteration 275/1000 | Loss: 0.00002261
Iteration 276/1000 | Loss: 0.00002261
Iteration 277/1000 | Loss: 0.00002261
Iteration 278/1000 | Loss: 0.00002261
Iteration 279/1000 | Loss: 0.00002261
Iteration 280/1000 | Loss: 0.00002260
Iteration 281/1000 | Loss: 0.00002260
Iteration 282/1000 | Loss: 0.00002260
Iteration 283/1000 | Loss: 0.00002260
Iteration 284/1000 | Loss: 0.00002260
Iteration 285/1000 | Loss: 0.00002260
Iteration 286/1000 | Loss: 0.00002260
Iteration 287/1000 | Loss: 0.00002260
Iteration 288/1000 | Loss: 0.00002260
Iteration 289/1000 | Loss: 0.00002260
Iteration 290/1000 | Loss: 0.00002260
Iteration 291/1000 | Loss: 0.00002260
Iteration 292/1000 | Loss: 0.00002260
Iteration 293/1000 | Loss: 0.00002260
Iteration 294/1000 | Loss: 0.00002260
Iteration 295/1000 | Loss: 0.00002260
Iteration 296/1000 | Loss: 0.00002259
Iteration 297/1000 | Loss: 0.00002259
Iteration 298/1000 | Loss: 0.00002259
Iteration 299/1000 | Loss: 0.00002259
Iteration 300/1000 | Loss: 0.00002259
Iteration 301/1000 | Loss: 0.00002259
Iteration 302/1000 | Loss: 0.00002259
Iteration 303/1000 | Loss: 0.00002259
Iteration 304/1000 | Loss: 0.00002259
Iteration 305/1000 | Loss: 0.00002259
Iteration 306/1000 | Loss: 0.00002259
Iteration 307/1000 | Loss: 0.00002259
Iteration 308/1000 | Loss: 0.00002258
Iteration 309/1000 | Loss: 0.00002258
Iteration 310/1000 | Loss: 0.00002258
Iteration 311/1000 | Loss: 0.00002258
Iteration 312/1000 | Loss: 0.00002258
Iteration 313/1000 | Loss: 0.00002258
Iteration 314/1000 | Loss: 0.00002258
Iteration 315/1000 | Loss: 0.00002258
Iteration 316/1000 | Loss: 0.00002258
Iteration 317/1000 | Loss: 0.00002258
Iteration 318/1000 | Loss: 0.00002258
Iteration 319/1000 | Loss: 0.00002258
Iteration 320/1000 | Loss: 0.00002258
Iteration 321/1000 | Loss: 0.00002258
Iteration 322/1000 | Loss: 0.00002258
Iteration 323/1000 | Loss: 0.00002258
Iteration 324/1000 | Loss: 0.00002258
Iteration 325/1000 | Loss: 0.00002258
Iteration 326/1000 | Loss: 0.00002258
Iteration 327/1000 | Loss: 0.00002258
Iteration 328/1000 | Loss: 0.00002258
Iteration 329/1000 | Loss: 0.00002258
Iteration 330/1000 | Loss: 0.00002258
Iteration 331/1000 | Loss: 0.00002258
Iteration 332/1000 | Loss: 0.00002258
Iteration 333/1000 | Loss: 0.00002258
Iteration 334/1000 | Loss: 0.00002258
Iteration 335/1000 | Loss: 0.00002258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [2.2575281036552042e-05, 2.2575281036552042e-05, 2.2575281036552042e-05, 2.2575281036552042e-05, 2.2575281036552042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2575281036552042e-05

Optimization complete. Final v2v error: 3.210332155227661 mm

Highest mean error: 19.599822998046875 mm for frame 29

Lowest mean error: 2.2164149284362793 mm for frame 168

Saving results

Total time: 255.96329188346863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867362
Iteration 2/25 | Loss: 0.00129302
Iteration 3/25 | Loss: 0.00105896
Iteration 4/25 | Loss: 0.00103653
Iteration 5/25 | Loss: 0.00103085
Iteration 6/25 | Loss: 0.00103020
Iteration 7/25 | Loss: 0.00103020
Iteration 8/25 | Loss: 0.00103020
Iteration 9/25 | Loss: 0.00103020
Iteration 10/25 | Loss: 0.00103020
Iteration 11/25 | Loss: 0.00103020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010302027221769094, 0.0010302027221769094, 0.0010302027221769094, 0.0010302027221769094, 0.0010302027221769094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010302027221769094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79627204
Iteration 2/25 | Loss: 0.00092848
Iteration 3/25 | Loss: 0.00092848
Iteration 4/25 | Loss: 0.00092848
Iteration 5/25 | Loss: 0.00092848
Iteration 6/25 | Loss: 0.00092848
Iteration 7/25 | Loss: 0.00092848
Iteration 8/25 | Loss: 0.00092848
Iteration 9/25 | Loss: 0.00092848
Iteration 10/25 | Loss: 0.00092848
Iteration 11/25 | Loss: 0.00092847
Iteration 12/25 | Loss: 0.00092847
Iteration 13/25 | Loss: 0.00092847
Iteration 14/25 | Loss: 0.00092847
Iteration 15/25 | Loss: 0.00092847
Iteration 16/25 | Loss: 0.00092847
Iteration 17/25 | Loss: 0.00092847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009284747648052871, 0.0009284747648052871, 0.0009284747648052871, 0.0009284747648052871, 0.0009284747648052871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009284747648052871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092847
Iteration 2/1000 | Loss: 0.00003704
Iteration 3/1000 | Loss: 0.00002940
Iteration 4/1000 | Loss: 0.00002663
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002375
Iteration 8/1000 | Loss: 0.00002354
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002316
Iteration 11/1000 | Loss: 0.00002309
Iteration 12/1000 | Loss: 0.00002293
Iteration 13/1000 | Loss: 0.00002288
Iteration 14/1000 | Loss: 0.00002287
Iteration 15/1000 | Loss: 0.00002287
Iteration 16/1000 | Loss: 0.00002287
Iteration 17/1000 | Loss: 0.00002286
Iteration 18/1000 | Loss: 0.00002286
Iteration 19/1000 | Loss: 0.00002286
Iteration 20/1000 | Loss: 0.00002286
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00002286
Iteration 23/1000 | Loss: 0.00002286
Iteration 24/1000 | Loss: 0.00002286
Iteration 25/1000 | Loss: 0.00002285
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00002285
Iteration 28/1000 | Loss: 0.00002285
Iteration 29/1000 | Loss: 0.00002285
Iteration 30/1000 | Loss: 0.00002285
Iteration 31/1000 | Loss: 0.00002285
Iteration 32/1000 | Loss: 0.00002285
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002281
Iteration 39/1000 | Loss: 0.00002281
Iteration 40/1000 | Loss: 0.00002281
Iteration 41/1000 | Loss: 0.00002281
Iteration 42/1000 | Loss: 0.00002281
Iteration 43/1000 | Loss: 0.00002281
Iteration 44/1000 | Loss: 0.00002281
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002280
Iteration 48/1000 | Loss: 0.00002280
Iteration 49/1000 | Loss: 0.00002280
Iteration 50/1000 | Loss: 0.00002280
Iteration 51/1000 | Loss: 0.00002280
Iteration 52/1000 | Loss: 0.00002280
Iteration 53/1000 | Loss: 0.00002280
Iteration 54/1000 | Loss: 0.00002280
Iteration 55/1000 | Loss: 0.00002280
Iteration 56/1000 | Loss: 0.00002280
Iteration 57/1000 | Loss: 0.00002280
Iteration 58/1000 | Loss: 0.00002279
Iteration 59/1000 | Loss: 0.00002279
Iteration 60/1000 | Loss: 0.00002279
Iteration 61/1000 | Loss: 0.00002279
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002279
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002279
Iteration 67/1000 | Loss: 0.00002279
Iteration 68/1000 | Loss: 0.00002279
Iteration 69/1000 | Loss: 0.00002278
Iteration 70/1000 | Loss: 0.00002278
Iteration 71/1000 | Loss: 0.00002278
Iteration 72/1000 | Loss: 0.00002278
Iteration 73/1000 | Loss: 0.00002278
Iteration 74/1000 | Loss: 0.00002278
Iteration 75/1000 | Loss: 0.00002278
Iteration 76/1000 | Loss: 0.00002278
Iteration 77/1000 | Loss: 0.00002278
Iteration 78/1000 | Loss: 0.00002277
Iteration 79/1000 | Loss: 0.00002277
Iteration 80/1000 | Loss: 0.00002277
Iteration 81/1000 | Loss: 0.00002277
Iteration 82/1000 | Loss: 0.00002277
Iteration 83/1000 | Loss: 0.00002277
Iteration 84/1000 | Loss: 0.00002277
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00002277
Iteration 87/1000 | Loss: 0.00002277
Iteration 88/1000 | Loss: 0.00002277
Iteration 89/1000 | Loss: 0.00002277
Iteration 90/1000 | Loss: 0.00002277
Iteration 91/1000 | Loss: 0.00002277
Iteration 92/1000 | Loss: 0.00002276
Iteration 93/1000 | Loss: 0.00002276
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00002276
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002276
Iteration 100/1000 | Loss: 0.00002276
Iteration 101/1000 | Loss: 0.00002276
Iteration 102/1000 | Loss: 0.00002276
Iteration 103/1000 | Loss: 0.00002276
Iteration 104/1000 | Loss: 0.00002276
Iteration 105/1000 | Loss: 0.00002276
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.2760445062885992e-05, 2.2760445062885992e-05, 2.2760445062885992e-05, 2.2760445062885992e-05, 2.2760445062885992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2760445062885992e-05

Optimization complete. Final v2v error: 3.9795496463775635 mm

Highest mean error: 4.06292724609375 mm for frame 14

Lowest mean error: 3.8737435340881348 mm for frame 177

Saving results

Total time: 30.602843523025513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034717
Iteration 2/25 | Loss: 0.01034717
Iteration 3/25 | Loss: 0.00241968
Iteration 4/25 | Loss: 0.00178274
Iteration 5/25 | Loss: 0.00162885
Iteration 6/25 | Loss: 0.00155188
Iteration 7/25 | Loss: 0.00146349
Iteration 8/25 | Loss: 0.00144531
Iteration 9/25 | Loss: 0.00138288
Iteration 10/25 | Loss: 0.00135694
Iteration 11/25 | Loss: 0.00131547
Iteration 12/25 | Loss: 0.00129358
Iteration 13/25 | Loss: 0.00128603
Iteration 14/25 | Loss: 0.00127744
Iteration 15/25 | Loss: 0.00126900
Iteration 16/25 | Loss: 0.00126400
Iteration 17/25 | Loss: 0.00126153
Iteration 18/25 | Loss: 0.00125589
Iteration 19/25 | Loss: 0.00125506
Iteration 20/25 | Loss: 0.00125442
Iteration 21/25 | Loss: 0.00125632
Iteration 22/25 | Loss: 0.00125262
Iteration 23/25 | Loss: 0.00125216
Iteration 24/25 | Loss: 0.00125206
Iteration 25/25 | Loss: 0.00125205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17105794
Iteration 2/25 | Loss: 0.00329967
Iteration 3/25 | Loss: 0.00329967
Iteration 4/25 | Loss: 0.00329967
Iteration 5/25 | Loss: 0.00329967
Iteration 6/25 | Loss: 0.00329967
Iteration 7/25 | Loss: 0.00329967
Iteration 8/25 | Loss: 0.00329967
Iteration 9/25 | Loss: 0.00329967
Iteration 10/25 | Loss: 0.00329967
Iteration 11/25 | Loss: 0.00329967
Iteration 12/25 | Loss: 0.00329967
Iteration 13/25 | Loss: 0.00329967
Iteration 14/25 | Loss: 0.00329967
Iteration 15/25 | Loss: 0.00329967
Iteration 16/25 | Loss: 0.00329967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003299666102975607, 0.003299666102975607, 0.003299666102975607, 0.003299666102975607, 0.003299666102975607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003299666102975607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329967
Iteration 2/1000 | Loss: 0.00029030
Iteration 3/1000 | Loss: 0.00033123
Iteration 4/1000 | Loss: 0.00021266
Iteration 5/1000 | Loss: 0.00015969
Iteration 6/1000 | Loss: 0.00014210
Iteration 7/1000 | Loss: 0.00013089
Iteration 8/1000 | Loss: 0.00012122
Iteration 9/1000 | Loss: 0.00011470
Iteration 10/1000 | Loss: 0.00011044
Iteration 11/1000 | Loss: 0.00010749
Iteration 12/1000 | Loss: 0.00010523
Iteration 13/1000 | Loss: 0.00010337
Iteration 14/1000 | Loss: 0.00010218
Iteration 15/1000 | Loss: 0.00010102
Iteration 16/1000 | Loss: 0.00009995
Iteration 17/1000 | Loss: 0.00009914
Iteration 18/1000 | Loss: 0.00009827
Iteration 19/1000 | Loss: 0.00009770
Iteration 20/1000 | Loss: 0.00009709
Iteration 21/1000 | Loss: 0.00009664
Iteration 22/1000 | Loss: 0.00009622
Iteration 23/1000 | Loss: 0.00009579
Iteration 24/1000 | Loss: 0.00009547
Iteration 25/1000 | Loss: 0.00009520
Iteration 26/1000 | Loss: 0.00009498
Iteration 27/1000 | Loss: 0.00037666
Iteration 28/1000 | Loss: 0.00251044
Iteration 29/1000 | Loss: 0.00137304
Iteration 30/1000 | Loss: 0.00023781
Iteration 31/1000 | Loss: 0.00015941
Iteration 32/1000 | Loss: 0.00012844
Iteration 33/1000 | Loss: 0.00008442
Iteration 34/1000 | Loss: 0.00006034
Iteration 35/1000 | Loss: 0.00004787
Iteration 36/1000 | Loss: 0.00004098
Iteration 37/1000 | Loss: 0.00003580
Iteration 38/1000 | Loss: 0.00003182
Iteration 39/1000 | Loss: 0.00003002
Iteration 40/1000 | Loss: 0.00002881
Iteration 41/1000 | Loss: 0.00002723
Iteration 42/1000 | Loss: 0.00002644
Iteration 43/1000 | Loss: 0.00002587
Iteration 44/1000 | Loss: 0.00002531
Iteration 45/1000 | Loss: 0.00002505
Iteration 46/1000 | Loss: 0.00002487
Iteration 47/1000 | Loss: 0.00002477
Iteration 48/1000 | Loss: 0.00002469
Iteration 49/1000 | Loss: 0.00002468
Iteration 50/1000 | Loss: 0.00002468
Iteration 51/1000 | Loss: 0.00002468
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002467
Iteration 54/1000 | Loss: 0.00002467
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002467
Iteration 57/1000 | Loss: 0.00002467
Iteration 58/1000 | Loss: 0.00002466
Iteration 59/1000 | Loss: 0.00002466
Iteration 60/1000 | Loss: 0.00002465
Iteration 61/1000 | Loss: 0.00002465
Iteration 62/1000 | Loss: 0.00002465
Iteration 63/1000 | Loss: 0.00002465
Iteration 64/1000 | Loss: 0.00002465
Iteration 65/1000 | Loss: 0.00002465
Iteration 66/1000 | Loss: 0.00002465
Iteration 67/1000 | Loss: 0.00002465
Iteration 68/1000 | Loss: 0.00002465
Iteration 69/1000 | Loss: 0.00002465
Iteration 70/1000 | Loss: 0.00002465
Iteration 71/1000 | Loss: 0.00002464
Iteration 72/1000 | Loss: 0.00002464
Iteration 73/1000 | Loss: 0.00002464
Iteration 74/1000 | Loss: 0.00002464
Iteration 75/1000 | Loss: 0.00002464
Iteration 76/1000 | Loss: 0.00002464
Iteration 77/1000 | Loss: 0.00002463
Iteration 78/1000 | Loss: 0.00002463
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002462
Iteration 81/1000 | Loss: 0.00002462
Iteration 82/1000 | Loss: 0.00002462
Iteration 83/1000 | Loss: 0.00002461
Iteration 84/1000 | Loss: 0.00002461
Iteration 85/1000 | Loss: 0.00002461
Iteration 86/1000 | Loss: 0.00002461
Iteration 87/1000 | Loss: 0.00002461
Iteration 88/1000 | Loss: 0.00002461
Iteration 89/1000 | Loss: 0.00002461
Iteration 90/1000 | Loss: 0.00002461
Iteration 91/1000 | Loss: 0.00002461
Iteration 92/1000 | Loss: 0.00002461
Iteration 93/1000 | Loss: 0.00002461
Iteration 94/1000 | Loss: 0.00002461
Iteration 95/1000 | Loss: 0.00002461
Iteration 96/1000 | Loss: 0.00002461
Iteration 97/1000 | Loss: 0.00002461
Iteration 98/1000 | Loss: 0.00002461
Iteration 99/1000 | Loss: 0.00002461
Iteration 100/1000 | Loss: 0.00002461
Iteration 101/1000 | Loss: 0.00002461
Iteration 102/1000 | Loss: 0.00002461
Iteration 103/1000 | Loss: 0.00002461
Iteration 104/1000 | Loss: 0.00002461
Iteration 105/1000 | Loss: 0.00002461
Iteration 106/1000 | Loss: 0.00002461
Iteration 107/1000 | Loss: 0.00002461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.4611479602754116e-05, 2.4611479602754116e-05, 2.4611479602754116e-05, 2.4611479602754116e-05, 2.4611479602754116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4611479602754116e-05

Optimization complete. Final v2v error: 3.0629022121429443 mm

Highest mean error: 11.054311752319336 mm for frame 196

Lowest mean error: 2.5305161476135254 mm for frame 111

Saving results

Total time: 125.5147294998169
