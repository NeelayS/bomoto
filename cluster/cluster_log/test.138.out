Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=138, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7728-7783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00343201
Iteration 2/25 | Loss: 0.00115794
Iteration 3/25 | Loss: 0.00100216
Iteration 4/25 | Loss: 0.00098795
Iteration 5/25 | Loss: 0.00098528
Iteration 6/25 | Loss: 0.00098418
Iteration 7/25 | Loss: 0.00098418
Iteration 8/25 | Loss: 0.00098418
Iteration 9/25 | Loss: 0.00098418
Iteration 10/25 | Loss: 0.00098418
Iteration 11/25 | Loss: 0.00098418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009841837454587221, 0.0009841837454587221, 0.0009841837454587221, 0.0009841837454587221, 0.0009841837454587221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009841837454587221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15202820
Iteration 2/25 | Loss: 0.00276356
Iteration 3/25 | Loss: 0.00276356
Iteration 4/25 | Loss: 0.00276356
Iteration 5/25 | Loss: 0.00276356
Iteration 6/25 | Loss: 0.00276356
Iteration 7/25 | Loss: 0.00276356
Iteration 8/25 | Loss: 0.00276356
Iteration 9/25 | Loss: 0.00276355
Iteration 10/25 | Loss: 0.00276355
Iteration 11/25 | Loss: 0.00276355
Iteration 12/25 | Loss: 0.00276355
Iteration 13/25 | Loss: 0.00276355
Iteration 14/25 | Loss: 0.00276355
Iteration 15/25 | Loss: 0.00276355
Iteration 16/25 | Loss: 0.00276355
Iteration 17/25 | Loss: 0.00276355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0027635544538497925, 0.0027635544538497925, 0.0027635544538497925, 0.0027635544538497925, 0.0027635544538497925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027635544538497925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276355
Iteration 2/1000 | Loss: 0.00004140
Iteration 3/1000 | Loss: 0.00002004
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00001097
Iteration 6/1000 | Loss: 0.00001008
Iteration 7/1000 | Loss: 0.00000948
Iteration 8/1000 | Loss: 0.00000921
Iteration 9/1000 | Loss: 0.00000910
Iteration 10/1000 | Loss: 0.00000898
Iteration 11/1000 | Loss: 0.00000897
Iteration 12/1000 | Loss: 0.00000896
Iteration 13/1000 | Loss: 0.00000895
Iteration 14/1000 | Loss: 0.00000886
Iteration 15/1000 | Loss: 0.00000869
Iteration 16/1000 | Loss: 0.00000864
Iteration 17/1000 | Loss: 0.00000854
Iteration 18/1000 | Loss: 0.00000843
Iteration 19/1000 | Loss: 0.00000843
Iteration 20/1000 | Loss: 0.00000843
Iteration 21/1000 | Loss: 0.00000842
Iteration 22/1000 | Loss: 0.00000841
Iteration 23/1000 | Loss: 0.00000841
Iteration 24/1000 | Loss: 0.00000841
Iteration 25/1000 | Loss: 0.00000840
Iteration 26/1000 | Loss: 0.00000840
Iteration 27/1000 | Loss: 0.00000840
Iteration 28/1000 | Loss: 0.00000839
Iteration 29/1000 | Loss: 0.00000839
Iteration 30/1000 | Loss: 0.00000838
Iteration 31/1000 | Loss: 0.00000838
Iteration 32/1000 | Loss: 0.00000838
Iteration 33/1000 | Loss: 0.00000837
Iteration 34/1000 | Loss: 0.00000836
Iteration 35/1000 | Loss: 0.00000836
Iteration 36/1000 | Loss: 0.00000836
Iteration 37/1000 | Loss: 0.00000836
Iteration 38/1000 | Loss: 0.00000835
Iteration 39/1000 | Loss: 0.00000835
Iteration 40/1000 | Loss: 0.00000833
Iteration 41/1000 | Loss: 0.00000833
Iteration 42/1000 | Loss: 0.00000832
Iteration 43/1000 | Loss: 0.00000831
Iteration 44/1000 | Loss: 0.00000831
Iteration 45/1000 | Loss: 0.00000830
Iteration 46/1000 | Loss: 0.00000830
Iteration 47/1000 | Loss: 0.00000829
Iteration 48/1000 | Loss: 0.00000829
Iteration 49/1000 | Loss: 0.00000829
Iteration 50/1000 | Loss: 0.00000828
Iteration 51/1000 | Loss: 0.00000828
Iteration 52/1000 | Loss: 0.00000827
Iteration 53/1000 | Loss: 0.00000827
Iteration 54/1000 | Loss: 0.00000827
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000827
Iteration 57/1000 | Loss: 0.00000826
Iteration 58/1000 | Loss: 0.00000826
Iteration 59/1000 | Loss: 0.00000826
Iteration 60/1000 | Loss: 0.00000825
Iteration 61/1000 | Loss: 0.00000825
Iteration 62/1000 | Loss: 0.00000825
Iteration 63/1000 | Loss: 0.00000825
Iteration 64/1000 | Loss: 0.00000825
Iteration 65/1000 | Loss: 0.00000825
Iteration 66/1000 | Loss: 0.00000824
Iteration 67/1000 | Loss: 0.00000824
Iteration 68/1000 | Loss: 0.00000823
Iteration 69/1000 | Loss: 0.00000823
Iteration 70/1000 | Loss: 0.00000823
Iteration 71/1000 | Loss: 0.00000823
Iteration 72/1000 | Loss: 0.00000822
Iteration 73/1000 | Loss: 0.00000822
Iteration 74/1000 | Loss: 0.00000822
Iteration 75/1000 | Loss: 0.00000822
Iteration 76/1000 | Loss: 0.00000822
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000822
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000821
Iteration 82/1000 | Loss: 0.00000821
Iteration 83/1000 | Loss: 0.00000821
Iteration 84/1000 | Loss: 0.00000820
Iteration 85/1000 | Loss: 0.00000820
Iteration 86/1000 | Loss: 0.00000820
Iteration 87/1000 | Loss: 0.00000820
Iteration 88/1000 | Loss: 0.00000820
Iteration 89/1000 | Loss: 0.00000820
Iteration 90/1000 | Loss: 0.00000820
Iteration 91/1000 | Loss: 0.00000820
Iteration 92/1000 | Loss: 0.00000820
Iteration 93/1000 | Loss: 0.00000820
Iteration 94/1000 | Loss: 0.00000819
Iteration 95/1000 | Loss: 0.00000819
Iteration 96/1000 | Loss: 0.00000819
Iteration 97/1000 | Loss: 0.00000819
Iteration 98/1000 | Loss: 0.00000819
Iteration 99/1000 | Loss: 0.00000819
Iteration 100/1000 | Loss: 0.00000819
Iteration 101/1000 | Loss: 0.00000818
Iteration 102/1000 | Loss: 0.00000818
Iteration 103/1000 | Loss: 0.00000818
Iteration 104/1000 | Loss: 0.00000818
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000817
Iteration 108/1000 | Loss: 0.00000817
Iteration 109/1000 | Loss: 0.00000817
Iteration 110/1000 | Loss: 0.00000817
Iteration 111/1000 | Loss: 0.00000817
Iteration 112/1000 | Loss: 0.00000817
Iteration 113/1000 | Loss: 0.00000817
Iteration 114/1000 | Loss: 0.00000817
Iteration 115/1000 | Loss: 0.00000817
Iteration 116/1000 | Loss: 0.00000817
Iteration 117/1000 | Loss: 0.00000816
Iteration 118/1000 | Loss: 0.00000816
Iteration 119/1000 | Loss: 0.00000816
Iteration 120/1000 | Loss: 0.00000816
Iteration 121/1000 | Loss: 0.00000816
Iteration 122/1000 | Loss: 0.00000816
Iteration 123/1000 | Loss: 0.00000816
Iteration 124/1000 | Loss: 0.00000816
Iteration 125/1000 | Loss: 0.00000816
Iteration 126/1000 | Loss: 0.00000816
Iteration 127/1000 | Loss: 0.00000816
Iteration 128/1000 | Loss: 0.00000815
Iteration 129/1000 | Loss: 0.00000815
Iteration 130/1000 | Loss: 0.00000815
Iteration 131/1000 | Loss: 0.00000815
Iteration 132/1000 | Loss: 0.00000815
Iteration 133/1000 | Loss: 0.00000815
Iteration 134/1000 | Loss: 0.00000814
Iteration 135/1000 | Loss: 0.00000814
Iteration 136/1000 | Loss: 0.00000814
Iteration 137/1000 | Loss: 0.00000813
Iteration 138/1000 | Loss: 0.00000813
Iteration 139/1000 | Loss: 0.00000813
Iteration 140/1000 | Loss: 0.00000813
Iteration 141/1000 | Loss: 0.00000813
Iteration 142/1000 | Loss: 0.00000813
Iteration 143/1000 | Loss: 0.00000813
Iteration 144/1000 | Loss: 0.00000813
Iteration 145/1000 | Loss: 0.00000812
Iteration 146/1000 | Loss: 0.00000812
Iteration 147/1000 | Loss: 0.00000812
Iteration 148/1000 | Loss: 0.00000812
Iteration 149/1000 | Loss: 0.00000812
Iteration 150/1000 | Loss: 0.00000812
Iteration 151/1000 | Loss: 0.00000812
Iteration 152/1000 | Loss: 0.00000812
Iteration 153/1000 | Loss: 0.00000812
Iteration 154/1000 | Loss: 0.00000811
Iteration 155/1000 | Loss: 0.00000811
Iteration 156/1000 | Loss: 0.00000811
Iteration 157/1000 | Loss: 0.00000811
Iteration 158/1000 | Loss: 0.00000811
Iteration 159/1000 | Loss: 0.00000811
Iteration 160/1000 | Loss: 0.00000811
Iteration 161/1000 | Loss: 0.00000811
Iteration 162/1000 | Loss: 0.00000811
Iteration 163/1000 | Loss: 0.00000811
Iteration 164/1000 | Loss: 0.00000811
Iteration 165/1000 | Loss: 0.00000811
Iteration 166/1000 | Loss: 0.00000811
Iteration 167/1000 | Loss: 0.00000811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [8.106260793283582e-06, 8.106260793283582e-06, 8.106260793283582e-06, 8.106260793283582e-06, 8.106260793283582e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.106260793283582e-06

Optimization complete. Final v2v error: 2.4085659980773926 mm

Highest mean error: 2.7698817253112793 mm for frame 73

Lowest mean error: 2.197166919708252 mm for frame 150

Saving results

Total time: 39.82155442237854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599397
Iteration 2/25 | Loss: 0.00132136
Iteration 3/25 | Loss: 0.00117326
Iteration 4/25 | Loss: 0.00098495
Iteration 5/25 | Loss: 0.00097664
Iteration 6/25 | Loss: 0.00097673
Iteration 7/25 | Loss: 0.00096715
Iteration 8/25 | Loss: 0.00096367
Iteration 9/25 | Loss: 0.00096193
Iteration 10/25 | Loss: 0.00096076
Iteration 11/25 | Loss: 0.00096002
Iteration 12/25 | Loss: 0.00095967
Iteration 13/25 | Loss: 0.00095947
Iteration 14/25 | Loss: 0.00096225
Iteration 15/25 | Loss: 0.00095747
Iteration 16/25 | Loss: 0.00095651
Iteration 17/25 | Loss: 0.00095605
Iteration 18/25 | Loss: 0.00095598
Iteration 19/25 | Loss: 0.00095597
Iteration 20/25 | Loss: 0.00095596
Iteration 21/25 | Loss: 0.00095596
Iteration 22/25 | Loss: 0.00095596
Iteration 23/25 | Loss: 0.00095596
Iteration 24/25 | Loss: 0.00095596
Iteration 25/25 | Loss: 0.00095596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04805636
Iteration 2/25 | Loss: 0.00186871
Iteration 3/25 | Loss: 0.00186870
Iteration 4/25 | Loss: 0.00186870
Iteration 5/25 | Loss: 0.00186870
Iteration 6/25 | Loss: 0.00186870
Iteration 7/25 | Loss: 0.00186870
Iteration 8/25 | Loss: 0.00186870
Iteration 9/25 | Loss: 0.00186870
Iteration 10/25 | Loss: 0.00186870
Iteration 11/25 | Loss: 0.00186870
Iteration 12/25 | Loss: 0.00186870
Iteration 13/25 | Loss: 0.00186870
Iteration 14/25 | Loss: 0.00186870
Iteration 15/25 | Loss: 0.00186870
Iteration 16/25 | Loss: 0.00186870
Iteration 17/25 | Loss: 0.00186870
Iteration 18/25 | Loss: 0.00186870
Iteration 19/25 | Loss: 0.00186870
Iteration 20/25 | Loss: 0.00186870
Iteration 21/25 | Loss: 0.00186870
Iteration 22/25 | Loss: 0.00186870
Iteration 23/25 | Loss: 0.00186870
Iteration 24/25 | Loss: 0.00186870
Iteration 25/25 | Loss: 0.00186870

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186870
Iteration 2/1000 | Loss: 0.00002284
Iteration 3/1000 | Loss: 0.00001202
Iteration 4/1000 | Loss: 0.00001039
Iteration 5/1000 | Loss: 0.00000957
Iteration 6/1000 | Loss: 0.00000902
Iteration 7/1000 | Loss: 0.00000861
Iteration 8/1000 | Loss: 0.00000828
Iteration 9/1000 | Loss: 0.00000826
Iteration 10/1000 | Loss: 0.00000814
Iteration 11/1000 | Loss: 0.00000804
Iteration 12/1000 | Loss: 0.00000802
Iteration 13/1000 | Loss: 0.00000793
Iteration 14/1000 | Loss: 0.00000787
Iteration 15/1000 | Loss: 0.00000783
Iteration 16/1000 | Loss: 0.00000782
Iteration 17/1000 | Loss: 0.00000779
Iteration 18/1000 | Loss: 0.00000778
Iteration 19/1000 | Loss: 0.00000777
Iteration 20/1000 | Loss: 0.00000776
Iteration 21/1000 | Loss: 0.00000775
Iteration 22/1000 | Loss: 0.00000775
Iteration 23/1000 | Loss: 0.00000775
Iteration 24/1000 | Loss: 0.00000775
Iteration 25/1000 | Loss: 0.00000774
Iteration 26/1000 | Loss: 0.00000774
Iteration 27/1000 | Loss: 0.00000773
Iteration 28/1000 | Loss: 0.00000773
Iteration 29/1000 | Loss: 0.00000772
Iteration 30/1000 | Loss: 0.00000772
Iteration 31/1000 | Loss: 0.00000771
Iteration 32/1000 | Loss: 0.00000770
Iteration 33/1000 | Loss: 0.00000770
Iteration 34/1000 | Loss: 0.00000770
Iteration 35/1000 | Loss: 0.00000770
Iteration 36/1000 | Loss: 0.00000769
Iteration 37/1000 | Loss: 0.00000765
Iteration 38/1000 | Loss: 0.00000765
Iteration 39/1000 | Loss: 0.00000765
Iteration 40/1000 | Loss: 0.00000765
Iteration 41/1000 | Loss: 0.00000764
Iteration 42/1000 | Loss: 0.00000760
Iteration 43/1000 | Loss: 0.00000760
Iteration 44/1000 | Loss: 0.00000760
Iteration 45/1000 | Loss: 0.00000760
Iteration 46/1000 | Loss: 0.00000759
Iteration 47/1000 | Loss: 0.00000759
Iteration 48/1000 | Loss: 0.00000759
Iteration 49/1000 | Loss: 0.00000759
Iteration 50/1000 | Loss: 0.00000758
Iteration 51/1000 | Loss: 0.00000758
Iteration 52/1000 | Loss: 0.00000758
Iteration 53/1000 | Loss: 0.00000758
Iteration 54/1000 | Loss: 0.00000757
Iteration 55/1000 | Loss: 0.00000757
Iteration 56/1000 | Loss: 0.00000757
Iteration 57/1000 | Loss: 0.00000757
Iteration 58/1000 | Loss: 0.00000757
Iteration 59/1000 | Loss: 0.00000757
Iteration 60/1000 | Loss: 0.00000757
Iteration 61/1000 | Loss: 0.00000757
Iteration 62/1000 | Loss: 0.00000757
Iteration 63/1000 | Loss: 0.00000757
Iteration 64/1000 | Loss: 0.00000757
Iteration 65/1000 | Loss: 0.00000757
Iteration 66/1000 | Loss: 0.00000757
Iteration 67/1000 | Loss: 0.00000756
Iteration 68/1000 | Loss: 0.00000756
Iteration 69/1000 | Loss: 0.00000756
Iteration 70/1000 | Loss: 0.00000756
Iteration 71/1000 | Loss: 0.00000756
Iteration 72/1000 | Loss: 0.00000756
Iteration 73/1000 | Loss: 0.00000756
Iteration 74/1000 | Loss: 0.00000755
Iteration 75/1000 | Loss: 0.00000755
Iteration 76/1000 | Loss: 0.00000755
Iteration 77/1000 | Loss: 0.00000755
Iteration 78/1000 | Loss: 0.00000754
Iteration 79/1000 | Loss: 0.00000754
Iteration 80/1000 | Loss: 0.00000754
Iteration 81/1000 | Loss: 0.00000754
Iteration 82/1000 | Loss: 0.00000754
Iteration 83/1000 | Loss: 0.00000754
Iteration 84/1000 | Loss: 0.00000754
Iteration 85/1000 | Loss: 0.00000754
Iteration 86/1000 | Loss: 0.00000753
Iteration 87/1000 | Loss: 0.00000753
Iteration 88/1000 | Loss: 0.00000753
Iteration 89/1000 | Loss: 0.00000753
Iteration 90/1000 | Loss: 0.00000752
Iteration 91/1000 | Loss: 0.00000752
Iteration 92/1000 | Loss: 0.00000752
Iteration 93/1000 | Loss: 0.00000752
Iteration 94/1000 | Loss: 0.00000752
Iteration 95/1000 | Loss: 0.00000752
Iteration 96/1000 | Loss: 0.00000752
Iteration 97/1000 | Loss: 0.00000752
Iteration 98/1000 | Loss: 0.00000752
Iteration 99/1000 | Loss: 0.00000752
Iteration 100/1000 | Loss: 0.00000752
Iteration 101/1000 | Loss: 0.00000752
Iteration 102/1000 | Loss: 0.00000752
Iteration 103/1000 | Loss: 0.00000752
Iteration 104/1000 | Loss: 0.00000752
Iteration 105/1000 | Loss: 0.00000752
Iteration 106/1000 | Loss: 0.00000752
Iteration 107/1000 | Loss: 0.00000752
Iteration 108/1000 | Loss: 0.00000752
Iteration 109/1000 | Loss: 0.00000752
Iteration 110/1000 | Loss: 0.00000752
Iteration 111/1000 | Loss: 0.00000752
Iteration 112/1000 | Loss: 0.00000752
Iteration 113/1000 | Loss: 0.00000752
Iteration 114/1000 | Loss: 0.00000752
Iteration 115/1000 | Loss: 0.00000752
Iteration 116/1000 | Loss: 0.00000752
Iteration 117/1000 | Loss: 0.00000752
Iteration 118/1000 | Loss: 0.00000752
Iteration 119/1000 | Loss: 0.00000752
Iteration 120/1000 | Loss: 0.00000752
Iteration 121/1000 | Loss: 0.00000752
Iteration 122/1000 | Loss: 0.00000752
Iteration 123/1000 | Loss: 0.00000752
Iteration 124/1000 | Loss: 0.00000752
Iteration 125/1000 | Loss: 0.00000752
Iteration 126/1000 | Loss: 0.00000752
Iteration 127/1000 | Loss: 0.00000752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [7.521832230850123e-06, 7.521832230850123e-06, 7.521832230850123e-06, 7.521832230850123e-06, 7.521832230850123e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.521832230850123e-06

Optimization complete. Final v2v error: 2.2973153591156006 mm

Highest mean error: 2.645787477493286 mm for frame 72

Lowest mean error: 1.9931622743606567 mm for frame 128

Saving results

Total time: 61.64764380455017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080054
Iteration 2/25 | Loss: 0.00370171
Iteration 3/25 | Loss: 0.00240346
Iteration 4/25 | Loss: 0.00198649
Iteration 5/25 | Loss: 0.00182509
Iteration 6/25 | Loss: 0.00169042
Iteration 7/25 | Loss: 0.00161811
Iteration 8/25 | Loss: 0.00154369
Iteration 9/25 | Loss: 0.00150570
Iteration 10/25 | Loss: 0.00146134
Iteration 11/25 | Loss: 0.00144044
Iteration 12/25 | Loss: 0.00142232
Iteration 13/25 | Loss: 0.00141569
Iteration 14/25 | Loss: 0.00140794
Iteration 15/25 | Loss: 0.00140332
Iteration 16/25 | Loss: 0.00139922
Iteration 17/25 | Loss: 0.00139972
Iteration 18/25 | Loss: 0.00140822
Iteration 19/25 | Loss: 0.00140207
Iteration 20/25 | Loss: 0.00139764
Iteration 21/25 | Loss: 0.00139546
Iteration 22/25 | Loss: 0.00139717
Iteration 23/25 | Loss: 0.00139478
Iteration 24/25 | Loss: 0.00139733
Iteration 25/25 | Loss: 0.00139977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15417004
Iteration 2/25 | Loss: 0.00451814
Iteration 3/25 | Loss: 0.00440525
Iteration 4/25 | Loss: 0.00440525
Iteration 5/25 | Loss: 0.00440525
Iteration 6/25 | Loss: 0.00440525
Iteration 7/25 | Loss: 0.00440525
Iteration 8/25 | Loss: 0.00440525
Iteration 9/25 | Loss: 0.00440524
Iteration 10/25 | Loss: 0.00440524
Iteration 11/25 | Loss: 0.00440524
Iteration 12/25 | Loss: 0.00440524
Iteration 13/25 | Loss: 0.00440524
Iteration 14/25 | Loss: 0.00440524
Iteration 15/25 | Loss: 0.00440524
Iteration 16/25 | Loss: 0.00440524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0044052437879145145, 0.0044052437879145145, 0.0044052437879145145, 0.0044052437879145145, 0.0044052437879145145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0044052437879145145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00440524
Iteration 2/1000 | Loss: 0.00188998
Iteration 3/1000 | Loss: 0.00213925
Iteration 4/1000 | Loss: 0.00051617
Iteration 5/1000 | Loss: 0.00060243
Iteration 6/1000 | Loss: 0.00056443
Iteration 7/1000 | Loss: 0.00044044
Iteration 8/1000 | Loss: 0.00042234
Iteration 9/1000 | Loss: 0.00051864
Iteration 10/1000 | Loss: 0.00031473
Iteration 11/1000 | Loss: 0.00047739
Iteration 12/1000 | Loss: 0.00030881
Iteration 13/1000 | Loss: 0.00048681
Iteration 14/1000 | Loss: 0.00036822
Iteration 15/1000 | Loss: 0.00059447
Iteration 16/1000 | Loss: 0.00115898
Iteration 17/1000 | Loss: 0.00142544
Iteration 18/1000 | Loss: 0.00073303
Iteration 19/1000 | Loss: 0.00070308
Iteration 20/1000 | Loss: 0.00047229
Iteration 21/1000 | Loss: 0.00053854
Iteration 22/1000 | Loss: 0.00025506
Iteration 23/1000 | Loss: 0.00032292
Iteration 24/1000 | Loss: 0.00038930
Iteration 25/1000 | Loss: 0.00072372
Iteration 26/1000 | Loss: 0.00097551
Iteration 27/1000 | Loss: 0.00164957
Iteration 28/1000 | Loss: 0.00190014
Iteration 29/1000 | Loss: 0.00344695
Iteration 30/1000 | Loss: 0.00443186
Iteration 31/1000 | Loss: 0.00169028
Iteration 32/1000 | Loss: 0.00208529
Iteration 33/1000 | Loss: 0.00228459
Iteration 34/1000 | Loss: 0.00222864
Iteration 35/1000 | Loss: 0.00124380
Iteration 36/1000 | Loss: 0.00062197
Iteration 37/1000 | Loss: 0.00053965
Iteration 38/1000 | Loss: 0.00077164
Iteration 39/1000 | Loss: 0.00034200
Iteration 40/1000 | Loss: 0.00071978
Iteration 41/1000 | Loss: 0.00105112
Iteration 42/1000 | Loss: 0.00073136
Iteration 43/1000 | Loss: 0.00071311
Iteration 44/1000 | Loss: 0.00048685
Iteration 45/1000 | Loss: 0.00071919
Iteration 46/1000 | Loss: 0.00052678
Iteration 47/1000 | Loss: 0.00045328
Iteration 48/1000 | Loss: 0.00020633
Iteration 49/1000 | Loss: 0.00123379
Iteration 50/1000 | Loss: 0.00232909
Iteration 51/1000 | Loss: 0.00264320
Iteration 52/1000 | Loss: 0.00209070
Iteration 53/1000 | Loss: 0.00197017
Iteration 54/1000 | Loss: 0.00265356
Iteration 55/1000 | Loss: 0.00224034
Iteration 56/1000 | Loss: 0.00172243
Iteration 57/1000 | Loss: 0.00145871
Iteration 58/1000 | Loss: 0.00162997
Iteration 59/1000 | Loss: 0.00117452
Iteration 60/1000 | Loss: 0.00196379
Iteration 61/1000 | Loss: 0.00215273
Iteration 62/1000 | Loss: 0.00252777
Iteration 63/1000 | Loss: 0.00134140
Iteration 64/1000 | Loss: 0.00401090
Iteration 65/1000 | Loss: 0.00223502
Iteration 66/1000 | Loss: 0.00433398
Iteration 67/1000 | Loss: 0.00184682
Iteration 68/1000 | Loss: 0.00207479
Iteration 69/1000 | Loss: 0.00221283
Iteration 70/1000 | Loss: 0.00193056
Iteration 71/1000 | Loss: 0.00222916
Iteration 72/1000 | Loss: 0.00528226
Iteration 73/1000 | Loss: 0.00170953
Iteration 74/1000 | Loss: 0.00164597
Iteration 75/1000 | Loss: 0.00128612
Iteration 76/1000 | Loss: 0.00110047
Iteration 77/1000 | Loss: 0.00081152
Iteration 78/1000 | Loss: 0.00083160
Iteration 79/1000 | Loss: 0.00047779
Iteration 80/1000 | Loss: 0.00059570
Iteration 81/1000 | Loss: 0.00033666
Iteration 82/1000 | Loss: 0.00070327
Iteration 83/1000 | Loss: 0.00248449
Iteration 84/1000 | Loss: 0.00154362
Iteration 85/1000 | Loss: 0.00120970
Iteration 86/1000 | Loss: 0.00428727
Iteration 87/1000 | Loss: 0.00064629
Iteration 88/1000 | Loss: 0.00028102
Iteration 89/1000 | Loss: 0.00049096
Iteration 90/1000 | Loss: 0.00014732
Iteration 91/1000 | Loss: 0.00036196
Iteration 92/1000 | Loss: 0.00043731
Iteration 93/1000 | Loss: 0.00028606
Iteration 94/1000 | Loss: 0.00034238
Iteration 95/1000 | Loss: 0.00050240
Iteration 96/1000 | Loss: 0.00050827
Iteration 97/1000 | Loss: 0.00014435
Iteration 98/1000 | Loss: 0.00012090
Iteration 99/1000 | Loss: 0.00121678
Iteration 100/1000 | Loss: 0.00148550
Iteration 101/1000 | Loss: 0.00053821
Iteration 102/1000 | Loss: 0.00063182
Iteration 103/1000 | Loss: 0.00070262
Iteration 104/1000 | Loss: 0.00084039
Iteration 105/1000 | Loss: 0.00025033
Iteration 106/1000 | Loss: 0.00011538
Iteration 107/1000 | Loss: 0.00013556
Iteration 108/1000 | Loss: 0.00010523
Iteration 109/1000 | Loss: 0.00010040
Iteration 110/1000 | Loss: 0.00012660
Iteration 111/1000 | Loss: 0.00011393
Iteration 112/1000 | Loss: 0.00010369
Iteration 113/1000 | Loss: 0.00041965
Iteration 114/1000 | Loss: 0.00025430
Iteration 115/1000 | Loss: 0.00058545
Iteration 116/1000 | Loss: 0.00022715
Iteration 117/1000 | Loss: 0.00049167
Iteration 118/1000 | Loss: 0.00022842
Iteration 119/1000 | Loss: 0.00038841
Iteration 120/1000 | Loss: 0.00049390
Iteration 121/1000 | Loss: 0.00020299
Iteration 122/1000 | Loss: 0.00029433
Iteration 123/1000 | Loss: 0.00069330
Iteration 124/1000 | Loss: 0.00034706
Iteration 125/1000 | Loss: 0.00063820
Iteration 126/1000 | Loss: 0.00067308
Iteration 127/1000 | Loss: 0.00081583
Iteration 128/1000 | Loss: 0.00066299
Iteration 129/1000 | Loss: 0.00058280
Iteration 130/1000 | Loss: 0.00065910
Iteration 131/1000 | Loss: 0.00073780
Iteration 132/1000 | Loss: 0.00104289
Iteration 133/1000 | Loss: 0.00018841
Iteration 134/1000 | Loss: 0.00059344
Iteration 135/1000 | Loss: 0.00038973
Iteration 136/1000 | Loss: 0.00013640
Iteration 137/1000 | Loss: 0.00023926
Iteration 138/1000 | Loss: 0.00009821
Iteration 139/1000 | Loss: 0.00011959
Iteration 140/1000 | Loss: 0.00011463
Iteration 141/1000 | Loss: 0.00011144
Iteration 142/1000 | Loss: 0.00035760
Iteration 143/1000 | Loss: 0.00165957
Iteration 144/1000 | Loss: 0.00042715
Iteration 145/1000 | Loss: 0.00036863
Iteration 146/1000 | Loss: 0.00048340
Iteration 147/1000 | Loss: 0.00029886
Iteration 148/1000 | Loss: 0.00025567
Iteration 149/1000 | Loss: 0.00011888
Iteration 150/1000 | Loss: 0.00010961
Iteration 151/1000 | Loss: 0.00011429
Iteration 152/1000 | Loss: 0.00011253
Iteration 153/1000 | Loss: 0.00012180
Iteration 154/1000 | Loss: 0.00011483
Iteration 155/1000 | Loss: 0.00012570
Iteration 156/1000 | Loss: 0.00044529
Iteration 157/1000 | Loss: 0.00087424
Iteration 158/1000 | Loss: 0.00083819
Iteration 159/1000 | Loss: 0.00041427
Iteration 160/1000 | Loss: 0.00011581
Iteration 161/1000 | Loss: 0.00039364
Iteration 162/1000 | Loss: 0.00024669
Iteration 163/1000 | Loss: 0.00011565
Iteration 164/1000 | Loss: 0.00012226
Iteration 165/1000 | Loss: 0.00008832
Iteration 166/1000 | Loss: 0.00010316
Iteration 167/1000 | Loss: 0.00011952
Iteration 168/1000 | Loss: 0.00011201
Iteration 169/1000 | Loss: 0.00011741
Iteration 170/1000 | Loss: 0.00010814
Iteration 171/1000 | Loss: 0.00010473
Iteration 172/1000 | Loss: 0.00076079
Iteration 173/1000 | Loss: 0.00011475
Iteration 174/1000 | Loss: 0.00011826
Iteration 175/1000 | Loss: 0.00010452
Iteration 176/1000 | Loss: 0.00011119
Iteration 177/1000 | Loss: 0.00010255
Iteration 178/1000 | Loss: 0.00011485
Iteration 179/1000 | Loss: 0.00011074
Iteration 180/1000 | Loss: 0.00012547
Iteration 181/1000 | Loss: 0.00010629
Iteration 182/1000 | Loss: 0.00012733
Iteration 183/1000 | Loss: 0.00010813
Iteration 184/1000 | Loss: 0.00010817
Iteration 185/1000 | Loss: 0.00010850
Iteration 186/1000 | Loss: 0.00011503
Iteration 187/1000 | Loss: 0.00010408
Iteration 188/1000 | Loss: 0.00008957
Iteration 189/1000 | Loss: 0.00010109
Iteration 190/1000 | Loss: 0.00009228
Iteration 191/1000 | Loss: 0.00009474
Iteration 192/1000 | Loss: 0.00010273
Iteration 193/1000 | Loss: 0.00009586
Iteration 194/1000 | Loss: 0.00007563
Iteration 195/1000 | Loss: 0.00010528
Iteration 196/1000 | Loss: 0.00010998
Iteration 197/1000 | Loss: 0.00010407
Iteration 198/1000 | Loss: 0.00010963
Iteration 199/1000 | Loss: 0.00010247
Iteration 200/1000 | Loss: 0.00011310
Iteration 201/1000 | Loss: 0.00010748
Iteration 202/1000 | Loss: 0.00013540
Iteration 203/1000 | Loss: 0.00010028
Iteration 204/1000 | Loss: 0.00011966
Iteration 205/1000 | Loss: 0.00012324
Iteration 206/1000 | Loss: 0.00011631
Iteration 207/1000 | Loss: 0.00011667
Iteration 208/1000 | Loss: 0.00009678
Iteration 209/1000 | Loss: 0.00010177
Iteration 210/1000 | Loss: 0.00010572
Iteration 211/1000 | Loss: 0.00010889
Iteration 212/1000 | Loss: 0.00011161
Iteration 213/1000 | Loss: 0.00010797
Iteration 214/1000 | Loss: 0.00010874
Iteration 215/1000 | Loss: 0.00011101
Iteration 216/1000 | Loss: 0.00010813
Iteration 217/1000 | Loss: 0.00011529
Iteration 218/1000 | Loss: 0.00016393
Iteration 219/1000 | Loss: 0.00011438
Iteration 220/1000 | Loss: 0.00008723
Iteration 221/1000 | Loss: 0.00008734
Iteration 222/1000 | Loss: 0.00007877
Iteration 223/1000 | Loss: 0.00007963
Iteration 224/1000 | Loss: 0.00009048
Iteration 225/1000 | Loss: 0.00009766
Iteration 226/1000 | Loss: 0.00008470
Iteration 227/1000 | Loss: 0.00009810
Iteration 228/1000 | Loss: 0.00009705
Iteration 229/1000 | Loss: 0.00008712
Iteration 230/1000 | Loss: 0.00009030
Iteration 231/1000 | Loss: 0.00009898
Iteration 232/1000 | Loss: 0.00009001
Iteration 233/1000 | Loss: 0.00009902
Iteration 234/1000 | Loss: 0.00011604
Iteration 235/1000 | Loss: 0.00008980
Iteration 236/1000 | Loss: 0.00008163
Iteration 237/1000 | Loss: 0.00009262
Iteration 238/1000 | Loss: 0.00009974
Iteration 239/1000 | Loss: 0.00009182
Iteration 240/1000 | Loss: 0.00009424
Iteration 241/1000 | Loss: 0.00009390
Iteration 242/1000 | Loss: 0.00010828
Iteration 243/1000 | Loss: 0.00008982
Iteration 244/1000 | Loss: 0.00011087
Iteration 245/1000 | Loss: 0.00010733
Iteration 246/1000 | Loss: 0.00009242
Iteration 247/1000 | Loss: 0.00008965
Iteration 248/1000 | Loss: 0.00011076
Iteration 249/1000 | Loss: 0.00009500
Iteration 250/1000 | Loss: 0.00008596
Iteration 251/1000 | Loss: 0.00009481
Iteration 252/1000 | Loss: 0.00008647
Iteration 253/1000 | Loss: 0.00009530
Iteration 254/1000 | Loss: 0.00009418
Iteration 255/1000 | Loss: 0.00012179
Iteration 256/1000 | Loss: 0.00009309
Iteration 257/1000 | Loss: 0.00010453
Iteration 258/1000 | Loss: 0.00009491
Iteration 259/1000 | Loss: 0.00009031
Iteration 260/1000 | Loss: 0.00013595
Iteration 261/1000 | Loss: 0.00031707
Iteration 262/1000 | Loss: 0.00020203
Iteration 263/1000 | Loss: 0.00021743
Iteration 264/1000 | Loss: 0.00041438
Iteration 265/1000 | Loss: 0.00021687
Iteration 266/1000 | Loss: 0.00022331
Iteration 267/1000 | Loss: 0.00010143
Iteration 268/1000 | Loss: 0.00009223
Iteration 269/1000 | Loss: 0.00007746
Iteration 270/1000 | Loss: 0.00009477
Iteration 271/1000 | Loss: 0.00009752
Iteration 272/1000 | Loss: 0.00009255
Iteration 273/1000 | Loss: 0.00009130
Iteration 274/1000 | Loss: 0.00009331
Iteration 275/1000 | Loss: 0.00009116
Iteration 276/1000 | Loss: 0.00009441
Iteration 277/1000 | Loss: 0.00009080
Iteration 278/1000 | Loss: 0.00009444
Iteration 279/1000 | Loss: 0.00007225
Iteration 280/1000 | Loss: 0.00009208
Iteration 281/1000 | Loss: 0.00009313
Iteration 282/1000 | Loss: 0.00008985
Iteration 283/1000 | Loss: 0.00009227
Iteration 284/1000 | Loss: 0.00009038
Iteration 285/1000 | Loss: 0.00009498
Iteration 286/1000 | Loss: 0.00008994
Iteration 287/1000 | Loss: 0.00009211
Iteration 288/1000 | Loss: 0.00009165
Iteration 289/1000 | Loss: 0.00009162
Iteration 290/1000 | Loss: 0.00009043
Iteration 291/1000 | Loss: 0.00009447
Iteration 292/1000 | Loss: 0.00009010
Iteration 293/1000 | Loss: 0.00009959
Iteration 294/1000 | Loss: 0.00007692
Iteration 295/1000 | Loss: 0.00008749
Iteration 296/1000 | Loss: 0.00007461
Iteration 297/1000 | Loss: 0.00007307
Iteration 298/1000 | Loss: 0.00008791
Iteration 299/1000 | Loss: 0.00009165
Iteration 300/1000 | Loss: 0.00008007
Iteration 301/1000 | Loss: 0.00009245
Iteration 302/1000 | Loss: 0.00009104
Iteration 303/1000 | Loss: 0.00008867
Iteration 304/1000 | Loss: 0.00008380
Iteration 305/1000 | Loss: 0.00009020
Iteration 306/1000 | Loss: 0.00008964
Iteration 307/1000 | Loss: 0.00009215
Iteration 308/1000 | Loss: 0.00007865
Iteration 309/1000 | Loss: 0.00008425
Iteration 310/1000 | Loss: 0.00007587
Iteration 311/1000 | Loss: 0.00007952
Iteration 312/1000 | Loss: 0.00006967
Iteration 313/1000 | Loss: 0.00008038
Iteration 314/1000 | Loss: 0.00006793
Iteration 315/1000 | Loss: 0.00006745
Iteration 316/1000 | Loss: 0.00006715
Iteration 317/1000 | Loss: 0.00006683
Iteration 318/1000 | Loss: 0.00006651
Iteration 319/1000 | Loss: 0.00006623
Iteration 320/1000 | Loss: 0.00006602
Iteration 321/1000 | Loss: 0.00006590
Iteration 322/1000 | Loss: 0.00006588
Iteration 323/1000 | Loss: 0.00006582
Iteration 324/1000 | Loss: 0.00006578
Iteration 325/1000 | Loss: 0.00006577
Iteration 326/1000 | Loss: 0.00006577
Iteration 327/1000 | Loss: 0.00006576
Iteration 328/1000 | Loss: 0.00006576
Iteration 329/1000 | Loss: 0.00006575
Iteration 330/1000 | Loss: 0.00006575
Iteration 331/1000 | Loss: 0.00006574
Iteration 332/1000 | Loss: 0.00006574
Iteration 333/1000 | Loss: 0.00006573
Iteration 334/1000 | Loss: 0.00006573
Iteration 335/1000 | Loss: 0.00006572
Iteration 336/1000 | Loss: 0.00006572
Iteration 337/1000 | Loss: 0.00006571
Iteration 338/1000 | Loss: 0.00006567
Iteration 339/1000 | Loss: 0.00006566
Iteration 340/1000 | Loss: 0.00006566
Iteration 341/1000 | Loss: 0.00006566
Iteration 342/1000 | Loss: 0.00006565
Iteration 343/1000 | Loss: 0.00006565
Iteration 344/1000 | Loss: 0.00006565
Iteration 345/1000 | Loss: 0.00006565
Iteration 346/1000 | Loss: 0.00006565
Iteration 347/1000 | Loss: 0.00006564
Iteration 348/1000 | Loss: 0.00006564
Iteration 349/1000 | Loss: 0.00006563
Iteration 350/1000 | Loss: 0.00006563
Iteration 351/1000 | Loss: 0.00006563
Iteration 352/1000 | Loss: 0.00006562
Iteration 353/1000 | Loss: 0.00006562
Iteration 354/1000 | Loss: 0.00006562
Iteration 355/1000 | Loss: 0.00006561
Iteration 356/1000 | Loss: 0.00006561
Iteration 357/1000 | Loss: 0.00006561
Iteration 358/1000 | Loss: 0.00006561
Iteration 359/1000 | Loss: 0.00006561
Iteration 360/1000 | Loss: 0.00006560
Iteration 361/1000 | Loss: 0.00006560
Iteration 362/1000 | Loss: 0.00006560
Iteration 363/1000 | Loss: 0.00006560
Iteration 364/1000 | Loss: 0.00006560
Iteration 365/1000 | Loss: 0.00006560
Iteration 366/1000 | Loss: 0.00006559
Iteration 367/1000 | Loss: 0.00006559
Iteration 368/1000 | Loss: 0.00006559
Iteration 369/1000 | Loss: 0.00006558
Iteration 370/1000 | Loss: 0.00006558
Iteration 371/1000 | Loss: 0.00006558
Iteration 372/1000 | Loss: 0.00006558
Iteration 373/1000 | Loss: 0.00006558
Iteration 374/1000 | Loss: 0.00006558
Iteration 375/1000 | Loss: 0.00006558
Iteration 376/1000 | Loss: 0.00006558
Iteration 377/1000 | Loss: 0.00006557
Iteration 378/1000 | Loss: 0.00006557
Iteration 379/1000 | Loss: 0.00006557
Iteration 380/1000 | Loss: 0.00006557
Iteration 381/1000 | Loss: 0.00006557
Iteration 382/1000 | Loss: 0.00006557
Iteration 383/1000 | Loss: 0.00006557
Iteration 384/1000 | Loss: 0.00006557
Iteration 385/1000 | Loss: 0.00006557
Iteration 386/1000 | Loss: 0.00006557
Iteration 387/1000 | Loss: 0.00006557
Iteration 388/1000 | Loss: 0.00006557
Iteration 389/1000 | Loss: 0.00006557
Iteration 390/1000 | Loss: 0.00006557
Iteration 391/1000 | Loss: 0.00006557
Iteration 392/1000 | Loss: 0.00006557
Iteration 393/1000 | Loss: 0.00006557
Iteration 394/1000 | Loss: 0.00006557
Iteration 395/1000 | Loss: 0.00006557
Iteration 396/1000 | Loss: 0.00006557
Iteration 397/1000 | Loss: 0.00006556
Iteration 398/1000 | Loss: 0.00006556
Iteration 399/1000 | Loss: 0.00006556
Iteration 400/1000 | Loss: 0.00006556
Iteration 401/1000 | Loss: 0.00006556
Iteration 402/1000 | Loss: 0.00006556
Iteration 403/1000 | Loss: 0.00006556
Iteration 404/1000 | Loss: 0.00006556
Iteration 405/1000 | Loss: 0.00006556
Iteration 406/1000 | Loss: 0.00006556
Iteration 407/1000 | Loss: 0.00006556
Iteration 408/1000 | Loss: 0.00006556
Iteration 409/1000 | Loss: 0.00006556
Iteration 410/1000 | Loss: 0.00006556
Iteration 411/1000 | Loss: 0.00006556
Iteration 412/1000 | Loss: 0.00006556
Iteration 413/1000 | Loss: 0.00006556
Iteration 414/1000 | Loss: 0.00006556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 414. Stopping optimization.
Last 5 losses: [6.55602925689891e-05, 6.55602925689891e-05, 6.55602925689891e-05, 6.55602925689891e-05, 6.55602925689891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.55602925689891e-05

Optimization complete. Final v2v error: 3.924203634262085 mm

Highest mean error: 11.49156379699707 mm for frame 141

Lowest mean error: 2.310983419418335 mm for frame 93

Saving results

Total time: 538.542316198349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495552
Iteration 2/25 | Loss: 0.00113065
Iteration 3/25 | Loss: 0.00101092
Iteration 4/25 | Loss: 0.00099560
Iteration 5/25 | Loss: 0.00099277
Iteration 6/25 | Loss: 0.00099246
Iteration 7/25 | Loss: 0.00099245
Iteration 8/25 | Loss: 0.00099245
Iteration 9/25 | Loss: 0.00099245
Iteration 10/25 | Loss: 0.00099245
Iteration 11/25 | Loss: 0.00099245
Iteration 12/25 | Loss: 0.00099245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009924463229253888, 0.0009924463229253888, 0.0009924463229253888, 0.0009924463229253888, 0.0009924463229253888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009924463229253888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22294927
Iteration 2/25 | Loss: 0.00159728
Iteration 3/25 | Loss: 0.00159726
Iteration 4/25 | Loss: 0.00159726
Iteration 5/25 | Loss: 0.00159726
Iteration 6/25 | Loss: 0.00159726
Iteration 7/25 | Loss: 0.00159726
Iteration 8/25 | Loss: 0.00159726
Iteration 9/25 | Loss: 0.00159726
Iteration 10/25 | Loss: 0.00159726
Iteration 11/25 | Loss: 0.00159726
Iteration 12/25 | Loss: 0.00159726
Iteration 13/25 | Loss: 0.00159726
Iteration 14/25 | Loss: 0.00159726
Iteration 15/25 | Loss: 0.00159726
Iteration 16/25 | Loss: 0.00159726
Iteration 17/25 | Loss: 0.00159726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015972605906426907, 0.0015972605906426907, 0.0015972605906426907, 0.0015972605906426907, 0.0015972605906426907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015972605906426907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159726
Iteration 2/1000 | Loss: 0.00002182
Iteration 3/1000 | Loss: 0.00001298
Iteration 4/1000 | Loss: 0.00001114
Iteration 5/1000 | Loss: 0.00001029
Iteration 6/1000 | Loss: 0.00000991
Iteration 7/1000 | Loss: 0.00000962
Iteration 8/1000 | Loss: 0.00000962
Iteration 9/1000 | Loss: 0.00000934
Iteration 10/1000 | Loss: 0.00000912
Iteration 11/1000 | Loss: 0.00000897
Iteration 12/1000 | Loss: 0.00000892
Iteration 13/1000 | Loss: 0.00000891
Iteration 14/1000 | Loss: 0.00000887
Iteration 15/1000 | Loss: 0.00000886
Iteration 16/1000 | Loss: 0.00000886
Iteration 17/1000 | Loss: 0.00000885
Iteration 18/1000 | Loss: 0.00000885
Iteration 19/1000 | Loss: 0.00000884
Iteration 20/1000 | Loss: 0.00000883
Iteration 21/1000 | Loss: 0.00000882
Iteration 22/1000 | Loss: 0.00000881
Iteration 23/1000 | Loss: 0.00000880
Iteration 24/1000 | Loss: 0.00000880
Iteration 25/1000 | Loss: 0.00000880
Iteration 26/1000 | Loss: 0.00000879
Iteration 27/1000 | Loss: 0.00000878
Iteration 28/1000 | Loss: 0.00000874
Iteration 29/1000 | Loss: 0.00000874
Iteration 30/1000 | Loss: 0.00000871
Iteration 31/1000 | Loss: 0.00000871
Iteration 32/1000 | Loss: 0.00000869
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000868
Iteration 35/1000 | Loss: 0.00000867
Iteration 36/1000 | Loss: 0.00000867
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000864
Iteration 39/1000 | Loss: 0.00000864
Iteration 40/1000 | Loss: 0.00000864
Iteration 41/1000 | Loss: 0.00000863
Iteration 42/1000 | Loss: 0.00000862
Iteration 43/1000 | Loss: 0.00000861
Iteration 44/1000 | Loss: 0.00000861
Iteration 45/1000 | Loss: 0.00000861
Iteration 46/1000 | Loss: 0.00000860
Iteration 47/1000 | Loss: 0.00000860
Iteration 48/1000 | Loss: 0.00000859
Iteration 49/1000 | Loss: 0.00000859
Iteration 50/1000 | Loss: 0.00000859
Iteration 51/1000 | Loss: 0.00000858
Iteration 52/1000 | Loss: 0.00000858
Iteration 53/1000 | Loss: 0.00000858
Iteration 54/1000 | Loss: 0.00000857
Iteration 55/1000 | Loss: 0.00000857
Iteration 56/1000 | Loss: 0.00000857
Iteration 57/1000 | Loss: 0.00000856
Iteration 58/1000 | Loss: 0.00000856
Iteration 59/1000 | Loss: 0.00000856
Iteration 60/1000 | Loss: 0.00000855
Iteration 61/1000 | Loss: 0.00000855
Iteration 62/1000 | Loss: 0.00000855
Iteration 63/1000 | Loss: 0.00000855
Iteration 64/1000 | Loss: 0.00000855
Iteration 65/1000 | Loss: 0.00000855
Iteration 66/1000 | Loss: 0.00000854
Iteration 67/1000 | Loss: 0.00000854
Iteration 68/1000 | Loss: 0.00000854
Iteration 69/1000 | Loss: 0.00000854
Iteration 70/1000 | Loss: 0.00000854
Iteration 71/1000 | Loss: 0.00000854
Iteration 72/1000 | Loss: 0.00000854
Iteration 73/1000 | Loss: 0.00000854
Iteration 74/1000 | Loss: 0.00000854
Iteration 75/1000 | Loss: 0.00000853
Iteration 76/1000 | Loss: 0.00000853
Iteration 77/1000 | Loss: 0.00000853
Iteration 78/1000 | Loss: 0.00000853
Iteration 79/1000 | Loss: 0.00000852
Iteration 80/1000 | Loss: 0.00000852
Iteration 81/1000 | Loss: 0.00000852
Iteration 82/1000 | Loss: 0.00000852
Iteration 83/1000 | Loss: 0.00000852
Iteration 84/1000 | Loss: 0.00000852
Iteration 85/1000 | Loss: 0.00000852
Iteration 86/1000 | Loss: 0.00000852
Iteration 87/1000 | Loss: 0.00000852
Iteration 88/1000 | Loss: 0.00000851
Iteration 89/1000 | Loss: 0.00000851
Iteration 90/1000 | Loss: 0.00000851
Iteration 91/1000 | Loss: 0.00000851
Iteration 92/1000 | Loss: 0.00000851
Iteration 93/1000 | Loss: 0.00000851
Iteration 94/1000 | Loss: 0.00000851
Iteration 95/1000 | Loss: 0.00000851
Iteration 96/1000 | Loss: 0.00000851
Iteration 97/1000 | Loss: 0.00000851
Iteration 98/1000 | Loss: 0.00000851
Iteration 99/1000 | Loss: 0.00000850
Iteration 100/1000 | Loss: 0.00000850
Iteration 101/1000 | Loss: 0.00000850
Iteration 102/1000 | Loss: 0.00000850
Iteration 103/1000 | Loss: 0.00000850
Iteration 104/1000 | Loss: 0.00000850
Iteration 105/1000 | Loss: 0.00000850
Iteration 106/1000 | Loss: 0.00000850
Iteration 107/1000 | Loss: 0.00000850
Iteration 108/1000 | Loss: 0.00000850
Iteration 109/1000 | Loss: 0.00000850
Iteration 110/1000 | Loss: 0.00000850
Iteration 111/1000 | Loss: 0.00000850
Iteration 112/1000 | Loss: 0.00000850
Iteration 113/1000 | Loss: 0.00000850
Iteration 114/1000 | Loss: 0.00000850
Iteration 115/1000 | Loss: 0.00000850
Iteration 116/1000 | Loss: 0.00000850
Iteration 117/1000 | Loss: 0.00000850
Iteration 118/1000 | Loss: 0.00000850
Iteration 119/1000 | Loss: 0.00000850
Iteration 120/1000 | Loss: 0.00000850
Iteration 121/1000 | Loss: 0.00000850
Iteration 122/1000 | Loss: 0.00000850
Iteration 123/1000 | Loss: 0.00000850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [8.495786460116506e-06, 8.495786460116506e-06, 8.495786460116506e-06, 8.495786460116506e-06, 8.495786460116506e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.495786460116506e-06

Optimization complete. Final v2v error: 2.4587607383728027 mm

Highest mean error: 2.7815730571746826 mm for frame 104

Lowest mean error: 2.147454023361206 mm for frame 33

Saving results

Total time: 37.21536707878113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840549
Iteration 2/25 | Loss: 0.00121932
Iteration 3/25 | Loss: 0.00100005
Iteration 4/25 | Loss: 0.00097802
Iteration 5/25 | Loss: 0.00097349
Iteration 6/25 | Loss: 0.00097312
Iteration 7/25 | Loss: 0.00097306
Iteration 8/25 | Loss: 0.00097306
Iteration 9/25 | Loss: 0.00097306
Iteration 10/25 | Loss: 0.00097306
Iteration 11/25 | Loss: 0.00097306
Iteration 12/25 | Loss: 0.00097306
Iteration 13/25 | Loss: 0.00097306
Iteration 14/25 | Loss: 0.00097306
Iteration 15/25 | Loss: 0.00097306
Iteration 16/25 | Loss: 0.00097306
Iteration 17/25 | Loss: 0.00097306
Iteration 18/25 | Loss: 0.00097306
Iteration 19/25 | Loss: 0.00097306
Iteration 20/25 | Loss: 0.00097306
Iteration 21/25 | Loss: 0.00097306
Iteration 22/25 | Loss: 0.00097306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009730601450428367, 0.0009730601450428367, 0.0009730601450428367, 0.0009730601450428367, 0.0009730601450428367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009730601450428367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20575976
Iteration 2/25 | Loss: 0.00190542
Iteration 3/25 | Loss: 0.00190542
Iteration 4/25 | Loss: 0.00190542
Iteration 5/25 | Loss: 0.00190542
Iteration 6/25 | Loss: 0.00190542
Iteration 7/25 | Loss: 0.00190542
Iteration 8/25 | Loss: 0.00190542
Iteration 9/25 | Loss: 0.00190542
Iteration 10/25 | Loss: 0.00190542
Iteration 11/25 | Loss: 0.00190542
Iteration 12/25 | Loss: 0.00190542
Iteration 13/25 | Loss: 0.00190542
Iteration 14/25 | Loss: 0.00190542
Iteration 15/25 | Loss: 0.00190542
Iteration 16/25 | Loss: 0.00190542
Iteration 17/25 | Loss: 0.00190542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019054183503612876, 0.0019054183503612876, 0.0019054183503612876, 0.0019054183503612876, 0.0019054183503612876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019054183503612876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190542
Iteration 2/1000 | Loss: 0.00002328
Iteration 3/1000 | Loss: 0.00001115
Iteration 4/1000 | Loss: 0.00000976
Iteration 5/1000 | Loss: 0.00000912
Iteration 6/1000 | Loss: 0.00000853
Iteration 7/1000 | Loss: 0.00000805
Iteration 8/1000 | Loss: 0.00000775
Iteration 9/1000 | Loss: 0.00000763
Iteration 10/1000 | Loss: 0.00000746
Iteration 11/1000 | Loss: 0.00000730
Iteration 12/1000 | Loss: 0.00000725
Iteration 13/1000 | Loss: 0.00000718
Iteration 14/1000 | Loss: 0.00000716
Iteration 15/1000 | Loss: 0.00000716
Iteration 16/1000 | Loss: 0.00000715
Iteration 17/1000 | Loss: 0.00000715
Iteration 18/1000 | Loss: 0.00000711
Iteration 19/1000 | Loss: 0.00000711
Iteration 20/1000 | Loss: 0.00000711
Iteration 21/1000 | Loss: 0.00000711
Iteration 22/1000 | Loss: 0.00000710
Iteration 23/1000 | Loss: 0.00000709
Iteration 24/1000 | Loss: 0.00000705
Iteration 25/1000 | Loss: 0.00000704
Iteration 26/1000 | Loss: 0.00000702
Iteration 27/1000 | Loss: 0.00000701
Iteration 28/1000 | Loss: 0.00000701
Iteration 29/1000 | Loss: 0.00000701
Iteration 30/1000 | Loss: 0.00000700
Iteration 31/1000 | Loss: 0.00000700
Iteration 32/1000 | Loss: 0.00000699
Iteration 33/1000 | Loss: 0.00000699
Iteration 34/1000 | Loss: 0.00000698
Iteration 35/1000 | Loss: 0.00000698
Iteration 36/1000 | Loss: 0.00000697
Iteration 37/1000 | Loss: 0.00000697
Iteration 38/1000 | Loss: 0.00000697
Iteration 39/1000 | Loss: 0.00000697
Iteration 40/1000 | Loss: 0.00000697
Iteration 41/1000 | Loss: 0.00000697
Iteration 42/1000 | Loss: 0.00000696
Iteration 43/1000 | Loss: 0.00000696
Iteration 44/1000 | Loss: 0.00000695
Iteration 45/1000 | Loss: 0.00000694
Iteration 46/1000 | Loss: 0.00000694
Iteration 47/1000 | Loss: 0.00000693
Iteration 48/1000 | Loss: 0.00000693
Iteration 49/1000 | Loss: 0.00000693
Iteration 50/1000 | Loss: 0.00000693
Iteration 51/1000 | Loss: 0.00000693
Iteration 52/1000 | Loss: 0.00000693
Iteration 53/1000 | Loss: 0.00000693
Iteration 54/1000 | Loss: 0.00000693
Iteration 55/1000 | Loss: 0.00000693
Iteration 56/1000 | Loss: 0.00000692
Iteration 57/1000 | Loss: 0.00000692
Iteration 58/1000 | Loss: 0.00000692
Iteration 59/1000 | Loss: 0.00000692
Iteration 60/1000 | Loss: 0.00000692
Iteration 61/1000 | Loss: 0.00000691
Iteration 62/1000 | Loss: 0.00000691
Iteration 63/1000 | Loss: 0.00000691
Iteration 64/1000 | Loss: 0.00000690
Iteration 65/1000 | Loss: 0.00000690
Iteration 66/1000 | Loss: 0.00000690
Iteration 67/1000 | Loss: 0.00000689
Iteration 68/1000 | Loss: 0.00000689
Iteration 69/1000 | Loss: 0.00000689
Iteration 70/1000 | Loss: 0.00000689
Iteration 71/1000 | Loss: 0.00000689
Iteration 72/1000 | Loss: 0.00000689
Iteration 73/1000 | Loss: 0.00000688
Iteration 74/1000 | Loss: 0.00000688
Iteration 75/1000 | Loss: 0.00000688
Iteration 76/1000 | Loss: 0.00000687
Iteration 77/1000 | Loss: 0.00000687
Iteration 78/1000 | Loss: 0.00000687
Iteration 79/1000 | Loss: 0.00000687
Iteration 80/1000 | Loss: 0.00000687
Iteration 81/1000 | Loss: 0.00000687
Iteration 82/1000 | Loss: 0.00000687
Iteration 83/1000 | Loss: 0.00000687
Iteration 84/1000 | Loss: 0.00000687
Iteration 85/1000 | Loss: 0.00000687
Iteration 86/1000 | Loss: 0.00000687
Iteration 87/1000 | Loss: 0.00000686
Iteration 88/1000 | Loss: 0.00000686
Iteration 89/1000 | Loss: 0.00000686
Iteration 90/1000 | Loss: 0.00000686
Iteration 91/1000 | Loss: 0.00000686
Iteration 92/1000 | Loss: 0.00000686
Iteration 93/1000 | Loss: 0.00000686
Iteration 94/1000 | Loss: 0.00000686
Iteration 95/1000 | Loss: 0.00000686
Iteration 96/1000 | Loss: 0.00000686
Iteration 97/1000 | Loss: 0.00000686
Iteration 98/1000 | Loss: 0.00000686
Iteration 99/1000 | Loss: 0.00000686
Iteration 100/1000 | Loss: 0.00000686
Iteration 101/1000 | Loss: 0.00000686
Iteration 102/1000 | Loss: 0.00000686
Iteration 103/1000 | Loss: 0.00000685
Iteration 104/1000 | Loss: 0.00000685
Iteration 105/1000 | Loss: 0.00000685
Iteration 106/1000 | Loss: 0.00000685
Iteration 107/1000 | Loss: 0.00000685
Iteration 108/1000 | Loss: 0.00000685
Iteration 109/1000 | Loss: 0.00000685
Iteration 110/1000 | Loss: 0.00000685
Iteration 111/1000 | Loss: 0.00000685
Iteration 112/1000 | Loss: 0.00000685
Iteration 113/1000 | Loss: 0.00000685
Iteration 114/1000 | Loss: 0.00000685
Iteration 115/1000 | Loss: 0.00000684
Iteration 116/1000 | Loss: 0.00000684
Iteration 117/1000 | Loss: 0.00000684
Iteration 118/1000 | Loss: 0.00000684
Iteration 119/1000 | Loss: 0.00000684
Iteration 120/1000 | Loss: 0.00000684
Iteration 121/1000 | Loss: 0.00000684
Iteration 122/1000 | Loss: 0.00000684
Iteration 123/1000 | Loss: 0.00000684
Iteration 124/1000 | Loss: 0.00000684
Iteration 125/1000 | Loss: 0.00000684
Iteration 126/1000 | Loss: 0.00000684
Iteration 127/1000 | Loss: 0.00000684
Iteration 128/1000 | Loss: 0.00000684
Iteration 129/1000 | Loss: 0.00000683
Iteration 130/1000 | Loss: 0.00000683
Iteration 131/1000 | Loss: 0.00000683
Iteration 132/1000 | Loss: 0.00000683
Iteration 133/1000 | Loss: 0.00000683
Iteration 134/1000 | Loss: 0.00000683
Iteration 135/1000 | Loss: 0.00000683
Iteration 136/1000 | Loss: 0.00000683
Iteration 137/1000 | Loss: 0.00000683
Iteration 138/1000 | Loss: 0.00000683
Iteration 139/1000 | Loss: 0.00000683
Iteration 140/1000 | Loss: 0.00000683
Iteration 141/1000 | Loss: 0.00000683
Iteration 142/1000 | Loss: 0.00000683
Iteration 143/1000 | Loss: 0.00000682
Iteration 144/1000 | Loss: 0.00000682
Iteration 145/1000 | Loss: 0.00000682
Iteration 146/1000 | Loss: 0.00000682
Iteration 147/1000 | Loss: 0.00000682
Iteration 148/1000 | Loss: 0.00000682
Iteration 149/1000 | Loss: 0.00000682
Iteration 150/1000 | Loss: 0.00000682
Iteration 151/1000 | Loss: 0.00000682
Iteration 152/1000 | Loss: 0.00000682
Iteration 153/1000 | Loss: 0.00000682
Iteration 154/1000 | Loss: 0.00000682
Iteration 155/1000 | Loss: 0.00000682
Iteration 156/1000 | Loss: 0.00000682
Iteration 157/1000 | Loss: 0.00000682
Iteration 158/1000 | Loss: 0.00000682
Iteration 159/1000 | Loss: 0.00000682
Iteration 160/1000 | Loss: 0.00000682
Iteration 161/1000 | Loss: 0.00000682
Iteration 162/1000 | Loss: 0.00000682
Iteration 163/1000 | Loss: 0.00000682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [6.8196213760529645e-06, 6.8196213760529645e-06, 6.8196213760529645e-06, 6.8196213760529645e-06, 6.8196213760529645e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.8196213760529645e-06

Optimization complete. Final v2v error: 2.2449052333831787 mm

Highest mean error: 2.5261032581329346 mm for frame 114

Lowest mean error: 1.988492727279663 mm for frame 61

Saving results

Total time: 40.80079364776611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085718
Iteration 2/25 | Loss: 0.01085718
Iteration 3/25 | Loss: 0.01085718
Iteration 4/25 | Loss: 0.01085717
Iteration 5/25 | Loss: 0.01085717
Iteration 6/25 | Loss: 0.01085716
Iteration 7/25 | Loss: 0.01085716
Iteration 8/25 | Loss: 0.01085716
Iteration 9/25 | Loss: 0.01085716
Iteration 10/25 | Loss: 0.01085716
Iteration 11/25 | Loss: 0.01085716
Iteration 12/25 | Loss: 0.01085716
Iteration 13/25 | Loss: 0.01085715
Iteration 14/25 | Loss: 0.01085715
Iteration 15/25 | Loss: 0.01085715
Iteration 16/25 | Loss: 0.01085715
Iteration 17/25 | Loss: 0.01085715
Iteration 18/25 | Loss: 0.01085714
Iteration 19/25 | Loss: 0.01085714
Iteration 20/25 | Loss: 0.01085714
Iteration 21/25 | Loss: 0.01085714
Iteration 22/25 | Loss: 0.01085714
Iteration 23/25 | Loss: 0.01085713
Iteration 24/25 | Loss: 0.01085713
Iteration 25/25 | Loss: 0.01085713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56085658
Iteration 2/25 | Loss: 0.08128496
Iteration 3/25 | Loss: 0.08127379
Iteration 4/25 | Loss: 0.08127378
Iteration 5/25 | Loss: 0.08127376
Iteration 6/25 | Loss: 0.08127376
Iteration 7/25 | Loss: 0.08127376
Iteration 8/25 | Loss: 0.08127376
Iteration 9/25 | Loss: 0.08127376
Iteration 10/25 | Loss: 0.08127376
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.08127376437187195, 0.08127376437187195, 0.08127376437187195, 0.08127376437187195, 0.08127376437187195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08127376437187195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08127376
Iteration 2/1000 | Loss: 0.00081282
Iteration 3/1000 | Loss: 0.00020566
Iteration 4/1000 | Loss: 0.00010677
Iteration 5/1000 | Loss: 0.00014285
Iteration 6/1000 | Loss: 0.00012008
Iteration 7/1000 | Loss: 0.00004344
Iteration 8/1000 | Loss: 0.00012625
Iteration 9/1000 | Loss: 0.00004225
Iteration 10/1000 | Loss: 0.00013341
Iteration 11/1000 | Loss: 0.00015224
Iteration 12/1000 | Loss: 0.00003206
Iteration 13/1000 | Loss: 0.00002979
Iteration 14/1000 | Loss: 0.00006778
Iteration 15/1000 | Loss: 0.00002721
Iteration 16/1000 | Loss: 0.00002532
Iteration 17/1000 | Loss: 0.00015103
Iteration 18/1000 | Loss: 0.00002350
Iteration 19/1000 | Loss: 0.00003681
Iteration 20/1000 | Loss: 0.00006271
Iteration 21/1000 | Loss: 0.00010688
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00010237
Iteration 25/1000 | Loss: 0.00006905
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00009257
Iteration 28/1000 | Loss: 0.00004983
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00002948
Iteration 31/1000 | Loss: 0.00006441
Iteration 32/1000 | Loss: 0.00001793
Iteration 33/1000 | Loss: 0.00011071
Iteration 34/1000 | Loss: 0.00002552
Iteration 35/1000 | Loss: 0.00002612
Iteration 36/1000 | Loss: 0.00004408
Iteration 37/1000 | Loss: 0.00002765
Iteration 38/1000 | Loss: 0.00004369
Iteration 39/1000 | Loss: 0.00003942
Iteration 40/1000 | Loss: 0.00002433
Iteration 41/1000 | Loss: 0.00004503
Iteration 42/1000 | Loss: 0.00006451
Iteration 43/1000 | Loss: 0.00009775
Iteration 44/1000 | Loss: 0.00014689
Iteration 45/1000 | Loss: 0.00004111
Iteration 46/1000 | Loss: 0.00006799
Iteration 47/1000 | Loss: 0.00005000
Iteration 48/1000 | Loss: 0.00003461
Iteration 49/1000 | Loss: 0.00007342
Iteration 50/1000 | Loss: 0.00004622
Iteration 51/1000 | Loss: 0.00004219
Iteration 52/1000 | Loss: 0.00005972
Iteration 53/1000 | Loss: 0.00004350
Iteration 54/1000 | Loss: 0.00025431
Iteration 55/1000 | Loss: 0.00016829
Iteration 56/1000 | Loss: 0.00010440
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00004261
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00004013
Iteration 61/1000 | Loss: 0.00008174
Iteration 62/1000 | Loss: 0.00004262
Iteration 63/1000 | Loss: 0.00004929
Iteration 64/1000 | Loss: 0.00004657
Iteration 65/1000 | Loss: 0.00009676
Iteration 66/1000 | Loss: 0.00006563
Iteration 67/1000 | Loss: 0.00004231
Iteration 68/1000 | Loss: 0.00003362
Iteration 69/1000 | Loss: 0.00002910
Iteration 70/1000 | Loss: 0.00003385
Iteration 71/1000 | Loss: 0.00002360
Iteration 72/1000 | Loss: 0.00002705
Iteration 73/1000 | Loss: 0.00002562
Iteration 74/1000 | Loss: 0.00002592
Iteration 75/1000 | Loss: 0.00004974
Iteration 76/1000 | Loss: 0.00002575
Iteration 77/1000 | Loss: 0.00002806
Iteration 78/1000 | Loss: 0.00005304
Iteration 79/1000 | Loss: 0.00002852
Iteration 80/1000 | Loss: 0.00003700
Iteration 81/1000 | Loss: 0.00003465
Iteration 82/1000 | Loss: 0.00003626
Iteration 83/1000 | Loss: 0.00024174
Iteration 84/1000 | Loss: 0.00009695
Iteration 85/1000 | Loss: 0.00004005
Iteration 86/1000 | Loss: 0.00003909
Iteration 87/1000 | Loss: 0.00013930
Iteration 88/1000 | Loss: 0.00006518
Iteration 89/1000 | Loss: 0.00003517
Iteration 90/1000 | Loss: 0.00003602
Iteration 91/1000 | Loss: 0.00003887
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001667
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001652
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00004117
Iteration 112/1000 | Loss: 0.00001657
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00003082
Iteration 115/1000 | Loss: 0.00001648
Iteration 116/1000 | Loss: 0.00001647
Iteration 117/1000 | Loss: 0.00001645
Iteration 118/1000 | Loss: 0.00001644
Iteration 119/1000 | Loss: 0.00001644
Iteration 120/1000 | Loss: 0.00004495
Iteration 121/1000 | Loss: 0.00001647
Iteration 122/1000 | Loss: 0.00004399
Iteration 123/1000 | Loss: 0.00002005
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00004536
Iteration 126/1000 | Loss: 0.00002167
Iteration 127/1000 | Loss: 0.00009060
Iteration 128/1000 | Loss: 0.00002248
Iteration 129/1000 | Loss: 0.00006677
Iteration 130/1000 | Loss: 0.00002629
Iteration 131/1000 | Loss: 0.00004266
Iteration 132/1000 | Loss: 0.00003571
Iteration 133/1000 | Loss: 0.00004209
Iteration 134/1000 | Loss: 0.00003362
Iteration 135/1000 | Loss: 0.00007185
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001674
Iteration 140/1000 | Loss: 0.00001670
Iteration 141/1000 | Loss: 0.00001670
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001668
Iteration 145/1000 | Loss: 0.00001656
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001654
Iteration 148/1000 | Loss: 0.00001654
Iteration 149/1000 | Loss: 0.00001653
Iteration 150/1000 | Loss: 0.00001653
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001647
Iteration 154/1000 | Loss: 0.00001642
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001632
Iteration 157/1000 | Loss: 0.00001632
Iteration 158/1000 | Loss: 0.00001626
Iteration 159/1000 | Loss: 0.00013158
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00003105
Iteration 162/1000 | Loss: 0.00002011
Iteration 163/1000 | Loss: 0.00001615
Iteration 164/1000 | Loss: 0.00002923
Iteration 165/1000 | Loss: 0.00001616
Iteration 166/1000 | Loss: 0.00001615
Iteration 167/1000 | Loss: 0.00001615
Iteration 168/1000 | Loss: 0.00001614
Iteration 169/1000 | Loss: 0.00001613
Iteration 170/1000 | Loss: 0.00001613
Iteration 171/1000 | Loss: 0.00001613
Iteration 172/1000 | Loss: 0.00001613
Iteration 173/1000 | Loss: 0.00001612
Iteration 174/1000 | Loss: 0.00001612
Iteration 175/1000 | Loss: 0.00001612
Iteration 176/1000 | Loss: 0.00001612
Iteration 177/1000 | Loss: 0.00001612
Iteration 178/1000 | Loss: 0.00001612
Iteration 179/1000 | Loss: 0.00001612
Iteration 180/1000 | Loss: 0.00001612
Iteration 181/1000 | Loss: 0.00001612
Iteration 182/1000 | Loss: 0.00001611
Iteration 183/1000 | Loss: 0.00001611
Iteration 184/1000 | Loss: 0.00001611
Iteration 185/1000 | Loss: 0.00001611
Iteration 186/1000 | Loss: 0.00001611
Iteration 187/1000 | Loss: 0.00001610
Iteration 188/1000 | Loss: 0.00001610
Iteration 189/1000 | Loss: 0.00001610
Iteration 190/1000 | Loss: 0.00001609
Iteration 191/1000 | Loss: 0.00001609
Iteration 192/1000 | Loss: 0.00001609
Iteration 193/1000 | Loss: 0.00001609
Iteration 194/1000 | Loss: 0.00001609
Iteration 195/1000 | Loss: 0.00001608
Iteration 196/1000 | Loss: 0.00001608
Iteration 197/1000 | Loss: 0.00001607
Iteration 198/1000 | Loss: 0.00001607
Iteration 199/1000 | Loss: 0.00001607
Iteration 200/1000 | Loss: 0.00001606
Iteration 201/1000 | Loss: 0.00001606
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Iteration 206/1000 | Loss: 0.00001606
Iteration 207/1000 | Loss: 0.00001606
Iteration 208/1000 | Loss: 0.00001606
Iteration 209/1000 | Loss: 0.00001606
Iteration 210/1000 | Loss: 0.00001606
Iteration 211/1000 | Loss: 0.00001606
Iteration 212/1000 | Loss: 0.00001606
Iteration 213/1000 | Loss: 0.00001606
Iteration 214/1000 | Loss: 0.00001606
Iteration 215/1000 | Loss: 0.00001606
Iteration 216/1000 | Loss: 0.00001606
Iteration 217/1000 | Loss: 0.00001606
Iteration 218/1000 | Loss: 0.00001605
Iteration 219/1000 | Loss: 0.00001605
Iteration 220/1000 | Loss: 0.00001605
Iteration 221/1000 | Loss: 0.00001605
Iteration 222/1000 | Loss: 0.00001605
Iteration 223/1000 | Loss: 0.00001605
Iteration 224/1000 | Loss: 0.00001605
Iteration 225/1000 | Loss: 0.00001605
Iteration 226/1000 | Loss: 0.00001604
Iteration 227/1000 | Loss: 0.00001604
Iteration 228/1000 | Loss: 0.00001604
Iteration 229/1000 | Loss: 0.00001604
Iteration 230/1000 | Loss: 0.00001604
Iteration 231/1000 | Loss: 0.00001604
Iteration 232/1000 | Loss: 0.00001604
Iteration 233/1000 | Loss: 0.00001603
Iteration 234/1000 | Loss: 0.00001603
Iteration 235/1000 | Loss: 0.00001603
Iteration 236/1000 | Loss: 0.00001603
Iteration 237/1000 | Loss: 0.00001603
Iteration 238/1000 | Loss: 0.00001603
Iteration 239/1000 | Loss: 0.00001603
Iteration 240/1000 | Loss: 0.00001603
Iteration 241/1000 | Loss: 0.00001603
Iteration 242/1000 | Loss: 0.00001603
Iteration 243/1000 | Loss: 0.00001603
Iteration 244/1000 | Loss: 0.00001603
Iteration 245/1000 | Loss: 0.00001603
Iteration 246/1000 | Loss: 0.00001603
Iteration 247/1000 | Loss: 0.00001603
Iteration 248/1000 | Loss: 0.00001603
Iteration 249/1000 | Loss: 0.00001603
Iteration 250/1000 | Loss: 0.00001603
Iteration 251/1000 | Loss: 0.00001603
Iteration 252/1000 | Loss: 0.00001602
Iteration 253/1000 | Loss: 0.00001602
Iteration 254/1000 | Loss: 0.00001602
Iteration 255/1000 | Loss: 0.00001602
Iteration 256/1000 | Loss: 0.00001602
Iteration 257/1000 | Loss: 0.00001602
Iteration 258/1000 | Loss: 0.00001602
Iteration 259/1000 | Loss: 0.00001602
Iteration 260/1000 | Loss: 0.00001602
Iteration 261/1000 | Loss: 0.00001602
Iteration 262/1000 | Loss: 0.00001602
Iteration 263/1000 | Loss: 0.00001602
Iteration 264/1000 | Loss: 0.00001602
Iteration 265/1000 | Loss: 0.00001602
Iteration 266/1000 | Loss: 0.00001602
Iteration 267/1000 | Loss: 0.00001602
Iteration 268/1000 | Loss: 0.00001602
Iteration 269/1000 | Loss: 0.00001602
Iteration 270/1000 | Loss: 0.00001602
Iteration 271/1000 | Loss: 0.00001601
Iteration 272/1000 | Loss: 0.00001601
Iteration 273/1000 | Loss: 0.00001601
Iteration 274/1000 | Loss: 0.00001601
Iteration 275/1000 | Loss: 0.00001601
Iteration 276/1000 | Loss: 0.00001601
Iteration 277/1000 | Loss: 0.00001601
Iteration 278/1000 | Loss: 0.00001601
Iteration 279/1000 | Loss: 0.00001601
Iteration 280/1000 | Loss: 0.00001601
Iteration 281/1000 | Loss: 0.00001601
Iteration 282/1000 | Loss: 0.00001601
Iteration 283/1000 | Loss: 0.00001601
Iteration 284/1000 | Loss: 0.00001601
Iteration 285/1000 | Loss: 0.00001601
Iteration 286/1000 | Loss: 0.00001601
Iteration 287/1000 | Loss: 0.00001601
Iteration 288/1000 | Loss: 0.00001601
Iteration 289/1000 | Loss: 0.00001601
Iteration 290/1000 | Loss: 0.00001601
Iteration 291/1000 | Loss: 0.00001601
Iteration 292/1000 | Loss: 0.00001601
Iteration 293/1000 | Loss: 0.00001600
Iteration 294/1000 | Loss: 0.00001600
Iteration 295/1000 | Loss: 0.00001600
Iteration 296/1000 | Loss: 0.00001600
Iteration 297/1000 | Loss: 0.00001600
Iteration 298/1000 | Loss: 0.00001600
Iteration 299/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [1.6003590644686483e-05, 1.6003590644686483e-05, 1.6003590644686483e-05, 1.6003590644686483e-05, 1.6003590644686483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6003590644686483e-05

Optimization complete. Final v2v error: 2.6265342235565186 mm

Highest mean error: 21.162004470825195 mm for frame 109

Lowest mean error: 2.1815686225891113 mm for frame 6

Saving results

Total time: 196.21549367904663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742802
Iteration 2/25 | Loss: 0.00158676
Iteration 3/25 | Loss: 0.00114970
Iteration 4/25 | Loss: 0.00110893
Iteration 5/25 | Loss: 0.00112842
Iteration 6/25 | Loss: 0.00109597
Iteration 7/25 | Loss: 0.00109387
Iteration 8/25 | Loss: 0.00109393
Iteration 9/25 | Loss: 0.00109303
Iteration 10/25 | Loss: 0.00109210
Iteration 11/25 | Loss: 0.00109398
Iteration 12/25 | Loss: 0.00109238
Iteration 13/25 | Loss: 0.00109235
Iteration 14/25 | Loss: 0.00109307
Iteration 15/25 | Loss: 0.00109158
Iteration 16/25 | Loss: 0.00109341
Iteration 17/25 | Loss: 0.00109206
Iteration 18/25 | Loss: 0.00109232
Iteration 19/25 | Loss: 0.00108907
Iteration 20/25 | Loss: 0.00108958
Iteration 21/25 | Loss: 0.00108755
Iteration 22/25 | Loss: 0.00108749
Iteration 23/25 | Loss: 0.00108670
Iteration 24/25 | Loss: 0.00108857
Iteration 25/25 | Loss: 0.00108553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54022610
Iteration 2/25 | Loss: 0.00180914
Iteration 3/25 | Loss: 0.00180914
Iteration 4/25 | Loss: 0.00180914
Iteration 5/25 | Loss: 0.00180914
Iteration 6/25 | Loss: 0.00180914
Iteration 7/25 | Loss: 0.00180914
Iteration 8/25 | Loss: 0.00180914
Iteration 9/25 | Loss: 0.00180914
Iteration 10/25 | Loss: 0.00180914
Iteration 11/25 | Loss: 0.00180914
Iteration 12/25 | Loss: 0.00180914
Iteration 13/25 | Loss: 0.00180914
Iteration 14/25 | Loss: 0.00180914
Iteration 15/25 | Loss: 0.00180914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018091386882588267, 0.0018091386882588267, 0.0018091386882588267, 0.0018091386882588267, 0.0018091386882588267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018091386882588267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180914
Iteration 2/1000 | Loss: 0.00018992
Iteration 3/1000 | Loss: 0.00033620
Iteration 4/1000 | Loss: 0.00020173
Iteration 5/1000 | Loss: 0.00008412
Iteration 6/1000 | Loss: 0.00016500
Iteration 7/1000 | Loss: 0.00029218
Iteration 8/1000 | Loss: 0.00024449
Iteration 9/1000 | Loss: 0.00008404
Iteration 10/1000 | Loss: 0.00027651
Iteration 11/1000 | Loss: 0.00024841
Iteration 12/1000 | Loss: 0.00015745
Iteration 13/1000 | Loss: 0.00011619
Iteration 14/1000 | Loss: 0.00023332
Iteration 15/1000 | Loss: 0.00018761
Iteration 16/1000 | Loss: 0.00041711
Iteration 17/1000 | Loss: 0.00022801
Iteration 18/1000 | Loss: 0.00015159
Iteration 19/1000 | Loss: 0.00003329
Iteration 20/1000 | Loss: 0.00036050
Iteration 21/1000 | Loss: 0.00027431
Iteration 22/1000 | Loss: 0.00019951
Iteration 23/1000 | Loss: 0.00024544
Iteration 24/1000 | Loss: 0.00022339
Iteration 25/1000 | Loss: 0.00027041
Iteration 26/1000 | Loss: 0.00003184
Iteration 27/1000 | Loss: 0.00047278
Iteration 28/1000 | Loss: 0.00030007
Iteration 29/1000 | Loss: 0.00036052
Iteration 30/1000 | Loss: 0.00018351
Iteration 31/1000 | Loss: 0.00003229
Iteration 32/1000 | Loss: 0.00042542
Iteration 33/1000 | Loss: 0.00013008
Iteration 34/1000 | Loss: 0.00002961
Iteration 35/1000 | Loss: 0.00042393
Iteration 36/1000 | Loss: 0.00028502
Iteration 37/1000 | Loss: 0.00041313
Iteration 38/1000 | Loss: 0.00006791
Iteration 39/1000 | Loss: 0.00002841
Iteration 40/1000 | Loss: 0.00002610
Iteration 41/1000 | Loss: 0.00002378
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00002209
Iteration 44/1000 | Loss: 0.00002204
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002174
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002161
Iteration 49/1000 | Loss: 0.00002160
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002159
Iteration 54/1000 | Loss: 0.00002158
Iteration 55/1000 | Loss: 0.00002158
Iteration 56/1000 | Loss: 0.00002157
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002152
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002145
Iteration 62/1000 | Loss: 0.00002139
Iteration 63/1000 | Loss: 0.00002138
Iteration 64/1000 | Loss: 0.00002136
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002135
Iteration 68/1000 | Loss: 0.00002135
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002134
Iteration 73/1000 | Loss: 0.00002134
Iteration 74/1000 | Loss: 0.00002134
Iteration 75/1000 | Loss: 0.00002133
Iteration 76/1000 | Loss: 0.00002133
Iteration 77/1000 | Loss: 0.00002133
Iteration 78/1000 | Loss: 0.00002132
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002131
Iteration 87/1000 | Loss: 0.00002130
Iteration 88/1000 | Loss: 0.00002130
Iteration 89/1000 | Loss: 0.00002130
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002129
Iteration 92/1000 | Loss: 0.00002128
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002127
Iteration 96/1000 | Loss: 0.00002126
Iteration 97/1000 | Loss: 0.00002126
Iteration 98/1000 | Loss: 0.00002126
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002123
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002123
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002123
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002122
Iteration 113/1000 | Loss: 0.00002122
Iteration 114/1000 | Loss: 0.00002121
Iteration 115/1000 | Loss: 0.00002121
Iteration 116/1000 | Loss: 0.00002121
Iteration 117/1000 | Loss: 0.00002120
Iteration 118/1000 | Loss: 0.00002120
Iteration 119/1000 | Loss: 0.00002119
Iteration 120/1000 | Loss: 0.00002119
Iteration 121/1000 | Loss: 0.00002118
Iteration 122/1000 | Loss: 0.00002118
Iteration 123/1000 | Loss: 0.00002118
Iteration 124/1000 | Loss: 0.00002117
Iteration 125/1000 | Loss: 0.00002117
Iteration 126/1000 | Loss: 0.00002117
Iteration 127/1000 | Loss: 0.00002116
Iteration 128/1000 | Loss: 0.00002116
Iteration 129/1000 | Loss: 0.00002116
Iteration 130/1000 | Loss: 0.00002116
Iteration 131/1000 | Loss: 0.00002115
Iteration 132/1000 | Loss: 0.00002114
Iteration 133/1000 | Loss: 0.00002114
Iteration 134/1000 | Loss: 0.00002114
Iteration 135/1000 | Loss: 0.00002114
Iteration 136/1000 | Loss: 0.00002112
Iteration 137/1000 | Loss: 0.00002112
Iteration 138/1000 | Loss: 0.00002112
Iteration 139/1000 | Loss: 0.00002112
Iteration 140/1000 | Loss: 0.00002112
Iteration 141/1000 | Loss: 0.00002111
Iteration 142/1000 | Loss: 0.00002111
Iteration 143/1000 | Loss: 0.00002110
Iteration 144/1000 | Loss: 0.00002110
Iteration 145/1000 | Loss: 0.00002110
Iteration 146/1000 | Loss: 0.00002108
Iteration 147/1000 | Loss: 0.00002108
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002107
Iteration 150/1000 | Loss: 0.00002106
Iteration 151/1000 | Loss: 0.00002106
Iteration 152/1000 | Loss: 0.00002106
Iteration 153/1000 | Loss: 0.00002105
Iteration 154/1000 | Loss: 0.00002105
Iteration 155/1000 | Loss: 0.00002105
Iteration 156/1000 | Loss: 0.00002104
Iteration 157/1000 | Loss: 0.00002104
Iteration 158/1000 | Loss: 0.00002104
Iteration 159/1000 | Loss: 0.00002103
Iteration 160/1000 | Loss: 0.00002103
Iteration 161/1000 | Loss: 0.00002103
Iteration 162/1000 | Loss: 0.00002103
Iteration 163/1000 | Loss: 0.00002102
Iteration 164/1000 | Loss: 0.00002102
Iteration 165/1000 | Loss: 0.00002102
Iteration 166/1000 | Loss: 0.00002102
Iteration 167/1000 | Loss: 0.00002102
Iteration 168/1000 | Loss: 0.00002102
Iteration 169/1000 | Loss: 0.00002101
Iteration 170/1000 | Loss: 0.00002101
Iteration 171/1000 | Loss: 0.00002101
Iteration 172/1000 | Loss: 0.00002100
Iteration 173/1000 | Loss: 0.00002100
Iteration 174/1000 | Loss: 0.00002100
Iteration 175/1000 | Loss: 0.00002100
Iteration 176/1000 | Loss: 0.00002100
Iteration 177/1000 | Loss: 0.00002100
Iteration 178/1000 | Loss: 0.00002100
Iteration 179/1000 | Loss: 0.00002100
Iteration 180/1000 | Loss: 0.00002099
Iteration 181/1000 | Loss: 0.00002099
Iteration 182/1000 | Loss: 0.00002099
Iteration 183/1000 | Loss: 0.00002099
Iteration 184/1000 | Loss: 0.00002099
Iteration 185/1000 | Loss: 0.00002099
Iteration 186/1000 | Loss: 0.00002098
Iteration 187/1000 | Loss: 0.00002098
Iteration 188/1000 | Loss: 0.00002098
Iteration 189/1000 | Loss: 0.00002098
Iteration 190/1000 | Loss: 0.00002098
Iteration 191/1000 | Loss: 0.00002098
Iteration 192/1000 | Loss: 0.00002098
Iteration 193/1000 | Loss: 0.00002097
Iteration 194/1000 | Loss: 0.00002097
Iteration 195/1000 | Loss: 0.00002097
Iteration 196/1000 | Loss: 0.00002097
Iteration 197/1000 | Loss: 0.00002096
Iteration 198/1000 | Loss: 0.00002096
Iteration 199/1000 | Loss: 0.00002096
Iteration 200/1000 | Loss: 0.00002096
Iteration 201/1000 | Loss: 0.00002096
Iteration 202/1000 | Loss: 0.00002096
Iteration 203/1000 | Loss: 0.00002096
Iteration 204/1000 | Loss: 0.00002096
Iteration 205/1000 | Loss: 0.00002096
Iteration 206/1000 | Loss: 0.00002095
Iteration 207/1000 | Loss: 0.00002095
Iteration 208/1000 | Loss: 0.00002095
Iteration 209/1000 | Loss: 0.00002095
Iteration 210/1000 | Loss: 0.00002095
Iteration 211/1000 | Loss: 0.00002095
Iteration 212/1000 | Loss: 0.00002095
Iteration 213/1000 | Loss: 0.00002095
Iteration 214/1000 | Loss: 0.00002095
Iteration 215/1000 | Loss: 0.00002095
Iteration 216/1000 | Loss: 0.00002094
Iteration 217/1000 | Loss: 0.00002094
Iteration 218/1000 | Loss: 0.00002094
Iteration 219/1000 | Loss: 0.00002094
Iteration 220/1000 | Loss: 0.00002094
Iteration 221/1000 | Loss: 0.00002094
Iteration 222/1000 | Loss: 0.00002094
Iteration 223/1000 | Loss: 0.00002094
Iteration 224/1000 | Loss: 0.00002094
Iteration 225/1000 | Loss: 0.00002093
Iteration 226/1000 | Loss: 0.00002093
Iteration 227/1000 | Loss: 0.00002093
Iteration 228/1000 | Loss: 0.00002093
Iteration 229/1000 | Loss: 0.00002093
Iteration 230/1000 | Loss: 0.00002093
Iteration 231/1000 | Loss: 0.00002093
Iteration 232/1000 | Loss: 0.00002093
Iteration 233/1000 | Loss: 0.00002092
Iteration 234/1000 | Loss: 0.00002092
Iteration 235/1000 | Loss: 0.00002092
Iteration 236/1000 | Loss: 0.00002092
Iteration 237/1000 | Loss: 0.00002092
Iteration 238/1000 | Loss: 0.00002092
Iteration 239/1000 | Loss: 0.00002092
Iteration 240/1000 | Loss: 0.00002092
Iteration 241/1000 | Loss: 0.00002092
Iteration 242/1000 | Loss: 0.00002092
Iteration 243/1000 | Loss: 0.00002092
Iteration 244/1000 | Loss: 0.00002092
Iteration 245/1000 | Loss: 0.00002091
Iteration 246/1000 | Loss: 0.00002091
Iteration 247/1000 | Loss: 0.00002091
Iteration 248/1000 | Loss: 0.00002091
Iteration 249/1000 | Loss: 0.00002091
Iteration 250/1000 | Loss: 0.00002091
Iteration 251/1000 | Loss: 0.00002091
Iteration 252/1000 | Loss: 0.00002091
Iteration 253/1000 | Loss: 0.00002091
Iteration 254/1000 | Loss: 0.00002091
Iteration 255/1000 | Loss: 0.00002091
Iteration 256/1000 | Loss: 0.00002091
Iteration 257/1000 | Loss: 0.00002091
Iteration 258/1000 | Loss: 0.00002091
Iteration 259/1000 | Loss: 0.00002091
Iteration 260/1000 | Loss: 0.00002091
Iteration 261/1000 | Loss: 0.00002091
Iteration 262/1000 | Loss: 0.00002091
Iteration 263/1000 | Loss: 0.00002091
Iteration 264/1000 | Loss: 0.00002091
Iteration 265/1000 | Loss: 0.00002091
Iteration 266/1000 | Loss: 0.00002091
Iteration 267/1000 | Loss: 0.00002091
Iteration 268/1000 | Loss: 0.00002091
Iteration 269/1000 | Loss: 0.00002091
Iteration 270/1000 | Loss: 0.00002091
Iteration 271/1000 | Loss: 0.00002091
Iteration 272/1000 | Loss: 0.00002091
Iteration 273/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [2.0911274987156503e-05, 2.0911274987156503e-05, 2.0911274987156503e-05, 2.0911274987156503e-05, 2.0911274987156503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0911274987156503e-05

Optimization complete. Final v2v error: 3.578606128692627 mm

Highest mean error: 9.310832977294922 mm for frame 22

Lowest mean error: 2.6426329612731934 mm for frame 189

Saving results

Total time: 151.54772090911865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986640
Iteration 2/25 | Loss: 0.00166595
Iteration 3/25 | Loss: 0.00121207
Iteration 4/25 | Loss: 0.00118099
Iteration 5/25 | Loss: 0.00117061
Iteration 6/25 | Loss: 0.00116807
Iteration 7/25 | Loss: 0.00116807
Iteration 8/25 | Loss: 0.00116807
Iteration 9/25 | Loss: 0.00116807
Iteration 10/25 | Loss: 0.00116807
Iteration 11/25 | Loss: 0.00116807
Iteration 12/25 | Loss: 0.00116807
Iteration 13/25 | Loss: 0.00116807
Iteration 14/25 | Loss: 0.00116807
Iteration 15/25 | Loss: 0.00116807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011680674506351352, 0.0011680674506351352, 0.0011680674506351352, 0.0011680674506351352, 0.0011680674506351352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011680674506351352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79527009
Iteration 2/25 | Loss: 0.00167459
Iteration 3/25 | Loss: 0.00167459
Iteration 4/25 | Loss: 0.00167459
Iteration 5/25 | Loss: 0.00167459
Iteration 6/25 | Loss: 0.00167459
Iteration 7/25 | Loss: 0.00167459
Iteration 8/25 | Loss: 0.00167459
Iteration 9/25 | Loss: 0.00167459
Iteration 10/25 | Loss: 0.00167459
Iteration 11/25 | Loss: 0.00167459
Iteration 12/25 | Loss: 0.00167459
Iteration 13/25 | Loss: 0.00167459
Iteration 14/25 | Loss: 0.00167459
Iteration 15/25 | Loss: 0.00167459
Iteration 16/25 | Loss: 0.00167459
Iteration 17/25 | Loss: 0.00167459
Iteration 18/25 | Loss: 0.00167459
Iteration 19/25 | Loss: 0.00167459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016745897009968758, 0.0016745897009968758, 0.0016745897009968758, 0.0016745897009968758, 0.0016745897009968758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016745897009968758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167459
Iteration 2/1000 | Loss: 0.00005882
Iteration 3/1000 | Loss: 0.00004554
Iteration 4/1000 | Loss: 0.00004243
Iteration 5/1000 | Loss: 0.00004012
Iteration 6/1000 | Loss: 0.00003888
Iteration 7/1000 | Loss: 0.00003823
Iteration 8/1000 | Loss: 0.00003774
Iteration 9/1000 | Loss: 0.00003725
Iteration 10/1000 | Loss: 0.00003689
Iteration 11/1000 | Loss: 0.00003662
Iteration 12/1000 | Loss: 0.00003638
Iteration 13/1000 | Loss: 0.00003608
Iteration 14/1000 | Loss: 0.00003572
Iteration 15/1000 | Loss: 0.00003543
Iteration 16/1000 | Loss: 0.00003524
Iteration 17/1000 | Loss: 0.00003505
Iteration 18/1000 | Loss: 0.00003492
Iteration 19/1000 | Loss: 0.00003479
Iteration 20/1000 | Loss: 0.00003472
Iteration 21/1000 | Loss: 0.00003466
Iteration 22/1000 | Loss: 0.00003462
Iteration 23/1000 | Loss: 0.00003461
Iteration 24/1000 | Loss: 0.00003459
Iteration 25/1000 | Loss: 0.00003457
Iteration 26/1000 | Loss: 0.00003457
Iteration 27/1000 | Loss: 0.00003455
Iteration 28/1000 | Loss: 0.00003454
Iteration 29/1000 | Loss: 0.00003454
Iteration 30/1000 | Loss: 0.00003453
Iteration 31/1000 | Loss: 0.00003453
Iteration 32/1000 | Loss: 0.00003453
Iteration 33/1000 | Loss: 0.00003452
Iteration 34/1000 | Loss: 0.00003452
Iteration 35/1000 | Loss: 0.00003452
Iteration 36/1000 | Loss: 0.00003452
Iteration 37/1000 | Loss: 0.00003452
Iteration 38/1000 | Loss: 0.00003452
Iteration 39/1000 | Loss: 0.00003452
Iteration 40/1000 | Loss: 0.00003452
Iteration 41/1000 | Loss: 0.00003452
Iteration 42/1000 | Loss: 0.00003452
Iteration 43/1000 | Loss: 0.00003451
Iteration 44/1000 | Loss: 0.00003451
Iteration 45/1000 | Loss: 0.00003451
Iteration 46/1000 | Loss: 0.00003451
Iteration 47/1000 | Loss: 0.00003451
Iteration 48/1000 | Loss: 0.00003450
Iteration 49/1000 | Loss: 0.00003450
Iteration 50/1000 | Loss: 0.00003450
Iteration 51/1000 | Loss: 0.00003450
Iteration 52/1000 | Loss: 0.00003450
Iteration 53/1000 | Loss: 0.00003450
Iteration 54/1000 | Loss: 0.00003449
Iteration 55/1000 | Loss: 0.00003449
Iteration 56/1000 | Loss: 0.00003449
Iteration 57/1000 | Loss: 0.00003449
Iteration 58/1000 | Loss: 0.00003449
Iteration 59/1000 | Loss: 0.00003449
Iteration 60/1000 | Loss: 0.00003449
Iteration 61/1000 | Loss: 0.00003449
Iteration 62/1000 | Loss: 0.00003449
Iteration 63/1000 | Loss: 0.00003449
Iteration 64/1000 | Loss: 0.00003448
Iteration 65/1000 | Loss: 0.00003448
Iteration 66/1000 | Loss: 0.00003448
Iteration 67/1000 | Loss: 0.00003448
Iteration 68/1000 | Loss: 0.00003448
Iteration 69/1000 | Loss: 0.00003448
Iteration 70/1000 | Loss: 0.00003447
Iteration 71/1000 | Loss: 0.00003447
Iteration 72/1000 | Loss: 0.00003447
Iteration 73/1000 | Loss: 0.00003447
Iteration 74/1000 | Loss: 0.00003447
Iteration 75/1000 | Loss: 0.00003447
Iteration 76/1000 | Loss: 0.00003447
Iteration 77/1000 | Loss: 0.00003447
Iteration 78/1000 | Loss: 0.00003446
Iteration 79/1000 | Loss: 0.00003446
Iteration 80/1000 | Loss: 0.00003446
Iteration 81/1000 | Loss: 0.00003446
Iteration 82/1000 | Loss: 0.00003446
Iteration 83/1000 | Loss: 0.00003446
Iteration 84/1000 | Loss: 0.00003446
Iteration 85/1000 | Loss: 0.00003446
Iteration 86/1000 | Loss: 0.00003446
Iteration 87/1000 | Loss: 0.00003446
Iteration 88/1000 | Loss: 0.00003446
Iteration 89/1000 | Loss: 0.00003446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.4457334550097585e-05, 3.4457334550097585e-05, 3.4457334550097585e-05, 3.4457334550097585e-05, 3.4457334550097585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4457334550097585e-05

Optimization complete. Final v2v error: 4.776978492736816 mm

Highest mean error: 5.372501850128174 mm for frame 95

Lowest mean error: 3.7791779041290283 mm for frame 49

Saving results

Total time: 49.92539381980896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00329626
Iteration 2/25 | Loss: 0.00115840
Iteration 3/25 | Loss: 0.00101423
Iteration 4/25 | Loss: 0.00099348
Iteration 5/25 | Loss: 0.00098855
Iteration 6/25 | Loss: 0.00098690
Iteration 7/25 | Loss: 0.00098686
Iteration 8/25 | Loss: 0.00098686
Iteration 9/25 | Loss: 0.00098686
Iteration 10/25 | Loss: 0.00098686
Iteration 11/25 | Loss: 0.00098686
Iteration 12/25 | Loss: 0.00098686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000986855011433363, 0.000986855011433363, 0.000986855011433363, 0.000986855011433363, 0.000986855011433363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000986855011433363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16900969
Iteration 2/25 | Loss: 0.00266832
Iteration 3/25 | Loss: 0.00266832
Iteration 4/25 | Loss: 0.00266832
Iteration 5/25 | Loss: 0.00266832
Iteration 6/25 | Loss: 0.00266832
Iteration 7/25 | Loss: 0.00266832
Iteration 8/25 | Loss: 0.00266832
Iteration 9/25 | Loss: 0.00266832
Iteration 10/25 | Loss: 0.00266832
Iteration 11/25 | Loss: 0.00266832
Iteration 12/25 | Loss: 0.00266832
Iteration 13/25 | Loss: 0.00266832
Iteration 14/25 | Loss: 0.00266832
Iteration 15/25 | Loss: 0.00266832
Iteration 16/25 | Loss: 0.00266832
Iteration 17/25 | Loss: 0.00266832
Iteration 18/25 | Loss: 0.00266832
Iteration 19/25 | Loss: 0.00266832
Iteration 20/25 | Loss: 0.00266832
Iteration 21/25 | Loss: 0.00266832
Iteration 22/25 | Loss: 0.00266832
Iteration 23/25 | Loss: 0.00266832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002668321132659912, 0.002668321132659912, 0.002668321132659912, 0.002668321132659912, 0.002668321132659912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002668321132659912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266832
Iteration 2/1000 | Loss: 0.00003658
Iteration 3/1000 | Loss: 0.00002191
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001243
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001220
Iteration 12/1000 | Loss: 0.00001214
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001201
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001173
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001171
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001170
Iteration 70/1000 | Loss: 0.00001170
Iteration 71/1000 | Loss: 0.00001170
Iteration 72/1000 | Loss: 0.00001170
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001170
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1587424523895606e-05, 1.1587424523895606e-05, 1.1587424523895606e-05, 1.1587424523895606e-05, 1.1587424523895606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1587424523895606e-05

Optimization complete. Final v2v error: 2.834261178970337 mm

Highest mean error: 3.6511709690093994 mm for frame 205

Lowest mean error: 2.0522351264953613 mm for frame 112

Saving results

Total time: 40.30480670928955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870027
Iteration 2/25 | Loss: 0.00111445
Iteration 3/25 | Loss: 0.00095979
Iteration 4/25 | Loss: 0.00094518
Iteration 5/25 | Loss: 0.00094215
Iteration 6/25 | Loss: 0.00094143
Iteration 7/25 | Loss: 0.00094143
Iteration 8/25 | Loss: 0.00094143
Iteration 9/25 | Loss: 0.00094143
Iteration 10/25 | Loss: 0.00094143
Iteration 11/25 | Loss: 0.00094143
Iteration 12/25 | Loss: 0.00094143
Iteration 13/25 | Loss: 0.00094143
Iteration 14/25 | Loss: 0.00094143
Iteration 15/25 | Loss: 0.00094143
Iteration 16/25 | Loss: 0.00094143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009414334199391305, 0.0009414334199391305, 0.0009414334199391305, 0.0009414334199391305, 0.0009414334199391305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009414334199391305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21703303
Iteration 2/25 | Loss: 0.00169443
Iteration 3/25 | Loss: 0.00169441
Iteration 4/25 | Loss: 0.00169441
Iteration 5/25 | Loss: 0.00169441
Iteration 6/25 | Loss: 0.00169441
Iteration 7/25 | Loss: 0.00169441
Iteration 8/25 | Loss: 0.00169441
Iteration 9/25 | Loss: 0.00169441
Iteration 10/25 | Loss: 0.00169441
Iteration 11/25 | Loss: 0.00169441
Iteration 12/25 | Loss: 0.00169441
Iteration 13/25 | Loss: 0.00169441
Iteration 14/25 | Loss: 0.00169441
Iteration 15/25 | Loss: 0.00169441
Iteration 16/25 | Loss: 0.00169441
Iteration 17/25 | Loss: 0.00169441
Iteration 18/25 | Loss: 0.00169441
Iteration 19/25 | Loss: 0.00169441
Iteration 20/25 | Loss: 0.00169441
Iteration 21/25 | Loss: 0.00169441
Iteration 22/25 | Loss: 0.00169441
Iteration 23/25 | Loss: 0.00169441
Iteration 24/25 | Loss: 0.00169441
Iteration 25/25 | Loss: 0.00169441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169441
Iteration 2/1000 | Loss: 0.00001931
Iteration 3/1000 | Loss: 0.00001066
Iteration 4/1000 | Loss: 0.00000914
Iteration 5/1000 | Loss: 0.00000814
Iteration 6/1000 | Loss: 0.00000769
Iteration 7/1000 | Loss: 0.00000720
Iteration 8/1000 | Loss: 0.00000706
Iteration 9/1000 | Loss: 0.00000691
Iteration 10/1000 | Loss: 0.00000671
Iteration 11/1000 | Loss: 0.00000664
Iteration 12/1000 | Loss: 0.00000657
Iteration 13/1000 | Loss: 0.00000653
Iteration 14/1000 | Loss: 0.00000652
Iteration 15/1000 | Loss: 0.00000651
Iteration 16/1000 | Loss: 0.00000651
Iteration 17/1000 | Loss: 0.00000647
Iteration 18/1000 | Loss: 0.00000642
Iteration 19/1000 | Loss: 0.00000640
Iteration 20/1000 | Loss: 0.00000639
Iteration 21/1000 | Loss: 0.00000639
Iteration 22/1000 | Loss: 0.00000639
Iteration 23/1000 | Loss: 0.00000638
Iteration 24/1000 | Loss: 0.00000636
Iteration 25/1000 | Loss: 0.00000636
Iteration 26/1000 | Loss: 0.00000636
Iteration 27/1000 | Loss: 0.00000635
Iteration 28/1000 | Loss: 0.00000635
Iteration 29/1000 | Loss: 0.00000634
Iteration 30/1000 | Loss: 0.00000634
Iteration 31/1000 | Loss: 0.00000634
Iteration 32/1000 | Loss: 0.00000634
Iteration 33/1000 | Loss: 0.00000634
Iteration 34/1000 | Loss: 0.00000634
Iteration 35/1000 | Loss: 0.00000634
Iteration 36/1000 | Loss: 0.00000634
Iteration 37/1000 | Loss: 0.00000633
Iteration 38/1000 | Loss: 0.00000633
Iteration 39/1000 | Loss: 0.00000633
Iteration 40/1000 | Loss: 0.00000633
Iteration 41/1000 | Loss: 0.00000632
Iteration 42/1000 | Loss: 0.00000631
Iteration 43/1000 | Loss: 0.00000631
Iteration 44/1000 | Loss: 0.00000631
Iteration 45/1000 | Loss: 0.00000631
Iteration 46/1000 | Loss: 0.00000631
Iteration 47/1000 | Loss: 0.00000631
Iteration 48/1000 | Loss: 0.00000631
Iteration 49/1000 | Loss: 0.00000631
Iteration 50/1000 | Loss: 0.00000631
Iteration 51/1000 | Loss: 0.00000631
Iteration 52/1000 | Loss: 0.00000631
Iteration 53/1000 | Loss: 0.00000631
Iteration 54/1000 | Loss: 0.00000631
Iteration 55/1000 | Loss: 0.00000631
Iteration 56/1000 | Loss: 0.00000631
Iteration 57/1000 | Loss: 0.00000631
Iteration 58/1000 | Loss: 0.00000631
Iteration 59/1000 | Loss: 0.00000631
Iteration 60/1000 | Loss: 0.00000631
Iteration 61/1000 | Loss: 0.00000631
Iteration 62/1000 | Loss: 0.00000631
Iteration 63/1000 | Loss: 0.00000631
Iteration 64/1000 | Loss: 0.00000631
Iteration 65/1000 | Loss: 0.00000631
Iteration 66/1000 | Loss: 0.00000631
Iteration 67/1000 | Loss: 0.00000631
Iteration 68/1000 | Loss: 0.00000631
Iteration 69/1000 | Loss: 0.00000631
Iteration 70/1000 | Loss: 0.00000631
Iteration 71/1000 | Loss: 0.00000631
Iteration 72/1000 | Loss: 0.00000631
Iteration 73/1000 | Loss: 0.00000631
Iteration 74/1000 | Loss: 0.00000631
Iteration 75/1000 | Loss: 0.00000631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [6.3097600104811136e-06, 6.3097600104811136e-06, 6.3097600104811136e-06, 6.3097600104811136e-06, 6.3097600104811136e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.3097600104811136e-06

Optimization complete. Final v2v error: 2.1243836879730225 mm

Highest mean error: 2.3238749504089355 mm for frame 27

Lowest mean error: 2.0165562629699707 mm for frame 13

Saving results

Total time: 28.87320351600647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081645
Iteration 2/25 | Loss: 0.00205385
Iteration 3/25 | Loss: 0.00145450
Iteration 4/25 | Loss: 0.00130206
Iteration 5/25 | Loss: 0.00119768
Iteration 6/25 | Loss: 0.00126294
Iteration 7/25 | Loss: 0.00117798
Iteration 8/25 | Loss: 0.00112748
Iteration 9/25 | Loss: 0.00111572
Iteration 10/25 | Loss: 0.00106112
Iteration 11/25 | Loss: 0.00104386
Iteration 12/25 | Loss: 0.00105994
Iteration 13/25 | Loss: 0.00103438
Iteration 14/25 | Loss: 0.00103002
Iteration 15/25 | Loss: 0.00102920
Iteration 16/25 | Loss: 0.00103112
Iteration 17/25 | Loss: 0.00102703
Iteration 18/25 | Loss: 0.00102472
Iteration 19/25 | Loss: 0.00102420
Iteration 20/25 | Loss: 0.00102411
Iteration 21/25 | Loss: 0.00102393
Iteration 22/25 | Loss: 0.00102387
Iteration 23/25 | Loss: 0.00102387
Iteration 24/25 | Loss: 0.00102387
Iteration 25/25 | Loss: 0.00102387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19076002
Iteration 2/25 | Loss: 0.00175678
Iteration 3/25 | Loss: 0.00175678
Iteration 4/25 | Loss: 0.00175678
Iteration 5/25 | Loss: 0.00175678
Iteration 6/25 | Loss: 0.00175678
Iteration 7/25 | Loss: 0.00175678
Iteration 8/25 | Loss: 0.00175678
Iteration 9/25 | Loss: 0.00175678
Iteration 10/25 | Loss: 0.00175678
Iteration 11/25 | Loss: 0.00175678
Iteration 12/25 | Loss: 0.00175678
Iteration 13/25 | Loss: 0.00175678
Iteration 14/25 | Loss: 0.00175678
Iteration 15/25 | Loss: 0.00175678
Iteration 16/25 | Loss: 0.00175678
Iteration 17/25 | Loss: 0.00175678
Iteration 18/25 | Loss: 0.00175678
Iteration 19/25 | Loss: 0.00175678
Iteration 20/25 | Loss: 0.00175678
Iteration 21/25 | Loss: 0.00175678
Iteration 22/25 | Loss: 0.00175678
Iteration 23/25 | Loss: 0.00175678
Iteration 24/25 | Loss: 0.00175678
Iteration 25/25 | Loss: 0.00175678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017567771719768643, 0.0017567771719768643, 0.0017567771719768643, 0.0017567771719768643, 0.0017567771719768643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017567771719768643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175678
Iteration 2/1000 | Loss: 0.00002869
Iteration 3/1000 | Loss: 0.00002240
Iteration 4/1000 | Loss: 0.00002089
Iteration 5/1000 | Loss: 0.00001990
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001869
Iteration 8/1000 | Loss: 0.00001831
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001779
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001747
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001744
Iteration 47/1000 | Loss: 0.00001744
Iteration 48/1000 | Loss: 0.00001744
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001744
Iteration 55/1000 | Loss: 0.00001744
Iteration 56/1000 | Loss: 0.00001744
Iteration 57/1000 | Loss: 0.00001743
Iteration 58/1000 | Loss: 0.00001743
Iteration 59/1000 | Loss: 0.00001743
Iteration 60/1000 | Loss: 0.00001743
Iteration 61/1000 | Loss: 0.00001743
Iteration 62/1000 | Loss: 0.00001743
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.742536733218003e-05, 1.742536733218003e-05, 1.742536733218003e-05, 1.742536733218003e-05, 1.742536733218003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.742536733218003e-05

Optimization complete. Final v2v error: 3.458529472351074 mm

Highest mean error: 3.749498128890991 mm for frame 239

Lowest mean error: 3.3071157932281494 mm for frame 162

Saving results

Total time: 68.1667058467865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996368
Iteration 2/25 | Loss: 0.00171296
Iteration 3/25 | Loss: 0.00122188
Iteration 4/25 | Loss: 0.00108057
Iteration 5/25 | Loss: 0.00104747
Iteration 6/25 | Loss: 0.00103811
Iteration 7/25 | Loss: 0.00103720
Iteration 8/25 | Loss: 0.00103691
Iteration 9/25 | Loss: 0.00103683
Iteration 10/25 | Loss: 0.00103682
Iteration 11/25 | Loss: 0.00103682
Iteration 12/25 | Loss: 0.00103681
Iteration 13/25 | Loss: 0.00103681
Iteration 14/25 | Loss: 0.00103681
Iteration 15/25 | Loss: 0.00103681
Iteration 16/25 | Loss: 0.00103681
Iteration 17/25 | Loss: 0.00103681
Iteration 18/25 | Loss: 0.00103681
Iteration 19/25 | Loss: 0.00103681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001036805915646255, 0.001036805915646255, 0.001036805915646255, 0.001036805915646255, 0.001036805915646255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001036805915646255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39554393
Iteration 2/25 | Loss: 0.00191857
Iteration 3/25 | Loss: 0.00191857
Iteration 4/25 | Loss: 0.00191856
Iteration 5/25 | Loss: 0.00191856
Iteration 6/25 | Loss: 0.00191856
Iteration 7/25 | Loss: 0.00191856
Iteration 8/25 | Loss: 0.00191856
Iteration 9/25 | Loss: 0.00191856
Iteration 10/25 | Loss: 0.00191856
Iteration 11/25 | Loss: 0.00191856
Iteration 12/25 | Loss: 0.00191856
Iteration 13/25 | Loss: 0.00191856
Iteration 14/25 | Loss: 0.00191856
Iteration 15/25 | Loss: 0.00191856
Iteration 16/25 | Loss: 0.00191856
Iteration 17/25 | Loss: 0.00191856
Iteration 18/25 | Loss: 0.00191856
Iteration 19/25 | Loss: 0.00191856
Iteration 20/25 | Loss: 0.00191856
Iteration 21/25 | Loss: 0.00191856
Iteration 22/25 | Loss: 0.00191856
Iteration 23/25 | Loss: 0.00191856
Iteration 24/25 | Loss: 0.00191856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019185631535947323, 0.0019185631535947323, 0.0019185631535947323, 0.0019185631535947323, 0.0019185631535947323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019185631535947323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191856
Iteration 2/1000 | Loss: 0.00003906
Iteration 3/1000 | Loss: 0.00002395
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001467
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001308
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001302
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001295
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001286
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001284
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001283
Iteration 47/1000 | Loss: 0.00001283
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001281
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001276
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.2759050150634721e-05, 1.2759050150634721e-05, 1.2759050150634721e-05, 1.2759050150634721e-05, 1.2759050150634721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2759050150634721e-05

Optimization complete. Final v2v error: 3.017392635345459 mm

Highest mean error: 3.987370014190674 mm for frame 165

Lowest mean error: 2.538235902786255 mm for frame 138

Saving results

Total time: 47.064292430877686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446177
Iteration 2/25 | Loss: 0.00124379
Iteration 3/25 | Loss: 0.00103231
Iteration 4/25 | Loss: 0.00101367
Iteration 5/25 | Loss: 0.00101177
Iteration 6/25 | Loss: 0.00101177
Iteration 7/25 | Loss: 0.00101177
Iteration 8/25 | Loss: 0.00101177
Iteration 9/25 | Loss: 0.00101177
Iteration 10/25 | Loss: 0.00101177
Iteration 11/25 | Loss: 0.00101177
Iteration 12/25 | Loss: 0.00101177
Iteration 13/25 | Loss: 0.00101177
Iteration 14/25 | Loss: 0.00101177
Iteration 15/25 | Loss: 0.00101177
Iteration 16/25 | Loss: 0.00101177
Iteration 17/25 | Loss: 0.00101177
Iteration 18/25 | Loss: 0.00101177
Iteration 19/25 | Loss: 0.00101177
Iteration 20/25 | Loss: 0.00101177
Iteration 21/25 | Loss: 0.00101177
Iteration 22/25 | Loss: 0.00101177
Iteration 23/25 | Loss: 0.00101177
Iteration 24/25 | Loss: 0.00101177
Iteration 25/25 | Loss: 0.00101177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17435789
Iteration 2/25 | Loss: 0.00176502
Iteration 3/25 | Loss: 0.00176502
Iteration 4/25 | Loss: 0.00176502
Iteration 5/25 | Loss: 0.00176502
Iteration 6/25 | Loss: 0.00176502
Iteration 7/25 | Loss: 0.00176502
Iteration 8/25 | Loss: 0.00176502
Iteration 9/25 | Loss: 0.00176502
Iteration 10/25 | Loss: 0.00176502
Iteration 11/25 | Loss: 0.00176502
Iteration 12/25 | Loss: 0.00176502
Iteration 13/25 | Loss: 0.00176502
Iteration 14/25 | Loss: 0.00176502
Iteration 15/25 | Loss: 0.00176502
Iteration 16/25 | Loss: 0.00176502
Iteration 17/25 | Loss: 0.00176502
Iteration 18/25 | Loss: 0.00176502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017650173977017403, 0.0017650173977017403, 0.0017650173977017403, 0.0017650173977017403, 0.0017650173977017403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017650173977017403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176502
Iteration 2/1000 | Loss: 0.00002500
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001150
Iteration 5/1000 | Loss: 0.00001046
Iteration 6/1000 | Loss: 0.00001001
Iteration 7/1000 | Loss: 0.00000948
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000870
Iteration 10/1000 | Loss: 0.00000835
Iteration 11/1000 | Loss: 0.00000821
Iteration 12/1000 | Loss: 0.00000805
Iteration 13/1000 | Loss: 0.00000802
Iteration 14/1000 | Loss: 0.00000801
Iteration 15/1000 | Loss: 0.00000801
Iteration 16/1000 | Loss: 0.00000801
Iteration 17/1000 | Loss: 0.00000801
Iteration 18/1000 | Loss: 0.00000800
Iteration 19/1000 | Loss: 0.00000800
Iteration 20/1000 | Loss: 0.00000800
Iteration 21/1000 | Loss: 0.00000792
Iteration 22/1000 | Loss: 0.00000792
Iteration 23/1000 | Loss: 0.00000790
Iteration 24/1000 | Loss: 0.00000789
Iteration 25/1000 | Loss: 0.00000788
Iteration 26/1000 | Loss: 0.00000788
Iteration 27/1000 | Loss: 0.00000787
Iteration 28/1000 | Loss: 0.00000785
Iteration 29/1000 | Loss: 0.00000784
Iteration 30/1000 | Loss: 0.00000784
Iteration 31/1000 | Loss: 0.00000783
Iteration 32/1000 | Loss: 0.00000783
Iteration 33/1000 | Loss: 0.00000783
Iteration 34/1000 | Loss: 0.00000783
Iteration 35/1000 | Loss: 0.00000782
Iteration 36/1000 | Loss: 0.00000782
Iteration 37/1000 | Loss: 0.00000782
Iteration 38/1000 | Loss: 0.00000782
Iteration 39/1000 | Loss: 0.00000782
Iteration 40/1000 | Loss: 0.00000782
Iteration 41/1000 | Loss: 0.00000781
Iteration 42/1000 | Loss: 0.00000781
Iteration 43/1000 | Loss: 0.00000781
Iteration 44/1000 | Loss: 0.00000780
Iteration 45/1000 | Loss: 0.00000780
Iteration 46/1000 | Loss: 0.00000780
Iteration 47/1000 | Loss: 0.00000780
Iteration 48/1000 | Loss: 0.00000779
Iteration 49/1000 | Loss: 0.00000779
Iteration 50/1000 | Loss: 0.00000778
Iteration 51/1000 | Loss: 0.00000775
Iteration 52/1000 | Loss: 0.00000775
Iteration 53/1000 | Loss: 0.00000774
Iteration 54/1000 | Loss: 0.00000774
Iteration 55/1000 | Loss: 0.00000774
Iteration 56/1000 | Loss: 0.00000773
Iteration 57/1000 | Loss: 0.00000772
Iteration 58/1000 | Loss: 0.00000772
Iteration 59/1000 | Loss: 0.00000771
Iteration 60/1000 | Loss: 0.00000771
Iteration 61/1000 | Loss: 0.00000771
Iteration 62/1000 | Loss: 0.00000770
Iteration 63/1000 | Loss: 0.00000769
Iteration 64/1000 | Loss: 0.00000769
Iteration 65/1000 | Loss: 0.00000768
Iteration 66/1000 | Loss: 0.00000768
Iteration 67/1000 | Loss: 0.00000768
Iteration 68/1000 | Loss: 0.00000767
Iteration 69/1000 | Loss: 0.00000767
Iteration 70/1000 | Loss: 0.00000767
Iteration 71/1000 | Loss: 0.00000767
Iteration 72/1000 | Loss: 0.00000767
Iteration 73/1000 | Loss: 0.00000767
Iteration 74/1000 | Loss: 0.00000767
Iteration 75/1000 | Loss: 0.00000767
Iteration 76/1000 | Loss: 0.00000767
Iteration 77/1000 | Loss: 0.00000767
Iteration 78/1000 | Loss: 0.00000767
Iteration 79/1000 | Loss: 0.00000767
Iteration 80/1000 | Loss: 0.00000767
Iteration 81/1000 | Loss: 0.00000767
Iteration 82/1000 | Loss: 0.00000767
Iteration 83/1000 | Loss: 0.00000767
Iteration 84/1000 | Loss: 0.00000767
Iteration 85/1000 | Loss: 0.00000767
Iteration 86/1000 | Loss: 0.00000767
Iteration 87/1000 | Loss: 0.00000767
Iteration 88/1000 | Loss: 0.00000767
Iteration 89/1000 | Loss: 0.00000767
Iteration 90/1000 | Loss: 0.00000767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [7.66897665016586e-06, 7.66897665016586e-06, 7.66897665016586e-06, 7.66897665016586e-06, 7.66897665016586e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.66897665016586e-06

Optimization complete. Final v2v error: 2.321636915206909 mm

Highest mean error: 2.590857982635498 mm for frame 92

Lowest mean error: 2.1525511741638184 mm for frame 177

Saving results

Total time: 32.298548221588135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033724
Iteration 2/25 | Loss: 0.00228205
Iteration 3/25 | Loss: 0.00183921
Iteration 4/25 | Loss: 0.00154856
Iteration 5/25 | Loss: 0.00146375
Iteration 6/25 | Loss: 0.00139289
Iteration 7/25 | Loss: 0.00138165
Iteration 8/25 | Loss: 0.00142521
Iteration 9/25 | Loss: 0.00133202
Iteration 10/25 | Loss: 0.00137027
Iteration 11/25 | Loss: 0.00130379
Iteration 12/25 | Loss: 0.00128304
Iteration 13/25 | Loss: 0.00128193
Iteration 14/25 | Loss: 0.00128099
Iteration 15/25 | Loss: 0.00128515
Iteration 16/25 | Loss: 0.00127680
Iteration 17/25 | Loss: 0.00127265
Iteration 18/25 | Loss: 0.00127120
Iteration 19/25 | Loss: 0.00127064
Iteration 20/25 | Loss: 0.00127022
Iteration 21/25 | Loss: 0.00127510
Iteration 22/25 | Loss: 0.00127117
Iteration 23/25 | Loss: 0.00126748
Iteration 24/25 | Loss: 0.00126654
Iteration 25/25 | Loss: 0.00126622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22375000
Iteration 2/25 | Loss: 0.00387087
Iteration 3/25 | Loss: 0.00344941
Iteration 4/25 | Loss: 0.00344941
Iteration 5/25 | Loss: 0.00344941
Iteration 6/25 | Loss: 0.00344941
Iteration 7/25 | Loss: 0.00344941
Iteration 8/25 | Loss: 0.00344941
Iteration 9/25 | Loss: 0.00344941
Iteration 10/25 | Loss: 0.00344941
Iteration 11/25 | Loss: 0.00344941
Iteration 12/25 | Loss: 0.00344941
Iteration 13/25 | Loss: 0.00344941
Iteration 14/25 | Loss: 0.00344941
Iteration 15/25 | Loss: 0.00344941
Iteration 16/25 | Loss: 0.00344941
Iteration 17/25 | Loss: 0.00344941
Iteration 18/25 | Loss: 0.00344940
Iteration 19/25 | Loss: 0.00344940
Iteration 20/25 | Loss: 0.00344940
Iteration 21/25 | Loss: 0.00344940
Iteration 22/25 | Loss: 0.00344940
Iteration 23/25 | Loss: 0.00344940
Iteration 24/25 | Loss: 0.00344940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003449404612183571, 0.003449404612183571, 0.003449404612183571, 0.003449404612183571, 0.003449404612183571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003449404612183571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344940
Iteration 2/1000 | Loss: 0.00028044
Iteration 3/1000 | Loss: 0.00052641
Iteration 4/1000 | Loss: 0.00016192
Iteration 5/1000 | Loss: 0.00024608
Iteration 6/1000 | Loss: 0.00019959
Iteration 7/1000 | Loss: 0.00043626
Iteration 8/1000 | Loss: 0.00025081
Iteration 9/1000 | Loss: 0.00030164
Iteration 10/1000 | Loss: 0.00023474
Iteration 11/1000 | Loss: 0.00029872
Iteration 12/1000 | Loss: 0.00021015
Iteration 13/1000 | Loss: 0.00031782
Iteration 14/1000 | Loss: 0.00059610
Iteration 15/1000 | Loss: 0.00031367
Iteration 16/1000 | Loss: 0.00038724
Iteration 17/1000 | Loss: 0.00058278
Iteration 18/1000 | Loss: 0.00067359
Iteration 19/1000 | Loss: 0.00026566
Iteration 20/1000 | Loss: 0.00026320
Iteration 21/1000 | Loss: 0.00028880
Iteration 22/1000 | Loss: 0.00027710
Iteration 23/1000 | Loss: 0.00033778
Iteration 24/1000 | Loss: 0.00014122
Iteration 25/1000 | Loss: 0.00025764
Iteration 26/1000 | Loss: 0.00017673
Iteration 27/1000 | Loss: 0.00018748
Iteration 28/1000 | Loss: 0.00009683
Iteration 29/1000 | Loss: 0.00009316
Iteration 30/1000 | Loss: 0.00026701
Iteration 31/1000 | Loss: 0.00019945
Iteration 32/1000 | Loss: 0.00014095
Iteration 33/1000 | Loss: 0.00013354
Iteration 34/1000 | Loss: 0.00013680
Iteration 35/1000 | Loss: 0.00040281
Iteration 36/1000 | Loss: 0.00052632
Iteration 37/1000 | Loss: 0.00060679
Iteration 38/1000 | Loss: 0.00098460
Iteration 39/1000 | Loss: 0.00055678
Iteration 40/1000 | Loss: 0.00044618
Iteration 41/1000 | Loss: 0.00073821
Iteration 42/1000 | Loss: 0.00093850
Iteration 43/1000 | Loss: 0.00045194
Iteration 44/1000 | Loss: 0.00036245
Iteration 45/1000 | Loss: 0.00010713
Iteration 46/1000 | Loss: 0.00010024
Iteration 47/1000 | Loss: 0.00010495
Iteration 48/1000 | Loss: 0.00008809
Iteration 49/1000 | Loss: 0.00009752
Iteration 50/1000 | Loss: 0.00007852
Iteration 51/1000 | Loss: 0.00007228
Iteration 52/1000 | Loss: 0.00006867
Iteration 53/1000 | Loss: 0.00011130
Iteration 54/1000 | Loss: 0.00009354
Iteration 55/1000 | Loss: 0.00009260
Iteration 56/1000 | Loss: 0.00008481
Iteration 57/1000 | Loss: 0.00006943
Iteration 58/1000 | Loss: 0.00010358
Iteration 59/1000 | Loss: 0.00013820
Iteration 60/1000 | Loss: 0.00012020
Iteration 61/1000 | Loss: 0.00017532
Iteration 62/1000 | Loss: 0.00015298
Iteration 63/1000 | Loss: 0.00013604
Iteration 64/1000 | Loss: 0.00013091
Iteration 65/1000 | Loss: 0.00017732
Iteration 66/1000 | Loss: 0.00010774
Iteration 67/1000 | Loss: 0.00018340
Iteration 68/1000 | Loss: 0.00010169
Iteration 69/1000 | Loss: 0.00019672
Iteration 70/1000 | Loss: 0.00009254
Iteration 71/1000 | Loss: 0.00007112
Iteration 72/1000 | Loss: 0.00019011
Iteration 73/1000 | Loss: 0.00014135
Iteration 74/1000 | Loss: 0.00008881
Iteration 75/1000 | Loss: 0.00018082
Iteration 76/1000 | Loss: 0.00034523
Iteration 77/1000 | Loss: 0.00048049
Iteration 78/1000 | Loss: 0.00019081
Iteration 79/1000 | Loss: 0.00027402
Iteration 80/1000 | Loss: 0.00009695
Iteration 81/1000 | Loss: 0.00018832
Iteration 82/1000 | Loss: 0.00019284
Iteration 83/1000 | Loss: 0.00015168
Iteration 84/1000 | Loss: 0.00017365
Iteration 85/1000 | Loss: 0.00014447
Iteration 86/1000 | Loss: 0.00015260
Iteration 87/1000 | Loss: 0.00010978
Iteration 88/1000 | Loss: 0.00023809
Iteration 89/1000 | Loss: 0.00018478
Iteration 90/1000 | Loss: 0.00010903
Iteration 91/1000 | Loss: 0.00014751
Iteration 92/1000 | Loss: 0.00023111
Iteration 93/1000 | Loss: 0.00007074
Iteration 94/1000 | Loss: 0.00006514
Iteration 95/1000 | Loss: 0.00006234
Iteration 96/1000 | Loss: 0.00006104
Iteration 97/1000 | Loss: 0.00006029
Iteration 98/1000 | Loss: 0.00005973
Iteration 99/1000 | Loss: 0.00038743
Iteration 100/1000 | Loss: 0.00051038
Iteration 101/1000 | Loss: 0.00049214
Iteration 102/1000 | Loss: 0.00056926
Iteration 103/1000 | Loss: 0.00065118
Iteration 104/1000 | Loss: 0.00018574
Iteration 105/1000 | Loss: 0.00097907
Iteration 106/1000 | Loss: 0.00081170
Iteration 107/1000 | Loss: 0.00007790
Iteration 108/1000 | Loss: 0.00006869
Iteration 109/1000 | Loss: 0.00035720
Iteration 110/1000 | Loss: 0.00006078
Iteration 111/1000 | Loss: 0.00063769
Iteration 112/1000 | Loss: 0.00006688
Iteration 113/1000 | Loss: 0.00005861
Iteration 114/1000 | Loss: 0.00038728
Iteration 115/1000 | Loss: 0.00230694
Iteration 116/1000 | Loss: 0.00045465
Iteration 117/1000 | Loss: 0.00112162
Iteration 118/1000 | Loss: 0.00115777
Iteration 119/1000 | Loss: 0.00073140
Iteration 120/1000 | Loss: 0.00010368
Iteration 121/1000 | Loss: 0.00006760
Iteration 122/1000 | Loss: 0.00005901
Iteration 123/1000 | Loss: 0.00005057
Iteration 124/1000 | Loss: 0.00010917
Iteration 125/1000 | Loss: 0.00037860
Iteration 126/1000 | Loss: 0.00040327
Iteration 127/1000 | Loss: 0.00035514
Iteration 128/1000 | Loss: 0.00032467
Iteration 129/1000 | Loss: 0.00007231
Iteration 130/1000 | Loss: 0.00020922
Iteration 131/1000 | Loss: 0.00004836
Iteration 132/1000 | Loss: 0.00007033
Iteration 133/1000 | Loss: 0.00004304
Iteration 134/1000 | Loss: 0.00004233
Iteration 135/1000 | Loss: 0.00023782
Iteration 136/1000 | Loss: 0.00015670
Iteration 137/1000 | Loss: 0.00055244
Iteration 138/1000 | Loss: 0.00014949
Iteration 139/1000 | Loss: 0.00033898
Iteration 140/1000 | Loss: 0.00005596
Iteration 141/1000 | Loss: 0.00004690
Iteration 142/1000 | Loss: 0.00004300
Iteration 143/1000 | Loss: 0.00003971
Iteration 144/1000 | Loss: 0.00028665
Iteration 145/1000 | Loss: 0.00015515
Iteration 146/1000 | Loss: 0.00032373
Iteration 147/1000 | Loss: 0.00027118
Iteration 148/1000 | Loss: 0.00019412
Iteration 149/1000 | Loss: 0.00077088
Iteration 150/1000 | Loss: 0.00081658
Iteration 151/1000 | Loss: 0.00073876
Iteration 152/1000 | Loss: 0.00046971
Iteration 153/1000 | Loss: 0.00037088
Iteration 154/1000 | Loss: 0.00025447
Iteration 155/1000 | Loss: 0.00041449
Iteration 156/1000 | Loss: 0.00026585
Iteration 157/1000 | Loss: 0.00040933
Iteration 158/1000 | Loss: 0.00021211
Iteration 159/1000 | Loss: 0.00022786
Iteration 160/1000 | Loss: 0.00022295
Iteration 161/1000 | Loss: 0.00025534
Iteration 162/1000 | Loss: 0.00047119
Iteration 163/1000 | Loss: 0.00025010
Iteration 164/1000 | Loss: 0.00023231
Iteration 165/1000 | Loss: 0.00017049
Iteration 166/1000 | Loss: 0.00031127
Iteration 167/1000 | Loss: 0.00005662
Iteration 168/1000 | Loss: 0.00006942
Iteration 169/1000 | Loss: 0.00003948
Iteration 170/1000 | Loss: 0.00035876
Iteration 171/1000 | Loss: 0.00019101
Iteration 172/1000 | Loss: 0.00029340
Iteration 173/1000 | Loss: 0.00018309
Iteration 174/1000 | Loss: 0.00053420
Iteration 175/1000 | Loss: 0.00032112
Iteration 176/1000 | Loss: 0.00006337
Iteration 177/1000 | Loss: 0.00004277
Iteration 178/1000 | Loss: 0.00003841
Iteration 179/1000 | Loss: 0.00003651
Iteration 180/1000 | Loss: 0.00003463
Iteration 181/1000 | Loss: 0.00003357
Iteration 182/1000 | Loss: 0.00003310
Iteration 183/1000 | Loss: 0.00003277
Iteration 184/1000 | Loss: 0.00003254
Iteration 185/1000 | Loss: 0.00003245
Iteration 186/1000 | Loss: 0.00033169
Iteration 187/1000 | Loss: 0.00020154
Iteration 188/1000 | Loss: 0.00035272
Iteration 189/1000 | Loss: 0.00003842
Iteration 190/1000 | Loss: 0.00003517
Iteration 191/1000 | Loss: 0.00035868
Iteration 192/1000 | Loss: 0.00011370
Iteration 193/1000 | Loss: 0.00048927
Iteration 194/1000 | Loss: 0.00004661
Iteration 195/1000 | Loss: 0.00003699
Iteration 196/1000 | Loss: 0.00003326
Iteration 197/1000 | Loss: 0.00003110
Iteration 198/1000 | Loss: 0.00002906
Iteration 199/1000 | Loss: 0.00002709
Iteration 200/1000 | Loss: 0.00002633
Iteration 201/1000 | Loss: 0.00002578
Iteration 202/1000 | Loss: 0.00002501
Iteration 203/1000 | Loss: 0.00002435
Iteration 204/1000 | Loss: 0.00002391
Iteration 205/1000 | Loss: 0.00002353
Iteration 206/1000 | Loss: 0.00002330
Iteration 207/1000 | Loss: 0.00002312
Iteration 208/1000 | Loss: 0.00002311
Iteration 209/1000 | Loss: 0.00002307
Iteration 210/1000 | Loss: 0.00002307
Iteration 211/1000 | Loss: 0.00002306
Iteration 212/1000 | Loss: 0.00002306
Iteration 213/1000 | Loss: 0.00002306
Iteration 214/1000 | Loss: 0.00002305
Iteration 215/1000 | Loss: 0.00002305
Iteration 216/1000 | Loss: 0.00002304
Iteration 217/1000 | Loss: 0.00002304
Iteration 218/1000 | Loss: 0.00002303
Iteration 219/1000 | Loss: 0.00002303
Iteration 220/1000 | Loss: 0.00002303
Iteration 221/1000 | Loss: 0.00002302
Iteration 222/1000 | Loss: 0.00002302
Iteration 223/1000 | Loss: 0.00002301
Iteration 224/1000 | Loss: 0.00002301
Iteration 225/1000 | Loss: 0.00002301
Iteration 226/1000 | Loss: 0.00002301
Iteration 227/1000 | Loss: 0.00002300
Iteration 228/1000 | Loss: 0.00002299
Iteration 229/1000 | Loss: 0.00002299
Iteration 230/1000 | Loss: 0.00002298
Iteration 231/1000 | Loss: 0.00002298
Iteration 232/1000 | Loss: 0.00002298
Iteration 233/1000 | Loss: 0.00002298
Iteration 234/1000 | Loss: 0.00002298
Iteration 235/1000 | Loss: 0.00002298
Iteration 236/1000 | Loss: 0.00002298
Iteration 237/1000 | Loss: 0.00002298
Iteration 238/1000 | Loss: 0.00002298
Iteration 239/1000 | Loss: 0.00002298
Iteration 240/1000 | Loss: 0.00002298
Iteration 241/1000 | Loss: 0.00002298
Iteration 242/1000 | Loss: 0.00002298
Iteration 243/1000 | Loss: 0.00002298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.2978791093919426e-05, 2.2978791093919426e-05, 2.2978791093919426e-05, 2.2978791093919426e-05, 2.2978791093919426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2978791093919426e-05

Optimization complete. Final v2v error: 3.1847574710845947 mm

Highest mean error: 18.490854263305664 mm for frame 94

Lowest mean error: 2.4037907123565674 mm for frame 87

Saving results

Total time: 337.4730050563812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062422
Iteration 2/25 | Loss: 0.00224141
Iteration 3/25 | Loss: 0.00198155
Iteration 4/25 | Loss: 0.00190791
Iteration 5/25 | Loss: 0.00171696
Iteration 6/25 | Loss: 0.00171359
Iteration 7/25 | Loss: 0.00147093
Iteration 8/25 | Loss: 0.00118617
Iteration 9/25 | Loss: 0.00106300
Iteration 10/25 | Loss: 0.00103538
Iteration 11/25 | Loss: 0.00100988
Iteration 12/25 | Loss: 0.00100790
Iteration 13/25 | Loss: 0.00100471
Iteration 14/25 | Loss: 0.00099900
Iteration 15/25 | Loss: 0.00099657
Iteration 16/25 | Loss: 0.00099596
Iteration 17/25 | Loss: 0.00099576
Iteration 18/25 | Loss: 0.00099575
Iteration 19/25 | Loss: 0.00099575
Iteration 20/25 | Loss: 0.00099575
Iteration 21/25 | Loss: 0.00099575
Iteration 22/25 | Loss: 0.00099575
Iteration 23/25 | Loss: 0.00099574
Iteration 24/25 | Loss: 0.00099574
Iteration 25/25 | Loss: 0.00099574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18167365
Iteration 2/25 | Loss: 0.00164218
Iteration 3/25 | Loss: 0.00164218
Iteration 4/25 | Loss: 0.00164218
Iteration 5/25 | Loss: 0.00164218
Iteration 6/25 | Loss: 0.00164218
Iteration 7/25 | Loss: 0.00164218
Iteration 8/25 | Loss: 0.00164218
Iteration 9/25 | Loss: 0.00164218
Iteration 10/25 | Loss: 0.00164218
Iteration 11/25 | Loss: 0.00164218
Iteration 12/25 | Loss: 0.00164218
Iteration 13/25 | Loss: 0.00164218
Iteration 14/25 | Loss: 0.00164218
Iteration 15/25 | Loss: 0.00164218
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001642177114263177, 0.001642177114263177, 0.001642177114263177, 0.001642177114263177, 0.001642177114263177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001642177114263177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164218
Iteration 2/1000 | Loss: 0.00004031
Iteration 3/1000 | Loss: 0.00002962
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002212
Iteration 7/1000 | Loss: 0.00002139
Iteration 8/1000 | Loss: 0.00002085
Iteration 9/1000 | Loss: 0.00002042
Iteration 10/1000 | Loss: 0.00002008
Iteration 11/1000 | Loss: 0.00001986
Iteration 12/1000 | Loss: 0.00001973
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001946
Iteration 15/1000 | Loss: 0.00001942
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001942
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001927
Iteration 35/1000 | Loss: 0.00001926
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001918
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001916
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001914
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001912
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001911
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001910
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001909
Iteration 73/1000 | Loss: 0.00001909
Iteration 74/1000 | Loss: 0.00001909
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001908
Iteration 77/1000 | Loss: 0.00001908
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001903
Iteration 87/1000 | Loss: 0.00001903
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001902
Iteration 92/1000 | Loss: 0.00001902
Iteration 93/1000 | Loss: 0.00001902
Iteration 94/1000 | Loss: 0.00001902
Iteration 95/1000 | Loss: 0.00001902
Iteration 96/1000 | Loss: 0.00001901
Iteration 97/1000 | Loss: 0.00001901
Iteration 98/1000 | Loss: 0.00001901
Iteration 99/1000 | Loss: 0.00001901
Iteration 100/1000 | Loss: 0.00001901
Iteration 101/1000 | Loss: 0.00001901
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001899
Iteration 104/1000 | Loss: 0.00001899
Iteration 105/1000 | Loss: 0.00001899
Iteration 106/1000 | Loss: 0.00001899
Iteration 107/1000 | Loss: 0.00001898
Iteration 108/1000 | Loss: 0.00001898
Iteration 109/1000 | Loss: 0.00001898
Iteration 110/1000 | Loss: 0.00001898
Iteration 111/1000 | Loss: 0.00001898
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001897
Iteration 114/1000 | Loss: 0.00001897
Iteration 115/1000 | Loss: 0.00001897
Iteration 116/1000 | Loss: 0.00001897
Iteration 117/1000 | Loss: 0.00001897
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001896
Iteration 120/1000 | Loss: 0.00001895
Iteration 121/1000 | Loss: 0.00001895
Iteration 122/1000 | Loss: 0.00001895
Iteration 123/1000 | Loss: 0.00001895
Iteration 124/1000 | Loss: 0.00001895
Iteration 125/1000 | Loss: 0.00001895
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001894
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001893
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001892
Iteration 139/1000 | Loss: 0.00001892
Iteration 140/1000 | Loss: 0.00001892
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001891
Iteration 144/1000 | Loss: 0.00001891
Iteration 145/1000 | Loss: 0.00001891
Iteration 146/1000 | Loss: 0.00001890
Iteration 147/1000 | Loss: 0.00001890
Iteration 148/1000 | Loss: 0.00001890
Iteration 149/1000 | Loss: 0.00001890
Iteration 150/1000 | Loss: 0.00001890
Iteration 151/1000 | Loss: 0.00001890
Iteration 152/1000 | Loss: 0.00001890
Iteration 153/1000 | Loss: 0.00001890
Iteration 154/1000 | Loss: 0.00001890
Iteration 155/1000 | Loss: 0.00001890
Iteration 156/1000 | Loss: 0.00001889
Iteration 157/1000 | Loss: 0.00001888
Iteration 158/1000 | Loss: 0.00001888
Iteration 159/1000 | Loss: 0.00001888
Iteration 160/1000 | Loss: 0.00001888
Iteration 161/1000 | Loss: 0.00001888
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001887
Iteration 166/1000 | Loss: 0.00001887
Iteration 167/1000 | Loss: 0.00001886
Iteration 168/1000 | Loss: 0.00001886
Iteration 169/1000 | Loss: 0.00001885
Iteration 170/1000 | Loss: 0.00001885
Iteration 171/1000 | Loss: 0.00001884
Iteration 172/1000 | Loss: 0.00001884
Iteration 173/1000 | Loss: 0.00001884
Iteration 174/1000 | Loss: 0.00001884
Iteration 175/1000 | Loss: 0.00001884
Iteration 176/1000 | Loss: 0.00001884
Iteration 177/1000 | Loss: 0.00001884
Iteration 178/1000 | Loss: 0.00001883
Iteration 179/1000 | Loss: 0.00001883
Iteration 180/1000 | Loss: 0.00001883
Iteration 181/1000 | Loss: 0.00001883
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001883
Iteration 185/1000 | Loss: 0.00001882
Iteration 186/1000 | Loss: 0.00001882
Iteration 187/1000 | Loss: 0.00001881
Iteration 188/1000 | Loss: 0.00001881
Iteration 189/1000 | Loss: 0.00001881
Iteration 190/1000 | Loss: 0.00001880
Iteration 191/1000 | Loss: 0.00001880
Iteration 192/1000 | Loss: 0.00001879
Iteration 193/1000 | Loss: 0.00001879
Iteration 194/1000 | Loss: 0.00001879
Iteration 195/1000 | Loss: 0.00001879
Iteration 196/1000 | Loss: 0.00001879
Iteration 197/1000 | Loss: 0.00001878
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001878
Iteration 200/1000 | Loss: 0.00001878
Iteration 201/1000 | Loss: 0.00001878
Iteration 202/1000 | Loss: 0.00001877
Iteration 203/1000 | Loss: 0.00001877
Iteration 204/1000 | Loss: 0.00001877
Iteration 205/1000 | Loss: 0.00001877
Iteration 206/1000 | Loss: 0.00001877
Iteration 207/1000 | Loss: 0.00001876
Iteration 208/1000 | Loss: 0.00001876
Iteration 209/1000 | Loss: 0.00001876
Iteration 210/1000 | Loss: 0.00001876
Iteration 211/1000 | Loss: 0.00001876
Iteration 212/1000 | Loss: 0.00001876
Iteration 213/1000 | Loss: 0.00001876
Iteration 214/1000 | Loss: 0.00001875
Iteration 215/1000 | Loss: 0.00001875
Iteration 216/1000 | Loss: 0.00001875
Iteration 217/1000 | Loss: 0.00001875
Iteration 218/1000 | Loss: 0.00001874
Iteration 219/1000 | Loss: 0.00001874
Iteration 220/1000 | Loss: 0.00001874
Iteration 221/1000 | Loss: 0.00001874
Iteration 222/1000 | Loss: 0.00001873
Iteration 223/1000 | Loss: 0.00001873
Iteration 224/1000 | Loss: 0.00001873
Iteration 225/1000 | Loss: 0.00001873
Iteration 226/1000 | Loss: 0.00001873
Iteration 227/1000 | Loss: 0.00001873
Iteration 228/1000 | Loss: 0.00001873
Iteration 229/1000 | Loss: 0.00001873
Iteration 230/1000 | Loss: 0.00001873
Iteration 231/1000 | Loss: 0.00001873
Iteration 232/1000 | Loss: 0.00001873
Iteration 233/1000 | Loss: 0.00001873
Iteration 234/1000 | Loss: 0.00001873
Iteration 235/1000 | Loss: 0.00001873
Iteration 236/1000 | Loss: 0.00001872
Iteration 237/1000 | Loss: 0.00001872
Iteration 238/1000 | Loss: 0.00001872
Iteration 239/1000 | Loss: 0.00001872
Iteration 240/1000 | Loss: 0.00001872
Iteration 241/1000 | Loss: 0.00001872
Iteration 242/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.8724877008935437e-05, 1.8724877008935437e-05, 1.8724877008935437e-05, 1.8724877008935437e-05, 1.8724877008935437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8724877008935437e-05

Optimization complete. Final v2v error: 2.8354077339172363 mm

Highest mean error: 12.063855171203613 mm for frame 81

Lowest mean error: 2.5944223403930664 mm for frame 67

Saving results

Total time: 78.10441970825195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069767
Iteration 2/25 | Loss: 0.01069766
Iteration 3/25 | Loss: 0.01069766
Iteration 4/25 | Loss: 0.01069766
Iteration 5/25 | Loss: 0.01069766
Iteration 6/25 | Loss: 0.01069766
Iteration 7/25 | Loss: 0.01069766
Iteration 8/25 | Loss: 0.01069766
Iteration 9/25 | Loss: 0.01069766
Iteration 10/25 | Loss: 0.01069765
Iteration 11/25 | Loss: 0.01069765
Iteration 12/25 | Loss: 0.01069765
Iteration 13/25 | Loss: 0.01069765
Iteration 14/25 | Loss: 0.01069765
Iteration 15/25 | Loss: 0.01069765
Iteration 16/25 | Loss: 0.01069765
Iteration 17/25 | Loss: 0.01069765
Iteration 18/25 | Loss: 0.01069764
Iteration 19/25 | Loss: 0.01069764
Iteration 20/25 | Loss: 0.01069764
Iteration 21/25 | Loss: 0.01069764
Iteration 22/25 | Loss: 0.01069764
Iteration 23/25 | Loss: 0.01069764
Iteration 24/25 | Loss: 0.01069764
Iteration 25/25 | Loss: 0.01069764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82119310
Iteration 2/25 | Loss: 0.17756957
Iteration 3/25 | Loss: 0.17257230
Iteration 4/25 | Loss: 0.17061290
Iteration 5/25 | Loss: 0.16952993
Iteration 6/25 | Loss: 0.16937867
Iteration 7/25 | Loss: 0.16937867
Iteration 8/25 | Loss: 0.16937867
Iteration 9/25 | Loss: 0.16937867
Iteration 10/25 | Loss: 0.16937864
Iteration 11/25 | Loss: 0.16937864
Iteration 12/25 | Loss: 0.16937864
Iteration 13/25 | Loss: 0.16937867
Iteration 14/25 | Loss: 0.16937867
Iteration 15/25 | Loss: 0.16937867
Iteration 16/25 | Loss: 0.16937867
Iteration 17/25 | Loss: 0.16937867
Iteration 18/25 | Loss: 0.16937867
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.16937866806983948, 0.16937866806983948, 0.16937866806983948, 0.16937866806983948, 0.16937866806983948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16937866806983948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16937867
Iteration 2/1000 | Loss: 0.00495368
Iteration 3/1000 | Loss: 0.00554351
Iteration 4/1000 | Loss: 0.00301239
Iteration 5/1000 | Loss: 0.01120519
Iteration 6/1000 | Loss: 0.00362105
Iteration 7/1000 | Loss: 0.00136490
Iteration 8/1000 | Loss: 0.00226884
Iteration 9/1000 | Loss: 0.00048570
Iteration 10/1000 | Loss: 0.00014569
Iteration 11/1000 | Loss: 0.00076706
Iteration 12/1000 | Loss: 0.00012490
Iteration 13/1000 | Loss: 0.00126522
Iteration 14/1000 | Loss: 0.00273345
Iteration 15/1000 | Loss: 0.00274151
Iteration 16/1000 | Loss: 0.00186792
Iteration 17/1000 | Loss: 0.00044945
Iteration 18/1000 | Loss: 0.00024801
Iteration 19/1000 | Loss: 0.00009634
Iteration 20/1000 | Loss: 0.00005180
Iteration 21/1000 | Loss: 0.00070793
Iteration 22/1000 | Loss: 0.00006222
Iteration 23/1000 | Loss: 0.00007926
Iteration 24/1000 | Loss: 0.00003323
Iteration 25/1000 | Loss: 0.00008790
Iteration 26/1000 | Loss: 0.00002957
Iteration 27/1000 | Loss: 0.00003138
Iteration 28/1000 | Loss: 0.00002950
Iteration 29/1000 | Loss: 0.00002674
Iteration 30/1000 | Loss: 0.00011538
Iteration 31/1000 | Loss: 0.00004693
Iteration 32/1000 | Loss: 0.00011354
Iteration 33/1000 | Loss: 0.00002378
Iteration 34/1000 | Loss: 0.00004183
Iteration 35/1000 | Loss: 0.00030484
Iteration 36/1000 | Loss: 0.00004683
Iteration 37/1000 | Loss: 0.00008368
Iteration 38/1000 | Loss: 0.00002794
Iteration 39/1000 | Loss: 0.00002900
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002280
Iteration 42/1000 | Loss: 0.00003160
Iteration 43/1000 | Loss: 0.00002258
Iteration 44/1000 | Loss: 0.00003086
Iteration 45/1000 | Loss: 0.00002070
Iteration 46/1000 | Loss: 0.00002248
Iteration 47/1000 | Loss: 0.00002199
Iteration 48/1000 | Loss: 0.00002004
Iteration 49/1000 | Loss: 0.00003987
Iteration 50/1000 | Loss: 0.00002833
Iteration 51/1000 | Loss: 0.00002187
Iteration 52/1000 | Loss: 0.00009136
Iteration 53/1000 | Loss: 0.00002736
Iteration 54/1000 | Loss: 0.00005875
Iteration 55/1000 | Loss: 0.00002297
Iteration 56/1000 | Loss: 0.00001979
Iteration 57/1000 | Loss: 0.00001881
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00003502
Iteration 76/1000 | Loss: 0.00003153
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001810
Iteration 79/1000 | Loss: 0.00001810
Iteration 80/1000 | Loss: 0.00001810
Iteration 81/1000 | Loss: 0.00001810
Iteration 82/1000 | Loss: 0.00001810
Iteration 83/1000 | Loss: 0.00001810
Iteration 84/1000 | Loss: 0.00001809
Iteration 85/1000 | Loss: 0.00001809
Iteration 86/1000 | Loss: 0.00001809
Iteration 87/1000 | Loss: 0.00001809
Iteration 88/1000 | Loss: 0.00001809
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001809
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00001809
Iteration 93/1000 | Loss: 0.00001809
Iteration 94/1000 | Loss: 0.00001809
Iteration 95/1000 | Loss: 0.00001808
Iteration 96/1000 | Loss: 0.00001808
Iteration 97/1000 | Loss: 0.00001808
Iteration 98/1000 | Loss: 0.00001808
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00003323
Iteration 101/1000 | Loss: 0.00001947
Iteration 102/1000 | Loss: 0.00002970
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001793
Iteration 105/1000 | Loss: 0.00001793
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001792
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001791
Iteration 111/1000 | Loss: 0.00001791
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001790
Iteration 114/1000 | Loss: 0.00001790
Iteration 115/1000 | Loss: 0.00001790
Iteration 116/1000 | Loss: 0.00001789
Iteration 117/1000 | Loss: 0.00002751
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001785
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001784
Iteration 123/1000 | Loss: 0.00001784
Iteration 124/1000 | Loss: 0.00001794
Iteration 125/1000 | Loss: 0.00001794
Iteration 126/1000 | Loss: 0.00004504
Iteration 127/1000 | Loss: 0.00002039
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Iteration 138/1000 | Loss: 0.00001781
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001824
Iteration 147/1000 | Loss: 0.00001779
Iteration 148/1000 | Loss: 0.00001779
Iteration 149/1000 | Loss: 0.00001779
Iteration 150/1000 | Loss: 0.00001779
Iteration 151/1000 | Loss: 0.00001779
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001779
Iteration 154/1000 | Loss: 0.00001779
Iteration 155/1000 | Loss: 0.00001779
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001778
Iteration 158/1000 | Loss: 0.00001778
Iteration 159/1000 | Loss: 0.00001778
Iteration 160/1000 | Loss: 0.00001778
Iteration 161/1000 | Loss: 0.00001778
Iteration 162/1000 | Loss: 0.00001778
Iteration 163/1000 | Loss: 0.00001778
Iteration 164/1000 | Loss: 0.00001778
Iteration 165/1000 | Loss: 0.00001778
Iteration 166/1000 | Loss: 0.00001778
Iteration 167/1000 | Loss: 0.00001778
Iteration 168/1000 | Loss: 0.00001778
Iteration 169/1000 | Loss: 0.00001777
Iteration 170/1000 | Loss: 0.00001777
Iteration 171/1000 | Loss: 0.00001777
Iteration 172/1000 | Loss: 0.00001777
Iteration 173/1000 | Loss: 0.00001777
Iteration 174/1000 | Loss: 0.00001777
Iteration 175/1000 | Loss: 0.00001777
Iteration 176/1000 | Loss: 0.00001777
Iteration 177/1000 | Loss: 0.00001777
Iteration 178/1000 | Loss: 0.00001777
Iteration 179/1000 | Loss: 0.00001777
Iteration 180/1000 | Loss: 0.00001777
Iteration 181/1000 | Loss: 0.00001777
Iteration 182/1000 | Loss: 0.00001777
Iteration 183/1000 | Loss: 0.00001777
Iteration 184/1000 | Loss: 0.00001777
Iteration 185/1000 | Loss: 0.00001777
Iteration 186/1000 | Loss: 0.00001777
Iteration 187/1000 | Loss: 0.00001776
Iteration 188/1000 | Loss: 0.00001776
Iteration 189/1000 | Loss: 0.00001776
Iteration 190/1000 | Loss: 0.00001776
Iteration 191/1000 | Loss: 0.00001776
Iteration 192/1000 | Loss: 0.00001776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.7764841686584987e-05, 1.7764841686584987e-05, 1.7764841686584987e-05, 1.7764841686584987e-05, 1.7764841686584987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7764841686584987e-05

Optimization complete. Final v2v error: 3.369581699371338 mm

Highest mean error: 9.506758689880371 mm for frame 93

Lowest mean error: 2.5739264488220215 mm for frame 30

Saving results

Total time: 131.1591351032257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818788
Iteration 2/25 | Loss: 0.00161592
Iteration 3/25 | Loss: 0.00111046
Iteration 4/25 | Loss: 0.00106770
Iteration 5/25 | Loss: 0.00106521
Iteration 6/25 | Loss: 0.00106521
Iteration 7/25 | Loss: 0.00106521
Iteration 8/25 | Loss: 0.00106521
Iteration 9/25 | Loss: 0.00106521
Iteration 10/25 | Loss: 0.00106521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010652082273736596, 0.0010652082273736596, 0.0010652082273736596, 0.0010652082273736596, 0.0010652082273736596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010652082273736596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13209093
Iteration 2/25 | Loss: 0.00159190
Iteration 3/25 | Loss: 0.00159190
Iteration 4/25 | Loss: 0.00159190
Iteration 5/25 | Loss: 0.00159190
Iteration 6/25 | Loss: 0.00159190
Iteration 7/25 | Loss: 0.00159190
Iteration 8/25 | Loss: 0.00159190
Iteration 9/25 | Loss: 0.00159190
Iteration 10/25 | Loss: 0.00159190
Iteration 11/25 | Loss: 0.00159190
Iteration 12/25 | Loss: 0.00159190
Iteration 13/25 | Loss: 0.00159190
Iteration 14/25 | Loss: 0.00159190
Iteration 15/25 | Loss: 0.00159190
Iteration 16/25 | Loss: 0.00159190
Iteration 17/25 | Loss: 0.00159190
Iteration 18/25 | Loss: 0.00159190
Iteration 19/25 | Loss: 0.00159190
Iteration 20/25 | Loss: 0.00159190
Iteration 21/25 | Loss: 0.00159190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015918973367661238, 0.0015918973367661238, 0.0015918973367661238, 0.0015918973367661238, 0.0015918973367661238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015918973367661238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159190
Iteration 2/1000 | Loss: 0.00003972
Iteration 3/1000 | Loss: 0.00002351
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001703
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001648
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001608
Iteration 22/1000 | Loss: 0.00001607
Iteration 23/1000 | Loss: 0.00001605
Iteration 24/1000 | Loss: 0.00001604
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001600
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001599
Iteration 33/1000 | Loss: 0.00001599
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001589
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001588
Iteration 55/1000 | Loss: 0.00001588
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001587
Iteration 65/1000 | Loss: 0.00001587
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001587
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001585
Iteration 71/1000 | Loss: 0.00001585
Iteration 72/1000 | Loss: 0.00001585
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.584519304742571e-05, 1.584519304742571e-05, 1.584519304742571e-05, 1.584519304742571e-05, 1.584519304742571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.584519304742571e-05

Optimization complete. Final v2v error: 3.176680564880371 mm

Highest mean error: 4.398952484130859 mm for frame 111

Lowest mean error: 2.6154415607452393 mm for frame 99

Saving results

Total time: 40.698157787323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093306
Iteration 2/25 | Loss: 0.00263483
Iteration 3/25 | Loss: 0.00199045
Iteration 4/25 | Loss: 0.00229051
Iteration 5/25 | Loss: 0.00217594
Iteration 6/25 | Loss: 0.00181737
Iteration 7/25 | Loss: 0.00170946
Iteration 8/25 | Loss: 0.00163190
Iteration 9/25 | Loss: 0.00153048
Iteration 10/25 | Loss: 0.00143468
Iteration 11/25 | Loss: 0.00137059
Iteration 12/25 | Loss: 0.00127076
Iteration 13/25 | Loss: 0.00121782
Iteration 14/25 | Loss: 0.00118449
Iteration 15/25 | Loss: 0.00117152
Iteration 16/25 | Loss: 0.00115871
Iteration 17/25 | Loss: 0.00115288
Iteration 18/25 | Loss: 0.00113973
Iteration 19/25 | Loss: 0.00114235
Iteration 20/25 | Loss: 0.00114569
Iteration 21/25 | Loss: 0.00114030
Iteration 22/25 | Loss: 0.00114816
Iteration 23/25 | Loss: 0.00114633
Iteration 24/25 | Loss: 0.00113905
Iteration 25/25 | Loss: 0.00113364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38824713
Iteration 2/25 | Loss: 0.00156764
Iteration 3/25 | Loss: 0.00156763
Iteration 4/25 | Loss: 0.00156763
Iteration 5/25 | Loss: 0.00156763
Iteration 6/25 | Loss: 0.00156763
Iteration 7/25 | Loss: 0.00156763
Iteration 8/25 | Loss: 0.00156763
Iteration 9/25 | Loss: 0.00156763
Iteration 10/25 | Loss: 0.00156763
Iteration 11/25 | Loss: 0.00156763
Iteration 12/25 | Loss: 0.00156763
Iteration 13/25 | Loss: 0.00156763
Iteration 14/25 | Loss: 0.00156763
Iteration 15/25 | Loss: 0.00156763
Iteration 16/25 | Loss: 0.00156763
Iteration 17/25 | Loss: 0.00156763
Iteration 18/25 | Loss: 0.00156763
Iteration 19/25 | Loss: 0.00156763
Iteration 20/25 | Loss: 0.00156763
Iteration 21/25 | Loss: 0.00156763
Iteration 22/25 | Loss: 0.00156763
Iteration 23/25 | Loss: 0.00156763
Iteration 24/25 | Loss: 0.00156763
Iteration 25/25 | Loss: 0.00156763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156763
Iteration 2/1000 | Loss: 0.00027704
Iteration 3/1000 | Loss: 0.00030104
Iteration 4/1000 | Loss: 0.00036646
Iteration 5/1000 | Loss: 0.00031393
Iteration 6/1000 | Loss: 0.00028552
Iteration 7/1000 | Loss: 0.00041568
Iteration 8/1000 | Loss: 0.00054071
Iteration 9/1000 | Loss: 0.00046798
Iteration 10/1000 | Loss: 0.00049378
Iteration 11/1000 | Loss: 0.00043702
Iteration 12/1000 | Loss: 0.00051808
Iteration 13/1000 | Loss: 0.00040694
Iteration 14/1000 | Loss: 0.00031945
Iteration 15/1000 | Loss: 0.00032523
Iteration 16/1000 | Loss: 0.00042913
Iteration 17/1000 | Loss: 0.00042704
Iteration 18/1000 | Loss: 0.00040688
Iteration 19/1000 | Loss: 0.00040387
Iteration 20/1000 | Loss: 0.00046660
Iteration 21/1000 | Loss: 0.00034958
Iteration 22/1000 | Loss: 0.00041772
Iteration 23/1000 | Loss: 0.00045161
Iteration 24/1000 | Loss: 0.00049983
Iteration 25/1000 | Loss: 0.00046575
Iteration 26/1000 | Loss: 0.00043637
Iteration 27/1000 | Loss: 0.00046197
Iteration 28/1000 | Loss: 0.00031956
Iteration 29/1000 | Loss: 0.00051693
Iteration 30/1000 | Loss: 0.00072477
Iteration 31/1000 | Loss: 0.00071858
Iteration 32/1000 | Loss: 0.00039733
Iteration 33/1000 | Loss: 0.00040156
Iteration 34/1000 | Loss: 0.00083245
Iteration 35/1000 | Loss: 0.00060055
Iteration 36/1000 | Loss: 0.00053415
Iteration 37/1000 | Loss: 0.00050917
Iteration 38/1000 | Loss: 0.00053813
Iteration 39/1000 | Loss: 0.00079462
Iteration 40/1000 | Loss: 0.00055415
Iteration 41/1000 | Loss: 0.00049892
Iteration 42/1000 | Loss: 0.00084580
Iteration 43/1000 | Loss: 0.00064578
Iteration 44/1000 | Loss: 0.00036135
Iteration 45/1000 | Loss: 0.00035718
Iteration 46/1000 | Loss: 0.00052928
Iteration 47/1000 | Loss: 0.00057847
Iteration 48/1000 | Loss: 0.00033355
Iteration 49/1000 | Loss: 0.00034295
Iteration 50/1000 | Loss: 0.00025915
Iteration 51/1000 | Loss: 0.00027015
Iteration 52/1000 | Loss: 0.00028741
Iteration 53/1000 | Loss: 0.00027804
Iteration 54/1000 | Loss: 0.00036381
Iteration 55/1000 | Loss: 0.00037346
Iteration 56/1000 | Loss: 0.00035666
Iteration 57/1000 | Loss: 0.00048398
Iteration 58/1000 | Loss: 0.00043652
Iteration 59/1000 | Loss: 0.00035925
Iteration 60/1000 | Loss: 0.00037632
Iteration 61/1000 | Loss: 0.00035493
Iteration 62/1000 | Loss: 0.00036216
Iteration 63/1000 | Loss: 0.00036077
Iteration 64/1000 | Loss: 0.00035669
Iteration 65/1000 | Loss: 0.00035299
Iteration 66/1000 | Loss: 0.00034747
Iteration 67/1000 | Loss: 0.00034706
Iteration 68/1000 | Loss: 0.00034935
Iteration 69/1000 | Loss: 0.00034867
Iteration 70/1000 | Loss: 0.00034903
Iteration 71/1000 | Loss: 0.00025078
Iteration 72/1000 | Loss: 0.00025236
Iteration 73/1000 | Loss: 0.00026756
Iteration 74/1000 | Loss: 0.00025484
Iteration 75/1000 | Loss: 0.00022518
Iteration 76/1000 | Loss: 0.00027367
Iteration 77/1000 | Loss: 0.00021976
Iteration 78/1000 | Loss: 0.00032741
Iteration 79/1000 | Loss: 0.00043624
Iteration 80/1000 | Loss: 0.00033140
Iteration 81/1000 | Loss: 0.00023222
Iteration 82/1000 | Loss: 0.00027715
Iteration 83/1000 | Loss: 0.00035955
Iteration 84/1000 | Loss: 0.00050593
Iteration 85/1000 | Loss: 0.00046101
Iteration 86/1000 | Loss: 0.00063114
Iteration 87/1000 | Loss: 0.00052458
Iteration 88/1000 | Loss: 0.00050508
Iteration 89/1000 | Loss: 0.00048540
Iteration 90/1000 | Loss: 0.00049929
Iteration 91/1000 | Loss: 0.00047660
Iteration 92/1000 | Loss: 0.00030859
Iteration 93/1000 | Loss: 0.00032012
Iteration 94/1000 | Loss: 0.00035631
Iteration 95/1000 | Loss: 0.00035445
Iteration 96/1000 | Loss: 0.00035825
Iteration 97/1000 | Loss: 0.00034760
Iteration 98/1000 | Loss: 0.00035326
Iteration 99/1000 | Loss: 0.00035500
Iteration 100/1000 | Loss: 0.00034780
Iteration 101/1000 | Loss: 0.00034665
Iteration 102/1000 | Loss: 0.00053446
Iteration 103/1000 | Loss: 0.00049496
Iteration 104/1000 | Loss: 0.00118511
Iteration 105/1000 | Loss: 0.00076742
Iteration 106/1000 | Loss: 0.00097785
Iteration 107/1000 | Loss: 0.00059465
Iteration 108/1000 | Loss: 0.00111246
Iteration 109/1000 | Loss: 0.00064447
Iteration 110/1000 | Loss: 0.00075409
Iteration 111/1000 | Loss: 0.00058680
Iteration 112/1000 | Loss: 0.00054380
Iteration 113/1000 | Loss: 0.00061737
Iteration 114/1000 | Loss: 0.00059118
Iteration 115/1000 | Loss: 0.00051307
Iteration 116/1000 | Loss: 0.00034043
Iteration 117/1000 | Loss: 0.00049844
Iteration 118/1000 | Loss: 0.00032724
Iteration 119/1000 | Loss: 0.00032713
Iteration 120/1000 | Loss: 0.00044329
Iteration 121/1000 | Loss: 0.00087432
Iteration 122/1000 | Loss: 0.00059232
Iteration 123/1000 | Loss: 0.00037547
Iteration 124/1000 | Loss: 0.00024881
Iteration 125/1000 | Loss: 0.00029992
Iteration 126/1000 | Loss: 0.00025466
Iteration 127/1000 | Loss: 0.00052934
Iteration 128/1000 | Loss: 0.00027398
Iteration 129/1000 | Loss: 0.00034533
Iteration 130/1000 | Loss: 0.00033314
Iteration 131/1000 | Loss: 0.00042678
Iteration 132/1000 | Loss: 0.00045250
Iteration 133/1000 | Loss: 0.00033917
Iteration 134/1000 | Loss: 0.00046240
Iteration 135/1000 | Loss: 0.00033412
Iteration 136/1000 | Loss: 0.00046048
Iteration 137/1000 | Loss: 0.00044224
Iteration 138/1000 | Loss: 0.00063211
Iteration 139/1000 | Loss: 0.00047399
Iteration 140/1000 | Loss: 0.00069539
Iteration 141/1000 | Loss: 0.00061033
Iteration 142/1000 | Loss: 0.00080498
Iteration 143/1000 | Loss: 0.00043788
Iteration 144/1000 | Loss: 0.00063414
Iteration 145/1000 | Loss: 0.00068518
Iteration 146/1000 | Loss: 0.00055241
Iteration 147/1000 | Loss: 0.00053790
Iteration 148/1000 | Loss: 0.00047953
Iteration 149/1000 | Loss: 0.00036272
Iteration 150/1000 | Loss: 0.00040559
Iteration 151/1000 | Loss: 0.00037263
Iteration 152/1000 | Loss: 0.00034953
Iteration 153/1000 | Loss: 0.00028833
Iteration 154/1000 | Loss: 0.00026573
Iteration 155/1000 | Loss: 0.00025044
Iteration 156/1000 | Loss: 0.00021925
Iteration 157/1000 | Loss: 0.00030770
Iteration 158/1000 | Loss: 0.00038828
Iteration 159/1000 | Loss: 0.00034948
Iteration 160/1000 | Loss: 0.00036295
Iteration 161/1000 | Loss: 0.00034825
Iteration 162/1000 | Loss: 0.00035760
Iteration 163/1000 | Loss: 0.00034316
Iteration 164/1000 | Loss: 0.00033555
Iteration 165/1000 | Loss: 0.00032493
Iteration 166/1000 | Loss: 0.00034402
Iteration 167/1000 | Loss: 0.00033588
Iteration 168/1000 | Loss: 0.00034397
Iteration 169/1000 | Loss: 0.00037308
Iteration 170/1000 | Loss: 0.00033571
Iteration 171/1000 | Loss: 0.00024856
Iteration 172/1000 | Loss: 0.00021842
Iteration 173/1000 | Loss: 0.00039310
Iteration 174/1000 | Loss: 0.00084208
Iteration 175/1000 | Loss: 0.00053412
Iteration 176/1000 | Loss: 0.00054811
Iteration 177/1000 | Loss: 0.00047954
Iteration 178/1000 | Loss: 0.00032474
Iteration 179/1000 | Loss: 0.00037039
Iteration 180/1000 | Loss: 0.00036835
Iteration 181/1000 | Loss: 0.00035009
Iteration 182/1000 | Loss: 0.00043297
Iteration 183/1000 | Loss: 0.00032823
Iteration 184/1000 | Loss: 0.00031122
Iteration 185/1000 | Loss: 0.00039654
Iteration 186/1000 | Loss: 0.00061463
Iteration 187/1000 | Loss: 0.00038356
Iteration 188/1000 | Loss: 0.00030371
Iteration 189/1000 | Loss: 0.00034673
Iteration 190/1000 | Loss: 0.00030514
Iteration 191/1000 | Loss: 0.00033344
Iteration 192/1000 | Loss: 0.00032407
Iteration 193/1000 | Loss: 0.00032640
Iteration 194/1000 | Loss: 0.00034511
Iteration 195/1000 | Loss: 0.00040340
Iteration 196/1000 | Loss: 0.00027883
Iteration 197/1000 | Loss: 0.00028274
Iteration 198/1000 | Loss: 0.00034930
Iteration 199/1000 | Loss: 0.00040603
Iteration 200/1000 | Loss: 0.00033032
Iteration 201/1000 | Loss: 0.00028296
Iteration 202/1000 | Loss: 0.00031147
Iteration 203/1000 | Loss: 0.00041316
Iteration 204/1000 | Loss: 0.00023223
Iteration 205/1000 | Loss: 0.00015642
Iteration 206/1000 | Loss: 0.00018359
Iteration 207/1000 | Loss: 0.00014635
Iteration 208/1000 | Loss: 0.00013665
Iteration 209/1000 | Loss: 0.00017996
Iteration 210/1000 | Loss: 0.00013933
Iteration 211/1000 | Loss: 0.00013836
Iteration 212/1000 | Loss: 0.00014196
Iteration 213/1000 | Loss: 0.00015836
Iteration 214/1000 | Loss: 0.00017941
Iteration 215/1000 | Loss: 0.00008571
Iteration 216/1000 | Loss: 0.00020704
Iteration 217/1000 | Loss: 0.00017025
Iteration 218/1000 | Loss: 0.00012478
Iteration 219/1000 | Loss: 0.00012305
Iteration 220/1000 | Loss: 0.00014073
Iteration 221/1000 | Loss: 0.00015999
Iteration 222/1000 | Loss: 0.00015934
Iteration 223/1000 | Loss: 0.00019766
Iteration 224/1000 | Loss: 0.00019211
Iteration 225/1000 | Loss: 0.00018739
Iteration 226/1000 | Loss: 0.00019660
Iteration 227/1000 | Loss: 0.00020157
Iteration 228/1000 | Loss: 0.00019388
Iteration 229/1000 | Loss: 0.00020880
Iteration 230/1000 | Loss: 0.00020073
Iteration 231/1000 | Loss: 0.00017306
Iteration 232/1000 | Loss: 0.00015337
Iteration 233/1000 | Loss: 0.00022825
Iteration 234/1000 | Loss: 0.00018687
Iteration 235/1000 | Loss: 0.00018761
Iteration 236/1000 | Loss: 0.00039120
Iteration 237/1000 | Loss: 0.00012777
Iteration 238/1000 | Loss: 0.00012190
Iteration 239/1000 | Loss: 0.00015729
Iteration 240/1000 | Loss: 0.00012399
Iteration 241/1000 | Loss: 0.00030787
Iteration 242/1000 | Loss: 0.00013038
Iteration 243/1000 | Loss: 0.00016999
Iteration 244/1000 | Loss: 0.00015089
Iteration 245/1000 | Loss: 0.00021615
Iteration 246/1000 | Loss: 0.00016573
Iteration 247/1000 | Loss: 0.00014746
Iteration 248/1000 | Loss: 0.00020245
Iteration 249/1000 | Loss: 0.00017873
Iteration 250/1000 | Loss: 0.00015709
Iteration 251/1000 | Loss: 0.00016161
Iteration 252/1000 | Loss: 0.00026979
Iteration 253/1000 | Loss: 0.00017128
Iteration 254/1000 | Loss: 0.00018284
Iteration 255/1000 | Loss: 0.00015725
Iteration 256/1000 | Loss: 0.00008965
Iteration 257/1000 | Loss: 0.00013456
Iteration 258/1000 | Loss: 0.00012265
Iteration 259/1000 | Loss: 0.00015261
Iteration 260/1000 | Loss: 0.00014600
Iteration 261/1000 | Loss: 0.00009111
Iteration 262/1000 | Loss: 0.00008656
Iteration 263/1000 | Loss: 0.00010550
Iteration 264/1000 | Loss: 0.00008328
Iteration 265/1000 | Loss: 0.00007993
Iteration 266/1000 | Loss: 0.00008431
Iteration 267/1000 | Loss: 0.00008373
Iteration 268/1000 | Loss: 0.00008755
Iteration 269/1000 | Loss: 0.00009986
Iteration 270/1000 | Loss: 0.00008714
Iteration 271/1000 | Loss: 0.00010997
Iteration 272/1000 | Loss: 0.00009586
Iteration 273/1000 | Loss: 0.00011040
Iteration 274/1000 | Loss: 0.00007766
Iteration 275/1000 | Loss: 0.00007597
Iteration 276/1000 | Loss: 0.00008162
Iteration 277/1000 | Loss: 0.00008840
Iteration 278/1000 | Loss: 0.00006422
Iteration 279/1000 | Loss: 0.00008838
Iteration 280/1000 | Loss: 0.00009544
Iteration 281/1000 | Loss: 0.00008162
Iteration 282/1000 | Loss: 0.00008653
Iteration 283/1000 | Loss: 0.00009982
Iteration 284/1000 | Loss: 0.00011396
Iteration 285/1000 | Loss: 0.00009600
Iteration 286/1000 | Loss: 0.00011795
Iteration 287/1000 | Loss: 0.00009901
Iteration 288/1000 | Loss: 0.00010637
Iteration 289/1000 | Loss: 0.00008392
Iteration 290/1000 | Loss: 0.00007136
Iteration 291/1000 | Loss: 0.00009121
Iteration 292/1000 | Loss: 0.00008932
Iteration 293/1000 | Loss: 0.00008022
Iteration 294/1000 | Loss: 0.00009133
Iteration 295/1000 | Loss: 0.00008722
Iteration 296/1000 | Loss: 0.00011097
Iteration 297/1000 | Loss: 0.00008830
Iteration 298/1000 | Loss: 0.00010791
Iteration 299/1000 | Loss: 0.00009056
Iteration 300/1000 | Loss: 0.00010880
Iteration 301/1000 | Loss: 0.00008840
Iteration 302/1000 | Loss: 0.00009419
Iteration 303/1000 | Loss: 0.00009283
Iteration 304/1000 | Loss: 0.00010171
Iteration 305/1000 | Loss: 0.00010793
Iteration 306/1000 | Loss: 0.00012730
Iteration 307/1000 | Loss: 0.00012665
Iteration 308/1000 | Loss: 0.00010955
Iteration 309/1000 | Loss: 0.00011803
Iteration 310/1000 | Loss: 0.00011272
Iteration 311/1000 | Loss: 0.00011835
Iteration 312/1000 | Loss: 0.00011730
Iteration 313/1000 | Loss: 0.00010742
Iteration 314/1000 | Loss: 0.00011006
Iteration 315/1000 | Loss: 0.00010781
Iteration 316/1000 | Loss: 0.00010568
Iteration 317/1000 | Loss: 0.00010524
Iteration 318/1000 | Loss: 0.00011789
Iteration 319/1000 | Loss: 0.00014618
Iteration 320/1000 | Loss: 0.00009007
Iteration 321/1000 | Loss: 0.00005314
Iteration 322/1000 | Loss: 0.00007937
Iteration 323/1000 | Loss: 0.00009162
Iteration 324/1000 | Loss: 0.00010638
Iteration 325/1000 | Loss: 0.00006958
Iteration 326/1000 | Loss: 0.00008445
Iteration 327/1000 | Loss: 0.00006916
Iteration 328/1000 | Loss: 0.00007590
Iteration 329/1000 | Loss: 0.00006389
Iteration 330/1000 | Loss: 0.00009262
Iteration 331/1000 | Loss: 0.00007004
Iteration 332/1000 | Loss: 0.00010296
Iteration 333/1000 | Loss: 0.00007085
Iteration 334/1000 | Loss: 0.00012093
Iteration 335/1000 | Loss: 0.00011870
Iteration 336/1000 | Loss: 0.00011270
Iteration 337/1000 | Loss: 0.00012424
Iteration 338/1000 | Loss: 0.00010742
Iteration 339/1000 | Loss: 0.00011721
Iteration 340/1000 | Loss: 0.00010962
Iteration 341/1000 | Loss: 0.00010565
Iteration 342/1000 | Loss: 0.00010653
Iteration 343/1000 | Loss: 0.00008765
Iteration 344/1000 | Loss: 0.00010120
Iteration 345/1000 | Loss: 0.00010964
Iteration 346/1000 | Loss: 0.00004550
Iteration 347/1000 | Loss: 0.00006151
Iteration 348/1000 | Loss: 0.00005583
Iteration 349/1000 | Loss: 0.00007035
Iteration 350/1000 | Loss: 0.00004330
Iteration 351/1000 | Loss: 0.00004944
Iteration 352/1000 | Loss: 0.00005607
Iteration 353/1000 | Loss: 0.00004739
Iteration 354/1000 | Loss: 0.00004662
Iteration 355/1000 | Loss: 0.00003690
Iteration 356/1000 | Loss: 0.00004300
Iteration 357/1000 | Loss: 0.00005254
Iteration 358/1000 | Loss: 0.00003085
Iteration 359/1000 | Loss: 0.00004877
Iteration 360/1000 | Loss: 0.00004530
Iteration 361/1000 | Loss: 0.00005828
Iteration 362/1000 | Loss: 0.00005619
Iteration 363/1000 | Loss: 0.00005858
Iteration 364/1000 | Loss: 0.00005053
Iteration 365/1000 | Loss: 0.00006281
Iteration 366/1000 | Loss: 0.00005287
Iteration 367/1000 | Loss: 0.00006158
Iteration 368/1000 | Loss: 0.00005051
Iteration 369/1000 | Loss: 0.00006410
Iteration 370/1000 | Loss: 0.00005259
Iteration 371/1000 | Loss: 0.00006949
Iteration 372/1000 | Loss: 0.00006058
Iteration 373/1000 | Loss: 0.00005699
Iteration 374/1000 | Loss: 0.00003463
Iteration 375/1000 | Loss: 0.00004850
Iteration 376/1000 | Loss: 0.00005502
Iteration 377/1000 | Loss: 0.00006280
Iteration 378/1000 | Loss: 0.00005499
Iteration 379/1000 | Loss: 0.00006462
Iteration 380/1000 | Loss: 0.00005547
Iteration 381/1000 | Loss: 0.00006353
Iteration 382/1000 | Loss: 0.00005743
Iteration 383/1000 | Loss: 0.00005998
Iteration 384/1000 | Loss: 0.00005516
Iteration 385/1000 | Loss: 0.00006368
Iteration 386/1000 | Loss: 0.00006425
Iteration 387/1000 | Loss: 0.00005858
Iteration 388/1000 | Loss: 0.00005098
Iteration 389/1000 | Loss: 0.00005902
Iteration 390/1000 | Loss: 0.00005503
Iteration 391/1000 | Loss: 0.00005636
Iteration 392/1000 | Loss: 0.00006295
Iteration 393/1000 | Loss: 0.00005612
Iteration 394/1000 | Loss: 0.00003131
Iteration 395/1000 | Loss: 0.00004625
Iteration 396/1000 | Loss: 0.00006509
Iteration 397/1000 | Loss: 0.00004179
Iteration 398/1000 | Loss: 0.00004281
Iteration 399/1000 | Loss: 0.00003882
Iteration 400/1000 | Loss: 0.00004201
Iteration 401/1000 | Loss: 0.00004160
Iteration 402/1000 | Loss: 0.00004603
Iteration 403/1000 | Loss: 0.00003853
Iteration 404/1000 | Loss: 0.00004315
Iteration 405/1000 | Loss: 0.00003760
Iteration 406/1000 | Loss: 0.00003368
Iteration 407/1000 | Loss: 0.00003499
Iteration 408/1000 | Loss: 0.00003898
Iteration 409/1000 | Loss: 0.00003375
Iteration 410/1000 | Loss: 0.00002999
Iteration 411/1000 | Loss: 0.00003566
Iteration 412/1000 | Loss: 0.00003455
Iteration 413/1000 | Loss: 0.00003682
Iteration 414/1000 | Loss: 0.00003861
Iteration 415/1000 | Loss: 0.00004134
Iteration 416/1000 | Loss: 0.00003920
Iteration 417/1000 | Loss: 0.00003355
Iteration 418/1000 | Loss: 0.00004424
Iteration 419/1000 | Loss: 0.00004987
Iteration 420/1000 | Loss: 0.00003586
Iteration 421/1000 | Loss: 0.00003879
Iteration 422/1000 | Loss: 0.00003418
Iteration 423/1000 | Loss: 0.00003889
Iteration 424/1000 | Loss: 0.00003417
Iteration 425/1000 | Loss: 0.00003398
Iteration 426/1000 | Loss: 0.00003752
Iteration 427/1000 | Loss: 0.00003569
Iteration 428/1000 | Loss: 0.00003771
Iteration 429/1000 | Loss: 0.00003207
Iteration 430/1000 | Loss: 0.00003705
Iteration 431/1000 | Loss: 0.00003713
Iteration 432/1000 | Loss: 0.00003551
Iteration 433/1000 | Loss: 0.00003704
Iteration 434/1000 | Loss: 0.00003924
Iteration 435/1000 | Loss: 0.00004403
Iteration 436/1000 | Loss: 0.00004285
Iteration 437/1000 | Loss: 0.00003298
Iteration 438/1000 | Loss: 0.00003952
Iteration 439/1000 | Loss: 0.00003373
Iteration 440/1000 | Loss: 0.00005656
Iteration 441/1000 | Loss: 0.00003195
Iteration 442/1000 | Loss: 0.00003837
Iteration 443/1000 | Loss: 0.00004089
Iteration 444/1000 | Loss: 0.00003220
Iteration 445/1000 | Loss: 0.00004048
Iteration 446/1000 | Loss: 0.00003734
Iteration 447/1000 | Loss: 0.00004018
Iteration 448/1000 | Loss: 0.00003606
Iteration 449/1000 | Loss: 0.00002708
Iteration 450/1000 | Loss: 0.00003954
Iteration 451/1000 | Loss: 0.00003935
Iteration 452/1000 | Loss: 0.00002302
Iteration 453/1000 | Loss: 0.00002261
Iteration 454/1000 | Loss: 0.00002238
Iteration 455/1000 | Loss: 0.00002233
Iteration 456/1000 | Loss: 0.00002213
Iteration 457/1000 | Loss: 0.00002202
Iteration 458/1000 | Loss: 0.00002200
Iteration 459/1000 | Loss: 0.00002192
Iteration 460/1000 | Loss: 0.00002192
Iteration 461/1000 | Loss: 0.00002192
Iteration 462/1000 | Loss: 0.00002192
Iteration 463/1000 | Loss: 0.00002191
Iteration 464/1000 | Loss: 0.00002191
Iteration 465/1000 | Loss: 0.00002191
Iteration 466/1000 | Loss: 0.00002191
Iteration 467/1000 | Loss: 0.00002191
Iteration 468/1000 | Loss: 0.00002191
Iteration 469/1000 | Loss: 0.00002191
Iteration 470/1000 | Loss: 0.00002191
Iteration 471/1000 | Loss: 0.00002191
Iteration 472/1000 | Loss: 0.00002191
Iteration 473/1000 | Loss: 0.00002191
Iteration 474/1000 | Loss: 0.00002191
Iteration 475/1000 | Loss: 0.00002191
Iteration 476/1000 | Loss: 0.00002191
Iteration 477/1000 | Loss: 0.00002191
Iteration 478/1000 | Loss: 0.00002191
Iteration 479/1000 | Loss: 0.00002190
Iteration 480/1000 | Loss: 0.00002190
Iteration 481/1000 | Loss: 0.00002190
Iteration 482/1000 | Loss: 0.00002190
Iteration 483/1000 | Loss: 0.00002190
Iteration 484/1000 | Loss: 0.00002190
Iteration 485/1000 | Loss: 0.00002190
Iteration 486/1000 | Loss: 0.00002190
Iteration 487/1000 | Loss: 0.00002190
Iteration 488/1000 | Loss: 0.00002190
Iteration 489/1000 | Loss: 0.00002190
Iteration 490/1000 | Loss: 0.00002190
Iteration 491/1000 | Loss: 0.00002189
Iteration 492/1000 | Loss: 0.00002189
Iteration 493/1000 | Loss: 0.00002189
Iteration 494/1000 | Loss: 0.00002189
Iteration 495/1000 | Loss: 0.00002189
Iteration 496/1000 | Loss: 0.00002188
Iteration 497/1000 | Loss: 0.00002188
Iteration 498/1000 | Loss: 0.00002188
Iteration 499/1000 | Loss: 0.00002187
Iteration 500/1000 | Loss: 0.00002186
Iteration 501/1000 | Loss: 0.00002185
Iteration 502/1000 | Loss: 0.00002185
Iteration 503/1000 | Loss: 0.00002185
Iteration 504/1000 | Loss: 0.00002185
Iteration 505/1000 | Loss: 0.00002185
Iteration 506/1000 | Loss: 0.00002185
Iteration 507/1000 | Loss: 0.00002185
Iteration 508/1000 | Loss: 0.00002184
Iteration 509/1000 | Loss: 0.00002184
Iteration 510/1000 | Loss: 0.00002184
Iteration 511/1000 | Loss: 0.00002184
Iteration 512/1000 | Loss: 0.00002184
Iteration 513/1000 | Loss: 0.00002184
Iteration 514/1000 | Loss: 0.00002183
Iteration 515/1000 | Loss: 0.00002183
Iteration 516/1000 | Loss: 0.00002183
Iteration 517/1000 | Loss: 0.00002183
Iteration 518/1000 | Loss: 0.00002183
Iteration 519/1000 | Loss: 0.00002183
Iteration 520/1000 | Loss: 0.00002183
Iteration 521/1000 | Loss: 0.00002183
Iteration 522/1000 | Loss: 0.00002183
Iteration 523/1000 | Loss: 0.00002183
Iteration 524/1000 | Loss: 0.00002183
Iteration 525/1000 | Loss: 0.00002182
Iteration 526/1000 | Loss: 0.00002182
Iteration 527/1000 | Loss: 0.00002182
Iteration 528/1000 | Loss: 0.00002182
Iteration 529/1000 | Loss: 0.00002182
Iteration 530/1000 | Loss: 0.00002182
Iteration 531/1000 | Loss: 0.00002182
Iteration 532/1000 | Loss: 0.00002182
Iteration 533/1000 | Loss: 0.00002182
Iteration 534/1000 | Loss: 0.00002182
Iteration 535/1000 | Loss: 0.00002182
Iteration 536/1000 | Loss: 0.00002182
Iteration 537/1000 | Loss: 0.00002181
Iteration 538/1000 | Loss: 0.00002181
Iteration 539/1000 | Loss: 0.00002181
Iteration 540/1000 | Loss: 0.00002181
Iteration 541/1000 | Loss: 0.00002181
Iteration 542/1000 | Loss: 0.00002181
Iteration 543/1000 | Loss: 0.00002181
Iteration 544/1000 | Loss: 0.00002181
Iteration 545/1000 | Loss: 0.00002181
Iteration 546/1000 | Loss: 0.00002181
Iteration 547/1000 | Loss: 0.00002181
Iteration 548/1000 | Loss: 0.00002181
Iteration 549/1000 | Loss: 0.00002181
Iteration 550/1000 | Loss: 0.00002181
Iteration 551/1000 | Loss: 0.00002181
Iteration 552/1000 | Loss: 0.00002181
Iteration 553/1000 | Loss: 0.00002181
Iteration 554/1000 | Loss: 0.00002181
Iteration 555/1000 | Loss: 0.00002180
Iteration 556/1000 | Loss: 0.00002180
Iteration 557/1000 | Loss: 0.00002180
Iteration 558/1000 | Loss: 0.00002180
Iteration 559/1000 | Loss: 0.00002180
Iteration 560/1000 | Loss: 0.00002180
Iteration 561/1000 | Loss: 0.00002180
Iteration 562/1000 | Loss: 0.00002180
Iteration 563/1000 | Loss: 0.00002180
Iteration 564/1000 | Loss: 0.00002180
Iteration 565/1000 | Loss: 0.00002180
Iteration 566/1000 | Loss: 0.00002180
Iteration 567/1000 | Loss: 0.00002180
Iteration 568/1000 | Loss: 0.00002180
Iteration 569/1000 | Loss: 0.00002180
Iteration 570/1000 | Loss: 0.00002180
Iteration 571/1000 | Loss: 0.00002180
Iteration 572/1000 | Loss: 0.00002180
Iteration 573/1000 | Loss: 0.00002180
Iteration 574/1000 | Loss: 0.00002180
Iteration 575/1000 | Loss: 0.00002180
Iteration 576/1000 | Loss: 0.00002180
Iteration 577/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 577. Stopping optimization.
Last 5 losses: [2.1796317014377564e-05, 2.1796317014377564e-05, 2.1796317014377564e-05, 2.1796317014377564e-05, 2.1796317014377564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1796317014377564e-05

Optimization complete. Final v2v error: 3.5731472969055176 mm

Highest mean error: 11.128033638000488 mm for frame 58

Lowest mean error: 3.0715510845184326 mm for frame 145

Saving results

Total time: 705.9843394756317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514164
Iteration 2/25 | Loss: 0.00151874
Iteration 3/25 | Loss: 0.00121669
Iteration 4/25 | Loss: 0.00117219
Iteration 5/25 | Loss: 0.00116581
Iteration 6/25 | Loss: 0.00117567
Iteration 7/25 | Loss: 0.00114838
Iteration 8/25 | Loss: 0.00113464
Iteration 9/25 | Loss: 0.00113133
Iteration 10/25 | Loss: 0.00113219
Iteration 11/25 | Loss: 0.00113004
Iteration 12/25 | Loss: 0.00112916
Iteration 13/25 | Loss: 0.00112890
Iteration 14/25 | Loss: 0.00112890
Iteration 15/25 | Loss: 0.00112890
Iteration 16/25 | Loss: 0.00112890
Iteration 17/25 | Loss: 0.00112889
Iteration 18/25 | Loss: 0.00112889
Iteration 19/25 | Loss: 0.00112889
Iteration 20/25 | Loss: 0.00112889
Iteration 21/25 | Loss: 0.00112889
Iteration 22/25 | Loss: 0.00112889
Iteration 23/25 | Loss: 0.00112889
Iteration 24/25 | Loss: 0.00112889
Iteration 25/25 | Loss: 0.00112889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18208468
Iteration 2/25 | Loss: 0.00213130
Iteration 3/25 | Loss: 0.00213129
Iteration 4/25 | Loss: 0.00213129
Iteration 5/25 | Loss: 0.00213129
Iteration 6/25 | Loss: 0.00213129
Iteration 7/25 | Loss: 0.00213129
Iteration 8/25 | Loss: 0.00213129
Iteration 9/25 | Loss: 0.00213129
Iteration 10/25 | Loss: 0.00213129
Iteration 11/25 | Loss: 0.00213129
Iteration 12/25 | Loss: 0.00213129
Iteration 13/25 | Loss: 0.00213129
Iteration 14/25 | Loss: 0.00213129
Iteration 15/25 | Loss: 0.00213129
Iteration 16/25 | Loss: 0.00213129
Iteration 17/25 | Loss: 0.00213129
Iteration 18/25 | Loss: 0.00213129
Iteration 19/25 | Loss: 0.00213129
Iteration 20/25 | Loss: 0.00213129
Iteration 21/25 | Loss: 0.00213129
Iteration 22/25 | Loss: 0.00213129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0021312865428626537, 0.0021312865428626537, 0.0021312865428626537, 0.0021312865428626537, 0.0021312865428626537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021312865428626537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213129
Iteration 2/1000 | Loss: 0.00029563
Iteration 3/1000 | Loss: 0.00079628
Iteration 4/1000 | Loss: 0.00021103
Iteration 5/1000 | Loss: 0.00020874
Iteration 6/1000 | Loss: 0.00034779
Iteration 7/1000 | Loss: 0.00018594
Iteration 8/1000 | Loss: 0.00024658
Iteration 9/1000 | Loss: 0.00020841
Iteration 10/1000 | Loss: 0.00018014
Iteration 11/1000 | Loss: 0.00010475
Iteration 12/1000 | Loss: 0.00006767
Iteration 13/1000 | Loss: 0.00004524
Iteration 14/1000 | Loss: 0.00004836
Iteration 15/1000 | Loss: 0.00008125
Iteration 16/1000 | Loss: 0.00007042
Iteration 17/1000 | Loss: 0.00007364
Iteration 18/1000 | Loss: 0.00012286
Iteration 19/1000 | Loss: 0.00030148
Iteration 20/1000 | Loss: 0.00010543
Iteration 21/1000 | Loss: 0.00008982
Iteration 22/1000 | Loss: 0.00007089
Iteration 23/1000 | Loss: 0.00007387
Iteration 24/1000 | Loss: 0.00007197
Iteration 25/1000 | Loss: 0.00007191
Iteration 26/1000 | Loss: 0.00006039
Iteration 27/1000 | Loss: 0.00006406
Iteration 28/1000 | Loss: 0.00005155
Iteration 29/1000 | Loss: 0.00003709
Iteration 30/1000 | Loss: 0.00003432
Iteration 31/1000 | Loss: 0.00003145
Iteration 32/1000 | Loss: 0.00003041
Iteration 33/1000 | Loss: 0.00011132
Iteration 34/1000 | Loss: 0.00004550
Iteration 35/1000 | Loss: 0.00006139
Iteration 36/1000 | Loss: 0.00003948
Iteration 37/1000 | Loss: 0.00007354
Iteration 38/1000 | Loss: 0.00007105
Iteration 39/1000 | Loss: 0.00009734
Iteration 40/1000 | Loss: 0.00005174
Iteration 41/1000 | Loss: 0.00010982
Iteration 42/1000 | Loss: 0.00005279
Iteration 43/1000 | Loss: 0.00009444
Iteration 44/1000 | Loss: 0.00007401
Iteration 45/1000 | Loss: 0.00009568
Iteration 46/1000 | Loss: 0.00007948
Iteration 47/1000 | Loss: 0.00003677
Iteration 48/1000 | Loss: 0.00003481
Iteration 49/1000 | Loss: 0.00003109
Iteration 50/1000 | Loss: 0.00002784
Iteration 51/1000 | Loss: 0.00002754
Iteration 52/1000 | Loss: 0.00010320
Iteration 53/1000 | Loss: 0.00006205
Iteration 54/1000 | Loss: 0.00003311
Iteration 55/1000 | Loss: 0.00003098
Iteration 56/1000 | Loss: 0.00002783
Iteration 57/1000 | Loss: 0.00002714
Iteration 58/1000 | Loss: 0.00013950
Iteration 59/1000 | Loss: 0.00011466
Iteration 60/1000 | Loss: 0.00005668
Iteration 61/1000 | Loss: 0.00002853
Iteration 62/1000 | Loss: 0.00002729
Iteration 63/1000 | Loss: 0.00002659
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002620
Iteration 66/1000 | Loss: 0.00002618
Iteration 67/1000 | Loss: 0.00002605
Iteration 68/1000 | Loss: 0.00002605
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002603
Iteration 72/1000 | Loss: 0.00002602
Iteration 73/1000 | Loss: 0.00002598
Iteration 74/1000 | Loss: 0.00002597
Iteration 75/1000 | Loss: 0.00002597
Iteration 76/1000 | Loss: 0.00002597
Iteration 77/1000 | Loss: 0.00002597
Iteration 78/1000 | Loss: 0.00002597
Iteration 79/1000 | Loss: 0.00002597
Iteration 80/1000 | Loss: 0.00002597
Iteration 81/1000 | Loss: 0.00002596
Iteration 82/1000 | Loss: 0.00002594
Iteration 83/1000 | Loss: 0.00002594
Iteration 84/1000 | Loss: 0.00002593
Iteration 85/1000 | Loss: 0.00002593
Iteration 86/1000 | Loss: 0.00002593
Iteration 87/1000 | Loss: 0.00002593
Iteration 88/1000 | Loss: 0.00002592
Iteration 89/1000 | Loss: 0.00002592
Iteration 90/1000 | Loss: 0.00002592
Iteration 91/1000 | Loss: 0.00002592
Iteration 92/1000 | Loss: 0.00002591
Iteration 93/1000 | Loss: 0.00002591
Iteration 94/1000 | Loss: 0.00002590
Iteration 95/1000 | Loss: 0.00002590
Iteration 96/1000 | Loss: 0.00002589
Iteration 97/1000 | Loss: 0.00002589
Iteration 98/1000 | Loss: 0.00002589
Iteration 99/1000 | Loss: 0.00002588
Iteration 100/1000 | Loss: 0.00002588
Iteration 101/1000 | Loss: 0.00008792
Iteration 102/1000 | Loss: 0.00042644
Iteration 103/1000 | Loss: 0.00018068
Iteration 104/1000 | Loss: 0.00009224
Iteration 105/1000 | Loss: 0.00003024
Iteration 106/1000 | Loss: 0.00002782
Iteration 107/1000 | Loss: 0.00002658
Iteration 108/1000 | Loss: 0.00002623
Iteration 109/1000 | Loss: 0.00002581
Iteration 110/1000 | Loss: 0.00002551
Iteration 111/1000 | Loss: 0.00008807
Iteration 112/1000 | Loss: 0.00004034
Iteration 113/1000 | Loss: 0.00002548
Iteration 114/1000 | Loss: 0.00002520
Iteration 115/1000 | Loss: 0.00002517
Iteration 116/1000 | Loss: 0.00002516
Iteration 117/1000 | Loss: 0.00002516
Iteration 118/1000 | Loss: 0.00002516
Iteration 119/1000 | Loss: 0.00002516
Iteration 120/1000 | Loss: 0.00002516
Iteration 121/1000 | Loss: 0.00002516
Iteration 122/1000 | Loss: 0.00002516
Iteration 123/1000 | Loss: 0.00002516
Iteration 124/1000 | Loss: 0.00002516
Iteration 125/1000 | Loss: 0.00002515
Iteration 126/1000 | Loss: 0.00002515
Iteration 127/1000 | Loss: 0.00002515
Iteration 128/1000 | Loss: 0.00002515
Iteration 129/1000 | Loss: 0.00002515
Iteration 130/1000 | Loss: 0.00002515
Iteration 131/1000 | Loss: 0.00002515
Iteration 132/1000 | Loss: 0.00002515
Iteration 133/1000 | Loss: 0.00002515
Iteration 134/1000 | Loss: 0.00002515
Iteration 135/1000 | Loss: 0.00002515
Iteration 136/1000 | Loss: 0.00002515
Iteration 137/1000 | Loss: 0.00002515
Iteration 138/1000 | Loss: 0.00002515
Iteration 139/1000 | Loss: 0.00002515
Iteration 140/1000 | Loss: 0.00002515
Iteration 141/1000 | Loss: 0.00002514
Iteration 142/1000 | Loss: 0.00002514
Iteration 143/1000 | Loss: 0.00002514
Iteration 144/1000 | Loss: 0.00002514
Iteration 145/1000 | Loss: 0.00002514
Iteration 146/1000 | Loss: 0.00002514
Iteration 147/1000 | Loss: 0.00002514
Iteration 148/1000 | Loss: 0.00002514
Iteration 149/1000 | Loss: 0.00002514
Iteration 150/1000 | Loss: 0.00002514
Iteration 151/1000 | Loss: 0.00002514
Iteration 152/1000 | Loss: 0.00002514
Iteration 153/1000 | Loss: 0.00002514
Iteration 154/1000 | Loss: 0.00002514
Iteration 155/1000 | Loss: 0.00002513
Iteration 156/1000 | Loss: 0.00002513
Iteration 157/1000 | Loss: 0.00002513
Iteration 158/1000 | Loss: 0.00002513
Iteration 159/1000 | Loss: 0.00002513
Iteration 160/1000 | Loss: 0.00002513
Iteration 161/1000 | Loss: 0.00002513
Iteration 162/1000 | Loss: 0.00002513
Iteration 163/1000 | Loss: 0.00002513
Iteration 164/1000 | Loss: 0.00002513
Iteration 165/1000 | Loss: 0.00002513
Iteration 166/1000 | Loss: 0.00002513
Iteration 167/1000 | Loss: 0.00002513
Iteration 168/1000 | Loss: 0.00002513
Iteration 169/1000 | Loss: 0.00002513
Iteration 170/1000 | Loss: 0.00002513
Iteration 171/1000 | Loss: 0.00002513
Iteration 172/1000 | Loss: 0.00002513
Iteration 173/1000 | Loss: 0.00009179
Iteration 174/1000 | Loss: 0.00032370
Iteration 175/1000 | Loss: 0.00011891
Iteration 176/1000 | Loss: 0.00003376
Iteration 177/1000 | Loss: 0.00002903
Iteration 178/1000 | Loss: 0.00002676
Iteration 179/1000 | Loss: 0.00002598
Iteration 180/1000 | Loss: 0.00002538
Iteration 181/1000 | Loss: 0.00002499
Iteration 182/1000 | Loss: 0.00002483
Iteration 183/1000 | Loss: 0.00002479
Iteration 184/1000 | Loss: 0.00002474
Iteration 185/1000 | Loss: 0.00002473
Iteration 186/1000 | Loss: 0.00002472
Iteration 187/1000 | Loss: 0.00002472
Iteration 188/1000 | Loss: 0.00002472
Iteration 189/1000 | Loss: 0.00002471
Iteration 190/1000 | Loss: 0.00002471
Iteration 191/1000 | Loss: 0.00002470
Iteration 192/1000 | Loss: 0.00002470
Iteration 193/1000 | Loss: 0.00002470
Iteration 194/1000 | Loss: 0.00002470
Iteration 195/1000 | Loss: 0.00002470
Iteration 196/1000 | Loss: 0.00002470
Iteration 197/1000 | Loss: 0.00002470
Iteration 198/1000 | Loss: 0.00002470
Iteration 199/1000 | Loss: 0.00002470
Iteration 200/1000 | Loss: 0.00002470
Iteration 201/1000 | Loss: 0.00002469
Iteration 202/1000 | Loss: 0.00002469
Iteration 203/1000 | Loss: 0.00002469
Iteration 204/1000 | Loss: 0.00002469
Iteration 205/1000 | Loss: 0.00002469
Iteration 206/1000 | Loss: 0.00002468
Iteration 207/1000 | Loss: 0.00002468
Iteration 208/1000 | Loss: 0.00002468
Iteration 209/1000 | Loss: 0.00002467
Iteration 210/1000 | Loss: 0.00002467
Iteration 211/1000 | Loss: 0.00002467
Iteration 212/1000 | Loss: 0.00002467
Iteration 213/1000 | Loss: 0.00002467
Iteration 214/1000 | Loss: 0.00002467
Iteration 215/1000 | Loss: 0.00002467
Iteration 216/1000 | Loss: 0.00002467
Iteration 217/1000 | Loss: 0.00002466
Iteration 218/1000 | Loss: 0.00002466
Iteration 219/1000 | Loss: 0.00002466
Iteration 220/1000 | Loss: 0.00002466
Iteration 221/1000 | Loss: 0.00002466
Iteration 222/1000 | Loss: 0.00002466
Iteration 223/1000 | Loss: 0.00002466
Iteration 224/1000 | Loss: 0.00002465
Iteration 225/1000 | Loss: 0.00002465
Iteration 226/1000 | Loss: 0.00002465
Iteration 227/1000 | Loss: 0.00002465
Iteration 228/1000 | Loss: 0.00002464
Iteration 229/1000 | Loss: 0.00002464
Iteration 230/1000 | Loss: 0.00002464
Iteration 231/1000 | Loss: 0.00002464
Iteration 232/1000 | Loss: 0.00002464
Iteration 233/1000 | Loss: 0.00002464
Iteration 234/1000 | Loss: 0.00002464
Iteration 235/1000 | Loss: 0.00002464
Iteration 236/1000 | Loss: 0.00002464
Iteration 237/1000 | Loss: 0.00002464
Iteration 238/1000 | Loss: 0.00002464
Iteration 239/1000 | Loss: 0.00002464
Iteration 240/1000 | Loss: 0.00002463
Iteration 241/1000 | Loss: 0.00002463
Iteration 242/1000 | Loss: 0.00002463
Iteration 243/1000 | Loss: 0.00002463
Iteration 244/1000 | Loss: 0.00002463
Iteration 245/1000 | Loss: 0.00002463
Iteration 246/1000 | Loss: 0.00002463
Iteration 247/1000 | Loss: 0.00002463
Iteration 248/1000 | Loss: 0.00002463
Iteration 249/1000 | Loss: 0.00002463
Iteration 250/1000 | Loss: 0.00002463
Iteration 251/1000 | Loss: 0.00002463
Iteration 252/1000 | Loss: 0.00002463
Iteration 253/1000 | Loss: 0.00002463
Iteration 254/1000 | Loss: 0.00002463
Iteration 255/1000 | Loss: 0.00002463
Iteration 256/1000 | Loss: 0.00002463
Iteration 257/1000 | Loss: 0.00002463
Iteration 258/1000 | Loss: 0.00002463
Iteration 259/1000 | Loss: 0.00002463
Iteration 260/1000 | Loss: 0.00002463
Iteration 261/1000 | Loss: 0.00002463
Iteration 262/1000 | Loss: 0.00002463
Iteration 263/1000 | Loss: 0.00002463
Iteration 264/1000 | Loss: 0.00002463
Iteration 265/1000 | Loss: 0.00002463
Iteration 266/1000 | Loss: 0.00002463
Iteration 267/1000 | Loss: 0.00002463
Iteration 268/1000 | Loss: 0.00002463
Iteration 269/1000 | Loss: 0.00002463
Iteration 270/1000 | Loss: 0.00002463
Iteration 271/1000 | Loss: 0.00002463
Iteration 272/1000 | Loss: 0.00002463
Iteration 273/1000 | Loss: 0.00002463
Iteration 274/1000 | Loss: 0.00002463
Iteration 275/1000 | Loss: 0.00002463
Iteration 276/1000 | Loss: 0.00002463
Iteration 277/1000 | Loss: 0.00002463
Iteration 278/1000 | Loss: 0.00002463
Iteration 279/1000 | Loss: 0.00002463
Iteration 280/1000 | Loss: 0.00002463
Iteration 281/1000 | Loss: 0.00002463
Iteration 282/1000 | Loss: 0.00002463
Iteration 283/1000 | Loss: 0.00002463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [2.462542943248991e-05, 2.462542943248991e-05, 2.462542943248991e-05, 2.462542943248991e-05, 2.462542943248991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.462542943248991e-05

Optimization complete. Final v2v error: 3.924121618270874 mm

Highest mean error: 5.454090118408203 mm for frame 215

Lowest mean error: 3.632706642150879 mm for frame 82

Saving results

Total time: 179.88356566429138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877861
Iteration 2/25 | Loss: 0.00135506
Iteration 3/25 | Loss: 0.00113165
Iteration 4/25 | Loss: 0.00110377
Iteration 5/25 | Loss: 0.00109945
Iteration 6/25 | Loss: 0.00109835
Iteration 7/25 | Loss: 0.00109835
Iteration 8/25 | Loss: 0.00109835
Iteration 9/25 | Loss: 0.00109835
Iteration 10/25 | Loss: 0.00109835
Iteration 11/25 | Loss: 0.00109835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010983542306348681, 0.0010983542306348681, 0.0010983542306348681, 0.0010983542306348681, 0.0010983542306348681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010983542306348681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11489785
Iteration 2/25 | Loss: 0.00162007
Iteration 3/25 | Loss: 0.00162004
Iteration 4/25 | Loss: 0.00162004
Iteration 5/25 | Loss: 0.00162004
Iteration 6/25 | Loss: 0.00162004
Iteration 7/25 | Loss: 0.00162004
Iteration 8/25 | Loss: 0.00162004
Iteration 9/25 | Loss: 0.00162004
Iteration 10/25 | Loss: 0.00162004
Iteration 11/25 | Loss: 0.00162004
Iteration 12/25 | Loss: 0.00162004
Iteration 13/25 | Loss: 0.00162004
Iteration 14/25 | Loss: 0.00162004
Iteration 15/25 | Loss: 0.00162004
Iteration 16/25 | Loss: 0.00162004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016200385289266706, 0.0016200385289266706, 0.0016200385289266706, 0.0016200385289266706, 0.0016200385289266706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016200385289266706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162004
Iteration 2/1000 | Loss: 0.00004491
Iteration 3/1000 | Loss: 0.00003087
Iteration 4/1000 | Loss: 0.00002433
Iteration 5/1000 | Loss: 0.00002228
Iteration 6/1000 | Loss: 0.00002131
Iteration 7/1000 | Loss: 0.00002069
Iteration 8/1000 | Loss: 0.00002029
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001963
Iteration 11/1000 | Loss: 0.00001940
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001883
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001867
Iteration 21/1000 | Loss: 0.00001860
Iteration 22/1000 | Loss: 0.00001859
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001859
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001850
Iteration 29/1000 | Loss: 0.00001846
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001840
Iteration 34/1000 | Loss: 0.00001840
Iteration 35/1000 | Loss: 0.00001840
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001840
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001837
Iteration 45/1000 | Loss: 0.00001837
Iteration 46/1000 | Loss: 0.00001837
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001835
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001832
Iteration 65/1000 | Loss: 0.00001832
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001826
Iteration 94/1000 | Loss: 0.00001826
Iteration 95/1000 | Loss: 0.00001826
Iteration 96/1000 | Loss: 0.00001826
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001825
Iteration 99/1000 | Loss: 0.00001825
Iteration 100/1000 | Loss: 0.00001825
Iteration 101/1000 | Loss: 0.00001825
Iteration 102/1000 | Loss: 0.00001825
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001824
Iteration 105/1000 | Loss: 0.00001824
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001823
Iteration 108/1000 | Loss: 0.00001823
Iteration 109/1000 | Loss: 0.00001822
Iteration 110/1000 | Loss: 0.00001822
Iteration 111/1000 | Loss: 0.00001822
Iteration 112/1000 | Loss: 0.00001822
Iteration 113/1000 | Loss: 0.00001822
Iteration 114/1000 | Loss: 0.00001822
Iteration 115/1000 | Loss: 0.00001822
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001821
Iteration 121/1000 | Loss: 0.00001821
Iteration 122/1000 | Loss: 0.00001821
Iteration 123/1000 | Loss: 0.00001821
Iteration 124/1000 | Loss: 0.00001821
Iteration 125/1000 | Loss: 0.00001821
Iteration 126/1000 | Loss: 0.00001821
Iteration 127/1000 | Loss: 0.00001821
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001819
Iteration 140/1000 | Loss: 0.00001819
Iteration 141/1000 | Loss: 0.00001819
Iteration 142/1000 | Loss: 0.00001819
Iteration 143/1000 | Loss: 0.00001819
Iteration 144/1000 | Loss: 0.00001819
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001819
Iteration 147/1000 | Loss: 0.00001819
Iteration 148/1000 | Loss: 0.00001819
Iteration 149/1000 | Loss: 0.00001819
Iteration 150/1000 | Loss: 0.00001819
Iteration 151/1000 | Loss: 0.00001819
Iteration 152/1000 | Loss: 0.00001819
Iteration 153/1000 | Loss: 0.00001819
Iteration 154/1000 | Loss: 0.00001819
Iteration 155/1000 | Loss: 0.00001819
Iteration 156/1000 | Loss: 0.00001819
Iteration 157/1000 | Loss: 0.00001819
Iteration 158/1000 | Loss: 0.00001818
Iteration 159/1000 | Loss: 0.00001818
Iteration 160/1000 | Loss: 0.00001818
Iteration 161/1000 | Loss: 0.00001818
Iteration 162/1000 | Loss: 0.00001818
Iteration 163/1000 | Loss: 0.00001818
Iteration 164/1000 | Loss: 0.00001817
Iteration 165/1000 | Loss: 0.00001817
Iteration 166/1000 | Loss: 0.00001817
Iteration 167/1000 | Loss: 0.00001817
Iteration 168/1000 | Loss: 0.00001817
Iteration 169/1000 | Loss: 0.00001817
Iteration 170/1000 | Loss: 0.00001817
Iteration 171/1000 | Loss: 0.00001817
Iteration 172/1000 | Loss: 0.00001817
Iteration 173/1000 | Loss: 0.00001817
Iteration 174/1000 | Loss: 0.00001817
Iteration 175/1000 | Loss: 0.00001817
Iteration 176/1000 | Loss: 0.00001817
Iteration 177/1000 | Loss: 0.00001817
Iteration 178/1000 | Loss: 0.00001817
Iteration 179/1000 | Loss: 0.00001817
Iteration 180/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.8170070688938722e-05, 1.8170070688938722e-05, 1.8170070688938722e-05, 1.8170070688938722e-05, 1.8170070688938722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8170070688938722e-05

Optimization complete. Final v2v error: 3.490415573120117 mm

Highest mean error: 3.9024765491485596 mm for frame 3

Lowest mean error: 3.186405897140503 mm for frame 139

Saving results

Total time: 42.19408440589905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867749
Iteration 2/25 | Loss: 0.00124384
Iteration 3/25 | Loss: 0.00101320
Iteration 4/25 | Loss: 0.00099301
Iteration 5/25 | Loss: 0.00099087
Iteration 6/25 | Loss: 0.00099087
Iteration 7/25 | Loss: 0.00099087
Iteration 8/25 | Loss: 0.00099087
Iteration 9/25 | Loss: 0.00099087
Iteration 10/25 | Loss: 0.00099087
Iteration 11/25 | Loss: 0.00099087
Iteration 12/25 | Loss: 0.00099087
Iteration 13/25 | Loss: 0.00099087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009908719221130013, 0.0009908719221130013, 0.0009908719221130013, 0.0009908719221130013, 0.0009908719221130013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009908719221130013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20598412
Iteration 2/25 | Loss: 0.00203388
Iteration 3/25 | Loss: 0.00203388
Iteration 4/25 | Loss: 0.00203388
Iteration 5/25 | Loss: 0.00203388
Iteration 6/25 | Loss: 0.00203388
Iteration 7/25 | Loss: 0.00203388
Iteration 8/25 | Loss: 0.00203387
Iteration 9/25 | Loss: 0.00203387
Iteration 10/25 | Loss: 0.00203387
Iteration 11/25 | Loss: 0.00203387
Iteration 12/25 | Loss: 0.00203387
Iteration 13/25 | Loss: 0.00203387
Iteration 14/25 | Loss: 0.00203387
Iteration 15/25 | Loss: 0.00203387
Iteration 16/25 | Loss: 0.00203387
Iteration 17/25 | Loss: 0.00203387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020338743925094604, 0.0020338743925094604, 0.0020338743925094604, 0.0020338743925094604, 0.0020338743925094604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020338743925094604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203387
Iteration 2/1000 | Loss: 0.00002704
Iteration 3/1000 | Loss: 0.00001496
Iteration 4/1000 | Loss: 0.00001282
Iteration 5/1000 | Loss: 0.00001192
Iteration 6/1000 | Loss: 0.00001109
Iteration 7/1000 | Loss: 0.00001053
Iteration 8/1000 | Loss: 0.00001010
Iteration 9/1000 | Loss: 0.00000968
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000909
Iteration 12/1000 | Loss: 0.00000898
Iteration 13/1000 | Loss: 0.00000896
Iteration 14/1000 | Loss: 0.00000893
Iteration 15/1000 | Loss: 0.00000892
Iteration 16/1000 | Loss: 0.00000890
Iteration 17/1000 | Loss: 0.00000889
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000877
Iteration 20/1000 | Loss: 0.00000877
Iteration 21/1000 | Loss: 0.00000877
Iteration 22/1000 | Loss: 0.00000877
Iteration 23/1000 | Loss: 0.00000877
Iteration 24/1000 | Loss: 0.00000871
Iteration 25/1000 | Loss: 0.00000871
Iteration 26/1000 | Loss: 0.00000871
Iteration 27/1000 | Loss: 0.00000871
Iteration 28/1000 | Loss: 0.00000870
Iteration 29/1000 | Loss: 0.00000869
Iteration 30/1000 | Loss: 0.00000867
Iteration 31/1000 | Loss: 0.00000866
Iteration 32/1000 | Loss: 0.00000866
Iteration 33/1000 | Loss: 0.00000866
Iteration 34/1000 | Loss: 0.00000865
Iteration 35/1000 | Loss: 0.00000863
Iteration 36/1000 | Loss: 0.00000861
Iteration 37/1000 | Loss: 0.00000861
Iteration 38/1000 | Loss: 0.00000860
Iteration 39/1000 | Loss: 0.00000859
Iteration 40/1000 | Loss: 0.00000858
Iteration 41/1000 | Loss: 0.00000858
Iteration 42/1000 | Loss: 0.00000858
Iteration 43/1000 | Loss: 0.00000858
Iteration 44/1000 | Loss: 0.00000857
Iteration 45/1000 | Loss: 0.00000856
Iteration 46/1000 | Loss: 0.00000855
Iteration 47/1000 | Loss: 0.00000855
Iteration 48/1000 | Loss: 0.00000855
Iteration 49/1000 | Loss: 0.00000855
Iteration 50/1000 | Loss: 0.00000854
Iteration 51/1000 | Loss: 0.00000854
Iteration 52/1000 | Loss: 0.00000854
Iteration 53/1000 | Loss: 0.00000854
Iteration 54/1000 | Loss: 0.00000853
Iteration 55/1000 | Loss: 0.00000851
Iteration 56/1000 | Loss: 0.00000851
Iteration 57/1000 | Loss: 0.00000850
Iteration 58/1000 | Loss: 0.00000849
Iteration 59/1000 | Loss: 0.00000849
Iteration 60/1000 | Loss: 0.00000848
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000848
Iteration 63/1000 | Loss: 0.00000848
Iteration 64/1000 | Loss: 0.00000848
Iteration 65/1000 | Loss: 0.00000848
Iteration 66/1000 | Loss: 0.00000847
Iteration 67/1000 | Loss: 0.00000847
Iteration 68/1000 | Loss: 0.00000846
Iteration 69/1000 | Loss: 0.00000846
Iteration 70/1000 | Loss: 0.00000845
Iteration 71/1000 | Loss: 0.00000845
Iteration 72/1000 | Loss: 0.00000844
Iteration 73/1000 | Loss: 0.00000844
Iteration 74/1000 | Loss: 0.00000844
Iteration 75/1000 | Loss: 0.00000843
Iteration 76/1000 | Loss: 0.00000843
Iteration 77/1000 | Loss: 0.00000843
Iteration 78/1000 | Loss: 0.00000842
Iteration 79/1000 | Loss: 0.00000842
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000841
Iteration 84/1000 | Loss: 0.00000840
Iteration 85/1000 | Loss: 0.00000840
Iteration 86/1000 | Loss: 0.00000839
Iteration 87/1000 | Loss: 0.00000839
Iteration 88/1000 | Loss: 0.00000839
Iteration 89/1000 | Loss: 0.00000839
Iteration 90/1000 | Loss: 0.00000839
Iteration 91/1000 | Loss: 0.00000839
Iteration 92/1000 | Loss: 0.00000839
Iteration 93/1000 | Loss: 0.00000839
Iteration 94/1000 | Loss: 0.00000839
Iteration 95/1000 | Loss: 0.00000839
Iteration 96/1000 | Loss: 0.00000839
Iteration 97/1000 | Loss: 0.00000838
Iteration 98/1000 | Loss: 0.00000838
Iteration 99/1000 | Loss: 0.00000838
Iteration 100/1000 | Loss: 0.00000838
Iteration 101/1000 | Loss: 0.00000838
Iteration 102/1000 | Loss: 0.00000838
Iteration 103/1000 | Loss: 0.00000837
Iteration 104/1000 | Loss: 0.00000837
Iteration 105/1000 | Loss: 0.00000837
Iteration 106/1000 | Loss: 0.00000837
Iteration 107/1000 | Loss: 0.00000836
Iteration 108/1000 | Loss: 0.00000836
Iteration 109/1000 | Loss: 0.00000836
Iteration 110/1000 | Loss: 0.00000835
Iteration 111/1000 | Loss: 0.00000835
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000835
Iteration 115/1000 | Loss: 0.00000835
Iteration 116/1000 | Loss: 0.00000835
Iteration 117/1000 | Loss: 0.00000834
Iteration 118/1000 | Loss: 0.00000834
Iteration 119/1000 | Loss: 0.00000834
Iteration 120/1000 | Loss: 0.00000834
Iteration 121/1000 | Loss: 0.00000834
Iteration 122/1000 | Loss: 0.00000834
Iteration 123/1000 | Loss: 0.00000834
Iteration 124/1000 | Loss: 0.00000834
Iteration 125/1000 | Loss: 0.00000833
Iteration 126/1000 | Loss: 0.00000833
Iteration 127/1000 | Loss: 0.00000833
Iteration 128/1000 | Loss: 0.00000833
Iteration 129/1000 | Loss: 0.00000833
Iteration 130/1000 | Loss: 0.00000833
Iteration 131/1000 | Loss: 0.00000833
Iteration 132/1000 | Loss: 0.00000833
Iteration 133/1000 | Loss: 0.00000832
Iteration 134/1000 | Loss: 0.00000832
Iteration 135/1000 | Loss: 0.00000832
Iteration 136/1000 | Loss: 0.00000832
Iteration 137/1000 | Loss: 0.00000832
Iteration 138/1000 | Loss: 0.00000832
Iteration 139/1000 | Loss: 0.00000832
Iteration 140/1000 | Loss: 0.00000832
Iteration 141/1000 | Loss: 0.00000832
Iteration 142/1000 | Loss: 0.00000832
Iteration 143/1000 | Loss: 0.00000832
Iteration 144/1000 | Loss: 0.00000832
Iteration 145/1000 | Loss: 0.00000832
Iteration 146/1000 | Loss: 0.00000832
Iteration 147/1000 | Loss: 0.00000832
Iteration 148/1000 | Loss: 0.00000832
Iteration 149/1000 | Loss: 0.00000832
Iteration 150/1000 | Loss: 0.00000832
Iteration 151/1000 | Loss: 0.00000832
Iteration 152/1000 | Loss: 0.00000832
Iteration 153/1000 | Loss: 0.00000832
Iteration 154/1000 | Loss: 0.00000832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [8.316381354234181e-06, 8.316381354234181e-06, 8.316381354234181e-06, 8.316381354234181e-06, 8.316381354234181e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.316381354234181e-06

Optimization complete. Final v2v error: 2.438929557800293 mm

Highest mean error: 2.9192183017730713 mm for frame 132

Lowest mean error: 2.0793774127960205 mm for frame 37

Saving results

Total time: 43.056952714920044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413296
Iteration 2/25 | Loss: 0.00105672
Iteration 3/25 | Loss: 0.00097160
Iteration 4/25 | Loss: 0.00095858
Iteration 5/25 | Loss: 0.00095484
Iteration 6/25 | Loss: 0.00095348
Iteration 7/25 | Loss: 0.00095348
Iteration 8/25 | Loss: 0.00095348
Iteration 9/25 | Loss: 0.00095348
Iteration 10/25 | Loss: 0.00095348
Iteration 11/25 | Loss: 0.00095348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009534806595183909, 0.0009534806595183909, 0.0009534806595183909, 0.0009534806595183909, 0.0009534806595183909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009534806595183909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.39196301
Iteration 2/25 | Loss: 0.00187329
Iteration 3/25 | Loss: 0.00187328
Iteration 4/25 | Loss: 0.00187328
Iteration 5/25 | Loss: 0.00187328
Iteration 6/25 | Loss: 0.00187328
Iteration 7/25 | Loss: 0.00187328
Iteration 8/25 | Loss: 0.00187328
Iteration 9/25 | Loss: 0.00187328
Iteration 10/25 | Loss: 0.00187328
Iteration 11/25 | Loss: 0.00187328
Iteration 12/25 | Loss: 0.00187328
Iteration 13/25 | Loss: 0.00187328
Iteration 14/25 | Loss: 0.00187328
Iteration 15/25 | Loss: 0.00187328
Iteration 16/25 | Loss: 0.00187328
Iteration 17/25 | Loss: 0.00187328
Iteration 18/25 | Loss: 0.00187328
Iteration 19/25 | Loss: 0.00187328
Iteration 20/25 | Loss: 0.00187328
Iteration 21/25 | Loss: 0.00187328
Iteration 22/25 | Loss: 0.00187328
Iteration 23/25 | Loss: 0.00187328
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00187327538151294, 0.00187327538151294, 0.00187327538151294, 0.00187327538151294, 0.00187327538151294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00187327538151294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187328
Iteration 2/1000 | Loss: 0.00001587
Iteration 3/1000 | Loss: 0.00001120
Iteration 4/1000 | Loss: 0.00001005
Iteration 5/1000 | Loss: 0.00000948
Iteration 6/1000 | Loss: 0.00000903
Iteration 7/1000 | Loss: 0.00000877
Iteration 8/1000 | Loss: 0.00000855
Iteration 9/1000 | Loss: 0.00000839
Iteration 10/1000 | Loss: 0.00000834
Iteration 11/1000 | Loss: 0.00000833
Iteration 12/1000 | Loss: 0.00000833
Iteration 13/1000 | Loss: 0.00000832
Iteration 14/1000 | Loss: 0.00000832
Iteration 15/1000 | Loss: 0.00000830
Iteration 16/1000 | Loss: 0.00000829
Iteration 17/1000 | Loss: 0.00000829
Iteration 18/1000 | Loss: 0.00000827
Iteration 19/1000 | Loss: 0.00000826
Iteration 20/1000 | Loss: 0.00000826
Iteration 21/1000 | Loss: 0.00000825
Iteration 22/1000 | Loss: 0.00000825
Iteration 23/1000 | Loss: 0.00000822
Iteration 24/1000 | Loss: 0.00000821
Iteration 25/1000 | Loss: 0.00000821
Iteration 26/1000 | Loss: 0.00000820
Iteration 27/1000 | Loss: 0.00000818
Iteration 28/1000 | Loss: 0.00000817
Iteration 29/1000 | Loss: 0.00000816
Iteration 30/1000 | Loss: 0.00000815
Iteration 31/1000 | Loss: 0.00000815
Iteration 32/1000 | Loss: 0.00000815
Iteration 33/1000 | Loss: 0.00000812
Iteration 34/1000 | Loss: 0.00000811
Iteration 35/1000 | Loss: 0.00000810
Iteration 36/1000 | Loss: 0.00000810
Iteration 37/1000 | Loss: 0.00000810
Iteration 38/1000 | Loss: 0.00000810
Iteration 39/1000 | Loss: 0.00000809
Iteration 40/1000 | Loss: 0.00000809
Iteration 41/1000 | Loss: 0.00000808
Iteration 42/1000 | Loss: 0.00000808
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000807
Iteration 45/1000 | Loss: 0.00000807
Iteration 46/1000 | Loss: 0.00000807
Iteration 47/1000 | Loss: 0.00000806
Iteration 48/1000 | Loss: 0.00000806
Iteration 49/1000 | Loss: 0.00000806
Iteration 50/1000 | Loss: 0.00000806
Iteration 51/1000 | Loss: 0.00000805
Iteration 52/1000 | Loss: 0.00000804
Iteration 53/1000 | Loss: 0.00000804
Iteration 54/1000 | Loss: 0.00000804
Iteration 55/1000 | Loss: 0.00000803
Iteration 56/1000 | Loss: 0.00000803
Iteration 57/1000 | Loss: 0.00000803
Iteration 58/1000 | Loss: 0.00000803
Iteration 59/1000 | Loss: 0.00000802
Iteration 60/1000 | Loss: 0.00000802
Iteration 61/1000 | Loss: 0.00000802
Iteration 62/1000 | Loss: 0.00000802
Iteration 63/1000 | Loss: 0.00000801
Iteration 64/1000 | Loss: 0.00000801
Iteration 65/1000 | Loss: 0.00000801
Iteration 66/1000 | Loss: 0.00000801
Iteration 67/1000 | Loss: 0.00000801
Iteration 68/1000 | Loss: 0.00000801
Iteration 69/1000 | Loss: 0.00000801
Iteration 70/1000 | Loss: 0.00000801
Iteration 71/1000 | Loss: 0.00000800
Iteration 72/1000 | Loss: 0.00000800
Iteration 73/1000 | Loss: 0.00000800
Iteration 74/1000 | Loss: 0.00000800
Iteration 75/1000 | Loss: 0.00000800
Iteration 76/1000 | Loss: 0.00000800
Iteration 77/1000 | Loss: 0.00000800
Iteration 78/1000 | Loss: 0.00000800
Iteration 79/1000 | Loss: 0.00000800
Iteration 80/1000 | Loss: 0.00000800
Iteration 81/1000 | Loss: 0.00000799
Iteration 82/1000 | Loss: 0.00000799
Iteration 83/1000 | Loss: 0.00000799
Iteration 84/1000 | Loss: 0.00000799
Iteration 85/1000 | Loss: 0.00000799
Iteration 86/1000 | Loss: 0.00000799
Iteration 87/1000 | Loss: 0.00000799
Iteration 88/1000 | Loss: 0.00000799
Iteration 89/1000 | Loss: 0.00000799
Iteration 90/1000 | Loss: 0.00000799
Iteration 91/1000 | Loss: 0.00000799
Iteration 92/1000 | Loss: 0.00000799
Iteration 93/1000 | Loss: 0.00000799
Iteration 94/1000 | Loss: 0.00000799
Iteration 95/1000 | Loss: 0.00000799
Iteration 96/1000 | Loss: 0.00000799
Iteration 97/1000 | Loss: 0.00000799
Iteration 98/1000 | Loss: 0.00000799
Iteration 99/1000 | Loss: 0.00000799
Iteration 100/1000 | Loss: 0.00000799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [7.991595339262858e-06, 7.991595339262858e-06, 7.991595339262858e-06, 7.991595339262858e-06, 7.991595339262858e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.991595339262858e-06

Optimization complete. Final v2v error: 2.4043965339660645 mm

Highest mean error: 2.7100069522857666 mm for frame 124

Lowest mean error: 2.0575339794158936 mm for frame 191

Saving results

Total time: 32.224825859069824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054518
Iteration 2/25 | Loss: 0.00178731
Iteration 3/25 | Loss: 0.00123466
Iteration 4/25 | Loss: 0.00120660
Iteration 5/25 | Loss: 0.00119798
Iteration 6/25 | Loss: 0.00119490
Iteration 7/25 | Loss: 0.00119454
Iteration 8/25 | Loss: 0.00119454
Iteration 9/25 | Loss: 0.00119454
Iteration 10/25 | Loss: 0.00119454
Iteration 11/25 | Loss: 0.00119454
Iteration 12/25 | Loss: 0.00119454
Iteration 13/25 | Loss: 0.00119454
Iteration 14/25 | Loss: 0.00119454
Iteration 15/25 | Loss: 0.00119454
Iteration 16/25 | Loss: 0.00119454
Iteration 17/25 | Loss: 0.00119454
Iteration 18/25 | Loss: 0.00119454
Iteration 19/25 | Loss: 0.00119454
Iteration 20/25 | Loss: 0.00119454
Iteration 21/25 | Loss: 0.00119454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011945361038669944, 0.0011945361038669944, 0.0011945361038669944, 0.0011945361038669944, 0.0011945361038669944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011945361038669944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53891456
Iteration 2/25 | Loss: 0.00074951
Iteration 3/25 | Loss: 0.00074950
Iteration 4/25 | Loss: 0.00074950
Iteration 5/25 | Loss: 0.00074950
Iteration 6/25 | Loss: 0.00074950
Iteration 7/25 | Loss: 0.00074950
Iteration 8/25 | Loss: 0.00074950
Iteration 9/25 | Loss: 0.00074950
Iteration 10/25 | Loss: 0.00074950
Iteration 11/25 | Loss: 0.00074950
Iteration 12/25 | Loss: 0.00074950
Iteration 13/25 | Loss: 0.00074950
Iteration 14/25 | Loss: 0.00074950
Iteration 15/25 | Loss: 0.00074950
Iteration 16/25 | Loss: 0.00074950
Iteration 17/25 | Loss: 0.00074950
Iteration 18/25 | Loss: 0.00074950
Iteration 19/25 | Loss: 0.00074950
Iteration 20/25 | Loss: 0.00074950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007494993624277413, 0.0007494993624277413, 0.0007494993624277413, 0.0007494993624277413, 0.0007494993624277413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007494993624277413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074950
Iteration 2/1000 | Loss: 0.00007780
Iteration 3/1000 | Loss: 0.00005620
Iteration 4/1000 | Loss: 0.00005004
Iteration 5/1000 | Loss: 0.00004782
Iteration 6/1000 | Loss: 0.00004609
Iteration 7/1000 | Loss: 0.00004512
Iteration 8/1000 | Loss: 0.00004434
Iteration 9/1000 | Loss: 0.00004371
Iteration 10/1000 | Loss: 0.00004327
Iteration 11/1000 | Loss: 0.00004288
Iteration 12/1000 | Loss: 0.00004253
Iteration 13/1000 | Loss: 0.00004213
Iteration 14/1000 | Loss: 0.00004182
Iteration 15/1000 | Loss: 0.00004154
Iteration 16/1000 | Loss: 0.00004125
Iteration 17/1000 | Loss: 0.00004102
Iteration 18/1000 | Loss: 0.00004079
Iteration 19/1000 | Loss: 0.00004061
Iteration 20/1000 | Loss: 0.00004043
Iteration 21/1000 | Loss: 0.00004031
Iteration 22/1000 | Loss: 0.00004030
Iteration 23/1000 | Loss: 0.00004020
Iteration 24/1000 | Loss: 0.00004005
Iteration 25/1000 | Loss: 0.00004001
Iteration 26/1000 | Loss: 0.00003996
Iteration 27/1000 | Loss: 0.00003996
Iteration 28/1000 | Loss: 0.00003994
Iteration 29/1000 | Loss: 0.00003994
Iteration 30/1000 | Loss: 0.00003993
Iteration 31/1000 | Loss: 0.00003992
Iteration 32/1000 | Loss: 0.00003992
Iteration 33/1000 | Loss: 0.00003991
Iteration 34/1000 | Loss: 0.00003990
Iteration 35/1000 | Loss: 0.00003990
Iteration 36/1000 | Loss: 0.00003990
Iteration 37/1000 | Loss: 0.00003990
Iteration 38/1000 | Loss: 0.00003990
Iteration 39/1000 | Loss: 0.00003990
Iteration 40/1000 | Loss: 0.00003990
Iteration 41/1000 | Loss: 0.00003990
Iteration 42/1000 | Loss: 0.00003990
Iteration 43/1000 | Loss: 0.00003990
Iteration 44/1000 | Loss: 0.00003989
Iteration 45/1000 | Loss: 0.00003989
Iteration 46/1000 | Loss: 0.00003989
Iteration 47/1000 | Loss: 0.00003989
Iteration 48/1000 | Loss: 0.00003988
Iteration 49/1000 | Loss: 0.00003988
Iteration 50/1000 | Loss: 0.00003987
Iteration 51/1000 | Loss: 0.00003987
Iteration 52/1000 | Loss: 0.00003987
Iteration 53/1000 | Loss: 0.00003987
Iteration 54/1000 | Loss: 0.00003987
Iteration 55/1000 | Loss: 0.00003987
Iteration 56/1000 | Loss: 0.00003987
Iteration 57/1000 | Loss: 0.00003987
Iteration 58/1000 | Loss: 0.00003987
Iteration 59/1000 | Loss: 0.00003987
Iteration 60/1000 | Loss: 0.00003987
Iteration 61/1000 | Loss: 0.00003986
Iteration 62/1000 | Loss: 0.00003986
Iteration 63/1000 | Loss: 0.00003986
Iteration 64/1000 | Loss: 0.00003985
Iteration 65/1000 | Loss: 0.00003985
Iteration 66/1000 | Loss: 0.00003985
Iteration 67/1000 | Loss: 0.00003985
Iteration 68/1000 | Loss: 0.00003985
Iteration 69/1000 | Loss: 0.00003985
Iteration 70/1000 | Loss: 0.00003984
Iteration 71/1000 | Loss: 0.00003984
Iteration 72/1000 | Loss: 0.00003984
Iteration 73/1000 | Loss: 0.00003984
Iteration 74/1000 | Loss: 0.00003984
Iteration 75/1000 | Loss: 0.00003984
Iteration 76/1000 | Loss: 0.00003984
Iteration 77/1000 | Loss: 0.00003984
Iteration 78/1000 | Loss: 0.00003984
Iteration 79/1000 | Loss: 0.00003983
Iteration 80/1000 | Loss: 0.00003983
Iteration 81/1000 | Loss: 0.00003983
Iteration 82/1000 | Loss: 0.00003983
Iteration 83/1000 | Loss: 0.00003982
Iteration 84/1000 | Loss: 0.00003982
Iteration 85/1000 | Loss: 0.00003982
Iteration 86/1000 | Loss: 0.00003981
Iteration 87/1000 | Loss: 0.00003981
Iteration 88/1000 | Loss: 0.00003981
Iteration 89/1000 | Loss: 0.00003981
Iteration 90/1000 | Loss: 0.00003980
Iteration 91/1000 | Loss: 0.00003980
Iteration 92/1000 | Loss: 0.00003980
Iteration 93/1000 | Loss: 0.00003980
Iteration 94/1000 | Loss: 0.00003980
Iteration 95/1000 | Loss: 0.00003980
Iteration 96/1000 | Loss: 0.00003980
Iteration 97/1000 | Loss: 0.00003980
Iteration 98/1000 | Loss: 0.00003979
Iteration 99/1000 | Loss: 0.00003979
Iteration 100/1000 | Loss: 0.00003979
Iteration 101/1000 | Loss: 0.00003979
Iteration 102/1000 | Loss: 0.00003979
Iteration 103/1000 | Loss: 0.00003979
Iteration 104/1000 | Loss: 0.00003979
Iteration 105/1000 | Loss: 0.00003979
Iteration 106/1000 | Loss: 0.00003978
Iteration 107/1000 | Loss: 0.00003978
Iteration 108/1000 | Loss: 0.00003978
Iteration 109/1000 | Loss: 0.00003978
Iteration 110/1000 | Loss: 0.00003977
Iteration 111/1000 | Loss: 0.00003977
Iteration 112/1000 | Loss: 0.00003977
Iteration 113/1000 | Loss: 0.00003977
Iteration 114/1000 | Loss: 0.00003977
Iteration 115/1000 | Loss: 0.00003976
Iteration 116/1000 | Loss: 0.00003976
Iteration 117/1000 | Loss: 0.00003976
Iteration 118/1000 | Loss: 0.00003976
Iteration 119/1000 | Loss: 0.00003976
Iteration 120/1000 | Loss: 0.00003976
Iteration 121/1000 | Loss: 0.00003975
Iteration 122/1000 | Loss: 0.00003975
Iteration 123/1000 | Loss: 0.00003975
Iteration 124/1000 | Loss: 0.00003975
Iteration 125/1000 | Loss: 0.00003975
Iteration 126/1000 | Loss: 0.00003975
Iteration 127/1000 | Loss: 0.00003975
Iteration 128/1000 | Loss: 0.00003975
Iteration 129/1000 | Loss: 0.00003975
Iteration 130/1000 | Loss: 0.00003974
Iteration 131/1000 | Loss: 0.00003974
Iteration 132/1000 | Loss: 0.00003974
Iteration 133/1000 | Loss: 0.00003974
Iteration 134/1000 | Loss: 0.00003974
Iteration 135/1000 | Loss: 0.00003974
Iteration 136/1000 | Loss: 0.00003974
Iteration 137/1000 | Loss: 0.00003974
Iteration 138/1000 | Loss: 0.00003974
Iteration 139/1000 | Loss: 0.00003974
Iteration 140/1000 | Loss: 0.00003973
Iteration 141/1000 | Loss: 0.00003973
Iteration 142/1000 | Loss: 0.00003973
Iteration 143/1000 | Loss: 0.00003973
Iteration 144/1000 | Loss: 0.00003973
Iteration 145/1000 | Loss: 0.00003973
Iteration 146/1000 | Loss: 0.00003973
Iteration 147/1000 | Loss: 0.00003973
Iteration 148/1000 | Loss: 0.00003973
Iteration 149/1000 | Loss: 0.00003973
Iteration 150/1000 | Loss: 0.00003973
Iteration 151/1000 | Loss: 0.00003973
Iteration 152/1000 | Loss: 0.00003973
Iteration 153/1000 | Loss: 0.00003973
Iteration 154/1000 | Loss: 0.00003973
Iteration 155/1000 | Loss: 0.00003973
Iteration 156/1000 | Loss: 0.00003973
Iteration 157/1000 | Loss: 0.00003973
Iteration 158/1000 | Loss: 0.00003973
Iteration 159/1000 | Loss: 0.00003972
Iteration 160/1000 | Loss: 0.00003972
Iteration 161/1000 | Loss: 0.00003972
Iteration 162/1000 | Loss: 0.00003972
Iteration 163/1000 | Loss: 0.00003972
Iteration 164/1000 | Loss: 0.00003972
Iteration 165/1000 | Loss: 0.00003972
Iteration 166/1000 | Loss: 0.00003972
Iteration 167/1000 | Loss: 0.00003972
Iteration 168/1000 | Loss: 0.00003971
Iteration 169/1000 | Loss: 0.00003971
Iteration 170/1000 | Loss: 0.00003971
Iteration 171/1000 | Loss: 0.00003971
Iteration 172/1000 | Loss: 0.00003971
Iteration 173/1000 | Loss: 0.00003971
Iteration 174/1000 | Loss: 0.00003971
Iteration 175/1000 | Loss: 0.00003971
Iteration 176/1000 | Loss: 0.00003971
Iteration 177/1000 | Loss: 0.00003971
Iteration 178/1000 | Loss: 0.00003970
Iteration 179/1000 | Loss: 0.00003970
Iteration 180/1000 | Loss: 0.00003970
Iteration 181/1000 | Loss: 0.00003970
Iteration 182/1000 | Loss: 0.00003970
Iteration 183/1000 | Loss: 0.00003970
Iteration 184/1000 | Loss: 0.00003970
Iteration 185/1000 | Loss: 0.00003970
Iteration 186/1000 | Loss: 0.00003970
Iteration 187/1000 | Loss: 0.00003970
Iteration 188/1000 | Loss: 0.00003970
Iteration 189/1000 | Loss: 0.00003970
Iteration 190/1000 | Loss: 0.00003970
Iteration 191/1000 | Loss: 0.00003970
Iteration 192/1000 | Loss: 0.00003970
Iteration 193/1000 | Loss: 0.00003970
Iteration 194/1000 | Loss: 0.00003969
Iteration 195/1000 | Loss: 0.00003969
Iteration 196/1000 | Loss: 0.00003969
Iteration 197/1000 | Loss: 0.00003969
Iteration 198/1000 | Loss: 0.00003969
Iteration 199/1000 | Loss: 0.00003969
Iteration 200/1000 | Loss: 0.00003969
Iteration 201/1000 | Loss: 0.00003969
Iteration 202/1000 | Loss: 0.00003969
Iteration 203/1000 | Loss: 0.00003969
Iteration 204/1000 | Loss: 0.00003969
Iteration 205/1000 | Loss: 0.00003969
Iteration 206/1000 | Loss: 0.00003969
Iteration 207/1000 | Loss: 0.00003969
Iteration 208/1000 | Loss: 0.00003969
Iteration 209/1000 | Loss: 0.00003969
Iteration 210/1000 | Loss: 0.00003969
Iteration 211/1000 | Loss: 0.00003969
Iteration 212/1000 | Loss: 0.00003969
Iteration 213/1000 | Loss: 0.00003969
Iteration 214/1000 | Loss: 0.00003969
Iteration 215/1000 | Loss: 0.00003969
Iteration 216/1000 | Loss: 0.00003969
Iteration 217/1000 | Loss: 0.00003969
Iteration 218/1000 | Loss: 0.00003969
Iteration 219/1000 | Loss: 0.00003969
Iteration 220/1000 | Loss: 0.00003969
Iteration 221/1000 | Loss: 0.00003969
Iteration 222/1000 | Loss: 0.00003969
Iteration 223/1000 | Loss: 0.00003969
Iteration 224/1000 | Loss: 0.00003969
Iteration 225/1000 | Loss: 0.00003969
Iteration 226/1000 | Loss: 0.00003969
Iteration 227/1000 | Loss: 0.00003969
Iteration 228/1000 | Loss: 0.00003969
Iteration 229/1000 | Loss: 0.00003969
Iteration 230/1000 | Loss: 0.00003969
Iteration 231/1000 | Loss: 0.00003969
Iteration 232/1000 | Loss: 0.00003969
Iteration 233/1000 | Loss: 0.00003969
Iteration 234/1000 | Loss: 0.00003969
Iteration 235/1000 | Loss: 0.00003969
Iteration 236/1000 | Loss: 0.00003969
Iteration 237/1000 | Loss: 0.00003969
Iteration 238/1000 | Loss: 0.00003969
Iteration 239/1000 | Loss: 0.00003969
Iteration 240/1000 | Loss: 0.00003969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [3.9685739466222e-05, 3.9685739466222e-05, 3.9685739466222e-05, 3.9685739466222e-05, 3.9685739466222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9685739466222e-05

Optimization complete. Final v2v error: 4.929874897003174 mm

Highest mean error: 6.056331634521484 mm for frame 40

Lowest mean error: 4.273402214050293 mm for frame 60

Saving results

Total time: 57.775126457214355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714307
Iteration 2/25 | Loss: 0.00114068
Iteration 3/25 | Loss: 0.00099999
Iteration 4/25 | Loss: 0.00095130
Iteration 5/25 | Loss: 0.00093944
Iteration 6/25 | Loss: 0.00093541
Iteration 7/25 | Loss: 0.00093720
Iteration 8/25 | Loss: 0.00093579
Iteration 9/25 | Loss: 0.00093282
Iteration 10/25 | Loss: 0.00093183
Iteration 11/25 | Loss: 0.00093127
Iteration 12/25 | Loss: 0.00093081
Iteration 13/25 | Loss: 0.00093058
Iteration 14/25 | Loss: 0.00093053
Iteration 15/25 | Loss: 0.00093053
Iteration 16/25 | Loss: 0.00093053
Iteration 17/25 | Loss: 0.00093053
Iteration 18/25 | Loss: 0.00093053
Iteration 19/25 | Loss: 0.00093053
Iteration 20/25 | Loss: 0.00093052
Iteration 21/25 | Loss: 0.00093052
Iteration 22/25 | Loss: 0.00093052
Iteration 23/25 | Loss: 0.00093052
Iteration 24/25 | Loss: 0.00093052
Iteration 25/25 | Loss: 0.00093052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45100307
Iteration 2/25 | Loss: 0.00064681
Iteration 3/25 | Loss: 0.00064681
Iteration 4/25 | Loss: 0.00064681
Iteration 5/25 | Loss: 0.00064681
Iteration 6/25 | Loss: 0.00064681
Iteration 7/25 | Loss: 0.00064681
Iteration 8/25 | Loss: 0.00064681
Iteration 9/25 | Loss: 0.00064681
Iteration 10/25 | Loss: 0.00064681
Iteration 11/25 | Loss: 0.00064681
Iteration 12/25 | Loss: 0.00064681
Iteration 13/25 | Loss: 0.00064681
Iteration 14/25 | Loss: 0.00064681
Iteration 15/25 | Loss: 0.00064681
Iteration 16/25 | Loss: 0.00064681
Iteration 17/25 | Loss: 0.00064681
Iteration 18/25 | Loss: 0.00064681
Iteration 19/25 | Loss: 0.00064681
Iteration 20/25 | Loss: 0.00064681
Iteration 21/25 | Loss: 0.00064681
Iteration 22/25 | Loss: 0.00064681
Iteration 23/25 | Loss: 0.00064681
Iteration 24/25 | Loss: 0.00064681
Iteration 25/25 | Loss: 0.00064681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064681
Iteration 2/1000 | Loss: 0.00001695
Iteration 3/1000 | Loss: 0.00001319
Iteration 4/1000 | Loss: 0.00001219
Iteration 5/1000 | Loss: 0.00001171
Iteration 6/1000 | Loss: 0.00001149
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001112
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001105
Iteration 12/1000 | Loss: 0.00001104
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001101
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001101
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001092
Iteration 46/1000 | Loss: 0.00001092
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001089
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001088
Iteration 74/1000 | Loss: 0.00001088
Iteration 75/1000 | Loss: 0.00001088
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001087
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001087
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001086
Iteration 85/1000 | Loss: 0.00001085
Iteration 86/1000 | Loss: 0.00001085
Iteration 87/1000 | Loss: 0.00001085
Iteration 88/1000 | Loss: 0.00001084
Iteration 89/1000 | Loss: 0.00001084
Iteration 90/1000 | Loss: 0.00001084
Iteration 91/1000 | Loss: 0.00001084
Iteration 92/1000 | Loss: 0.00001084
Iteration 93/1000 | Loss: 0.00001084
Iteration 94/1000 | Loss: 0.00001083
Iteration 95/1000 | Loss: 0.00001083
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001081
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001080
Iteration 105/1000 | Loss: 0.00001080
Iteration 106/1000 | Loss: 0.00001080
Iteration 107/1000 | Loss: 0.00001080
Iteration 108/1000 | Loss: 0.00001080
Iteration 109/1000 | Loss: 0.00001080
Iteration 110/1000 | Loss: 0.00001080
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001079
Iteration 113/1000 | Loss: 0.00001079
Iteration 114/1000 | Loss: 0.00001079
Iteration 115/1000 | Loss: 0.00001079
Iteration 116/1000 | Loss: 0.00001079
Iteration 117/1000 | Loss: 0.00001079
Iteration 118/1000 | Loss: 0.00001079
Iteration 119/1000 | Loss: 0.00001079
Iteration 120/1000 | Loss: 0.00001078
Iteration 121/1000 | Loss: 0.00001078
Iteration 122/1000 | Loss: 0.00001078
Iteration 123/1000 | Loss: 0.00001078
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001078
Iteration 126/1000 | Loss: 0.00001078
Iteration 127/1000 | Loss: 0.00001078
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001078
Iteration 139/1000 | Loss: 0.00001078
Iteration 140/1000 | Loss: 0.00001078
Iteration 141/1000 | Loss: 0.00001078
Iteration 142/1000 | Loss: 0.00001078
Iteration 143/1000 | Loss: 0.00001078
Iteration 144/1000 | Loss: 0.00001078
Iteration 145/1000 | Loss: 0.00001078
Iteration 146/1000 | Loss: 0.00001078
Iteration 147/1000 | Loss: 0.00001078
Iteration 148/1000 | Loss: 0.00001078
Iteration 149/1000 | Loss: 0.00001078
Iteration 150/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.07770511021954e-05, 1.07770511021954e-05, 1.07770511021954e-05, 1.07770511021954e-05, 1.07770511021954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.07770511021954e-05

Optimization complete. Final v2v error: 2.7573554515838623 mm

Highest mean error: 3.2226412296295166 mm for frame 72

Lowest mean error: 2.4586586952209473 mm for frame 229

Saving results

Total time: 50.5995671749115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833495
Iteration 2/25 | Loss: 0.00109643
Iteration 3/25 | Loss: 0.00095457
Iteration 4/25 | Loss: 0.00094459
Iteration 5/25 | Loss: 0.00094242
Iteration 6/25 | Loss: 0.00094175
Iteration 7/25 | Loss: 0.00094175
Iteration 8/25 | Loss: 0.00094175
Iteration 9/25 | Loss: 0.00094175
Iteration 10/25 | Loss: 0.00094175
Iteration 11/25 | Loss: 0.00094175
Iteration 12/25 | Loss: 0.00094175
Iteration 13/25 | Loss: 0.00094175
Iteration 14/25 | Loss: 0.00094175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009417546098120511, 0.0009417546098120511, 0.0009417546098120511, 0.0009417546098120511, 0.0009417546098120511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009417546098120511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33153379
Iteration 2/25 | Loss: 0.00067907
Iteration 3/25 | Loss: 0.00067907
Iteration 4/25 | Loss: 0.00067907
Iteration 5/25 | Loss: 0.00067907
Iteration 6/25 | Loss: 0.00067907
Iteration 7/25 | Loss: 0.00067906
Iteration 8/25 | Loss: 0.00067906
Iteration 9/25 | Loss: 0.00067906
Iteration 10/25 | Loss: 0.00067906
Iteration 11/25 | Loss: 0.00067906
Iteration 12/25 | Loss: 0.00067906
Iteration 13/25 | Loss: 0.00067906
Iteration 14/25 | Loss: 0.00067906
Iteration 15/25 | Loss: 0.00067906
Iteration 16/25 | Loss: 0.00067906
Iteration 17/25 | Loss: 0.00067906
Iteration 18/25 | Loss: 0.00067906
Iteration 19/25 | Loss: 0.00067906
Iteration 20/25 | Loss: 0.00067906
Iteration 21/25 | Loss: 0.00067906
Iteration 22/25 | Loss: 0.00067906
Iteration 23/25 | Loss: 0.00067906
Iteration 24/25 | Loss: 0.00067906
Iteration 25/25 | Loss: 0.00067906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067906
Iteration 2/1000 | Loss: 0.00002786
Iteration 3/1000 | Loss: 0.00001552
Iteration 4/1000 | Loss: 0.00001072
Iteration 5/1000 | Loss: 0.00001003
Iteration 6/1000 | Loss: 0.00000946
Iteration 7/1000 | Loss: 0.00000916
Iteration 8/1000 | Loss: 0.00000890
Iteration 9/1000 | Loss: 0.00000883
Iteration 10/1000 | Loss: 0.00000875
Iteration 11/1000 | Loss: 0.00000872
Iteration 12/1000 | Loss: 0.00000871
Iteration 13/1000 | Loss: 0.00000869
Iteration 14/1000 | Loss: 0.00000864
Iteration 15/1000 | Loss: 0.00000861
Iteration 16/1000 | Loss: 0.00000859
Iteration 17/1000 | Loss: 0.00000856
Iteration 18/1000 | Loss: 0.00000855
Iteration 19/1000 | Loss: 0.00000855
Iteration 20/1000 | Loss: 0.00000855
Iteration 21/1000 | Loss: 0.00000855
Iteration 22/1000 | Loss: 0.00000853
Iteration 23/1000 | Loss: 0.00000852
Iteration 24/1000 | Loss: 0.00000852
Iteration 25/1000 | Loss: 0.00000852
Iteration 26/1000 | Loss: 0.00000851
Iteration 27/1000 | Loss: 0.00000851
Iteration 28/1000 | Loss: 0.00000851
Iteration 29/1000 | Loss: 0.00000850
Iteration 30/1000 | Loss: 0.00000850
Iteration 31/1000 | Loss: 0.00000849
Iteration 32/1000 | Loss: 0.00000848
Iteration 33/1000 | Loss: 0.00000848
Iteration 34/1000 | Loss: 0.00000848
Iteration 35/1000 | Loss: 0.00000847
Iteration 36/1000 | Loss: 0.00000847
Iteration 37/1000 | Loss: 0.00000847
Iteration 38/1000 | Loss: 0.00000846
Iteration 39/1000 | Loss: 0.00000846
Iteration 40/1000 | Loss: 0.00000846
Iteration 41/1000 | Loss: 0.00000846
Iteration 42/1000 | Loss: 0.00000845
Iteration 43/1000 | Loss: 0.00000845
Iteration 44/1000 | Loss: 0.00000845
Iteration 45/1000 | Loss: 0.00000844
Iteration 46/1000 | Loss: 0.00000844
Iteration 47/1000 | Loss: 0.00000844
Iteration 48/1000 | Loss: 0.00000844
Iteration 49/1000 | Loss: 0.00000843
Iteration 50/1000 | Loss: 0.00000843
Iteration 51/1000 | Loss: 0.00000843
Iteration 52/1000 | Loss: 0.00000842
Iteration 53/1000 | Loss: 0.00000842
Iteration 54/1000 | Loss: 0.00000842
Iteration 55/1000 | Loss: 0.00000842
Iteration 56/1000 | Loss: 0.00000842
Iteration 57/1000 | Loss: 0.00000841
Iteration 58/1000 | Loss: 0.00000841
Iteration 59/1000 | Loss: 0.00000840
Iteration 60/1000 | Loss: 0.00000840
Iteration 61/1000 | Loss: 0.00000839
Iteration 62/1000 | Loss: 0.00000839
Iteration 63/1000 | Loss: 0.00000839
Iteration 64/1000 | Loss: 0.00000839
Iteration 65/1000 | Loss: 0.00000838
Iteration 66/1000 | Loss: 0.00000838
Iteration 67/1000 | Loss: 0.00000838
Iteration 68/1000 | Loss: 0.00000838
Iteration 69/1000 | Loss: 0.00000838
Iteration 70/1000 | Loss: 0.00000838
Iteration 71/1000 | Loss: 0.00000838
Iteration 72/1000 | Loss: 0.00000838
Iteration 73/1000 | Loss: 0.00000837
Iteration 74/1000 | Loss: 0.00000837
Iteration 75/1000 | Loss: 0.00000837
Iteration 76/1000 | Loss: 0.00000837
Iteration 77/1000 | Loss: 0.00000837
Iteration 78/1000 | Loss: 0.00000836
Iteration 79/1000 | Loss: 0.00000836
Iteration 80/1000 | Loss: 0.00000836
Iteration 81/1000 | Loss: 0.00000836
Iteration 82/1000 | Loss: 0.00000836
Iteration 83/1000 | Loss: 0.00000835
Iteration 84/1000 | Loss: 0.00000835
Iteration 85/1000 | Loss: 0.00000835
Iteration 86/1000 | Loss: 0.00000835
Iteration 87/1000 | Loss: 0.00000835
Iteration 88/1000 | Loss: 0.00000835
Iteration 89/1000 | Loss: 0.00000834
Iteration 90/1000 | Loss: 0.00000834
Iteration 91/1000 | Loss: 0.00000834
Iteration 92/1000 | Loss: 0.00000834
Iteration 93/1000 | Loss: 0.00000833
Iteration 94/1000 | Loss: 0.00000833
Iteration 95/1000 | Loss: 0.00000833
Iteration 96/1000 | Loss: 0.00000832
Iteration 97/1000 | Loss: 0.00000832
Iteration 98/1000 | Loss: 0.00000832
Iteration 99/1000 | Loss: 0.00000832
Iteration 100/1000 | Loss: 0.00000832
Iteration 101/1000 | Loss: 0.00000832
Iteration 102/1000 | Loss: 0.00000832
Iteration 103/1000 | Loss: 0.00000832
Iteration 104/1000 | Loss: 0.00000831
Iteration 105/1000 | Loss: 0.00000831
Iteration 106/1000 | Loss: 0.00000831
Iteration 107/1000 | Loss: 0.00000831
Iteration 108/1000 | Loss: 0.00000831
Iteration 109/1000 | Loss: 0.00000831
Iteration 110/1000 | Loss: 0.00000830
Iteration 111/1000 | Loss: 0.00000830
Iteration 112/1000 | Loss: 0.00000830
Iteration 113/1000 | Loss: 0.00000830
Iteration 114/1000 | Loss: 0.00000830
Iteration 115/1000 | Loss: 0.00000830
Iteration 116/1000 | Loss: 0.00000830
Iteration 117/1000 | Loss: 0.00000829
Iteration 118/1000 | Loss: 0.00000829
Iteration 119/1000 | Loss: 0.00000829
Iteration 120/1000 | Loss: 0.00000829
Iteration 121/1000 | Loss: 0.00000829
Iteration 122/1000 | Loss: 0.00000829
Iteration 123/1000 | Loss: 0.00000829
Iteration 124/1000 | Loss: 0.00000829
Iteration 125/1000 | Loss: 0.00000829
Iteration 126/1000 | Loss: 0.00000829
Iteration 127/1000 | Loss: 0.00000829
Iteration 128/1000 | Loss: 0.00000829
Iteration 129/1000 | Loss: 0.00000829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [8.29340388008859e-06, 8.29340388008859e-06, 8.29340388008859e-06, 8.29340388008859e-06, 8.29340388008859e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.29340388008859e-06

Optimization complete. Final v2v error: 2.4549620151519775 mm

Highest mean error: 2.985004186630249 mm for frame 100

Lowest mean error: 2.2032904624938965 mm for frame 146

Saving results

Total time: 35.747637033462524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357649
Iteration 2/25 | Loss: 0.00115017
Iteration 3/25 | Loss: 0.00102289
Iteration 4/25 | Loss: 0.00100233
Iteration 5/25 | Loss: 0.00099435
Iteration 6/25 | Loss: 0.00099240
Iteration 7/25 | Loss: 0.00099240
Iteration 8/25 | Loss: 0.00099240
Iteration 9/25 | Loss: 0.00099240
Iteration 10/25 | Loss: 0.00099240
Iteration 11/25 | Loss: 0.00099240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000992399756796658, 0.000992399756796658, 0.000992399756796658, 0.000992399756796658, 0.000992399756796658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000992399756796658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28114057
Iteration 2/25 | Loss: 0.00086162
Iteration 3/25 | Loss: 0.00086162
Iteration 4/25 | Loss: 0.00086162
Iteration 5/25 | Loss: 0.00086162
Iteration 6/25 | Loss: 0.00086162
Iteration 7/25 | Loss: 0.00086162
Iteration 8/25 | Loss: 0.00086162
Iteration 9/25 | Loss: 0.00086162
Iteration 10/25 | Loss: 0.00086162
Iteration 11/25 | Loss: 0.00086162
Iteration 12/25 | Loss: 0.00086162
Iteration 13/25 | Loss: 0.00086162
Iteration 14/25 | Loss: 0.00086162
Iteration 15/25 | Loss: 0.00086162
Iteration 16/25 | Loss: 0.00086162
Iteration 17/25 | Loss: 0.00086162
Iteration 18/25 | Loss: 0.00086162
Iteration 19/25 | Loss: 0.00086162
Iteration 20/25 | Loss: 0.00086162
Iteration 21/25 | Loss: 0.00086162
Iteration 22/25 | Loss: 0.00086162
Iteration 23/25 | Loss: 0.00086162
Iteration 24/25 | Loss: 0.00086162
Iteration 25/25 | Loss: 0.00086162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086162
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002229
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001621
Iteration 6/1000 | Loss: 0.00001531
Iteration 7/1000 | Loss: 0.00001479
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001428
Iteration 10/1000 | Loss: 0.00001408
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001371
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001360
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001360
Iteration 94/1000 | Loss: 0.00001360
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001358
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001357
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001357
Iteration 129/1000 | Loss: 0.00001357
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001357
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001356
Iteration 135/1000 | Loss: 0.00001356
Iteration 136/1000 | Loss: 0.00001356
Iteration 137/1000 | Loss: 0.00001356
Iteration 138/1000 | Loss: 0.00001356
Iteration 139/1000 | Loss: 0.00001356
Iteration 140/1000 | Loss: 0.00001356
Iteration 141/1000 | Loss: 0.00001356
Iteration 142/1000 | Loss: 0.00001355
Iteration 143/1000 | Loss: 0.00001355
Iteration 144/1000 | Loss: 0.00001355
Iteration 145/1000 | Loss: 0.00001355
Iteration 146/1000 | Loss: 0.00001355
Iteration 147/1000 | Loss: 0.00001355
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001355
Iteration 150/1000 | Loss: 0.00001355
Iteration 151/1000 | Loss: 0.00001355
Iteration 152/1000 | Loss: 0.00001355
Iteration 153/1000 | Loss: 0.00001354
Iteration 154/1000 | Loss: 0.00001354
Iteration 155/1000 | Loss: 0.00001354
Iteration 156/1000 | Loss: 0.00001354
Iteration 157/1000 | Loss: 0.00001354
Iteration 158/1000 | Loss: 0.00001354
Iteration 159/1000 | Loss: 0.00001354
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001354
Iteration 163/1000 | Loss: 0.00001354
Iteration 164/1000 | Loss: 0.00001353
Iteration 165/1000 | Loss: 0.00001353
Iteration 166/1000 | Loss: 0.00001353
Iteration 167/1000 | Loss: 0.00001353
Iteration 168/1000 | Loss: 0.00001353
Iteration 169/1000 | Loss: 0.00001353
Iteration 170/1000 | Loss: 0.00001353
Iteration 171/1000 | Loss: 0.00001353
Iteration 172/1000 | Loss: 0.00001353
Iteration 173/1000 | Loss: 0.00001353
Iteration 174/1000 | Loss: 0.00001353
Iteration 175/1000 | Loss: 0.00001353
Iteration 176/1000 | Loss: 0.00001353
Iteration 177/1000 | Loss: 0.00001353
Iteration 178/1000 | Loss: 0.00001353
Iteration 179/1000 | Loss: 0.00001352
Iteration 180/1000 | Loss: 0.00001352
Iteration 181/1000 | Loss: 0.00001352
Iteration 182/1000 | Loss: 0.00001352
Iteration 183/1000 | Loss: 0.00001352
Iteration 184/1000 | Loss: 0.00001352
Iteration 185/1000 | Loss: 0.00001352
Iteration 186/1000 | Loss: 0.00001352
Iteration 187/1000 | Loss: 0.00001352
Iteration 188/1000 | Loss: 0.00001352
Iteration 189/1000 | Loss: 0.00001352
Iteration 190/1000 | Loss: 0.00001352
Iteration 191/1000 | Loss: 0.00001352
Iteration 192/1000 | Loss: 0.00001351
Iteration 193/1000 | Loss: 0.00001351
Iteration 194/1000 | Loss: 0.00001351
Iteration 195/1000 | Loss: 0.00001351
Iteration 196/1000 | Loss: 0.00001351
Iteration 197/1000 | Loss: 0.00001351
Iteration 198/1000 | Loss: 0.00001351
Iteration 199/1000 | Loss: 0.00001351
Iteration 200/1000 | Loss: 0.00001351
Iteration 201/1000 | Loss: 0.00001351
Iteration 202/1000 | Loss: 0.00001351
Iteration 203/1000 | Loss: 0.00001351
Iteration 204/1000 | Loss: 0.00001351
Iteration 205/1000 | Loss: 0.00001350
Iteration 206/1000 | Loss: 0.00001350
Iteration 207/1000 | Loss: 0.00001350
Iteration 208/1000 | Loss: 0.00001350
Iteration 209/1000 | Loss: 0.00001350
Iteration 210/1000 | Loss: 0.00001350
Iteration 211/1000 | Loss: 0.00001350
Iteration 212/1000 | Loss: 0.00001350
Iteration 213/1000 | Loss: 0.00001350
Iteration 214/1000 | Loss: 0.00001350
Iteration 215/1000 | Loss: 0.00001350
Iteration 216/1000 | Loss: 0.00001350
Iteration 217/1000 | Loss: 0.00001350
Iteration 218/1000 | Loss: 0.00001350
Iteration 219/1000 | Loss: 0.00001350
Iteration 220/1000 | Loss: 0.00001350
Iteration 221/1000 | Loss: 0.00001350
Iteration 222/1000 | Loss: 0.00001349
Iteration 223/1000 | Loss: 0.00001349
Iteration 224/1000 | Loss: 0.00001349
Iteration 225/1000 | Loss: 0.00001349
Iteration 226/1000 | Loss: 0.00001349
Iteration 227/1000 | Loss: 0.00001349
Iteration 228/1000 | Loss: 0.00001349
Iteration 229/1000 | Loss: 0.00001349
Iteration 230/1000 | Loss: 0.00001349
Iteration 231/1000 | Loss: 0.00001349
Iteration 232/1000 | Loss: 0.00001349
Iteration 233/1000 | Loss: 0.00001349
Iteration 234/1000 | Loss: 0.00001349
Iteration 235/1000 | Loss: 0.00001349
Iteration 236/1000 | Loss: 0.00001349
Iteration 237/1000 | Loss: 0.00001349
Iteration 238/1000 | Loss: 0.00001349
Iteration 239/1000 | Loss: 0.00001349
Iteration 240/1000 | Loss: 0.00001349
Iteration 241/1000 | Loss: 0.00001349
Iteration 242/1000 | Loss: 0.00001349
Iteration 243/1000 | Loss: 0.00001349
Iteration 244/1000 | Loss: 0.00001349
Iteration 245/1000 | Loss: 0.00001349
Iteration 246/1000 | Loss: 0.00001349
Iteration 247/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.3487368960340973e-05, 1.3487368960340973e-05, 1.3487368960340973e-05, 1.3487368960340973e-05, 1.3487368960340973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3487368960340973e-05

Optimization complete. Final v2v error: 3.052001714706421 mm

Highest mean error: 3.4496936798095703 mm for frame 205

Lowest mean error: 2.6039326190948486 mm for frame 195

Saving results

Total time: 47.586427450180054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013127
Iteration 2/25 | Loss: 0.00433937
Iteration 3/25 | Loss: 0.00288069
Iteration 4/25 | Loss: 0.00256296
Iteration 5/25 | Loss: 0.00230981
Iteration 6/25 | Loss: 0.00223630
Iteration 7/25 | Loss: 0.00214328
Iteration 8/25 | Loss: 0.00200537
Iteration 9/25 | Loss: 0.00185233
Iteration 10/25 | Loss: 0.00174009
Iteration 11/25 | Loss: 0.00167301
Iteration 12/25 | Loss: 0.00163338
Iteration 13/25 | Loss: 0.00157727
Iteration 14/25 | Loss: 0.00157093
Iteration 15/25 | Loss: 0.00157071
Iteration 16/25 | Loss: 0.00154653
Iteration 17/25 | Loss: 0.00153212
Iteration 18/25 | Loss: 0.00153088
Iteration 19/25 | Loss: 0.00151804
Iteration 20/25 | Loss: 0.00151268
Iteration 21/25 | Loss: 0.00150782
Iteration 22/25 | Loss: 0.00148284
Iteration 23/25 | Loss: 0.00146919
Iteration 24/25 | Loss: 0.00145835
Iteration 25/25 | Loss: 0.00146298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40630758
Iteration 2/25 | Loss: 0.00976772
Iteration 3/25 | Loss: 0.00552925
Iteration 4/25 | Loss: 0.00516773
Iteration 5/25 | Loss: 0.00497820
Iteration 6/25 | Loss: 0.00497820
Iteration 7/25 | Loss: 0.00497820
Iteration 8/25 | Loss: 0.00497820
Iteration 9/25 | Loss: 0.00497819
Iteration 10/25 | Loss: 0.00497819
Iteration 11/25 | Loss: 0.00497819
Iteration 12/25 | Loss: 0.00497819
Iteration 13/25 | Loss: 0.00497819
Iteration 14/25 | Loss: 0.00497819
Iteration 15/25 | Loss: 0.00497819
Iteration 16/25 | Loss: 0.00497819
Iteration 17/25 | Loss: 0.00497819
Iteration 18/25 | Loss: 0.00497819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004978193901479244, 0.004978193901479244, 0.004978193901479244, 0.004978193901479244, 0.004978193901479244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004978193901479244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00497819
Iteration 2/1000 | Loss: 0.00496820
Iteration 3/1000 | Loss: 0.00499605
Iteration 4/1000 | Loss: 0.01016257
Iteration 5/1000 | Loss: 0.00115155
Iteration 6/1000 | Loss: 0.00152163
Iteration 7/1000 | Loss: 0.00106733
Iteration 8/1000 | Loss: 0.00137578
Iteration 9/1000 | Loss: 0.00065612
Iteration 10/1000 | Loss: 0.00200018
Iteration 11/1000 | Loss: 0.00241811
Iteration 12/1000 | Loss: 0.00147811
Iteration 13/1000 | Loss: 0.00117925
Iteration 14/1000 | Loss: 0.00273146
Iteration 15/1000 | Loss: 0.00063399
Iteration 16/1000 | Loss: 0.00052110
Iteration 17/1000 | Loss: 0.00051125
Iteration 18/1000 | Loss: 0.00047945
Iteration 19/1000 | Loss: 0.00053391
Iteration 20/1000 | Loss: 0.00117543
Iteration 21/1000 | Loss: 0.00103918
Iteration 22/1000 | Loss: 0.00061258
Iteration 23/1000 | Loss: 0.00052298
Iteration 24/1000 | Loss: 0.00139320
Iteration 25/1000 | Loss: 0.00033019
Iteration 26/1000 | Loss: 0.00022384
Iteration 27/1000 | Loss: 0.00052652
Iteration 28/1000 | Loss: 0.00129413
Iteration 29/1000 | Loss: 0.00018029
Iteration 30/1000 | Loss: 0.00027368
Iteration 31/1000 | Loss: 0.00030864
Iteration 32/1000 | Loss: 0.00014255
Iteration 33/1000 | Loss: 0.00065496
Iteration 34/1000 | Loss: 0.00030926
Iteration 35/1000 | Loss: 0.00293943
Iteration 36/1000 | Loss: 0.00380758
Iteration 37/1000 | Loss: 0.00134496
Iteration 38/1000 | Loss: 0.00089275
Iteration 39/1000 | Loss: 0.00107071
Iteration 40/1000 | Loss: 0.00034353
Iteration 41/1000 | Loss: 0.00044893
Iteration 42/1000 | Loss: 0.00039605
Iteration 43/1000 | Loss: 0.00033130
Iteration 44/1000 | Loss: 0.00092085
Iteration 45/1000 | Loss: 0.00050953
Iteration 46/1000 | Loss: 0.00055123
Iteration 47/1000 | Loss: 0.00076647
Iteration 48/1000 | Loss: 0.00078916
Iteration 49/1000 | Loss: 0.00071580
Iteration 50/1000 | Loss: 0.00064423
Iteration 51/1000 | Loss: 0.00048015
Iteration 52/1000 | Loss: 0.00065002
Iteration 53/1000 | Loss: 0.00029792
Iteration 54/1000 | Loss: 0.00059571
Iteration 55/1000 | Loss: 0.00060960
Iteration 56/1000 | Loss: 0.00036401
Iteration 57/1000 | Loss: 0.00023692
Iteration 58/1000 | Loss: 0.00017567
Iteration 59/1000 | Loss: 0.00033134
Iteration 60/1000 | Loss: 0.00017483
Iteration 61/1000 | Loss: 0.00024206
Iteration 62/1000 | Loss: 0.00061867
Iteration 63/1000 | Loss: 0.00015399
Iteration 64/1000 | Loss: 0.00030658
Iteration 65/1000 | Loss: 0.00042979
Iteration 66/1000 | Loss: 0.00101583
Iteration 67/1000 | Loss: 0.00054365
Iteration 68/1000 | Loss: 0.00074820
Iteration 69/1000 | Loss: 0.00114758
Iteration 70/1000 | Loss: 0.00106214
Iteration 71/1000 | Loss: 0.00032361
Iteration 72/1000 | Loss: 0.00033113
Iteration 73/1000 | Loss: 0.00042491
Iteration 74/1000 | Loss: 0.00018491
Iteration 75/1000 | Loss: 0.00036758
Iteration 76/1000 | Loss: 0.00014364
Iteration 77/1000 | Loss: 0.00016972
Iteration 78/1000 | Loss: 0.00013899
Iteration 79/1000 | Loss: 0.00041308
Iteration 80/1000 | Loss: 0.00050910
Iteration 81/1000 | Loss: 0.00035721
Iteration 82/1000 | Loss: 0.00026774
Iteration 83/1000 | Loss: 0.00038440
Iteration 84/1000 | Loss: 0.00017713
Iteration 85/1000 | Loss: 0.00020731
Iteration 86/1000 | Loss: 0.00028000
Iteration 87/1000 | Loss: 0.00017796
Iteration 88/1000 | Loss: 0.00053548
Iteration 89/1000 | Loss: 0.00022993
Iteration 90/1000 | Loss: 0.00045254
Iteration 91/1000 | Loss: 0.00036200
Iteration 92/1000 | Loss: 0.00024670
Iteration 93/1000 | Loss: 0.00037387
Iteration 94/1000 | Loss: 0.00081558
Iteration 95/1000 | Loss: 0.00104700
Iteration 96/1000 | Loss: 0.00012054
Iteration 97/1000 | Loss: 0.00015256
Iteration 98/1000 | Loss: 0.00024378
Iteration 99/1000 | Loss: 0.00011503
Iteration 100/1000 | Loss: 0.00014227
Iteration 101/1000 | Loss: 0.00014360
Iteration 102/1000 | Loss: 0.00021411
Iteration 103/1000 | Loss: 0.00020007
Iteration 104/1000 | Loss: 0.00047100
Iteration 105/1000 | Loss: 0.00024377
Iteration 106/1000 | Loss: 0.00014116
Iteration 107/1000 | Loss: 0.00022869
Iteration 108/1000 | Loss: 0.00072531
Iteration 109/1000 | Loss: 0.00030779
Iteration 110/1000 | Loss: 0.00015870
Iteration 111/1000 | Loss: 0.00014347
Iteration 112/1000 | Loss: 0.00015917
Iteration 113/1000 | Loss: 0.00030225
Iteration 114/1000 | Loss: 0.00022112
Iteration 115/1000 | Loss: 0.00011344
Iteration 116/1000 | Loss: 0.00009850
Iteration 117/1000 | Loss: 0.00021604
Iteration 118/1000 | Loss: 0.00158849
Iteration 119/1000 | Loss: 0.00058456
Iteration 120/1000 | Loss: 0.00026365
Iteration 121/1000 | Loss: 0.00042171
Iteration 122/1000 | Loss: 0.00039928
Iteration 123/1000 | Loss: 0.00035219
Iteration 124/1000 | Loss: 0.00032255
Iteration 125/1000 | Loss: 0.00042769
Iteration 126/1000 | Loss: 0.00016793
Iteration 127/1000 | Loss: 0.00027395
Iteration 128/1000 | Loss: 0.00022000
Iteration 129/1000 | Loss: 0.00056214
Iteration 130/1000 | Loss: 0.00027976
Iteration 131/1000 | Loss: 0.00017134
Iteration 132/1000 | Loss: 0.00010707
Iteration 133/1000 | Loss: 0.00021409
Iteration 134/1000 | Loss: 0.00010602
Iteration 135/1000 | Loss: 0.00008938
Iteration 136/1000 | Loss: 0.00060308
Iteration 137/1000 | Loss: 0.00021424
Iteration 138/1000 | Loss: 0.00023799
Iteration 139/1000 | Loss: 0.00107738
Iteration 140/1000 | Loss: 0.00020493
Iteration 141/1000 | Loss: 0.00009694
Iteration 142/1000 | Loss: 0.00024437
Iteration 143/1000 | Loss: 0.00009831
Iteration 144/1000 | Loss: 0.00008659
Iteration 145/1000 | Loss: 0.00030186
Iteration 146/1000 | Loss: 0.00008189
Iteration 147/1000 | Loss: 0.00035018
Iteration 148/1000 | Loss: 0.00017438
Iteration 149/1000 | Loss: 0.00008170
Iteration 150/1000 | Loss: 0.00007907
Iteration 151/1000 | Loss: 0.00032627
Iteration 152/1000 | Loss: 0.00008771
Iteration 153/1000 | Loss: 0.00008001
Iteration 154/1000 | Loss: 0.00032758
Iteration 155/1000 | Loss: 0.00021604
Iteration 156/1000 | Loss: 0.00021643
Iteration 157/1000 | Loss: 0.00048993
Iteration 158/1000 | Loss: 0.00011215
Iteration 159/1000 | Loss: 0.00013031
Iteration 160/1000 | Loss: 0.00007703
Iteration 161/1000 | Loss: 0.00018759
Iteration 162/1000 | Loss: 0.00024426
Iteration 163/1000 | Loss: 0.00012538
Iteration 164/1000 | Loss: 0.00048220
Iteration 165/1000 | Loss: 0.00010634
Iteration 166/1000 | Loss: 0.00028358
Iteration 167/1000 | Loss: 0.00007613
Iteration 168/1000 | Loss: 0.00007105
Iteration 169/1000 | Loss: 0.00006821
Iteration 170/1000 | Loss: 0.00006598
Iteration 171/1000 | Loss: 0.00006487
Iteration 172/1000 | Loss: 0.00006421
Iteration 173/1000 | Loss: 0.00006370
Iteration 174/1000 | Loss: 0.00006330
Iteration 175/1000 | Loss: 0.00006293
Iteration 176/1000 | Loss: 0.00006266
Iteration 177/1000 | Loss: 0.00006241
Iteration 178/1000 | Loss: 0.00028413
Iteration 179/1000 | Loss: 0.00007408
Iteration 180/1000 | Loss: 0.00006611
Iteration 181/1000 | Loss: 0.00042573
Iteration 182/1000 | Loss: 0.00053865
Iteration 183/1000 | Loss: 0.00050867
Iteration 184/1000 | Loss: 0.00055802
Iteration 185/1000 | Loss: 0.00034812
Iteration 186/1000 | Loss: 0.00050771
Iteration 187/1000 | Loss: 0.00025622
Iteration 188/1000 | Loss: 0.00031936
Iteration 189/1000 | Loss: 0.00027364
Iteration 190/1000 | Loss: 0.00006768
Iteration 191/1000 | Loss: 0.00008902
Iteration 192/1000 | Loss: 0.00006162
Iteration 193/1000 | Loss: 0.00006011
Iteration 194/1000 | Loss: 0.00005949
Iteration 195/1000 | Loss: 0.00005905
Iteration 196/1000 | Loss: 0.00005879
Iteration 197/1000 | Loss: 0.00005868
Iteration 198/1000 | Loss: 0.00005861
Iteration 199/1000 | Loss: 0.00005854
Iteration 200/1000 | Loss: 0.00005851
Iteration 201/1000 | Loss: 0.00005850
Iteration 202/1000 | Loss: 0.00005850
Iteration 203/1000 | Loss: 0.00005846
Iteration 204/1000 | Loss: 0.00005845
Iteration 205/1000 | Loss: 0.00005844
Iteration 206/1000 | Loss: 0.00005843
Iteration 207/1000 | Loss: 0.00005842
Iteration 208/1000 | Loss: 0.00005841
Iteration 209/1000 | Loss: 0.00005840
Iteration 210/1000 | Loss: 0.00005840
Iteration 211/1000 | Loss: 0.00005839
Iteration 212/1000 | Loss: 0.00005839
Iteration 213/1000 | Loss: 0.00005838
Iteration 214/1000 | Loss: 0.00005838
Iteration 215/1000 | Loss: 0.00005838
Iteration 216/1000 | Loss: 0.00005838
Iteration 217/1000 | Loss: 0.00005837
Iteration 218/1000 | Loss: 0.00005837
Iteration 219/1000 | Loss: 0.00005837
Iteration 220/1000 | Loss: 0.00005836
Iteration 221/1000 | Loss: 0.00005836
Iteration 222/1000 | Loss: 0.00005836
Iteration 223/1000 | Loss: 0.00005836
Iteration 224/1000 | Loss: 0.00005836
Iteration 225/1000 | Loss: 0.00005836
Iteration 226/1000 | Loss: 0.00005836
Iteration 227/1000 | Loss: 0.00005836
Iteration 228/1000 | Loss: 0.00005836
Iteration 229/1000 | Loss: 0.00005836
Iteration 230/1000 | Loss: 0.00005835
Iteration 231/1000 | Loss: 0.00005835
Iteration 232/1000 | Loss: 0.00005834
Iteration 233/1000 | Loss: 0.00005832
Iteration 234/1000 | Loss: 0.00005827
Iteration 235/1000 | Loss: 0.00005826
Iteration 236/1000 | Loss: 0.00005826
Iteration 237/1000 | Loss: 0.00005825
Iteration 238/1000 | Loss: 0.00005825
Iteration 239/1000 | Loss: 0.00005825
Iteration 240/1000 | Loss: 0.00005825
Iteration 241/1000 | Loss: 0.00005824
Iteration 242/1000 | Loss: 0.00005824
Iteration 243/1000 | Loss: 0.00005824
Iteration 244/1000 | Loss: 0.00005822
Iteration 245/1000 | Loss: 0.00005822
Iteration 246/1000 | Loss: 0.00005821
Iteration 247/1000 | Loss: 0.00005821
Iteration 248/1000 | Loss: 0.00005821
Iteration 249/1000 | Loss: 0.00005821
Iteration 250/1000 | Loss: 0.00005821
Iteration 251/1000 | Loss: 0.00005821
Iteration 252/1000 | Loss: 0.00005820
Iteration 253/1000 | Loss: 0.00005820
Iteration 254/1000 | Loss: 0.00005820
Iteration 255/1000 | Loss: 0.00005819
Iteration 256/1000 | Loss: 0.00016689
Iteration 257/1000 | Loss: 0.00006419
Iteration 258/1000 | Loss: 0.00006025
Iteration 259/1000 | Loss: 0.00005889
Iteration 260/1000 | Loss: 0.00015526
Iteration 261/1000 | Loss: 0.00006267
Iteration 262/1000 | Loss: 0.00005707
Iteration 263/1000 | Loss: 0.00005678
Iteration 264/1000 | Loss: 0.00005653
Iteration 265/1000 | Loss: 0.00005650
Iteration 266/1000 | Loss: 0.00005650
Iteration 267/1000 | Loss: 0.00005649
Iteration 268/1000 | Loss: 0.00005649
Iteration 269/1000 | Loss: 0.00021164
Iteration 270/1000 | Loss: 0.00006776
Iteration 271/1000 | Loss: 0.00005997
Iteration 272/1000 | Loss: 0.00005806
Iteration 273/1000 | Loss: 0.00045273
Iteration 274/1000 | Loss: 0.00059702
Iteration 275/1000 | Loss: 0.00008134
Iteration 276/1000 | Loss: 0.00006503
Iteration 277/1000 | Loss: 0.00006136
Iteration 278/1000 | Loss: 0.00005900
Iteration 279/1000 | Loss: 0.00041173
Iteration 280/1000 | Loss: 0.00023184
Iteration 281/1000 | Loss: 0.00005768
Iteration 282/1000 | Loss: 0.00005604
Iteration 283/1000 | Loss: 0.00038699
Iteration 284/1000 | Loss: 0.00041956
Iteration 285/1000 | Loss: 0.00036145
Iteration 286/1000 | Loss: 0.00012062
Iteration 287/1000 | Loss: 0.00033456
Iteration 288/1000 | Loss: 0.00014155
Iteration 289/1000 | Loss: 0.00131387
Iteration 290/1000 | Loss: 0.00023919
Iteration 291/1000 | Loss: 0.00162913
Iteration 292/1000 | Loss: 0.00053255
Iteration 293/1000 | Loss: 0.00041433
Iteration 294/1000 | Loss: 0.00009413
Iteration 295/1000 | Loss: 0.00006468
Iteration 296/1000 | Loss: 0.00005361
Iteration 297/1000 | Loss: 0.00015696
Iteration 298/1000 | Loss: 0.00005107
Iteration 299/1000 | Loss: 0.00005007
Iteration 300/1000 | Loss: 0.00014013
Iteration 301/1000 | Loss: 0.00007906
Iteration 302/1000 | Loss: 0.00004916
Iteration 303/1000 | Loss: 0.00004884
Iteration 304/1000 | Loss: 0.00004858
Iteration 305/1000 | Loss: 0.00009304
Iteration 306/1000 | Loss: 0.00025795
Iteration 307/1000 | Loss: 0.00024602
Iteration 308/1000 | Loss: 0.00006455
Iteration 309/1000 | Loss: 0.00009978
Iteration 310/1000 | Loss: 0.00010404
Iteration 311/1000 | Loss: 0.00004843
Iteration 312/1000 | Loss: 0.00004830
Iteration 313/1000 | Loss: 0.00004827
Iteration 314/1000 | Loss: 0.00004824
Iteration 315/1000 | Loss: 0.00004820
Iteration 316/1000 | Loss: 0.00004817
Iteration 317/1000 | Loss: 0.00004816
Iteration 318/1000 | Loss: 0.00004815
Iteration 319/1000 | Loss: 0.00004814
Iteration 320/1000 | Loss: 0.00004814
Iteration 321/1000 | Loss: 0.00004813
Iteration 322/1000 | Loss: 0.00004813
Iteration 323/1000 | Loss: 0.00004813
Iteration 324/1000 | Loss: 0.00004812
Iteration 325/1000 | Loss: 0.00004812
Iteration 326/1000 | Loss: 0.00004811
Iteration 327/1000 | Loss: 0.00004811
Iteration 328/1000 | Loss: 0.00004811
Iteration 329/1000 | Loss: 0.00004811
Iteration 330/1000 | Loss: 0.00004810
Iteration 331/1000 | Loss: 0.00004810
Iteration 332/1000 | Loss: 0.00004810
Iteration 333/1000 | Loss: 0.00004810
Iteration 334/1000 | Loss: 0.00004810
Iteration 335/1000 | Loss: 0.00004809
Iteration 336/1000 | Loss: 0.00004809
Iteration 337/1000 | Loss: 0.00004809
Iteration 338/1000 | Loss: 0.00004809
Iteration 339/1000 | Loss: 0.00004809
Iteration 340/1000 | Loss: 0.00004808
Iteration 341/1000 | Loss: 0.00004808
Iteration 342/1000 | Loss: 0.00004808
Iteration 343/1000 | Loss: 0.00004808
Iteration 344/1000 | Loss: 0.00004808
Iteration 345/1000 | Loss: 0.00004808
Iteration 346/1000 | Loss: 0.00004808
Iteration 347/1000 | Loss: 0.00004808
Iteration 348/1000 | Loss: 0.00004808
Iteration 349/1000 | Loss: 0.00004807
Iteration 350/1000 | Loss: 0.00004807
Iteration 351/1000 | Loss: 0.00004807
Iteration 352/1000 | Loss: 0.00004807
Iteration 353/1000 | Loss: 0.00004807
Iteration 354/1000 | Loss: 0.00004807
Iteration 355/1000 | Loss: 0.00004807
Iteration 356/1000 | Loss: 0.00004807
Iteration 357/1000 | Loss: 0.00004807
Iteration 358/1000 | Loss: 0.00004807
Iteration 359/1000 | Loss: 0.00004807
Iteration 360/1000 | Loss: 0.00004807
Iteration 361/1000 | Loss: 0.00004807
Iteration 362/1000 | Loss: 0.00004807
Iteration 363/1000 | Loss: 0.00004807
Iteration 364/1000 | Loss: 0.00004807
Iteration 365/1000 | Loss: 0.00004807
Iteration 366/1000 | Loss: 0.00004806
Iteration 367/1000 | Loss: 0.00004806
Iteration 368/1000 | Loss: 0.00004806
Iteration 369/1000 | Loss: 0.00004806
Iteration 370/1000 | Loss: 0.00004806
Iteration 371/1000 | Loss: 0.00004806
Iteration 372/1000 | Loss: 0.00004806
Iteration 373/1000 | Loss: 0.00004806
Iteration 374/1000 | Loss: 0.00004806
Iteration 375/1000 | Loss: 0.00004806
Iteration 376/1000 | Loss: 0.00004806
Iteration 377/1000 | Loss: 0.00004806
Iteration 378/1000 | Loss: 0.00004806
Iteration 379/1000 | Loss: 0.00004806
Iteration 380/1000 | Loss: 0.00004806
Iteration 381/1000 | Loss: 0.00004806
Iteration 382/1000 | Loss: 0.00004806
Iteration 383/1000 | Loss: 0.00004806
Iteration 384/1000 | Loss: 0.00004806
Iteration 385/1000 | Loss: 0.00004806
Iteration 386/1000 | Loss: 0.00004805
Iteration 387/1000 | Loss: 0.00004805
Iteration 388/1000 | Loss: 0.00004805
Iteration 389/1000 | Loss: 0.00004805
Iteration 390/1000 | Loss: 0.00004805
Iteration 391/1000 | Loss: 0.00004805
Iteration 392/1000 | Loss: 0.00004805
Iteration 393/1000 | Loss: 0.00004805
Iteration 394/1000 | Loss: 0.00004805
Iteration 395/1000 | Loss: 0.00004805
Iteration 396/1000 | Loss: 0.00004804
Iteration 397/1000 | Loss: 0.00004804
Iteration 398/1000 | Loss: 0.00004804
Iteration 399/1000 | Loss: 0.00004804
Iteration 400/1000 | Loss: 0.00004804
Iteration 401/1000 | Loss: 0.00004804
Iteration 402/1000 | Loss: 0.00004804
Iteration 403/1000 | Loss: 0.00004804
Iteration 404/1000 | Loss: 0.00004804
Iteration 405/1000 | Loss: 0.00004804
Iteration 406/1000 | Loss: 0.00004804
Iteration 407/1000 | Loss: 0.00004804
Iteration 408/1000 | Loss: 0.00004804
Iteration 409/1000 | Loss: 0.00004804
Iteration 410/1000 | Loss: 0.00004803
Iteration 411/1000 | Loss: 0.00004803
Iteration 412/1000 | Loss: 0.00004803
Iteration 413/1000 | Loss: 0.00004803
Iteration 414/1000 | Loss: 0.00004803
Iteration 415/1000 | Loss: 0.00004803
Iteration 416/1000 | Loss: 0.00004803
Iteration 417/1000 | Loss: 0.00004803
Iteration 418/1000 | Loss: 0.00004803
Iteration 419/1000 | Loss: 0.00004803
Iteration 420/1000 | Loss: 0.00004803
Iteration 421/1000 | Loss: 0.00004803
Iteration 422/1000 | Loss: 0.00004803
Iteration 423/1000 | Loss: 0.00004803
Iteration 424/1000 | Loss: 0.00004803
Iteration 425/1000 | Loss: 0.00004803
Iteration 426/1000 | Loss: 0.00004803
Iteration 427/1000 | Loss: 0.00004802
Iteration 428/1000 | Loss: 0.00004802
Iteration 429/1000 | Loss: 0.00004802
Iteration 430/1000 | Loss: 0.00004802
Iteration 431/1000 | Loss: 0.00004802
Iteration 432/1000 | Loss: 0.00004802
Iteration 433/1000 | Loss: 0.00004802
Iteration 434/1000 | Loss: 0.00004802
Iteration 435/1000 | Loss: 0.00004802
Iteration 436/1000 | Loss: 0.00004802
Iteration 437/1000 | Loss: 0.00004802
Iteration 438/1000 | Loss: 0.00004802
Iteration 439/1000 | Loss: 0.00004801
Iteration 440/1000 | Loss: 0.00004801
Iteration 441/1000 | Loss: 0.00004801
Iteration 442/1000 | Loss: 0.00004801
Iteration 443/1000 | Loss: 0.00004801
Iteration 444/1000 | Loss: 0.00004801
Iteration 445/1000 | Loss: 0.00004801
Iteration 446/1000 | Loss: 0.00004801
Iteration 447/1000 | Loss: 0.00004801
Iteration 448/1000 | Loss: 0.00004801
Iteration 449/1000 | Loss: 0.00004801
Iteration 450/1000 | Loss: 0.00004801
Iteration 451/1000 | Loss: 0.00004801
Iteration 452/1000 | Loss: 0.00004801
Iteration 453/1000 | Loss: 0.00004800
Iteration 454/1000 | Loss: 0.00004800
Iteration 455/1000 | Loss: 0.00004800
Iteration 456/1000 | Loss: 0.00004800
Iteration 457/1000 | Loss: 0.00004800
Iteration 458/1000 | Loss: 0.00004800
Iteration 459/1000 | Loss: 0.00004800
Iteration 460/1000 | Loss: 0.00004799
Iteration 461/1000 | Loss: 0.00004799
Iteration 462/1000 | Loss: 0.00004799
Iteration 463/1000 | Loss: 0.00004799
Iteration 464/1000 | Loss: 0.00004799
Iteration 465/1000 | Loss: 0.00004799
Iteration 466/1000 | Loss: 0.00004799
Iteration 467/1000 | Loss: 0.00004799
Iteration 468/1000 | Loss: 0.00004799
Iteration 469/1000 | Loss: 0.00004799
Iteration 470/1000 | Loss: 0.00004799
Iteration 471/1000 | Loss: 0.00004799
Iteration 472/1000 | Loss: 0.00004799
Iteration 473/1000 | Loss: 0.00004799
Iteration 474/1000 | Loss: 0.00004799
Iteration 475/1000 | Loss: 0.00004798
Iteration 476/1000 | Loss: 0.00004798
Iteration 477/1000 | Loss: 0.00004798
Iteration 478/1000 | Loss: 0.00004798
Iteration 479/1000 | Loss: 0.00004798
Iteration 480/1000 | Loss: 0.00004798
Iteration 481/1000 | Loss: 0.00004798
Iteration 482/1000 | Loss: 0.00004798
Iteration 483/1000 | Loss: 0.00004798
Iteration 484/1000 | Loss: 0.00004798
Iteration 485/1000 | Loss: 0.00004798
Iteration 486/1000 | Loss: 0.00004798
Iteration 487/1000 | Loss: 0.00004798
Iteration 488/1000 | Loss: 0.00004798
Iteration 489/1000 | Loss: 0.00004798
Iteration 490/1000 | Loss: 0.00004798
Iteration 491/1000 | Loss: 0.00004798
Iteration 492/1000 | Loss: 0.00004798
Iteration 493/1000 | Loss: 0.00004798
Iteration 494/1000 | Loss: 0.00004798
Iteration 495/1000 | Loss: 0.00004798
Iteration 496/1000 | Loss: 0.00004798
Iteration 497/1000 | Loss: 0.00004798
Iteration 498/1000 | Loss: 0.00004798
Iteration 499/1000 | Loss: 0.00004798
Iteration 500/1000 | Loss: 0.00004798
Iteration 501/1000 | Loss: 0.00004798
Iteration 502/1000 | Loss: 0.00004798
Iteration 503/1000 | Loss: 0.00004798
Iteration 504/1000 | Loss: 0.00004798
Iteration 505/1000 | Loss: 0.00004798
Iteration 506/1000 | Loss: 0.00004798
Iteration 507/1000 | Loss: 0.00004798
Iteration 508/1000 | Loss: 0.00004798
Iteration 509/1000 | Loss: 0.00004798
Iteration 510/1000 | Loss: 0.00004798
Iteration 511/1000 | Loss: 0.00004798
Iteration 512/1000 | Loss: 0.00004798
Iteration 513/1000 | Loss: 0.00004798
Iteration 514/1000 | Loss: 0.00004798
Iteration 515/1000 | Loss: 0.00004798
Iteration 516/1000 | Loss: 0.00004798
Iteration 517/1000 | Loss: 0.00004798
Iteration 518/1000 | Loss: 0.00004798
Iteration 519/1000 | Loss: 0.00004798
Iteration 520/1000 | Loss: 0.00004798
Iteration 521/1000 | Loss: 0.00004798
Iteration 522/1000 | Loss: 0.00004798
Iteration 523/1000 | Loss: 0.00004798
Iteration 524/1000 | Loss: 0.00004798
Iteration 525/1000 | Loss: 0.00004798
Iteration 526/1000 | Loss: 0.00004798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 526. Stopping optimization.
Last 5 losses: [4.797684960067272e-05, 4.797684960067272e-05, 4.797684960067272e-05, 4.797684960067272e-05, 4.797684960067272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.797684960067272e-05

Optimization complete. Final v2v error: 3.584062337875366 mm

Highest mean error: 12.23306655883789 mm for frame 107

Lowest mean error: 2.4013590812683105 mm for frame 171

Saving results

Total time: 480.1743948459625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851465
Iteration 2/25 | Loss: 0.00118381
Iteration 3/25 | Loss: 0.00105131
Iteration 4/25 | Loss: 0.00101734
Iteration 5/25 | Loss: 0.00100126
Iteration 6/25 | Loss: 0.00099686
Iteration 7/25 | Loss: 0.00099502
Iteration 8/25 | Loss: 0.00099434
Iteration 9/25 | Loss: 0.00099434
Iteration 10/25 | Loss: 0.00099434
Iteration 11/25 | Loss: 0.00099434
Iteration 12/25 | Loss: 0.00099434
Iteration 13/25 | Loss: 0.00099434
Iteration 14/25 | Loss: 0.00099434
Iteration 15/25 | Loss: 0.00099433
Iteration 16/25 | Loss: 0.00099433
Iteration 17/25 | Loss: 0.00099433
Iteration 18/25 | Loss: 0.00099433
Iteration 19/25 | Loss: 0.00099433
Iteration 20/25 | Loss: 0.00099433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009943315526470542, 0.0009943315526470542, 0.0009943315526470542, 0.0009943315526470542, 0.0009943315526470542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009943315526470542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44559503
Iteration 2/25 | Loss: 0.00078311
Iteration 3/25 | Loss: 0.00078311
Iteration 4/25 | Loss: 0.00078311
Iteration 5/25 | Loss: 0.00078311
Iteration 6/25 | Loss: 0.00078311
Iteration 7/25 | Loss: 0.00078311
Iteration 8/25 | Loss: 0.00078311
Iteration 9/25 | Loss: 0.00078310
Iteration 10/25 | Loss: 0.00078310
Iteration 11/25 | Loss: 0.00078310
Iteration 12/25 | Loss: 0.00078310
Iteration 13/25 | Loss: 0.00078310
Iteration 14/25 | Loss: 0.00078310
Iteration 15/25 | Loss: 0.00078310
Iteration 16/25 | Loss: 0.00078310
Iteration 17/25 | Loss: 0.00078310
Iteration 18/25 | Loss: 0.00078310
Iteration 19/25 | Loss: 0.00078310
Iteration 20/25 | Loss: 0.00078310
Iteration 21/25 | Loss: 0.00078310
Iteration 22/25 | Loss: 0.00078310
Iteration 23/25 | Loss: 0.00078310
Iteration 24/25 | Loss: 0.00078310
Iteration 25/25 | Loss: 0.00078310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078310
Iteration 2/1000 | Loss: 0.00005122
Iteration 3/1000 | Loss: 0.00002986
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001874
Iteration 9/1000 | Loss: 0.00001849
Iteration 10/1000 | Loss: 0.00001826
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001804
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001790
Iteration 26/1000 | Loss: 0.00001790
Iteration 27/1000 | Loss: 0.00001790
Iteration 28/1000 | Loss: 0.00001789
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00001784
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001777
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001776
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001772
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001771
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001771
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001770
Iteration 53/1000 | Loss: 0.00001770
Iteration 54/1000 | Loss: 0.00001769
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001768
Iteration 57/1000 | Loss: 0.00001768
Iteration 58/1000 | Loss: 0.00001768
Iteration 59/1000 | Loss: 0.00001768
Iteration 60/1000 | Loss: 0.00001767
Iteration 61/1000 | Loss: 0.00001767
Iteration 62/1000 | Loss: 0.00001767
Iteration 63/1000 | Loss: 0.00001767
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001766
Iteration 68/1000 | Loss: 0.00001765
Iteration 69/1000 | Loss: 0.00001765
Iteration 70/1000 | Loss: 0.00001765
Iteration 71/1000 | Loss: 0.00001765
Iteration 72/1000 | Loss: 0.00001765
Iteration 73/1000 | Loss: 0.00001765
Iteration 74/1000 | Loss: 0.00001764
Iteration 75/1000 | Loss: 0.00001764
Iteration 76/1000 | Loss: 0.00001764
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001764
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001763
Iteration 87/1000 | Loss: 0.00001763
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001762
Iteration 99/1000 | Loss: 0.00001762
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001762
Iteration 104/1000 | Loss: 0.00001762
Iteration 105/1000 | Loss: 0.00001762
Iteration 106/1000 | Loss: 0.00001762
Iteration 107/1000 | Loss: 0.00001762
Iteration 108/1000 | Loss: 0.00001762
Iteration 109/1000 | Loss: 0.00001761
Iteration 110/1000 | Loss: 0.00001761
Iteration 111/1000 | Loss: 0.00001761
Iteration 112/1000 | Loss: 0.00001761
Iteration 113/1000 | Loss: 0.00001761
Iteration 114/1000 | Loss: 0.00001761
Iteration 115/1000 | Loss: 0.00001761
Iteration 116/1000 | Loss: 0.00001760
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001760
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001760
Iteration 122/1000 | Loss: 0.00001760
Iteration 123/1000 | Loss: 0.00001760
Iteration 124/1000 | Loss: 0.00001760
Iteration 125/1000 | Loss: 0.00001760
Iteration 126/1000 | Loss: 0.00001760
Iteration 127/1000 | Loss: 0.00001760
Iteration 128/1000 | Loss: 0.00001760
Iteration 129/1000 | Loss: 0.00001760
Iteration 130/1000 | Loss: 0.00001760
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Iteration 139/1000 | Loss: 0.00001759
Iteration 140/1000 | Loss: 0.00001759
Iteration 141/1000 | Loss: 0.00001759
Iteration 142/1000 | Loss: 0.00001759
Iteration 143/1000 | Loss: 0.00001759
Iteration 144/1000 | Loss: 0.00001759
Iteration 145/1000 | Loss: 0.00001758
Iteration 146/1000 | Loss: 0.00001758
Iteration 147/1000 | Loss: 0.00001758
Iteration 148/1000 | Loss: 0.00001758
Iteration 149/1000 | Loss: 0.00001758
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001758
Iteration 154/1000 | Loss: 0.00001758
Iteration 155/1000 | Loss: 0.00001758
Iteration 156/1000 | Loss: 0.00001758
Iteration 157/1000 | Loss: 0.00001757
Iteration 158/1000 | Loss: 0.00001757
Iteration 159/1000 | Loss: 0.00001757
Iteration 160/1000 | Loss: 0.00001757
Iteration 161/1000 | Loss: 0.00001757
Iteration 162/1000 | Loss: 0.00001757
Iteration 163/1000 | Loss: 0.00001757
Iteration 164/1000 | Loss: 0.00001757
Iteration 165/1000 | Loss: 0.00001757
Iteration 166/1000 | Loss: 0.00001757
Iteration 167/1000 | Loss: 0.00001757
Iteration 168/1000 | Loss: 0.00001757
Iteration 169/1000 | Loss: 0.00001757
Iteration 170/1000 | Loss: 0.00001757
Iteration 171/1000 | Loss: 0.00001757
Iteration 172/1000 | Loss: 0.00001757
Iteration 173/1000 | Loss: 0.00001757
Iteration 174/1000 | Loss: 0.00001756
Iteration 175/1000 | Loss: 0.00001756
Iteration 176/1000 | Loss: 0.00001756
Iteration 177/1000 | Loss: 0.00001756
Iteration 178/1000 | Loss: 0.00001756
Iteration 179/1000 | Loss: 0.00001756
Iteration 180/1000 | Loss: 0.00001756
Iteration 181/1000 | Loss: 0.00001756
Iteration 182/1000 | Loss: 0.00001756
Iteration 183/1000 | Loss: 0.00001756
Iteration 184/1000 | Loss: 0.00001756
Iteration 185/1000 | Loss: 0.00001756
Iteration 186/1000 | Loss: 0.00001756
Iteration 187/1000 | Loss: 0.00001756
Iteration 188/1000 | Loss: 0.00001756
Iteration 189/1000 | Loss: 0.00001756
Iteration 190/1000 | Loss: 0.00001756
Iteration 191/1000 | Loss: 0.00001756
Iteration 192/1000 | Loss: 0.00001756
Iteration 193/1000 | Loss: 0.00001756
Iteration 194/1000 | Loss: 0.00001756
Iteration 195/1000 | Loss: 0.00001756
Iteration 196/1000 | Loss: 0.00001756
Iteration 197/1000 | Loss: 0.00001756
Iteration 198/1000 | Loss: 0.00001756
Iteration 199/1000 | Loss: 0.00001756
Iteration 200/1000 | Loss: 0.00001756
Iteration 201/1000 | Loss: 0.00001756
Iteration 202/1000 | Loss: 0.00001755
Iteration 203/1000 | Loss: 0.00001755
Iteration 204/1000 | Loss: 0.00001755
Iteration 205/1000 | Loss: 0.00001755
Iteration 206/1000 | Loss: 0.00001755
Iteration 207/1000 | Loss: 0.00001755
Iteration 208/1000 | Loss: 0.00001755
Iteration 209/1000 | Loss: 0.00001755
Iteration 210/1000 | Loss: 0.00001755
Iteration 211/1000 | Loss: 0.00001755
Iteration 212/1000 | Loss: 0.00001755
Iteration 213/1000 | Loss: 0.00001755
Iteration 214/1000 | Loss: 0.00001755
Iteration 215/1000 | Loss: 0.00001755
Iteration 216/1000 | Loss: 0.00001755
Iteration 217/1000 | Loss: 0.00001755
Iteration 218/1000 | Loss: 0.00001755
Iteration 219/1000 | Loss: 0.00001754
Iteration 220/1000 | Loss: 0.00001754
Iteration 221/1000 | Loss: 0.00001754
Iteration 222/1000 | Loss: 0.00001754
Iteration 223/1000 | Loss: 0.00001754
Iteration 224/1000 | Loss: 0.00001754
Iteration 225/1000 | Loss: 0.00001754
Iteration 226/1000 | Loss: 0.00001754
Iteration 227/1000 | Loss: 0.00001754
Iteration 228/1000 | Loss: 0.00001754
Iteration 229/1000 | Loss: 0.00001754
Iteration 230/1000 | Loss: 0.00001754
Iteration 231/1000 | Loss: 0.00001754
Iteration 232/1000 | Loss: 0.00001754
Iteration 233/1000 | Loss: 0.00001754
Iteration 234/1000 | Loss: 0.00001754
Iteration 235/1000 | Loss: 0.00001754
Iteration 236/1000 | Loss: 0.00001754
Iteration 237/1000 | Loss: 0.00001754
Iteration 238/1000 | Loss: 0.00001754
Iteration 239/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.7543261492392048e-05, 1.7543261492392048e-05, 1.7543261492392048e-05, 1.7543261492392048e-05, 1.7543261492392048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7543261492392048e-05

Optimization complete. Final v2v error: 3.435652017593384 mm

Highest mean error: 5.3661627769470215 mm for frame 128

Lowest mean error: 2.5486562252044678 mm for frame 36

Saving results

Total time: 43.46833634376526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762580
Iteration 2/25 | Loss: 0.00133319
Iteration 3/25 | Loss: 0.00110173
Iteration 4/25 | Loss: 0.00105962
Iteration 5/25 | Loss: 0.00104895
Iteration 6/25 | Loss: 0.00104503
Iteration 7/25 | Loss: 0.00104476
Iteration 8/25 | Loss: 0.00104476
Iteration 9/25 | Loss: 0.00104476
Iteration 10/25 | Loss: 0.00104476
Iteration 11/25 | Loss: 0.00104476
Iteration 12/25 | Loss: 0.00104476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010447577806189656, 0.0010447577806189656, 0.0010447577806189656, 0.0010447577806189656, 0.0010447577806189656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010447577806189656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55864298
Iteration 2/25 | Loss: 0.00089239
Iteration 3/25 | Loss: 0.00089239
Iteration 4/25 | Loss: 0.00089239
Iteration 5/25 | Loss: 0.00089239
Iteration 6/25 | Loss: 0.00089239
Iteration 7/25 | Loss: 0.00089239
Iteration 8/25 | Loss: 0.00089239
Iteration 9/25 | Loss: 0.00089239
Iteration 10/25 | Loss: 0.00089239
Iteration 11/25 | Loss: 0.00089239
Iteration 12/25 | Loss: 0.00089239
Iteration 13/25 | Loss: 0.00089239
Iteration 14/25 | Loss: 0.00089239
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008923893328756094, 0.0008923893328756094, 0.0008923893328756094, 0.0008923893328756094, 0.0008923893328756094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008923893328756094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089239
Iteration 2/1000 | Loss: 0.00005923
Iteration 3/1000 | Loss: 0.00003248
Iteration 4/1000 | Loss: 0.00002370
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001928
Iteration 8/1000 | Loss: 0.00001874
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001804
Iteration 11/1000 | Loss: 0.00001799
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001750
Iteration 17/1000 | Loss: 0.00001749
Iteration 18/1000 | Loss: 0.00001748
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001728
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001718
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001716
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001714
Iteration 125/1000 | Loss: 0.00001714
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001714
Iteration 134/1000 | Loss: 0.00001714
Iteration 135/1000 | Loss: 0.00001714
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001712
Iteration 146/1000 | Loss: 0.00001712
Iteration 147/1000 | Loss: 0.00001712
Iteration 148/1000 | Loss: 0.00001712
Iteration 149/1000 | Loss: 0.00001712
Iteration 150/1000 | Loss: 0.00001712
Iteration 151/1000 | Loss: 0.00001712
Iteration 152/1000 | Loss: 0.00001711
Iteration 153/1000 | Loss: 0.00001711
Iteration 154/1000 | Loss: 0.00001711
Iteration 155/1000 | Loss: 0.00001711
Iteration 156/1000 | Loss: 0.00001711
Iteration 157/1000 | Loss: 0.00001711
Iteration 158/1000 | Loss: 0.00001711
Iteration 159/1000 | Loss: 0.00001711
Iteration 160/1000 | Loss: 0.00001711
Iteration 161/1000 | Loss: 0.00001711
Iteration 162/1000 | Loss: 0.00001710
Iteration 163/1000 | Loss: 0.00001710
Iteration 164/1000 | Loss: 0.00001710
Iteration 165/1000 | Loss: 0.00001710
Iteration 166/1000 | Loss: 0.00001710
Iteration 167/1000 | Loss: 0.00001710
Iteration 168/1000 | Loss: 0.00001710
Iteration 169/1000 | Loss: 0.00001710
Iteration 170/1000 | Loss: 0.00001710
Iteration 171/1000 | Loss: 0.00001710
Iteration 172/1000 | Loss: 0.00001710
Iteration 173/1000 | Loss: 0.00001710
Iteration 174/1000 | Loss: 0.00001710
Iteration 175/1000 | Loss: 0.00001710
Iteration 176/1000 | Loss: 0.00001710
Iteration 177/1000 | Loss: 0.00001710
Iteration 178/1000 | Loss: 0.00001710
Iteration 179/1000 | Loss: 0.00001710
Iteration 180/1000 | Loss: 0.00001710
Iteration 181/1000 | Loss: 0.00001710
Iteration 182/1000 | Loss: 0.00001710
Iteration 183/1000 | Loss: 0.00001710
Iteration 184/1000 | Loss: 0.00001710
Iteration 185/1000 | Loss: 0.00001710
Iteration 186/1000 | Loss: 0.00001710
Iteration 187/1000 | Loss: 0.00001710
Iteration 188/1000 | Loss: 0.00001710
Iteration 189/1000 | Loss: 0.00001710
Iteration 190/1000 | Loss: 0.00001710
Iteration 191/1000 | Loss: 0.00001710
Iteration 192/1000 | Loss: 0.00001710
Iteration 193/1000 | Loss: 0.00001710
Iteration 194/1000 | Loss: 0.00001710
Iteration 195/1000 | Loss: 0.00001710
Iteration 196/1000 | Loss: 0.00001710
Iteration 197/1000 | Loss: 0.00001710
Iteration 198/1000 | Loss: 0.00001710
Iteration 199/1000 | Loss: 0.00001710
Iteration 200/1000 | Loss: 0.00001710
Iteration 201/1000 | Loss: 0.00001710
Iteration 202/1000 | Loss: 0.00001710
Iteration 203/1000 | Loss: 0.00001710
Iteration 204/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.7097214367822744e-05, 1.7097214367822744e-05, 1.7097214367822744e-05, 1.7097214367822744e-05, 1.7097214367822744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7097214367822744e-05

Optimization complete. Final v2v error: 3.493515729904175 mm

Highest mean error: 4.900932312011719 mm for frame 65

Lowest mean error: 2.8211281299591064 mm for frame 102

Saving results

Total time: 47.95906472206116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045755
Iteration 2/25 | Loss: 0.00267650
Iteration 3/25 | Loss: 0.00162468
Iteration 4/25 | Loss: 0.00146010
Iteration 5/25 | Loss: 0.00143831
Iteration 6/25 | Loss: 0.00130400
Iteration 7/25 | Loss: 0.00124045
Iteration 8/25 | Loss: 0.00122598
Iteration 9/25 | Loss: 0.00117033
Iteration 10/25 | Loss: 0.00115635
Iteration 11/25 | Loss: 0.00113358
Iteration 12/25 | Loss: 0.00113400
Iteration 13/25 | Loss: 0.00113550
Iteration 14/25 | Loss: 0.00111850
Iteration 15/25 | Loss: 0.00111003
Iteration 16/25 | Loss: 0.00110628
Iteration 17/25 | Loss: 0.00110281
Iteration 18/25 | Loss: 0.00109861
Iteration 19/25 | Loss: 0.00109597
Iteration 20/25 | Loss: 0.00109399
Iteration 21/25 | Loss: 0.00109382
Iteration 22/25 | Loss: 0.00109377
Iteration 23/25 | Loss: 0.00109377
Iteration 24/25 | Loss: 0.00109377
Iteration 25/25 | Loss: 0.00109373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37177944
Iteration 2/25 | Loss: 0.00168339
Iteration 3/25 | Loss: 0.00155731
Iteration 4/25 | Loss: 0.00155729
Iteration 5/25 | Loss: 0.00155729
Iteration 6/25 | Loss: 0.00155729
Iteration 7/25 | Loss: 0.00155729
Iteration 8/25 | Loss: 0.00155729
Iteration 9/25 | Loss: 0.00155729
Iteration 10/25 | Loss: 0.00155729
Iteration 11/25 | Loss: 0.00155729
Iteration 12/25 | Loss: 0.00155729
Iteration 13/25 | Loss: 0.00155729
Iteration 14/25 | Loss: 0.00155729
Iteration 15/25 | Loss: 0.00155729
Iteration 16/25 | Loss: 0.00155729
Iteration 17/25 | Loss: 0.00155729
Iteration 18/25 | Loss: 0.00155729
Iteration 19/25 | Loss: 0.00155729
Iteration 20/25 | Loss: 0.00155729
Iteration 21/25 | Loss: 0.00155729
Iteration 22/25 | Loss: 0.00155729
Iteration 23/25 | Loss: 0.00155729
Iteration 24/25 | Loss: 0.00155729
Iteration 25/25 | Loss: 0.00155729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155729
Iteration 2/1000 | Loss: 0.00036538
Iteration 3/1000 | Loss: 0.00018424
Iteration 4/1000 | Loss: 0.00023589
Iteration 5/1000 | Loss: 0.00036328
Iteration 6/1000 | Loss: 0.00010370
Iteration 7/1000 | Loss: 0.00021472
Iteration 8/1000 | Loss: 0.00007725
Iteration 9/1000 | Loss: 0.00021984
Iteration 10/1000 | Loss: 0.00007813
Iteration 11/1000 | Loss: 0.00009472
Iteration 12/1000 | Loss: 0.00010194
Iteration 13/1000 | Loss: 0.00008052
Iteration 14/1000 | Loss: 0.00006583
Iteration 15/1000 | Loss: 0.00005212
Iteration 16/1000 | Loss: 0.00021776
Iteration 17/1000 | Loss: 0.00012498
Iteration 18/1000 | Loss: 0.00078692
Iteration 19/1000 | Loss: 0.00096202
Iteration 20/1000 | Loss: 0.00021379
Iteration 21/1000 | Loss: 0.00043151
Iteration 22/1000 | Loss: 0.00019421
Iteration 23/1000 | Loss: 0.00006737
Iteration 24/1000 | Loss: 0.00012610
Iteration 25/1000 | Loss: 0.00004880
Iteration 26/1000 | Loss: 0.00004367
Iteration 27/1000 | Loss: 0.00008681
Iteration 28/1000 | Loss: 0.00003704
Iteration 29/1000 | Loss: 0.00022507
Iteration 30/1000 | Loss: 0.00016648
Iteration 31/1000 | Loss: 0.00020352
Iteration 32/1000 | Loss: 0.00018080
Iteration 33/1000 | Loss: 0.00006953
Iteration 34/1000 | Loss: 0.00056685
Iteration 35/1000 | Loss: 0.00004028
Iteration 36/1000 | Loss: 0.00012275
Iteration 37/1000 | Loss: 0.00003512
Iteration 38/1000 | Loss: 0.00003454
Iteration 39/1000 | Loss: 0.00004724
Iteration 40/1000 | Loss: 0.00003340
Iteration 41/1000 | Loss: 0.00005483
Iteration 42/1000 | Loss: 0.00005630
Iteration 43/1000 | Loss: 0.00006155
Iteration 44/1000 | Loss: 0.00016146
Iteration 45/1000 | Loss: 0.00003165
Iteration 46/1000 | Loss: 0.00004132
Iteration 47/1000 | Loss: 0.00004702
Iteration 48/1000 | Loss: 0.00003750
Iteration 49/1000 | Loss: 0.00004287
Iteration 50/1000 | Loss: 0.00003185
Iteration 51/1000 | Loss: 0.00004239
Iteration 52/1000 | Loss: 0.00005128
Iteration 53/1000 | Loss: 0.00003173
Iteration 54/1000 | Loss: 0.00003971
Iteration 55/1000 | Loss: 0.00002844
Iteration 56/1000 | Loss: 0.00002834
Iteration 57/1000 | Loss: 0.00002815
Iteration 58/1000 | Loss: 0.00002786
Iteration 59/1000 | Loss: 0.00002772
Iteration 60/1000 | Loss: 0.00002765
Iteration 61/1000 | Loss: 0.00002760
Iteration 62/1000 | Loss: 0.00003622
Iteration 63/1000 | Loss: 0.00003276
Iteration 64/1000 | Loss: 0.00004112
Iteration 65/1000 | Loss: 0.00002743
Iteration 66/1000 | Loss: 0.00002739
Iteration 67/1000 | Loss: 0.00002739
Iteration 68/1000 | Loss: 0.00002738
Iteration 69/1000 | Loss: 0.00002738
Iteration 70/1000 | Loss: 0.00002738
Iteration 71/1000 | Loss: 0.00002737
Iteration 72/1000 | Loss: 0.00002737
Iteration 73/1000 | Loss: 0.00002737
Iteration 74/1000 | Loss: 0.00002737
Iteration 75/1000 | Loss: 0.00002736
Iteration 76/1000 | Loss: 0.00002736
Iteration 77/1000 | Loss: 0.00002736
Iteration 78/1000 | Loss: 0.00002736
Iteration 79/1000 | Loss: 0.00002736
Iteration 80/1000 | Loss: 0.00002736
Iteration 81/1000 | Loss: 0.00002736
Iteration 82/1000 | Loss: 0.00002736
Iteration 83/1000 | Loss: 0.00002736
Iteration 84/1000 | Loss: 0.00002736
Iteration 85/1000 | Loss: 0.00002736
Iteration 86/1000 | Loss: 0.00002735
Iteration 87/1000 | Loss: 0.00002735
Iteration 88/1000 | Loss: 0.00002735
Iteration 89/1000 | Loss: 0.00002735
Iteration 90/1000 | Loss: 0.00002735
Iteration 91/1000 | Loss: 0.00002735
Iteration 92/1000 | Loss: 0.00002734
Iteration 93/1000 | Loss: 0.00002734
Iteration 94/1000 | Loss: 0.00002734
Iteration 95/1000 | Loss: 0.00002734
Iteration 96/1000 | Loss: 0.00002734
Iteration 97/1000 | Loss: 0.00002734
Iteration 98/1000 | Loss: 0.00002733
Iteration 99/1000 | Loss: 0.00002733
Iteration 100/1000 | Loss: 0.00002733
Iteration 101/1000 | Loss: 0.00002733
Iteration 102/1000 | Loss: 0.00002733
Iteration 103/1000 | Loss: 0.00002733
Iteration 104/1000 | Loss: 0.00002733
Iteration 105/1000 | Loss: 0.00002733
Iteration 106/1000 | Loss: 0.00002732
Iteration 107/1000 | Loss: 0.00002732
Iteration 108/1000 | Loss: 0.00002732
Iteration 109/1000 | Loss: 0.00002732
Iteration 110/1000 | Loss: 0.00002732
Iteration 111/1000 | Loss: 0.00002732
Iteration 112/1000 | Loss: 0.00002732
Iteration 113/1000 | Loss: 0.00002732
Iteration 114/1000 | Loss: 0.00002732
Iteration 115/1000 | Loss: 0.00002731
Iteration 116/1000 | Loss: 0.00002731
Iteration 117/1000 | Loss: 0.00002731
Iteration 118/1000 | Loss: 0.00002731
Iteration 119/1000 | Loss: 0.00002731
Iteration 120/1000 | Loss: 0.00002730
Iteration 121/1000 | Loss: 0.00002730
Iteration 122/1000 | Loss: 0.00002730
Iteration 123/1000 | Loss: 0.00004740
Iteration 124/1000 | Loss: 0.00002734
Iteration 125/1000 | Loss: 0.00002725
Iteration 126/1000 | Loss: 0.00002725
Iteration 127/1000 | Loss: 0.00002725
Iteration 128/1000 | Loss: 0.00002725
Iteration 129/1000 | Loss: 0.00002725
Iteration 130/1000 | Loss: 0.00002725
Iteration 131/1000 | Loss: 0.00002725
Iteration 132/1000 | Loss: 0.00002725
Iteration 133/1000 | Loss: 0.00002724
Iteration 134/1000 | Loss: 0.00002724
Iteration 135/1000 | Loss: 0.00002724
Iteration 136/1000 | Loss: 0.00002724
Iteration 137/1000 | Loss: 0.00002724
Iteration 138/1000 | Loss: 0.00002724
Iteration 139/1000 | Loss: 0.00002724
Iteration 140/1000 | Loss: 0.00002724
Iteration 141/1000 | Loss: 0.00002724
Iteration 142/1000 | Loss: 0.00002724
Iteration 143/1000 | Loss: 0.00002724
Iteration 144/1000 | Loss: 0.00002724
Iteration 145/1000 | Loss: 0.00002724
Iteration 146/1000 | Loss: 0.00002724
Iteration 147/1000 | Loss: 0.00002724
Iteration 148/1000 | Loss: 0.00002724
Iteration 149/1000 | Loss: 0.00002724
Iteration 150/1000 | Loss: 0.00002724
Iteration 151/1000 | Loss: 0.00002724
Iteration 152/1000 | Loss: 0.00002724
Iteration 153/1000 | Loss: 0.00002724
Iteration 154/1000 | Loss: 0.00002724
Iteration 155/1000 | Loss: 0.00002724
Iteration 156/1000 | Loss: 0.00002724
Iteration 157/1000 | Loss: 0.00002724
Iteration 158/1000 | Loss: 0.00002724
Iteration 159/1000 | Loss: 0.00002724
Iteration 160/1000 | Loss: 0.00002724
Iteration 161/1000 | Loss: 0.00002724
Iteration 162/1000 | Loss: 0.00002724
Iteration 163/1000 | Loss: 0.00002724
Iteration 164/1000 | Loss: 0.00002724
Iteration 165/1000 | Loss: 0.00002724
Iteration 166/1000 | Loss: 0.00002724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.7235355446464382e-05, 2.7235355446464382e-05, 2.7235355446464382e-05, 2.7235355446464382e-05, 2.7235355446464382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7235355446464382e-05

Optimization complete. Final v2v error: 3.367403745651245 mm

Highest mean error: 11.298357963562012 mm for frame 0

Lowest mean error: 2.1402528285980225 mm for frame 167

Saving results

Total time: 156.47503280639648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874323
Iteration 2/25 | Loss: 0.00106473
Iteration 3/25 | Loss: 0.00092141
Iteration 4/25 | Loss: 0.00090581
Iteration 5/25 | Loss: 0.00090203
Iteration 6/25 | Loss: 0.00090118
Iteration 7/25 | Loss: 0.00090118
Iteration 8/25 | Loss: 0.00090118
Iteration 9/25 | Loss: 0.00090118
Iteration 10/25 | Loss: 0.00090118
Iteration 11/25 | Loss: 0.00090118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009011794463731349, 0.0009011794463731349, 0.0009011794463731349, 0.0009011794463731349, 0.0009011794463731349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009011794463731349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34060657
Iteration 2/25 | Loss: 0.00051008
Iteration 3/25 | Loss: 0.00051007
Iteration 4/25 | Loss: 0.00051006
Iteration 5/25 | Loss: 0.00051006
Iteration 6/25 | Loss: 0.00051006
Iteration 7/25 | Loss: 0.00051006
Iteration 8/25 | Loss: 0.00051006
Iteration 9/25 | Loss: 0.00051006
Iteration 10/25 | Loss: 0.00051006
Iteration 11/25 | Loss: 0.00051006
Iteration 12/25 | Loss: 0.00051006
Iteration 13/25 | Loss: 0.00051006
Iteration 14/25 | Loss: 0.00051006
Iteration 15/25 | Loss: 0.00051006
Iteration 16/25 | Loss: 0.00051006
Iteration 17/25 | Loss: 0.00051006
Iteration 18/25 | Loss: 0.00051006
Iteration 19/25 | Loss: 0.00051006
Iteration 20/25 | Loss: 0.00051006
Iteration 21/25 | Loss: 0.00051006
Iteration 22/25 | Loss: 0.00051006
Iteration 23/25 | Loss: 0.00051006
Iteration 24/25 | Loss: 0.00051006
Iteration 25/25 | Loss: 0.00051006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051006
Iteration 2/1000 | Loss: 0.00001785
Iteration 3/1000 | Loss: 0.00001204
Iteration 4/1000 | Loss: 0.00001007
Iteration 5/1000 | Loss: 0.00000927
Iteration 6/1000 | Loss: 0.00000863
Iteration 7/1000 | Loss: 0.00000836
Iteration 8/1000 | Loss: 0.00000815
Iteration 9/1000 | Loss: 0.00000814
Iteration 10/1000 | Loss: 0.00000812
Iteration 11/1000 | Loss: 0.00000812
Iteration 12/1000 | Loss: 0.00000811
Iteration 13/1000 | Loss: 0.00000801
Iteration 14/1000 | Loss: 0.00000797
Iteration 15/1000 | Loss: 0.00000797
Iteration 16/1000 | Loss: 0.00000796
Iteration 17/1000 | Loss: 0.00000796
Iteration 18/1000 | Loss: 0.00000796
Iteration 19/1000 | Loss: 0.00000795
Iteration 20/1000 | Loss: 0.00000795
Iteration 21/1000 | Loss: 0.00000795
Iteration 22/1000 | Loss: 0.00000794
Iteration 23/1000 | Loss: 0.00000794
Iteration 24/1000 | Loss: 0.00000792
Iteration 25/1000 | Loss: 0.00000792
Iteration 26/1000 | Loss: 0.00000791
Iteration 27/1000 | Loss: 0.00000791
Iteration 28/1000 | Loss: 0.00000791
Iteration 29/1000 | Loss: 0.00000791
Iteration 30/1000 | Loss: 0.00000790
Iteration 31/1000 | Loss: 0.00000789
Iteration 32/1000 | Loss: 0.00000788
Iteration 33/1000 | Loss: 0.00000788
Iteration 34/1000 | Loss: 0.00000787
Iteration 35/1000 | Loss: 0.00000787
Iteration 36/1000 | Loss: 0.00000786
Iteration 37/1000 | Loss: 0.00000785
Iteration 38/1000 | Loss: 0.00000785
Iteration 39/1000 | Loss: 0.00000784
Iteration 40/1000 | Loss: 0.00000783
Iteration 41/1000 | Loss: 0.00000782
Iteration 42/1000 | Loss: 0.00000782
Iteration 43/1000 | Loss: 0.00000781
Iteration 44/1000 | Loss: 0.00000781
Iteration 45/1000 | Loss: 0.00000781
Iteration 46/1000 | Loss: 0.00000780
Iteration 47/1000 | Loss: 0.00000779
Iteration 48/1000 | Loss: 0.00000778
Iteration 49/1000 | Loss: 0.00000777
Iteration 50/1000 | Loss: 0.00000777
Iteration 51/1000 | Loss: 0.00000776
Iteration 52/1000 | Loss: 0.00000776
Iteration 53/1000 | Loss: 0.00000776
Iteration 54/1000 | Loss: 0.00000776
Iteration 55/1000 | Loss: 0.00000775
Iteration 56/1000 | Loss: 0.00000775
Iteration 57/1000 | Loss: 0.00000775
Iteration 58/1000 | Loss: 0.00000775
Iteration 59/1000 | Loss: 0.00000774
Iteration 60/1000 | Loss: 0.00000774
Iteration 61/1000 | Loss: 0.00000774
Iteration 62/1000 | Loss: 0.00000774
Iteration 63/1000 | Loss: 0.00000773
Iteration 64/1000 | Loss: 0.00000773
Iteration 65/1000 | Loss: 0.00000773
Iteration 66/1000 | Loss: 0.00000773
Iteration 67/1000 | Loss: 0.00000773
Iteration 68/1000 | Loss: 0.00000773
Iteration 69/1000 | Loss: 0.00000773
Iteration 70/1000 | Loss: 0.00000773
Iteration 71/1000 | Loss: 0.00000772
Iteration 72/1000 | Loss: 0.00000772
Iteration 73/1000 | Loss: 0.00000772
Iteration 74/1000 | Loss: 0.00000772
Iteration 75/1000 | Loss: 0.00000772
Iteration 76/1000 | Loss: 0.00000772
Iteration 77/1000 | Loss: 0.00000772
Iteration 78/1000 | Loss: 0.00000772
Iteration 79/1000 | Loss: 0.00000771
Iteration 80/1000 | Loss: 0.00000771
Iteration 81/1000 | Loss: 0.00000770
Iteration 82/1000 | Loss: 0.00000770
Iteration 83/1000 | Loss: 0.00000770
Iteration 84/1000 | Loss: 0.00000770
Iteration 85/1000 | Loss: 0.00000770
Iteration 86/1000 | Loss: 0.00000770
Iteration 87/1000 | Loss: 0.00000770
Iteration 88/1000 | Loss: 0.00000770
Iteration 89/1000 | Loss: 0.00000770
Iteration 90/1000 | Loss: 0.00000769
Iteration 91/1000 | Loss: 0.00000769
Iteration 92/1000 | Loss: 0.00000769
Iteration 93/1000 | Loss: 0.00000769
Iteration 94/1000 | Loss: 0.00000769
Iteration 95/1000 | Loss: 0.00000769
Iteration 96/1000 | Loss: 0.00000769
Iteration 97/1000 | Loss: 0.00000769
Iteration 98/1000 | Loss: 0.00000769
Iteration 99/1000 | Loss: 0.00000769
Iteration 100/1000 | Loss: 0.00000769
Iteration 101/1000 | Loss: 0.00000769
Iteration 102/1000 | Loss: 0.00000769
Iteration 103/1000 | Loss: 0.00000768
Iteration 104/1000 | Loss: 0.00000768
Iteration 105/1000 | Loss: 0.00000768
Iteration 106/1000 | Loss: 0.00000768
Iteration 107/1000 | Loss: 0.00000768
Iteration 108/1000 | Loss: 0.00000768
Iteration 109/1000 | Loss: 0.00000768
Iteration 110/1000 | Loss: 0.00000768
Iteration 111/1000 | Loss: 0.00000768
Iteration 112/1000 | Loss: 0.00000768
Iteration 113/1000 | Loss: 0.00000768
Iteration 114/1000 | Loss: 0.00000768
Iteration 115/1000 | Loss: 0.00000768
Iteration 116/1000 | Loss: 0.00000768
Iteration 117/1000 | Loss: 0.00000768
Iteration 118/1000 | Loss: 0.00000768
Iteration 119/1000 | Loss: 0.00000768
Iteration 120/1000 | Loss: 0.00000768
Iteration 121/1000 | Loss: 0.00000768
Iteration 122/1000 | Loss: 0.00000767
Iteration 123/1000 | Loss: 0.00000767
Iteration 124/1000 | Loss: 0.00000767
Iteration 125/1000 | Loss: 0.00000767
Iteration 126/1000 | Loss: 0.00000767
Iteration 127/1000 | Loss: 0.00000767
Iteration 128/1000 | Loss: 0.00000767
Iteration 129/1000 | Loss: 0.00000767
Iteration 130/1000 | Loss: 0.00000767
Iteration 131/1000 | Loss: 0.00000767
Iteration 132/1000 | Loss: 0.00000767
Iteration 133/1000 | Loss: 0.00000767
Iteration 134/1000 | Loss: 0.00000767
Iteration 135/1000 | Loss: 0.00000767
Iteration 136/1000 | Loss: 0.00000767
Iteration 137/1000 | Loss: 0.00000767
Iteration 138/1000 | Loss: 0.00000767
Iteration 139/1000 | Loss: 0.00000767
Iteration 140/1000 | Loss: 0.00000767
Iteration 141/1000 | Loss: 0.00000767
Iteration 142/1000 | Loss: 0.00000767
Iteration 143/1000 | Loss: 0.00000766
Iteration 144/1000 | Loss: 0.00000766
Iteration 145/1000 | Loss: 0.00000766
Iteration 146/1000 | Loss: 0.00000766
Iteration 147/1000 | Loss: 0.00000766
Iteration 148/1000 | Loss: 0.00000766
Iteration 149/1000 | Loss: 0.00000766
Iteration 150/1000 | Loss: 0.00000766
Iteration 151/1000 | Loss: 0.00000766
Iteration 152/1000 | Loss: 0.00000766
Iteration 153/1000 | Loss: 0.00000766
Iteration 154/1000 | Loss: 0.00000766
Iteration 155/1000 | Loss: 0.00000766
Iteration 156/1000 | Loss: 0.00000766
Iteration 157/1000 | Loss: 0.00000766
Iteration 158/1000 | Loss: 0.00000766
Iteration 159/1000 | Loss: 0.00000766
Iteration 160/1000 | Loss: 0.00000766
Iteration 161/1000 | Loss: 0.00000766
Iteration 162/1000 | Loss: 0.00000766
Iteration 163/1000 | Loss: 0.00000766
Iteration 164/1000 | Loss: 0.00000766
Iteration 165/1000 | Loss: 0.00000766
Iteration 166/1000 | Loss: 0.00000766
Iteration 167/1000 | Loss: 0.00000766
Iteration 168/1000 | Loss: 0.00000766
Iteration 169/1000 | Loss: 0.00000766
Iteration 170/1000 | Loss: 0.00000766
Iteration 171/1000 | Loss: 0.00000766
Iteration 172/1000 | Loss: 0.00000766
Iteration 173/1000 | Loss: 0.00000766
Iteration 174/1000 | Loss: 0.00000766
Iteration 175/1000 | Loss: 0.00000766
Iteration 176/1000 | Loss: 0.00000766
Iteration 177/1000 | Loss: 0.00000766
Iteration 178/1000 | Loss: 0.00000766
Iteration 179/1000 | Loss: 0.00000766
Iteration 180/1000 | Loss: 0.00000766
Iteration 181/1000 | Loss: 0.00000766
Iteration 182/1000 | Loss: 0.00000766
Iteration 183/1000 | Loss: 0.00000766
Iteration 184/1000 | Loss: 0.00000766
Iteration 185/1000 | Loss: 0.00000766
Iteration 186/1000 | Loss: 0.00000766
Iteration 187/1000 | Loss: 0.00000766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [7.661247764190193e-06, 7.661247764190193e-06, 7.661247764190193e-06, 7.661247764190193e-06, 7.661247764190193e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.661247764190193e-06

Optimization complete. Final v2v error: 2.3825020790100098 mm

Highest mean error: 2.5247254371643066 mm for frame 72

Lowest mean error: 2.28608775138855 mm for frame 40

Saving results

Total time: 32.991610288619995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875222
Iteration 2/25 | Loss: 0.00120881
Iteration 3/25 | Loss: 0.00103900
Iteration 4/25 | Loss: 0.00100597
Iteration 5/25 | Loss: 0.00099292
Iteration 6/25 | Loss: 0.00098940
Iteration 7/25 | Loss: 0.00098868
Iteration 8/25 | Loss: 0.00098868
Iteration 9/25 | Loss: 0.00098868
Iteration 10/25 | Loss: 0.00098868
Iteration 11/25 | Loss: 0.00098868
Iteration 12/25 | Loss: 0.00098868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000988675281405449, 0.000988675281405449, 0.000988675281405449, 0.000988675281405449, 0.000988675281405449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000988675281405449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48392391
Iteration 2/25 | Loss: 0.00074651
Iteration 3/25 | Loss: 0.00074651
Iteration 4/25 | Loss: 0.00074651
Iteration 5/25 | Loss: 0.00074651
Iteration 6/25 | Loss: 0.00074650
Iteration 7/25 | Loss: 0.00074650
Iteration 8/25 | Loss: 0.00074650
Iteration 9/25 | Loss: 0.00074650
Iteration 10/25 | Loss: 0.00074650
Iteration 11/25 | Loss: 0.00074650
Iteration 12/25 | Loss: 0.00074650
Iteration 13/25 | Loss: 0.00074650
Iteration 14/25 | Loss: 0.00074650
Iteration 15/25 | Loss: 0.00074650
Iteration 16/25 | Loss: 0.00074650
Iteration 17/25 | Loss: 0.00074650
Iteration 18/25 | Loss: 0.00074650
Iteration 19/25 | Loss: 0.00074650
Iteration 20/25 | Loss: 0.00074650
Iteration 21/25 | Loss: 0.00074650
Iteration 22/25 | Loss: 0.00074650
Iteration 23/25 | Loss: 0.00074650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000746503530535847, 0.000746503530535847, 0.000746503530535847, 0.000746503530535847, 0.000746503530535847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000746503530535847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074650
Iteration 2/1000 | Loss: 0.00004809
Iteration 3/1000 | Loss: 0.00002909
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001878
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001811
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001761
Iteration 15/1000 | Loss: 0.00001760
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001744
Iteration 38/1000 | Loss: 0.00001744
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001740
Iteration 64/1000 | Loss: 0.00001740
Iteration 65/1000 | Loss: 0.00001740
Iteration 66/1000 | Loss: 0.00001739
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001739
Iteration 69/1000 | Loss: 0.00001739
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001739
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001739
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001738
Iteration 82/1000 | Loss: 0.00001738
Iteration 83/1000 | Loss: 0.00001737
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001736
Iteration 87/1000 | Loss: 0.00001736
Iteration 88/1000 | Loss: 0.00001736
Iteration 89/1000 | Loss: 0.00001735
Iteration 90/1000 | Loss: 0.00001735
Iteration 91/1000 | Loss: 0.00001735
Iteration 92/1000 | Loss: 0.00001735
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001734
Iteration 96/1000 | Loss: 0.00001734
Iteration 97/1000 | Loss: 0.00001734
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001731
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001731
Iteration 111/1000 | Loss: 0.00001731
Iteration 112/1000 | Loss: 0.00001730
Iteration 113/1000 | Loss: 0.00001730
Iteration 114/1000 | Loss: 0.00001730
Iteration 115/1000 | Loss: 0.00001730
Iteration 116/1000 | Loss: 0.00001730
Iteration 117/1000 | Loss: 0.00001730
Iteration 118/1000 | Loss: 0.00001730
Iteration 119/1000 | Loss: 0.00001730
Iteration 120/1000 | Loss: 0.00001730
Iteration 121/1000 | Loss: 0.00001730
Iteration 122/1000 | Loss: 0.00001730
Iteration 123/1000 | Loss: 0.00001729
Iteration 124/1000 | Loss: 0.00001729
Iteration 125/1000 | Loss: 0.00001729
Iteration 126/1000 | Loss: 0.00001729
Iteration 127/1000 | Loss: 0.00001729
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001729
Iteration 130/1000 | Loss: 0.00001729
Iteration 131/1000 | Loss: 0.00001729
Iteration 132/1000 | Loss: 0.00001729
Iteration 133/1000 | Loss: 0.00001728
Iteration 134/1000 | Loss: 0.00001728
Iteration 135/1000 | Loss: 0.00001728
Iteration 136/1000 | Loss: 0.00001728
Iteration 137/1000 | Loss: 0.00001728
Iteration 138/1000 | Loss: 0.00001728
Iteration 139/1000 | Loss: 0.00001728
Iteration 140/1000 | Loss: 0.00001728
Iteration 141/1000 | Loss: 0.00001728
Iteration 142/1000 | Loss: 0.00001727
Iteration 143/1000 | Loss: 0.00001727
Iteration 144/1000 | Loss: 0.00001727
Iteration 145/1000 | Loss: 0.00001727
Iteration 146/1000 | Loss: 0.00001727
Iteration 147/1000 | Loss: 0.00001727
Iteration 148/1000 | Loss: 0.00001727
Iteration 149/1000 | Loss: 0.00001727
Iteration 150/1000 | Loss: 0.00001727
Iteration 151/1000 | Loss: 0.00001727
Iteration 152/1000 | Loss: 0.00001727
Iteration 153/1000 | Loss: 0.00001727
Iteration 154/1000 | Loss: 0.00001726
Iteration 155/1000 | Loss: 0.00001726
Iteration 156/1000 | Loss: 0.00001726
Iteration 157/1000 | Loss: 0.00001726
Iteration 158/1000 | Loss: 0.00001726
Iteration 159/1000 | Loss: 0.00001726
Iteration 160/1000 | Loss: 0.00001726
Iteration 161/1000 | Loss: 0.00001726
Iteration 162/1000 | Loss: 0.00001726
Iteration 163/1000 | Loss: 0.00001726
Iteration 164/1000 | Loss: 0.00001726
Iteration 165/1000 | Loss: 0.00001726
Iteration 166/1000 | Loss: 0.00001726
Iteration 167/1000 | Loss: 0.00001726
Iteration 168/1000 | Loss: 0.00001726
Iteration 169/1000 | Loss: 0.00001725
Iteration 170/1000 | Loss: 0.00001725
Iteration 171/1000 | Loss: 0.00001725
Iteration 172/1000 | Loss: 0.00001725
Iteration 173/1000 | Loss: 0.00001725
Iteration 174/1000 | Loss: 0.00001724
Iteration 175/1000 | Loss: 0.00001724
Iteration 176/1000 | Loss: 0.00001724
Iteration 177/1000 | Loss: 0.00001724
Iteration 178/1000 | Loss: 0.00001724
Iteration 179/1000 | Loss: 0.00001724
Iteration 180/1000 | Loss: 0.00001724
Iteration 181/1000 | Loss: 0.00001724
Iteration 182/1000 | Loss: 0.00001723
Iteration 183/1000 | Loss: 0.00001723
Iteration 184/1000 | Loss: 0.00001723
Iteration 185/1000 | Loss: 0.00001723
Iteration 186/1000 | Loss: 0.00001723
Iteration 187/1000 | Loss: 0.00001723
Iteration 188/1000 | Loss: 0.00001723
Iteration 189/1000 | Loss: 0.00001723
Iteration 190/1000 | Loss: 0.00001723
Iteration 191/1000 | Loss: 0.00001723
Iteration 192/1000 | Loss: 0.00001723
Iteration 193/1000 | Loss: 0.00001723
Iteration 194/1000 | Loss: 0.00001723
Iteration 195/1000 | Loss: 0.00001723
Iteration 196/1000 | Loss: 0.00001723
Iteration 197/1000 | Loss: 0.00001723
Iteration 198/1000 | Loss: 0.00001723
Iteration 199/1000 | Loss: 0.00001723
Iteration 200/1000 | Loss: 0.00001723
Iteration 201/1000 | Loss: 0.00001723
Iteration 202/1000 | Loss: 0.00001723
Iteration 203/1000 | Loss: 0.00001723
Iteration 204/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.7233389371540397e-05, 1.7233389371540397e-05, 1.7233389371540397e-05, 1.7233389371540397e-05, 1.7233389371540397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7233389371540397e-05

Optimization complete. Final v2v error: 3.353391170501709 mm

Highest mean error: 5.76972770690918 mm for frame 59

Lowest mean error: 2.46579647064209 mm for frame 81

Saving results

Total time: 42.10333967208862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822738
Iteration 2/25 | Loss: 0.00123091
Iteration 3/25 | Loss: 0.00100844
Iteration 4/25 | Loss: 0.00097590
Iteration 5/25 | Loss: 0.00096869
Iteration 6/25 | Loss: 0.00096684
Iteration 7/25 | Loss: 0.00096652
Iteration 8/25 | Loss: 0.00096652
Iteration 9/25 | Loss: 0.00096652
Iteration 10/25 | Loss: 0.00096652
Iteration 11/25 | Loss: 0.00096652
Iteration 12/25 | Loss: 0.00096652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009665211546234787, 0.0009665211546234787, 0.0009665211546234787, 0.0009665211546234787, 0.0009665211546234787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009665211546234787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23186266
Iteration 2/25 | Loss: 0.00050687
Iteration 3/25 | Loss: 0.00050685
Iteration 4/25 | Loss: 0.00050685
Iteration 5/25 | Loss: 0.00050685
Iteration 6/25 | Loss: 0.00050685
Iteration 7/25 | Loss: 0.00050685
Iteration 8/25 | Loss: 0.00050685
Iteration 9/25 | Loss: 0.00050685
Iteration 10/25 | Loss: 0.00050685
Iteration 11/25 | Loss: 0.00050685
Iteration 12/25 | Loss: 0.00050685
Iteration 13/25 | Loss: 0.00050685
Iteration 14/25 | Loss: 0.00050685
Iteration 15/25 | Loss: 0.00050685
Iteration 16/25 | Loss: 0.00050685
Iteration 17/25 | Loss: 0.00050685
Iteration 18/25 | Loss: 0.00050685
Iteration 19/25 | Loss: 0.00050685
Iteration 20/25 | Loss: 0.00050685
Iteration 21/25 | Loss: 0.00050685
Iteration 22/25 | Loss: 0.00050685
Iteration 23/25 | Loss: 0.00050685
Iteration 24/25 | Loss: 0.00050685
Iteration 25/25 | Loss: 0.00050685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050685
Iteration 2/1000 | Loss: 0.00003261
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001406
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001168
Iteration 13/1000 | Loss: 0.00001162
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001149
Iteration 24/1000 | Loss: 0.00001149
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001148
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001147
Iteration 29/1000 | Loss: 0.00001146
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001144
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001143
Iteration 36/1000 | Loss: 0.00001143
Iteration 37/1000 | Loss: 0.00001141
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001138
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001137
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001136
Iteration 51/1000 | Loss: 0.00001136
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001136
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001135
Iteration 57/1000 | Loss: 0.00001135
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001134
Iteration 62/1000 | Loss: 0.00001134
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001133
Iteration 65/1000 | Loss: 0.00001133
Iteration 66/1000 | Loss: 0.00001133
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001130
Iteration 87/1000 | Loss: 0.00001130
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001130
Iteration 93/1000 | Loss: 0.00001130
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001129
Iteration 100/1000 | Loss: 0.00001129
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001128
Iteration 104/1000 | Loss: 0.00001128
Iteration 105/1000 | Loss: 0.00001128
Iteration 106/1000 | Loss: 0.00001128
Iteration 107/1000 | Loss: 0.00001128
Iteration 108/1000 | Loss: 0.00001128
Iteration 109/1000 | Loss: 0.00001128
Iteration 110/1000 | Loss: 0.00001128
Iteration 111/1000 | Loss: 0.00001128
Iteration 112/1000 | Loss: 0.00001128
Iteration 113/1000 | Loss: 0.00001128
Iteration 114/1000 | Loss: 0.00001128
Iteration 115/1000 | Loss: 0.00001128
Iteration 116/1000 | Loss: 0.00001128
Iteration 117/1000 | Loss: 0.00001128
Iteration 118/1000 | Loss: 0.00001128
Iteration 119/1000 | Loss: 0.00001128
Iteration 120/1000 | Loss: 0.00001127
Iteration 121/1000 | Loss: 0.00001127
Iteration 122/1000 | Loss: 0.00001127
Iteration 123/1000 | Loss: 0.00001127
Iteration 124/1000 | Loss: 0.00001127
Iteration 125/1000 | Loss: 0.00001127
Iteration 126/1000 | Loss: 0.00001127
Iteration 127/1000 | Loss: 0.00001127
Iteration 128/1000 | Loss: 0.00001127
Iteration 129/1000 | Loss: 0.00001127
Iteration 130/1000 | Loss: 0.00001127
Iteration 131/1000 | Loss: 0.00001127
Iteration 132/1000 | Loss: 0.00001127
Iteration 133/1000 | Loss: 0.00001127
Iteration 134/1000 | Loss: 0.00001127
Iteration 135/1000 | Loss: 0.00001127
Iteration 136/1000 | Loss: 0.00001126
Iteration 137/1000 | Loss: 0.00001126
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001126
Iteration 142/1000 | Loss: 0.00001126
Iteration 143/1000 | Loss: 0.00001126
Iteration 144/1000 | Loss: 0.00001126
Iteration 145/1000 | Loss: 0.00001126
Iteration 146/1000 | Loss: 0.00001126
Iteration 147/1000 | Loss: 0.00001126
Iteration 148/1000 | Loss: 0.00001126
Iteration 149/1000 | Loss: 0.00001126
Iteration 150/1000 | Loss: 0.00001126
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001126
Iteration 167/1000 | Loss: 0.00001126
Iteration 168/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.1263582564424723e-05, 1.1263582564424723e-05, 1.1263582564424723e-05, 1.1263582564424723e-05, 1.1263582564424723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1263582564424723e-05

Optimization complete. Final v2v error: 2.86857271194458 mm

Highest mean error: 3.2016992568969727 mm for frame 105

Lowest mean error: 2.3955507278442383 mm for frame 63

Saving results

Total time: 37.703112840652466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795774
Iteration 2/25 | Loss: 0.00116434
Iteration 3/25 | Loss: 0.00099697
Iteration 4/25 | Loss: 0.00098193
Iteration 5/25 | Loss: 0.00097677
Iteration 6/25 | Loss: 0.00097599
Iteration 7/25 | Loss: 0.00097599
Iteration 8/25 | Loss: 0.00097599
Iteration 9/25 | Loss: 0.00097599
Iteration 10/25 | Loss: 0.00097599
Iteration 11/25 | Loss: 0.00097599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00097599474247545, 0.00097599474247545, 0.00097599474247545, 0.00097599474247545, 0.00097599474247545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00097599474247545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28575850
Iteration 2/25 | Loss: 0.00072115
Iteration 3/25 | Loss: 0.00072113
Iteration 4/25 | Loss: 0.00072112
Iteration 5/25 | Loss: 0.00072112
Iteration 6/25 | Loss: 0.00072112
Iteration 7/25 | Loss: 0.00072112
Iteration 8/25 | Loss: 0.00072112
Iteration 9/25 | Loss: 0.00072112
Iteration 10/25 | Loss: 0.00072112
Iteration 11/25 | Loss: 0.00072112
Iteration 12/25 | Loss: 0.00072112
Iteration 13/25 | Loss: 0.00072112
Iteration 14/25 | Loss: 0.00072112
Iteration 15/25 | Loss: 0.00072112
Iteration 16/25 | Loss: 0.00072112
Iteration 17/25 | Loss: 0.00072112
Iteration 18/25 | Loss: 0.00072112
Iteration 19/25 | Loss: 0.00072112
Iteration 20/25 | Loss: 0.00072112
Iteration 21/25 | Loss: 0.00072112
Iteration 22/25 | Loss: 0.00072112
Iteration 23/25 | Loss: 0.00072112
Iteration 24/25 | Loss: 0.00072112
Iteration 25/25 | Loss: 0.00072112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072112
Iteration 2/1000 | Loss: 0.00003042
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001215
Iteration 9/1000 | Loss: 0.00001190
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001141
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001136
Iteration 64/1000 | Loss: 0.00001136
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001135
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001129
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001125
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001125
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001124
Iteration 141/1000 | Loss: 0.00001124
Iteration 142/1000 | Loss: 0.00001124
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001124
Iteration 145/1000 | Loss: 0.00001124
Iteration 146/1000 | Loss: 0.00001124
Iteration 147/1000 | Loss: 0.00001124
Iteration 148/1000 | Loss: 0.00001124
Iteration 149/1000 | Loss: 0.00001124
Iteration 150/1000 | Loss: 0.00001124
Iteration 151/1000 | Loss: 0.00001124
Iteration 152/1000 | Loss: 0.00001124
Iteration 153/1000 | Loss: 0.00001124
Iteration 154/1000 | Loss: 0.00001124
Iteration 155/1000 | Loss: 0.00001124
Iteration 156/1000 | Loss: 0.00001124
Iteration 157/1000 | Loss: 0.00001124
Iteration 158/1000 | Loss: 0.00001124
Iteration 159/1000 | Loss: 0.00001124
Iteration 160/1000 | Loss: 0.00001124
Iteration 161/1000 | Loss: 0.00001124
Iteration 162/1000 | Loss: 0.00001124
Iteration 163/1000 | Loss: 0.00001124
Iteration 164/1000 | Loss: 0.00001124
Iteration 165/1000 | Loss: 0.00001124
Iteration 166/1000 | Loss: 0.00001124
Iteration 167/1000 | Loss: 0.00001124
Iteration 168/1000 | Loss: 0.00001124
Iteration 169/1000 | Loss: 0.00001124
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001124
Iteration 177/1000 | Loss: 0.00001124
Iteration 178/1000 | Loss: 0.00001124
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001124
Iteration 181/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1239822015340906e-05, 1.1239822015340906e-05, 1.1239822015340906e-05, 1.1239822015340906e-05, 1.1239822015340906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1239822015340906e-05

Optimization complete. Final v2v error: 2.8731133937835693 mm

Highest mean error: 3.340395927429199 mm for frame 210

Lowest mean error: 2.626842975616455 mm for frame 62

Saving results

Total time: 39.84970951080322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875227
Iteration 2/25 | Loss: 0.00133792
Iteration 3/25 | Loss: 0.00105244
Iteration 4/25 | Loss: 0.00101095
Iteration 5/25 | Loss: 0.00100299
Iteration 6/25 | Loss: 0.00100158
Iteration 7/25 | Loss: 0.00100153
Iteration 8/25 | Loss: 0.00100153
Iteration 9/25 | Loss: 0.00100153
Iteration 10/25 | Loss: 0.00100153
Iteration 11/25 | Loss: 0.00100153
Iteration 12/25 | Loss: 0.00100153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010015267180278897, 0.0010015267180278897, 0.0010015267180278897, 0.0010015267180278897, 0.0010015267180278897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010015267180278897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93763441
Iteration 2/25 | Loss: 0.00034655
Iteration 3/25 | Loss: 0.00034654
Iteration 4/25 | Loss: 0.00034654
Iteration 5/25 | Loss: 0.00034654
Iteration 6/25 | Loss: 0.00034654
Iteration 7/25 | Loss: 0.00034654
Iteration 8/25 | Loss: 0.00034654
Iteration 9/25 | Loss: 0.00034654
Iteration 10/25 | Loss: 0.00034654
Iteration 11/25 | Loss: 0.00034653
Iteration 12/25 | Loss: 0.00034653
Iteration 13/25 | Loss: 0.00034653
Iteration 14/25 | Loss: 0.00034653
Iteration 15/25 | Loss: 0.00034653
Iteration 16/25 | Loss: 0.00034653
Iteration 17/25 | Loss: 0.00034653
Iteration 18/25 | Loss: 0.00034653
Iteration 19/25 | Loss: 0.00034653
Iteration 20/25 | Loss: 0.00034653
Iteration 21/25 | Loss: 0.00034653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003465348854660988, 0.0003465348854660988, 0.0003465348854660988, 0.0003465348854660988, 0.0003465348854660988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003465348854660988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034653
Iteration 2/1000 | Loss: 0.00003755
Iteration 3/1000 | Loss: 0.00002875
Iteration 4/1000 | Loss: 0.00002539
Iteration 5/1000 | Loss: 0.00002349
Iteration 6/1000 | Loss: 0.00002154
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00002031
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001970
Iteration 11/1000 | Loss: 0.00001957
Iteration 12/1000 | Loss: 0.00001951
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001937
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001936
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001936
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001933
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001928
Iteration 49/1000 | Loss: 0.00001928
Iteration 50/1000 | Loss: 0.00001928
Iteration 51/1000 | Loss: 0.00001928
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001927
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001926
Iteration 73/1000 | Loss: 0.00001926
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001926
Iteration 76/1000 | Loss: 0.00001926
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.9255132428952493e-05, 1.9255132428952493e-05, 1.9255132428952493e-05, 1.9255132428952493e-05, 1.9255132428952493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9255132428952493e-05

Optimization complete. Final v2v error: 3.755646228790283 mm

Highest mean error: 4.128101825714111 mm for frame 131

Lowest mean error: 3.584986448287964 mm for frame 41

Saving results

Total time: 30.84172248840332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904435
Iteration 2/25 | Loss: 0.00151882
Iteration 3/25 | Loss: 0.00121008
Iteration 4/25 | Loss: 0.00114705
Iteration 5/25 | Loss: 0.00112975
Iteration 6/25 | Loss: 0.00112586
Iteration 7/25 | Loss: 0.00112494
Iteration 8/25 | Loss: 0.00112485
Iteration 9/25 | Loss: 0.00112485
Iteration 10/25 | Loss: 0.00112485
Iteration 11/25 | Loss: 0.00112485
Iteration 12/25 | Loss: 0.00112485
Iteration 13/25 | Loss: 0.00112485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011248524533584714, 0.0011248524533584714, 0.0011248524533584714, 0.0011248524533584714, 0.0011248524533584714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011248524533584714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26057899
Iteration 2/25 | Loss: 0.00103791
Iteration 3/25 | Loss: 0.00103789
Iteration 4/25 | Loss: 0.00103789
Iteration 5/25 | Loss: 0.00103789
Iteration 6/25 | Loss: 0.00103789
Iteration 7/25 | Loss: 0.00103789
Iteration 8/25 | Loss: 0.00103789
Iteration 9/25 | Loss: 0.00103789
Iteration 10/25 | Loss: 0.00103789
Iteration 11/25 | Loss: 0.00103789
Iteration 12/25 | Loss: 0.00103789
Iteration 13/25 | Loss: 0.00103789
Iteration 14/25 | Loss: 0.00103789
Iteration 15/25 | Loss: 0.00103789
Iteration 16/25 | Loss: 0.00103789
Iteration 17/25 | Loss: 0.00103789
Iteration 18/25 | Loss: 0.00103789
Iteration 19/25 | Loss: 0.00103789
Iteration 20/25 | Loss: 0.00103789
Iteration 21/25 | Loss: 0.00103789
Iteration 22/25 | Loss: 0.00103789
Iteration 23/25 | Loss: 0.00103789
Iteration 24/25 | Loss: 0.00103789
Iteration 25/25 | Loss: 0.00103789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103789
Iteration 2/1000 | Loss: 0.00014919
Iteration 3/1000 | Loss: 0.00010848
Iteration 4/1000 | Loss: 0.00009027
Iteration 5/1000 | Loss: 0.00008264
Iteration 6/1000 | Loss: 0.00007778
Iteration 7/1000 | Loss: 0.00063275
Iteration 8/1000 | Loss: 0.00008113
Iteration 9/1000 | Loss: 0.00007235
Iteration 10/1000 | Loss: 0.00006794
Iteration 11/1000 | Loss: 0.00006609
Iteration 12/1000 | Loss: 0.00026373
Iteration 13/1000 | Loss: 0.00019814
Iteration 14/1000 | Loss: 0.00006090
Iteration 15/1000 | Loss: 0.00014246
Iteration 16/1000 | Loss: 0.00036005
Iteration 17/1000 | Loss: 0.00016100
Iteration 18/1000 | Loss: 0.00005517
Iteration 19/1000 | Loss: 0.00005358
Iteration 20/1000 | Loss: 0.00030582
Iteration 21/1000 | Loss: 0.00017562
Iteration 22/1000 | Loss: 0.00017124
Iteration 23/1000 | Loss: 0.00060819
Iteration 24/1000 | Loss: 0.00085892
Iteration 25/1000 | Loss: 0.00006860
Iteration 26/1000 | Loss: 0.00005648
Iteration 27/1000 | Loss: 0.00004966
Iteration 28/1000 | Loss: 0.00004689
Iteration 29/1000 | Loss: 0.00004478
Iteration 30/1000 | Loss: 0.00004398
Iteration 31/1000 | Loss: 0.00004348
Iteration 32/1000 | Loss: 0.00004307
Iteration 33/1000 | Loss: 0.00004256
Iteration 34/1000 | Loss: 0.00004196
Iteration 35/1000 | Loss: 0.00004157
Iteration 36/1000 | Loss: 0.00004138
Iteration 37/1000 | Loss: 0.00004120
Iteration 38/1000 | Loss: 0.00004117
Iteration 39/1000 | Loss: 0.00004113
Iteration 40/1000 | Loss: 0.00004112
Iteration 41/1000 | Loss: 0.00004106
Iteration 42/1000 | Loss: 0.00004105
Iteration 43/1000 | Loss: 0.00004103
Iteration 44/1000 | Loss: 0.00004090
Iteration 45/1000 | Loss: 0.00004076
Iteration 46/1000 | Loss: 0.00004074
Iteration 47/1000 | Loss: 0.00054162
Iteration 48/1000 | Loss: 0.00047445
Iteration 49/1000 | Loss: 0.00005786
Iteration 50/1000 | Loss: 0.00004627
Iteration 51/1000 | Loss: 0.00004433
Iteration 52/1000 | Loss: 0.00004316
Iteration 53/1000 | Loss: 0.00004254
Iteration 54/1000 | Loss: 0.00004201
Iteration 55/1000 | Loss: 0.00049686
Iteration 56/1000 | Loss: 0.00333529
Iteration 57/1000 | Loss: 0.00151352
Iteration 58/1000 | Loss: 0.00011728
Iteration 59/1000 | Loss: 0.00091169
Iteration 60/1000 | Loss: 0.00039376
Iteration 61/1000 | Loss: 0.00180565
Iteration 62/1000 | Loss: 0.00047742
Iteration 63/1000 | Loss: 0.00200300
Iteration 64/1000 | Loss: 0.00146905
Iteration 65/1000 | Loss: 0.00073688
Iteration 66/1000 | Loss: 0.00024731
Iteration 67/1000 | Loss: 0.00005487
Iteration 68/1000 | Loss: 0.00069495
Iteration 69/1000 | Loss: 0.00035027
Iteration 70/1000 | Loss: 0.00113794
Iteration 71/1000 | Loss: 0.00033375
Iteration 72/1000 | Loss: 0.00034094
Iteration 73/1000 | Loss: 0.00028587
Iteration 74/1000 | Loss: 0.00028246
Iteration 75/1000 | Loss: 0.00004573
Iteration 76/1000 | Loss: 0.00003813
Iteration 77/1000 | Loss: 0.00003462
Iteration 78/1000 | Loss: 0.00005649
Iteration 79/1000 | Loss: 0.00003334
Iteration 80/1000 | Loss: 0.00003143
Iteration 81/1000 | Loss: 0.00003343
Iteration 82/1000 | Loss: 0.00003407
Iteration 83/1000 | Loss: 0.00002857
Iteration 84/1000 | Loss: 0.00053175
Iteration 85/1000 | Loss: 0.00023955
Iteration 86/1000 | Loss: 0.00002681
Iteration 87/1000 | Loss: 0.00002597
Iteration 88/1000 | Loss: 0.00002560
Iteration 89/1000 | Loss: 0.00002528
Iteration 90/1000 | Loss: 0.00002516
Iteration 91/1000 | Loss: 0.00002506
Iteration 92/1000 | Loss: 0.00002488
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002487
Iteration 95/1000 | Loss: 0.00048067
Iteration 96/1000 | Loss: 0.00049927
Iteration 97/1000 | Loss: 0.00042586
Iteration 98/1000 | Loss: 0.00050355
Iteration 99/1000 | Loss: 0.00038843
Iteration 100/1000 | Loss: 0.00002759
Iteration 101/1000 | Loss: 0.00002596
Iteration 102/1000 | Loss: 0.00002506
Iteration 103/1000 | Loss: 0.00002413
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002310
Iteration 106/1000 | Loss: 0.00002280
Iteration 107/1000 | Loss: 0.00002273
Iteration 108/1000 | Loss: 0.00002269
Iteration 109/1000 | Loss: 0.00002267
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002264
Iteration 112/1000 | Loss: 0.00002263
Iteration 113/1000 | Loss: 0.00002263
Iteration 114/1000 | Loss: 0.00002262
Iteration 115/1000 | Loss: 0.00002262
Iteration 116/1000 | Loss: 0.00002259
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002258
Iteration 119/1000 | Loss: 0.00002258
Iteration 120/1000 | Loss: 0.00002257
Iteration 121/1000 | Loss: 0.00002256
Iteration 122/1000 | Loss: 0.00002255
Iteration 123/1000 | Loss: 0.00002255
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002251
Iteration 126/1000 | Loss: 0.00002250
Iteration 127/1000 | Loss: 0.00002250
Iteration 128/1000 | Loss: 0.00002250
Iteration 129/1000 | Loss: 0.00002250
Iteration 130/1000 | Loss: 0.00002250
Iteration 131/1000 | Loss: 0.00002250
Iteration 132/1000 | Loss: 0.00002250
Iteration 133/1000 | Loss: 0.00002250
Iteration 134/1000 | Loss: 0.00002250
Iteration 135/1000 | Loss: 0.00002250
Iteration 136/1000 | Loss: 0.00002250
Iteration 137/1000 | Loss: 0.00002250
Iteration 138/1000 | Loss: 0.00002249
Iteration 139/1000 | Loss: 0.00002249
Iteration 140/1000 | Loss: 0.00002248
Iteration 141/1000 | Loss: 0.00002248
Iteration 142/1000 | Loss: 0.00002247
Iteration 143/1000 | Loss: 0.00002247
Iteration 144/1000 | Loss: 0.00002247
Iteration 145/1000 | Loss: 0.00002246
Iteration 146/1000 | Loss: 0.00002246
Iteration 147/1000 | Loss: 0.00002246
Iteration 148/1000 | Loss: 0.00002245
Iteration 149/1000 | Loss: 0.00002245
Iteration 150/1000 | Loss: 0.00002245
Iteration 151/1000 | Loss: 0.00002245
Iteration 152/1000 | Loss: 0.00002245
Iteration 153/1000 | Loss: 0.00002244
Iteration 154/1000 | Loss: 0.00002244
Iteration 155/1000 | Loss: 0.00002244
Iteration 156/1000 | Loss: 0.00002244
Iteration 157/1000 | Loss: 0.00002244
Iteration 158/1000 | Loss: 0.00002244
Iteration 159/1000 | Loss: 0.00002244
Iteration 160/1000 | Loss: 0.00002243
Iteration 161/1000 | Loss: 0.00002243
Iteration 162/1000 | Loss: 0.00002243
Iteration 163/1000 | Loss: 0.00002243
Iteration 164/1000 | Loss: 0.00002243
Iteration 165/1000 | Loss: 0.00002243
Iteration 166/1000 | Loss: 0.00002243
Iteration 167/1000 | Loss: 0.00002243
Iteration 168/1000 | Loss: 0.00002243
Iteration 169/1000 | Loss: 0.00002243
Iteration 170/1000 | Loss: 0.00002242
Iteration 171/1000 | Loss: 0.00002242
Iteration 172/1000 | Loss: 0.00002242
Iteration 173/1000 | Loss: 0.00002242
Iteration 174/1000 | Loss: 0.00002242
Iteration 175/1000 | Loss: 0.00002242
Iteration 176/1000 | Loss: 0.00002242
Iteration 177/1000 | Loss: 0.00002242
Iteration 178/1000 | Loss: 0.00002242
Iteration 179/1000 | Loss: 0.00002241
Iteration 180/1000 | Loss: 0.00002241
Iteration 181/1000 | Loss: 0.00002241
Iteration 182/1000 | Loss: 0.00002241
Iteration 183/1000 | Loss: 0.00002241
Iteration 184/1000 | Loss: 0.00002241
Iteration 185/1000 | Loss: 0.00002241
Iteration 186/1000 | Loss: 0.00002241
Iteration 187/1000 | Loss: 0.00002241
Iteration 188/1000 | Loss: 0.00002241
Iteration 189/1000 | Loss: 0.00002240
Iteration 190/1000 | Loss: 0.00002240
Iteration 191/1000 | Loss: 0.00002240
Iteration 192/1000 | Loss: 0.00002240
Iteration 193/1000 | Loss: 0.00002240
Iteration 194/1000 | Loss: 0.00002240
Iteration 195/1000 | Loss: 0.00002240
Iteration 196/1000 | Loss: 0.00002240
Iteration 197/1000 | Loss: 0.00002240
Iteration 198/1000 | Loss: 0.00002240
Iteration 199/1000 | Loss: 0.00002240
Iteration 200/1000 | Loss: 0.00002240
Iteration 201/1000 | Loss: 0.00002240
Iteration 202/1000 | Loss: 0.00002240
Iteration 203/1000 | Loss: 0.00002240
Iteration 204/1000 | Loss: 0.00002240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.2402282411349006e-05, 2.2402282411349006e-05, 2.2402282411349006e-05, 2.2402282411349006e-05, 2.2402282411349006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2402282411349006e-05

Optimization complete. Final v2v error: 3.8188488483428955 mm

Highest mean error: 5.9171671867370605 mm for frame 59

Lowest mean error: 3.4276328086853027 mm for frame 179

Saving results

Total time: 178.28418493270874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509703
Iteration 2/25 | Loss: 0.00115135
Iteration 3/25 | Loss: 0.00099432
Iteration 4/25 | Loss: 0.00098046
Iteration 5/25 | Loss: 0.00097677
Iteration 6/25 | Loss: 0.00097566
Iteration 7/25 | Loss: 0.00097555
Iteration 8/25 | Loss: 0.00097555
Iteration 9/25 | Loss: 0.00097555
Iteration 10/25 | Loss: 0.00097555
Iteration 11/25 | Loss: 0.00097555
Iteration 12/25 | Loss: 0.00097555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009755518403835595, 0.0009755518403835595, 0.0009755518403835595, 0.0009755518403835595, 0.0009755518403835595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009755518403835595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39329195
Iteration 2/25 | Loss: 0.00059610
Iteration 3/25 | Loss: 0.00059602
Iteration 4/25 | Loss: 0.00059602
Iteration 5/25 | Loss: 0.00059602
Iteration 6/25 | Loss: 0.00059602
Iteration 7/25 | Loss: 0.00059601
Iteration 8/25 | Loss: 0.00059601
Iteration 9/25 | Loss: 0.00059601
Iteration 10/25 | Loss: 0.00059601
Iteration 11/25 | Loss: 0.00059601
Iteration 12/25 | Loss: 0.00059601
Iteration 13/25 | Loss: 0.00059601
Iteration 14/25 | Loss: 0.00059601
Iteration 15/25 | Loss: 0.00059601
Iteration 16/25 | Loss: 0.00059601
Iteration 17/25 | Loss: 0.00059601
Iteration 18/25 | Loss: 0.00059601
Iteration 19/25 | Loss: 0.00059601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005960142007097602, 0.0005960142007097602, 0.0005960142007097602, 0.0005960142007097602, 0.0005960142007097602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005960142007097602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059601
Iteration 2/1000 | Loss: 0.00005056
Iteration 3/1000 | Loss: 0.00002677
Iteration 4/1000 | Loss: 0.00002078
Iteration 5/1000 | Loss: 0.00001951
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001713
Iteration 10/1000 | Loss: 0.00001675
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001610
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001605
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001605
Iteration 26/1000 | Loss: 0.00001605
Iteration 27/1000 | Loss: 0.00001605
Iteration 28/1000 | Loss: 0.00001605
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001604
Iteration 31/1000 | Loss: 0.00001604
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00001602
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001601
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001601
Iteration 39/1000 | Loss: 0.00001601
Iteration 40/1000 | Loss: 0.00001601
Iteration 41/1000 | Loss: 0.00001601
Iteration 42/1000 | Loss: 0.00001601
Iteration 43/1000 | Loss: 0.00001601
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [1.6009527826099657e-05, 1.6009527826099657e-05, 1.6009527826099657e-05, 1.6009527826099657e-05, 1.6009527826099657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6009527826099657e-05

Optimization complete. Final v2v error: 3.074488401412964 mm

Highest mean error: 5.124197959899902 mm for frame 59

Lowest mean error: 2.32891583442688 mm for frame 86

Saving results

Total time: 32.40755867958069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435799
Iteration 2/25 | Loss: 0.00117839
Iteration 3/25 | Loss: 0.00097647
Iteration 4/25 | Loss: 0.00095792
Iteration 5/25 | Loss: 0.00095582
Iteration 6/25 | Loss: 0.00095548
Iteration 7/25 | Loss: 0.00095548
Iteration 8/25 | Loss: 0.00095548
Iteration 9/25 | Loss: 0.00095548
Iteration 10/25 | Loss: 0.00095548
Iteration 11/25 | Loss: 0.00095548
Iteration 12/25 | Loss: 0.00095548
Iteration 13/25 | Loss: 0.00095548
Iteration 14/25 | Loss: 0.00095548
Iteration 15/25 | Loss: 0.00095548
Iteration 16/25 | Loss: 0.00095548
Iteration 17/25 | Loss: 0.00095548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009554842836223543, 0.0009554842836223543, 0.0009554842836223543, 0.0009554842836223543, 0.0009554842836223543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009554842836223543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76667976
Iteration 2/25 | Loss: 0.00038960
Iteration 3/25 | Loss: 0.00038958
Iteration 4/25 | Loss: 0.00038958
Iteration 5/25 | Loss: 0.00038958
Iteration 6/25 | Loss: 0.00038958
Iteration 7/25 | Loss: 0.00038958
Iteration 8/25 | Loss: 0.00038958
Iteration 9/25 | Loss: 0.00038958
Iteration 10/25 | Loss: 0.00038958
Iteration 11/25 | Loss: 0.00038958
Iteration 12/25 | Loss: 0.00038958
Iteration 13/25 | Loss: 0.00038958
Iteration 14/25 | Loss: 0.00038958
Iteration 15/25 | Loss: 0.00038958
Iteration 16/25 | Loss: 0.00038958
Iteration 17/25 | Loss: 0.00038958
Iteration 18/25 | Loss: 0.00038958
Iteration 19/25 | Loss: 0.00038958
Iteration 20/25 | Loss: 0.00038958
Iteration 21/25 | Loss: 0.00038958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00038957869401201606, 0.00038957869401201606, 0.00038957869401201606, 0.00038957869401201606, 0.00038957869401201606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038957869401201606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038958
Iteration 2/1000 | Loss: 0.00002331
Iteration 3/1000 | Loss: 0.00001521
Iteration 4/1000 | Loss: 0.00001281
Iteration 5/1000 | Loss: 0.00001167
Iteration 6/1000 | Loss: 0.00001132
Iteration 7/1000 | Loss: 0.00001094
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001048
Iteration 11/1000 | Loss: 0.00001048
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001043
Iteration 14/1000 | Loss: 0.00001043
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001038
Iteration 20/1000 | Loss: 0.00001037
Iteration 21/1000 | Loss: 0.00001037
Iteration 22/1000 | Loss: 0.00001037
Iteration 23/1000 | Loss: 0.00001037
Iteration 24/1000 | Loss: 0.00001034
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001034
Iteration 27/1000 | Loss: 0.00001034
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001033
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001031
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001030
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001030
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001029
Iteration 48/1000 | Loss: 0.00001029
Iteration 49/1000 | Loss: 0.00001028
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001027
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001025
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001025
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001022
Iteration 83/1000 | Loss: 0.00001022
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001021
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001020
Iteration 100/1000 | Loss: 0.00001020
Iteration 101/1000 | Loss: 0.00001020
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001019
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001018
Iteration 112/1000 | Loss: 0.00001018
Iteration 113/1000 | Loss: 0.00001018
Iteration 114/1000 | Loss: 0.00001018
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001018
Iteration 121/1000 | Loss: 0.00001018
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001018
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001018
Iteration 130/1000 | Loss: 0.00001018
Iteration 131/1000 | Loss: 0.00001018
Iteration 132/1000 | Loss: 0.00001018
Iteration 133/1000 | Loss: 0.00001018
Iteration 134/1000 | Loss: 0.00001018
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001018
Iteration 141/1000 | Loss: 0.00001018
Iteration 142/1000 | Loss: 0.00001018
Iteration 143/1000 | Loss: 0.00001018
Iteration 144/1000 | Loss: 0.00001018
Iteration 145/1000 | Loss: 0.00001018
Iteration 146/1000 | Loss: 0.00001018
Iteration 147/1000 | Loss: 0.00001018
Iteration 148/1000 | Loss: 0.00001018
Iteration 149/1000 | Loss: 0.00001018
Iteration 150/1000 | Loss: 0.00001018
Iteration 151/1000 | Loss: 0.00001018
Iteration 152/1000 | Loss: 0.00001018
Iteration 153/1000 | Loss: 0.00001018
Iteration 154/1000 | Loss: 0.00001018
Iteration 155/1000 | Loss: 0.00001018
Iteration 156/1000 | Loss: 0.00001018
Iteration 157/1000 | Loss: 0.00001018
Iteration 158/1000 | Loss: 0.00001018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.018068451230647e-05, 1.018068451230647e-05, 1.018068451230647e-05, 1.018068451230647e-05, 1.018068451230647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.018068451230647e-05

Optimization complete. Final v2v error: 2.6942617893218994 mm

Highest mean error: 3.28029465675354 mm for frame 67

Lowest mean error: 2.4384684562683105 mm for frame 53

Saving results

Total time: 31.01307773590088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972061
Iteration 2/25 | Loss: 0.00128676
Iteration 3/25 | Loss: 0.00106937
Iteration 4/25 | Loss: 0.00104610
Iteration 5/25 | Loss: 0.00103830
Iteration 6/25 | Loss: 0.00103723
Iteration 7/25 | Loss: 0.00103723
Iteration 8/25 | Loss: 0.00103723
Iteration 9/25 | Loss: 0.00103723
Iteration 10/25 | Loss: 0.00103723
Iteration 11/25 | Loss: 0.00103723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010372286196798086, 0.0010372286196798086, 0.0010372286196798086, 0.0010372286196798086, 0.0010372286196798086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010372286196798086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98180330
Iteration 2/25 | Loss: 0.00066991
Iteration 3/25 | Loss: 0.00066991
Iteration 4/25 | Loss: 0.00066991
Iteration 5/25 | Loss: 0.00066991
Iteration 6/25 | Loss: 0.00066991
Iteration 7/25 | Loss: 0.00066991
Iteration 8/25 | Loss: 0.00066991
Iteration 9/25 | Loss: 0.00066991
Iteration 10/25 | Loss: 0.00066991
Iteration 11/25 | Loss: 0.00066991
Iteration 12/25 | Loss: 0.00066991
Iteration 13/25 | Loss: 0.00066991
Iteration 14/25 | Loss: 0.00066991
Iteration 15/25 | Loss: 0.00066991
Iteration 16/25 | Loss: 0.00066991
Iteration 17/25 | Loss: 0.00066991
Iteration 18/25 | Loss: 0.00066991
Iteration 19/25 | Loss: 0.00066991
Iteration 20/25 | Loss: 0.00066991
Iteration 21/25 | Loss: 0.00066991
Iteration 22/25 | Loss: 0.00066991
Iteration 23/25 | Loss: 0.00066991
Iteration 24/25 | Loss: 0.00066991
Iteration 25/25 | Loss: 0.00066991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066991
Iteration 2/1000 | Loss: 0.00005400
Iteration 3/1000 | Loss: 0.00003508
Iteration 4/1000 | Loss: 0.00002942
Iteration 5/1000 | Loss: 0.00002799
Iteration 6/1000 | Loss: 0.00002715
Iteration 7/1000 | Loss: 0.00002683
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002629
Iteration 10/1000 | Loss: 0.00002608
Iteration 11/1000 | Loss: 0.00002588
Iteration 12/1000 | Loss: 0.00002584
Iteration 13/1000 | Loss: 0.00002578
Iteration 14/1000 | Loss: 0.00002575
Iteration 15/1000 | Loss: 0.00002558
Iteration 16/1000 | Loss: 0.00002544
Iteration 17/1000 | Loss: 0.00002544
Iteration 18/1000 | Loss: 0.00002536
Iteration 19/1000 | Loss: 0.00002526
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002508
Iteration 22/1000 | Loss: 0.00002497
Iteration 23/1000 | Loss: 0.00002494
Iteration 24/1000 | Loss: 0.00002483
Iteration 25/1000 | Loss: 0.00002474
Iteration 26/1000 | Loss: 0.00002469
Iteration 27/1000 | Loss: 0.00002469
Iteration 28/1000 | Loss: 0.00002468
Iteration 29/1000 | Loss: 0.00002468
Iteration 30/1000 | Loss: 0.00002468
Iteration 31/1000 | Loss: 0.00002464
Iteration 32/1000 | Loss: 0.00002464
Iteration 33/1000 | Loss: 0.00002462
Iteration 34/1000 | Loss: 0.00002459
Iteration 35/1000 | Loss: 0.00002459
Iteration 36/1000 | Loss: 0.00002455
Iteration 37/1000 | Loss: 0.00002453
Iteration 38/1000 | Loss: 0.00002453
Iteration 39/1000 | Loss: 0.00002453
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002453
Iteration 42/1000 | Loss: 0.00002453
Iteration 43/1000 | Loss: 0.00002452
Iteration 44/1000 | Loss: 0.00002452
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002450
Iteration 52/1000 | Loss: 0.00002450
Iteration 53/1000 | Loss: 0.00002450
Iteration 54/1000 | Loss: 0.00002449
Iteration 55/1000 | Loss: 0.00002449
Iteration 56/1000 | Loss: 0.00002449
Iteration 57/1000 | Loss: 0.00002449
Iteration 58/1000 | Loss: 0.00002448
Iteration 59/1000 | Loss: 0.00002448
Iteration 60/1000 | Loss: 0.00002448
Iteration 61/1000 | Loss: 0.00002448
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002447
Iteration 64/1000 | Loss: 0.00002447
Iteration 65/1000 | Loss: 0.00002447
Iteration 66/1000 | Loss: 0.00002447
Iteration 67/1000 | Loss: 0.00002447
Iteration 68/1000 | Loss: 0.00002447
Iteration 69/1000 | Loss: 0.00002447
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002446
Iteration 76/1000 | Loss: 0.00002446
Iteration 77/1000 | Loss: 0.00002446
Iteration 78/1000 | Loss: 0.00002446
Iteration 79/1000 | Loss: 0.00002445
Iteration 80/1000 | Loss: 0.00002445
Iteration 81/1000 | Loss: 0.00002445
Iteration 82/1000 | Loss: 0.00002445
Iteration 83/1000 | Loss: 0.00002444
Iteration 84/1000 | Loss: 0.00002444
Iteration 85/1000 | Loss: 0.00002444
Iteration 86/1000 | Loss: 0.00002444
Iteration 87/1000 | Loss: 0.00002444
Iteration 88/1000 | Loss: 0.00002444
Iteration 89/1000 | Loss: 0.00002444
Iteration 90/1000 | Loss: 0.00002444
Iteration 91/1000 | Loss: 0.00002444
Iteration 92/1000 | Loss: 0.00002444
Iteration 93/1000 | Loss: 0.00002443
Iteration 94/1000 | Loss: 0.00002443
Iteration 95/1000 | Loss: 0.00002443
Iteration 96/1000 | Loss: 0.00002443
Iteration 97/1000 | Loss: 0.00002443
Iteration 98/1000 | Loss: 0.00002443
Iteration 99/1000 | Loss: 0.00002442
Iteration 100/1000 | Loss: 0.00002442
Iteration 101/1000 | Loss: 0.00002442
Iteration 102/1000 | Loss: 0.00002442
Iteration 103/1000 | Loss: 0.00002442
Iteration 104/1000 | Loss: 0.00002442
Iteration 105/1000 | Loss: 0.00002442
Iteration 106/1000 | Loss: 0.00002442
Iteration 107/1000 | Loss: 0.00002442
Iteration 108/1000 | Loss: 0.00002441
Iteration 109/1000 | Loss: 0.00002441
Iteration 110/1000 | Loss: 0.00002441
Iteration 111/1000 | Loss: 0.00002441
Iteration 112/1000 | Loss: 0.00002441
Iteration 113/1000 | Loss: 0.00002441
Iteration 114/1000 | Loss: 0.00002441
Iteration 115/1000 | Loss: 0.00002441
Iteration 116/1000 | Loss: 0.00002441
Iteration 117/1000 | Loss: 0.00002441
Iteration 118/1000 | Loss: 0.00002441
Iteration 119/1000 | Loss: 0.00002440
Iteration 120/1000 | Loss: 0.00002440
Iteration 121/1000 | Loss: 0.00002440
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00002440
Iteration 124/1000 | Loss: 0.00002440
Iteration 125/1000 | Loss: 0.00002440
Iteration 126/1000 | Loss: 0.00002440
Iteration 127/1000 | Loss: 0.00002439
Iteration 128/1000 | Loss: 0.00002439
Iteration 129/1000 | Loss: 0.00002439
Iteration 130/1000 | Loss: 0.00002439
Iteration 131/1000 | Loss: 0.00002439
Iteration 132/1000 | Loss: 0.00002439
Iteration 133/1000 | Loss: 0.00002439
Iteration 134/1000 | Loss: 0.00002439
Iteration 135/1000 | Loss: 0.00002439
Iteration 136/1000 | Loss: 0.00002439
Iteration 137/1000 | Loss: 0.00002439
Iteration 138/1000 | Loss: 0.00002439
Iteration 139/1000 | Loss: 0.00002439
Iteration 140/1000 | Loss: 0.00002439
Iteration 141/1000 | Loss: 0.00002439
Iteration 142/1000 | Loss: 0.00002439
Iteration 143/1000 | Loss: 0.00002439
Iteration 144/1000 | Loss: 0.00002439
Iteration 145/1000 | Loss: 0.00002439
Iteration 146/1000 | Loss: 0.00002439
Iteration 147/1000 | Loss: 0.00002439
Iteration 148/1000 | Loss: 0.00002439
Iteration 149/1000 | Loss: 0.00002439
Iteration 150/1000 | Loss: 0.00002439
Iteration 151/1000 | Loss: 0.00002439
Iteration 152/1000 | Loss: 0.00002439
Iteration 153/1000 | Loss: 0.00002439
Iteration 154/1000 | Loss: 0.00002439
Iteration 155/1000 | Loss: 0.00002439
Iteration 156/1000 | Loss: 0.00002439
Iteration 157/1000 | Loss: 0.00002439
Iteration 158/1000 | Loss: 0.00002439
Iteration 159/1000 | Loss: 0.00002439
Iteration 160/1000 | Loss: 0.00002439
Iteration 161/1000 | Loss: 0.00002439
Iteration 162/1000 | Loss: 0.00002439
Iteration 163/1000 | Loss: 0.00002439
Iteration 164/1000 | Loss: 0.00002439
Iteration 165/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.438997216813732e-05, 2.438997216813732e-05, 2.438997216813732e-05, 2.438997216813732e-05, 2.438997216813732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.438997216813732e-05

Optimization complete. Final v2v error: 3.9666671752929688 mm

Highest mean error: 4.816110610961914 mm for frame 81

Lowest mean error: 3.3119289875030518 mm for frame 0

Saving results

Total time: 48.18121838569641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878859
Iteration 2/25 | Loss: 0.00105803
Iteration 3/25 | Loss: 0.00094335
Iteration 4/25 | Loss: 0.00093175
Iteration 5/25 | Loss: 0.00092804
Iteration 6/25 | Loss: 0.00092703
Iteration 7/25 | Loss: 0.00092703
Iteration 8/25 | Loss: 0.00092703
Iteration 9/25 | Loss: 0.00092703
Iteration 10/25 | Loss: 0.00092703
Iteration 11/25 | Loss: 0.00092703
Iteration 12/25 | Loss: 0.00092703
Iteration 13/25 | Loss: 0.00092703
Iteration 14/25 | Loss: 0.00092703
Iteration 15/25 | Loss: 0.00092703
Iteration 16/25 | Loss: 0.00092703
Iteration 17/25 | Loss: 0.00092703
Iteration 18/25 | Loss: 0.00092703
Iteration 19/25 | Loss: 0.00092703
Iteration 20/25 | Loss: 0.00092703
Iteration 21/25 | Loss: 0.00092703
Iteration 22/25 | Loss: 0.00092703
Iteration 23/25 | Loss: 0.00092703
Iteration 24/25 | Loss: 0.00092703
Iteration 25/25 | Loss: 0.00092703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57023931
Iteration 2/25 | Loss: 0.00068367
Iteration 3/25 | Loss: 0.00068367
Iteration 4/25 | Loss: 0.00068367
Iteration 5/25 | Loss: 0.00068367
Iteration 6/25 | Loss: 0.00068367
Iteration 7/25 | Loss: 0.00068367
Iteration 8/25 | Loss: 0.00068367
Iteration 9/25 | Loss: 0.00068367
Iteration 10/25 | Loss: 0.00068367
Iteration 11/25 | Loss: 0.00068367
Iteration 12/25 | Loss: 0.00068367
Iteration 13/25 | Loss: 0.00068367
Iteration 14/25 | Loss: 0.00068367
Iteration 15/25 | Loss: 0.00068367
Iteration 16/25 | Loss: 0.00068367
Iteration 17/25 | Loss: 0.00068367
Iteration 18/25 | Loss: 0.00068367
Iteration 19/25 | Loss: 0.00068367
Iteration 20/25 | Loss: 0.00068367
Iteration 21/25 | Loss: 0.00068367
Iteration 22/25 | Loss: 0.00068367
Iteration 23/25 | Loss: 0.00068367
Iteration 24/25 | Loss: 0.00068367
Iteration 25/25 | Loss: 0.00068367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068367
Iteration 2/1000 | Loss: 0.00002514
Iteration 3/1000 | Loss: 0.00001379
Iteration 4/1000 | Loss: 0.00001249
Iteration 5/1000 | Loss: 0.00001183
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001114
Iteration 8/1000 | Loss: 0.00001098
Iteration 9/1000 | Loss: 0.00001093
Iteration 10/1000 | Loss: 0.00001092
Iteration 11/1000 | Loss: 0.00001089
Iteration 12/1000 | Loss: 0.00001089
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001088
Iteration 15/1000 | Loss: 0.00001088
Iteration 16/1000 | Loss: 0.00001088
Iteration 17/1000 | Loss: 0.00001086
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001084
Iteration 20/1000 | Loss: 0.00001084
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001084
Iteration 23/1000 | Loss: 0.00001084
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001083
Iteration 27/1000 | Loss: 0.00001083
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001080
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001076
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001076
Iteration 45/1000 | Loss: 0.00001076
Iteration 46/1000 | Loss: 0.00001076
Iteration 47/1000 | Loss: 0.00001076
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001072
Iteration 59/1000 | Loss: 0.00001072
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001071
Iteration 62/1000 | Loss: 0.00001070
Iteration 63/1000 | Loss: 0.00001069
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001068
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001067
Iteration 70/1000 | Loss: 0.00001067
Iteration 71/1000 | Loss: 0.00001067
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001066
Iteration 76/1000 | Loss: 0.00001066
Iteration 77/1000 | Loss: 0.00001065
Iteration 78/1000 | Loss: 0.00001065
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001064
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001061
Iteration 96/1000 | Loss: 0.00001061
Iteration 97/1000 | Loss: 0.00001061
Iteration 98/1000 | Loss: 0.00001061
Iteration 99/1000 | Loss: 0.00001061
Iteration 100/1000 | Loss: 0.00001061
Iteration 101/1000 | Loss: 0.00001061
Iteration 102/1000 | Loss: 0.00001061
Iteration 103/1000 | Loss: 0.00001060
Iteration 104/1000 | Loss: 0.00001060
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001060
Iteration 107/1000 | Loss: 0.00001060
Iteration 108/1000 | Loss: 0.00001060
Iteration 109/1000 | Loss: 0.00001060
Iteration 110/1000 | Loss: 0.00001060
Iteration 111/1000 | Loss: 0.00001060
Iteration 112/1000 | Loss: 0.00001060
Iteration 113/1000 | Loss: 0.00001060
Iteration 114/1000 | Loss: 0.00001060
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001059
Iteration 118/1000 | Loss: 0.00001059
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001059
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001059
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001059
Iteration 128/1000 | Loss: 0.00001059
Iteration 129/1000 | Loss: 0.00001059
Iteration 130/1000 | Loss: 0.00001059
Iteration 131/1000 | Loss: 0.00001059
Iteration 132/1000 | Loss: 0.00001059
Iteration 133/1000 | Loss: 0.00001059
Iteration 134/1000 | Loss: 0.00001059
Iteration 135/1000 | Loss: 0.00001059
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001059
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001058
Iteration 145/1000 | Loss: 0.00001058
Iteration 146/1000 | Loss: 0.00001058
Iteration 147/1000 | Loss: 0.00001058
Iteration 148/1000 | Loss: 0.00001058
Iteration 149/1000 | Loss: 0.00001058
Iteration 150/1000 | Loss: 0.00001058
Iteration 151/1000 | Loss: 0.00001058
Iteration 152/1000 | Loss: 0.00001058
Iteration 153/1000 | Loss: 0.00001058
Iteration 154/1000 | Loss: 0.00001058
Iteration 155/1000 | Loss: 0.00001058
Iteration 156/1000 | Loss: 0.00001058
Iteration 157/1000 | Loss: 0.00001058
Iteration 158/1000 | Loss: 0.00001058
Iteration 159/1000 | Loss: 0.00001058
Iteration 160/1000 | Loss: 0.00001058
Iteration 161/1000 | Loss: 0.00001058
Iteration 162/1000 | Loss: 0.00001058
Iteration 163/1000 | Loss: 0.00001057
Iteration 164/1000 | Loss: 0.00001057
Iteration 165/1000 | Loss: 0.00001057
Iteration 166/1000 | Loss: 0.00001057
Iteration 167/1000 | Loss: 0.00001057
Iteration 168/1000 | Loss: 0.00001057
Iteration 169/1000 | Loss: 0.00001057
Iteration 170/1000 | Loss: 0.00001057
Iteration 171/1000 | Loss: 0.00001057
Iteration 172/1000 | Loss: 0.00001057
Iteration 173/1000 | Loss: 0.00001057
Iteration 174/1000 | Loss: 0.00001057
Iteration 175/1000 | Loss: 0.00001057
Iteration 176/1000 | Loss: 0.00001057
Iteration 177/1000 | Loss: 0.00001057
Iteration 178/1000 | Loss: 0.00001057
Iteration 179/1000 | Loss: 0.00001057
Iteration 180/1000 | Loss: 0.00001057
Iteration 181/1000 | Loss: 0.00001057
Iteration 182/1000 | Loss: 0.00001057
Iteration 183/1000 | Loss: 0.00001057
Iteration 184/1000 | Loss: 0.00001057
Iteration 185/1000 | Loss: 0.00001057
Iteration 186/1000 | Loss: 0.00001057
Iteration 187/1000 | Loss: 0.00001057
Iteration 188/1000 | Loss: 0.00001057
Iteration 189/1000 | Loss: 0.00001057
Iteration 190/1000 | Loss: 0.00001057
Iteration 191/1000 | Loss: 0.00001057
Iteration 192/1000 | Loss: 0.00001057
Iteration 193/1000 | Loss: 0.00001057
Iteration 194/1000 | Loss: 0.00001057
Iteration 195/1000 | Loss: 0.00001057
Iteration 196/1000 | Loss: 0.00001057
Iteration 197/1000 | Loss: 0.00001057
Iteration 198/1000 | Loss: 0.00001057
Iteration 199/1000 | Loss: 0.00001057
Iteration 200/1000 | Loss: 0.00001057
Iteration 201/1000 | Loss: 0.00001057
Iteration 202/1000 | Loss: 0.00001057
Iteration 203/1000 | Loss: 0.00001057
Iteration 204/1000 | Loss: 0.00001057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0568422112555709e-05, 1.0568422112555709e-05, 1.0568422112555709e-05, 1.0568422112555709e-05, 1.0568422112555709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0568422112555709e-05

Optimization complete. Final v2v error: 2.725436210632324 mm

Highest mean error: 3.4041993618011475 mm for frame 69

Lowest mean error: 2.5073893070220947 mm for frame 112

Saving results

Total time: 32.39312934875488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891424
Iteration 2/25 | Loss: 0.00146379
Iteration 3/25 | Loss: 0.00116786
Iteration 4/25 | Loss: 0.00112749
Iteration 5/25 | Loss: 0.00109690
Iteration 6/25 | Loss: 0.00102205
Iteration 7/25 | Loss: 0.00101572
Iteration 8/25 | Loss: 0.00101503
Iteration 9/25 | Loss: 0.00101484
Iteration 10/25 | Loss: 0.00101482
Iteration 11/25 | Loss: 0.00101482
Iteration 12/25 | Loss: 0.00101482
Iteration 13/25 | Loss: 0.00101482
Iteration 14/25 | Loss: 0.00101482
Iteration 15/25 | Loss: 0.00101482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010148240253329277, 0.0010148240253329277, 0.0010148240253329277, 0.0010148240253329277, 0.0010148240253329277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010148240253329277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89217234
Iteration 2/25 | Loss: 0.00026762
Iteration 3/25 | Loss: 0.00026762
Iteration 4/25 | Loss: 0.00026762
Iteration 5/25 | Loss: 0.00026762
Iteration 6/25 | Loss: 0.00026762
Iteration 7/25 | Loss: 0.00026762
Iteration 8/25 | Loss: 0.00026762
Iteration 9/25 | Loss: 0.00026762
Iteration 10/25 | Loss: 0.00026762
Iteration 11/25 | Loss: 0.00026762
Iteration 12/25 | Loss: 0.00026762
Iteration 13/25 | Loss: 0.00026762
Iteration 14/25 | Loss: 0.00026762
Iteration 15/25 | Loss: 0.00026762
Iteration 16/25 | Loss: 0.00026762
Iteration 17/25 | Loss: 0.00026762
Iteration 18/25 | Loss: 0.00026762
Iteration 19/25 | Loss: 0.00026762
Iteration 20/25 | Loss: 0.00026762
Iteration 21/25 | Loss: 0.00026762
Iteration 22/25 | Loss: 0.00026762
Iteration 23/25 | Loss: 0.00026762
Iteration 24/25 | Loss: 0.00026762
Iteration 25/25 | Loss: 0.00026762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026762
Iteration 2/1000 | Loss: 0.00002750
Iteration 3/1000 | Loss: 0.00002268
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001814
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001747
Iteration 12/1000 | Loss: 0.00001744
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001741
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001734
Iteration 27/1000 | Loss: 0.00001733
Iteration 28/1000 | Loss: 0.00001733
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001733
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001732
Iteration 35/1000 | Loss: 0.00001732
Iteration 36/1000 | Loss: 0.00001732
Iteration 37/1000 | Loss: 0.00001732
Iteration 38/1000 | Loss: 0.00001732
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001731
Iteration 48/1000 | Loss: 0.00001731
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001731
Iteration 51/1000 | Loss: 0.00001731
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001729
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00001729
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.728609458950814e-05, 1.728609458950814e-05, 1.728609458950814e-05, 1.728609458950814e-05, 1.728609458950814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728609458950814e-05

Optimization complete. Final v2v error: 3.4958107471466064 mm

Highest mean error: 3.6078765392303467 mm for frame 72

Lowest mean error: 3.3654842376708984 mm for frame 0

Saving results

Total time: 34.33601641654968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495633
Iteration 2/25 | Loss: 0.00116574
Iteration 3/25 | Loss: 0.00099674
Iteration 4/25 | Loss: 0.00098450
Iteration 5/25 | Loss: 0.00098111
Iteration 6/25 | Loss: 0.00097998
Iteration 7/25 | Loss: 0.00097998
Iteration 8/25 | Loss: 0.00097998
Iteration 9/25 | Loss: 0.00097998
Iteration 10/25 | Loss: 0.00097998
Iteration 11/25 | Loss: 0.00097998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009799802210181952, 0.0009799802210181952, 0.0009799802210181952, 0.0009799802210181952, 0.0009799802210181952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009799802210181952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42082536
Iteration 2/25 | Loss: 0.00064376
Iteration 3/25 | Loss: 0.00064374
Iteration 4/25 | Loss: 0.00064374
Iteration 5/25 | Loss: 0.00064374
Iteration 6/25 | Loss: 0.00064374
Iteration 7/25 | Loss: 0.00064374
Iteration 8/25 | Loss: 0.00064374
Iteration 9/25 | Loss: 0.00064374
Iteration 10/25 | Loss: 0.00064374
Iteration 11/25 | Loss: 0.00064374
Iteration 12/25 | Loss: 0.00064374
Iteration 13/25 | Loss: 0.00064374
Iteration 14/25 | Loss: 0.00064374
Iteration 15/25 | Loss: 0.00064374
Iteration 16/25 | Loss: 0.00064374
Iteration 17/25 | Loss: 0.00064374
Iteration 18/25 | Loss: 0.00064374
Iteration 19/25 | Loss: 0.00064374
Iteration 20/25 | Loss: 0.00064374
Iteration 21/25 | Loss: 0.00064374
Iteration 22/25 | Loss: 0.00064374
Iteration 23/25 | Loss: 0.00064374
Iteration 24/25 | Loss: 0.00064374
Iteration 25/25 | Loss: 0.00064374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064374
Iteration 2/1000 | Loss: 0.00002850
Iteration 3/1000 | Loss: 0.00001948
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001549
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001484
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001483
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001457
Iteration 41/1000 | Loss: 0.00001457
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001456
Iteration 45/1000 | Loss: 0.00001456
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001456
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001450
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001445
Iteration 119/1000 | Loss: 0.00001445
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001445
Iteration 122/1000 | Loss: 0.00001445
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001442
Iteration 133/1000 | Loss: 0.00001442
Iteration 134/1000 | Loss: 0.00001442
Iteration 135/1000 | Loss: 0.00001442
Iteration 136/1000 | Loss: 0.00001442
Iteration 137/1000 | Loss: 0.00001442
Iteration 138/1000 | Loss: 0.00001442
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001442
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001442
Iteration 152/1000 | Loss: 0.00001442
Iteration 153/1000 | Loss: 0.00001442
Iteration 154/1000 | Loss: 0.00001442
Iteration 155/1000 | Loss: 0.00001442
Iteration 156/1000 | Loss: 0.00001442
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001442
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Iteration 161/1000 | Loss: 0.00001442
Iteration 162/1000 | Loss: 0.00001442
Iteration 163/1000 | Loss: 0.00001442
Iteration 164/1000 | Loss: 0.00001442
Iteration 165/1000 | Loss: 0.00001442
Iteration 166/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.44166133395629e-05, 1.44166133395629e-05, 1.44166133395629e-05, 1.44166133395629e-05, 1.44166133395629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.44166133395629e-05

Optimization complete. Final v2v error: 3.048781156539917 mm

Highest mean error: 4.3024773597717285 mm for frame 110

Lowest mean error: 2.4085466861724854 mm for frame 18

Saving results

Total time: 37.064006328582764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903430
Iteration 2/25 | Loss: 0.00150663
Iteration 3/25 | Loss: 0.00116455
Iteration 4/25 | Loss: 0.00107844
Iteration 5/25 | Loss: 0.00102170
Iteration 6/25 | Loss: 0.00100427
Iteration 7/25 | Loss: 0.00101068
Iteration 8/25 | Loss: 0.00102101
Iteration 9/25 | Loss: 0.00101198
Iteration 10/25 | Loss: 0.00099608
Iteration 11/25 | Loss: 0.00099425
Iteration 12/25 | Loss: 0.00101131
Iteration 13/25 | Loss: 0.00100005
Iteration 14/25 | Loss: 0.00098863
Iteration 15/25 | Loss: 0.00098781
Iteration 16/25 | Loss: 0.00097872
Iteration 17/25 | Loss: 0.00097786
Iteration 18/25 | Loss: 0.00097774
Iteration 19/25 | Loss: 0.00097773
Iteration 20/25 | Loss: 0.00097773
Iteration 21/25 | Loss: 0.00097772
Iteration 22/25 | Loss: 0.00097772
Iteration 23/25 | Loss: 0.00097772
Iteration 24/25 | Loss: 0.00098408
Iteration 25/25 | Loss: 0.00098018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.27516270
Iteration 2/25 | Loss: 0.00082658
Iteration 3/25 | Loss: 0.00082646
Iteration 4/25 | Loss: 0.00082646
Iteration 5/25 | Loss: 0.00082646
Iteration 6/25 | Loss: 0.00082646
Iteration 7/25 | Loss: 0.00082646
Iteration 8/25 | Loss: 0.00082646
Iteration 9/25 | Loss: 0.00082646
Iteration 10/25 | Loss: 0.00082646
Iteration 11/25 | Loss: 0.00082646
Iteration 12/25 | Loss: 0.00082646
Iteration 13/25 | Loss: 0.00082646
Iteration 14/25 | Loss: 0.00082646
Iteration 15/25 | Loss: 0.00082646
Iteration 16/25 | Loss: 0.00082646
Iteration 17/25 | Loss: 0.00082646
Iteration 18/25 | Loss: 0.00082646
Iteration 19/25 | Loss: 0.00082646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008264605421572924, 0.0008264605421572924, 0.0008264605421572924, 0.0008264605421572924, 0.0008264605421572924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008264605421572924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082646
Iteration 2/1000 | Loss: 0.00014737
Iteration 3/1000 | Loss: 0.00009608
Iteration 4/1000 | Loss: 0.00029084
Iteration 5/1000 | Loss: 0.00028388
Iteration 6/1000 | Loss: 0.00006779
Iteration 7/1000 | Loss: 0.00018465
Iteration 8/1000 | Loss: 0.00012238
Iteration 9/1000 | Loss: 0.00015783
Iteration 10/1000 | Loss: 0.00012769
Iteration 11/1000 | Loss: 0.00004203
Iteration 12/1000 | Loss: 0.00025719
Iteration 13/1000 | Loss: 0.00012906
Iteration 14/1000 | Loss: 0.00012045
Iteration 15/1000 | Loss: 0.00009354
Iteration 16/1000 | Loss: 0.00002678
Iteration 17/1000 | Loss: 0.00012720
Iteration 18/1000 | Loss: 0.00010748
Iteration 19/1000 | Loss: 0.00016312
Iteration 20/1000 | Loss: 0.00007018
Iteration 21/1000 | Loss: 0.00012629
Iteration 22/1000 | Loss: 0.00016641
Iteration 23/1000 | Loss: 0.00014294
Iteration 24/1000 | Loss: 0.00008942
Iteration 25/1000 | Loss: 0.00015780
Iteration 26/1000 | Loss: 0.00006177
Iteration 27/1000 | Loss: 0.00006467
Iteration 28/1000 | Loss: 0.00006251
Iteration 29/1000 | Loss: 0.00005976
Iteration 30/1000 | Loss: 0.00006629
Iteration 31/1000 | Loss: 0.00002726
Iteration 32/1000 | Loss: 0.00016893
Iteration 33/1000 | Loss: 0.00005213
Iteration 34/1000 | Loss: 0.00002241
Iteration 35/1000 | Loss: 0.00009305
Iteration 36/1000 | Loss: 0.00002356
Iteration 37/1000 | Loss: 0.00010995
Iteration 38/1000 | Loss: 0.00010905
Iteration 39/1000 | Loss: 0.00055240
Iteration 40/1000 | Loss: 0.00044748
Iteration 41/1000 | Loss: 0.00032046
Iteration 42/1000 | Loss: 0.00054774
Iteration 43/1000 | Loss: 0.00046293
Iteration 44/1000 | Loss: 0.00020027
Iteration 45/1000 | Loss: 0.00013264
Iteration 46/1000 | Loss: 0.00013834
Iteration 47/1000 | Loss: 0.00007563
Iteration 48/1000 | Loss: 0.00021284
Iteration 49/1000 | Loss: 0.00023413
Iteration 50/1000 | Loss: 0.00032367
Iteration 51/1000 | Loss: 0.00024945
Iteration 52/1000 | Loss: 0.00021077
Iteration 53/1000 | Loss: 0.00019970
Iteration 54/1000 | Loss: 0.00022624
Iteration 55/1000 | Loss: 0.00016499
Iteration 56/1000 | Loss: 0.00006230
Iteration 57/1000 | Loss: 0.00004343
Iteration 58/1000 | Loss: 0.00007285
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00006839
Iteration 61/1000 | Loss: 0.00003571
Iteration 62/1000 | Loss: 0.00003905
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00012002
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00004733
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001559
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00005935
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00002099
Iteration 100/1000 | Loss: 0.00001530
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001514
Iteration 107/1000 | Loss: 0.00001512
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00012933
Iteration 110/1000 | Loss: 0.00005851
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00003902
Iteration 113/1000 | Loss: 0.00002022
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00013671
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001523
Iteration 118/1000 | Loss: 0.00001711
Iteration 119/1000 | Loss: 0.00001501
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00007691
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001813
Iteration 129/1000 | Loss: 0.00001483
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001375
Iteration 138/1000 | Loss: 0.00007262
Iteration 139/1000 | Loss: 0.00011532
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00006612
Iteration 143/1000 | Loss: 0.00007851
Iteration 144/1000 | Loss: 0.00007216
Iteration 145/1000 | Loss: 0.00002755
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00002280
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001359
Iteration 158/1000 | Loss: 0.00001359
Iteration 159/1000 | Loss: 0.00001359
Iteration 160/1000 | Loss: 0.00001359
Iteration 161/1000 | Loss: 0.00001359
Iteration 162/1000 | Loss: 0.00001358
Iteration 163/1000 | Loss: 0.00001358
Iteration 164/1000 | Loss: 0.00001358
Iteration 165/1000 | Loss: 0.00001358
Iteration 166/1000 | Loss: 0.00001358
Iteration 167/1000 | Loss: 0.00001358
Iteration 168/1000 | Loss: 0.00001358
Iteration 169/1000 | Loss: 0.00001358
Iteration 170/1000 | Loss: 0.00001358
Iteration 171/1000 | Loss: 0.00001358
Iteration 172/1000 | Loss: 0.00001358
Iteration 173/1000 | Loss: 0.00001357
Iteration 174/1000 | Loss: 0.00001357
Iteration 175/1000 | Loss: 0.00001357
Iteration 176/1000 | Loss: 0.00001357
Iteration 177/1000 | Loss: 0.00001357
Iteration 178/1000 | Loss: 0.00001357
Iteration 179/1000 | Loss: 0.00001357
Iteration 180/1000 | Loss: 0.00001357
Iteration 181/1000 | Loss: 0.00001357
Iteration 182/1000 | Loss: 0.00001357
Iteration 183/1000 | Loss: 0.00001357
Iteration 184/1000 | Loss: 0.00001357
Iteration 185/1000 | Loss: 0.00001356
Iteration 186/1000 | Loss: 0.00001356
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001353
Iteration 189/1000 | Loss: 0.00001353
Iteration 190/1000 | Loss: 0.00001353
Iteration 191/1000 | Loss: 0.00001353
Iteration 192/1000 | Loss: 0.00001353
Iteration 193/1000 | Loss: 0.00001353
Iteration 194/1000 | Loss: 0.00001353
Iteration 195/1000 | Loss: 0.00001353
Iteration 196/1000 | Loss: 0.00001353
Iteration 197/1000 | Loss: 0.00003391
Iteration 198/1000 | Loss: 0.00001368
Iteration 199/1000 | Loss: 0.00001355
Iteration 200/1000 | Loss: 0.00001350
Iteration 201/1000 | Loss: 0.00001350
Iteration 202/1000 | Loss: 0.00001350
Iteration 203/1000 | Loss: 0.00001350
Iteration 204/1000 | Loss: 0.00001350
Iteration 205/1000 | Loss: 0.00001350
Iteration 206/1000 | Loss: 0.00001349
Iteration 207/1000 | Loss: 0.00001349
Iteration 208/1000 | Loss: 0.00001349
Iteration 209/1000 | Loss: 0.00001348
Iteration 210/1000 | Loss: 0.00001348
Iteration 211/1000 | Loss: 0.00001348
Iteration 212/1000 | Loss: 0.00001348
Iteration 213/1000 | Loss: 0.00001347
Iteration 214/1000 | Loss: 0.00001346
Iteration 215/1000 | Loss: 0.00001346
Iteration 216/1000 | Loss: 0.00001345
Iteration 217/1000 | Loss: 0.00001345
Iteration 218/1000 | Loss: 0.00001345
Iteration 219/1000 | Loss: 0.00001345
Iteration 220/1000 | Loss: 0.00001345
Iteration 221/1000 | Loss: 0.00001344
Iteration 222/1000 | Loss: 0.00001344
Iteration 223/1000 | Loss: 0.00001344
Iteration 224/1000 | Loss: 0.00001343
Iteration 225/1000 | Loss: 0.00001343
Iteration 226/1000 | Loss: 0.00001343
Iteration 227/1000 | Loss: 0.00001343
Iteration 228/1000 | Loss: 0.00001343
Iteration 229/1000 | Loss: 0.00001343
Iteration 230/1000 | Loss: 0.00001342
Iteration 231/1000 | Loss: 0.00001342
Iteration 232/1000 | Loss: 0.00001342
Iteration 233/1000 | Loss: 0.00001342
Iteration 234/1000 | Loss: 0.00001342
Iteration 235/1000 | Loss: 0.00001342
Iteration 236/1000 | Loss: 0.00001342
Iteration 237/1000 | Loss: 0.00001342
Iteration 238/1000 | Loss: 0.00001342
Iteration 239/1000 | Loss: 0.00001342
Iteration 240/1000 | Loss: 0.00001342
Iteration 241/1000 | Loss: 0.00001342
Iteration 242/1000 | Loss: 0.00001342
Iteration 243/1000 | Loss: 0.00001342
Iteration 244/1000 | Loss: 0.00001342
Iteration 245/1000 | Loss: 0.00001342
Iteration 246/1000 | Loss: 0.00001342
Iteration 247/1000 | Loss: 0.00001342
Iteration 248/1000 | Loss: 0.00001342
Iteration 249/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.3423333257378545e-05, 1.3423333257378545e-05, 1.3423333257378545e-05, 1.3423333257378545e-05, 1.3423333257378545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3423333257378545e-05

Optimization complete. Final v2v error: 3.0060551166534424 mm

Highest mean error: 4.000668048858643 mm for frame 134

Lowest mean error: 2.3642354011535645 mm for frame 2

Saving results

Total time: 201.49105381965637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797007
Iteration 2/25 | Loss: 0.00135269
Iteration 3/25 | Loss: 0.00113729
Iteration 4/25 | Loss: 0.00107699
Iteration 5/25 | Loss: 0.00106206
Iteration 6/25 | Loss: 0.00105513
Iteration 7/25 | Loss: 0.00105202
Iteration 8/25 | Loss: 0.00104463
Iteration 9/25 | Loss: 0.00104408
Iteration 10/25 | Loss: 0.00104388
Iteration 11/25 | Loss: 0.00105093
Iteration 12/25 | Loss: 0.00104690
Iteration 13/25 | Loss: 0.00103494
Iteration 14/25 | Loss: 0.00103440
Iteration 15/25 | Loss: 0.00103407
Iteration 16/25 | Loss: 0.00103374
Iteration 17/25 | Loss: 0.00103356
Iteration 18/25 | Loss: 0.00103331
Iteration 19/25 | Loss: 0.00103300
Iteration 20/25 | Loss: 0.00103313
Iteration 21/25 | Loss: 0.00103261
Iteration 22/25 | Loss: 0.00103216
Iteration 23/25 | Loss: 0.00103176
Iteration 24/25 | Loss: 0.00103080
Iteration 25/25 | Loss: 0.00103368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87421620
Iteration 2/25 | Loss: 0.00141671
Iteration 3/25 | Loss: 0.00133945
Iteration 4/25 | Loss: 0.00133944
Iteration 5/25 | Loss: 0.00133944
Iteration 6/25 | Loss: 0.00133944
Iteration 7/25 | Loss: 0.00133944
Iteration 8/25 | Loss: 0.00133944
Iteration 9/25 | Loss: 0.00133944
Iteration 10/25 | Loss: 0.00133944
Iteration 11/25 | Loss: 0.00133944
Iteration 12/25 | Loss: 0.00133944
Iteration 13/25 | Loss: 0.00133944
Iteration 14/25 | Loss: 0.00133944
Iteration 15/25 | Loss: 0.00133944
Iteration 16/25 | Loss: 0.00133944
Iteration 17/25 | Loss: 0.00133944
Iteration 18/25 | Loss: 0.00133944
Iteration 19/25 | Loss: 0.00133944
Iteration 20/25 | Loss: 0.00133944
Iteration 21/25 | Loss: 0.00133944
Iteration 22/25 | Loss: 0.00133944
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013394411653280258, 0.0013394411653280258, 0.0013394411653280258, 0.0013394411653280258, 0.0013394411653280258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013394411653280258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133944
Iteration 2/1000 | Loss: 0.00044109
Iteration 3/1000 | Loss: 0.00065148
Iteration 4/1000 | Loss: 0.00015974
Iteration 5/1000 | Loss: 0.00048581
Iteration 6/1000 | Loss: 0.00016568
Iteration 7/1000 | Loss: 0.00005381
Iteration 8/1000 | Loss: 0.00009311
Iteration 9/1000 | Loss: 0.00010065
Iteration 10/1000 | Loss: 0.00013742
Iteration 11/1000 | Loss: 0.00018212
Iteration 12/1000 | Loss: 0.00030418
Iteration 13/1000 | Loss: 0.00020659
Iteration 14/1000 | Loss: 0.00010096
Iteration 15/1000 | Loss: 0.00011064
Iteration 16/1000 | Loss: 0.00046745
Iteration 17/1000 | Loss: 0.00021817
Iteration 18/1000 | Loss: 0.00009789
Iteration 19/1000 | Loss: 0.00016125
Iteration 20/1000 | Loss: 0.00007551
Iteration 21/1000 | Loss: 0.00010298
Iteration 22/1000 | Loss: 0.00014820
Iteration 23/1000 | Loss: 0.00005309
Iteration 24/1000 | Loss: 0.00012433
Iteration 25/1000 | Loss: 0.00010938
Iteration 26/1000 | Loss: 0.00010451
Iteration 27/1000 | Loss: 0.00010754
Iteration 28/1000 | Loss: 0.00008632
Iteration 29/1000 | Loss: 0.00003443
Iteration 30/1000 | Loss: 0.00003448
Iteration 31/1000 | Loss: 0.00007588
Iteration 32/1000 | Loss: 0.00038672
Iteration 33/1000 | Loss: 0.00032583
Iteration 34/1000 | Loss: 0.00033877
Iteration 35/1000 | Loss: 0.00016910
Iteration 36/1000 | Loss: 0.00104302
Iteration 37/1000 | Loss: 0.00055779
Iteration 38/1000 | Loss: 0.00007558
Iteration 39/1000 | Loss: 0.00004632
Iteration 40/1000 | Loss: 0.00003788
Iteration 41/1000 | Loss: 0.00003241
Iteration 42/1000 | Loss: 0.00002888
Iteration 43/1000 | Loss: 0.00002696
Iteration 44/1000 | Loss: 0.00056696
Iteration 45/1000 | Loss: 0.00006380
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00034185
Iteration 48/1000 | Loss: 0.00002638
Iteration 49/1000 | Loss: 0.00004878
Iteration 50/1000 | Loss: 0.00021737
Iteration 51/1000 | Loss: 0.00013309
Iteration 52/1000 | Loss: 0.00016378
Iteration 53/1000 | Loss: 0.00015894
Iteration 54/1000 | Loss: 0.00003402
Iteration 55/1000 | Loss: 0.00012903
Iteration 56/1000 | Loss: 0.00024726
Iteration 57/1000 | Loss: 0.00009375
Iteration 58/1000 | Loss: 0.00036279
Iteration 59/1000 | Loss: 0.00032214
Iteration 60/1000 | Loss: 0.00006854
Iteration 61/1000 | Loss: 0.00003070
Iteration 62/1000 | Loss: 0.00002408
Iteration 63/1000 | Loss: 0.00046097
Iteration 64/1000 | Loss: 0.00002358
Iteration 65/1000 | Loss: 0.00002124
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001838
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001823
Iteration 72/1000 | Loss: 0.00001822
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001816
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00047243
Iteration 78/1000 | Loss: 0.00002407
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001716
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001573
Iteration 87/1000 | Loss: 0.00001569
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001568
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001563
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001557
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001552
Iteration 120/1000 | Loss: 0.00001552
Iteration 121/1000 | Loss: 0.00001552
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001550
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001545
Iteration 155/1000 | Loss: 0.00001545
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001543
Iteration 159/1000 | Loss: 0.00001543
Iteration 160/1000 | Loss: 0.00001543
Iteration 161/1000 | Loss: 0.00001543
Iteration 162/1000 | Loss: 0.00001543
Iteration 163/1000 | Loss: 0.00001543
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001542
Iteration 166/1000 | Loss: 0.00001542
Iteration 167/1000 | Loss: 0.00001542
Iteration 168/1000 | Loss: 0.00001542
Iteration 169/1000 | Loss: 0.00001542
Iteration 170/1000 | Loss: 0.00001542
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001541
Iteration 173/1000 | Loss: 0.00001541
Iteration 174/1000 | Loss: 0.00001541
Iteration 175/1000 | Loss: 0.00001541
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001540
Iteration 181/1000 | Loss: 0.00001540
Iteration 182/1000 | Loss: 0.00001539
Iteration 183/1000 | Loss: 0.00001539
Iteration 184/1000 | Loss: 0.00001539
Iteration 185/1000 | Loss: 0.00001539
Iteration 186/1000 | Loss: 0.00001539
Iteration 187/1000 | Loss: 0.00001539
Iteration 188/1000 | Loss: 0.00001539
Iteration 189/1000 | Loss: 0.00001539
Iteration 190/1000 | Loss: 0.00001539
Iteration 191/1000 | Loss: 0.00001539
Iteration 192/1000 | Loss: 0.00001539
Iteration 193/1000 | Loss: 0.00001539
Iteration 194/1000 | Loss: 0.00001539
Iteration 195/1000 | Loss: 0.00001539
Iteration 196/1000 | Loss: 0.00001539
Iteration 197/1000 | Loss: 0.00001539
Iteration 198/1000 | Loss: 0.00001539
Iteration 199/1000 | Loss: 0.00001539
Iteration 200/1000 | Loss: 0.00001539
Iteration 201/1000 | Loss: 0.00001539
Iteration 202/1000 | Loss: 0.00001539
Iteration 203/1000 | Loss: 0.00001539
Iteration 204/1000 | Loss: 0.00001539
Iteration 205/1000 | Loss: 0.00001539
Iteration 206/1000 | Loss: 0.00001539
Iteration 207/1000 | Loss: 0.00001539
Iteration 208/1000 | Loss: 0.00001539
Iteration 209/1000 | Loss: 0.00001539
Iteration 210/1000 | Loss: 0.00001539
Iteration 211/1000 | Loss: 0.00001539
Iteration 212/1000 | Loss: 0.00001539
Iteration 213/1000 | Loss: 0.00001539
Iteration 214/1000 | Loss: 0.00001539
Iteration 215/1000 | Loss: 0.00001539
Iteration 216/1000 | Loss: 0.00001539
Iteration 217/1000 | Loss: 0.00001539
Iteration 218/1000 | Loss: 0.00001539
Iteration 219/1000 | Loss: 0.00001539
Iteration 220/1000 | Loss: 0.00001539
Iteration 221/1000 | Loss: 0.00001539
Iteration 222/1000 | Loss: 0.00001539
Iteration 223/1000 | Loss: 0.00001539
Iteration 224/1000 | Loss: 0.00001539
Iteration 225/1000 | Loss: 0.00001539
Iteration 226/1000 | Loss: 0.00001539
Iteration 227/1000 | Loss: 0.00001539
Iteration 228/1000 | Loss: 0.00001539
Iteration 229/1000 | Loss: 0.00001539
Iteration 230/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.538799551781267e-05, 1.538799551781267e-05, 1.538799551781267e-05, 1.538799551781267e-05, 1.538799551781267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.538799551781267e-05

Optimization complete. Final v2v error: 3.3598084449768066 mm

Highest mean error: 4.8573455810546875 mm for frame 233

Lowest mean error: 2.92134165763855 mm for frame 212

Saving results

Total time: 191.69117879867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615955
Iteration 2/25 | Loss: 0.00135934
Iteration 3/25 | Loss: 0.00104638
Iteration 4/25 | Loss: 0.00100948
Iteration 5/25 | Loss: 0.00100508
Iteration 6/25 | Loss: 0.00100422
Iteration 7/25 | Loss: 0.00100410
Iteration 8/25 | Loss: 0.00100410
Iteration 9/25 | Loss: 0.00100410
Iteration 10/25 | Loss: 0.00100410
Iteration 11/25 | Loss: 0.00100410
Iteration 12/25 | Loss: 0.00100410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010041019413620234, 0.0010041019413620234, 0.0010041019413620234, 0.0010041019413620234, 0.0010041019413620234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010041019413620234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15306556
Iteration 2/25 | Loss: 0.00036445
Iteration 3/25 | Loss: 0.00036445
Iteration 4/25 | Loss: 0.00036445
Iteration 5/25 | Loss: 0.00036445
Iteration 6/25 | Loss: 0.00036445
Iteration 7/25 | Loss: 0.00036445
Iteration 8/25 | Loss: 0.00036445
Iteration 9/25 | Loss: 0.00036445
Iteration 10/25 | Loss: 0.00036445
Iteration 11/25 | Loss: 0.00036445
Iteration 12/25 | Loss: 0.00036445
Iteration 13/25 | Loss: 0.00036445
Iteration 14/25 | Loss: 0.00036445
Iteration 15/25 | Loss: 0.00036445
Iteration 16/25 | Loss: 0.00036445
Iteration 17/25 | Loss: 0.00036445
Iteration 18/25 | Loss: 0.00036445
Iteration 19/25 | Loss: 0.00036445
Iteration 20/25 | Loss: 0.00036445
Iteration 21/25 | Loss: 0.00036445
Iteration 22/25 | Loss: 0.00036445
Iteration 23/25 | Loss: 0.00036445
Iteration 24/25 | Loss: 0.00036445
Iteration 25/25 | Loss: 0.00036445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036445
Iteration 2/1000 | Loss: 0.00003393
Iteration 3/1000 | Loss: 0.00002179
Iteration 4/1000 | Loss: 0.00001738
Iteration 5/1000 | Loss: 0.00001647
Iteration 6/1000 | Loss: 0.00001570
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001517
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001465
Iteration 15/1000 | Loss: 0.00001465
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001464
Iteration 26/1000 | Loss: 0.00001464
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001464
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001449
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001443
Iteration 69/1000 | Loss: 0.00001443
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001442
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001431
Iteration 117/1000 | Loss: 0.00001431
Iteration 118/1000 | Loss: 0.00001431
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001429
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001428
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001427
Iteration 149/1000 | Loss: 0.00001427
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001427
Iteration 173/1000 | Loss: 0.00001427
Iteration 174/1000 | Loss: 0.00001427
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001427
Iteration 177/1000 | Loss: 0.00001427
Iteration 178/1000 | Loss: 0.00001427
Iteration 179/1000 | Loss: 0.00001427
Iteration 180/1000 | Loss: 0.00001427
Iteration 181/1000 | Loss: 0.00001427
Iteration 182/1000 | Loss: 0.00001427
Iteration 183/1000 | Loss: 0.00001427
Iteration 184/1000 | Loss: 0.00001427
Iteration 185/1000 | Loss: 0.00001427
Iteration 186/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.4272852240537759e-05, 1.4272852240537759e-05, 1.4272852240537759e-05, 1.4272852240537759e-05, 1.4272852240537759e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4272852240537759e-05

Optimization complete. Final v2v error: 2.9346296787261963 mm

Highest mean error: 5.37764835357666 mm for frame 55

Lowest mean error: 2.4275732040405273 mm for frame 141

Saving results

Total time: 36.569732427597046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880654
Iteration 2/25 | Loss: 0.00119015
Iteration 3/25 | Loss: 0.00103975
Iteration 4/25 | Loss: 0.00102288
Iteration 5/25 | Loss: 0.00101911
Iteration 6/25 | Loss: 0.00101861
Iteration 7/25 | Loss: 0.00101861
Iteration 8/25 | Loss: 0.00101861
Iteration 9/25 | Loss: 0.00101861
Iteration 10/25 | Loss: 0.00101861
Iteration 11/25 | Loss: 0.00101861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001018610317260027, 0.001018610317260027, 0.001018610317260027, 0.001018610317260027, 0.001018610317260027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001018610317260027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31163526
Iteration 2/25 | Loss: 0.00056183
Iteration 3/25 | Loss: 0.00056175
Iteration 4/25 | Loss: 0.00056175
Iteration 5/25 | Loss: 0.00056175
Iteration 6/25 | Loss: 0.00056175
Iteration 7/25 | Loss: 0.00056175
Iteration 8/25 | Loss: 0.00056175
Iteration 9/25 | Loss: 0.00056175
Iteration 10/25 | Loss: 0.00056175
Iteration 11/25 | Loss: 0.00056175
Iteration 12/25 | Loss: 0.00056175
Iteration 13/25 | Loss: 0.00056175
Iteration 14/25 | Loss: 0.00056175
Iteration 15/25 | Loss: 0.00056175
Iteration 16/25 | Loss: 0.00056175
Iteration 17/25 | Loss: 0.00056175
Iteration 18/25 | Loss: 0.00056175
Iteration 19/25 | Loss: 0.00056175
Iteration 20/25 | Loss: 0.00056175
Iteration 21/25 | Loss: 0.00056175
Iteration 22/25 | Loss: 0.00056175
Iteration 23/25 | Loss: 0.00056175
Iteration 24/25 | Loss: 0.00056175
Iteration 25/25 | Loss: 0.00056175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056175
Iteration 2/1000 | Loss: 0.00003480
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00001827
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001598
Iteration 7/1000 | Loss: 0.00001546
Iteration 8/1000 | Loss: 0.00001517
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001469
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001450
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001432
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001428
Iteration 19/1000 | Loss: 0.00001427
Iteration 20/1000 | Loss: 0.00001427
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001426
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001425
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001403
Iteration 55/1000 | Loss: 0.00001403
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001401
Iteration 86/1000 | Loss: 0.00001401
Iteration 87/1000 | Loss: 0.00001401
Iteration 88/1000 | Loss: 0.00001401
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.4006222954776604e-05, 1.4006222954776604e-05, 1.4006222954776604e-05, 1.4006222954776604e-05, 1.4006222954776604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4006222954776604e-05

Optimization complete. Final v2v error: 3.139404773712158 mm

Highest mean error: 3.7923696041107178 mm for frame 173

Lowest mean error: 2.594606876373291 mm for frame 24

Saving results

Total time: 37.709259033203125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805945
Iteration 2/25 | Loss: 0.00131373
Iteration 3/25 | Loss: 0.00103227
Iteration 4/25 | Loss: 0.00100070
Iteration 5/25 | Loss: 0.00099400
Iteration 6/25 | Loss: 0.00099160
Iteration 7/25 | Loss: 0.00099155
Iteration 8/25 | Loss: 0.00099146
Iteration 9/25 | Loss: 0.00099146
Iteration 10/25 | Loss: 0.00099146
Iteration 11/25 | Loss: 0.00099146
Iteration 12/25 | Loss: 0.00099146
Iteration 13/25 | Loss: 0.00099146
Iteration 14/25 | Loss: 0.00099146
Iteration 15/25 | Loss: 0.00099146
Iteration 16/25 | Loss: 0.00099146
Iteration 17/25 | Loss: 0.00099146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00099145935382694, 0.00099145935382694, 0.00099145935382694, 0.00099145935382694, 0.00099145935382694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00099145935382694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.35400295
Iteration 2/25 | Loss: 0.00066566
Iteration 3/25 | Loss: 0.00066565
Iteration 4/25 | Loss: 0.00066565
Iteration 5/25 | Loss: 0.00066565
Iteration 6/25 | Loss: 0.00066565
Iteration 7/25 | Loss: 0.00066564
Iteration 8/25 | Loss: 0.00066564
Iteration 9/25 | Loss: 0.00066564
Iteration 10/25 | Loss: 0.00066564
Iteration 11/25 | Loss: 0.00066564
Iteration 12/25 | Loss: 0.00066564
Iteration 13/25 | Loss: 0.00066564
Iteration 14/25 | Loss: 0.00066564
Iteration 15/25 | Loss: 0.00066564
Iteration 16/25 | Loss: 0.00066564
Iteration 17/25 | Loss: 0.00066564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006656434270553291, 0.0006656434270553291, 0.0006656434270553291, 0.0006656434270553291, 0.0006656434270553291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006656434270553291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066564
Iteration 2/1000 | Loss: 0.00003714
Iteration 3/1000 | Loss: 0.00001952
Iteration 4/1000 | Loss: 0.00001712
Iteration 5/1000 | Loss: 0.00001597
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001467
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001458
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001449
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001440
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001435
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001426
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001422
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001421
Iteration 40/1000 | Loss: 0.00001421
Iteration 41/1000 | Loss: 0.00001420
Iteration 42/1000 | Loss: 0.00001420
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001418
Iteration 46/1000 | Loss: 0.00001418
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001415
Iteration 51/1000 | Loss: 0.00001415
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001414
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001412
Iteration 59/1000 | Loss: 0.00001412
Iteration 60/1000 | Loss: 0.00001412
Iteration 61/1000 | Loss: 0.00001412
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001411
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001411
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001411
Iteration 70/1000 | Loss: 0.00001411
Iteration 71/1000 | Loss: 0.00001411
Iteration 72/1000 | Loss: 0.00001411
Iteration 73/1000 | Loss: 0.00001411
Iteration 74/1000 | Loss: 0.00001411
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001411
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001411
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001411
Iteration 113/1000 | Loss: 0.00001411
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4108622053754516e-05, 1.4108622053754516e-05, 1.4108622053754516e-05, 1.4108622053754516e-05, 1.4108622053754516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4108622053754516e-05

Optimization complete. Final v2v error: 3.133172035217285 mm

Highest mean error: 3.9682183265686035 mm for frame 0

Lowest mean error: 2.6217739582061768 mm for frame 239

Saving results

Total time: 36.39584255218506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984006
Iteration 2/25 | Loss: 0.00984006
Iteration 3/25 | Loss: 0.00290822
Iteration 4/25 | Loss: 0.00188719
Iteration 5/25 | Loss: 0.00159353
Iteration 6/25 | Loss: 0.00125318
Iteration 7/25 | Loss: 0.00110533
Iteration 8/25 | Loss: 0.00105331
Iteration 9/25 | Loss: 0.00102325
Iteration 10/25 | Loss: 0.00101084
Iteration 11/25 | Loss: 0.00100541
Iteration 12/25 | Loss: 0.00100330
Iteration 13/25 | Loss: 0.00100226
Iteration 14/25 | Loss: 0.00100184
Iteration 15/25 | Loss: 0.00100152
Iteration 16/25 | Loss: 0.00100121
Iteration 17/25 | Loss: 0.00100098
Iteration 18/25 | Loss: 0.00100077
Iteration 19/25 | Loss: 0.00100057
Iteration 20/25 | Loss: 0.00100044
Iteration 21/25 | Loss: 0.00100037
Iteration 22/25 | Loss: 0.00100035
Iteration 23/25 | Loss: 0.00100034
Iteration 24/25 | Loss: 0.00100034
Iteration 25/25 | Loss: 0.00100034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31689799
Iteration 2/25 | Loss: 0.00084393
Iteration 3/25 | Loss: 0.00079200
Iteration 4/25 | Loss: 0.00079200
Iteration 5/25 | Loss: 0.00079199
Iteration 6/25 | Loss: 0.00079199
Iteration 7/25 | Loss: 0.00079199
Iteration 8/25 | Loss: 0.00079199
Iteration 9/25 | Loss: 0.00079199
Iteration 10/25 | Loss: 0.00079199
Iteration 11/25 | Loss: 0.00079199
Iteration 12/25 | Loss: 0.00079199
Iteration 13/25 | Loss: 0.00079199
Iteration 14/25 | Loss: 0.00079199
Iteration 15/25 | Loss: 0.00079199
Iteration 16/25 | Loss: 0.00079199
Iteration 17/25 | Loss: 0.00079199
Iteration 18/25 | Loss: 0.00079199
Iteration 19/25 | Loss: 0.00079199
Iteration 20/25 | Loss: 0.00079199
Iteration 21/25 | Loss: 0.00079199
Iteration 22/25 | Loss: 0.00079199
Iteration 23/25 | Loss: 0.00079199
Iteration 24/25 | Loss: 0.00079199
Iteration 25/25 | Loss: 0.00079199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079199
Iteration 2/1000 | Loss: 0.00045262
Iteration 3/1000 | Loss: 0.00029701
Iteration 4/1000 | Loss: 0.00028083
Iteration 5/1000 | Loss: 0.00016963
Iteration 6/1000 | Loss: 0.00052224
Iteration 7/1000 | Loss: 0.00005371
Iteration 8/1000 | Loss: 0.00004179
Iteration 9/1000 | Loss: 0.00010824
Iteration 10/1000 | Loss: 0.00022205
Iteration 11/1000 | Loss: 0.00004181
Iteration 12/1000 | Loss: 0.00056613
Iteration 13/1000 | Loss: 0.00003030
Iteration 14/1000 | Loss: 0.00002657
Iteration 15/1000 | Loss: 0.00002393
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00002158
Iteration 18/1000 | Loss: 0.00037636
Iteration 19/1000 | Loss: 0.00012681
Iteration 20/1000 | Loss: 0.00024098
Iteration 21/1000 | Loss: 0.00008794
Iteration 22/1000 | Loss: 0.00003658
Iteration 23/1000 | Loss: 0.00004141
Iteration 24/1000 | Loss: 0.00002050
Iteration 25/1000 | Loss: 0.00007002
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00001521
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00009979
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001297
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001107
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001105
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001101
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001098
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001092
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001091
Iteration 74/1000 | Loss: 0.00001091
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001090
Iteration 78/1000 | Loss: 0.00001090
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001088
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001087
Iteration 86/1000 | Loss: 0.00001087
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001086
Iteration 89/1000 | Loss: 0.00001085
Iteration 90/1000 | Loss: 0.00001085
Iteration 91/1000 | Loss: 0.00001085
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001085
Iteration 94/1000 | Loss: 0.00001085
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001084
Iteration 97/1000 | Loss: 0.00001084
Iteration 98/1000 | Loss: 0.00001084
Iteration 99/1000 | Loss: 0.00001084
Iteration 100/1000 | Loss: 0.00001084
Iteration 101/1000 | Loss: 0.00001084
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001084
Iteration 106/1000 | Loss: 0.00001084
Iteration 107/1000 | Loss: 0.00001084
Iteration 108/1000 | Loss: 0.00001084
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001084
Iteration 111/1000 | Loss: 0.00001084
Iteration 112/1000 | Loss: 0.00001084
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.0841488801816013e-05, 1.0841488801816013e-05, 1.0841488801816013e-05, 1.0841488801816013e-05, 1.0841488801816013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0841488801816013e-05

Optimization complete. Final v2v error: 2.6356379985809326 mm

Highest mean error: 10.391114234924316 mm for frame 2

Lowest mean error: 2.3133351802825928 mm for frame 129

Saving results

Total time: 111.76098442077637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855796
Iteration 2/25 | Loss: 0.00142511
Iteration 3/25 | Loss: 0.00108489
Iteration 4/25 | Loss: 0.00104751
Iteration 5/25 | Loss: 0.00103786
Iteration 6/25 | Loss: 0.00103572
Iteration 7/25 | Loss: 0.00103572
Iteration 8/25 | Loss: 0.00103572
Iteration 9/25 | Loss: 0.00103572
Iteration 10/25 | Loss: 0.00103572
Iteration 11/25 | Loss: 0.00103572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00103572232183069, 0.00103572232183069, 0.00103572232183069, 0.00103572232183069, 0.00103572232183069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00103572232183069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31086266
Iteration 2/25 | Loss: 0.00059726
Iteration 3/25 | Loss: 0.00059726
Iteration 4/25 | Loss: 0.00059726
Iteration 5/25 | Loss: 0.00059726
Iteration 6/25 | Loss: 0.00059726
Iteration 7/25 | Loss: 0.00059726
Iteration 8/25 | Loss: 0.00059726
Iteration 9/25 | Loss: 0.00059726
Iteration 10/25 | Loss: 0.00059726
Iteration 11/25 | Loss: 0.00059726
Iteration 12/25 | Loss: 0.00059726
Iteration 13/25 | Loss: 0.00059726
Iteration 14/25 | Loss: 0.00059726
Iteration 15/25 | Loss: 0.00059726
Iteration 16/25 | Loss: 0.00059726
Iteration 17/25 | Loss: 0.00059726
Iteration 18/25 | Loss: 0.00059726
Iteration 19/25 | Loss: 0.00059726
Iteration 20/25 | Loss: 0.00059726
Iteration 21/25 | Loss: 0.00059726
Iteration 22/25 | Loss: 0.00059726
Iteration 23/25 | Loss: 0.00059726
Iteration 24/25 | Loss: 0.00059726
Iteration 25/25 | Loss: 0.00059726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059726
Iteration 2/1000 | Loss: 0.00002662
Iteration 3/1000 | Loss: 0.00001948
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00001739
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001590
Iteration 12/1000 | Loss: 0.00001576
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001566
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001559
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001554
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001551
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001544
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001541
Iteration 95/1000 | Loss: 0.00001541
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001539
Iteration 114/1000 | Loss: 0.00001539
Iteration 115/1000 | Loss: 0.00001539
Iteration 116/1000 | Loss: 0.00001539
Iteration 117/1000 | Loss: 0.00001539
Iteration 118/1000 | Loss: 0.00001539
Iteration 119/1000 | Loss: 0.00001538
Iteration 120/1000 | Loss: 0.00001538
Iteration 121/1000 | Loss: 0.00001538
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001537
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001536
Iteration 135/1000 | Loss: 0.00001536
Iteration 136/1000 | Loss: 0.00001536
Iteration 137/1000 | Loss: 0.00001536
Iteration 138/1000 | Loss: 0.00001536
Iteration 139/1000 | Loss: 0.00001536
Iteration 140/1000 | Loss: 0.00001536
Iteration 141/1000 | Loss: 0.00001536
Iteration 142/1000 | Loss: 0.00001536
Iteration 143/1000 | Loss: 0.00001536
Iteration 144/1000 | Loss: 0.00001536
Iteration 145/1000 | Loss: 0.00001536
Iteration 146/1000 | Loss: 0.00001536
Iteration 147/1000 | Loss: 0.00001536
Iteration 148/1000 | Loss: 0.00001536
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001536
Iteration 156/1000 | Loss: 0.00001536
Iteration 157/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5359086319222115e-05, 1.5359086319222115e-05, 1.5359086319222115e-05, 1.5359086319222115e-05, 1.5359086319222115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5359086319222115e-05

Optimization complete. Final v2v error: 3.1782901287078857 mm

Highest mean error: 3.714951992034912 mm for frame 189

Lowest mean error: 2.9056951999664307 mm for frame 8

Saving results

Total time: 38.840975761413574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932698
Iteration 2/25 | Loss: 0.00135960
Iteration 3/25 | Loss: 0.00110327
Iteration 4/25 | Loss: 0.00106334
Iteration 5/25 | Loss: 0.00105422
Iteration 6/25 | Loss: 0.00105197
Iteration 7/25 | Loss: 0.00105182
Iteration 8/25 | Loss: 0.00105182
Iteration 9/25 | Loss: 0.00105182
Iteration 10/25 | Loss: 0.00105182
Iteration 11/25 | Loss: 0.00105182
Iteration 12/25 | Loss: 0.00105182
Iteration 13/25 | Loss: 0.00105182
Iteration 14/25 | Loss: 0.00105182
Iteration 15/25 | Loss: 0.00105182
Iteration 16/25 | Loss: 0.00105182
Iteration 17/25 | Loss: 0.00105182
Iteration 18/25 | Loss: 0.00105182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010518213966861367, 0.0010518213966861367, 0.0010518213966861367, 0.0010518213966861367, 0.0010518213966861367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010518213966861367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15880859
Iteration 2/25 | Loss: 0.00077733
Iteration 3/25 | Loss: 0.00077733
Iteration 4/25 | Loss: 0.00077733
Iteration 5/25 | Loss: 0.00077733
Iteration 6/25 | Loss: 0.00077732
Iteration 7/25 | Loss: 0.00077732
Iteration 8/25 | Loss: 0.00077732
Iteration 9/25 | Loss: 0.00077732
Iteration 10/25 | Loss: 0.00077732
Iteration 11/25 | Loss: 0.00077732
Iteration 12/25 | Loss: 0.00077732
Iteration 13/25 | Loss: 0.00077732
Iteration 14/25 | Loss: 0.00077732
Iteration 15/25 | Loss: 0.00077732
Iteration 16/25 | Loss: 0.00077732
Iteration 17/25 | Loss: 0.00077732
Iteration 18/25 | Loss: 0.00077732
Iteration 19/25 | Loss: 0.00077732
Iteration 20/25 | Loss: 0.00077732
Iteration 21/25 | Loss: 0.00077732
Iteration 22/25 | Loss: 0.00077732
Iteration 23/25 | Loss: 0.00077732
Iteration 24/25 | Loss: 0.00077732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007773240213282406, 0.0007773240213282406, 0.0007773240213282406, 0.0007773240213282406, 0.0007773240213282406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007773240213282406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077732
Iteration 2/1000 | Loss: 0.00005106
Iteration 3/1000 | Loss: 0.00003007
Iteration 4/1000 | Loss: 0.00002457
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002173
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002048
Iteration 9/1000 | Loss: 0.00002008
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001955
Iteration 12/1000 | Loss: 0.00001954
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00001938
Iteration 15/1000 | Loss: 0.00001931
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00001928
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00001926
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001916
Iteration 24/1000 | Loss: 0.00001916
Iteration 25/1000 | Loss: 0.00001914
Iteration 26/1000 | Loss: 0.00001914
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001912
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001912
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001907
Iteration 40/1000 | Loss: 0.00001907
Iteration 41/1000 | Loss: 0.00001907
Iteration 42/1000 | Loss: 0.00001906
Iteration 43/1000 | Loss: 0.00001906
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001902
Iteration 47/1000 | Loss: 0.00001901
Iteration 48/1000 | Loss: 0.00001901
Iteration 49/1000 | Loss: 0.00001901
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001900
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001898
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001895
Iteration 72/1000 | Loss: 0.00001895
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001888
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001888
Iteration 109/1000 | Loss: 0.00001888
Iteration 110/1000 | Loss: 0.00001888
Iteration 111/1000 | Loss: 0.00001887
Iteration 112/1000 | Loss: 0.00001887
Iteration 113/1000 | Loss: 0.00001887
Iteration 114/1000 | Loss: 0.00001887
Iteration 115/1000 | Loss: 0.00001886
Iteration 116/1000 | Loss: 0.00001886
Iteration 117/1000 | Loss: 0.00001886
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001885
Iteration 121/1000 | Loss: 0.00001885
Iteration 122/1000 | Loss: 0.00001885
Iteration 123/1000 | Loss: 0.00001885
Iteration 124/1000 | Loss: 0.00001885
Iteration 125/1000 | Loss: 0.00001885
Iteration 126/1000 | Loss: 0.00001884
Iteration 127/1000 | Loss: 0.00001884
Iteration 128/1000 | Loss: 0.00001884
Iteration 129/1000 | Loss: 0.00001884
Iteration 130/1000 | Loss: 0.00001884
Iteration 131/1000 | Loss: 0.00001883
Iteration 132/1000 | Loss: 0.00001883
Iteration 133/1000 | Loss: 0.00001883
Iteration 134/1000 | Loss: 0.00001883
Iteration 135/1000 | Loss: 0.00001883
Iteration 136/1000 | Loss: 0.00001883
Iteration 137/1000 | Loss: 0.00001883
Iteration 138/1000 | Loss: 0.00001883
Iteration 139/1000 | Loss: 0.00001883
Iteration 140/1000 | Loss: 0.00001883
Iteration 141/1000 | Loss: 0.00001883
Iteration 142/1000 | Loss: 0.00001883
Iteration 143/1000 | Loss: 0.00001883
Iteration 144/1000 | Loss: 0.00001883
Iteration 145/1000 | Loss: 0.00001883
Iteration 146/1000 | Loss: 0.00001882
Iteration 147/1000 | Loss: 0.00001882
Iteration 148/1000 | Loss: 0.00001882
Iteration 149/1000 | Loss: 0.00001882
Iteration 150/1000 | Loss: 0.00001882
Iteration 151/1000 | Loss: 0.00001882
Iteration 152/1000 | Loss: 0.00001882
Iteration 153/1000 | Loss: 0.00001882
Iteration 154/1000 | Loss: 0.00001882
Iteration 155/1000 | Loss: 0.00001882
Iteration 156/1000 | Loss: 0.00001882
Iteration 157/1000 | Loss: 0.00001882
Iteration 158/1000 | Loss: 0.00001882
Iteration 159/1000 | Loss: 0.00001882
Iteration 160/1000 | Loss: 0.00001882
Iteration 161/1000 | Loss: 0.00001882
Iteration 162/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.882352626125794e-05, 1.882352626125794e-05, 1.882352626125794e-05, 1.882352626125794e-05, 1.882352626125794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.882352626125794e-05

Optimization complete. Final v2v error: 3.642439603805542 mm

Highest mean error: 4.248220920562744 mm for frame 207

Lowest mean error: 3.059884786605835 mm for frame 237

Saving results

Total time: 45.20608949661255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376640
Iteration 2/25 | Loss: 0.00100537
Iteration 3/25 | Loss: 0.00091598
Iteration 4/25 | Loss: 0.00090765
Iteration 5/25 | Loss: 0.00090573
Iteration 6/25 | Loss: 0.00090492
Iteration 7/25 | Loss: 0.00090492
Iteration 8/25 | Loss: 0.00090492
Iteration 9/25 | Loss: 0.00090492
Iteration 10/25 | Loss: 0.00090492
Iteration 11/25 | Loss: 0.00090492
Iteration 12/25 | Loss: 0.00090492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000904918008018285, 0.000904918008018285, 0.000904918008018285, 0.000904918008018285, 0.000904918008018285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000904918008018285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63064289
Iteration 2/25 | Loss: 0.00060353
Iteration 3/25 | Loss: 0.00060352
Iteration 4/25 | Loss: 0.00060352
Iteration 5/25 | Loss: 0.00060352
Iteration 6/25 | Loss: 0.00060352
Iteration 7/25 | Loss: 0.00060352
Iteration 8/25 | Loss: 0.00060352
Iteration 9/25 | Loss: 0.00060352
Iteration 10/25 | Loss: 0.00060352
Iteration 11/25 | Loss: 0.00060352
Iteration 12/25 | Loss: 0.00060352
Iteration 13/25 | Loss: 0.00060352
Iteration 14/25 | Loss: 0.00060352
Iteration 15/25 | Loss: 0.00060352
Iteration 16/25 | Loss: 0.00060352
Iteration 17/25 | Loss: 0.00060352
Iteration 18/25 | Loss: 0.00060352
Iteration 19/25 | Loss: 0.00060352
Iteration 20/25 | Loss: 0.00060352
Iteration 21/25 | Loss: 0.00060352
Iteration 22/25 | Loss: 0.00060352
Iteration 23/25 | Loss: 0.00060352
Iteration 24/25 | Loss: 0.00060352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006035218248143792, 0.0006035218248143792, 0.0006035218248143792, 0.0006035218248143792, 0.0006035218248143792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006035218248143792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060352
Iteration 2/1000 | Loss: 0.00002664
Iteration 3/1000 | Loss: 0.00001120
Iteration 4/1000 | Loss: 0.00000948
Iteration 5/1000 | Loss: 0.00000885
Iteration 6/1000 | Loss: 0.00000854
Iteration 7/1000 | Loss: 0.00000851
Iteration 8/1000 | Loss: 0.00000825
Iteration 9/1000 | Loss: 0.00000808
Iteration 10/1000 | Loss: 0.00000806
Iteration 11/1000 | Loss: 0.00000802
Iteration 12/1000 | Loss: 0.00000802
Iteration 13/1000 | Loss: 0.00000802
Iteration 14/1000 | Loss: 0.00000801
Iteration 15/1000 | Loss: 0.00000801
Iteration 16/1000 | Loss: 0.00000800
Iteration 17/1000 | Loss: 0.00000800
Iteration 18/1000 | Loss: 0.00000799
Iteration 19/1000 | Loss: 0.00000799
Iteration 20/1000 | Loss: 0.00000794
Iteration 21/1000 | Loss: 0.00000794
Iteration 22/1000 | Loss: 0.00000791
Iteration 23/1000 | Loss: 0.00000790
Iteration 24/1000 | Loss: 0.00000789
Iteration 25/1000 | Loss: 0.00000789
Iteration 26/1000 | Loss: 0.00000788
Iteration 27/1000 | Loss: 0.00000788
Iteration 28/1000 | Loss: 0.00000787
Iteration 29/1000 | Loss: 0.00000785
Iteration 30/1000 | Loss: 0.00000784
Iteration 31/1000 | Loss: 0.00000784
Iteration 32/1000 | Loss: 0.00000783
Iteration 33/1000 | Loss: 0.00000781
Iteration 34/1000 | Loss: 0.00000781
Iteration 35/1000 | Loss: 0.00000780
Iteration 36/1000 | Loss: 0.00000780
Iteration 37/1000 | Loss: 0.00000780
Iteration 38/1000 | Loss: 0.00000779
Iteration 39/1000 | Loss: 0.00000779
Iteration 40/1000 | Loss: 0.00000778
Iteration 41/1000 | Loss: 0.00000777
Iteration 42/1000 | Loss: 0.00000777
Iteration 43/1000 | Loss: 0.00000777
Iteration 44/1000 | Loss: 0.00000777
Iteration 45/1000 | Loss: 0.00000776
Iteration 46/1000 | Loss: 0.00000776
Iteration 47/1000 | Loss: 0.00000776
Iteration 48/1000 | Loss: 0.00000775
Iteration 49/1000 | Loss: 0.00000774
Iteration 50/1000 | Loss: 0.00000774
Iteration 51/1000 | Loss: 0.00000773
Iteration 52/1000 | Loss: 0.00000773
Iteration 53/1000 | Loss: 0.00000772
Iteration 54/1000 | Loss: 0.00000772
Iteration 55/1000 | Loss: 0.00000772
Iteration 56/1000 | Loss: 0.00000772
Iteration 57/1000 | Loss: 0.00000771
Iteration 58/1000 | Loss: 0.00000771
Iteration 59/1000 | Loss: 0.00000771
Iteration 60/1000 | Loss: 0.00000771
Iteration 61/1000 | Loss: 0.00000770
Iteration 62/1000 | Loss: 0.00000770
Iteration 63/1000 | Loss: 0.00000770
Iteration 64/1000 | Loss: 0.00000770
Iteration 65/1000 | Loss: 0.00000770
Iteration 66/1000 | Loss: 0.00000770
Iteration 67/1000 | Loss: 0.00000769
Iteration 68/1000 | Loss: 0.00000769
Iteration 69/1000 | Loss: 0.00000769
Iteration 70/1000 | Loss: 0.00000769
Iteration 71/1000 | Loss: 0.00000769
Iteration 72/1000 | Loss: 0.00000769
Iteration 73/1000 | Loss: 0.00000769
Iteration 74/1000 | Loss: 0.00000769
Iteration 75/1000 | Loss: 0.00000769
Iteration 76/1000 | Loss: 0.00000769
Iteration 77/1000 | Loss: 0.00000768
Iteration 78/1000 | Loss: 0.00000768
Iteration 79/1000 | Loss: 0.00000768
Iteration 80/1000 | Loss: 0.00000768
Iteration 81/1000 | Loss: 0.00000768
Iteration 82/1000 | Loss: 0.00000768
Iteration 83/1000 | Loss: 0.00000768
Iteration 84/1000 | Loss: 0.00000767
Iteration 85/1000 | Loss: 0.00000767
Iteration 86/1000 | Loss: 0.00000767
Iteration 87/1000 | Loss: 0.00000767
Iteration 88/1000 | Loss: 0.00000767
Iteration 89/1000 | Loss: 0.00000767
Iteration 90/1000 | Loss: 0.00000766
Iteration 91/1000 | Loss: 0.00000766
Iteration 92/1000 | Loss: 0.00000766
Iteration 93/1000 | Loss: 0.00000766
Iteration 94/1000 | Loss: 0.00000766
Iteration 95/1000 | Loss: 0.00000766
Iteration 96/1000 | Loss: 0.00000766
Iteration 97/1000 | Loss: 0.00000766
Iteration 98/1000 | Loss: 0.00000766
Iteration 99/1000 | Loss: 0.00000765
Iteration 100/1000 | Loss: 0.00000765
Iteration 101/1000 | Loss: 0.00000765
Iteration 102/1000 | Loss: 0.00000765
Iteration 103/1000 | Loss: 0.00000765
Iteration 104/1000 | Loss: 0.00000764
Iteration 105/1000 | Loss: 0.00000764
Iteration 106/1000 | Loss: 0.00000764
Iteration 107/1000 | Loss: 0.00000764
Iteration 108/1000 | Loss: 0.00000764
Iteration 109/1000 | Loss: 0.00000763
Iteration 110/1000 | Loss: 0.00000763
Iteration 111/1000 | Loss: 0.00000763
Iteration 112/1000 | Loss: 0.00000763
Iteration 113/1000 | Loss: 0.00000763
Iteration 114/1000 | Loss: 0.00000763
Iteration 115/1000 | Loss: 0.00000763
Iteration 116/1000 | Loss: 0.00000763
Iteration 117/1000 | Loss: 0.00000763
Iteration 118/1000 | Loss: 0.00000762
Iteration 119/1000 | Loss: 0.00000762
Iteration 120/1000 | Loss: 0.00000762
Iteration 121/1000 | Loss: 0.00000762
Iteration 122/1000 | Loss: 0.00000762
Iteration 123/1000 | Loss: 0.00000762
Iteration 124/1000 | Loss: 0.00000762
Iteration 125/1000 | Loss: 0.00000762
Iteration 126/1000 | Loss: 0.00000762
Iteration 127/1000 | Loss: 0.00000762
Iteration 128/1000 | Loss: 0.00000762
Iteration 129/1000 | Loss: 0.00000762
Iteration 130/1000 | Loss: 0.00000762
Iteration 131/1000 | Loss: 0.00000762
Iteration 132/1000 | Loss: 0.00000762
Iteration 133/1000 | Loss: 0.00000762
Iteration 134/1000 | Loss: 0.00000762
Iteration 135/1000 | Loss: 0.00000762
Iteration 136/1000 | Loss: 0.00000762
Iteration 137/1000 | Loss: 0.00000762
Iteration 138/1000 | Loss: 0.00000762
Iteration 139/1000 | Loss: 0.00000762
Iteration 140/1000 | Loss: 0.00000762
Iteration 141/1000 | Loss: 0.00000762
Iteration 142/1000 | Loss: 0.00000762
Iteration 143/1000 | Loss: 0.00000762
Iteration 144/1000 | Loss: 0.00000762
Iteration 145/1000 | Loss: 0.00000762
Iteration 146/1000 | Loss: 0.00000762
Iteration 147/1000 | Loss: 0.00000762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [7.621015356562566e-06, 7.621015356562566e-06, 7.621015356562566e-06, 7.621015356562566e-06, 7.621015356562566e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.621015356562566e-06

Optimization complete. Final v2v error: 2.3509697914123535 mm

Highest mean error: 2.7376582622528076 mm for frame 75

Lowest mean error: 2.2213199138641357 mm for frame 64

Saving results

Total time: 31.988489627838135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410481
Iteration 2/25 | Loss: 0.00107378
Iteration 3/25 | Loss: 0.00095189
Iteration 4/25 | Loss: 0.00093793
Iteration 5/25 | Loss: 0.00093468
Iteration 6/25 | Loss: 0.00093362
Iteration 7/25 | Loss: 0.00093355
Iteration 8/25 | Loss: 0.00093355
Iteration 9/25 | Loss: 0.00093355
Iteration 10/25 | Loss: 0.00093355
Iteration 11/25 | Loss: 0.00093355
Iteration 12/25 | Loss: 0.00093355
Iteration 13/25 | Loss: 0.00093355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009335460490547121, 0.0009335460490547121, 0.0009335460490547121, 0.0009335460490547121, 0.0009335460490547121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009335460490547121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32608807
Iteration 2/25 | Loss: 0.00072674
Iteration 3/25 | Loss: 0.00072671
Iteration 4/25 | Loss: 0.00072671
Iteration 5/25 | Loss: 0.00072671
Iteration 6/25 | Loss: 0.00072671
Iteration 7/25 | Loss: 0.00072671
Iteration 8/25 | Loss: 0.00072671
Iteration 9/25 | Loss: 0.00072671
Iteration 10/25 | Loss: 0.00072671
Iteration 11/25 | Loss: 0.00072671
Iteration 12/25 | Loss: 0.00072671
Iteration 13/25 | Loss: 0.00072671
Iteration 14/25 | Loss: 0.00072671
Iteration 15/25 | Loss: 0.00072671
Iteration 16/25 | Loss: 0.00072671
Iteration 17/25 | Loss: 0.00072671
Iteration 18/25 | Loss: 0.00072671
Iteration 19/25 | Loss: 0.00072671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007267112960107625, 0.0007267112960107625, 0.0007267112960107625, 0.0007267112960107625, 0.0007267112960107625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007267112960107625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072671
Iteration 2/1000 | Loss: 0.00002100
Iteration 3/1000 | Loss: 0.00001156
Iteration 4/1000 | Loss: 0.00001009
Iteration 5/1000 | Loss: 0.00000923
Iteration 6/1000 | Loss: 0.00000886
Iteration 7/1000 | Loss: 0.00000881
Iteration 8/1000 | Loss: 0.00000870
Iteration 9/1000 | Loss: 0.00000868
Iteration 10/1000 | Loss: 0.00000867
Iteration 11/1000 | Loss: 0.00000866
Iteration 12/1000 | Loss: 0.00000865
Iteration 13/1000 | Loss: 0.00000851
Iteration 14/1000 | Loss: 0.00000850
Iteration 15/1000 | Loss: 0.00000845
Iteration 16/1000 | Loss: 0.00000845
Iteration 17/1000 | Loss: 0.00000844
Iteration 18/1000 | Loss: 0.00000844
Iteration 19/1000 | Loss: 0.00000844
Iteration 20/1000 | Loss: 0.00000843
Iteration 21/1000 | Loss: 0.00000842
Iteration 22/1000 | Loss: 0.00000842
Iteration 23/1000 | Loss: 0.00000841
Iteration 24/1000 | Loss: 0.00000841
Iteration 25/1000 | Loss: 0.00000840
Iteration 26/1000 | Loss: 0.00000840
Iteration 27/1000 | Loss: 0.00000839
Iteration 28/1000 | Loss: 0.00000839
Iteration 29/1000 | Loss: 0.00000838
Iteration 30/1000 | Loss: 0.00000838
Iteration 31/1000 | Loss: 0.00000836
Iteration 32/1000 | Loss: 0.00000836
Iteration 33/1000 | Loss: 0.00000835
Iteration 34/1000 | Loss: 0.00000835
Iteration 35/1000 | Loss: 0.00000835
Iteration 36/1000 | Loss: 0.00000835
Iteration 37/1000 | Loss: 0.00000834
Iteration 38/1000 | Loss: 0.00000834
Iteration 39/1000 | Loss: 0.00000834
Iteration 40/1000 | Loss: 0.00000834
Iteration 41/1000 | Loss: 0.00000834
Iteration 42/1000 | Loss: 0.00000833
Iteration 43/1000 | Loss: 0.00000833
Iteration 44/1000 | Loss: 0.00000833
Iteration 45/1000 | Loss: 0.00000832
Iteration 46/1000 | Loss: 0.00000832
Iteration 47/1000 | Loss: 0.00000832
Iteration 48/1000 | Loss: 0.00000831
Iteration 49/1000 | Loss: 0.00000830
Iteration 50/1000 | Loss: 0.00000830
Iteration 51/1000 | Loss: 0.00000829
Iteration 52/1000 | Loss: 0.00000828
Iteration 53/1000 | Loss: 0.00000827
Iteration 54/1000 | Loss: 0.00000827
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000826
Iteration 57/1000 | Loss: 0.00000826
Iteration 58/1000 | Loss: 0.00000826
Iteration 59/1000 | Loss: 0.00000826
Iteration 60/1000 | Loss: 0.00000826
Iteration 61/1000 | Loss: 0.00000826
Iteration 62/1000 | Loss: 0.00000825
Iteration 63/1000 | Loss: 0.00000825
Iteration 64/1000 | Loss: 0.00000823
Iteration 65/1000 | Loss: 0.00000823
Iteration 66/1000 | Loss: 0.00000823
Iteration 67/1000 | Loss: 0.00000823
Iteration 68/1000 | Loss: 0.00000823
Iteration 69/1000 | Loss: 0.00000823
Iteration 70/1000 | Loss: 0.00000823
Iteration 71/1000 | Loss: 0.00000823
Iteration 72/1000 | Loss: 0.00000823
Iteration 73/1000 | Loss: 0.00000823
Iteration 74/1000 | Loss: 0.00000822
Iteration 75/1000 | Loss: 0.00000822
Iteration 76/1000 | Loss: 0.00000822
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000821
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000821
Iteration 82/1000 | Loss: 0.00000821
Iteration 83/1000 | Loss: 0.00000821
Iteration 84/1000 | Loss: 0.00000821
Iteration 85/1000 | Loss: 0.00000821
Iteration 86/1000 | Loss: 0.00000821
Iteration 87/1000 | Loss: 0.00000821
Iteration 88/1000 | Loss: 0.00000821
Iteration 89/1000 | Loss: 0.00000820
Iteration 90/1000 | Loss: 0.00000820
Iteration 91/1000 | Loss: 0.00000820
Iteration 92/1000 | Loss: 0.00000819
Iteration 93/1000 | Loss: 0.00000819
Iteration 94/1000 | Loss: 0.00000819
Iteration 95/1000 | Loss: 0.00000819
Iteration 96/1000 | Loss: 0.00000819
Iteration 97/1000 | Loss: 0.00000819
Iteration 98/1000 | Loss: 0.00000819
Iteration 99/1000 | Loss: 0.00000819
Iteration 100/1000 | Loss: 0.00000819
Iteration 101/1000 | Loss: 0.00000819
Iteration 102/1000 | Loss: 0.00000819
Iteration 103/1000 | Loss: 0.00000819
Iteration 104/1000 | Loss: 0.00000819
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000818
Iteration 108/1000 | Loss: 0.00000818
Iteration 109/1000 | Loss: 0.00000817
Iteration 110/1000 | Loss: 0.00000817
Iteration 111/1000 | Loss: 0.00000817
Iteration 112/1000 | Loss: 0.00000817
Iteration 113/1000 | Loss: 0.00000816
Iteration 114/1000 | Loss: 0.00000816
Iteration 115/1000 | Loss: 0.00000816
Iteration 116/1000 | Loss: 0.00000816
Iteration 117/1000 | Loss: 0.00000816
Iteration 118/1000 | Loss: 0.00000816
Iteration 119/1000 | Loss: 0.00000816
Iteration 120/1000 | Loss: 0.00000815
Iteration 121/1000 | Loss: 0.00000815
Iteration 122/1000 | Loss: 0.00000815
Iteration 123/1000 | Loss: 0.00000815
Iteration 124/1000 | Loss: 0.00000815
Iteration 125/1000 | Loss: 0.00000814
Iteration 126/1000 | Loss: 0.00000814
Iteration 127/1000 | Loss: 0.00000814
Iteration 128/1000 | Loss: 0.00000814
Iteration 129/1000 | Loss: 0.00000814
Iteration 130/1000 | Loss: 0.00000814
Iteration 131/1000 | Loss: 0.00000814
Iteration 132/1000 | Loss: 0.00000814
Iteration 133/1000 | Loss: 0.00000814
Iteration 134/1000 | Loss: 0.00000814
Iteration 135/1000 | Loss: 0.00000814
Iteration 136/1000 | Loss: 0.00000814
Iteration 137/1000 | Loss: 0.00000813
Iteration 138/1000 | Loss: 0.00000813
Iteration 139/1000 | Loss: 0.00000813
Iteration 140/1000 | Loss: 0.00000813
Iteration 141/1000 | Loss: 0.00000812
Iteration 142/1000 | Loss: 0.00000812
Iteration 143/1000 | Loss: 0.00000812
Iteration 144/1000 | Loss: 0.00000811
Iteration 145/1000 | Loss: 0.00000811
Iteration 146/1000 | Loss: 0.00000811
Iteration 147/1000 | Loss: 0.00000811
Iteration 148/1000 | Loss: 0.00000811
Iteration 149/1000 | Loss: 0.00000811
Iteration 150/1000 | Loss: 0.00000811
Iteration 151/1000 | Loss: 0.00000811
Iteration 152/1000 | Loss: 0.00000810
Iteration 153/1000 | Loss: 0.00000810
Iteration 154/1000 | Loss: 0.00000810
Iteration 155/1000 | Loss: 0.00000810
Iteration 156/1000 | Loss: 0.00000810
Iteration 157/1000 | Loss: 0.00000810
Iteration 158/1000 | Loss: 0.00000810
Iteration 159/1000 | Loss: 0.00000810
Iteration 160/1000 | Loss: 0.00000810
Iteration 161/1000 | Loss: 0.00000810
Iteration 162/1000 | Loss: 0.00000810
Iteration 163/1000 | Loss: 0.00000810
Iteration 164/1000 | Loss: 0.00000810
Iteration 165/1000 | Loss: 0.00000810
Iteration 166/1000 | Loss: 0.00000810
Iteration 167/1000 | Loss: 0.00000810
Iteration 168/1000 | Loss: 0.00000810
Iteration 169/1000 | Loss: 0.00000810
Iteration 170/1000 | Loss: 0.00000810
Iteration 171/1000 | Loss: 0.00000810
Iteration 172/1000 | Loss: 0.00000810
Iteration 173/1000 | Loss: 0.00000810
Iteration 174/1000 | Loss: 0.00000810
Iteration 175/1000 | Loss: 0.00000810
Iteration 176/1000 | Loss: 0.00000809
Iteration 177/1000 | Loss: 0.00000809
Iteration 178/1000 | Loss: 0.00000810
Iteration 179/1000 | Loss: 0.00000810
Iteration 180/1000 | Loss: 0.00000810
Iteration 181/1000 | Loss: 0.00000810
Iteration 182/1000 | Loss: 0.00000810
Iteration 183/1000 | Loss: 0.00000810
Iteration 184/1000 | Loss: 0.00000810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [8.095000339380931e-06, 8.095000339380931e-06, 8.095000339380931e-06, 8.095000339380931e-06, 8.095000339380931e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.095000339380931e-06

Optimization complete. Final v2v error: 2.4461891651153564 mm

Highest mean error: 2.7936549186706543 mm for frame 84

Lowest mean error: 2.1927311420440674 mm for frame 24

Saving results

Total time: 32.76032733917236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540216
Iteration 2/25 | Loss: 0.00122608
Iteration 3/25 | Loss: 0.00098931
Iteration 4/25 | Loss: 0.00096326
Iteration 5/25 | Loss: 0.00095882
Iteration 6/25 | Loss: 0.00095804
Iteration 7/25 | Loss: 0.00095804
Iteration 8/25 | Loss: 0.00095804
Iteration 9/25 | Loss: 0.00095804
Iteration 10/25 | Loss: 0.00095804
Iteration 11/25 | Loss: 0.00095804
Iteration 12/25 | Loss: 0.00095804
Iteration 13/25 | Loss: 0.00095804
Iteration 14/25 | Loss: 0.00095804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000958036573138088, 0.000958036573138088, 0.000958036573138088, 0.000958036573138088, 0.000958036573138088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000958036573138088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52264237
Iteration 2/25 | Loss: 0.00052223
Iteration 3/25 | Loss: 0.00052222
Iteration 4/25 | Loss: 0.00052222
Iteration 5/25 | Loss: 0.00052222
Iteration 6/25 | Loss: 0.00052222
Iteration 7/25 | Loss: 0.00052222
Iteration 8/25 | Loss: 0.00052222
Iteration 9/25 | Loss: 0.00052221
Iteration 10/25 | Loss: 0.00052221
Iteration 11/25 | Loss: 0.00052221
Iteration 12/25 | Loss: 0.00052221
Iteration 13/25 | Loss: 0.00052221
Iteration 14/25 | Loss: 0.00052221
Iteration 15/25 | Loss: 0.00052221
Iteration 16/25 | Loss: 0.00052221
Iteration 17/25 | Loss: 0.00052221
Iteration 18/25 | Loss: 0.00052221
Iteration 19/25 | Loss: 0.00052221
Iteration 20/25 | Loss: 0.00052221
Iteration 21/25 | Loss: 0.00052221
Iteration 22/25 | Loss: 0.00052221
Iteration 23/25 | Loss: 0.00052221
Iteration 24/25 | Loss: 0.00052221
Iteration 25/25 | Loss: 0.00052221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052221
Iteration 2/1000 | Loss: 0.00002094
Iteration 3/1000 | Loss: 0.00001410
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001158
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001077
Iteration 8/1000 | Loss: 0.00001049
Iteration 9/1000 | Loss: 0.00001035
Iteration 10/1000 | Loss: 0.00001034
Iteration 11/1000 | Loss: 0.00001019
Iteration 12/1000 | Loss: 0.00001017
Iteration 13/1000 | Loss: 0.00001013
Iteration 14/1000 | Loss: 0.00001012
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001010
Iteration 17/1000 | Loss: 0.00001003
Iteration 18/1000 | Loss: 0.00001003
Iteration 19/1000 | Loss: 0.00001002
Iteration 20/1000 | Loss: 0.00001001
Iteration 21/1000 | Loss: 0.00001001
Iteration 22/1000 | Loss: 0.00001001
Iteration 23/1000 | Loss: 0.00001001
Iteration 24/1000 | Loss: 0.00001001
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000998
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000997
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000992
Iteration 43/1000 | Loss: 0.00000992
Iteration 44/1000 | Loss: 0.00000992
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000990
Iteration 49/1000 | Loss: 0.00000990
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000990
Iteration 52/1000 | Loss: 0.00000990
Iteration 53/1000 | Loss: 0.00000989
Iteration 54/1000 | Loss: 0.00000989
Iteration 55/1000 | Loss: 0.00000989
Iteration 56/1000 | Loss: 0.00000989
Iteration 57/1000 | Loss: 0.00000989
Iteration 58/1000 | Loss: 0.00000989
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000988
Iteration 61/1000 | Loss: 0.00000988
Iteration 62/1000 | Loss: 0.00000988
Iteration 63/1000 | Loss: 0.00000988
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000987
Iteration 67/1000 | Loss: 0.00000987
Iteration 68/1000 | Loss: 0.00000987
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000986
Iteration 71/1000 | Loss: 0.00000986
Iteration 72/1000 | Loss: 0.00000986
Iteration 73/1000 | Loss: 0.00000986
Iteration 74/1000 | Loss: 0.00000986
Iteration 75/1000 | Loss: 0.00000985
Iteration 76/1000 | Loss: 0.00000985
Iteration 77/1000 | Loss: 0.00000985
Iteration 78/1000 | Loss: 0.00000985
Iteration 79/1000 | Loss: 0.00000985
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000984
Iteration 86/1000 | Loss: 0.00000983
Iteration 87/1000 | Loss: 0.00000983
Iteration 88/1000 | Loss: 0.00000983
Iteration 89/1000 | Loss: 0.00000983
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [9.831869647314306e-06, 9.831869647314306e-06, 9.831869647314306e-06, 9.831869647314306e-06, 9.831869647314306e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.831869647314306e-06

Optimization complete. Final v2v error: 2.684404134750366 mm

Highest mean error: 3.0063507556915283 mm for frame 12

Lowest mean error: 2.327622413635254 mm for frame 120

Saving results

Total time: 35.012563943862915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076350
Iteration 2/25 | Loss: 0.00249958
Iteration 3/25 | Loss: 0.00152256
Iteration 4/25 | Loss: 0.00129264
Iteration 5/25 | Loss: 0.00117462
Iteration 6/25 | Loss: 0.00110989
Iteration 7/25 | Loss: 0.00108849
Iteration 8/25 | Loss: 0.00108118
Iteration 9/25 | Loss: 0.00107273
Iteration 10/25 | Loss: 0.00107383
Iteration 11/25 | Loss: 0.00107694
Iteration 12/25 | Loss: 0.00107115
Iteration 13/25 | Loss: 0.00105917
Iteration 14/25 | Loss: 0.00105350
Iteration 15/25 | Loss: 0.00105061
Iteration 16/25 | Loss: 0.00105497
Iteration 17/25 | Loss: 0.00105390
Iteration 18/25 | Loss: 0.00107404
Iteration 19/25 | Loss: 0.00104309
Iteration 20/25 | Loss: 0.00103437
Iteration 21/25 | Loss: 0.00102837
Iteration 22/25 | Loss: 0.00103061
Iteration 23/25 | Loss: 0.00102754
Iteration 24/25 | Loss: 0.00101876
Iteration 25/25 | Loss: 0.00102237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29830241
Iteration 2/25 | Loss: 0.00176266
Iteration 3/25 | Loss: 0.00176266
Iteration 4/25 | Loss: 0.00176266
Iteration 5/25 | Loss: 0.00176266
Iteration 6/25 | Loss: 0.00176266
Iteration 7/25 | Loss: 0.00176265
Iteration 8/25 | Loss: 0.00176265
Iteration 9/25 | Loss: 0.00176265
Iteration 10/25 | Loss: 0.00176265
Iteration 11/25 | Loss: 0.00176265
Iteration 12/25 | Loss: 0.00176265
Iteration 13/25 | Loss: 0.00176265
Iteration 14/25 | Loss: 0.00176265
Iteration 15/25 | Loss: 0.00176265
Iteration 16/25 | Loss: 0.00176265
Iteration 17/25 | Loss: 0.00176265
Iteration 18/25 | Loss: 0.00176265
Iteration 19/25 | Loss: 0.00176265
Iteration 20/25 | Loss: 0.00176265
Iteration 21/25 | Loss: 0.00176265
Iteration 22/25 | Loss: 0.00176265
Iteration 23/25 | Loss: 0.00176265
Iteration 24/25 | Loss: 0.00176265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017626537010073662, 0.0017626537010073662, 0.0017626537010073662, 0.0017626537010073662, 0.0017626537010073662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017626537010073662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176265
Iteration 2/1000 | Loss: 0.00481675
Iteration 3/1000 | Loss: 0.00166406
Iteration 4/1000 | Loss: 0.00041645
Iteration 5/1000 | Loss: 0.00021910
Iteration 6/1000 | Loss: 0.00020095
Iteration 7/1000 | Loss: 0.00029667
Iteration 8/1000 | Loss: 0.00024277
Iteration 9/1000 | Loss: 0.00031561
Iteration 10/1000 | Loss: 0.00035348
Iteration 11/1000 | Loss: 0.00025566
Iteration 12/1000 | Loss: 0.00024969
Iteration 13/1000 | Loss: 0.00022012
Iteration 14/1000 | Loss: 0.00022482
Iteration 15/1000 | Loss: 0.00037124
Iteration 16/1000 | Loss: 0.00021592
Iteration 17/1000 | Loss: 0.00019596
Iteration 18/1000 | Loss: 0.00014090
Iteration 19/1000 | Loss: 0.00047258
Iteration 20/1000 | Loss: 0.00043583
Iteration 21/1000 | Loss: 0.00026111
Iteration 22/1000 | Loss: 0.00033155
Iteration 23/1000 | Loss: 0.00036905
Iteration 24/1000 | Loss: 0.00045618
Iteration 25/1000 | Loss: 0.00043892
Iteration 26/1000 | Loss: 0.00013713
Iteration 27/1000 | Loss: 0.00028190
Iteration 28/1000 | Loss: 0.00024195
Iteration 29/1000 | Loss: 0.00018315
Iteration 30/1000 | Loss: 0.00027540
Iteration 31/1000 | Loss: 0.00032061
Iteration 32/1000 | Loss: 0.00070863
Iteration 33/1000 | Loss: 0.00053385
Iteration 34/1000 | Loss: 0.00056225
Iteration 35/1000 | Loss: 0.00030245
Iteration 36/1000 | Loss: 0.00033212
Iteration 37/1000 | Loss: 0.00027596
Iteration 38/1000 | Loss: 0.00033004
Iteration 39/1000 | Loss: 0.00046489
Iteration 40/1000 | Loss: 0.00018163
Iteration 41/1000 | Loss: 0.00013284
Iteration 42/1000 | Loss: 0.00012749
Iteration 43/1000 | Loss: 0.00014952
Iteration 44/1000 | Loss: 0.00013440
Iteration 45/1000 | Loss: 0.00016101
Iteration 46/1000 | Loss: 0.00020651
Iteration 47/1000 | Loss: 0.00011570
Iteration 48/1000 | Loss: 0.00016735
Iteration 49/1000 | Loss: 0.00033355
Iteration 50/1000 | Loss: 0.00023315
Iteration 51/1000 | Loss: 0.00021564
Iteration 52/1000 | Loss: 0.00021650
Iteration 53/1000 | Loss: 0.00010897
Iteration 54/1000 | Loss: 0.00013587
Iteration 55/1000 | Loss: 0.00010374
Iteration 56/1000 | Loss: 0.00010911
Iteration 57/1000 | Loss: 0.00011962
Iteration 58/1000 | Loss: 0.00014224
Iteration 59/1000 | Loss: 0.00019360
Iteration 60/1000 | Loss: 0.00016877
Iteration 61/1000 | Loss: 0.00020610
Iteration 62/1000 | Loss: 0.00017979
Iteration 63/1000 | Loss: 0.00038131
Iteration 64/1000 | Loss: 0.00036267
Iteration 65/1000 | Loss: 0.00041305
Iteration 66/1000 | Loss: 0.00018059
Iteration 67/1000 | Loss: 0.00026774
Iteration 68/1000 | Loss: 0.00024862
Iteration 69/1000 | Loss: 0.00049691
Iteration 70/1000 | Loss: 0.00019628
Iteration 71/1000 | Loss: 0.00043957
Iteration 72/1000 | Loss: 0.00038137
Iteration 73/1000 | Loss: 0.00016421
Iteration 74/1000 | Loss: 0.00013268
Iteration 75/1000 | Loss: 0.00053848
Iteration 76/1000 | Loss: 0.00039514
Iteration 77/1000 | Loss: 0.00012084
Iteration 78/1000 | Loss: 0.00021354
Iteration 79/1000 | Loss: 0.00012595
Iteration 80/1000 | Loss: 0.00021526
Iteration 81/1000 | Loss: 0.00020068
Iteration 82/1000 | Loss: 0.00016504
Iteration 83/1000 | Loss: 0.00026337
Iteration 84/1000 | Loss: 0.00015658
Iteration 85/1000 | Loss: 0.00024205
Iteration 86/1000 | Loss: 0.00016024
Iteration 87/1000 | Loss: 0.00011915
Iteration 88/1000 | Loss: 0.00010836
Iteration 89/1000 | Loss: 0.00020302
Iteration 90/1000 | Loss: 0.00019492
Iteration 91/1000 | Loss: 0.00021646
Iteration 92/1000 | Loss: 0.00020352
Iteration 93/1000 | Loss: 0.00018829
Iteration 94/1000 | Loss: 0.00012453
Iteration 95/1000 | Loss: 0.00011828
Iteration 96/1000 | Loss: 0.00016366
Iteration 97/1000 | Loss: 0.00018666
Iteration 98/1000 | Loss: 0.00008082
Iteration 99/1000 | Loss: 0.00016633
Iteration 100/1000 | Loss: 0.00015357
Iteration 101/1000 | Loss: 0.00025227
Iteration 102/1000 | Loss: 0.00018369
Iteration 103/1000 | Loss: 0.00017094
Iteration 104/1000 | Loss: 0.00010372
Iteration 105/1000 | Loss: 0.00016083
Iteration 106/1000 | Loss: 0.00020490
Iteration 107/1000 | Loss: 0.00018465
Iteration 108/1000 | Loss: 0.00021816
Iteration 109/1000 | Loss: 0.00016466
Iteration 110/1000 | Loss: 0.00021791
Iteration 111/1000 | Loss: 0.00022436
Iteration 112/1000 | Loss: 0.00020087
Iteration 113/1000 | Loss: 0.00022651
Iteration 114/1000 | Loss: 0.00021493
Iteration 115/1000 | Loss: 0.00018206
Iteration 116/1000 | Loss: 0.00018048
Iteration 117/1000 | Loss: 0.00020143
Iteration 118/1000 | Loss: 0.00020446
Iteration 119/1000 | Loss: 0.00019961
Iteration 120/1000 | Loss: 0.00022157
Iteration 121/1000 | Loss: 0.00018473
Iteration 122/1000 | Loss: 0.00020717
Iteration 123/1000 | Loss: 0.00015586
Iteration 124/1000 | Loss: 0.00019452
Iteration 125/1000 | Loss: 0.00021151
Iteration 126/1000 | Loss: 0.00015203
Iteration 127/1000 | Loss: 0.00017000
Iteration 128/1000 | Loss: 0.00015380
Iteration 129/1000 | Loss: 0.00019577
Iteration 130/1000 | Loss: 0.00021883
Iteration 131/1000 | Loss: 0.00019809
Iteration 132/1000 | Loss: 0.00022957
Iteration 133/1000 | Loss: 0.00020358
Iteration 134/1000 | Loss: 0.00022680
Iteration 135/1000 | Loss: 0.00021699
Iteration 136/1000 | Loss: 0.00021663
Iteration 137/1000 | Loss: 0.00018388
Iteration 138/1000 | Loss: 0.00018928
Iteration 139/1000 | Loss: 0.00021750
Iteration 140/1000 | Loss: 0.00349328
Iteration 141/1000 | Loss: 0.00119149
Iteration 142/1000 | Loss: 0.00272945
Iteration 143/1000 | Loss: 0.00117267
Iteration 144/1000 | Loss: 0.00030474
Iteration 145/1000 | Loss: 0.00048965
Iteration 146/1000 | Loss: 0.00068818
Iteration 147/1000 | Loss: 0.00013830
Iteration 148/1000 | Loss: 0.00360121
Iteration 149/1000 | Loss: 0.00009430
Iteration 150/1000 | Loss: 0.00014668
Iteration 151/1000 | Loss: 0.00004554
Iteration 152/1000 | Loss: 0.00003073
Iteration 153/1000 | Loss: 0.00002738
Iteration 154/1000 | Loss: 0.00018991
Iteration 155/1000 | Loss: 0.00002673
Iteration 156/1000 | Loss: 0.00027860
Iteration 157/1000 | Loss: 0.00006597
Iteration 158/1000 | Loss: 0.00026934
Iteration 159/1000 | Loss: 0.00004391
Iteration 160/1000 | Loss: 0.00012711
Iteration 161/1000 | Loss: 0.00029947
Iteration 162/1000 | Loss: 0.00029109
Iteration 163/1000 | Loss: 0.00029363
Iteration 164/1000 | Loss: 0.00033174
Iteration 165/1000 | Loss: 0.00009615
Iteration 166/1000 | Loss: 0.00036511
Iteration 167/1000 | Loss: 0.00028979
Iteration 168/1000 | Loss: 0.00028757
Iteration 169/1000 | Loss: 0.00032105
Iteration 170/1000 | Loss: 0.00028277
Iteration 171/1000 | Loss: 0.00003280
Iteration 172/1000 | Loss: 0.00002401
Iteration 173/1000 | Loss: 0.00002022
Iteration 174/1000 | Loss: 0.00001930
Iteration 175/1000 | Loss: 0.00031545
Iteration 176/1000 | Loss: 0.00023879
Iteration 177/1000 | Loss: 0.00021570
Iteration 178/1000 | Loss: 0.00014107
Iteration 179/1000 | Loss: 0.00027570
Iteration 180/1000 | Loss: 0.00012790
Iteration 181/1000 | Loss: 0.00017004
Iteration 182/1000 | Loss: 0.00006788
Iteration 183/1000 | Loss: 0.00028072
Iteration 184/1000 | Loss: 0.00034218
Iteration 185/1000 | Loss: 0.00029844
Iteration 186/1000 | Loss: 0.00008080
Iteration 187/1000 | Loss: 0.00037175
Iteration 188/1000 | Loss: 0.00029585
Iteration 189/1000 | Loss: 0.00014223
Iteration 190/1000 | Loss: 0.00010872
Iteration 191/1000 | Loss: 0.00002290
Iteration 192/1000 | Loss: 0.00001912
Iteration 193/1000 | Loss: 0.00001861
Iteration 194/1000 | Loss: 0.00001850
Iteration 195/1000 | Loss: 0.00025233
Iteration 196/1000 | Loss: 0.00003255
Iteration 197/1000 | Loss: 0.00023134
Iteration 198/1000 | Loss: 0.00026497
Iteration 199/1000 | Loss: 0.00033703
Iteration 200/1000 | Loss: 0.00003664
Iteration 201/1000 | Loss: 0.00027084
Iteration 202/1000 | Loss: 0.00020793
Iteration 203/1000 | Loss: 0.00008686
Iteration 204/1000 | Loss: 0.00008593
Iteration 205/1000 | Loss: 0.00005061
Iteration 206/1000 | Loss: 0.00001900
Iteration 207/1000 | Loss: 0.00030259
Iteration 208/1000 | Loss: 0.00001913
Iteration 209/1000 | Loss: 0.00001804
Iteration 210/1000 | Loss: 0.00001770
Iteration 211/1000 | Loss: 0.00001766
Iteration 212/1000 | Loss: 0.00005987
Iteration 213/1000 | Loss: 0.00005987
Iteration 214/1000 | Loss: 0.00002017
Iteration 215/1000 | Loss: 0.00001842
Iteration 216/1000 | Loss: 0.00001784
Iteration 217/1000 | Loss: 0.00001761
Iteration 218/1000 | Loss: 0.00001751
Iteration 219/1000 | Loss: 0.00001735
Iteration 220/1000 | Loss: 0.00001733
Iteration 221/1000 | Loss: 0.00001733
Iteration 222/1000 | Loss: 0.00001730
Iteration 223/1000 | Loss: 0.00001729
Iteration 224/1000 | Loss: 0.00001729
Iteration 225/1000 | Loss: 0.00001729
Iteration 226/1000 | Loss: 0.00001729
Iteration 227/1000 | Loss: 0.00001728
Iteration 228/1000 | Loss: 0.00001727
Iteration 229/1000 | Loss: 0.00001727
Iteration 230/1000 | Loss: 0.00001724
Iteration 231/1000 | Loss: 0.00001710
Iteration 232/1000 | Loss: 0.00001707
Iteration 233/1000 | Loss: 0.00001702
Iteration 234/1000 | Loss: 0.00001699
Iteration 235/1000 | Loss: 0.00001697
Iteration 236/1000 | Loss: 0.00001695
Iteration 237/1000 | Loss: 0.00001694
Iteration 238/1000 | Loss: 0.00035966
Iteration 239/1000 | Loss: 0.00002614
Iteration 240/1000 | Loss: 0.00042580
Iteration 241/1000 | Loss: 0.00020353
Iteration 242/1000 | Loss: 0.00051749
Iteration 243/1000 | Loss: 0.00024872
Iteration 244/1000 | Loss: 0.00067090
Iteration 245/1000 | Loss: 0.00003485
Iteration 246/1000 | Loss: 0.00002269
Iteration 247/1000 | Loss: 0.00001974
Iteration 248/1000 | Loss: 0.00001865
Iteration 249/1000 | Loss: 0.00001755
Iteration 250/1000 | Loss: 0.00001669
Iteration 251/1000 | Loss: 0.00001580
Iteration 252/1000 | Loss: 0.00001505
Iteration 253/1000 | Loss: 0.00001474
Iteration 254/1000 | Loss: 0.00001471
Iteration 255/1000 | Loss: 0.00001467
Iteration 256/1000 | Loss: 0.00001464
Iteration 257/1000 | Loss: 0.00001460
Iteration 258/1000 | Loss: 0.00001457
Iteration 259/1000 | Loss: 0.00001456
Iteration 260/1000 | Loss: 0.00001456
Iteration 261/1000 | Loss: 0.00001455
Iteration 262/1000 | Loss: 0.00001455
Iteration 263/1000 | Loss: 0.00001454
Iteration 264/1000 | Loss: 0.00001453
Iteration 265/1000 | Loss: 0.00001451
Iteration 266/1000 | Loss: 0.00001449
Iteration 267/1000 | Loss: 0.00001448
Iteration 268/1000 | Loss: 0.00001448
Iteration 269/1000 | Loss: 0.00001448
Iteration 270/1000 | Loss: 0.00001448
Iteration 271/1000 | Loss: 0.00001447
Iteration 272/1000 | Loss: 0.00001447
Iteration 273/1000 | Loss: 0.00001446
Iteration 274/1000 | Loss: 0.00001446
Iteration 275/1000 | Loss: 0.00001446
Iteration 276/1000 | Loss: 0.00001446
Iteration 277/1000 | Loss: 0.00001445
Iteration 278/1000 | Loss: 0.00001445
Iteration 279/1000 | Loss: 0.00001445
Iteration 280/1000 | Loss: 0.00001445
Iteration 281/1000 | Loss: 0.00001444
Iteration 282/1000 | Loss: 0.00001444
Iteration 283/1000 | Loss: 0.00001444
Iteration 284/1000 | Loss: 0.00001444
Iteration 285/1000 | Loss: 0.00001443
Iteration 286/1000 | Loss: 0.00001443
Iteration 287/1000 | Loss: 0.00001443
Iteration 288/1000 | Loss: 0.00001443
Iteration 289/1000 | Loss: 0.00001442
Iteration 290/1000 | Loss: 0.00001442
Iteration 291/1000 | Loss: 0.00001442
Iteration 292/1000 | Loss: 0.00001442
Iteration 293/1000 | Loss: 0.00001442
Iteration 294/1000 | Loss: 0.00001442
Iteration 295/1000 | Loss: 0.00001441
Iteration 296/1000 | Loss: 0.00001441
Iteration 297/1000 | Loss: 0.00001441
Iteration 298/1000 | Loss: 0.00001441
Iteration 299/1000 | Loss: 0.00001441
Iteration 300/1000 | Loss: 0.00001441
Iteration 301/1000 | Loss: 0.00001441
Iteration 302/1000 | Loss: 0.00001441
Iteration 303/1000 | Loss: 0.00001441
Iteration 304/1000 | Loss: 0.00001441
Iteration 305/1000 | Loss: 0.00001441
Iteration 306/1000 | Loss: 0.00001441
Iteration 307/1000 | Loss: 0.00001440
Iteration 308/1000 | Loss: 0.00001440
Iteration 309/1000 | Loss: 0.00001440
Iteration 310/1000 | Loss: 0.00001440
Iteration 311/1000 | Loss: 0.00001439
Iteration 312/1000 | Loss: 0.00001439
Iteration 313/1000 | Loss: 0.00001439
Iteration 314/1000 | Loss: 0.00001439
Iteration 315/1000 | Loss: 0.00001439
Iteration 316/1000 | Loss: 0.00001439
Iteration 317/1000 | Loss: 0.00001438
Iteration 318/1000 | Loss: 0.00001438
Iteration 319/1000 | Loss: 0.00001438
Iteration 320/1000 | Loss: 0.00001438
Iteration 321/1000 | Loss: 0.00001437
Iteration 322/1000 | Loss: 0.00001437
Iteration 323/1000 | Loss: 0.00001437
Iteration 324/1000 | Loss: 0.00001437
Iteration 325/1000 | Loss: 0.00001437
Iteration 326/1000 | Loss: 0.00001437
Iteration 327/1000 | Loss: 0.00001437
Iteration 328/1000 | Loss: 0.00001437
Iteration 329/1000 | Loss: 0.00001437
Iteration 330/1000 | Loss: 0.00001436
Iteration 331/1000 | Loss: 0.00001436
Iteration 332/1000 | Loss: 0.00001436
Iteration 333/1000 | Loss: 0.00001436
Iteration 334/1000 | Loss: 0.00001436
Iteration 335/1000 | Loss: 0.00001436
Iteration 336/1000 | Loss: 0.00001436
Iteration 337/1000 | Loss: 0.00001436
Iteration 338/1000 | Loss: 0.00001436
Iteration 339/1000 | Loss: 0.00001436
Iteration 340/1000 | Loss: 0.00001436
Iteration 341/1000 | Loss: 0.00001435
Iteration 342/1000 | Loss: 0.00001435
Iteration 343/1000 | Loss: 0.00001435
Iteration 344/1000 | Loss: 0.00001435
Iteration 345/1000 | Loss: 0.00001435
Iteration 346/1000 | Loss: 0.00001435
Iteration 347/1000 | Loss: 0.00001435
Iteration 348/1000 | Loss: 0.00001435
Iteration 349/1000 | Loss: 0.00001435
Iteration 350/1000 | Loss: 0.00001435
Iteration 351/1000 | Loss: 0.00001435
Iteration 352/1000 | Loss: 0.00001435
Iteration 353/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.4349132470670156e-05, 1.4349132470670156e-05, 1.4349132470670156e-05, 1.4349132470670156e-05, 1.4349132470670156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4349132470670156e-05

Optimization complete. Final v2v error: 2.921414613723755 mm

Highest mean error: 16.92654800415039 mm for frame 135

Lowest mean error: 2.4912939071655273 mm for frame 179

Saving results

Total time: 440.20573925971985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414250
Iteration 2/25 | Loss: 0.00102780
Iteration 3/25 | Loss: 0.00093659
Iteration 4/25 | Loss: 0.00092323
Iteration 5/25 | Loss: 0.00091882
Iteration 6/25 | Loss: 0.00091767
Iteration 7/25 | Loss: 0.00091762
Iteration 8/25 | Loss: 0.00091762
Iteration 9/25 | Loss: 0.00091762
Iteration 10/25 | Loss: 0.00091762
Iteration 11/25 | Loss: 0.00091762
Iteration 12/25 | Loss: 0.00091762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009176162420772016, 0.0009176162420772016, 0.0009176162420772016, 0.0009176162420772016, 0.0009176162420772016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009176162420772016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.41966510
Iteration 2/25 | Loss: 0.00061082
Iteration 3/25 | Loss: 0.00061081
Iteration 4/25 | Loss: 0.00061081
Iteration 5/25 | Loss: 0.00061081
Iteration 6/25 | Loss: 0.00061081
Iteration 7/25 | Loss: 0.00061081
Iteration 8/25 | Loss: 0.00061081
Iteration 9/25 | Loss: 0.00061081
Iteration 10/25 | Loss: 0.00061081
Iteration 11/25 | Loss: 0.00061081
Iteration 12/25 | Loss: 0.00061081
Iteration 13/25 | Loss: 0.00061081
Iteration 14/25 | Loss: 0.00061081
Iteration 15/25 | Loss: 0.00061081
Iteration 16/25 | Loss: 0.00061081
Iteration 17/25 | Loss: 0.00061081
Iteration 18/25 | Loss: 0.00061081
Iteration 19/25 | Loss: 0.00061081
Iteration 20/25 | Loss: 0.00061081
Iteration 21/25 | Loss: 0.00061081
Iteration 22/25 | Loss: 0.00061081
Iteration 23/25 | Loss: 0.00061081
Iteration 24/25 | Loss: 0.00061081
Iteration 25/25 | Loss: 0.00061081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061081
Iteration 2/1000 | Loss: 0.00001815
Iteration 3/1000 | Loss: 0.00001203
Iteration 4/1000 | Loss: 0.00001116
Iteration 5/1000 | Loss: 0.00001049
Iteration 6/1000 | Loss: 0.00001024
Iteration 7/1000 | Loss: 0.00001006
Iteration 8/1000 | Loss: 0.00001002
Iteration 9/1000 | Loss: 0.00000998
Iteration 10/1000 | Loss: 0.00000997
Iteration 11/1000 | Loss: 0.00000997
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000990
Iteration 14/1000 | Loss: 0.00000990
Iteration 15/1000 | Loss: 0.00000990
Iteration 16/1000 | Loss: 0.00000990
Iteration 17/1000 | Loss: 0.00000990
Iteration 18/1000 | Loss: 0.00000989
Iteration 19/1000 | Loss: 0.00000989
Iteration 20/1000 | Loss: 0.00000986
Iteration 21/1000 | Loss: 0.00000986
Iteration 22/1000 | Loss: 0.00000985
Iteration 23/1000 | Loss: 0.00000985
Iteration 24/1000 | Loss: 0.00000983
Iteration 25/1000 | Loss: 0.00000982
Iteration 26/1000 | Loss: 0.00000981
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000980
Iteration 30/1000 | Loss: 0.00000980
Iteration 31/1000 | Loss: 0.00000979
Iteration 32/1000 | Loss: 0.00000979
Iteration 33/1000 | Loss: 0.00000978
Iteration 34/1000 | Loss: 0.00000977
Iteration 35/1000 | Loss: 0.00000977
Iteration 36/1000 | Loss: 0.00000976
Iteration 37/1000 | Loss: 0.00000976
Iteration 38/1000 | Loss: 0.00000976
Iteration 39/1000 | Loss: 0.00000975
Iteration 40/1000 | Loss: 0.00000975
Iteration 41/1000 | Loss: 0.00000974
Iteration 42/1000 | Loss: 0.00000973
Iteration 43/1000 | Loss: 0.00000972
Iteration 44/1000 | Loss: 0.00000972
Iteration 45/1000 | Loss: 0.00000972
Iteration 46/1000 | Loss: 0.00000972
Iteration 47/1000 | Loss: 0.00000972
Iteration 48/1000 | Loss: 0.00000972
Iteration 49/1000 | Loss: 0.00000971
Iteration 50/1000 | Loss: 0.00000971
Iteration 51/1000 | Loss: 0.00000971
Iteration 52/1000 | Loss: 0.00000971
Iteration 53/1000 | Loss: 0.00000970
Iteration 54/1000 | Loss: 0.00000970
Iteration 55/1000 | Loss: 0.00000970
Iteration 56/1000 | Loss: 0.00000969
Iteration 57/1000 | Loss: 0.00000969
Iteration 58/1000 | Loss: 0.00000969
Iteration 59/1000 | Loss: 0.00000969
Iteration 60/1000 | Loss: 0.00000969
Iteration 61/1000 | Loss: 0.00000969
Iteration 62/1000 | Loss: 0.00000969
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000968
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000967
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000966
Iteration 70/1000 | Loss: 0.00000966
Iteration 71/1000 | Loss: 0.00000966
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000965
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000963
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000962
Iteration 101/1000 | Loss: 0.00000962
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000961
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000960
Iteration 118/1000 | Loss: 0.00000960
Iteration 119/1000 | Loss: 0.00000960
Iteration 120/1000 | Loss: 0.00000960
Iteration 121/1000 | Loss: 0.00000960
Iteration 122/1000 | Loss: 0.00000960
Iteration 123/1000 | Loss: 0.00000960
Iteration 124/1000 | Loss: 0.00000959
Iteration 125/1000 | Loss: 0.00000959
Iteration 126/1000 | Loss: 0.00000959
Iteration 127/1000 | Loss: 0.00000959
Iteration 128/1000 | Loss: 0.00000959
Iteration 129/1000 | Loss: 0.00000959
Iteration 130/1000 | Loss: 0.00000959
Iteration 131/1000 | Loss: 0.00000959
Iteration 132/1000 | Loss: 0.00000959
Iteration 133/1000 | Loss: 0.00000959
Iteration 134/1000 | Loss: 0.00000959
Iteration 135/1000 | Loss: 0.00000959
Iteration 136/1000 | Loss: 0.00000959
Iteration 137/1000 | Loss: 0.00000959
Iteration 138/1000 | Loss: 0.00000959
Iteration 139/1000 | Loss: 0.00000959
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000959
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000959
Iteration 151/1000 | Loss: 0.00000959
Iteration 152/1000 | Loss: 0.00000959
Iteration 153/1000 | Loss: 0.00000959
Iteration 154/1000 | Loss: 0.00000959
Iteration 155/1000 | Loss: 0.00000959
Iteration 156/1000 | Loss: 0.00000959
Iteration 157/1000 | Loss: 0.00000959
Iteration 158/1000 | Loss: 0.00000959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [9.58881719270721e-06, 9.58881719270721e-06, 9.58881719270721e-06, 9.58881719270721e-06, 9.58881719270721e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.58881719270721e-06

Optimization complete. Final v2v error: 2.6271893978118896 mm

Highest mean error: 3.1868975162506104 mm for frame 71

Lowest mean error: 2.364553451538086 mm for frame 46

Saving results

Total time: 30.192248582839966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402502
Iteration 2/25 | Loss: 0.00102703
Iteration 3/25 | Loss: 0.00094066
Iteration 4/25 | Loss: 0.00092345
Iteration 5/25 | Loss: 0.00091895
Iteration 6/25 | Loss: 0.00091800
Iteration 7/25 | Loss: 0.00091799
Iteration 8/25 | Loss: 0.00091799
Iteration 9/25 | Loss: 0.00091799
Iteration 10/25 | Loss: 0.00091799
Iteration 11/25 | Loss: 0.00091799
Iteration 12/25 | Loss: 0.00091799
Iteration 13/25 | Loss: 0.00091799
Iteration 14/25 | Loss: 0.00091799
Iteration 15/25 | Loss: 0.00091799
Iteration 16/25 | Loss: 0.00091799
Iteration 17/25 | Loss: 0.00091799
Iteration 18/25 | Loss: 0.00091799
Iteration 19/25 | Loss: 0.00091799
Iteration 20/25 | Loss: 0.00091799
Iteration 21/25 | Loss: 0.00091799
Iteration 22/25 | Loss: 0.00091799
Iteration 23/25 | Loss: 0.00091799
Iteration 24/25 | Loss: 0.00091799
Iteration 25/25 | Loss: 0.00091799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76128197
Iteration 2/25 | Loss: 0.00061435
Iteration 3/25 | Loss: 0.00061435
Iteration 4/25 | Loss: 0.00061435
Iteration 5/25 | Loss: 0.00061435
Iteration 6/25 | Loss: 0.00061435
Iteration 7/25 | Loss: 0.00061435
Iteration 8/25 | Loss: 0.00061435
Iteration 9/25 | Loss: 0.00061435
Iteration 10/25 | Loss: 0.00061435
Iteration 11/25 | Loss: 0.00061435
Iteration 12/25 | Loss: 0.00061435
Iteration 13/25 | Loss: 0.00061435
Iteration 14/25 | Loss: 0.00061435
Iteration 15/25 | Loss: 0.00061435
Iteration 16/25 | Loss: 0.00061435
Iteration 17/25 | Loss: 0.00061435
Iteration 18/25 | Loss: 0.00061435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006143507780507207, 0.0006143507780507207, 0.0006143507780507207, 0.0006143507780507207, 0.0006143507780507207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006143507780507207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061435
Iteration 2/1000 | Loss: 0.00002881
Iteration 3/1000 | Loss: 0.00001688
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00001226
Iteration 6/1000 | Loss: 0.00001173
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001095
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001061
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001053
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001051
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001051
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001035
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001033
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001032
Iteration 63/1000 | Loss: 0.00001032
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001031
Iteration 71/1000 | Loss: 0.00001031
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001030
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001029
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001028
Iteration 92/1000 | Loss: 0.00001028
Iteration 93/1000 | Loss: 0.00001028
Iteration 94/1000 | Loss: 0.00001028
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001027
Iteration 97/1000 | Loss: 0.00001027
Iteration 98/1000 | Loss: 0.00001027
Iteration 99/1000 | Loss: 0.00001027
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001027
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001027
Iteration 117/1000 | Loss: 0.00001027
Iteration 118/1000 | Loss: 0.00001027
Iteration 119/1000 | Loss: 0.00001027
Iteration 120/1000 | Loss: 0.00001027
Iteration 121/1000 | Loss: 0.00001027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.0268894584442023e-05, 1.0268894584442023e-05, 1.0268894584442023e-05, 1.0268894584442023e-05, 1.0268894584442023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0268894584442023e-05

Optimization complete. Final v2v error: 2.7017343044281006 mm

Highest mean error: 3.4840543270111084 mm for frame 63

Lowest mean error: 2.4177751541137695 mm for frame 79

Saving results

Total time: 30.9890456199646
