Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=87, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4872-4927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413139
Iteration 2/25 | Loss: 0.00111565
Iteration 3/25 | Loss: 0.00101919
Iteration 4/25 | Loss: 0.00100567
Iteration 5/25 | Loss: 0.00100181
Iteration 6/25 | Loss: 0.00100074
Iteration 7/25 | Loss: 0.00100074
Iteration 8/25 | Loss: 0.00100074
Iteration 9/25 | Loss: 0.00100074
Iteration 10/25 | Loss: 0.00100074
Iteration 11/25 | Loss: 0.00100074
Iteration 12/25 | Loss: 0.00100074
Iteration 13/25 | Loss: 0.00100074
Iteration 14/25 | Loss: 0.00100074
Iteration 15/25 | Loss: 0.00100074
Iteration 16/25 | Loss: 0.00100074
Iteration 17/25 | Loss: 0.00100074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010007374221459031, 0.0010007374221459031, 0.0010007374221459031, 0.0010007374221459031, 0.0010007374221459031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010007374221459031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09163630
Iteration 2/25 | Loss: 0.00084620
Iteration 3/25 | Loss: 0.00084618
Iteration 4/25 | Loss: 0.00084618
Iteration 5/25 | Loss: 0.00084618
Iteration 6/25 | Loss: 0.00084618
Iteration 7/25 | Loss: 0.00084618
Iteration 8/25 | Loss: 0.00084618
Iteration 9/25 | Loss: 0.00084618
Iteration 10/25 | Loss: 0.00084618
Iteration 11/25 | Loss: 0.00084618
Iteration 12/25 | Loss: 0.00084618
Iteration 13/25 | Loss: 0.00084618
Iteration 14/25 | Loss: 0.00084618
Iteration 15/25 | Loss: 0.00084618
Iteration 16/25 | Loss: 0.00084618
Iteration 17/25 | Loss: 0.00084618
Iteration 18/25 | Loss: 0.00084618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000846178038045764, 0.000846178038045764, 0.000846178038045764, 0.000846178038045764, 0.000846178038045764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000846178038045764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084618
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00001566
Iteration 4/1000 | Loss: 0.00001262
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001107
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001028
Iteration 9/1000 | Loss: 0.00001000
Iteration 10/1000 | Loss: 0.00000977
Iteration 11/1000 | Loss: 0.00000962
Iteration 12/1000 | Loss: 0.00000960
Iteration 13/1000 | Loss: 0.00000940
Iteration 14/1000 | Loss: 0.00000938
Iteration 15/1000 | Loss: 0.00000938
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000936
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000935
Iteration 20/1000 | Loss: 0.00000935
Iteration 21/1000 | Loss: 0.00000934
Iteration 22/1000 | Loss: 0.00000933
Iteration 23/1000 | Loss: 0.00000933
Iteration 24/1000 | Loss: 0.00000933
Iteration 25/1000 | Loss: 0.00000932
Iteration 26/1000 | Loss: 0.00000931
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000926
Iteration 30/1000 | Loss: 0.00000922
Iteration 31/1000 | Loss: 0.00000921
Iteration 32/1000 | Loss: 0.00000920
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000913
Iteration 35/1000 | Loss: 0.00000913
Iteration 36/1000 | Loss: 0.00000912
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000911
Iteration 39/1000 | Loss: 0.00000911
Iteration 40/1000 | Loss: 0.00000910
Iteration 41/1000 | Loss: 0.00000910
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000910
Iteration 44/1000 | Loss: 0.00000909
Iteration 45/1000 | Loss: 0.00000909
Iteration 46/1000 | Loss: 0.00000909
Iteration 47/1000 | Loss: 0.00000909
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000907
Iteration 50/1000 | Loss: 0.00000907
Iteration 51/1000 | Loss: 0.00000907
Iteration 52/1000 | Loss: 0.00000906
Iteration 53/1000 | Loss: 0.00000906
Iteration 54/1000 | Loss: 0.00000906
Iteration 55/1000 | Loss: 0.00000906
Iteration 56/1000 | Loss: 0.00000906
Iteration 57/1000 | Loss: 0.00000906
Iteration 58/1000 | Loss: 0.00000906
Iteration 59/1000 | Loss: 0.00000905
Iteration 60/1000 | Loss: 0.00000905
Iteration 61/1000 | Loss: 0.00000905
Iteration 62/1000 | Loss: 0.00000904
Iteration 63/1000 | Loss: 0.00000904
Iteration 64/1000 | Loss: 0.00000904
Iteration 65/1000 | Loss: 0.00000903
Iteration 66/1000 | Loss: 0.00000903
Iteration 67/1000 | Loss: 0.00000903
Iteration 68/1000 | Loss: 0.00000903
Iteration 69/1000 | Loss: 0.00000903
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000901
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000901
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000901
Iteration 78/1000 | Loss: 0.00000901
Iteration 79/1000 | Loss: 0.00000901
Iteration 80/1000 | Loss: 0.00000901
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000901
Iteration 83/1000 | Loss: 0.00000901
Iteration 84/1000 | Loss: 0.00000901
Iteration 85/1000 | Loss: 0.00000901
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000900
Iteration 88/1000 | Loss: 0.00000900
Iteration 89/1000 | Loss: 0.00000900
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000900
Iteration 92/1000 | Loss: 0.00000900
Iteration 93/1000 | Loss: 0.00000900
Iteration 94/1000 | Loss: 0.00000899
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000899
Iteration 101/1000 | Loss: 0.00000899
Iteration 102/1000 | Loss: 0.00000899
Iteration 103/1000 | Loss: 0.00000899
Iteration 104/1000 | Loss: 0.00000899
Iteration 105/1000 | Loss: 0.00000899
Iteration 106/1000 | Loss: 0.00000899
Iteration 107/1000 | Loss: 0.00000899
Iteration 108/1000 | Loss: 0.00000899
Iteration 109/1000 | Loss: 0.00000899
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000899
Iteration 113/1000 | Loss: 0.00000899
Iteration 114/1000 | Loss: 0.00000899
Iteration 115/1000 | Loss: 0.00000899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [8.985084605228622e-06, 8.985084605228622e-06, 8.985084605228622e-06, 8.985084605228622e-06, 8.985084605228622e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.985084605228622e-06

Optimization complete. Final v2v error: 2.6014087200164795 mm

Highest mean error: 2.6256649494171143 mm for frame 88

Lowest mean error: 2.5878007411956787 mm for frame 117

Saving results

Total time: 35.10432529449463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831259
Iteration 2/25 | Loss: 0.00110178
Iteration 3/25 | Loss: 0.00099903
Iteration 4/25 | Loss: 0.00098984
Iteration 5/25 | Loss: 0.00098770
Iteration 6/25 | Loss: 0.00098752
Iteration 7/25 | Loss: 0.00098752
Iteration 8/25 | Loss: 0.00098752
Iteration 9/25 | Loss: 0.00098752
Iteration 10/25 | Loss: 0.00098752
Iteration 11/25 | Loss: 0.00098752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009875234682112932, 0.0009875234682112932, 0.0009875234682112932, 0.0009875234682112932, 0.0009875234682112932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009875234682112932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30973613
Iteration 2/25 | Loss: 0.00088129
Iteration 3/25 | Loss: 0.00088127
Iteration 4/25 | Loss: 0.00088127
Iteration 5/25 | Loss: 0.00088127
Iteration 6/25 | Loss: 0.00088127
Iteration 7/25 | Loss: 0.00088127
Iteration 8/25 | Loss: 0.00088127
Iteration 9/25 | Loss: 0.00088127
Iteration 10/25 | Loss: 0.00088127
Iteration 11/25 | Loss: 0.00088127
Iteration 12/25 | Loss: 0.00088127
Iteration 13/25 | Loss: 0.00088127
Iteration 14/25 | Loss: 0.00088127
Iteration 15/25 | Loss: 0.00088127
Iteration 16/25 | Loss: 0.00088127
Iteration 17/25 | Loss: 0.00088127
Iteration 18/25 | Loss: 0.00088127
Iteration 19/25 | Loss: 0.00088127
Iteration 20/25 | Loss: 0.00088127
Iteration 21/25 | Loss: 0.00088127
Iteration 22/25 | Loss: 0.00088127
Iteration 23/25 | Loss: 0.00088127
Iteration 24/25 | Loss: 0.00088127
Iteration 25/25 | Loss: 0.00088127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088127
Iteration 2/1000 | Loss: 0.00001713
Iteration 3/1000 | Loss: 0.00001118
Iteration 4/1000 | Loss: 0.00000986
Iteration 5/1000 | Loss: 0.00000884
Iteration 6/1000 | Loss: 0.00000839
Iteration 7/1000 | Loss: 0.00000796
Iteration 8/1000 | Loss: 0.00000772
Iteration 9/1000 | Loss: 0.00000771
Iteration 10/1000 | Loss: 0.00000765
Iteration 11/1000 | Loss: 0.00000765
Iteration 12/1000 | Loss: 0.00000763
Iteration 13/1000 | Loss: 0.00000744
Iteration 14/1000 | Loss: 0.00000740
Iteration 15/1000 | Loss: 0.00000736
Iteration 16/1000 | Loss: 0.00000735
Iteration 17/1000 | Loss: 0.00000735
Iteration 18/1000 | Loss: 0.00000735
Iteration 19/1000 | Loss: 0.00000734
Iteration 20/1000 | Loss: 0.00000733
Iteration 21/1000 | Loss: 0.00000733
Iteration 22/1000 | Loss: 0.00000731
Iteration 23/1000 | Loss: 0.00000729
Iteration 24/1000 | Loss: 0.00000729
Iteration 25/1000 | Loss: 0.00000728
Iteration 26/1000 | Loss: 0.00000728
Iteration 27/1000 | Loss: 0.00000728
Iteration 28/1000 | Loss: 0.00000723
Iteration 29/1000 | Loss: 0.00000721
Iteration 30/1000 | Loss: 0.00000721
Iteration 31/1000 | Loss: 0.00000720
Iteration 32/1000 | Loss: 0.00000718
Iteration 33/1000 | Loss: 0.00000718
Iteration 34/1000 | Loss: 0.00000717
Iteration 35/1000 | Loss: 0.00000717
Iteration 36/1000 | Loss: 0.00000717
Iteration 37/1000 | Loss: 0.00000716
Iteration 38/1000 | Loss: 0.00000716
Iteration 39/1000 | Loss: 0.00000716
Iteration 40/1000 | Loss: 0.00000715
Iteration 41/1000 | Loss: 0.00000715
Iteration 42/1000 | Loss: 0.00000714
Iteration 43/1000 | Loss: 0.00000714
Iteration 44/1000 | Loss: 0.00000712
Iteration 45/1000 | Loss: 0.00000712
Iteration 46/1000 | Loss: 0.00000712
Iteration 47/1000 | Loss: 0.00000712
Iteration 48/1000 | Loss: 0.00000712
Iteration 49/1000 | Loss: 0.00000712
Iteration 50/1000 | Loss: 0.00000712
Iteration 51/1000 | Loss: 0.00000711
Iteration 52/1000 | Loss: 0.00000711
Iteration 53/1000 | Loss: 0.00000711
Iteration 54/1000 | Loss: 0.00000711
Iteration 55/1000 | Loss: 0.00000711
Iteration 56/1000 | Loss: 0.00000711
Iteration 57/1000 | Loss: 0.00000711
Iteration 58/1000 | Loss: 0.00000711
Iteration 59/1000 | Loss: 0.00000711
Iteration 60/1000 | Loss: 0.00000710
Iteration 61/1000 | Loss: 0.00000709
Iteration 62/1000 | Loss: 0.00000708
Iteration 63/1000 | Loss: 0.00000708
Iteration 64/1000 | Loss: 0.00000708
Iteration 65/1000 | Loss: 0.00000708
Iteration 66/1000 | Loss: 0.00000707
Iteration 67/1000 | Loss: 0.00000707
Iteration 68/1000 | Loss: 0.00000707
Iteration 69/1000 | Loss: 0.00000707
Iteration 70/1000 | Loss: 0.00000707
Iteration 71/1000 | Loss: 0.00000707
Iteration 72/1000 | Loss: 0.00000707
Iteration 73/1000 | Loss: 0.00000707
Iteration 74/1000 | Loss: 0.00000706
Iteration 75/1000 | Loss: 0.00000706
Iteration 76/1000 | Loss: 0.00000706
Iteration 77/1000 | Loss: 0.00000706
Iteration 78/1000 | Loss: 0.00000705
Iteration 79/1000 | Loss: 0.00000705
Iteration 80/1000 | Loss: 0.00000705
Iteration 81/1000 | Loss: 0.00000705
Iteration 82/1000 | Loss: 0.00000704
Iteration 83/1000 | Loss: 0.00000704
Iteration 84/1000 | Loss: 0.00000704
Iteration 85/1000 | Loss: 0.00000704
Iteration 86/1000 | Loss: 0.00000704
Iteration 87/1000 | Loss: 0.00000704
Iteration 88/1000 | Loss: 0.00000704
Iteration 89/1000 | Loss: 0.00000704
Iteration 90/1000 | Loss: 0.00000704
Iteration 91/1000 | Loss: 0.00000704
Iteration 92/1000 | Loss: 0.00000704
Iteration 93/1000 | Loss: 0.00000704
Iteration 94/1000 | Loss: 0.00000703
Iteration 95/1000 | Loss: 0.00000703
Iteration 96/1000 | Loss: 0.00000703
Iteration 97/1000 | Loss: 0.00000703
Iteration 98/1000 | Loss: 0.00000703
Iteration 99/1000 | Loss: 0.00000703
Iteration 100/1000 | Loss: 0.00000703
Iteration 101/1000 | Loss: 0.00000703
Iteration 102/1000 | Loss: 0.00000703
Iteration 103/1000 | Loss: 0.00000702
Iteration 104/1000 | Loss: 0.00000702
Iteration 105/1000 | Loss: 0.00000702
Iteration 106/1000 | Loss: 0.00000702
Iteration 107/1000 | Loss: 0.00000702
Iteration 108/1000 | Loss: 0.00000702
Iteration 109/1000 | Loss: 0.00000702
Iteration 110/1000 | Loss: 0.00000701
Iteration 111/1000 | Loss: 0.00000701
Iteration 112/1000 | Loss: 0.00000701
Iteration 113/1000 | Loss: 0.00000701
Iteration 114/1000 | Loss: 0.00000701
Iteration 115/1000 | Loss: 0.00000700
Iteration 116/1000 | Loss: 0.00000700
Iteration 117/1000 | Loss: 0.00000700
Iteration 118/1000 | Loss: 0.00000700
Iteration 119/1000 | Loss: 0.00000700
Iteration 120/1000 | Loss: 0.00000699
Iteration 121/1000 | Loss: 0.00000699
Iteration 122/1000 | Loss: 0.00000699
Iteration 123/1000 | Loss: 0.00000699
Iteration 124/1000 | Loss: 0.00000699
Iteration 125/1000 | Loss: 0.00000699
Iteration 126/1000 | Loss: 0.00000699
Iteration 127/1000 | Loss: 0.00000699
Iteration 128/1000 | Loss: 0.00000699
Iteration 129/1000 | Loss: 0.00000699
Iteration 130/1000 | Loss: 0.00000699
Iteration 131/1000 | Loss: 0.00000698
Iteration 132/1000 | Loss: 0.00000698
Iteration 133/1000 | Loss: 0.00000698
Iteration 134/1000 | Loss: 0.00000698
Iteration 135/1000 | Loss: 0.00000698
Iteration 136/1000 | Loss: 0.00000698
Iteration 137/1000 | Loss: 0.00000698
Iteration 138/1000 | Loss: 0.00000698
Iteration 139/1000 | Loss: 0.00000698
Iteration 140/1000 | Loss: 0.00000698
Iteration 141/1000 | Loss: 0.00000697
Iteration 142/1000 | Loss: 0.00000697
Iteration 143/1000 | Loss: 0.00000697
Iteration 144/1000 | Loss: 0.00000697
Iteration 145/1000 | Loss: 0.00000697
Iteration 146/1000 | Loss: 0.00000697
Iteration 147/1000 | Loss: 0.00000697
Iteration 148/1000 | Loss: 0.00000697
Iteration 149/1000 | Loss: 0.00000697
Iteration 150/1000 | Loss: 0.00000697
Iteration 151/1000 | Loss: 0.00000697
Iteration 152/1000 | Loss: 0.00000697
Iteration 153/1000 | Loss: 0.00000696
Iteration 154/1000 | Loss: 0.00000696
Iteration 155/1000 | Loss: 0.00000696
Iteration 156/1000 | Loss: 0.00000696
Iteration 157/1000 | Loss: 0.00000696
Iteration 158/1000 | Loss: 0.00000696
Iteration 159/1000 | Loss: 0.00000695
Iteration 160/1000 | Loss: 0.00000695
Iteration 161/1000 | Loss: 0.00000695
Iteration 162/1000 | Loss: 0.00000695
Iteration 163/1000 | Loss: 0.00000695
Iteration 164/1000 | Loss: 0.00000695
Iteration 165/1000 | Loss: 0.00000694
Iteration 166/1000 | Loss: 0.00000694
Iteration 167/1000 | Loss: 0.00000694
Iteration 168/1000 | Loss: 0.00000694
Iteration 169/1000 | Loss: 0.00000694
Iteration 170/1000 | Loss: 0.00000694
Iteration 171/1000 | Loss: 0.00000694
Iteration 172/1000 | Loss: 0.00000693
Iteration 173/1000 | Loss: 0.00000693
Iteration 174/1000 | Loss: 0.00000693
Iteration 175/1000 | Loss: 0.00000692
Iteration 176/1000 | Loss: 0.00000692
Iteration 177/1000 | Loss: 0.00000692
Iteration 178/1000 | Loss: 0.00000692
Iteration 179/1000 | Loss: 0.00000692
Iteration 180/1000 | Loss: 0.00000692
Iteration 181/1000 | Loss: 0.00000691
Iteration 182/1000 | Loss: 0.00000691
Iteration 183/1000 | Loss: 0.00000691
Iteration 184/1000 | Loss: 0.00000691
Iteration 185/1000 | Loss: 0.00000691
Iteration 186/1000 | Loss: 0.00000691
Iteration 187/1000 | Loss: 0.00000690
Iteration 188/1000 | Loss: 0.00000690
Iteration 189/1000 | Loss: 0.00000690
Iteration 190/1000 | Loss: 0.00000690
Iteration 191/1000 | Loss: 0.00000690
Iteration 192/1000 | Loss: 0.00000690
Iteration 193/1000 | Loss: 0.00000690
Iteration 194/1000 | Loss: 0.00000690
Iteration 195/1000 | Loss: 0.00000690
Iteration 196/1000 | Loss: 0.00000689
Iteration 197/1000 | Loss: 0.00000689
Iteration 198/1000 | Loss: 0.00000689
Iteration 199/1000 | Loss: 0.00000689
Iteration 200/1000 | Loss: 0.00000689
Iteration 201/1000 | Loss: 0.00000689
Iteration 202/1000 | Loss: 0.00000689
Iteration 203/1000 | Loss: 0.00000689
Iteration 204/1000 | Loss: 0.00000689
Iteration 205/1000 | Loss: 0.00000689
Iteration 206/1000 | Loss: 0.00000689
Iteration 207/1000 | Loss: 0.00000689
Iteration 208/1000 | Loss: 0.00000689
Iteration 209/1000 | Loss: 0.00000688
Iteration 210/1000 | Loss: 0.00000688
Iteration 211/1000 | Loss: 0.00000688
Iteration 212/1000 | Loss: 0.00000688
Iteration 213/1000 | Loss: 0.00000688
Iteration 214/1000 | Loss: 0.00000688
Iteration 215/1000 | Loss: 0.00000688
Iteration 216/1000 | Loss: 0.00000688
Iteration 217/1000 | Loss: 0.00000688
Iteration 218/1000 | Loss: 0.00000688
Iteration 219/1000 | Loss: 0.00000688
Iteration 220/1000 | Loss: 0.00000688
Iteration 221/1000 | Loss: 0.00000688
Iteration 222/1000 | Loss: 0.00000688
Iteration 223/1000 | Loss: 0.00000688
Iteration 224/1000 | Loss: 0.00000688
Iteration 225/1000 | Loss: 0.00000688
Iteration 226/1000 | Loss: 0.00000688
Iteration 227/1000 | Loss: 0.00000688
Iteration 228/1000 | Loss: 0.00000688
Iteration 229/1000 | Loss: 0.00000687
Iteration 230/1000 | Loss: 0.00000687
Iteration 231/1000 | Loss: 0.00000687
Iteration 232/1000 | Loss: 0.00000687
Iteration 233/1000 | Loss: 0.00000687
Iteration 234/1000 | Loss: 0.00000687
Iteration 235/1000 | Loss: 0.00000687
Iteration 236/1000 | Loss: 0.00000687
Iteration 237/1000 | Loss: 0.00000687
Iteration 238/1000 | Loss: 0.00000687
Iteration 239/1000 | Loss: 0.00000687
Iteration 240/1000 | Loss: 0.00000687
Iteration 241/1000 | Loss: 0.00000687
Iteration 242/1000 | Loss: 0.00000687
Iteration 243/1000 | Loss: 0.00000687
Iteration 244/1000 | Loss: 0.00000687
Iteration 245/1000 | Loss: 0.00000687
Iteration 246/1000 | Loss: 0.00000687
Iteration 247/1000 | Loss: 0.00000687
Iteration 248/1000 | Loss: 0.00000687
Iteration 249/1000 | Loss: 0.00000687
Iteration 250/1000 | Loss: 0.00000687
Iteration 251/1000 | Loss: 0.00000687
Iteration 252/1000 | Loss: 0.00000687
Iteration 253/1000 | Loss: 0.00000687
Iteration 254/1000 | Loss: 0.00000687
Iteration 255/1000 | Loss: 0.00000687
Iteration 256/1000 | Loss: 0.00000687
Iteration 257/1000 | Loss: 0.00000687
Iteration 258/1000 | Loss: 0.00000687
Iteration 259/1000 | Loss: 0.00000687
Iteration 260/1000 | Loss: 0.00000687
Iteration 261/1000 | Loss: 0.00000687
Iteration 262/1000 | Loss: 0.00000687
Iteration 263/1000 | Loss: 0.00000687
Iteration 264/1000 | Loss: 0.00000687
Iteration 265/1000 | Loss: 0.00000687
Iteration 266/1000 | Loss: 0.00000687
Iteration 267/1000 | Loss: 0.00000687
Iteration 268/1000 | Loss: 0.00000687
Iteration 269/1000 | Loss: 0.00000687
Iteration 270/1000 | Loss: 0.00000687
Iteration 271/1000 | Loss: 0.00000687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [6.867197043902706e-06, 6.867197043902706e-06, 6.867197043902706e-06, 6.867197043902706e-06, 6.867197043902706e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.867197043902706e-06

Optimization complete. Final v2v error: 2.2699618339538574 mm

Highest mean error: 2.3773746490478516 mm for frame 25

Lowest mean error: 2.1811797618865967 mm for frame 44

Saving results

Total time: 40.56793832778931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786865
Iteration 2/25 | Loss: 0.00142712
Iteration 3/25 | Loss: 0.00129794
Iteration 4/25 | Loss: 0.00115542
Iteration 5/25 | Loss: 0.00117404
Iteration 6/25 | Loss: 0.00111431
Iteration 7/25 | Loss: 0.00111652
Iteration 8/25 | Loss: 0.00110525
Iteration 9/25 | Loss: 0.00109196
Iteration 10/25 | Loss: 0.00108961
Iteration 11/25 | Loss: 0.00110089
Iteration 12/25 | Loss: 0.00109203
Iteration 13/25 | Loss: 0.00108932
Iteration 14/25 | Loss: 0.00108967
Iteration 15/25 | Loss: 0.00108903
Iteration 16/25 | Loss: 0.00108969
Iteration 17/25 | Loss: 0.00108967
Iteration 18/25 | Loss: 0.00109035
Iteration 19/25 | Loss: 0.00108950
Iteration 20/25 | Loss: 0.00108950
Iteration 21/25 | Loss: 0.00108950
Iteration 22/25 | Loss: 0.00108884
Iteration 23/25 | Loss: 0.00108983
Iteration 24/25 | Loss: 0.00109050
Iteration 25/25 | Loss: 0.00108795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43874598
Iteration 2/25 | Loss: 0.00115531
Iteration 3/25 | Loss: 0.00115529
Iteration 4/25 | Loss: 0.00115529
Iteration 5/25 | Loss: 0.00115529
Iteration 6/25 | Loss: 0.00115529
Iteration 7/25 | Loss: 0.00115529
Iteration 8/25 | Loss: 0.00115529
Iteration 9/25 | Loss: 0.00115529
Iteration 10/25 | Loss: 0.00115529
Iteration 11/25 | Loss: 0.00115529
Iteration 12/25 | Loss: 0.00115529
Iteration 13/25 | Loss: 0.00115529
Iteration 14/25 | Loss: 0.00115529
Iteration 15/25 | Loss: 0.00115529
Iteration 16/25 | Loss: 0.00115529
Iteration 17/25 | Loss: 0.00115529
Iteration 18/25 | Loss: 0.00115529
Iteration 19/25 | Loss: 0.00115529
Iteration 20/25 | Loss: 0.00115529
Iteration 21/25 | Loss: 0.00115529
Iteration 22/25 | Loss: 0.00115529
Iteration 23/25 | Loss: 0.00115529
Iteration 24/25 | Loss: 0.00115529
Iteration 25/25 | Loss: 0.00115529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115529
Iteration 2/1000 | Loss: 0.00004234
Iteration 3/1000 | Loss: 0.00002778
Iteration 4/1000 | Loss: 0.00002253
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001739
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001600
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001541
Iteration 14/1000 | Loss: 0.00001527
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001519
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001504
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001490
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001484
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001483
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001482
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.4815727809036616e-05, 1.4815727809036616e-05, 1.4815727809036616e-05, 1.4815727809036616e-05, 1.4815727809036616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4815727809036616e-05

Optimization complete. Final v2v error: 3.143277406692505 mm

Highest mean error: 4.523555755615234 mm for frame 131

Lowest mean error: 2.303001880645752 mm for frame 168

Saving results

Total time: 81.57965326309204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979414
Iteration 2/25 | Loss: 0.00164241
Iteration 3/25 | Loss: 0.00120558
Iteration 4/25 | Loss: 0.00116900
Iteration 5/25 | Loss: 0.00113546
Iteration 6/25 | Loss: 0.00111383
Iteration 7/25 | Loss: 0.00109814
Iteration 8/25 | Loss: 0.00108804
Iteration 9/25 | Loss: 0.00109094
Iteration 10/25 | Loss: 0.00108708
Iteration 11/25 | Loss: 0.00106676
Iteration 12/25 | Loss: 0.00106443
Iteration 13/25 | Loss: 0.00105417
Iteration 14/25 | Loss: 0.00104496
Iteration 15/25 | Loss: 0.00104387
Iteration 16/25 | Loss: 0.00103956
Iteration 17/25 | Loss: 0.00103885
Iteration 18/25 | Loss: 0.00103454
Iteration 19/25 | Loss: 0.00103379
Iteration 20/25 | Loss: 0.00103355
Iteration 21/25 | Loss: 0.00103353
Iteration 22/25 | Loss: 0.00103352
Iteration 23/25 | Loss: 0.00103352
Iteration 24/25 | Loss: 0.00103352
Iteration 25/25 | Loss: 0.00103352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41699076
Iteration 2/25 | Loss: 0.00134260
Iteration 3/25 | Loss: 0.00134260
Iteration 4/25 | Loss: 0.00134260
Iteration 5/25 | Loss: 0.00134260
Iteration 6/25 | Loss: 0.00134260
Iteration 7/25 | Loss: 0.00134260
Iteration 8/25 | Loss: 0.00134260
Iteration 9/25 | Loss: 0.00134260
Iteration 10/25 | Loss: 0.00134260
Iteration 11/25 | Loss: 0.00134260
Iteration 12/25 | Loss: 0.00134260
Iteration 13/25 | Loss: 0.00134260
Iteration 14/25 | Loss: 0.00134260
Iteration 15/25 | Loss: 0.00134260
Iteration 16/25 | Loss: 0.00134260
Iteration 17/25 | Loss: 0.00134260
Iteration 18/25 | Loss: 0.00134260
Iteration 19/25 | Loss: 0.00134260
Iteration 20/25 | Loss: 0.00134260
Iteration 21/25 | Loss: 0.00134260
Iteration 22/25 | Loss: 0.00134260
Iteration 23/25 | Loss: 0.00134260
Iteration 24/25 | Loss: 0.00134260
Iteration 25/25 | Loss: 0.00134260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134260
Iteration 2/1000 | Loss: 0.00005154
Iteration 3/1000 | Loss: 0.00006111
Iteration 4/1000 | Loss: 0.00003283
Iteration 5/1000 | Loss: 0.00008079
Iteration 6/1000 | Loss: 0.00002165
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00002901
Iteration 9/1000 | Loss: 0.00001865
Iteration 10/1000 | Loss: 0.00003961
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001758
Iteration 14/1000 | Loss: 0.00004715
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00011204
Iteration 18/1000 | Loss: 0.00007916
Iteration 19/1000 | Loss: 0.00003578
Iteration 20/1000 | Loss: 0.00010316
Iteration 21/1000 | Loss: 0.00004778
Iteration 22/1000 | Loss: 0.00003633
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001492
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00004954
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00013989
Iteration 31/1000 | Loss: 0.00004639
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00012261
Iteration 35/1000 | Loss: 0.00012295
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00012487
Iteration 38/1000 | Loss: 0.00010868
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001355
Iteration 42/1000 | Loss: 0.00030408
Iteration 43/1000 | Loss: 0.00007579
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00020412
Iteration 46/1000 | Loss: 0.00007150
Iteration 47/1000 | Loss: 0.00002869
Iteration 48/1000 | Loss: 0.00012521
Iteration 49/1000 | Loss: 0.00004930
Iteration 50/1000 | Loss: 0.00002122
Iteration 51/1000 | Loss: 0.00004810
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001147
Iteration 54/1000 | Loss: 0.00002694
Iteration 55/1000 | Loss: 0.00004398
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001088
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001053
Iteration 63/1000 | Loss: 0.00003919
Iteration 64/1000 | Loss: 0.00003324
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00005472
Iteration 67/1000 | Loss: 0.00004247
Iteration 68/1000 | Loss: 0.00047069
Iteration 69/1000 | Loss: 0.00001032
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001014
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001013
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001009
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001009
Iteration 92/1000 | Loss: 0.00001009
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001008
Iteration 95/1000 | Loss: 0.00001008
Iteration 96/1000 | Loss: 0.00002962
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001007
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001007
Iteration 102/1000 | Loss: 0.00001007
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001006
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001006
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001005
Iteration 109/1000 | Loss: 0.00001005
Iteration 110/1000 | Loss: 0.00001005
Iteration 111/1000 | Loss: 0.00001005
Iteration 112/1000 | Loss: 0.00001005
Iteration 113/1000 | Loss: 0.00001005
Iteration 114/1000 | Loss: 0.00001005
Iteration 115/1000 | Loss: 0.00001005
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001005
Iteration 124/1000 | Loss: 0.00001005
Iteration 125/1000 | Loss: 0.00001005
Iteration 126/1000 | Loss: 0.00001005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.0049555385194253e-05, 1.0049555385194253e-05, 1.0049555385194253e-05, 1.0049555385194253e-05, 1.0049555385194253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0049555385194253e-05

Optimization complete. Final v2v error: 2.672122001647949 mm

Highest mean error: 4.686596870422363 mm for frame 58

Lowest mean error: 2.062824249267578 mm for frame 98

Saving results

Total time: 129.55399775505066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818217
Iteration 2/25 | Loss: 0.00113791
Iteration 3/25 | Loss: 0.00102371
Iteration 4/25 | Loss: 0.00101101
Iteration 5/25 | Loss: 0.00100824
Iteration 6/25 | Loss: 0.00100822
Iteration 7/25 | Loss: 0.00100822
Iteration 8/25 | Loss: 0.00100822
Iteration 9/25 | Loss: 0.00100822
Iteration 10/25 | Loss: 0.00100822
Iteration 11/25 | Loss: 0.00100822
Iteration 12/25 | Loss: 0.00100822
Iteration 13/25 | Loss: 0.00100822
Iteration 14/25 | Loss: 0.00100822
Iteration 15/25 | Loss: 0.00100822
Iteration 16/25 | Loss: 0.00100822
Iteration 17/25 | Loss: 0.00100822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010082183871418238, 0.0010082183871418238, 0.0010082183871418238, 0.0010082183871418238, 0.0010082183871418238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010082183871418238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31041837
Iteration 2/25 | Loss: 0.00112938
Iteration 3/25 | Loss: 0.00112938
Iteration 4/25 | Loss: 0.00112938
Iteration 5/25 | Loss: 0.00112938
Iteration 6/25 | Loss: 0.00112938
Iteration 7/25 | Loss: 0.00112938
Iteration 8/25 | Loss: 0.00112938
Iteration 9/25 | Loss: 0.00112938
Iteration 10/25 | Loss: 0.00112938
Iteration 11/25 | Loss: 0.00112938
Iteration 12/25 | Loss: 0.00112938
Iteration 13/25 | Loss: 0.00112938
Iteration 14/25 | Loss: 0.00112938
Iteration 15/25 | Loss: 0.00112938
Iteration 16/25 | Loss: 0.00112938
Iteration 17/25 | Loss: 0.00112938
Iteration 18/25 | Loss: 0.00112938
Iteration 19/25 | Loss: 0.00112938
Iteration 20/25 | Loss: 0.00112938
Iteration 21/25 | Loss: 0.00112938
Iteration 22/25 | Loss: 0.00112938
Iteration 23/25 | Loss: 0.00112938
Iteration 24/25 | Loss: 0.00112938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011293794959783554, 0.0011293794959783554, 0.0011293794959783554, 0.0011293794959783554, 0.0011293794959783554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011293794959783554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112938
Iteration 2/1000 | Loss: 0.00002480
Iteration 3/1000 | Loss: 0.00001598
Iteration 4/1000 | Loss: 0.00001226
Iteration 5/1000 | Loss: 0.00001100
Iteration 6/1000 | Loss: 0.00001020
Iteration 7/1000 | Loss: 0.00000977
Iteration 8/1000 | Loss: 0.00000932
Iteration 9/1000 | Loss: 0.00000913
Iteration 10/1000 | Loss: 0.00000887
Iteration 11/1000 | Loss: 0.00000871
Iteration 12/1000 | Loss: 0.00000865
Iteration 13/1000 | Loss: 0.00000861
Iteration 14/1000 | Loss: 0.00000855
Iteration 15/1000 | Loss: 0.00000854
Iteration 16/1000 | Loss: 0.00000854
Iteration 17/1000 | Loss: 0.00000851
Iteration 18/1000 | Loss: 0.00000850
Iteration 19/1000 | Loss: 0.00000849
Iteration 20/1000 | Loss: 0.00000849
Iteration 21/1000 | Loss: 0.00000847
Iteration 22/1000 | Loss: 0.00000846
Iteration 23/1000 | Loss: 0.00000845
Iteration 24/1000 | Loss: 0.00000842
Iteration 25/1000 | Loss: 0.00000841
Iteration 26/1000 | Loss: 0.00000841
Iteration 27/1000 | Loss: 0.00000839
Iteration 28/1000 | Loss: 0.00000839
Iteration 29/1000 | Loss: 0.00000838
Iteration 30/1000 | Loss: 0.00000838
Iteration 31/1000 | Loss: 0.00000837
Iteration 32/1000 | Loss: 0.00000837
Iteration 33/1000 | Loss: 0.00000837
Iteration 34/1000 | Loss: 0.00000837
Iteration 35/1000 | Loss: 0.00000836
Iteration 36/1000 | Loss: 0.00000836
Iteration 37/1000 | Loss: 0.00000835
Iteration 38/1000 | Loss: 0.00000835
Iteration 39/1000 | Loss: 0.00000834
Iteration 40/1000 | Loss: 0.00000834
Iteration 41/1000 | Loss: 0.00000833
Iteration 42/1000 | Loss: 0.00000833
Iteration 43/1000 | Loss: 0.00000832
Iteration 44/1000 | Loss: 0.00000831
Iteration 45/1000 | Loss: 0.00000831
Iteration 46/1000 | Loss: 0.00000831
Iteration 47/1000 | Loss: 0.00000831
Iteration 48/1000 | Loss: 0.00000830
Iteration 49/1000 | Loss: 0.00000830
Iteration 50/1000 | Loss: 0.00000829
Iteration 51/1000 | Loss: 0.00000829
Iteration 52/1000 | Loss: 0.00000828
Iteration 53/1000 | Loss: 0.00000828
Iteration 54/1000 | Loss: 0.00000828
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000826
Iteration 57/1000 | Loss: 0.00000826
Iteration 58/1000 | Loss: 0.00000825
Iteration 59/1000 | Loss: 0.00000825
Iteration 60/1000 | Loss: 0.00000825
Iteration 61/1000 | Loss: 0.00000825
Iteration 62/1000 | Loss: 0.00000824
Iteration 63/1000 | Loss: 0.00000824
Iteration 64/1000 | Loss: 0.00000824
Iteration 65/1000 | Loss: 0.00000823
Iteration 66/1000 | Loss: 0.00000823
Iteration 67/1000 | Loss: 0.00000823
Iteration 68/1000 | Loss: 0.00000823
Iteration 69/1000 | Loss: 0.00000823
Iteration 70/1000 | Loss: 0.00000822
Iteration 71/1000 | Loss: 0.00000822
Iteration 72/1000 | Loss: 0.00000822
Iteration 73/1000 | Loss: 0.00000822
Iteration 74/1000 | Loss: 0.00000822
Iteration 75/1000 | Loss: 0.00000821
Iteration 76/1000 | Loss: 0.00000821
Iteration 77/1000 | Loss: 0.00000821
Iteration 78/1000 | Loss: 0.00000821
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000820
Iteration 81/1000 | Loss: 0.00000820
Iteration 82/1000 | Loss: 0.00000820
Iteration 83/1000 | Loss: 0.00000820
Iteration 84/1000 | Loss: 0.00000819
Iteration 85/1000 | Loss: 0.00000818
Iteration 86/1000 | Loss: 0.00000818
Iteration 87/1000 | Loss: 0.00000817
Iteration 88/1000 | Loss: 0.00000817
Iteration 89/1000 | Loss: 0.00000817
Iteration 90/1000 | Loss: 0.00000816
Iteration 91/1000 | Loss: 0.00000816
Iteration 92/1000 | Loss: 0.00000816
Iteration 93/1000 | Loss: 0.00000816
Iteration 94/1000 | Loss: 0.00000816
Iteration 95/1000 | Loss: 0.00000816
Iteration 96/1000 | Loss: 0.00000816
Iteration 97/1000 | Loss: 0.00000816
Iteration 98/1000 | Loss: 0.00000815
Iteration 99/1000 | Loss: 0.00000815
Iteration 100/1000 | Loss: 0.00000815
Iteration 101/1000 | Loss: 0.00000815
Iteration 102/1000 | Loss: 0.00000814
Iteration 103/1000 | Loss: 0.00000814
Iteration 104/1000 | Loss: 0.00000814
Iteration 105/1000 | Loss: 0.00000814
Iteration 106/1000 | Loss: 0.00000814
Iteration 107/1000 | Loss: 0.00000814
Iteration 108/1000 | Loss: 0.00000814
Iteration 109/1000 | Loss: 0.00000813
Iteration 110/1000 | Loss: 0.00000813
Iteration 111/1000 | Loss: 0.00000813
Iteration 112/1000 | Loss: 0.00000813
Iteration 113/1000 | Loss: 0.00000813
Iteration 114/1000 | Loss: 0.00000813
Iteration 115/1000 | Loss: 0.00000813
Iteration 116/1000 | Loss: 0.00000813
Iteration 117/1000 | Loss: 0.00000812
Iteration 118/1000 | Loss: 0.00000812
Iteration 119/1000 | Loss: 0.00000812
Iteration 120/1000 | Loss: 0.00000812
Iteration 121/1000 | Loss: 0.00000812
Iteration 122/1000 | Loss: 0.00000812
Iteration 123/1000 | Loss: 0.00000812
Iteration 124/1000 | Loss: 0.00000812
Iteration 125/1000 | Loss: 0.00000812
Iteration 126/1000 | Loss: 0.00000811
Iteration 127/1000 | Loss: 0.00000810
Iteration 128/1000 | Loss: 0.00000810
Iteration 129/1000 | Loss: 0.00000810
Iteration 130/1000 | Loss: 0.00000810
Iteration 131/1000 | Loss: 0.00000810
Iteration 132/1000 | Loss: 0.00000810
Iteration 133/1000 | Loss: 0.00000810
Iteration 134/1000 | Loss: 0.00000810
Iteration 135/1000 | Loss: 0.00000810
Iteration 136/1000 | Loss: 0.00000810
Iteration 137/1000 | Loss: 0.00000809
Iteration 138/1000 | Loss: 0.00000809
Iteration 139/1000 | Loss: 0.00000809
Iteration 140/1000 | Loss: 0.00000808
Iteration 141/1000 | Loss: 0.00000808
Iteration 142/1000 | Loss: 0.00000808
Iteration 143/1000 | Loss: 0.00000808
Iteration 144/1000 | Loss: 0.00000807
Iteration 145/1000 | Loss: 0.00000807
Iteration 146/1000 | Loss: 0.00000807
Iteration 147/1000 | Loss: 0.00000807
Iteration 148/1000 | Loss: 0.00000807
Iteration 149/1000 | Loss: 0.00000807
Iteration 150/1000 | Loss: 0.00000807
Iteration 151/1000 | Loss: 0.00000807
Iteration 152/1000 | Loss: 0.00000807
Iteration 153/1000 | Loss: 0.00000807
Iteration 154/1000 | Loss: 0.00000807
Iteration 155/1000 | Loss: 0.00000807
Iteration 156/1000 | Loss: 0.00000806
Iteration 157/1000 | Loss: 0.00000806
Iteration 158/1000 | Loss: 0.00000806
Iteration 159/1000 | Loss: 0.00000806
Iteration 160/1000 | Loss: 0.00000806
Iteration 161/1000 | Loss: 0.00000806
Iteration 162/1000 | Loss: 0.00000806
Iteration 163/1000 | Loss: 0.00000806
Iteration 164/1000 | Loss: 0.00000806
Iteration 165/1000 | Loss: 0.00000806
Iteration 166/1000 | Loss: 0.00000806
Iteration 167/1000 | Loss: 0.00000806
Iteration 168/1000 | Loss: 0.00000806
Iteration 169/1000 | Loss: 0.00000805
Iteration 170/1000 | Loss: 0.00000805
Iteration 171/1000 | Loss: 0.00000805
Iteration 172/1000 | Loss: 0.00000805
Iteration 173/1000 | Loss: 0.00000805
Iteration 174/1000 | Loss: 0.00000805
Iteration 175/1000 | Loss: 0.00000805
Iteration 176/1000 | Loss: 0.00000805
Iteration 177/1000 | Loss: 0.00000805
Iteration 178/1000 | Loss: 0.00000805
Iteration 179/1000 | Loss: 0.00000805
Iteration 180/1000 | Loss: 0.00000805
Iteration 181/1000 | Loss: 0.00000804
Iteration 182/1000 | Loss: 0.00000804
Iteration 183/1000 | Loss: 0.00000804
Iteration 184/1000 | Loss: 0.00000804
Iteration 185/1000 | Loss: 0.00000804
Iteration 186/1000 | Loss: 0.00000804
Iteration 187/1000 | Loss: 0.00000804
Iteration 188/1000 | Loss: 0.00000804
Iteration 189/1000 | Loss: 0.00000804
Iteration 190/1000 | Loss: 0.00000804
Iteration 191/1000 | Loss: 0.00000804
Iteration 192/1000 | Loss: 0.00000804
Iteration 193/1000 | Loss: 0.00000804
Iteration 194/1000 | Loss: 0.00000804
Iteration 195/1000 | Loss: 0.00000804
Iteration 196/1000 | Loss: 0.00000804
Iteration 197/1000 | Loss: 0.00000803
Iteration 198/1000 | Loss: 0.00000803
Iteration 199/1000 | Loss: 0.00000803
Iteration 200/1000 | Loss: 0.00000803
Iteration 201/1000 | Loss: 0.00000803
Iteration 202/1000 | Loss: 0.00000803
Iteration 203/1000 | Loss: 0.00000803
Iteration 204/1000 | Loss: 0.00000803
Iteration 205/1000 | Loss: 0.00000803
Iteration 206/1000 | Loss: 0.00000803
Iteration 207/1000 | Loss: 0.00000803
Iteration 208/1000 | Loss: 0.00000803
Iteration 209/1000 | Loss: 0.00000803
Iteration 210/1000 | Loss: 0.00000803
Iteration 211/1000 | Loss: 0.00000803
Iteration 212/1000 | Loss: 0.00000802
Iteration 213/1000 | Loss: 0.00000802
Iteration 214/1000 | Loss: 0.00000802
Iteration 215/1000 | Loss: 0.00000802
Iteration 216/1000 | Loss: 0.00000802
Iteration 217/1000 | Loss: 0.00000802
Iteration 218/1000 | Loss: 0.00000802
Iteration 219/1000 | Loss: 0.00000802
Iteration 220/1000 | Loss: 0.00000802
Iteration 221/1000 | Loss: 0.00000802
Iteration 222/1000 | Loss: 0.00000802
Iteration 223/1000 | Loss: 0.00000802
Iteration 224/1000 | Loss: 0.00000802
Iteration 225/1000 | Loss: 0.00000802
Iteration 226/1000 | Loss: 0.00000802
Iteration 227/1000 | Loss: 0.00000802
Iteration 228/1000 | Loss: 0.00000802
Iteration 229/1000 | Loss: 0.00000802
Iteration 230/1000 | Loss: 0.00000802
Iteration 231/1000 | Loss: 0.00000802
Iteration 232/1000 | Loss: 0.00000802
Iteration 233/1000 | Loss: 0.00000802
Iteration 234/1000 | Loss: 0.00000802
Iteration 235/1000 | Loss: 0.00000802
Iteration 236/1000 | Loss: 0.00000802
Iteration 237/1000 | Loss: 0.00000802
Iteration 238/1000 | Loss: 0.00000802
Iteration 239/1000 | Loss: 0.00000802
Iteration 240/1000 | Loss: 0.00000802
Iteration 241/1000 | Loss: 0.00000802
Iteration 242/1000 | Loss: 0.00000802
Iteration 243/1000 | Loss: 0.00000802
Iteration 244/1000 | Loss: 0.00000802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [8.024732778721955e-06, 8.024732778721955e-06, 8.024732778721955e-06, 8.024732778721955e-06, 8.024732778721955e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.024732778721955e-06

Optimization complete. Final v2v error: 2.3718183040618896 mm

Highest mean error: 3.4456231594085693 mm for frame 66

Lowest mean error: 2.045377254486084 mm for frame 139

Saving results

Total time: 42.373756647109985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053360
Iteration 2/25 | Loss: 0.00195135
Iteration 3/25 | Loss: 0.00136396
Iteration 4/25 | Loss: 0.00128354
Iteration 5/25 | Loss: 0.00124032
Iteration 6/25 | Loss: 0.00123535
Iteration 7/25 | Loss: 0.00120708
Iteration 8/25 | Loss: 0.00118314
Iteration 9/25 | Loss: 0.00117651
Iteration 10/25 | Loss: 0.00117646
Iteration 11/25 | Loss: 0.00117491
Iteration 12/25 | Loss: 0.00117451
Iteration 13/25 | Loss: 0.00117451
Iteration 14/25 | Loss: 0.00117451
Iteration 15/25 | Loss: 0.00117450
Iteration 16/25 | Loss: 0.00117450
Iteration 17/25 | Loss: 0.00117450
Iteration 18/25 | Loss: 0.00117450
Iteration 19/25 | Loss: 0.00117450
Iteration 20/25 | Loss: 0.00117450
Iteration 21/25 | Loss: 0.00117449
Iteration 22/25 | Loss: 0.00117449
Iteration 23/25 | Loss: 0.00117449
Iteration 24/25 | Loss: 0.00117449
Iteration 25/25 | Loss: 0.00117449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32145190
Iteration 2/25 | Loss: 0.00124885
Iteration 3/25 | Loss: 0.00120285
Iteration 4/25 | Loss: 0.00120285
Iteration 5/25 | Loss: 0.00120285
Iteration 6/25 | Loss: 0.00120285
Iteration 7/25 | Loss: 0.00120285
Iteration 8/25 | Loss: 0.00120285
Iteration 9/25 | Loss: 0.00120285
Iteration 10/25 | Loss: 0.00120285
Iteration 11/25 | Loss: 0.00120285
Iteration 12/25 | Loss: 0.00120285
Iteration 13/25 | Loss: 0.00120285
Iteration 14/25 | Loss: 0.00120285
Iteration 15/25 | Loss: 0.00120285
Iteration 16/25 | Loss: 0.00120285
Iteration 17/25 | Loss: 0.00120285
Iteration 18/25 | Loss: 0.00120285
Iteration 19/25 | Loss: 0.00120285
Iteration 20/25 | Loss: 0.00120285
Iteration 21/25 | Loss: 0.00120285
Iteration 22/25 | Loss: 0.00120285
Iteration 23/25 | Loss: 0.00120285
Iteration 24/25 | Loss: 0.00120285
Iteration 25/25 | Loss: 0.00120285
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012028493219986558, 0.0012028493219986558, 0.0012028493219986558, 0.0012028493219986558, 0.0012028493219986558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012028493219986558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120285
Iteration 2/1000 | Loss: 0.00006869
Iteration 3/1000 | Loss: 0.00003368
Iteration 4/1000 | Loss: 0.00007293
Iteration 5/1000 | Loss: 0.00044024
Iteration 6/1000 | Loss: 0.00023083
Iteration 7/1000 | Loss: 0.00006945
Iteration 8/1000 | Loss: 0.00003976
Iteration 9/1000 | Loss: 0.00039039
Iteration 10/1000 | Loss: 0.00020882
Iteration 11/1000 | Loss: 0.00004799
Iteration 12/1000 | Loss: 0.00006054
Iteration 13/1000 | Loss: 0.00002792
Iteration 14/1000 | Loss: 0.00007971
Iteration 15/1000 | Loss: 0.00003566
Iteration 16/1000 | Loss: 0.00004680
Iteration 17/1000 | Loss: 0.00003659
Iteration 18/1000 | Loss: 0.00023469
Iteration 19/1000 | Loss: 0.00003735
Iteration 20/1000 | Loss: 0.00004717
Iteration 21/1000 | Loss: 0.00003371
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00016272
Iteration 24/1000 | Loss: 0.00002485
Iteration 25/1000 | Loss: 0.00002456
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00004835
Iteration 28/1000 | Loss: 0.00002436
Iteration 29/1000 | Loss: 0.00012990
Iteration 30/1000 | Loss: 0.00002444
Iteration 31/1000 | Loss: 0.00005392
Iteration 32/1000 | Loss: 0.00002398
Iteration 33/1000 | Loss: 0.00002394
Iteration 34/1000 | Loss: 0.00002394
Iteration 35/1000 | Loss: 0.00002393
Iteration 36/1000 | Loss: 0.00003938
Iteration 37/1000 | Loss: 0.00004591
Iteration 38/1000 | Loss: 0.00003548
Iteration 39/1000 | Loss: 0.00002920
Iteration 40/1000 | Loss: 0.00002383
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002382
Iteration 43/1000 | Loss: 0.00002382
Iteration 44/1000 | Loss: 0.00002382
Iteration 45/1000 | Loss: 0.00002382
Iteration 46/1000 | Loss: 0.00002382
Iteration 47/1000 | Loss: 0.00002382
Iteration 48/1000 | Loss: 0.00002382
Iteration 49/1000 | Loss: 0.00003023
Iteration 50/1000 | Loss: 0.00002378
Iteration 51/1000 | Loss: 0.00002378
Iteration 52/1000 | Loss: 0.00002378
Iteration 53/1000 | Loss: 0.00002378
Iteration 54/1000 | Loss: 0.00002378
Iteration 55/1000 | Loss: 0.00002378
Iteration 56/1000 | Loss: 0.00002378
Iteration 57/1000 | Loss: 0.00002378
Iteration 58/1000 | Loss: 0.00002377
Iteration 59/1000 | Loss: 0.00002377
Iteration 60/1000 | Loss: 0.00002377
Iteration 61/1000 | Loss: 0.00002377
Iteration 62/1000 | Loss: 0.00002374
Iteration 63/1000 | Loss: 0.00002373
Iteration 64/1000 | Loss: 0.00002373
Iteration 65/1000 | Loss: 0.00002368
Iteration 66/1000 | Loss: 0.00002367
Iteration 67/1000 | Loss: 0.00002366
Iteration 68/1000 | Loss: 0.00002366
Iteration 69/1000 | Loss: 0.00002361
Iteration 70/1000 | Loss: 0.00002361
Iteration 71/1000 | Loss: 0.00002360
Iteration 72/1000 | Loss: 0.00002360
Iteration 73/1000 | Loss: 0.00002360
Iteration 74/1000 | Loss: 0.00002360
Iteration 75/1000 | Loss: 0.00002359
Iteration 76/1000 | Loss: 0.00006989
Iteration 77/1000 | Loss: 0.00085727
Iteration 78/1000 | Loss: 0.00018008
Iteration 79/1000 | Loss: 0.00004056
Iteration 80/1000 | Loss: 0.00002478
Iteration 81/1000 | Loss: 0.00002991
Iteration 82/1000 | Loss: 0.00003724
Iteration 83/1000 | Loss: 0.00002357
Iteration 84/1000 | Loss: 0.00002354
Iteration 85/1000 | Loss: 0.00002354
Iteration 86/1000 | Loss: 0.00002354
Iteration 87/1000 | Loss: 0.00002353
Iteration 88/1000 | Loss: 0.00002353
Iteration 89/1000 | Loss: 0.00002353
Iteration 90/1000 | Loss: 0.00002353
Iteration 91/1000 | Loss: 0.00002353
Iteration 92/1000 | Loss: 0.00002352
Iteration 93/1000 | Loss: 0.00002352
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002350
Iteration 96/1000 | Loss: 0.00002350
Iteration 97/1000 | Loss: 0.00002350
Iteration 98/1000 | Loss: 0.00002350
Iteration 99/1000 | Loss: 0.00002350
Iteration 100/1000 | Loss: 0.00002350
Iteration 101/1000 | Loss: 0.00002349
Iteration 102/1000 | Loss: 0.00002349
Iteration 103/1000 | Loss: 0.00002349
Iteration 104/1000 | Loss: 0.00002349
Iteration 105/1000 | Loss: 0.00002349
Iteration 106/1000 | Loss: 0.00002349
Iteration 107/1000 | Loss: 0.00002349
Iteration 108/1000 | Loss: 0.00002349
Iteration 109/1000 | Loss: 0.00002349
Iteration 110/1000 | Loss: 0.00002348
Iteration 111/1000 | Loss: 0.00004081
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002346
Iteration 114/1000 | Loss: 0.00005688
Iteration 115/1000 | Loss: 0.00003225
Iteration 116/1000 | Loss: 0.00002658
Iteration 117/1000 | Loss: 0.00002684
Iteration 118/1000 | Loss: 0.00002355
Iteration 119/1000 | Loss: 0.00002353
Iteration 120/1000 | Loss: 0.00002353
Iteration 121/1000 | Loss: 0.00002350
Iteration 122/1000 | Loss: 0.00004590
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002349
Iteration 125/1000 | Loss: 0.00002349
Iteration 126/1000 | Loss: 0.00002348
Iteration 127/1000 | Loss: 0.00002348
Iteration 128/1000 | Loss: 0.00002347
Iteration 129/1000 | Loss: 0.00002347
Iteration 130/1000 | Loss: 0.00002346
Iteration 131/1000 | Loss: 0.00003426
Iteration 132/1000 | Loss: 0.00002922
Iteration 133/1000 | Loss: 0.00002351
Iteration 134/1000 | Loss: 0.00002350
Iteration 135/1000 | Loss: 0.00002348
Iteration 136/1000 | Loss: 0.00002348
Iteration 137/1000 | Loss: 0.00002348
Iteration 138/1000 | Loss: 0.00002347
Iteration 139/1000 | Loss: 0.00002346
Iteration 140/1000 | Loss: 0.00002346
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00002348
Iteration 143/1000 | Loss: 0.00002347
Iteration 144/1000 | Loss: 0.00002346
Iteration 145/1000 | Loss: 0.00002346
Iteration 146/1000 | Loss: 0.00002346
Iteration 147/1000 | Loss: 0.00002345
Iteration 148/1000 | Loss: 0.00002345
Iteration 149/1000 | Loss: 0.00002345
Iteration 150/1000 | Loss: 0.00002388
Iteration 151/1000 | Loss: 0.00002353
Iteration 152/1000 | Loss: 0.00002347
Iteration 153/1000 | Loss: 0.00002347
Iteration 154/1000 | Loss: 0.00002347
Iteration 155/1000 | Loss: 0.00002347
Iteration 156/1000 | Loss: 0.00002347
Iteration 157/1000 | Loss: 0.00002346
Iteration 158/1000 | Loss: 0.00002346
Iteration 159/1000 | Loss: 0.00002346
Iteration 160/1000 | Loss: 0.00002346
Iteration 161/1000 | Loss: 0.00002346
Iteration 162/1000 | Loss: 0.00002346
Iteration 163/1000 | Loss: 0.00002371
Iteration 164/1000 | Loss: 0.00002348
Iteration 165/1000 | Loss: 0.00002346
Iteration 166/1000 | Loss: 0.00002346
Iteration 167/1000 | Loss: 0.00002345
Iteration 168/1000 | Loss: 0.00002345
Iteration 169/1000 | Loss: 0.00002345
Iteration 170/1000 | Loss: 0.00002345
Iteration 171/1000 | Loss: 0.00002345
Iteration 172/1000 | Loss: 0.00002345
Iteration 173/1000 | Loss: 0.00002345
Iteration 174/1000 | Loss: 0.00002345
Iteration 175/1000 | Loss: 0.00002345
Iteration 176/1000 | Loss: 0.00002345
Iteration 177/1000 | Loss: 0.00002345
Iteration 178/1000 | Loss: 0.00002345
Iteration 179/1000 | Loss: 0.00002345
Iteration 180/1000 | Loss: 0.00002345
Iteration 181/1000 | Loss: 0.00002345
Iteration 182/1000 | Loss: 0.00002345
Iteration 183/1000 | Loss: 0.00002345
Iteration 184/1000 | Loss: 0.00002345
Iteration 185/1000 | Loss: 0.00002345
Iteration 186/1000 | Loss: 0.00002345
Iteration 187/1000 | Loss: 0.00002345
Iteration 188/1000 | Loss: 0.00002345
Iteration 189/1000 | Loss: 0.00002345
Iteration 190/1000 | Loss: 0.00002345
Iteration 191/1000 | Loss: 0.00002345
Iteration 192/1000 | Loss: 0.00002345
Iteration 193/1000 | Loss: 0.00002345
Iteration 194/1000 | Loss: 0.00002345
Iteration 195/1000 | Loss: 0.00002345
Iteration 196/1000 | Loss: 0.00002345
Iteration 197/1000 | Loss: 0.00002345
Iteration 198/1000 | Loss: 0.00002345
Iteration 199/1000 | Loss: 0.00002345
Iteration 200/1000 | Loss: 0.00002345
Iteration 201/1000 | Loss: 0.00002345
Iteration 202/1000 | Loss: 0.00002345
Iteration 203/1000 | Loss: 0.00002345
Iteration 204/1000 | Loss: 0.00002345
Iteration 205/1000 | Loss: 0.00002345
Iteration 206/1000 | Loss: 0.00002345
Iteration 207/1000 | Loss: 0.00002345
Iteration 208/1000 | Loss: 0.00002345
Iteration 209/1000 | Loss: 0.00002345
Iteration 210/1000 | Loss: 0.00002345
Iteration 211/1000 | Loss: 0.00002345
Iteration 212/1000 | Loss: 0.00002345
Iteration 213/1000 | Loss: 0.00002345
Iteration 214/1000 | Loss: 0.00002345
Iteration 215/1000 | Loss: 0.00002345
Iteration 216/1000 | Loss: 0.00002345
Iteration 217/1000 | Loss: 0.00002345
Iteration 218/1000 | Loss: 0.00002345
Iteration 219/1000 | Loss: 0.00002345
Iteration 220/1000 | Loss: 0.00002345
Iteration 221/1000 | Loss: 0.00002345
Iteration 222/1000 | Loss: 0.00002345
Iteration 223/1000 | Loss: 0.00002345
Iteration 224/1000 | Loss: 0.00002345
Iteration 225/1000 | Loss: 0.00002345
Iteration 226/1000 | Loss: 0.00002345
Iteration 227/1000 | Loss: 0.00002345
Iteration 228/1000 | Loss: 0.00002345
Iteration 229/1000 | Loss: 0.00002345
Iteration 230/1000 | Loss: 0.00002345
Iteration 231/1000 | Loss: 0.00002345
Iteration 232/1000 | Loss: 0.00002345
Iteration 233/1000 | Loss: 0.00002345
Iteration 234/1000 | Loss: 0.00002345
Iteration 235/1000 | Loss: 0.00002345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.3446244085789658e-05, 2.3446244085789658e-05, 2.3446244085789658e-05, 2.3446244085789658e-05, 2.3446244085789658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3446244085789658e-05

Optimization complete. Final v2v error: 4.006109237670898 mm

Highest mean error: 4.726656913757324 mm for frame 48

Lowest mean error: 3.674588680267334 mm for frame 130

Saving results

Total time: 109.68836259841919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762665
Iteration 2/25 | Loss: 0.00112486
Iteration 3/25 | Loss: 0.00101007
Iteration 4/25 | Loss: 0.00099577
Iteration 5/25 | Loss: 0.00099211
Iteration 6/25 | Loss: 0.00099141
Iteration 7/25 | Loss: 0.00099141
Iteration 8/25 | Loss: 0.00099141
Iteration 9/25 | Loss: 0.00099141
Iteration 10/25 | Loss: 0.00099141
Iteration 11/25 | Loss: 0.00099141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009914078982546926, 0.0009914078982546926, 0.0009914078982546926, 0.0009914078982546926, 0.0009914078982546926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009914078982546926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86903739
Iteration 2/25 | Loss: 0.00105134
Iteration 3/25 | Loss: 0.00105134
Iteration 4/25 | Loss: 0.00105133
Iteration 5/25 | Loss: 0.00105133
Iteration 6/25 | Loss: 0.00105133
Iteration 7/25 | Loss: 0.00105133
Iteration 8/25 | Loss: 0.00105133
Iteration 9/25 | Loss: 0.00105133
Iteration 10/25 | Loss: 0.00105133
Iteration 11/25 | Loss: 0.00105133
Iteration 12/25 | Loss: 0.00105133
Iteration 13/25 | Loss: 0.00105133
Iteration 14/25 | Loss: 0.00105133
Iteration 15/25 | Loss: 0.00105133
Iteration 16/25 | Loss: 0.00105133
Iteration 17/25 | Loss: 0.00105133
Iteration 18/25 | Loss: 0.00105133
Iteration 19/25 | Loss: 0.00105133
Iteration 20/25 | Loss: 0.00105133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010513321030884981, 0.0010513321030884981, 0.0010513321030884981, 0.0010513321030884981, 0.0010513321030884981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010513321030884981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105133
Iteration 2/1000 | Loss: 0.00001641
Iteration 3/1000 | Loss: 0.00001253
Iteration 4/1000 | Loss: 0.00001087
Iteration 5/1000 | Loss: 0.00001023
Iteration 6/1000 | Loss: 0.00000977
Iteration 7/1000 | Loss: 0.00000937
Iteration 8/1000 | Loss: 0.00000903
Iteration 9/1000 | Loss: 0.00000884
Iteration 10/1000 | Loss: 0.00000858
Iteration 11/1000 | Loss: 0.00000856
Iteration 12/1000 | Loss: 0.00000855
Iteration 13/1000 | Loss: 0.00000854
Iteration 14/1000 | Loss: 0.00000854
Iteration 15/1000 | Loss: 0.00000853
Iteration 16/1000 | Loss: 0.00000852
Iteration 17/1000 | Loss: 0.00000845
Iteration 18/1000 | Loss: 0.00000834
Iteration 19/1000 | Loss: 0.00000834
Iteration 20/1000 | Loss: 0.00000829
Iteration 21/1000 | Loss: 0.00000828
Iteration 22/1000 | Loss: 0.00000827
Iteration 23/1000 | Loss: 0.00000826
Iteration 24/1000 | Loss: 0.00000826
Iteration 25/1000 | Loss: 0.00000825
Iteration 26/1000 | Loss: 0.00000825
Iteration 27/1000 | Loss: 0.00000824
Iteration 28/1000 | Loss: 0.00000824
Iteration 29/1000 | Loss: 0.00000821
Iteration 30/1000 | Loss: 0.00000821
Iteration 31/1000 | Loss: 0.00000821
Iteration 32/1000 | Loss: 0.00000821
Iteration 33/1000 | Loss: 0.00000821
Iteration 34/1000 | Loss: 0.00000821
Iteration 35/1000 | Loss: 0.00000821
Iteration 36/1000 | Loss: 0.00000821
Iteration 37/1000 | Loss: 0.00000821
Iteration 38/1000 | Loss: 0.00000821
Iteration 39/1000 | Loss: 0.00000821
Iteration 40/1000 | Loss: 0.00000819
Iteration 41/1000 | Loss: 0.00000818
Iteration 42/1000 | Loss: 0.00000817
Iteration 43/1000 | Loss: 0.00000816
Iteration 44/1000 | Loss: 0.00000816
Iteration 45/1000 | Loss: 0.00000815
Iteration 46/1000 | Loss: 0.00000815
Iteration 47/1000 | Loss: 0.00000815
Iteration 48/1000 | Loss: 0.00000815
Iteration 49/1000 | Loss: 0.00000814
Iteration 50/1000 | Loss: 0.00000814
Iteration 51/1000 | Loss: 0.00000814
Iteration 52/1000 | Loss: 0.00000813
Iteration 53/1000 | Loss: 0.00000812
Iteration 54/1000 | Loss: 0.00000812
Iteration 55/1000 | Loss: 0.00000811
Iteration 56/1000 | Loss: 0.00000811
Iteration 57/1000 | Loss: 0.00000811
Iteration 58/1000 | Loss: 0.00000811
Iteration 59/1000 | Loss: 0.00000810
Iteration 60/1000 | Loss: 0.00000810
Iteration 61/1000 | Loss: 0.00000809
Iteration 62/1000 | Loss: 0.00000809
Iteration 63/1000 | Loss: 0.00000807
Iteration 64/1000 | Loss: 0.00000807
Iteration 65/1000 | Loss: 0.00000806
Iteration 66/1000 | Loss: 0.00000806
Iteration 67/1000 | Loss: 0.00000806
Iteration 68/1000 | Loss: 0.00000806
Iteration 69/1000 | Loss: 0.00000806
Iteration 70/1000 | Loss: 0.00000806
Iteration 71/1000 | Loss: 0.00000806
Iteration 72/1000 | Loss: 0.00000806
Iteration 73/1000 | Loss: 0.00000806
Iteration 74/1000 | Loss: 0.00000806
Iteration 75/1000 | Loss: 0.00000803
Iteration 76/1000 | Loss: 0.00000802
Iteration 77/1000 | Loss: 0.00000802
Iteration 78/1000 | Loss: 0.00000802
Iteration 79/1000 | Loss: 0.00000801
Iteration 80/1000 | Loss: 0.00000801
Iteration 81/1000 | Loss: 0.00000801
Iteration 82/1000 | Loss: 0.00000800
Iteration 83/1000 | Loss: 0.00000800
Iteration 84/1000 | Loss: 0.00000799
Iteration 85/1000 | Loss: 0.00000799
Iteration 86/1000 | Loss: 0.00000799
Iteration 87/1000 | Loss: 0.00000799
Iteration 88/1000 | Loss: 0.00000799
Iteration 89/1000 | Loss: 0.00000799
Iteration 90/1000 | Loss: 0.00000799
Iteration 91/1000 | Loss: 0.00000799
Iteration 92/1000 | Loss: 0.00000799
Iteration 93/1000 | Loss: 0.00000799
Iteration 94/1000 | Loss: 0.00000799
Iteration 95/1000 | Loss: 0.00000799
Iteration 96/1000 | Loss: 0.00000798
Iteration 97/1000 | Loss: 0.00000798
Iteration 98/1000 | Loss: 0.00000798
Iteration 99/1000 | Loss: 0.00000797
Iteration 100/1000 | Loss: 0.00000797
Iteration 101/1000 | Loss: 0.00000797
Iteration 102/1000 | Loss: 0.00000797
Iteration 103/1000 | Loss: 0.00000797
Iteration 104/1000 | Loss: 0.00000797
Iteration 105/1000 | Loss: 0.00000797
Iteration 106/1000 | Loss: 0.00000797
Iteration 107/1000 | Loss: 0.00000796
Iteration 108/1000 | Loss: 0.00000796
Iteration 109/1000 | Loss: 0.00000796
Iteration 110/1000 | Loss: 0.00000795
Iteration 111/1000 | Loss: 0.00000795
Iteration 112/1000 | Loss: 0.00000795
Iteration 113/1000 | Loss: 0.00000795
Iteration 114/1000 | Loss: 0.00000795
Iteration 115/1000 | Loss: 0.00000794
Iteration 116/1000 | Loss: 0.00000794
Iteration 117/1000 | Loss: 0.00000793
Iteration 118/1000 | Loss: 0.00000793
Iteration 119/1000 | Loss: 0.00000793
Iteration 120/1000 | Loss: 0.00000793
Iteration 121/1000 | Loss: 0.00000793
Iteration 122/1000 | Loss: 0.00000793
Iteration 123/1000 | Loss: 0.00000793
Iteration 124/1000 | Loss: 0.00000792
Iteration 125/1000 | Loss: 0.00000792
Iteration 126/1000 | Loss: 0.00000792
Iteration 127/1000 | Loss: 0.00000792
Iteration 128/1000 | Loss: 0.00000792
Iteration 129/1000 | Loss: 0.00000792
Iteration 130/1000 | Loss: 0.00000792
Iteration 131/1000 | Loss: 0.00000792
Iteration 132/1000 | Loss: 0.00000792
Iteration 133/1000 | Loss: 0.00000792
Iteration 134/1000 | Loss: 0.00000791
Iteration 135/1000 | Loss: 0.00000791
Iteration 136/1000 | Loss: 0.00000791
Iteration 137/1000 | Loss: 0.00000791
Iteration 138/1000 | Loss: 0.00000791
Iteration 139/1000 | Loss: 0.00000791
Iteration 140/1000 | Loss: 0.00000791
Iteration 141/1000 | Loss: 0.00000791
Iteration 142/1000 | Loss: 0.00000791
Iteration 143/1000 | Loss: 0.00000791
Iteration 144/1000 | Loss: 0.00000791
Iteration 145/1000 | Loss: 0.00000791
Iteration 146/1000 | Loss: 0.00000791
Iteration 147/1000 | Loss: 0.00000791
Iteration 148/1000 | Loss: 0.00000791
Iteration 149/1000 | Loss: 0.00000790
Iteration 150/1000 | Loss: 0.00000790
Iteration 151/1000 | Loss: 0.00000790
Iteration 152/1000 | Loss: 0.00000790
Iteration 153/1000 | Loss: 0.00000790
Iteration 154/1000 | Loss: 0.00000790
Iteration 155/1000 | Loss: 0.00000790
Iteration 156/1000 | Loss: 0.00000790
Iteration 157/1000 | Loss: 0.00000790
Iteration 158/1000 | Loss: 0.00000790
Iteration 159/1000 | Loss: 0.00000790
Iteration 160/1000 | Loss: 0.00000789
Iteration 161/1000 | Loss: 0.00000789
Iteration 162/1000 | Loss: 0.00000789
Iteration 163/1000 | Loss: 0.00000789
Iteration 164/1000 | Loss: 0.00000789
Iteration 165/1000 | Loss: 0.00000789
Iteration 166/1000 | Loss: 0.00000789
Iteration 167/1000 | Loss: 0.00000789
Iteration 168/1000 | Loss: 0.00000789
Iteration 169/1000 | Loss: 0.00000789
Iteration 170/1000 | Loss: 0.00000789
Iteration 171/1000 | Loss: 0.00000789
Iteration 172/1000 | Loss: 0.00000788
Iteration 173/1000 | Loss: 0.00000788
Iteration 174/1000 | Loss: 0.00000788
Iteration 175/1000 | Loss: 0.00000788
Iteration 176/1000 | Loss: 0.00000788
Iteration 177/1000 | Loss: 0.00000788
Iteration 178/1000 | Loss: 0.00000788
Iteration 179/1000 | Loss: 0.00000788
Iteration 180/1000 | Loss: 0.00000788
Iteration 181/1000 | Loss: 0.00000788
Iteration 182/1000 | Loss: 0.00000788
Iteration 183/1000 | Loss: 0.00000788
Iteration 184/1000 | Loss: 0.00000788
Iteration 185/1000 | Loss: 0.00000788
Iteration 186/1000 | Loss: 0.00000788
Iteration 187/1000 | Loss: 0.00000788
Iteration 188/1000 | Loss: 0.00000787
Iteration 189/1000 | Loss: 0.00000787
Iteration 190/1000 | Loss: 0.00000787
Iteration 191/1000 | Loss: 0.00000787
Iteration 192/1000 | Loss: 0.00000787
Iteration 193/1000 | Loss: 0.00000787
Iteration 194/1000 | Loss: 0.00000787
Iteration 195/1000 | Loss: 0.00000787
Iteration 196/1000 | Loss: 0.00000787
Iteration 197/1000 | Loss: 0.00000787
Iteration 198/1000 | Loss: 0.00000787
Iteration 199/1000 | Loss: 0.00000787
Iteration 200/1000 | Loss: 0.00000787
Iteration 201/1000 | Loss: 0.00000787
Iteration 202/1000 | Loss: 0.00000787
Iteration 203/1000 | Loss: 0.00000787
Iteration 204/1000 | Loss: 0.00000786
Iteration 205/1000 | Loss: 0.00000786
Iteration 206/1000 | Loss: 0.00000786
Iteration 207/1000 | Loss: 0.00000786
Iteration 208/1000 | Loss: 0.00000786
Iteration 209/1000 | Loss: 0.00000786
Iteration 210/1000 | Loss: 0.00000786
Iteration 211/1000 | Loss: 0.00000786
Iteration 212/1000 | Loss: 0.00000786
Iteration 213/1000 | Loss: 0.00000786
Iteration 214/1000 | Loss: 0.00000786
Iteration 215/1000 | Loss: 0.00000786
Iteration 216/1000 | Loss: 0.00000786
Iteration 217/1000 | Loss: 0.00000786
Iteration 218/1000 | Loss: 0.00000786
Iteration 219/1000 | Loss: 0.00000786
Iteration 220/1000 | Loss: 0.00000786
Iteration 221/1000 | Loss: 0.00000785
Iteration 222/1000 | Loss: 0.00000785
Iteration 223/1000 | Loss: 0.00000785
Iteration 224/1000 | Loss: 0.00000785
Iteration 225/1000 | Loss: 0.00000785
Iteration 226/1000 | Loss: 0.00000785
Iteration 227/1000 | Loss: 0.00000785
Iteration 228/1000 | Loss: 0.00000785
Iteration 229/1000 | Loss: 0.00000785
Iteration 230/1000 | Loss: 0.00000785
Iteration 231/1000 | Loss: 0.00000785
Iteration 232/1000 | Loss: 0.00000785
Iteration 233/1000 | Loss: 0.00000785
Iteration 234/1000 | Loss: 0.00000785
Iteration 235/1000 | Loss: 0.00000785
Iteration 236/1000 | Loss: 0.00000785
Iteration 237/1000 | Loss: 0.00000785
Iteration 238/1000 | Loss: 0.00000785
Iteration 239/1000 | Loss: 0.00000785
Iteration 240/1000 | Loss: 0.00000785
Iteration 241/1000 | Loss: 0.00000785
Iteration 242/1000 | Loss: 0.00000784
Iteration 243/1000 | Loss: 0.00000784
Iteration 244/1000 | Loss: 0.00000784
Iteration 245/1000 | Loss: 0.00000784
Iteration 246/1000 | Loss: 0.00000784
Iteration 247/1000 | Loss: 0.00000784
Iteration 248/1000 | Loss: 0.00000784
Iteration 249/1000 | Loss: 0.00000784
Iteration 250/1000 | Loss: 0.00000784
Iteration 251/1000 | Loss: 0.00000784
Iteration 252/1000 | Loss: 0.00000784
Iteration 253/1000 | Loss: 0.00000784
Iteration 254/1000 | Loss: 0.00000784
Iteration 255/1000 | Loss: 0.00000784
Iteration 256/1000 | Loss: 0.00000784
Iteration 257/1000 | Loss: 0.00000783
Iteration 258/1000 | Loss: 0.00000783
Iteration 259/1000 | Loss: 0.00000783
Iteration 260/1000 | Loss: 0.00000783
Iteration 261/1000 | Loss: 0.00000783
Iteration 262/1000 | Loss: 0.00000783
Iteration 263/1000 | Loss: 0.00000783
Iteration 264/1000 | Loss: 0.00000783
Iteration 265/1000 | Loss: 0.00000783
Iteration 266/1000 | Loss: 0.00000783
Iteration 267/1000 | Loss: 0.00000783
Iteration 268/1000 | Loss: 0.00000783
Iteration 269/1000 | Loss: 0.00000783
Iteration 270/1000 | Loss: 0.00000782
Iteration 271/1000 | Loss: 0.00000782
Iteration 272/1000 | Loss: 0.00000782
Iteration 273/1000 | Loss: 0.00000782
Iteration 274/1000 | Loss: 0.00000782
Iteration 275/1000 | Loss: 0.00000782
Iteration 276/1000 | Loss: 0.00000782
Iteration 277/1000 | Loss: 0.00000782
Iteration 278/1000 | Loss: 0.00000782
Iteration 279/1000 | Loss: 0.00000782
Iteration 280/1000 | Loss: 0.00000782
Iteration 281/1000 | Loss: 0.00000782
Iteration 282/1000 | Loss: 0.00000782
Iteration 283/1000 | Loss: 0.00000782
Iteration 284/1000 | Loss: 0.00000782
Iteration 285/1000 | Loss: 0.00000781
Iteration 286/1000 | Loss: 0.00000781
Iteration 287/1000 | Loss: 0.00000781
Iteration 288/1000 | Loss: 0.00000781
Iteration 289/1000 | Loss: 0.00000781
Iteration 290/1000 | Loss: 0.00000781
Iteration 291/1000 | Loss: 0.00000780
Iteration 292/1000 | Loss: 0.00000780
Iteration 293/1000 | Loss: 0.00000780
Iteration 294/1000 | Loss: 0.00000780
Iteration 295/1000 | Loss: 0.00000780
Iteration 296/1000 | Loss: 0.00000780
Iteration 297/1000 | Loss: 0.00000780
Iteration 298/1000 | Loss: 0.00000780
Iteration 299/1000 | Loss: 0.00000780
Iteration 300/1000 | Loss: 0.00000780
Iteration 301/1000 | Loss: 0.00000780
Iteration 302/1000 | Loss: 0.00000780
Iteration 303/1000 | Loss: 0.00000780
Iteration 304/1000 | Loss: 0.00000780
Iteration 305/1000 | Loss: 0.00000780
Iteration 306/1000 | Loss: 0.00000780
Iteration 307/1000 | Loss: 0.00000780
Iteration 308/1000 | Loss: 0.00000780
Iteration 309/1000 | Loss: 0.00000780
Iteration 310/1000 | Loss: 0.00000780
Iteration 311/1000 | Loss: 0.00000779
Iteration 312/1000 | Loss: 0.00000779
Iteration 313/1000 | Loss: 0.00000779
Iteration 314/1000 | Loss: 0.00000779
Iteration 315/1000 | Loss: 0.00000779
Iteration 316/1000 | Loss: 0.00000779
Iteration 317/1000 | Loss: 0.00000779
Iteration 318/1000 | Loss: 0.00000779
Iteration 319/1000 | Loss: 0.00000779
Iteration 320/1000 | Loss: 0.00000779
Iteration 321/1000 | Loss: 0.00000779
Iteration 322/1000 | Loss: 0.00000779
Iteration 323/1000 | Loss: 0.00000779
Iteration 324/1000 | Loss: 0.00000779
Iteration 325/1000 | Loss: 0.00000779
Iteration 326/1000 | Loss: 0.00000779
Iteration 327/1000 | Loss: 0.00000779
Iteration 328/1000 | Loss: 0.00000779
Iteration 329/1000 | Loss: 0.00000779
Iteration 330/1000 | Loss: 0.00000779
Iteration 331/1000 | Loss: 0.00000779
Iteration 332/1000 | Loss: 0.00000779
Iteration 333/1000 | Loss: 0.00000779
Iteration 334/1000 | Loss: 0.00000779
Iteration 335/1000 | Loss: 0.00000779
Iteration 336/1000 | Loss: 0.00000779
Iteration 337/1000 | Loss: 0.00000779
Iteration 338/1000 | Loss: 0.00000779
Iteration 339/1000 | Loss: 0.00000779
Iteration 340/1000 | Loss: 0.00000779
Iteration 341/1000 | Loss: 0.00000779
Iteration 342/1000 | Loss: 0.00000779
Iteration 343/1000 | Loss: 0.00000779
Iteration 344/1000 | Loss: 0.00000779
Iteration 345/1000 | Loss: 0.00000779
Iteration 346/1000 | Loss: 0.00000779
Iteration 347/1000 | Loss: 0.00000779
Iteration 348/1000 | Loss: 0.00000779
Iteration 349/1000 | Loss: 0.00000779
Iteration 350/1000 | Loss: 0.00000779
Iteration 351/1000 | Loss: 0.00000779
Iteration 352/1000 | Loss: 0.00000779
Iteration 353/1000 | Loss: 0.00000779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [7.789834853610955e-06, 7.789834853610955e-06, 7.789834853610955e-06, 7.789834853610955e-06, 7.789834853610955e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.789834853610955e-06

Optimization complete. Final v2v error: 2.4306600093841553 mm

Highest mean error: 2.5929322242736816 mm for frame 84

Lowest mean error: 2.287559986114502 mm for frame 0

Saving results

Total time: 47.62793445587158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984629
Iteration 2/25 | Loss: 0.00147913
Iteration 3/25 | Loss: 0.00124258
Iteration 4/25 | Loss: 0.00129490
Iteration 5/25 | Loss: 0.00120665
Iteration 6/25 | Loss: 0.00119221
Iteration 7/25 | Loss: 0.00117511
Iteration 8/25 | Loss: 0.00116782
Iteration 9/25 | Loss: 0.00115807
Iteration 10/25 | Loss: 0.00114499
Iteration 11/25 | Loss: 0.00114208
Iteration 12/25 | Loss: 0.00114104
Iteration 13/25 | Loss: 0.00114061
Iteration 14/25 | Loss: 0.00114386
Iteration 15/25 | Loss: 0.00114363
Iteration 16/25 | Loss: 0.00114038
Iteration 17/25 | Loss: 0.00113897
Iteration 18/25 | Loss: 0.00113832
Iteration 19/25 | Loss: 0.00114094
Iteration 20/25 | Loss: 0.00113893
Iteration 21/25 | Loss: 0.00113748
Iteration 22/25 | Loss: 0.00113634
Iteration 23/25 | Loss: 0.00113582
Iteration 24/25 | Loss: 0.00113602
Iteration 25/25 | Loss: 0.00113465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23972023
Iteration 2/25 | Loss: 0.00137546
Iteration 3/25 | Loss: 0.00137543
Iteration 4/25 | Loss: 0.00137543
Iteration 5/25 | Loss: 0.00137543
Iteration 6/25 | Loss: 0.00137543
Iteration 7/25 | Loss: 0.00137543
Iteration 8/25 | Loss: 0.00137543
Iteration 9/25 | Loss: 0.00137542
Iteration 10/25 | Loss: 0.00137542
Iteration 11/25 | Loss: 0.00137542
Iteration 12/25 | Loss: 0.00137542
Iteration 13/25 | Loss: 0.00137542
Iteration 14/25 | Loss: 0.00137542
Iteration 15/25 | Loss: 0.00137542
Iteration 16/25 | Loss: 0.00137542
Iteration 17/25 | Loss: 0.00137542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013754242099821568, 0.0013754242099821568, 0.0013754242099821568, 0.0013754242099821568, 0.0013754242099821568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013754242099821568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137542
Iteration 2/1000 | Loss: 0.00005496
Iteration 3/1000 | Loss: 0.00003096
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002322
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002120
Iteration 8/1000 | Loss: 0.00002070
Iteration 9/1000 | Loss: 0.00002028
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001949
Iteration 13/1000 | Loss: 0.00001933
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001916
Iteration 16/1000 | Loss: 0.00001913
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001906
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001904
Iteration 29/1000 | Loss: 0.00001903
Iteration 30/1000 | Loss: 0.00001899
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001897
Iteration 33/1000 | Loss: 0.00001897
Iteration 34/1000 | Loss: 0.00001897
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001892
Iteration 39/1000 | Loss: 0.00001891
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001886
Iteration 44/1000 | Loss: 0.00001886
Iteration 45/1000 | Loss: 0.00001886
Iteration 46/1000 | Loss: 0.00001886
Iteration 47/1000 | Loss: 0.00001885
Iteration 48/1000 | Loss: 0.00001884
Iteration 49/1000 | Loss: 0.00001884
Iteration 50/1000 | Loss: 0.00001884
Iteration 51/1000 | Loss: 0.00001883
Iteration 52/1000 | Loss: 0.00001883
Iteration 53/1000 | Loss: 0.00001882
Iteration 54/1000 | Loss: 0.00001882
Iteration 55/1000 | Loss: 0.00001882
Iteration 56/1000 | Loss: 0.00001882
Iteration 57/1000 | Loss: 0.00001881
Iteration 58/1000 | Loss: 0.00001881
Iteration 59/1000 | Loss: 0.00001881
Iteration 60/1000 | Loss: 0.00001880
Iteration 61/1000 | Loss: 0.00001880
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001878
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001876
Iteration 83/1000 | Loss: 0.00001876
Iteration 84/1000 | Loss: 0.00001876
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001875
Iteration 88/1000 | Loss: 0.00001875
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001870
Iteration 108/1000 | Loss: 0.00001870
Iteration 109/1000 | Loss: 0.00001870
Iteration 110/1000 | Loss: 0.00001870
Iteration 111/1000 | Loss: 0.00001870
Iteration 112/1000 | Loss: 0.00001870
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.8701852241065353e-05, 1.8701852241065353e-05, 1.8701852241065353e-05, 1.8701852241065353e-05, 1.8701852241065353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8701852241065353e-05

Optimization complete. Final v2v error: 3.4916019439697266 mm

Highest mean error: 4.520750999450684 mm for frame 78

Lowest mean error: 2.5436160564422607 mm for frame 168

Saving results

Total time: 82.70845818519592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860548
Iteration 2/25 | Loss: 0.00242876
Iteration 3/25 | Loss: 0.00164032
Iteration 4/25 | Loss: 0.00144989
Iteration 5/25 | Loss: 0.00139467
Iteration 6/25 | Loss: 0.00129467
Iteration 7/25 | Loss: 0.00124159
Iteration 8/25 | Loss: 0.00120566
Iteration 9/25 | Loss: 0.00119101
Iteration 10/25 | Loss: 0.00117958
Iteration 11/25 | Loss: 0.00117685
Iteration 12/25 | Loss: 0.00116869
Iteration 13/25 | Loss: 0.00116698
Iteration 14/25 | Loss: 0.00116527
Iteration 15/25 | Loss: 0.00116402
Iteration 16/25 | Loss: 0.00116373
Iteration 17/25 | Loss: 0.00116414
Iteration 18/25 | Loss: 0.00116229
Iteration 19/25 | Loss: 0.00116204
Iteration 20/25 | Loss: 0.00116201
Iteration 21/25 | Loss: 0.00116194
Iteration 22/25 | Loss: 0.00116193
Iteration 23/25 | Loss: 0.00116192
Iteration 24/25 | Loss: 0.00116192
Iteration 25/25 | Loss: 0.00116192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92611247
Iteration 2/25 | Loss: 0.00077965
Iteration 3/25 | Loss: 0.00077965
Iteration 4/25 | Loss: 0.00077965
Iteration 5/25 | Loss: 0.00077965
Iteration 6/25 | Loss: 0.00077965
Iteration 7/25 | Loss: 0.00077965
Iteration 8/25 | Loss: 0.00077965
Iteration 9/25 | Loss: 0.00077965
Iteration 10/25 | Loss: 0.00077965
Iteration 11/25 | Loss: 0.00077965
Iteration 12/25 | Loss: 0.00077965
Iteration 13/25 | Loss: 0.00077965
Iteration 14/25 | Loss: 0.00077965
Iteration 15/25 | Loss: 0.00077965
Iteration 16/25 | Loss: 0.00077965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007796454010531306, 0.0007796454010531306, 0.0007796454010531306, 0.0007796454010531306, 0.0007796454010531306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007796454010531306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077965
Iteration 2/1000 | Loss: 0.00005213
Iteration 3/1000 | Loss: 0.00002875
Iteration 4/1000 | Loss: 0.00002463
Iteration 5/1000 | Loss: 0.00002350
Iteration 6/1000 | Loss: 0.00002262
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002180
Iteration 9/1000 | Loss: 0.00002149
Iteration 10/1000 | Loss: 0.00002122
Iteration 11/1000 | Loss: 0.00002105
Iteration 12/1000 | Loss: 0.00002091
Iteration 13/1000 | Loss: 0.00002079
Iteration 14/1000 | Loss: 0.00002077
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002046
Iteration 18/1000 | Loss: 0.00010252
Iteration 19/1000 | Loss: 0.00002161
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00001972
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001944
Iteration 27/1000 | Loss: 0.00001937
Iteration 28/1000 | Loss: 0.00001936
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001934
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001933
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001932
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001931
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001931
Iteration 43/1000 | Loss: 0.00001931
Iteration 44/1000 | Loss: 0.00001931
Iteration 45/1000 | Loss: 0.00001931
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001930
Iteration 52/1000 | Loss: 0.00001930
Iteration 53/1000 | Loss: 0.00001930
Iteration 54/1000 | Loss: 0.00001930
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001929
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001928
Iteration 65/1000 | Loss: 0.00001928
Iteration 66/1000 | Loss: 0.00001928
Iteration 67/1000 | Loss: 0.00001928
Iteration 68/1000 | Loss: 0.00001927
Iteration 69/1000 | Loss: 0.00001927
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001927
Iteration 72/1000 | Loss: 0.00001927
Iteration 73/1000 | Loss: 0.00001926
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001926
Iteration 76/1000 | Loss: 0.00001926
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001925
Iteration 80/1000 | Loss: 0.00001925
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001924
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001924
Iteration 87/1000 | Loss: 0.00001924
Iteration 88/1000 | Loss: 0.00001924
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001924
Iteration 91/1000 | Loss: 0.00001924
Iteration 92/1000 | Loss: 0.00001924
Iteration 93/1000 | Loss: 0.00001924
Iteration 94/1000 | Loss: 0.00001924
Iteration 95/1000 | Loss: 0.00001924
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001923
Iteration 98/1000 | Loss: 0.00001923
Iteration 99/1000 | Loss: 0.00001923
Iteration 100/1000 | Loss: 0.00001923
Iteration 101/1000 | Loss: 0.00001923
Iteration 102/1000 | Loss: 0.00001923
Iteration 103/1000 | Loss: 0.00001923
Iteration 104/1000 | Loss: 0.00001923
Iteration 105/1000 | Loss: 0.00001923
Iteration 106/1000 | Loss: 0.00001923
Iteration 107/1000 | Loss: 0.00001923
Iteration 108/1000 | Loss: 0.00001923
Iteration 109/1000 | Loss: 0.00001923
Iteration 110/1000 | Loss: 0.00001923
Iteration 111/1000 | Loss: 0.00001923
Iteration 112/1000 | Loss: 0.00001923
Iteration 113/1000 | Loss: 0.00001923
Iteration 114/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.9234368664911017e-05, 1.9234368664911017e-05, 1.9234368664911017e-05, 1.9234368664911017e-05, 1.9234368664911017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9234368664911017e-05

Optimization complete. Final v2v error: 3.6244990825653076 mm

Highest mean error: 4.670394420623779 mm for frame 141

Lowest mean error: 3.072868824005127 mm for frame 63

Saving results

Total time: 82.8530650138855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986408
Iteration 2/25 | Loss: 0.00169218
Iteration 3/25 | Loss: 0.00118871
Iteration 4/25 | Loss: 0.00111416
Iteration 5/25 | Loss: 0.00110275
Iteration 6/25 | Loss: 0.00110227
Iteration 7/25 | Loss: 0.00110227
Iteration 8/25 | Loss: 0.00110227
Iteration 9/25 | Loss: 0.00110227
Iteration 10/25 | Loss: 0.00110227
Iteration 11/25 | Loss: 0.00110227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011022717226296663, 0.0011022717226296663, 0.0011022717226296663, 0.0011022717226296663, 0.0011022717226296663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011022717226296663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29772055
Iteration 2/25 | Loss: 0.00081866
Iteration 3/25 | Loss: 0.00081866
Iteration 4/25 | Loss: 0.00081866
Iteration 5/25 | Loss: 0.00081866
Iteration 6/25 | Loss: 0.00081866
Iteration 7/25 | Loss: 0.00081866
Iteration 8/25 | Loss: 0.00081866
Iteration 9/25 | Loss: 0.00081866
Iteration 10/25 | Loss: 0.00081866
Iteration 11/25 | Loss: 0.00081866
Iteration 12/25 | Loss: 0.00081866
Iteration 13/25 | Loss: 0.00081866
Iteration 14/25 | Loss: 0.00081866
Iteration 15/25 | Loss: 0.00081866
Iteration 16/25 | Loss: 0.00081866
Iteration 17/25 | Loss: 0.00081866
Iteration 18/25 | Loss: 0.00081866
Iteration 19/25 | Loss: 0.00081866
Iteration 20/25 | Loss: 0.00081866
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008186587947420776, 0.0008186587947420776, 0.0008186587947420776, 0.0008186587947420776, 0.0008186587947420776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008186587947420776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081866
Iteration 2/1000 | Loss: 0.00002662
Iteration 3/1000 | Loss: 0.00002031
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00001708
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001702
Iteration 15/1000 | Loss: 0.00001698
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001688
Iteration 19/1000 | Loss: 0.00001685
Iteration 20/1000 | Loss: 0.00001685
Iteration 21/1000 | Loss: 0.00001685
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001684
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001677
Iteration 27/1000 | Loss: 0.00001677
Iteration 28/1000 | Loss: 0.00001677
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001676
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001674
Iteration 36/1000 | Loss: 0.00001673
Iteration 37/1000 | Loss: 0.00001673
Iteration 38/1000 | Loss: 0.00001673
Iteration 39/1000 | Loss: 0.00001672
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001668
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001667
Iteration 53/1000 | Loss: 0.00001665
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001665
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001663
Iteration 70/1000 | Loss: 0.00001663
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001662
Iteration 73/1000 | Loss: 0.00001662
Iteration 74/1000 | Loss: 0.00001662
Iteration 75/1000 | Loss: 0.00001662
Iteration 76/1000 | Loss: 0.00001662
Iteration 77/1000 | Loss: 0.00001662
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001661
Iteration 80/1000 | Loss: 0.00001661
Iteration 81/1000 | Loss: 0.00001661
Iteration 82/1000 | Loss: 0.00001661
Iteration 83/1000 | Loss: 0.00001661
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001659
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001658
Iteration 105/1000 | Loss: 0.00001658
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001658
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6581394447712228e-05, 1.6581394447712228e-05, 1.6581394447712228e-05, 1.6581394447712228e-05, 1.6581394447712228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6581394447712228e-05

Optimization complete. Final v2v error: 3.399611711502075 mm

Highest mean error: 3.4434256553649902 mm for frame 171

Lowest mean error: 3.1323957443237305 mm for frame 0

Saving results

Total time: 33.60963797569275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779481
Iteration 2/25 | Loss: 0.00126225
Iteration 3/25 | Loss: 0.00107062
Iteration 4/25 | Loss: 0.00105086
Iteration 5/25 | Loss: 0.00104864
Iteration 6/25 | Loss: 0.00104864
Iteration 7/25 | Loss: 0.00104864
Iteration 8/25 | Loss: 0.00104864
Iteration 9/25 | Loss: 0.00104864
Iteration 10/25 | Loss: 0.00104864
Iteration 11/25 | Loss: 0.00104864
Iteration 12/25 | Loss: 0.00104864
Iteration 13/25 | Loss: 0.00104864
Iteration 14/25 | Loss: 0.00104864
Iteration 15/25 | Loss: 0.00104864
Iteration 16/25 | Loss: 0.00104864
Iteration 17/25 | Loss: 0.00104864
Iteration 18/25 | Loss: 0.00104864
Iteration 19/25 | Loss: 0.00104864
Iteration 20/25 | Loss: 0.00104864
Iteration 21/25 | Loss: 0.00104864
Iteration 22/25 | Loss: 0.00104864
Iteration 23/25 | Loss: 0.00104864
Iteration 24/25 | Loss: 0.00104864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010486365063115954, 0.0010486365063115954, 0.0010486365063115954, 0.0010486365063115954, 0.0010486365063115954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010486365063115954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28148758
Iteration 2/25 | Loss: 0.00080812
Iteration 3/25 | Loss: 0.00080810
Iteration 4/25 | Loss: 0.00080810
Iteration 5/25 | Loss: 0.00080810
Iteration 6/25 | Loss: 0.00080810
Iteration 7/25 | Loss: 0.00080810
Iteration 8/25 | Loss: 0.00080810
Iteration 9/25 | Loss: 0.00080810
Iteration 10/25 | Loss: 0.00080810
Iteration 11/25 | Loss: 0.00080810
Iteration 12/25 | Loss: 0.00080810
Iteration 13/25 | Loss: 0.00080810
Iteration 14/25 | Loss: 0.00080810
Iteration 15/25 | Loss: 0.00080810
Iteration 16/25 | Loss: 0.00080810
Iteration 17/25 | Loss: 0.00080810
Iteration 18/25 | Loss: 0.00080810
Iteration 19/25 | Loss: 0.00080810
Iteration 20/25 | Loss: 0.00080810
Iteration 21/25 | Loss: 0.00080810
Iteration 22/25 | Loss: 0.00080810
Iteration 23/25 | Loss: 0.00080810
Iteration 24/25 | Loss: 0.00080810
Iteration 25/25 | Loss: 0.00080810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080810
Iteration 2/1000 | Loss: 0.00002397
Iteration 3/1000 | Loss: 0.00001688
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001371
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001286
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001165
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001152
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001151
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001149
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001148
Iteration 66/1000 | Loss: 0.00001148
Iteration 67/1000 | Loss: 0.00001148
Iteration 68/1000 | Loss: 0.00001147
Iteration 69/1000 | Loss: 0.00001147
Iteration 70/1000 | Loss: 0.00001147
Iteration 71/1000 | Loss: 0.00001147
Iteration 72/1000 | Loss: 0.00001146
Iteration 73/1000 | Loss: 0.00001146
Iteration 74/1000 | Loss: 0.00001146
Iteration 75/1000 | Loss: 0.00001146
Iteration 76/1000 | Loss: 0.00001146
Iteration 77/1000 | Loss: 0.00001146
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001141
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001139
Iteration 94/1000 | Loss: 0.00001138
Iteration 95/1000 | Loss: 0.00001138
Iteration 96/1000 | Loss: 0.00001138
Iteration 97/1000 | Loss: 0.00001138
Iteration 98/1000 | Loss: 0.00001138
Iteration 99/1000 | Loss: 0.00001137
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001137
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001136
Iteration 105/1000 | Loss: 0.00001136
Iteration 106/1000 | Loss: 0.00001136
Iteration 107/1000 | Loss: 0.00001136
Iteration 108/1000 | Loss: 0.00001136
Iteration 109/1000 | Loss: 0.00001136
Iteration 110/1000 | Loss: 0.00001136
Iteration 111/1000 | Loss: 0.00001136
Iteration 112/1000 | Loss: 0.00001136
Iteration 113/1000 | Loss: 0.00001136
Iteration 114/1000 | Loss: 0.00001136
Iteration 115/1000 | Loss: 0.00001136
Iteration 116/1000 | Loss: 0.00001136
Iteration 117/1000 | Loss: 0.00001136
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001136
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.1360023563611321e-05, 1.1360023563611321e-05, 1.1360023563611321e-05, 1.1360023563611321e-05, 1.1360023563611321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1360023563611321e-05

Optimization complete. Final v2v error: 2.8351736068725586 mm

Highest mean error: 3.357956886291504 mm for frame 234

Lowest mean error: 2.217216968536377 mm for frame 79

Saving results

Total time: 43.32900905609131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034550
Iteration 2/25 | Loss: 0.00150726
Iteration 3/25 | Loss: 0.00130395
Iteration 4/25 | Loss: 0.00130688
Iteration 5/25 | Loss: 0.00131471
Iteration 6/25 | Loss: 0.00114426
Iteration 7/25 | Loss: 0.00115460
Iteration 8/25 | Loss: 0.00113714
Iteration 9/25 | Loss: 0.00111540
Iteration 10/25 | Loss: 0.00115572
Iteration 11/25 | Loss: 0.00113734
Iteration 12/25 | Loss: 0.00113516
Iteration 13/25 | Loss: 0.00109449
Iteration 14/25 | Loss: 0.00112508
Iteration 15/25 | Loss: 0.00108120
Iteration 16/25 | Loss: 0.00107686
Iteration 17/25 | Loss: 0.00107626
Iteration 18/25 | Loss: 0.00108140
Iteration 19/25 | Loss: 0.00108392
Iteration 20/25 | Loss: 0.00108031
Iteration 21/25 | Loss: 0.00107141
Iteration 22/25 | Loss: 0.00106977
Iteration 23/25 | Loss: 0.00106944
Iteration 24/25 | Loss: 0.00106940
Iteration 25/25 | Loss: 0.00106940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48816240
Iteration 2/25 | Loss: 0.00116947
Iteration 3/25 | Loss: 0.00116947
Iteration 4/25 | Loss: 0.00116947
Iteration 5/25 | Loss: 0.00116947
Iteration 6/25 | Loss: 0.00116947
Iteration 7/25 | Loss: 0.00116947
Iteration 8/25 | Loss: 0.00116946
Iteration 9/25 | Loss: 0.00116946
Iteration 10/25 | Loss: 0.00116946
Iteration 11/25 | Loss: 0.00116946
Iteration 12/25 | Loss: 0.00116946
Iteration 13/25 | Loss: 0.00116946
Iteration 14/25 | Loss: 0.00116946
Iteration 15/25 | Loss: 0.00116946
Iteration 16/25 | Loss: 0.00116946
Iteration 17/25 | Loss: 0.00116946
Iteration 18/25 | Loss: 0.00116946
Iteration 19/25 | Loss: 0.00116946
Iteration 20/25 | Loss: 0.00116946
Iteration 21/25 | Loss: 0.00116946
Iteration 22/25 | Loss: 0.00116946
Iteration 23/25 | Loss: 0.00116946
Iteration 24/25 | Loss: 0.00116946
Iteration 25/25 | Loss: 0.00116946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116946
Iteration 2/1000 | Loss: 0.00003698
Iteration 3/1000 | Loss: 0.00031110
Iteration 4/1000 | Loss: 0.00005037
Iteration 5/1000 | Loss: 0.00003463
Iteration 6/1000 | Loss: 0.00002611
Iteration 7/1000 | Loss: 0.00002495
Iteration 8/1000 | Loss: 0.00003118
Iteration 9/1000 | Loss: 0.00002659
Iteration 10/1000 | Loss: 0.00002370
Iteration 11/1000 | Loss: 0.00002581
Iteration 12/1000 | Loss: 0.00002340
Iteration 13/1000 | Loss: 0.00002305
Iteration 14/1000 | Loss: 0.00003599
Iteration 15/1000 | Loss: 0.00002294
Iteration 16/1000 | Loss: 0.00002264
Iteration 17/1000 | Loss: 0.00002246
Iteration 18/1000 | Loss: 0.00002240
Iteration 19/1000 | Loss: 0.00002232
Iteration 20/1000 | Loss: 0.00002231
Iteration 21/1000 | Loss: 0.00002231
Iteration 22/1000 | Loss: 0.00002229
Iteration 23/1000 | Loss: 0.00002224
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00002223
Iteration 26/1000 | Loss: 0.00002220
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002214
Iteration 29/1000 | Loss: 0.00002214
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002195
Iteration 34/1000 | Loss: 0.00002194
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002194
Iteration 37/1000 | Loss: 0.00002192
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00003541
Iteration 43/1000 | Loss: 0.00002185
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002179
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002179
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002179
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002177
Iteration 66/1000 | Loss: 0.00002177
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002176
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002176
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002173
Iteration 80/1000 | Loss: 0.00002173
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002172
Iteration 83/1000 | Loss: 0.00002172
Iteration 84/1000 | Loss: 0.00002172
Iteration 85/1000 | Loss: 0.00003499
Iteration 86/1000 | Loss: 0.00002170
Iteration 87/1000 | Loss: 0.00002170
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002168
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002168
Iteration 93/1000 | Loss: 0.00002168
Iteration 94/1000 | Loss: 0.00002168
Iteration 95/1000 | Loss: 0.00002168
Iteration 96/1000 | Loss: 0.00002168
Iteration 97/1000 | Loss: 0.00002168
Iteration 98/1000 | Loss: 0.00002168
Iteration 99/1000 | Loss: 0.00002168
Iteration 100/1000 | Loss: 0.00002168
Iteration 101/1000 | Loss: 0.00002168
Iteration 102/1000 | Loss: 0.00002168
Iteration 103/1000 | Loss: 0.00002167
Iteration 104/1000 | Loss: 0.00002167
Iteration 105/1000 | Loss: 0.00002167
Iteration 106/1000 | Loss: 0.00002167
Iteration 107/1000 | Loss: 0.00002167
Iteration 108/1000 | Loss: 0.00002167
Iteration 109/1000 | Loss: 0.00002167
Iteration 110/1000 | Loss: 0.00002167
Iteration 111/1000 | Loss: 0.00002167
Iteration 112/1000 | Loss: 0.00002166
Iteration 113/1000 | Loss: 0.00002943
Iteration 114/1000 | Loss: 0.00002166
Iteration 115/1000 | Loss: 0.00002165
Iteration 116/1000 | Loss: 0.00002165
Iteration 117/1000 | Loss: 0.00002165
Iteration 118/1000 | Loss: 0.00002165
Iteration 119/1000 | Loss: 0.00002164
Iteration 120/1000 | Loss: 0.00002164
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002163
Iteration 123/1000 | Loss: 0.00002162
Iteration 124/1000 | Loss: 0.00002162
Iteration 125/1000 | Loss: 0.00002162
Iteration 126/1000 | Loss: 0.00002162
Iteration 127/1000 | Loss: 0.00002162
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002161
Iteration 132/1000 | Loss: 0.00002161
Iteration 133/1000 | Loss: 0.00002161
Iteration 134/1000 | Loss: 0.00002161
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002161
Iteration 137/1000 | Loss: 0.00002161
Iteration 138/1000 | Loss: 0.00002161
Iteration 139/1000 | Loss: 0.00002161
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002161
Iteration 142/1000 | Loss: 0.00002161
Iteration 143/1000 | Loss: 0.00002161
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00002160
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00003050
Iteration 153/1000 | Loss: 0.00002160
Iteration 154/1000 | Loss: 0.00002160
Iteration 155/1000 | Loss: 0.00002160
Iteration 156/1000 | Loss: 0.00002160
Iteration 157/1000 | Loss: 0.00002160
Iteration 158/1000 | Loss: 0.00002160
Iteration 159/1000 | Loss: 0.00002160
Iteration 160/1000 | Loss: 0.00002160
Iteration 161/1000 | Loss: 0.00002160
Iteration 162/1000 | Loss: 0.00002160
Iteration 163/1000 | Loss: 0.00002160
Iteration 164/1000 | Loss: 0.00002160
Iteration 165/1000 | Loss: 0.00002159
Iteration 166/1000 | Loss: 0.00002159
Iteration 167/1000 | Loss: 0.00002159
Iteration 168/1000 | Loss: 0.00002159
Iteration 169/1000 | Loss: 0.00002159
Iteration 170/1000 | Loss: 0.00002159
Iteration 171/1000 | Loss: 0.00002159
Iteration 172/1000 | Loss: 0.00002159
Iteration 173/1000 | Loss: 0.00002159
Iteration 174/1000 | Loss: 0.00002159
Iteration 175/1000 | Loss: 0.00002159
Iteration 176/1000 | Loss: 0.00002158
Iteration 177/1000 | Loss: 0.00002568
Iteration 178/1000 | Loss: 0.00002160
Iteration 179/1000 | Loss: 0.00002160
Iteration 180/1000 | Loss: 0.00002160
Iteration 181/1000 | Loss: 0.00002160
Iteration 182/1000 | Loss: 0.00002160
Iteration 183/1000 | Loss: 0.00002160
Iteration 184/1000 | Loss: 0.00002159
Iteration 185/1000 | Loss: 0.00002159
Iteration 186/1000 | Loss: 0.00002159
Iteration 187/1000 | Loss: 0.00002159
Iteration 188/1000 | Loss: 0.00002159
Iteration 189/1000 | Loss: 0.00002159
Iteration 190/1000 | Loss: 0.00002158
Iteration 191/1000 | Loss: 0.00002158
Iteration 192/1000 | Loss: 0.00002158
Iteration 193/1000 | Loss: 0.00002158
Iteration 194/1000 | Loss: 0.00002158
Iteration 195/1000 | Loss: 0.00002158
Iteration 196/1000 | Loss: 0.00002158
Iteration 197/1000 | Loss: 0.00002158
Iteration 198/1000 | Loss: 0.00002158
Iteration 199/1000 | Loss: 0.00002158
Iteration 200/1000 | Loss: 0.00002158
Iteration 201/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.157844210159965e-05, 2.157844210159965e-05, 2.157844210159965e-05, 2.157844210159965e-05, 2.157844210159965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.157844210159965e-05

Optimization complete. Final v2v error: 2.8130266666412354 mm

Highest mean error: 21.003612518310547 mm for frame 67

Lowest mean error: 2.335794448852539 mm for frame 135

Saving results

Total time: 90.91421890258789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419420
Iteration 2/25 | Loss: 0.00117118
Iteration 3/25 | Loss: 0.00105298
Iteration 4/25 | Loss: 0.00103648
Iteration 5/25 | Loss: 0.00103184
Iteration 6/25 | Loss: 0.00103106
Iteration 7/25 | Loss: 0.00103106
Iteration 8/25 | Loss: 0.00103106
Iteration 9/25 | Loss: 0.00103106
Iteration 10/25 | Loss: 0.00103106
Iteration 11/25 | Loss: 0.00103106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010310608195140958, 0.0010310608195140958, 0.0010310608195140958, 0.0010310608195140958, 0.0010310608195140958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010310608195140958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34498370
Iteration 2/25 | Loss: 0.00129916
Iteration 3/25 | Loss: 0.00129916
Iteration 4/25 | Loss: 0.00129916
Iteration 5/25 | Loss: 0.00129916
Iteration 6/25 | Loss: 0.00129916
Iteration 7/25 | Loss: 0.00129916
Iteration 8/25 | Loss: 0.00129915
Iteration 9/25 | Loss: 0.00129915
Iteration 10/25 | Loss: 0.00129915
Iteration 11/25 | Loss: 0.00129915
Iteration 12/25 | Loss: 0.00129915
Iteration 13/25 | Loss: 0.00129915
Iteration 14/25 | Loss: 0.00129915
Iteration 15/25 | Loss: 0.00129915
Iteration 16/25 | Loss: 0.00129915
Iteration 17/25 | Loss: 0.00129915
Iteration 18/25 | Loss: 0.00129915
Iteration 19/25 | Loss: 0.00129915
Iteration 20/25 | Loss: 0.00129915
Iteration 21/25 | Loss: 0.00129915
Iteration 22/25 | Loss: 0.00129915
Iteration 23/25 | Loss: 0.00129915
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012991537805646658, 0.0012991537805646658, 0.0012991537805646658, 0.0012991537805646658, 0.0012991537805646658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012991537805646658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129915
Iteration 2/1000 | Loss: 0.00002541
Iteration 3/1000 | Loss: 0.00001767
Iteration 4/1000 | Loss: 0.00001602
Iteration 5/1000 | Loss: 0.00001529
Iteration 6/1000 | Loss: 0.00001469
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001328
Iteration 11/1000 | Loss: 0.00001310
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001289
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001279
Iteration 35/1000 | Loss: 0.00001279
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001277
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001276
Iteration 43/1000 | Loss: 0.00001275
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001274
Iteration 47/1000 | Loss: 0.00001274
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001273
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001272
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001269
Iteration 59/1000 | Loss: 0.00001269
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001260
Iteration 87/1000 | Loss: 0.00001260
Iteration 88/1000 | Loss: 0.00001260
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001258
Iteration 99/1000 | Loss: 0.00001258
Iteration 100/1000 | Loss: 0.00001258
Iteration 101/1000 | Loss: 0.00001258
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001257
Iteration 107/1000 | Loss: 0.00001257
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001255
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001255
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001254
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001252
Iteration 127/1000 | Loss: 0.00001252
Iteration 128/1000 | Loss: 0.00001252
Iteration 129/1000 | Loss: 0.00001252
Iteration 130/1000 | Loss: 0.00001252
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001251
Iteration 133/1000 | Loss: 0.00001251
Iteration 134/1000 | Loss: 0.00001251
Iteration 135/1000 | Loss: 0.00001251
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001249
Iteration 142/1000 | Loss: 0.00001249
Iteration 143/1000 | Loss: 0.00001249
Iteration 144/1000 | Loss: 0.00001249
Iteration 145/1000 | Loss: 0.00001249
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001247
Iteration 155/1000 | Loss: 0.00001247
Iteration 156/1000 | Loss: 0.00001247
Iteration 157/1000 | Loss: 0.00001247
Iteration 158/1000 | Loss: 0.00001247
Iteration 159/1000 | Loss: 0.00001246
Iteration 160/1000 | Loss: 0.00001246
Iteration 161/1000 | Loss: 0.00001246
Iteration 162/1000 | Loss: 0.00001246
Iteration 163/1000 | Loss: 0.00001246
Iteration 164/1000 | Loss: 0.00001246
Iteration 165/1000 | Loss: 0.00001246
Iteration 166/1000 | Loss: 0.00001246
Iteration 167/1000 | Loss: 0.00001246
Iteration 168/1000 | Loss: 0.00001246
Iteration 169/1000 | Loss: 0.00001246
Iteration 170/1000 | Loss: 0.00001246
Iteration 171/1000 | Loss: 0.00001246
Iteration 172/1000 | Loss: 0.00001246
Iteration 173/1000 | Loss: 0.00001246
Iteration 174/1000 | Loss: 0.00001245
Iteration 175/1000 | Loss: 0.00001245
Iteration 176/1000 | Loss: 0.00001245
Iteration 177/1000 | Loss: 0.00001245
Iteration 178/1000 | Loss: 0.00001245
Iteration 179/1000 | Loss: 0.00001245
Iteration 180/1000 | Loss: 0.00001245
Iteration 181/1000 | Loss: 0.00001245
Iteration 182/1000 | Loss: 0.00001245
Iteration 183/1000 | Loss: 0.00001245
Iteration 184/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.245357452717144e-05, 1.245357452717144e-05, 1.245357452717144e-05, 1.245357452717144e-05, 1.245357452717144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.245357452717144e-05

Optimization complete. Final v2v error: 2.9483678340911865 mm

Highest mean error: 3.701807737350464 mm for frame 15

Lowest mean error: 2.4119346141815186 mm for frame 170

Saving results

Total time: 46.04383897781372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805067
Iteration 2/25 | Loss: 0.00113082
Iteration 3/25 | Loss: 0.00104684
Iteration 4/25 | Loss: 0.00103921
Iteration 5/25 | Loss: 0.00103680
Iteration 6/25 | Loss: 0.00103669
Iteration 7/25 | Loss: 0.00103669
Iteration 8/25 | Loss: 0.00103669
Iteration 9/25 | Loss: 0.00103669
Iteration 10/25 | Loss: 0.00103669
Iteration 11/25 | Loss: 0.00103669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010366927599534392, 0.0010366927599534392, 0.0010366927599534392, 0.0010366927599534392, 0.0010366927599534392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010366927599534392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39074385
Iteration 2/25 | Loss: 0.00101342
Iteration 3/25 | Loss: 0.00101342
Iteration 4/25 | Loss: 0.00101342
Iteration 5/25 | Loss: 0.00101342
Iteration 6/25 | Loss: 0.00101342
Iteration 7/25 | Loss: 0.00101342
Iteration 8/25 | Loss: 0.00101342
Iteration 9/25 | Loss: 0.00101342
Iteration 10/25 | Loss: 0.00101342
Iteration 11/25 | Loss: 0.00101342
Iteration 12/25 | Loss: 0.00101342
Iteration 13/25 | Loss: 0.00101342
Iteration 14/25 | Loss: 0.00101342
Iteration 15/25 | Loss: 0.00101342
Iteration 16/25 | Loss: 0.00101342
Iteration 17/25 | Loss: 0.00101342
Iteration 18/25 | Loss: 0.00101342
Iteration 19/25 | Loss: 0.00101342
Iteration 20/25 | Loss: 0.00101342
Iteration 21/25 | Loss: 0.00101342
Iteration 22/25 | Loss: 0.00101342
Iteration 23/25 | Loss: 0.00101342
Iteration 24/25 | Loss: 0.00101342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010134167969226837, 0.0010134167969226837, 0.0010134167969226837, 0.0010134167969226837, 0.0010134167969226837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010134167969226837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101342
Iteration 2/1000 | Loss: 0.00002673
Iteration 3/1000 | Loss: 0.00001614
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001377
Iteration 6/1000 | Loss: 0.00001325
Iteration 7/1000 | Loss: 0.00001286
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001219
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001196
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001172
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001172
Iteration 115/1000 | Loss: 0.00001172
Iteration 116/1000 | Loss: 0.00001172
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.1716396329575218e-05, 1.1716396329575218e-05, 1.1716396329575218e-05, 1.1716396329575218e-05, 1.1716396329575218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1716396329575218e-05

Optimization complete. Final v2v error: 2.839531660079956 mm

Highest mean error: 4.0296478271484375 mm for frame 136

Lowest mean error: 2.2636187076568604 mm for frame 200

Saving results

Total time: 37.239232301712036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384482
Iteration 2/25 | Loss: 0.00114155
Iteration 3/25 | Loss: 0.00101205
Iteration 4/25 | Loss: 0.00099448
Iteration 5/25 | Loss: 0.00099211
Iteration 6/25 | Loss: 0.00099159
Iteration 7/25 | Loss: 0.00099159
Iteration 8/25 | Loss: 0.00099159
Iteration 9/25 | Loss: 0.00099159
Iteration 10/25 | Loss: 0.00099159
Iteration 11/25 | Loss: 0.00099159
Iteration 12/25 | Loss: 0.00099159
Iteration 13/25 | Loss: 0.00099159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000991593231447041, 0.000991593231447041, 0.000991593231447041, 0.000991593231447041, 0.000991593231447041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991593231447041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30015445
Iteration 2/25 | Loss: 0.00087327
Iteration 3/25 | Loss: 0.00087327
Iteration 4/25 | Loss: 0.00087327
Iteration 5/25 | Loss: 0.00087327
Iteration 6/25 | Loss: 0.00087327
Iteration 7/25 | Loss: 0.00087327
Iteration 8/25 | Loss: 0.00087327
Iteration 9/25 | Loss: 0.00087327
Iteration 10/25 | Loss: 0.00087327
Iteration 11/25 | Loss: 0.00087327
Iteration 12/25 | Loss: 0.00087327
Iteration 13/25 | Loss: 0.00087327
Iteration 14/25 | Loss: 0.00087327
Iteration 15/25 | Loss: 0.00087327
Iteration 16/25 | Loss: 0.00087327
Iteration 17/25 | Loss: 0.00087327
Iteration 18/25 | Loss: 0.00087327
Iteration 19/25 | Loss: 0.00087327
Iteration 20/25 | Loss: 0.00087327
Iteration 21/25 | Loss: 0.00087327
Iteration 22/25 | Loss: 0.00087327
Iteration 23/25 | Loss: 0.00087327
Iteration 24/25 | Loss: 0.00087327
Iteration 25/25 | Loss: 0.00087327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087327
Iteration 2/1000 | Loss: 0.00002092
Iteration 3/1000 | Loss: 0.00001354
Iteration 4/1000 | Loss: 0.00001188
Iteration 5/1000 | Loss: 0.00001094
Iteration 6/1000 | Loss: 0.00001029
Iteration 7/1000 | Loss: 0.00000991
Iteration 8/1000 | Loss: 0.00000953
Iteration 9/1000 | Loss: 0.00000926
Iteration 10/1000 | Loss: 0.00000906
Iteration 11/1000 | Loss: 0.00000900
Iteration 12/1000 | Loss: 0.00000893
Iteration 13/1000 | Loss: 0.00000880
Iteration 14/1000 | Loss: 0.00000877
Iteration 15/1000 | Loss: 0.00000877
Iteration 16/1000 | Loss: 0.00000874
Iteration 17/1000 | Loss: 0.00000872
Iteration 18/1000 | Loss: 0.00000871
Iteration 19/1000 | Loss: 0.00000871
Iteration 20/1000 | Loss: 0.00000871
Iteration 21/1000 | Loss: 0.00000869
Iteration 22/1000 | Loss: 0.00000869
Iteration 23/1000 | Loss: 0.00000868
Iteration 24/1000 | Loss: 0.00000868
Iteration 25/1000 | Loss: 0.00000867
Iteration 26/1000 | Loss: 0.00000867
Iteration 27/1000 | Loss: 0.00000867
Iteration 28/1000 | Loss: 0.00000865
Iteration 29/1000 | Loss: 0.00000864
Iteration 30/1000 | Loss: 0.00000864
Iteration 31/1000 | Loss: 0.00000863
Iteration 32/1000 | Loss: 0.00000862
Iteration 33/1000 | Loss: 0.00000861
Iteration 34/1000 | Loss: 0.00000861
Iteration 35/1000 | Loss: 0.00000860
Iteration 36/1000 | Loss: 0.00000860
Iteration 37/1000 | Loss: 0.00000859
Iteration 38/1000 | Loss: 0.00000859
Iteration 39/1000 | Loss: 0.00000856
Iteration 40/1000 | Loss: 0.00000856
Iteration 41/1000 | Loss: 0.00000855
Iteration 42/1000 | Loss: 0.00000854
Iteration 43/1000 | Loss: 0.00000854
Iteration 44/1000 | Loss: 0.00000854
Iteration 45/1000 | Loss: 0.00000854
Iteration 46/1000 | Loss: 0.00000853
Iteration 47/1000 | Loss: 0.00000853
Iteration 48/1000 | Loss: 0.00000852
Iteration 49/1000 | Loss: 0.00000850
Iteration 50/1000 | Loss: 0.00000850
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000850
Iteration 53/1000 | Loss: 0.00000850
Iteration 54/1000 | Loss: 0.00000850
Iteration 55/1000 | Loss: 0.00000850
Iteration 56/1000 | Loss: 0.00000850
Iteration 57/1000 | Loss: 0.00000849
Iteration 58/1000 | Loss: 0.00000849
Iteration 59/1000 | Loss: 0.00000848
Iteration 60/1000 | Loss: 0.00000847
Iteration 61/1000 | Loss: 0.00000846
Iteration 62/1000 | Loss: 0.00000846
Iteration 63/1000 | Loss: 0.00000845
Iteration 64/1000 | Loss: 0.00000845
Iteration 65/1000 | Loss: 0.00000845
Iteration 66/1000 | Loss: 0.00000845
Iteration 67/1000 | Loss: 0.00000845
Iteration 68/1000 | Loss: 0.00000845
Iteration 69/1000 | Loss: 0.00000845
Iteration 70/1000 | Loss: 0.00000845
Iteration 71/1000 | Loss: 0.00000845
Iteration 72/1000 | Loss: 0.00000842
Iteration 73/1000 | Loss: 0.00000842
Iteration 74/1000 | Loss: 0.00000842
Iteration 75/1000 | Loss: 0.00000842
Iteration 76/1000 | Loss: 0.00000842
Iteration 77/1000 | Loss: 0.00000842
Iteration 78/1000 | Loss: 0.00000842
Iteration 79/1000 | Loss: 0.00000842
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000841
Iteration 82/1000 | Loss: 0.00000840
Iteration 83/1000 | Loss: 0.00000840
Iteration 84/1000 | Loss: 0.00000838
Iteration 85/1000 | Loss: 0.00000838
Iteration 86/1000 | Loss: 0.00000837
Iteration 87/1000 | Loss: 0.00000837
Iteration 88/1000 | Loss: 0.00000837
Iteration 89/1000 | Loss: 0.00000837
Iteration 90/1000 | Loss: 0.00000837
Iteration 91/1000 | Loss: 0.00000836
Iteration 92/1000 | Loss: 0.00000835
Iteration 93/1000 | Loss: 0.00000834
Iteration 94/1000 | Loss: 0.00000834
Iteration 95/1000 | Loss: 0.00000833
Iteration 96/1000 | Loss: 0.00000833
Iteration 97/1000 | Loss: 0.00000833
Iteration 98/1000 | Loss: 0.00000833
Iteration 99/1000 | Loss: 0.00000832
Iteration 100/1000 | Loss: 0.00000832
Iteration 101/1000 | Loss: 0.00000832
Iteration 102/1000 | Loss: 0.00000832
Iteration 103/1000 | Loss: 0.00000832
Iteration 104/1000 | Loss: 0.00000832
Iteration 105/1000 | Loss: 0.00000832
Iteration 106/1000 | Loss: 0.00000831
Iteration 107/1000 | Loss: 0.00000831
Iteration 108/1000 | Loss: 0.00000831
Iteration 109/1000 | Loss: 0.00000831
Iteration 110/1000 | Loss: 0.00000831
Iteration 111/1000 | Loss: 0.00000831
Iteration 112/1000 | Loss: 0.00000831
Iteration 113/1000 | Loss: 0.00000831
Iteration 114/1000 | Loss: 0.00000831
Iteration 115/1000 | Loss: 0.00000831
Iteration 116/1000 | Loss: 0.00000831
Iteration 117/1000 | Loss: 0.00000831
Iteration 118/1000 | Loss: 0.00000831
Iteration 119/1000 | Loss: 0.00000830
Iteration 120/1000 | Loss: 0.00000830
Iteration 121/1000 | Loss: 0.00000830
Iteration 122/1000 | Loss: 0.00000830
Iteration 123/1000 | Loss: 0.00000830
Iteration 124/1000 | Loss: 0.00000830
Iteration 125/1000 | Loss: 0.00000830
Iteration 126/1000 | Loss: 0.00000830
Iteration 127/1000 | Loss: 0.00000830
Iteration 128/1000 | Loss: 0.00000830
Iteration 129/1000 | Loss: 0.00000830
Iteration 130/1000 | Loss: 0.00000830
Iteration 131/1000 | Loss: 0.00000830
Iteration 132/1000 | Loss: 0.00000829
Iteration 133/1000 | Loss: 0.00000829
Iteration 134/1000 | Loss: 0.00000829
Iteration 135/1000 | Loss: 0.00000829
Iteration 136/1000 | Loss: 0.00000828
Iteration 137/1000 | Loss: 0.00000828
Iteration 138/1000 | Loss: 0.00000828
Iteration 139/1000 | Loss: 0.00000828
Iteration 140/1000 | Loss: 0.00000827
Iteration 141/1000 | Loss: 0.00000827
Iteration 142/1000 | Loss: 0.00000827
Iteration 143/1000 | Loss: 0.00000827
Iteration 144/1000 | Loss: 0.00000827
Iteration 145/1000 | Loss: 0.00000827
Iteration 146/1000 | Loss: 0.00000827
Iteration 147/1000 | Loss: 0.00000827
Iteration 148/1000 | Loss: 0.00000826
Iteration 149/1000 | Loss: 0.00000826
Iteration 150/1000 | Loss: 0.00000826
Iteration 151/1000 | Loss: 0.00000826
Iteration 152/1000 | Loss: 0.00000826
Iteration 153/1000 | Loss: 0.00000826
Iteration 154/1000 | Loss: 0.00000826
Iteration 155/1000 | Loss: 0.00000826
Iteration 156/1000 | Loss: 0.00000826
Iteration 157/1000 | Loss: 0.00000826
Iteration 158/1000 | Loss: 0.00000826
Iteration 159/1000 | Loss: 0.00000826
Iteration 160/1000 | Loss: 0.00000826
Iteration 161/1000 | Loss: 0.00000826
Iteration 162/1000 | Loss: 0.00000826
Iteration 163/1000 | Loss: 0.00000826
Iteration 164/1000 | Loss: 0.00000826
Iteration 165/1000 | Loss: 0.00000825
Iteration 166/1000 | Loss: 0.00000825
Iteration 167/1000 | Loss: 0.00000825
Iteration 168/1000 | Loss: 0.00000825
Iteration 169/1000 | Loss: 0.00000825
Iteration 170/1000 | Loss: 0.00000825
Iteration 171/1000 | Loss: 0.00000825
Iteration 172/1000 | Loss: 0.00000824
Iteration 173/1000 | Loss: 0.00000824
Iteration 174/1000 | Loss: 0.00000824
Iteration 175/1000 | Loss: 0.00000824
Iteration 176/1000 | Loss: 0.00000824
Iteration 177/1000 | Loss: 0.00000824
Iteration 178/1000 | Loss: 0.00000824
Iteration 179/1000 | Loss: 0.00000824
Iteration 180/1000 | Loss: 0.00000824
Iteration 181/1000 | Loss: 0.00000824
Iteration 182/1000 | Loss: 0.00000824
Iteration 183/1000 | Loss: 0.00000823
Iteration 184/1000 | Loss: 0.00000823
Iteration 185/1000 | Loss: 0.00000823
Iteration 186/1000 | Loss: 0.00000823
Iteration 187/1000 | Loss: 0.00000823
Iteration 188/1000 | Loss: 0.00000823
Iteration 189/1000 | Loss: 0.00000823
Iteration 190/1000 | Loss: 0.00000823
Iteration 191/1000 | Loss: 0.00000823
Iteration 192/1000 | Loss: 0.00000823
Iteration 193/1000 | Loss: 0.00000823
Iteration 194/1000 | Loss: 0.00000823
Iteration 195/1000 | Loss: 0.00000823
Iteration 196/1000 | Loss: 0.00000823
Iteration 197/1000 | Loss: 0.00000823
Iteration 198/1000 | Loss: 0.00000822
Iteration 199/1000 | Loss: 0.00000822
Iteration 200/1000 | Loss: 0.00000822
Iteration 201/1000 | Loss: 0.00000822
Iteration 202/1000 | Loss: 0.00000822
Iteration 203/1000 | Loss: 0.00000822
Iteration 204/1000 | Loss: 0.00000822
Iteration 205/1000 | Loss: 0.00000822
Iteration 206/1000 | Loss: 0.00000822
Iteration 207/1000 | Loss: 0.00000822
Iteration 208/1000 | Loss: 0.00000822
Iteration 209/1000 | Loss: 0.00000822
Iteration 210/1000 | Loss: 0.00000822
Iteration 211/1000 | Loss: 0.00000822
Iteration 212/1000 | Loss: 0.00000822
Iteration 213/1000 | Loss: 0.00000822
Iteration 214/1000 | Loss: 0.00000822
Iteration 215/1000 | Loss: 0.00000822
Iteration 216/1000 | Loss: 0.00000822
Iteration 217/1000 | Loss: 0.00000821
Iteration 218/1000 | Loss: 0.00000821
Iteration 219/1000 | Loss: 0.00000821
Iteration 220/1000 | Loss: 0.00000821
Iteration 221/1000 | Loss: 0.00000821
Iteration 222/1000 | Loss: 0.00000821
Iteration 223/1000 | Loss: 0.00000821
Iteration 224/1000 | Loss: 0.00000821
Iteration 225/1000 | Loss: 0.00000821
Iteration 226/1000 | Loss: 0.00000820
Iteration 227/1000 | Loss: 0.00000820
Iteration 228/1000 | Loss: 0.00000820
Iteration 229/1000 | Loss: 0.00000820
Iteration 230/1000 | Loss: 0.00000820
Iteration 231/1000 | Loss: 0.00000820
Iteration 232/1000 | Loss: 0.00000820
Iteration 233/1000 | Loss: 0.00000819
Iteration 234/1000 | Loss: 0.00000819
Iteration 235/1000 | Loss: 0.00000819
Iteration 236/1000 | Loss: 0.00000819
Iteration 237/1000 | Loss: 0.00000819
Iteration 238/1000 | Loss: 0.00000819
Iteration 239/1000 | Loss: 0.00000818
Iteration 240/1000 | Loss: 0.00000818
Iteration 241/1000 | Loss: 0.00000818
Iteration 242/1000 | Loss: 0.00000818
Iteration 243/1000 | Loss: 0.00000818
Iteration 244/1000 | Loss: 0.00000818
Iteration 245/1000 | Loss: 0.00000818
Iteration 246/1000 | Loss: 0.00000818
Iteration 247/1000 | Loss: 0.00000818
Iteration 248/1000 | Loss: 0.00000818
Iteration 249/1000 | Loss: 0.00000818
Iteration 250/1000 | Loss: 0.00000818
Iteration 251/1000 | Loss: 0.00000818
Iteration 252/1000 | Loss: 0.00000818
Iteration 253/1000 | Loss: 0.00000818
Iteration 254/1000 | Loss: 0.00000818
Iteration 255/1000 | Loss: 0.00000818
Iteration 256/1000 | Loss: 0.00000818
Iteration 257/1000 | Loss: 0.00000818
Iteration 258/1000 | Loss: 0.00000818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [8.177908057405148e-06, 8.177908057405148e-06, 8.177908057405148e-06, 8.177908057405148e-06, 8.177908057405148e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.177908057405148e-06

Optimization complete. Final v2v error: 2.440075397491455 mm

Highest mean error: 2.8298540115356445 mm for frame 77

Lowest mean error: 2.2333736419677734 mm for frame 166

Saving results

Total time: 45.80940580368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897508
Iteration 2/25 | Loss: 0.00198157
Iteration 3/25 | Loss: 0.00149554
Iteration 4/25 | Loss: 0.00135213
Iteration 5/25 | Loss: 0.00134844
Iteration 6/25 | Loss: 0.00121860
Iteration 7/25 | Loss: 0.00122390
Iteration 8/25 | Loss: 0.00120777
Iteration 9/25 | Loss: 0.00118888
Iteration 10/25 | Loss: 0.00113483
Iteration 11/25 | Loss: 0.00112323
Iteration 12/25 | Loss: 0.00111619
Iteration 13/25 | Loss: 0.00112294
Iteration 14/25 | Loss: 0.00110905
Iteration 15/25 | Loss: 0.00110227
Iteration 16/25 | Loss: 0.00110646
Iteration 17/25 | Loss: 0.00109958
Iteration 18/25 | Loss: 0.00109803
Iteration 19/25 | Loss: 0.00109774
Iteration 20/25 | Loss: 0.00109762
Iteration 21/25 | Loss: 0.00109762
Iteration 22/25 | Loss: 0.00109761
Iteration 23/25 | Loss: 0.00109761
Iteration 24/25 | Loss: 0.00109761
Iteration 25/25 | Loss: 0.00109761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43600464
Iteration 2/25 | Loss: 0.00100014
Iteration 3/25 | Loss: 0.00100012
Iteration 4/25 | Loss: 0.00100012
Iteration 5/25 | Loss: 0.00100012
Iteration 6/25 | Loss: 0.00100012
Iteration 7/25 | Loss: 0.00100012
Iteration 8/25 | Loss: 0.00100012
Iteration 9/25 | Loss: 0.00100012
Iteration 10/25 | Loss: 0.00100012
Iteration 11/25 | Loss: 0.00100012
Iteration 12/25 | Loss: 0.00100012
Iteration 13/25 | Loss: 0.00100012
Iteration 14/25 | Loss: 0.00100012
Iteration 15/25 | Loss: 0.00100012
Iteration 16/25 | Loss: 0.00100012
Iteration 17/25 | Loss: 0.00100012
Iteration 18/25 | Loss: 0.00100012
Iteration 19/25 | Loss: 0.00100012
Iteration 20/25 | Loss: 0.00100012
Iteration 21/25 | Loss: 0.00100012
Iteration 22/25 | Loss: 0.00100012
Iteration 23/25 | Loss: 0.00100012
Iteration 24/25 | Loss: 0.00100012
Iteration 25/25 | Loss: 0.00100012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100012
Iteration 2/1000 | Loss: 0.00007081
Iteration 3/1000 | Loss: 0.00004561
Iteration 4/1000 | Loss: 0.00003874
Iteration 5/1000 | Loss: 0.00003589
Iteration 6/1000 | Loss: 0.00003345
Iteration 7/1000 | Loss: 0.00003186
Iteration 8/1000 | Loss: 0.00003109
Iteration 9/1000 | Loss: 0.00033157
Iteration 10/1000 | Loss: 0.00073722
Iteration 11/1000 | Loss: 0.00003653
Iteration 12/1000 | Loss: 0.00002897
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00001828
Iteration 16/1000 | Loss: 0.00001710
Iteration 17/1000 | Loss: 0.00001639
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001553
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001429
Iteration 27/1000 | Loss: 0.00001426
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001417
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001414
Iteration 38/1000 | Loss: 0.00001414
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001412
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001408
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001407
Iteration 67/1000 | Loss: 0.00001407
Iteration 68/1000 | Loss: 0.00001407
Iteration 69/1000 | Loss: 0.00001407
Iteration 70/1000 | Loss: 0.00001407
Iteration 71/1000 | Loss: 0.00001406
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001403
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001403
Iteration 118/1000 | Loss: 0.00001403
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.402154430252267e-05, 1.402154430252267e-05, 1.402154430252267e-05, 1.402154430252267e-05, 1.402154430252267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.402154430252267e-05

Optimization complete. Final v2v error: 3.145307779312134 mm

Highest mean error: 3.845064640045166 mm for frame 95

Lowest mean error: 2.5509073734283447 mm for frame 140

Saving results

Total time: 77.29019522666931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848452
Iteration 2/25 | Loss: 0.00157943
Iteration 3/25 | Loss: 0.00122216
Iteration 4/25 | Loss: 0.00117151
Iteration 5/25 | Loss: 0.00115487
Iteration 6/25 | Loss: 0.00117023
Iteration 7/25 | Loss: 0.00109900
Iteration 8/25 | Loss: 0.00108811
Iteration 9/25 | Loss: 0.00105272
Iteration 10/25 | Loss: 0.00104960
Iteration 11/25 | Loss: 0.00104933
Iteration 12/25 | Loss: 0.00105393
Iteration 13/25 | Loss: 0.00104763
Iteration 14/25 | Loss: 0.00104643
Iteration 15/25 | Loss: 0.00104622
Iteration 16/25 | Loss: 0.00104620
Iteration 17/25 | Loss: 0.00104620
Iteration 18/25 | Loss: 0.00104620
Iteration 19/25 | Loss: 0.00104620
Iteration 20/25 | Loss: 0.00104619
Iteration 21/25 | Loss: 0.00104619
Iteration 22/25 | Loss: 0.00104619
Iteration 23/25 | Loss: 0.00104619
Iteration 24/25 | Loss: 0.00104618
Iteration 25/25 | Loss: 0.00104618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88889670
Iteration 2/25 | Loss: 0.00049063
Iteration 3/25 | Loss: 0.00049062
Iteration 4/25 | Loss: 0.00049062
Iteration 5/25 | Loss: 0.00049062
Iteration 6/25 | Loss: 0.00049062
Iteration 7/25 | Loss: 0.00049061
Iteration 8/25 | Loss: 0.00049061
Iteration 9/25 | Loss: 0.00049061
Iteration 10/25 | Loss: 0.00049061
Iteration 11/25 | Loss: 0.00049061
Iteration 12/25 | Loss: 0.00049061
Iteration 13/25 | Loss: 0.00049061
Iteration 14/25 | Loss: 0.00049061
Iteration 15/25 | Loss: 0.00049061
Iteration 16/25 | Loss: 0.00049061
Iteration 17/25 | Loss: 0.00049061
Iteration 18/25 | Loss: 0.00049061
Iteration 19/25 | Loss: 0.00049061
Iteration 20/25 | Loss: 0.00049061
Iteration 21/25 | Loss: 0.00049061
Iteration 22/25 | Loss: 0.00049061
Iteration 23/25 | Loss: 0.00049061
Iteration 24/25 | Loss: 0.00049061
Iteration 25/25 | Loss: 0.00049061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049061
Iteration 2/1000 | Loss: 0.00002859
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001914
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001738
Iteration 8/1000 | Loss: 0.00001706
Iteration 9/1000 | Loss: 0.00001682
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001650
Iteration 12/1000 | Loss: 0.00001644
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001644
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001643
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001633
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001632
Iteration 44/1000 | Loss: 0.00001632
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001620
Iteration 99/1000 | Loss: 0.00001620
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001619
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001618
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.6182564650080167e-05, 1.6182564650080167e-05, 1.6182564650080167e-05, 1.6182564650080167e-05, 1.6182564650080167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6182564650080167e-05

Optimization complete. Final v2v error: 3.409148693084717 mm

Highest mean error: 3.55924654006958 mm for frame 19

Lowest mean error: 3.28777813911438 mm for frame 0

Saving results

Total time: 53.02310538291931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377089
Iteration 2/25 | Loss: 0.00118663
Iteration 3/25 | Loss: 0.00104805
Iteration 4/25 | Loss: 0.00102265
Iteration 5/25 | Loss: 0.00101560
Iteration 6/25 | Loss: 0.00101355
Iteration 7/25 | Loss: 0.00101350
Iteration 8/25 | Loss: 0.00101350
Iteration 9/25 | Loss: 0.00101350
Iteration 10/25 | Loss: 0.00101350
Iteration 11/25 | Loss: 0.00101327
Iteration 12/25 | Loss: 0.00101327
Iteration 13/25 | Loss: 0.00101327
Iteration 14/25 | Loss: 0.00101327
Iteration 15/25 | Loss: 0.00101327
Iteration 16/25 | Loss: 0.00101327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010132677853107452, 0.0010132677853107452, 0.0010132677853107452, 0.0010132677853107452, 0.0010132677853107452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010132677853107452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26572025
Iteration 2/25 | Loss: 0.00120906
Iteration 3/25 | Loss: 0.00120906
Iteration 4/25 | Loss: 0.00120906
Iteration 5/25 | Loss: 0.00120906
Iteration 6/25 | Loss: 0.00120906
Iteration 7/25 | Loss: 0.00120906
Iteration 8/25 | Loss: 0.00120906
Iteration 9/25 | Loss: 0.00120906
Iteration 10/25 | Loss: 0.00120906
Iteration 11/25 | Loss: 0.00120906
Iteration 12/25 | Loss: 0.00120906
Iteration 13/25 | Loss: 0.00120906
Iteration 14/25 | Loss: 0.00120906
Iteration 15/25 | Loss: 0.00120906
Iteration 16/25 | Loss: 0.00120906
Iteration 17/25 | Loss: 0.00120906
Iteration 18/25 | Loss: 0.00120906
Iteration 19/25 | Loss: 0.00120906
Iteration 20/25 | Loss: 0.00120906
Iteration 21/25 | Loss: 0.00120906
Iteration 22/25 | Loss: 0.00120906
Iteration 23/25 | Loss: 0.00120906
Iteration 24/25 | Loss: 0.00120906
Iteration 25/25 | Loss: 0.00120906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120906
Iteration 2/1000 | Loss: 0.00004146
Iteration 3/1000 | Loss: 0.00002839
Iteration 4/1000 | Loss: 0.00002288
Iteration 5/1000 | Loss: 0.00001963
Iteration 6/1000 | Loss: 0.00001782
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001504
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001401
Iteration 15/1000 | Loss: 0.00001393
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001391
Iteration 18/1000 | Loss: 0.00001391
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001374
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001351
Iteration 92/1000 | Loss: 0.00001351
Iteration 93/1000 | Loss: 0.00001351
Iteration 94/1000 | Loss: 0.00001351
Iteration 95/1000 | Loss: 0.00001351
Iteration 96/1000 | Loss: 0.00001351
Iteration 97/1000 | Loss: 0.00001351
Iteration 98/1000 | Loss: 0.00001350
Iteration 99/1000 | Loss: 0.00001350
Iteration 100/1000 | Loss: 0.00001350
Iteration 101/1000 | Loss: 0.00001350
Iteration 102/1000 | Loss: 0.00001350
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001349
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001347
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001346
Iteration 125/1000 | Loss: 0.00001346
Iteration 126/1000 | Loss: 0.00001346
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001344
Iteration 150/1000 | Loss: 0.00001344
Iteration 151/1000 | Loss: 0.00001344
Iteration 152/1000 | Loss: 0.00001343
Iteration 153/1000 | Loss: 0.00001343
Iteration 154/1000 | Loss: 0.00001343
Iteration 155/1000 | Loss: 0.00001343
Iteration 156/1000 | Loss: 0.00001343
Iteration 157/1000 | Loss: 0.00001343
Iteration 158/1000 | Loss: 0.00001343
Iteration 159/1000 | Loss: 0.00001343
Iteration 160/1000 | Loss: 0.00001343
Iteration 161/1000 | Loss: 0.00001343
Iteration 162/1000 | Loss: 0.00001343
Iteration 163/1000 | Loss: 0.00001343
Iteration 164/1000 | Loss: 0.00001343
Iteration 165/1000 | Loss: 0.00001343
Iteration 166/1000 | Loss: 0.00001343
Iteration 167/1000 | Loss: 0.00001343
Iteration 168/1000 | Loss: 0.00001343
Iteration 169/1000 | Loss: 0.00001343
Iteration 170/1000 | Loss: 0.00001343
Iteration 171/1000 | Loss: 0.00001343
Iteration 172/1000 | Loss: 0.00001343
Iteration 173/1000 | Loss: 0.00001343
Iteration 174/1000 | Loss: 0.00001343
Iteration 175/1000 | Loss: 0.00001343
Iteration 176/1000 | Loss: 0.00001343
Iteration 177/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3431233128358144e-05, 1.3431233128358144e-05, 1.3431233128358144e-05, 1.3431233128358144e-05, 1.3431233128358144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3431233128358144e-05

Optimization complete. Final v2v error: 3.069408416748047 mm

Highest mean error: 3.676044225692749 mm for frame 50

Lowest mean error: 2.3667845726013184 mm for frame 10

Saving results

Total time: 45.48667931556702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410547
Iteration 2/25 | Loss: 0.00106364
Iteration 3/25 | Loss: 0.00098922
Iteration 4/25 | Loss: 0.00098238
Iteration 5/25 | Loss: 0.00098037
Iteration 6/25 | Loss: 0.00097990
Iteration 7/25 | Loss: 0.00097990
Iteration 8/25 | Loss: 0.00097990
Iteration 9/25 | Loss: 0.00097990
Iteration 10/25 | Loss: 0.00097990
Iteration 11/25 | Loss: 0.00097990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009799042018130422, 0.0009799042018130422, 0.0009799042018130422, 0.0009799042018130422, 0.0009799042018130422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009799042018130422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.99816155
Iteration 2/25 | Loss: 0.00102743
Iteration 3/25 | Loss: 0.00102743
Iteration 4/25 | Loss: 0.00102743
Iteration 5/25 | Loss: 0.00102743
Iteration 6/25 | Loss: 0.00102742
Iteration 7/25 | Loss: 0.00102742
Iteration 8/25 | Loss: 0.00102742
Iteration 9/25 | Loss: 0.00102742
Iteration 10/25 | Loss: 0.00102742
Iteration 11/25 | Loss: 0.00102742
Iteration 12/25 | Loss: 0.00102742
Iteration 13/25 | Loss: 0.00102742
Iteration 14/25 | Loss: 0.00102742
Iteration 15/25 | Loss: 0.00102742
Iteration 16/25 | Loss: 0.00102742
Iteration 17/25 | Loss: 0.00102742
Iteration 18/25 | Loss: 0.00102742
Iteration 19/25 | Loss: 0.00102742
Iteration 20/25 | Loss: 0.00102742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010274230735376477, 0.0010274230735376477, 0.0010274230735376477, 0.0010274230735376477, 0.0010274230735376477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010274230735376477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102742
Iteration 2/1000 | Loss: 0.00001547
Iteration 3/1000 | Loss: 0.00001130
Iteration 4/1000 | Loss: 0.00001021
Iteration 5/1000 | Loss: 0.00000950
Iteration 6/1000 | Loss: 0.00000916
Iteration 7/1000 | Loss: 0.00000881
Iteration 8/1000 | Loss: 0.00000878
Iteration 9/1000 | Loss: 0.00000863
Iteration 10/1000 | Loss: 0.00000847
Iteration 11/1000 | Loss: 0.00000841
Iteration 12/1000 | Loss: 0.00000835
Iteration 13/1000 | Loss: 0.00000832
Iteration 14/1000 | Loss: 0.00000830
Iteration 15/1000 | Loss: 0.00000828
Iteration 16/1000 | Loss: 0.00000827
Iteration 17/1000 | Loss: 0.00000826
Iteration 18/1000 | Loss: 0.00000826
Iteration 19/1000 | Loss: 0.00000819
Iteration 20/1000 | Loss: 0.00000819
Iteration 21/1000 | Loss: 0.00000814
Iteration 22/1000 | Loss: 0.00000814
Iteration 23/1000 | Loss: 0.00000813
Iteration 24/1000 | Loss: 0.00000813
Iteration 25/1000 | Loss: 0.00000813
Iteration 26/1000 | Loss: 0.00000812
Iteration 27/1000 | Loss: 0.00000812
Iteration 28/1000 | Loss: 0.00000812
Iteration 29/1000 | Loss: 0.00000812
Iteration 30/1000 | Loss: 0.00000812
Iteration 31/1000 | Loss: 0.00000811
Iteration 32/1000 | Loss: 0.00000811
Iteration 33/1000 | Loss: 0.00000810
Iteration 34/1000 | Loss: 0.00000809
Iteration 35/1000 | Loss: 0.00000808
Iteration 36/1000 | Loss: 0.00000808
Iteration 37/1000 | Loss: 0.00000808
Iteration 38/1000 | Loss: 0.00000807
Iteration 39/1000 | Loss: 0.00000807
Iteration 40/1000 | Loss: 0.00000807
Iteration 41/1000 | Loss: 0.00000807
Iteration 42/1000 | Loss: 0.00000807
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000806
Iteration 45/1000 | Loss: 0.00000805
Iteration 46/1000 | Loss: 0.00000805
Iteration 47/1000 | Loss: 0.00000804
Iteration 48/1000 | Loss: 0.00000803
Iteration 49/1000 | Loss: 0.00000803
Iteration 50/1000 | Loss: 0.00000803
Iteration 51/1000 | Loss: 0.00000803
Iteration 52/1000 | Loss: 0.00000803
Iteration 53/1000 | Loss: 0.00000803
Iteration 54/1000 | Loss: 0.00000802
Iteration 55/1000 | Loss: 0.00000802
Iteration 56/1000 | Loss: 0.00000802
Iteration 57/1000 | Loss: 0.00000802
Iteration 58/1000 | Loss: 0.00000799
Iteration 59/1000 | Loss: 0.00000799
Iteration 60/1000 | Loss: 0.00000798
Iteration 61/1000 | Loss: 0.00000798
Iteration 62/1000 | Loss: 0.00000798
Iteration 63/1000 | Loss: 0.00000796
Iteration 64/1000 | Loss: 0.00000796
Iteration 65/1000 | Loss: 0.00000796
Iteration 66/1000 | Loss: 0.00000795
Iteration 67/1000 | Loss: 0.00000795
Iteration 68/1000 | Loss: 0.00000795
Iteration 69/1000 | Loss: 0.00000793
Iteration 70/1000 | Loss: 0.00000793
Iteration 71/1000 | Loss: 0.00000793
Iteration 72/1000 | Loss: 0.00000793
Iteration 73/1000 | Loss: 0.00000792
Iteration 74/1000 | Loss: 0.00000792
Iteration 75/1000 | Loss: 0.00000792
Iteration 76/1000 | Loss: 0.00000792
Iteration 77/1000 | Loss: 0.00000788
Iteration 78/1000 | Loss: 0.00000788
Iteration 79/1000 | Loss: 0.00000787
Iteration 80/1000 | Loss: 0.00000787
Iteration 81/1000 | Loss: 0.00000787
Iteration 82/1000 | Loss: 0.00000787
Iteration 83/1000 | Loss: 0.00000785
Iteration 84/1000 | Loss: 0.00000785
Iteration 85/1000 | Loss: 0.00000784
Iteration 86/1000 | Loss: 0.00000784
Iteration 87/1000 | Loss: 0.00000784
Iteration 88/1000 | Loss: 0.00000784
Iteration 89/1000 | Loss: 0.00000784
Iteration 90/1000 | Loss: 0.00000784
Iteration 91/1000 | Loss: 0.00000784
Iteration 92/1000 | Loss: 0.00000784
Iteration 93/1000 | Loss: 0.00000784
Iteration 94/1000 | Loss: 0.00000784
Iteration 95/1000 | Loss: 0.00000783
Iteration 96/1000 | Loss: 0.00000783
Iteration 97/1000 | Loss: 0.00000783
Iteration 98/1000 | Loss: 0.00000782
Iteration 99/1000 | Loss: 0.00000782
Iteration 100/1000 | Loss: 0.00000782
Iteration 101/1000 | Loss: 0.00000782
Iteration 102/1000 | Loss: 0.00000781
Iteration 103/1000 | Loss: 0.00000781
Iteration 104/1000 | Loss: 0.00000781
Iteration 105/1000 | Loss: 0.00000781
Iteration 106/1000 | Loss: 0.00000781
Iteration 107/1000 | Loss: 0.00000780
Iteration 108/1000 | Loss: 0.00000780
Iteration 109/1000 | Loss: 0.00000779
Iteration 110/1000 | Loss: 0.00000779
Iteration 111/1000 | Loss: 0.00000779
Iteration 112/1000 | Loss: 0.00000779
Iteration 113/1000 | Loss: 0.00000779
Iteration 114/1000 | Loss: 0.00000779
Iteration 115/1000 | Loss: 0.00000779
Iteration 116/1000 | Loss: 0.00000779
Iteration 117/1000 | Loss: 0.00000779
Iteration 118/1000 | Loss: 0.00000779
Iteration 119/1000 | Loss: 0.00000779
Iteration 120/1000 | Loss: 0.00000779
Iteration 121/1000 | Loss: 0.00000779
Iteration 122/1000 | Loss: 0.00000779
Iteration 123/1000 | Loss: 0.00000779
Iteration 124/1000 | Loss: 0.00000779
Iteration 125/1000 | Loss: 0.00000779
Iteration 126/1000 | Loss: 0.00000779
Iteration 127/1000 | Loss: 0.00000779
Iteration 128/1000 | Loss: 0.00000779
Iteration 129/1000 | Loss: 0.00000779
Iteration 130/1000 | Loss: 0.00000779
Iteration 131/1000 | Loss: 0.00000779
Iteration 132/1000 | Loss: 0.00000779
Iteration 133/1000 | Loss: 0.00000779
Iteration 134/1000 | Loss: 0.00000779
Iteration 135/1000 | Loss: 0.00000779
Iteration 136/1000 | Loss: 0.00000779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [7.785371053614654e-06, 7.785371053614654e-06, 7.785371053614654e-06, 7.785371053614654e-06, 7.785371053614654e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.785371053614654e-06

Optimization complete. Final v2v error: 2.4437878131866455 mm

Highest mean error: 2.7200233936309814 mm for frame 89

Lowest mean error: 2.2714388370513916 mm for frame 177

Saving results

Total time: 35.54154300689697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820000
Iteration 2/25 | Loss: 0.00120305
Iteration 3/25 | Loss: 0.00106141
Iteration 4/25 | Loss: 0.00105174
Iteration 5/25 | Loss: 0.00104892
Iteration 6/25 | Loss: 0.00104845
Iteration 7/25 | Loss: 0.00104845
Iteration 8/25 | Loss: 0.00104845
Iteration 9/25 | Loss: 0.00104845
Iteration 10/25 | Loss: 0.00104845
Iteration 11/25 | Loss: 0.00104845
Iteration 12/25 | Loss: 0.00104845
Iteration 13/25 | Loss: 0.00104845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010484453523531556, 0.0010484453523531556, 0.0010484453523531556, 0.0010484453523531556, 0.0010484453523531556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010484453523531556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73185921
Iteration 2/25 | Loss: 0.00098319
Iteration 3/25 | Loss: 0.00098316
Iteration 4/25 | Loss: 0.00098316
Iteration 5/25 | Loss: 0.00098316
Iteration 6/25 | Loss: 0.00098316
Iteration 7/25 | Loss: 0.00098316
Iteration 8/25 | Loss: 0.00098315
Iteration 9/25 | Loss: 0.00098315
Iteration 10/25 | Loss: 0.00098315
Iteration 11/25 | Loss: 0.00098315
Iteration 12/25 | Loss: 0.00098315
Iteration 13/25 | Loss: 0.00098315
Iteration 14/25 | Loss: 0.00098315
Iteration 15/25 | Loss: 0.00098315
Iteration 16/25 | Loss: 0.00098315
Iteration 17/25 | Loss: 0.00098315
Iteration 18/25 | Loss: 0.00098315
Iteration 19/25 | Loss: 0.00098315
Iteration 20/25 | Loss: 0.00098315
Iteration 21/25 | Loss: 0.00098315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000983153935521841, 0.000983153935521841, 0.000983153935521841, 0.000983153935521841, 0.000983153935521841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983153935521841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098315
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00002174
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001695
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001511
Iteration 12/1000 | Loss: 0.00001502
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001486
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001472
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001468
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001460
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001457
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001453
Iteration 74/1000 | Loss: 0.00001453
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001452
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001448
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001447
Iteration 104/1000 | Loss: 0.00001447
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001446
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001445
Iteration 117/1000 | Loss: 0.00001445
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001443
Iteration 126/1000 | Loss: 0.00001443
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001442
Iteration 132/1000 | Loss: 0.00001442
Iteration 133/1000 | Loss: 0.00001442
Iteration 134/1000 | Loss: 0.00001442
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001441
Iteration 137/1000 | Loss: 0.00001441
Iteration 138/1000 | Loss: 0.00001441
Iteration 139/1000 | Loss: 0.00001441
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001439
Iteration 151/1000 | Loss: 0.00001439
Iteration 152/1000 | Loss: 0.00001439
Iteration 153/1000 | Loss: 0.00001439
Iteration 154/1000 | Loss: 0.00001439
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001438
Iteration 162/1000 | Loss: 0.00001438
Iteration 163/1000 | Loss: 0.00001438
Iteration 164/1000 | Loss: 0.00001438
Iteration 165/1000 | Loss: 0.00001438
Iteration 166/1000 | Loss: 0.00001438
Iteration 167/1000 | Loss: 0.00001438
Iteration 168/1000 | Loss: 0.00001438
Iteration 169/1000 | Loss: 0.00001438
Iteration 170/1000 | Loss: 0.00001438
Iteration 171/1000 | Loss: 0.00001437
Iteration 172/1000 | Loss: 0.00001437
Iteration 173/1000 | Loss: 0.00001437
Iteration 174/1000 | Loss: 0.00001437
Iteration 175/1000 | Loss: 0.00001437
Iteration 176/1000 | Loss: 0.00001437
Iteration 177/1000 | Loss: 0.00001437
Iteration 178/1000 | Loss: 0.00001437
Iteration 179/1000 | Loss: 0.00001437
Iteration 180/1000 | Loss: 0.00001437
Iteration 181/1000 | Loss: 0.00001437
Iteration 182/1000 | Loss: 0.00001437
Iteration 183/1000 | Loss: 0.00001437
Iteration 184/1000 | Loss: 0.00001437
Iteration 185/1000 | Loss: 0.00001437
Iteration 186/1000 | Loss: 0.00001437
Iteration 187/1000 | Loss: 0.00001437
Iteration 188/1000 | Loss: 0.00001437
Iteration 189/1000 | Loss: 0.00001437
Iteration 190/1000 | Loss: 0.00001437
Iteration 191/1000 | Loss: 0.00001437
Iteration 192/1000 | Loss: 0.00001437
Iteration 193/1000 | Loss: 0.00001437
Iteration 194/1000 | Loss: 0.00001437
Iteration 195/1000 | Loss: 0.00001437
Iteration 196/1000 | Loss: 0.00001437
Iteration 197/1000 | Loss: 0.00001437
Iteration 198/1000 | Loss: 0.00001437
Iteration 199/1000 | Loss: 0.00001437
Iteration 200/1000 | Loss: 0.00001437
Iteration 201/1000 | Loss: 0.00001437
Iteration 202/1000 | Loss: 0.00001437
Iteration 203/1000 | Loss: 0.00001437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.4368924894370139e-05, 1.4368924894370139e-05, 1.4368924894370139e-05, 1.4368924894370139e-05, 1.4368924894370139e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4368924894370139e-05

Optimization complete. Final v2v error: 3.205153703689575 mm

Highest mean error: 4.145582675933838 mm for frame 2

Lowest mean error: 2.64707612991333 mm for frame 109

Saving results

Total time: 47.5383996963501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830626
Iteration 2/25 | Loss: 0.00113604
Iteration 3/25 | Loss: 0.00101315
Iteration 4/25 | Loss: 0.00100236
Iteration 5/25 | Loss: 0.00100064
Iteration 6/25 | Loss: 0.00100054
Iteration 7/25 | Loss: 0.00100054
Iteration 8/25 | Loss: 0.00100054
Iteration 9/25 | Loss: 0.00100054
Iteration 10/25 | Loss: 0.00100054
Iteration 11/25 | Loss: 0.00100054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010005369549617171, 0.0010005369549617171, 0.0010005369549617171, 0.0010005369549617171, 0.0010005369549617171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010005369549617171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30252826
Iteration 2/25 | Loss: 0.00093725
Iteration 3/25 | Loss: 0.00093724
Iteration 4/25 | Loss: 0.00093724
Iteration 5/25 | Loss: 0.00093723
Iteration 6/25 | Loss: 0.00093723
Iteration 7/25 | Loss: 0.00093723
Iteration 8/25 | Loss: 0.00093723
Iteration 9/25 | Loss: 0.00093723
Iteration 10/25 | Loss: 0.00093723
Iteration 11/25 | Loss: 0.00093723
Iteration 12/25 | Loss: 0.00093723
Iteration 13/25 | Loss: 0.00093723
Iteration 14/25 | Loss: 0.00093723
Iteration 15/25 | Loss: 0.00093723
Iteration 16/25 | Loss: 0.00093723
Iteration 17/25 | Loss: 0.00093723
Iteration 18/25 | Loss: 0.00093723
Iteration 19/25 | Loss: 0.00093723
Iteration 20/25 | Loss: 0.00093723
Iteration 21/25 | Loss: 0.00093723
Iteration 22/25 | Loss: 0.00093723
Iteration 23/25 | Loss: 0.00093723
Iteration 24/25 | Loss: 0.00093723
Iteration 25/25 | Loss: 0.00093723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093723
Iteration 2/1000 | Loss: 0.00002086
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001119
Iteration 5/1000 | Loss: 0.00001040
Iteration 6/1000 | Loss: 0.00000976
Iteration 7/1000 | Loss: 0.00000940
Iteration 8/1000 | Loss: 0.00000914
Iteration 9/1000 | Loss: 0.00000908
Iteration 10/1000 | Loss: 0.00000884
Iteration 11/1000 | Loss: 0.00000882
Iteration 12/1000 | Loss: 0.00000869
Iteration 13/1000 | Loss: 0.00000862
Iteration 14/1000 | Loss: 0.00000860
Iteration 15/1000 | Loss: 0.00000860
Iteration 16/1000 | Loss: 0.00000859
Iteration 17/1000 | Loss: 0.00000853
Iteration 18/1000 | Loss: 0.00000845
Iteration 19/1000 | Loss: 0.00000844
Iteration 20/1000 | Loss: 0.00000841
Iteration 21/1000 | Loss: 0.00000840
Iteration 22/1000 | Loss: 0.00000840
Iteration 23/1000 | Loss: 0.00000839
Iteration 24/1000 | Loss: 0.00000839
Iteration 25/1000 | Loss: 0.00000839
Iteration 26/1000 | Loss: 0.00000837
Iteration 27/1000 | Loss: 0.00000837
Iteration 28/1000 | Loss: 0.00000837
Iteration 29/1000 | Loss: 0.00000836
Iteration 30/1000 | Loss: 0.00000835
Iteration 31/1000 | Loss: 0.00000835
Iteration 32/1000 | Loss: 0.00000834
Iteration 33/1000 | Loss: 0.00000833
Iteration 34/1000 | Loss: 0.00000833
Iteration 35/1000 | Loss: 0.00000833
Iteration 36/1000 | Loss: 0.00000832
Iteration 37/1000 | Loss: 0.00000832
Iteration 38/1000 | Loss: 0.00000832
Iteration 39/1000 | Loss: 0.00000832
Iteration 40/1000 | Loss: 0.00000831
Iteration 41/1000 | Loss: 0.00000831
Iteration 42/1000 | Loss: 0.00000831
Iteration 43/1000 | Loss: 0.00000831
Iteration 44/1000 | Loss: 0.00000830
Iteration 45/1000 | Loss: 0.00000830
Iteration 46/1000 | Loss: 0.00000830
Iteration 47/1000 | Loss: 0.00000829
Iteration 48/1000 | Loss: 0.00000829
Iteration 49/1000 | Loss: 0.00000828
Iteration 50/1000 | Loss: 0.00000827
Iteration 51/1000 | Loss: 0.00000827
Iteration 52/1000 | Loss: 0.00000826
Iteration 53/1000 | Loss: 0.00000825
Iteration 54/1000 | Loss: 0.00000825
Iteration 55/1000 | Loss: 0.00000823
Iteration 56/1000 | Loss: 0.00000823
Iteration 57/1000 | Loss: 0.00000823
Iteration 58/1000 | Loss: 0.00000822
Iteration 59/1000 | Loss: 0.00000822
Iteration 60/1000 | Loss: 0.00000822
Iteration 61/1000 | Loss: 0.00000822
Iteration 62/1000 | Loss: 0.00000822
Iteration 63/1000 | Loss: 0.00000822
Iteration 64/1000 | Loss: 0.00000822
Iteration 65/1000 | Loss: 0.00000822
Iteration 66/1000 | Loss: 0.00000822
Iteration 67/1000 | Loss: 0.00000822
Iteration 68/1000 | Loss: 0.00000822
Iteration 69/1000 | Loss: 0.00000822
Iteration 70/1000 | Loss: 0.00000822
Iteration 71/1000 | Loss: 0.00000822
Iteration 72/1000 | Loss: 0.00000822
Iteration 73/1000 | Loss: 0.00000822
Iteration 74/1000 | Loss: 0.00000822
Iteration 75/1000 | Loss: 0.00000822
Iteration 76/1000 | Loss: 0.00000822
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000822
Iteration 79/1000 | Loss: 0.00000822
Iteration 80/1000 | Loss: 0.00000822
Iteration 81/1000 | Loss: 0.00000822
Iteration 82/1000 | Loss: 0.00000822
Iteration 83/1000 | Loss: 0.00000822
Iteration 84/1000 | Loss: 0.00000822
Iteration 85/1000 | Loss: 0.00000822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [8.21791218186263e-06, 8.21791218186263e-06, 8.21791218186263e-06, 8.21791218186263e-06, 8.21791218186263e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.21791218186263e-06

Optimization complete. Final v2v error: 2.4419353008270264 mm

Highest mean error: 2.6379177570343018 mm for frame 30

Lowest mean error: 2.3288469314575195 mm for frame 155

Saving results

Total time: 32.745351791381836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800672
Iteration 2/25 | Loss: 0.00125472
Iteration 3/25 | Loss: 0.00109433
Iteration 4/25 | Loss: 0.00107691
Iteration 5/25 | Loss: 0.00107180
Iteration 6/25 | Loss: 0.00107058
Iteration 7/25 | Loss: 0.00107056
Iteration 8/25 | Loss: 0.00107056
Iteration 9/25 | Loss: 0.00107056
Iteration 10/25 | Loss: 0.00107056
Iteration 11/25 | Loss: 0.00107056
Iteration 12/25 | Loss: 0.00107056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010705621680244803, 0.0010705621680244803, 0.0010705621680244803, 0.0010705621680244803, 0.0010705621680244803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010705621680244803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22406411
Iteration 2/25 | Loss: 0.00075306
Iteration 3/25 | Loss: 0.00075301
Iteration 4/25 | Loss: 0.00075301
Iteration 5/25 | Loss: 0.00075301
Iteration 6/25 | Loss: 0.00075301
Iteration 7/25 | Loss: 0.00075300
Iteration 8/25 | Loss: 0.00075300
Iteration 9/25 | Loss: 0.00075300
Iteration 10/25 | Loss: 0.00075300
Iteration 11/25 | Loss: 0.00075300
Iteration 12/25 | Loss: 0.00075300
Iteration 13/25 | Loss: 0.00075300
Iteration 14/25 | Loss: 0.00075300
Iteration 15/25 | Loss: 0.00075300
Iteration 16/25 | Loss: 0.00075300
Iteration 17/25 | Loss: 0.00075300
Iteration 18/25 | Loss: 0.00075300
Iteration 19/25 | Loss: 0.00075300
Iteration 20/25 | Loss: 0.00075300
Iteration 21/25 | Loss: 0.00075300
Iteration 22/25 | Loss: 0.00075300
Iteration 23/25 | Loss: 0.00075300
Iteration 24/25 | Loss: 0.00075300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007530036382377148, 0.0007530036382377148, 0.0007530036382377148, 0.0007530036382377148, 0.0007530036382377148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007530036382377148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075300
Iteration 2/1000 | Loss: 0.00004661
Iteration 3/1000 | Loss: 0.00002617
Iteration 4/1000 | Loss: 0.00002089
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001498
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001472
Iteration 18/1000 | Loss: 0.00001470
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001455
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001454
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001445
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001440
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001439
Iteration 92/1000 | Loss: 0.00001439
Iteration 93/1000 | Loss: 0.00001439
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001438
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001438
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00001438
Iteration 103/1000 | Loss: 0.00001438
Iteration 104/1000 | Loss: 0.00001438
Iteration 105/1000 | Loss: 0.00001437
Iteration 106/1000 | Loss: 0.00001437
Iteration 107/1000 | Loss: 0.00001437
Iteration 108/1000 | Loss: 0.00001437
Iteration 109/1000 | Loss: 0.00001437
Iteration 110/1000 | Loss: 0.00001437
Iteration 111/1000 | Loss: 0.00001437
Iteration 112/1000 | Loss: 0.00001437
Iteration 113/1000 | Loss: 0.00001437
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001436
Iteration 116/1000 | Loss: 0.00001436
Iteration 117/1000 | Loss: 0.00001436
Iteration 118/1000 | Loss: 0.00001436
Iteration 119/1000 | Loss: 0.00001436
Iteration 120/1000 | Loss: 0.00001436
Iteration 121/1000 | Loss: 0.00001436
Iteration 122/1000 | Loss: 0.00001436
Iteration 123/1000 | Loss: 0.00001436
Iteration 124/1000 | Loss: 0.00001436
Iteration 125/1000 | Loss: 0.00001436
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001435
Iteration 128/1000 | Loss: 0.00001435
Iteration 129/1000 | Loss: 0.00001435
Iteration 130/1000 | Loss: 0.00001435
Iteration 131/1000 | Loss: 0.00001435
Iteration 132/1000 | Loss: 0.00001435
Iteration 133/1000 | Loss: 0.00001435
Iteration 134/1000 | Loss: 0.00001435
Iteration 135/1000 | Loss: 0.00001435
Iteration 136/1000 | Loss: 0.00001435
Iteration 137/1000 | Loss: 0.00001435
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4351840036397334e-05, 1.4351840036397334e-05, 1.4351840036397334e-05, 1.4351840036397334e-05, 1.4351840036397334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4351840036397334e-05

Optimization complete. Final v2v error: 3.1403539180755615 mm

Highest mean error: 4.4216084480285645 mm for frame 54

Lowest mean error: 2.4593992233276367 mm for frame 84

Saving results

Total time: 37.9910843372345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503030
Iteration 2/25 | Loss: 0.00118092
Iteration 3/25 | Loss: 0.00105526
Iteration 4/25 | Loss: 0.00104024
Iteration 5/25 | Loss: 0.00103609
Iteration 6/25 | Loss: 0.00103561
Iteration 7/25 | Loss: 0.00103561
Iteration 8/25 | Loss: 0.00103561
Iteration 9/25 | Loss: 0.00103561
Iteration 10/25 | Loss: 0.00103561
Iteration 11/25 | Loss: 0.00103561
Iteration 12/25 | Loss: 0.00103561
Iteration 13/25 | Loss: 0.00103561
Iteration 14/25 | Loss: 0.00103561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010356097482144833, 0.0010356097482144833, 0.0010356097482144833, 0.0010356097482144833, 0.0010356097482144833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010356097482144833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73476863
Iteration 2/25 | Loss: 0.00098214
Iteration 3/25 | Loss: 0.00098213
Iteration 4/25 | Loss: 0.00098213
Iteration 5/25 | Loss: 0.00098213
Iteration 6/25 | Loss: 0.00098213
Iteration 7/25 | Loss: 0.00098213
Iteration 8/25 | Loss: 0.00098213
Iteration 9/25 | Loss: 0.00098213
Iteration 10/25 | Loss: 0.00098213
Iteration 11/25 | Loss: 0.00098213
Iteration 12/25 | Loss: 0.00098213
Iteration 13/25 | Loss: 0.00098213
Iteration 14/25 | Loss: 0.00098213
Iteration 15/25 | Loss: 0.00098213
Iteration 16/25 | Loss: 0.00098213
Iteration 17/25 | Loss: 0.00098213
Iteration 18/25 | Loss: 0.00098213
Iteration 19/25 | Loss: 0.00098213
Iteration 20/25 | Loss: 0.00098213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009821299463510513, 0.0009821299463510513, 0.0009821299463510513, 0.0009821299463510513, 0.0009821299463510513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009821299463510513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098213
Iteration 2/1000 | Loss: 0.00002583
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001362
Iteration 16/1000 | Loss: 0.00001361
Iteration 17/1000 | Loss: 0.00001359
Iteration 18/1000 | Loss: 0.00001359
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001348
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001328
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.3280636267154478e-05, 1.3280636267154478e-05, 1.3280636267154478e-05, 1.3280636267154478e-05, 1.3280636267154478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3280636267154478e-05

Optimization complete. Final v2v error: 3.090555429458618 mm

Highest mean error: 3.467007875442505 mm for frame 158

Lowest mean error: 2.743687391281128 mm for frame 217

Saving results

Total time: 39.622761487960815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769434
Iteration 2/25 | Loss: 0.00151566
Iteration 3/25 | Loss: 0.00117491
Iteration 4/25 | Loss: 0.00111469
Iteration 5/25 | Loss: 0.00110475
Iteration 6/25 | Loss: 0.00110199
Iteration 7/25 | Loss: 0.00110138
Iteration 8/25 | Loss: 0.00110138
Iteration 9/25 | Loss: 0.00110138
Iteration 10/25 | Loss: 0.00110138
Iteration 11/25 | Loss: 0.00110138
Iteration 12/25 | Loss: 0.00110138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011013809125870466, 0.0011013809125870466, 0.0011013809125870466, 0.0011013809125870466, 0.0011013809125870466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011013809125870466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35233939
Iteration 2/25 | Loss: 0.00084898
Iteration 3/25 | Loss: 0.00084898
Iteration 4/25 | Loss: 0.00084898
Iteration 5/25 | Loss: 0.00084898
Iteration 6/25 | Loss: 0.00084898
Iteration 7/25 | Loss: 0.00084898
Iteration 8/25 | Loss: 0.00084898
Iteration 9/25 | Loss: 0.00084898
Iteration 10/25 | Loss: 0.00084898
Iteration 11/25 | Loss: 0.00084898
Iteration 12/25 | Loss: 0.00084898
Iteration 13/25 | Loss: 0.00084898
Iteration 14/25 | Loss: 0.00084898
Iteration 15/25 | Loss: 0.00084898
Iteration 16/25 | Loss: 0.00084898
Iteration 17/25 | Loss: 0.00084898
Iteration 18/25 | Loss: 0.00084898
Iteration 19/25 | Loss: 0.00084898
Iteration 20/25 | Loss: 0.00084898
Iteration 21/25 | Loss: 0.00084898
Iteration 22/25 | Loss: 0.00084898
Iteration 23/25 | Loss: 0.00084898
Iteration 24/25 | Loss: 0.00084898
Iteration 25/25 | Loss: 0.00084898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084898
Iteration 2/1000 | Loss: 0.00007490
Iteration 3/1000 | Loss: 0.00004619
Iteration 4/1000 | Loss: 0.00003063
Iteration 5/1000 | Loss: 0.00002781
Iteration 6/1000 | Loss: 0.00002665
Iteration 7/1000 | Loss: 0.00002570
Iteration 8/1000 | Loss: 0.00002511
Iteration 9/1000 | Loss: 0.00002476
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002420
Iteration 12/1000 | Loss: 0.00002403
Iteration 13/1000 | Loss: 0.00002385
Iteration 14/1000 | Loss: 0.00002365
Iteration 15/1000 | Loss: 0.00002362
Iteration 16/1000 | Loss: 0.00002361
Iteration 17/1000 | Loss: 0.00002354
Iteration 18/1000 | Loss: 0.00002353
Iteration 19/1000 | Loss: 0.00002353
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002352
Iteration 22/1000 | Loss: 0.00002348
Iteration 23/1000 | Loss: 0.00002347
Iteration 24/1000 | Loss: 0.00002346
Iteration 25/1000 | Loss: 0.00002346
Iteration 26/1000 | Loss: 0.00002345
Iteration 27/1000 | Loss: 0.00002345
Iteration 28/1000 | Loss: 0.00002344
Iteration 29/1000 | Loss: 0.00002343
Iteration 30/1000 | Loss: 0.00002343
Iteration 31/1000 | Loss: 0.00002341
Iteration 32/1000 | Loss: 0.00002341
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002341
Iteration 35/1000 | Loss: 0.00002340
Iteration 36/1000 | Loss: 0.00002338
Iteration 37/1000 | Loss: 0.00002337
Iteration 38/1000 | Loss: 0.00002337
Iteration 39/1000 | Loss: 0.00002337
Iteration 40/1000 | Loss: 0.00002336
Iteration 41/1000 | Loss: 0.00002335
Iteration 42/1000 | Loss: 0.00002334
Iteration 43/1000 | Loss: 0.00002334
Iteration 44/1000 | Loss: 0.00002333
Iteration 45/1000 | Loss: 0.00002332
Iteration 46/1000 | Loss: 0.00002332
Iteration 47/1000 | Loss: 0.00002332
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002330
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002330
Iteration 57/1000 | Loss: 0.00002329
Iteration 58/1000 | Loss: 0.00002329
Iteration 59/1000 | Loss: 0.00002329
Iteration 60/1000 | Loss: 0.00002329
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002328
Iteration 63/1000 | Loss: 0.00002328
Iteration 64/1000 | Loss: 0.00002328
Iteration 65/1000 | Loss: 0.00002328
Iteration 66/1000 | Loss: 0.00002328
Iteration 67/1000 | Loss: 0.00002328
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002327
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002325
Iteration 76/1000 | Loss: 0.00002325
Iteration 77/1000 | Loss: 0.00002325
Iteration 78/1000 | Loss: 0.00002325
Iteration 79/1000 | Loss: 0.00002325
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002325
Iteration 87/1000 | Loss: 0.00002325
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002322
Iteration 94/1000 | Loss: 0.00002322
Iteration 95/1000 | Loss: 0.00002322
Iteration 96/1000 | Loss: 0.00002322
Iteration 97/1000 | Loss: 0.00002322
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00002322
Iteration 100/1000 | Loss: 0.00002322
Iteration 101/1000 | Loss: 0.00002322
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002321
Iteration 106/1000 | Loss: 0.00002321
Iteration 107/1000 | Loss: 0.00002321
Iteration 108/1000 | Loss: 0.00002321
Iteration 109/1000 | Loss: 0.00002321
Iteration 110/1000 | Loss: 0.00002321
Iteration 111/1000 | Loss: 0.00002321
Iteration 112/1000 | Loss: 0.00002321
Iteration 113/1000 | Loss: 0.00002321
Iteration 114/1000 | Loss: 0.00002321
Iteration 115/1000 | Loss: 0.00002320
Iteration 116/1000 | Loss: 0.00002320
Iteration 117/1000 | Loss: 0.00002320
Iteration 118/1000 | Loss: 0.00002320
Iteration 119/1000 | Loss: 0.00002320
Iteration 120/1000 | Loss: 0.00002320
Iteration 121/1000 | Loss: 0.00002320
Iteration 122/1000 | Loss: 0.00002320
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002319
Iteration 128/1000 | Loss: 0.00002319
Iteration 129/1000 | Loss: 0.00002319
Iteration 130/1000 | Loss: 0.00002319
Iteration 131/1000 | Loss: 0.00002319
Iteration 132/1000 | Loss: 0.00002319
Iteration 133/1000 | Loss: 0.00002318
Iteration 134/1000 | Loss: 0.00002318
Iteration 135/1000 | Loss: 0.00002318
Iteration 136/1000 | Loss: 0.00002318
Iteration 137/1000 | Loss: 0.00002318
Iteration 138/1000 | Loss: 0.00002318
Iteration 139/1000 | Loss: 0.00002318
Iteration 140/1000 | Loss: 0.00002317
Iteration 141/1000 | Loss: 0.00002317
Iteration 142/1000 | Loss: 0.00002317
Iteration 143/1000 | Loss: 0.00002317
Iteration 144/1000 | Loss: 0.00002317
Iteration 145/1000 | Loss: 0.00002317
Iteration 146/1000 | Loss: 0.00002317
Iteration 147/1000 | Loss: 0.00002316
Iteration 148/1000 | Loss: 0.00002316
Iteration 149/1000 | Loss: 0.00002316
Iteration 150/1000 | Loss: 0.00002316
Iteration 151/1000 | Loss: 0.00002316
Iteration 152/1000 | Loss: 0.00002316
Iteration 153/1000 | Loss: 0.00002316
Iteration 154/1000 | Loss: 0.00002315
Iteration 155/1000 | Loss: 0.00002315
Iteration 156/1000 | Loss: 0.00002315
Iteration 157/1000 | Loss: 0.00002315
Iteration 158/1000 | Loss: 0.00002315
Iteration 159/1000 | Loss: 0.00002315
Iteration 160/1000 | Loss: 0.00002315
Iteration 161/1000 | Loss: 0.00002315
Iteration 162/1000 | Loss: 0.00002315
Iteration 163/1000 | Loss: 0.00002315
Iteration 164/1000 | Loss: 0.00002315
Iteration 165/1000 | Loss: 0.00002314
Iteration 166/1000 | Loss: 0.00002314
Iteration 167/1000 | Loss: 0.00002314
Iteration 168/1000 | Loss: 0.00002314
Iteration 169/1000 | Loss: 0.00002314
Iteration 170/1000 | Loss: 0.00002314
Iteration 171/1000 | Loss: 0.00002314
Iteration 172/1000 | Loss: 0.00002314
Iteration 173/1000 | Loss: 0.00002314
Iteration 174/1000 | Loss: 0.00002314
Iteration 175/1000 | Loss: 0.00002313
Iteration 176/1000 | Loss: 0.00002313
Iteration 177/1000 | Loss: 0.00002313
Iteration 178/1000 | Loss: 0.00002313
Iteration 179/1000 | Loss: 0.00002313
Iteration 180/1000 | Loss: 0.00002313
Iteration 181/1000 | Loss: 0.00002313
Iteration 182/1000 | Loss: 0.00002313
Iteration 183/1000 | Loss: 0.00002313
Iteration 184/1000 | Loss: 0.00002313
Iteration 185/1000 | Loss: 0.00002313
Iteration 186/1000 | Loss: 0.00002313
Iteration 187/1000 | Loss: 0.00002313
Iteration 188/1000 | Loss: 0.00002313
Iteration 189/1000 | Loss: 0.00002312
Iteration 190/1000 | Loss: 0.00002312
Iteration 191/1000 | Loss: 0.00002312
Iteration 192/1000 | Loss: 0.00002312
Iteration 193/1000 | Loss: 0.00002312
Iteration 194/1000 | Loss: 0.00002312
Iteration 195/1000 | Loss: 0.00002312
Iteration 196/1000 | Loss: 0.00002312
Iteration 197/1000 | Loss: 0.00002312
Iteration 198/1000 | Loss: 0.00002312
Iteration 199/1000 | Loss: 0.00002312
Iteration 200/1000 | Loss: 0.00002312
Iteration 201/1000 | Loss: 0.00002312
Iteration 202/1000 | Loss: 0.00002312
Iteration 203/1000 | Loss: 0.00002312
Iteration 204/1000 | Loss: 0.00002312
Iteration 205/1000 | Loss: 0.00002312
Iteration 206/1000 | Loss: 0.00002311
Iteration 207/1000 | Loss: 0.00002311
Iteration 208/1000 | Loss: 0.00002311
Iteration 209/1000 | Loss: 0.00002311
Iteration 210/1000 | Loss: 0.00002311
Iteration 211/1000 | Loss: 0.00002311
Iteration 212/1000 | Loss: 0.00002311
Iteration 213/1000 | Loss: 0.00002311
Iteration 214/1000 | Loss: 0.00002311
Iteration 215/1000 | Loss: 0.00002311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.311392017873004e-05, 2.311392017873004e-05, 2.311392017873004e-05, 2.311392017873004e-05, 2.311392017873004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.311392017873004e-05

Optimization complete. Final v2v error: 3.9458274841308594 mm

Highest mean error: 5.5363078117370605 mm for frame 27

Lowest mean error: 2.961911678314209 mm for frame 0

Saving results

Total time: 46.79790115356445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00214324
Iteration 2/25 | Loss: 0.00112104
Iteration 3/25 | Loss: 0.00100998
Iteration 4/25 | Loss: 0.00098970
Iteration 5/25 | Loss: 0.00098260
Iteration 6/25 | Loss: 0.00098063
Iteration 7/25 | Loss: 0.00097982
Iteration 8/25 | Loss: 0.00097981
Iteration 9/25 | Loss: 0.00097981
Iteration 10/25 | Loss: 0.00097981
Iteration 11/25 | Loss: 0.00097981
Iteration 12/25 | Loss: 0.00097981
Iteration 13/25 | Loss: 0.00097981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009798111859709024, 0.0009798111859709024, 0.0009798111859709024, 0.0009798111859709024, 0.0009798111859709024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009798111859709024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26153147
Iteration 2/25 | Loss: 0.00199955
Iteration 3/25 | Loss: 0.00199955
Iteration 4/25 | Loss: 0.00199955
Iteration 5/25 | Loss: 0.00199955
Iteration 6/25 | Loss: 0.00199955
Iteration 7/25 | Loss: 0.00199955
Iteration 8/25 | Loss: 0.00199955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.001999550499022007, 0.001999550499022007, 0.001999550499022007, 0.001999550499022007, 0.001999550499022007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001999550499022007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199955
Iteration 2/1000 | Loss: 0.00003544
Iteration 3/1000 | Loss: 0.00002122
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001270
Iteration 6/1000 | Loss: 0.00001188
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001082
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001064
Iteration 13/1000 | Loss: 0.00001060
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001059
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001046
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001044
Iteration 24/1000 | Loss: 0.00001044
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001043
Iteration 27/1000 | Loss: 0.00001042
Iteration 28/1000 | Loss: 0.00001042
Iteration 29/1000 | Loss: 0.00001042
Iteration 30/1000 | Loss: 0.00001041
Iteration 31/1000 | Loss: 0.00001041
Iteration 32/1000 | Loss: 0.00001041
Iteration 33/1000 | Loss: 0.00001040
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001037
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001036
Iteration 41/1000 | Loss: 0.00001036
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001035
Iteration 44/1000 | Loss: 0.00001035
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001034
Iteration 47/1000 | Loss: 0.00001034
Iteration 48/1000 | Loss: 0.00001034
Iteration 49/1000 | Loss: 0.00001033
Iteration 50/1000 | Loss: 0.00001033
Iteration 51/1000 | Loss: 0.00001033
Iteration 52/1000 | Loss: 0.00001033
Iteration 53/1000 | Loss: 0.00001032
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001031
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001030
Iteration 58/1000 | Loss: 0.00001030
Iteration 59/1000 | Loss: 0.00001030
Iteration 60/1000 | Loss: 0.00001030
Iteration 61/1000 | Loss: 0.00001029
Iteration 62/1000 | Loss: 0.00001029
Iteration 63/1000 | Loss: 0.00001029
Iteration 64/1000 | Loss: 0.00001028
Iteration 65/1000 | Loss: 0.00001028
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001025
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001024
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.0221442607871722e-05, 1.0221442607871722e-05, 1.0221442607871722e-05, 1.0221442607871722e-05, 1.0221442607871722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0221442607871722e-05

Optimization complete. Final v2v error: 2.7305800914764404 mm

Highest mean error: 3.0719962120056152 mm for frame 109

Lowest mean error: 2.4063403606414795 mm for frame 120

Saving results

Total time: 32.576579570770264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721221
Iteration 2/25 | Loss: 0.00121685
Iteration 3/25 | Loss: 0.00111087
Iteration 4/25 | Loss: 0.00110589
Iteration 5/25 | Loss: 0.00110419
Iteration 6/25 | Loss: 0.00110406
Iteration 7/25 | Loss: 0.00110406
Iteration 8/25 | Loss: 0.00110406
Iteration 9/25 | Loss: 0.00110406
Iteration 10/25 | Loss: 0.00110406
Iteration 11/25 | Loss: 0.00110406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011040575336664915, 0.0011040575336664915, 0.0011040575336664915, 0.0011040575336664915, 0.0011040575336664915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011040575336664915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24310601
Iteration 2/25 | Loss: 0.00106375
Iteration 3/25 | Loss: 0.00106374
Iteration 4/25 | Loss: 0.00106374
Iteration 5/25 | Loss: 0.00106374
Iteration 6/25 | Loss: 0.00106374
Iteration 7/25 | Loss: 0.00106374
Iteration 8/25 | Loss: 0.00106374
Iteration 9/25 | Loss: 0.00106374
Iteration 10/25 | Loss: 0.00106373
Iteration 11/25 | Loss: 0.00106373
Iteration 12/25 | Loss: 0.00106374
Iteration 13/25 | Loss: 0.00106373
Iteration 14/25 | Loss: 0.00106373
Iteration 15/25 | Loss: 0.00106373
Iteration 16/25 | Loss: 0.00106373
Iteration 17/25 | Loss: 0.00106373
Iteration 18/25 | Loss: 0.00106373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001063734875060618, 0.001063734875060618, 0.001063734875060618, 0.001063734875060618, 0.001063734875060618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001063734875060618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106373
Iteration 2/1000 | Loss: 0.00002946
Iteration 3/1000 | Loss: 0.00002172
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001634
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001569
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001566
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001553
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001553
Iteration 30/1000 | Loss: 0.00001552
Iteration 31/1000 | Loss: 0.00001552
Iteration 32/1000 | Loss: 0.00001552
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001550
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001550
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001549
Iteration 42/1000 | Loss: 0.00001549
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001547
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.546122439322062e-05, 1.546122439322062e-05, 1.546122439322062e-05, 1.546122439322062e-05, 1.546122439322062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.546122439322062e-05

Optimization complete. Final v2v error: 3.253844738006592 mm

Highest mean error: 3.511171579360962 mm for frame 26

Lowest mean error: 3.0444462299346924 mm for frame 78

Saving results

Total time: 29.29244303703308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551529
Iteration 2/25 | Loss: 0.00130770
Iteration 3/25 | Loss: 0.00108002
Iteration 4/25 | Loss: 0.00104580
Iteration 5/25 | Loss: 0.00103987
Iteration 6/25 | Loss: 0.00103774
Iteration 7/25 | Loss: 0.00103711
Iteration 8/25 | Loss: 0.00103701
Iteration 9/25 | Loss: 0.00103699
Iteration 10/25 | Loss: 0.00103699
Iteration 11/25 | Loss: 0.00103699
Iteration 12/25 | Loss: 0.00103699
Iteration 13/25 | Loss: 0.00103699
Iteration 14/25 | Loss: 0.00103699
Iteration 15/25 | Loss: 0.00103698
Iteration 16/25 | Loss: 0.00103698
Iteration 17/25 | Loss: 0.00103698
Iteration 18/25 | Loss: 0.00103698
Iteration 19/25 | Loss: 0.00103698
Iteration 20/25 | Loss: 0.00103698
Iteration 21/25 | Loss: 0.00103698
Iteration 22/25 | Loss: 0.00103698
Iteration 23/25 | Loss: 0.00103698
Iteration 24/25 | Loss: 0.00103698
Iteration 25/25 | Loss: 0.00103698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010369836818426847, 0.0010369836818426847, 0.0010369836818426847, 0.0010369836818426847, 0.0010369836818426847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010369836818426847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.18078232
Iteration 2/25 | Loss: 0.00092857
Iteration 3/25 | Loss: 0.00092857
Iteration 4/25 | Loss: 0.00092857
Iteration 5/25 | Loss: 0.00092857
Iteration 6/25 | Loss: 0.00092857
Iteration 7/25 | Loss: 0.00092856
Iteration 8/25 | Loss: 0.00092856
Iteration 9/25 | Loss: 0.00092856
Iteration 10/25 | Loss: 0.00092856
Iteration 11/25 | Loss: 0.00092856
Iteration 12/25 | Loss: 0.00092856
Iteration 13/25 | Loss: 0.00092856
Iteration 14/25 | Loss: 0.00092856
Iteration 15/25 | Loss: 0.00092856
Iteration 16/25 | Loss: 0.00092856
Iteration 17/25 | Loss: 0.00092856
Iteration 18/25 | Loss: 0.00092856
Iteration 19/25 | Loss: 0.00092856
Iteration 20/25 | Loss: 0.00092856
Iteration 21/25 | Loss: 0.00092856
Iteration 22/25 | Loss: 0.00092856
Iteration 23/25 | Loss: 0.00092856
Iteration 24/25 | Loss: 0.00092856
Iteration 25/25 | Loss: 0.00092856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092856
Iteration 2/1000 | Loss: 0.00001974
Iteration 3/1000 | Loss: 0.00007631
Iteration 4/1000 | Loss: 0.00021610
Iteration 5/1000 | Loss: 0.00005471
Iteration 6/1000 | Loss: 0.00009715
Iteration 7/1000 | Loss: 0.00002487
Iteration 8/1000 | Loss: 0.00001132
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001057
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001025
Iteration 13/1000 | Loss: 0.00002520
Iteration 14/1000 | Loss: 0.00008198
Iteration 15/1000 | Loss: 0.00002684
Iteration 16/1000 | Loss: 0.00001187
Iteration 17/1000 | Loss: 0.00000996
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000990
Iteration 27/1000 | Loss: 0.00000990
Iteration 28/1000 | Loss: 0.00000989
Iteration 29/1000 | Loss: 0.00000989
Iteration 30/1000 | Loss: 0.00000989
Iteration 31/1000 | Loss: 0.00000988
Iteration 32/1000 | Loss: 0.00000988
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000987
Iteration 35/1000 | Loss: 0.00000987
Iteration 36/1000 | Loss: 0.00000987
Iteration 37/1000 | Loss: 0.00000987
Iteration 38/1000 | Loss: 0.00000987
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000986
Iteration 41/1000 | Loss: 0.00000986
Iteration 42/1000 | Loss: 0.00000986
Iteration 43/1000 | Loss: 0.00000986
Iteration 44/1000 | Loss: 0.00000985
Iteration 45/1000 | Loss: 0.00000985
Iteration 46/1000 | Loss: 0.00000984
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000984
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000984
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000983
Iteration 53/1000 | Loss: 0.00000983
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000982
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000981
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00005551
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000980
Iteration 64/1000 | Loss: 0.00000978
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000978
Iteration 67/1000 | Loss: 0.00000978
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000978
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000977
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000975
Iteration 80/1000 | Loss: 0.00000974
Iteration 81/1000 | Loss: 0.00000974
Iteration 82/1000 | Loss: 0.00000974
Iteration 83/1000 | Loss: 0.00000974
Iteration 84/1000 | Loss: 0.00000974
Iteration 85/1000 | Loss: 0.00000974
Iteration 86/1000 | Loss: 0.00000974
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000974
Iteration 89/1000 | Loss: 0.00000974
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00000973
Iteration 95/1000 | Loss: 0.00000973
Iteration 96/1000 | Loss: 0.00000973
Iteration 97/1000 | Loss: 0.00000973
Iteration 98/1000 | Loss: 0.00000973
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000973
Iteration 101/1000 | Loss: 0.00000973
Iteration 102/1000 | Loss: 0.00000973
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000972
Iteration 107/1000 | Loss: 0.00000972
Iteration 108/1000 | Loss: 0.00000972
Iteration 109/1000 | Loss: 0.00000972
Iteration 110/1000 | Loss: 0.00000972
Iteration 111/1000 | Loss: 0.00000972
Iteration 112/1000 | Loss: 0.00000972
Iteration 113/1000 | Loss: 0.00000972
Iteration 114/1000 | Loss: 0.00000972
Iteration 115/1000 | Loss: 0.00000972
Iteration 116/1000 | Loss: 0.00000972
Iteration 117/1000 | Loss: 0.00000972
Iteration 118/1000 | Loss: 0.00000972
Iteration 119/1000 | Loss: 0.00000972
Iteration 120/1000 | Loss: 0.00000972
Iteration 121/1000 | Loss: 0.00000972
Iteration 122/1000 | Loss: 0.00000972
Iteration 123/1000 | Loss: 0.00000972
Iteration 124/1000 | Loss: 0.00000972
Iteration 125/1000 | Loss: 0.00000972
Iteration 126/1000 | Loss: 0.00000972
Iteration 127/1000 | Loss: 0.00000972
Iteration 128/1000 | Loss: 0.00000972
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000972
Iteration 133/1000 | Loss: 0.00000972
Iteration 134/1000 | Loss: 0.00000972
Iteration 135/1000 | Loss: 0.00000972
Iteration 136/1000 | Loss: 0.00000972
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000972
Iteration 140/1000 | Loss: 0.00000972
Iteration 141/1000 | Loss: 0.00000972
Iteration 142/1000 | Loss: 0.00000972
Iteration 143/1000 | Loss: 0.00000972
Iteration 144/1000 | Loss: 0.00000972
Iteration 145/1000 | Loss: 0.00000972
Iteration 146/1000 | Loss: 0.00000972
Iteration 147/1000 | Loss: 0.00000972
Iteration 148/1000 | Loss: 0.00000972
Iteration 149/1000 | Loss: 0.00000972
Iteration 150/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [9.720155503600836e-06, 9.720155503600836e-06, 9.720155503600836e-06, 9.720155503600836e-06, 9.720155503600836e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.720155503600836e-06

Optimization complete. Final v2v error: 2.6543028354644775 mm

Highest mean error: 3.0095269680023193 mm for frame 189

Lowest mean error: 2.4239742755889893 mm for frame 176

Saving results

Total time: 52.56192231178284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913959
Iteration 2/25 | Loss: 0.00238971
Iteration 3/25 | Loss: 0.00172421
Iteration 4/25 | Loss: 0.00169912
Iteration 5/25 | Loss: 0.00154681
Iteration 6/25 | Loss: 0.00142207
Iteration 7/25 | Loss: 0.00133735
Iteration 8/25 | Loss: 0.00131172
Iteration 9/25 | Loss: 0.00129621
Iteration 10/25 | Loss: 0.00128337
Iteration 11/25 | Loss: 0.00127476
Iteration 12/25 | Loss: 0.00126737
Iteration 13/25 | Loss: 0.00127366
Iteration 14/25 | Loss: 0.00125984
Iteration 15/25 | Loss: 0.00121752
Iteration 16/25 | Loss: 0.00122202
Iteration 17/25 | Loss: 0.00119192
Iteration 18/25 | Loss: 0.00118293
Iteration 19/25 | Loss: 0.00116438
Iteration 20/25 | Loss: 0.00116568
Iteration 21/25 | Loss: 0.00115366
Iteration 22/25 | Loss: 0.00115198
Iteration 23/25 | Loss: 0.00115123
Iteration 24/25 | Loss: 0.00115238
Iteration 25/25 | Loss: 0.00114846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26951706
Iteration 2/25 | Loss: 0.00116971
Iteration 3/25 | Loss: 0.00106082
Iteration 4/25 | Loss: 0.00106082
Iteration 5/25 | Loss: 0.00106082
Iteration 6/25 | Loss: 0.00106082
Iteration 7/25 | Loss: 0.00106082
Iteration 8/25 | Loss: 0.00106082
Iteration 9/25 | Loss: 0.00106082
Iteration 10/25 | Loss: 0.00106082
Iteration 11/25 | Loss: 0.00106082
Iteration 12/25 | Loss: 0.00106082
Iteration 13/25 | Loss: 0.00106082
Iteration 14/25 | Loss: 0.00106082
Iteration 15/25 | Loss: 0.00106082
Iteration 16/25 | Loss: 0.00106082
Iteration 17/25 | Loss: 0.00106082
Iteration 18/25 | Loss: 0.00106082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010608193697407842, 0.0010608193697407842, 0.0010608193697407842, 0.0010608193697407842, 0.0010608193697407842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010608193697407842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106082
Iteration 2/1000 | Loss: 0.00032171
Iteration 3/1000 | Loss: 0.00003786
Iteration 4/1000 | Loss: 0.00002913
Iteration 5/1000 | Loss: 0.00002426
Iteration 6/1000 | Loss: 0.00002195
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001961
Iteration 10/1000 | Loss: 0.00009318
Iteration 11/1000 | Loss: 0.00005579
Iteration 12/1000 | Loss: 0.00002390
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001813
Iteration 15/1000 | Loss: 0.00001765
Iteration 16/1000 | Loss: 0.00001733
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00012049
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001666
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001636
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00008658
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001624
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001623
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001621
Iteration 42/1000 | Loss: 0.00001621
Iteration 43/1000 | Loss: 0.00001621
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001621
Iteration 46/1000 | Loss: 0.00001621
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [1.620665716473013e-05, 1.620665716473013e-05, 1.620665716473013e-05, 1.620665716473013e-05, 1.620665716473013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.620665716473013e-05

Optimization complete. Final v2v error: 3.3498189449310303 mm

Highest mean error: 4.560545444488525 mm for frame 146

Lowest mean error: 3.1341052055358887 mm for frame 28

Saving results

Total time: 95.37278270721436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443659
Iteration 2/25 | Loss: 0.00113184
Iteration 3/25 | Loss: 0.00105004
Iteration 4/25 | Loss: 0.00103881
Iteration 5/25 | Loss: 0.00103590
Iteration 6/25 | Loss: 0.00103559
Iteration 7/25 | Loss: 0.00103559
Iteration 8/25 | Loss: 0.00103559
Iteration 9/25 | Loss: 0.00103559
Iteration 10/25 | Loss: 0.00103559
Iteration 11/25 | Loss: 0.00103559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001035588327795267, 0.001035588327795267, 0.001035588327795267, 0.001035588327795267, 0.001035588327795267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001035588327795267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63074303
Iteration 2/25 | Loss: 0.00092872
Iteration 3/25 | Loss: 0.00092872
Iteration 4/25 | Loss: 0.00092871
Iteration 5/25 | Loss: 0.00092871
Iteration 6/25 | Loss: 0.00092871
Iteration 7/25 | Loss: 0.00092871
Iteration 8/25 | Loss: 0.00092871
Iteration 9/25 | Loss: 0.00092871
Iteration 10/25 | Loss: 0.00092871
Iteration 11/25 | Loss: 0.00092871
Iteration 12/25 | Loss: 0.00092871
Iteration 13/25 | Loss: 0.00092871
Iteration 14/25 | Loss: 0.00092871
Iteration 15/25 | Loss: 0.00092871
Iteration 16/25 | Loss: 0.00092871
Iteration 17/25 | Loss: 0.00092871
Iteration 18/25 | Loss: 0.00092871
Iteration 19/25 | Loss: 0.00092871
Iteration 20/25 | Loss: 0.00092871
Iteration 21/25 | Loss: 0.00092871
Iteration 22/25 | Loss: 0.00092871
Iteration 23/25 | Loss: 0.00092871
Iteration 24/25 | Loss: 0.00092871
Iteration 25/25 | Loss: 0.00092871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092871
Iteration 2/1000 | Loss: 0.00001823
Iteration 3/1000 | Loss: 0.00001399
Iteration 4/1000 | Loss: 0.00001310
Iteration 5/1000 | Loss: 0.00001263
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00001190
Iteration 8/1000 | Loss: 0.00001168
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001106
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001098
Iteration 18/1000 | Loss: 0.00001098
Iteration 19/1000 | Loss: 0.00001094
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001094
Iteration 23/1000 | Loss: 0.00001094
Iteration 24/1000 | Loss: 0.00001094
Iteration 25/1000 | Loss: 0.00001093
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001092
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001082
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001080
Iteration 44/1000 | Loss: 0.00001080
Iteration 45/1000 | Loss: 0.00001080
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001079
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001076
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001076
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001072
Iteration 75/1000 | Loss: 0.00001072
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001070
Iteration 93/1000 | Loss: 0.00001070
Iteration 94/1000 | Loss: 0.00001070
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001069
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001069
Iteration 100/1000 | Loss: 0.00001068
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001068
Iteration 103/1000 | Loss: 0.00001068
Iteration 104/1000 | Loss: 0.00001068
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001065
Iteration 121/1000 | Loss: 0.00001065
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001065
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001065
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001064
Iteration 144/1000 | Loss: 0.00001064
Iteration 145/1000 | Loss: 0.00001064
Iteration 146/1000 | Loss: 0.00001064
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001063
Iteration 158/1000 | Loss: 0.00001063
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001062
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001062
Iteration 172/1000 | Loss: 0.00001062
Iteration 173/1000 | Loss: 0.00001062
Iteration 174/1000 | Loss: 0.00001062
Iteration 175/1000 | Loss: 0.00001062
Iteration 176/1000 | Loss: 0.00001062
Iteration 177/1000 | Loss: 0.00001062
Iteration 178/1000 | Loss: 0.00001062
Iteration 179/1000 | Loss: 0.00001062
Iteration 180/1000 | Loss: 0.00001062
Iteration 181/1000 | Loss: 0.00001062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.0620806278893724e-05, 1.0620806278893724e-05, 1.0620806278893724e-05, 1.0620806278893724e-05, 1.0620806278893724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0620806278893724e-05

Optimization complete. Final v2v error: 2.749497652053833 mm

Highest mean error: 3.2455315589904785 mm for frame 87

Lowest mean error: 2.434033155441284 mm for frame 38

Saving results

Total time: 38.83164381980896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488286
Iteration 2/25 | Loss: 0.00125000
Iteration 3/25 | Loss: 0.00106795
Iteration 4/25 | Loss: 0.00104777
Iteration 5/25 | Loss: 0.00104012
Iteration 6/25 | Loss: 0.00103833
Iteration 7/25 | Loss: 0.00103833
Iteration 8/25 | Loss: 0.00103833
Iteration 9/25 | Loss: 0.00103833
Iteration 10/25 | Loss: 0.00103833
Iteration 11/25 | Loss: 0.00103833
Iteration 12/25 | Loss: 0.00103833
Iteration 13/25 | Loss: 0.00103833
Iteration 14/25 | Loss: 0.00103833
Iteration 15/25 | Loss: 0.00103833
Iteration 16/25 | Loss: 0.00103833
Iteration 17/25 | Loss: 0.00103833
Iteration 18/25 | Loss: 0.00103833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010383309563621879, 0.0010383309563621879, 0.0010383309563621879, 0.0010383309563621879, 0.0010383309563621879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010383309563621879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73611867
Iteration 2/25 | Loss: 0.00094949
Iteration 3/25 | Loss: 0.00094949
Iteration 4/25 | Loss: 0.00094949
Iteration 5/25 | Loss: 0.00094949
Iteration 6/25 | Loss: 0.00094949
Iteration 7/25 | Loss: 0.00094949
Iteration 8/25 | Loss: 0.00094949
Iteration 9/25 | Loss: 0.00094949
Iteration 10/25 | Loss: 0.00094949
Iteration 11/25 | Loss: 0.00094949
Iteration 12/25 | Loss: 0.00094949
Iteration 13/25 | Loss: 0.00094949
Iteration 14/25 | Loss: 0.00094949
Iteration 15/25 | Loss: 0.00094949
Iteration 16/25 | Loss: 0.00094949
Iteration 17/25 | Loss: 0.00094949
Iteration 18/25 | Loss: 0.00094949
Iteration 19/25 | Loss: 0.00094949
Iteration 20/25 | Loss: 0.00094949
Iteration 21/25 | Loss: 0.00094949
Iteration 22/25 | Loss: 0.00094949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009494858677498996, 0.0009494858677498996, 0.0009494858677498996, 0.0009494858677498996, 0.0009494858677498996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009494858677498996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094949
Iteration 2/1000 | Loss: 0.00003615
Iteration 3/1000 | Loss: 0.00002480
Iteration 4/1000 | Loss: 0.00002243
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001913
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001801
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001782
Iteration 21/1000 | Loss: 0.00001782
Iteration 22/1000 | Loss: 0.00001782
Iteration 23/1000 | Loss: 0.00001782
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001781
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001775
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001765
Iteration 39/1000 | Loss: 0.00001765
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001762
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001752
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001749
Iteration 70/1000 | Loss: 0.00001749
Iteration 71/1000 | Loss: 0.00001748
Iteration 72/1000 | Loss: 0.00001748
Iteration 73/1000 | Loss: 0.00001748
Iteration 74/1000 | Loss: 0.00001748
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001744
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001739
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001738
Iteration 109/1000 | Loss: 0.00001738
Iteration 110/1000 | Loss: 0.00001737
Iteration 111/1000 | Loss: 0.00001737
Iteration 112/1000 | Loss: 0.00001737
Iteration 113/1000 | Loss: 0.00001736
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001734
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001727
Iteration 121/1000 | Loss: 0.00001727
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001723
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001722
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001719
Iteration 141/1000 | Loss: 0.00001719
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001718
Iteration 144/1000 | Loss: 0.00001718
Iteration 145/1000 | Loss: 0.00001718
Iteration 146/1000 | Loss: 0.00001718
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001717
Iteration 152/1000 | Loss: 0.00001717
Iteration 153/1000 | Loss: 0.00001717
Iteration 154/1000 | Loss: 0.00001717
Iteration 155/1000 | Loss: 0.00001717
Iteration 156/1000 | Loss: 0.00001717
Iteration 157/1000 | Loss: 0.00001717
Iteration 158/1000 | Loss: 0.00001717
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001714
Iteration 175/1000 | Loss: 0.00001714
Iteration 176/1000 | Loss: 0.00001714
Iteration 177/1000 | Loss: 0.00001714
Iteration 178/1000 | Loss: 0.00001714
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001713
Iteration 181/1000 | Loss: 0.00001713
Iteration 182/1000 | Loss: 0.00001713
Iteration 183/1000 | Loss: 0.00001713
Iteration 184/1000 | Loss: 0.00001713
Iteration 185/1000 | Loss: 0.00001713
Iteration 186/1000 | Loss: 0.00001713
Iteration 187/1000 | Loss: 0.00001712
Iteration 188/1000 | Loss: 0.00001712
Iteration 189/1000 | Loss: 0.00001712
Iteration 190/1000 | Loss: 0.00001712
Iteration 191/1000 | Loss: 0.00001712
Iteration 192/1000 | Loss: 0.00001712
Iteration 193/1000 | Loss: 0.00001712
Iteration 194/1000 | Loss: 0.00001712
Iteration 195/1000 | Loss: 0.00001712
Iteration 196/1000 | Loss: 0.00001712
Iteration 197/1000 | Loss: 0.00001712
Iteration 198/1000 | Loss: 0.00001712
Iteration 199/1000 | Loss: 0.00001712
Iteration 200/1000 | Loss: 0.00001712
Iteration 201/1000 | Loss: 0.00001712
Iteration 202/1000 | Loss: 0.00001712
Iteration 203/1000 | Loss: 0.00001712
Iteration 204/1000 | Loss: 0.00001712
Iteration 205/1000 | Loss: 0.00001712
Iteration 206/1000 | Loss: 0.00001712
Iteration 207/1000 | Loss: 0.00001712
Iteration 208/1000 | Loss: 0.00001712
Iteration 209/1000 | Loss: 0.00001712
Iteration 210/1000 | Loss: 0.00001712
Iteration 211/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.7119817130151205e-05, 1.7119817130151205e-05, 1.7119817130151205e-05, 1.7119817130151205e-05, 1.7119817130151205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7119817130151205e-05

Optimization complete. Final v2v error: 3.5341155529022217 mm

Highest mean error: 4.071404933929443 mm for frame 246

Lowest mean error: 3.397650957107544 mm for frame 154

Saving results

Total time: 58.85213017463684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783791
Iteration 2/25 | Loss: 0.00156341
Iteration 3/25 | Loss: 0.00124988
Iteration 4/25 | Loss: 0.00121024
Iteration 5/25 | Loss: 0.00118603
Iteration 6/25 | Loss: 0.00118233
Iteration 7/25 | Loss: 0.00118987
Iteration 8/25 | Loss: 0.00118920
Iteration 9/25 | Loss: 0.00117403
Iteration 10/25 | Loss: 0.00116215
Iteration 11/25 | Loss: 0.00115758
Iteration 12/25 | Loss: 0.00115448
Iteration 13/25 | Loss: 0.00115629
Iteration 14/25 | Loss: 0.00115898
Iteration 15/25 | Loss: 0.00115022
Iteration 16/25 | Loss: 0.00114549
Iteration 17/25 | Loss: 0.00114942
Iteration 18/25 | Loss: 0.00114673
Iteration 19/25 | Loss: 0.00114681
Iteration 20/25 | Loss: 0.00114214
Iteration 21/25 | Loss: 0.00114015
Iteration 22/25 | Loss: 0.00114808
Iteration 23/25 | Loss: 0.00113944
Iteration 24/25 | Loss: 0.00113726
Iteration 25/25 | Loss: 0.00113601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27145743
Iteration 2/25 | Loss: 0.00087026
Iteration 3/25 | Loss: 0.00087025
Iteration 4/25 | Loss: 0.00087025
Iteration 5/25 | Loss: 0.00087025
Iteration 6/25 | Loss: 0.00087025
Iteration 7/25 | Loss: 0.00087025
Iteration 8/25 | Loss: 0.00087025
Iteration 9/25 | Loss: 0.00087025
Iteration 10/25 | Loss: 0.00087025
Iteration 11/25 | Loss: 0.00087025
Iteration 12/25 | Loss: 0.00087025
Iteration 13/25 | Loss: 0.00087025
Iteration 14/25 | Loss: 0.00087025
Iteration 15/25 | Loss: 0.00087025
Iteration 16/25 | Loss: 0.00087025
Iteration 17/25 | Loss: 0.00087025
Iteration 18/25 | Loss: 0.00087025
Iteration 19/25 | Loss: 0.00087025
Iteration 20/25 | Loss: 0.00087025
Iteration 21/25 | Loss: 0.00087025
Iteration 22/25 | Loss: 0.00087025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008702486520633101, 0.0008702486520633101, 0.0008702486520633101, 0.0008702486520633101, 0.0008702486520633101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008702486520633101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087025
Iteration 2/1000 | Loss: 0.00005092
Iteration 3/1000 | Loss: 0.00002685
Iteration 4/1000 | Loss: 0.00003432
Iteration 5/1000 | Loss: 0.00003971
Iteration 6/1000 | Loss: 0.00002569
Iteration 7/1000 | Loss: 0.00003613
Iteration 8/1000 | Loss: 0.00005174
Iteration 9/1000 | Loss: 0.00004060
Iteration 10/1000 | Loss: 0.00004581
Iteration 11/1000 | Loss: 0.00004143
Iteration 12/1000 | Loss: 0.00004988
Iteration 13/1000 | Loss: 0.00002807
Iteration 14/1000 | Loss: 0.00002421
Iteration 15/1000 | Loss: 0.00003009
Iteration 16/1000 | Loss: 0.00002261
Iteration 17/1000 | Loss: 0.00003200
Iteration 18/1000 | Loss: 0.00002430
Iteration 19/1000 | Loss: 0.00002631
Iteration 20/1000 | Loss: 0.00002519
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00002611
Iteration 23/1000 | Loss: 0.00003380
Iteration 24/1000 | Loss: 0.00002604
Iteration 25/1000 | Loss: 0.00002706
Iteration 26/1000 | Loss: 0.00002647
Iteration 27/1000 | Loss: 0.00002799
Iteration 28/1000 | Loss: 0.00003380
Iteration 29/1000 | Loss: 0.00002632
Iteration 30/1000 | Loss: 0.00003162
Iteration 31/1000 | Loss: 0.00003007
Iteration 32/1000 | Loss: 0.00003326
Iteration 33/1000 | Loss: 0.00002990
Iteration 34/1000 | Loss: 0.00002338
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002061
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001960
Iteration 43/1000 | Loss: 0.00001958
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001957
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001956
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001953
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001949
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001947
Iteration 55/1000 | Loss: 0.00001947
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001945
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001944
Iteration 63/1000 | Loss: 0.00001943
Iteration 64/1000 | Loss: 0.00001943
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001942
Iteration 67/1000 | Loss: 0.00001941
Iteration 68/1000 | Loss: 0.00001939
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001929
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001924
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001924
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001922
Iteration 90/1000 | Loss: 0.00001922
Iteration 91/1000 | Loss: 0.00001922
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001921
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001917
Iteration 104/1000 | Loss: 0.00001917
Iteration 105/1000 | Loss: 0.00001917
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001916
Iteration 110/1000 | Loss: 0.00001916
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001916
Iteration 114/1000 | Loss: 0.00001915
Iteration 115/1000 | Loss: 0.00001915
Iteration 116/1000 | Loss: 0.00001915
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001914
Iteration 120/1000 | Loss: 0.00001914
Iteration 121/1000 | Loss: 0.00001913
Iteration 122/1000 | Loss: 0.00001913
Iteration 123/1000 | Loss: 0.00001913
Iteration 124/1000 | Loss: 0.00001912
Iteration 125/1000 | Loss: 0.00001912
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001911
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001910
Iteration 132/1000 | Loss: 0.00001910
Iteration 133/1000 | Loss: 0.00001909
Iteration 134/1000 | Loss: 0.00001909
Iteration 135/1000 | Loss: 0.00001909
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001908
Iteration 138/1000 | Loss: 0.00001907
Iteration 139/1000 | Loss: 0.00001907
Iteration 140/1000 | Loss: 0.00001906
Iteration 141/1000 | Loss: 0.00001906
Iteration 142/1000 | Loss: 0.00001905
Iteration 143/1000 | Loss: 0.00001905
Iteration 144/1000 | Loss: 0.00001904
Iteration 145/1000 | Loss: 0.00001904
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001901
Iteration 156/1000 | Loss: 0.00001901
Iteration 157/1000 | Loss: 0.00001901
Iteration 158/1000 | Loss: 0.00001901
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00001898
Iteration 161/1000 | Loss: 0.00001898
Iteration 162/1000 | Loss: 0.00001898
Iteration 163/1000 | Loss: 0.00001898
Iteration 164/1000 | Loss: 0.00001898
Iteration 165/1000 | Loss: 0.00001898
Iteration 166/1000 | Loss: 0.00001897
Iteration 167/1000 | Loss: 0.00001897
Iteration 168/1000 | Loss: 0.00001897
Iteration 169/1000 | Loss: 0.00001897
Iteration 170/1000 | Loss: 0.00001897
Iteration 171/1000 | Loss: 0.00001897
Iteration 172/1000 | Loss: 0.00001897
Iteration 173/1000 | Loss: 0.00001896
Iteration 174/1000 | Loss: 0.00001896
Iteration 175/1000 | Loss: 0.00001896
Iteration 176/1000 | Loss: 0.00001896
Iteration 177/1000 | Loss: 0.00001896
Iteration 178/1000 | Loss: 0.00001896
Iteration 179/1000 | Loss: 0.00001896
Iteration 180/1000 | Loss: 0.00001896
Iteration 181/1000 | Loss: 0.00001895
Iteration 182/1000 | Loss: 0.00001895
Iteration 183/1000 | Loss: 0.00001895
Iteration 184/1000 | Loss: 0.00001895
Iteration 185/1000 | Loss: 0.00001895
Iteration 186/1000 | Loss: 0.00001895
Iteration 187/1000 | Loss: 0.00001894
Iteration 188/1000 | Loss: 0.00001894
Iteration 189/1000 | Loss: 0.00001894
Iteration 190/1000 | Loss: 0.00001894
Iteration 191/1000 | Loss: 0.00001894
Iteration 192/1000 | Loss: 0.00001893
Iteration 193/1000 | Loss: 0.00001893
Iteration 194/1000 | Loss: 0.00001893
Iteration 195/1000 | Loss: 0.00001893
Iteration 196/1000 | Loss: 0.00001893
Iteration 197/1000 | Loss: 0.00001893
Iteration 198/1000 | Loss: 0.00001893
Iteration 199/1000 | Loss: 0.00001893
Iteration 200/1000 | Loss: 0.00001893
Iteration 201/1000 | Loss: 0.00001893
Iteration 202/1000 | Loss: 0.00001893
Iteration 203/1000 | Loss: 0.00001893
Iteration 204/1000 | Loss: 0.00001892
Iteration 205/1000 | Loss: 0.00001892
Iteration 206/1000 | Loss: 0.00001892
Iteration 207/1000 | Loss: 0.00001892
Iteration 208/1000 | Loss: 0.00001892
Iteration 209/1000 | Loss: 0.00001892
Iteration 210/1000 | Loss: 0.00001891
Iteration 211/1000 | Loss: 0.00001891
Iteration 212/1000 | Loss: 0.00001891
Iteration 213/1000 | Loss: 0.00001891
Iteration 214/1000 | Loss: 0.00001891
Iteration 215/1000 | Loss: 0.00001891
Iteration 216/1000 | Loss: 0.00001891
Iteration 217/1000 | Loss: 0.00001891
Iteration 218/1000 | Loss: 0.00001891
Iteration 219/1000 | Loss: 0.00001891
Iteration 220/1000 | Loss: 0.00001891
Iteration 221/1000 | Loss: 0.00001891
Iteration 222/1000 | Loss: 0.00001890
Iteration 223/1000 | Loss: 0.00001890
Iteration 224/1000 | Loss: 0.00001890
Iteration 225/1000 | Loss: 0.00001890
Iteration 226/1000 | Loss: 0.00001890
Iteration 227/1000 | Loss: 0.00001890
Iteration 228/1000 | Loss: 0.00001890
Iteration 229/1000 | Loss: 0.00001890
Iteration 230/1000 | Loss: 0.00001890
Iteration 231/1000 | Loss: 0.00001890
Iteration 232/1000 | Loss: 0.00001890
Iteration 233/1000 | Loss: 0.00001890
Iteration 234/1000 | Loss: 0.00001890
Iteration 235/1000 | Loss: 0.00001890
Iteration 236/1000 | Loss: 0.00001890
Iteration 237/1000 | Loss: 0.00001890
Iteration 238/1000 | Loss: 0.00001890
Iteration 239/1000 | Loss: 0.00001890
Iteration 240/1000 | Loss: 0.00001890
Iteration 241/1000 | Loss: 0.00001890
Iteration 242/1000 | Loss: 0.00001890
Iteration 243/1000 | Loss: 0.00001890
Iteration 244/1000 | Loss: 0.00001889
Iteration 245/1000 | Loss: 0.00001889
Iteration 246/1000 | Loss: 0.00001889
Iteration 247/1000 | Loss: 0.00001889
Iteration 248/1000 | Loss: 0.00001889
Iteration 249/1000 | Loss: 0.00001889
Iteration 250/1000 | Loss: 0.00001889
Iteration 251/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.8894679669756442e-05, 1.8894679669756442e-05, 1.8894679669756442e-05, 1.8894679669756442e-05, 1.8894679669756442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8894679669756442e-05

Optimization complete. Final v2v error: 3.547884702682495 mm

Highest mean error: 4.718359470367432 mm for frame 124

Lowest mean error: 2.8917813301086426 mm for frame 209

Saving results

Total time: 135.6799054145813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843389
Iteration 2/25 | Loss: 0.00145168
Iteration 3/25 | Loss: 0.00116393
Iteration 4/25 | Loss: 0.00113713
Iteration 5/25 | Loss: 0.00113316
Iteration 6/25 | Loss: 0.00113296
Iteration 7/25 | Loss: 0.00113296
Iteration 8/25 | Loss: 0.00113296
Iteration 9/25 | Loss: 0.00113296
Iteration 10/25 | Loss: 0.00113296
Iteration 11/25 | Loss: 0.00113296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011329581029713154, 0.0011329581029713154, 0.0011329581029713154, 0.0011329581029713154, 0.0011329581029713154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011329581029713154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09529400
Iteration 2/25 | Loss: 0.00081517
Iteration 3/25 | Loss: 0.00081517
Iteration 4/25 | Loss: 0.00081517
Iteration 5/25 | Loss: 0.00081517
Iteration 6/25 | Loss: 0.00081517
Iteration 7/25 | Loss: 0.00081517
Iteration 8/25 | Loss: 0.00081517
Iteration 9/25 | Loss: 0.00081517
Iteration 10/25 | Loss: 0.00081517
Iteration 11/25 | Loss: 0.00081517
Iteration 12/25 | Loss: 0.00081517
Iteration 13/25 | Loss: 0.00081517
Iteration 14/25 | Loss: 0.00081517
Iteration 15/25 | Loss: 0.00081517
Iteration 16/25 | Loss: 0.00081517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008151692454703152, 0.0008151692454703152, 0.0008151692454703152, 0.0008151692454703152, 0.0008151692454703152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008151692454703152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081517
Iteration 2/1000 | Loss: 0.00004327
Iteration 3/1000 | Loss: 0.00002562
Iteration 4/1000 | Loss: 0.00002026
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00001697
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001558
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001523
Iteration 15/1000 | Loss: 0.00001521
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001516
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001508
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001506
Iteration 41/1000 | Loss: 0.00001506
Iteration 42/1000 | Loss: 0.00001506
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001504
Iteration 50/1000 | Loss: 0.00001504
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001504
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001502
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001501
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001501
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001500
Iteration 72/1000 | Loss: 0.00001500
Iteration 73/1000 | Loss: 0.00001500
Iteration 74/1000 | Loss: 0.00001500
Iteration 75/1000 | Loss: 0.00001500
Iteration 76/1000 | Loss: 0.00001500
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001499
Iteration 94/1000 | Loss: 0.00001499
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001496
Iteration 121/1000 | Loss: 0.00001496
Iteration 122/1000 | Loss: 0.00001495
Iteration 123/1000 | Loss: 0.00001495
Iteration 124/1000 | Loss: 0.00001495
Iteration 125/1000 | Loss: 0.00001495
Iteration 126/1000 | Loss: 0.00001495
Iteration 127/1000 | Loss: 0.00001495
Iteration 128/1000 | Loss: 0.00001495
Iteration 129/1000 | Loss: 0.00001495
Iteration 130/1000 | Loss: 0.00001494
Iteration 131/1000 | Loss: 0.00001494
Iteration 132/1000 | Loss: 0.00001494
Iteration 133/1000 | Loss: 0.00001494
Iteration 134/1000 | Loss: 0.00001494
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00001493
Iteration 140/1000 | Loss: 0.00001493
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001493
Iteration 143/1000 | Loss: 0.00001493
Iteration 144/1000 | Loss: 0.00001493
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001493
Iteration 151/1000 | Loss: 0.00001493
Iteration 152/1000 | Loss: 0.00001493
Iteration 153/1000 | Loss: 0.00001493
Iteration 154/1000 | Loss: 0.00001493
Iteration 155/1000 | Loss: 0.00001493
Iteration 156/1000 | Loss: 0.00001493
Iteration 157/1000 | Loss: 0.00001493
Iteration 158/1000 | Loss: 0.00001493
Iteration 159/1000 | Loss: 0.00001493
Iteration 160/1000 | Loss: 0.00001493
Iteration 161/1000 | Loss: 0.00001493
Iteration 162/1000 | Loss: 0.00001493
Iteration 163/1000 | Loss: 0.00001493
Iteration 164/1000 | Loss: 0.00001493
Iteration 165/1000 | Loss: 0.00001493
Iteration 166/1000 | Loss: 0.00001493
Iteration 167/1000 | Loss: 0.00001493
Iteration 168/1000 | Loss: 0.00001493
Iteration 169/1000 | Loss: 0.00001493
Iteration 170/1000 | Loss: 0.00001493
Iteration 171/1000 | Loss: 0.00001493
Iteration 172/1000 | Loss: 0.00001493
Iteration 173/1000 | Loss: 0.00001493
Iteration 174/1000 | Loss: 0.00001493
Iteration 175/1000 | Loss: 0.00001493
Iteration 176/1000 | Loss: 0.00001493
Iteration 177/1000 | Loss: 0.00001493
Iteration 178/1000 | Loss: 0.00001493
Iteration 179/1000 | Loss: 0.00001493
Iteration 180/1000 | Loss: 0.00001493
Iteration 181/1000 | Loss: 0.00001493
Iteration 182/1000 | Loss: 0.00001493
Iteration 183/1000 | Loss: 0.00001493
Iteration 184/1000 | Loss: 0.00001493
Iteration 185/1000 | Loss: 0.00001493
Iteration 186/1000 | Loss: 0.00001493
Iteration 187/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.4925762116035912e-05, 1.4925762116035912e-05, 1.4925762116035912e-05, 1.4925762116035912e-05, 1.4925762116035912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4925762116035912e-05

Optimization complete. Final v2v error: 3.244359016418457 mm

Highest mean error: 3.677328586578369 mm for frame 1

Lowest mean error: 2.8349595069885254 mm for frame 78

Saving results

Total time: 41.000529766082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388253
Iteration 2/25 | Loss: 0.00116380
Iteration 3/25 | Loss: 0.00101896
Iteration 4/25 | Loss: 0.00099892
Iteration 5/25 | Loss: 0.00099630
Iteration 6/25 | Loss: 0.00099568
Iteration 7/25 | Loss: 0.00099568
Iteration 8/25 | Loss: 0.00099568
Iteration 9/25 | Loss: 0.00099568
Iteration 10/25 | Loss: 0.00099568
Iteration 11/25 | Loss: 0.00099568
Iteration 12/25 | Loss: 0.00099568
Iteration 13/25 | Loss: 0.00099568
Iteration 14/25 | Loss: 0.00099568
Iteration 15/25 | Loss: 0.00099568
Iteration 16/25 | Loss: 0.00099568
Iteration 17/25 | Loss: 0.00099568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000995683134533465, 0.000995683134533465, 0.000995683134533465, 0.000995683134533465, 0.000995683134533465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000995683134533465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29869151
Iteration 2/25 | Loss: 0.00084709
Iteration 3/25 | Loss: 0.00084709
Iteration 4/25 | Loss: 0.00084708
Iteration 5/25 | Loss: 0.00084708
Iteration 6/25 | Loss: 0.00084708
Iteration 7/25 | Loss: 0.00084708
Iteration 8/25 | Loss: 0.00084708
Iteration 9/25 | Loss: 0.00084708
Iteration 10/25 | Loss: 0.00084708
Iteration 11/25 | Loss: 0.00084708
Iteration 12/25 | Loss: 0.00084708
Iteration 13/25 | Loss: 0.00084708
Iteration 14/25 | Loss: 0.00084708
Iteration 15/25 | Loss: 0.00084708
Iteration 16/25 | Loss: 0.00084708
Iteration 17/25 | Loss: 0.00084708
Iteration 18/25 | Loss: 0.00084708
Iteration 19/25 | Loss: 0.00084708
Iteration 20/25 | Loss: 0.00084708
Iteration 21/25 | Loss: 0.00084708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008470825850963593, 0.0008470825850963593, 0.0008470825850963593, 0.0008470825850963593, 0.0008470825850963593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008470825850963593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084708
Iteration 2/1000 | Loss: 0.00001915
Iteration 3/1000 | Loss: 0.00001296
Iteration 4/1000 | Loss: 0.00001160
Iteration 5/1000 | Loss: 0.00001083
Iteration 6/1000 | Loss: 0.00001041
Iteration 7/1000 | Loss: 0.00000996
Iteration 8/1000 | Loss: 0.00000972
Iteration 9/1000 | Loss: 0.00000970
Iteration 10/1000 | Loss: 0.00000950
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000941
Iteration 13/1000 | Loss: 0.00000939
Iteration 14/1000 | Loss: 0.00000923
Iteration 15/1000 | Loss: 0.00000920
Iteration 16/1000 | Loss: 0.00000920
Iteration 17/1000 | Loss: 0.00000912
Iteration 18/1000 | Loss: 0.00000911
Iteration 19/1000 | Loss: 0.00000910
Iteration 20/1000 | Loss: 0.00000910
Iteration 21/1000 | Loss: 0.00000908
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000902
Iteration 25/1000 | Loss: 0.00000901
Iteration 26/1000 | Loss: 0.00000900
Iteration 27/1000 | Loss: 0.00000900
Iteration 28/1000 | Loss: 0.00000898
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000897
Iteration 31/1000 | Loss: 0.00000895
Iteration 32/1000 | Loss: 0.00000895
Iteration 33/1000 | Loss: 0.00000894
Iteration 34/1000 | Loss: 0.00000893
Iteration 35/1000 | Loss: 0.00000893
Iteration 36/1000 | Loss: 0.00000892
Iteration 37/1000 | Loss: 0.00000890
Iteration 38/1000 | Loss: 0.00000890
Iteration 39/1000 | Loss: 0.00000889
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000889
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000888
Iteration 44/1000 | Loss: 0.00000885
Iteration 45/1000 | Loss: 0.00000885
Iteration 46/1000 | Loss: 0.00000884
Iteration 47/1000 | Loss: 0.00000884
Iteration 48/1000 | Loss: 0.00000883
Iteration 49/1000 | Loss: 0.00000882
Iteration 50/1000 | Loss: 0.00000881
Iteration 51/1000 | Loss: 0.00000880
Iteration 52/1000 | Loss: 0.00000880
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000879
Iteration 55/1000 | Loss: 0.00000879
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000876
Iteration 60/1000 | Loss: 0.00000876
Iteration 61/1000 | Loss: 0.00000875
Iteration 62/1000 | Loss: 0.00000875
Iteration 63/1000 | Loss: 0.00000874
Iteration 64/1000 | Loss: 0.00000874
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000872
Iteration 67/1000 | Loss: 0.00000872
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000871
Iteration 74/1000 | Loss: 0.00000871
Iteration 75/1000 | Loss: 0.00000870
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000867
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000867
Iteration 82/1000 | Loss: 0.00000867
Iteration 83/1000 | Loss: 0.00000867
Iteration 84/1000 | Loss: 0.00000867
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000866
Iteration 87/1000 | Loss: 0.00000866
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000865
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000863
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000862
Iteration 95/1000 | Loss: 0.00000862
Iteration 96/1000 | Loss: 0.00000861
Iteration 97/1000 | Loss: 0.00000861
Iteration 98/1000 | Loss: 0.00000861
Iteration 99/1000 | Loss: 0.00000861
Iteration 100/1000 | Loss: 0.00000861
Iteration 101/1000 | Loss: 0.00000860
Iteration 102/1000 | Loss: 0.00000859
Iteration 103/1000 | Loss: 0.00000859
Iteration 104/1000 | Loss: 0.00000859
Iteration 105/1000 | Loss: 0.00000859
Iteration 106/1000 | Loss: 0.00000859
Iteration 107/1000 | Loss: 0.00000858
Iteration 108/1000 | Loss: 0.00000857
Iteration 109/1000 | Loss: 0.00000857
Iteration 110/1000 | Loss: 0.00000857
Iteration 111/1000 | Loss: 0.00000856
Iteration 112/1000 | Loss: 0.00000856
Iteration 113/1000 | Loss: 0.00000856
Iteration 114/1000 | Loss: 0.00000856
Iteration 115/1000 | Loss: 0.00000855
Iteration 116/1000 | Loss: 0.00000855
Iteration 117/1000 | Loss: 0.00000854
Iteration 118/1000 | Loss: 0.00000854
Iteration 119/1000 | Loss: 0.00000854
Iteration 120/1000 | Loss: 0.00000854
Iteration 121/1000 | Loss: 0.00000853
Iteration 122/1000 | Loss: 0.00000853
Iteration 123/1000 | Loss: 0.00000853
Iteration 124/1000 | Loss: 0.00000853
Iteration 125/1000 | Loss: 0.00000853
Iteration 126/1000 | Loss: 0.00000853
Iteration 127/1000 | Loss: 0.00000852
Iteration 128/1000 | Loss: 0.00000852
Iteration 129/1000 | Loss: 0.00000852
Iteration 130/1000 | Loss: 0.00000852
Iteration 131/1000 | Loss: 0.00000852
Iteration 132/1000 | Loss: 0.00000852
Iteration 133/1000 | Loss: 0.00000852
Iteration 134/1000 | Loss: 0.00000852
Iteration 135/1000 | Loss: 0.00000852
Iteration 136/1000 | Loss: 0.00000852
Iteration 137/1000 | Loss: 0.00000852
Iteration 138/1000 | Loss: 0.00000852
Iteration 139/1000 | Loss: 0.00000852
Iteration 140/1000 | Loss: 0.00000852
Iteration 141/1000 | Loss: 0.00000852
Iteration 142/1000 | Loss: 0.00000851
Iteration 143/1000 | Loss: 0.00000851
Iteration 144/1000 | Loss: 0.00000851
Iteration 145/1000 | Loss: 0.00000851
Iteration 146/1000 | Loss: 0.00000851
Iteration 147/1000 | Loss: 0.00000851
Iteration 148/1000 | Loss: 0.00000851
Iteration 149/1000 | Loss: 0.00000850
Iteration 150/1000 | Loss: 0.00000850
Iteration 151/1000 | Loss: 0.00000850
Iteration 152/1000 | Loss: 0.00000850
Iteration 153/1000 | Loss: 0.00000850
Iteration 154/1000 | Loss: 0.00000850
Iteration 155/1000 | Loss: 0.00000850
Iteration 156/1000 | Loss: 0.00000850
Iteration 157/1000 | Loss: 0.00000850
Iteration 158/1000 | Loss: 0.00000849
Iteration 159/1000 | Loss: 0.00000849
Iteration 160/1000 | Loss: 0.00000849
Iteration 161/1000 | Loss: 0.00000849
Iteration 162/1000 | Loss: 0.00000849
Iteration 163/1000 | Loss: 0.00000849
Iteration 164/1000 | Loss: 0.00000849
Iteration 165/1000 | Loss: 0.00000849
Iteration 166/1000 | Loss: 0.00000849
Iteration 167/1000 | Loss: 0.00000849
Iteration 168/1000 | Loss: 0.00000849
Iteration 169/1000 | Loss: 0.00000849
Iteration 170/1000 | Loss: 0.00000849
Iteration 171/1000 | Loss: 0.00000849
Iteration 172/1000 | Loss: 0.00000849
Iteration 173/1000 | Loss: 0.00000849
Iteration 174/1000 | Loss: 0.00000849
Iteration 175/1000 | Loss: 0.00000848
Iteration 176/1000 | Loss: 0.00000848
Iteration 177/1000 | Loss: 0.00000848
Iteration 178/1000 | Loss: 0.00000848
Iteration 179/1000 | Loss: 0.00000848
Iteration 180/1000 | Loss: 0.00000848
Iteration 181/1000 | Loss: 0.00000848
Iteration 182/1000 | Loss: 0.00000848
Iteration 183/1000 | Loss: 0.00000848
Iteration 184/1000 | Loss: 0.00000848
Iteration 185/1000 | Loss: 0.00000848
Iteration 186/1000 | Loss: 0.00000848
Iteration 187/1000 | Loss: 0.00000848
Iteration 188/1000 | Loss: 0.00000848
Iteration 189/1000 | Loss: 0.00000848
Iteration 190/1000 | Loss: 0.00000848
Iteration 191/1000 | Loss: 0.00000848
Iteration 192/1000 | Loss: 0.00000848
Iteration 193/1000 | Loss: 0.00000848
Iteration 194/1000 | Loss: 0.00000848
Iteration 195/1000 | Loss: 0.00000848
Iteration 196/1000 | Loss: 0.00000848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [8.476642506138887e-06, 8.476642506138887e-06, 8.476642506138887e-06, 8.476642506138887e-06, 8.476642506138887e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.476642506138887e-06

Optimization complete. Final v2v error: 2.487543821334839 mm

Highest mean error: 2.8283913135528564 mm for frame 85

Lowest mean error: 2.238422393798828 mm for frame 2

Saving results

Total time: 39.81622552871704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558503
Iteration 2/25 | Loss: 0.00132371
Iteration 3/25 | Loss: 0.00114809
Iteration 4/25 | Loss: 0.00112828
Iteration 5/25 | Loss: 0.00112283
Iteration 6/25 | Loss: 0.00112085
Iteration 7/25 | Loss: 0.00112085
Iteration 8/25 | Loss: 0.00112085
Iteration 9/25 | Loss: 0.00112085
Iteration 10/25 | Loss: 0.00112085
Iteration 11/25 | Loss: 0.00112085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001120849046856165, 0.001120849046856165, 0.001120849046856165, 0.001120849046856165, 0.001120849046856165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001120849046856165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52222300
Iteration 2/25 | Loss: 0.00115014
Iteration 3/25 | Loss: 0.00115014
Iteration 4/25 | Loss: 0.00115014
Iteration 5/25 | Loss: 0.00115014
Iteration 6/25 | Loss: 0.00115014
Iteration 7/25 | Loss: 0.00115014
Iteration 8/25 | Loss: 0.00115014
Iteration 9/25 | Loss: 0.00115014
Iteration 10/25 | Loss: 0.00115014
Iteration 11/25 | Loss: 0.00115014
Iteration 12/25 | Loss: 0.00115014
Iteration 13/25 | Loss: 0.00115014
Iteration 14/25 | Loss: 0.00115014
Iteration 15/25 | Loss: 0.00115014
Iteration 16/25 | Loss: 0.00115014
Iteration 17/25 | Loss: 0.00115014
Iteration 18/25 | Loss: 0.00115014
Iteration 19/25 | Loss: 0.00115014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011501351837068796, 0.0011501351837068796, 0.0011501351837068796, 0.0011501351837068796, 0.0011501351837068796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011501351837068796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115014
Iteration 2/1000 | Loss: 0.00002500
Iteration 3/1000 | Loss: 0.00001957
Iteration 4/1000 | Loss: 0.00001762
Iteration 5/1000 | Loss: 0.00001675
Iteration 6/1000 | Loss: 0.00001619
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001494
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001476
Iteration 32/1000 | Loss: 0.00001475
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001470
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001468
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001464
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001460
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.4584367818315513e-05, 1.4584367818315513e-05, 1.4584367818315513e-05, 1.4584367818315513e-05, 1.4584367818315513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4584367818315513e-05

Optimization complete. Final v2v error: 3.2178022861480713 mm

Highest mean error: 3.8941423892974854 mm for frame 104

Lowest mean error: 2.8097519874572754 mm for frame 45

Saving results

Total time: 38.315417766571045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043000
Iteration 2/25 | Loss: 0.00194354
Iteration 3/25 | Loss: 0.00148034
Iteration 4/25 | Loss: 0.00118745
Iteration 5/25 | Loss: 0.00115206
Iteration 6/25 | Loss: 0.00118134
Iteration 7/25 | Loss: 0.00112495
Iteration 8/25 | Loss: 0.00108422
Iteration 9/25 | Loss: 0.00106870
Iteration 10/25 | Loss: 0.00104336
Iteration 11/25 | Loss: 0.00104418
Iteration 12/25 | Loss: 0.00103953
Iteration 13/25 | Loss: 0.00103086
Iteration 14/25 | Loss: 0.00102748
Iteration 15/25 | Loss: 0.00102488
Iteration 16/25 | Loss: 0.00102721
Iteration 17/25 | Loss: 0.00102693
Iteration 18/25 | Loss: 0.00101987
Iteration 19/25 | Loss: 0.00101992
Iteration 20/25 | Loss: 0.00101360
Iteration 21/25 | Loss: 0.00101457
Iteration 22/25 | Loss: 0.00101131
Iteration 23/25 | Loss: 0.00101368
Iteration 24/25 | Loss: 0.00100908
Iteration 25/25 | Loss: 0.00100777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81912684
Iteration 2/25 | Loss: 0.00135164
Iteration 3/25 | Loss: 0.00121731
Iteration 4/25 | Loss: 0.00121731
Iteration 5/25 | Loss: 0.00121731
Iteration 6/25 | Loss: 0.00121731
Iteration 7/25 | Loss: 0.00121731
Iteration 8/25 | Loss: 0.00121731
Iteration 9/25 | Loss: 0.00121731
Iteration 10/25 | Loss: 0.00121731
Iteration 11/25 | Loss: 0.00121731
Iteration 12/25 | Loss: 0.00121731
Iteration 13/25 | Loss: 0.00121731
Iteration 14/25 | Loss: 0.00121731
Iteration 15/25 | Loss: 0.00121731
Iteration 16/25 | Loss: 0.00121731
Iteration 17/25 | Loss: 0.00121731
Iteration 18/25 | Loss: 0.00121731
Iteration 19/25 | Loss: 0.00121731
Iteration 20/25 | Loss: 0.00121731
Iteration 21/25 | Loss: 0.00121731
Iteration 22/25 | Loss: 0.00121731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012173062423244119, 0.0012173062423244119, 0.0012173062423244119, 0.0012173062423244119, 0.0012173062423244119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012173062423244119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121731
Iteration 2/1000 | Loss: 0.00011672
Iteration 3/1000 | Loss: 0.00007698
Iteration 4/1000 | Loss: 0.00015162
Iteration 5/1000 | Loss: 0.00005326
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00007097
Iteration 8/1000 | Loss: 0.00004071
Iteration 9/1000 | Loss: 0.00002436
Iteration 10/1000 | Loss: 0.00019963
Iteration 11/1000 | Loss: 0.00003917
Iteration 12/1000 | Loss: 0.00002385
Iteration 13/1000 | Loss: 0.00002231
Iteration 14/1000 | Loss: 0.00023848
Iteration 15/1000 | Loss: 0.00018550
Iteration 16/1000 | Loss: 0.00021953
Iteration 17/1000 | Loss: 0.00024819
Iteration 18/1000 | Loss: 0.00020779
Iteration 19/1000 | Loss: 0.00016523
Iteration 20/1000 | Loss: 0.00037877
Iteration 21/1000 | Loss: 0.00047341
Iteration 22/1000 | Loss: 0.00009375
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001796
Iteration 26/1000 | Loss: 0.00002897
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00002214
Iteration 30/1000 | Loss: 0.00047976
Iteration 31/1000 | Loss: 0.00037076
Iteration 32/1000 | Loss: 0.00008667
Iteration 33/1000 | Loss: 0.00003277
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00002614
Iteration 36/1000 | Loss: 0.00001278
Iteration 37/1000 | Loss: 0.00031585
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00002736
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00002436
Iteration 42/1000 | Loss: 0.00002047
Iteration 43/1000 | Loss: 0.00001085
Iteration 44/1000 | Loss: 0.00002193
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001024
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001012
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00000997
Iteration 59/1000 | Loss: 0.00000997
Iteration 60/1000 | Loss: 0.00000996
Iteration 61/1000 | Loss: 0.00000996
Iteration 62/1000 | Loss: 0.00000996
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000996
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000996
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000995
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000993
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001026
Iteration 79/1000 | Loss: 0.00000985
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000984
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000984
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000984
Iteration 99/1000 | Loss: 0.00000984
Iteration 100/1000 | Loss: 0.00000984
Iteration 101/1000 | Loss: 0.00000984
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [9.841181054071058e-06, 9.841181054071058e-06, 9.841181054071058e-06, 9.841181054071058e-06, 9.841181054071058e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.841181054071058e-06

Optimization complete. Final v2v error: 2.694690227508545 mm

Highest mean error: 4.147194862365723 mm for frame 10

Lowest mean error: 2.471869945526123 mm for frame 140

Saving results

Total time: 125.26927900314331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393016
Iteration 2/25 | Loss: 0.00125718
Iteration 3/25 | Loss: 0.00103411
Iteration 4/25 | Loss: 0.00101455
Iteration 5/25 | Loss: 0.00101191
Iteration 6/25 | Loss: 0.00101128
Iteration 7/25 | Loss: 0.00101128
Iteration 8/25 | Loss: 0.00101128
Iteration 9/25 | Loss: 0.00101128
Iteration 10/25 | Loss: 0.00101128
Iteration 11/25 | Loss: 0.00101128
Iteration 12/25 | Loss: 0.00101128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001011275453492999, 0.001011275453492999, 0.001011275453492999, 0.001011275453492999, 0.001011275453492999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001011275453492999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30017066
Iteration 2/25 | Loss: 0.00080244
Iteration 3/25 | Loss: 0.00080243
Iteration 4/25 | Loss: 0.00080243
Iteration 5/25 | Loss: 0.00080243
Iteration 6/25 | Loss: 0.00080243
Iteration 7/25 | Loss: 0.00080243
Iteration 8/25 | Loss: 0.00080243
Iteration 9/25 | Loss: 0.00080243
Iteration 10/25 | Loss: 0.00080243
Iteration 11/25 | Loss: 0.00080243
Iteration 12/25 | Loss: 0.00080243
Iteration 13/25 | Loss: 0.00080243
Iteration 14/25 | Loss: 0.00080243
Iteration 15/25 | Loss: 0.00080243
Iteration 16/25 | Loss: 0.00080243
Iteration 17/25 | Loss: 0.00080243
Iteration 18/25 | Loss: 0.00080243
Iteration 19/25 | Loss: 0.00080243
Iteration 20/25 | Loss: 0.00080243
Iteration 21/25 | Loss: 0.00080243
Iteration 22/25 | Loss: 0.00080243
Iteration 23/25 | Loss: 0.00080243
Iteration 24/25 | Loss: 0.00080243
Iteration 25/25 | Loss: 0.00080243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080243
Iteration 2/1000 | Loss: 0.00002206
Iteration 3/1000 | Loss: 0.00001381
Iteration 4/1000 | Loss: 0.00001224
Iteration 5/1000 | Loss: 0.00001143
Iteration 6/1000 | Loss: 0.00001084
Iteration 7/1000 | Loss: 0.00001040
Iteration 8/1000 | Loss: 0.00001007
Iteration 9/1000 | Loss: 0.00000985
Iteration 10/1000 | Loss: 0.00000972
Iteration 11/1000 | Loss: 0.00000972
Iteration 12/1000 | Loss: 0.00000969
Iteration 13/1000 | Loss: 0.00000966
Iteration 14/1000 | Loss: 0.00000963
Iteration 15/1000 | Loss: 0.00000963
Iteration 16/1000 | Loss: 0.00000959
Iteration 17/1000 | Loss: 0.00000958
Iteration 18/1000 | Loss: 0.00000958
Iteration 19/1000 | Loss: 0.00000958
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000954
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000953
Iteration 24/1000 | Loss: 0.00000953
Iteration 25/1000 | Loss: 0.00000952
Iteration 26/1000 | Loss: 0.00000952
Iteration 27/1000 | Loss: 0.00000948
Iteration 28/1000 | Loss: 0.00000948
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000948
Iteration 32/1000 | Loss: 0.00000947
Iteration 33/1000 | Loss: 0.00000947
Iteration 34/1000 | Loss: 0.00000947
Iteration 35/1000 | Loss: 0.00000947
Iteration 36/1000 | Loss: 0.00000946
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000937
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000935
Iteration 45/1000 | Loss: 0.00000935
Iteration 46/1000 | Loss: 0.00000935
Iteration 47/1000 | Loss: 0.00000935
Iteration 48/1000 | Loss: 0.00000934
Iteration 49/1000 | Loss: 0.00000934
Iteration 50/1000 | Loss: 0.00000933
Iteration 51/1000 | Loss: 0.00000932
Iteration 52/1000 | Loss: 0.00000932
Iteration 53/1000 | Loss: 0.00000931
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000928
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000928
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000927
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000927
Iteration 67/1000 | Loss: 0.00000927
Iteration 68/1000 | Loss: 0.00000926
Iteration 69/1000 | Loss: 0.00000926
Iteration 70/1000 | Loss: 0.00000926
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000923
Iteration 76/1000 | Loss: 0.00000923
Iteration 77/1000 | Loss: 0.00000923
Iteration 78/1000 | Loss: 0.00000923
Iteration 79/1000 | Loss: 0.00000923
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000920
Iteration 89/1000 | Loss: 0.00000920
Iteration 90/1000 | Loss: 0.00000919
Iteration 91/1000 | Loss: 0.00000919
Iteration 92/1000 | Loss: 0.00000919
Iteration 93/1000 | Loss: 0.00000919
Iteration 94/1000 | Loss: 0.00000918
Iteration 95/1000 | Loss: 0.00000918
Iteration 96/1000 | Loss: 0.00000916
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000914
Iteration 101/1000 | Loss: 0.00000914
Iteration 102/1000 | Loss: 0.00000914
Iteration 103/1000 | Loss: 0.00000914
Iteration 104/1000 | Loss: 0.00000913
Iteration 105/1000 | Loss: 0.00000913
Iteration 106/1000 | Loss: 0.00000912
Iteration 107/1000 | Loss: 0.00000912
Iteration 108/1000 | Loss: 0.00000911
Iteration 109/1000 | Loss: 0.00000911
Iteration 110/1000 | Loss: 0.00000911
Iteration 111/1000 | Loss: 0.00000911
Iteration 112/1000 | Loss: 0.00000911
Iteration 113/1000 | Loss: 0.00000910
Iteration 114/1000 | Loss: 0.00000910
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000910
Iteration 117/1000 | Loss: 0.00000910
Iteration 118/1000 | Loss: 0.00000910
Iteration 119/1000 | Loss: 0.00000910
Iteration 120/1000 | Loss: 0.00000909
Iteration 121/1000 | Loss: 0.00000909
Iteration 122/1000 | Loss: 0.00000909
Iteration 123/1000 | Loss: 0.00000909
Iteration 124/1000 | Loss: 0.00000909
Iteration 125/1000 | Loss: 0.00000909
Iteration 126/1000 | Loss: 0.00000909
Iteration 127/1000 | Loss: 0.00000908
Iteration 128/1000 | Loss: 0.00000908
Iteration 129/1000 | Loss: 0.00000908
Iteration 130/1000 | Loss: 0.00000908
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000907
Iteration 134/1000 | Loss: 0.00000906
Iteration 135/1000 | Loss: 0.00000906
Iteration 136/1000 | Loss: 0.00000906
Iteration 137/1000 | Loss: 0.00000906
Iteration 138/1000 | Loss: 0.00000905
Iteration 139/1000 | Loss: 0.00000905
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000905
Iteration 142/1000 | Loss: 0.00000904
Iteration 143/1000 | Loss: 0.00000904
Iteration 144/1000 | Loss: 0.00000904
Iteration 145/1000 | Loss: 0.00000904
Iteration 146/1000 | Loss: 0.00000904
Iteration 147/1000 | Loss: 0.00000904
Iteration 148/1000 | Loss: 0.00000903
Iteration 149/1000 | Loss: 0.00000903
Iteration 150/1000 | Loss: 0.00000903
Iteration 151/1000 | Loss: 0.00000903
Iteration 152/1000 | Loss: 0.00000903
Iteration 153/1000 | Loss: 0.00000903
Iteration 154/1000 | Loss: 0.00000903
Iteration 155/1000 | Loss: 0.00000903
Iteration 156/1000 | Loss: 0.00000902
Iteration 157/1000 | Loss: 0.00000902
Iteration 158/1000 | Loss: 0.00000902
Iteration 159/1000 | Loss: 0.00000902
Iteration 160/1000 | Loss: 0.00000902
Iteration 161/1000 | Loss: 0.00000902
Iteration 162/1000 | Loss: 0.00000902
Iteration 163/1000 | Loss: 0.00000901
Iteration 164/1000 | Loss: 0.00000901
Iteration 165/1000 | Loss: 0.00000901
Iteration 166/1000 | Loss: 0.00000901
Iteration 167/1000 | Loss: 0.00000901
Iteration 168/1000 | Loss: 0.00000901
Iteration 169/1000 | Loss: 0.00000901
Iteration 170/1000 | Loss: 0.00000901
Iteration 171/1000 | Loss: 0.00000901
Iteration 172/1000 | Loss: 0.00000901
Iteration 173/1000 | Loss: 0.00000901
Iteration 174/1000 | Loss: 0.00000901
Iteration 175/1000 | Loss: 0.00000901
Iteration 176/1000 | Loss: 0.00000901
Iteration 177/1000 | Loss: 0.00000901
Iteration 178/1000 | Loss: 0.00000901
Iteration 179/1000 | Loss: 0.00000901
Iteration 180/1000 | Loss: 0.00000901
Iteration 181/1000 | Loss: 0.00000901
Iteration 182/1000 | Loss: 0.00000901
Iteration 183/1000 | Loss: 0.00000901
Iteration 184/1000 | Loss: 0.00000901
Iteration 185/1000 | Loss: 0.00000900
Iteration 186/1000 | Loss: 0.00000900
Iteration 187/1000 | Loss: 0.00000900
Iteration 188/1000 | Loss: 0.00000900
Iteration 189/1000 | Loss: 0.00000900
Iteration 190/1000 | Loss: 0.00000900
Iteration 191/1000 | Loss: 0.00000900
Iteration 192/1000 | Loss: 0.00000900
Iteration 193/1000 | Loss: 0.00000900
Iteration 194/1000 | Loss: 0.00000900
Iteration 195/1000 | Loss: 0.00000900
Iteration 196/1000 | Loss: 0.00000900
Iteration 197/1000 | Loss: 0.00000900
Iteration 198/1000 | Loss: 0.00000900
Iteration 199/1000 | Loss: 0.00000900
Iteration 200/1000 | Loss: 0.00000900
Iteration 201/1000 | Loss: 0.00000900
Iteration 202/1000 | Loss: 0.00000900
Iteration 203/1000 | Loss: 0.00000900
Iteration 204/1000 | Loss: 0.00000900
Iteration 205/1000 | Loss: 0.00000900
Iteration 206/1000 | Loss: 0.00000900
Iteration 207/1000 | Loss: 0.00000900
Iteration 208/1000 | Loss: 0.00000900
Iteration 209/1000 | Loss: 0.00000900
Iteration 210/1000 | Loss: 0.00000900
Iteration 211/1000 | Loss: 0.00000900
Iteration 212/1000 | Loss: 0.00000900
Iteration 213/1000 | Loss: 0.00000900
Iteration 214/1000 | Loss: 0.00000900
Iteration 215/1000 | Loss: 0.00000900
Iteration 216/1000 | Loss: 0.00000900
Iteration 217/1000 | Loss: 0.00000900
Iteration 218/1000 | Loss: 0.00000900
Iteration 219/1000 | Loss: 0.00000900
Iteration 220/1000 | Loss: 0.00000900
Iteration 221/1000 | Loss: 0.00000900
Iteration 222/1000 | Loss: 0.00000900
Iteration 223/1000 | Loss: 0.00000900
Iteration 224/1000 | Loss: 0.00000900
Iteration 225/1000 | Loss: 0.00000900
Iteration 226/1000 | Loss: 0.00000900
Iteration 227/1000 | Loss: 0.00000900
Iteration 228/1000 | Loss: 0.00000900
Iteration 229/1000 | Loss: 0.00000900
Iteration 230/1000 | Loss: 0.00000900
Iteration 231/1000 | Loss: 0.00000900
Iteration 232/1000 | Loss: 0.00000900
Iteration 233/1000 | Loss: 0.00000900
Iteration 234/1000 | Loss: 0.00000900
Iteration 235/1000 | Loss: 0.00000900
Iteration 236/1000 | Loss: 0.00000900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [9.003731975099072e-06, 9.003731975099072e-06, 9.003731975099072e-06, 9.003731975099072e-06, 9.003731975099072e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.003731975099072e-06

Optimization complete. Final v2v error: 2.595364570617676 mm

Highest mean error: 2.689213514328003 mm for frame 24

Lowest mean error: 2.493968963623047 mm for frame 99

Saving results

Total time: 40.553767919540405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037684
Iteration 2/25 | Loss: 0.01037683
Iteration 3/25 | Loss: 0.01037683
Iteration 4/25 | Loss: 0.01037683
Iteration 5/25 | Loss: 0.01037683
Iteration 6/25 | Loss: 0.01037683
Iteration 7/25 | Loss: 0.01037682
Iteration 8/25 | Loss: 0.01037682
Iteration 9/25 | Loss: 0.01037682
Iteration 10/25 | Loss: 0.01037682
Iteration 11/25 | Loss: 0.01037682
Iteration 12/25 | Loss: 0.01037682
Iteration 13/25 | Loss: 0.01037681
Iteration 14/25 | Loss: 0.01037681
Iteration 15/25 | Loss: 0.01037681
Iteration 16/25 | Loss: 0.01037681
Iteration 17/25 | Loss: 0.01037680
Iteration 18/25 | Loss: 0.01037680
Iteration 19/25 | Loss: 0.01037680
Iteration 20/25 | Loss: 0.01037680
Iteration 21/25 | Loss: 0.01037680
Iteration 22/25 | Loss: 0.01037679
Iteration 23/25 | Loss: 0.01037679
Iteration 24/25 | Loss: 0.01037679
Iteration 25/25 | Loss: 0.01037679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58572698
Iteration 2/25 | Loss: 0.08300746
Iteration 3/25 | Loss: 0.08287156
Iteration 4/25 | Loss: 0.08281773
Iteration 5/25 | Loss: 0.08281770
Iteration 6/25 | Loss: 0.08281769
Iteration 7/25 | Loss: 0.08281769
Iteration 8/25 | Loss: 0.08281769
Iteration 9/25 | Loss: 0.08281769
Iteration 10/25 | Loss: 0.08281769
Iteration 11/25 | Loss: 0.08281769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0828176885843277, 0.0828176885843277, 0.0828176885843277, 0.0828176885843277, 0.0828176885843277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0828176885843277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08281769
Iteration 2/1000 | Loss: 0.00087313
Iteration 3/1000 | Loss: 0.00035915
Iteration 4/1000 | Loss: 0.00024339
Iteration 5/1000 | Loss: 0.00015134
Iteration 6/1000 | Loss: 0.00021873
Iteration 7/1000 | Loss: 0.00012281
Iteration 8/1000 | Loss: 0.00009425
Iteration 9/1000 | Loss: 0.00006591
Iteration 10/1000 | Loss: 0.00004895
Iteration 11/1000 | Loss: 0.00009489
Iteration 12/1000 | Loss: 0.00004073
Iteration 13/1000 | Loss: 0.00003668
Iteration 14/1000 | Loss: 0.00004601
Iteration 15/1000 | Loss: 0.00007341
Iteration 16/1000 | Loss: 0.00016197
Iteration 17/1000 | Loss: 0.00003111
Iteration 18/1000 | Loss: 0.00005794
Iteration 19/1000 | Loss: 0.00002981
Iteration 20/1000 | Loss: 0.00003112
Iteration 21/1000 | Loss: 0.00005578
Iteration 22/1000 | Loss: 0.00017412
Iteration 23/1000 | Loss: 0.00004421
Iteration 24/1000 | Loss: 0.00003698
Iteration 25/1000 | Loss: 0.00003356
Iteration 26/1000 | Loss: 0.00003377
Iteration 27/1000 | Loss: 0.00003340
Iteration 28/1000 | Loss: 0.00003063
Iteration 29/1000 | Loss: 0.00002710
Iteration 30/1000 | Loss: 0.00003227
Iteration 31/1000 | Loss: 0.00002443
Iteration 32/1000 | Loss: 0.00003506
Iteration 33/1000 | Loss: 0.00002478
Iteration 34/1000 | Loss: 0.00002807
Iteration 35/1000 | Loss: 0.00004489
Iteration 36/1000 | Loss: 0.00002491
Iteration 37/1000 | Loss: 0.00004643
Iteration 38/1000 | Loss: 0.00002718
Iteration 39/1000 | Loss: 0.00003771
Iteration 40/1000 | Loss: 0.00002719
Iteration 41/1000 | Loss: 0.00002283
Iteration 42/1000 | Loss: 0.00003192
Iteration 43/1000 | Loss: 0.00002614
Iteration 44/1000 | Loss: 0.00003672
Iteration 45/1000 | Loss: 0.00002946
Iteration 46/1000 | Loss: 0.00007153
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00002548
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00005426
Iteration 52/1000 | Loss: 0.00003061
Iteration 53/1000 | Loss: 0.00002811
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00002121
Iteration 57/1000 | Loss: 0.00002121
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002120
Iteration 61/1000 | Loss: 0.00002120
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002120
Iteration 65/1000 | Loss: 0.00002120
Iteration 66/1000 | Loss: 0.00002120
Iteration 67/1000 | Loss: 0.00002928
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002116
Iteration 71/1000 | Loss: 0.00002116
Iteration 72/1000 | Loss: 0.00002116
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002115
Iteration 76/1000 | Loss: 0.00002115
Iteration 77/1000 | Loss: 0.00002115
Iteration 78/1000 | Loss: 0.00002147
Iteration 79/1000 | Loss: 0.00002112
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002111
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002106
Iteration 85/1000 | Loss: 0.00002308
Iteration 86/1000 | Loss: 0.00002090
Iteration 87/1000 | Loss: 0.00003149
Iteration 88/1000 | Loss: 0.00009521
Iteration 89/1000 | Loss: 0.00002293
Iteration 90/1000 | Loss: 0.00002505
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002072
Iteration 93/1000 | Loss: 0.00002072
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002071
Iteration 97/1000 | Loss: 0.00002071
Iteration 98/1000 | Loss: 0.00002071
Iteration 99/1000 | Loss: 0.00002071
Iteration 100/1000 | Loss: 0.00002071
Iteration 101/1000 | Loss: 0.00002071
Iteration 102/1000 | Loss: 0.00002071
Iteration 103/1000 | Loss: 0.00002071
Iteration 104/1000 | Loss: 0.00002071
Iteration 105/1000 | Loss: 0.00002071
Iteration 106/1000 | Loss: 0.00002071
Iteration 107/1000 | Loss: 0.00002071
Iteration 108/1000 | Loss: 0.00002071
Iteration 109/1000 | Loss: 0.00002071
Iteration 110/1000 | Loss: 0.00002071
Iteration 111/1000 | Loss: 0.00002071
Iteration 112/1000 | Loss: 0.00002071
Iteration 113/1000 | Loss: 0.00002071
Iteration 114/1000 | Loss: 0.00002071
Iteration 115/1000 | Loss: 0.00002071
Iteration 116/1000 | Loss: 0.00002071
Iteration 117/1000 | Loss: 0.00002071
Iteration 118/1000 | Loss: 0.00002071
Iteration 119/1000 | Loss: 0.00002071
Iteration 120/1000 | Loss: 0.00002071
Iteration 121/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.0712332116090693e-05, 2.0712332116090693e-05, 2.0712332116090693e-05, 2.0712332116090693e-05, 2.0712332116090693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0712332116090693e-05

Optimization complete. Final v2v error: 2.8965630531311035 mm

Highest mean error: 21.00527000427246 mm for frame 86

Lowest mean error: 2.402099370956421 mm for frame 21

Saving results

Total time: 99.78141021728516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075663
Iteration 2/25 | Loss: 0.00248708
Iteration 3/25 | Loss: 0.00172793
Iteration 4/25 | Loss: 0.00162009
Iteration 5/25 | Loss: 0.00177348
Iteration 6/25 | Loss: 0.00174113
Iteration 7/25 | Loss: 0.00161635
Iteration 8/25 | Loss: 0.00158811
Iteration 9/25 | Loss: 0.00144889
Iteration 10/25 | Loss: 0.00137241
Iteration 11/25 | Loss: 0.00129337
Iteration 12/25 | Loss: 0.00125860
Iteration 13/25 | Loss: 0.00124465
Iteration 14/25 | Loss: 0.00122379
Iteration 15/25 | Loss: 0.00119316
Iteration 16/25 | Loss: 0.00118489
Iteration 17/25 | Loss: 0.00118865
Iteration 18/25 | Loss: 0.00116716
Iteration 19/25 | Loss: 0.00116003
Iteration 20/25 | Loss: 0.00115787
Iteration 21/25 | Loss: 0.00114526
Iteration 22/25 | Loss: 0.00113453
Iteration 23/25 | Loss: 0.00112621
Iteration 24/25 | Loss: 0.00111654
Iteration 25/25 | Loss: 0.00111524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98642659
Iteration 2/25 | Loss: 0.00071663
Iteration 3/25 | Loss: 0.00071662
Iteration 4/25 | Loss: 0.00071662
Iteration 5/25 | Loss: 0.00071662
Iteration 6/25 | Loss: 0.00071662
Iteration 7/25 | Loss: 0.00071662
Iteration 8/25 | Loss: 0.00071662
Iteration 9/25 | Loss: 0.00071662
Iteration 10/25 | Loss: 0.00071662
Iteration 11/25 | Loss: 0.00071662
Iteration 12/25 | Loss: 0.00071662
Iteration 13/25 | Loss: 0.00071662
Iteration 14/25 | Loss: 0.00071662
Iteration 15/25 | Loss: 0.00071662
Iteration 16/25 | Loss: 0.00071662
Iteration 17/25 | Loss: 0.00071662
Iteration 18/25 | Loss: 0.00071662
Iteration 19/25 | Loss: 0.00071662
Iteration 20/25 | Loss: 0.00071662
Iteration 21/25 | Loss: 0.00071662
Iteration 22/25 | Loss: 0.00071662
Iteration 23/25 | Loss: 0.00071662
Iteration 24/25 | Loss: 0.00071662
Iteration 25/25 | Loss: 0.00071662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071662
Iteration 2/1000 | Loss: 0.00009809
Iteration 3/1000 | Loss: 0.00007351
Iteration 4/1000 | Loss: 0.00013395
Iteration 5/1000 | Loss: 0.00005887
Iteration 6/1000 | Loss: 0.00003837
Iteration 7/1000 | Loss: 0.00015178
Iteration 8/1000 | Loss: 0.00016592
Iteration 9/1000 | Loss: 0.00004610
Iteration 10/1000 | Loss: 0.00003125
Iteration 11/1000 | Loss: 0.00041505
Iteration 12/1000 | Loss: 0.00027467
Iteration 13/1000 | Loss: 0.00020052
Iteration 14/1000 | Loss: 0.00021953
Iteration 15/1000 | Loss: 0.00017949
Iteration 16/1000 | Loss: 0.00004198
Iteration 17/1000 | Loss: 0.00005178
Iteration 18/1000 | Loss: 0.00004905
Iteration 19/1000 | Loss: 0.00007179
Iteration 20/1000 | Loss: 0.00006617
Iteration 21/1000 | Loss: 0.00003547
Iteration 22/1000 | Loss: 0.00005337
Iteration 23/1000 | Loss: 0.00004810
Iteration 24/1000 | Loss: 0.00006337
Iteration 25/1000 | Loss: 0.00005219
Iteration 26/1000 | Loss: 0.00012380
Iteration 27/1000 | Loss: 0.00004774
Iteration 28/1000 | Loss: 0.00004803
Iteration 29/1000 | Loss: 0.00006136
Iteration 30/1000 | Loss: 0.00005425
Iteration 31/1000 | Loss: 0.00006004
Iteration 32/1000 | Loss: 0.00005916
Iteration 33/1000 | Loss: 0.00005489
Iteration 34/1000 | Loss: 0.00006218
Iteration 35/1000 | Loss: 0.00008423
Iteration 36/1000 | Loss: 0.00006026
Iteration 37/1000 | Loss: 0.00006559
Iteration 38/1000 | Loss: 0.00006327
Iteration 39/1000 | Loss: 0.00006617
Iteration 40/1000 | Loss: 0.00006370
Iteration 41/1000 | Loss: 0.00004974
Iteration 42/1000 | Loss: 0.00006168
Iteration 43/1000 | Loss: 0.00004554
Iteration 44/1000 | Loss: 0.00005687
Iteration 45/1000 | Loss: 0.00006227
Iteration 46/1000 | Loss: 0.00004316
Iteration 47/1000 | Loss: 0.00004009
Iteration 48/1000 | Loss: 0.00004370
Iteration 49/1000 | Loss: 0.00004402
Iteration 50/1000 | Loss: 0.00004718
Iteration 51/1000 | Loss: 0.00004636
Iteration 52/1000 | Loss: 0.00005202
Iteration 53/1000 | Loss: 0.00006907
Iteration 54/1000 | Loss: 0.00005171
Iteration 55/1000 | Loss: 0.00004819
Iteration 56/1000 | Loss: 0.00004494
Iteration 57/1000 | Loss: 0.00004464
Iteration 58/1000 | Loss: 0.00003130
Iteration 59/1000 | Loss: 0.00003358
Iteration 60/1000 | Loss: 0.00004359
Iteration 61/1000 | Loss: 0.00003781
Iteration 62/1000 | Loss: 0.00003769
Iteration 63/1000 | Loss: 0.00005149
Iteration 64/1000 | Loss: 0.00004433
Iteration 65/1000 | Loss: 0.00003385
Iteration 66/1000 | Loss: 0.00002478
Iteration 67/1000 | Loss: 0.00002622
Iteration 68/1000 | Loss: 0.00002368
Iteration 69/1000 | Loss: 0.00005167
Iteration 70/1000 | Loss: 0.00004125
Iteration 71/1000 | Loss: 0.00005513
Iteration 72/1000 | Loss: 0.00005174
Iteration 73/1000 | Loss: 0.00006138
Iteration 74/1000 | Loss: 0.00006207
Iteration 75/1000 | Loss: 0.00004282
Iteration 76/1000 | Loss: 0.00004481
Iteration 77/1000 | Loss: 0.00003157
Iteration 78/1000 | Loss: 0.00003272
Iteration 79/1000 | Loss: 0.00005721
Iteration 80/1000 | Loss: 0.00005517
Iteration 81/1000 | Loss: 0.00004865
Iteration 82/1000 | Loss: 0.00007082
Iteration 83/1000 | Loss: 0.00006080
Iteration 84/1000 | Loss: 0.00003680
Iteration 85/1000 | Loss: 0.00002206
Iteration 86/1000 | Loss: 0.00006490
Iteration 87/1000 | Loss: 0.00004591
Iteration 88/1000 | Loss: 0.00001971
Iteration 89/1000 | Loss: 0.00001833
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001877
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00002653
Iteration 96/1000 | Loss: 0.00003050
Iteration 97/1000 | Loss: 0.00002643
Iteration 98/1000 | Loss: 0.00003384
Iteration 99/1000 | Loss: 0.00003218
Iteration 100/1000 | Loss: 0.00003281
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00002993
Iteration 103/1000 | Loss: 0.00001729
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00002512
Iteration 107/1000 | Loss: 0.00002043
Iteration 108/1000 | Loss: 0.00003298
Iteration 109/1000 | Loss: 0.00002591
Iteration 110/1000 | Loss: 0.00003154
Iteration 111/1000 | Loss: 0.00003319
Iteration 112/1000 | Loss: 0.00003170
Iteration 113/1000 | Loss: 0.00002382
Iteration 114/1000 | Loss: 0.00003217
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001643
Iteration 118/1000 | Loss: 0.00002077
Iteration 119/1000 | Loss: 0.00001861
Iteration 120/1000 | Loss: 0.00003756
Iteration 121/1000 | Loss: 0.00002379
Iteration 122/1000 | Loss: 0.00003029
Iteration 123/1000 | Loss: 0.00003235
Iteration 124/1000 | Loss: 0.00003091
Iteration 125/1000 | Loss: 0.00002452
Iteration 126/1000 | Loss: 0.00002933
Iteration 127/1000 | Loss: 0.00003101
Iteration 128/1000 | Loss: 0.00002993
Iteration 129/1000 | Loss: 0.00003076
Iteration 130/1000 | Loss: 0.00002037
Iteration 131/1000 | Loss: 0.00004783
Iteration 132/1000 | Loss: 0.00002973
Iteration 133/1000 | Loss: 0.00003267
Iteration 134/1000 | Loss: 0.00002987
Iteration 135/1000 | Loss: 0.00003156
Iteration 136/1000 | Loss: 0.00002880
Iteration 137/1000 | Loss: 0.00003554
Iteration 138/1000 | Loss: 0.00002832
Iteration 139/1000 | Loss: 0.00002908
Iteration 140/1000 | Loss: 0.00001877
Iteration 141/1000 | Loss: 0.00003615
Iteration 142/1000 | Loss: 0.00002287
Iteration 143/1000 | Loss: 0.00002747
Iteration 144/1000 | Loss: 0.00003072
Iteration 145/1000 | Loss: 0.00004326
Iteration 146/1000 | Loss: 0.00003505
Iteration 147/1000 | Loss: 0.00003434
Iteration 148/1000 | Loss: 0.00003081
Iteration 149/1000 | Loss: 0.00002130
Iteration 150/1000 | Loss: 0.00004437
Iteration 151/1000 | Loss: 0.00004370
Iteration 152/1000 | Loss: 0.00003769
Iteration 153/1000 | Loss: 0.00004240
Iteration 154/1000 | Loss: 0.00003723
Iteration 155/1000 | Loss: 0.00005198
Iteration 156/1000 | Loss: 0.00003013
Iteration 157/1000 | Loss: 0.00003257
Iteration 158/1000 | Loss: 0.00003578
Iteration 159/1000 | Loss: 0.00003729
Iteration 160/1000 | Loss: 0.00012115
Iteration 161/1000 | Loss: 0.00003010
Iteration 162/1000 | Loss: 0.00002529
Iteration 163/1000 | Loss: 0.00002231
Iteration 164/1000 | Loss: 0.00002048
Iteration 165/1000 | Loss: 0.00002242
Iteration 166/1000 | Loss: 0.00001854
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001527
Iteration 169/1000 | Loss: 0.00001471
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001413
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001377
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001373
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001372
Iteration 178/1000 | Loss: 0.00001372
Iteration 179/1000 | Loss: 0.00001372
Iteration 180/1000 | Loss: 0.00001372
Iteration 181/1000 | Loss: 0.00001372
Iteration 182/1000 | Loss: 0.00001372
Iteration 183/1000 | Loss: 0.00001372
Iteration 184/1000 | Loss: 0.00001371
Iteration 185/1000 | Loss: 0.00001371
Iteration 186/1000 | Loss: 0.00001371
Iteration 187/1000 | Loss: 0.00001371
Iteration 188/1000 | Loss: 0.00001371
Iteration 189/1000 | Loss: 0.00001371
Iteration 190/1000 | Loss: 0.00001371
Iteration 191/1000 | Loss: 0.00001371
Iteration 192/1000 | Loss: 0.00001371
Iteration 193/1000 | Loss: 0.00001371
Iteration 194/1000 | Loss: 0.00001371
Iteration 195/1000 | Loss: 0.00001370
Iteration 196/1000 | Loss: 0.00001370
Iteration 197/1000 | Loss: 0.00001370
Iteration 198/1000 | Loss: 0.00001370
Iteration 199/1000 | Loss: 0.00001370
Iteration 200/1000 | Loss: 0.00001370
Iteration 201/1000 | Loss: 0.00001370
Iteration 202/1000 | Loss: 0.00001370
Iteration 203/1000 | Loss: 0.00001370
Iteration 204/1000 | Loss: 0.00001370
Iteration 205/1000 | Loss: 0.00001370
Iteration 206/1000 | Loss: 0.00001370
Iteration 207/1000 | Loss: 0.00001367
Iteration 208/1000 | Loss: 0.00001367
Iteration 209/1000 | Loss: 0.00001367
Iteration 210/1000 | Loss: 0.00001366
Iteration 211/1000 | Loss: 0.00001366
Iteration 212/1000 | Loss: 0.00001366
Iteration 213/1000 | Loss: 0.00001365
Iteration 214/1000 | Loss: 0.00002871
Iteration 215/1000 | Loss: 0.00001685
Iteration 216/1000 | Loss: 0.00001487
Iteration 217/1000 | Loss: 0.00001418
Iteration 218/1000 | Loss: 0.00001372
Iteration 219/1000 | Loss: 0.00001338
Iteration 220/1000 | Loss: 0.00001338
Iteration 221/1000 | Loss: 0.00001338
Iteration 222/1000 | Loss: 0.00001333
Iteration 223/1000 | Loss: 0.00001330
Iteration 224/1000 | Loss: 0.00001330
Iteration 225/1000 | Loss: 0.00001329
Iteration 226/1000 | Loss: 0.00001329
Iteration 227/1000 | Loss: 0.00001323
Iteration 228/1000 | Loss: 0.00001322
Iteration 229/1000 | Loss: 0.00001322
Iteration 230/1000 | Loss: 0.00001321
Iteration 231/1000 | Loss: 0.00001321
Iteration 232/1000 | Loss: 0.00001321
Iteration 233/1000 | Loss: 0.00001321
Iteration 234/1000 | Loss: 0.00001321
Iteration 235/1000 | Loss: 0.00001321
Iteration 236/1000 | Loss: 0.00001321
Iteration 237/1000 | Loss: 0.00001321
Iteration 238/1000 | Loss: 0.00001321
Iteration 239/1000 | Loss: 0.00001321
Iteration 240/1000 | Loss: 0.00001320
Iteration 241/1000 | Loss: 0.00001320
Iteration 242/1000 | Loss: 0.00001320
Iteration 243/1000 | Loss: 0.00001320
Iteration 244/1000 | Loss: 0.00001320
Iteration 245/1000 | Loss: 0.00001320
Iteration 246/1000 | Loss: 0.00001320
Iteration 247/1000 | Loss: 0.00001320
Iteration 248/1000 | Loss: 0.00001319
Iteration 249/1000 | Loss: 0.00001319
Iteration 250/1000 | Loss: 0.00001319
Iteration 251/1000 | Loss: 0.00001319
Iteration 252/1000 | Loss: 0.00001319
Iteration 253/1000 | Loss: 0.00001319
Iteration 254/1000 | Loss: 0.00001319
Iteration 255/1000 | Loss: 0.00001319
Iteration 256/1000 | Loss: 0.00001318
Iteration 257/1000 | Loss: 0.00001318
Iteration 258/1000 | Loss: 0.00001318
Iteration 259/1000 | Loss: 0.00001317
Iteration 260/1000 | Loss: 0.00001317
Iteration 261/1000 | Loss: 0.00001316
Iteration 262/1000 | Loss: 0.00001315
Iteration 263/1000 | Loss: 0.00001313
Iteration 264/1000 | Loss: 0.00001313
Iteration 265/1000 | Loss: 0.00001313
Iteration 266/1000 | Loss: 0.00001313
Iteration 267/1000 | Loss: 0.00001313
Iteration 268/1000 | Loss: 0.00001313
Iteration 269/1000 | Loss: 0.00001313
Iteration 270/1000 | Loss: 0.00001313
Iteration 271/1000 | Loss: 0.00001313
Iteration 272/1000 | Loss: 0.00001313
Iteration 273/1000 | Loss: 0.00001312
Iteration 274/1000 | Loss: 0.00001312
Iteration 275/1000 | Loss: 0.00001311
Iteration 276/1000 | Loss: 0.00001311
Iteration 277/1000 | Loss: 0.00001311
Iteration 278/1000 | Loss: 0.00001310
Iteration 279/1000 | Loss: 0.00001310
Iteration 280/1000 | Loss: 0.00001310
Iteration 281/1000 | Loss: 0.00001310
Iteration 282/1000 | Loss: 0.00001310
Iteration 283/1000 | Loss: 0.00001309
Iteration 284/1000 | Loss: 0.00001309
Iteration 285/1000 | Loss: 0.00001309
Iteration 286/1000 | Loss: 0.00001309
Iteration 287/1000 | Loss: 0.00001309
Iteration 288/1000 | Loss: 0.00001308
Iteration 289/1000 | Loss: 0.00001307
Iteration 290/1000 | Loss: 0.00001307
Iteration 291/1000 | Loss: 0.00001307
Iteration 292/1000 | Loss: 0.00001307
Iteration 293/1000 | Loss: 0.00001307
Iteration 294/1000 | Loss: 0.00001307
Iteration 295/1000 | Loss: 0.00001307
Iteration 296/1000 | Loss: 0.00001307
Iteration 297/1000 | Loss: 0.00001307
Iteration 298/1000 | Loss: 0.00001307
Iteration 299/1000 | Loss: 0.00001307
Iteration 300/1000 | Loss: 0.00001307
Iteration 301/1000 | Loss: 0.00001307
Iteration 302/1000 | Loss: 0.00001306
Iteration 303/1000 | Loss: 0.00001306
Iteration 304/1000 | Loss: 0.00001306
Iteration 305/1000 | Loss: 0.00001306
Iteration 306/1000 | Loss: 0.00001306
Iteration 307/1000 | Loss: 0.00001306
Iteration 308/1000 | Loss: 0.00001306
Iteration 309/1000 | Loss: 0.00001306
Iteration 310/1000 | Loss: 0.00001306
Iteration 311/1000 | Loss: 0.00001306
Iteration 312/1000 | Loss: 0.00001306
Iteration 313/1000 | Loss: 0.00001305
Iteration 314/1000 | Loss: 0.00001305
Iteration 315/1000 | Loss: 0.00001305
Iteration 316/1000 | Loss: 0.00001305
Iteration 317/1000 | Loss: 0.00001305
Iteration 318/1000 | Loss: 0.00001305
Iteration 319/1000 | Loss: 0.00001305
Iteration 320/1000 | Loss: 0.00001305
Iteration 321/1000 | Loss: 0.00001305
Iteration 322/1000 | Loss: 0.00001305
Iteration 323/1000 | Loss: 0.00001305
Iteration 324/1000 | Loss: 0.00001304
Iteration 325/1000 | Loss: 0.00001304
Iteration 326/1000 | Loss: 0.00001304
Iteration 327/1000 | Loss: 0.00001304
Iteration 328/1000 | Loss: 0.00001304
Iteration 329/1000 | Loss: 0.00001304
Iteration 330/1000 | Loss: 0.00001304
Iteration 331/1000 | Loss: 0.00001303
Iteration 332/1000 | Loss: 0.00001303
Iteration 333/1000 | Loss: 0.00001303
Iteration 334/1000 | Loss: 0.00001303
Iteration 335/1000 | Loss: 0.00001303
Iteration 336/1000 | Loss: 0.00001303
Iteration 337/1000 | Loss: 0.00001303
Iteration 338/1000 | Loss: 0.00001303
Iteration 339/1000 | Loss: 0.00001303
Iteration 340/1000 | Loss: 0.00001303
Iteration 341/1000 | Loss: 0.00001303
Iteration 342/1000 | Loss: 0.00001303
Iteration 343/1000 | Loss: 0.00001303
Iteration 344/1000 | Loss: 0.00001303
Iteration 345/1000 | Loss: 0.00001303
Iteration 346/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 346. Stopping optimization.
Last 5 losses: [1.3028295143158175e-05, 1.3028295143158175e-05, 1.3028295143158175e-05, 1.3028295143158175e-05, 1.3028295143158175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3028295143158175e-05

Optimization complete. Final v2v error: 2.989393472671509 mm

Highest mean error: 5.46695613861084 mm for frame 10

Lowest mean error: 2.722073554992676 mm for frame 8

Saving results

Total time: 323.2117042541504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053765
Iteration 2/25 | Loss: 0.00265417
Iteration 3/25 | Loss: 0.00151654
Iteration 4/25 | Loss: 0.00125308
Iteration 5/25 | Loss: 0.00116324
Iteration 6/25 | Loss: 0.00119628
Iteration 7/25 | Loss: 0.00118332
Iteration 8/25 | Loss: 0.00116158
Iteration 9/25 | Loss: 0.00111483
Iteration 10/25 | Loss: 0.00112451
Iteration 11/25 | Loss: 0.00112578
Iteration 12/25 | Loss: 0.00111219
Iteration 13/25 | Loss: 0.00109021
Iteration 14/25 | Loss: 0.00108533
Iteration 15/25 | Loss: 0.00107998
Iteration 16/25 | Loss: 0.00108627
Iteration 17/25 | Loss: 0.00107354
Iteration 18/25 | Loss: 0.00106993
Iteration 19/25 | Loss: 0.00106652
Iteration 20/25 | Loss: 0.00106187
Iteration 21/25 | Loss: 0.00106254
Iteration 22/25 | Loss: 0.00106249
Iteration 23/25 | Loss: 0.00106191
Iteration 24/25 | Loss: 0.00106501
Iteration 25/25 | Loss: 0.00106426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.72977209
Iteration 2/25 | Loss: 0.00135379
Iteration 3/25 | Loss: 0.00127221
Iteration 4/25 | Loss: 0.00125204
Iteration 5/25 | Loss: 0.00125203
Iteration 6/25 | Loss: 0.00125203
Iteration 7/25 | Loss: 0.00125203
Iteration 8/25 | Loss: 0.00125203
Iteration 9/25 | Loss: 0.00125203
Iteration 10/25 | Loss: 0.00125203
Iteration 11/25 | Loss: 0.00125203
Iteration 12/25 | Loss: 0.00125203
Iteration 13/25 | Loss: 0.00125203
Iteration 14/25 | Loss: 0.00125203
Iteration 15/25 | Loss: 0.00125203
Iteration 16/25 | Loss: 0.00125203
Iteration 17/25 | Loss: 0.00125203
Iteration 18/25 | Loss: 0.00125203
Iteration 19/25 | Loss: 0.00125203
Iteration 20/25 | Loss: 0.00125203
Iteration 21/25 | Loss: 0.00125203
Iteration 22/25 | Loss: 0.00125203
Iteration 23/25 | Loss: 0.00125203
Iteration 24/25 | Loss: 0.00125203
Iteration 25/25 | Loss: 0.00125203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125203
Iteration 2/1000 | Loss: 0.00027803
Iteration 3/1000 | Loss: 0.00029964
Iteration 4/1000 | Loss: 0.00020003
Iteration 5/1000 | Loss: 0.00026942
Iteration 6/1000 | Loss: 0.00022310
Iteration 7/1000 | Loss: 0.00028682
Iteration 8/1000 | Loss: 0.00046272
Iteration 9/1000 | Loss: 0.00032232
Iteration 10/1000 | Loss: 0.00024922
Iteration 11/1000 | Loss: 0.00027698
Iteration 12/1000 | Loss: 0.00032693
Iteration 13/1000 | Loss: 0.00028570
Iteration 14/1000 | Loss: 0.00022587
Iteration 15/1000 | Loss: 0.00030536
Iteration 16/1000 | Loss: 0.00015105
Iteration 17/1000 | Loss: 0.00041910
Iteration 18/1000 | Loss: 0.00028574
Iteration 19/1000 | Loss: 0.00017664
Iteration 20/1000 | Loss: 0.00033181
Iteration 21/1000 | Loss: 0.00020757
Iteration 22/1000 | Loss: 0.00025292
Iteration 23/1000 | Loss: 0.00018278
Iteration 24/1000 | Loss: 0.00017499
Iteration 25/1000 | Loss: 0.00038420
Iteration 26/1000 | Loss: 0.00055080
Iteration 27/1000 | Loss: 0.00024244
Iteration 28/1000 | Loss: 0.00021028
Iteration 29/1000 | Loss: 0.00058834
Iteration 30/1000 | Loss: 0.00014257
Iteration 31/1000 | Loss: 0.00006982
Iteration 32/1000 | Loss: 0.00034783
Iteration 33/1000 | Loss: 0.00005773
Iteration 34/1000 | Loss: 0.00003056
Iteration 35/1000 | Loss: 0.00005683
Iteration 36/1000 | Loss: 0.00015572
Iteration 37/1000 | Loss: 0.00093804
Iteration 38/1000 | Loss: 0.00062130
Iteration 39/1000 | Loss: 0.00063969
Iteration 40/1000 | Loss: 0.00032406
Iteration 41/1000 | Loss: 0.00036615
Iteration 42/1000 | Loss: 0.00017732
Iteration 43/1000 | Loss: 0.00041449
Iteration 44/1000 | Loss: 0.00009335
Iteration 45/1000 | Loss: 0.00033768
Iteration 46/1000 | Loss: 0.00018942
Iteration 47/1000 | Loss: 0.00024352
Iteration 48/1000 | Loss: 0.00018128
Iteration 49/1000 | Loss: 0.00008153
Iteration 50/1000 | Loss: 0.00020550
Iteration 51/1000 | Loss: 0.00002275
Iteration 52/1000 | Loss: 0.00002008
Iteration 53/1000 | Loss: 0.00001852
Iteration 54/1000 | Loss: 0.00055324
Iteration 55/1000 | Loss: 0.00018793
Iteration 56/1000 | Loss: 0.00005987
Iteration 57/1000 | Loss: 0.00052753
Iteration 58/1000 | Loss: 0.00034805
Iteration 59/1000 | Loss: 0.00002487
Iteration 60/1000 | Loss: 0.00014235
Iteration 61/1000 | Loss: 0.00017986
Iteration 62/1000 | Loss: 0.00027273
Iteration 63/1000 | Loss: 0.00018155
Iteration 64/1000 | Loss: 0.00023468
Iteration 65/1000 | Loss: 0.00017545
Iteration 66/1000 | Loss: 0.00011634
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00018810
Iteration 69/1000 | Loss: 0.00002088
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00004116
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00003507
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00003714
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001130
Iteration 87/1000 | Loss: 0.00001130
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001129
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001124
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001120
Iteration 104/1000 | Loss: 0.00001120
Iteration 105/1000 | Loss: 0.00001120
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001114
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001104
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001103
Iteration 158/1000 | Loss: 0.00001103
Iteration 159/1000 | Loss: 0.00001103
Iteration 160/1000 | Loss: 0.00001103
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001100
Iteration 190/1000 | Loss: 0.00001100
Iteration 191/1000 | Loss: 0.00001100
Iteration 192/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.099944711313583e-05, 1.099944711313583e-05, 1.099944711313583e-05, 1.099944711313583e-05, 1.099944711313583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.099944711313583e-05

Optimization complete. Final v2v error: 2.7398269176483154 mm

Highest mean error: 5.783007621765137 mm for frame 90

Lowest mean error: 2.454615354537964 mm for frame 151

Saving results

Total time: 178.19818305969238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470341
Iteration 2/25 | Loss: 0.00127925
Iteration 3/25 | Loss: 0.00112578
Iteration 4/25 | Loss: 0.00111337
Iteration 5/25 | Loss: 0.00111062
Iteration 6/25 | Loss: 0.00111062
Iteration 7/25 | Loss: 0.00111062
Iteration 8/25 | Loss: 0.00111062
Iteration 9/25 | Loss: 0.00111062
Iteration 10/25 | Loss: 0.00111062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001110618351958692, 0.001110618351958692, 0.001110618351958692, 0.001110618351958692, 0.001110618351958692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001110618351958692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28765428
Iteration 2/25 | Loss: 0.00117398
Iteration 3/25 | Loss: 0.00117398
Iteration 4/25 | Loss: 0.00117398
Iteration 5/25 | Loss: 0.00117398
Iteration 6/25 | Loss: 0.00117398
Iteration 7/25 | Loss: 0.00117398
Iteration 8/25 | Loss: 0.00117398
Iteration 9/25 | Loss: 0.00117398
Iteration 10/25 | Loss: 0.00117398
Iteration 11/25 | Loss: 0.00117398
Iteration 12/25 | Loss: 0.00117398
Iteration 13/25 | Loss: 0.00117398
Iteration 14/25 | Loss: 0.00117398
Iteration 15/25 | Loss: 0.00117398
Iteration 16/25 | Loss: 0.00117398
Iteration 17/25 | Loss: 0.00117398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011739793699234724, 0.0011739793699234724, 0.0011739793699234724, 0.0011739793699234724, 0.0011739793699234724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011739793699234724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117398
Iteration 2/1000 | Loss: 0.00003562
Iteration 3/1000 | Loss: 0.00002062
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001424
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001347
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001337
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001336
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001329
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001318
Iteration 54/1000 | Loss: 0.00001315
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001306
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001302
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001300
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.2999342288821936e-05, 1.2999342288821936e-05, 1.2999342288821936e-05, 1.2999342288821936e-05, 1.2999342288821936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2999342288821936e-05

Optimization complete. Final v2v error: 3.052391767501831 mm

Highest mean error: 3.482548236846924 mm for frame 91

Lowest mean error: 2.6371819972991943 mm for frame 210

Saving results

Total time: 40.37296748161316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339443
Iteration 2/25 | Loss: 0.00138603
Iteration 3/25 | Loss: 0.00108390
Iteration 4/25 | Loss: 0.00103707
Iteration 5/25 | Loss: 0.00102908
Iteration 6/25 | Loss: 0.00102647
Iteration 7/25 | Loss: 0.00102566
Iteration 8/25 | Loss: 0.00102566
Iteration 9/25 | Loss: 0.00102566
Iteration 10/25 | Loss: 0.00102566
Iteration 11/25 | Loss: 0.00102566
Iteration 12/25 | Loss: 0.00102566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010256589157506824, 0.0010256589157506824, 0.0010256589157506824, 0.0010256589157506824, 0.0010256589157506824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010256589157506824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32840717
Iteration 2/25 | Loss: 0.00123298
Iteration 3/25 | Loss: 0.00123298
Iteration 4/25 | Loss: 0.00123298
Iteration 5/25 | Loss: 0.00123298
Iteration 6/25 | Loss: 0.00123298
Iteration 7/25 | Loss: 0.00123298
Iteration 8/25 | Loss: 0.00123298
Iteration 9/25 | Loss: 0.00123298
Iteration 10/25 | Loss: 0.00123298
Iteration 11/25 | Loss: 0.00123298
Iteration 12/25 | Loss: 0.00123298
Iteration 13/25 | Loss: 0.00123298
Iteration 14/25 | Loss: 0.00123298
Iteration 15/25 | Loss: 0.00123298
Iteration 16/25 | Loss: 0.00123298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001232982031069696, 0.001232982031069696, 0.001232982031069696, 0.001232982031069696, 0.001232982031069696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001232982031069696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123298
Iteration 2/1000 | Loss: 0.00004394
Iteration 3/1000 | Loss: 0.00002160
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001242
Iteration 6/1000 | Loss: 0.00001147
Iteration 7/1000 | Loss: 0.00001099
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001033
Iteration 10/1000 | Loss: 0.00001010
Iteration 11/1000 | Loss: 0.00000995
Iteration 12/1000 | Loss: 0.00000993
Iteration 13/1000 | Loss: 0.00000985
Iteration 14/1000 | Loss: 0.00000982
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000981
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000980
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000979
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000978
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000977
Iteration 26/1000 | Loss: 0.00000977
Iteration 27/1000 | Loss: 0.00000976
Iteration 28/1000 | Loss: 0.00000976
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000975
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000974
Iteration 34/1000 | Loss: 0.00000974
Iteration 35/1000 | Loss: 0.00000974
Iteration 36/1000 | Loss: 0.00000974
Iteration 37/1000 | Loss: 0.00000973
Iteration 38/1000 | Loss: 0.00000973
Iteration 39/1000 | Loss: 0.00000973
Iteration 40/1000 | Loss: 0.00000972
Iteration 41/1000 | Loss: 0.00000972
Iteration 42/1000 | Loss: 0.00000972
Iteration 43/1000 | Loss: 0.00000971
Iteration 44/1000 | Loss: 0.00000971
Iteration 45/1000 | Loss: 0.00000971
Iteration 46/1000 | Loss: 0.00000971
Iteration 47/1000 | Loss: 0.00000970
Iteration 48/1000 | Loss: 0.00000970
Iteration 49/1000 | Loss: 0.00000970
Iteration 50/1000 | Loss: 0.00000970
Iteration 51/1000 | Loss: 0.00000969
Iteration 52/1000 | Loss: 0.00000969
Iteration 53/1000 | Loss: 0.00000969
Iteration 54/1000 | Loss: 0.00000968
Iteration 55/1000 | Loss: 0.00000968
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000967
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000966
Iteration 64/1000 | Loss: 0.00000966
Iteration 65/1000 | Loss: 0.00000966
Iteration 66/1000 | Loss: 0.00000965
Iteration 67/1000 | Loss: 0.00000965
Iteration 68/1000 | Loss: 0.00000965
Iteration 69/1000 | Loss: 0.00000964
Iteration 70/1000 | Loss: 0.00000964
Iteration 71/1000 | Loss: 0.00000964
Iteration 72/1000 | Loss: 0.00000964
Iteration 73/1000 | Loss: 0.00000964
Iteration 74/1000 | Loss: 0.00000963
Iteration 75/1000 | Loss: 0.00000963
Iteration 76/1000 | Loss: 0.00000963
Iteration 77/1000 | Loss: 0.00000963
Iteration 78/1000 | Loss: 0.00000963
Iteration 79/1000 | Loss: 0.00000962
Iteration 80/1000 | Loss: 0.00000962
Iteration 81/1000 | Loss: 0.00000962
Iteration 82/1000 | Loss: 0.00000962
Iteration 83/1000 | Loss: 0.00000962
Iteration 84/1000 | Loss: 0.00000962
Iteration 85/1000 | Loss: 0.00000962
Iteration 86/1000 | Loss: 0.00000962
Iteration 87/1000 | Loss: 0.00000961
Iteration 88/1000 | Loss: 0.00000961
Iteration 89/1000 | Loss: 0.00000961
Iteration 90/1000 | Loss: 0.00000961
Iteration 91/1000 | Loss: 0.00000961
Iteration 92/1000 | Loss: 0.00000961
Iteration 93/1000 | Loss: 0.00000961
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000961
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000960
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000960
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000958
Iteration 120/1000 | Loss: 0.00000958
Iteration 121/1000 | Loss: 0.00000958
Iteration 122/1000 | Loss: 0.00000958
Iteration 123/1000 | Loss: 0.00000958
Iteration 124/1000 | Loss: 0.00000958
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000958
Iteration 129/1000 | Loss: 0.00000958
Iteration 130/1000 | Loss: 0.00000957
Iteration 131/1000 | Loss: 0.00000957
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000957
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000956
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000956
Iteration 147/1000 | Loss: 0.00000955
Iteration 148/1000 | Loss: 0.00000955
Iteration 149/1000 | Loss: 0.00000955
Iteration 150/1000 | Loss: 0.00000955
Iteration 151/1000 | Loss: 0.00000955
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000955
Iteration 155/1000 | Loss: 0.00000955
Iteration 156/1000 | Loss: 0.00000955
Iteration 157/1000 | Loss: 0.00000955
Iteration 158/1000 | Loss: 0.00000955
Iteration 159/1000 | Loss: 0.00000955
Iteration 160/1000 | Loss: 0.00000954
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000954
Iteration 163/1000 | Loss: 0.00000954
Iteration 164/1000 | Loss: 0.00000954
Iteration 165/1000 | Loss: 0.00000954
Iteration 166/1000 | Loss: 0.00000954
Iteration 167/1000 | Loss: 0.00000954
Iteration 168/1000 | Loss: 0.00000954
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000954
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000954
Iteration 178/1000 | Loss: 0.00000954
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000953
Iteration 186/1000 | Loss: 0.00000953
Iteration 187/1000 | Loss: 0.00000953
Iteration 188/1000 | Loss: 0.00000953
Iteration 189/1000 | Loss: 0.00000953
Iteration 190/1000 | Loss: 0.00000953
Iteration 191/1000 | Loss: 0.00000953
Iteration 192/1000 | Loss: 0.00000953
Iteration 193/1000 | Loss: 0.00000953
Iteration 194/1000 | Loss: 0.00000953
Iteration 195/1000 | Loss: 0.00000953
Iteration 196/1000 | Loss: 0.00000952
Iteration 197/1000 | Loss: 0.00000952
Iteration 198/1000 | Loss: 0.00000952
Iteration 199/1000 | Loss: 0.00000952
Iteration 200/1000 | Loss: 0.00000952
Iteration 201/1000 | Loss: 0.00000952
Iteration 202/1000 | Loss: 0.00000952
Iteration 203/1000 | Loss: 0.00000952
Iteration 204/1000 | Loss: 0.00000952
Iteration 205/1000 | Loss: 0.00000952
Iteration 206/1000 | Loss: 0.00000952
Iteration 207/1000 | Loss: 0.00000952
Iteration 208/1000 | Loss: 0.00000951
Iteration 209/1000 | Loss: 0.00000951
Iteration 210/1000 | Loss: 0.00000951
Iteration 211/1000 | Loss: 0.00000951
Iteration 212/1000 | Loss: 0.00000951
Iteration 213/1000 | Loss: 0.00000951
Iteration 214/1000 | Loss: 0.00000951
Iteration 215/1000 | Loss: 0.00000951
Iteration 216/1000 | Loss: 0.00000951
Iteration 217/1000 | Loss: 0.00000951
Iteration 218/1000 | Loss: 0.00000951
Iteration 219/1000 | Loss: 0.00000951
Iteration 220/1000 | Loss: 0.00000951
Iteration 221/1000 | Loss: 0.00000951
Iteration 222/1000 | Loss: 0.00000951
Iteration 223/1000 | Loss: 0.00000951
Iteration 224/1000 | Loss: 0.00000951
Iteration 225/1000 | Loss: 0.00000951
Iteration 226/1000 | Loss: 0.00000950
Iteration 227/1000 | Loss: 0.00000950
Iteration 228/1000 | Loss: 0.00000950
Iteration 229/1000 | Loss: 0.00000950
Iteration 230/1000 | Loss: 0.00000950
Iteration 231/1000 | Loss: 0.00000950
Iteration 232/1000 | Loss: 0.00000950
Iteration 233/1000 | Loss: 0.00000950
Iteration 234/1000 | Loss: 0.00000950
Iteration 235/1000 | Loss: 0.00000950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [9.50340381677961e-06, 9.50340381677961e-06, 9.50340381677961e-06, 9.50340381677961e-06, 9.50340381677961e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.50340381677961e-06

Optimization complete. Final v2v error: 2.6523189544677734 mm

Highest mean error: 3.037327289581299 mm for frame 28

Lowest mean error: 2.2133171558380127 mm for frame 10

Saving results

Total time: 40.986807107925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461732
Iteration 2/25 | Loss: 0.00113883
Iteration 3/25 | Loss: 0.00103884
Iteration 4/25 | Loss: 0.00102781
Iteration 5/25 | Loss: 0.00102550
Iteration 6/25 | Loss: 0.00102550
Iteration 7/25 | Loss: 0.00102550
Iteration 8/25 | Loss: 0.00102550
Iteration 9/25 | Loss: 0.00102550
Iteration 10/25 | Loss: 0.00102550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010255001252517104, 0.0010255001252517104, 0.0010255001252517104, 0.0010255001252517104, 0.0010255001252517104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010255001252517104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.79452467
Iteration 2/25 | Loss: 0.00088704
Iteration 3/25 | Loss: 0.00088704
Iteration 4/25 | Loss: 0.00088704
Iteration 5/25 | Loss: 0.00088704
Iteration 6/25 | Loss: 0.00088704
Iteration 7/25 | Loss: 0.00088704
Iteration 8/25 | Loss: 0.00088704
Iteration 9/25 | Loss: 0.00088704
Iteration 10/25 | Loss: 0.00088704
Iteration 11/25 | Loss: 0.00088704
Iteration 12/25 | Loss: 0.00088704
Iteration 13/25 | Loss: 0.00088704
Iteration 14/25 | Loss: 0.00088704
Iteration 15/25 | Loss: 0.00088704
Iteration 16/25 | Loss: 0.00088704
Iteration 17/25 | Loss: 0.00088704
Iteration 18/25 | Loss: 0.00088704
Iteration 19/25 | Loss: 0.00088704
Iteration 20/25 | Loss: 0.00088704
Iteration 21/25 | Loss: 0.00088704
Iteration 22/25 | Loss: 0.00088704
Iteration 23/25 | Loss: 0.00088704
Iteration 24/25 | Loss: 0.00088704
Iteration 25/25 | Loss: 0.00088704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088704
Iteration 2/1000 | Loss: 0.00001740
Iteration 3/1000 | Loss: 0.00001362
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001142
Iteration 7/1000 | Loss: 0.00001107
Iteration 8/1000 | Loss: 0.00001083
Iteration 9/1000 | Loss: 0.00001064
Iteration 10/1000 | Loss: 0.00001046
Iteration 11/1000 | Loss: 0.00001044
Iteration 12/1000 | Loss: 0.00001041
Iteration 13/1000 | Loss: 0.00001035
Iteration 14/1000 | Loss: 0.00001035
Iteration 15/1000 | Loss: 0.00001034
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001028
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001019
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001011
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001009
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00001000
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00000999
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000999
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000999
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000999
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000999
Iteration 47/1000 | Loss: 0.00000999
Iteration 48/1000 | Loss: 0.00000999
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000996
Iteration 51/1000 | Loss: 0.00000996
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000995
Iteration 55/1000 | Loss: 0.00000995
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000994
Iteration 62/1000 | Loss: 0.00000993
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000992
Iteration 71/1000 | Loss: 0.00000992
Iteration 72/1000 | Loss: 0.00000992
Iteration 73/1000 | Loss: 0.00000992
Iteration 74/1000 | Loss: 0.00000992
Iteration 75/1000 | Loss: 0.00000991
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000991
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000989
Iteration 84/1000 | Loss: 0.00000989
Iteration 85/1000 | Loss: 0.00000989
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000989
Iteration 90/1000 | Loss: 0.00000989
Iteration 91/1000 | Loss: 0.00000988
Iteration 92/1000 | Loss: 0.00000988
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000986
Iteration 96/1000 | Loss: 0.00000986
Iteration 97/1000 | Loss: 0.00000986
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000983
Iteration 105/1000 | Loss: 0.00000983
Iteration 106/1000 | Loss: 0.00000983
Iteration 107/1000 | Loss: 0.00000982
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000981
Iteration 110/1000 | Loss: 0.00000981
Iteration 111/1000 | Loss: 0.00000981
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000979
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000978
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000977
Iteration 125/1000 | Loss: 0.00000977
Iteration 126/1000 | Loss: 0.00000977
Iteration 127/1000 | Loss: 0.00000977
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000976
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000975
Iteration 146/1000 | Loss: 0.00000975
Iteration 147/1000 | Loss: 0.00000975
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000974
Iteration 150/1000 | Loss: 0.00000974
Iteration 151/1000 | Loss: 0.00000974
Iteration 152/1000 | Loss: 0.00000974
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000974
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000974
Iteration 157/1000 | Loss: 0.00000974
Iteration 158/1000 | Loss: 0.00000974
Iteration 159/1000 | Loss: 0.00000974
Iteration 160/1000 | Loss: 0.00000974
Iteration 161/1000 | Loss: 0.00000974
Iteration 162/1000 | Loss: 0.00000974
Iteration 163/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [9.73743499343982e-06, 9.73743499343982e-06, 9.73743499343982e-06, 9.73743499343982e-06, 9.73743499343982e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.73743499343982e-06

Optimization complete. Final v2v error: 2.6786956787109375 mm

Highest mean error: 2.9078452587127686 mm for frame 90

Lowest mean error: 2.513326644897461 mm for frame 214

Saving results

Total time: 43.650320053100586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376721
Iteration 2/25 | Loss: 0.00107112
Iteration 3/25 | Loss: 0.00100294
Iteration 4/25 | Loss: 0.00099303
Iteration 5/25 | Loss: 0.00098964
Iteration 6/25 | Loss: 0.00098853
Iteration 7/25 | Loss: 0.00098845
Iteration 8/25 | Loss: 0.00098845
Iteration 9/25 | Loss: 0.00098845
Iteration 10/25 | Loss: 0.00098845
Iteration 11/25 | Loss: 0.00098845
Iteration 12/25 | Loss: 0.00098845
Iteration 13/25 | Loss: 0.00098845
Iteration 14/25 | Loss: 0.00098845
Iteration 15/25 | Loss: 0.00098845
Iteration 16/25 | Loss: 0.00098845
Iteration 17/25 | Loss: 0.00098845
Iteration 18/25 | Loss: 0.00098845
Iteration 19/25 | Loss: 0.00098845
Iteration 20/25 | Loss: 0.00098845
Iteration 21/25 | Loss: 0.00098845
Iteration 22/25 | Loss: 0.00098845
Iteration 23/25 | Loss: 0.00098845
Iteration 24/25 | Loss: 0.00098845
Iteration 25/25 | Loss: 0.00098845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81932092
Iteration 2/25 | Loss: 0.00106213
Iteration 3/25 | Loss: 0.00106213
Iteration 4/25 | Loss: 0.00106213
Iteration 5/25 | Loss: 0.00106213
Iteration 6/25 | Loss: 0.00106213
Iteration 7/25 | Loss: 0.00106213
Iteration 8/25 | Loss: 0.00106213
Iteration 9/25 | Loss: 0.00106213
Iteration 10/25 | Loss: 0.00106213
Iteration 11/25 | Loss: 0.00106213
Iteration 12/25 | Loss: 0.00106213
Iteration 13/25 | Loss: 0.00106213
Iteration 14/25 | Loss: 0.00106213
Iteration 15/25 | Loss: 0.00106213
Iteration 16/25 | Loss: 0.00106213
Iteration 17/25 | Loss: 0.00106212
Iteration 18/25 | Loss: 0.00106212
Iteration 19/25 | Loss: 0.00106212
Iteration 20/25 | Loss: 0.00106212
Iteration 21/25 | Loss: 0.00106212
Iteration 22/25 | Loss: 0.00106212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010621249675750732, 0.0010621249675750732, 0.0010621249675750732, 0.0010621249675750732, 0.0010621249675750732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010621249675750732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106212
Iteration 2/1000 | Loss: 0.00001911
Iteration 3/1000 | Loss: 0.00001260
Iteration 4/1000 | Loss: 0.00001011
Iteration 5/1000 | Loss: 0.00000944
Iteration 6/1000 | Loss: 0.00000896
Iteration 7/1000 | Loss: 0.00000863
Iteration 8/1000 | Loss: 0.00000828
Iteration 9/1000 | Loss: 0.00000825
Iteration 10/1000 | Loss: 0.00000817
Iteration 11/1000 | Loss: 0.00000816
Iteration 12/1000 | Loss: 0.00000816
Iteration 13/1000 | Loss: 0.00000815
Iteration 14/1000 | Loss: 0.00000809
Iteration 15/1000 | Loss: 0.00000804
Iteration 16/1000 | Loss: 0.00000794
Iteration 17/1000 | Loss: 0.00000792
Iteration 18/1000 | Loss: 0.00000791
Iteration 19/1000 | Loss: 0.00000790
Iteration 20/1000 | Loss: 0.00000789
Iteration 21/1000 | Loss: 0.00000789
Iteration 22/1000 | Loss: 0.00000788
Iteration 23/1000 | Loss: 0.00000784
Iteration 24/1000 | Loss: 0.00000784
Iteration 25/1000 | Loss: 0.00000782
Iteration 26/1000 | Loss: 0.00000779
Iteration 27/1000 | Loss: 0.00000778
Iteration 28/1000 | Loss: 0.00000777
Iteration 29/1000 | Loss: 0.00000777
Iteration 30/1000 | Loss: 0.00000776
Iteration 31/1000 | Loss: 0.00000773
Iteration 32/1000 | Loss: 0.00000773
Iteration 33/1000 | Loss: 0.00000771
Iteration 34/1000 | Loss: 0.00000770
Iteration 35/1000 | Loss: 0.00000769
Iteration 36/1000 | Loss: 0.00000769
Iteration 37/1000 | Loss: 0.00000769
Iteration 38/1000 | Loss: 0.00000769
Iteration 39/1000 | Loss: 0.00000769
Iteration 40/1000 | Loss: 0.00000769
Iteration 41/1000 | Loss: 0.00000769
Iteration 42/1000 | Loss: 0.00000768
Iteration 43/1000 | Loss: 0.00000768
Iteration 44/1000 | Loss: 0.00000768
Iteration 45/1000 | Loss: 0.00000768
Iteration 46/1000 | Loss: 0.00000768
Iteration 47/1000 | Loss: 0.00000768
Iteration 48/1000 | Loss: 0.00000768
Iteration 49/1000 | Loss: 0.00000767
Iteration 50/1000 | Loss: 0.00000767
Iteration 51/1000 | Loss: 0.00000767
Iteration 52/1000 | Loss: 0.00000766
Iteration 53/1000 | Loss: 0.00000766
Iteration 54/1000 | Loss: 0.00000766
Iteration 55/1000 | Loss: 0.00000766
Iteration 56/1000 | Loss: 0.00000765
Iteration 57/1000 | Loss: 0.00000765
Iteration 58/1000 | Loss: 0.00000765
Iteration 59/1000 | Loss: 0.00000765
Iteration 60/1000 | Loss: 0.00000764
Iteration 61/1000 | Loss: 0.00000764
Iteration 62/1000 | Loss: 0.00000764
Iteration 63/1000 | Loss: 0.00000764
Iteration 64/1000 | Loss: 0.00000764
Iteration 65/1000 | Loss: 0.00000764
Iteration 66/1000 | Loss: 0.00000763
Iteration 67/1000 | Loss: 0.00000763
Iteration 68/1000 | Loss: 0.00000763
Iteration 69/1000 | Loss: 0.00000763
Iteration 70/1000 | Loss: 0.00000762
Iteration 71/1000 | Loss: 0.00000762
Iteration 72/1000 | Loss: 0.00000761
Iteration 73/1000 | Loss: 0.00000761
Iteration 74/1000 | Loss: 0.00000761
Iteration 75/1000 | Loss: 0.00000761
Iteration 76/1000 | Loss: 0.00000761
Iteration 77/1000 | Loss: 0.00000760
Iteration 78/1000 | Loss: 0.00000760
Iteration 79/1000 | Loss: 0.00000760
Iteration 80/1000 | Loss: 0.00000760
Iteration 81/1000 | Loss: 0.00000760
Iteration 82/1000 | Loss: 0.00000760
Iteration 83/1000 | Loss: 0.00000760
Iteration 84/1000 | Loss: 0.00000760
Iteration 85/1000 | Loss: 0.00000760
Iteration 86/1000 | Loss: 0.00000759
Iteration 87/1000 | Loss: 0.00000758
Iteration 88/1000 | Loss: 0.00000758
Iteration 89/1000 | Loss: 0.00000757
Iteration 90/1000 | Loss: 0.00000757
Iteration 91/1000 | Loss: 0.00000757
Iteration 92/1000 | Loss: 0.00000757
Iteration 93/1000 | Loss: 0.00000756
Iteration 94/1000 | Loss: 0.00000756
Iteration 95/1000 | Loss: 0.00000756
Iteration 96/1000 | Loss: 0.00000756
Iteration 97/1000 | Loss: 0.00000755
Iteration 98/1000 | Loss: 0.00000755
Iteration 99/1000 | Loss: 0.00000755
Iteration 100/1000 | Loss: 0.00000755
Iteration 101/1000 | Loss: 0.00000754
Iteration 102/1000 | Loss: 0.00000754
Iteration 103/1000 | Loss: 0.00000754
Iteration 104/1000 | Loss: 0.00000754
Iteration 105/1000 | Loss: 0.00000754
Iteration 106/1000 | Loss: 0.00000754
Iteration 107/1000 | Loss: 0.00000753
Iteration 108/1000 | Loss: 0.00000753
Iteration 109/1000 | Loss: 0.00000753
Iteration 110/1000 | Loss: 0.00000753
Iteration 111/1000 | Loss: 0.00000753
Iteration 112/1000 | Loss: 0.00000753
Iteration 113/1000 | Loss: 0.00000752
Iteration 114/1000 | Loss: 0.00000752
Iteration 115/1000 | Loss: 0.00000751
Iteration 116/1000 | Loss: 0.00000751
Iteration 117/1000 | Loss: 0.00000751
Iteration 118/1000 | Loss: 0.00000751
Iteration 119/1000 | Loss: 0.00000751
Iteration 120/1000 | Loss: 0.00000751
Iteration 121/1000 | Loss: 0.00000751
Iteration 122/1000 | Loss: 0.00000750
Iteration 123/1000 | Loss: 0.00000750
Iteration 124/1000 | Loss: 0.00000750
Iteration 125/1000 | Loss: 0.00000750
Iteration 126/1000 | Loss: 0.00000750
Iteration 127/1000 | Loss: 0.00000750
Iteration 128/1000 | Loss: 0.00000750
Iteration 129/1000 | Loss: 0.00000750
Iteration 130/1000 | Loss: 0.00000750
Iteration 131/1000 | Loss: 0.00000750
Iteration 132/1000 | Loss: 0.00000750
Iteration 133/1000 | Loss: 0.00000750
Iteration 134/1000 | Loss: 0.00000750
Iteration 135/1000 | Loss: 0.00000750
Iteration 136/1000 | Loss: 0.00000750
Iteration 137/1000 | Loss: 0.00000750
Iteration 138/1000 | Loss: 0.00000750
Iteration 139/1000 | Loss: 0.00000750
Iteration 140/1000 | Loss: 0.00000750
Iteration 141/1000 | Loss: 0.00000750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [7.500154879380716e-06, 7.500154879380716e-06, 7.500154879380716e-06, 7.500154879380716e-06, 7.500154879380716e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.500154879380716e-06

Optimization complete. Final v2v error: 2.358412027359009 mm

Highest mean error: 2.7901694774627686 mm for frame 56

Lowest mean error: 2.269151210784912 mm for frame 6

Saving results

Total time: 36.33173370361328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568956
Iteration 2/25 | Loss: 0.00107508
Iteration 3/25 | Loss: 0.00099886
Iteration 4/25 | Loss: 0.00098862
Iteration 5/25 | Loss: 0.00098517
Iteration 6/25 | Loss: 0.00098424
Iteration 7/25 | Loss: 0.00098424
Iteration 8/25 | Loss: 0.00098424
Iteration 9/25 | Loss: 0.00098424
Iteration 10/25 | Loss: 0.00098424
Iteration 11/25 | Loss: 0.00098424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009842438157647848, 0.0009842438157647848, 0.0009842438157647848, 0.0009842438157647848, 0.0009842438157647848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009842438157647848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80603337
Iteration 2/25 | Loss: 0.00100784
Iteration 3/25 | Loss: 0.00100784
Iteration 4/25 | Loss: 0.00100784
Iteration 5/25 | Loss: 0.00100784
Iteration 6/25 | Loss: 0.00100784
Iteration 7/25 | Loss: 0.00100784
Iteration 8/25 | Loss: 0.00100784
Iteration 9/25 | Loss: 0.00100784
Iteration 10/25 | Loss: 0.00100783
Iteration 11/25 | Loss: 0.00100783
Iteration 12/25 | Loss: 0.00100783
Iteration 13/25 | Loss: 0.00100783
Iteration 14/25 | Loss: 0.00100783
Iteration 15/25 | Loss: 0.00100783
Iteration 16/25 | Loss: 0.00100783
Iteration 17/25 | Loss: 0.00100783
Iteration 18/25 | Loss: 0.00100783
Iteration 19/25 | Loss: 0.00100783
Iteration 20/25 | Loss: 0.00100783
Iteration 21/25 | Loss: 0.00100783
Iteration 22/25 | Loss: 0.00100783
Iteration 23/25 | Loss: 0.00100783
Iteration 24/25 | Loss: 0.00100783
Iteration 25/25 | Loss: 0.00100783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100783
Iteration 2/1000 | Loss: 0.00001516
Iteration 3/1000 | Loss: 0.00001085
Iteration 4/1000 | Loss: 0.00000962
Iteration 5/1000 | Loss: 0.00000908
Iteration 6/1000 | Loss: 0.00000867
Iteration 7/1000 | Loss: 0.00000836
Iteration 8/1000 | Loss: 0.00000820
Iteration 9/1000 | Loss: 0.00000800
Iteration 10/1000 | Loss: 0.00000800
Iteration 11/1000 | Loss: 0.00000789
Iteration 12/1000 | Loss: 0.00000782
Iteration 13/1000 | Loss: 0.00000777
Iteration 14/1000 | Loss: 0.00000772
Iteration 15/1000 | Loss: 0.00000769
Iteration 16/1000 | Loss: 0.00000768
Iteration 17/1000 | Loss: 0.00000768
Iteration 18/1000 | Loss: 0.00000768
Iteration 19/1000 | Loss: 0.00000768
Iteration 20/1000 | Loss: 0.00000768
Iteration 21/1000 | Loss: 0.00000767
Iteration 22/1000 | Loss: 0.00000767
Iteration 23/1000 | Loss: 0.00000766
Iteration 24/1000 | Loss: 0.00000765
Iteration 25/1000 | Loss: 0.00000762
Iteration 26/1000 | Loss: 0.00000762
Iteration 27/1000 | Loss: 0.00000762
Iteration 28/1000 | Loss: 0.00000761
Iteration 29/1000 | Loss: 0.00000760
Iteration 30/1000 | Loss: 0.00000759
Iteration 31/1000 | Loss: 0.00000759
Iteration 32/1000 | Loss: 0.00000758
Iteration 33/1000 | Loss: 0.00000758
Iteration 34/1000 | Loss: 0.00000757
Iteration 35/1000 | Loss: 0.00000756
Iteration 36/1000 | Loss: 0.00000756
Iteration 37/1000 | Loss: 0.00000751
Iteration 38/1000 | Loss: 0.00000750
Iteration 39/1000 | Loss: 0.00000750
Iteration 40/1000 | Loss: 0.00000749
Iteration 41/1000 | Loss: 0.00000748
Iteration 42/1000 | Loss: 0.00000748
Iteration 43/1000 | Loss: 0.00000748
Iteration 44/1000 | Loss: 0.00000747
Iteration 45/1000 | Loss: 0.00000747
Iteration 46/1000 | Loss: 0.00000747
Iteration 47/1000 | Loss: 0.00000747
Iteration 48/1000 | Loss: 0.00000746
Iteration 49/1000 | Loss: 0.00000746
Iteration 50/1000 | Loss: 0.00000745
Iteration 51/1000 | Loss: 0.00000745
Iteration 52/1000 | Loss: 0.00000744
Iteration 53/1000 | Loss: 0.00000744
Iteration 54/1000 | Loss: 0.00000744
Iteration 55/1000 | Loss: 0.00000740
Iteration 56/1000 | Loss: 0.00000740
Iteration 57/1000 | Loss: 0.00000739
Iteration 58/1000 | Loss: 0.00000739
Iteration 59/1000 | Loss: 0.00000739
Iteration 60/1000 | Loss: 0.00000739
Iteration 61/1000 | Loss: 0.00000739
Iteration 62/1000 | Loss: 0.00000739
Iteration 63/1000 | Loss: 0.00000739
Iteration 64/1000 | Loss: 0.00000739
Iteration 65/1000 | Loss: 0.00000739
Iteration 66/1000 | Loss: 0.00000738
Iteration 67/1000 | Loss: 0.00000738
Iteration 68/1000 | Loss: 0.00000737
Iteration 69/1000 | Loss: 0.00000737
Iteration 70/1000 | Loss: 0.00000736
Iteration 71/1000 | Loss: 0.00000736
Iteration 72/1000 | Loss: 0.00000736
Iteration 73/1000 | Loss: 0.00000736
Iteration 74/1000 | Loss: 0.00000736
Iteration 75/1000 | Loss: 0.00000736
Iteration 76/1000 | Loss: 0.00000736
Iteration 77/1000 | Loss: 0.00000736
Iteration 78/1000 | Loss: 0.00000736
Iteration 79/1000 | Loss: 0.00000736
Iteration 80/1000 | Loss: 0.00000736
Iteration 81/1000 | Loss: 0.00000735
Iteration 82/1000 | Loss: 0.00000735
Iteration 83/1000 | Loss: 0.00000735
Iteration 84/1000 | Loss: 0.00000735
Iteration 85/1000 | Loss: 0.00000734
Iteration 86/1000 | Loss: 0.00000734
Iteration 87/1000 | Loss: 0.00000734
Iteration 88/1000 | Loss: 0.00000734
Iteration 89/1000 | Loss: 0.00000734
Iteration 90/1000 | Loss: 0.00000734
Iteration 91/1000 | Loss: 0.00000733
Iteration 92/1000 | Loss: 0.00000733
Iteration 93/1000 | Loss: 0.00000733
Iteration 94/1000 | Loss: 0.00000733
Iteration 95/1000 | Loss: 0.00000733
Iteration 96/1000 | Loss: 0.00000733
Iteration 97/1000 | Loss: 0.00000733
Iteration 98/1000 | Loss: 0.00000733
Iteration 99/1000 | Loss: 0.00000733
Iteration 100/1000 | Loss: 0.00000733
Iteration 101/1000 | Loss: 0.00000733
Iteration 102/1000 | Loss: 0.00000733
Iteration 103/1000 | Loss: 0.00000733
Iteration 104/1000 | Loss: 0.00000733
Iteration 105/1000 | Loss: 0.00000732
Iteration 106/1000 | Loss: 0.00000732
Iteration 107/1000 | Loss: 0.00000732
Iteration 108/1000 | Loss: 0.00000732
Iteration 109/1000 | Loss: 0.00000731
Iteration 110/1000 | Loss: 0.00000731
Iteration 111/1000 | Loss: 0.00000731
Iteration 112/1000 | Loss: 0.00000731
Iteration 113/1000 | Loss: 0.00000731
Iteration 114/1000 | Loss: 0.00000731
Iteration 115/1000 | Loss: 0.00000731
Iteration 116/1000 | Loss: 0.00000731
Iteration 117/1000 | Loss: 0.00000731
Iteration 118/1000 | Loss: 0.00000730
Iteration 119/1000 | Loss: 0.00000730
Iteration 120/1000 | Loss: 0.00000730
Iteration 121/1000 | Loss: 0.00000730
Iteration 122/1000 | Loss: 0.00000730
Iteration 123/1000 | Loss: 0.00000730
Iteration 124/1000 | Loss: 0.00000730
Iteration 125/1000 | Loss: 0.00000729
Iteration 126/1000 | Loss: 0.00000729
Iteration 127/1000 | Loss: 0.00000729
Iteration 128/1000 | Loss: 0.00000729
Iteration 129/1000 | Loss: 0.00000729
Iteration 130/1000 | Loss: 0.00000729
Iteration 131/1000 | Loss: 0.00000729
Iteration 132/1000 | Loss: 0.00000729
Iteration 133/1000 | Loss: 0.00000729
Iteration 134/1000 | Loss: 0.00000729
Iteration 135/1000 | Loss: 0.00000729
Iteration 136/1000 | Loss: 0.00000729
Iteration 137/1000 | Loss: 0.00000729
Iteration 138/1000 | Loss: 0.00000729
Iteration 139/1000 | Loss: 0.00000729
Iteration 140/1000 | Loss: 0.00000729
Iteration 141/1000 | Loss: 0.00000729
Iteration 142/1000 | Loss: 0.00000729
Iteration 143/1000 | Loss: 0.00000729
Iteration 144/1000 | Loss: 0.00000729
Iteration 145/1000 | Loss: 0.00000729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [7.290030225703958e-06, 7.290030225703958e-06, 7.290030225703958e-06, 7.290030225703958e-06, 7.290030225703958e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.290030225703958e-06

Optimization complete. Final v2v error: 2.345315456390381 mm

Highest mean error: 2.5833871364593506 mm for frame 108

Lowest mean error: 2.225935697555542 mm for frame 156

Saving results

Total time: 36.00855755805969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821154
Iteration 2/25 | Loss: 0.00109502
Iteration 3/25 | Loss: 0.00099095
Iteration 4/25 | Loss: 0.00098433
Iteration 5/25 | Loss: 0.00098286
Iteration 6/25 | Loss: 0.00098286
Iteration 7/25 | Loss: 0.00098286
Iteration 8/25 | Loss: 0.00098286
Iteration 9/25 | Loss: 0.00098286
Iteration 10/25 | Loss: 0.00098286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009828571928665042, 0.0009828571928665042, 0.0009828571928665042, 0.0009828571928665042, 0.0009828571928665042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009828571928665042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30939305
Iteration 2/25 | Loss: 0.00100193
Iteration 3/25 | Loss: 0.00100193
Iteration 4/25 | Loss: 0.00100193
Iteration 5/25 | Loss: 0.00100193
Iteration 6/25 | Loss: 0.00100193
Iteration 7/25 | Loss: 0.00100193
Iteration 8/25 | Loss: 0.00100193
Iteration 9/25 | Loss: 0.00100193
Iteration 10/25 | Loss: 0.00100193
Iteration 11/25 | Loss: 0.00100193
Iteration 12/25 | Loss: 0.00100193
Iteration 13/25 | Loss: 0.00100193
Iteration 14/25 | Loss: 0.00100193
Iteration 15/25 | Loss: 0.00100193
Iteration 16/25 | Loss: 0.00100193
Iteration 17/25 | Loss: 0.00100193
Iteration 18/25 | Loss: 0.00100193
Iteration 19/25 | Loss: 0.00100193
Iteration 20/25 | Loss: 0.00100193
Iteration 21/25 | Loss: 0.00100193
Iteration 22/25 | Loss: 0.00100193
Iteration 23/25 | Loss: 0.00100193
Iteration 24/25 | Loss: 0.00100193
Iteration 25/25 | Loss: 0.00100193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100193
Iteration 2/1000 | Loss: 0.00001637
Iteration 3/1000 | Loss: 0.00001112
Iteration 4/1000 | Loss: 0.00001009
Iteration 5/1000 | Loss: 0.00000925
Iteration 6/1000 | Loss: 0.00000872
Iteration 7/1000 | Loss: 0.00000838
Iteration 8/1000 | Loss: 0.00000817
Iteration 9/1000 | Loss: 0.00000792
Iteration 10/1000 | Loss: 0.00000778
Iteration 11/1000 | Loss: 0.00000776
Iteration 12/1000 | Loss: 0.00000769
Iteration 13/1000 | Loss: 0.00000762
Iteration 14/1000 | Loss: 0.00000759
Iteration 15/1000 | Loss: 0.00000759
Iteration 16/1000 | Loss: 0.00000758
Iteration 17/1000 | Loss: 0.00000757
Iteration 18/1000 | Loss: 0.00000757
Iteration 19/1000 | Loss: 0.00000757
Iteration 20/1000 | Loss: 0.00000756
Iteration 21/1000 | Loss: 0.00000755
Iteration 22/1000 | Loss: 0.00000754
Iteration 23/1000 | Loss: 0.00000754
Iteration 24/1000 | Loss: 0.00000753
Iteration 25/1000 | Loss: 0.00000753
Iteration 26/1000 | Loss: 0.00000753
Iteration 27/1000 | Loss: 0.00000753
Iteration 28/1000 | Loss: 0.00000753
Iteration 29/1000 | Loss: 0.00000752
Iteration 30/1000 | Loss: 0.00000752
Iteration 31/1000 | Loss: 0.00000751
Iteration 32/1000 | Loss: 0.00000751
Iteration 33/1000 | Loss: 0.00000750
Iteration 34/1000 | Loss: 0.00000750
Iteration 35/1000 | Loss: 0.00000750
Iteration 36/1000 | Loss: 0.00000749
Iteration 37/1000 | Loss: 0.00000749
Iteration 38/1000 | Loss: 0.00000749
Iteration 39/1000 | Loss: 0.00000748
Iteration 40/1000 | Loss: 0.00000748
Iteration 41/1000 | Loss: 0.00000746
Iteration 42/1000 | Loss: 0.00000745
Iteration 43/1000 | Loss: 0.00000745
Iteration 44/1000 | Loss: 0.00000745
Iteration 45/1000 | Loss: 0.00000745
Iteration 46/1000 | Loss: 0.00000745
Iteration 47/1000 | Loss: 0.00000745
Iteration 48/1000 | Loss: 0.00000745
Iteration 49/1000 | Loss: 0.00000745
Iteration 50/1000 | Loss: 0.00000745
Iteration 51/1000 | Loss: 0.00000745
Iteration 52/1000 | Loss: 0.00000745
Iteration 53/1000 | Loss: 0.00000744
Iteration 54/1000 | Loss: 0.00000744
Iteration 55/1000 | Loss: 0.00000744
Iteration 56/1000 | Loss: 0.00000741
Iteration 57/1000 | Loss: 0.00000740
Iteration 58/1000 | Loss: 0.00000740
Iteration 59/1000 | Loss: 0.00000740
Iteration 60/1000 | Loss: 0.00000740
Iteration 61/1000 | Loss: 0.00000739
Iteration 62/1000 | Loss: 0.00000739
Iteration 63/1000 | Loss: 0.00000738
Iteration 64/1000 | Loss: 0.00000737
Iteration 65/1000 | Loss: 0.00000736
Iteration 66/1000 | Loss: 0.00000736
Iteration 67/1000 | Loss: 0.00000735
Iteration 68/1000 | Loss: 0.00000735
Iteration 69/1000 | Loss: 0.00000735
Iteration 70/1000 | Loss: 0.00000734
Iteration 71/1000 | Loss: 0.00000734
Iteration 72/1000 | Loss: 0.00000734
Iteration 73/1000 | Loss: 0.00000733
Iteration 74/1000 | Loss: 0.00000732
Iteration 75/1000 | Loss: 0.00000731
Iteration 76/1000 | Loss: 0.00000729
Iteration 77/1000 | Loss: 0.00000729
Iteration 78/1000 | Loss: 0.00000729
Iteration 79/1000 | Loss: 0.00000729
Iteration 80/1000 | Loss: 0.00000728
Iteration 81/1000 | Loss: 0.00000728
Iteration 82/1000 | Loss: 0.00000728
Iteration 83/1000 | Loss: 0.00000726
Iteration 84/1000 | Loss: 0.00000726
Iteration 85/1000 | Loss: 0.00000726
Iteration 86/1000 | Loss: 0.00000726
Iteration 87/1000 | Loss: 0.00000725
Iteration 88/1000 | Loss: 0.00000725
Iteration 89/1000 | Loss: 0.00000725
Iteration 90/1000 | Loss: 0.00000725
Iteration 91/1000 | Loss: 0.00000725
Iteration 92/1000 | Loss: 0.00000725
Iteration 93/1000 | Loss: 0.00000724
Iteration 94/1000 | Loss: 0.00000724
Iteration 95/1000 | Loss: 0.00000724
Iteration 96/1000 | Loss: 0.00000723
Iteration 97/1000 | Loss: 0.00000723
Iteration 98/1000 | Loss: 0.00000723
Iteration 99/1000 | Loss: 0.00000723
Iteration 100/1000 | Loss: 0.00000722
Iteration 101/1000 | Loss: 0.00000722
Iteration 102/1000 | Loss: 0.00000721
Iteration 103/1000 | Loss: 0.00000721
Iteration 104/1000 | Loss: 0.00000721
Iteration 105/1000 | Loss: 0.00000721
Iteration 106/1000 | Loss: 0.00000720
Iteration 107/1000 | Loss: 0.00000720
Iteration 108/1000 | Loss: 0.00000720
Iteration 109/1000 | Loss: 0.00000720
Iteration 110/1000 | Loss: 0.00000720
Iteration 111/1000 | Loss: 0.00000720
Iteration 112/1000 | Loss: 0.00000719
Iteration 113/1000 | Loss: 0.00000719
Iteration 114/1000 | Loss: 0.00000719
Iteration 115/1000 | Loss: 0.00000719
Iteration 116/1000 | Loss: 0.00000718
Iteration 117/1000 | Loss: 0.00000718
Iteration 118/1000 | Loss: 0.00000718
Iteration 119/1000 | Loss: 0.00000718
Iteration 120/1000 | Loss: 0.00000717
Iteration 121/1000 | Loss: 0.00000717
Iteration 122/1000 | Loss: 0.00000716
Iteration 123/1000 | Loss: 0.00000716
Iteration 124/1000 | Loss: 0.00000716
Iteration 125/1000 | Loss: 0.00000716
Iteration 126/1000 | Loss: 0.00000716
Iteration 127/1000 | Loss: 0.00000716
Iteration 128/1000 | Loss: 0.00000716
Iteration 129/1000 | Loss: 0.00000715
Iteration 130/1000 | Loss: 0.00000715
Iteration 131/1000 | Loss: 0.00000715
Iteration 132/1000 | Loss: 0.00000715
Iteration 133/1000 | Loss: 0.00000715
Iteration 134/1000 | Loss: 0.00000715
Iteration 135/1000 | Loss: 0.00000715
Iteration 136/1000 | Loss: 0.00000715
Iteration 137/1000 | Loss: 0.00000715
Iteration 138/1000 | Loss: 0.00000715
Iteration 139/1000 | Loss: 0.00000714
Iteration 140/1000 | Loss: 0.00000714
Iteration 141/1000 | Loss: 0.00000714
Iteration 142/1000 | Loss: 0.00000714
Iteration 143/1000 | Loss: 0.00000714
Iteration 144/1000 | Loss: 0.00000714
Iteration 145/1000 | Loss: 0.00000714
Iteration 146/1000 | Loss: 0.00000714
Iteration 147/1000 | Loss: 0.00000714
Iteration 148/1000 | Loss: 0.00000714
Iteration 149/1000 | Loss: 0.00000714
Iteration 150/1000 | Loss: 0.00000714
Iteration 151/1000 | Loss: 0.00000714
Iteration 152/1000 | Loss: 0.00000714
Iteration 153/1000 | Loss: 0.00000713
Iteration 154/1000 | Loss: 0.00000713
Iteration 155/1000 | Loss: 0.00000713
Iteration 156/1000 | Loss: 0.00000713
Iteration 157/1000 | Loss: 0.00000713
Iteration 158/1000 | Loss: 0.00000713
Iteration 159/1000 | Loss: 0.00000713
Iteration 160/1000 | Loss: 0.00000713
Iteration 161/1000 | Loss: 0.00000713
Iteration 162/1000 | Loss: 0.00000713
Iteration 163/1000 | Loss: 0.00000712
Iteration 164/1000 | Loss: 0.00000712
Iteration 165/1000 | Loss: 0.00000712
Iteration 166/1000 | Loss: 0.00000712
Iteration 167/1000 | Loss: 0.00000712
Iteration 168/1000 | Loss: 0.00000712
Iteration 169/1000 | Loss: 0.00000712
Iteration 170/1000 | Loss: 0.00000712
Iteration 171/1000 | Loss: 0.00000712
Iteration 172/1000 | Loss: 0.00000712
Iteration 173/1000 | Loss: 0.00000712
Iteration 174/1000 | Loss: 0.00000712
Iteration 175/1000 | Loss: 0.00000712
Iteration 176/1000 | Loss: 0.00000712
Iteration 177/1000 | Loss: 0.00000712
Iteration 178/1000 | Loss: 0.00000712
Iteration 179/1000 | Loss: 0.00000712
Iteration 180/1000 | Loss: 0.00000712
Iteration 181/1000 | Loss: 0.00000712
Iteration 182/1000 | Loss: 0.00000712
Iteration 183/1000 | Loss: 0.00000712
Iteration 184/1000 | Loss: 0.00000712
Iteration 185/1000 | Loss: 0.00000712
Iteration 186/1000 | Loss: 0.00000712
Iteration 187/1000 | Loss: 0.00000712
Iteration 188/1000 | Loss: 0.00000712
Iteration 189/1000 | Loss: 0.00000712
Iteration 190/1000 | Loss: 0.00000712
Iteration 191/1000 | Loss: 0.00000712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [7.121763246686896e-06, 7.121763246686896e-06, 7.121763246686896e-06, 7.121763246686896e-06, 7.121763246686896e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.121763246686896e-06

Optimization complete. Final v2v error: 2.2860705852508545 mm

Highest mean error: 2.5214390754699707 mm for frame 58

Lowest mean error: 2.0880250930786133 mm for frame 208

Saving results

Total time: 42.33359336853027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039109
Iteration 2/25 | Loss: 0.00185583
Iteration 3/25 | Loss: 0.00128956
Iteration 4/25 | Loss: 0.00125498
Iteration 5/25 | Loss: 0.00121428
Iteration 6/25 | Loss: 0.00114606
Iteration 7/25 | Loss: 0.00114781
Iteration 8/25 | Loss: 0.00109220
Iteration 9/25 | Loss: 0.00107759
Iteration 10/25 | Loss: 0.00106206
Iteration 11/25 | Loss: 0.00104618
Iteration 12/25 | Loss: 0.00104322
Iteration 13/25 | Loss: 0.00103246
Iteration 14/25 | Loss: 0.00102894
Iteration 15/25 | Loss: 0.00102825
Iteration 16/25 | Loss: 0.00102800
Iteration 17/25 | Loss: 0.00102786
Iteration 18/25 | Loss: 0.00102784
Iteration 19/25 | Loss: 0.00102783
Iteration 20/25 | Loss: 0.00102783
Iteration 21/25 | Loss: 0.00102783
Iteration 22/25 | Loss: 0.00102783
Iteration 23/25 | Loss: 0.00102783
Iteration 24/25 | Loss: 0.00102782
Iteration 25/25 | Loss: 0.00102782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34124076
Iteration 2/25 | Loss: 0.00120605
Iteration 3/25 | Loss: 0.00120605
Iteration 4/25 | Loss: 0.00120605
Iteration 5/25 | Loss: 0.00120605
Iteration 6/25 | Loss: 0.00137076
Iteration 7/25 | Loss: 0.00120606
Iteration 8/25 | Loss: 0.00120605
Iteration 9/25 | Loss: 0.00120605
Iteration 10/25 | Loss: 0.00120605
Iteration 11/25 | Loss: 0.00120605
Iteration 12/25 | Loss: 0.00120605
Iteration 13/25 | Loss: 0.00120605
Iteration 14/25 | Loss: 0.00120605
Iteration 15/25 | Loss: 0.00120605
Iteration 16/25 | Loss: 0.00120605
Iteration 17/25 | Loss: 0.00120605
Iteration 18/25 | Loss: 0.00120605
Iteration 19/25 | Loss: 0.00120605
Iteration 20/25 | Loss: 0.00120605
Iteration 21/25 | Loss: 0.00120605
Iteration 22/25 | Loss: 0.00120605
Iteration 23/25 | Loss: 0.00120605
Iteration 24/25 | Loss: 0.00120605
Iteration 25/25 | Loss: 0.00120605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120605
Iteration 2/1000 | Loss: 0.00004307
Iteration 3/1000 | Loss: 0.00015915
Iteration 4/1000 | Loss: 0.00016140
Iteration 5/1000 | Loss: 0.00002585
Iteration 6/1000 | Loss: 0.00002390
Iteration 7/1000 | Loss: 0.00188455
Iteration 8/1000 | Loss: 0.00005545
Iteration 9/1000 | Loss: 0.00003403
Iteration 10/1000 | Loss: 0.00002312
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002078
Iteration 14/1000 | Loss: 0.00002041
Iteration 15/1000 | Loss: 0.00060029
Iteration 16/1000 | Loss: 0.00239683
Iteration 17/1000 | Loss: 0.00029235
Iteration 18/1000 | Loss: 0.00010584
Iteration 19/1000 | Loss: 0.00003415
Iteration 20/1000 | Loss: 0.00002175
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00000985
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000876
Iteration 30/1000 | Loss: 0.00000862
Iteration 31/1000 | Loss: 0.00000854
Iteration 32/1000 | Loss: 0.00000853
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000852
Iteration 35/1000 | Loss: 0.00000850
Iteration 36/1000 | Loss: 0.00000844
Iteration 37/1000 | Loss: 0.00000834
Iteration 38/1000 | Loss: 0.00000833
Iteration 39/1000 | Loss: 0.00000832
Iteration 40/1000 | Loss: 0.00000831
Iteration 41/1000 | Loss: 0.00000831
Iteration 42/1000 | Loss: 0.00000829
Iteration 43/1000 | Loss: 0.00000828
Iteration 44/1000 | Loss: 0.00000828
Iteration 45/1000 | Loss: 0.00000827
Iteration 46/1000 | Loss: 0.00000827
Iteration 47/1000 | Loss: 0.00000827
Iteration 48/1000 | Loss: 0.00000827
Iteration 49/1000 | Loss: 0.00000826
Iteration 50/1000 | Loss: 0.00000826
Iteration 51/1000 | Loss: 0.00000826
Iteration 52/1000 | Loss: 0.00000826
Iteration 53/1000 | Loss: 0.00000826
Iteration 54/1000 | Loss: 0.00000824
Iteration 55/1000 | Loss: 0.00000824
Iteration 56/1000 | Loss: 0.00000824
Iteration 57/1000 | Loss: 0.00000824
Iteration 58/1000 | Loss: 0.00000824
Iteration 59/1000 | Loss: 0.00000823
Iteration 60/1000 | Loss: 0.00000823
Iteration 61/1000 | Loss: 0.00000823
Iteration 62/1000 | Loss: 0.00000823
Iteration 63/1000 | Loss: 0.00000823
Iteration 64/1000 | Loss: 0.00000823
Iteration 65/1000 | Loss: 0.00000823
Iteration 66/1000 | Loss: 0.00000823
Iteration 67/1000 | Loss: 0.00000823
Iteration 68/1000 | Loss: 0.00000822
Iteration 69/1000 | Loss: 0.00000822
Iteration 70/1000 | Loss: 0.00000822
Iteration 71/1000 | Loss: 0.00000822
Iteration 72/1000 | Loss: 0.00000822
Iteration 73/1000 | Loss: 0.00000821
Iteration 74/1000 | Loss: 0.00000821
Iteration 75/1000 | Loss: 0.00000821
Iteration 76/1000 | Loss: 0.00000821
Iteration 77/1000 | Loss: 0.00000820
Iteration 78/1000 | Loss: 0.00000820
Iteration 79/1000 | Loss: 0.00000819
Iteration 80/1000 | Loss: 0.00000819
Iteration 81/1000 | Loss: 0.00000819
Iteration 82/1000 | Loss: 0.00000819
Iteration 83/1000 | Loss: 0.00000819
Iteration 84/1000 | Loss: 0.00000819
Iteration 85/1000 | Loss: 0.00000819
Iteration 86/1000 | Loss: 0.00000819
Iteration 87/1000 | Loss: 0.00000819
Iteration 88/1000 | Loss: 0.00000819
Iteration 89/1000 | Loss: 0.00000819
Iteration 90/1000 | Loss: 0.00000819
Iteration 91/1000 | Loss: 0.00000819
Iteration 92/1000 | Loss: 0.00000818
Iteration 93/1000 | Loss: 0.00000818
Iteration 94/1000 | Loss: 0.00000818
Iteration 95/1000 | Loss: 0.00000818
Iteration 96/1000 | Loss: 0.00000818
Iteration 97/1000 | Loss: 0.00000818
Iteration 98/1000 | Loss: 0.00000818
Iteration 99/1000 | Loss: 0.00000818
Iteration 100/1000 | Loss: 0.00000818
Iteration 101/1000 | Loss: 0.00000818
Iteration 102/1000 | Loss: 0.00000817
Iteration 103/1000 | Loss: 0.00000817
Iteration 104/1000 | Loss: 0.00000817
Iteration 105/1000 | Loss: 0.00000817
Iteration 106/1000 | Loss: 0.00000817
Iteration 107/1000 | Loss: 0.00000817
Iteration 108/1000 | Loss: 0.00000817
Iteration 109/1000 | Loss: 0.00000816
Iteration 110/1000 | Loss: 0.00000816
Iteration 111/1000 | Loss: 0.00000816
Iteration 112/1000 | Loss: 0.00000816
Iteration 113/1000 | Loss: 0.00000816
Iteration 114/1000 | Loss: 0.00000816
Iteration 115/1000 | Loss: 0.00000816
Iteration 116/1000 | Loss: 0.00000816
Iteration 117/1000 | Loss: 0.00000816
Iteration 118/1000 | Loss: 0.00000816
Iteration 119/1000 | Loss: 0.00000815
Iteration 120/1000 | Loss: 0.00000815
Iteration 121/1000 | Loss: 0.00000815
Iteration 122/1000 | Loss: 0.00000815
Iteration 123/1000 | Loss: 0.00000815
Iteration 124/1000 | Loss: 0.00000815
Iteration 125/1000 | Loss: 0.00000815
Iteration 126/1000 | Loss: 0.00000815
Iteration 127/1000 | Loss: 0.00000815
Iteration 128/1000 | Loss: 0.00000815
Iteration 129/1000 | Loss: 0.00000815
Iteration 130/1000 | Loss: 0.00000815
Iteration 131/1000 | Loss: 0.00000815
Iteration 132/1000 | Loss: 0.00000815
Iteration 133/1000 | Loss: 0.00000815
Iteration 134/1000 | Loss: 0.00000815
Iteration 135/1000 | Loss: 0.00000814
Iteration 136/1000 | Loss: 0.00000814
Iteration 137/1000 | Loss: 0.00000814
Iteration 138/1000 | Loss: 0.00000814
Iteration 139/1000 | Loss: 0.00000814
Iteration 140/1000 | Loss: 0.00000814
Iteration 141/1000 | Loss: 0.00000814
Iteration 142/1000 | Loss: 0.00000814
Iteration 143/1000 | Loss: 0.00000814
Iteration 144/1000 | Loss: 0.00000814
Iteration 145/1000 | Loss: 0.00000814
Iteration 146/1000 | Loss: 0.00000814
Iteration 147/1000 | Loss: 0.00000814
Iteration 148/1000 | Loss: 0.00000814
Iteration 149/1000 | Loss: 0.00000814
Iteration 150/1000 | Loss: 0.00000814
Iteration 151/1000 | Loss: 0.00000814
Iteration 152/1000 | Loss: 0.00000814
Iteration 153/1000 | Loss: 0.00000814
Iteration 154/1000 | Loss: 0.00000814
Iteration 155/1000 | Loss: 0.00000814
Iteration 156/1000 | Loss: 0.00000814
Iteration 157/1000 | Loss: 0.00000814
Iteration 158/1000 | Loss: 0.00000814
Iteration 159/1000 | Loss: 0.00000814
Iteration 160/1000 | Loss: 0.00000814
Iteration 161/1000 | Loss: 0.00000814
Iteration 162/1000 | Loss: 0.00000814
Iteration 163/1000 | Loss: 0.00000814
Iteration 164/1000 | Loss: 0.00000814
Iteration 165/1000 | Loss: 0.00000814
Iteration 166/1000 | Loss: 0.00000814
Iteration 167/1000 | Loss: 0.00000814
Iteration 168/1000 | Loss: 0.00000814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [8.138841621985193e-06, 8.138841621985193e-06, 8.138841621985193e-06, 8.138841621985193e-06, 8.138841621985193e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.138841621985193e-06

Optimization complete. Final v2v error: 2.4179821014404297 mm

Highest mean error: 3.135617256164551 mm for frame 95

Lowest mean error: 2.166879177093506 mm for frame 36

Saving results

Total time: 86.68496704101562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394604
Iteration 2/25 | Loss: 0.00115857
Iteration 3/25 | Loss: 0.00102643
Iteration 4/25 | Loss: 0.00101639
Iteration 5/25 | Loss: 0.00101311
Iteration 6/25 | Loss: 0.00101304
Iteration 7/25 | Loss: 0.00101304
Iteration 8/25 | Loss: 0.00101304
Iteration 9/25 | Loss: 0.00101304
Iteration 10/25 | Loss: 0.00101304
Iteration 11/25 | Loss: 0.00101304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010130361188203096, 0.0010130361188203096, 0.0010130361188203096, 0.0010130361188203096, 0.0010130361188203096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010130361188203096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56843925
Iteration 2/25 | Loss: 0.00088860
Iteration 3/25 | Loss: 0.00088857
Iteration 4/25 | Loss: 0.00088857
Iteration 5/25 | Loss: 0.00088857
Iteration 6/25 | Loss: 0.00088857
Iteration 7/25 | Loss: 0.00088857
Iteration 8/25 | Loss: 0.00088857
Iteration 9/25 | Loss: 0.00088857
Iteration 10/25 | Loss: 0.00088857
Iteration 11/25 | Loss: 0.00088857
Iteration 12/25 | Loss: 0.00088857
Iteration 13/25 | Loss: 0.00088857
Iteration 14/25 | Loss: 0.00088857
Iteration 15/25 | Loss: 0.00088857
Iteration 16/25 | Loss: 0.00088857
Iteration 17/25 | Loss: 0.00088857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000888570211827755, 0.000888570211827755, 0.000888570211827755, 0.000888570211827755, 0.000888570211827755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000888570211827755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088857
Iteration 2/1000 | Loss: 0.00002336
Iteration 3/1000 | Loss: 0.00001428
Iteration 4/1000 | Loss: 0.00001045
Iteration 5/1000 | Loss: 0.00000957
Iteration 6/1000 | Loss: 0.00000901
Iteration 7/1000 | Loss: 0.00000876
Iteration 8/1000 | Loss: 0.00000856
Iteration 9/1000 | Loss: 0.00000845
Iteration 10/1000 | Loss: 0.00000818
Iteration 11/1000 | Loss: 0.00000800
Iteration 12/1000 | Loss: 0.00000798
Iteration 13/1000 | Loss: 0.00000790
Iteration 14/1000 | Loss: 0.00000790
Iteration 15/1000 | Loss: 0.00000784
Iteration 16/1000 | Loss: 0.00000774
Iteration 17/1000 | Loss: 0.00000772
Iteration 18/1000 | Loss: 0.00000771
Iteration 19/1000 | Loss: 0.00000771
Iteration 20/1000 | Loss: 0.00000769
Iteration 21/1000 | Loss: 0.00000769
Iteration 22/1000 | Loss: 0.00000768
Iteration 23/1000 | Loss: 0.00000767
Iteration 24/1000 | Loss: 0.00000767
Iteration 25/1000 | Loss: 0.00000767
Iteration 26/1000 | Loss: 0.00000766
Iteration 27/1000 | Loss: 0.00000766
Iteration 28/1000 | Loss: 0.00000765
Iteration 29/1000 | Loss: 0.00000764
Iteration 30/1000 | Loss: 0.00000764
Iteration 31/1000 | Loss: 0.00000764
Iteration 32/1000 | Loss: 0.00000764
Iteration 33/1000 | Loss: 0.00000764
Iteration 34/1000 | Loss: 0.00000763
Iteration 35/1000 | Loss: 0.00000763
Iteration 36/1000 | Loss: 0.00000762
Iteration 37/1000 | Loss: 0.00000761
Iteration 38/1000 | Loss: 0.00000761
Iteration 39/1000 | Loss: 0.00000761
Iteration 40/1000 | Loss: 0.00000760
Iteration 41/1000 | Loss: 0.00000760
Iteration 42/1000 | Loss: 0.00000760
Iteration 43/1000 | Loss: 0.00000760
Iteration 44/1000 | Loss: 0.00000760
Iteration 45/1000 | Loss: 0.00000760
Iteration 46/1000 | Loss: 0.00000760
Iteration 47/1000 | Loss: 0.00000759
Iteration 48/1000 | Loss: 0.00000759
Iteration 49/1000 | Loss: 0.00000759
Iteration 50/1000 | Loss: 0.00000759
Iteration 51/1000 | Loss: 0.00000759
Iteration 52/1000 | Loss: 0.00000758
Iteration 53/1000 | Loss: 0.00000758
Iteration 54/1000 | Loss: 0.00000757
Iteration 55/1000 | Loss: 0.00000756
Iteration 56/1000 | Loss: 0.00000756
Iteration 57/1000 | Loss: 0.00000756
Iteration 58/1000 | Loss: 0.00000756
Iteration 59/1000 | Loss: 0.00000755
Iteration 60/1000 | Loss: 0.00000755
Iteration 61/1000 | Loss: 0.00000755
Iteration 62/1000 | Loss: 0.00000754
Iteration 63/1000 | Loss: 0.00000754
Iteration 64/1000 | Loss: 0.00000754
Iteration 65/1000 | Loss: 0.00000754
Iteration 66/1000 | Loss: 0.00000754
Iteration 67/1000 | Loss: 0.00000753
Iteration 68/1000 | Loss: 0.00000753
Iteration 69/1000 | Loss: 0.00000753
Iteration 70/1000 | Loss: 0.00000753
Iteration 71/1000 | Loss: 0.00000753
Iteration 72/1000 | Loss: 0.00000753
Iteration 73/1000 | Loss: 0.00000752
Iteration 74/1000 | Loss: 0.00000752
Iteration 75/1000 | Loss: 0.00000752
Iteration 76/1000 | Loss: 0.00000752
Iteration 77/1000 | Loss: 0.00000752
Iteration 78/1000 | Loss: 0.00000752
Iteration 79/1000 | Loss: 0.00000752
Iteration 80/1000 | Loss: 0.00000752
Iteration 81/1000 | Loss: 0.00000751
Iteration 82/1000 | Loss: 0.00000751
Iteration 83/1000 | Loss: 0.00000751
Iteration 84/1000 | Loss: 0.00000751
Iteration 85/1000 | Loss: 0.00000751
Iteration 86/1000 | Loss: 0.00000751
Iteration 87/1000 | Loss: 0.00000751
Iteration 88/1000 | Loss: 0.00000750
Iteration 89/1000 | Loss: 0.00000750
Iteration 90/1000 | Loss: 0.00000750
Iteration 91/1000 | Loss: 0.00000750
Iteration 92/1000 | Loss: 0.00000750
Iteration 93/1000 | Loss: 0.00000750
Iteration 94/1000 | Loss: 0.00000750
Iteration 95/1000 | Loss: 0.00000750
Iteration 96/1000 | Loss: 0.00000750
Iteration 97/1000 | Loss: 0.00000750
Iteration 98/1000 | Loss: 0.00000750
Iteration 99/1000 | Loss: 0.00000750
Iteration 100/1000 | Loss: 0.00000750
Iteration 101/1000 | Loss: 0.00000749
Iteration 102/1000 | Loss: 0.00000749
Iteration 103/1000 | Loss: 0.00000749
Iteration 104/1000 | Loss: 0.00000749
Iteration 105/1000 | Loss: 0.00000749
Iteration 106/1000 | Loss: 0.00000749
Iteration 107/1000 | Loss: 0.00000749
Iteration 108/1000 | Loss: 0.00000749
Iteration 109/1000 | Loss: 0.00000749
Iteration 110/1000 | Loss: 0.00000749
Iteration 111/1000 | Loss: 0.00000749
Iteration 112/1000 | Loss: 0.00000749
Iteration 113/1000 | Loss: 0.00000748
Iteration 114/1000 | Loss: 0.00000748
Iteration 115/1000 | Loss: 0.00000748
Iteration 116/1000 | Loss: 0.00000748
Iteration 117/1000 | Loss: 0.00000747
Iteration 118/1000 | Loss: 0.00000747
Iteration 119/1000 | Loss: 0.00000747
Iteration 120/1000 | Loss: 0.00000747
Iteration 121/1000 | Loss: 0.00000747
Iteration 122/1000 | Loss: 0.00000746
Iteration 123/1000 | Loss: 0.00000746
Iteration 124/1000 | Loss: 0.00000746
Iteration 125/1000 | Loss: 0.00000746
Iteration 126/1000 | Loss: 0.00000746
Iteration 127/1000 | Loss: 0.00000745
Iteration 128/1000 | Loss: 0.00000745
Iteration 129/1000 | Loss: 0.00000745
Iteration 130/1000 | Loss: 0.00000745
Iteration 131/1000 | Loss: 0.00000745
Iteration 132/1000 | Loss: 0.00000745
Iteration 133/1000 | Loss: 0.00000745
Iteration 134/1000 | Loss: 0.00000745
Iteration 135/1000 | Loss: 0.00000744
Iteration 136/1000 | Loss: 0.00000744
Iteration 137/1000 | Loss: 0.00000744
Iteration 138/1000 | Loss: 0.00000744
Iteration 139/1000 | Loss: 0.00000744
Iteration 140/1000 | Loss: 0.00000744
Iteration 141/1000 | Loss: 0.00000744
Iteration 142/1000 | Loss: 0.00000743
Iteration 143/1000 | Loss: 0.00000743
Iteration 144/1000 | Loss: 0.00000743
Iteration 145/1000 | Loss: 0.00000743
Iteration 146/1000 | Loss: 0.00000743
Iteration 147/1000 | Loss: 0.00000743
Iteration 148/1000 | Loss: 0.00000742
Iteration 149/1000 | Loss: 0.00000742
Iteration 150/1000 | Loss: 0.00000742
Iteration 151/1000 | Loss: 0.00000742
Iteration 152/1000 | Loss: 0.00000742
Iteration 153/1000 | Loss: 0.00000742
Iteration 154/1000 | Loss: 0.00000742
Iteration 155/1000 | Loss: 0.00000742
Iteration 156/1000 | Loss: 0.00000742
Iteration 157/1000 | Loss: 0.00000741
Iteration 158/1000 | Loss: 0.00000741
Iteration 159/1000 | Loss: 0.00000741
Iteration 160/1000 | Loss: 0.00000741
Iteration 161/1000 | Loss: 0.00000741
Iteration 162/1000 | Loss: 0.00000741
Iteration 163/1000 | Loss: 0.00000741
Iteration 164/1000 | Loss: 0.00000741
Iteration 165/1000 | Loss: 0.00000741
Iteration 166/1000 | Loss: 0.00000741
Iteration 167/1000 | Loss: 0.00000741
Iteration 168/1000 | Loss: 0.00000740
Iteration 169/1000 | Loss: 0.00000740
Iteration 170/1000 | Loss: 0.00000740
Iteration 171/1000 | Loss: 0.00000740
Iteration 172/1000 | Loss: 0.00000740
Iteration 173/1000 | Loss: 0.00000740
Iteration 174/1000 | Loss: 0.00000739
Iteration 175/1000 | Loss: 0.00000739
Iteration 176/1000 | Loss: 0.00000739
Iteration 177/1000 | Loss: 0.00000739
Iteration 178/1000 | Loss: 0.00000739
Iteration 179/1000 | Loss: 0.00000739
Iteration 180/1000 | Loss: 0.00000739
Iteration 181/1000 | Loss: 0.00000739
Iteration 182/1000 | Loss: 0.00000739
Iteration 183/1000 | Loss: 0.00000739
Iteration 184/1000 | Loss: 0.00000739
Iteration 185/1000 | Loss: 0.00000739
Iteration 186/1000 | Loss: 0.00000739
Iteration 187/1000 | Loss: 0.00000739
Iteration 188/1000 | Loss: 0.00000739
Iteration 189/1000 | Loss: 0.00000739
Iteration 190/1000 | Loss: 0.00000739
Iteration 191/1000 | Loss: 0.00000739
Iteration 192/1000 | Loss: 0.00000739
Iteration 193/1000 | Loss: 0.00000739
Iteration 194/1000 | Loss: 0.00000739
Iteration 195/1000 | Loss: 0.00000739
Iteration 196/1000 | Loss: 0.00000739
Iteration 197/1000 | Loss: 0.00000739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [7.38799144528457e-06, 7.38799144528457e-06, 7.38799144528457e-06, 7.38799144528457e-06, 7.38799144528457e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.38799144528457e-06

Optimization complete. Final v2v error: 2.352055311203003 mm

Highest mean error: 2.522371768951416 mm for frame 50

Lowest mean error: 2.2320797443389893 mm for frame 45

Saving results

Total time: 44.941593647003174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768355
Iteration 2/25 | Loss: 0.00177357
Iteration 3/25 | Loss: 0.00121121
Iteration 4/25 | Loss: 0.00115724
Iteration 5/25 | Loss: 0.00113127
Iteration 6/25 | Loss: 0.00112930
Iteration 7/25 | Loss: 0.00110824
Iteration 8/25 | Loss: 0.00108490
Iteration 9/25 | Loss: 0.00107836
Iteration 10/25 | Loss: 0.00107639
Iteration 11/25 | Loss: 0.00107597
Iteration 12/25 | Loss: 0.00107562
Iteration 13/25 | Loss: 0.00107548
Iteration 14/25 | Loss: 0.00107541
Iteration 15/25 | Loss: 0.00107541
Iteration 16/25 | Loss: 0.00107541
Iteration 17/25 | Loss: 0.00107541
Iteration 18/25 | Loss: 0.00107541
Iteration 19/25 | Loss: 0.00107541
Iteration 20/25 | Loss: 0.00107541
Iteration 21/25 | Loss: 0.00107540
Iteration 22/25 | Loss: 0.00107540
Iteration 23/25 | Loss: 0.00107540
Iteration 24/25 | Loss: 0.00107540
Iteration 25/25 | Loss: 0.00107540

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.82507038
Iteration 2/25 | Loss: 0.00061869
Iteration 3/25 | Loss: 0.00061847
Iteration 4/25 | Loss: 0.00061847
Iteration 5/25 | Loss: 0.00061847
Iteration 6/25 | Loss: 0.00061847
Iteration 7/25 | Loss: 0.00061847
Iteration 8/25 | Loss: 0.00061846
Iteration 9/25 | Loss: 0.00061846
Iteration 10/25 | Loss: 0.00061846
Iteration 11/25 | Loss: 0.00061846
Iteration 12/25 | Loss: 0.00061846
Iteration 13/25 | Loss: 0.00061846
Iteration 14/25 | Loss: 0.00061846
Iteration 15/25 | Loss: 0.00061846
Iteration 16/25 | Loss: 0.00061846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006184644880704582, 0.0006184644880704582, 0.0006184644880704582, 0.0006184644880704582, 0.0006184644880704582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006184644880704582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061846
Iteration 2/1000 | Loss: 0.00002466
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001600
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00007295
Iteration 9/1000 | Loss: 0.00001490
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001227
Iteration 14/1000 | Loss: 0.00001211
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001179
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001161
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001160
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001158
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Iteration 152/1000 | Loss: 0.00001158
Iteration 153/1000 | Loss: 0.00001158
Iteration 154/1000 | Loss: 0.00001158
Iteration 155/1000 | Loss: 0.00001158
Iteration 156/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1584033018152695e-05, 1.1584033018152695e-05, 1.1584033018152695e-05, 1.1584033018152695e-05, 1.1584033018152695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1584033018152695e-05

Optimization complete. Final v2v error: 2.9193856716156006 mm

Highest mean error: 4.008942127227783 mm for frame 224

Lowest mean error: 2.660280466079712 mm for frame 97

Saving results

Total time: 63.353591442108154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854225
Iteration 2/25 | Loss: 0.00154180
Iteration 3/25 | Loss: 0.00119159
Iteration 4/25 | Loss: 0.00115436
Iteration 5/25 | Loss: 0.00114861
Iteration 6/25 | Loss: 0.00114830
Iteration 7/25 | Loss: 0.00114830
Iteration 8/25 | Loss: 0.00114830
Iteration 9/25 | Loss: 0.00114830
Iteration 10/25 | Loss: 0.00114830
Iteration 11/25 | Loss: 0.00114830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011482966365292668, 0.0011482966365292668, 0.0011482966365292668, 0.0011482966365292668, 0.0011482966365292668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011482966365292668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88369846
Iteration 2/25 | Loss: 0.00098017
Iteration 3/25 | Loss: 0.00098017
Iteration 4/25 | Loss: 0.00098017
Iteration 5/25 | Loss: 0.00098017
Iteration 6/25 | Loss: 0.00098017
Iteration 7/25 | Loss: 0.00098017
Iteration 8/25 | Loss: 0.00098017
Iteration 9/25 | Loss: 0.00098017
Iteration 10/25 | Loss: 0.00098017
Iteration 11/25 | Loss: 0.00098017
Iteration 12/25 | Loss: 0.00098017
Iteration 13/25 | Loss: 0.00098017
Iteration 14/25 | Loss: 0.00098017
Iteration 15/25 | Loss: 0.00098017
Iteration 16/25 | Loss: 0.00098017
Iteration 17/25 | Loss: 0.00098017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009801676496863365, 0.0009801676496863365, 0.0009801676496863365, 0.0009801676496863365, 0.0009801676496863365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009801676496863365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098017
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001557
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001304
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001302
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.3016759112360887e-05, 1.3016759112360887e-05, 1.3016759112360887e-05, 1.3016759112360887e-05, 1.3016759112360887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3016759112360887e-05

Optimization complete. Final v2v error: 3.0404775142669678 mm

Highest mean error: 3.418093204498291 mm for frame 0

Lowest mean error: 2.7900466918945312 mm for frame 76

Saving results

Total time: 34.41139578819275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018485
Iteration 2/25 | Loss: 0.00229213
Iteration 3/25 | Loss: 0.00185315
Iteration 4/25 | Loss: 0.00133688
Iteration 5/25 | Loss: 0.00127147
Iteration 6/25 | Loss: 0.00122049
Iteration 7/25 | Loss: 0.00113989
Iteration 8/25 | Loss: 0.00110071
Iteration 9/25 | Loss: 0.00109584
Iteration 10/25 | Loss: 0.00108687
Iteration 11/25 | Loss: 0.00108434
Iteration 12/25 | Loss: 0.00108253
Iteration 13/25 | Loss: 0.00107857
Iteration 14/25 | Loss: 0.00107814
Iteration 15/25 | Loss: 0.00107801
Iteration 16/25 | Loss: 0.00107796
Iteration 17/25 | Loss: 0.00107796
Iteration 18/25 | Loss: 0.00107796
Iteration 19/25 | Loss: 0.00107796
Iteration 20/25 | Loss: 0.00107796
Iteration 21/25 | Loss: 0.00107796
Iteration 22/25 | Loss: 0.00107796
Iteration 23/25 | Loss: 0.00107796
Iteration 24/25 | Loss: 0.00107796
Iteration 25/25 | Loss: 0.00107796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31100607
Iteration 2/25 | Loss: 0.00109475
Iteration 3/25 | Loss: 0.00103421
Iteration 4/25 | Loss: 0.00103421
Iteration 5/25 | Loss: 0.00103421
Iteration 6/25 | Loss: 0.00103421
Iteration 7/25 | Loss: 0.00103421
Iteration 8/25 | Loss: 0.00103421
Iteration 9/25 | Loss: 0.00103421
Iteration 10/25 | Loss: 0.00103421
Iteration 11/25 | Loss: 0.00103421
Iteration 12/25 | Loss: 0.00103421
Iteration 13/25 | Loss: 0.00103420
Iteration 14/25 | Loss: 0.00103420
Iteration 15/25 | Loss: 0.00103420
Iteration 16/25 | Loss: 0.00103420
Iteration 17/25 | Loss: 0.00103420
Iteration 18/25 | Loss: 0.00103420
Iteration 19/25 | Loss: 0.00103420
Iteration 20/25 | Loss: 0.00103420
Iteration 21/25 | Loss: 0.00103420
Iteration 22/25 | Loss: 0.00103420
Iteration 23/25 | Loss: 0.00103420
Iteration 24/25 | Loss: 0.00103420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001034204731695354, 0.001034204731695354, 0.001034204731695354, 0.001034204731695354, 0.001034204731695354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001034204731695354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103420
Iteration 2/1000 | Loss: 0.00002970
Iteration 3/1000 | Loss: 0.00009471
Iteration 4/1000 | Loss: 0.00009579
Iteration 5/1000 | Loss: 0.00001755
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00017111
Iteration 8/1000 | Loss: 0.00010512
Iteration 9/1000 | Loss: 0.00006307
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001529
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00008574
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00007124
Iteration 16/1000 | Loss: 0.00002500
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00010963
Iteration 33/1000 | Loss: 0.00005356
Iteration 34/1000 | Loss: 0.00014950
Iteration 35/1000 | Loss: 0.00011473
Iteration 36/1000 | Loss: 0.00002321
Iteration 37/1000 | Loss: 0.00010460
Iteration 38/1000 | Loss: 0.00004263
Iteration 39/1000 | Loss: 0.00004241
Iteration 40/1000 | Loss: 0.00001752
Iteration 41/1000 | Loss: 0.00003421
Iteration 42/1000 | Loss: 0.00007218
Iteration 43/1000 | Loss: 0.00012135
Iteration 44/1000 | Loss: 0.00007503
Iteration 45/1000 | Loss: 0.00003167
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001395
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001390
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001389
Iteration 76/1000 | Loss: 0.00001389
Iteration 77/1000 | Loss: 0.00001389
Iteration 78/1000 | Loss: 0.00001389
Iteration 79/1000 | Loss: 0.00001389
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.3884246072848327e-05, 1.3884246072848327e-05, 1.3884246072848327e-05, 1.3884246072848327e-05, 1.3884246072848327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3884246072848327e-05

Optimization complete. Final v2v error: 3.1321895122528076 mm

Highest mean error: 9.042319297790527 mm for frame 106

Lowest mean error: 2.881718635559082 mm for frame 194

Saving results

Total time: 96.14584422111511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040367
Iteration 2/25 | Loss: 0.00208449
Iteration 3/25 | Loss: 0.00130171
Iteration 4/25 | Loss: 0.00153033
Iteration 5/25 | Loss: 0.00131737
Iteration 6/25 | Loss: 0.00118487
Iteration 7/25 | Loss: 0.00120386
Iteration 8/25 | Loss: 0.00115868
Iteration 9/25 | Loss: 0.00114078
Iteration 10/25 | Loss: 0.00113499
Iteration 11/25 | Loss: 0.00113242
Iteration 12/25 | Loss: 0.00113102
Iteration 13/25 | Loss: 0.00124112
Iteration 14/25 | Loss: 0.00107968
Iteration 15/25 | Loss: 0.00107052
Iteration 16/25 | Loss: 0.00106886
Iteration 17/25 | Loss: 0.00106802
Iteration 18/25 | Loss: 0.00106749
Iteration 19/25 | Loss: 0.00106725
Iteration 20/25 | Loss: 0.00106720
Iteration 21/25 | Loss: 0.00106719
Iteration 22/25 | Loss: 0.00106719
Iteration 23/25 | Loss: 0.00106719
Iteration 24/25 | Loss: 0.00106719
Iteration 25/25 | Loss: 0.00106719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.55722523
Iteration 2/25 | Loss: 0.00071496
Iteration 3/25 | Loss: 0.00071496
Iteration 4/25 | Loss: 0.00071495
Iteration 5/25 | Loss: 0.00071495
Iteration 6/25 | Loss: 0.00071495
Iteration 7/25 | Loss: 0.00071495
Iteration 8/25 | Loss: 0.00071495
Iteration 9/25 | Loss: 0.00071495
Iteration 10/25 | Loss: 0.00071495
Iteration 11/25 | Loss: 0.00071495
Iteration 12/25 | Loss: 0.00071495
Iteration 13/25 | Loss: 0.00071495
Iteration 14/25 | Loss: 0.00071495
Iteration 15/25 | Loss: 0.00071495
Iteration 16/25 | Loss: 0.00071495
Iteration 17/25 | Loss: 0.00071495
Iteration 18/25 | Loss: 0.00071495
Iteration 19/25 | Loss: 0.00071495
Iteration 20/25 | Loss: 0.00071495
Iteration 21/25 | Loss: 0.00071495
Iteration 22/25 | Loss: 0.00071495
Iteration 23/25 | Loss: 0.00071495
Iteration 24/25 | Loss: 0.00071495
Iteration 25/25 | Loss: 0.00071495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071495
Iteration 2/1000 | Loss: 0.00003433
Iteration 3/1000 | Loss: 0.00002270
Iteration 4/1000 | Loss: 0.00057853
Iteration 5/1000 | Loss: 0.00006948
Iteration 6/1000 | Loss: 0.00002697
Iteration 7/1000 | Loss: 0.00001841
Iteration 8/1000 | Loss: 0.00001459
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00036014
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001314
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001071
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001068
Iteration 24/1000 | Loss: 0.00001068
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001055
Iteration 27/1000 | Loss: 0.00001054
Iteration 28/1000 | Loss: 0.00001054
Iteration 29/1000 | Loss: 0.00001054
Iteration 30/1000 | Loss: 0.00001054
Iteration 31/1000 | Loss: 0.00001052
Iteration 32/1000 | Loss: 0.00001052
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001036
Iteration 41/1000 | Loss: 0.00001036
Iteration 42/1000 | Loss: 0.00001036
Iteration 43/1000 | Loss: 0.00001035
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001033
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001032
Iteration 53/1000 | Loss: 0.00001032
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001032
Iteration 57/1000 | Loss: 0.00001032
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001032
Iteration 63/1000 | Loss: 0.00001032
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001031
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001029
Iteration 83/1000 | Loss: 0.00001029
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001028
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001028
Iteration 92/1000 | Loss: 0.00001028
Iteration 93/1000 | Loss: 0.00001028
Iteration 94/1000 | Loss: 0.00001028
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001028
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001027
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001027
Iteration 117/1000 | Loss: 0.00001026
Iteration 118/1000 | Loss: 0.00001026
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001026
Iteration 126/1000 | Loss: 0.00001026
Iteration 127/1000 | Loss: 0.00001026
Iteration 128/1000 | Loss: 0.00001026
Iteration 129/1000 | Loss: 0.00001026
Iteration 130/1000 | Loss: 0.00001026
Iteration 131/1000 | Loss: 0.00001026
Iteration 132/1000 | Loss: 0.00001026
Iteration 133/1000 | Loss: 0.00001026
Iteration 134/1000 | Loss: 0.00001026
Iteration 135/1000 | Loss: 0.00001026
Iteration 136/1000 | Loss: 0.00001026
Iteration 137/1000 | Loss: 0.00001026
Iteration 138/1000 | Loss: 0.00001026
Iteration 139/1000 | Loss: 0.00001026
Iteration 140/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0256706445943564e-05, 1.0256706445943564e-05, 1.0256706445943564e-05, 1.0256706445943564e-05, 1.0256706445943564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0256706445943564e-05

Optimization complete. Final v2v error: 2.706505537033081 mm

Highest mean error: 3.856081247329712 mm for frame 165

Lowest mean error: 2.4072461128234863 mm for frame 100

Saving results

Total time: 74.45010304450989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713578
Iteration 2/25 | Loss: 0.00134698
Iteration 3/25 | Loss: 0.00117681
Iteration 4/25 | Loss: 0.00114973
Iteration 5/25 | Loss: 0.00114126
Iteration 6/25 | Loss: 0.00113866
Iteration 7/25 | Loss: 0.00113866
Iteration 8/25 | Loss: 0.00113866
Iteration 9/25 | Loss: 0.00113866
Iteration 10/25 | Loss: 0.00113866
Iteration 11/25 | Loss: 0.00113866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001138661289587617, 0.001138661289587617, 0.001138661289587617, 0.001138661289587617, 0.001138661289587617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001138661289587617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04080486
Iteration 2/25 | Loss: 0.00124798
Iteration 3/25 | Loss: 0.00124798
Iteration 4/25 | Loss: 0.00124798
Iteration 5/25 | Loss: 0.00124798
Iteration 6/25 | Loss: 0.00124798
Iteration 7/25 | Loss: 0.00124798
Iteration 8/25 | Loss: 0.00124798
Iteration 9/25 | Loss: 0.00124798
Iteration 10/25 | Loss: 0.00124798
Iteration 11/25 | Loss: 0.00124798
Iteration 12/25 | Loss: 0.00124797
Iteration 13/25 | Loss: 0.00124797
Iteration 14/25 | Loss: 0.00124797
Iteration 15/25 | Loss: 0.00124797
Iteration 16/25 | Loss: 0.00124797
Iteration 17/25 | Loss: 0.00124797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012479749275371432, 0.0012479749275371432, 0.0012479749275371432, 0.0012479749275371432, 0.0012479749275371432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012479749275371432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124797
Iteration 2/1000 | Loss: 0.00003280
Iteration 3/1000 | Loss: 0.00002380
Iteration 4/1000 | Loss: 0.00002159
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00002017
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001942
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001867
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001817
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001793
Iteration 17/1000 | Loss: 0.00001790
Iteration 18/1000 | Loss: 0.00001790
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001789
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001782
Iteration 37/1000 | Loss: 0.00001782
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001774
Iteration 73/1000 | Loss: 0.00001774
Iteration 74/1000 | Loss: 0.00001774
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001774
Iteration 77/1000 | Loss: 0.00001774
Iteration 78/1000 | Loss: 0.00001774
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001774
Iteration 104/1000 | Loss: 0.00001774
Iteration 105/1000 | Loss: 0.00001774
Iteration 106/1000 | Loss: 0.00001774
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001774
Iteration 112/1000 | Loss: 0.00001774
Iteration 113/1000 | Loss: 0.00001774
Iteration 114/1000 | Loss: 0.00001774
Iteration 115/1000 | Loss: 0.00001774
Iteration 116/1000 | Loss: 0.00001774
Iteration 117/1000 | Loss: 0.00001774
Iteration 118/1000 | Loss: 0.00001774
Iteration 119/1000 | Loss: 0.00001774
Iteration 120/1000 | Loss: 0.00001774
Iteration 121/1000 | Loss: 0.00001774
Iteration 122/1000 | Loss: 0.00001774
Iteration 123/1000 | Loss: 0.00001774
Iteration 124/1000 | Loss: 0.00001774
Iteration 125/1000 | Loss: 0.00001774
Iteration 126/1000 | Loss: 0.00001774
Iteration 127/1000 | Loss: 0.00001774
Iteration 128/1000 | Loss: 0.00001774
Iteration 129/1000 | Loss: 0.00001774
Iteration 130/1000 | Loss: 0.00001774
Iteration 131/1000 | Loss: 0.00001774
Iteration 132/1000 | Loss: 0.00001774
Iteration 133/1000 | Loss: 0.00001774
Iteration 134/1000 | Loss: 0.00001774
Iteration 135/1000 | Loss: 0.00001774
Iteration 136/1000 | Loss: 0.00001774
Iteration 137/1000 | Loss: 0.00001774
Iteration 138/1000 | Loss: 0.00001774
Iteration 139/1000 | Loss: 0.00001774
Iteration 140/1000 | Loss: 0.00001774
Iteration 141/1000 | Loss: 0.00001774
Iteration 142/1000 | Loss: 0.00001774
Iteration 143/1000 | Loss: 0.00001774
Iteration 144/1000 | Loss: 0.00001774
Iteration 145/1000 | Loss: 0.00001774
Iteration 146/1000 | Loss: 0.00001774
Iteration 147/1000 | Loss: 0.00001774
Iteration 148/1000 | Loss: 0.00001774
Iteration 149/1000 | Loss: 0.00001774
Iteration 150/1000 | Loss: 0.00001774
Iteration 151/1000 | Loss: 0.00001774
Iteration 152/1000 | Loss: 0.00001774
Iteration 153/1000 | Loss: 0.00001774
Iteration 154/1000 | Loss: 0.00001774
Iteration 155/1000 | Loss: 0.00001774
Iteration 156/1000 | Loss: 0.00001774
Iteration 157/1000 | Loss: 0.00001774
Iteration 158/1000 | Loss: 0.00001774
Iteration 159/1000 | Loss: 0.00001774
Iteration 160/1000 | Loss: 0.00001774
Iteration 161/1000 | Loss: 0.00001774
Iteration 162/1000 | Loss: 0.00001774
Iteration 163/1000 | Loss: 0.00001774
Iteration 164/1000 | Loss: 0.00001774
Iteration 165/1000 | Loss: 0.00001774
Iteration 166/1000 | Loss: 0.00001774
Iteration 167/1000 | Loss: 0.00001774
Iteration 168/1000 | Loss: 0.00001774
Iteration 169/1000 | Loss: 0.00001774
Iteration 170/1000 | Loss: 0.00001774
Iteration 171/1000 | Loss: 0.00001774
Iteration 172/1000 | Loss: 0.00001774
Iteration 173/1000 | Loss: 0.00001774
Iteration 174/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.7741624105838127e-05, 1.7741624105838127e-05, 1.7741624105838127e-05, 1.7741624105838127e-05, 1.7741624105838127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7741624105838127e-05

Optimization complete. Final v2v error: 3.536597728729248 mm

Highest mean error: 4.176805019378662 mm for frame 180

Lowest mean error: 2.965350389480591 mm for frame 234

Saving results

Total time: 41.750330448150635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917566
Iteration 2/25 | Loss: 0.00123441
Iteration 3/25 | Loss: 0.00109260
Iteration 4/25 | Loss: 0.00106910
Iteration 5/25 | Loss: 0.00106090
Iteration 6/25 | Loss: 0.00105831
Iteration 7/25 | Loss: 0.00105805
Iteration 8/25 | Loss: 0.00105805
Iteration 9/25 | Loss: 0.00105805
Iteration 10/25 | Loss: 0.00105805
Iteration 11/25 | Loss: 0.00105805
Iteration 12/25 | Loss: 0.00105805
Iteration 13/25 | Loss: 0.00105805
Iteration 14/25 | Loss: 0.00105805
Iteration 15/25 | Loss: 0.00105805
Iteration 16/25 | Loss: 0.00105805
Iteration 17/25 | Loss: 0.00105805
Iteration 18/25 | Loss: 0.00105805
Iteration 19/25 | Loss: 0.00105805
Iteration 20/25 | Loss: 0.00105805
Iteration 21/25 | Loss: 0.00105805
Iteration 22/25 | Loss: 0.00105805
Iteration 23/25 | Loss: 0.00105805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010580508969724178, 0.0010580508969724178, 0.0010580508969724178, 0.0010580508969724178, 0.0010580508969724178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010580508969724178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31521225
Iteration 2/25 | Loss: 0.00105217
Iteration 3/25 | Loss: 0.00105215
Iteration 4/25 | Loss: 0.00105215
Iteration 5/25 | Loss: 0.00105215
Iteration 6/25 | Loss: 0.00105215
Iteration 7/25 | Loss: 0.00105215
Iteration 8/25 | Loss: 0.00105215
Iteration 9/25 | Loss: 0.00105215
Iteration 10/25 | Loss: 0.00105215
Iteration 11/25 | Loss: 0.00105215
Iteration 12/25 | Loss: 0.00105215
Iteration 13/25 | Loss: 0.00105215
Iteration 14/25 | Loss: 0.00105215
Iteration 15/25 | Loss: 0.00105215
Iteration 16/25 | Loss: 0.00105215
Iteration 17/25 | Loss: 0.00105215
Iteration 18/25 | Loss: 0.00105215
Iteration 19/25 | Loss: 0.00105215
Iteration 20/25 | Loss: 0.00105215
Iteration 21/25 | Loss: 0.00105215
Iteration 22/25 | Loss: 0.00105215
Iteration 23/25 | Loss: 0.00105215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010521499207243323, 0.0010521499207243323, 0.0010521499207243323, 0.0010521499207243323, 0.0010521499207243323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010521499207243323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105215
Iteration 2/1000 | Loss: 0.00004301
Iteration 3/1000 | Loss: 0.00002870
Iteration 4/1000 | Loss: 0.00002321
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00002004
Iteration 7/1000 | Loss: 0.00001937
Iteration 8/1000 | Loss: 0.00001889
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001789
Iteration 12/1000 | Loss: 0.00001779
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001757
Iteration 20/1000 | Loss: 0.00001756
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001745
Iteration 24/1000 | Loss: 0.00001745
Iteration 25/1000 | Loss: 0.00001744
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001738
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001738
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001737
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001735
Iteration 47/1000 | Loss: 0.00001735
Iteration 48/1000 | Loss: 0.00001735
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001734
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001733
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001732
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001731
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001730
Iteration 68/1000 | Loss: 0.00001730
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001723
Iteration 106/1000 | Loss: 0.00001723
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001722
Iteration 110/1000 | Loss: 0.00001722
Iteration 111/1000 | Loss: 0.00001722
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001721
Iteration 114/1000 | Loss: 0.00001721
Iteration 115/1000 | Loss: 0.00001721
Iteration 116/1000 | Loss: 0.00001721
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001721
Iteration 119/1000 | Loss: 0.00001721
Iteration 120/1000 | Loss: 0.00001721
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001720
Iteration 128/1000 | Loss: 0.00001720
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001719
Iteration 137/1000 | Loss: 0.00001719
Iteration 138/1000 | Loss: 0.00001719
Iteration 139/1000 | Loss: 0.00001719
Iteration 140/1000 | Loss: 0.00001719
Iteration 141/1000 | Loss: 0.00001719
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001719
Iteration 144/1000 | Loss: 0.00001718
Iteration 145/1000 | Loss: 0.00001718
Iteration 146/1000 | Loss: 0.00001718
Iteration 147/1000 | Loss: 0.00001718
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001717
Iteration 152/1000 | Loss: 0.00001717
Iteration 153/1000 | Loss: 0.00001717
Iteration 154/1000 | Loss: 0.00001717
Iteration 155/1000 | Loss: 0.00001717
Iteration 156/1000 | Loss: 0.00001717
Iteration 157/1000 | Loss: 0.00001717
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001716
Iteration 169/1000 | Loss: 0.00001716
Iteration 170/1000 | Loss: 0.00001716
Iteration 171/1000 | Loss: 0.00001716
Iteration 172/1000 | Loss: 0.00001716
Iteration 173/1000 | Loss: 0.00001716
Iteration 174/1000 | Loss: 0.00001716
Iteration 175/1000 | Loss: 0.00001716
Iteration 176/1000 | Loss: 0.00001716
Iteration 177/1000 | Loss: 0.00001716
Iteration 178/1000 | Loss: 0.00001716
Iteration 179/1000 | Loss: 0.00001716
Iteration 180/1000 | Loss: 0.00001716
Iteration 181/1000 | Loss: 0.00001716
Iteration 182/1000 | Loss: 0.00001716
Iteration 183/1000 | Loss: 0.00001716
Iteration 184/1000 | Loss: 0.00001716
Iteration 185/1000 | Loss: 0.00001716
Iteration 186/1000 | Loss: 0.00001716
Iteration 187/1000 | Loss: 0.00001716
Iteration 188/1000 | Loss: 0.00001716
Iteration 189/1000 | Loss: 0.00001716
Iteration 190/1000 | Loss: 0.00001716
Iteration 191/1000 | Loss: 0.00001716
Iteration 192/1000 | Loss: 0.00001716
Iteration 193/1000 | Loss: 0.00001716
Iteration 194/1000 | Loss: 0.00001716
Iteration 195/1000 | Loss: 0.00001716
Iteration 196/1000 | Loss: 0.00001716
Iteration 197/1000 | Loss: 0.00001716
Iteration 198/1000 | Loss: 0.00001716
Iteration 199/1000 | Loss: 0.00001716
Iteration 200/1000 | Loss: 0.00001716
Iteration 201/1000 | Loss: 0.00001716
Iteration 202/1000 | Loss: 0.00001716
Iteration 203/1000 | Loss: 0.00001716
Iteration 204/1000 | Loss: 0.00001716
Iteration 205/1000 | Loss: 0.00001716
Iteration 206/1000 | Loss: 0.00001716
Iteration 207/1000 | Loss: 0.00001716
Iteration 208/1000 | Loss: 0.00001716
Iteration 209/1000 | Loss: 0.00001716
Iteration 210/1000 | Loss: 0.00001716
Iteration 211/1000 | Loss: 0.00001716
Iteration 212/1000 | Loss: 0.00001716
Iteration 213/1000 | Loss: 0.00001716
Iteration 214/1000 | Loss: 0.00001716
Iteration 215/1000 | Loss: 0.00001716
Iteration 216/1000 | Loss: 0.00001716
Iteration 217/1000 | Loss: 0.00001716
Iteration 218/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.7158379705506377e-05, 1.7158379705506377e-05, 1.7158379705506377e-05, 1.7158379705506377e-05, 1.7158379705506377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7158379705506377e-05

Optimization complete. Final v2v error: 3.4107253551483154 mm

Highest mean error: 5.55360221862793 mm for frame 70

Lowest mean error: 2.882096767425537 mm for frame 2

Saving results

Total time: 43.022892475128174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408235
Iteration 2/25 | Loss: 0.00136818
Iteration 3/25 | Loss: 0.00129857
Iteration 4/25 | Loss: 0.00129318
Iteration 5/25 | Loss: 0.00129058
Iteration 6/25 | Loss: 0.00129046
Iteration 7/25 | Loss: 0.00129046
Iteration 8/25 | Loss: 0.00129046
Iteration 9/25 | Loss: 0.00129046
Iteration 10/25 | Loss: 0.00129046
Iteration 11/25 | Loss: 0.00129046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012904554605484009, 0.0012904554605484009, 0.0012904554605484009, 0.0012904554605484009, 0.0012904554605484009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012904554605484009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64380372
Iteration 2/25 | Loss: 0.00075798
Iteration 3/25 | Loss: 0.00075798
Iteration 4/25 | Loss: 0.00075798
Iteration 5/25 | Loss: 0.00075798
Iteration 6/25 | Loss: 0.00075798
Iteration 7/25 | Loss: 0.00075798
Iteration 8/25 | Loss: 0.00075798
Iteration 9/25 | Loss: 0.00075798
Iteration 10/25 | Loss: 0.00075798
Iteration 11/25 | Loss: 0.00075798
Iteration 12/25 | Loss: 0.00075798
Iteration 13/25 | Loss: 0.00075798
Iteration 14/25 | Loss: 0.00075798
Iteration 15/25 | Loss: 0.00075798
Iteration 16/25 | Loss: 0.00075798
Iteration 17/25 | Loss: 0.00075798
Iteration 18/25 | Loss: 0.00075798
Iteration 19/25 | Loss: 0.00075798
Iteration 20/25 | Loss: 0.00075798
Iteration 21/25 | Loss: 0.00075798
Iteration 22/25 | Loss: 0.00075798
Iteration 23/25 | Loss: 0.00075798
Iteration 24/25 | Loss: 0.00075798
Iteration 25/25 | Loss: 0.00075798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075798
Iteration 2/1000 | Loss: 0.00002710
Iteration 3/1000 | Loss: 0.00001837
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001563
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001325
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001305
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001291
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001291
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001290
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001284
Iteration 74/1000 | Loss: 0.00001284
Iteration 75/1000 | Loss: 0.00001284
Iteration 76/1000 | Loss: 0.00001284
Iteration 77/1000 | Loss: 0.00001284
Iteration 78/1000 | Loss: 0.00001284
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00001283
Iteration 87/1000 | Loss: 0.00001283
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001282
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001278
Iteration 103/1000 | Loss: 0.00001278
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001277
Iteration 115/1000 | Loss: 0.00001277
Iteration 116/1000 | Loss: 0.00001277
Iteration 117/1000 | Loss: 0.00001277
Iteration 118/1000 | Loss: 0.00001277
Iteration 119/1000 | Loss: 0.00001277
Iteration 120/1000 | Loss: 0.00001277
Iteration 121/1000 | Loss: 0.00001276
Iteration 122/1000 | Loss: 0.00001276
Iteration 123/1000 | Loss: 0.00001276
Iteration 124/1000 | Loss: 0.00001276
Iteration 125/1000 | Loss: 0.00001276
Iteration 126/1000 | Loss: 0.00001276
Iteration 127/1000 | Loss: 0.00001275
Iteration 128/1000 | Loss: 0.00001275
Iteration 129/1000 | Loss: 0.00001275
Iteration 130/1000 | Loss: 0.00001275
Iteration 131/1000 | Loss: 0.00001275
Iteration 132/1000 | Loss: 0.00001275
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001273
Iteration 141/1000 | Loss: 0.00001273
Iteration 142/1000 | Loss: 0.00001273
Iteration 143/1000 | Loss: 0.00001272
Iteration 144/1000 | Loss: 0.00001272
Iteration 145/1000 | Loss: 0.00001272
Iteration 146/1000 | Loss: 0.00001272
Iteration 147/1000 | Loss: 0.00001272
Iteration 148/1000 | Loss: 0.00001272
Iteration 149/1000 | Loss: 0.00001272
Iteration 150/1000 | Loss: 0.00001272
Iteration 151/1000 | Loss: 0.00001272
Iteration 152/1000 | Loss: 0.00001272
Iteration 153/1000 | Loss: 0.00001272
Iteration 154/1000 | Loss: 0.00001272
Iteration 155/1000 | Loss: 0.00001272
Iteration 156/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2719099686364643e-05, 1.2719099686364643e-05, 1.2719099686364643e-05, 1.2719099686364643e-05, 1.2719099686364643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2719099686364643e-05

Optimization complete. Final v2v error: 3.025315761566162 mm

Highest mean error: 3.4093422889709473 mm for frame 202

Lowest mean error: 2.798539161682129 mm for frame 107

Saving results

Total time: 42.207775354385376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806829
Iteration 2/25 | Loss: 0.00138680
Iteration 3/25 | Loss: 0.00128508
Iteration 4/25 | Loss: 0.00127480
Iteration 5/25 | Loss: 0.00127309
Iteration 6/25 | Loss: 0.00127291
Iteration 7/25 | Loss: 0.00127291
Iteration 8/25 | Loss: 0.00127291
Iteration 9/25 | Loss: 0.00127291
Iteration 10/25 | Loss: 0.00127291
Iteration 11/25 | Loss: 0.00127291
Iteration 12/25 | Loss: 0.00127291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001272907480597496, 0.001272907480597496, 0.001272907480597496, 0.001272907480597496, 0.001272907480597496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001272907480597496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39295113
Iteration 2/25 | Loss: 0.00078932
Iteration 3/25 | Loss: 0.00078930
Iteration 4/25 | Loss: 0.00078930
Iteration 5/25 | Loss: 0.00078930
Iteration 6/25 | Loss: 0.00078930
Iteration 7/25 | Loss: 0.00078930
Iteration 8/25 | Loss: 0.00078930
Iteration 9/25 | Loss: 0.00078930
Iteration 10/25 | Loss: 0.00078930
Iteration 11/25 | Loss: 0.00078930
Iteration 12/25 | Loss: 0.00078930
Iteration 13/25 | Loss: 0.00078930
Iteration 14/25 | Loss: 0.00078930
Iteration 15/25 | Loss: 0.00078930
Iteration 16/25 | Loss: 0.00078930
Iteration 17/25 | Loss: 0.00078930
Iteration 18/25 | Loss: 0.00078930
Iteration 19/25 | Loss: 0.00078930
Iteration 20/25 | Loss: 0.00078930
Iteration 21/25 | Loss: 0.00078930
Iteration 22/25 | Loss: 0.00078930
Iteration 23/25 | Loss: 0.00078930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007892987341620028, 0.0007892987341620028, 0.0007892987341620028, 0.0007892987341620028, 0.0007892987341620028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007892987341620028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078930
Iteration 2/1000 | Loss: 0.00002296
Iteration 3/1000 | Loss: 0.00001561
Iteration 4/1000 | Loss: 0.00001421
Iteration 5/1000 | Loss: 0.00001309
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001135
Iteration 15/1000 | Loss: 0.00001133
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001107
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001103
Iteration 28/1000 | Loss: 0.00001101
Iteration 29/1000 | Loss: 0.00001101
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001100
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001100
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001100
Iteration 40/1000 | Loss: 0.00001100
Iteration 41/1000 | Loss: 0.00001100
Iteration 42/1000 | Loss: 0.00001100
Iteration 43/1000 | Loss: 0.00001099
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001095
Iteration 55/1000 | Loss: 0.00001095
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001094
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001094
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001093
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001093
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001091
Iteration 73/1000 | Loss: 0.00001091
Iteration 74/1000 | Loss: 0.00001091
Iteration 75/1000 | Loss: 0.00001090
Iteration 76/1000 | Loss: 0.00001090
Iteration 77/1000 | Loss: 0.00001090
Iteration 78/1000 | Loss: 0.00001090
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001088
Iteration 90/1000 | Loss: 0.00001088
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001086
Iteration 93/1000 | Loss: 0.00001086
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001085
Iteration 99/1000 | Loss: 0.00001085
Iteration 100/1000 | Loss: 0.00001084
Iteration 101/1000 | Loss: 0.00001084
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001084
Iteration 106/1000 | Loss: 0.00001084
Iteration 107/1000 | Loss: 0.00001083
Iteration 108/1000 | Loss: 0.00001083
Iteration 109/1000 | Loss: 0.00001082
Iteration 110/1000 | Loss: 0.00001082
Iteration 111/1000 | Loss: 0.00001082
Iteration 112/1000 | Loss: 0.00001082
Iteration 113/1000 | Loss: 0.00001082
Iteration 114/1000 | Loss: 0.00001082
Iteration 115/1000 | Loss: 0.00001082
Iteration 116/1000 | Loss: 0.00001081
Iteration 117/1000 | Loss: 0.00001081
Iteration 118/1000 | Loss: 0.00001081
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001079
Iteration 125/1000 | Loss: 0.00001079
Iteration 126/1000 | Loss: 0.00001079
Iteration 127/1000 | Loss: 0.00001078
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001077
Iteration 132/1000 | Loss: 0.00001077
Iteration 133/1000 | Loss: 0.00001077
Iteration 134/1000 | Loss: 0.00001077
Iteration 135/1000 | Loss: 0.00001076
Iteration 136/1000 | Loss: 0.00001076
Iteration 137/1000 | Loss: 0.00001076
Iteration 138/1000 | Loss: 0.00001076
Iteration 139/1000 | Loss: 0.00001076
Iteration 140/1000 | Loss: 0.00001076
Iteration 141/1000 | Loss: 0.00001075
Iteration 142/1000 | Loss: 0.00001075
Iteration 143/1000 | Loss: 0.00001075
Iteration 144/1000 | Loss: 0.00001075
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001074
Iteration 147/1000 | Loss: 0.00001074
Iteration 148/1000 | Loss: 0.00001074
Iteration 149/1000 | Loss: 0.00001074
Iteration 150/1000 | Loss: 0.00001074
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001070
Iteration 168/1000 | Loss: 0.00001070
Iteration 169/1000 | Loss: 0.00001070
Iteration 170/1000 | Loss: 0.00001070
Iteration 171/1000 | Loss: 0.00001070
Iteration 172/1000 | Loss: 0.00001070
Iteration 173/1000 | Loss: 0.00001070
Iteration 174/1000 | Loss: 0.00001069
Iteration 175/1000 | Loss: 0.00001069
Iteration 176/1000 | Loss: 0.00001069
Iteration 177/1000 | Loss: 0.00001069
Iteration 178/1000 | Loss: 0.00001069
Iteration 179/1000 | Loss: 0.00001069
Iteration 180/1000 | Loss: 0.00001069
Iteration 181/1000 | Loss: 0.00001069
Iteration 182/1000 | Loss: 0.00001069
Iteration 183/1000 | Loss: 0.00001069
Iteration 184/1000 | Loss: 0.00001068
Iteration 185/1000 | Loss: 0.00001068
Iteration 186/1000 | Loss: 0.00001068
Iteration 187/1000 | Loss: 0.00001068
Iteration 188/1000 | Loss: 0.00001068
Iteration 189/1000 | Loss: 0.00001068
Iteration 190/1000 | Loss: 0.00001068
Iteration 191/1000 | Loss: 0.00001067
Iteration 192/1000 | Loss: 0.00001067
Iteration 193/1000 | Loss: 0.00001067
Iteration 194/1000 | Loss: 0.00001067
Iteration 195/1000 | Loss: 0.00001067
Iteration 196/1000 | Loss: 0.00001067
Iteration 197/1000 | Loss: 0.00001067
Iteration 198/1000 | Loss: 0.00001067
Iteration 199/1000 | Loss: 0.00001067
Iteration 200/1000 | Loss: 0.00001067
Iteration 201/1000 | Loss: 0.00001067
Iteration 202/1000 | Loss: 0.00001067
Iteration 203/1000 | Loss: 0.00001067
Iteration 204/1000 | Loss: 0.00001067
Iteration 205/1000 | Loss: 0.00001067
Iteration 206/1000 | Loss: 0.00001067
Iteration 207/1000 | Loss: 0.00001067
Iteration 208/1000 | Loss: 0.00001067
Iteration 209/1000 | Loss: 0.00001067
Iteration 210/1000 | Loss: 0.00001067
Iteration 211/1000 | Loss: 0.00001067
Iteration 212/1000 | Loss: 0.00001067
Iteration 213/1000 | Loss: 0.00001067
Iteration 214/1000 | Loss: 0.00001067
Iteration 215/1000 | Loss: 0.00001066
Iteration 216/1000 | Loss: 0.00001066
Iteration 217/1000 | Loss: 0.00001066
Iteration 218/1000 | Loss: 0.00001066
Iteration 219/1000 | Loss: 0.00001066
Iteration 220/1000 | Loss: 0.00001066
Iteration 221/1000 | Loss: 0.00001066
Iteration 222/1000 | Loss: 0.00001066
Iteration 223/1000 | Loss: 0.00001066
Iteration 224/1000 | Loss: 0.00001066
Iteration 225/1000 | Loss: 0.00001066
Iteration 226/1000 | Loss: 0.00001066
Iteration 227/1000 | Loss: 0.00001066
Iteration 228/1000 | Loss: 0.00001066
Iteration 229/1000 | Loss: 0.00001066
Iteration 230/1000 | Loss: 0.00001066
Iteration 231/1000 | Loss: 0.00001066
Iteration 232/1000 | Loss: 0.00001065
Iteration 233/1000 | Loss: 0.00001065
Iteration 234/1000 | Loss: 0.00001065
Iteration 235/1000 | Loss: 0.00001065
Iteration 236/1000 | Loss: 0.00001065
Iteration 237/1000 | Loss: 0.00001065
Iteration 238/1000 | Loss: 0.00001065
Iteration 239/1000 | Loss: 0.00001065
Iteration 240/1000 | Loss: 0.00001065
Iteration 241/1000 | Loss: 0.00001065
Iteration 242/1000 | Loss: 0.00001065
Iteration 243/1000 | Loss: 0.00001064
Iteration 244/1000 | Loss: 0.00001064
Iteration 245/1000 | Loss: 0.00001064
Iteration 246/1000 | Loss: 0.00001064
Iteration 247/1000 | Loss: 0.00001064
Iteration 248/1000 | Loss: 0.00001064
Iteration 249/1000 | Loss: 0.00001064
Iteration 250/1000 | Loss: 0.00001064
Iteration 251/1000 | Loss: 0.00001064
Iteration 252/1000 | Loss: 0.00001064
Iteration 253/1000 | Loss: 0.00001064
Iteration 254/1000 | Loss: 0.00001064
Iteration 255/1000 | Loss: 0.00001064
Iteration 256/1000 | Loss: 0.00001064
Iteration 257/1000 | Loss: 0.00001064
Iteration 258/1000 | Loss: 0.00001064
Iteration 259/1000 | Loss: 0.00001064
Iteration 260/1000 | Loss: 0.00001064
Iteration 261/1000 | Loss: 0.00001064
Iteration 262/1000 | Loss: 0.00001064
Iteration 263/1000 | Loss: 0.00001064
Iteration 264/1000 | Loss: 0.00001064
Iteration 265/1000 | Loss: 0.00001063
Iteration 266/1000 | Loss: 0.00001063
Iteration 267/1000 | Loss: 0.00001063
Iteration 268/1000 | Loss: 0.00001063
Iteration 269/1000 | Loss: 0.00001063
Iteration 270/1000 | Loss: 0.00001063
Iteration 271/1000 | Loss: 0.00001063
Iteration 272/1000 | Loss: 0.00001063
Iteration 273/1000 | Loss: 0.00001063
Iteration 274/1000 | Loss: 0.00001063
Iteration 275/1000 | Loss: 0.00001063
Iteration 276/1000 | Loss: 0.00001063
Iteration 277/1000 | Loss: 0.00001063
Iteration 278/1000 | Loss: 0.00001063
Iteration 279/1000 | Loss: 0.00001063
Iteration 280/1000 | Loss: 0.00001063
Iteration 281/1000 | Loss: 0.00001063
Iteration 282/1000 | Loss: 0.00001063
Iteration 283/1000 | Loss: 0.00001063
Iteration 284/1000 | Loss: 0.00001063
Iteration 285/1000 | Loss: 0.00001063
Iteration 286/1000 | Loss: 0.00001063
Iteration 287/1000 | Loss: 0.00001062
Iteration 288/1000 | Loss: 0.00001062
Iteration 289/1000 | Loss: 0.00001062
Iteration 290/1000 | Loss: 0.00001062
Iteration 291/1000 | Loss: 0.00001062
Iteration 292/1000 | Loss: 0.00001062
Iteration 293/1000 | Loss: 0.00001062
Iteration 294/1000 | Loss: 0.00001062
Iteration 295/1000 | Loss: 0.00001062
Iteration 296/1000 | Loss: 0.00001062
Iteration 297/1000 | Loss: 0.00001062
Iteration 298/1000 | Loss: 0.00001062
Iteration 299/1000 | Loss: 0.00001062
Iteration 300/1000 | Loss: 0.00001062
Iteration 301/1000 | Loss: 0.00001062
Iteration 302/1000 | Loss: 0.00001062
Iteration 303/1000 | Loss: 0.00001062
Iteration 304/1000 | Loss: 0.00001062
Iteration 305/1000 | Loss: 0.00001062
Iteration 306/1000 | Loss: 0.00001062
Iteration 307/1000 | Loss: 0.00001062
Iteration 308/1000 | Loss: 0.00001062
Iteration 309/1000 | Loss: 0.00001062
Iteration 310/1000 | Loss: 0.00001062
Iteration 311/1000 | Loss: 0.00001062
Iteration 312/1000 | Loss: 0.00001062
Iteration 313/1000 | Loss: 0.00001061
Iteration 314/1000 | Loss: 0.00001061
Iteration 315/1000 | Loss: 0.00001061
Iteration 316/1000 | Loss: 0.00001061
Iteration 317/1000 | Loss: 0.00001061
Iteration 318/1000 | Loss: 0.00001061
Iteration 319/1000 | Loss: 0.00001061
Iteration 320/1000 | Loss: 0.00001061
Iteration 321/1000 | Loss: 0.00001061
Iteration 322/1000 | Loss: 0.00001061
Iteration 323/1000 | Loss: 0.00001061
Iteration 324/1000 | Loss: 0.00001061
Iteration 325/1000 | Loss: 0.00001061
Iteration 326/1000 | Loss: 0.00001061
Iteration 327/1000 | Loss: 0.00001061
Iteration 328/1000 | Loss: 0.00001061
Iteration 329/1000 | Loss: 0.00001061
Iteration 330/1000 | Loss: 0.00001061
Iteration 331/1000 | Loss: 0.00001061
Iteration 332/1000 | Loss: 0.00001061
Iteration 333/1000 | Loss: 0.00001061
Iteration 334/1000 | Loss: 0.00001061
Iteration 335/1000 | Loss: 0.00001061
Iteration 336/1000 | Loss: 0.00001061
Iteration 337/1000 | Loss: 0.00001061
Iteration 338/1000 | Loss: 0.00001061
Iteration 339/1000 | Loss: 0.00001061
Iteration 340/1000 | Loss: 0.00001061
Iteration 341/1000 | Loss: 0.00001061
Iteration 342/1000 | Loss: 0.00001061
Iteration 343/1000 | Loss: 0.00001061
Iteration 344/1000 | Loss: 0.00001061
Iteration 345/1000 | Loss: 0.00001061
Iteration 346/1000 | Loss: 0.00001061
Iteration 347/1000 | Loss: 0.00001061
Iteration 348/1000 | Loss: 0.00001061
Iteration 349/1000 | Loss: 0.00001061
Iteration 350/1000 | Loss: 0.00001061
Iteration 351/1000 | Loss: 0.00001061
Iteration 352/1000 | Loss: 0.00001061
Iteration 353/1000 | Loss: 0.00001061
Iteration 354/1000 | Loss: 0.00001061
Iteration 355/1000 | Loss: 0.00001061
Iteration 356/1000 | Loss: 0.00001061
Iteration 357/1000 | Loss: 0.00001061
Iteration 358/1000 | Loss: 0.00001061
Iteration 359/1000 | Loss: 0.00001061
Iteration 360/1000 | Loss: 0.00001061
Iteration 361/1000 | Loss: 0.00001061
Iteration 362/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [1.0614729035296477e-05, 1.0614729035296477e-05, 1.0614729035296477e-05, 1.0614729035296477e-05, 1.0614729035296477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0614729035296477e-05

Optimization complete. Final v2v error: 2.7947144508361816 mm

Highest mean error: 2.9537830352783203 mm for frame 2

Lowest mean error: 2.6929550170898438 mm for frame 104

Saving results

Total time: 46.42196702957153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_027/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_027/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997718
Iteration 2/25 | Loss: 0.00997718
Iteration 3/25 | Loss: 0.00997718
Iteration 4/25 | Loss: 0.00997718
Iteration 5/25 | Loss: 0.00997718
Iteration 6/25 | Loss: 0.00997718
Iteration 7/25 | Loss: 0.00997718
Iteration 8/25 | Loss: 0.00997718
Iteration 9/25 | Loss: 0.00997718
Iteration 10/25 | Loss: 0.00997718
Iteration 11/25 | Loss: 0.00997718
Iteration 12/25 | Loss: 0.00997718
Iteration 13/25 | Loss: 0.00997717
Iteration 14/25 | Loss: 0.00997717
Iteration 15/25 | Loss: 0.00997717
Iteration 16/25 | Loss: 0.00997717
Iteration 17/25 | Loss: 0.00997717
Iteration 18/25 | Loss: 0.00997717
Iteration 19/25 | Loss: 0.00997717
Iteration 20/25 | Loss: 0.00997717
Iteration 21/25 | Loss: 0.00997717
Iteration 22/25 | Loss: 0.00997717
Iteration 23/25 | Loss: 0.00997717
Iteration 24/25 | Loss: 0.00997717
Iteration 25/25 | Loss: 0.00997717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04666877
Iteration 2/25 | Loss: 0.00445652
Iteration 3/25 | Loss: 0.00445357
Iteration 4/25 | Loss: 0.00445357
Iteration 5/25 | Loss: 0.00445357
Iteration 6/25 | Loss: 0.00445357
Iteration 7/25 | Loss: 0.00445357
Iteration 8/25 | Loss: 0.00445357
Iteration 9/25 | Loss: 0.00445357
Iteration 10/25 | Loss: 0.00445357
Iteration 11/25 | Loss: 0.00445357
Iteration 12/25 | Loss: 0.00445357
Iteration 13/25 | Loss: 0.00445357
Iteration 14/25 | Loss: 0.00445357
Iteration 15/25 | Loss: 0.00445357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004453569184988737, 0.004453569184988737, 0.004453569184988737, 0.004453569184988737, 0.004453569184988737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004453569184988737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00445357
Iteration 2/1000 | Loss: 0.00922289
Iteration 3/1000 | Loss: 0.00696728
Iteration 4/1000 | Loss: 0.00138262
Iteration 5/1000 | Loss: 0.01073984
Iteration 6/1000 | Loss: 0.00046410
Iteration 7/1000 | Loss: 0.00105669
Iteration 8/1000 | Loss: 0.00037122
Iteration 9/1000 | Loss: 0.00044841
Iteration 10/1000 | Loss: 0.00110536
Iteration 11/1000 | Loss: 0.00011733
Iteration 12/1000 | Loss: 0.00173164
Iteration 13/1000 | Loss: 0.00315616
Iteration 14/1000 | Loss: 0.00009092
Iteration 15/1000 | Loss: 0.00027804
Iteration 16/1000 | Loss: 0.00022015
Iteration 17/1000 | Loss: 0.00075249
Iteration 18/1000 | Loss: 0.00005491
Iteration 19/1000 | Loss: 0.00020413
Iteration 20/1000 | Loss: 0.00022147
Iteration 21/1000 | Loss: 0.00036375
Iteration 22/1000 | Loss: 0.00005773
Iteration 23/1000 | Loss: 0.00018504
Iteration 24/1000 | Loss: 0.00033063
Iteration 25/1000 | Loss: 0.00023147
Iteration 26/1000 | Loss: 0.00024473
Iteration 27/1000 | Loss: 0.00006910
Iteration 28/1000 | Loss: 0.00013121
Iteration 29/1000 | Loss: 0.00048494
Iteration 30/1000 | Loss: 0.00430249
Iteration 31/1000 | Loss: 0.00445703
Iteration 32/1000 | Loss: 0.00020110
Iteration 33/1000 | Loss: 0.00044082
Iteration 34/1000 | Loss: 0.00120792
Iteration 35/1000 | Loss: 0.00004472
Iteration 36/1000 | Loss: 0.00020117
Iteration 37/1000 | Loss: 0.00021706
Iteration 38/1000 | Loss: 0.00007918
Iteration 39/1000 | Loss: 0.00011458
Iteration 40/1000 | Loss: 0.00035569
Iteration 41/1000 | Loss: 0.00006786
Iteration 42/1000 | Loss: 0.00002622
Iteration 43/1000 | Loss: 0.00011045
Iteration 44/1000 | Loss: 0.00007249
Iteration 45/1000 | Loss: 0.00018498
Iteration 46/1000 | Loss: 0.00067577
Iteration 47/1000 | Loss: 0.00133296
Iteration 48/1000 | Loss: 0.00018183
Iteration 49/1000 | Loss: 0.00046786
Iteration 50/1000 | Loss: 0.00025060
Iteration 51/1000 | Loss: 0.00013994
Iteration 52/1000 | Loss: 0.00046515
Iteration 53/1000 | Loss: 0.00003666
Iteration 54/1000 | Loss: 0.00016493
Iteration 55/1000 | Loss: 0.00021383
Iteration 56/1000 | Loss: 0.00006655
Iteration 57/1000 | Loss: 0.00014694
Iteration 58/1000 | Loss: 0.00016611
Iteration 59/1000 | Loss: 0.00004433
Iteration 60/1000 | Loss: 0.00014570
Iteration 61/1000 | Loss: 0.00052530
Iteration 62/1000 | Loss: 0.00009551
Iteration 63/1000 | Loss: 0.00004879
Iteration 64/1000 | Loss: 0.00021228
Iteration 65/1000 | Loss: 0.00017125
Iteration 66/1000 | Loss: 0.00077977
Iteration 67/1000 | Loss: 0.00022274
Iteration 68/1000 | Loss: 0.00016118
Iteration 69/1000 | Loss: 0.00002984
Iteration 70/1000 | Loss: 0.00002484
Iteration 71/1000 | Loss: 0.00022449
Iteration 72/1000 | Loss: 0.00042920
Iteration 73/1000 | Loss: 0.00070194
Iteration 74/1000 | Loss: 0.00002815
Iteration 75/1000 | Loss: 0.00032569
Iteration 76/1000 | Loss: 0.00016195
Iteration 77/1000 | Loss: 0.00005895
Iteration 78/1000 | Loss: 0.00012185
Iteration 79/1000 | Loss: 0.00015726
Iteration 80/1000 | Loss: 0.00006548
Iteration 81/1000 | Loss: 0.00009557
Iteration 82/1000 | Loss: 0.00014236
Iteration 83/1000 | Loss: 0.00010444
Iteration 84/1000 | Loss: 0.00007720
Iteration 85/1000 | Loss: 0.00007480
Iteration 86/1000 | Loss: 0.00019812
Iteration 87/1000 | Loss: 0.00011363
Iteration 88/1000 | Loss: 0.00006186
Iteration 89/1000 | Loss: 0.00008118
Iteration 90/1000 | Loss: 0.00060629
Iteration 91/1000 | Loss: 0.00018646
Iteration 92/1000 | Loss: 0.00014174
Iteration 93/1000 | Loss: 0.00004017
Iteration 94/1000 | Loss: 0.00002818
Iteration 95/1000 | Loss: 0.00005552
Iteration 96/1000 | Loss: 0.00001976
Iteration 97/1000 | Loss: 0.00056176
Iteration 98/1000 | Loss: 0.00005338
Iteration 99/1000 | Loss: 0.00006945
Iteration 100/1000 | Loss: 0.00014054
Iteration 101/1000 | Loss: 0.00020414
Iteration 102/1000 | Loss: 0.00005102
Iteration 103/1000 | Loss: 0.00029805
Iteration 104/1000 | Loss: 0.00007944
Iteration 105/1000 | Loss: 0.00001887
Iteration 106/1000 | Loss: 0.00028654
Iteration 107/1000 | Loss: 0.00002999
Iteration 108/1000 | Loss: 0.00010932
Iteration 109/1000 | Loss: 0.00018921
Iteration 110/1000 | Loss: 0.00002054
Iteration 111/1000 | Loss: 0.00010255
Iteration 112/1000 | Loss: 0.00016351
Iteration 113/1000 | Loss: 0.00002704
Iteration 114/1000 | Loss: 0.00001811
Iteration 115/1000 | Loss: 0.00001807
Iteration 116/1000 | Loss: 0.00001807
Iteration 117/1000 | Loss: 0.00003572
Iteration 118/1000 | Loss: 0.00006240
Iteration 119/1000 | Loss: 0.00005249
Iteration 120/1000 | Loss: 0.00003533
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001802
Iteration 126/1000 | Loss: 0.00002604
Iteration 127/1000 | Loss: 0.00002021
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001798
Iteration 146/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.7982380086323246e-05, 1.7982380086323246e-05, 1.7982380086323246e-05, 1.7982380086323246e-05, 1.7982380086323246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7982380086323246e-05

Optimization complete. Final v2v error: 3.6469368934631348 mm

Highest mean error: 3.9435248374938965 mm for frame 2

Lowest mean error: 3.4257652759552 mm for frame 112

Saving results

Total time: 175.0105686187744
