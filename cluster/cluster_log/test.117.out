Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=117, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6552-6607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604870
Iteration 2/25 | Loss: 0.00165269
Iteration 3/25 | Loss: 0.00140681
Iteration 4/25 | Loss: 0.00137301
Iteration 5/25 | Loss: 0.00136940
Iteration 6/25 | Loss: 0.00136940
Iteration 7/25 | Loss: 0.00136940
Iteration 8/25 | Loss: 0.00136940
Iteration 9/25 | Loss: 0.00136940
Iteration 10/25 | Loss: 0.00136940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013693950604647398, 0.0013693950604647398, 0.0013693950604647398, 0.0013693950604647398, 0.0013693950604647398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013693950604647398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05154610
Iteration 2/25 | Loss: 0.00067816
Iteration 3/25 | Loss: 0.00067815
Iteration 4/25 | Loss: 0.00067814
Iteration 5/25 | Loss: 0.00067814
Iteration 6/25 | Loss: 0.00067814
Iteration 7/25 | Loss: 0.00067814
Iteration 8/25 | Loss: 0.00067814
Iteration 9/25 | Loss: 0.00067814
Iteration 10/25 | Loss: 0.00067814
Iteration 11/25 | Loss: 0.00067814
Iteration 12/25 | Loss: 0.00067814
Iteration 13/25 | Loss: 0.00067814
Iteration 14/25 | Loss: 0.00067814
Iteration 15/25 | Loss: 0.00067814
Iteration 16/25 | Loss: 0.00067814
Iteration 17/25 | Loss: 0.00067814
Iteration 18/25 | Loss: 0.00067814
Iteration 19/25 | Loss: 0.00067814
Iteration 20/25 | Loss: 0.00067814
Iteration 21/25 | Loss: 0.00067814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006781423580832779, 0.0006781423580832779, 0.0006781423580832779, 0.0006781423580832779, 0.0006781423580832779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006781423580832779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067814
Iteration 2/1000 | Loss: 0.00004257
Iteration 3/1000 | Loss: 0.00003175
Iteration 4/1000 | Loss: 0.00002958
Iteration 5/1000 | Loss: 0.00002786
Iteration 6/1000 | Loss: 0.00002731
Iteration 7/1000 | Loss: 0.00002683
Iteration 8/1000 | Loss: 0.00002642
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002543
Iteration 12/1000 | Loss: 0.00002510
Iteration 13/1000 | Loss: 0.00002487
Iteration 14/1000 | Loss: 0.00002465
Iteration 15/1000 | Loss: 0.00002460
Iteration 16/1000 | Loss: 0.00002460
Iteration 17/1000 | Loss: 0.00002460
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00002460
Iteration 20/1000 | Loss: 0.00002459
Iteration 21/1000 | Loss: 0.00002459
Iteration 22/1000 | Loss: 0.00002459
Iteration 23/1000 | Loss: 0.00002459
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002458
Iteration 26/1000 | Loss: 0.00002458
Iteration 27/1000 | Loss: 0.00002457
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002455
Iteration 30/1000 | Loss: 0.00002455
Iteration 31/1000 | Loss: 0.00002455
Iteration 32/1000 | Loss: 0.00002455
Iteration 33/1000 | Loss: 0.00002455
Iteration 34/1000 | Loss: 0.00002455
Iteration 35/1000 | Loss: 0.00002455
Iteration 36/1000 | Loss: 0.00002455
Iteration 37/1000 | Loss: 0.00002455
Iteration 38/1000 | Loss: 0.00002455
Iteration 39/1000 | Loss: 0.00002454
Iteration 40/1000 | Loss: 0.00002454
Iteration 41/1000 | Loss: 0.00002454
Iteration 42/1000 | Loss: 0.00002454
Iteration 43/1000 | Loss: 0.00002454
Iteration 44/1000 | Loss: 0.00002454
Iteration 45/1000 | Loss: 0.00002454
Iteration 46/1000 | Loss: 0.00002454
Iteration 47/1000 | Loss: 0.00002453
Iteration 48/1000 | Loss: 0.00002453
Iteration 49/1000 | Loss: 0.00002453
Iteration 50/1000 | Loss: 0.00002453
Iteration 51/1000 | Loss: 0.00002452
Iteration 52/1000 | Loss: 0.00002452
Iteration 53/1000 | Loss: 0.00002452
Iteration 54/1000 | Loss: 0.00002452
Iteration 55/1000 | Loss: 0.00002452
Iteration 56/1000 | Loss: 0.00002452
Iteration 57/1000 | Loss: 0.00002451
Iteration 58/1000 | Loss: 0.00002451
Iteration 59/1000 | Loss: 0.00002451
Iteration 60/1000 | Loss: 0.00002451
Iteration 61/1000 | Loss: 0.00002451
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00002450
Iteration 65/1000 | Loss: 0.00002450
Iteration 66/1000 | Loss: 0.00002449
Iteration 67/1000 | Loss: 0.00002449
Iteration 68/1000 | Loss: 0.00002449
Iteration 69/1000 | Loss: 0.00002448
Iteration 70/1000 | Loss: 0.00002448
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002447
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002447
Iteration 78/1000 | Loss: 0.00002447
Iteration 79/1000 | Loss: 0.00002447
Iteration 80/1000 | Loss: 0.00002447
Iteration 81/1000 | Loss: 0.00002446
Iteration 82/1000 | Loss: 0.00002446
Iteration 83/1000 | Loss: 0.00002446
Iteration 84/1000 | Loss: 0.00002446
Iteration 85/1000 | Loss: 0.00002445
Iteration 86/1000 | Loss: 0.00002445
Iteration 87/1000 | Loss: 0.00002445
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002444
Iteration 90/1000 | Loss: 0.00002444
Iteration 91/1000 | Loss: 0.00002444
Iteration 92/1000 | Loss: 0.00002443
Iteration 93/1000 | Loss: 0.00002443
Iteration 94/1000 | Loss: 0.00002443
Iteration 95/1000 | Loss: 0.00002443
Iteration 96/1000 | Loss: 0.00002443
Iteration 97/1000 | Loss: 0.00002443
Iteration 98/1000 | Loss: 0.00002442
Iteration 99/1000 | Loss: 0.00002442
Iteration 100/1000 | Loss: 0.00002442
Iteration 101/1000 | Loss: 0.00002442
Iteration 102/1000 | Loss: 0.00002442
Iteration 103/1000 | Loss: 0.00002441
Iteration 104/1000 | Loss: 0.00002441
Iteration 105/1000 | Loss: 0.00002441
Iteration 106/1000 | Loss: 0.00002440
Iteration 107/1000 | Loss: 0.00002440
Iteration 108/1000 | Loss: 0.00002440
Iteration 109/1000 | Loss: 0.00002440
Iteration 110/1000 | Loss: 0.00002440
Iteration 111/1000 | Loss: 0.00002440
Iteration 112/1000 | Loss: 0.00002440
Iteration 113/1000 | Loss: 0.00002440
Iteration 114/1000 | Loss: 0.00002440
Iteration 115/1000 | Loss: 0.00002440
Iteration 116/1000 | Loss: 0.00002440
Iteration 117/1000 | Loss: 0.00002440
Iteration 118/1000 | Loss: 0.00002440
Iteration 119/1000 | Loss: 0.00002440
Iteration 120/1000 | Loss: 0.00002440
Iteration 121/1000 | Loss: 0.00002440
Iteration 122/1000 | Loss: 0.00002439
Iteration 123/1000 | Loss: 0.00002439
Iteration 124/1000 | Loss: 0.00002439
Iteration 125/1000 | Loss: 0.00002439
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002439
Iteration 128/1000 | Loss: 0.00002439
Iteration 129/1000 | Loss: 0.00002439
Iteration 130/1000 | Loss: 0.00002439
Iteration 131/1000 | Loss: 0.00002439
Iteration 132/1000 | Loss: 0.00002439
Iteration 133/1000 | Loss: 0.00002439
Iteration 134/1000 | Loss: 0.00002439
Iteration 135/1000 | Loss: 0.00002439
Iteration 136/1000 | Loss: 0.00002439
Iteration 137/1000 | Loss: 0.00002439
Iteration 138/1000 | Loss: 0.00002439
Iteration 139/1000 | Loss: 0.00002439
Iteration 140/1000 | Loss: 0.00002439
Iteration 141/1000 | Loss: 0.00002439
Iteration 142/1000 | Loss: 0.00002439
Iteration 143/1000 | Loss: 0.00002439
Iteration 144/1000 | Loss: 0.00002439
Iteration 145/1000 | Loss: 0.00002439
Iteration 146/1000 | Loss: 0.00002439
Iteration 147/1000 | Loss: 0.00002439
Iteration 148/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.4386579752899706e-05, 2.4386579752899706e-05, 2.4386579752899706e-05, 2.4386579752899706e-05, 2.4386579752899706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4386579752899706e-05

Optimization complete. Final v2v error: 4.1594953536987305 mm

Highest mean error: 4.2112627029418945 mm for frame 39

Lowest mean error: 4.117259502410889 mm for frame 217

Saving results

Total time: 42.11118125915527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885591
Iteration 2/25 | Loss: 0.00167677
Iteration 3/25 | Loss: 0.00140492
Iteration 4/25 | Loss: 0.00137896
Iteration 5/25 | Loss: 0.00137595
Iteration 6/25 | Loss: 0.00135803
Iteration 7/25 | Loss: 0.00135754
Iteration 8/25 | Loss: 0.00134594
Iteration 9/25 | Loss: 0.00134400
Iteration 10/25 | Loss: 0.00134262
Iteration 11/25 | Loss: 0.00134138
Iteration 12/25 | Loss: 0.00133988
Iteration 13/25 | Loss: 0.00133970
Iteration 14/25 | Loss: 0.00133968
Iteration 15/25 | Loss: 0.00133967
Iteration 16/25 | Loss: 0.00133967
Iteration 17/25 | Loss: 0.00133967
Iteration 18/25 | Loss: 0.00133967
Iteration 19/25 | Loss: 0.00133967
Iteration 20/25 | Loss: 0.00133967
Iteration 21/25 | Loss: 0.00133967
Iteration 22/25 | Loss: 0.00133967
Iteration 23/25 | Loss: 0.00133967
Iteration 24/25 | Loss: 0.00133967
Iteration 25/25 | Loss: 0.00133966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42754459
Iteration 2/25 | Loss: 0.00088933
Iteration 3/25 | Loss: 0.00088932
Iteration 4/25 | Loss: 0.00088932
Iteration 5/25 | Loss: 0.00088932
Iteration 6/25 | Loss: 0.00088932
Iteration 7/25 | Loss: 0.00088932
Iteration 8/25 | Loss: 0.00088932
Iteration 9/25 | Loss: 0.00088932
Iteration 10/25 | Loss: 0.00088932
Iteration 11/25 | Loss: 0.00088932
Iteration 12/25 | Loss: 0.00088932
Iteration 13/25 | Loss: 0.00088932
Iteration 14/25 | Loss: 0.00088932
Iteration 15/25 | Loss: 0.00088932
Iteration 16/25 | Loss: 0.00088932
Iteration 17/25 | Loss: 0.00088932
Iteration 18/25 | Loss: 0.00088932
Iteration 19/25 | Loss: 0.00088932
Iteration 20/25 | Loss: 0.00088932
Iteration 21/25 | Loss: 0.00088932
Iteration 22/25 | Loss: 0.00088932
Iteration 23/25 | Loss: 0.00088932
Iteration 24/25 | Loss: 0.00088932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008893210324458778, 0.0008893210324458778, 0.0008893210324458778, 0.0008893210324458778, 0.0008893210324458778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008893210324458778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088932
Iteration 2/1000 | Loss: 0.00016054
Iteration 3/1000 | Loss: 0.00005683
Iteration 4/1000 | Loss: 0.00004283
Iteration 5/1000 | Loss: 0.00003752
Iteration 6/1000 | Loss: 0.00019241
Iteration 7/1000 | Loss: 0.00013160
Iteration 8/1000 | Loss: 0.00020234
Iteration 9/1000 | Loss: 0.00019577
Iteration 10/1000 | Loss: 0.00018422
Iteration 11/1000 | Loss: 0.00020113
Iteration 12/1000 | Loss: 0.00003212
Iteration 13/1000 | Loss: 0.00003032
Iteration 14/1000 | Loss: 0.00002930
Iteration 15/1000 | Loss: 0.00032431
Iteration 16/1000 | Loss: 0.00004573
Iteration 17/1000 | Loss: 0.00003345
Iteration 18/1000 | Loss: 0.00033090
Iteration 19/1000 | Loss: 0.00003544
Iteration 20/1000 | Loss: 0.00003183
Iteration 21/1000 | Loss: 0.00015881
Iteration 22/1000 | Loss: 0.00002989
Iteration 23/1000 | Loss: 0.00002851
Iteration 24/1000 | Loss: 0.00002758
Iteration 25/1000 | Loss: 0.00002645
Iteration 26/1000 | Loss: 0.00002538
Iteration 27/1000 | Loss: 0.00002479
Iteration 28/1000 | Loss: 0.00002441
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002420
Iteration 31/1000 | Loss: 0.00002407
Iteration 32/1000 | Loss: 0.00002406
Iteration 33/1000 | Loss: 0.00002406
Iteration 34/1000 | Loss: 0.00002405
Iteration 35/1000 | Loss: 0.00002405
Iteration 36/1000 | Loss: 0.00002403
Iteration 37/1000 | Loss: 0.00018021
Iteration 38/1000 | Loss: 0.00014761
Iteration 39/1000 | Loss: 0.00015647
Iteration 40/1000 | Loss: 0.00016182
Iteration 41/1000 | Loss: 0.00003377
Iteration 42/1000 | Loss: 0.00002988
Iteration 43/1000 | Loss: 0.00002899
Iteration 44/1000 | Loss: 0.00037654
Iteration 45/1000 | Loss: 0.00022229
Iteration 46/1000 | Loss: 0.00002802
Iteration 47/1000 | Loss: 0.00022982
Iteration 48/1000 | Loss: 0.00013983
Iteration 49/1000 | Loss: 0.00022821
Iteration 50/1000 | Loss: 0.00013760
Iteration 51/1000 | Loss: 0.00020833
Iteration 52/1000 | Loss: 0.00015115
Iteration 53/1000 | Loss: 0.00016020
Iteration 54/1000 | Loss: 0.00011841
Iteration 55/1000 | Loss: 0.00003053
Iteration 56/1000 | Loss: 0.00002756
Iteration 57/1000 | Loss: 0.00002642
Iteration 58/1000 | Loss: 0.00002573
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00018849
Iteration 61/1000 | Loss: 0.00003681
Iteration 62/1000 | Loss: 0.00002675
Iteration 63/1000 | Loss: 0.00002507
Iteration 64/1000 | Loss: 0.00002439
Iteration 65/1000 | Loss: 0.00002418
Iteration 66/1000 | Loss: 0.00002391
Iteration 67/1000 | Loss: 0.00002373
Iteration 68/1000 | Loss: 0.00002367
Iteration 69/1000 | Loss: 0.00002351
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002336
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002333
Iteration 74/1000 | Loss: 0.00002332
Iteration 75/1000 | Loss: 0.00002332
Iteration 76/1000 | Loss: 0.00002331
Iteration 77/1000 | Loss: 0.00002330
Iteration 78/1000 | Loss: 0.00002328
Iteration 79/1000 | Loss: 0.00002327
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002324
Iteration 82/1000 | Loss: 0.00002324
Iteration 83/1000 | Loss: 0.00002324
Iteration 84/1000 | Loss: 0.00002324
Iteration 85/1000 | Loss: 0.00002324
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002322
Iteration 90/1000 | Loss: 0.00002322
Iteration 91/1000 | Loss: 0.00002321
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002321
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002320
Iteration 98/1000 | Loss: 0.00002320
Iteration 99/1000 | Loss: 0.00002319
Iteration 100/1000 | Loss: 0.00002319
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002317
Iteration 103/1000 | Loss: 0.00002317
Iteration 104/1000 | Loss: 0.00002316
Iteration 105/1000 | Loss: 0.00002316
Iteration 106/1000 | Loss: 0.00002316
Iteration 107/1000 | Loss: 0.00002316
Iteration 108/1000 | Loss: 0.00002315
Iteration 109/1000 | Loss: 0.00002315
Iteration 110/1000 | Loss: 0.00002315
Iteration 111/1000 | Loss: 0.00002315
Iteration 112/1000 | Loss: 0.00002315
Iteration 113/1000 | Loss: 0.00002315
Iteration 114/1000 | Loss: 0.00002315
Iteration 115/1000 | Loss: 0.00002314
Iteration 116/1000 | Loss: 0.00002314
Iteration 117/1000 | Loss: 0.00002314
Iteration 118/1000 | Loss: 0.00002314
Iteration 119/1000 | Loss: 0.00002313
Iteration 120/1000 | Loss: 0.00002313
Iteration 121/1000 | Loss: 0.00002313
Iteration 122/1000 | Loss: 0.00002313
Iteration 123/1000 | Loss: 0.00002313
Iteration 124/1000 | Loss: 0.00002313
Iteration 125/1000 | Loss: 0.00002313
Iteration 126/1000 | Loss: 0.00002313
Iteration 127/1000 | Loss: 0.00002312
Iteration 128/1000 | Loss: 0.00002312
Iteration 129/1000 | Loss: 0.00002312
Iteration 130/1000 | Loss: 0.00002312
Iteration 131/1000 | Loss: 0.00002312
Iteration 132/1000 | Loss: 0.00002312
Iteration 133/1000 | Loss: 0.00002312
Iteration 134/1000 | Loss: 0.00002312
Iteration 135/1000 | Loss: 0.00002312
Iteration 136/1000 | Loss: 0.00002311
Iteration 137/1000 | Loss: 0.00002311
Iteration 138/1000 | Loss: 0.00002311
Iteration 139/1000 | Loss: 0.00002311
Iteration 140/1000 | Loss: 0.00002311
Iteration 141/1000 | Loss: 0.00002310
Iteration 142/1000 | Loss: 0.00002310
Iteration 143/1000 | Loss: 0.00002310
Iteration 144/1000 | Loss: 0.00002309
Iteration 145/1000 | Loss: 0.00002309
Iteration 146/1000 | Loss: 0.00002309
Iteration 147/1000 | Loss: 0.00002308
Iteration 148/1000 | Loss: 0.00002308
Iteration 149/1000 | Loss: 0.00002308
Iteration 150/1000 | Loss: 0.00002308
Iteration 151/1000 | Loss: 0.00002308
Iteration 152/1000 | Loss: 0.00002307
Iteration 153/1000 | Loss: 0.00002307
Iteration 154/1000 | Loss: 0.00002307
Iteration 155/1000 | Loss: 0.00002307
Iteration 156/1000 | Loss: 0.00002307
Iteration 157/1000 | Loss: 0.00002307
Iteration 158/1000 | Loss: 0.00002307
Iteration 159/1000 | Loss: 0.00002307
Iteration 160/1000 | Loss: 0.00002307
Iteration 161/1000 | Loss: 0.00002307
Iteration 162/1000 | Loss: 0.00002307
Iteration 163/1000 | Loss: 0.00002307
Iteration 164/1000 | Loss: 0.00002307
Iteration 165/1000 | Loss: 0.00002307
Iteration 166/1000 | Loss: 0.00002307
Iteration 167/1000 | Loss: 0.00002307
Iteration 168/1000 | Loss: 0.00002307
Iteration 169/1000 | Loss: 0.00002307
Iteration 170/1000 | Loss: 0.00002307
Iteration 171/1000 | Loss: 0.00002307
Iteration 172/1000 | Loss: 0.00002307
Iteration 173/1000 | Loss: 0.00002307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.3073744159773923e-05, 2.3073744159773923e-05, 2.3073744159773923e-05, 2.3073744159773923e-05, 2.3073744159773923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3073744159773923e-05

Optimization complete. Final v2v error: 3.995393753051758 mm

Highest mean error: 5.526627063751221 mm for frame 222

Lowest mean error: 3.3450963497161865 mm for frame 231

Saving results

Total time: 137.91811180114746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363139
Iteration 2/25 | Loss: 0.00137290
Iteration 3/25 | Loss: 0.00123774
Iteration 4/25 | Loss: 0.00121593
Iteration 5/25 | Loss: 0.00121119
Iteration 6/25 | Loss: 0.00121058
Iteration 7/25 | Loss: 0.00121058
Iteration 8/25 | Loss: 0.00121058
Iteration 9/25 | Loss: 0.00121058
Iteration 10/25 | Loss: 0.00121058
Iteration 11/25 | Loss: 0.00121058
Iteration 12/25 | Loss: 0.00121058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012105802306905389, 0.0012105802306905389, 0.0012105802306905389, 0.0012105802306905389, 0.0012105802306905389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012105802306905389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43676674
Iteration 2/25 | Loss: 0.00073932
Iteration 3/25 | Loss: 0.00073932
Iteration 4/25 | Loss: 0.00073932
Iteration 5/25 | Loss: 0.00073932
Iteration 6/25 | Loss: 0.00073932
Iteration 7/25 | Loss: 0.00073932
Iteration 8/25 | Loss: 0.00073932
Iteration 9/25 | Loss: 0.00073932
Iteration 10/25 | Loss: 0.00073932
Iteration 11/25 | Loss: 0.00073932
Iteration 12/25 | Loss: 0.00073932
Iteration 13/25 | Loss: 0.00073932
Iteration 14/25 | Loss: 0.00073932
Iteration 15/25 | Loss: 0.00073932
Iteration 16/25 | Loss: 0.00073932
Iteration 17/25 | Loss: 0.00073932
Iteration 18/25 | Loss: 0.00073932
Iteration 19/25 | Loss: 0.00073932
Iteration 20/25 | Loss: 0.00073932
Iteration 21/25 | Loss: 0.00073932
Iteration 22/25 | Loss: 0.00073932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007393187843263149, 0.0007393187843263149, 0.0007393187843263149, 0.0007393187843263149, 0.0007393187843263149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007393187843263149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073932
Iteration 2/1000 | Loss: 0.00005478
Iteration 3/1000 | Loss: 0.00003573
Iteration 4/1000 | Loss: 0.00002893
Iteration 5/1000 | Loss: 0.00002696
Iteration 6/1000 | Loss: 0.00002573
Iteration 7/1000 | Loss: 0.00002480
Iteration 8/1000 | Loss: 0.00002429
Iteration 9/1000 | Loss: 0.00002391
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002335
Iteration 12/1000 | Loss: 0.00002310
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002287
Iteration 15/1000 | Loss: 0.00002282
Iteration 16/1000 | Loss: 0.00002273
Iteration 17/1000 | Loss: 0.00002268
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002254
Iteration 20/1000 | Loss: 0.00002253
Iteration 21/1000 | Loss: 0.00002253
Iteration 22/1000 | Loss: 0.00002253
Iteration 23/1000 | Loss: 0.00002252
Iteration 24/1000 | Loss: 0.00002252
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002251
Iteration 27/1000 | Loss: 0.00002250
Iteration 28/1000 | Loss: 0.00002250
Iteration 29/1000 | Loss: 0.00002247
Iteration 30/1000 | Loss: 0.00002247
Iteration 31/1000 | Loss: 0.00002246
Iteration 32/1000 | Loss: 0.00002245
Iteration 33/1000 | Loss: 0.00002245
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002243
Iteration 36/1000 | Loss: 0.00002243
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002241
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002239
Iteration 42/1000 | Loss: 0.00002238
Iteration 43/1000 | Loss: 0.00002238
Iteration 44/1000 | Loss: 0.00002238
Iteration 45/1000 | Loss: 0.00002238
Iteration 46/1000 | Loss: 0.00002237
Iteration 47/1000 | Loss: 0.00002237
Iteration 48/1000 | Loss: 0.00002236
Iteration 49/1000 | Loss: 0.00002236
Iteration 50/1000 | Loss: 0.00002235
Iteration 51/1000 | Loss: 0.00002235
Iteration 52/1000 | Loss: 0.00002235
Iteration 53/1000 | Loss: 0.00002235
Iteration 54/1000 | Loss: 0.00002234
Iteration 55/1000 | Loss: 0.00002234
Iteration 56/1000 | Loss: 0.00002234
Iteration 57/1000 | Loss: 0.00002232
Iteration 58/1000 | Loss: 0.00002232
Iteration 59/1000 | Loss: 0.00002232
Iteration 60/1000 | Loss: 0.00002231
Iteration 61/1000 | Loss: 0.00002231
Iteration 62/1000 | Loss: 0.00002231
Iteration 63/1000 | Loss: 0.00002230
Iteration 64/1000 | Loss: 0.00002230
Iteration 65/1000 | Loss: 0.00002229
Iteration 66/1000 | Loss: 0.00002229
Iteration 67/1000 | Loss: 0.00002228
Iteration 68/1000 | Loss: 0.00002228
Iteration 69/1000 | Loss: 0.00002228
Iteration 70/1000 | Loss: 0.00002227
Iteration 71/1000 | Loss: 0.00002227
Iteration 72/1000 | Loss: 0.00002226
Iteration 73/1000 | Loss: 0.00002226
Iteration 74/1000 | Loss: 0.00002226
Iteration 75/1000 | Loss: 0.00002226
Iteration 76/1000 | Loss: 0.00002225
Iteration 77/1000 | Loss: 0.00002225
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002224
Iteration 80/1000 | Loss: 0.00002223
Iteration 81/1000 | Loss: 0.00002223
Iteration 82/1000 | Loss: 0.00002223
Iteration 83/1000 | Loss: 0.00002222
Iteration 84/1000 | Loss: 0.00002222
Iteration 85/1000 | Loss: 0.00002221
Iteration 86/1000 | Loss: 0.00002221
Iteration 87/1000 | Loss: 0.00002221
Iteration 88/1000 | Loss: 0.00002220
Iteration 89/1000 | Loss: 0.00002220
Iteration 90/1000 | Loss: 0.00002219
Iteration 91/1000 | Loss: 0.00002219
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002218
Iteration 94/1000 | Loss: 0.00002218
Iteration 95/1000 | Loss: 0.00002218
Iteration 96/1000 | Loss: 0.00002218
Iteration 97/1000 | Loss: 0.00002218
Iteration 98/1000 | Loss: 0.00002217
Iteration 99/1000 | Loss: 0.00002217
Iteration 100/1000 | Loss: 0.00002217
Iteration 101/1000 | Loss: 0.00002217
Iteration 102/1000 | Loss: 0.00002217
Iteration 103/1000 | Loss: 0.00002217
Iteration 104/1000 | Loss: 0.00002217
Iteration 105/1000 | Loss: 0.00002216
Iteration 106/1000 | Loss: 0.00002216
Iteration 107/1000 | Loss: 0.00002216
Iteration 108/1000 | Loss: 0.00002215
Iteration 109/1000 | Loss: 0.00002215
Iteration 110/1000 | Loss: 0.00002215
Iteration 111/1000 | Loss: 0.00002215
Iteration 112/1000 | Loss: 0.00002215
Iteration 113/1000 | Loss: 0.00002215
Iteration 114/1000 | Loss: 0.00002214
Iteration 115/1000 | Loss: 0.00002214
Iteration 116/1000 | Loss: 0.00002214
Iteration 117/1000 | Loss: 0.00002214
Iteration 118/1000 | Loss: 0.00002214
Iteration 119/1000 | Loss: 0.00002214
Iteration 120/1000 | Loss: 0.00002214
Iteration 121/1000 | Loss: 0.00002214
Iteration 122/1000 | Loss: 0.00002214
Iteration 123/1000 | Loss: 0.00002214
Iteration 124/1000 | Loss: 0.00002214
Iteration 125/1000 | Loss: 0.00002213
Iteration 126/1000 | Loss: 0.00002213
Iteration 127/1000 | Loss: 0.00002213
Iteration 128/1000 | Loss: 0.00002213
Iteration 129/1000 | Loss: 0.00002213
Iteration 130/1000 | Loss: 0.00002213
Iteration 131/1000 | Loss: 0.00002213
Iteration 132/1000 | Loss: 0.00002213
Iteration 133/1000 | Loss: 0.00002213
Iteration 134/1000 | Loss: 0.00002213
Iteration 135/1000 | Loss: 0.00002213
Iteration 136/1000 | Loss: 0.00002213
Iteration 137/1000 | Loss: 0.00002213
Iteration 138/1000 | Loss: 0.00002213
Iteration 139/1000 | Loss: 0.00002213
Iteration 140/1000 | Loss: 0.00002213
Iteration 141/1000 | Loss: 0.00002213
Iteration 142/1000 | Loss: 0.00002213
Iteration 143/1000 | Loss: 0.00002213
Iteration 144/1000 | Loss: 0.00002213
Iteration 145/1000 | Loss: 0.00002213
Iteration 146/1000 | Loss: 0.00002213
Iteration 147/1000 | Loss: 0.00002213
Iteration 148/1000 | Loss: 0.00002213
Iteration 149/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.2126481781015173e-05, 2.2126481781015173e-05, 2.2126481781015173e-05, 2.2126481781015173e-05, 2.2126481781015173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2126481781015173e-05

Optimization complete. Final v2v error: 3.887564182281494 mm

Highest mean error: 4.9888014793396 mm for frame 189

Lowest mean error: 2.851261615753174 mm for frame 220

Saving results

Total time: 47.04676651954651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808837
Iteration 2/25 | Loss: 0.00139088
Iteration 3/25 | Loss: 0.00124226
Iteration 4/25 | Loss: 0.00123046
Iteration 5/25 | Loss: 0.00122860
Iteration 6/25 | Loss: 0.00122860
Iteration 7/25 | Loss: 0.00122860
Iteration 8/25 | Loss: 0.00122860
Iteration 9/25 | Loss: 0.00122860
Iteration 10/25 | Loss: 0.00122860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001228597480803728, 0.001228597480803728, 0.001228597480803728, 0.001228597480803728, 0.001228597480803728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001228597480803728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44546998
Iteration 2/25 | Loss: 0.00076216
Iteration 3/25 | Loss: 0.00076216
Iteration 4/25 | Loss: 0.00076216
Iteration 5/25 | Loss: 0.00076216
Iteration 6/25 | Loss: 0.00076216
Iteration 7/25 | Loss: 0.00076216
Iteration 8/25 | Loss: 0.00076215
Iteration 9/25 | Loss: 0.00076215
Iteration 10/25 | Loss: 0.00076215
Iteration 11/25 | Loss: 0.00076215
Iteration 12/25 | Loss: 0.00076215
Iteration 13/25 | Loss: 0.00076215
Iteration 14/25 | Loss: 0.00076215
Iteration 15/25 | Loss: 0.00076215
Iteration 16/25 | Loss: 0.00076215
Iteration 17/25 | Loss: 0.00076215
Iteration 18/25 | Loss: 0.00076215
Iteration 19/25 | Loss: 0.00076215
Iteration 20/25 | Loss: 0.00076215
Iteration 21/25 | Loss: 0.00076215
Iteration 22/25 | Loss: 0.00076215
Iteration 23/25 | Loss: 0.00076215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007621535914950073, 0.0007621535914950073, 0.0007621535914950073, 0.0007621535914950073, 0.0007621535914950073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007621535914950073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076215
Iteration 2/1000 | Loss: 0.00002363
Iteration 3/1000 | Loss: 0.00001684
Iteration 4/1000 | Loss: 0.00001504
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001233
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001224
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001190
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.190321290778229e-05, 1.190321290778229e-05, 1.190321290778229e-05, 1.190321290778229e-05, 1.190321290778229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.190321290778229e-05

Optimization complete. Final v2v error: 2.956029176712036 mm

Highest mean error: 3.131718635559082 mm for frame 150

Lowest mean error: 2.859365224838257 mm for frame 118

Saving results

Total time: 39.96580410003662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395088
Iteration 2/25 | Loss: 0.00134041
Iteration 3/25 | Loss: 0.00126253
Iteration 4/25 | Loss: 0.00125031
Iteration 5/25 | Loss: 0.00124608
Iteration 6/25 | Loss: 0.00124542
Iteration 7/25 | Loss: 0.00124542
Iteration 8/25 | Loss: 0.00124542
Iteration 9/25 | Loss: 0.00124542
Iteration 10/25 | Loss: 0.00124542
Iteration 11/25 | Loss: 0.00124542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012454177485778928, 0.0012454177485778928, 0.0012454177485778928, 0.0012454177485778928, 0.0012454177485778928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012454177485778928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90587980
Iteration 2/25 | Loss: 0.00087970
Iteration 3/25 | Loss: 0.00087969
Iteration 4/25 | Loss: 0.00087969
Iteration 5/25 | Loss: 0.00087969
Iteration 6/25 | Loss: 0.00087969
Iteration 7/25 | Loss: 0.00087969
Iteration 8/25 | Loss: 0.00087969
Iteration 9/25 | Loss: 0.00087969
Iteration 10/25 | Loss: 0.00087969
Iteration 11/25 | Loss: 0.00087969
Iteration 12/25 | Loss: 0.00087969
Iteration 13/25 | Loss: 0.00087969
Iteration 14/25 | Loss: 0.00087969
Iteration 15/25 | Loss: 0.00087969
Iteration 16/25 | Loss: 0.00087969
Iteration 17/25 | Loss: 0.00087969
Iteration 18/25 | Loss: 0.00087969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008796876645646989, 0.0008796876645646989, 0.0008796876645646989, 0.0008796876645646989, 0.0008796876645646989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008796876645646989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087969
Iteration 2/1000 | Loss: 0.00003761
Iteration 3/1000 | Loss: 0.00002546
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00002022
Iteration 7/1000 | Loss: 0.00001953
Iteration 8/1000 | Loss: 0.00001907
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001849
Iteration 11/1000 | Loss: 0.00001824
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001758
Iteration 15/1000 | Loss: 0.00001758
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001692
Iteration 23/1000 | Loss: 0.00001691
Iteration 24/1000 | Loss: 0.00001689
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001680
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001676
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001674
Iteration 36/1000 | Loss: 0.00001674
Iteration 37/1000 | Loss: 0.00001673
Iteration 38/1000 | Loss: 0.00001673
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001668
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001666
Iteration 58/1000 | Loss: 0.00001666
Iteration 59/1000 | Loss: 0.00001666
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001663
Iteration 75/1000 | Loss: 0.00001663
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001663
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001662
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001660
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001655
Iteration 110/1000 | Loss: 0.00001655
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001654
Iteration 114/1000 | Loss: 0.00001653
Iteration 115/1000 | Loss: 0.00001652
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001648
Iteration 128/1000 | Loss: 0.00001648
Iteration 129/1000 | Loss: 0.00001648
Iteration 130/1000 | Loss: 0.00001648
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001647
Iteration 134/1000 | Loss: 0.00001647
Iteration 135/1000 | Loss: 0.00001647
Iteration 136/1000 | Loss: 0.00001647
Iteration 137/1000 | Loss: 0.00001646
Iteration 138/1000 | Loss: 0.00001646
Iteration 139/1000 | Loss: 0.00001646
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001646
Iteration 144/1000 | Loss: 0.00001645
Iteration 145/1000 | Loss: 0.00001645
Iteration 146/1000 | Loss: 0.00001645
Iteration 147/1000 | Loss: 0.00001644
Iteration 148/1000 | Loss: 0.00001644
Iteration 149/1000 | Loss: 0.00001644
Iteration 150/1000 | Loss: 0.00001644
Iteration 151/1000 | Loss: 0.00001644
Iteration 152/1000 | Loss: 0.00001644
Iteration 153/1000 | Loss: 0.00001644
Iteration 154/1000 | Loss: 0.00001644
Iteration 155/1000 | Loss: 0.00001644
Iteration 156/1000 | Loss: 0.00001644
Iteration 157/1000 | Loss: 0.00001644
Iteration 158/1000 | Loss: 0.00001644
Iteration 159/1000 | Loss: 0.00001644
Iteration 160/1000 | Loss: 0.00001644
Iteration 161/1000 | Loss: 0.00001644
Iteration 162/1000 | Loss: 0.00001643
Iteration 163/1000 | Loss: 0.00001643
Iteration 164/1000 | Loss: 0.00001643
Iteration 165/1000 | Loss: 0.00001643
Iteration 166/1000 | Loss: 0.00001643
Iteration 167/1000 | Loss: 0.00001643
Iteration 168/1000 | Loss: 0.00001643
Iteration 169/1000 | Loss: 0.00001643
Iteration 170/1000 | Loss: 0.00001643
Iteration 171/1000 | Loss: 0.00001643
Iteration 172/1000 | Loss: 0.00001643
Iteration 173/1000 | Loss: 0.00001643
Iteration 174/1000 | Loss: 0.00001643
Iteration 175/1000 | Loss: 0.00001643
Iteration 176/1000 | Loss: 0.00001643
Iteration 177/1000 | Loss: 0.00001643
Iteration 178/1000 | Loss: 0.00001643
Iteration 179/1000 | Loss: 0.00001642
Iteration 180/1000 | Loss: 0.00001642
Iteration 181/1000 | Loss: 0.00001642
Iteration 182/1000 | Loss: 0.00001642
Iteration 183/1000 | Loss: 0.00001642
Iteration 184/1000 | Loss: 0.00001642
Iteration 185/1000 | Loss: 0.00001642
Iteration 186/1000 | Loss: 0.00001642
Iteration 187/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.6424281056970358e-05, 1.6424281056970358e-05, 1.6424281056970358e-05, 1.6424281056970358e-05, 1.6424281056970358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6424281056970358e-05

Optimization complete. Final v2v error: 3.4004385471343994 mm

Highest mean error: 3.525038242340088 mm for frame 3

Lowest mean error: 3.3480796813964844 mm for frame 186

Saving results

Total time: 50.13206958770752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825613
Iteration 2/25 | Loss: 0.00146799
Iteration 3/25 | Loss: 0.00132989
Iteration 4/25 | Loss: 0.00130947
Iteration 5/25 | Loss: 0.00130401
Iteration 6/25 | Loss: 0.00130345
Iteration 7/25 | Loss: 0.00130345
Iteration 8/25 | Loss: 0.00130345
Iteration 9/25 | Loss: 0.00130345
Iteration 10/25 | Loss: 0.00130345
Iteration 11/25 | Loss: 0.00130345
Iteration 12/25 | Loss: 0.00130345
Iteration 13/25 | Loss: 0.00130345
Iteration 14/25 | Loss: 0.00130345
Iteration 15/25 | Loss: 0.00130345
Iteration 16/25 | Loss: 0.00130345
Iteration 17/25 | Loss: 0.00130345
Iteration 18/25 | Loss: 0.00130345
Iteration 19/25 | Loss: 0.00130345
Iteration 20/25 | Loss: 0.00130345
Iteration 21/25 | Loss: 0.00130345
Iteration 22/25 | Loss: 0.00130345
Iteration 23/25 | Loss: 0.00130345
Iteration 24/25 | Loss: 0.00130345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013034498551860452, 0.0013034498551860452, 0.0013034498551860452, 0.0013034498551860452, 0.0013034498551860452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013034498551860452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40468049
Iteration 2/25 | Loss: 0.00087502
Iteration 3/25 | Loss: 0.00087496
Iteration 4/25 | Loss: 0.00087496
Iteration 5/25 | Loss: 0.00087496
Iteration 6/25 | Loss: 0.00087496
Iteration 7/25 | Loss: 0.00087496
Iteration 8/25 | Loss: 0.00087496
Iteration 9/25 | Loss: 0.00087496
Iteration 10/25 | Loss: 0.00087496
Iteration 11/25 | Loss: 0.00087496
Iteration 12/25 | Loss: 0.00087496
Iteration 13/25 | Loss: 0.00087496
Iteration 14/25 | Loss: 0.00087496
Iteration 15/25 | Loss: 0.00087496
Iteration 16/25 | Loss: 0.00087496
Iteration 17/25 | Loss: 0.00087496
Iteration 18/25 | Loss: 0.00087496
Iteration 19/25 | Loss: 0.00087496
Iteration 20/25 | Loss: 0.00087496
Iteration 21/25 | Loss: 0.00087496
Iteration 22/25 | Loss: 0.00087496
Iteration 23/25 | Loss: 0.00087496
Iteration 24/25 | Loss: 0.00087496
Iteration 25/25 | Loss: 0.00087496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008749613771215081, 0.0008749613771215081, 0.0008749613771215081, 0.0008749613771215081, 0.0008749613771215081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008749613771215081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087496
Iteration 2/1000 | Loss: 0.00005272
Iteration 3/1000 | Loss: 0.00003372
Iteration 4/1000 | Loss: 0.00002851
Iteration 5/1000 | Loss: 0.00002636
Iteration 6/1000 | Loss: 0.00002452
Iteration 7/1000 | Loss: 0.00002343
Iteration 8/1000 | Loss: 0.00002253
Iteration 9/1000 | Loss: 0.00002203
Iteration 10/1000 | Loss: 0.00002154
Iteration 11/1000 | Loss: 0.00002120
Iteration 12/1000 | Loss: 0.00002091
Iteration 13/1000 | Loss: 0.00002071
Iteration 14/1000 | Loss: 0.00002064
Iteration 15/1000 | Loss: 0.00002052
Iteration 16/1000 | Loss: 0.00002044
Iteration 17/1000 | Loss: 0.00002041
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002030
Iteration 21/1000 | Loss: 0.00002028
Iteration 22/1000 | Loss: 0.00002027
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00002026
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002023
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002019
Iteration 30/1000 | Loss: 0.00002015
Iteration 31/1000 | Loss: 0.00002015
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002013
Iteration 35/1000 | Loss: 0.00002013
Iteration 36/1000 | Loss: 0.00002012
Iteration 37/1000 | Loss: 0.00002012
Iteration 38/1000 | Loss: 0.00002012
Iteration 39/1000 | Loss: 0.00002011
Iteration 40/1000 | Loss: 0.00002011
Iteration 41/1000 | Loss: 0.00002011
Iteration 42/1000 | Loss: 0.00002010
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002010
Iteration 47/1000 | Loss: 0.00002010
Iteration 48/1000 | Loss: 0.00002009
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002008
Iteration 51/1000 | Loss: 0.00002008
Iteration 52/1000 | Loss: 0.00002008
Iteration 53/1000 | Loss: 0.00002008
Iteration 54/1000 | Loss: 0.00002008
Iteration 55/1000 | Loss: 0.00002008
Iteration 56/1000 | Loss: 0.00002008
Iteration 57/1000 | Loss: 0.00002008
Iteration 58/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.007840885198675e-05, 2.007840885198675e-05, 2.007840885198675e-05, 2.007840885198675e-05, 2.007840885198675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.007840885198675e-05

Optimization complete. Final v2v error: 3.7190370559692383 mm

Highest mean error: 4.227842330932617 mm for frame 221

Lowest mean error: 3.4460787773132324 mm for frame 10

Saving results

Total time: 40.96376872062683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961162
Iteration 2/25 | Loss: 0.00167431
Iteration 3/25 | Loss: 0.00144490
Iteration 4/25 | Loss: 0.00142185
Iteration 5/25 | Loss: 0.00141440
Iteration 6/25 | Loss: 0.00141386
Iteration 7/25 | Loss: 0.00141386
Iteration 8/25 | Loss: 0.00141386
Iteration 9/25 | Loss: 0.00141386
Iteration 10/25 | Loss: 0.00141386
Iteration 11/25 | Loss: 0.00141386
Iteration 12/25 | Loss: 0.00141386
Iteration 13/25 | Loss: 0.00141386
Iteration 14/25 | Loss: 0.00141386
Iteration 15/25 | Loss: 0.00141386
Iteration 16/25 | Loss: 0.00141386
Iteration 17/25 | Loss: 0.00141386
Iteration 18/25 | Loss: 0.00141386
Iteration 19/25 | Loss: 0.00141386
Iteration 20/25 | Loss: 0.00141386
Iteration 21/25 | Loss: 0.00141386
Iteration 22/25 | Loss: 0.00141386
Iteration 23/25 | Loss: 0.00141386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001413864316418767, 0.001413864316418767, 0.001413864316418767, 0.001413864316418767, 0.001413864316418767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001413864316418767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85513252
Iteration 2/25 | Loss: 0.00088788
Iteration 3/25 | Loss: 0.00088788
Iteration 4/25 | Loss: 0.00088788
Iteration 5/25 | Loss: 0.00088788
Iteration 6/25 | Loss: 0.00088788
Iteration 7/25 | Loss: 0.00088788
Iteration 8/25 | Loss: 0.00088787
Iteration 9/25 | Loss: 0.00088787
Iteration 10/25 | Loss: 0.00088787
Iteration 11/25 | Loss: 0.00088787
Iteration 12/25 | Loss: 0.00088787
Iteration 13/25 | Loss: 0.00088787
Iteration 14/25 | Loss: 0.00088787
Iteration 15/25 | Loss: 0.00088787
Iteration 16/25 | Loss: 0.00088787
Iteration 17/25 | Loss: 0.00088787
Iteration 18/25 | Loss: 0.00088787
Iteration 19/25 | Loss: 0.00088787
Iteration 20/25 | Loss: 0.00088787
Iteration 21/25 | Loss: 0.00088787
Iteration 22/25 | Loss: 0.00088787
Iteration 23/25 | Loss: 0.00088787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008878739317879081, 0.0008878739317879081, 0.0008878739317879081, 0.0008878739317879081, 0.0008878739317879081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008878739317879081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088787
Iteration 2/1000 | Loss: 0.00007372
Iteration 3/1000 | Loss: 0.00005045
Iteration 4/1000 | Loss: 0.00004737
Iteration 5/1000 | Loss: 0.00004473
Iteration 6/1000 | Loss: 0.00004304
Iteration 7/1000 | Loss: 0.00004185
Iteration 8/1000 | Loss: 0.00004116
Iteration 9/1000 | Loss: 0.00004056
Iteration 10/1000 | Loss: 0.00004013
Iteration 11/1000 | Loss: 0.00003967
Iteration 12/1000 | Loss: 0.00003916
Iteration 13/1000 | Loss: 0.00003875
Iteration 14/1000 | Loss: 0.00003836
Iteration 15/1000 | Loss: 0.00003797
Iteration 16/1000 | Loss: 0.00003761
Iteration 17/1000 | Loss: 0.00003737
Iteration 18/1000 | Loss: 0.00003716
Iteration 19/1000 | Loss: 0.00003705
Iteration 20/1000 | Loss: 0.00003689
Iteration 21/1000 | Loss: 0.00003677
Iteration 22/1000 | Loss: 0.00003676
Iteration 23/1000 | Loss: 0.00003672
Iteration 24/1000 | Loss: 0.00003666
Iteration 25/1000 | Loss: 0.00003666
Iteration 26/1000 | Loss: 0.00003663
Iteration 27/1000 | Loss: 0.00003662
Iteration 28/1000 | Loss: 0.00003662
Iteration 29/1000 | Loss: 0.00003662
Iteration 30/1000 | Loss: 0.00003662
Iteration 31/1000 | Loss: 0.00003661
Iteration 32/1000 | Loss: 0.00003661
Iteration 33/1000 | Loss: 0.00003661
Iteration 34/1000 | Loss: 0.00003661
Iteration 35/1000 | Loss: 0.00003660
Iteration 36/1000 | Loss: 0.00003660
Iteration 37/1000 | Loss: 0.00003659
Iteration 38/1000 | Loss: 0.00003658
Iteration 39/1000 | Loss: 0.00003658
Iteration 40/1000 | Loss: 0.00003658
Iteration 41/1000 | Loss: 0.00003657
Iteration 42/1000 | Loss: 0.00003657
Iteration 43/1000 | Loss: 0.00003657
Iteration 44/1000 | Loss: 0.00003657
Iteration 45/1000 | Loss: 0.00003656
Iteration 46/1000 | Loss: 0.00003656
Iteration 47/1000 | Loss: 0.00003655
Iteration 48/1000 | Loss: 0.00003655
Iteration 49/1000 | Loss: 0.00003654
Iteration 50/1000 | Loss: 0.00003654
Iteration 51/1000 | Loss: 0.00003654
Iteration 52/1000 | Loss: 0.00003654
Iteration 53/1000 | Loss: 0.00003654
Iteration 54/1000 | Loss: 0.00003653
Iteration 55/1000 | Loss: 0.00003653
Iteration 56/1000 | Loss: 0.00003653
Iteration 57/1000 | Loss: 0.00003653
Iteration 58/1000 | Loss: 0.00003653
Iteration 59/1000 | Loss: 0.00003653
Iteration 60/1000 | Loss: 0.00003653
Iteration 61/1000 | Loss: 0.00003653
Iteration 62/1000 | Loss: 0.00003653
Iteration 63/1000 | Loss: 0.00003652
Iteration 64/1000 | Loss: 0.00003651
Iteration 65/1000 | Loss: 0.00003651
Iteration 66/1000 | Loss: 0.00003651
Iteration 67/1000 | Loss: 0.00003651
Iteration 68/1000 | Loss: 0.00003650
Iteration 69/1000 | Loss: 0.00003650
Iteration 70/1000 | Loss: 0.00003650
Iteration 71/1000 | Loss: 0.00003650
Iteration 72/1000 | Loss: 0.00003649
Iteration 73/1000 | Loss: 0.00003648
Iteration 74/1000 | Loss: 0.00003648
Iteration 75/1000 | Loss: 0.00003648
Iteration 76/1000 | Loss: 0.00003648
Iteration 77/1000 | Loss: 0.00003648
Iteration 78/1000 | Loss: 0.00003648
Iteration 79/1000 | Loss: 0.00003648
Iteration 80/1000 | Loss: 0.00003648
Iteration 81/1000 | Loss: 0.00003647
Iteration 82/1000 | Loss: 0.00003647
Iteration 83/1000 | Loss: 0.00003647
Iteration 84/1000 | Loss: 0.00003646
Iteration 85/1000 | Loss: 0.00003646
Iteration 86/1000 | Loss: 0.00003646
Iteration 87/1000 | Loss: 0.00003645
Iteration 88/1000 | Loss: 0.00003645
Iteration 89/1000 | Loss: 0.00003645
Iteration 90/1000 | Loss: 0.00003645
Iteration 91/1000 | Loss: 0.00003645
Iteration 92/1000 | Loss: 0.00003644
Iteration 93/1000 | Loss: 0.00003644
Iteration 94/1000 | Loss: 0.00003644
Iteration 95/1000 | Loss: 0.00003644
Iteration 96/1000 | Loss: 0.00003644
Iteration 97/1000 | Loss: 0.00003643
Iteration 98/1000 | Loss: 0.00003643
Iteration 99/1000 | Loss: 0.00003643
Iteration 100/1000 | Loss: 0.00003643
Iteration 101/1000 | Loss: 0.00003643
Iteration 102/1000 | Loss: 0.00003643
Iteration 103/1000 | Loss: 0.00003643
Iteration 104/1000 | Loss: 0.00003643
Iteration 105/1000 | Loss: 0.00003643
Iteration 106/1000 | Loss: 0.00003643
Iteration 107/1000 | Loss: 0.00003643
Iteration 108/1000 | Loss: 0.00003643
Iteration 109/1000 | Loss: 0.00003643
Iteration 110/1000 | Loss: 0.00003642
Iteration 111/1000 | Loss: 0.00003642
Iteration 112/1000 | Loss: 0.00003642
Iteration 113/1000 | Loss: 0.00003642
Iteration 114/1000 | Loss: 0.00003641
Iteration 115/1000 | Loss: 0.00003641
Iteration 116/1000 | Loss: 0.00003640
Iteration 117/1000 | Loss: 0.00003640
Iteration 118/1000 | Loss: 0.00003640
Iteration 119/1000 | Loss: 0.00003640
Iteration 120/1000 | Loss: 0.00003640
Iteration 121/1000 | Loss: 0.00003640
Iteration 122/1000 | Loss: 0.00003640
Iteration 123/1000 | Loss: 0.00003639
Iteration 124/1000 | Loss: 0.00003639
Iteration 125/1000 | Loss: 0.00003639
Iteration 126/1000 | Loss: 0.00003639
Iteration 127/1000 | Loss: 0.00003639
Iteration 128/1000 | Loss: 0.00003639
Iteration 129/1000 | Loss: 0.00003638
Iteration 130/1000 | Loss: 0.00003638
Iteration 131/1000 | Loss: 0.00003638
Iteration 132/1000 | Loss: 0.00003638
Iteration 133/1000 | Loss: 0.00003637
Iteration 134/1000 | Loss: 0.00003637
Iteration 135/1000 | Loss: 0.00003637
Iteration 136/1000 | Loss: 0.00003637
Iteration 137/1000 | Loss: 0.00003637
Iteration 138/1000 | Loss: 0.00003637
Iteration 139/1000 | Loss: 0.00003637
Iteration 140/1000 | Loss: 0.00003637
Iteration 141/1000 | Loss: 0.00003637
Iteration 142/1000 | Loss: 0.00003637
Iteration 143/1000 | Loss: 0.00003637
Iteration 144/1000 | Loss: 0.00003637
Iteration 145/1000 | Loss: 0.00003637
Iteration 146/1000 | Loss: 0.00003636
Iteration 147/1000 | Loss: 0.00003636
Iteration 148/1000 | Loss: 0.00003635
Iteration 149/1000 | Loss: 0.00003635
Iteration 150/1000 | Loss: 0.00003635
Iteration 151/1000 | Loss: 0.00003635
Iteration 152/1000 | Loss: 0.00003635
Iteration 153/1000 | Loss: 0.00003634
Iteration 154/1000 | Loss: 0.00003634
Iteration 155/1000 | Loss: 0.00003634
Iteration 156/1000 | Loss: 0.00003634
Iteration 157/1000 | Loss: 0.00003634
Iteration 158/1000 | Loss: 0.00003633
Iteration 159/1000 | Loss: 0.00003633
Iteration 160/1000 | Loss: 0.00003633
Iteration 161/1000 | Loss: 0.00003632
Iteration 162/1000 | Loss: 0.00003632
Iteration 163/1000 | Loss: 0.00003632
Iteration 164/1000 | Loss: 0.00003632
Iteration 165/1000 | Loss: 0.00003632
Iteration 166/1000 | Loss: 0.00003632
Iteration 167/1000 | Loss: 0.00003632
Iteration 168/1000 | Loss: 0.00003632
Iteration 169/1000 | Loss: 0.00003632
Iteration 170/1000 | Loss: 0.00003632
Iteration 171/1000 | Loss: 0.00003631
Iteration 172/1000 | Loss: 0.00003631
Iteration 173/1000 | Loss: 0.00003631
Iteration 174/1000 | Loss: 0.00003631
Iteration 175/1000 | Loss: 0.00003631
Iteration 176/1000 | Loss: 0.00003630
Iteration 177/1000 | Loss: 0.00003630
Iteration 178/1000 | Loss: 0.00003630
Iteration 179/1000 | Loss: 0.00003630
Iteration 180/1000 | Loss: 0.00003630
Iteration 181/1000 | Loss: 0.00003630
Iteration 182/1000 | Loss: 0.00003630
Iteration 183/1000 | Loss: 0.00003630
Iteration 184/1000 | Loss: 0.00003630
Iteration 185/1000 | Loss: 0.00003630
Iteration 186/1000 | Loss: 0.00003629
Iteration 187/1000 | Loss: 0.00003629
Iteration 188/1000 | Loss: 0.00003629
Iteration 189/1000 | Loss: 0.00003629
Iteration 190/1000 | Loss: 0.00003629
Iteration 191/1000 | Loss: 0.00003629
Iteration 192/1000 | Loss: 0.00003629
Iteration 193/1000 | Loss: 0.00003629
Iteration 194/1000 | Loss: 0.00003629
Iteration 195/1000 | Loss: 0.00003629
Iteration 196/1000 | Loss: 0.00003629
Iteration 197/1000 | Loss: 0.00003629
Iteration 198/1000 | Loss: 0.00003629
Iteration 199/1000 | Loss: 0.00003628
Iteration 200/1000 | Loss: 0.00003628
Iteration 201/1000 | Loss: 0.00003628
Iteration 202/1000 | Loss: 0.00003628
Iteration 203/1000 | Loss: 0.00003628
Iteration 204/1000 | Loss: 0.00003628
Iteration 205/1000 | Loss: 0.00003628
Iteration 206/1000 | Loss: 0.00003628
Iteration 207/1000 | Loss: 0.00003628
Iteration 208/1000 | Loss: 0.00003628
Iteration 209/1000 | Loss: 0.00003628
Iteration 210/1000 | Loss: 0.00003628
Iteration 211/1000 | Loss: 0.00003628
Iteration 212/1000 | Loss: 0.00003628
Iteration 213/1000 | Loss: 0.00003628
Iteration 214/1000 | Loss: 0.00003628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [3.6283963709138334e-05, 3.6283963709138334e-05, 3.6283963709138334e-05, 3.6283963709138334e-05, 3.6283963709138334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6283963709138334e-05

Optimization complete. Final v2v error: 5.057439804077148 mm

Highest mean error: 5.546197891235352 mm for frame 97

Lowest mean error: 4.147584915161133 mm for frame 48

Saving results

Total time: 61.02976083755493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853481
Iteration 2/25 | Loss: 0.00184592
Iteration 3/25 | Loss: 0.00162476
Iteration 4/25 | Loss: 0.00161421
Iteration 5/25 | Loss: 0.00161115
Iteration 6/25 | Loss: 0.00161067
Iteration 7/25 | Loss: 0.00161067
Iteration 8/25 | Loss: 0.00161067
Iteration 9/25 | Loss: 0.00161067
Iteration 10/25 | Loss: 0.00161067
Iteration 11/25 | Loss: 0.00161067
Iteration 12/25 | Loss: 0.00161067
Iteration 13/25 | Loss: 0.00161067
Iteration 14/25 | Loss: 0.00161067
Iteration 15/25 | Loss: 0.00161067
Iteration 16/25 | Loss: 0.00161067
Iteration 17/25 | Loss: 0.00161067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016106695402413607, 0.0016106695402413607, 0.0016106695402413607, 0.0016106695402413607, 0.0016106695402413607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016106695402413607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51714826
Iteration 2/25 | Loss: 0.00107966
Iteration 3/25 | Loss: 0.00107966
Iteration 4/25 | Loss: 0.00107966
Iteration 5/25 | Loss: 0.00107966
Iteration 6/25 | Loss: 0.00107966
Iteration 7/25 | Loss: 0.00107966
Iteration 8/25 | Loss: 0.00107966
Iteration 9/25 | Loss: 0.00107966
Iteration 10/25 | Loss: 0.00107966
Iteration 11/25 | Loss: 0.00107966
Iteration 12/25 | Loss: 0.00107966
Iteration 13/25 | Loss: 0.00107966
Iteration 14/25 | Loss: 0.00107966
Iteration 15/25 | Loss: 0.00107966
Iteration 16/25 | Loss: 0.00107966
Iteration 17/25 | Loss: 0.00107966
Iteration 18/25 | Loss: 0.00107966
Iteration 19/25 | Loss: 0.00107966
Iteration 20/25 | Loss: 0.00107966
Iteration 21/25 | Loss: 0.00107966
Iteration 22/25 | Loss: 0.00107966
Iteration 23/25 | Loss: 0.00107966
Iteration 24/25 | Loss: 0.00107966
Iteration 25/25 | Loss: 0.00107966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107966
Iteration 2/1000 | Loss: 0.00012940
Iteration 3/1000 | Loss: 0.00007207
Iteration 4/1000 | Loss: 0.00005782
Iteration 5/1000 | Loss: 0.00005329
Iteration 6/1000 | Loss: 0.00005182
Iteration 7/1000 | Loss: 0.00005065
Iteration 8/1000 | Loss: 0.00004941
Iteration 9/1000 | Loss: 0.00004833
Iteration 10/1000 | Loss: 0.00004750
Iteration 11/1000 | Loss: 0.00004671
Iteration 12/1000 | Loss: 0.00004604
Iteration 13/1000 | Loss: 0.00004547
Iteration 14/1000 | Loss: 0.00004496
Iteration 15/1000 | Loss: 0.00004453
Iteration 16/1000 | Loss: 0.00004417
Iteration 17/1000 | Loss: 0.00004391
Iteration 18/1000 | Loss: 0.00004364
Iteration 19/1000 | Loss: 0.00004342
Iteration 20/1000 | Loss: 0.00004331
Iteration 21/1000 | Loss: 0.00004331
Iteration 22/1000 | Loss: 0.00004330
Iteration 23/1000 | Loss: 0.00004330
Iteration 24/1000 | Loss: 0.00004330
Iteration 25/1000 | Loss: 0.00004330
Iteration 26/1000 | Loss: 0.00004329
Iteration 27/1000 | Loss: 0.00004329
Iteration 28/1000 | Loss: 0.00004317
Iteration 29/1000 | Loss: 0.00004316
Iteration 30/1000 | Loss: 0.00004303
Iteration 31/1000 | Loss: 0.00004297
Iteration 32/1000 | Loss: 0.00004284
Iteration 33/1000 | Loss: 0.00004281
Iteration 34/1000 | Loss: 0.00004279
Iteration 35/1000 | Loss: 0.00004276
Iteration 36/1000 | Loss: 0.00004276
Iteration 37/1000 | Loss: 0.00004276
Iteration 38/1000 | Loss: 0.00004276
Iteration 39/1000 | Loss: 0.00004276
Iteration 40/1000 | Loss: 0.00004276
Iteration 41/1000 | Loss: 0.00004275
Iteration 42/1000 | Loss: 0.00004275
Iteration 43/1000 | Loss: 0.00004275
Iteration 44/1000 | Loss: 0.00004275
Iteration 45/1000 | Loss: 0.00004275
Iteration 46/1000 | Loss: 0.00004273
Iteration 47/1000 | Loss: 0.00004273
Iteration 48/1000 | Loss: 0.00004273
Iteration 49/1000 | Loss: 0.00004273
Iteration 50/1000 | Loss: 0.00004273
Iteration 51/1000 | Loss: 0.00004272
Iteration 52/1000 | Loss: 0.00004272
Iteration 53/1000 | Loss: 0.00004272
Iteration 54/1000 | Loss: 0.00004272
Iteration 55/1000 | Loss: 0.00004272
Iteration 56/1000 | Loss: 0.00004272
Iteration 57/1000 | Loss: 0.00004272
Iteration 58/1000 | Loss: 0.00004272
Iteration 59/1000 | Loss: 0.00004272
Iteration 60/1000 | Loss: 0.00004271
Iteration 61/1000 | Loss: 0.00004271
Iteration 62/1000 | Loss: 0.00004271
Iteration 63/1000 | Loss: 0.00004271
Iteration 64/1000 | Loss: 0.00004271
Iteration 65/1000 | Loss: 0.00004271
Iteration 66/1000 | Loss: 0.00004271
Iteration 67/1000 | Loss: 0.00004271
Iteration 68/1000 | Loss: 0.00004270
Iteration 69/1000 | Loss: 0.00004270
Iteration 70/1000 | Loss: 0.00004270
Iteration 71/1000 | Loss: 0.00004270
Iteration 72/1000 | Loss: 0.00004270
Iteration 73/1000 | Loss: 0.00004269
Iteration 74/1000 | Loss: 0.00004269
Iteration 75/1000 | Loss: 0.00004269
Iteration 76/1000 | Loss: 0.00004269
Iteration 77/1000 | Loss: 0.00004269
Iteration 78/1000 | Loss: 0.00004269
Iteration 79/1000 | Loss: 0.00004269
Iteration 80/1000 | Loss: 0.00004269
Iteration 81/1000 | Loss: 0.00004269
Iteration 82/1000 | Loss: 0.00004269
Iteration 83/1000 | Loss: 0.00004268
Iteration 84/1000 | Loss: 0.00004268
Iteration 85/1000 | Loss: 0.00004268
Iteration 86/1000 | Loss: 0.00004268
Iteration 87/1000 | Loss: 0.00004267
Iteration 88/1000 | Loss: 0.00004267
Iteration 89/1000 | Loss: 0.00004267
Iteration 90/1000 | Loss: 0.00004267
Iteration 91/1000 | Loss: 0.00004267
Iteration 92/1000 | Loss: 0.00004267
Iteration 93/1000 | Loss: 0.00004267
Iteration 94/1000 | Loss: 0.00004267
Iteration 95/1000 | Loss: 0.00004267
Iteration 96/1000 | Loss: 0.00004267
Iteration 97/1000 | Loss: 0.00004267
Iteration 98/1000 | Loss: 0.00004267
Iteration 99/1000 | Loss: 0.00004267
Iteration 100/1000 | Loss: 0.00004267
Iteration 101/1000 | Loss: 0.00004267
Iteration 102/1000 | Loss: 0.00004267
Iteration 103/1000 | Loss: 0.00004267
Iteration 104/1000 | Loss: 0.00004267
Iteration 105/1000 | Loss: 0.00004267
Iteration 106/1000 | Loss: 0.00004266
Iteration 107/1000 | Loss: 0.00004266
Iteration 108/1000 | Loss: 0.00004266
Iteration 109/1000 | Loss: 0.00004266
Iteration 110/1000 | Loss: 0.00004266
Iteration 111/1000 | Loss: 0.00004266
Iteration 112/1000 | Loss: 0.00004265
Iteration 113/1000 | Loss: 0.00004265
Iteration 114/1000 | Loss: 0.00004265
Iteration 115/1000 | Loss: 0.00004265
Iteration 116/1000 | Loss: 0.00004265
Iteration 117/1000 | Loss: 0.00004264
Iteration 118/1000 | Loss: 0.00004264
Iteration 119/1000 | Loss: 0.00004264
Iteration 120/1000 | Loss: 0.00004264
Iteration 121/1000 | Loss: 0.00004264
Iteration 122/1000 | Loss: 0.00004264
Iteration 123/1000 | Loss: 0.00004263
Iteration 124/1000 | Loss: 0.00004263
Iteration 125/1000 | Loss: 0.00004261
Iteration 126/1000 | Loss: 0.00004261
Iteration 127/1000 | Loss: 0.00004260
Iteration 128/1000 | Loss: 0.00004260
Iteration 129/1000 | Loss: 0.00004260
Iteration 130/1000 | Loss: 0.00004259
Iteration 131/1000 | Loss: 0.00004259
Iteration 132/1000 | Loss: 0.00004259
Iteration 133/1000 | Loss: 0.00004259
Iteration 134/1000 | Loss: 0.00004259
Iteration 135/1000 | Loss: 0.00004259
Iteration 136/1000 | Loss: 0.00004258
Iteration 137/1000 | Loss: 0.00004258
Iteration 138/1000 | Loss: 0.00004258
Iteration 139/1000 | Loss: 0.00004258
Iteration 140/1000 | Loss: 0.00004258
Iteration 141/1000 | Loss: 0.00004258
Iteration 142/1000 | Loss: 0.00004258
Iteration 143/1000 | Loss: 0.00004258
Iteration 144/1000 | Loss: 0.00004258
Iteration 145/1000 | Loss: 0.00004258
Iteration 146/1000 | Loss: 0.00004258
Iteration 147/1000 | Loss: 0.00004258
Iteration 148/1000 | Loss: 0.00004258
Iteration 149/1000 | Loss: 0.00004258
Iteration 150/1000 | Loss: 0.00004258
Iteration 151/1000 | Loss: 0.00004258
Iteration 152/1000 | Loss: 0.00004258
Iteration 153/1000 | Loss: 0.00004258
Iteration 154/1000 | Loss: 0.00004258
Iteration 155/1000 | Loss: 0.00004258
Iteration 156/1000 | Loss: 0.00004258
Iteration 157/1000 | Loss: 0.00004258
Iteration 158/1000 | Loss: 0.00004258
Iteration 159/1000 | Loss: 0.00004258
Iteration 160/1000 | Loss: 0.00004258
Iteration 161/1000 | Loss: 0.00004258
Iteration 162/1000 | Loss: 0.00004258
Iteration 163/1000 | Loss: 0.00004258
Iteration 164/1000 | Loss: 0.00004258
Iteration 165/1000 | Loss: 0.00004258
Iteration 166/1000 | Loss: 0.00004258
Iteration 167/1000 | Loss: 0.00004258
Iteration 168/1000 | Loss: 0.00004258
Iteration 169/1000 | Loss: 0.00004258
Iteration 170/1000 | Loss: 0.00004258
Iteration 171/1000 | Loss: 0.00004258
Iteration 172/1000 | Loss: 0.00004258
Iteration 173/1000 | Loss: 0.00004258
Iteration 174/1000 | Loss: 0.00004258
Iteration 175/1000 | Loss: 0.00004258
Iteration 176/1000 | Loss: 0.00004258
Iteration 177/1000 | Loss: 0.00004258
Iteration 178/1000 | Loss: 0.00004258
Iteration 179/1000 | Loss: 0.00004258
Iteration 180/1000 | Loss: 0.00004258
Iteration 181/1000 | Loss: 0.00004258
Iteration 182/1000 | Loss: 0.00004258
Iteration 183/1000 | Loss: 0.00004258
Iteration 184/1000 | Loss: 0.00004258
Iteration 185/1000 | Loss: 0.00004258
Iteration 186/1000 | Loss: 0.00004258
Iteration 187/1000 | Loss: 0.00004258
Iteration 188/1000 | Loss: 0.00004258
Iteration 189/1000 | Loss: 0.00004258
Iteration 190/1000 | Loss: 0.00004258
Iteration 191/1000 | Loss: 0.00004258
Iteration 192/1000 | Loss: 0.00004258
Iteration 193/1000 | Loss: 0.00004258
Iteration 194/1000 | Loss: 0.00004258
Iteration 195/1000 | Loss: 0.00004258
Iteration 196/1000 | Loss: 0.00004258
Iteration 197/1000 | Loss: 0.00004258
Iteration 198/1000 | Loss: 0.00004258
Iteration 199/1000 | Loss: 0.00004258
Iteration 200/1000 | Loss: 0.00004258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [4.2579871660564095e-05, 4.2579871660564095e-05, 4.2579871660564095e-05, 4.2579871660564095e-05, 4.2579871660564095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2579871660564095e-05

Optimization complete. Final v2v error: 5.364983081817627 mm

Highest mean error: 6.370983600616455 mm for frame 143

Lowest mean error: 5.053300380706787 mm for frame 37

Saving results

Total time: 59.268938064575195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069416
Iteration 2/25 | Loss: 0.01069416
Iteration 3/25 | Loss: 0.01069416
Iteration 4/25 | Loss: 0.01069416
Iteration 5/25 | Loss: 0.01069416
Iteration 6/25 | Loss: 0.01069416
Iteration 7/25 | Loss: 0.01069416
Iteration 8/25 | Loss: 0.01069416
Iteration 9/25 | Loss: 0.01069415
Iteration 10/25 | Loss: 0.01069415
Iteration 11/25 | Loss: 0.01069415
Iteration 12/25 | Loss: 0.01069415
Iteration 13/25 | Loss: 0.01069415
Iteration 14/25 | Loss: 0.01069415
Iteration 15/25 | Loss: 0.01069415
Iteration 16/25 | Loss: 0.01069414
Iteration 17/25 | Loss: 0.01069414
Iteration 18/25 | Loss: 0.01069414
Iteration 19/25 | Loss: 0.01069414
Iteration 20/25 | Loss: 0.01069414
Iteration 21/25 | Loss: 0.01069414
Iteration 22/25 | Loss: 0.01069413
Iteration 23/25 | Loss: 0.01069413
Iteration 24/25 | Loss: 0.01069413
Iteration 25/25 | Loss: 0.01069413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70089698
Iteration 2/25 | Loss: 0.06043409
Iteration 3/25 | Loss: 0.06035027
Iteration 4/25 | Loss: 0.06033197
Iteration 5/25 | Loss: 0.06033197
Iteration 6/25 | Loss: 0.06033197
Iteration 7/25 | Loss: 0.06033197
Iteration 8/25 | Loss: 0.06033197
Iteration 9/25 | Loss: 0.06033197
Iteration 10/25 | Loss: 0.06033197
Iteration 11/25 | Loss: 0.06033197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06033196672797203, 0.06033196672797203, 0.06033196672797203, 0.06033196672797203, 0.06033196672797203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06033196672797203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06033197
Iteration 2/1000 | Loss: 0.01032594
Iteration 3/1000 | Loss: 0.00158588
Iteration 4/1000 | Loss: 0.00349459
Iteration 5/1000 | Loss: 0.00060596
Iteration 6/1000 | Loss: 0.00189155
Iteration 7/1000 | Loss: 0.00015892
Iteration 8/1000 | Loss: 0.00115704
Iteration 9/1000 | Loss: 0.00012470
Iteration 10/1000 | Loss: 0.00012286
Iteration 11/1000 | Loss: 0.00040223
Iteration 12/1000 | Loss: 0.00033788
Iteration 13/1000 | Loss: 0.00125702
Iteration 14/1000 | Loss: 0.00012007
Iteration 15/1000 | Loss: 0.00005805
Iteration 16/1000 | Loss: 0.00014434
Iteration 17/1000 | Loss: 0.00018480
Iteration 18/1000 | Loss: 0.00017750
Iteration 19/1000 | Loss: 0.00056698
Iteration 20/1000 | Loss: 0.00005731
Iteration 21/1000 | Loss: 0.00019555
Iteration 22/1000 | Loss: 0.00091814
Iteration 23/1000 | Loss: 0.00013309
Iteration 24/1000 | Loss: 0.00014664
Iteration 25/1000 | Loss: 0.00003977
Iteration 26/1000 | Loss: 0.00004487
Iteration 27/1000 | Loss: 0.00017245
Iteration 28/1000 | Loss: 0.00003542
Iteration 29/1000 | Loss: 0.00008351
Iteration 30/1000 | Loss: 0.00008342
Iteration 31/1000 | Loss: 0.00004042
Iteration 32/1000 | Loss: 0.00007098
Iteration 33/1000 | Loss: 0.00031870
Iteration 34/1000 | Loss: 0.00021949
Iteration 35/1000 | Loss: 0.00006329
Iteration 36/1000 | Loss: 0.00011107
Iteration 37/1000 | Loss: 0.00008836
Iteration 38/1000 | Loss: 0.00002917
Iteration 39/1000 | Loss: 0.00002362
Iteration 40/1000 | Loss: 0.00012538
Iteration 41/1000 | Loss: 0.00008791
Iteration 42/1000 | Loss: 0.00006964
Iteration 43/1000 | Loss: 0.00003588
Iteration 44/1000 | Loss: 0.00004687
Iteration 45/1000 | Loss: 0.00096489
Iteration 46/1000 | Loss: 0.00014294
Iteration 47/1000 | Loss: 0.00011211
Iteration 48/1000 | Loss: 0.00006880
Iteration 49/1000 | Loss: 0.00005332
Iteration 50/1000 | Loss: 0.00004563
Iteration 51/1000 | Loss: 0.00004767
Iteration 52/1000 | Loss: 0.00003014
Iteration 53/1000 | Loss: 0.00005548
Iteration 54/1000 | Loss: 0.00007539
Iteration 55/1000 | Loss: 0.00002788
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00003706
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00003211
Iteration 60/1000 | Loss: 0.00002045
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00010709
Iteration 63/1000 | Loss: 0.00003788
Iteration 64/1000 | Loss: 0.00002172
Iteration 65/1000 | Loss: 0.00008988
Iteration 66/1000 | Loss: 0.00004935
Iteration 67/1000 | Loss: 0.00004653
Iteration 68/1000 | Loss: 0.00002656
Iteration 69/1000 | Loss: 0.00001989
Iteration 70/1000 | Loss: 0.00002914
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00004177
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001973
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001972
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001971
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00004152
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001959
Iteration 100/1000 | Loss: 0.00001959
Iteration 101/1000 | Loss: 0.00001959
Iteration 102/1000 | Loss: 0.00001959
Iteration 103/1000 | Loss: 0.00001959
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001958
Iteration 107/1000 | Loss: 0.00001958
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001958
Iteration 112/1000 | Loss: 0.00001958
Iteration 113/1000 | Loss: 0.00001957
Iteration 114/1000 | Loss: 0.00001957
Iteration 115/1000 | Loss: 0.00001957
Iteration 116/1000 | Loss: 0.00001957
Iteration 117/1000 | Loss: 0.00001957
Iteration 118/1000 | Loss: 0.00001957
Iteration 119/1000 | Loss: 0.00001957
Iteration 120/1000 | Loss: 0.00001957
Iteration 121/1000 | Loss: 0.00001956
Iteration 122/1000 | Loss: 0.00001956
Iteration 123/1000 | Loss: 0.00001956
Iteration 124/1000 | Loss: 0.00001956
Iteration 125/1000 | Loss: 0.00001956
Iteration 126/1000 | Loss: 0.00001956
Iteration 127/1000 | Loss: 0.00001955
Iteration 128/1000 | Loss: 0.00001955
Iteration 129/1000 | Loss: 0.00001955
Iteration 130/1000 | Loss: 0.00001955
Iteration 131/1000 | Loss: 0.00001955
Iteration 132/1000 | Loss: 0.00001955
Iteration 133/1000 | Loss: 0.00001955
Iteration 134/1000 | Loss: 0.00001955
Iteration 135/1000 | Loss: 0.00001955
Iteration 136/1000 | Loss: 0.00001955
Iteration 137/1000 | Loss: 0.00001954
Iteration 138/1000 | Loss: 0.00001954
Iteration 139/1000 | Loss: 0.00001954
Iteration 140/1000 | Loss: 0.00001953
Iteration 141/1000 | Loss: 0.00001953
Iteration 142/1000 | Loss: 0.00001953
Iteration 143/1000 | Loss: 0.00001953
Iteration 144/1000 | Loss: 0.00001952
Iteration 145/1000 | Loss: 0.00001952
Iteration 146/1000 | Loss: 0.00001952
Iteration 147/1000 | Loss: 0.00001952
Iteration 148/1000 | Loss: 0.00001952
Iteration 149/1000 | Loss: 0.00001951
Iteration 150/1000 | Loss: 0.00001951
Iteration 151/1000 | Loss: 0.00001951
Iteration 152/1000 | Loss: 0.00001951
Iteration 153/1000 | Loss: 0.00001950
Iteration 154/1000 | Loss: 0.00001950
Iteration 155/1000 | Loss: 0.00001950
Iteration 156/1000 | Loss: 0.00001950
Iteration 157/1000 | Loss: 0.00001950
Iteration 158/1000 | Loss: 0.00001949
Iteration 159/1000 | Loss: 0.00001949
Iteration 160/1000 | Loss: 0.00001949
Iteration 161/1000 | Loss: 0.00001949
Iteration 162/1000 | Loss: 0.00001949
Iteration 163/1000 | Loss: 0.00001949
Iteration 164/1000 | Loss: 0.00001948
Iteration 165/1000 | Loss: 0.00001948
Iteration 166/1000 | Loss: 0.00001948
Iteration 167/1000 | Loss: 0.00001948
Iteration 168/1000 | Loss: 0.00001948
Iteration 169/1000 | Loss: 0.00001947
Iteration 170/1000 | Loss: 0.00003991
Iteration 171/1000 | Loss: 0.00002962
Iteration 172/1000 | Loss: 0.00001974
Iteration 173/1000 | Loss: 0.00001951
Iteration 174/1000 | Loss: 0.00001948
Iteration 175/1000 | Loss: 0.00002619
Iteration 176/1000 | Loss: 0.00001947
Iteration 177/1000 | Loss: 0.00001946
Iteration 178/1000 | Loss: 0.00001946
Iteration 179/1000 | Loss: 0.00001946
Iteration 180/1000 | Loss: 0.00001946
Iteration 181/1000 | Loss: 0.00001946
Iteration 182/1000 | Loss: 0.00003060
Iteration 183/1000 | Loss: 0.00002488
Iteration 184/1000 | Loss: 0.00001946
Iteration 185/1000 | Loss: 0.00001946
Iteration 186/1000 | Loss: 0.00001946
Iteration 187/1000 | Loss: 0.00001946
Iteration 188/1000 | Loss: 0.00001946
Iteration 189/1000 | Loss: 0.00001945
Iteration 190/1000 | Loss: 0.00001945
Iteration 191/1000 | Loss: 0.00001945
Iteration 192/1000 | Loss: 0.00002566
Iteration 193/1000 | Loss: 0.00001944
Iteration 194/1000 | Loss: 0.00001944
Iteration 195/1000 | Loss: 0.00002366
Iteration 196/1000 | Loss: 0.00001998
Iteration 197/1000 | Loss: 0.00001946
Iteration 198/1000 | Loss: 0.00001945
Iteration 199/1000 | Loss: 0.00001945
Iteration 200/1000 | Loss: 0.00001945
Iteration 201/1000 | Loss: 0.00001945
Iteration 202/1000 | Loss: 0.00001945
Iteration 203/1000 | Loss: 0.00001945
Iteration 204/1000 | Loss: 0.00001945
Iteration 205/1000 | Loss: 0.00001945
Iteration 206/1000 | Loss: 0.00001945
Iteration 207/1000 | Loss: 0.00001945
Iteration 208/1000 | Loss: 0.00001945
Iteration 209/1000 | Loss: 0.00001944
Iteration 210/1000 | Loss: 0.00001964
Iteration 211/1000 | Loss: 0.00001965
Iteration 212/1000 | Loss: 0.00001951
Iteration 213/1000 | Loss: 0.00001945
Iteration 214/1000 | Loss: 0.00001944
Iteration 215/1000 | Loss: 0.00001944
Iteration 216/1000 | Loss: 0.00001944
Iteration 217/1000 | Loss: 0.00001944
Iteration 218/1000 | Loss: 0.00001944
Iteration 219/1000 | Loss: 0.00001944
Iteration 220/1000 | Loss: 0.00001944
Iteration 221/1000 | Loss: 0.00001944
Iteration 222/1000 | Loss: 0.00001944
Iteration 223/1000 | Loss: 0.00001944
Iteration 224/1000 | Loss: 0.00001944
Iteration 225/1000 | Loss: 0.00001944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.944142786669545e-05, 1.944142786669545e-05, 1.944142786669545e-05, 1.944142786669545e-05, 1.944142786669545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.944142786669545e-05

Optimization complete. Final v2v error: 3.5923755168914795 mm

Highest mean error: 4.822825908660889 mm for frame 58

Lowest mean error: 2.905647039413452 mm for frame 240

Saving results

Total time: 151.4644272327423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786876
Iteration 2/25 | Loss: 0.00144314
Iteration 3/25 | Loss: 0.00126951
Iteration 4/25 | Loss: 0.00124486
Iteration 5/25 | Loss: 0.00123953
Iteration 6/25 | Loss: 0.00123833
Iteration 7/25 | Loss: 0.00123830
Iteration 8/25 | Loss: 0.00123830
Iteration 9/25 | Loss: 0.00123830
Iteration 10/25 | Loss: 0.00123830
Iteration 11/25 | Loss: 0.00123830
Iteration 12/25 | Loss: 0.00123830
Iteration 13/25 | Loss: 0.00123830
Iteration 14/25 | Loss: 0.00123830
Iteration 15/25 | Loss: 0.00123830
Iteration 16/25 | Loss: 0.00123830
Iteration 17/25 | Loss: 0.00123830
Iteration 18/25 | Loss: 0.00123830
Iteration 19/25 | Loss: 0.00123830
Iteration 20/25 | Loss: 0.00123830
Iteration 21/25 | Loss: 0.00123830
Iteration 22/25 | Loss: 0.00123830
Iteration 23/25 | Loss: 0.00123830
Iteration 24/25 | Loss: 0.00123830
Iteration 25/25 | Loss: 0.00123830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43909991
Iteration 2/25 | Loss: 0.00069790
Iteration 3/25 | Loss: 0.00069790
Iteration 4/25 | Loss: 0.00069790
Iteration 5/25 | Loss: 0.00069790
Iteration 6/25 | Loss: 0.00069790
Iteration 7/25 | Loss: 0.00069790
Iteration 8/25 | Loss: 0.00069790
Iteration 9/25 | Loss: 0.00069790
Iteration 10/25 | Loss: 0.00069790
Iteration 11/25 | Loss: 0.00069790
Iteration 12/25 | Loss: 0.00069790
Iteration 13/25 | Loss: 0.00069790
Iteration 14/25 | Loss: 0.00069790
Iteration 15/25 | Loss: 0.00069790
Iteration 16/25 | Loss: 0.00069790
Iteration 17/25 | Loss: 0.00069790
Iteration 18/25 | Loss: 0.00069790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006978993769735098, 0.0006978993769735098, 0.0006978993769735098, 0.0006978993769735098, 0.0006978993769735098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006978993769735098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069790
Iteration 2/1000 | Loss: 0.00004044
Iteration 3/1000 | Loss: 0.00002555
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001940
Iteration 7/1000 | Loss: 0.00001861
Iteration 8/1000 | Loss: 0.00001798
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001699
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001666
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001661
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001649
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001648
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001641
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001639
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001636
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001635
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001635
Iteration 92/1000 | Loss: 0.00001635
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001634
Iteration 96/1000 | Loss: 0.00001634
Iteration 97/1000 | Loss: 0.00001634
Iteration 98/1000 | Loss: 0.00001634
Iteration 99/1000 | Loss: 0.00001634
Iteration 100/1000 | Loss: 0.00001634
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001634
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001633
Iteration 111/1000 | Loss: 0.00001633
Iteration 112/1000 | Loss: 0.00001633
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00001633
Iteration 116/1000 | Loss: 0.00001633
Iteration 117/1000 | Loss: 0.00001633
Iteration 118/1000 | Loss: 0.00001633
Iteration 119/1000 | Loss: 0.00001633
Iteration 120/1000 | Loss: 0.00001633
Iteration 121/1000 | Loss: 0.00001633
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001632
Iteration 124/1000 | Loss: 0.00001632
Iteration 125/1000 | Loss: 0.00001632
Iteration 126/1000 | Loss: 0.00001632
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001630
Iteration 132/1000 | Loss: 0.00001630
Iteration 133/1000 | Loss: 0.00001630
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001630
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001630
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001629
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001628
Iteration 150/1000 | Loss: 0.00001628
Iteration 151/1000 | Loss: 0.00001628
Iteration 152/1000 | Loss: 0.00001628
Iteration 153/1000 | Loss: 0.00001628
Iteration 154/1000 | Loss: 0.00001628
Iteration 155/1000 | Loss: 0.00001628
Iteration 156/1000 | Loss: 0.00001628
Iteration 157/1000 | Loss: 0.00001628
Iteration 158/1000 | Loss: 0.00001628
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001628
Iteration 163/1000 | Loss: 0.00001628
Iteration 164/1000 | Loss: 0.00001627
Iteration 165/1000 | Loss: 0.00001627
Iteration 166/1000 | Loss: 0.00001627
Iteration 167/1000 | Loss: 0.00001627
Iteration 168/1000 | Loss: 0.00001627
Iteration 169/1000 | Loss: 0.00001627
Iteration 170/1000 | Loss: 0.00001627
Iteration 171/1000 | Loss: 0.00001627
Iteration 172/1000 | Loss: 0.00001627
Iteration 173/1000 | Loss: 0.00001627
Iteration 174/1000 | Loss: 0.00001627
Iteration 175/1000 | Loss: 0.00001627
Iteration 176/1000 | Loss: 0.00001627
Iteration 177/1000 | Loss: 0.00001627
Iteration 178/1000 | Loss: 0.00001627
Iteration 179/1000 | Loss: 0.00001627
Iteration 180/1000 | Loss: 0.00001627
Iteration 181/1000 | Loss: 0.00001627
Iteration 182/1000 | Loss: 0.00001627
Iteration 183/1000 | Loss: 0.00001627
Iteration 184/1000 | Loss: 0.00001627
Iteration 185/1000 | Loss: 0.00001627
Iteration 186/1000 | Loss: 0.00001627
Iteration 187/1000 | Loss: 0.00001627
Iteration 188/1000 | Loss: 0.00001627
Iteration 189/1000 | Loss: 0.00001627
Iteration 190/1000 | Loss: 0.00001627
Iteration 191/1000 | Loss: 0.00001627
Iteration 192/1000 | Loss: 0.00001627
Iteration 193/1000 | Loss: 0.00001627
Iteration 194/1000 | Loss: 0.00001627
Iteration 195/1000 | Loss: 0.00001627
Iteration 196/1000 | Loss: 0.00001627
Iteration 197/1000 | Loss: 0.00001627
Iteration 198/1000 | Loss: 0.00001627
Iteration 199/1000 | Loss: 0.00001627
Iteration 200/1000 | Loss: 0.00001627
Iteration 201/1000 | Loss: 0.00001627
Iteration 202/1000 | Loss: 0.00001627
Iteration 203/1000 | Loss: 0.00001627
Iteration 204/1000 | Loss: 0.00001627
Iteration 205/1000 | Loss: 0.00001627
Iteration 206/1000 | Loss: 0.00001627
Iteration 207/1000 | Loss: 0.00001627
Iteration 208/1000 | Loss: 0.00001627
Iteration 209/1000 | Loss: 0.00001627
Iteration 210/1000 | Loss: 0.00001627
Iteration 211/1000 | Loss: 0.00001627
Iteration 212/1000 | Loss: 0.00001627
Iteration 213/1000 | Loss: 0.00001627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.6272819266305305e-05, 1.6272819266305305e-05, 1.6272819266305305e-05, 1.6272819266305305e-05, 1.6272819266305305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6272819266305305e-05

Optimization complete. Final v2v error: 3.393305540084839 mm

Highest mean error: 4.131651878356934 mm for frame 88

Lowest mean error: 3.0052196979522705 mm for frame 55

Saving results

Total time: 40.64096188545227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806123
Iteration 2/25 | Loss: 0.00128819
Iteration 3/25 | Loss: 0.00120187
Iteration 4/25 | Loss: 0.00117818
Iteration 5/25 | Loss: 0.00117172
Iteration 6/25 | Loss: 0.00117120
Iteration 7/25 | Loss: 0.00117120
Iteration 8/25 | Loss: 0.00117120
Iteration 9/25 | Loss: 0.00117120
Iteration 10/25 | Loss: 0.00117120
Iteration 11/25 | Loss: 0.00117120
Iteration 12/25 | Loss: 0.00117120
Iteration 13/25 | Loss: 0.00117120
Iteration 14/25 | Loss: 0.00117120
Iteration 15/25 | Loss: 0.00117120
Iteration 16/25 | Loss: 0.00117120
Iteration 17/25 | Loss: 0.00117120
Iteration 18/25 | Loss: 0.00117120
Iteration 19/25 | Loss: 0.00117120
Iteration 20/25 | Loss: 0.00117120
Iteration 21/25 | Loss: 0.00117120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011712018167600036, 0.0011712018167600036, 0.0011712018167600036, 0.0011712018167600036, 0.0011712018167600036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011712018167600036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.65982437
Iteration 2/25 | Loss: 0.00082701
Iteration 3/25 | Loss: 0.00082696
Iteration 4/25 | Loss: 0.00082696
Iteration 5/25 | Loss: 0.00082696
Iteration 6/25 | Loss: 0.00082696
Iteration 7/25 | Loss: 0.00082696
Iteration 8/25 | Loss: 0.00082696
Iteration 9/25 | Loss: 0.00082696
Iteration 10/25 | Loss: 0.00082696
Iteration 11/25 | Loss: 0.00082696
Iteration 12/25 | Loss: 0.00082696
Iteration 13/25 | Loss: 0.00082696
Iteration 14/25 | Loss: 0.00082696
Iteration 15/25 | Loss: 0.00082696
Iteration 16/25 | Loss: 0.00082696
Iteration 17/25 | Loss: 0.00082696
Iteration 18/25 | Loss: 0.00082696
Iteration 19/25 | Loss: 0.00082696
Iteration 20/25 | Loss: 0.00082696
Iteration 21/25 | Loss: 0.00082696
Iteration 22/25 | Loss: 0.00082696
Iteration 23/25 | Loss: 0.00082696
Iteration 24/25 | Loss: 0.00082696
Iteration 25/25 | Loss: 0.00082696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082696
Iteration 2/1000 | Loss: 0.00003006
Iteration 3/1000 | Loss: 0.00002006
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001704
Iteration 6/1000 | Loss: 0.00001627
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001542
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001444
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001423
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001417
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001407
Iteration 61/1000 | Loss: 0.00001407
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001406
Iteration 65/1000 | Loss: 0.00001406
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001403
Iteration 79/1000 | Loss: 0.00001403
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001402
Iteration 83/1000 | Loss: 0.00001402
Iteration 84/1000 | Loss: 0.00001402
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001402
Iteration 90/1000 | Loss: 0.00001401
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001401
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001400
Iteration 106/1000 | Loss: 0.00001400
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001400
Iteration 127/1000 | Loss: 0.00001400
Iteration 128/1000 | Loss: 0.00001400
Iteration 129/1000 | Loss: 0.00001400
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.3995800145494286e-05, 1.3995800145494286e-05, 1.3995800145494286e-05, 1.3995800145494286e-05, 1.3995800145494286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3995800145494286e-05

Optimization complete. Final v2v error: 3.19736647605896 mm

Highest mean error: 3.577962636947632 mm for frame 86

Lowest mean error: 2.8975021839141846 mm for frame 232

Saving results

Total time: 39.85774374008179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440810
Iteration 2/25 | Loss: 0.00144964
Iteration 3/25 | Loss: 0.00127199
Iteration 4/25 | Loss: 0.00126267
Iteration 5/25 | Loss: 0.00126100
Iteration 6/25 | Loss: 0.00126071
Iteration 7/25 | Loss: 0.00126071
Iteration 8/25 | Loss: 0.00126071
Iteration 9/25 | Loss: 0.00126071
Iteration 10/25 | Loss: 0.00126071
Iteration 11/25 | Loss: 0.00126071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012607139069586992, 0.0012607139069586992, 0.0012607139069586992, 0.0012607139069586992, 0.0012607139069586992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012607139069586992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.35369778
Iteration 2/25 | Loss: 0.00078670
Iteration 3/25 | Loss: 0.00078670
Iteration 4/25 | Loss: 0.00078670
Iteration 5/25 | Loss: 0.00078670
Iteration 6/25 | Loss: 0.00078670
Iteration 7/25 | Loss: 0.00078670
Iteration 8/25 | Loss: 0.00078670
Iteration 9/25 | Loss: 0.00078670
Iteration 10/25 | Loss: 0.00078670
Iteration 11/25 | Loss: 0.00078670
Iteration 12/25 | Loss: 0.00078670
Iteration 13/25 | Loss: 0.00078670
Iteration 14/25 | Loss: 0.00078670
Iteration 15/25 | Loss: 0.00078670
Iteration 16/25 | Loss: 0.00078670
Iteration 17/25 | Loss: 0.00078670
Iteration 18/25 | Loss: 0.00078670
Iteration 19/25 | Loss: 0.00078670
Iteration 20/25 | Loss: 0.00078670
Iteration 21/25 | Loss: 0.00078670
Iteration 22/25 | Loss: 0.00078670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007866976666264236, 0.0007866976666264236, 0.0007866976666264236, 0.0007866976666264236, 0.0007866976666264236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007866976666264236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078670
Iteration 2/1000 | Loss: 0.00003684
Iteration 3/1000 | Loss: 0.00002467
Iteration 4/1000 | Loss: 0.00002010
Iteration 5/1000 | Loss: 0.00001873
Iteration 6/1000 | Loss: 0.00001791
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001631
Iteration 11/1000 | Loss: 0.00001611
Iteration 12/1000 | Loss: 0.00001607
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001600
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001590
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001573
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001568
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001567
Iteration 36/1000 | Loss: 0.00001566
Iteration 37/1000 | Loss: 0.00001565
Iteration 38/1000 | Loss: 0.00001565
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001564
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001559
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00001558
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001555
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001550
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001546
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001542
Iteration 130/1000 | Loss: 0.00001542
Iteration 131/1000 | Loss: 0.00001542
Iteration 132/1000 | Loss: 0.00001542
Iteration 133/1000 | Loss: 0.00001542
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001541
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001541
Iteration 143/1000 | Loss: 0.00001541
Iteration 144/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5411402273457497e-05, 1.5411402273457497e-05, 1.5411402273457497e-05, 1.5411402273457497e-05, 1.5411402273457497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5411402273457497e-05

Optimization complete. Final v2v error: 3.3071463108062744 mm

Highest mean error: 4.033354759216309 mm for frame 71

Lowest mean error: 2.8813767433166504 mm for frame 101

Saving results

Total time: 37.278810024261475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770900
Iteration 2/25 | Loss: 0.00134051
Iteration 3/25 | Loss: 0.00124641
Iteration 4/25 | Loss: 0.00122664
Iteration 5/25 | Loss: 0.00122036
Iteration 6/25 | Loss: 0.00121931
Iteration 7/25 | Loss: 0.00121926
Iteration 8/25 | Loss: 0.00121926
Iteration 9/25 | Loss: 0.00121926
Iteration 10/25 | Loss: 0.00121926
Iteration 11/25 | Loss: 0.00121926
Iteration 12/25 | Loss: 0.00121926
Iteration 13/25 | Loss: 0.00121926
Iteration 14/25 | Loss: 0.00121926
Iteration 15/25 | Loss: 0.00121926
Iteration 16/25 | Loss: 0.00121926
Iteration 17/25 | Loss: 0.00121926
Iteration 18/25 | Loss: 0.00121926
Iteration 19/25 | Loss: 0.00121926
Iteration 20/25 | Loss: 0.00121926
Iteration 21/25 | Loss: 0.00121926
Iteration 22/25 | Loss: 0.00121926
Iteration 23/25 | Loss: 0.00121926
Iteration 24/25 | Loss: 0.00121926
Iteration 25/25 | Loss: 0.00121926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012192626018077135, 0.0012192626018077135, 0.0012192626018077135, 0.0012192626018077135, 0.0012192626018077135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012192626018077135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38396168
Iteration 2/25 | Loss: 0.00081718
Iteration 3/25 | Loss: 0.00081718
Iteration 4/25 | Loss: 0.00081718
Iteration 5/25 | Loss: 0.00081718
Iteration 6/25 | Loss: 0.00081718
Iteration 7/25 | Loss: 0.00081718
Iteration 8/25 | Loss: 0.00081718
Iteration 9/25 | Loss: 0.00081718
Iteration 10/25 | Loss: 0.00081718
Iteration 11/25 | Loss: 0.00081718
Iteration 12/25 | Loss: 0.00081718
Iteration 13/25 | Loss: 0.00081718
Iteration 14/25 | Loss: 0.00081718
Iteration 15/25 | Loss: 0.00081718
Iteration 16/25 | Loss: 0.00081718
Iteration 17/25 | Loss: 0.00081718
Iteration 18/25 | Loss: 0.00081718
Iteration 19/25 | Loss: 0.00081718
Iteration 20/25 | Loss: 0.00081718
Iteration 21/25 | Loss: 0.00081718
Iteration 22/25 | Loss: 0.00081718
Iteration 23/25 | Loss: 0.00081718
Iteration 24/25 | Loss: 0.00081718
Iteration 25/25 | Loss: 0.00081718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081718
Iteration 2/1000 | Loss: 0.00003940
Iteration 3/1000 | Loss: 0.00002684
Iteration 4/1000 | Loss: 0.00002118
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001651
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001631
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001630
Iteration 18/1000 | Loss: 0.00001629
Iteration 19/1000 | Loss: 0.00001629
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001624
Iteration 26/1000 | Loss: 0.00001624
Iteration 27/1000 | Loss: 0.00001624
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001622
Iteration 31/1000 | Loss: 0.00001622
Iteration 32/1000 | Loss: 0.00001622
Iteration 33/1000 | Loss: 0.00001621
Iteration 34/1000 | Loss: 0.00001620
Iteration 35/1000 | Loss: 0.00001619
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001602
Iteration 49/1000 | Loss: 0.00001602
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001602
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001601
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001600
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001596
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001595
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001594
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001594
Iteration 80/1000 | Loss: 0.00001594
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001593
Iteration 84/1000 | Loss: 0.00001593
Iteration 85/1000 | Loss: 0.00001593
Iteration 86/1000 | Loss: 0.00001593
Iteration 87/1000 | Loss: 0.00001592
Iteration 88/1000 | Loss: 0.00001592
Iteration 89/1000 | Loss: 0.00001592
Iteration 90/1000 | Loss: 0.00001592
Iteration 91/1000 | Loss: 0.00001592
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001591
Iteration 95/1000 | Loss: 0.00001591
Iteration 96/1000 | Loss: 0.00001591
Iteration 97/1000 | Loss: 0.00001591
Iteration 98/1000 | Loss: 0.00001590
Iteration 99/1000 | Loss: 0.00001590
Iteration 100/1000 | Loss: 0.00001590
Iteration 101/1000 | Loss: 0.00001590
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001590
Iteration 105/1000 | Loss: 0.00001590
Iteration 106/1000 | Loss: 0.00001590
Iteration 107/1000 | Loss: 0.00001590
Iteration 108/1000 | Loss: 0.00001590
Iteration 109/1000 | Loss: 0.00001590
Iteration 110/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.589995554240886e-05, 1.589995554240886e-05, 1.589995554240886e-05, 1.589995554240886e-05, 1.589995554240886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.589995554240886e-05

Optimization complete. Final v2v error: 3.3710808753967285 mm

Highest mean error: 3.728914976119995 mm for frame 92

Lowest mean error: 3.0131618976593018 mm for frame 21

Saving results

Total time: 39.50117206573486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887894
Iteration 2/25 | Loss: 0.00156753
Iteration 3/25 | Loss: 0.00135703
Iteration 4/25 | Loss: 0.00130917
Iteration 5/25 | Loss: 0.00129643
Iteration 6/25 | Loss: 0.00129374
Iteration 7/25 | Loss: 0.00129310
Iteration 8/25 | Loss: 0.00129310
Iteration 9/25 | Loss: 0.00129310
Iteration 10/25 | Loss: 0.00129310
Iteration 11/25 | Loss: 0.00129310
Iteration 12/25 | Loss: 0.00129310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012931033270433545, 0.0012931033270433545, 0.0012931033270433545, 0.0012931033270433545, 0.0012931033270433545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012931033270433545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41560113
Iteration 2/25 | Loss: 0.00089677
Iteration 3/25 | Loss: 0.00089676
Iteration 4/25 | Loss: 0.00089676
Iteration 5/25 | Loss: 0.00089676
Iteration 6/25 | Loss: 0.00089676
Iteration 7/25 | Loss: 0.00089676
Iteration 8/25 | Loss: 0.00089676
Iteration 9/25 | Loss: 0.00089676
Iteration 10/25 | Loss: 0.00089676
Iteration 11/25 | Loss: 0.00089676
Iteration 12/25 | Loss: 0.00089676
Iteration 13/25 | Loss: 0.00089676
Iteration 14/25 | Loss: 0.00089676
Iteration 15/25 | Loss: 0.00089676
Iteration 16/25 | Loss: 0.00089676
Iteration 17/25 | Loss: 0.00089676
Iteration 18/25 | Loss: 0.00089676
Iteration 19/25 | Loss: 0.00089676
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008967572939582169, 0.0008967572939582169, 0.0008967572939582169, 0.0008967572939582169, 0.0008967572939582169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008967572939582169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089676
Iteration 2/1000 | Loss: 0.00006890
Iteration 3/1000 | Loss: 0.00004888
Iteration 4/1000 | Loss: 0.00003866
Iteration 5/1000 | Loss: 0.00003638
Iteration 6/1000 | Loss: 0.00003447
Iteration 7/1000 | Loss: 0.00003315
Iteration 8/1000 | Loss: 0.00003204
Iteration 9/1000 | Loss: 0.00003122
Iteration 10/1000 | Loss: 0.00003070
Iteration 11/1000 | Loss: 0.00003031
Iteration 12/1000 | Loss: 0.00003001
Iteration 13/1000 | Loss: 0.00002976
Iteration 14/1000 | Loss: 0.00002952
Iteration 15/1000 | Loss: 0.00002938
Iteration 16/1000 | Loss: 0.00002920
Iteration 17/1000 | Loss: 0.00002906
Iteration 18/1000 | Loss: 0.00002894
Iteration 19/1000 | Loss: 0.00002894
Iteration 20/1000 | Loss: 0.00002893
Iteration 21/1000 | Loss: 0.00002893
Iteration 22/1000 | Loss: 0.00002889
Iteration 23/1000 | Loss: 0.00002885
Iteration 24/1000 | Loss: 0.00002882
Iteration 25/1000 | Loss: 0.00002882
Iteration 26/1000 | Loss: 0.00002881
Iteration 27/1000 | Loss: 0.00002880
Iteration 28/1000 | Loss: 0.00002880
Iteration 29/1000 | Loss: 0.00002880
Iteration 30/1000 | Loss: 0.00002879
Iteration 31/1000 | Loss: 0.00002879
Iteration 32/1000 | Loss: 0.00002878
Iteration 33/1000 | Loss: 0.00002878
Iteration 34/1000 | Loss: 0.00002877
Iteration 35/1000 | Loss: 0.00002877
Iteration 36/1000 | Loss: 0.00002876
Iteration 37/1000 | Loss: 0.00002876
Iteration 38/1000 | Loss: 0.00002874
Iteration 39/1000 | Loss: 0.00002874
Iteration 40/1000 | Loss: 0.00002873
Iteration 41/1000 | Loss: 0.00002873
Iteration 42/1000 | Loss: 0.00002872
Iteration 43/1000 | Loss: 0.00002872
Iteration 44/1000 | Loss: 0.00002871
Iteration 45/1000 | Loss: 0.00002871
Iteration 46/1000 | Loss: 0.00002871
Iteration 47/1000 | Loss: 0.00002870
Iteration 48/1000 | Loss: 0.00002870
Iteration 49/1000 | Loss: 0.00002870
Iteration 50/1000 | Loss: 0.00002869
Iteration 51/1000 | Loss: 0.00002869
Iteration 52/1000 | Loss: 0.00002869
Iteration 53/1000 | Loss: 0.00002868
Iteration 54/1000 | Loss: 0.00002868
Iteration 55/1000 | Loss: 0.00002868
Iteration 56/1000 | Loss: 0.00002867
Iteration 57/1000 | Loss: 0.00002867
Iteration 58/1000 | Loss: 0.00002867
Iteration 59/1000 | Loss: 0.00002866
Iteration 60/1000 | Loss: 0.00002866
Iteration 61/1000 | Loss: 0.00002866
Iteration 62/1000 | Loss: 0.00002866
Iteration 63/1000 | Loss: 0.00002866
Iteration 64/1000 | Loss: 0.00002865
Iteration 65/1000 | Loss: 0.00002865
Iteration 66/1000 | Loss: 0.00002865
Iteration 67/1000 | Loss: 0.00002865
Iteration 68/1000 | Loss: 0.00002864
Iteration 69/1000 | Loss: 0.00002864
Iteration 70/1000 | Loss: 0.00002864
Iteration 71/1000 | Loss: 0.00002864
Iteration 72/1000 | Loss: 0.00002864
Iteration 73/1000 | Loss: 0.00002863
Iteration 74/1000 | Loss: 0.00002863
Iteration 75/1000 | Loss: 0.00002863
Iteration 76/1000 | Loss: 0.00002863
Iteration 77/1000 | Loss: 0.00002863
Iteration 78/1000 | Loss: 0.00002863
Iteration 79/1000 | Loss: 0.00002863
Iteration 80/1000 | Loss: 0.00002863
Iteration 81/1000 | Loss: 0.00002862
Iteration 82/1000 | Loss: 0.00002862
Iteration 83/1000 | Loss: 0.00002862
Iteration 84/1000 | Loss: 0.00002862
Iteration 85/1000 | Loss: 0.00002862
Iteration 86/1000 | Loss: 0.00002861
Iteration 87/1000 | Loss: 0.00002861
Iteration 88/1000 | Loss: 0.00002861
Iteration 89/1000 | Loss: 0.00002861
Iteration 90/1000 | Loss: 0.00002861
Iteration 91/1000 | Loss: 0.00002861
Iteration 92/1000 | Loss: 0.00002861
Iteration 93/1000 | Loss: 0.00002860
Iteration 94/1000 | Loss: 0.00002860
Iteration 95/1000 | Loss: 0.00002860
Iteration 96/1000 | Loss: 0.00002860
Iteration 97/1000 | Loss: 0.00002860
Iteration 98/1000 | Loss: 0.00002860
Iteration 99/1000 | Loss: 0.00002860
Iteration 100/1000 | Loss: 0.00002860
Iteration 101/1000 | Loss: 0.00002860
Iteration 102/1000 | Loss: 0.00002860
Iteration 103/1000 | Loss: 0.00002860
Iteration 104/1000 | Loss: 0.00002860
Iteration 105/1000 | Loss: 0.00002860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.860389940906316e-05, 2.860389940906316e-05, 2.860389940906316e-05, 2.860389940906316e-05, 2.860389940906316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.860389940906316e-05

Optimization complete. Final v2v error: 4.366405487060547 mm

Highest mean error: 7.275747299194336 mm for frame 116

Lowest mean error: 3.0800108909606934 mm for frame 5

Saving results

Total time: 44.30672264099121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926329
Iteration 2/25 | Loss: 0.00172355
Iteration 3/25 | Loss: 0.00137578
Iteration 4/25 | Loss: 0.00135185
Iteration 5/25 | Loss: 0.00134693
Iteration 6/25 | Loss: 0.00134557
Iteration 7/25 | Loss: 0.00134557
Iteration 8/25 | Loss: 0.00134557
Iteration 9/25 | Loss: 0.00134557
Iteration 10/25 | Loss: 0.00134557
Iteration 11/25 | Loss: 0.00134557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001345570432022214, 0.001345570432022214, 0.001345570432022214, 0.001345570432022214, 0.001345570432022214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001345570432022214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99353468
Iteration 2/25 | Loss: 0.00081869
Iteration 3/25 | Loss: 0.00081868
Iteration 4/25 | Loss: 0.00081868
Iteration 5/25 | Loss: 0.00081868
Iteration 6/25 | Loss: 0.00081868
Iteration 7/25 | Loss: 0.00081868
Iteration 8/25 | Loss: 0.00081868
Iteration 9/25 | Loss: 0.00081868
Iteration 10/25 | Loss: 0.00081868
Iteration 11/25 | Loss: 0.00081868
Iteration 12/25 | Loss: 0.00081868
Iteration 13/25 | Loss: 0.00081868
Iteration 14/25 | Loss: 0.00081868
Iteration 15/25 | Loss: 0.00081868
Iteration 16/25 | Loss: 0.00081868
Iteration 17/25 | Loss: 0.00081868
Iteration 18/25 | Loss: 0.00081868
Iteration 19/25 | Loss: 0.00081868
Iteration 20/25 | Loss: 0.00081868
Iteration 21/25 | Loss: 0.00081868
Iteration 22/25 | Loss: 0.00081868
Iteration 23/25 | Loss: 0.00081868
Iteration 24/25 | Loss: 0.00081868
Iteration 25/25 | Loss: 0.00081868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081868
Iteration 2/1000 | Loss: 0.00006260
Iteration 3/1000 | Loss: 0.00003617
Iteration 4/1000 | Loss: 0.00002881
Iteration 5/1000 | Loss: 0.00002699
Iteration 6/1000 | Loss: 0.00002571
Iteration 7/1000 | Loss: 0.00002481
Iteration 8/1000 | Loss: 0.00002402
Iteration 9/1000 | Loss: 0.00002356
Iteration 10/1000 | Loss: 0.00002324
Iteration 11/1000 | Loss: 0.00002295
Iteration 12/1000 | Loss: 0.00002275
Iteration 13/1000 | Loss: 0.00002256
Iteration 14/1000 | Loss: 0.00002238
Iteration 15/1000 | Loss: 0.00002236
Iteration 16/1000 | Loss: 0.00002227
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002213
Iteration 19/1000 | Loss: 0.00002208
Iteration 20/1000 | Loss: 0.00002208
Iteration 21/1000 | Loss: 0.00002199
Iteration 22/1000 | Loss: 0.00002196
Iteration 23/1000 | Loss: 0.00002196
Iteration 24/1000 | Loss: 0.00002195
Iteration 25/1000 | Loss: 0.00002195
Iteration 26/1000 | Loss: 0.00002191
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002186
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002183
Iteration 31/1000 | Loss: 0.00002182
Iteration 32/1000 | Loss: 0.00002181
Iteration 33/1000 | Loss: 0.00002179
Iteration 34/1000 | Loss: 0.00002178
Iteration 35/1000 | Loss: 0.00002177
Iteration 36/1000 | Loss: 0.00002175
Iteration 37/1000 | Loss: 0.00002175
Iteration 38/1000 | Loss: 0.00002175
Iteration 39/1000 | Loss: 0.00002174
Iteration 40/1000 | Loss: 0.00002174
Iteration 41/1000 | Loss: 0.00002173
Iteration 42/1000 | Loss: 0.00002173
Iteration 43/1000 | Loss: 0.00002173
Iteration 44/1000 | Loss: 0.00002171
Iteration 45/1000 | Loss: 0.00002171
Iteration 46/1000 | Loss: 0.00002171
Iteration 47/1000 | Loss: 0.00002170
Iteration 48/1000 | Loss: 0.00002170
Iteration 49/1000 | Loss: 0.00002170
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002169
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002169
Iteration 54/1000 | Loss: 0.00002169
Iteration 55/1000 | Loss: 0.00002169
Iteration 56/1000 | Loss: 0.00002169
Iteration 57/1000 | Loss: 0.00002169
Iteration 58/1000 | Loss: 0.00002168
Iteration 59/1000 | Loss: 0.00002168
Iteration 60/1000 | Loss: 0.00002168
Iteration 61/1000 | Loss: 0.00002168
Iteration 62/1000 | Loss: 0.00002168
Iteration 63/1000 | Loss: 0.00002168
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002166
Iteration 71/1000 | Loss: 0.00002166
Iteration 72/1000 | Loss: 0.00002166
Iteration 73/1000 | Loss: 0.00002166
Iteration 74/1000 | Loss: 0.00002166
Iteration 75/1000 | Loss: 0.00002166
Iteration 76/1000 | Loss: 0.00002166
Iteration 77/1000 | Loss: 0.00002166
Iteration 78/1000 | Loss: 0.00002166
Iteration 79/1000 | Loss: 0.00002166
Iteration 80/1000 | Loss: 0.00002166
Iteration 81/1000 | Loss: 0.00002165
Iteration 82/1000 | Loss: 0.00002165
Iteration 83/1000 | Loss: 0.00002165
Iteration 84/1000 | Loss: 0.00002165
Iteration 85/1000 | Loss: 0.00002165
Iteration 86/1000 | Loss: 0.00002164
Iteration 87/1000 | Loss: 0.00002164
Iteration 88/1000 | Loss: 0.00002164
Iteration 89/1000 | Loss: 0.00002164
Iteration 90/1000 | Loss: 0.00002164
Iteration 91/1000 | Loss: 0.00002164
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002163
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00002163
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00002163
Iteration 100/1000 | Loss: 0.00002163
Iteration 101/1000 | Loss: 0.00002163
Iteration 102/1000 | Loss: 0.00002163
Iteration 103/1000 | Loss: 0.00002163
Iteration 104/1000 | Loss: 0.00002162
Iteration 105/1000 | Loss: 0.00002162
Iteration 106/1000 | Loss: 0.00002162
Iteration 107/1000 | Loss: 0.00002162
Iteration 108/1000 | Loss: 0.00002162
Iteration 109/1000 | Loss: 0.00002162
Iteration 110/1000 | Loss: 0.00002162
Iteration 111/1000 | Loss: 0.00002162
Iteration 112/1000 | Loss: 0.00002162
Iteration 113/1000 | Loss: 0.00002161
Iteration 114/1000 | Loss: 0.00002161
Iteration 115/1000 | Loss: 0.00002161
Iteration 116/1000 | Loss: 0.00002161
Iteration 117/1000 | Loss: 0.00002161
Iteration 118/1000 | Loss: 0.00002161
Iteration 119/1000 | Loss: 0.00002161
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002161
Iteration 123/1000 | Loss: 0.00002160
Iteration 124/1000 | Loss: 0.00002160
Iteration 125/1000 | Loss: 0.00002160
Iteration 126/1000 | Loss: 0.00002160
Iteration 127/1000 | Loss: 0.00002160
Iteration 128/1000 | Loss: 0.00002160
Iteration 129/1000 | Loss: 0.00002160
Iteration 130/1000 | Loss: 0.00002160
Iteration 131/1000 | Loss: 0.00002160
Iteration 132/1000 | Loss: 0.00002159
Iteration 133/1000 | Loss: 0.00002159
Iteration 134/1000 | Loss: 0.00002159
Iteration 135/1000 | Loss: 0.00002159
Iteration 136/1000 | Loss: 0.00002159
Iteration 137/1000 | Loss: 0.00002159
Iteration 138/1000 | Loss: 0.00002159
Iteration 139/1000 | Loss: 0.00002159
Iteration 140/1000 | Loss: 0.00002159
Iteration 141/1000 | Loss: 0.00002159
Iteration 142/1000 | Loss: 0.00002159
Iteration 143/1000 | Loss: 0.00002159
Iteration 144/1000 | Loss: 0.00002159
Iteration 145/1000 | Loss: 0.00002158
Iteration 146/1000 | Loss: 0.00002158
Iteration 147/1000 | Loss: 0.00002158
Iteration 148/1000 | Loss: 0.00002158
Iteration 149/1000 | Loss: 0.00002158
Iteration 150/1000 | Loss: 0.00002158
Iteration 151/1000 | Loss: 0.00002158
Iteration 152/1000 | Loss: 0.00002158
Iteration 153/1000 | Loss: 0.00002158
Iteration 154/1000 | Loss: 0.00002158
Iteration 155/1000 | Loss: 0.00002158
Iteration 156/1000 | Loss: 0.00002158
Iteration 157/1000 | Loss: 0.00002158
Iteration 158/1000 | Loss: 0.00002158
Iteration 159/1000 | Loss: 0.00002158
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002157
Iteration 165/1000 | Loss: 0.00002157
Iteration 166/1000 | Loss: 0.00002157
Iteration 167/1000 | Loss: 0.00002157
Iteration 168/1000 | Loss: 0.00002157
Iteration 169/1000 | Loss: 0.00002157
Iteration 170/1000 | Loss: 0.00002156
Iteration 171/1000 | Loss: 0.00002156
Iteration 172/1000 | Loss: 0.00002156
Iteration 173/1000 | Loss: 0.00002156
Iteration 174/1000 | Loss: 0.00002156
Iteration 175/1000 | Loss: 0.00002156
Iteration 176/1000 | Loss: 0.00002156
Iteration 177/1000 | Loss: 0.00002156
Iteration 178/1000 | Loss: 0.00002156
Iteration 179/1000 | Loss: 0.00002156
Iteration 180/1000 | Loss: 0.00002156
Iteration 181/1000 | Loss: 0.00002156
Iteration 182/1000 | Loss: 0.00002156
Iteration 183/1000 | Loss: 0.00002156
Iteration 184/1000 | Loss: 0.00002156
Iteration 185/1000 | Loss: 0.00002156
Iteration 186/1000 | Loss: 0.00002156
Iteration 187/1000 | Loss: 0.00002156
Iteration 188/1000 | Loss: 0.00002156
Iteration 189/1000 | Loss: 0.00002156
Iteration 190/1000 | Loss: 0.00002156
Iteration 191/1000 | Loss: 0.00002156
Iteration 192/1000 | Loss: 0.00002156
Iteration 193/1000 | Loss: 0.00002156
Iteration 194/1000 | Loss: 0.00002156
Iteration 195/1000 | Loss: 0.00002156
Iteration 196/1000 | Loss: 0.00002156
Iteration 197/1000 | Loss: 0.00002156
Iteration 198/1000 | Loss: 0.00002156
Iteration 199/1000 | Loss: 0.00002156
Iteration 200/1000 | Loss: 0.00002156
Iteration 201/1000 | Loss: 0.00002156
Iteration 202/1000 | Loss: 0.00002156
Iteration 203/1000 | Loss: 0.00002156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.15602067328291e-05, 2.15602067328291e-05, 2.15602067328291e-05, 2.15602067328291e-05, 2.15602067328291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.15602067328291e-05

Optimization complete. Final v2v error: 3.8897931575775146 mm

Highest mean error: 4.429362773895264 mm for frame 137

Lowest mean error: 3.2023210525512695 mm for frame 27

Saving results

Total time: 47.04743838310242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868590
Iteration 2/25 | Loss: 0.00172272
Iteration 3/25 | Loss: 0.00137647
Iteration 4/25 | Loss: 0.00131887
Iteration 5/25 | Loss: 0.00133774
Iteration 6/25 | Loss: 0.00129615
Iteration 7/25 | Loss: 0.00127232
Iteration 8/25 | Loss: 0.00125457
Iteration 9/25 | Loss: 0.00124618
Iteration 10/25 | Loss: 0.00124296
Iteration 11/25 | Loss: 0.00124224
Iteration 12/25 | Loss: 0.00124205
Iteration 13/25 | Loss: 0.00124205
Iteration 14/25 | Loss: 0.00124205
Iteration 15/25 | Loss: 0.00124205
Iteration 16/25 | Loss: 0.00124205
Iteration 17/25 | Loss: 0.00124205
Iteration 18/25 | Loss: 0.00124205
Iteration 19/25 | Loss: 0.00124205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001242047525011003, 0.001242047525011003, 0.001242047525011003, 0.001242047525011003, 0.001242047525011003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242047525011003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30079627
Iteration 2/25 | Loss: 0.00078858
Iteration 3/25 | Loss: 0.00078857
Iteration 4/25 | Loss: 0.00078857
Iteration 5/25 | Loss: 0.00078857
Iteration 6/25 | Loss: 0.00078857
Iteration 7/25 | Loss: 0.00078857
Iteration 8/25 | Loss: 0.00078857
Iteration 9/25 | Loss: 0.00078857
Iteration 10/25 | Loss: 0.00078857
Iteration 11/25 | Loss: 0.00078857
Iteration 12/25 | Loss: 0.00078857
Iteration 13/25 | Loss: 0.00078857
Iteration 14/25 | Loss: 0.00078857
Iteration 15/25 | Loss: 0.00078857
Iteration 16/25 | Loss: 0.00078857
Iteration 17/25 | Loss: 0.00078857
Iteration 18/25 | Loss: 0.00078857
Iteration 19/25 | Loss: 0.00078857
Iteration 20/25 | Loss: 0.00078857
Iteration 21/25 | Loss: 0.00078857
Iteration 22/25 | Loss: 0.00078857
Iteration 23/25 | Loss: 0.00078857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007885692175477743, 0.0007885692175477743, 0.0007885692175477743, 0.0007885692175477743, 0.0007885692175477743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007885692175477743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078857
Iteration 2/1000 | Loss: 0.00003366
Iteration 3/1000 | Loss: 0.00002587
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002230
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002149
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002086
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002064
Iteration 14/1000 | Loss: 0.00002063
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002045
Iteration 17/1000 | Loss: 0.00002034
Iteration 18/1000 | Loss: 0.00002030
Iteration 19/1000 | Loss: 0.00002028
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002024
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002022
Iteration 25/1000 | Loss: 0.00002020
Iteration 26/1000 | Loss: 0.00002019
Iteration 27/1000 | Loss: 0.00002019
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002017
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00002017
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002015
Iteration 40/1000 | Loss: 0.00002015
Iteration 41/1000 | Loss: 0.00002014
Iteration 42/1000 | Loss: 0.00002014
Iteration 43/1000 | Loss: 0.00002013
Iteration 44/1000 | Loss: 0.00002013
Iteration 45/1000 | Loss: 0.00002011
Iteration 46/1000 | Loss: 0.00002011
Iteration 47/1000 | Loss: 0.00002011
Iteration 48/1000 | Loss: 0.00002010
Iteration 49/1000 | Loss: 0.00002010
Iteration 50/1000 | Loss: 0.00002010
Iteration 51/1000 | Loss: 0.00002009
Iteration 52/1000 | Loss: 0.00002006
Iteration 53/1000 | Loss: 0.00002005
Iteration 54/1000 | Loss: 0.00002005
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00002000
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001998
Iteration 69/1000 | Loss: 0.00001998
Iteration 70/1000 | Loss: 0.00001998
Iteration 71/1000 | Loss: 0.00001998
Iteration 72/1000 | Loss: 0.00001998
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001998
Iteration 75/1000 | Loss: 0.00001998
Iteration 76/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.9979946955572814e-05, 1.9979946955572814e-05, 1.9979946955572814e-05, 1.9979946955572814e-05, 1.9979946955572814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9979946955572814e-05

Optimization complete. Final v2v error: 3.727508068084717 mm

Highest mean error: 4.231061935424805 mm for frame 94

Lowest mean error: 3.1580238342285156 mm for frame 25

Saving results

Total time: 51.84098267555237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806143
Iteration 2/25 | Loss: 0.00145053
Iteration 3/25 | Loss: 0.00131433
Iteration 4/25 | Loss: 0.00129758
Iteration 5/25 | Loss: 0.00129345
Iteration 6/25 | Loss: 0.00129230
Iteration 7/25 | Loss: 0.00129228
Iteration 8/25 | Loss: 0.00129228
Iteration 9/25 | Loss: 0.00129228
Iteration 10/25 | Loss: 0.00129228
Iteration 11/25 | Loss: 0.00129228
Iteration 12/25 | Loss: 0.00129228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012922835303470492, 0.0012922835303470492, 0.0012922835303470492, 0.0012922835303470492, 0.0012922835303470492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012922835303470492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43981326
Iteration 2/25 | Loss: 0.00183343
Iteration 3/25 | Loss: 0.00183342
Iteration 4/25 | Loss: 0.00183342
Iteration 5/25 | Loss: 0.00183342
Iteration 6/25 | Loss: 0.00183342
Iteration 7/25 | Loss: 0.00183341
Iteration 8/25 | Loss: 0.00183341
Iteration 9/25 | Loss: 0.00183341
Iteration 10/25 | Loss: 0.00183341
Iteration 11/25 | Loss: 0.00183341
Iteration 12/25 | Loss: 0.00183341
Iteration 13/25 | Loss: 0.00183341
Iteration 14/25 | Loss: 0.00183341
Iteration 15/25 | Loss: 0.00183341
Iteration 16/25 | Loss: 0.00183341
Iteration 17/25 | Loss: 0.00183341
Iteration 18/25 | Loss: 0.00183341
Iteration 19/25 | Loss: 0.00183341
Iteration 20/25 | Loss: 0.00183341
Iteration 21/25 | Loss: 0.00183341
Iteration 22/25 | Loss: 0.00183341
Iteration 23/25 | Loss: 0.00183341
Iteration 24/25 | Loss: 0.00183341
Iteration 25/25 | Loss: 0.00183341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183341
Iteration 2/1000 | Loss: 0.00015131
Iteration 3/1000 | Loss: 0.00009561
Iteration 4/1000 | Loss: 0.00008020
Iteration 5/1000 | Loss: 0.00006993
Iteration 6/1000 | Loss: 0.00006599
Iteration 7/1000 | Loss: 0.00006269
Iteration 8/1000 | Loss: 0.00006103
Iteration 9/1000 | Loss: 0.00005903
Iteration 10/1000 | Loss: 0.00005758
Iteration 11/1000 | Loss: 0.00005662
Iteration 12/1000 | Loss: 0.00005614
Iteration 13/1000 | Loss: 0.00005579
Iteration 14/1000 | Loss: 0.00005552
Iteration 15/1000 | Loss: 0.00005531
Iteration 16/1000 | Loss: 0.00005529
Iteration 17/1000 | Loss: 0.00005513
Iteration 18/1000 | Loss: 0.00005494
Iteration 19/1000 | Loss: 0.00005490
Iteration 20/1000 | Loss: 0.00005488
Iteration 21/1000 | Loss: 0.00005472
Iteration 22/1000 | Loss: 0.00005451
Iteration 23/1000 | Loss: 0.00005420
Iteration 24/1000 | Loss: 0.00005387
Iteration 25/1000 | Loss: 0.00023864
Iteration 26/1000 | Loss: 0.00018341
Iteration 27/1000 | Loss: 0.00023472
Iteration 28/1000 | Loss: 0.00054749
Iteration 29/1000 | Loss: 0.00132555
Iteration 30/1000 | Loss: 0.00067340
Iteration 31/1000 | Loss: 0.00016584
Iteration 32/1000 | Loss: 0.00027117
Iteration 33/1000 | Loss: 0.00032475
Iteration 34/1000 | Loss: 0.00015762
Iteration 35/1000 | Loss: 0.00019722
Iteration 36/1000 | Loss: 0.00021747
Iteration 37/1000 | Loss: 0.00017326
Iteration 38/1000 | Loss: 0.00019330
Iteration 39/1000 | Loss: 0.00005389
Iteration 40/1000 | Loss: 0.00005168
Iteration 41/1000 | Loss: 0.00005056
Iteration 42/1000 | Loss: 0.00004968
Iteration 43/1000 | Loss: 0.00004928
Iteration 44/1000 | Loss: 0.00004891
Iteration 45/1000 | Loss: 0.00004848
Iteration 46/1000 | Loss: 0.00004818
Iteration 47/1000 | Loss: 0.00004796
Iteration 48/1000 | Loss: 0.00004767
Iteration 49/1000 | Loss: 0.00004736
Iteration 50/1000 | Loss: 0.00004710
Iteration 51/1000 | Loss: 0.00004684
Iteration 52/1000 | Loss: 0.00004662
Iteration 53/1000 | Loss: 0.00004639
Iteration 54/1000 | Loss: 0.00004631
Iteration 55/1000 | Loss: 0.00004617
Iteration 56/1000 | Loss: 0.00004615
Iteration 57/1000 | Loss: 0.00004597
Iteration 58/1000 | Loss: 0.00004594
Iteration 59/1000 | Loss: 0.00004585
Iteration 60/1000 | Loss: 0.00004578
Iteration 61/1000 | Loss: 0.00004578
Iteration 62/1000 | Loss: 0.00004577
Iteration 63/1000 | Loss: 0.00004577
Iteration 64/1000 | Loss: 0.00004576
Iteration 65/1000 | Loss: 0.00004576
Iteration 66/1000 | Loss: 0.00004575
Iteration 67/1000 | Loss: 0.00050829
Iteration 68/1000 | Loss: 0.00052627
Iteration 69/1000 | Loss: 0.00035955
Iteration 70/1000 | Loss: 0.00006409
Iteration 71/1000 | Loss: 0.00004693
Iteration 72/1000 | Loss: 0.00004434
Iteration 73/1000 | Loss: 0.00004322
Iteration 74/1000 | Loss: 0.00004275
Iteration 75/1000 | Loss: 0.00004221
Iteration 76/1000 | Loss: 0.00004179
Iteration 77/1000 | Loss: 0.00004146
Iteration 78/1000 | Loss: 0.00004131
Iteration 79/1000 | Loss: 0.00004130
Iteration 80/1000 | Loss: 0.00004122
Iteration 81/1000 | Loss: 0.00004120
Iteration 82/1000 | Loss: 0.00004119
Iteration 83/1000 | Loss: 0.00004119
Iteration 84/1000 | Loss: 0.00004118
Iteration 85/1000 | Loss: 0.00004117
Iteration 86/1000 | Loss: 0.00004117
Iteration 87/1000 | Loss: 0.00004115
Iteration 88/1000 | Loss: 0.00004102
Iteration 89/1000 | Loss: 0.00004100
Iteration 90/1000 | Loss: 0.00004097
Iteration 91/1000 | Loss: 0.00004097
Iteration 92/1000 | Loss: 0.00004096
Iteration 93/1000 | Loss: 0.00004096
Iteration 94/1000 | Loss: 0.00004094
Iteration 95/1000 | Loss: 0.00004090
Iteration 96/1000 | Loss: 0.00004086
Iteration 97/1000 | Loss: 0.00004086
Iteration 98/1000 | Loss: 0.00004086
Iteration 99/1000 | Loss: 0.00004086
Iteration 100/1000 | Loss: 0.00004086
Iteration 101/1000 | Loss: 0.00004086
Iteration 102/1000 | Loss: 0.00004086
Iteration 103/1000 | Loss: 0.00004086
Iteration 104/1000 | Loss: 0.00004085
Iteration 105/1000 | Loss: 0.00004085
Iteration 106/1000 | Loss: 0.00004085
Iteration 107/1000 | Loss: 0.00004085
Iteration 108/1000 | Loss: 0.00004085
Iteration 109/1000 | Loss: 0.00004085
Iteration 110/1000 | Loss: 0.00004084
Iteration 111/1000 | Loss: 0.00004083
Iteration 112/1000 | Loss: 0.00004082
Iteration 113/1000 | Loss: 0.00004082
Iteration 114/1000 | Loss: 0.00004081
Iteration 115/1000 | Loss: 0.00004081
Iteration 116/1000 | Loss: 0.00004080
Iteration 117/1000 | Loss: 0.00004080
Iteration 118/1000 | Loss: 0.00004079
Iteration 119/1000 | Loss: 0.00004078
Iteration 120/1000 | Loss: 0.00004076
Iteration 121/1000 | Loss: 0.00004075
Iteration 122/1000 | Loss: 0.00004074
Iteration 123/1000 | Loss: 0.00004074
Iteration 124/1000 | Loss: 0.00004074
Iteration 125/1000 | Loss: 0.00004073
Iteration 126/1000 | Loss: 0.00004073
Iteration 127/1000 | Loss: 0.00004072
Iteration 128/1000 | Loss: 0.00004072
Iteration 129/1000 | Loss: 0.00004072
Iteration 130/1000 | Loss: 0.00004072
Iteration 131/1000 | Loss: 0.00004072
Iteration 132/1000 | Loss: 0.00004071
Iteration 133/1000 | Loss: 0.00004071
Iteration 134/1000 | Loss: 0.00004071
Iteration 135/1000 | Loss: 0.00004071
Iteration 136/1000 | Loss: 0.00004070
Iteration 137/1000 | Loss: 0.00004070
Iteration 138/1000 | Loss: 0.00004070
Iteration 139/1000 | Loss: 0.00004070
Iteration 140/1000 | Loss: 0.00004070
Iteration 141/1000 | Loss: 0.00004069
Iteration 142/1000 | Loss: 0.00004069
Iteration 143/1000 | Loss: 0.00004069
Iteration 144/1000 | Loss: 0.00004066
Iteration 145/1000 | Loss: 0.00004066
Iteration 146/1000 | Loss: 0.00004066
Iteration 147/1000 | Loss: 0.00004066
Iteration 148/1000 | Loss: 0.00004065
Iteration 149/1000 | Loss: 0.00004065
Iteration 150/1000 | Loss: 0.00004065
Iteration 151/1000 | Loss: 0.00004065
Iteration 152/1000 | Loss: 0.00004065
Iteration 153/1000 | Loss: 0.00004065
Iteration 154/1000 | Loss: 0.00004065
Iteration 155/1000 | Loss: 0.00004065
Iteration 156/1000 | Loss: 0.00004065
Iteration 157/1000 | Loss: 0.00004064
Iteration 158/1000 | Loss: 0.00004064
Iteration 159/1000 | Loss: 0.00004064
Iteration 160/1000 | Loss: 0.00004064
Iteration 161/1000 | Loss: 0.00004064
Iteration 162/1000 | Loss: 0.00004064
Iteration 163/1000 | Loss: 0.00004063
Iteration 164/1000 | Loss: 0.00004063
Iteration 165/1000 | Loss: 0.00004063
Iteration 166/1000 | Loss: 0.00004062
Iteration 167/1000 | Loss: 0.00004062
Iteration 168/1000 | Loss: 0.00004062
Iteration 169/1000 | Loss: 0.00004061
Iteration 170/1000 | Loss: 0.00004061
Iteration 171/1000 | Loss: 0.00004061
Iteration 172/1000 | Loss: 0.00004061
Iteration 173/1000 | Loss: 0.00004060
Iteration 174/1000 | Loss: 0.00004060
Iteration 175/1000 | Loss: 0.00004060
Iteration 176/1000 | Loss: 0.00004060
Iteration 177/1000 | Loss: 0.00004060
Iteration 178/1000 | Loss: 0.00004060
Iteration 179/1000 | Loss: 0.00004060
Iteration 180/1000 | Loss: 0.00004059
Iteration 181/1000 | Loss: 0.00004059
Iteration 182/1000 | Loss: 0.00004059
Iteration 183/1000 | Loss: 0.00004059
Iteration 184/1000 | Loss: 0.00004059
Iteration 185/1000 | Loss: 0.00004058
Iteration 186/1000 | Loss: 0.00004058
Iteration 187/1000 | Loss: 0.00004058
Iteration 188/1000 | Loss: 0.00004058
Iteration 189/1000 | Loss: 0.00004058
Iteration 190/1000 | Loss: 0.00004058
Iteration 191/1000 | Loss: 0.00004058
Iteration 192/1000 | Loss: 0.00004058
Iteration 193/1000 | Loss: 0.00004058
Iteration 194/1000 | Loss: 0.00004058
Iteration 195/1000 | Loss: 0.00004058
Iteration 196/1000 | Loss: 0.00004058
Iteration 197/1000 | Loss: 0.00004058
Iteration 198/1000 | Loss: 0.00004057
Iteration 199/1000 | Loss: 0.00004057
Iteration 200/1000 | Loss: 0.00004057
Iteration 201/1000 | Loss: 0.00004057
Iteration 202/1000 | Loss: 0.00004057
Iteration 203/1000 | Loss: 0.00004057
Iteration 204/1000 | Loss: 0.00004056
Iteration 205/1000 | Loss: 0.00004056
Iteration 206/1000 | Loss: 0.00004056
Iteration 207/1000 | Loss: 0.00004056
Iteration 208/1000 | Loss: 0.00004056
Iteration 209/1000 | Loss: 0.00004056
Iteration 210/1000 | Loss: 0.00004056
Iteration 211/1000 | Loss: 0.00004056
Iteration 212/1000 | Loss: 0.00004056
Iteration 213/1000 | Loss: 0.00004056
Iteration 214/1000 | Loss: 0.00004055
Iteration 215/1000 | Loss: 0.00004055
Iteration 216/1000 | Loss: 0.00004055
Iteration 217/1000 | Loss: 0.00004055
Iteration 218/1000 | Loss: 0.00004054
Iteration 219/1000 | Loss: 0.00004054
Iteration 220/1000 | Loss: 0.00004054
Iteration 221/1000 | Loss: 0.00004053
Iteration 222/1000 | Loss: 0.00004053
Iteration 223/1000 | Loss: 0.00004053
Iteration 224/1000 | Loss: 0.00004053
Iteration 225/1000 | Loss: 0.00004053
Iteration 226/1000 | Loss: 0.00004052
Iteration 227/1000 | Loss: 0.00004052
Iteration 228/1000 | Loss: 0.00004052
Iteration 229/1000 | Loss: 0.00004052
Iteration 230/1000 | Loss: 0.00004052
Iteration 231/1000 | Loss: 0.00004052
Iteration 232/1000 | Loss: 0.00004052
Iteration 233/1000 | Loss: 0.00004052
Iteration 234/1000 | Loss: 0.00004051
Iteration 235/1000 | Loss: 0.00004051
Iteration 236/1000 | Loss: 0.00004051
Iteration 237/1000 | Loss: 0.00004051
Iteration 238/1000 | Loss: 0.00004051
Iteration 239/1000 | Loss: 0.00004051
Iteration 240/1000 | Loss: 0.00004051
Iteration 241/1000 | Loss: 0.00004051
Iteration 242/1000 | Loss: 0.00004051
Iteration 243/1000 | Loss: 0.00004051
Iteration 244/1000 | Loss: 0.00004051
Iteration 245/1000 | Loss: 0.00004050
Iteration 246/1000 | Loss: 0.00004050
Iteration 247/1000 | Loss: 0.00004050
Iteration 248/1000 | Loss: 0.00004050
Iteration 249/1000 | Loss: 0.00004050
Iteration 250/1000 | Loss: 0.00004050
Iteration 251/1000 | Loss: 0.00004050
Iteration 252/1000 | Loss: 0.00004050
Iteration 253/1000 | Loss: 0.00004050
Iteration 254/1000 | Loss: 0.00004050
Iteration 255/1000 | Loss: 0.00004049
Iteration 256/1000 | Loss: 0.00004049
Iteration 257/1000 | Loss: 0.00004049
Iteration 258/1000 | Loss: 0.00004049
Iteration 259/1000 | Loss: 0.00004049
Iteration 260/1000 | Loss: 0.00004049
Iteration 261/1000 | Loss: 0.00004049
Iteration 262/1000 | Loss: 0.00004049
Iteration 263/1000 | Loss: 0.00004049
Iteration 264/1000 | Loss: 0.00004049
Iteration 265/1000 | Loss: 0.00004049
Iteration 266/1000 | Loss: 0.00004049
Iteration 267/1000 | Loss: 0.00004049
Iteration 268/1000 | Loss: 0.00004049
Iteration 269/1000 | Loss: 0.00004049
Iteration 270/1000 | Loss: 0.00004049
Iteration 271/1000 | Loss: 0.00004049
Iteration 272/1000 | Loss: 0.00004049
Iteration 273/1000 | Loss: 0.00004049
Iteration 274/1000 | Loss: 0.00004048
Iteration 275/1000 | Loss: 0.00004048
Iteration 276/1000 | Loss: 0.00004048
Iteration 277/1000 | Loss: 0.00004048
Iteration 278/1000 | Loss: 0.00004048
Iteration 279/1000 | Loss: 0.00004048
Iteration 280/1000 | Loss: 0.00004048
Iteration 281/1000 | Loss: 0.00004048
Iteration 282/1000 | Loss: 0.00004048
Iteration 283/1000 | Loss: 0.00004048
Iteration 284/1000 | Loss: 0.00004048
Iteration 285/1000 | Loss: 0.00004048
Iteration 286/1000 | Loss: 0.00004048
Iteration 287/1000 | Loss: 0.00004048
Iteration 288/1000 | Loss: 0.00004048
Iteration 289/1000 | Loss: 0.00004048
Iteration 290/1000 | Loss: 0.00004047
Iteration 291/1000 | Loss: 0.00004047
Iteration 292/1000 | Loss: 0.00004047
Iteration 293/1000 | Loss: 0.00004047
Iteration 294/1000 | Loss: 0.00004047
Iteration 295/1000 | Loss: 0.00004047
Iteration 296/1000 | Loss: 0.00004047
Iteration 297/1000 | Loss: 0.00004047
Iteration 298/1000 | Loss: 0.00004047
Iteration 299/1000 | Loss: 0.00004047
Iteration 300/1000 | Loss: 0.00004046
Iteration 301/1000 | Loss: 0.00004046
Iteration 302/1000 | Loss: 0.00004046
Iteration 303/1000 | Loss: 0.00004046
Iteration 304/1000 | Loss: 0.00004046
Iteration 305/1000 | Loss: 0.00004046
Iteration 306/1000 | Loss: 0.00004046
Iteration 307/1000 | Loss: 0.00004046
Iteration 308/1000 | Loss: 0.00004046
Iteration 309/1000 | Loss: 0.00004045
Iteration 310/1000 | Loss: 0.00004045
Iteration 311/1000 | Loss: 0.00004045
Iteration 312/1000 | Loss: 0.00004045
Iteration 313/1000 | Loss: 0.00004045
Iteration 314/1000 | Loss: 0.00004045
Iteration 315/1000 | Loss: 0.00004045
Iteration 316/1000 | Loss: 0.00004045
Iteration 317/1000 | Loss: 0.00004045
Iteration 318/1000 | Loss: 0.00004045
Iteration 319/1000 | Loss: 0.00004044
Iteration 320/1000 | Loss: 0.00004044
Iteration 321/1000 | Loss: 0.00004044
Iteration 322/1000 | Loss: 0.00004044
Iteration 323/1000 | Loss: 0.00004044
Iteration 324/1000 | Loss: 0.00004044
Iteration 325/1000 | Loss: 0.00004044
Iteration 326/1000 | Loss: 0.00004044
Iteration 327/1000 | Loss: 0.00004044
Iteration 328/1000 | Loss: 0.00004044
Iteration 329/1000 | Loss: 0.00004044
Iteration 330/1000 | Loss: 0.00004044
Iteration 331/1000 | Loss: 0.00004044
Iteration 332/1000 | Loss: 0.00004044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [4.04415259254165e-05, 4.04415259254165e-05, 4.04415259254165e-05, 4.04415259254165e-05, 4.04415259254165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.04415259254165e-05

Optimization complete. Final v2v error: 3.5113446712493896 mm

Highest mean error: 11.336182594299316 mm for frame 104

Lowest mean error: 2.5590882301330566 mm for frame 57

Saving results

Total time: 133.80673027038574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00669038
Iteration 2/25 | Loss: 0.00153748
Iteration 3/25 | Loss: 0.00135182
Iteration 4/25 | Loss: 0.00133740
Iteration 5/25 | Loss: 0.00133552
Iteration 6/25 | Loss: 0.00133552
Iteration 7/25 | Loss: 0.00133552
Iteration 8/25 | Loss: 0.00133552
Iteration 9/25 | Loss: 0.00133552
Iteration 10/25 | Loss: 0.00133552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013355198316276073, 0.0013355198316276073, 0.0013355198316276073, 0.0013355198316276073, 0.0013355198316276073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013355198316276073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75833440
Iteration 2/25 | Loss: 0.00086878
Iteration 3/25 | Loss: 0.00086875
Iteration 4/25 | Loss: 0.00086875
Iteration 5/25 | Loss: 0.00086875
Iteration 6/25 | Loss: 0.00086875
Iteration 7/25 | Loss: 0.00086875
Iteration 8/25 | Loss: 0.00086875
Iteration 9/25 | Loss: 0.00086875
Iteration 10/25 | Loss: 0.00086875
Iteration 11/25 | Loss: 0.00086875
Iteration 12/25 | Loss: 0.00086875
Iteration 13/25 | Loss: 0.00086875
Iteration 14/25 | Loss: 0.00086875
Iteration 15/25 | Loss: 0.00086875
Iteration 16/25 | Loss: 0.00086875
Iteration 17/25 | Loss: 0.00086875
Iteration 18/25 | Loss: 0.00086875
Iteration 19/25 | Loss: 0.00086875
Iteration 20/25 | Loss: 0.00086875
Iteration 21/25 | Loss: 0.00086875
Iteration 22/25 | Loss: 0.00086875
Iteration 23/25 | Loss: 0.00086875
Iteration 24/25 | Loss: 0.00086875
Iteration 25/25 | Loss: 0.00086875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086875
Iteration 2/1000 | Loss: 0.00004083
Iteration 3/1000 | Loss: 0.00002834
Iteration 4/1000 | Loss: 0.00002431
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002215
Iteration 7/1000 | Loss: 0.00002157
Iteration 8/1000 | Loss: 0.00002123
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00002036
Iteration 12/1000 | Loss: 0.00002033
Iteration 13/1000 | Loss: 0.00002016
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001998
Iteration 19/1000 | Loss: 0.00001997
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001990
Iteration 24/1000 | Loss: 0.00001989
Iteration 25/1000 | Loss: 0.00001988
Iteration 26/1000 | Loss: 0.00001983
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001980
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001975
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001974
Iteration 39/1000 | Loss: 0.00001974
Iteration 40/1000 | Loss: 0.00001973
Iteration 41/1000 | Loss: 0.00001973
Iteration 42/1000 | Loss: 0.00001972
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001971
Iteration 45/1000 | Loss: 0.00001971
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001964
Iteration 54/1000 | Loss: 0.00001964
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001962
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001959
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001958
Iteration 68/1000 | Loss: 0.00001958
Iteration 69/1000 | Loss: 0.00001958
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00001958
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001957
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001956
Iteration 79/1000 | Loss: 0.00001956
Iteration 80/1000 | Loss: 0.00001955
Iteration 81/1000 | Loss: 0.00001955
Iteration 82/1000 | Loss: 0.00001955
Iteration 83/1000 | Loss: 0.00001954
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001954
Iteration 87/1000 | Loss: 0.00001953
Iteration 88/1000 | Loss: 0.00001953
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001952
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001950
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001949
Iteration 104/1000 | Loss: 0.00001949
Iteration 105/1000 | Loss: 0.00001949
Iteration 106/1000 | Loss: 0.00001949
Iteration 107/1000 | Loss: 0.00001949
Iteration 108/1000 | Loss: 0.00001949
Iteration 109/1000 | Loss: 0.00001949
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001948
Iteration 113/1000 | Loss: 0.00001948
Iteration 114/1000 | Loss: 0.00001947
Iteration 115/1000 | Loss: 0.00001947
Iteration 116/1000 | Loss: 0.00001947
Iteration 117/1000 | Loss: 0.00001947
Iteration 118/1000 | Loss: 0.00001947
Iteration 119/1000 | Loss: 0.00001947
Iteration 120/1000 | Loss: 0.00001947
Iteration 121/1000 | Loss: 0.00001946
Iteration 122/1000 | Loss: 0.00001946
Iteration 123/1000 | Loss: 0.00001946
Iteration 124/1000 | Loss: 0.00001945
Iteration 125/1000 | Loss: 0.00001945
Iteration 126/1000 | Loss: 0.00001945
Iteration 127/1000 | Loss: 0.00001945
Iteration 128/1000 | Loss: 0.00001945
Iteration 129/1000 | Loss: 0.00001945
Iteration 130/1000 | Loss: 0.00001944
Iteration 131/1000 | Loss: 0.00001944
Iteration 132/1000 | Loss: 0.00001944
Iteration 133/1000 | Loss: 0.00001943
Iteration 134/1000 | Loss: 0.00001943
Iteration 135/1000 | Loss: 0.00001943
Iteration 136/1000 | Loss: 0.00001943
Iteration 137/1000 | Loss: 0.00001943
Iteration 138/1000 | Loss: 0.00001943
Iteration 139/1000 | Loss: 0.00001942
Iteration 140/1000 | Loss: 0.00001942
Iteration 141/1000 | Loss: 0.00001942
Iteration 142/1000 | Loss: 0.00001942
Iteration 143/1000 | Loss: 0.00001942
Iteration 144/1000 | Loss: 0.00001942
Iteration 145/1000 | Loss: 0.00001942
Iteration 146/1000 | Loss: 0.00001942
Iteration 147/1000 | Loss: 0.00001942
Iteration 148/1000 | Loss: 0.00001942
Iteration 149/1000 | Loss: 0.00001941
Iteration 150/1000 | Loss: 0.00001941
Iteration 151/1000 | Loss: 0.00001941
Iteration 152/1000 | Loss: 0.00001941
Iteration 153/1000 | Loss: 0.00001941
Iteration 154/1000 | Loss: 0.00001940
Iteration 155/1000 | Loss: 0.00001940
Iteration 156/1000 | Loss: 0.00001940
Iteration 157/1000 | Loss: 0.00001940
Iteration 158/1000 | Loss: 0.00001940
Iteration 159/1000 | Loss: 0.00001940
Iteration 160/1000 | Loss: 0.00001940
Iteration 161/1000 | Loss: 0.00001940
Iteration 162/1000 | Loss: 0.00001940
Iteration 163/1000 | Loss: 0.00001940
Iteration 164/1000 | Loss: 0.00001940
Iteration 165/1000 | Loss: 0.00001940
Iteration 166/1000 | Loss: 0.00001939
Iteration 167/1000 | Loss: 0.00001939
Iteration 168/1000 | Loss: 0.00001939
Iteration 169/1000 | Loss: 0.00001939
Iteration 170/1000 | Loss: 0.00001939
Iteration 171/1000 | Loss: 0.00001939
Iteration 172/1000 | Loss: 0.00001938
Iteration 173/1000 | Loss: 0.00001938
Iteration 174/1000 | Loss: 0.00001938
Iteration 175/1000 | Loss: 0.00001938
Iteration 176/1000 | Loss: 0.00001938
Iteration 177/1000 | Loss: 0.00001938
Iteration 178/1000 | Loss: 0.00001938
Iteration 179/1000 | Loss: 0.00001938
Iteration 180/1000 | Loss: 0.00001938
Iteration 181/1000 | Loss: 0.00001938
Iteration 182/1000 | Loss: 0.00001938
Iteration 183/1000 | Loss: 0.00001938
Iteration 184/1000 | Loss: 0.00001938
Iteration 185/1000 | Loss: 0.00001937
Iteration 186/1000 | Loss: 0.00001937
Iteration 187/1000 | Loss: 0.00001937
Iteration 188/1000 | Loss: 0.00001937
Iteration 189/1000 | Loss: 0.00001937
Iteration 190/1000 | Loss: 0.00001937
Iteration 191/1000 | Loss: 0.00001937
Iteration 192/1000 | Loss: 0.00001937
Iteration 193/1000 | Loss: 0.00001937
Iteration 194/1000 | Loss: 0.00001937
Iteration 195/1000 | Loss: 0.00001937
Iteration 196/1000 | Loss: 0.00001937
Iteration 197/1000 | Loss: 0.00001936
Iteration 198/1000 | Loss: 0.00001936
Iteration 199/1000 | Loss: 0.00001936
Iteration 200/1000 | Loss: 0.00001936
Iteration 201/1000 | Loss: 0.00001936
Iteration 202/1000 | Loss: 0.00001936
Iteration 203/1000 | Loss: 0.00001936
Iteration 204/1000 | Loss: 0.00001936
Iteration 205/1000 | Loss: 0.00001936
Iteration 206/1000 | Loss: 0.00001936
Iteration 207/1000 | Loss: 0.00001936
Iteration 208/1000 | Loss: 0.00001936
Iteration 209/1000 | Loss: 0.00001936
Iteration 210/1000 | Loss: 0.00001936
Iteration 211/1000 | Loss: 0.00001936
Iteration 212/1000 | Loss: 0.00001936
Iteration 213/1000 | Loss: 0.00001936
Iteration 214/1000 | Loss: 0.00001936
Iteration 215/1000 | Loss: 0.00001936
Iteration 216/1000 | Loss: 0.00001936
Iteration 217/1000 | Loss: 0.00001936
Iteration 218/1000 | Loss: 0.00001936
Iteration 219/1000 | Loss: 0.00001936
Iteration 220/1000 | Loss: 0.00001936
Iteration 221/1000 | Loss: 0.00001936
Iteration 222/1000 | Loss: 0.00001936
Iteration 223/1000 | Loss: 0.00001936
Iteration 224/1000 | Loss: 0.00001936
Iteration 225/1000 | Loss: 0.00001936
Iteration 226/1000 | Loss: 0.00001936
Iteration 227/1000 | Loss: 0.00001936
Iteration 228/1000 | Loss: 0.00001936
Iteration 229/1000 | Loss: 0.00001936
Iteration 230/1000 | Loss: 0.00001936
Iteration 231/1000 | Loss: 0.00001936
Iteration 232/1000 | Loss: 0.00001936
Iteration 233/1000 | Loss: 0.00001936
Iteration 234/1000 | Loss: 0.00001936
Iteration 235/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.9360184523975477e-05, 1.9360184523975477e-05, 1.9360184523975477e-05, 1.9360184523975477e-05, 1.9360184523975477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9360184523975477e-05

Optimization complete. Final v2v error: 3.6035501956939697 mm

Highest mean error: 4.478010654449463 mm for frame 176

Lowest mean error: 3.0449538230895996 mm for frame 62

Saving results

Total time: 45.106151819229126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983251
Iteration 2/25 | Loss: 0.00208975
Iteration 3/25 | Loss: 0.00138118
Iteration 4/25 | Loss: 0.00133218
Iteration 5/25 | Loss: 0.00132891
Iteration 6/25 | Loss: 0.00132891
Iteration 7/25 | Loss: 0.00132891
Iteration 8/25 | Loss: 0.00132891
Iteration 9/25 | Loss: 0.00132891
Iteration 10/25 | Loss: 0.00132891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013289094204083085, 0.0013289094204083085, 0.0013289094204083085, 0.0013289094204083085, 0.0013289094204083085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013289094204083085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40583181
Iteration 2/25 | Loss: 0.00084561
Iteration 3/25 | Loss: 0.00084560
Iteration 4/25 | Loss: 0.00084560
Iteration 5/25 | Loss: 0.00084560
Iteration 6/25 | Loss: 0.00084560
Iteration 7/25 | Loss: 0.00084560
Iteration 8/25 | Loss: 0.00084560
Iteration 9/25 | Loss: 0.00084560
Iteration 10/25 | Loss: 0.00084560
Iteration 11/25 | Loss: 0.00084560
Iteration 12/25 | Loss: 0.00084560
Iteration 13/25 | Loss: 0.00084560
Iteration 14/25 | Loss: 0.00084560
Iteration 15/25 | Loss: 0.00084560
Iteration 16/25 | Loss: 0.00084560
Iteration 17/25 | Loss: 0.00084560
Iteration 18/25 | Loss: 0.00084560
Iteration 19/25 | Loss: 0.00084560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008456010255031288, 0.0008456010255031288, 0.0008456010255031288, 0.0008456010255031288, 0.0008456010255031288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008456010255031288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084560
Iteration 2/1000 | Loss: 0.00003529
Iteration 3/1000 | Loss: 0.00002321
Iteration 4/1000 | Loss: 0.00002093
Iteration 5/1000 | Loss: 0.00001977
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001850
Iteration 8/1000 | Loss: 0.00001804
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001724
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001715
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001705
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001695
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001693
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001692
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001690
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001689
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001687
Iteration 69/1000 | Loss: 0.00001687
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001687
Iteration 72/1000 | Loss: 0.00001687
Iteration 73/1000 | Loss: 0.00001687
Iteration 74/1000 | Loss: 0.00001687
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001686
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001685
Iteration 93/1000 | Loss: 0.00001685
Iteration 94/1000 | Loss: 0.00001685
Iteration 95/1000 | Loss: 0.00001685
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001684
Iteration 100/1000 | Loss: 0.00001684
Iteration 101/1000 | Loss: 0.00001684
Iteration 102/1000 | Loss: 0.00001683
Iteration 103/1000 | Loss: 0.00001683
Iteration 104/1000 | Loss: 0.00001683
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001683
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001682
Iteration 120/1000 | Loss: 0.00001682
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001681
Iteration 127/1000 | Loss: 0.00001681
Iteration 128/1000 | Loss: 0.00001681
Iteration 129/1000 | Loss: 0.00001681
Iteration 130/1000 | Loss: 0.00001681
Iteration 131/1000 | Loss: 0.00001681
Iteration 132/1000 | Loss: 0.00001681
Iteration 133/1000 | Loss: 0.00001681
Iteration 134/1000 | Loss: 0.00001681
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001680
Iteration 141/1000 | Loss: 0.00001679
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001679
Iteration 144/1000 | Loss: 0.00001679
Iteration 145/1000 | Loss: 0.00001679
Iteration 146/1000 | Loss: 0.00001679
Iteration 147/1000 | Loss: 0.00001679
Iteration 148/1000 | Loss: 0.00001679
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001679
Iteration 151/1000 | Loss: 0.00001679
Iteration 152/1000 | Loss: 0.00001679
Iteration 153/1000 | Loss: 0.00001679
Iteration 154/1000 | Loss: 0.00001678
Iteration 155/1000 | Loss: 0.00001678
Iteration 156/1000 | Loss: 0.00001678
Iteration 157/1000 | Loss: 0.00001678
Iteration 158/1000 | Loss: 0.00001678
Iteration 159/1000 | Loss: 0.00001678
Iteration 160/1000 | Loss: 0.00001678
Iteration 161/1000 | Loss: 0.00001678
Iteration 162/1000 | Loss: 0.00001678
Iteration 163/1000 | Loss: 0.00001678
Iteration 164/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.678210901445709e-05, 1.678210901445709e-05, 1.678210901445709e-05, 1.678210901445709e-05, 1.678210901445709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.678210901445709e-05

Optimization complete. Final v2v error: 3.413796901702881 mm

Highest mean error: 4.1802239418029785 mm for frame 227

Lowest mean error: 3.0080742835998535 mm for frame 172

Saving results

Total time: 38.93172359466553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829670
Iteration 2/25 | Loss: 0.00145959
Iteration 3/25 | Loss: 0.00131077
Iteration 4/25 | Loss: 0.00128170
Iteration 5/25 | Loss: 0.00127378
Iteration 6/25 | Loss: 0.00127256
Iteration 7/25 | Loss: 0.00127256
Iteration 8/25 | Loss: 0.00127256
Iteration 9/25 | Loss: 0.00127256
Iteration 10/25 | Loss: 0.00127256
Iteration 11/25 | Loss: 0.00127256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012725593987852335, 0.0012725593987852335, 0.0012725593987852335, 0.0012725593987852335, 0.0012725593987852335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012725593987852335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97475135
Iteration 2/25 | Loss: 0.00072359
Iteration 3/25 | Loss: 0.00072359
Iteration 4/25 | Loss: 0.00072359
Iteration 5/25 | Loss: 0.00072359
Iteration 6/25 | Loss: 0.00072358
Iteration 7/25 | Loss: 0.00072358
Iteration 8/25 | Loss: 0.00072358
Iteration 9/25 | Loss: 0.00072358
Iteration 10/25 | Loss: 0.00072358
Iteration 11/25 | Loss: 0.00072358
Iteration 12/25 | Loss: 0.00072358
Iteration 13/25 | Loss: 0.00072358
Iteration 14/25 | Loss: 0.00072358
Iteration 15/25 | Loss: 0.00072358
Iteration 16/25 | Loss: 0.00072358
Iteration 17/25 | Loss: 0.00072358
Iteration 18/25 | Loss: 0.00072358
Iteration 19/25 | Loss: 0.00072358
Iteration 20/25 | Loss: 0.00072358
Iteration 21/25 | Loss: 0.00072358
Iteration 22/25 | Loss: 0.00072358
Iteration 23/25 | Loss: 0.00072358
Iteration 24/25 | Loss: 0.00072358
Iteration 25/25 | Loss: 0.00072358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072358
Iteration 2/1000 | Loss: 0.00005054
Iteration 3/1000 | Loss: 0.00003655
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002849
Iteration 6/1000 | Loss: 0.00002760
Iteration 7/1000 | Loss: 0.00002663
Iteration 8/1000 | Loss: 0.00002593
Iteration 9/1000 | Loss: 0.00002528
Iteration 10/1000 | Loss: 0.00002486
Iteration 11/1000 | Loss: 0.00002455
Iteration 12/1000 | Loss: 0.00002429
Iteration 13/1000 | Loss: 0.00002407
Iteration 14/1000 | Loss: 0.00002401
Iteration 15/1000 | Loss: 0.00002385
Iteration 16/1000 | Loss: 0.00002367
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002363
Iteration 19/1000 | Loss: 0.00002362
Iteration 20/1000 | Loss: 0.00002361
Iteration 21/1000 | Loss: 0.00002359
Iteration 22/1000 | Loss: 0.00002356
Iteration 23/1000 | Loss: 0.00002355
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002351
Iteration 26/1000 | Loss: 0.00002349
Iteration 27/1000 | Loss: 0.00002348
Iteration 28/1000 | Loss: 0.00002348
Iteration 29/1000 | Loss: 0.00002348
Iteration 30/1000 | Loss: 0.00002347
Iteration 31/1000 | Loss: 0.00002347
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002346
Iteration 34/1000 | Loss: 0.00002346
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00002345
Iteration 37/1000 | Loss: 0.00002344
Iteration 38/1000 | Loss: 0.00002344
Iteration 39/1000 | Loss: 0.00002344
Iteration 40/1000 | Loss: 0.00002344
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002343
Iteration 44/1000 | Loss: 0.00002342
Iteration 45/1000 | Loss: 0.00002342
Iteration 46/1000 | Loss: 0.00002342
Iteration 47/1000 | Loss: 0.00002341
Iteration 48/1000 | Loss: 0.00002341
Iteration 49/1000 | Loss: 0.00002341
Iteration 50/1000 | Loss: 0.00002341
Iteration 51/1000 | Loss: 0.00002341
Iteration 52/1000 | Loss: 0.00002341
Iteration 53/1000 | Loss: 0.00002341
Iteration 54/1000 | Loss: 0.00002341
Iteration 55/1000 | Loss: 0.00002341
Iteration 56/1000 | Loss: 0.00002341
Iteration 57/1000 | Loss: 0.00002341
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002340
Iteration 60/1000 | Loss: 0.00002340
Iteration 61/1000 | Loss: 0.00002340
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002340
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002340
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002338
Iteration 70/1000 | Loss: 0.00002338
Iteration 71/1000 | Loss: 0.00002338
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00002337
Iteration 74/1000 | Loss: 0.00002337
Iteration 75/1000 | Loss: 0.00002337
Iteration 76/1000 | Loss: 0.00002336
Iteration 77/1000 | Loss: 0.00002336
Iteration 78/1000 | Loss: 0.00002336
Iteration 79/1000 | Loss: 0.00002336
Iteration 80/1000 | Loss: 0.00002336
Iteration 81/1000 | Loss: 0.00002336
Iteration 82/1000 | Loss: 0.00002335
Iteration 83/1000 | Loss: 0.00002335
Iteration 84/1000 | Loss: 0.00002335
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002335
Iteration 87/1000 | Loss: 0.00002334
Iteration 88/1000 | Loss: 0.00002334
Iteration 89/1000 | Loss: 0.00002334
Iteration 90/1000 | Loss: 0.00002334
Iteration 91/1000 | Loss: 0.00002334
Iteration 92/1000 | Loss: 0.00002334
Iteration 93/1000 | Loss: 0.00002334
Iteration 94/1000 | Loss: 0.00002334
Iteration 95/1000 | Loss: 0.00002334
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00002333
Iteration 98/1000 | Loss: 0.00002333
Iteration 99/1000 | Loss: 0.00002333
Iteration 100/1000 | Loss: 0.00002333
Iteration 101/1000 | Loss: 0.00002333
Iteration 102/1000 | Loss: 0.00002333
Iteration 103/1000 | Loss: 0.00002333
Iteration 104/1000 | Loss: 0.00002332
Iteration 105/1000 | Loss: 0.00002332
Iteration 106/1000 | Loss: 0.00002332
Iteration 107/1000 | Loss: 0.00002332
Iteration 108/1000 | Loss: 0.00002332
Iteration 109/1000 | Loss: 0.00002332
Iteration 110/1000 | Loss: 0.00002332
Iteration 111/1000 | Loss: 0.00002332
Iteration 112/1000 | Loss: 0.00002332
Iteration 113/1000 | Loss: 0.00002331
Iteration 114/1000 | Loss: 0.00002331
Iteration 115/1000 | Loss: 0.00002331
Iteration 116/1000 | Loss: 0.00002331
Iteration 117/1000 | Loss: 0.00002331
Iteration 118/1000 | Loss: 0.00002331
Iteration 119/1000 | Loss: 0.00002331
Iteration 120/1000 | Loss: 0.00002331
Iteration 121/1000 | Loss: 0.00002330
Iteration 122/1000 | Loss: 0.00002330
Iteration 123/1000 | Loss: 0.00002330
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002330
Iteration 126/1000 | Loss: 0.00002330
Iteration 127/1000 | Loss: 0.00002330
Iteration 128/1000 | Loss: 0.00002330
Iteration 129/1000 | Loss: 0.00002330
Iteration 130/1000 | Loss: 0.00002330
Iteration 131/1000 | Loss: 0.00002330
Iteration 132/1000 | Loss: 0.00002330
Iteration 133/1000 | Loss: 0.00002330
Iteration 134/1000 | Loss: 0.00002330
Iteration 135/1000 | Loss: 0.00002330
Iteration 136/1000 | Loss: 0.00002330
Iteration 137/1000 | Loss: 0.00002330
Iteration 138/1000 | Loss: 0.00002330
Iteration 139/1000 | Loss: 0.00002330
Iteration 140/1000 | Loss: 0.00002330
Iteration 141/1000 | Loss: 0.00002330
Iteration 142/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.329537892364897e-05, 2.329537892364897e-05, 2.329537892364897e-05, 2.329537892364897e-05, 2.329537892364897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.329537892364897e-05

Optimization complete. Final v2v error: 4.0247802734375 mm

Highest mean error: 4.469632625579834 mm for frame 52

Lowest mean error: 3.7563316822052 mm for frame 130

Saving results

Total time: 38.48418736457825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047938
Iteration 2/25 | Loss: 0.01047938
Iteration 3/25 | Loss: 0.01047938
Iteration 4/25 | Loss: 0.01047938
Iteration 5/25 | Loss: 0.01047938
Iteration 6/25 | Loss: 0.01047938
Iteration 7/25 | Loss: 0.01047938
Iteration 8/25 | Loss: 0.01047937
Iteration 9/25 | Loss: 0.01047937
Iteration 10/25 | Loss: 0.01047937
Iteration 11/25 | Loss: 0.01047937
Iteration 12/25 | Loss: 0.01047937
Iteration 13/25 | Loss: 0.01047937
Iteration 14/25 | Loss: 0.01047937
Iteration 15/25 | Loss: 0.01047937
Iteration 16/25 | Loss: 0.01047936
Iteration 17/25 | Loss: 0.01047936
Iteration 18/25 | Loss: 0.01047936
Iteration 19/25 | Loss: 0.01047936
Iteration 20/25 | Loss: 0.01047936
Iteration 21/25 | Loss: 0.01047936
Iteration 22/25 | Loss: 0.01047936
Iteration 23/25 | Loss: 0.01047935
Iteration 24/25 | Loss: 0.01047935
Iteration 25/25 | Loss: 0.01047935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76748884
Iteration 2/25 | Loss: 0.08320279
Iteration 3/25 | Loss: 0.08315811
Iteration 4/25 | Loss: 0.08315810
Iteration 5/25 | Loss: 0.08315810
Iteration 6/25 | Loss: 0.08315810
Iteration 7/25 | Loss: 0.08315810
Iteration 8/25 | Loss: 0.08315810
Iteration 9/25 | Loss: 0.08315809
Iteration 10/25 | Loss: 0.08315809
Iteration 11/25 | Loss: 0.08315809
Iteration 12/25 | Loss: 0.08315809
Iteration 13/25 | Loss: 0.08315809
Iteration 14/25 | Loss: 0.08315809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.08315809071063995, 0.08315809071063995, 0.08315809071063995, 0.08315809071063995, 0.08315809071063995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08315809071063995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08315809
Iteration 2/1000 | Loss: 0.00251878
Iteration 3/1000 | Loss: 0.00130110
Iteration 4/1000 | Loss: 0.00025906
Iteration 5/1000 | Loss: 0.00050811
Iteration 6/1000 | Loss: 0.00022823
Iteration 7/1000 | Loss: 0.00028125
Iteration 8/1000 | Loss: 0.00007351
Iteration 9/1000 | Loss: 0.00007528
Iteration 10/1000 | Loss: 0.00014959
Iteration 11/1000 | Loss: 0.00003754
Iteration 12/1000 | Loss: 0.00003288
Iteration 13/1000 | Loss: 0.00003011
Iteration 14/1000 | Loss: 0.00002812
Iteration 15/1000 | Loss: 0.00002676
Iteration 16/1000 | Loss: 0.00002579
Iteration 17/1000 | Loss: 0.00003604
Iteration 18/1000 | Loss: 0.00002448
Iteration 19/1000 | Loss: 0.00003312
Iteration 20/1000 | Loss: 0.00002582
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00004734
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002070
Iteration 25/1000 | Loss: 0.00004266
Iteration 26/1000 | Loss: 0.00003793
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001953
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001895
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001881
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001842
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00006164
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001837
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001835
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001834
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001834
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001834
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001833
Iteration 72/1000 | Loss: 0.00001833
Iteration 73/1000 | Loss: 0.00001833
Iteration 74/1000 | Loss: 0.00001833
Iteration 75/1000 | Loss: 0.00001832
Iteration 76/1000 | Loss: 0.00001832
Iteration 77/1000 | Loss: 0.00001832
Iteration 78/1000 | Loss: 0.00001832
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001831
Iteration 89/1000 | Loss: 0.00001831
Iteration 90/1000 | Loss: 0.00001831
Iteration 91/1000 | Loss: 0.00001831
Iteration 92/1000 | Loss: 0.00001831
Iteration 93/1000 | Loss: 0.00001831
Iteration 94/1000 | Loss: 0.00001831
Iteration 95/1000 | Loss: 0.00001831
Iteration 96/1000 | Loss: 0.00001831
Iteration 97/1000 | Loss: 0.00001831
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001831
Iteration 100/1000 | Loss: 0.00001831
Iteration 101/1000 | Loss: 0.00001831
Iteration 102/1000 | Loss: 0.00001831
Iteration 103/1000 | Loss: 0.00001831
Iteration 104/1000 | Loss: 0.00001831
Iteration 105/1000 | Loss: 0.00001831
Iteration 106/1000 | Loss: 0.00001831
Iteration 107/1000 | Loss: 0.00001831
Iteration 108/1000 | Loss: 0.00001831
Iteration 109/1000 | Loss: 0.00001831
Iteration 110/1000 | Loss: 0.00001831
Iteration 111/1000 | Loss: 0.00001831
Iteration 112/1000 | Loss: 0.00001831
Iteration 113/1000 | Loss: 0.00001831
Iteration 114/1000 | Loss: 0.00001831
Iteration 115/1000 | Loss: 0.00001831
Iteration 116/1000 | Loss: 0.00001831
Iteration 117/1000 | Loss: 0.00001831
Iteration 118/1000 | Loss: 0.00001831
Iteration 119/1000 | Loss: 0.00001831
Iteration 120/1000 | Loss: 0.00001831
Iteration 121/1000 | Loss: 0.00001831
Iteration 122/1000 | Loss: 0.00001831
Iteration 123/1000 | Loss: 0.00001831
Iteration 124/1000 | Loss: 0.00001831
Iteration 125/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.83092852239497e-05, 1.83092852239497e-05, 1.83092852239497e-05, 1.83092852239497e-05, 1.83092852239497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.83092852239497e-05

Optimization complete. Final v2v error: 3.670379877090454 mm

Highest mean error: 4.097868919372559 mm for frame 8

Lowest mean error: 3.368342161178589 mm for frame 12

Saving results

Total time: 59.681950092315674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840051
Iteration 2/25 | Loss: 0.00161793
Iteration 3/25 | Loss: 0.00130295
Iteration 4/25 | Loss: 0.00126976
Iteration 5/25 | Loss: 0.00126499
Iteration 6/25 | Loss: 0.00126457
Iteration 7/25 | Loss: 0.00126457
Iteration 8/25 | Loss: 0.00126457
Iteration 9/25 | Loss: 0.00126457
Iteration 10/25 | Loss: 0.00126457
Iteration 11/25 | Loss: 0.00126457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012645740061998367, 0.0012645740061998367, 0.0012645740061998367, 0.0012645740061998367, 0.0012645740061998367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012645740061998367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40112686
Iteration 2/25 | Loss: 0.00074689
Iteration 3/25 | Loss: 0.00074687
Iteration 4/25 | Loss: 0.00074687
Iteration 5/25 | Loss: 0.00074687
Iteration 6/25 | Loss: 0.00074687
Iteration 7/25 | Loss: 0.00074687
Iteration 8/25 | Loss: 0.00074687
Iteration 9/25 | Loss: 0.00074687
Iteration 10/25 | Loss: 0.00074687
Iteration 11/25 | Loss: 0.00074687
Iteration 12/25 | Loss: 0.00074687
Iteration 13/25 | Loss: 0.00074687
Iteration 14/25 | Loss: 0.00074687
Iteration 15/25 | Loss: 0.00074687
Iteration 16/25 | Loss: 0.00074687
Iteration 17/25 | Loss: 0.00074687
Iteration 18/25 | Loss: 0.00074687
Iteration 19/25 | Loss: 0.00074687
Iteration 20/25 | Loss: 0.00074687
Iteration 21/25 | Loss: 0.00074687
Iteration 22/25 | Loss: 0.00074687
Iteration 23/25 | Loss: 0.00074687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007468721596524119, 0.0007468721596524119, 0.0007468721596524119, 0.0007468721596524119, 0.0007468721596524119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007468721596524119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074687
Iteration 2/1000 | Loss: 0.00004052
Iteration 3/1000 | Loss: 0.00002371
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001836
Iteration 7/1000 | Loss: 0.00001773
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001697
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001637
Iteration 12/1000 | Loss: 0.00001622
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001598
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001566
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001551
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001548
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001537
Iteration 76/1000 | Loss: 0.00001537
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001534
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001533
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001533
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001532
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001532
Iteration 107/1000 | Loss: 0.00001532
Iteration 108/1000 | Loss: 0.00001532
Iteration 109/1000 | Loss: 0.00001532
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001532
Iteration 116/1000 | Loss: 0.00001532
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.531753150629811e-05, 1.531753150629811e-05, 1.531753150629811e-05, 1.531753150629811e-05, 1.531753150629811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.531753150629811e-05

Optimization complete. Final v2v error: 3.3155882358551025 mm

Highest mean error: 3.8894460201263428 mm for frame 1

Lowest mean error: 2.9889299869537354 mm for frame 239

Saving results

Total time: 43.970374584198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454306
Iteration 2/25 | Loss: 0.00126655
Iteration 3/25 | Loss: 0.00118629
Iteration 4/25 | Loss: 0.00117869
Iteration 5/25 | Loss: 0.00117651
Iteration 6/25 | Loss: 0.00117597
Iteration 7/25 | Loss: 0.00117597
Iteration 8/25 | Loss: 0.00117597
Iteration 9/25 | Loss: 0.00117597
Iteration 10/25 | Loss: 0.00117597
Iteration 11/25 | Loss: 0.00117597
Iteration 12/25 | Loss: 0.00117597
Iteration 13/25 | Loss: 0.00117597
Iteration 14/25 | Loss: 0.00117597
Iteration 15/25 | Loss: 0.00117597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011759728658944368, 0.0011759728658944368, 0.0011759728658944368, 0.0011759728658944368, 0.0011759728658944368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011759728658944368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38421202
Iteration 2/25 | Loss: 0.00069389
Iteration 3/25 | Loss: 0.00069388
Iteration 4/25 | Loss: 0.00069388
Iteration 5/25 | Loss: 0.00069388
Iteration 6/25 | Loss: 0.00069387
Iteration 7/25 | Loss: 0.00069387
Iteration 8/25 | Loss: 0.00069387
Iteration 9/25 | Loss: 0.00069387
Iteration 10/25 | Loss: 0.00069387
Iteration 11/25 | Loss: 0.00069387
Iteration 12/25 | Loss: 0.00069387
Iteration 13/25 | Loss: 0.00069387
Iteration 14/25 | Loss: 0.00069387
Iteration 15/25 | Loss: 0.00069387
Iteration 16/25 | Loss: 0.00069387
Iteration 17/25 | Loss: 0.00069387
Iteration 18/25 | Loss: 0.00069387
Iteration 19/25 | Loss: 0.00069387
Iteration 20/25 | Loss: 0.00069387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006938728620298207, 0.0006938728620298207, 0.0006938728620298207, 0.0006938728620298207, 0.0006938728620298207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006938728620298207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069387
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001597
Iteration 5/1000 | Loss: 0.00001490
Iteration 6/1000 | Loss: 0.00001420
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001229
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001226
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001206
Iteration 85/1000 | Loss: 0.00001206
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001205
Iteration 88/1000 | Loss: 0.00001205
Iteration 89/1000 | Loss: 0.00001205
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001201
Iteration 109/1000 | Loss: 0.00001201
Iteration 110/1000 | Loss: 0.00001201
Iteration 111/1000 | Loss: 0.00001201
Iteration 112/1000 | Loss: 0.00001201
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001201
Iteration 115/1000 | Loss: 0.00001201
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001200
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001200
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001199
Iteration 128/1000 | Loss: 0.00001199
Iteration 129/1000 | Loss: 0.00001199
Iteration 130/1000 | Loss: 0.00001199
Iteration 131/1000 | Loss: 0.00001199
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001198
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00001196
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001195
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Iteration 166/1000 | Loss: 0.00001194
Iteration 167/1000 | Loss: 0.00001194
Iteration 168/1000 | Loss: 0.00001193
Iteration 169/1000 | Loss: 0.00001193
Iteration 170/1000 | Loss: 0.00001193
Iteration 171/1000 | Loss: 0.00001193
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001191
Iteration 177/1000 | Loss: 0.00001191
Iteration 178/1000 | Loss: 0.00001190
Iteration 179/1000 | Loss: 0.00001190
Iteration 180/1000 | Loss: 0.00001190
Iteration 181/1000 | Loss: 0.00001190
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001189
Iteration 191/1000 | Loss: 0.00001189
Iteration 192/1000 | Loss: 0.00001189
Iteration 193/1000 | Loss: 0.00001189
Iteration 194/1000 | Loss: 0.00001189
Iteration 195/1000 | Loss: 0.00001188
Iteration 196/1000 | Loss: 0.00001188
Iteration 197/1000 | Loss: 0.00001188
Iteration 198/1000 | Loss: 0.00001188
Iteration 199/1000 | Loss: 0.00001188
Iteration 200/1000 | Loss: 0.00001188
Iteration 201/1000 | Loss: 0.00001188
Iteration 202/1000 | Loss: 0.00001188
Iteration 203/1000 | Loss: 0.00001188
Iteration 204/1000 | Loss: 0.00001188
Iteration 205/1000 | Loss: 0.00001188
Iteration 206/1000 | Loss: 0.00001188
Iteration 207/1000 | Loss: 0.00001188
Iteration 208/1000 | Loss: 0.00001188
Iteration 209/1000 | Loss: 0.00001188
Iteration 210/1000 | Loss: 0.00001187
Iteration 211/1000 | Loss: 0.00001187
Iteration 212/1000 | Loss: 0.00001187
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001187
Iteration 216/1000 | Loss: 0.00001187
Iteration 217/1000 | Loss: 0.00001187
Iteration 218/1000 | Loss: 0.00001187
Iteration 219/1000 | Loss: 0.00001187
Iteration 220/1000 | Loss: 0.00001187
Iteration 221/1000 | Loss: 0.00001187
Iteration 222/1000 | Loss: 0.00001187
Iteration 223/1000 | Loss: 0.00001187
Iteration 224/1000 | Loss: 0.00001187
Iteration 225/1000 | Loss: 0.00001187
Iteration 226/1000 | Loss: 0.00001187
Iteration 227/1000 | Loss: 0.00001187
Iteration 228/1000 | Loss: 0.00001187
Iteration 229/1000 | Loss: 0.00001187
Iteration 230/1000 | Loss: 0.00001187
Iteration 231/1000 | Loss: 0.00001186
Iteration 232/1000 | Loss: 0.00001186
Iteration 233/1000 | Loss: 0.00001186
Iteration 234/1000 | Loss: 0.00001186
Iteration 235/1000 | Loss: 0.00001186
Iteration 236/1000 | Loss: 0.00001186
Iteration 237/1000 | Loss: 0.00001186
Iteration 238/1000 | Loss: 0.00001186
Iteration 239/1000 | Loss: 0.00001186
Iteration 240/1000 | Loss: 0.00001185
Iteration 241/1000 | Loss: 0.00001185
Iteration 242/1000 | Loss: 0.00001185
Iteration 243/1000 | Loss: 0.00001185
Iteration 244/1000 | Loss: 0.00001185
Iteration 245/1000 | Loss: 0.00001185
Iteration 246/1000 | Loss: 0.00001185
Iteration 247/1000 | Loss: 0.00001185
Iteration 248/1000 | Loss: 0.00001185
Iteration 249/1000 | Loss: 0.00001184
Iteration 250/1000 | Loss: 0.00001184
Iteration 251/1000 | Loss: 0.00001184
Iteration 252/1000 | Loss: 0.00001184
Iteration 253/1000 | Loss: 0.00001184
Iteration 254/1000 | Loss: 0.00001184
Iteration 255/1000 | Loss: 0.00001184
Iteration 256/1000 | Loss: 0.00001184
Iteration 257/1000 | Loss: 0.00001184
Iteration 258/1000 | Loss: 0.00001184
Iteration 259/1000 | Loss: 0.00001184
Iteration 260/1000 | Loss: 0.00001184
Iteration 261/1000 | Loss: 0.00001184
Iteration 262/1000 | Loss: 0.00001184
Iteration 263/1000 | Loss: 0.00001184
Iteration 264/1000 | Loss: 0.00001184
Iteration 265/1000 | Loss: 0.00001183
Iteration 266/1000 | Loss: 0.00001183
Iteration 267/1000 | Loss: 0.00001183
Iteration 268/1000 | Loss: 0.00001183
Iteration 269/1000 | Loss: 0.00001183
Iteration 270/1000 | Loss: 0.00001183
Iteration 271/1000 | Loss: 0.00001183
Iteration 272/1000 | Loss: 0.00001183
Iteration 273/1000 | Loss: 0.00001183
Iteration 274/1000 | Loss: 0.00001183
Iteration 275/1000 | Loss: 0.00001183
Iteration 276/1000 | Loss: 0.00001183
Iteration 277/1000 | Loss: 0.00001183
Iteration 278/1000 | Loss: 0.00001183
Iteration 279/1000 | Loss: 0.00001183
Iteration 280/1000 | Loss: 0.00001183
Iteration 281/1000 | Loss: 0.00001183
Iteration 282/1000 | Loss: 0.00001183
Iteration 283/1000 | Loss: 0.00001183
Iteration 284/1000 | Loss: 0.00001183
Iteration 285/1000 | Loss: 0.00001182
Iteration 286/1000 | Loss: 0.00001182
Iteration 287/1000 | Loss: 0.00001182
Iteration 288/1000 | Loss: 0.00001182
Iteration 289/1000 | Loss: 0.00001182
Iteration 290/1000 | Loss: 0.00001182
Iteration 291/1000 | Loss: 0.00001182
Iteration 292/1000 | Loss: 0.00001182
Iteration 293/1000 | Loss: 0.00001182
Iteration 294/1000 | Loss: 0.00001182
Iteration 295/1000 | Loss: 0.00001182
Iteration 296/1000 | Loss: 0.00001182
Iteration 297/1000 | Loss: 0.00001182
Iteration 298/1000 | Loss: 0.00001182
Iteration 299/1000 | Loss: 0.00001182
Iteration 300/1000 | Loss: 0.00001182
Iteration 301/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.1818565326393582e-05, 1.1818565326393582e-05, 1.1818565326393582e-05, 1.1818565326393582e-05, 1.1818565326393582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1818565326393582e-05

Optimization complete. Final v2v error: 2.933396339416504 mm

Highest mean error: 5.089687824249268 mm for frame 220

Lowest mean error: 2.694122552871704 mm for frame 81

Saving results

Total time: 55.12165832519531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629557
Iteration 2/25 | Loss: 0.00128751
Iteration 3/25 | Loss: 0.00120719
Iteration 4/25 | Loss: 0.00119935
Iteration 5/25 | Loss: 0.00119623
Iteration 6/25 | Loss: 0.00119578
Iteration 7/25 | Loss: 0.00119578
Iteration 8/25 | Loss: 0.00119578
Iteration 9/25 | Loss: 0.00119578
Iteration 10/25 | Loss: 0.00119578
Iteration 11/25 | Loss: 0.00119578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011957822134718299, 0.0011957822134718299, 0.0011957822134718299, 0.0011957822134718299, 0.0011957822134718299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011957822134718299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48609638
Iteration 2/25 | Loss: 0.00079430
Iteration 3/25 | Loss: 0.00079430
Iteration 4/25 | Loss: 0.00079430
Iteration 5/25 | Loss: 0.00079430
Iteration 6/25 | Loss: 0.00079430
Iteration 7/25 | Loss: 0.00079430
Iteration 8/25 | Loss: 0.00079430
Iteration 9/25 | Loss: 0.00079430
Iteration 10/25 | Loss: 0.00079430
Iteration 11/25 | Loss: 0.00079430
Iteration 12/25 | Loss: 0.00079430
Iteration 13/25 | Loss: 0.00079430
Iteration 14/25 | Loss: 0.00079430
Iteration 15/25 | Loss: 0.00079430
Iteration 16/25 | Loss: 0.00079430
Iteration 17/25 | Loss: 0.00079430
Iteration 18/25 | Loss: 0.00079430
Iteration 19/25 | Loss: 0.00079430
Iteration 20/25 | Loss: 0.00079430
Iteration 21/25 | Loss: 0.00079430
Iteration 22/25 | Loss: 0.00079430
Iteration 23/25 | Loss: 0.00079430
Iteration 24/25 | Loss: 0.00079430
Iteration 25/25 | Loss: 0.00079430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079430
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001477
Iteration 4/1000 | Loss: 0.00001325
Iteration 5/1000 | Loss: 0.00001261
Iteration 6/1000 | Loss: 0.00001221
Iteration 7/1000 | Loss: 0.00001219
Iteration 8/1000 | Loss: 0.00001181
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001167
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001156
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00001132
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001114
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001113
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001108
Iteration 33/1000 | Loss: 0.00001108
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001104
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001103
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001102
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001100
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001098
Iteration 47/1000 | Loss: 0.00001098
Iteration 48/1000 | Loss: 0.00001097
Iteration 49/1000 | Loss: 0.00001097
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001097
Iteration 54/1000 | Loss: 0.00001097
Iteration 55/1000 | Loss: 0.00001097
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001093
Iteration 91/1000 | Loss: 0.00001093
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.093420996767236e-05, 1.093420996767236e-05, 1.093420996767236e-05, 1.093420996767236e-05, 1.093420996767236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.093420996767236e-05

Optimization complete. Final v2v error: 2.826674699783325 mm

Highest mean error: 3.448768138885498 mm for frame 80

Lowest mean error: 2.629913568496704 mm for frame 104

Saving results

Total time: 28.94692087173462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002134
Iteration 2/25 | Loss: 0.00225731
Iteration 3/25 | Loss: 0.00180181
Iteration 4/25 | Loss: 0.00158155
Iteration 5/25 | Loss: 0.00160265
Iteration 6/25 | Loss: 0.00148821
Iteration 7/25 | Loss: 0.00146670
Iteration 8/25 | Loss: 0.00145066
Iteration 9/25 | Loss: 0.00143117
Iteration 10/25 | Loss: 0.00141104
Iteration 11/25 | Loss: 0.00141566
Iteration 12/25 | Loss: 0.00140769
Iteration 13/25 | Loss: 0.00142897
Iteration 14/25 | Loss: 0.00141330
Iteration 15/25 | Loss: 0.00139953
Iteration 16/25 | Loss: 0.00140290
Iteration 17/25 | Loss: 0.00140504
Iteration 18/25 | Loss: 0.00140104
Iteration 19/25 | Loss: 0.00139275
Iteration 20/25 | Loss: 0.00139024
Iteration 21/25 | Loss: 0.00138289
Iteration 22/25 | Loss: 0.00138074
Iteration 23/25 | Loss: 0.00137060
Iteration 24/25 | Loss: 0.00137163
Iteration 25/25 | Loss: 0.00136547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43549502
Iteration 2/25 | Loss: 0.00172639
Iteration 3/25 | Loss: 0.00147610
Iteration 4/25 | Loss: 0.00147610
Iteration 5/25 | Loss: 0.00147609
Iteration 6/25 | Loss: 0.00147609
Iteration 7/25 | Loss: 0.00147609
Iteration 8/25 | Loss: 0.00147609
Iteration 9/25 | Loss: 0.00147609
Iteration 10/25 | Loss: 0.00147609
Iteration 11/25 | Loss: 0.00147609
Iteration 12/25 | Loss: 0.00147609
Iteration 13/25 | Loss: 0.00147609
Iteration 14/25 | Loss: 0.00147609
Iteration 15/25 | Loss: 0.00147609
Iteration 16/25 | Loss: 0.00147609
Iteration 17/25 | Loss: 0.00147609
Iteration 18/25 | Loss: 0.00147609
Iteration 19/25 | Loss: 0.00147609
Iteration 20/25 | Loss: 0.00147609
Iteration 21/25 | Loss: 0.00147609
Iteration 22/25 | Loss: 0.00147609
Iteration 23/25 | Loss: 0.00147609
Iteration 24/25 | Loss: 0.00147609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014760923804715276, 0.0014760923804715276, 0.0014760923804715276, 0.0014760923804715276, 0.0014760923804715276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014760923804715276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147609
Iteration 2/1000 | Loss: 0.00025258
Iteration 3/1000 | Loss: 0.00049881
Iteration 4/1000 | Loss: 0.00050108
Iteration 5/1000 | Loss: 0.00013439
Iteration 6/1000 | Loss: 0.00018373
Iteration 7/1000 | Loss: 0.00019791
Iteration 8/1000 | Loss: 0.00033574
Iteration 9/1000 | Loss: 0.00026309
Iteration 10/1000 | Loss: 0.00016085
Iteration 11/1000 | Loss: 0.00016565
Iteration 12/1000 | Loss: 0.00016867
Iteration 13/1000 | Loss: 0.00080470
Iteration 14/1000 | Loss: 0.00072841
Iteration 15/1000 | Loss: 0.00074707
Iteration 16/1000 | Loss: 0.00041391
Iteration 17/1000 | Loss: 0.00044135
Iteration 18/1000 | Loss: 0.00038255
Iteration 19/1000 | Loss: 0.00042614
Iteration 20/1000 | Loss: 0.00055451
Iteration 21/1000 | Loss: 0.00054257
Iteration 22/1000 | Loss: 0.00051707
Iteration 23/1000 | Loss: 0.00063641
Iteration 24/1000 | Loss: 0.00051172
Iteration 25/1000 | Loss: 0.00048111
Iteration 26/1000 | Loss: 0.00048267
Iteration 27/1000 | Loss: 0.00035348
Iteration 28/1000 | Loss: 0.00051694
Iteration 29/1000 | Loss: 0.00033145
Iteration 30/1000 | Loss: 0.00040145
Iteration 31/1000 | Loss: 0.00035670
Iteration 32/1000 | Loss: 0.00033979
Iteration 33/1000 | Loss: 0.00042864
Iteration 34/1000 | Loss: 0.00036474
Iteration 35/1000 | Loss: 0.00043627
Iteration 36/1000 | Loss: 0.00043639
Iteration 37/1000 | Loss: 0.00037303
Iteration 38/1000 | Loss: 0.00028089
Iteration 39/1000 | Loss: 0.00017266
Iteration 40/1000 | Loss: 0.00017004
Iteration 41/1000 | Loss: 0.00020358
Iteration 42/1000 | Loss: 0.00031026
Iteration 43/1000 | Loss: 0.00018123
Iteration 44/1000 | Loss: 0.00018788
Iteration 45/1000 | Loss: 0.00018278
Iteration 46/1000 | Loss: 0.00014911
Iteration 47/1000 | Loss: 0.00015649
Iteration 48/1000 | Loss: 0.00017537
Iteration 49/1000 | Loss: 0.00018176
Iteration 50/1000 | Loss: 0.00018273
Iteration 51/1000 | Loss: 0.00045818
Iteration 52/1000 | Loss: 0.00042836
Iteration 53/1000 | Loss: 0.00097360
Iteration 54/1000 | Loss: 0.00050815
Iteration 55/1000 | Loss: 0.00077015
Iteration 56/1000 | Loss: 0.00034140
Iteration 57/1000 | Loss: 0.00117297
Iteration 58/1000 | Loss: 0.00020229
Iteration 59/1000 | Loss: 0.00018982
Iteration 60/1000 | Loss: 0.00018418
Iteration 61/1000 | Loss: 0.00018374
Iteration 62/1000 | Loss: 0.00017559
Iteration 63/1000 | Loss: 0.00018407
Iteration 64/1000 | Loss: 0.00019134
Iteration 65/1000 | Loss: 0.00039541
Iteration 66/1000 | Loss: 0.00017276
Iteration 67/1000 | Loss: 0.00021519
Iteration 68/1000 | Loss: 0.00018140
Iteration 69/1000 | Loss: 0.00021088
Iteration 70/1000 | Loss: 0.00029233
Iteration 71/1000 | Loss: 0.00017996
Iteration 72/1000 | Loss: 0.00023883
Iteration 73/1000 | Loss: 0.00033913
Iteration 74/1000 | Loss: 0.00018201
Iteration 75/1000 | Loss: 0.00017522
Iteration 76/1000 | Loss: 0.00017526
Iteration 77/1000 | Loss: 0.00017643
Iteration 78/1000 | Loss: 0.00021010
Iteration 79/1000 | Loss: 0.00041262
Iteration 80/1000 | Loss: 0.00221710
Iteration 81/1000 | Loss: 0.00107205
Iteration 82/1000 | Loss: 0.00040960
Iteration 83/1000 | Loss: 0.00010152
Iteration 84/1000 | Loss: 0.00011314
Iteration 85/1000 | Loss: 0.00024854
Iteration 86/1000 | Loss: 0.00058768
Iteration 87/1000 | Loss: 0.00081581
Iteration 88/1000 | Loss: 0.00035940
Iteration 89/1000 | Loss: 0.00015756
Iteration 90/1000 | Loss: 0.00205041
Iteration 91/1000 | Loss: 0.00278278
Iteration 92/1000 | Loss: 0.00793338
Iteration 93/1000 | Loss: 0.00377345
Iteration 94/1000 | Loss: 0.00108530
Iteration 95/1000 | Loss: 0.00035413
Iteration 96/1000 | Loss: 0.00086497
Iteration 97/1000 | Loss: 0.00010865
Iteration 98/1000 | Loss: 0.00036416
Iteration 99/1000 | Loss: 0.00026245
Iteration 100/1000 | Loss: 0.00004812
Iteration 101/1000 | Loss: 0.00059444
Iteration 102/1000 | Loss: 0.00006095
Iteration 103/1000 | Loss: 0.00017698
Iteration 104/1000 | Loss: 0.00003725
Iteration 105/1000 | Loss: 0.00002941
Iteration 106/1000 | Loss: 0.00024133
Iteration 107/1000 | Loss: 0.00002538
Iteration 108/1000 | Loss: 0.00003887
Iteration 109/1000 | Loss: 0.00003035
Iteration 110/1000 | Loss: 0.00061089
Iteration 111/1000 | Loss: 0.00084682
Iteration 112/1000 | Loss: 0.00025837
Iteration 113/1000 | Loss: 0.00002980
Iteration 114/1000 | Loss: 0.00002152
Iteration 115/1000 | Loss: 0.00027614
Iteration 116/1000 | Loss: 0.00003568
Iteration 117/1000 | Loss: 0.00008798
Iteration 118/1000 | Loss: 0.00001589
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001452
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001389
Iteration 123/1000 | Loss: 0.00001362
Iteration 124/1000 | Loss: 0.00001362
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001354
Iteration 128/1000 | Loss: 0.00001351
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001347
Iteration 131/1000 | Loss: 0.00001347
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001338
Iteration 139/1000 | Loss: 0.00001338
Iteration 140/1000 | Loss: 0.00001338
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001337
Iteration 143/1000 | Loss: 0.00001337
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001335
Iteration 153/1000 | Loss: 0.00001335
Iteration 154/1000 | Loss: 0.00001335
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001334
Iteration 157/1000 | Loss: 0.00001333
Iteration 158/1000 | Loss: 0.00001333
Iteration 159/1000 | Loss: 0.00001333
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001333
Iteration 162/1000 | Loss: 0.00001333
Iteration 163/1000 | Loss: 0.00001333
Iteration 164/1000 | Loss: 0.00001333
Iteration 165/1000 | Loss: 0.00001333
Iteration 166/1000 | Loss: 0.00001332
Iteration 167/1000 | Loss: 0.00001332
Iteration 168/1000 | Loss: 0.00001332
Iteration 169/1000 | Loss: 0.00001332
Iteration 170/1000 | Loss: 0.00001332
Iteration 171/1000 | Loss: 0.00001332
Iteration 172/1000 | Loss: 0.00001332
Iteration 173/1000 | Loss: 0.00001332
Iteration 174/1000 | Loss: 0.00001332
Iteration 175/1000 | Loss: 0.00001332
Iteration 176/1000 | Loss: 0.00001332
Iteration 177/1000 | Loss: 0.00001332
Iteration 178/1000 | Loss: 0.00001332
Iteration 179/1000 | Loss: 0.00001332
Iteration 180/1000 | Loss: 0.00001332
Iteration 181/1000 | Loss: 0.00001332
Iteration 182/1000 | Loss: 0.00001332
Iteration 183/1000 | Loss: 0.00001332
Iteration 184/1000 | Loss: 0.00001332
Iteration 185/1000 | Loss: 0.00001332
Iteration 186/1000 | Loss: 0.00001332
Iteration 187/1000 | Loss: 0.00001332
Iteration 188/1000 | Loss: 0.00001332
Iteration 189/1000 | Loss: 0.00001332
Iteration 190/1000 | Loss: 0.00001332
Iteration 191/1000 | Loss: 0.00001332
Iteration 192/1000 | Loss: 0.00001332
Iteration 193/1000 | Loss: 0.00001332
Iteration 194/1000 | Loss: 0.00001332
Iteration 195/1000 | Loss: 0.00001332
Iteration 196/1000 | Loss: 0.00001332
Iteration 197/1000 | Loss: 0.00001332
Iteration 198/1000 | Loss: 0.00001332
Iteration 199/1000 | Loss: 0.00001332
Iteration 200/1000 | Loss: 0.00001332
Iteration 201/1000 | Loss: 0.00001332
Iteration 202/1000 | Loss: 0.00001332
Iteration 203/1000 | Loss: 0.00001332
Iteration 204/1000 | Loss: 0.00001332
Iteration 205/1000 | Loss: 0.00001332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.332410829490982e-05, 1.332410829490982e-05, 1.332410829490982e-05, 1.332410829490982e-05, 1.332410829490982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.332410829490982e-05

Optimization complete. Final v2v error: 3.0500330924987793 mm

Highest mean error: 5.070773124694824 mm for frame 56

Lowest mean error: 2.7720584869384766 mm for frame 36

Saving results

Total time: 220.6981430053711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049845
Iteration 2/25 | Loss: 0.00205399
Iteration 3/25 | Loss: 0.00133183
Iteration 4/25 | Loss: 0.00124601
Iteration 5/25 | Loss: 0.00119160
Iteration 6/25 | Loss: 0.00117969
Iteration 7/25 | Loss: 0.00117457
Iteration 8/25 | Loss: 0.00116531
Iteration 9/25 | Loss: 0.00116990
Iteration 10/25 | Loss: 0.00116227
Iteration 11/25 | Loss: 0.00116436
Iteration 12/25 | Loss: 0.00116517
Iteration 13/25 | Loss: 0.00115864
Iteration 14/25 | Loss: 0.00115866
Iteration 15/25 | Loss: 0.00116111
Iteration 16/25 | Loss: 0.00115768
Iteration 17/25 | Loss: 0.00115778
Iteration 18/25 | Loss: 0.00115793
Iteration 19/25 | Loss: 0.00115793
Iteration 20/25 | Loss: 0.00116382
Iteration 21/25 | Loss: 0.00115879
Iteration 22/25 | Loss: 0.00116384
Iteration 23/25 | Loss: 0.00116752
Iteration 24/25 | Loss: 0.00115906
Iteration 25/25 | Loss: 0.00115812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35442984
Iteration 2/25 | Loss: 0.00097430
Iteration 3/25 | Loss: 0.00083777
Iteration 4/25 | Loss: 0.00083777
Iteration 5/25 | Loss: 0.00083777
Iteration 6/25 | Loss: 0.00083777
Iteration 7/25 | Loss: 0.00083777
Iteration 8/25 | Loss: 0.00083777
Iteration 9/25 | Loss: 0.00083777
Iteration 10/25 | Loss: 0.00083777
Iteration 11/25 | Loss: 0.00083777
Iteration 12/25 | Loss: 0.00083777
Iteration 13/25 | Loss: 0.00083777
Iteration 14/25 | Loss: 0.00083777
Iteration 15/25 | Loss: 0.00083777
Iteration 16/25 | Loss: 0.00083777
Iteration 17/25 | Loss: 0.00083777
Iteration 18/25 | Loss: 0.00083777
Iteration 19/25 | Loss: 0.00083777
Iteration 20/25 | Loss: 0.00083777
Iteration 21/25 | Loss: 0.00083777
Iteration 22/25 | Loss: 0.00083777
Iteration 23/25 | Loss: 0.00083777
Iteration 24/25 | Loss: 0.00083777
Iteration 25/25 | Loss: 0.00083777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083777
Iteration 2/1000 | Loss: 0.00007001
Iteration 3/1000 | Loss: 0.00028020
Iteration 4/1000 | Loss: 0.00003131
Iteration 5/1000 | Loss: 0.00002282
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001545
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001542
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001464
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001458
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001456
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001456
Iteration 53/1000 | Loss: 0.00001456
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001455
Iteration 59/1000 | Loss: 0.00001455
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001455
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001453
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001450
Iteration 80/1000 | Loss: 0.00001450
Iteration 81/1000 | Loss: 0.00001450
Iteration 82/1000 | Loss: 0.00001450
Iteration 83/1000 | Loss: 0.00001450
Iteration 84/1000 | Loss: 0.00001449
Iteration 85/1000 | Loss: 0.00001448
Iteration 86/1000 | Loss: 0.00001448
Iteration 87/1000 | Loss: 0.00001448
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001442
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001434
Iteration 145/1000 | Loss: 0.00001434
Iteration 146/1000 | Loss: 0.00001434
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001433
Iteration 149/1000 | Loss: 0.00001433
Iteration 150/1000 | Loss: 0.00001433
Iteration 151/1000 | Loss: 0.00001433
Iteration 152/1000 | Loss: 0.00001433
Iteration 153/1000 | Loss: 0.00001433
Iteration 154/1000 | Loss: 0.00001433
Iteration 155/1000 | Loss: 0.00001433
Iteration 156/1000 | Loss: 0.00001433
Iteration 157/1000 | Loss: 0.00001432
Iteration 158/1000 | Loss: 0.00001432
Iteration 159/1000 | Loss: 0.00001432
Iteration 160/1000 | Loss: 0.00001432
Iteration 161/1000 | Loss: 0.00001432
Iteration 162/1000 | Loss: 0.00001432
Iteration 163/1000 | Loss: 0.00001432
Iteration 164/1000 | Loss: 0.00001432
Iteration 165/1000 | Loss: 0.00001431
Iteration 166/1000 | Loss: 0.00001431
Iteration 167/1000 | Loss: 0.00001431
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001429
Iteration 176/1000 | Loss: 0.00001429
Iteration 177/1000 | Loss: 0.00001428
Iteration 178/1000 | Loss: 0.00001428
Iteration 179/1000 | Loss: 0.00001428
Iteration 180/1000 | Loss: 0.00001427
Iteration 181/1000 | Loss: 0.00001427
Iteration 182/1000 | Loss: 0.00001427
Iteration 183/1000 | Loss: 0.00001427
Iteration 184/1000 | Loss: 0.00001426
Iteration 185/1000 | Loss: 0.00001426
Iteration 186/1000 | Loss: 0.00001426
Iteration 187/1000 | Loss: 0.00001426
Iteration 188/1000 | Loss: 0.00002801
Iteration 189/1000 | Loss: 0.00001430
Iteration 190/1000 | Loss: 0.00001379
Iteration 191/1000 | Loss: 0.00001370
Iteration 192/1000 | Loss: 0.00001367
Iteration 193/1000 | Loss: 0.00001365
Iteration 194/1000 | Loss: 0.00001363
Iteration 195/1000 | Loss: 0.00001359
Iteration 196/1000 | Loss: 0.00001359
Iteration 197/1000 | Loss: 0.00001358
Iteration 198/1000 | Loss: 0.00001356
Iteration 199/1000 | Loss: 0.00001349
Iteration 200/1000 | Loss: 0.00001348
Iteration 201/1000 | Loss: 0.00002266
Iteration 202/1000 | Loss: 0.00003348
Iteration 203/1000 | Loss: 0.00001342
Iteration 204/1000 | Loss: 0.00001342
Iteration 205/1000 | Loss: 0.00001342
Iteration 206/1000 | Loss: 0.00001342
Iteration 207/1000 | Loss: 0.00001342
Iteration 208/1000 | Loss: 0.00001342
Iteration 209/1000 | Loss: 0.00001342
Iteration 210/1000 | Loss: 0.00001341
Iteration 211/1000 | Loss: 0.00001341
Iteration 212/1000 | Loss: 0.00001341
Iteration 213/1000 | Loss: 0.00001341
Iteration 214/1000 | Loss: 0.00001341
Iteration 215/1000 | Loss: 0.00001341
Iteration 216/1000 | Loss: 0.00001340
Iteration 217/1000 | Loss: 0.00001339
Iteration 218/1000 | Loss: 0.00001339
Iteration 219/1000 | Loss: 0.00001339
Iteration 220/1000 | Loss: 0.00001339
Iteration 221/1000 | Loss: 0.00001339
Iteration 222/1000 | Loss: 0.00001339
Iteration 223/1000 | Loss: 0.00001339
Iteration 224/1000 | Loss: 0.00001339
Iteration 225/1000 | Loss: 0.00001339
Iteration 226/1000 | Loss: 0.00001338
Iteration 227/1000 | Loss: 0.00001338
Iteration 228/1000 | Loss: 0.00001338
Iteration 229/1000 | Loss: 0.00001338
Iteration 230/1000 | Loss: 0.00001338
Iteration 231/1000 | Loss: 0.00001338
Iteration 232/1000 | Loss: 0.00001338
Iteration 233/1000 | Loss: 0.00001337
Iteration 234/1000 | Loss: 0.00001337
Iteration 235/1000 | Loss: 0.00001337
Iteration 236/1000 | Loss: 0.00001337
Iteration 237/1000 | Loss: 0.00001337
Iteration 238/1000 | Loss: 0.00001337
Iteration 239/1000 | Loss: 0.00001337
Iteration 240/1000 | Loss: 0.00001337
Iteration 241/1000 | Loss: 0.00001337
Iteration 242/1000 | Loss: 0.00001337
Iteration 243/1000 | Loss: 0.00001337
Iteration 244/1000 | Loss: 0.00001336
Iteration 245/1000 | Loss: 0.00001336
Iteration 246/1000 | Loss: 0.00001336
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001336
Iteration 249/1000 | Loss: 0.00001336
Iteration 250/1000 | Loss: 0.00001336
Iteration 251/1000 | Loss: 0.00001336
Iteration 252/1000 | Loss: 0.00001335
Iteration 253/1000 | Loss: 0.00001335
Iteration 254/1000 | Loss: 0.00001335
Iteration 255/1000 | Loss: 0.00001335
Iteration 256/1000 | Loss: 0.00001335
Iteration 257/1000 | Loss: 0.00001334
Iteration 258/1000 | Loss: 0.00001334
Iteration 259/1000 | Loss: 0.00001334
Iteration 260/1000 | Loss: 0.00001334
Iteration 261/1000 | Loss: 0.00001332
Iteration 262/1000 | Loss: 0.00001332
Iteration 263/1000 | Loss: 0.00001332
Iteration 264/1000 | Loss: 0.00001332
Iteration 265/1000 | Loss: 0.00001331
Iteration 266/1000 | Loss: 0.00001331
Iteration 267/1000 | Loss: 0.00001331
Iteration 268/1000 | Loss: 0.00001330
Iteration 269/1000 | Loss: 0.00001330
Iteration 270/1000 | Loss: 0.00001330
Iteration 271/1000 | Loss: 0.00001330
Iteration 272/1000 | Loss: 0.00001330
Iteration 273/1000 | Loss: 0.00001330
Iteration 274/1000 | Loss: 0.00001330
Iteration 275/1000 | Loss: 0.00001330
Iteration 276/1000 | Loss: 0.00001330
Iteration 277/1000 | Loss: 0.00001329
Iteration 278/1000 | Loss: 0.00001329
Iteration 279/1000 | Loss: 0.00001329
Iteration 280/1000 | Loss: 0.00001329
Iteration 281/1000 | Loss: 0.00001328
Iteration 282/1000 | Loss: 0.00001328
Iteration 283/1000 | Loss: 0.00001328
Iteration 284/1000 | Loss: 0.00001328
Iteration 285/1000 | Loss: 0.00001327
Iteration 286/1000 | Loss: 0.00001327
Iteration 287/1000 | Loss: 0.00001327
Iteration 288/1000 | Loss: 0.00001326
Iteration 289/1000 | Loss: 0.00001326
Iteration 290/1000 | Loss: 0.00001326
Iteration 291/1000 | Loss: 0.00001326
Iteration 292/1000 | Loss: 0.00001326
Iteration 293/1000 | Loss: 0.00001326
Iteration 294/1000 | Loss: 0.00001326
Iteration 295/1000 | Loss: 0.00001326
Iteration 296/1000 | Loss: 0.00001326
Iteration 297/1000 | Loss: 0.00001326
Iteration 298/1000 | Loss: 0.00001326
Iteration 299/1000 | Loss: 0.00001326
Iteration 300/1000 | Loss: 0.00001326
Iteration 301/1000 | Loss: 0.00001326
Iteration 302/1000 | Loss: 0.00001325
Iteration 303/1000 | Loss: 0.00001325
Iteration 304/1000 | Loss: 0.00001325
Iteration 305/1000 | Loss: 0.00001325
Iteration 306/1000 | Loss: 0.00001325
Iteration 307/1000 | Loss: 0.00001325
Iteration 308/1000 | Loss: 0.00001325
Iteration 309/1000 | Loss: 0.00001325
Iteration 310/1000 | Loss: 0.00001325
Iteration 311/1000 | Loss: 0.00001325
Iteration 312/1000 | Loss: 0.00001325
Iteration 313/1000 | Loss: 0.00001325
Iteration 314/1000 | Loss: 0.00001325
Iteration 315/1000 | Loss: 0.00001325
Iteration 316/1000 | Loss: 0.00001325
Iteration 317/1000 | Loss: 0.00001325
Iteration 318/1000 | Loss: 0.00001325
Iteration 319/1000 | Loss: 0.00001325
Iteration 320/1000 | Loss: 0.00001325
Iteration 321/1000 | Loss: 0.00001325
Iteration 322/1000 | Loss: 0.00001325
Iteration 323/1000 | Loss: 0.00001325
Iteration 324/1000 | Loss: 0.00001325
Iteration 325/1000 | Loss: 0.00001325
Iteration 326/1000 | Loss: 0.00001325
Iteration 327/1000 | Loss: 0.00001325
Iteration 328/1000 | Loss: 0.00001325
Iteration 329/1000 | Loss: 0.00001325
Iteration 330/1000 | Loss: 0.00001325
Iteration 331/1000 | Loss: 0.00001325
Iteration 332/1000 | Loss: 0.00001325
Iteration 333/1000 | Loss: 0.00001325
Iteration 334/1000 | Loss: 0.00001325
Iteration 335/1000 | Loss: 0.00001325
Iteration 336/1000 | Loss: 0.00001325
Iteration 337/1000 | Loss: 0.00001325
Iteration 338/1000 | Loss: 0.00001325
Iteration 339/1000 | Loss: 0.00001325
Iteration 340/1000 | Loss: 0.00001325
Iteration 341/1000 | Loss: 0.00001325
Iteration 342/1000 | Loss: 0.00001325
Iteration 343/1000 | Loss: 0.00001325
Iteration 344/1000 | Loss: 0.00001325
Iteration 345/1000 | Loss: 0.00001325
Iteration 346/1000 | Loss: 0.00001325
Iteration 347/1000 | Loss: 0.00001325
Iteration 348/1000 | Loss: 0.00001325
Iteration 349/1000 | Loss: 0.00001325
Iteration 350/1000 | Loss: 0.00001325
Iteration 351/1000 | Loss: 0.00001325
Iteration 352/1000 | Loss: 0.00001325
Iteration 353/1000 | Loss: 0.00001325
Iteration 354/1000 | Loss: 0.00001325
Iteration 355/1000 | Loss: 0.00001325
Iteration 356/1000 | Loss: 0.00001325
Iteration 357/1000 | Loss: 0.00001325
Iteration 358/1000 | Loss: 0.00001325
Iteration 359/1000 | Loss: 0.00001325
Iteration 360/1000 | Loss: 0.00001325
Iteration 361/1000 | Loss: 0.00001325
Iteration 362/1000 | Loss: 0.00001325
Iteration 363/1000 | Loss: 0.00001325
Iteration 364/1000 | Loss: 0.00001325
Iteration 365/1000 | Loss: 0.00001325
Iteration 366/1000 | Loss: 0.00001325
Iteration 367/1000 | Loss: 0.00001325
Iteration 368/1000 | Loss: 0.00001325
Iteration 369/1000 | Loss: 0.00001325
Iteration 370/1000 | Loss: 0.00001325
Iteration 371/1000 | Loss: 0.00001325
Iteration 372/1000 | Loss: 0.00001325
Iteration 373/1000 | Loss: 0.00001325
Iteration 374/1000 | Loss: 0.00001325
Iteration 375/1000 | Loss: 0.00001325
Iteration 376/1000 | Loss: 0.00001325
Iteration 377/1000 | Loss: 0.00001325
Iteration 378/1000 | Loss: 0.00001325
Iteration 379/1000 | Loss: 0.00001325
Iteration 380/1000 | Loss: 0.00001325
Iteration 381/1000 | Loss: 0.00001325
Iteration 382/1000 | Loss: 0.00001325
Iteration 383/1000 | Loss: 0.00001325
Iteration 384/1000 | Loss: 0.00001325
Iteration 385/1000 | Loss: 0.00001325
Iteration 386/1000 | Loss: 0.00001325
Iteration 387/1000 | Loss: 0.00001325
Iteration 388/1000 | Loss: 0.00001325
Iteration 389/1000 | Loss: 0.00001325
Iteration 390/1000 | Loss: 0.00001325
Iteration 391/1000 | Loss: 0.00001325
Iteration 392/1000 | Loss: 0.00001325
Iteration 393/1000 | Loss: 0.00001325
Iteration 394/1000 | Loss: 0.00001325
Iteration 395/1000 | Loss: 0.00001325
Iteration 396/1000 | Loss: 0.00001325
Iteration 397/1000 | Loss: 0.00001325
Iteration 398/1000 | Loss: 0.00001325
Iteration 399/1000 | Loss: 0.00001325
Iteration 400/1000 | Loss: 0.00001325
Iteration 401/1000 | Loss: 0.00001325
Iteration 402/1000 | Loss: 0.00001325
Iteration 403/1000 | Loss: 0.00001325
Iteration 404/1000 | Loss: 0.00001325
Iteration 405/1000 | Loss: 0.00001325
Iteration 406/1000 | Loss: 0.00001325
Iteration 407/1000 | Loss: 0.00001325
Iteration 408/1000 | Loss: 0.00001325
Iteration 409/1000 | Loss: 0.00001325
Iteration 410/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 410. Stopping optimization.
Last 5 losses: [1.3252633834781591e-05, 1.3252633834781591e-05, 1.3252633834781591e-05, 1.3252633834781591e-05, 1.3252633834781591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3252633834781591e-05

Optimization complete. Final v2v error: 3.0693726539611816 mm

Highest mean error: 5.2639031410217285 mm for frame 147

Lowest mean error: 2.758267641067505 mm for frame 47

Saving results

Total time: 94.72179174423218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020579
Iteration 2/25 | Loss: 0.00188521
Iteration 3/25 | Loss: 0.00151956
Iteration 4/25 | Loss: 0.00139370
Iteration 5/25 | Loss: 0.00144487
Iteration 6/25 | Loss: 0.00135633
Iteration 7/25 | Loss: 0.00134234
Iteration 8/25 | Loss: 0.00132709
Iteration 9/25 | Loss: 0.00132910
Iteration 10/25 | Loss: 0.00132077
Iteration 11/25 | Loss: 0.00131432
Iteration 12/25 | Loss: 0.00131254
Iteration 13/25 | Loss: 0.00131243
Iteration 14/25 | Loss: 0.00129661
Iteration 15/25 | Loss: 0.00129996
Iteration 16/25 | Loss: 0.00130514
Iteration 17/25 | Loss: 0.00130354
Iteration 18/25 | Loss: 0.00128884
Iteration 19/25 | Loss: 0.00128708
Iteration 20/25 | Loss: 0.00129081
Iteration 21/25 | Loss: 0.00128910
Iteration 22/25 | Loss: 0.00129021
Iteration 23/25 | Loss: 0.00128499
Iteration 24/25 | Loss: 0.00128577
Iteration 25/25 | Loss: 0.00128435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44580734
Iteration 2/25 | Loss: 0.00142462
Iteration 3/25 | Loss: 0.00120057
Iteration 4/25 | Loss: 0.00120057
Iteration 5/25 | Loss: 0.00120057
Iteration 6/25 | Loss: 0.00120057
Iteration 7/25 | Loss: 0.00120057
Iteration 8/25 | Loss: 0.00119075
Iteration 9/25 | Loss: 0.00118664
Iteration 10/25 | Loss: 0.00118664
Iteration 11/25 | Loss: 0.00118664
Iteration 12/25 | Loss: 0.00118664
Iteration 13/25 | Loss: 0.00118664
Iteration 14/25 | Loss: 0.00118664
Iteration 15/25 | Loss: 0.00118664
Iteration 16/25 | Loss: 0.00118664
Iteration 17/25 | Loss: 0.00118664
Iteration 18/25 | Loss: 0.00118664
Iteration 19/25 | Loss: 0.00118664
Iteration 20/25 | Loss: 0.00118664
Iteration 21/25 | Loss: 0.00118664
Iteration 22/25 | Loss: 0.00118664
Iteration 23/25 | Loss: 0.00118664
Iteration 24/25 | Loss: 0.00118664
Iteration 25/25 | Loss: 0.00118664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118664
Iteration 2/1000 | Loss: 0.00068962
Iteration 3/1000 | Loss: 0.00090639
Iteration 4/1000 | Loss: 0.00014185
Iteration 5/1000 | Loss: 0.00043024
Iteration 6/1000 | Loss: 0.00037945
Iteration 7/1000 | Loss: 0.00011998
Iteration 8/1000 | Loss: 0.00045639
Iteration 9/1000 | Loss: 0.00068453
Iteration 10/1000 | Loss: 0.00021114
Iteration 11/1000 | Loss: 0.00012271
Iteration 12/1000 | Loss: 0.00012659
Iteration 13/1000 | Loss: 0.00052224
Iteration 14/1000 | Loss: 0.00030167
Iteration 15/1000 | Loss: 0.00017594
Iteration 16/1000 | Loss: 0.00015198
Iteration 17/1000 | Loss: 0.00012348
Iteration 18/1000 | Loss: 0.00006172
Iteration 19/1000 | Loss: 0.00006814
Iteration 20/1000 | Loss: 0.00012201
Iteration 21/1000 | Loss: 0.00010397
Iteration 22/1000 | Loss: 0.00055063
Iteration 23/1000 | Loss: 0.00007271
Iteration 24/1000 | Loss: 0.00014203
Iteration 25/1000 | Loss: 0.00062738
Iteration 26/1000 | Loss: 0.00010325
Iteration 27/1000 | Loss: 0.00008858
Iteration 28/1000 | Loss: 0.00007128
Iteration 29/1000 | Loss: 0.00021136
Iteration 30/1000 | Loss: 0.00028467
Iteration 31/1000 | Loss: 0.00009752
Iteration 32/1000 | Loss: 0.00008927
Iteration 33/1000 | Loss: 0.00006429
Iteration 34/1000 | Loss: 0.00008335
Iteration 35/1000 | Loss: 0.00047659
Iteration 36/1000 | Loss: 0.00018441
Iteration 37/1000 | Loss: 0.00061029
Iteration 38/1000 | Loss: 0.00028942
Iteration 39/1000 | Loss: 0.00058353
Iteration 40/1000 | Loss: 0.00057335
Iteration 41/1000 | Loss: 0.00031685
Iteration 42/1000 | Loss: 0.00016437
Iteration 43/1000 | Loss: 0.00011099
Iteration 44/1000 | Loss: 0.00019129
Iteration 45/1000 | Loss: 0.00016732
Iteration 46/1000 | Loss: 0.00017535
Iteration 47/1000 | Loss: 0.00016523
Iteration 48/1000 | Loss: 0.00016364
Iteration 49/1000 | Loss: 0.00017252
Iteration 50/1000 | Loss: 0.00016192
Iteration 51/1000 | Loss: 0.00024687
Iteration 52/1000 | Loss: 0.00015708
Iteration 53/1000 | Loss: 0.00018594
Iteration 54/1000 | Loss: 0.00028341
Iteration 55/1000 | Loss: 0.00011147
Iteration 56/1000 | Loss: 0.00016041
Iteration 57/1000 | Loss: 0.00006551
Iteration 58/1000 | Loss: 0.00019847
Iteration 59/1000 | Loss: 0.00020722
Iteration 60/1000 | Loss: 0.00027448
Iteration 61/1000 | Loss: 0.00008009
Iteration 62/1000 | Loss: 0.00005041
Iteration 63/1000 | Loss: 0.00009259
Iteration 64/1000 | Loss: 0.00006051
Iteration 65/1000 | Loss: 0.00006347
Iteration 66/1000 | Loss: 0.00006370
Iteration 67/1000 | Loss: 0.00018363
Iteration 68/1000 | Loss: 0.00010710
Iteration 69/1000 | Loss: 0.00009014
Iteration 70/1000 | Loss: 0.00004709
Iteration 71/1000 | Loss: 0.00012476
Iteration 72/1000 | Loss: 0.00006648
Iteration 73/1000 | Loss: 0.00006618
Iteration 74/1000 | Loss: 0.00005861
Iteration 75/1000 | Loss: 0.00005836
Iteration 76/1000 | Loss: 0.00019367
Iteration 77/1000 | Loss: 0.00014003
Iteration 78/1000 | Loss: 0.00021611
Iteration 79/1000 | Loss: 0.00011104
Iteration 80/1000 | Loss: 0.00045311
Iteration 81/1000 | Loss: 0.00006286
Iteration 82/1000 | Loss: 0.00007157
Iteration 83/1000 | Loss: 0.00006271
Iteration 84/1000 | Loss: 0.00008199
Iteration 85/1000 | Loss: 0.00005954
Iteration 86/1000 | Loss: 0.00007650
Iteration 87/1000 | Loss: 0.00006119
Iteration 88/1000 | Loss: 0.00006235
Iteration 89/1000 | Loss: 0.00011916
Iteration 90/1000 | Loss: 0.00009702
Iteration 91/1000 | Loss: 0.00005299
Iteration 92/1000 | Loss: 0.00006609
Iteration 93/1000 | Loss: 0.00005540
Iteration 94/1000 | Loss: 0.00005563
Iteration 95/1000 | Loss: 0.00005741
Iteration 96/1000 | Loss: 0.00005705
Iteration 97/1000 | Loss: 0.00006721
Iteration 98/1000 | Loss: 0.00005580
Iteration 99/1000 | Loss: 0.00005497
Iteration 100/1000 | Loss: 0.00005518
Iteration 101/1000 | Loss: 0.00005348
Iteration 102/1000 | Loss: 0.00005897
Iteration 103/1000 | Loss: 0.00004250
Iteration 104/1000 | Loss: 0.00008822
Iteration 105/1000 | Loss: 0.00005321
Iteration 106/1000 | Loss: 0.00006893
Iteration 107/1000 | Loss: 0.00004943
Iteration 108/1000 | Loss: 0.00005944
Iteration 109/1000 | Loss: 0.00005597
Iteration 110/1000 | Loss: 0.00005441
Iteration 111/1000 | Loss: 0.00007926
Iteration 112/1000 | Loss: 0.00007914
Iteration 113/1000 | Loss: 0.00011866
Iteration 114/1000 | Loss: 0.00005705
Iteration 115/1000 | Loss: 0.00004645
Iteration 116/1000 | Loss: 0.00006708
Iteration 117/1000 | Loss: 0.00004647
Iteration 118/1000 | Loss: 0.00004917
Iteration 119/1000 | Loss: 0.00005431
Iteration 120/1000 | Loss: 0.00006137
Iteration 121/1000 | Loss: 0.00008764
Iteration 122/1000 | Loss: 0.00005761
Iteration 123/1000 | Loss: 0.00006067
Iteration 124/1000 | Loss: 0.00007149
Iteration 125/1000 | Loss: 0.00005654
Iteration 126/1000 | Loss: 0.00006119
Iteration 127/1000 | Loss: 0.00005632
Iteration 128/1000 | Loss: 0.00005286
Iteration 129/1000 | Loss: 0.00005441
Iteration 130/1000 | Loss: 0.00007462
Iteration 131/1000 | Loss: 0.00008591
Iteration 132/1000 | Loss: 0.00007061
Iteration 133/1000 | Loss: 0.00006083
Iteration 134/1000 | Loss: 0.00008164
Iteration 135/1000 | Loss: 0.00007622
Iteration 136/1000 | Loss: 0.00017907
Iteration 137/1000 | Loss: 0.00013528
Iteration 138/1000 | Loss: 0.00005731
Iteration 139/1000 | Loss: 0.00007740
Iteration 140/1000 | Loss: 0.00013286
Iteration 141/1000 | Loss: 0.00007702
Iteration 142/1000 | Loss: 0.00009014
Iteration 143/1000 | Loss: 0.00007058
Iteration 144/1000 | Loss: 0.00016387
Iteration 145/1000 | Loss: 0.00010798
Iteration 146/1000 | Loss: 0.00008560
Iteration 147/1000 | Loss: 0.00005582
Iteration 148/1000 | Loss: 0.00005895
Iteration 149/1000 | Loss: 0.00006274
Iteration 150/1000 | Loss: 0.00005614
Iteration 151/1000 | Loss: 0.00005475
Iteration 152/1000 | Loss: 0.00005837
Iteration 153/1000 | Loss: 0.00006022
Iteration 154/1000 | Loss: 0.00005351
Iteration 155/1000 | Loss: 0.00005355
Iteration 156/1000 | Loss: 0.00006359
Iteration 157/1000 | Loss: 0.00009491
Iteration 158/1000 | Loss: 0.00006767
Iteration 159/1000 | Loss: 0.00004961
Iteration 160/1000 | Loss: 0.00005165
Iteration 161/1000 | Loss: 0.00006349
Iteration 162/1000 | Loss: 0.00005360
Iteration 163/1000 | Loss: 0.00005693
Iteration 164/1000 | Loss: 0.00005408
Iteration 165/1000 | Loss: 0.00006165
Iteration 166/1000 | Loss: 0.00006446
Iteration 167/1000 | Loss: 0.00005750
Iteration 168/1000 | Loss: 0.00004631
Iteration 169/1000 | Loss: 0.00005496
Iteration 170/1000 | Loss: 0.00004171
Iteration 171/1000 | Loss: 0.00004138
Iteration 172/1000 | Loss: 0.00005237
Iteration 173/1000 | Loss: 0.00005507
Iteration 174/1000 | Loss: 0.00005566
Iteration 175/1000 | Loss: 0.00005110
Iteration 176/1000 | Loss: 0.00005531
Iteration 177/1000 | Loss: 0.00006110
Iteration 178/1000 | Loss: 0.00005726
Iteration 179/1000 | Loss: 0.00005538
Iteration 180/1000 | Loss: 0.00008910
Iteration 181/1000 | Loss: 0.00006399
Iteration 182/1000 | Loss: 0.00004144
Iteration 183/1000 | Loss: 0.00006738
Iteration 184/1000 | Loss: 0.00005715
Iteration 185/1000 | Loss: 0.00005477
Iteration 186/1000 | Loss: 0.00005880
Iteration 187/1000 | Loss: 0.00005486
Iteration 188/1000 | Loss: 0.00006473
Iteration 189/1000 | Loss: 0.00005500
Iteration 190/1000 | Loss: 0.00005728
Iteration 191/1000 | Loss: 0.00005278
Iteration 192/1000 | Loss: 0.00005673
Iteration 193/1000 | Loss: 0.00004557
Iteration 194/1000 | Loss: 0.00004824
Iteration 195/1000 | Loss: 0.00005491
Iteration 196/1000 | Loss: 0.00005740
Iteration 197/1000 | Loss: 0.00007705
Iteration 198/1000 | Loss: 0.00005880
Iteration 199/1000 | Loss: 0.00005422
Iteration 200/1000 | Loss: 0.00005956
Iteration 201/1000 | Loss: 0.00006112
Iteration 202/1000 | Loss: 0.00013863
Iteration 203/1000 | Loss: 0.00009554
Iteration 204/1000 | Loss: 0.00007171
Iteration 205/1000 | Loss: 0.00010359
Iteration 206/1000 | Loss: 0.00008381
Iteration 207/1000 | Loss: 0.00012276
Iteration 208/1000 | Loss: 0.00011540
Iteration 209/1000 | Loss: 0.00014890
Iteration 210/1000 | Loss: 0.00015303
Iteration 211/1000 | Loss: 0.00007522
Iteration 212/1000 | Loss: 0.00006045
Iteration 213/1000 | Loss: 0.00007287
Iteration 214/1000 | Loss: 0.00006902
Iteration 215/1000 | Loss: 0.00006394
Iteration 216/1000 | Loss: 0.00005935
Iteration 217/1000 | Loss: 0.00005609
Iteration 218/1000 | Loss: 0.00005482
Iteration 219/1000 | Loss: 0.00014094
Iteration 220/1000 | Loss: 0.00010820
Iteration 221/1000 | Loss: 0.00011542
Iteration 222/1000 | Loss: 0.00012691
Iteration 223/1000 | Loss: 0.00011731
Iteration 224/1000 | Loss: 0.00009757
Iteration 225/1000 | Loss: 0.00016774
Iteration 226/1000 | Loss: 0.00005408
Iteration 227/1000 | Loss: 0.00008630
Iteration 228/1000 | Loss: 0.00004752
Iteration 229/1000 | Loss: 0.00004771
Iteration 230/1000 | Loss: 0.00005499
Iteration 231/1000 | Loss: 0.00006091
Iteration 232/1000 | Loss: 0.00005498
Iteration 233/1000 | Loss: 0.00005594
Iteration 234/1000 | Loss: 0.00005460
Iteration 235/1000 | Loss: 0.00005620
Iteration 236/1000 | Loss: 0.00005416
Iteration 237/1000 | Loss: 0.00024295
Iteration 238/1000 | Loss: 0.00005714
Iteration 239/1000 | Loss: 0.00005612
Iteration 240/1000 | Loss: 0.00005343
Iteration 241/1000 | Loss: 0.00004468
Iteration 242/1000 | Loss: 0.00016765
Iteration 243/1000 | Loss: 0.00008359
Iteration 244/1000 | Loss: 0.00005247
Iteration 245/1000 | Loss: 0.00006112
Iteration 246/1000 | Loss: 0.00004315
Iteration 247/1000 | Loss: 0.00004366
Iteration 248/1000 | Loss: 0.00005765
Iteration 249/1000 | Loss: 0.00005475
Iteration 250/1000 | Loss: 0.00007663
Iteration 251/1000 | Loss: 0.00005644
Iteration 252/1000 | Loss: 0.00005945
Iteration 253/1000 | Loss: 0.00005455
Iteration 254/1000 | Loss: 0.00004388
Iteration 255/1000 | Loss: 0.00004618
Iteration 256/1000 | Loss: 0.00004138
Iteration 257/1000 | Loss: 0.00009762
Iteration 258/1000 | Loss: 0.00007808
Iteration 259/1000 | Loss: 0.00005664
Iteration 260/1000 | Loss: 0.00007027
Iteration 261/1000 | Loss: 0.00005401
Iteration 262/1000 | Loss: 0.00004243
Iteration 263/1000 | Loss: 0.00003807
Iteration 264/1000 | Loss: 0.00005189
Iteration 265/1000 | Loss: 0.00005231
Iteration 266/1000 | Loss: 0.00005198
Iteration 267/1000 | Loss: 0.00005830
Iteration 268/1000 | Loss: 0.00004558
Iteration 269/1000 | Loss: 0.00005227
Iteration 270/1000 | Loss: 0.00005083
Iteration 271/1000 | Loss: 0.00005431
Iteration 272/1000 | Loss: 0.00005379
Iteration 273/1000 | Loss: 0.00005213
Iteration 274/1000 | Loss: 0.00006283
Iteration 275/1000 | Loss: 0.00005063
Iteration 276/1000 | Loss: 0.00006268
Iteration 277/1000 | Loss: 0.00007328
Iteration 278/1000 | Loss: 0.00005419
Iteration 279/1000 | Loss: 0.00004677
Iteration 280/1000 | Loss: 0.00005278
Iteration 281/1000 | Loss: 0.00005176
Iteration 282/1000 | Loss: 0.00005349
Iteration 283/1000 | Loss: 0.00006470
Iteration 284/1000 | Loss: 0.00005087
Iteration 285/1000 | Loss: 0.00004132
Iteration 286/1000 | Loss: 0.00005088
Iteration 287/1000 | Loss: 0.00005577
Iteration 288/1000 | Loss: 0.00004578
Iteration 289/1000 | Loss: 0.00005213
Iteration 290/1000 | Loss: 0.00005111
Iteration 291/1000 | Loss: 0.00005378
Iteration 292/1000 | Loss: 0.00004264
Iteration 293/1000 | Loss: 0.00006547
Iteration 294/1000 | Loss: 0.00004832
Iteration 295/1000 | Loss: 0.00004571
Iteration 296/1000 | Loss: 0.00004665
Iteration 297/1000 | Loss: 0.00004979
Iteration 298/1000 | Loss: 0.00004647
Iteration 299/1000 | Loss: 0.00005272
Iteration 300/1000 | Loss: 0.00004549
Iteration 301/1000 | Loss: 0.00004896
Iteration 302/1000 | Loss: 0.00037779
Iteration 303/1000 | Loss: 0.00064927
Iteration 304/1000 | Loss: 0.00012382
Iteration 305/1000 | Loss: 0.00005625
Iteration 306/1000 | Loss: 0.00005093
Iteration 307/1000 | Loss: 0.00005307
Iteration 308/1000 | Loss: 0.00005043
Iteration 309/1000 | Loss: 0.00005305
Iteration 310/1000 | Loss: 0.00005372
Iteration 311/1000 | Loss: 0.00007053
Iteration 312/1000 | Loss: 0.00005174
Iteration 313/1000 | Loss: 0.00007625
Iteration 314/1000 | Loss: 0.00005651
Iteration 315/1000 | Loss: 0.00003782
Iteration 316/1000 | Loss: 0.00003751
Iteration 317/1000 | Loss: 0.00005086
Iteration 318/1000 | Loss: 0.00004711
Iteration 319/1000 | Loss: 0.00005006
Iteration 320/1000 | Loss: 0.00029537
Iteration 321/1000 | Loss: 0.00039147
Iteration 322/1000 | Loss: 0.00011740
Iteration 323/1000 | Loss: 0.00005404
Iteration 324/1000 | Loss: 0.00028362
Iteration 325/1000 | Loss: 0.00013531
Iteration 326/1000 | Loss: 0.00004437
Iteration 327/1000 | Loss: 0.00005369
Iteration 328/1000 | Loss: 0.00004026
Iteration 329/1000 | Loss: 0.00004567
Iteration 330/1000 | Loss: 0.00036810
Iteration 331/1000 | Loss: 0.00063182
Iteration 332/1000 | Loss: 0.00049861
Iteration 333/1000 | Loss: 0.00062396
Iteration 334/1000 | Loss: 0.00063011
Iteration 335/1000 | Loss: 0.00029625
Iteration 336/1000 | Loss: 0.00016909
Iteration 337/1000 | Loss: 0.00012286
Iteration 338/1000 | Loss: 0.00011468
Iteration 339/1000 | Loss: 0.00004505
Iteration 340/1000 | Loss: 0.00013071
Iteration 341/1000 | Loss: 0.00006299
Iteration 342/1000 | Loss: 0.00003950
Iteration 343/1000 | Loss: 0.00003991
Iteration 344/1000 | Loss: 0.00003718
Iteration 345/1000 | Loss: 0.00005461
Iteration 346/1000 | Loss: 0.00003674
Iteration 347/1000 | Loss: 0.00003649
Iteration 348/1000 | Loss: 0.00015525
Iteration 349/1000 | Loss: 0.00007837
Iteration 350/1000 | Loss: 0.00003765
Iteration 351/1000 | Loss: 0.00012350
Iteration 352/1000 | Loss: 0.00008671
Iteration 353/1000 | Loss: 0.00012675
Iteration 354/1000 | Loss: 0.00009049
Iteration 355/1000 | Loss: 0.00008766
Iteration 356/1000 | Loss: 0.00012224
Iteration 357/1000 | Loss: 0.00009579
Iteration 358/1000 | Loss: 0.00004097
Iteration 359/1000 | Loss: 0.00012525
Iteration 360/1000 | Loss: 0.00011987
Iteration 361/1000 | Loss: 0.00038449
Iteration 362/1000 | Loss: 0.00036619
Iteration 363/1000 | Loss: 0.00014565
Iteration 364/1000 | Loss: 0.00022698
Iteration 365/1000 | Loss: 0.00014348
Iteration 366/1000 | Loss: 0.00019257
Iteration 367/1000 | Loss: 0.00013856
Iteration 368/1000 | Loss: 0.00003837
Iteration 369/1000 | Loss: 0.00003649
Iteration 370/1000 | Loss: 0.00004361
Iteration 371/1000 | Loss: 0.00012495
Iteration 372/1000 | Loss: 0.00007155
Iteration 373/1000 | Loss: 0.00003530
Iteration 374/1000 | Loss: 0.00003479
Iteration 375/1000 | Loss: 0.00006958
Iteration 376/1000 | Loss: 0.00039855
Iteration 377/1000 | Loss: 0.00039180
Iteration 378/1000 | Loss: 0.00043938
Iteration 379/1000 | Loss: 0.00042389
Iteration 380/1000 | Loss: 0.00033805
Iteration 381/1000 | Loss: 0.00025462
Iteration 382/1000 | Loss: 0.00023404
Iteration 383/1000 | Loss: 0.00015923
Iteration 384/1000 | Loss: 0.00018902
Iteration 385/1000 | Loss: 0.00013242
Iteration 386/1000 | Loss: 0.00018108
Iteration 387/1000 | Loss: 0.00010304
Iteration 388/1000 | Loss: 0.00017420
Iteration 389/1000 | Loss: 0.00011153
Iteration 390/1000 | Loss: 0.00015517
Iteration 391/1000 | Loss: 0.00012125
Iteration 392/1000 | Loss: 0.00014711
Iteration 393/1000 | Loss: 0.00009736
Iteration 394/1000 | Loss: 0.00017310
Iteration 395/1000 | Loss: 0.00031266
Iteration 396/1000 | Loss: 0.00021350
Iteration 397/1000 | Loss: 0.00055018
Iteration 398/1000 | Loss: 0.00023115
Iteration 399/1000 | Loss: 0.00026275
Iteration 400/1000 | Loss: 0.00031103
Iteration 401/1000 | Loss: 0.00007591
Iteration 402/1000 | Loss: 0.00029568
Iteration 403/1000 | Loss: 0.00056190
Iteration 404/1000 | Loss: 0.00018362
Iteration 405/1000 | Loss: 0.00021791
Iteration 406/1000 | Loss: 0.00004534
Iteration 407/1000 | Loss: 0.00014612
Iteration 408/1000 | Loss: 0.00003719
Iteration 409/1000 | Loss: 0.00004906
Iteration 410/1000 | Loss: 0.00003500
Iteration 411/1000 | Loss: 0.00004234
Iteration 412/1000 | Loss: 0.00003429
Iteration 413/1000 | Loss: 0.00003403
Iteration 414/1000 | Loss: 0.00004428
Iteration 415/1000 | Loss: 0.00008914
Iteration 416/1000 | Loss: 0.00005776
Iteration 417/1000 | Loss: 0.00004176
Iteration 418/1000 | Loss: 0.00003603
Iteration 419/1000 | Loss: 0.00006395
Iteration 420/1000 | Loss: 0.00006479
Iteration 421/1000 | Loss: 0.00010501
Iteration 422/1000 | Loss: 0.00007576
Iteration 423/1000 | Loss: 0.00007299
Iteration 424/1000 | Loss: 0.00003711
Iteration 425/1000 | Loss: 0.00003517
Iteration 426/1000 | Loss: 0.00003355
Iteration 427/1000 | Loss: 0.00003355
Iteration 428/1000 | Loss: 0.00003354
Iteration 429/1000 | Loss: 0.00003353
Iteration 430/1000 | Loss: 0.00003862
Iteration 431/1000 | Loss: 0.00003345
Iteration 432/1000 | Loss: 0.00003342
Iteration 433/1000 | Loss: 0.00005407
Iteration 434/1000 | Loss: 0.00003340
Iteration 435/1000 | Loss: 0.00003338
Iteration 436/1000 | Loss: 0.00003338
Iteration 437/1000 | Loss: 0.00003338
Iteration 438/1000 | Loss: 0.00003338
Iteration 439/1000 | Loss: 0.00003338
Iteration 440/1000 | Loss: 0.00003338
Iteration 441/1000 | Loss: 0.00003338
Iteration 442/1000 | Loss: 0.00003338
Iteration 443/1000 | Loss: 0.00003338
Iteration 444/1000 | Loss: 0.00003338
Iteration 445/1000 | Loss: 0.00003338
Iteration 446/1000 | Loss: 0.00003338
Iteration 447/1000 | Loss: 0.00003337
Iteration 448/1000 | Loss: 0.00003337
Iteration 449/1000 | Loss: 0.00003337
Iteration 450/1000 | Loss: 0.00003337
Iteration 451/1000 | Loss: 0.00003337
Iteration 452/1000 | Loss: 0.00003336
Iteration 453/1000 | Loss: 0.00003336
Iteration 454/1000 | Loss: 0.00003336
Iteration 455/1000 | Loss: 0.00003336
Iteration 456/1000 | Loss: 0.00003336
Iteration 457/1000 | Loss: 0.00003336
Iteration 458/1000 | Loss: 0.00003336
Iteration 459/1000 | Loss: 0.00004346
Iteration 460/1000 | Loss: 0.00003886
Iteration 461/1000 | Loss: 0.00004096
Iteration 462/1000 | Loss: 0.00003331
Iteration 463/1000 | Loss: 0.00003330
Iteration 464/1000 | Loss: 0.00003330
Iteration 465/1000 | Loss: 0.00003330
Iteration 466/1000 | Loss: 0.00003330
Iteration 467/1000 | Loss: 0.00003330
Iteration 468/1000 | Loss: 0.00003330
Iteration 469/1000 | Loss: 0.00003330
Iteration 470/1000 | Loss: 0.00003330
Iteration 471/1000 | Loss: 0.00003330
Iteration 472/1000 | Loss: 0.00003330
Iteration 473/1000 | Loss: 0.00003329
Iteration 474/1000 | Loss: 0.00003329
Iteration 475/1000 | Loss: 0.00003329
Iteration 476/1000 | Loss: 0.00003329
Iteration 477/1000 | Loss: 0.00003329
Iteration 478/1000 | Loss: 0.00003329
Iteration 479/1000 | Loss: 0.00003329
Iteration 480/1000 | Loss: 0.00003329
Iteration 481/1000 | Loss: 0.00003329
Iteration 482/1000 | Loss: 0.00003329
Iteration 483/1000 | Loss: 0.00003329
Iteration 484/1000 | Loss: 0.00003329
Iteration 485/1000 | Loss: 0.00003329
Iteration 486/1000 | Loss: 0.00003329
Iteration 487/1000 | Loss: 0.00003329
Iteration 488/1000 | Loss: 0.00003329
Iteration 489/1000 | Loss: 0.00003328
Iteration 490/1000 | Loss: 0.00003328
Iteration 491/1000 | Loss: 0.00003328
Iteration 492/1000 | Loss: 0.00003328
Iteration 493/1000 | Loss: 0.00003328
Iteration 494/1000 | Loss: 0.00003328
Iteration 495/1000 | Loss: 0.00003328
Iteration 496/1000 | Loss: 0.00003328
Iteration 497/1000 | Loss: 0.00003328
Iteration 498/1000 | Loss: 0.00003328
Iteration 499/1000 | Loss: 0.00003328
Iteration 500/1000 | Loss: 0.00003328
Iteration 501/1000 | Loss: 0.00003328
Iteration 502/1000 | Loss: 0.00003328
Iteration 503/1000 | Loss: 0.00003328
Iteration 504/1000 | Loss: 0.00003328
Iteration 505/1000 | Loss: 0.00003328
Iteration 506/1000 | Loss: 0.00003328
Iteration 507/1000 | Loss: 0.00003328
Iteration 508/1000 | Loss: 0.00003328
Iteration 509/1000 | Loss: 0.00003328
Iteration 510/1000 | Loss: 0.00003328
Iteration 511/1000 | Loss: 0.00003328
Iteration 512/1000 | Loss: 0.00003328
Iteration 513/1000 | Loss: 0.00003328
Iteration 514/1000 | Loss: 0.00003328
Iteration 515/1000 | Loss: 0.00003328
Iteration 516/1000 | Loss: 0.00003328
Iteration 517/1000 | Loss: 0.00003328
Iteration 518/1000 | Loss: 0.00003328
Iteration 519/1000 | Loss: 0.00003328
Iteration 520/1000 | Loss: 0.00003328
Iteration 521/1000 | Loss: 0.00003328
Iteration 522/1000 | Loss: 0.00003328
Iteration 523/1000 | Loss: 0.00003328
Iteration 524/1000 | Loss: 0.00003328
Iteration 525/1000 | Loss: 0.00003328
Iteration 526/1000 | Loss: 0.00003328
Iteration 527/1000 | Loss: 0.00003328
Iteration 528/1000 | Loss: 0.00003328
Iteration 529/1000 | Loss: 0.00003328
Iteration 530/1000 | Loss: 0.00003328
Iteration 531/1000 | Loss: 0.00003328
Iteration 532/1000 | Loss: 0.00003328
Iteration 533/1000 | Loss: 0.00003328
Iteration 534/1000 | Loss: 0.00003328
Iteration 535/1000 | Loss: 0.00003328
Iteration 536/1000 | Loss: 0.00003328
Iteration 537/1000 | Loss: 0.00003328
Iteration 538/1000 | Loss: 0.00003328
Iteration 539/1000 | Loss: 0.00003328
Iteration 540/1000 | Loss: 0.00003328
Iteration 541/1000 | Loss: 0.00003328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 541. Stopping optimization.
Last 5 losses: [3.328491220599972e-05, 3.328491220599972e-05, 3.328491220599972e-05, 3.328491220599972e-05, 3.328491220599972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.328491220599972e-05

Optimization complete. Final v2v error: 3.5631237030029297 mm

Highest mean error: 10.38586711883545 mm for frame 179

Lowest mean error: 2.927971601486206 mm for frame 32

Saving results

Total time: 741.2581782341003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426946
Iteration 2/25 | Loss: 0.00128478
Iteration 3/25 | Loss: 0.00122820
Iteration 4/25 | Loss: 0.00121392
Iteration 5/25 | Loss: 0.00121021
Iteration 6/25 | Loss: 0.00120938
Iteration 7/25 | Loss: 0.00120920
Iteration 8/25 | Loss: 0.00120920
Iteration 9/25 | Loss: 0.00120920
Iteration 10/25 | Loss: 0.00120919
Iteration 11/25 | Loss: 0.00120919
Iteration 12/25 | Loss: 0.00120919
Iteration 13/25 | Loss: 0.00120919
Iteration 14/25 | Loss: 0.00120919
Iteration 15/25 | Loss: 0.00120919
Iteration 16/25 | Loss: 0.00120919
Iteration 17/25 | Loss: 0.00120919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012091929093003273, 0.0012091929093003273, 0.0012091929093003273, 0.0012091929093003273, 0.0012091929093003273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012091929093003273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60628808
Iteration 2/25 | Loss: 0.00075421
Iteration 3/25 | Loss: 0.00075421
Iteration 4/25 | Loss: 0.00075421
Iteration 5/25 | Loss: 0.00075421
Iteration 6/25 | Loss: 0.00075421
Iteration 7/25 | Loss: 0.00075421
Iteration 8/25 | Loss: 0.00075421
Iteration 9/25 | Loss: 0.00075421
Iteration 10/25 | Loss: 0.00075421
Iteration 11/25 | Loss: 0.00075421
Iteration 12/25 | Loss: 0.00075421
Iteration 13/25 | Loss: 0.00075421
Iteration 14/25 | Loss: 0.00075421
Iteration 15/25 | Loss: 0.00075421
Iteration 16/25 | Loss: 0.00075421
Iteration 17/25 | Loss: 0.00075421
Iteration 18/25 | Loss: 0.00075421
Iteration 19/25 | Loss: 0.00075421
Iteration 20/25 | Loss: 0.00075421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007542052771896124, 0.0007542052771896124, 0.0007542052771896124, 0.0007542052771896124, 0.0007542052771896124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007542052771896124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075421
Iteration 2/1000 | Loss: 0.00002998
Iteration 3/1000 | Loss: 0.00001908
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001504
Iteration 12/1000 | Loss: 0.00001480
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001461
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001446
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001443
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001441
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001434
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001434
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001434
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001432
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001431
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001428
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001427
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001423
Iteration 77/1000 | Loss: 0.00001423
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001420
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001417
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001416
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001415
Iteration 95/1000 | Loss: 0.00001415
Iteration 96/1000 | Loss: 0.00001415
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001414
Iteration 101/1000 | Loss: 0.00001414
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001413
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001412
Iteration 108/1000 | Loss: 0.00001412
Iteration 109/1000 | Loss: 0.00001412
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001412
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001409
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001409
Iteration 137/1000 | Loss: 0.00001408
Iteration 138/1000 | Loss: 0.00001408
Iteration 139/1000 | Loss: 0.00001408
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001406
Iteration 150/1000 | Loss: 0.00001406
Iteration 151/1000 | Loss: 0.00001406
Iteration 152/1000 | Loss: 0.00001406
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Iteration 168/1000 | Loss: 0.00001406
Iteration 169/1000 | Loss: 0.00001406
Iteration 170/1000 | Loss: 0.00001406
Iteration 171/1000 | Loss: 0.00001406
Iteration 172/1000 | Loss: 0.00001406
Iteration 173/1000 | Loss: 0.00001406
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001406
Iteration 177/1000 | Loss: 0.00001406
Iteration 178/1000 | Loss: 0.00001406
Iteration 179/1000 | Loss: 0.00001406
Iteration 180/1000 | Loss: 0.00001406
Iteration 181/1000 | Loss: 0.00001406
Iteration 182/1000 | Loss: 0.00001406
Iteration 183/1000 | Loss: 0.00001406
Iteration 184/1000 | Loss: 0.00001406
Iteration 185/1000 | Loss: 0.00001406
Iteration 186/1000 | Loss: 0.00001406
Iteration 187/1000 | Loss: 0.00001406
Iteration 188/1000 | Loss: 0.00001406
Iteration 189/1000 | Loss: 0.00001406
Iteration 190/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.4057251974008977e-05, 1.4057251974008977e-05, 1.4057251974008977e-05, 1.4057251974008977e-05, 1.4057251974008977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4057251974008977e-05

Optimization complete. Final v2v error: 3.214961051940918 mm

Highest mean error: 3.6376538276672363 mm for frame 89

Lowest mean error: 3.0814332962036133 mm for frame 125

Saving results

Total time: 40.43049645423889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920585
Iteration 2/25 | Loss: 0.00368898
Iteration 3/25 | Loss: 0.00313941
Iteration 4/25 | Loss: 0.00257524
Iteration 5/25 | Loss: 0.00216230
Iteration 6/25 | Loss: 0.00206239
Iteration 7/25 | Loss: 0.00199515
Iteration 8/25 | Loss: 0.00183943
Iteration 9/25 | Loss: 0.00230824
Iteration 10/25 | Loss: 0.00135077
Iteration 11/25 | Loss: 0.00131080
Iteration 12/25 | Loss: 0.00130416
Iteration 13/25 | Loss: 0.00130298
Iteration 14/25 | Loss: 0.00130287
Iteration 15/25 | Loss: 0.00130287
Iteration 16/25 | Loss: 0.00130287
Iteration 17/25 | Loss: 0.00130287
Iteration 18/25 | Loss: 0.00130287
Iteration 19/25 | Loss: 0.00130287
Iteration 20/25 | Loss: 0.00130287
Iteration 21/25 | Loss: 0.00130287
Iteration 22/25 | Loss: 0.00130287
Iteration 23/25 | Loss: 0.00130287
Iteration 24/25 | Loss: 0.00130287
Iteration 25/25 | Loss: 0.00130287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41085374
Iteration 2/25 | Loss: 0.00141961
Iteration 3/25 | Loss: 0.00072224
Iteration 4/25 | Loss: 0.00072208
Iteration 5/25 | Loss: 0.00072208
Iteration 6/25 | Loss: 0.00072208
Iteration 7/25 | Loss: 0.00072208
Iteration 8/25 | Loss: 0.00072208
Iteration 9/25 | Loss: 0.00072208
Iteration 10/25 | Loss: 0.00072208
Iteration 11/25 | Loss: 0.00072208
Iteration 12/25 | Loss: 0.00072208
Iteration 13/25 | Loss: 0.00072208
Iteration 14/25 | Loss: 0.00072208
Iteration 15/25 | Loss: 0.00072208
Iteration 16/25 | Loss: 0.00072208
Iteration 17/25 | Loss: 0.00072208
Iteration 18/25 | Loss: 0.00072208
Iteration 19/25 | Loss: 0.00072208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007220751140266657, 0.0007220751140266657, 0.0007220751140266657, 0.0007220751140266657, 0.0007220751140266657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007220751140266657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072208
Iteration 2/1000 | Loss: 0.00020748
Iteration 3/1000 | Loss: 0.00003883
Iteration 4/1000 | Loss: 0.00057019
Iteration 5/1000 | Loss: 0.00062206
Iteration 6/1000 | Loss: 0.00003934
Iteration 7/1000 | Loss: 0.00003176
Iteration 8/1000 | Loss: 0.00002926
Iteration 9/1000 | Loss: 0.00002802
Iteration 10/1000 | Loss: 0.00002676
Iteration 11/1000 | Loss: 0.00002571
Iteration 12/1000 | Loss: 0.00002503
Iteration 13/1000 | Loss: 0.00002439
Iteration 14/1000 | Loss: 0.00002406
Iteration 15/1000 | Loss: 0.00002365
Iteration 16/1000 | Loss: 0.00002334
Iteration 17/1000 | Loss: 0.00002309
Iteration 18/1000 | Loss: 0.00002307
Iteration 19/1000 | Loss: 0.00002289
Iteration 20/1000 | Loss: 0.00033684
Iteration 21/1000 | Loss: 0.00012045
Iteration 22/1000 | Loss: 0.00002313
Iteration 23/1000 | Loss: 0.00033558
Iteration 24/1000 | Loss: 0.00011369
Iteration 25/1000 | Loss: 0.00124164
Iteration 26/1000 | Loss: 0.00157655
Iteration 27/1000 | Loss: 0.00050356
Iteration 28/1000 | Loss: 0.00034323
Iteration 29/1000 | Loss: 0.00145979
Iteration 30/1000 | Loss: 0.00035986
Iteration 31/1000 | Loss: 0.00011094
Iteration 32/1000 | Loss: 0.00003742
Iteration 33/1000 | Loss: 0.00029815
Iteration 34/1000 | Loss: 0.00005872
Iteration 35/1000 | Loss: 0.00003620
Iteration 36/1000 | Loss: 0.00002693
Iteration 37/1000 | Loss: 0.00022360
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00027105
Iteration 45/1000 | Loss: 0.00001685
Iteration 46/1000 | Loss: 0.00025778
Iteration 47/1000 | Loss: 0.00025380
Iteration 48/1000 | Loss: 0.00059577
Iteration 49/1000 | Loss: 0.00027624
Iteration 50/1000 | Loss: 0.00042320
Iteration 51/1000 | Loss: 0.00002995
Iteration 52/1000 | Loss: 0.00002165
Iteration 53/1000 | Loss: 0.00012078
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00029324
Iteration 56/1000 | Loss: 0.00002321
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00009274
Iteration 59/1000 | Loss: 0.00002752
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00005422
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001422
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001421
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001415
Iteration 76/1000 | Loss: 0.00001415
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001413
Iteration 81/1000 | Loss: 0.00001413
Iteration 82/1000 | Loss: 0.00001413
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001407
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4070503311813809e-05, 1.4070503311813809e-05, 1.4070503311813809e-05, 1.4070503311813809e-05, 1.4070503311813809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4070503311813809e-05

Optimization complete. Final v2v error: 3.1578633785247803 mm

Highest mean error: 4.356691837310791 mm for frame 4

Lowest mean error: 3.0683364868164062 mm for frame 164

Saving results

Total time: 120.47568321228027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047280
Iteration 2/25 | Loss: 0.01047280
Iteration 3/25 | Loss: 0.00207584
Iteration 4/25 | Loss: 0.00146754
Iteration 5/25 | Loss: 0.00140119
Iteration 6/25 | Loss: 0.00136339
Iteration 7/25 | Loss: 0.00137293
Iteration 8/25 | Loss: 0.00133260
Iteration 9/25 | Loss: 0.00130327
Iteration 10/25 | Loss: 0.00130952
Iteration 11/25 | Loss: 0.00129647
Iteration 12/25 | Loss: 0.00129391
Iteration 13/25 | Loss: 0.00127618
Iteration 14/25 | Loss: 0.00127227
Iteration 15/25 | Loss: 0.00127066
Iteration 16/25 | Loss: 0.00127017
Iteration 17/25 | Loss: 0.00126990
Iteration 18/25 | Loss: 0.00126969
Iteration 19/25 | Loss: 0.00127264
Iteration 20/25 | Loss: 0.00127137
Iteration 21/25 | Loss: 0.00127930
Iteration 22/25 | Loss: 0.00126987
Iteration 23/25 | Loss: 0.00126760
Iteration 24/25 | Loss: 0.00127313
Iteration 25/25 | Loss: 0.00126732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60470760
Iteration 2/25 | Loss: 0.00105152
Iteration 3/25 | Loss: 0.00105152
Iteration 4/25 | Loss: 0.00105152
Iteration 5/25 | Loss: 0.00105152
Iteration 6/25 | Loss: 0.00105152
Iteration 7/25 | Loss: 0.00105152
Iteration 8/25 | Loss: 0.00105152
Iteration 9/25 | Loss: 0.00105152
Iteration 10/25 | Loss: 0.00105152
Iteration 11/25 | Loss: 0.00105152
Iteration 12/25 | Loss: 0.00105152
Iteration 13/25 | Loss: 0.00105152
Iteration 14/25 | Loss: 0.00105152
Iteration 15/25 | Loss: 0.00105152
Iteration 16/25 | Loss: 0.00105152
Iteration 17/25 | Loss: 0.00105152
Iteration 18/25 | Loss: 0.00105152
Iteration 19/25 | Loss: 0.00105152
Iteration 20/25 | Loss: 0.00105152
Iteration 21/25 | Loss: 0.00105152
Iteration 22/25 | Loss: 0.00105152
Iteration 23/25 | Loss: 0.00105152
Iteration 24/25 | Loss: 0.00105152
Iteration 25/25 | Loss: 0.00105152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105152
Iteration 2/1000 | Loss: 0.00004914
Iteration 3/1000 | Loss: 0.00002905
Iteration 4/1000 | Loss: 0.00002344
Iteration 5/1000 | Loss: 0.00023468
Iteration 6/1000 | Loss: 0.00141814
Iteration 7/1000 | Loss: 0.00069367
Iteration 8/1000 | Loss: 0.00009356
Iteration 9/1000 | Loss: 0.00002460
Iteration 10/1000 | Loss: 0.00002108
Iteration 11/1000 | Loss: 0.00002041
Iteration 12/1000 | Loss: 0.00018595
Iteration 13/1000 | Loss: 0.00012862
Iteration 14/1000 | Loss: 0.00004036
Iteration 15/1000 | Loss: 0.00002117
Iteration 16/1000 | Loss: 0.00001983
Iteration 17/1000 | Loss: 0.00007517
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001936
Iteration 20/1000 | Loss: 0.00001918
Iteration 21/1000 | Loss: 0.00001902
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001886
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001878
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001877
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00014303
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001865
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001864
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001863
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001863
Iteration 52/1000 | Loss: 0.00001863
Iteration 53/1000 | Loss: 0.00001863
Iteration 54/1000 | Loss: 0.00001862
Iteration 55/1000 | Loss: 0.00001862
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001861
Iteration 62/1000 | Loss: 0.00001861
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001857
Iteration 90/1000 | Loss: 0.00001857
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001857
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Iteration 100/1000 | Loss: 0.00001855
Iteration 101/1000 | Loss: 0.00001855
Iteration 102/1000 | Loss: 0.00001855
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001855
Iteration 106/1000 | Loss: 0.00001855
Iteration 107/1000 | Loss: 0.00001855
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001854
Iteration 112/1000 | Loss: 0.00001854
Iteration 113/1000 | Loss: 0.00001854
Iteration 114/1000 | Loss: 0.00001854
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001853
Iteration 118/1000 | Loss: 0.00001853
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001851
Iteration 135/1000 | Loss: 0.00001851
Iteration 136/1000 | Loss: 0.00001851
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001851
Iteration 139/1000 | Loss: 0.00001851
Iteration 140/1000 | Loss: 0.00001851
Iteration 141/1000 | Loss: 0.00001851
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.85059652721975e-05, 1.85059652721975e-05, 1.85059652721975e-05, 1.85059652721975e-05, 1.85059652721975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.85059652721975e-05

Optimization complete. Final v2v error: 3.681365728378296 mm

Highest mean error: 4.4544267654418945 mm for frame 98

Lowest mean error: 3.180741310119629 mm for frame 0

Saving results

Total time: 85.49612593650818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346778
Iteration 2/25 | Loss: 0.00131933
Iteration 3/25 | Loss: 0.00122759
Iteration 4/25 | Loss: 0.00121251
Iteration 5/25 | Loss: 0.00120706
Iteration 6/25 | Loss: 0.00120700
Iteration 7/25 | Loss: 0.00120700
Iteration 8/25 | Loss: 0.00120700
Iteration 9/25 | Loss: 0.00120700
Iteration 10/25 | Loss: 0.00120700
Iteration 11/25 | Loss: 0.00120700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012069966178387403, 0.0012069966178387403, 0.0012069966178387403, 0.0012069966178387403, 0.0012069966178387403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012069966178387403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.85699081
Iteration 2/25 | Loss: 0.00086531
Iteration 3/25 | Loss: 0.00086527
Iteration 4/25 | Loss: 0.00086527
Iteration 5/25 | Loss: 0.00086527
Iteration 6/25 | Loss: 0.00086527
Iteration 7/25 | Loss: 0.00086527
Iteration 8/25 | Loss: 0.00086527
Iteration 9/25 | Loss: 0.00086527
Iteration 10/25 | Loss: 0.00086527
Iteration 11/25 | Loss: 0.00086527
Iteration 12/25 | Loss: 0.00086527
Iteration 13/25 | Loss: 0.00086527
Iteration 14/25 | Loss: 0.00086527
Iteration 15/25 | Loss: 0.00086527
Iteration 16/25 | Loss: 0.00086527
Iteration 17/25 | Loss: 0.00086527
Iteration 18/25 | Loss: 0.00086527
Iteration 19/25 | Loss: 0.00086527
Iteration 20/25 | Loss: 0.00086527
Iteration 21/25 | Loss: 0.00086527
Iteration 22/25 | Loss: 0.00086527
Iteration 23/25 | Loss: 0.00086527
Iteration 24/25 | Loss: 0.00086527
Iteration 25/25 | Loss: 0.00086527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086527
Iteration 2/1000 | Loss: 0.00003492
Iteration 3/1000 | Loss: 0.00002170
Iteration 4/1000 | Loss: 0.00001948
Iteration 5/1000 | Loss: 0.00001831
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001637
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001525
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001505
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001503
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001495
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001493
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001490
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001485
Iteration 45/1000 | Loss: 0.00001485
Iteration 46/1000 | Loss: 0.00001484
Iteration 47/1000 | Loss: 0.00001484
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001477
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001476
Iteration 72/1000 | Loss: 0.00001476
Iteration 73/1000 | Loss: 0.00001476
Iteration 74/1000 | Loss: 0.00001476
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001473
Iteration 89/1000 | Loss: 0.00001473
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001470
Iteration 112/1000 | Loss: 0.00001470
Iteration 113/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.4700926840305328e-05, 1.4700926840305328e-05, 1.4700926840305328e-05, 1.4700926840305328e-05, 1.4700926840305328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4700926840305328e-05

Optimization complete. Final v2v error: 3.28301739692688 mm

Highest mean error: 3.6865146160125732 mm for frame 153

Lowest mean error: 3.0115954875946045 mm for frame 222

Saving results

Total time: 39.55928993225098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065385
Iteration 2/25 | Loss: 0.00218465
Iteration 3/25 | Loss: 0.00157207
Iteration 4/25 | Loss: 0.00139264
Iteration 5/25 | Loss: 0.00137820
Iteration 6/25 | Loss: 0.00134186
Iteration 7/25 | Loss: 0.00130189
Iteration 8/25 | Loss: 0.00128335
Iteration 9/25 | Loss: 0.00126437
Iteration 10/25 | Loss: 0.00125942
Iteration 11/25 | Loss: 0.00124780
Iteration 12/25 | Loss: 0.00123816
Iteration 13/25 | Loss: 0.00123833
Iteration 14/25 | Loss: 0.00123887
Iteration 15/25 | Loss: 0.00123318
Iteration 16/25 | Loss: 0.00123282
Iteration 17/25 | Loss: 0.00123265
Iteration 18/25 | Loss: 0.00123262
Iteration 19/25 | Loss: 0.00123262
Iteration 20/25 | Loss: 0.00123262
Iteration 21/25 | Loss: 0.00123262
Iteration 22/25 | Loss: 0.00123261
Iteration 23/25 | Loss: 0.00123261
Iteration 24/25 | Loss: 0.00123261
Iteration 25/25 | Loss: 0.00123261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.59529829
Iteration 2/25 | Loss: 0.00079421
Iteration 3/25 | Loss: 0.00074584
Iteration 4/25 | Loss: 0.00074584
Iteration 5/25 | Loss: 0.00074584
Iteration 6/25 | Loss: 0.00074584
Iteration 7/25 | Loss: 0.00074584
Iteration 8/25 | Loss: 0.00074584
Iteration 9/25 | Loss: 0.00074583
Iteration 10/25 | Loss: 0.00074583
Iteration 11/25 | Loss: 0.00074583
Iteration 12/25 | Loss: 0.00074583
Iteration 13/25 | Loss: 0.00074583
Iteration 14/25 | Loss: 0.00074583
Iteration 15/25 | Loss: 0.00074583
Iteration 16/25 | Loss: 0.00074583
Iteration 17/25 | Loss: 0.00074583
Iteration 18/25 | Loss: 0.00074583
Iteration 19/25 | Loss: 0.00074583
Iteration 20/25 | Loss: 0.00074583
Iteration 21/25 | Loss: 0.00074583
Iteration 22/25 | Loss: 0.00074583
Iteration 23/25 | Loss: 0.00074583
Iteration 24/25 | Loss: 0.00074583
Iteration 25/25 | Loss: 0.00074583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074583
Iteration 2/1000 | Loss: 0.00006252
Iteration 3/1000 | Loss: 0.00002506
Iteration 4/1000 | Loss: 0.00008567
Iteration 5/1000 | Loss: 0.00003063
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00009580
Iteration 8/1000 | Loss: 0.00042823
Iteration 9/1000 | Loss: 0.00005565
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002753
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001748
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00003372
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001652
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001637
Iteration 23/1000 | Loss: 0.00001636
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001631
Iteration 27/1000 | Loss: 0.00001631
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001625
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001612
Iteration 103/1000 | Loss: 0.00001612
Iteration 104/1000 | Loss: 0.00001612
Iteration 105/1000 | Loss: 0.00001611
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001611
Iteration 122/1000 | Loss: 0.00001611
Iteration 123/1000 | Loss: 0.00001611
Iteration 124/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6111836885102093e-05, 1.6111836885102093e-05, 1.6111836885102093e-05, 1.6111836885102093e-05, 1.6111836885102093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6111836885102093e-05

Optimization complete. Final v2v error: 3.4098377227783203 mm

Highest mean error: 4.2168192863464355 mm for frame 143

Lowest mean error: 3.0722601413726807 mm for frame 94

Saving results

Total time: 76.97050189971924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403028
Iteration 2/25 | Loss: 0.00125759
Iteration 3/25 | Loss: 0.00119116
Iteration 4/25 | Loss: 0.00117993
Iteration 5/25 | Loss: 0.00117594
Iteration 6/25 | Loss: 0.00117573
Iteration 7/25 | Loss: 0.00117573
Iteration 8/25 | Loss: 0.00117573
Iteration 9/25 | Loss: 0.00117573
Iteration 10/25 | Loss: 0.00117573
Iteration 11/25 | Loss: 0.00117573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011757290922105312, 0.0011757290922105312, 0.0011757290922105312, 0.0011757290922105312, 0.0011757290922105312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011757290922105312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91641712
Iteration 2/25 | Loss: 0.00074809
Iteration 3/25 | Loss: 0.00074809
Iteration 4/25 | Loss: 0.00074809
Iteration 5/25 | Loss: 0.00074809
Iteration 6/25 | Loss: 0.00074809
Iteration 7/25 | Loss: 0.00074809
Iteration 8/25 | Loss: 0.00074809
Iteration 9/25 | Loss: 0.00074809
Iteration 10/25 | Loss: 0.00074809
Iteration 11/25 | Loss: 0.00074809
Iteration 12/25 | Loss: 0.00074809
Iteration 13/25 | Loss: 0.00074809
Iteration 14/25 | Loss: 0.00074809
Iteration 15/25 | Loss: 0.00074809
Iteration 16/25 | Loss: 0.00074809
Iteration 17/25 | Loss: 0.00074809
Iteration 18/25 | Loss: 0.00074809
Iteration 19/25 | Loss: 0.00074809
Iteration 20/25 | Loss: 0.00074809
Iteration 21/25 | Loss: 0.00074809
Iteration 22/25 | Loss: 0.00074809
Iteration 23/25 | Loss: 0.00074809
Iteration 24/25 | Loss: 0.00074809
Iteration 25/25 | Loss: 0.00074809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074809
Iteration 2/1000 | Loss: 0.00002300
Iteration 3/1000 | Loss: 0.00001584
Iteration 4/1000 | Loss: 0.00001420
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001255
Iteration 8/1000 | Loss: 0.00001226
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001190
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001172
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001151
Iteration 40/1000 | Loss: 0.00001150
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001144
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001138
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001135
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001133
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001132
Iteration 65/1000 | Loss: 0.00001132
Iteration 66/1000 | Loss: 0.00001132
Iteration 67/1000 | Loss: 0.00001131
Iteration 68/1000 | Loss: 0.00001131
Iteration 69/1000 | Loss: 0.00001131
Iteration 70/1000 | Loss: 0.00001131
Iteration 71/1000 | Loss: 0.00001131
Iteration 72/1000 | Loss: 0.00001130
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001125
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001122
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001120
Iteration 103/1000 | Loss: 0.00001120
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001118
Iteration 113/1000 | Loss: 0.00001118
Iteration 114/1000 | Loss: 0.00001118
Iteration 115/1000 | Loss: 0.00001118
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001116
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001116
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001115
Iteration 134/1000 | Loss: 0.00001115
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001115
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001115
Iteration 144/1000 | Loss: 0.00001115
Iteration 145/1000 | Loss: 0.00001115
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001115
Iteration 148/1000 | Loss: 0.00001115
Iteration 149/1000 | Loss: 0.00001115
Iteration 150/1000 | Loss: 0.00001115
Iteration 151/1000 | Loss: 0.00001115
Iteration 152/1000 | Loss: 0.00001115
Iteration 153/1000 | Loss: 0.00001115
Iteration 154/1000 | Loss: 0.00001115
Iteration 155/1000 | Loss: 0.00001115
Iteration 156/1000 | Loss: 0.00001115
Iteration 157/1000 | Loss: 0.00001115
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001115
Iteration 163/1000 | Loss: 0.00001115
Iteration 164/1000 | Loss: 0.00001115
Iteration 165/1000 | Loss: 0.00001115
Iteration 166/1000 | Loss: 0.00001115
Iteration 167/1000 | Loss: 0.00001115
Iteration 168/1000 | Loss: 0.00001115
Iteration 169/1000 | Loss: 0.00001115
Iteration 170/1000 | Loss: 0.00001115
Iteration 171/1000 | Loss: 0.00001115
Iteration 172/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.1146481483592652e-05, 1.1146481483592652e-05, 1.1146481483592652e-05, 1.1146481483592652e-05, 1.1146481483592652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1146481483592652e-05

Optimization complete. Final v2v error: 2.8796701431274414 mm

Highest mean error: 3.120328664779663 mm for frame 115

Lowest mean error: 2.769625186920166 mm for frame 31

Saving results

Total time: 41.39296817779541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991057
Iteration 2/25 | Loss: 0.00798680
Iteration 3/25 | Loss: 0.00165930
Iteration 4/25 | Loss: 0.00137091
Iteration 5/25 | Loss: 0.00131081
Iteration 6/25 | Loss: 0.00129889
Iteration 7/25 | Loss: 0.00131000
Iteration 8/25 | Loss: 0.00128930
Iteration 9/25 | Loss: 0.00127838
Iteration 10/25 | Loss: 0.00127141
Iteration 11/25 | Loss: 0.00128149
Iteration 12/25 | Loss: 0.00127550
Iteration 13/25 | Loss: 0.00127115
Iteration 14/25 | Loss: 0.00126746
Iteration 15/25 | Loss: 0.00126956
Iteration 16/25 | Loss: 0.00126825
Iteration 17/25 | Loss: 0.00126704
Iteration 18/25 | Loss: 0.00126597
Iteration 19/25 | Loss: 0.00126551
Iteration 20/25 | Loss: 0.00126327
Iteration 21/25 | Loss: 0.00126280
Iteration 22/25 | Loss: 0.00126403
Iteration 23/25 | Loss: 0.00126467
Iteration 24/25 | Loss: 0.00126279
Iteration 25/25 | Loss: 0.00125873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51602161
Iteration 2/25 | Loss: 0.00084803
Iteration 3/25 | Loss: 0.00084803
Iteration 4/25 | Loss: 0.00084803
Iteration 5/25 | Loss: 0.00084803
Iteration 6/25 | Loss: 0.00084803
Iteration 7/25 | Loss: 0.00084803
Iteration 8/25 | Loss: 0.00084803
Iteration 9/25 | Loss: 0.00084803
Iteration 10/25 | Loss: 0.00084803
Iteration 11/25 | Loss: 0.00084803
Iteration 12/25 | Loss: 0.00084803
Iteration 13/25 | Loss: 0.00084803
Iteration 14/25 | Loss: 0.00084803
Iteration 15/25 | Loss: 0.00084803
Iteration 16/25 | Loss: 0.00084803
Iteration 17/25 | Loss: 0.00084803
Iteration 18/25 | Loss: 0.00084803
Iteration 19/25 | Loss: 0.00084802
Iteration 20/25 | Loss: 0.00084802
Iteration 21/25 | Loss: 0.00084802
Iteration 22/25 | Loss: 0.00084802
Iteration 23/25 | Loss: 0.00084802
Iteration 24/25 | Loss: 0.00084802
Iteration 25/25 | Loss: 0.00084802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084802
Iteration 2/1000 | Loss: 0.00003352
Iteration 3/1000 | Loss: 0.00002284
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001926
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001763
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001736
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001709
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001701
Iteration 54/1000 | Loss: 0.00001701
Iteration 55/1000 | Loss: 0.00001701
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001697
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001690
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.6896583474590443e-05, 1.6896583474590443e-05, 1.6896583474590443e-05, 1.6896583474590443e-05, 1.6896583474590443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6896583474590443e-05

Optimization complete. Final v2v error: 3.436915397644043 mm

Highest mean error: 5.781890392303467 mm for frame 186

Lowest mean error: 3.0875728130340576 mm for frame 16

Saving results

Total time: 85.04630613327026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730732
Iteration 2/25 | Loss: 0.00193710
Iteration 3/25 | Loss: 0.00162761
Iteration 4/25 | Loss: 0.00154700
Iteration 5/25 | Loss: 0.00140306
Iteration 6/25 | Loss: 0.00129572
Iteration 7/25 | Loss: 0.00123557
Iteration 8/25 | Loss: 0.00122653
Iteration 9/25 | Loss: 0.00121964
Iteration 10/25 | Loss: 0.00121585
Iteration 11/25 | Loss: 0.00121482
Iteration 12/25 | Loss: 0.00121258
Iteration 13/25 | Loss: 0.00121199
Iteration 14/25 | Loss: 0.00121195
Iteration 15/25 | Loss: 0.00121195
Iteration 16/25 | Loss: 0.00121195
Iteration 17/25 | Loss: 0.00121195
Iteration 18/25 | Loss: 0.00121195
Iteration 19/25 | Loss: 0.00121195
Iteration 20/25 | Loss: 0.00121195
Iteration 21/25 | Loss: 0.00121195
Iteration 22/25 | Loss: 0.00121195
Iteration 23/25 | Loss: 0.00121195
Iteration 24/25 | Loss: 0.00121195
Iteration 25/25 | Loss: 0.00121195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40666640
Iteration 2/25 | Loss: 0.00044825
Iteration 3/25 | Loss: 0.00044824
Iteration 4/25 | Loss: 0.00044824
Iteration 5/25 | Loss: 0.00044824
Iteration 6/25 | Loss: 0.00044823
Iteration 7/25 | Loss: 0.00044823
Iteration 8/25 | Loss: 0.00044823
Iteration 9/25 | Loss: 0.00044823
Iteration 10/25 | Loss: 0.00044823
Iteration 11/25 | Loss: 0.00044823
Iteration 12/25 | Loss: 0.00044823
Iteration 13/25 | Loss: 0.00044823
Iteration 14/25 | Loss: 0.00044823
Iteration 15/25 | Loss: 0.00044823
Iteration 16/25 | Loss: 0.00044823
Iteration 17/25 | Loss: 0.00044823
Iteration 18/25 | Loss: 0.00044823
Iteration 19/25 | Loss: 0.00044823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004482335352804512, 0.0004482335352804512, 0.0004482335352804512, 0.0004482335352804512, 0.0004482335352804512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004482335352804512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044823
Iteration 2/1000 | Loss: 0.00003385
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001882
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001652
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001557
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001547
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00001546
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001537
Iteration 27/1000 | Loss: 0.00001537
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001534
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001528
Iteration 54/1000 | Loss: 0.00001528
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001528
Iteration 57/1000 | Loss: 0.00001528
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001527
Iteration 60/1000 | Loss: 0.00001527
Iteration 61/1000 | Loss: 0.00001527
Iteration 62/1000 | Loss: 0.00001527
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001526
Iteration 66/1000 | Loss: 0.00001526
Iteration 67/1000 | Loss: 0.00001526
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001524
Iteration 99/1000 | Loss: 0.00001524
Iteration 100/1000 | Loss: 0.00001524
Iteration 101/1000 | Loss: 0.00001524
Iteration 102/1000 | Loss: 0.00001524
Iteration 103/1000 | Loss: 0.00001524
Iteration 104/1000 | Loss: 0.00001524
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001524
Iteration 107/1000 | Loss: 0.00001524
Iteration 108/1000 | Loss: 0.00001524
Iteration 109/1000 | Loss: 0.00001524
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001524
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001524
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001524
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001524
Iteration 121/1000 | Loss: 0.00001524
Iteration 122/1000 | Loss: 0.00001524
Iteration 123/1000 | Loss: 0.00001524
Iteration 124/1000 | Loss: 0.00001524
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001524
Iteration 132/1000 | Loss: 0.00001524
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001524
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00001524
Iteration 137/1000 | Loss: 0.00001524
Iteration 138/1000 | Loss: 0.00001524
Iteration 139/1000 | Loss: 0.00001524
Iteration 140/1000 | Loss: 0.00001524
Iteration 141/1000 | Loss: 0.00001524
Iteration 142/1000 | Loss: 0.00001524
Iteration 143/1000 | Loss: 0.00001524
Iteration 144/1000 | Loss: 0.00001524
Iteration 145/1000 | Loss: 0.00001524
Iteration 146/1000 | Loss: 0.00001524
Iteration 147/1000 | Loss: 0.00001524
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001524
Iteration 150/1000 | Loss: 0.00001524
Iteration 151/1000 | Loss: 0.00001524
Iteration 152/1000 | Loss: 0.00001524
Iteration 153/1000 | Loss: 0.00001524
Iteration 154/1000 | Loss: 0.00001524
Iteration 155/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.5238896594382823e-05, 1.5238896594382823e-05, 1.5238896594382823e-05, 1.5238896594382823e-05, 1.5238896594382823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5238896594382823e-05

Optimization complete. Final v2v error: 3.325089931488037 mm

Highest mean error: 3.473344087600708 mm for frame 19

Lowest mean error: 3.2237601280212402 mm for frame 76

Saving results

Total time: 54.632357358932495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379722
Iteration 2/25 | Loss: 0.00128897
Iteration 3/25 | Loss: 0.00120452
Iteration 4/25 | Loss: 0.00119367
Iteration 5/25 | Loss: 0.00119065
Iteration 6/25 | Loss: 0.00119046
Iteration 7/25 | Loss: 0.00119046
Iteration 8/25 | Loss: 0.00119046
Iteration 9/25 | Loss: 0.00119046
Iteration 10/25 | Loss: 0.00119046
Iteration 11/25 | Loss: 0.00119046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001190457958728075, 0.001190457958728075, 0.001190457958728075, 0.001190457958728075, 0.001190457958728075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190457958728075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50743496
Iteration 2/25 | Loss: 0.00077016
Iteration 3/25 | Loss: 0.00077016
Iteration 4/25 | Loss: 0.00077016
Iteration 5/25 | Loss: 0.00077015
Iteration 6/25 | Loss: 0.00077015
Iteration 7/25 | Loss: 0.00077015
Iteration 8/25 | Loss: 0.00077015
Iteration 9/25 | Loss: 0.00077015
Iteration 10/25 | Loss: 0.00077015
Iteration 11/25 | Loss: 0.00077015
Iteration 12/25 | Loss: 0.00077015
Iteration 13/25 | Loss: 0.00077015
Iteration 14/25 | Loss: 0.00077015
Iteration 15/25 | Loss: 0.00077015
Iteration 16/25 | Loss: 0.00077015
Iteration 17/25 | Loss: 0.00077015
Iteration 18/25 | Loss: 0.00077015
Iteration 19/25 | Loss: 0.00077015
Iteration 20/25 | Loss: 0.00077015
Iteration 21/25 | Loss: 0.00077015
Iteration 22/25 | Loss: 0.00077015
Iteration 23/25 | Loss: 0.00077015
Iteration 24/25 | Loss: 0.00077015
Iteration 25/25 | Loss: 0.00077015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077015
Iteration 2/1000 | Loss: 0.00002793
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001424
Iteration 5/1000 | Loss: 0.00001309
Iteration 6/1000 | Loss: 0.00001230
Iteration 7/1000 | Loss: 0.00001179
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001107
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001099
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001094
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001094
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001091
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001080
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001080
Iteration 64/1000 | Loss: 0.00001080
Iteration 65/1000 | Loss: 0.00001079
Iteration 66/1000 | Loss: 0.00001079
Iteration 67/1000 | Loss: 0.00001079
Iteration 68/1000 | Loss: 0.00001079
Iteration 69/1000 | Loss: 0.00001079
Iteration 70/1000 | Loss: 0.00001078
Iteration 71/1000 | Loss: 0.00001078
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001076
Iteration 78/1000 | Loss: 0.00001076
Iteration 79/1000 | Loss: 0.00001076
Iteration 80/1000 | Loss: 0.00001076
Iteration 81/1000 | Loss: 0.00001076
Iteration 82/1000 | Loss: 0.00001076
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001073
Iteration 94/1000 | Loss: 0.00001073
Iteration 95/1000 | Loss: 0.00001073
Iteration 96/1000 | Loss: 0.00001072
Iteration 97/1000 | Loss: 0.00001072
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001071
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001068
Iteration 103/1000 | Loss: 0.00001068
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001067
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001066
Iteration 110/1000 | Loss: 0.00001065
Iteration 111/1000 | Loss: 0.00001065
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001065
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001063
Iteration 119/1000 | Loss: 0.00001063
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001062
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001062
Iteration 126/1000 | Loss: 0.00001062
Iteration 127/1000 | Loss: 0.00001062
Iteration 128/1000 | Loss: 0.00001062
Iteration 129/1000 | Loss: 0.00001062
Iteration 130/1000 | Loss: 0.00001062
Iteration 131/1000 | Loss: 0.00001062
Iteration 132/1000 | Loss: 0.00001061
Iteration 133/1000 | Loss: 0.00001061
Iteration 134/1000 | Loss: 0.00001061
Iteration 135/1000 | Loss: 0.00001061
Iteration 136/1000 | Loss: 0.00001061
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001058
Iteration 146/1000 | Loss: 0.00001058
Iteration 147/1000 | Loss: 0.00001058
Iteration 148/1000 | Loss: 0.00001058
Iteration 149/1000 | Loss: 0.00001058
Iteration 150/1000 | Loss: 0.00001058
Iteration 151/1000 | Loss: 0.00001058
Iteration 152/1000 | Loss: 0.00001058
Iteration 153/1000 | Loss: 0.00001058
Iteration 154/1000 | Loss: 0.00001058
Iteration 155/1000 | Loss: 0.00001058
Iteration 156/1000 | Loss: 0.00001058
Iteration 157/1000 | Loss: 0.00001058
Iteration 158/1000 | Loss: 0.00001057
Iteration 159/1000 | Loss: 0.00001057
Iteration 160/1000 | Loss: 0.00001055
Iteration 161/1000 | Loss: 0.00001055
Iteration 162/1000 | Loss: 0.00001055
Iteration 163/1000 | Loss: 0.00001055
Iteration 164/1000 | Loss: 0.00001055
Iteration 165/1000 | Loss: 0.00001055
Iteration 166/1000 | Loss: 0.00001054
Iteration 167/1000 | Loss: 0.00001054
Iteration 168/1000 | Loss: 0.00001054
Iteration 169/1000 | Loss: 0.00001054
Iteration 170/1000 | Loss: 0.00001054
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001052
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001051
Iteration 175/1000 | Loss: 0.00001051
Iteration 176/1000 | Loss: 0.00001051
Iteration 177/1000 | Loss: 0.00001051
Iteration 178/1000 | Loss: 0.00001051
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001050
Iteration 181/1000 | Loss: 0.00001050
Iteration 182/1000 | Loss: 0.00001050
Iteration 183/1000 | Loss: 0.00001050
Iteration 184/1000 | Loss: 0.00001050
Iteration 185/1000 | Loss: 0.00001050
Iteration 186/1000 | Loss: 0.00001050
Iteration 187/1000 | Loss: 0.00001050
Iteration 188/1000 | Loss: 0.00001050
Iteration 189/1000 | Loss: 0.00001050
Iteration 190/1000 | Loss: 0.00001050
Iteration 191/1000 | Loss: 0.00001049
Iteration 192/1000 | Loss: 0.00001049
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001049
Iteration 195/1000 | Loss: 0.00001049
Iteration 196/1000 | Loss: 0.00001049
Iteration 197/1000 | Loss: 0.00001049
Iteration 198/1000 | Loss: 0.00001048
Iteration 199/1000 | Loss: 0.00001048
Iteration 200/1000 | Loss: 0.00001048
Iteration 201/1000 | Loss: 0.00001048
Iteration 202/1000 | Loss: 0.00001048
Iteration 203/1000 | Loss: 0.00001048
Iteration 204/1000 | Loss: 0.00001048
Iteration 205/1000 | Loss: 0.00001048
Iteration 206/1000 | Loss: 0.00001048
Iteration 207/1000 | Loss: 0.00001048
Iteration 208/1000 | Loss: 0.00001048
Iteration 209/1000 | Loss: 0.00001048
Iteration 210/1000 | Loss: 0.00001048
Iteration 211/1000 | Loss: 0.00001048
Iteration 212/1000 | Loss: 0.00001048
Iteration 213/1000 | Loss: 0.00001048
Iteration 214/1000 | Loss: 0.00001048
Iteration 215/1000 | Loss: 0.00001048
Iteration 216/1000 | Loss: 0.00001048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.0481096978764981e-05, 1.0481096978764981e-05, 1.0481096978764981e-05, 1.0481096978764981e-05, 1.0481096978764981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0481096978764981e-05

Optimization complete. Final v2v error: 2.7723464965820312 mm

Highest mean error: 3.297517776489258 mm for frame 98

Lowest mean error: 2.6543335914611816 mm for frame 191

Saving results

Total time: 40.69611477851868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057431
Iteration 2/25 | Loss: 0.01057430
Iteration 3/25 | Loss: 0.00169447
Iteration 4/25 | Loss: 0.00140914
Iteration 5/25 | Loss: 0.00130674
Iteration 6/25 | Loss: 0.00131474
Iteration 7/25 | Loss: 0.00127257
Iteration 8/25 | Loss: 0.00126853
Iteration 9/25 | Loss: 0.00126935
Iteration 10/25 | Loss: 0.00126569
Iteration 11/25 | Loss: 0.00126313
Iteration 12/25 | Loss: 0.00126460
Iteration 13/25 | Loss: 0.00126567
Iteration 14/25 | Loss: 0.00126559
Iteration 15/25 | Loss: 0.00126099
Iteration 16/25 | Loss: 0.00126009
Iteration 17/25 | Loss: 0.00125841
Iteration 18/25 | Loss: 0.00125780
Iteration 19/25 | Loss: 0.00125711
Iteration 20/25 | Loss: 0.00125683
Iteration 21/25 | Loss: 0.00125680
Iteration 22/25 | Loss: 0.00125680
Iteration 23/25 | Loss: 0.00125680
Iteration 24/25 | Loss: 0.00125680
Iteration 25/25 | Loss: 0.00125679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.52104473
Iteration 2/25 | Loss: 0.00087538
Iteration 3/25 | Loss: 0.00087538
Iteration 4/25 | Loss: 0.00087538
Iteration 5/25 | Loss: 0.00087538
Iteration 6/25 | Loss: 0.00087538
Iteration 7/25 | Loss: 0.00087538
Iteration 8/25 | Loss: 0.00087538
Iteration 9/25 | Loss: 0.00087538
Iteration 10/25 | Loss: 0.00087538
Iteration 11/25 | Loss: 0.00087538
Iteration 12/25 | Loss: 0.00087538
Iteration 13/25 | Loss: 0.00087538
Iteration 14/25 | Loss: 0.00087538
Iteration 15/25 | Loss: 0.00087538
Iteration 16/25 | Loss: 0.00087538
Iteration 17/25 | Loss: 0.00087538
Iteration 18/25 | Loss: 0.00087538
Iteration 19/25 | Loss: 0.00087538
Iteration 20/25 | Loss: 0.00087538
Iteration 21/25 | Loss: 0.00087538
Iteration 22/25 | Loss: 0.00087538
Iteration 23/25 | Loss: 0.00087538
Iteration 24/25 | Loss: 0.00087538
Iteration 25/25 | Loss: 0.00087538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087538
Iteration 2/1000 | Loss: 0.00007755
Iteration 3/1000 | Loss: 0.00001783
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00012370
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00005968
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001433
Iteration 10/1000 | Loss: 0.00004693
Iteration 11/1000 | Loss: 0.00003622
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001361
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00002259
Iteration 23/1000 | Loss: 0.00001344
Iteration 24/1000 | Loss: 0.00001340
Iteration 25/1000 | Loss: 0.00001340
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001340
Iteration 28/1000 | Loss: 0.00001339
Iteration 29/1000 | Loss: 0.00001339
Iteration 30/1000 | Loss: 0.00001339
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001338
Iteration 33/1000 | Loss: 0.00001338
Iteration 34/1000 | Loss: 0.00001337
Iteration 35/1000 | Loss: 0.00001337
Iteration 36/1000 | Loss: 0.00001337
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001337
Iteration 39/1000 | Loss: 0.00001337
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001518
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001320
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00003173
Iteration 79/1000 | Loss: 0.00003173
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001315
Iteration 88/1000 | Loss: 0.00001315
Iteration 89/1000 | Loss: 0.00001315
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001307
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001307
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001306
Iteration 144/1000 | Loss: 0.00001306
Iteration 145/1000 | Loss: 0.00001306
Iteration 146/1000 | Loss: 0.00001306
Iteration 147/1000 | Loss: 0.00001306
Iteration 148/1000 | Loss: 0.00001306
Iteration 149/1000 | Loss: 0.00001306
Iteration 150/1000 | Loss: 0.00001305
Iteration 151/1000 | Loss: 0.00001305
Iteration 152/1000 | Loss: 0.00001305
Iteration 153/1000 | Loss: 0.00001305
Iteration 154/1000 | Loss: 0.00001305
Iteration 155/1000 | Loss: 0.00001305
Iteration 156/1000 | Loss: 0.00001305
Iteration 157/1000 | Loss: 0.00001305
Iteration 158/1000 | Loss: 0.00001305
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001304
Iteration 165/1000 | Loss: 0.00001304
Iteration 166/1000 | Loss: 0.00001304
Iteration 167/1000 | Loss: 0.00001304
Iteration 168/1000 | Loss: 0.00001303
Iteration 169/1000 | Loss: 0.00001303
Iteration 170/1000 | Loss: 0.00001303
Iteration 171/1000 | Loss: 0.00001303
Iteration 172/1000 | Loss: 0.00001303
Iteration 173/1000 | Loss: 0.00001303
Iteration 174/1000 | Loss: 0.00001303
Iteration 175/1000 | Loss: 0.00001303
Iteration 176/1000 | Loss: 0.00001303
Iteration 177/1000 | Loss: 0.00001303
Iteration 178/1000 | Loss: 0.00001303
Iteration 179/1000 | Loss: 0.00001303
Iteration 180/1000 | Loss: 0.00001303
Iteration 181/1000 | Loss: 0.00001303
Iteration 182/1000 | Loss: 0.00001303
Iteration 183/1000 | Loss: 0.00001303
Iteration 184/1000 | Loss: 0.00001303
Iteration 185/1000 | Loss: 0.00001303
Iteration 186/1000 | Loss: 0.00001303
Iteration 187/1000 | Loss: 0.00001303
Iteration 188/1000 | Loss: 0.00001303
Iteration 189/1000 | Loss: 0.00001303
Iteration 190/1000 | Loss: 0.00001303
Iteration 191/1000 | Loss: 0.00001303
Iteration 192/1000 | Loss: 0.00001303
Iteration 193/1000 | Loss: 0.00001303
Iteration 194/1000 | Loss: 0.00001303
Iteration 195/1000 | Loss: 0.00001303
Iteration 196/1000 | Loss: 0.00001303
Iteration 197/1000 | Loss: 0.00001303
Iteration 198/1000 | Loss: 0.00001303
Iteration 199/1000 | Loss: 0.00001303
Iteration 200/1000 | Loss: 0.00001303
Iteration 201/1000 | Loss: 0.00001303
Iteration 202/1000 | Loss: 0.00001303
Iteration 203/1000 | Loss: 0.00001303
Iteration 204/1000 | Loss: 0.00001303
Iteration 205/1000 | Loss: 0.00001303
Iteration 206/1000 | Loss: 0.00001303
Iteration 207/1000 | Loss: 0.00001303
Iteration 208/1000 | Loss: 0.00001303
Iteration 209/1000 | Loss: 0.00001303
Iteration 210/1000 | Loss: 0.00001303
Iteration 211/1000 | Loss: 0.00001303
Iteration 212/1000 | Loss: 0.00001303
Iteration 213/1000 | Loss: 0.00001303
Iteration 214/1000 | Loss: 0.00001303
Iteration 215/1000 | Loss: 0.00001303
Iteration 216/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.3027854038227815e-05, 1.3027854038227815e-05, 1.3027854038227815e-05, 1.3027854038227815e-05, 1.3027854038227815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3027854038227815e-05

Optimization complete. Final v2v error: 3.0279266834259033 mm

Highest mean error: 3.2830214500427246 mm for frame 119

Lowest mean error: 2.8120954036712646 mm for frame 159

Saving results

Total time: 81.10634565353394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799199
Iteration 2/25 | Loss: 0.00189505
Iteration 3/25 | Loss: 0.00168335
Iteration 4/25 | Loss: 0.00170684
Iteration 5/25 | Loss: 0.00182462
Iteration 6/25 | Loss: 0.00156312
Iteration 7/25 | Loss: 0.00142137
Iteration 8/25 | Loss: 0.00135676
Iteration 9/25 | Loss: 0.00126030
Iteration 10/25 | Loss: 0.00124912
Iteration 11/25 | Loss: 0.00127743
Iteration 12/25 | Loss: 0.00122877
Iteration 13/25 | Loss: 0.00122638
Iteration 14/25 | Loss: 0.00122618
Iteration 15/25 | Loss: 0.00122617
Iteration 16/25 | Loss: 0.00122617
Iteration 17/25 | Loss: 0.00122617
Iteration 18/25 | Loss: 0.00122617
Iteration 19/25 | Loss: 0.00122617
Iteration 20/25 | Loss: 0.00122617
Iteration 21/25 | Loss: 0.00122617
Iteration 22/25 | Loss: 0.00122617
Iteration 23/25 | Loss: 0.00122617
Iteration 24/25 | Loss: 0.00122617
Iteration 25/25 | Loss: 0.00122617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42894697
Iteration 2/25 | Loss: 0.00110784
Iteration 3/25 | Loss: 0.00081165
Iteration 4/25 | Loss: 0.00081165
Iteration 5/25 | Loss: 0.00081165
Iteration 6/25 | Loss: 0.00081165
Iteration 7/25 | Loss: 0.00081165
Iteration 8/25 | Loss: 0.00081165
Iteration 9/25 | Loss: 0.00081165
Iteration 10/25 | Loss: 0.00081165
Iteration 11/25 | Loss: 0.00081165
Iteration 12/25 | Loss: 0.00081165
Iteration 13/25 | Loss: 0.00081165
Iteration 14/25 | Loss: 0.00081165
Iteration 15/25 | Loss: 0.00081165
Iteration 16/25 | Loss: 0.00081165
Iteration 17/25 | Loss: 0.00081165
Iteration 18/25 | Loss: 0.00081165
Iteration 19/25 | Loss: 0.00081165
Iteration 20/25 | Loss: 0.00081165
Iteration 21/25 | Loss: 0.00081165
Iteration 22/25 | Loss: 0.00081165
Iteration 23/25 | Loss: 0.00081165
Iteration 24/25 | Loss: 0.00081165
Iteration 25/25 | Loss: 0.00081165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081165
Iteration 2/1000 | Loss: 0.00037709
Iteration 3/1000 | Loss: 0.00006526
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001750
Iteration 8/1000 | Loss: 0.00001690
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001611
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001566
Iteration 16/1000 | Loss: 0.00001565
Iteration 17/1000 | Loss: 0.00001564
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001562
Iteration 20/1000 | Loss: 0.00001562
Iteration 21/1000 | Loss: 0.00001562
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001561
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001544
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001543
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001542
Iteration 55/1000 | Loss: 0.00001542
Iteration 56/1000 | Loss: 0.00001542
Iteration 57/1000 | Loss: 0.00001542
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001539
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001536
Iteration 88/1000 | Loss: 0.00001536
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001535
Iteration 92/1000 | Loss: 0.00001535
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001535
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001532
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001532
Iteration 120/1000 | Loss: 0.00001532
Iteration 121/1000 | Loss: 0.00001532
Iteration 122/1000 | Loss: 0.00001532
Iteration 123/1000 | Loss: 0.00001532
Iteration 124/1000 | Loss: 0.00001532
Iteration 125/1000 | Loss: 0.00001532
Iteration 126/1000 | Loss: 0.00001532
Iteration 127/1000 | Loss: 0.00001531
Iteration 128/1000 | Loss: 0.00001531
Iteration 129/1000 | Loss: 0.00001531
Iteration 130/1000 | Loss: 0.00001531
Iteration 131/1000 | Loss: 0.00001531
Iteration 132/1000 | Loss: 0.00001531
Iteration 133/1000 | Loss: 0.00001531
Iteration 134/1000 | Loss: 0.00001531
Iteration 135/1000 | Loss: 0.00001531
Iteration 136/1000 | Loss: 0.00001531
Iteration 137/1000 | Loss: 0.00001531
Iteration 138/1000 | Loss: 0.00001531
Iteration 139/1000 | Loss: 0.00001531
Iteration 140/1000 | Loss: 0.00001531
Iteration 141/1000 | Loss: 0.00001531
Iteration 142/1000 | Loss: 0.00001531
Iteration 143/1000 | Loss: 0.00001531
Iteration 144/1000 | Loss: 0.00001530
Iteration 145/1000 | Loss: 0.00001530
Iteration 146/1000 | Loss: 0.00001530
Iteration 147/1000 | Loss: 0.00001530
Iteration 148/1000 | Loss: 0.00001530
Iteration 149/1000 | Loss: 0.00001530
Iteration 150/1000 | Loss: 0.00001530
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001530
Iteration 154/1000 | Loss: 0.00001530
Iteration 155/1000 | Loss: 0.00001530
Iteration 156/1000 | Loss: 0.00001530
Iteration 157/1000 | Loss: 0.00001530
Iteration 158/1000 | Loss: 0.00001530
Iteration 159/1000 | Loss: 0.00001530
Iteration 160/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.53031105583068e-05, 1.53031105583068e-05, 1.53031105583068e-05, 1.53031105583068e-05, 1.53031105583068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53031105583068e-05

Optimization complete. Final v2v error: 3.2471072673797607 mm

Highest mean error: 4.315045356750488 mm for frame 49

Lowest mean error: 2.6709706783294678 mm for frame 8

Saving results

Total time: 60.31193780899048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922218
Iteration 2/25 | Loss: 0.00229151
Iteration 3/25 | Loss: 0.00162652
Iteration 4/25 | Loss: 0.00152178
Iteration 5/25 | Loss: 0.00144671
Iteration 6/25 | Loss: 0.00144439
Iteration 7/25 | Loss: 0.00142181
Iteration 8/25 | Loss: 0.00140438
Iteration 9/25 | Loss: 0.00140772
Iteration 10/25 | Loss: 0.00140232
Iteration 11/25 | Loss: 0.00139915
Iteration 12/25 | Loss: 0.00139850
Iteration 13/25 | Loss: 0.00139807
Iteration 14/25 | Loss: 0.00140141
Iteration 15/25 | Loss: 0.00139676
Iteration 16/25 | Loss: 0.00139621
Iteration 17/25 | Loss: 0.00139617
Iteration 18/25 | Loss: 0.00139616
Iteration 19/25 | Loss: 0.00139616
Iteration 20/25 | Loss: 0.00139616
Iteration 21/25 | Loss: 0.00139616
Iteration 22/25 | Loss: 0.00139616
Iteration 23/25 | Loss: 0.00139616
Iteration 24/25 | Loss: 0.00139616
Iteration 25/25 | Loss: 0.00139616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95274019
Iteration 2/25 | Loss: 0.00080996
Iteration 3/25 | Loss: 0.00080992
Iteration 4/25 | Loss: 0.00080992
Iteration 5/25 | Loss: 0.00080992
Iteration 6/25 | Loss: 0.00080992
Iteration 7/25 | Loss: 0.00080992
Iteration 8/25 | Loss: 0.00080992
Iteration 9/25 | Loss: 0.00080992
Iteration 10/25 | Loss: 0.00080992
Iteration 11/25 | Loss: 0.00080992
Iteration 12/25 | Loss: 0.00080992
Iteration 13/25 | Loss: 0.00080992
Iteration 14/25 | Loss: 0.00080992
Iteration 15/25 | Loss: 0.00080992
Iteration 16/25 | Loss: 0.00080992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000809917168226093, 0.000809917168226093, 0.000809917168226093, 0.000809917168226093, 0.000809917168226093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000809917168226093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080992
Iteration 2/1000 | Loss: 0.00020669
Iteration 3/1000 | Loss: 0.00016999
Iteration 4/1000 | Loss: 0.00003455
Iteration 5/1000 | Loss: 0.00003101
Iteration 6/1000 | Loss: 0.00015928
Iteration 7/1000 | Loss: 0.00002779
Iteration 8/1000 | Loss: 0.00002675
Iteration 9/1000 | Loss: 0.00015313
Iteration 10/1000 | Loss: 0.00027072
Iteration 11/1000 | Loss: 0.00007613
Iteration 12/1000 | Loss: 0.00002568
Iteration 13/1000 | Loss: 0.00002519
Iteration 14/1000 | Loss: 0.00002489
Iteration 15/1000 | Loss: 0.00002465
Iteration 16/1000 | Loss: 0.00002456
Iteration 17/1000 | Loss: 0.00002454
Iteration 18/1000 | Loss: 0.00002451
Iteration 19/1000 | Loss: 0.00002446
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002437
Iteration 23/1000 | Loss: 0.00002436
Iteration 24/1000 | Loss: 0.00002433
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002428
Iteration 28/1000 | Loss: 0.00002427
Iteration 29/1000 | Loss: 0.00002427
Iteration 30/1000 | Loss: 0.00002426
Iteration 31/1000 | Loss: 0.00002426
Iteration 32/1000 | Loss: 0.00002425
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002422
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002421
Iteration 37/1000 | Loss: 0.00002420
Iteration 38/1000 | Loss: 0.00002419
Iteration 39/1000 | Loss: 0.00002419
Iteration 40/1000 | Loss: 0.00002419
Iteration 41/1000 | Loss: 0.00002418
Iteration 42/1000 | Loss: 0.00002414
Iteration 43/1000 | Loss: 0.00002414
Iteration 44/1000 | Loss: 0.00002413
Iteration 45/1000 | Loss: 0.00002412
Iteration 46/1000 | Loss: 0.00002412
Iteration 47/1000 | Loss: 0.00002411
Iteration 48/1000 | Loss: 0.00002411
Iteration 49/1000 | Loss: 0.00002410
Iteration 50/1000 | Loss: 0.00002409
Iteration 51/1000 | Loss: 0.00002409
Iteration 52/1000 | Loss: 0.00002405
Iteration 53/1000 | Loss: 0.00002405
Iteration 54/1000 | Loss: 0.00002405
Iteration 55/1000 | Loss: 0.00002405
Iteration 56/1000 | Loss: 0.00002405
Iteration 57/1000 | Loss: 0.00002405
Iteration 58/1000 | Loss: 0.00002405
Iteration 59/1000 | Loss: 0.00002404
Iteration 60/1000 | Loss: 0.00002404
Iteration 61/1000 | Loss: 0.00002404
Iteration 62/1000 | Loss: 0.00002404
Iteration 63/1000 | Loss: 0.00002404
Iteration 64/1000 | Loss: 0.00002403
Iteration 65/1000 | Loss: 0.00002403
Iteration 66/1000 | Loss: 0.00002402
Iteration 67/1000 | Loss: 0.00002402
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002402
Iteration 70/1000 | Loss: 0.00002402
Iteration 71/1000 | Loss: 0.00002402
Iteration 72/1000 | Loss: 0.00002401
Iteration 73/1000 | Loss: 0.00002401
Iteration 74/1000 | Loss: 0.00002401
Iteration 75/1000 | Loss: 0.00002401
Iteration 76/1000 | Loss: 0.00002401
Iteration 77/1000 | Loss: 0.00002400
Iteration 78/1000 | Loss: 0.00002400
Iteration 79/1000 | Loss: 0.00002400
Iteration 80/1000 | Loss: 0.00002399
Iteration 81/1000 | Loss: 0.00002399
Iteration 82/1000 | Loss: 0.00002399
Iteration 83/1000 | Loss: 0.00002399
Iteration 84/1000 | Loss: 0.00002398
Iteration 85/1000 | Loss: 0.00002398
Iteration 86/1000 | Loss: 0.00002398
Iteration 87/1000 | Loss: 0.00002398
Iteration 88/1000 | Loss: 0.00002398
Iteration 89/1000 | Loss: 0.00002398
Iteration 90/1000 | Loss: 0.00002398
Iteration 91/1000 | Loss: 0.00002398
Iteration 92/1000 | Loss: 0.00002398
Iteration 93/1000 | Loss: 0.00002398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.3977696400834247e-05, 2.3977696400834247e-05, 2.3977696400834247e-05, 2.3977696400834247e-05, 2.3977696400834247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3977696400834247e-05

Optimization complete. Final v2v error: 3.936824321746826 mm

Highest mean error: 5.191385269165039 mm for frame 11

Lowest mean error: 3.6503918170928955 mm for frame 151

Saving results

Total time: 68.4607605934143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813515
Iteration 2/25 | Loss: 0.00157959
Iteration 3/25 | Loss: 0.00133804
Iteration 4/25 | Loss: 0.00129242
Iteration 5/25 | Loss: 0.00128093
Iteration 6/25 | Loss: 0.00127743
Iteration 7/25 | Loss: 0.00127640
Iteration 8/25 | Loss: 0.00127609
Iteration 9/25 | Loss: 0.00127594
Iteration 10/25 | Loss: 0.00127583
Iteration 11/25 | Loss: 0.00127575
Iteration 12/25 | Loss: 0.00127899
Iteration 13/25 | Loss: 0.00127732
Iteration 14/25 | Loss: 0.00127594
Iteration 15/25 | Loss: 0.00127545
Iteration 16/25 | Loss: 0.00127339
Iteration 17/25 | Loss: 0.00127277
Iteration 18/25 | Loss: 0.00127257
Iteration 19/25 | Loss: 0.00127254
Iteration 20/25 | Loss: 0.00127254
Iteration 21/25 | Loss: 0.00127254
Iteration 22/25 | Loss: 0.00127253
Iteration 23/25 | Loss: 0.00127253
Iteration 24/25 | Loss: 0.00127253
Iteration 25/25 | Loss: 0.00127253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.72721243
Iteration 2/25 | Loss: 0.00082530
Iteration 3/25 | Loss: 0.00082530
Iteration 4/25 | Loss: 0.00082530
Iteration 5/25 | Loss: 0.00082530
Iteration 6/25 | Loss: 0.00082530
Iteration 7/25 | Loss: 0.00082530
Iteration 8/25 | Loss: 0.00082530
Iteration 9/25 | Loss: 0.00082530
Iteration 10/25 | Loss: 0.00082530
Iteration 11/25 | Loss: 0.00082530
Iteration 12/25 | Loss: 0.00082530
Iteration 13/25 | Loss: 0.00082530
Iteration 14/25 | Loss: 0.00082530
Iteration 15/25 | Loss: 0.00082530
Iteration 16/25 | Loss: 0.00082530
Iteration 17/25 | Loss: 0.00082530
Iteration 18/25 | Loss: 0.00082530
Iteration 19/25 | Loss: 0.00082530
Iteration 20/25 | Loss: 0.00082530
Iteration 21/25 | Loss: 0.00082530
Iteration 22/25 | Loss: 0.00082530
Iteration 23/25 | Loss: 0.00082530
Iteration 24/25 | Loss: 0.00082530
Iteration 25/25 | Loss: 0.00082530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082530
Iteration 2/1000 | Loss: 0.00006807
Iteration 3/1000 | Loss: 0.00004496
Iteration 4/1000 | Loss: 0.00003661
Iteration 5/1000 | Loss: 0.00003367
Iteration 6/1000 | Loss: 0.00003196
Iteration 7/1000 | Loss: 0.00003059
Iteration 8/1000 | Loss: 0.00002957
Iteration 9/1000 | Loss: 0.00002865
Iteration 10/1000 | Loss: 0.00002809
Iteration 11/1000 | Loss: 0.00002765
Iteration 12/1000 | Loss: 0.00002740
Iteration 13/1000 | Loss: 0.00002710
Iteration 14/1000 | Loss: 0.00002685
Iteration 15/1000 | Loss: 0.00002671
Iteration 16/1000 | Loss: 0.00002667
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00002662
Iteration 19/1000 | Loss: 0.00002645
Iteration 20/1000 | Loss: 0.00002636
Iteration 21/1000 | Loss: 0.00002635
Iteration 22/1000 | Loss: 0.00002630
Iteration 23/1000 | Loss: 0.00002623
Iteration 24/1000 | Loss: 0.00002621
Iteration 25/1000 | Loss: 0.00002620
Iteration 26/1000 | Loss: 0.00002620
Iteration 27/1000 | Loss: 0.00002620
Iteration 28/1000 | Loss: 0.00002619
Iteration 29/1000 | Loss: 0.00002619
Iteration 30/1000 | Loss: 0.00002618
Iteration 31/1000 | Loss: 0.00002618
Iteration 32/1000 | Loss: 0.00002617
Iteration 33/1000 | Loss: 0.00002617
Iteration 34/1000 | Loss: 0.00002616
Iteration 35/1000 | Loss: 0.00002615
Iteration 36/1000 | Loss: 0.00002615
Iteration 37/1000 | Loss: 0.00002613
Iteration 38/1000 | Loss: 0.00002613
Iteration 39/1000 | Loss: 0.00002613
Iteration 40/1000 | Loss: 0.00002613
Iteration 41/1000 | Loss: 0.00002613
Iteration 42/1000 | Loss: 0.00002612
Iteration 43/1000 | Loss: 0.00002612
Iteration 44/1000 | Loss: 0.00002611
Iteration 45/1000 | Loss: 0.00002611
Iteration 46/1000 | Loss: 0.00002610
Iteration 47/1000 | Loss: 0.00002610
Iteration 48/1000 | Loss: 0.00002610
Iteration 49/1000 | Loss: 0.00002609
Iteration 50/1000 | Loss: 0.00002609
Iteration 51/1000 | Loss: 0.00002609
Iteration 52/1000 | Loss: 0.00002608
Iteration 53/1000 | Loss: 0.00002608
Iteration 54/1000 | Loss: 0.00002608
Iteration 55/1000 | Loss: 0.00002607
Iteration 56/1000 | Loss: 0.00002607
Iteration 57/1000 | Loss: 0.00002607
Iteration 58/1000 | Loss: 0.00002607
Iteration 59/1000 | Loss: 0.00002606
Iteration 60/1000 | Loss: 0.00002606
Iteration 61/1000 | Loss: 0.00002606
Iteration 62/1000 | Loss: 0.00002606
Iteration 63/1000 | Loss: 0.00002606
Iteration 64/1000 | Loss: 0.00002606
Iteration 65/1000 | Loss: 0.00002606
Iteration 66/1000 | Loss: 0.00002606
Iteration 67/1000 | Loss: 0.00002605
Iteration 68/1000 | Loss: 0.00002605
Iteration 69/1000 | Loss: 0.00002605
Iteration 70/1000 | Loss: 0.00002604
Iteration 71/1000 | Loss: 0.00002604
Iteration 72/1000 | Loss: 0.00002604
Iteration 73/1000 | Loss: 0.00002604
Iteration 74/1000 | Loss: 0.00002604
Iteration 75/1000 | Loss: 0.00002603
Iteration 76/1000 | Loss: 0.00002603
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002603
Iteration 79/1000 | Loss: 0.00002603
Iteration 80/1000 | Loss: 0.00002602
Iteration 81/1000 | Loss: 0.00002602
Iteration 82/1000 | Loss: 0.00002601
Iteration 83/1000 | Loss: 0.00002601
Iteration 84/1000 | Loss: 0.00002601
Iteration 85/1000 | Loss: 0.00002600
Iteration 86/1000 | Loss: 0.00002600
Iteration 87/1000 | Loss: 0.00002600
Iteration 88/1000 | Loss: 0.00002600
Iteration 89/1000 | Loss: 0.00002600
Iteration 90/1000 | Loss: 0.00002599
Iteration 91/1000 | Loss: 0.00002599
Iteration 92/1000 | Loss: 0.00002599
Iteration 93/1000 | Loss: 0.00002599
Iteration 94/1000 | Loss: 0.00002599
Iteration 95/1000 | Loss: 0.00002599
Iteration 96/1000 | Loss: 0.00002599
Iteration 97/1000 | Loss: 0.00002599
Iteration 98/1000 | Loss: 0.00002599
Iteration 99/1000 | Loss: 0.00002598
Iteration 100/1000 | Loss: 0.00002598
Iteration 101/1000 | Loss: 0.00002598
Iteration 102/1000 | Loss: 0.00002598
Iteration 103/1000 | Loss: 0.00002598
Iteration 104/1000 | Loss: 0.00002598
Iteration 105/1000 | Loss: 0.00002598
Iteration 106/1000 | Loss: 0.00002598
Iteration 107/1000 | Loss: 0.00002597
Iteration 108/1000 | Loss: 0.00002597
Iteration 109/1000 | Loss: 0.00002597
Iteration 110/1000 | Loss: 0.00002597
Iteration 111/1000 | Loss: 0.00002596
Iteration 112/1000 | Loss: 0.00002596
Iteration 113/1000 | Loss: 0.00002596
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002595
Iteration 116/1000 | Loss: 0.00002595
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002595
Iteration 119/1000 | Loss: 0.00002594
Iteration 120/1000 | Loss: 0.00002594
Iteration 121/1000 | Loss: 0.00002594
Iteration 122/1000 | Loss: 0.00002594
Iteration 123/1000 | Loss: 0.00002594
Iteration 124/1000 | Loss: 0.00002593
Iteration 125/1000 | Loss: 0.00002593
Iteration 126/1000 | Loss: 0.00002593
Iteration 127/1000 | Loss: 0.00002593
Iteration 128/1000 | Loss: 0.00002592
Iteration 129/1000 | Loss: 0.00002592
Iteration 130/1000 | Loss: 0.00002592
Iteration 131/1000 | Loss: 0.00002591
Iteration 132/1000 | Loss: 0.00002591
Iteration 133/1000 | Loss: 0.00002591
Iteration 134/1000 | Loss: 0.00002591
Iteration 135/1000 | Loss: 0.00002590
Iteration 136/1000 | Loss: 0.00002590
Iteration 137/1000 | Loss: 0.00002590
Iteration 138/1000 | Loss: 0.00002590
Iteration 139/1000 | Loss: 0.00002590
Iteration 140/1000 | Loss: 0.00002590
Iteration 141/1000 | Loss: 0.00002589
Iteration 142/1000 | Loss: 0.00002589
Iteration 143/1000 | Loss: 0.00002589
Iteration 144/1000 | Loss: 0.00002589
Iteration 145/1000 | Loss: 0.00002589
Iteration 146/1000 | Loss: 0.00002589
Iteration 147/1000 | Loss: 0.00002589
Iteration 148/1000 | Loss: 0.00002589
Iteration 149/1000 | Loss: 0.00002589
Iteration 150/1000 | Loss: 0.00002589
Iteration 151/1000 | Loss: 0.00002589
Iteration 152/1000 | Loss: 0.00002589
Iteration 153/1000 | Loss: 0.00002589
Iteration 154/1000 | Loss: 0.00002588
Iteration 155/1000 | Loss: 0.00002588
Iteration 156/1000 | Loss: 0.00002588
Iteration 157/1000 | Loss: 0.00002588
Iteration 158/1000 | Loss: 0.00002588
Iteration 159/1000 | Loss: 0.00002588
Iteration 160/1000 | Loss: 0.00002588
Iteration 161/1000 | Loss: 0.00002587
Iteration 162/1000 | Loss: 0.00002587
Iteration 163/1000 | Loss: 0.00002587
Iteration 164/1000 | Loss: 0.00002587
Iteration 165/1000 | Loss: 0.00002587
Iteration 166/1000 | Loss: 0.00002587
Iteration 167/1000 | Loss: 0.00002587
Iteration 168/1000 | Loss: 0.00002587
Iteration 169/1000 | Loss: 0.00002587
Iteration 170/1000 | Loss: 0.00002587
Iteration 171/1000 | Loss: 0.00002587
Iteration 172/1000 | Loss: 0.00002587
Iteration 173/1000 | Loss: 0.00002586
Iteration 174/1000 | Loss: 0.00002586
Iteration 175/1000 | Loss: 0.00002586
Iteration 176/1000 | Loss: 0.00002586
Iteration 177/1000 | Loss: 0.00002586
Iteration 178/1000 | Loss: 0.00002586
Iteration 179/1000 | Loss: 0.00002586
Iteration 180/1000 | Loss: 0.00002586
Iteration 181/1000 | Loss: 0.00002585
Iteration 182/1000 | Loss: 0.00002585
Iteration 183/1000 | Loss: 0.00002585
Iteration 184/1000 | Loss: 0.00002585
Iteration 185/1000 | Loss: 0.00002585
Iteration 186/1000 | Loss: 0.00002585
Iteration 187/1000 | Loss: 0.00002585
Iteration 188/1000 | Loss: 0.00002585
Iteration 189/1000 | Loss: 0.00002585
Iteration 190/1000 | Loss: 0.00002585
Iteration 191/1000 | Loss: 0.00002585
Iteration 192/1000 | Loss: 0.00002585
Iteration 193/1000 | Loss: 0.00002584
Iteration 194/1000 | Loss: 0.00002584
Iteration 195/1000 | Loss: 0.00002584
Iteration 196/1000 | Loss: 0.00002584
Iteration 197/1000 | Loss: 0.00002584
Iteration 198/1000 | Loss: 0.00002584
Iteration 199/1000 | Loss: 0.00002584
Iteration 200/1000 | Loss: 0.00002584
Iteration 201/1000 | Loss: 0.00002584
Iteration 202/1000 | Loss: 0.00002584
Iteration 203/1000 | Loss: 0.00002584
Iteration 204/1000 | Loss: 0.00002584
Iteration 205/1000 | Loss: 0.00002583
Iteration 206/1000 | Loss: 0.00002583
Iteration 207/1000 | Loss: 0.00002583
Iteration 208/1000 | Loss: 0.00002583
Iteration 209/1000 | Loss: 0.00002583
Iteration 210/1000 | Loss: 0.00002583
Iteration 211/1000 | Loss: 0.00002583
Iteration 212/1000 | Loss: 0.00002583
Iteration 213/1000 | Loss: 0.00002583
Iteration 214/1000 | Loss: 0.00002583
Iteration 215/1000 | Loss: 0.00002583
Iteration 216/1000 | Loss: 0.00002583
Iteration 217/1000 | Loss: 0.00002583
Iteration 218/1000 | Loss: 0.00002583
Iteration 219/1000 | Loss: 0.00002583
Iteration 220/1000 | Loss: 0.00002583
Iteration 221/1000 | Loss: 0.00002582
Iteration 222/1000 | Loss: 0.00002582
Iteration 223/1000 | Loss: 0.00002582
Iteration 224/1000 | Loss: 0.00002582
Iteration 225/1000 | Loss: 0.00002582
Iteration 226/1000 | Loss: 0.00002582
Iteration 227/1000 | Loss: 0.00002582
Iteration 228/1000 | Loss: 0.00002582
Iteration 229/1000 | Loss: 0.00002582
Iteration 230/1000 | Loss: 0.00002582
Iteration 231/1000 | Loss: 0.00002582
Iteration 232/1000 | Loss: 0.00002582
Iteration 233/1000 | Loss: 0.00002582
Iteration 234/1000 | Loss: 0.00002582
Iteration 235/1000 | Loss: 0.00002582
Iteration 236/1000 | Loss: 0.00002582
Iteration 237/1000 | Loss: 0.00002582
Iteration 238/1000 | Loss: 0.00002582
Iteration 239/1000 | Loss: 0.00002582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.581516673672013e-05, 2.581516673672013e-05, 2.581516673672013e-05, 2.581516673672013e-05, 2.581516673672013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.581516673672013e-05

Optimization complete. Final v2v error: 4.2486252784729 mm

Highest mean error: 5.832836151123047 mm for frame 50

Lowest mean error: 3.2373480796813965 mm for frame 27

Saving results

Total time: 70.55093455314636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115551
Iteration 2/25 | Loss: 0.00186979
Iteration 3/25 | Loss: 0.00149804
Iteration 4/25 | Loss: 0.00145575
Iteration 5/25 | Loss: 0.00146046
Iteration 6/25 | Loss: 0.00145272
Iteration 7/25 | Loss: 0.00145462
Iteration 8/25 | Loss: 0.00144642
Iteration 9/25 | Loss: 0.00144656
Iteration 10/25 | Loss: 0.00144042
Iteration 11/25 | Loss: 0.00143899
Iteration 12/25 | Loss: 0.00144202
Iteration 13/25 | Loss: 0.00143609
Iteration 14/25 | Loss: 0.00143580
Iteration 15/25 | Loss: 0.00143211
Iteration 16/25 | Loss: 0.00143852
Iteration 17/25 | Loss: 0.00143031
Iteration 18/25 | Loss: 0.00142807
Iteration 19/25 | Loss: 0.00142244
Iteration 20/25 | Loss: 0.00141351
Iteration 21/25 | Loss: 0.00140717
Iteration 22/25 | Loss: 0.00140546
Iteration 23/25 | Loss: 0.00140655
Iteration 24/25 | Loss: 0.00140564
Iteration 25/25 | Loss: 0.00140391

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30751765
Iteration 2/25 | Loss: 0.00113994
Iteration 3/25 | Loss: 0.00113994
Iteration 4/25 | Loss: 0.00113994
Iteration 5/25 | Loss: 0.00113994
Iteration 6/25 | Loss: 0.00113994
Iteration 7/25 | Loss: 0.00113994
Iteration 8/25 | Loss: 0.00113994
Iteration 9/25 | Loss: 0.00113993
Iteration 10/25 | Loss: 0.00113993
Iteration 11/25 | Loss: 0.00113993
Iteration 12/25 | Loss: 0.00113993
Iteration 13/25 | Loss: 0.00113993
Iteration 14/25 | Loss: 0.00113993
Iteration 15/25 | Loss: 0.00113993
Iteration 16/25 | Loss: 0.00113993
Iteration 17/25 | Loss: 0.00113993
Iteration 18/25 | Loss: 0.00113993
Iteration 19/25 | Loss: 0.00113993
Iteration 20/25 | Loss: 0.00113993
Iteration 21/25 | Loss: 0.00113993
Iteration 22/25 | Loss: 0.00113993
Iteration 23/25 | Loss: 0.00113993
Iteration 24/25 | Loss: 0.00113993
Iteration 25/25 | Loss: 0.00113993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113993
Iteration 2/1000 | Loss: 0.00030061
Iteration 3/1000 | Loss: 0.00025556
Iteration 4/1000 | Loss: 0.00013333
Iteration 5/1000 | Loss: 0.00026097
Iteration 6/1000 | Loss: 0.00022733
Iteration 7/1000 | Loss: 0.00021883
Iteration 8/1000 | Loss: 0.00024818
Iteration 9/1000 | Loss: 0.00026587
Iteration 10/1000 | Loss: 0.00030472
Iteration 11/1000 | Loss: 0.00023918
Iteration 12/1000 | Loss: 0.00015398
Iteration 13/1000 | Loss: 0.00005894
Iteration 14/1000 | Loss: 0.00007733
Iteration 15/1000 | Loss: 0.00014211
Iteration 16/1000 | Loss: 0.00017178
Iteration 17/1000 | Loss: 0.00007958
Iteration 18/1000 | Loss: 0.00009472
Iteration 19/1000 | Loss: 0.00004867
Iteration 20/1000 | Loss: 0.00004985
Iteration 21/1000 | Loss: 0.00004697
Iteration 22/1000 | Loss: 0.00011814
Iteration 23/1000 | Loss: 0.00010501
Iteration 24/1000 | Loss: 0.00005632
Iteration 25/1000 | Loss: 0.00012286
Iteration 26/1000 | Loss: 0.00014204
Iteration 27/1000 | Loss: 0.00005482
Iteration 28/1000 | Loss: 0.00019023
Iteration 29/1000 | Loss: 0.00008734
Iteration 30/1000 | Loss: 0.00008704
Iteration 31/1000 | Loss: 0.00013513
Iteration 32/1000 | Loss: 0.00018003
Iteration 33/1000 | Loss: 0.00015143
Iteration 34/1000 | Loss: 0.00014961
Iteration 35/1000 | Loss: 0.00012673
Iteration 36/1000 | Loss: 0.00004568
Iteration 37/1000 | Loss: 0.00003991
Iteration 38/1000 | Loss: 0.00003342
Iteration 39/1000 | Loss: 0.00004543
Iteration 40/1000 | Loss: 0.00003477
Iteration 41/1000 | Loss: 0.00003280
Iteration 42/1000 | Loss: 0.00003219
Iteration 43/1000 | Loss: 0.00003127
Iteration 44/1000 | Loss: 0.00003070
Iteration 45/1000 | Loss: 0.00003041
Iteration 46/1000 | Loss: 0.00003009
Iteration 47/1000 | Loss: 0.00002984
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002943
Iteration 50/1000 | Loss: 0.00002924
Iteration 51/1000 | Loss: 0.00002907
Iteration 52/1000 | Loss: 0.00002900
Iteration 53/1000 | Loss: 0.00002900
Iteration 54/1000 | Loss: 0.00002896
Iteration 55/1000 | Loss: 0.00002896
Iteration 56/1000 | Loss: 0.00002896
Iteration 57/1000 | Loss: 0.00002888
Iteration 58/1000 | Loss: 0.00002888
Iteration 59/1000 | Loss: 0.00002888
Iteration 60/1000 | Loss: 0.00002887
Iteration 61/1000 | Loss: 0.00002885
Iteration 62/1000 | Loss: 0.00002885
Iteration 63/1000 | Loss: 0.00002884
Iteration 64/1000 | Loss: 0.00002883
Iteration 65/1000 | Loss: 0.00002882
Iteration 66/1000 | Loss: 0.00002882
Iteration 67/1000 | Loss: 0.00002881
Iteration 68/1000 | Loss: 0.00002881
Iteration 69/1000 | Loss: 0.00002880
Iteration 70/1000 | Loss: 0.00002880
Iteration 71/1000 | Loss: 0.00002880
Iteration 72/1000 | Loss: 0.00002876
Iteration 73/1000 | Loss: 0.00002876
Iteration 74/1000 | Loss: 0.00002875
Iteration 75/1000 | Loss: 0.00002873
Iteration 76/1000 | Loss: 0.00002872
Iteration 77/1000 | Loss: 0.00002868
Iteration 78/1000 | Loss: 0.00002866
Iteration 79/1000 | Loss: 0.00002866
Iteration 80/1000 | Loss: 0.00002865
Iteration 81/1000 | Loss: 0.00002865
Iteration 82/1000 | Loss: 0.00002864
Iteration 83/1000 | Loss: 0.00002864
Iteration 84/1000 | Loss: 0.00002863
Iteration 85/1000 | Loss: 0.00002863
Iteration 86/1000 | Loss: 0.00002862
Iteration 87/1000 | Loss: 0.00002862
Iteration 88/1000 | Loss: 0.00002860
Iteration 89/1000 | Loss: 0.00002859
Iteration 90/1000 | Loss: 0.00002859
Iteration 91/1000 | Loss: 0.00002859
Iteration 92/1000 | Loss: 0.00002858
Iteration 93/1000 | Loss: 0.00002858
Iteration 94/1000 | Loss: 0.00002858
Iteration 95/1000 | Loss: 0.00002858
Iteration 96/1000 | Loss: 0.00002858
Iteration 97/1000 | Loss: 0.00002858
Iteration 98/1000 | Loss: 0.00002857
Iteration 99/1000 | Loss: 0.00002857
Iteration 100/1000 | Loss: 0.00002857
Iteration 101/1000 | Loss: 0.00002857
Iteration 102/1000 | Loss: 0.00002856
Iteration 103/1000 | Loss: 0.00002856
Iteration 104/1000 | Loss: 0.00002856
Iteration 105/1000 | Loss: 0.00002856
Iteration 106/1000 | Loss: 0.00002856
Iteration 107/1000 | Loss: 0.00002856
Iteration 108/1000 | Loss: 0.00002856
Iteration 109/1000 | Loss: 0.00002856
Iteration 110/1000 | Loss: 0.00002856
Iteration 111/1000 | Loss: 0.00002856
Iteration 112/1000 | Loss: 0.00002856
Iteration 113/1000 | Loss: 0.00002856
Iteration 114/1000 | Loss: 0.00002856
Iteration 115/1000 | Loss: 0.00002855
Iteration 116/1000 | Loss: 0.00002855
Iteration 117/1000 | Loss: 0.00002855
Iteration 118/1000 | Loss: 0.00002854
Iteration 119/1000 | Loss: 0.00002854
Iteration 120/1000 | Loss: 0.00002854
Iteration 121/1000 | Loss: 0.00002854
Iteration 122/1000 | Loss: 0.00002854
Iteration 123/1000 | Loss: 0.00002854
Iteration 124/1000 | Loss: 0.00002854
Iteration 125/1000 | Loss: 0.00002854
Iteration 126/1000 | Loss: 0.00002854
Iteration 127/1000 | Loss: 0.00002854
Iteration 128/1000 | Loss: 0.00002854
Iteration 129/1000 | Loss: 0.00002854
Iteration 130/1000 | Loss: 0.00002853
Iteration 131/1000 | Loss: 0.00002853
Iteration 132/1000 | Loss: 0.00002853
Iteration 133/1000 | Loss: 0.00002853
Iteration 134/1000 | Loss: 0.00002853
Iteration 135/1000 | Loss: 0.00002853
Iteration 136/1000 | Loss: 0.00002853
Iteration 137/1000 | Loss: 0.00002853
Iteration 138/1000 | Loss: 0.00002853
Iteration 139/1000 | Loss: 0.00002853
Iteration 140/1000 | Loss: 0.00002852
Iteration 141/1000 | Loss: 0.00002852
Iteration 142/1000 | Loss: 0.00002852
Iteration 143/1000 | Loss: 0.00002852
Iteration 144/1000 | Loss: 0.00002852
Iteration 145/1000 | Loss: 0.00002852
Iteration 146/1000 | Loss: 0.00002852
Iteration 147/1000 | Loss: 0.00002852
Iteration 148/1000 | Loss: 0.00002852
Iteration 149/1000 | Loss: 0.00002851
Iteration 150/1000 | Loss: 0.00002851
Iteration 151/1000 | Loss: 0.00002851
Iteration 152/1000 | Loss: 0.00002851
Iteration 153/1000 | Loss: 0.00002851
Iteration 154/1000 | Loss: 0.00002850
Iteration 155/1000 | Loss: 0.00002850
Iteration 156/1000 | Loss: 0.00002850
Iteration 157/1000 | Loss: 0.00002850
Iteration 158/1000 | Loss: 0.00002850
Iteration 159/1000 | Loss: 0.00002850
Iteration 160/1000 | Loss: 0.00002849
Iteration 161/1000 | Loss: 0.00002849
Iteration 162/1000 | Loss: 0.00002849
Iteration 163/1000 | Loss: 0.00002849
Iteration 164/1000 | Loss: 0.00002849
Iteration 165/1000 | Loss: 0.00002849
Iteration 166/1000 | Loss: 0.00002849
Iteration 167/1000 | Loss: 0.00002849
Iteration 168/1000 | Loss: 0.00002849
Iteration 169/1000 | Loss: 0.00002848
Iteration 170/1000 | Loss: 0.00002848
Iteration 171/1000 | Loss: 0.00002848
Iteration 172/1000 | Loss: 0.00002848
Iteration 173/1000 | Loss: 0.00002848
Iteration 174/1000 | Loss: 0.00002848
Iteration 175/1000 | Loss: 0.00002848
Iteration 176/1000 | Loss: 0.00002848
Iteration 177/1000 | Loss: 0.00002848
Iteration 178/1000 | Loss: 0.00002847
Iteration 179/1000 | Loss: 0.00002847
Iteration 180/1000 | Loss: 0.00002847
Iteration 181/1000 | Loss: 0.00002847
Iteration 182/1000 | Loss: 0.00002846
Iteration 183/1000 | Loss: 0.00002846
Iteration 184/1000 | Loss: 0.00002846
Iteration 185/1000 | Loss: 0.00002846
Iteration 186/1000 | Loss: 0.00002845
Iteration 187/1000 | Loss: 0.00002845
Iteration 188/1000 | Loss: 0.00002845
Iteration 189/1000 | Loss: 0.00002845
Iteration 190/1000 | Loss: 0.00002844
Iteration 191/1000 | Loss: 0.00002844
Iteration 192/1000 | Loss: 0.00002844
Iteration 193/1000 | Loss: 0.00002844
Iteration 194/1000 | Loss: 0.00002844
Iteration 195/1000 | Loss: 0.00002844
Iteration 196/1000 | Loss: 0.00002844
Iteration 197/1000 | Loss: 0.00002844
Iteration 198/1000 | Loss: 0.00002844
Iteration 199/1000 | Loss: 0.00002844
Iteration 200/1000 | Loss: 0.00002844
Iteration 201/1000 | Loss: 0.00002844
Iteration 202/1000 | Loss: 0.00002844
Iteration 203/1000 | Loss: 0.00002843
Iteration 204/1000 | Loss: 0.00002843
Iteration 205/1000 | Loss: 0.00002843
Iteration 206/1000 | Loss: 0.00002843
Iteration 207/1000 | Loss: 0.00002843
Iteration 208/1000 | Loss: 0.00002843
Iteration 209/1000 | Loss: 0.00002843
Iteration 210/1000 | Loss: 0.00002843
Iteration 211/1000 | Loss: 0.00002843
Iteration 212/1000 | Loss: 0.00002843
Iteration 213/1000 | Loss: 0.00002843
Iteration 214/1000 | Loss: 0.00002843
Iteration 215/1000 | Loss: 0.00002843
Iteration 216/1000 | Loss: 0.00002843
Iteration 217/1000 | Loss: 0.00002843
Iteration 218/1000 | Loss: 0.00002843
Iteration 219/1000 | Loss: 0.00002842
Iteration 220/1000 | Loss: 0.00002842
Iteration 221/1000 | Loss: 0.00002842
Iteration 222/1000 | Loss: 0.00002842
Iteration 223/1000 | Loss: 0.00002842
Iteration 224/1000 | Loss: 0.00002842
Iteration 225/1000 | Loss: 0.00002842
Iteration 226/1000 | Loss: 0.00002842
Iteration 227/1000 | Loss: 0.00002842
Iteration 228/1000 | Loss: 0.00002842
Iteration 229/1000 | Loss: 0.00002842
Iteration 230/1000 | Loss: 0.00002842
Iteration 231/1000 | Loss: 0.00002842
Iteration 232/1000 | Loss: 0.00002842
Iteration 233/1000 | Loss: 0.00002842
Iteration 234/1000 | Loss: 0.00002842
Iteration 235/1000 | Loss: 0.00002842
Iteration 236/1000 | Loss: 0.00002842
Iteration 237/1000 | Loss: 0.00002842
Iteration 238/1000 | Loss: 0.00002842
Iteration 239/1000 | Loss: 0.00002842
Iteration 240/1000 | Loss: 0.00002842
Iteration 241/1000 | Loss: 0.00002842
Iteration 242/1000 | Loss: 0.00002842
Iteration 243/1000 | Loss: 0.00002842
Iteration 244/1000 | Loss: 0.00002842
Iteration 245/1000 | Loss: 0.00002842
Iteration 246/1000 | Loss: 0.00002842
Iteration 247/1000 | Loss: 0.00002842
Iteration 248/1000 | Loss: 0.00002842
Iteration 249/1000 | Loss: 0.00002842
Iteration 250/1000 | Loss: 0.00002842
Iteration 251/1000 | Loss: 0.00002842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.841981768142432e-05, 2.841981768142432e-05, 2.841981768142432e-05, 2.841981768142432e-05, 2.841981768142432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.841981768142432e-05

Optimization complete. Final v2v error: 4.384089469909668 mm

Highest mean error: 5.399041175842285 mm for frame 89

Lowest mean error: 3.595008134841919 mm for frame 36

Saving results

Total time: 155.0502152442932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083694
Iteration 2/25 | Loss: 0.00344360
Iteration 3/25 | Loss: 0.00299568
Iteration 4/25 | Loss: 0.00188609
Iteration 5/25 | Loss: 0.00192363
Iteration 6/25 | Loss: 0.00172795
Iteration 7/25 | Loss: 0.00163338
Iteration 8/25 | Loss: 0.00155446
Iteration 9/25 | Loss: 0.00151062
Iteration 10/25 | Loss: 0.00150168
Iteration 11/25 | Loss: 0.00145945
Iteration 12/25 | Loss: 0.00143654
Iteration 13/25 | Loss: 0.00143686
Iteration 14/25 | Loss: 0.00143281
Iteration 15/25 | Loss: 0.00145364
Iteration 16/25 | Loss: 0.00143319
Iteration 17/25 | Loss: 0.00143580
Iteration 18/25 | Loss: 0.00143843
Iteration 19/25 | Loss: 0.00142664
Iteration 20/25 | Loss: 0.00143712
Iteration 21/25 | Loss: 0.00142401
Iteration 22/25 | Loss: 0.00142917
Iteration 23/25 | Loss: 0.00143324
Iteration 24/25 | Loss: 0.00141877
Iteration 25/25 | Loss: 0.00141660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84642005
Iteration 2/25 | Loss: 0.00284176
Iteration 3/25 | Loss: 0.00130480
Iteration 4/25 | Loss: 0.00110270
Iteration 5/25 | Loss: 0.00110270
Iteration 6/25 | Loss: 0.00110270
Iteration 7/25 | Loss: 0.00110270
Iteration 8/25 | Loss: 0.00110270
Iteration 9/25 | Loss: 0.00110270
Iteration 10/25 | Loss: 0.00110270
Iteration 11/25 | Loss: 0.00110269
Iteration 12/25 | Loss: 0.00110269
Iteration 13/25 | Loss: 0.00110269
Iteration 14/25 | Loss: 0.00110269
Iteration 15/25 | Loss: 0.00110269
Iteration 16/25 | Loss: 0.00110269
Iteration 17/25 | Loss: 0.00110269
Iteration 18/25 | Loss: 0.00110269
Iteration 19/25 | Loss: 0.00110269
Iteration 20/25 | Loss: 0.00110269
Iteration 21/25 | Loss: 0.00110269
Iteration 22/25 | Loss: 0.00110269
Iteration 23/25 | Loss: 0.00110269
Iteration 24/25 | Loss: 0.00110269
Iteration 25/25 | Loss: 0.00110269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110269
Iteration 2/1000 | Loss: 0.00006002
Iteration 3/1000 | Loss: 0.00005834
Iteration 4/1000 | Loss: 0.00003559
Iteration 5/1000 | Loss: 0.00005703
Iteration 6/1000 | Loss: 0.00003236
Iteration 7/1000 | Loss: 0.00046576
Iteration 8/1000 | Loss: 0.00003169
Iteration 9/1000 | Loss: 0.00005710
Iteration 10/1000 | Loss: 0.00003063
Iteration 11/1000 | Loss: 0.00003006
Iteration 12/1000 | Loss: 0.00002965
Iteration 13/1000 | Loss: 0.00002937
Iteration 14/1000 | Loss: 0.00002907
Iteration 15/1000 | Loss: 0.00023552
Iteration 16/1000 | Loss: 0.00056728
Iteration 17/1000 | Loss: 0.00033873
Iteration 18/1000 | Loss: 0.00003234
Iteration 19/1000 | Loss: 0.00003015
Iteration 20/1000 | Loss: 0.00008662
Iteration 21/1000 | Loss: 0.00002823
Iteration 22/1000 | Loss: 0.00002729
Iteration 23/1000 | Loss: 0.00002665
Iteration 24/1000 | Loss: 0.00002628
Iteration 25/1000 | Loss: 0.00002611
Iteration 26/1000 | Loss: 0.00002595
Iteration 27/1000 | Loss: 0.00015429
Iteration 28/1000 | Loss: 0.00015428
Iteration 29/1000 | Loss: 0.00016442
Iteration 30/1000 | Loss: 0.00032309
Iteration 31/1000 | Loss: 0.00005652
Iteration 32/1000 | Loss: 0.00020229
Iteration 33/1000 | Loss: 0.00004385
Iteration 34/1000 | Loss: 0.00003616
Iteration 35/1000 | Loss: 0.00003882
Iteration 36/1000 | Loss: 0.00003132
Iteration 37/1000 | Loss: 0.00002962
Iteration 38/1000 | Loss: 0.00018678
Iteration 39/1000 | Loss: 0.00030546
Iteration 40/1000 | Loss: 0.00018029
Iteration 41/1000 | Loss: 0.00034976
Iteration 42/1000 | Loss: 0.00006780
Iteration 43/1000 | Loss: 0.00003046
Iteration 44/1000 | Loss: 0.00002896
Iteration 45/1000 | Loss: 0.00037338
Iteration 46/1000 | Loss: 0.00008468
Iteration 47/1000 | Loss: 0.00002829
Iteration 48/1000 | Loss: 0.00025715
Iteration 49/1000 | Loss: 0.00020558
Iteration 50/1000 | Loss: 0.00017788
Iteration 51/1000 | Loss: 0.00014256
Iteration 52/1000 | Loss: 0.00006303
Iteration 53/1000 | Loss: 0.00002808
Iteration 54/1000 | Loss: 0.00002698
Iteration 55/1000 | Loss: 0.00004112
Iteration 56/1000 | Loss: 0.00002804
Iteration 57/1000 | Loss: 0.00003234
Iteration 58/1000 | Loss: 0.00025204
Iteration 59/1000 | Loss: 0.00003917
Iteration 60/1000 | Loss: 0.00003003
Iteration 61/1000 | Loss: 0.00004628
Iteration 62/1000 | Loss: 0.00003333
Iteration 63/1000 | Loss: 0.00039240
Iteration 64/1000 | Loss: 0.00023717
Iteration 65/1000 | Loss: 0.00073857
Iteration 66/1000 | Loss: 0.00034419
Iteration 67/1000 | Loss: 0.00021516
Iteration 68/1000 | Loss: 0.00014324
Iteration 69/1000 | Loss: 0.00018301
Iteration 70/1000 | Loss: 0.00049354
Iteration 71/1000 | Loss: 0.00045189
Iteration 72/1000 | Loss: 0.00084170
Iteration 73/1000 | Loss: 0.00049915
Iteration 74/1000 | Loss: 0.00048679
Iteration 75/1000 | Loss: 0.00004772
Iteration 76/1000 | Loss: 0.00037061
Iteration 77/1000 | Loss: 0.00071004
Iteration 78/1000 | Loss: 0.00054513
Iteration 79/1000 | Loss: 0.00013786
Iteration 80/1000 | Loss: 0.00006796
Iteration 81/1000 | Loss: 0.00048808
Iteration 82/1000 | Loss: 0.00004829
Iteration 83/1000 | Loss: 0.00034569
Iteration 84/1000 | Loss: 0.00021722
Iteration 85/1000 | Loss: 0.00020201
Iteration 86/1000 | Loss: 0.00020868
Iteration 87/1000 | Loss: 0.00003922
Iteration 88/1000 | Loss: 0.00040276
Iteration 89/1000 | Loss: 0.00003346
Iteration 90/1000 | Loss: 0.00003106
Iteration 91/1000 | Loss: 0.00002950
Iteration 92/1000 | Loss: 0.00002829
Iteration 93/1000 | Loss: 0.00002754
Iteration 94/1000 | Loss: 0.00015972
Iteration 95/1000 | Loss: 0.00007579
Iteration 96/1000 | Loss: 0.00009180
Iteration 97/1000 | Loss: 0.00002872
Iteration 98/1000 | Loss: 0.00002622
Iteration 99/1000 | Loss: 0.00002546
Iteration 100/1000 | Loss: 0.00003397
Iteration 101/1000 | Loss: 0.00003076
Iteration 102/1000 | Loss: 0.00003267
Iteration 103/1000 | Loss: 0.00002755
Iteration 104/1000 | Loss: 0.00002661
Iteration 105/1000 | Loss: 0.00002579
Iteration 106/1000 | Loss: 0.00002535
Iteration 107/1000 | Loss: 0.00002514
Iteration 108/1000 | Loss: 0.00002493
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002467
Iteration 111/1000 | Loss: 0.00002457
Iteration 112/1000 | Loss: 0.00002456
Iteration 113/1000 | Loss: 0.00002456
Iteration 114/1000 | Loss: 0.00002455
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002452
Iteration 117/1000 | Loss: 0.00002451
Iteration 118/1000 | Loss: 0.00002447
Iteration 119/1000 | Loss: 0.00002444
Iteration 120/1000 | Loss: 0.00002443
Iteration 121/1000 | Loss: 0.00002443
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00003437
Iteration 124/1000 | Loss: 0.00002967
Iteration 125/1000 | Loss: 0.00002473
Iteration 126/1000 | Loss: 0.00003046
Iteration 127/1000 | Loss: 0.00002798
Iteration 128/1000 | Loss: 0.00002472
Iteration 129/1000 | Loss: 0.00002421
Iteration 130/1000 | Loss: 0.00003052
Iteration 131/1000 | Loss: 0.00002446
Iteration 132/1000 | Loss: 0.00003419
Iteration 133/1000 | Loss: 0.00002421
Iteration 134/1000 | Loss: 0.00003492
Iteration 135/1000 | Loss: 0.00002396
Iteration 136/1000 | Loss: 0.00002362
Iteration 137/1000 | Loss: 0.00002343
Iteration 138/1000 | Loss: 0.00002329
Iteration 139/1000 | Loss: 0.00002321
Iteration 140/1000 | Loss: 0.00002320
Iteration 141/1000 | Loss: 0.00002319
Iteration 142/1000 | Loss: 0.00002319
Iteration 143/1000 | Loss: 0.00002318
Iteration 144/1000 | Loss: 0.00002318
Iteration 145/1000 | Loss: 0.00002318
Iteration 146/1000 | Loss: 0.00002318
Iteration 147/1000 | Loss: 0.00002317
Iteration 148/1000 | Loss: 0.00002317
Iteration 149/1000 | Loss: 0.00002317
Iteration 150/1000 | Loss: 0.00002317
Iteration 151/1000 | Loss: 0.00002317
Iteration 152/1000 | Loss: 0.00002317
Iteration 153/1000 | Loss: 0.00002316
Iteration 154/1000 | Loss: 0.00002316
Iteration 155/1000 | Loss: 0.00002315
Iteration 156/1000 | Loss: 0.00002314
Iteration 157/1000 | Loss: 0.00002314
Iteration 158/1000 | Loss: 0.00002313
Iteration 159/1000 | Loss: 0.00002313
Iteration 160/1000 | Loss: 0.00002313
Iteration 161/1000 | Loss: 0.00002313
Iteration 162/1000 | Loss: 0.00002312
Iteration 163/1000 | Loss: 0.00002312
Iteration 164/1000 | Loss: 0.00002312
Iteration 165/1000 | Loss: 0.00002312
Iteration 166/1000 | Loss: 0.00002312
Iteration 167/1000 | Loss: 0.00002312
Iteration 168/1000 | Loss: 0.00002311
Iteration 169/1000 | Loss: 0.00002311
Iteration 170/1000 | Loss: 0.00002311
Iteration 171/1000 | Loss: 0.00002311
Iteration 172/1000 | Loss: 0.00002311
Iteration 173/1000 | Loss: 0.00002311
Iteration 174/1000 | Loss: 0.00002311
Iteration 175/1000 | Loss: 0.00002311
Iteration 176/1000 | Loss: 0.00002311
Iteration 177/1000 | Loss: 0.00002311
Iteration 178/1000 | Loss: 0.00002311
Iteration 179/1000 | Loss: 0.00002311
Iteration 180/1000 | Loss: 0.00002311
Iteration 181/1000 | Loss: 0.00002311
Iteration 182/1000 | Loss: 0.00002310
Iteration 183/1000 | Loss: 0.00002310
Iteration 184/1000 | Loss: 0.00002310
Iteration 185/1000 | Loss: 0.00002310
Iteration 186/1000 | Loss: 0.00002310
Iteration 187/1000 | Loss: 0.00002310
Iteration 188/1000 | Loss: 0.00002310
Iteration 189/1000 | Loss: 0.00002310
Iteration 190/1000 | Loss: 0.00002310
Iteration 191/1000 | Loss: 0.00002310
Iteration 192/1000 | Loss: 0.00002310
Iteration 193/1000 | Loss: 0.00002310
Iteration 194/1000 | Loss: 0.00002310
Iteration 195/1000 | Loss: 0.00002310
Iteration 196/1000 | Loss: 0.00002310
Iteration 197/1000 | Loss: 0.00002309
Iteration 198/1000 | Loss: 0.00002309
Iteration 199/1000 | Loss: 0.00002309
Iteration 200/1000 | Loss: 0.00002309
Iteration 201/1000 | Loss: 0.00002309
Iteration 202/1000 | Loss: 0.00002309
Iteration 203/1000 | Loss: 0.00002309
Iteration 204/1000 | Loss: 0.00002309
Iteration 205/1000 | Loss: 0.00002309
Iteration 206/1000 | Loss: 0.00002309
Iteration 207/1000 | Loss: 0.00002309
Iteration 208/1000 | Loss: 0.00002309
Iteration 209/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.3090997274266556e-05, 2.3090997274266556e-05, 2.3090997274266556e-05, 2.3090997274266556e-05, 2.3090997274266556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3090997274266556e-05

Optimization complete. Final v2v error: 3.9792423248291016 mm

Highest mean error: 6.719818592071533 mm for frame 48

Lowest mean error: 3.2444005012512207 mm for frame 80

Saving results

Total time: 264.57918906211853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839322
Iteration 2/25 | Loss: 0.00129727
Iteration 3/25 | Loss: 0.00122649
Iteration 4/25 | Loss: 0.00121062
Iteration 5/25 | Loss: 0.00120647
Iteration 6/25 | Loss: 0.00120647
Iteration 7/25 | Loss: 0.00120647
Iteration 8/25 | Loss: 0.00120647
Iteration 9/25 | Loss: 0.00120647
Iteration 10/25 | Loss: 0.00120647
Iteration 11/25 | Loss: 0.00120647
Iteration 12/25 | Loss: 0.00120647
Iteration 13/25 | Loss: 0.00120647
Iteration 14/25 | Loss: 0.00120647
Iteration 15/25 | Loss: 0.00120647
Iteration 16/25 | Loss: 0.00120647
Iteration 17/25 | Loss: 0.00120647
Iteration 18/25 | Loss: 0.00120647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012064711190760136, 0.0012064711190760136, 0.0012064711190760136, 0.0012064711190760136, 0.0012064711190760136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012064711190760136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45096755
Iteration 2/25 | Loss: 0.00069262
Iteration 3/25 | Loss: 0.00069261
Iteration 4/25 | Loss: 0.00069261
Iteration 5/25 | Loss: 0.00069261
Iteration 6/25 | Loss: 0.00069261
Iteration 7/25 | Loss: 0.00069261
Iteration 8/25 | Loss: 0.00069261
Iteration 9/25 | Loss: 0.00069261
Iteration 10/25 | Loss: 0.00069261
Iteration 11/25 | Loss: 0.00069261
Iteration 12/25 | Loss: 0.00069261
Iteration 13/25 | Loss: 0.00069261
Iteration 14/25 | Loss: 0.00069261
Iteration 15/25 | Loss: 0.00069261
Iteration 16/25 | Loss: 0.00069261
Iteration 17/25 | Loss: 0.00069261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006926050409674644, 0.0006926050409674644, 0.0006926050409674644, 0.0006926050409674644, 0.0006926050409674644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006926050409674644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069261
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001863
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001473
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001456
Iteration 12/1000 | Loss: 0.00001455
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001442
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001440
Iteration 19/1000 | Loss: 0.00001440
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001422
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001407
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001393
Iteration 42/1000 | Loss: 0.00001393
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001384
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001384
Iteration 73/1000 | Loss: 0.00001384
Iteration 74/1000 | Loss: 0.00001384
Iteration 75/1000 | Loss: 0.00001384
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001383
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001382
Iteration 84/1000 | Loss: 0.00001382
Iteration 85/1000 | Loss: 0.00001382
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001381
Iteration 89/1000 | Loss: 0.00001380
Iteration 90/1000 | Loss: 0.00001380
Iteration 91/1000 | Loss: 0.00001380
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001379
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001375
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001375
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001372
Iteration 117/1000 | Loss: 0.00001372
Iteration 118/1000 | Loss: 0.00001372
Iteration 119/1000 | Loss: 0.00001372
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001368
Iteration 127/1000 | Loss: 0.00001368
Iteration 128/1000 | Loss: 0.00001368
Iteration 129/1000 | Loss: 0.00001368
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001367
Iteration 137/1000 | Loss: 0.00001367
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001366
Iteration 140/1000 | Loss: 0.00001366
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001365
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001364
Iteration 150/1000 | Loss: 0.00001364
Iteration 151/1000 | Loss: 0.00001364
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001364
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001364
Iteration 156/1000 | Loss: 0.00001363
Iteration 157/1000 | Loss: 0.00001363
Iteration 158/1000 | Loss: 0.00001363
Iteration 159/1000 | Loss: 0.00001363
Iteration 160/1000 | Loss: 0.00001363
Iteration 161/1000 | Loss: 0.00001363
Iteration 162/1000 | Loss: 0.00001363
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Iteration 167/1000 | Loss: 0.00001363
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001362
Iteration 175/1000 | Loss: 0.00001362
Iteration 176/1000 | Loss: 0.00001362
Iteration 177/1000 | Loss: 0.00001362
Iteration 178/1000 | Loss: 0.00001362
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001362
Iteration 185/1000 | Loss: 0.00001362
Iteration 186/1000 | Loss: 0.00001362
Iteration 187/1000 | Loss: 0.00001362
Iteration 188/1000 | Loss: 0.00001362
Iteration 189/1000 | Loss: 0.00001362
Iteration 190/1000 | Loss: 0.00001362
Iteration 191/1000 | Loss: 0.00001362
Iteration 192/1000 | Loss: 0.00001362
Iteration 193/1000 | Loss: 0.00001362
Iteration 194/1000 | Loss: 0.00001362
Iteration 195/1000 | Loss: 0.00001362
Iteration 196/1000 | Loss: 0.00001362
Iteration 197/1000 | Loss: 0.00001362
Iteration 198/1000 | Loss: 0.00001362
Iteration 199/1000 | Loss: 0.00001362
Iteration 200/1000 | Loss: 0.00001362
Iteration 201/1000 | Loss: 0.00001362
Iteration 202/1000 | Loss: 0.00001362
Iteration 203/1000 | Loss: 0.00001362
Iteration 204/1000 | Loss: 0.00001362
Iteration 205/1000 | Loss: 0.00001362
Iteration 206/1000 | Loss: 0.00001362
Iteration 207/1000 | Loss: 0.00001362
Iteration 208/1000 | Loss: 0.00001362
Iteration 209/1000 | Loss: 0.00001362
Iteration 210/1000 | Loss: 0.00001362
Iteration 211/1000 | Loss: 0.00001362
Iteration 212/1000 | Loss: 0.00001362
Iteration 213/1000 | Loss: 0.00001362
Iteration 214/1000 | Loss: 0.00001362
Iteration 215/1000 | Loss: 0.00001362
Iteration 216/1000 | Loss: 0.00001362
Iteration 217/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.3616249816550408e-05, 1.3616249816550408e-05, 1.3616249816550408e-05, 1.3616249816550408e-05, 1.3616249816550408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3616249816550408e-05

Optimization complete. Final v2v error: 3.1520280838012695 mm

Highest mean error: 3.5132739543914795 mm for frame 115

Lowest mean error: 3.0196995735168457 mm for frame 204

Saving results

Total time: 46.5746636390686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584838
Iteration 2/25 | Loss: 0.00147431
Iteration 3/25 | Loss: 0.00133304
Iteration 4/25 | Loss: 0.00129982
Iteration 5/25 | Loss: 0.00128942
Iteration 6/25 | Loss: 0.00128739
Iteration 7/25 | Loss: 0.00128681
Iteration 8/25 | Loss: 0.00128681
Iteration 9/25 | Loss: 0.00128681
Iteration 10/25 | Loss: 0.00128681
Iteration 11/25 | Loss: 0.00128681
Iteration 12/25 | Loss: 0.00128681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012868146877735853, 0.0012868146877735853, 0.0012868146877735853, 0.0012868146877735853, 0.0012868146877735853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012868146877735853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40766013
Iteration 2/25 | Loss: 0.00072214
Iteration 3/25 | Loss: 0.00072209
Iteration 4/25 | Loss: 0.00072209
Iteration 5/25 | Loss: 0.00072209
Iteration 6/25 | Loss: 0.00072209
Iteration 7/25 | Loss: 0.00072209
Iteration 8/25 | Loss: 0.00072209
Iteration 9/25 | Loss: 0.00072209
Iteration 10/25 | Loss: 0.00072209
Iteration 11/25 | Loss: 0.00072209
Iteration 12/25 | Loss: 0.00072209
Iteration 13/25 | Loss: 0.00072209
Iteration 14/25 | Loss: 0.00072209
Iteration 15/25 | Loss: 0.00072209
Iteration 16/25 | Loss: 0.00072209
Iteration 17/25 | Loss: 0.00072209
Iteration 18/25 | Loss: 0.00072209
Iteration 19/25 | Loss: 0.00072209
Iteration 20/25 | Loss: 0.00072209
Iteration 21/25 | Loss: 0.00072209
Iteration 22/25 | Loss: 0.00072209
Iteration 23/25 | Loss: 0.00072209
Iteration 24/25 | Loss: 0.00072209
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007220902480185032, 0.0007220902480185032, 0.0007220902480185032, 0.0007220902480185032, 0.0007220902480185032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007220902480185032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072209
Iteration 2/1000 | Loss: 0.00007325
Iteration 3/1000 | Loss: 0.00005063
Iteration 4/1000 | Loss: 0.00004299
Iteration 5/1000 | Loss: 0.00004056
Iteration 6/1000 | Loss: 0.00003943
Iteration 7/1000 | Loss: 0.00003860
Iteration 8/1000 | Loss: 0.00003812
Iteration 9/1000 | Loss: 0.00003751
Iteration 10/1000 | Loss: 0.00003712
Iteration 11/1000 | Loss: 0.00003681
Iteration 12/1000 | Loss: 0.00003663
Iteration 13/1000 | Loss: 0.00003653
Iteration 14/1000 | Loss: 0.00003634
Iteration 15/1000 | Loss: 0.00003622
Iteration 16/1000 | Loss: 0.00003620
Iteration 17/1000 | Loss: 0.00003612
Iteration 18/1000 | Loss: 0.00003606
Iteration 19/1000 | Loss: 0.00003601
Iteration 20/1000 | Loss: 0.00003594
Iteration 21/1000 | Loss: 0.00003593
Iteration 22/1000 | Loss: 0.00003593
Iteration 23/1000 | Loss: 0.00003592
Iteration 24/1000 | Loss: 0.00003586
Iteration 25/1000 | Loss: 0.00003585
Iteration 26/1000 | Loss: 0.00003585
Iteration 27/1000 | Loss: 0.00003584
Iteration 28/1000 | Loss: 0.00003584
Iteration 29/1000 | Loss: 0.00003583
Iteration 30/1000 | Loss: 0.00003582
Iteration 31/1000 | Loss: 0.00003582
Iteration 32/1000 | Loss: 0.00003581
Iteration 33/1000 | Loss: 0.00003581
Iteration 34/1000 | Loss: 0.00003581
Iteration 35/1000 | Loss: 0.00003580
Iteration 36/1000 | Loss: 0.00003580
Iteration 37/1000 | Loss: 0.00003580
Iteration 38/1000 | Loss: 0.00003579
Iteration 39/1000 | Loss: 0.00003579
Iteration 40/1000 | Loss: 0.00003579
Iteration 41/1000 | Loss: 0.00003578
Iteration 42/1000 | Loss: 0.00003578
Iteration 43/1000 | Loss: 0.00003578
Iteration 44/1000 | Loss: 0.00003578
Iteration 45/1000 | Loss: 0.00003577
Iteration 46/1000 | Loss: 0.00003577
Iteration 47/1000 | Loss: 0.00003577
Iteration 48/1000 | Loss: 0.00003576
Iteration 49/1000 | Loss: 0.00003576
Iteration 50/1000 | Loss: 0.00003576
Iteration 51/1000 | Loss: 0.00003576
Iteration 52/1000 | Loss: 0.00003576
Iteration 53/1000 | Loss: 0.00003575
Iteration 54/1000 | Loss: 0.00003575
Iteration 55/1000 | Loss: 0.00003575
Iteration 56/1000 | Loss: 0.00003574
Iteration 57/1000 | Loss: 0.00003574
Iteration 58/1000 | Loss: 0.00003574
Iteration 59/1000 | Loss: 0.00003574
Iteration 60/1000 | Loss: 0.00003574
Iteration 61/1000 | Loss: 0.00003573
Iteration 62/1000 | Loss: 0.00003573
Iteration 63/1000 | Loss: 0.00003573
Iteration 64/1000 | Loss: 0.00003573
Iteration 65/1000 | Loss: 0.00003573
Iteration 66/1000 | Loss: 0.00003572
Iteration 67/1000 | Loss: 0.00003572
Iteration 68/1000 | Loss: 0.00003572
Iteration 69/1000 | Loss: 0.00003572
Iteration 70/1000 | Loss: 0.00003572
Iteration 71/1000 | Loss: 0.00003571
Iteration 72/1000 | Loss: 0.00003571
Iteration 73/1000 | Loss: 0.00003571
Iteration 74/1000 | Loss: 0.00003570
Iteration 75/1000 | Loss: 0.00003570
Iteration 76/1000 | Loss: 0.00003570
Iteration 77/1000 | Loss: 0.00003569
Iteration 78/1000 | Loss: 0.00003569
Iteration 79/1000 | Loss: 0.00003569
Iteration 80/1000 | Loss: 0.00003569
Iteration 81/1000 | Loss: 0.00003568
Iteration 82/1000 | Loss: 0.00003568
Iteration 83/1000 | Loss: 0.00003568
Iteration 84/1000 | Loss: 0.00003568
Iteration 85/1000 | Loss: 0.00003567
Iteration 86/1000 | Loss: 0.00003567
Iteration 87/1000 | Loss: 0.00003567
Iteration 88/1000 | Loss: 0.00003567
Iteration 89/1000 | Loss: 0.00003566
Iteration 90/1000 | Loss: 0.00003566
Iteration 91/1000 | Loss: 0.00003566
Iteration 92/1000 | Loss: 0.00003565
Iteration 93/1000 | Loss: 0.00003565
Iteration 94/1000 | Loss: 0.00003565
Iteration 95/1000 | Loss: 0.00003565
Iteration 96/1000 | Loss: 0.00003565
Iteration 97/1000 | Loss: 0.00003565
Iteration 98/1000 | Loss: 0.00003565
Iteration 99/1000 | Loss: 0.00003565
Iteration 100/1000 | Loss: 0.00003565
Iteration 101/1000 | Loss: 0.00003565
Iteration 102/1000 | Loss: 0.00003565
Iteration 103/1000 | Loss: 0.00003564
Iteration 104/1000 | Loss: 0.00003564
Iteration 105/1000 | Loss: 0.00003564
Iteration 106/1000 | Loss: 0.00003564
Iteration 107/1000 | Loss: 0.00003564
Iteration 108/1000 | Loss: 0.00003564
Iteration 109/1000 | Loss: 0.00003563
Iteration 110/1000 | Loss: 0.00003563
Iteration 111/1000 | Loss: 0.00003563
Iteration 112/1000 | Loss: 0.00003563
Iteration 113/1000 | Loss: 0.00003563
Iteration 114/1000 | Loss: 0.00003563
Iteration 115/1000 | Loss: 0.00003563
Iteration 116/1000 | Loss: 0.00003563
Iteration 117/1000 | Loss: 0.00003563
Iteration 118/1000 | Loss: 0.00003563
Iteration 119/1000 | Loss: 0.00003563
Iteration 120/1000 | Loss: 0.00003563
Iteration 121/1000 | Loss: 0.00003563
Iteration 122/1000 | Loss: 0.00003562
Iteration 123/1000 | Loss: 0.00003562
Iteration 124/1000 | Loss: 0.00003562
Iteration 125/1000 | Loss: 0.00003562
Iteration 126/1000 | Loss: 0.00003562
Iteration 127/1000 | Loss: 0.00003562
Iteration 128/1000 | Loss: 0.00003562
Iteration 129/1000 | Loss: 0.00003562
Iteration 130/1000 | Loss: 0.00003562
Iteration 131/1000 | Loss: 0.00003561
Iteration 132/1000 | Loss: 0.00003561
Iteration 133/1000 | Loss: 0.00003561
Iteration 134/1000 | Loss: 0.00003561
Iteration 135/1000 | Loss: 0.00003561
Iteration 136/1000 | Loss: 0.00003561
Iteration 137/1000 | Loss: 0.00003561
Iteration 138/1000 | Loss: 0.00003561
Iteration 139/1000 | Loss: 0.00003561
Iteration 140/1000 | Loss: 0.00003561
Iteration 141/1000 | Loss: 0.00003561
Iteration 142/1000 | Loss: 0.00003561
Iteration 143/1000 | Loss: 0.00003561
Iteration 144/1000 | Loss: 0.00003561
Iteration 145/1000 | Loss: 0.00003561
Iteration 146/1000 | Loss: 0.00003561
Iteration 147/1000 | Loss: 0.00003561
Iteration 148/1000 | Loss: 0.00003561
Iteration 149/1000 | Loss: 0.00003561
Iteration 150/1000 | Loss: 0.00003561
Iteration 151/1000 | Loss: 0.00003561
Iteration 152/1000 | Loss: 0.00003561
Iteration 153/1000 | Loss: 0.00003560
Iteration 154/1000 | Loss: 0.00003560
Iteration 155/1000 | Loss: 0.00003560
Iteration 156/1000 | Loss: 0.00003560
Iteration 157/1000 | Loss: 0.00003560
Iteration 158/1000 | Loss: 0.00003560
Iteration 159/1000 | Loss: 0.00003560
Iteration 160/1000 | Loss: 0.00003560
Iteration 161/1000 | Loss: 0.00003560
Iteration 162/1000 | Loss: 0.00003560
Iteration 163/1000 | Loss: 0.00003560
Iteration 164/1000 | Loss: 0.00003559
Iteration 165/1000 | Loss: 0.00003559
Iteration 166/1000 | Loss: 0.00003559
Iteration 167/1000 | Loss: 0.00003559
Iteration 168/1000 | Loss: 0.00003559
Iteration 169/1000 | Loss: 0.00003559
Iteration 170/1000 | Loss: 0.00003559
Iteration 171/1000 | Loss: 0.00003559
Iteration 172/1000 | Loss: 0.00003559
Iteration 173/1000 | Loss: 0.00003559
Iteration 174/1000 | Loss: 0.00003559
Iteration 175/1000 | Loss: 0.00003559
Iteration 176/1000 | Loss: 0.00003558
Iteration 177/1000 | Loss: 0.00003558
Iteration 178/1000 | Loss: 0.00003558
Iteration 179/1000 | Loss: 0.00003558
Iteration 180/1000 | Loss: 0.00003558
Iteration 181/1000 | Loss: 0.00003558
Iteration 182/1000 | Loss: 0.00003558
Iteration 183/1000 | Loss: 0.00003558
Iteration 184/1000 | Loss: 0.00003558
Iteration 185/1000 | Loss: 0.00003558
Iteration 186/1000 | Loss: 0.00003558
Iteration 187/1000 | Loss: 0.00003558
Iteration 188/1000 | Loss: 0.00003558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [3.5582306736614555e-05, 3.5582306736614555e-05, 3.5582306736614555e-05, 3.5582306736614555e-05, 3.5582306736614555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5582306736614555e-05

Optimization complete. Final v2v error: 4.947458744049072 mm

Highest mean error: 5.621665954589844 mm for frame 121

Lowest mean error: 3.8447489738464355 mm for frame 17

Saving results

Total time: 43.69683003425598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040004
Iteration 2/25 | Loss: 0.00165910
Iteration 3/25 | Loss: 0.00138080
Iteration 4/25 | Loss: 0.00136819
Iteration 5/25 | Loss: 0.00136471
Iteration 6/25 | Loss: 0.00136456
Iteration 7/25 | Loss: 0.00136456
Iteration 8/25 | Loss: 0.00136456
Iteration 9/25 | Loss: 0.00136456
Iteration 10/25 | Loss: 0.00136456
Iteration 11/25 | Loss: 0.00136456
Iteration 12/25 | Loss: 0.00136456
Iteration 13/25 | Loss: 0.00136456
Iteration 14/25 | Loss: 0.00136456
Iteration 15/25 | Loss: 0.00136456
Iteration 16/25 | Loss: 0.00136456
Iteration 17/25 | Loss: 0.00136456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013645619619637728, 0.0013645619619637728, 0.0013645619619637728, 0.0013645619619637728, 0.0013645619619637728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013645619619637728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.38610935
Iteration 2/25 | Loss: 0.00082445
Iteration 3/25 | Loss: 0.00082445
Iteration 4/25 | Loss: 0.00082445
Iteration 5/25 | Loss: 0.00082445
Iteration 6/25 | Loss: 0.00082445
Iteration 7/25 | Loss: 0.00082445
Iteration 8/25 | Loss: 0.00082445
Iteration 9/25 | Loss: 0.00082445
Iteration 10/25 | Loss: 0.00082445
Iteration 11/25 | Loss: 0.00082445
Iteration 12/25 | Loss: 0.00082445
Iteration 13/25 | Loss: 0.00082445
Iteration 14/25 | Loss: 0.00082445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008244498749263585, 0.0008244498749263585, 0.0008244498749263585, 0.0008244498749263585, 0.0008244498749263585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008244498749263585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082445
Iteration 2/1000 | Loss: 0.00005882
Iteration 3/1000 | Loss: 0.00003829
Iteration 4/1000 | Loss: 0.00003122
Iteration 5/1000 | Loss: 0.00002877
Iteration 6/1000 | Loss: 0.00002748
Iteration 7/1000 | Loss: 0.00002640
Iteration 8/1000 | Loss: 0.00002582
Iteration 9/1000 | Loss: 0.00002530
Iteration 10/1000 | Loss: 0.00002492
Iteration 11/1000 | Loss: 0.00002466
Iteration 12/1000 | Loss: 0.00002446
Iteration 13/1000 | Loss: 0.00002429
Iteration 14/1000 | Loss: 0.00002413
Iteration 15/1000 | Loss: 0.00002412
Iteration 16/1000 | Loss: 0.00002410
Iteration 17/1000 | Loss: 0.00002403
Iteration 18/1000 | Loss: 0.00002398
Iteration 19/1000 | Loss: 0.00002398
Iteration 20/1000 | Loss: 0.00002398
Iteration 21/1000 | Loss: 0.00002398
Iteration 22/1000 | Loss: 0.00002397
Iteration 23/1000 | Loss: 0.00002397
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002391
Iteration 26/1000 | Loss: 0.00002384
Iteration 27/1000 | Loss: 0.00002384
Iteration 28/1000 | Loss: 0.00002383
Iteration 29/1000 | Loss: 0.00002380
Iteration 30/1000 | Loss: 0.00002380
Iteration 31/1000 | Loss: 0.00002380
Iteration 32/1000 | Loss: 0.00002380
Iteration 33/1000 | Loss: 0.00002379
Iteration 34/1000 | Loss: 0.00002379
Iteration 35/1000 | Loss: 0.00002377
Iteration 36/1000 | Loss: 0.00002376
Iteration 37/1000 | Loss: 0.00002376
Iteration 38/1000 | Loss: 0.00002375
Iteration 39/1000 | Loss: 0.00002374
Iteration 40/1000 | Loss: 0.00002372
Iteration 41/1000 | Loss: 0.00002372
Iteration 42/1000 | Loss: 0.00002372
Iteration 43/1000 | Loss: 0.00002372
Iteration 44/1000 | Loss: 0.00002372
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002370
Iteration 48/1000 | Loss: 0.00002370
Iteration 49/1000 | Loss: 0.00002369
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002368
Iteration 52/1000 | Loss: 0.00002368
Iteration 53/1000 | Loss: 0.00002367
Iteration 54/1000 | Loss: 0.00002367
Iteration 55/1000 | Loss: 0.00002367
Iteration 56/1000 | Loss: 0.00002367
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002367
Iteration 59/1000 | Loss: 0.00002367
Iteration 60/1000 | Loss: 0.00002367
Iteration 61/1000 | Loss: 0.00002367
Iteration 62/1000 | Loss: 0.00002367
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002367
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002366
Iteration 67/1000 | Loss: 0.00002366
Iteration 68/1000 | Loss: 0.00002365
Iteration 69/1000 | Loss: 0.00002365
Iteration 70/1000 | Loss: 0.00002365
Iteration 71/1000 | Loss: 0.00002365
Iteration 72/1000 | Loss: 0.00002365
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002363
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002363
Iteration 86/1000 | Loss: 0.00002363
Iteration 87/1000 | Loss: 0.00002363
Iteration 88/1000 | Loss: 0.00002363
Iteration 89/1000 | Loss: 0.00002363
Iteration 90/1000 | Loss: 0.00002362
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002362
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002362
Iteration 96/1000 | Loss: 0.00002362
Iteration 97/1000 | Loss: 0.00002362
Iteration 98/1000 | Loss: 0.00002362
Iteration 99/1000 | Loss: 0.00002362
Iteration 100/1000 | Loss: 0.00002362
Iteration 101/1000 | Loss: 0.00002361
Iteration 102/1000 | Loss: 0.00002361
Iteration 103/1000 | Loss: 0.00002361
Iteration 104/1000 | Loss: 0.00002361
Iteration 105/1000 | Loss: 0.00002361
Iteration 106/1000 | Loss: 0.00002360
Iteration 107/1000 | Loss: 0.00002360
Iteration 108/1000 | Loss: 0.00002360
Iteration 109/1000 | Loss: 0.00002360
Iteration 110/1000 | Loss: 0.00002360
Iteration 111/1000 | Loss: 0.00002360
Iteration 112/1000 | Loss: 0.00002359
Iteration 113/1000 | Loss: 0.00002359
Iteration 114/1000 | Loss: 0.00002359
Iteration 115/1000 | Loss: 0.00002359
Iteration 116/1000 | Loss: 0.00002359
Iteration 117/1000 | Loss: 0.00002359
Iteration 118/1000 | Loss: 0.00002359
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002359
Iteration 121/1000 | Loss: 0.00002359
Iteration 122/1000 | Loss: 0.00002359
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002359
Iteration 127/1000 | Loss: 0.00002359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.3591061108163558e-05, 2.3591061108163558e-05, 2.3591061108163558e-05, 2.3591061108163558e-05, 2.3591061108163558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3591061108163558e-05

Optimization complete. Final v2v error: 4.017780303955078 mm

Highest mean error: 4.684683799743652 mm for frame 79

Lowest mean error: 3.0869641304016113 mm for frame 0

Saving results

Total time: 41.63126277923584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773073
Iteration 2/25 | Loss: 0.00145114
Iteration 3/25 | Loss: 0.00130822
Iteration 4/25 | Loss: 0.00128984
Iteration 5/25 | Loss: 0.00128385
Iteration 6/25 | Loss: 0.00128235
Iteration 7/25 | Loss: 0.00128235
Iteration 8/25 | Loss: 0.00128235
Iteration 9/25 | Loss: 0.00128235
Iteration 10/25 | Loss: 0.00128235
Iteration 11/25 | Loss: 0.00128235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012823548167943954, 0.0012823548167943954, 0.0012823548167943954, 0.0012823548167943954, 0.0012823548167943954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012823548167943954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43318045
Iteration 2/25 | Loss: 0.00081126
Iteration 3/25 | Loss: 0.00081126
Iteration 4/25 | Loss: 0.00081126
Iteration 5/25 | Loss: 0.00081126
Iteration 6/25 | Loss: 0.00081126
Iteration 7/25 | Loss: 0.00081126
Iteration 8/25 | Loss: 0.00081126
Iteration 9/25 | Loss: 0.00081126
Iteration 10/25 | Loss: 0.00081126
Iteration 11/25 | Loss: 0.00081126
Iteration 12/25 | Loss: 0.00081126
Iteration 13/25 | Loss: 0.00081126
Iteration 14/25 | Loss: 0.00081126
Iteration 15/25 | Loss: 0.00081126
Iteration 16/25 | Loss: 0.00081126
Iteration 17/25 | Loss: 0.00081126
Iteration 18/25 | Loss: 0.00081126
Iteration 19/25 | Loss: 0.00081126
Iteration 20/25 | Loss: 0.00081126
Iteration 21/25 | Loss: 0.00081126
Iteration 22/25 | Loss: 0.00081126
Iteration 23/25 | Loss: 0.00081126
Iteration 24/25 | Loss: 0.00081126
Iteration 25/25 | Loss: 0.00081126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008112573996186256, 0.0008112573996186256, 0.0008112573996186256, 0.0008112573996186256, 0.0008112573996186256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008112573996186256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081126
Iteration 2/1000 | Loss: 0.00004813
Iteration 3/1000 | Loss: 0.00003155
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002607
Iteration 6/1000 | Loss: 0.00002444
Iteration 7/1000 | Loss: 0.00002323
Iteration 8/1000 | Loss: 0.00002248
Iteration 9/1000 | Loss: 0.00002198
Iteration 10/1000 | Loss: 0.00002149
Iteration 11/1000 | Loss: 0.00002114
Iteration 12/1000 | Loss: 0.00002088
Iteration 13/1000 | Loss: 0.00002068
Iteration 14/1000 | Loss: 0.00002062
Iteration 15/1000 | Loss: 0.00002060
Iteration 16/1000 | Loss: 0.00002059
Iteration 17/1000 | Loss: 0.00002058
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002053
Iteration 22/1000 | Loss: 0.00002053
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00002050
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00002048
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002043
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002040
Iteration 32/1000 | Loss: 0.00002040
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002037
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002036
Iteration 39/1000 | Loss: 0.00002035
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002033
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002033
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002032
Iteration 47/1000 | Loss: 0.00002031
Iteration 48/1000 | Loss: 0.00002031
Iteration 49/1000 | Loss: 0.00002031
Iteration 50/1000 | Loss: 0.00002030
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002029
Iteration 53/1000 | Loss: 0.00002029
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002028
Iteration 56/1000 | Loss: 0.00002028
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002028
Iteration 61/1000 | Loss: 0.00002028
Iteration 62/1000 | Loss: 0.00002028
Iteration 63/1000 | Loss: 0.00002028
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002026
Iteration 67/1000 | Loss: 0.00002026
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00002025
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002024
Iteration 72/1000 | Loss: 0.00002024
Iteration 73/1000 | Loss: 0.00002023
Iteration 74/1000 | Loss: 0.00002023
Iteration 75/1000 | Loss: 0.00002023
Iteration 76/1000 | Loss: 0.00002023
Iteration 77/1000 | Loss: 0.00002023
Iteration 78/1000 | Loss: 0.00002023
Iteration 79/1000 | Loss: 0.00002023
Iteration 80/1000 | Loss: 0.00002023
Iteration 81/1000 | Loss: 0.00002023
Iteration 82/1000 | Loss: 0.00002023
Iteration 83/1000 | Loss: 0.00002023
Iteration 84/1000 | Loss: 0.00002023
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002022
Iteration 87/1000 | Loss: 0.00002022
Iteration 88/1000 | Loss: 0.00002021
Iteration 89/1000 | Loss: 0.00002021
Iteration 90/1000 | Loss: 0.00002021
Iteration 91/1000 | Loss: 0.00002021
Iteration 92/1000 | Loss: 0.00002020
Iteration 93/1000 | Loss: 0.00002020
Iteration 94/1000 | Loss: 0.00002020
Iteration 95/1000 | Loss: 0.00002019
Iteration 96/1000 | Loss: 0.00002019
Iteration 97/1000 | Loss: 0.00002019
Iteration 98/1000 | Loss: 0.00002018
Iteration 99/1000 | Loss: 0.00002018
Iteration 100/1000 | Loss: 0.00002018
Iteration 101/1000 | Loss: 0.00002017
Iteration 102/1000 | Loss: 0.00002017
Iteration 103/1000 | Loss: 0.00002017
Iteration 104/1000 | Loss: 0.00002017
Iteration 105/1000 | Loss: 0.00002017
Iteration 106/1000 | Loss: 0.00002017
Iteration 107/1000 | Loss: 0.00002016
Iteration 108/1000 | Loss: 0.00002016
Iteration 109/1000 | Loss: 0.00002016
Iteration 110/1000 | Loss: 0.00002016
Iteration 111/1000 | Loss: 0.00002015
Iteration 112/1000 | Loss: 0.00002015
Iteration 113/1000 | Loss: 0.00002015
Iteration 114/1000 | Loss: 0.00002015
Iteration 115/1000 | Loss: 0.00002014
Iteration 116/1000 | Loss: 0.00002014
Iteration 117/1000 | Loss: 0.00002014
Iteration 118/1000 | Loss: 0.00002014
Iteration 119/1000 | Loss: 0.00002014
Iteration 120/1000 | Loss: 0.00002014
Iteration 121/1000 | Loss: 0.00002014
Iteration 122/1000 | Loss: 0.00002014
Iteration 123/1000 | Loss: 0.00002014
Iteration 124/1000 | Loss: 0.00002014
Iteration 125/1000 | Loss: 0.00002013
Iteration 126/1000 | Loss: 0.00002013
Iteration 127/1000 | Loss: 0.00002013
Iteration 128/1000 | Loss: 0.00002013
Iteration 129/1000 | Loss: 0.00002013
Iteration 130/1000 | Loss: 0.00002013
Iteration 131/1000 | Loss: 0.00002013
Iteration 132/1000 | Loss: 0.00002012
Iteration 133/1000 | Loss: 0.00002012
Iteration 134/1000 | Loss: 0.00002012
Iteration 135/1000 | Loss: 0.00002012
Iteration 136/1000 | Loss: 0.00002011
Iteration 137/1000 | Loss: 0.00002011
Iteration 138/1000 | Loss: 0.00002011
Iteration 139/1000 | Loss: 0.00002011
Iteration 140/1000 | Loss: 0.00002011
Iteration 141/1000 | Loss: 0.00002011
Iteration 142/1000 | Loss: 0.00002011
Iteration 143/1000 | Loss: 0.00002011
Iteration 144/1000 | Loss: 0.00002011
Iteration 145/1000 | Loss: 0.00002011
Iteration 146/1000 | Loss: 0.00002010
Iteration 147/1000 | Loss: 0.00002010
Iteration 148/1000 | Loss: 0.00002010
Iteration 149/1000 | Loss: 0.00002010
Iteration 150/1000 | Loss: 0.00002010
Iteration 151/1000 | Loss: 0.00002010
Iteration 152/1000 | Loss: 0.00002010
Iteration 153/1000 | Loss: 0.00002010
Iteration 154/1000 | Loss: 0.00002010
Iteration 155/1000 | Loss: 0.00002010
Iteration 156/1000 | Loss: 0.00002010
Iteration 157/1000 | Loss: 0.00002010
Iteration 158/1000 | Loss: 0.00002009
Iteration 159/1000 | Loss: 0.00002009
Iteration 160/1000 | Loss: 0.00002009
Iteration 161/1000 | Loss: 0.00002009
Iteration 162/1000 | Loss: 0.00002009
Iteration 163/1000 | Loss: 0.00002009
Iteration 164/1000 | Loss: 0.00002009
Iteration 165/1000 | Loss: 0.00002009
Iteration 166/1000 | Loss: 0.00002008
Iteration 167/1000 | Loss: 0.00002008
Iteration 168/1000 | Loss: 0.00002008
Iteration 169/1000 | Loss: 0.00002008
Iteration 170/1000 | Loss: 0.00002008
Iteration 171/1000 | Loss: 0.00002008
Iteration 172/1000 | Loss: 0.00002008
Iteration 173/1000 | Loss: 0.00002008
Iteration 174/1000 | Loss: 0.00002008
Iteration 175/1000 | Loss: 0.00002008
Iteration 176/1000 | Loss: 0.00002008
Iteration 177/1000 | Loss: 0.00002008
Iteration 178/1000 | Loss: 0.00002008
Iteration 179/1000 | Loss: 0.00002008
Iteration 180/1000 | Loss: 0.00002007
Iteration 181/1000 | Loss: 0.00002007
Iteration 182/1000 | Loss: 0.00002007
Iteration 183/1000 | Loss: 0.00002007
Iteration 184/1000 | Loss: 0.00002007
Iteration 185/1000 | Loss: 0.00002007
Iteration 186/1000 | Loss: 0.00002007
Iteration 187/1000 | Loss: 0.00002007
Iteration 188/1000 | Loss: 0.00002007
Iteration 189/1000 | Loss: 0.00002007
Iteration 190/1000 | Loss: 0.00002007
Iteration 191/1000 | Loss: 0.00002007
Iteration 192/1000 | Loss: 0.00002007
Iteration 193/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.0069242964382283e-05, 2.0069242964382283e-05, 2.0069242964382283e-05, 2.0069242964382283e-05, 2.0069242964382283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0069242964382283e-05

Optimization complete. Final v2v error: 3.7267580032348633 mm

Highest mean error: 4.664641857147217 mm for frame 187

Lowest mean error: 3.012721061706543 mm for frame 211

Saving results

Total time: 47.304957151412964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782938
Iteration 2/25 | Loss: 0.00160853
Iteration 3/25 | Loss: 0.00136845
Iteration 4/25 | Loss: 0.00128857
Iteration 5/25 | Loss: 0.00128438
Iteration 6/25 | Loss: 0.00126194
Iteration 7/25 | Loss: 0.00125949
Iteration 8/25 | Loss: 0.00125913
Iteration 9/25 | Loss: 0.00125897
Iteration 10/25 | Loss: 0.00125882
Iteration 11/25 | Loss: 0.00125869
Iteration 12/25 | Loss: 0.00125932
Iteration 13/25 | Loss: 0.00125957
Iteration 14/25 | Loss: 0.00125807
Iteration 15/25 | Loss: 0.00125663
Iteration 16/25 | Loss: 0.00125613
Iteration 17/25 | Loss: 0.00125590
Iteration 18/25 | Loss: 0.00125588
Iteration 19/25 | Loss: 0.00125588
Iteration 20/25 | Loss: 0.00125588
Iteration 21/25 | Loss: 0.00125588
Iteration 22/25 | Loss: 0.00125588
Iteration 23/25 | Loss: 0.00125588
Iteration 24/25 | Loss: 0.00125588
Iteration 25/25 | Loss: 0.00125588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22825098
Iteration 2/25 | Loss: 0.00086946
Iteration 3/25 | Loss: 0.00083527
Iteration 4/25 | Loss: 0.00083527
Iteration 5/25 | Loss: 0.00083527
Iteration 6/25 | Loss: 0.00083527
Iteration 7/25 | Loss: 0.00083527
Iteration 8/25 | Loss: 0.00083527
Iteration 9/25 | Loss: 0.00083527
Iteration 10/25 | Loss: 0.00083527
Iteration 11/25 | Loss: 0.00083527
Iteration 12/25 | Loss: 0.00083527
Iteration 13/25 | Loss: 0.00083527
Iteration 14/25 | Loss: 0.00083527
Iteration 15/25 | Loss: 0.00083527
Iteration 16/25 | Loss: 0.00083527
Iteration 17/25 | Loss: 0.00083527
Iteration 18/25 | Loss: 0.00083527
Iteration 19/25 | Loss: 0.00083527
Iteration 20/25 | Loss: 0.00083527
Iteration 21/25 | Loss: 0.00083527
Iteration 22/25 | Loss: 0.00083527
Iteration 23/25 | Loss: 0.00083527
Iteration 24/25 | Loss: 0.00083527
Iteration 25/25 | Loss: 0.00083527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083527
Iteration 2/1000 | Loss: 0.00006969
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00011806
Iteration 5/1000 | Loss: 0.00002727
Iteration 6/1000 | Loss: 0.00004431
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00006650
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00001741
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001719
Iteration 17/1000 | Loss: 0.00001716
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001700
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001699
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001697
Iteration 30/1000 | Loss: 0.00001697
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001688
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001687
Iteration 61/1000 | Loss: 0.00001687
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001682
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001682
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001679
Iteration 99/1000 | Loss: 0.00001679
Iteration 100/1000 | Loss: 0.00001679
Iteration 101/1000 | Loss: 0.00001679
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001678
Iteration 107/1000 | Loss: 0.00001678
Iteration 108/1000 | Loss: 0.00001677
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001676
Iteration 116/1000 | Loss: 0.00001676
Iteration 117/1000 | Loss: 0.00001676
Iteration 118/1000 | Loss: 0.00001676
Iteration 119/1000 | Loss: 0.00001676
Iteration 120/1000 | Loss: 0.00001676
Iteration 121/1000 | Loss: 0.00001676
Iteration 122/1000 | Loss: 0.00001676
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001675
Iteration 126/1000 | Loss: 0.00001675
Iteration 127/1000 | Loss: 0.00001675
Iteration 128/1000 | Loss: 0.00001675
Iteration 129/1000 | Loss: 0.00001675
Iteration 130/1000 | Loss: 0.00001675
Iteration 131/1000 | Loss: 0.00001674
Iteration 132/1000 | Loss: 0.00001674
Iteration 133/1000 | Loss: 0.00001674
Iteration 134/1000 | Loss: 0.00001674
Iteration 135/1000 | Loss: 0.00001674
Iteration 136/1000 | Loss: 0.00001674
Iteration 137/1000 | Loss: 0.00001674
Iteration 138/1000 | Loss: 0.00001674
Iteration 139/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.674319719313644e-05, 1.674319719313644e-05, 1.674319719313644e-05, 1.674319719313644e-05, 1.674319719313644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.674319719313644e-05

Optimization complete. Final v2v error: 3.44006609916687 mm

Highest mean error: 3.7623257637023926 mm for frame 141

Lowest mean error: 3.072148323059082 mm for frame 205

Saving results

Total time: 67.36780285835266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529032
Iteration 2/25 | Loss: 0.00156184
Iteration 3/25 | Loss: 0.00133973
Iteration 4/25 | Loss: 0.00132441
Iteration 5/25 | Loss: 0.00132005
Iteration 6/25 | Loss: 0.00131964
Iteration 7/25 | Loss: 0.00131964
Iteration 8/25 | Loss: 0.00131964
Iteration 9/25 | Loss: 0.00131964
Iteration 10/25 | Loss: 0.00131964
Iteration 11/25 | Loss: 0.00131964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013196357758715749, 0.0013196357758715749, 0.0013196357758715749, 0.0013196357758715749, 0.0013196357758715749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013196357758715749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80161786
Iteration 2/25 | Loss: 0.00063804
Iteration 3/25 | Loss: 0.00063799
Iteration 4/25 | Loss: 0.00063799
Iteration 5/25 | Loss: 0.00063799
Iteration 6/25 | Loss: 0.00063799
Iteration 7/25 | Loss: 0.00063799
Iteration 8/25 | Loss: 0.00063799
Iteration 9/25 | Loss: 0.00063799
Iteration 10/25 | Loss: 0.00063799
Iteration 11/25 | Loss: 0.00063799
Iteration 12/25 | Loss: 0.00063799
Iteration 13/25 | Loss: 0.00063799
Iteration 14/25 | Loss: 0.00063799
Iteration 15/25 | Loss: 0.00063799
Iteration 16/25 | Loss: 0.00063799
Iteration 17/25 | Loss: 0.00063799
Iteration 18/25 | Loss: 0.00063799
Iteration 19/25 | Loss: 0.00063799
Iteration 20/25 | Loss: 0.00063799
Iteration 21/25 | Loss: 0.00063799
Iteration 22/25 | Loss: 0.00063799
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006379868718795478, 0.0006379868718795478, 0.0006379868718795478, 0.0006379868718795478, 0.0006379868718795478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006379868718795478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063799
Iteration 2/1000 | Loss: 0.00005617
Iteration 3/1000 | Loss: 0.00003472
Iteration 4/1000 | Loss: 0.00003118
Iteration 5/1000 | Loss: 0.00002906
Iteration 6/1000 | Loss: 0.00002754
Iteration 7/1000 | Loss: 0.00002676
Iteration 8/1000 | Loss: 0.00002620
Iteration 9/1000 | Loss: 0.00002586
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002553
Iteration 12/1000 | Loss: 0.00002549
Iteration 13/1000 | Loss: 0.00002548
Iteration 14/1000 | Loss: 0.00002547
Iteration 15/1000 | Loss: 0.00002534
Iteration 16/1000 | Loss: 0.00002530
Iteration 17/1000 | Loss: 0.00002530
Iteration 18/1000 | Loss: 0.00002530
Iteration 19/1000 | Loss: 0.00002530
Iteration 20/1000 | Loss: 0.00002530
Iteration 21/1000 | Loss: 0.00002529
Iteration 22/1000 | Loss: 0.00002529
Iteration 23/1000 | Loss: 0.00002529
Iteration 24/1000 | Loss: 0.00002527
Iteration 25/1000 | Loss: 0.00002526
Iteration 26/1000 | Loss: 0.00002526
Iteration 27/1000 | Loss: 0.00002525
Iteration 28/1000 | Loss: 0.00002522
Iteration 29/1000 | Loss: 0.00002521
Iteration 30/1000 | Loss: 0.00002521
Iteration 31/1000 | Loss: 0.00002520
Iteration 32/1000 | Loss: 0.00002520
Iteration 33/1000 | Loss: 0.00002519
Iteration 34/1000 | Loss: 0.00002519
Iteration 35/1000 | Loss: 0.00002518
Iteration 36/1000 | Loss: 0.00002518
Iteration 37/1000 | Loss: 0.00002518
Iteration 38/1000 | Loss: 0.00002518
Iteration 39/1000 | Loss: 0.00002517
Iteration 40/1000 | Loss: 0.00002517
Iteration 41/1000 | Loss: 0.00002517
Iteration 42/1000 | Loss: 0.00002517
Iteration 43/1000 | Loss: 0.00002517
Iteration 44/1000 | Loss: 0.00002516
Iteration 45/1000 | Loss: 0.00002516
Iteration 46/1000 | Loss: 0.00002516
Iteration 47/1000 | Loss: 0.00002516
Iteration 48/1000 | Loss: 0.00002516
Iteration 49/1000 | Loss: 0.00002516
Iteration 50/1000 | Loss: 0.00002516
Iteration 51/1000 | Loss: 0.00002515
Iteration 52/1000 | Loss: 0.00002515
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002514
Iteration 57/1000 | Loss: 0.00002514
Iteration 58/1000 | Loss: 0.00002514
Iteration 59/1000 | Loss: 0.00002514
Iteration 60/1000 | Loss: 0.00002514
Iteration 61/1000 | Loss: 0.00002513
Iteration 62/1000 | Loss: 0.00002513
Iteration 63/1000 | Loss: 0.00002513
Iteration 64/1000 | Loss: 0.00002513
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002513
Iteration 68/1000 | Loss: 0.00002513
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002513
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002512
Iteration 76/1000 | Loss: 0.00002512
Iteration 77/1000 | Loss: 0.00002512
Iteration 78/1000 | Loss: 0.00002512
Iteration 79/1000 | Loss: 0.00002512
Iteration 80/1000 | Loss: 0.00002512
Iteration 81/1000 | Loss: 0.00002512
Iteration 82/1000 | Loss: 0.00002512
Iteration 83/1000 | Loss: 0.00002512
Iteration 84/1000 | Loss: 0.00002511
Iteration 85/1000 | Loss: 0.00002511
Iteration 86/1000 | Loss: 0.00002511
Iteration 87/1000 | Loss: 0.00002511
Iteration 88/1000 | Loss: 0.00002511
Iteration 89/1000 | Loss: 0.00002511
Iteration 90/1000 | Loss: 0.00002511
Iteration 91/1000 | Loss: 0.00002511
Iteration 92/1000 | Loss: 0.00002510
Iteration 93/1000 | Loss: 0.00002510
Iteration 94/1000 | Loss: 0.00002510
Iteration 95/1000 | Loss: 0.00002510
Iteration 96/1000 | Loss: 0.00002510
Iteration 97/1000 | Loss: 0.00002510
Iteration 98/1000 | Loss: 0.00002509
Iteration 99/1000 | Loss: 0.00002509
Iteration 100/1000 | Loss: 0.00002509
Iteration 101/1000 | Loss: 0.00002509
Iteration 102/1000 | Loss: 0.00002509
Iteration 103/1000 | Loss: 0.00002509
Iteration 104/1000 | Loss: 0.00002509
Iteration 105/1000 | Loss: 0.00002509
Iteration 106/1000 | Loss: 0.00002509
Iteration 107/1000 | Loss: 0.00002509
Iteration 108/1000 | Loss: 0.00002509
Iteration 109/1000 | Loss: 0.00002509
Iteration 110/1000 | Loss: 0.00002509
Iteration 111/1000 | Loss: 0.00002509
Iteration 112/1000 | Loss: 0.00002509
Iteration 113/1000 | Loss: 0.00002509
Iteration 114/1000 | Loss: 0.00002509
Iteration 115/1000 | Loss: 0.00002509
Iteration 116/1000 | Loss: 0.00002509
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002509
Iteration 119/1000 | Loss: 0.00002509
Iteration 120/1000 | Loss: 0.00002509
Iteration 121/1000 | Loss: 0.00002509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.5085055312956683e-05, 2.5085055312956683e-05, 2.5085055312956683e-05, 2.5085055312956683e-05, 2.5085055312956683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5085055312956683e-05

Optimization complete. Final v2v error: 4.213003158569336 mm

Highest mean error: 4.485408782958984 mm for frame 17

Lowest mean error: 3.9326157569885254 mm for frame 34

Saving results

Total time: 32.135658502578735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002696
Iteration 2/25 | Loss: 0.00156679
Iteration 3/25 | Loss: 0.00146205
Iteration 4/25 | Loss: 0.00143561
Iteration 5/25 | Loss: 0.00143019
Iteration 6/25 | Loss: 0.00142855
Iteration 7/25 | Loss: 0.00142826
Iteration 8/25 | Loss: 0.00142826
Iteration 9/25 | Loss: 0.00142826
Iteration 10/25 | Loss: 0.00142826
Iteration 11/25 | Loss: 0.00142826
Iteration 12/25 | Loss: 0.00142826
Iteration 13/25 | Loss: 0.00142826
Iteration 14/25 | Loss: 0.00142826
Iteration 15/25 | Loss: 0.00142826
Iteration 16/25 | Loss: 0.00142826
Iteration 17/25 | Loss: 0.00142826
Iteration 18/25 | Loss: 0.00142826
Iteration 19/25 | Loss: 0.00142826
Iteration 20/25 | Loss: 0.00142826
Iteration 21/25 | Loss: 0.00142826
Iteration 22/25 | Loss: 0.00142826
Iteration 23/25 | Loss: 0.00142826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014282596530392766, 0.0014282596530392766, 0.0014282596530392766, 0.0014282596530392766, 0.0014282596530392766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014282596530392766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51854777
Iteration 2/25 | Loss: 0.00168234
Iteration 3/25 | Loss: 0.00168234
Iteration 4/25 | Loss: 0.00168234
Iteration 5/25 | Loss: 0.00168234
Iteration 6/25 | Loss: 0.00168234
Iteration 7/25 | Loss: 0.00168234
Iteration 8/25 | Loss: 0.00168234
Iteration 9/25 | Loss: 0.00168234
Iteration 10/25 | Loss: 0.00168234
Iteration 11/25 | Loss: 0.00168234
Iteration 12/25 | Loss: 0.00168234
Iteration 13/25 | Loss: 0.00168234
Iteration 14/25 | Loss: 0.00168234
Iteration 15/25 | Loss: 0.00168234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016823385376483202, 0.0016823385376483202, 0.0016823385376483202, 0.0016823385376483202, 0.0016823385376483202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016823385376483202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168234
Iteration 2/1000 | Loss: 0.00006894
Iteration 3/1000 | Loss: 0.00004778
Iteration 4/1000 | Loss: 0.00004092
Iteration 5/1000 | Loss: 0.00003880
Iteration 6/1000 | Loss: 0.00003695
Iteration 7/1000 | Loss: 0.00003578
Iteration 8/1000 | Loss: 0.00003507
Iteration 9/1000 | Loss: 0.00003464
Iteration 10/1000 | Loss: 0.00003442
Iteration 11/1000 | Loss: 0.00003432
Iteration 12/1000 | Loss: 0.00003429
Iteration 13/1000 | Loss: 0.00003425
Iteration 14/1000 | Loss: 0.00003425
Iteration 15/1000 | Loss: 0.00003424
Iteration 16/1000 | Loss: 0.00003424
Iteration 17/1000 | Loss: 0.00003424
Iteration 18/1000 | Loss: 0.00003424
Iteration 19/1000 | Loss: 0.00003423
Iteration 20/1000 | Loss: 0.00003423
Iteration 21/1000 | Loss: 0.00003423
Iteration 22/1000 | Loss: 0.00003422
Iteration 23/1000 | Loss: 0.00003421
Iteration 24/1000 | Loss: 0.00003421
Iteration 25/1000 | Loss: 0.00003421
Iteration 26/1000 | Loss: 0.00003421
Iteration 27/1000 | Loss: 0.00003421
Iteration 28/1000 | Loss: 0.00003420
Iteration 29/1000 | Loss: 0.00003419
Iteration 30/1000 | Loss: 0.00003418
Iteration 31/1000 | Loss: 0.00003418
Iteration 32/1000 | Loss: 0.00003417
Iteration 33/1000 | Loss: 0.00003416
Iteration 34/1000 | Loss: 0.00003416
Iteration 35/1000 | Loss: 0.00003416
Iteration 36/1000 | Loss: 0.00003416
Iteration 37/1000 | Loss: 0.00003416
Iteration 38/1000 | Loss: 0.00003416
Iteration 39/1000 | Loss: 0.00003415
Iteration 40/1000 | Loss: 0.00003415
Iteration 41/1000 | Loss: 0.00003415
Iteration 42/1000 | Loss: 0.00003414
Iteration 43/1000 | Loss: 0.00003414
Iteration 44/1000 | Loss: 0.00003414
Iteration 45/1000 | Loss: 0.00003414
Iteration 46/1000 | Loss: 0.00003413
Iteration 47/1000 | Loss: 0.00003413
Iteration 48/1000 | Loss: 0.00003413
Iteration 49/1000 | Loss: 0.00003413
Iteration 50/1000 | Loss: 0.00003413
Iteration 51/1000 | Loss: 0.00003413
Iteration 52/1000 | Loss: 0.00003413
Iteration 53/1000 | Loss: 0.00003413
Iteration 54/1000 | Loss: 0.00003413
Iteration 55/1000 | Loss: 0.00003413
Iteration 56/1000 | Loss: 0.00003413
Iteration 57/1000 | Loss: 0.00003412
Iteration 58/1000 | Loss: 0.00003412
Iteration 59/1000 | Loss: 0.00003412
Iteration 60/1000 | Loss: 0.00003412
Iteration 61/1000 | Loss: 0.00003411
Iteration 62/1000 | Loss: 0.00003411
Iteration 63/1000 | Loss: 0.00003411
Iteration 64/1000 | Loss: 0.00003411
Iteration 65/1000 | Loss: 0.00003411
Iteration 66/1000 | Loss: 0.00003411
Iteration 67/1000 | Loss: 0.00003410
Iteration 68/1000 | Loss: 0.00003410
Iteration 69/1000 | Loss: 0.00003410
Iteration 70/1000 | Loss: 0.00003410
Iteration 71/1000 | Loss: 0.00003410
Iteration 72/1000 | Loss: 0.00003410
Iteration 73/1000 | Loss: 0.00003410
Iteration 74/1000 | Loss: 0.00003409
Iteration 75/1000 | Loss: 0.00003409
Iteration 76/1000 | Loss: 0.00003409
Iteration 77/1000 | Loss: 0.00003409
Iteration 78/1000 | Loss: 0.00003408
Iteration 79/1000 | Loss: 0.00003408
Iteration 80/1000 | Loss: 0.00003407
Iteration 81/1000 | Loss: 0.00003406
Iteration 82/1000 | Loss: 0.00003406
Iteration 83/1000 | Loss: 0.00003406
Iteration 84/1000 | Loss: 0.00003406
Iteration 85/1000 | Loss: 0.00003405
Iteration 86/1000 | Loss: 0.00003405
Iteration 87/1000 | Loss: 0.00003405
Iteration 88/1000 | Loss: 0.00003405
Iteration 89/1000 | Loss: 0.00003405
Iteration 90/1000 | Loss: 0.00003405
Iteration 91/1000 | Loss: 0.00003405
Iteration 92/1000 | Loss: 0.00003405
Iteration 93/1000 | Loss: 0.00003405
Iteration 94/1000 | Loss: 0.00003405
Iteration 95/1000 | Loss: 0.00003405
Iteration 96/1000 | Loss: 0.00003404
Iteration 97/1000 | Loss: 0.00003404
Iteration 98/1000 | Loss: 0.00003404
Iteration 99/1000 | Loss: 0.00003404
Iteration 100/1000 | Loss: 0.00003403
Iteration 101/1000 | Loss: 0.00003403
Iteration 102/1000 | Loss: 0.00003403
Iteration 103/1000 | Loss: 0.00003403
Iteration 104/1000 | Loss: 0.00003403
Iteration 105/1000 | Loss: 0.00003403
Iteration 106/1000 | Loss: 0.00003403
Iteration 107/1000 | Loss: 0.00003403
Iteration 108/1000 | Loss: 0.00003403
Iteration 109/1000 | Loss: 0.00003403
Iteration 110/1000 | Loss: 0.00003403
Iteration 111/1000 | Loss: 0.00003403
Iteration 112/1000 | Loss: 0.00003403
Iteration 113/1000 | Loss: 0.00003403
Iteration 114/1000 | Loss: 0.00003403
Iteration 115/1000 | Loss: 0.00003402
Iteration 116/1000 | Loss: 0.00003402
Iteration 117/1000 | Loss: 0.00003402
Iteration 118/1000 | Loss: 0.00003402
Iteration 119/1000 | Loss: 0.00003402
Iteration 120/1000 | Loss: 0.00003402
Iteration 121/1000 | Loss: 0.00003402
Iteration 122/1000 | Loss: 0.00003402
Iteration 123/1000 | Loss: 0.00003402
Iteration 124/1000 | Loss: 0.00003402
Iteration 125/1000 | Loss: 0.00003402
Iteration 126/1000 | Loss: 0.00003402
Iteration 127/1000 | Loss: 0.00003402
Iteration 128/1000 | Loss: 0.00003402
Iteration 129/1000 | Loss: 0.00003402
Iteration 130/1000 | Loss: 0.00003401
Iteration 131/1000 | Loss: 0.00003401
Iteration 132/1000 | Loss: 0.00003401
Iteration 133/1000 | Loss: 0.00003401
Iteration 134/1000 | Loss: 0.00003401
Iteration 135/1000 | Loss: 0.00003401
Iteration 136/1000 | Loss: 0.00003401
Iteration 137/1000 | Loss: 0.00003401
Iteration 138/1000 | Loss: 0.00003401
Iteration 139/1000 | Loss: 0.00003401
Iteration 140/1000 | Loss: 0.00003400
Iteration 141/1000 | Loss: 0.00003400
Iteration 142/1000 | Loss: 0.00003400
Iteration 143/1000 | Loss: 0.00003400
Iteration 144/1000 | Loss: 0.00003399
Iteration 145/1000 | Loss: 0.00003399
Iteration 146/1000 | Loss: 0.00003399
Iteration 147/1000 | Loss: 0.00003399
Iteration 148/1000 | Loss: 0.00003399
Iteration 149/1000 | Loss: 0.00003399
Iteration 150/1000 | Loss: 0.00003399
Iteration 151/1000 | Loss: 0.00003399
Iteration 152/1000 | Loss: 0.00003398
Iteration 153/1000 | Loss: 0.00003398
Iteration 154/1000 | Loss: 0.00003398
Iteration 155/1000 | Loss: 0.00003398
Iteration 156/1000 | Loss: 0.00003398
Iteration 157/1000 | Loss: 0.00003398
Iteration 158/1000 | Loss: 0.00003398
Iteration 159/1000 | Loss: 0.00003398
Iteration 160/1000 | Loss: 0.00003398
Iteration 161/1000 | Loss: 0.00003398
Iteration 162/1000 | Loss: 0.00003398
Iteration 163/1000 | Loss: 0.00003398
Iteration 164/1000 | Loss: 0.00003398
Iteration 165/1000 | Loss: 0.00003398
Iteration 166/1000 | Loss: 0.00003398
Iteration 167/1000 | Loss: 0.00003398
Iteration 168/1000 | Loss: 0.00003398
Iteration 169/1000 | Loss: 0.00003398
Iteration 170/1000 | Loss: 0.00003398
Iteration 171/1000 | Loss: 0.00003398
Iteration 172/1000 | Loss: 0.00003398
Iteration 173/1000 | Loss: 0.00003398
Iteration 174/1000 | Loss: 0.00003398
Iteration 175/1000 | Loss: 0.00003398
Iteration 176/1000 | Loss: 0.00003398
Iteration 177/1000 | Loss: 0.00003398
Iteration 178/1000 | Loss: 0.00003398
Iteration 179/1000 | Loss: 0.00003398
Iteration 180/1000 | Loss: 0.00003398
Iteration 181/1000 | Loss: 0.00003398
Iteration 182/1000 | Loss: 0.00003398
Iteration 183/1000 | Loss: 0.00003398
Iteration 184/1000 | Loss: 0.00003398
Iteration 185/1000 | Loss: 0.00003398
Iteration 186/1000 | Loss: 0.00003398
Iteration 187/1000 | Loss: 0.00003398
Iteration 188/1000 | Loss: 0.00003398
Iteration 189/1000 | Loss: 0.00003398
Iteration 190/1000 | Loss: 0.00003398
Iteration 191/1000 | Loss: 0.00003398
Iteration 192/1000 | Loss: 0.00003398
Iteration 193/1000 | Loss: 0.00003398
Iteration 194/1000 | Loss: 0.00003398
Iteration 195/1000 | Loss: 0.00003398
Iteration 196/1000 | Loss: 0.00003398
Iteration 197/1000 | Loss: 0.00003398
Iteration 198/1000 | Loss: 0.00003398
Iteration 199/1000 | Loss: 0.00003398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [3.39785692631267e-05, 3.39785692631267e-05, 3.39785692631267e-05, 3.39785692631267e-05, 3.39785692631267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.39785692631267e-05

Optimization complete. Final v2v error: 4.914717197418213 mm

Highest mean error: 5.278094291687012 mm for frame 7

Lowest mean error: 4.677766799926758 mm for frame 44

Saving results

Total time: 34.45867729187012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445474
Iteration 2/25 | Loss: 0.00139881
Iteration 3/25 | Loss: 0.00132275
Iteration 4/25 | Loss: 0.00131221
Iteration 5/25 | Loss: 0.00130853
Iteration 6/25 | Loss: 0.00130770
Iteration 7/25 | Loss: 0.00130744
Iteration 8/25 | Loss: 0.00130744
Iteration 9/25 | Loss: 0.00130744
Iteration 10/25 | Loss: 0.00130744
Iteration 11/25 | Loss: 0.00130744
Iteration 12/25 | Loss: 0.00130744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001307439524680376, 0.001307439524680376, 0.001307439524680376, 0.001307439524680376, 0.001307439524680376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001307439524680376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57237232
Iteration 2/25 | Loss: 0.00152134
Iteration 3/25 | Loss: 0.00152134
Iteration 4/25 | Loss: 0.00152134
Iteration 5/25 | Loss: 0.00152134
Iteration 6/25 | Loss: 0.00152134
Iteration 7/25 | Loss: 0.00152134
Iteration 8/25 | Loss: 0.00152134
Iteration 9/25 | Loss: 0.00152134
Iteration 10/25 | Loss: 0.00152134
Iteration 11/25 | Loss: 0.00152134
Iteration 12/25 | Loss: 0.00152134
Iteration 13/25 | Loss: 0.00152134
Iteration 14/25 | Loss: 0.00152134
Iteration 15/25 | Loss: 0.00152134
Iteration 16/25 | Loss: 0.00152134
Iteration 17/25 | Loss: 0.00152134
Iteration 18/25 | Loss: 0.00152134
Iteration 19/25 | Loss: 0.00152134
Iteration 20/25 | Loss: 0.00152134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001521337660960853, 0.001521337660960853, 0.001521337660960853, 0.001521337660960853, 0.001521337660960853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001521337660960853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152134
Iteration 2/1000 | Loss: 0.00004740
Iteration 3/1000 | Loss: 0.00002783
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002256
Iteration 6/1000 | Loss: 0.00002161
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002045
Iteration 10/1000 | Loss: 0.00002021
Iteration 11/1000 | Loss: 0.00002005
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00002000
Iteration 14/1000 | Loss: 0.00001996
Iteration 15/1000 | Loss: 0.00001995
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001994
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001990
Iteration 31/1000 | Loss: 0.00001990
Iteration 32/1000 | Loss: 0.00001989
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001987
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001986
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001986
Iteration 52/1000 | Loss: 0.00001985
Iteration 53/1000 | Loss: 0.00001985
Iteration 54/1000 | Loss: 0.00001985
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001984
Iteration 57/1000 | Loss: 0.00001984
Iteration 58/1000 | Loss: 0.00001984
Iteration 59/1000 | Loss: 0.00001984
Iteration 60/1000 | Loss: 0.00001984
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001984
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001983
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001982
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001982
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001982
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001982
Iteration 85/1000 | Loss: 0.00001982
Iteration 86/1000 | Loss: 0.00001982
Iteration 87/1000 | Loss: 0.00001982
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001982
Iteration 91/1000 | Loss: 0.00001982
Iteration 92/1000 | Loss: 0.00001982
Iteration 93/1000 | Loss: 0.00001982
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001982
Iteration 96/1000 | Loss: 0.00001982
Iteration 97/1000 | Loss: 0.00001982
Iteration 98/1000 | Loss: 0.00001982
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001982
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.981683999474626e-05, 1.981683999474626e-05, 1.981683999474626e-05, 1.981683999474626e-05, 1.981683999474626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.981683999474626e-05

Optimization complete. Final v2v error: 3.881305694580078 mm

Highest mean error: 4.787235736846924 mm for frame 35

Lowest mean error: 3.4751665592193604 mm for frame 73

Saving results

Total time: 31.347392320632935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976996
Iteration 2/25 | Loss: 0.00199010
Iteration 3/25 | Loss: 0.00172259
Iteration 4/25 | Loss: 0.00166395
Iteration 5/25 | Loss: 0.00164649
Iteration 6/25 | Loss: 0.00164270
Iteration 7/25 | Loss: 0.00164146
Iteration 8/25 | Loss: 0.00164146
Iteration 9/25 | Loss: 0.00164146
Iteration 10/25 | Loss: 0.00164146
Iteration 11/25 | Loss: 0.00164146
Iteration 12/25 | Loss: 0.00164146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001641458598896861, 0.001641458598896861, 0.001641458598896861, 0.001641458598896861, 0.001641458598896861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001641458598896861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53136063
Iteration 2/25 | Loss: 0.00305023
Iteration 3/25 | Loss: 0.00305021
Iteration 4/25 | Loss: 0.00305021
Iteration 5/25 | Loss: 0.00305021
Iteration 6/25 | Loss: 0.00305021
Iteration 7/25 | Loss: 0.00305021
Iteration 8/25 | Loss: 0.00305021
Iteration 9/25 | Loss: 0.00305020
Iteration 10/25 | Loss: 0.00305020
Iteration 11/25 | Loss: 0.00305020
Iteration 12/25 | Loss: 0.00305020
Iteration 13/25 | Loss: 0.00305020
Iteration 14/25 | Loss: 0.00305020
Iteration 15/25 | Loss: 0.00305020
Iteration 16/25 | Loss: 0.00305020
Iteration 17/25 | Loss: 0.00305021
Iteration 18/25 | Loss: 0.00305021
Iteration 19/25 | Loss: 0.00305021
Iteration 20/25 | Loss: 0.00305021
Iteration 21/25 | Loss: 0.00305021
Iteration 22/25 | Loss: 0.00305021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0030502050649374723, 0.0030502050649374723, 0.0030502050649374723, 0.0030502050649374723, 0.0030502050649374723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030502050649374723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305021
Iteration 2/1000 | Loss: 0.00027869
Iteration 3/1000 | Loss: 0.00020033
Iteration 4/1000 | Loss: 0.00016001
Iteration 5/1000 | Loss: 0.00195595
Iteration 6/1000 | Loss: 0.00120112
Iteration 7/1000 | Loss: 0.00214024
Iteration 8/1000 | Loss: 0.00100939
Iteration 9/1000 | Loss: 0.00028920
Iteration 10/1000 | Loss: 0.00008733
Iteration 11/1000 | Loss: 0.00053315
Iteration 12/1000 | Loss: 0.00023207
Iteration 13/1000 | Loss: 0.00090532
Iteration 14/1000 | Loss: 0.00006815
Iteration 15/1000 | Loss: 0.00029728
Iteration 16/1000 | Loss: 0.00005872
Iteration 17/1000 | Loss: 0.00011548
Iteration 18/1000 | Loss: 0.00029819
Iteration 19/1000 | Loss: 0.00054580
Iteration 20/1000 | Loss: 0.00048224
Iteration 21/1000 | Loss: 0.00048674
Iteration 22/1000 | Loss: 0.00040103
Iteration 23/1000 | Loss: 0.00006926
Iteration 24/1000 | Loss: 0.00021981
Iteration 25/1000 | Loss: 0.00042973
Iteration 26/1000 | Loss: 0.00059041
Iteration 27/1000 | Loss: 0.00053909
Iteration 28/1000 | Loss: 0.00078139
Iteration 29/1000 | Loss: 0.00058930
Iteration 30/1000 | Loss: 0.00068388
Iteration 31/1000 | Loss: 0.00005438
Iteration 32/1000 | Loss: 0.00041113
Iteration 33/1000 | Loss: 0.00081513
Iteration 34/1000 | Loss: 0.00036075
Iteration 35/1000 | Loss: 0.00032073
Iteration 36/1000 | Loss: 0.00014469
Iteration 37/1000 | Loss: 0.00055285
Iteration 38/1000 | Loss: 0.00024531
Iteration 39/1000 | Loss: 0.00039176
Iteration 40/1000 | Loss: 0.00031159
Iteration 41/1000 | Loss: 0.00036742
Iteration 42/1000 | Loss: 0.00024134
Iteration 43/1000 | Loss: 0.00007556
Iteration 44/1000 | Loss: 0.00039460
Iteration 45/1000 | Loss: 0.00095892
Iteration 46/1000 | Loss: 0.00012636
Iteration 47/1000 | Loss: 0.00081460
Iteration 48/1000 | Loss: 0.00056478
Iteration 49/1000 | Loss: 0.00006226
Iteration 50/1000 | Loss: 0.00008989
Iteration 51/1000 | Loss: 0.00030832
Iteration 52/1000 | Loss: 0.00044909
Iteration 53/1000 | Loss: 0.00042969
Iteration 54/1000 | Loss: 0.00015654
Iteration 55/1000 | Loss: 0.00010320
Iteration 56/1000 | Loss: 0.00008395
Iteration 57/1000 | Loss: 0.00026209
Iteration 58/1000 | Loss: 0.00032400
Iteration 59/1000 | Loss: 0.00036237
Iteration 60/1000 | Loss: 0.00004843
Iteration 61/1000 | Loss: 0.00037750
Iteration 62/1000 | Loss: 0.00048397
Iteration 63/1000 | Loss: 0.00028508
Iteration 64/1000 | Loss: 0.00033452
Iteration 65/1000 | Loss: 0.00048507
Iteration 66/1000 | Loss: 0.00033085
Iteration 67/1000 | Loss: 0.00020916
Iteration 68/1000 | Loss: 0.00025374
Iteration 69/1000 | Loss: 0.00043871
Iteration 70/1000 | Loss: 0.00033416
Iteration 71/1000 | Loss: 0.00042915
Iteration 72/1000 | Loss: 0.00009302
Iteration 73/1000 | Loss: 0.00005234
Iteration 74/1000 | Loss: 0.00004701
Iteration 75/1000 | Loss: 0.00004371
Iteration 76/1000 | Loss: 0.00004221
Iteration 77/1000 | Loss: 0.00004105
Iteration 78/1000 | Loss: 0.00004047
Iteration 79/1000 | Loss: 0.00003996
Iteration 80/1000 | Loss: 0.00003956
Iteration 81/1000 | Loss: 0.00003909
Iteration 82/1000 | Loss: 0.00003877
Iteration 83/1000 | Loss: 0.00003856
Iteration 84/1000 | Loss: 0.00003850
Iteration 85/1000 | Loss: 0.00003843
Iteration 86/1000 | Loss: 0.00003843
Iteration 87/1000 | Loss: 0.00003838
Iteration 88/1000 | Loss: 0.00003838
Iteration 89/1000 | Loss: 0.00003838
Iteration 90/1000 | Loss: 0.00003837
Iteration 91/1000 | Loss: 0.00003837
Iteration 92/1000 | Loss: 0.00003836
Iteration 93/1000 | Loss: 0.00003836
Iteration 94/1000 | Loss: 0.00003836
Iteration 95/1000 | Loss: 0.00003835
Iteration 96/1000 | Loss: 0.00003835
Iteration 97/1000 | Loss: 0.00003834
Iteration 98/1000 | Loss: 0.00003834
Iteration 99/1000 | Loss: 0.00003834
Iteration 100/1000 | Loss: 0.00003833
Iteration 101/1000 | Loss: 0.00003833
Iteration 102/1000 | Loss: 0.00003833
Iteration 103/1000 | Loss: 0.00003833
Iteration 104/1000 | Loss: 0.00003832
Iteration 105/1000 | Loss: 0.00003832
Iteration 106/1000 | Loss: 0.00003832
Iteration 107/1000 | Loss: 0.00003832
Iteration 108/1000 | Loss: 0.00003831
Iteration 109/1000 | Loss: 0.00003831
Iteration 110/1000 | Loss: 0.00003830
Iteration 111/1000 | Loss: 0.00003830
Iteration 112/1000 | Loss: 0.00003829
Iteration 113/1000 | Loss: 0.00003829
Iteration 114/1000 | Loss: 0.00003829
Iteration 115/1000 | Loss: 0.00003829
Iteration 116/1000 | Loss: 0.00003829
Iteration 117/1000 | Loss: 0.00003829
Iteration 118/1000 | Loss: 0.00003829
Iteration 119/1000 | Loss: 0.00003828
Iteration 120/1000 | Loss: 0.00003828
Iteration 121/1000 | Loss: 0.00003828
Iteration 122/1000 | Loss: 0.00003828
Iteration 123/1000 | Loss: 0.00003827
Iteration 124/1000 | Loss: 0.00003827
Iteration 125/1000 | Loss: 0.00003827
Iteration 126/1000 | Loss: 0.00003826
Iteration 127/1000 | Loss: 0.00003826
Iteration 128/1000 | Loss: 0.00003826
Iteration 129/1000 | Loss: 0.00003824
Iteration 130/1000 | Loss: 0.00003824
Iteration 131/1000 | Loss: 0.00003824
Iteration 132/1000 | Loss: 0.00003824
Iteration 133/1000 | Loss: 0.00003820
Iteration 134/1000 | Loss: 0.00003820
Iteration 135/1000 | Loss: 0.00003820
Iteration 136/1000 | Loss: 0.00003820
Iteration 137/1000 | Loss: 0.00003819
Iteration 138/1000 | Loss: 0.00003819
Iteration 139/1000 | Loss: 0.00003818
Iteration 140/1000 | Loss: 0.00003817
Iteration 141/1000 | Loss: 0.00003817
Iteration 142/1000 | Loss: 0.00003817
Iteration 143/1000 | Loss: 0.00003817
Iteration 144/1000 | Loss: 0.00003816
Iteration 145/1000 | Loss: 0.00003816
Iteration 146/1000 | Loss: 0.00003816
Iteration 147/1000 | Loss: 0.00003816
Iteration 148/1000 | Loss: 0.00003816
Iteration 149/1000 | Loss: 0.00003816
Iteration 150/1000 | Loss: 0.00003816
Iteration 151/1000 | Loss: 0.00003816
Iteration 152/1000 | Loss: 0.00003816
Iteration 153/1000 | Loss: 0.00003816
Iteration 154/1000 | Loss: 0.00003816
Iteration 155/1000 | Loss: 0.00003816
Iteration 156/1000 | Loss: 0.00003815
Iteration 157/1000 | Loss: 0.00003815
Iteration 158/1000 | Loss: 0.00003815
Iteration 159/1000 | Loss: 0.00003815
Iteration 160/1000 | Loss: 0.00003815
Iteration 161/1000 | Loss: 0.00003814
Iteration 162/1000 | Loss: 0.00003814
Iteration 163/1000 | Loss: 0.00003814
Iteration 164/1000 | Loss: 0.00003814
Iteration 165/1000 | Loss: 0.00003813
Iteration 166/1000 | Loss: 0.00003813
Iteration 167/1000 | Loss: 0.00003813
Iteration 168/1000 | Loss: 0.00003813
Iteration 169/1000 | Loss: 0.00003813
Iteration 170/1000 | Loss: 0.00003813
Iteration 171/1000 | Loss: 0.00003813
Iteration 172/1000 | Loss: 0.00003813
Iteration 173/1000 | Loss: 0.00003813
Iteration 174/1000 | Loss: 0.00003813
Iteration 175/1000 | Loss: 0.00003813
Iteration 176/1000 | Loss: 0.00003813
Iteration 177/1000 | Loss: 0.00003813
Iteration 178/1000 | Loss: 0.00003813
Iteration 179/1000 | Loss: 0.00003813
Iteration 180/1000 | Loss: 0.00003813
Iteration 181/1000 | Loss: 0.00003813
Iteration 182/1000 | Loss: 0.00003813
Iteration 183/1000 | Loss: 0.00003812
Iteration 184/1000 | Loss: 0.00003812
Iteration 185/1000 | Loss: 0.00003812
Iteration 186/1000 | Loss: 0.00003812
Iteration 187/1000 | Loss: 0.00003812
Iteration 188/1000 | Loss: 0.00003812
Iteration 189/1000 | Loss: 0.00003811
Iteration 190/1000 | Loss: 0.00003811
Iteration 191/1000 | Loss: 0.00003811
Iteration 192/1000 | Loss: 0.00003811
Iteration 193/1000 | Loss: 0.00003811
Iteration 194/1000 | Loss: 0.00003811
Iteration 195/1000 | Loss: 0.00003811
Iteration 196/1000 | Loss: 0.00003811
Iteration 197/1000 | Loss: 0.00003811
Iteration 198/1000 | Loss: 0.00003811
Iteration 199/1000 | Loss: 0.00003811
Iteration 200/1000 | Loss: 0.00003811
Iteration 201/1000 | Loss: 0.00003811
Iteration 202/1000 | Loss: 0.00003811
Iteration 203/1000 | Loss: 0.00003811
Iteration 204/1000 | Loss: 0.00003811
Iteration 205/1000 | Loss: 0.00003811
Iteration 206/1000 | Loss: 0.00003811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.810718408203684e-05, 3.810718408203684e-05, 3.810718408203684e-05, 3.810718408203684e-05, 3.810718408203684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.810718408203684e-05

Optimization complete. Final v2v error: 5.074886798858643 mm

Highest mean error: 7.316931247711182 mm for frame 55

Lowest mean error: 4.405219554901123 mm for frame 148

Saving results

Total time: 156.54634547233582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918131
Iteration 2/25 | Loss: 0.00171142
Iteration 3/25 | Loss: 0.00144269
Iteration 4/25 | Loss: 0.00141476
Iteration 5/25 | Loss: 0.00140178
Iteration 6/25 | Loss: 0.00139843
Iteration 7/25 | Loss: 0.00139728
Iteration 8/25 | Loss: 0.00139718
Iteration 9/25 | Loss: 0.00139718
Iteration 10/25 | Loss: 0.00139718
Iteration 11/25 | Loss: 0.00139718
Iteration 12/25 | Loss: 0.00139718
Iteration 13/25 | Loss: 0.00139718
Iteration 14/25 | Loss: 0.00139718
Iteration 15/25 | Loss: 0.00139718
Iteration 16/25 | Loss: 0.00139718
Iteration 17/25 | Loss: 0.00139718
Iteration 18/25 | Loss: 0.00139718
Iteration 19/25 | Loss: 0.00139718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001397180836647749, 0.001397180836647749, 0.001397180836647749, 0.001397180836647749, 0.001397180836647749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001397180836647749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31236100
Iteration 2/25 | Loss: 0.00172079
Iteration 3/25 | Loss: 0.00172078
Iteration 4/25 | Loss: 0.00172078
Iteration 5/25 | Loss: 0.00172078
Iteration 6/25 | Loss: 0.00172078
Iteration 7/25 | Loss: 0.00172078
Iteration 8/25 | Loss: 0.00172078
Iteration 9/25 | Loss: 0.00172078
Iteration 10/25 | Loss: 0.00172078
Iteration 11/25 | Loss: 0.00172078
Iteration 12/25 | Loss: 0.00172078
Iteration 13/25 | Loss: 0.00172078
Iteration 14/25 | Loss: 0.00172078
Iteration 15/25 | Loss: 0.00172078
Iteration 16/25 | Loss: 0.00172078
Iteration 17/25 | Loss: 0.00172078
Iteration 18/25 | Loss: 0.00172078
Iteration 19/25 | Loss: 0.00172078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017207772471010685, 0.0017207772471010685, 0.0017207772471010685, 0.0017207772471010685, 0.0017207772471010685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017207772471010685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172078
Iteration 2/1000 | Loss: 0.00006424
Iteration 3/1000 | Loss: 0.00004129
Iteration 4/1000 | Loss: 0.00003470
Iteration 5/1000 | Loss: 0.00003198
Iteration 6/1000 | Loss: 0.00003086
Iteration 7/1000 | Loss: 0.00003000
Iteration 8/1000 | Loss: 0.00002922
Iteration 9/1000 | Loss: 0.00002865
Iteration 10/1000 | Loss: 0.00002817
Iteration 11/1000 | Loss: 0.00002784
Iteration 12/1000 | Loss: 0.00002755
Iteration 13/1000 | Loss: 0.00002741
Iteration 14/1000 | Loss: 0.00002735
Iteration 15/1000 | Loss: 0.00002735
Iteration 16/1000 | Loss: 0.00002733
Iteration 17/1000 | Loss: 0.00002729
Iteration 18/1000 | Loss: 0.00002728
Iteration 19/1000 | Loss: 0.00002727
Iteration 20/1000 | Loss: 0.00002727
Iteration 21/1000 | Loss: 0.00002726
Iteration 22/1000 | Loss: 0.00002726
Iteration 23/1000 | Loss: 0.00002720
Iteration 24/1000 | Loss: 0.00002719
Iteration 25/1000 | Loss: 0.00002714
Iteration 26/1000 | Loss: 0.00002714
Iteration 27/1000 | Loss: 0.00002714
Iteration 28/1000 | Loss: 0.00002714
Iteration 29/1000 | Loss: 0.00002714
Iteration 30/1000 | Loss: 0.00002713
Iteration 31/1000 | Loss: 0.00002713
Iteration 32/1000 | Loss: 0.00002712
Iteration 33/1000 | Loss: 0.00002712
Iteration 34/1000 | Loss: 0.00002712
Iteration 35/1000 | Loss: 0.00002712
Iteration 36/1000 | Loss: 0.00002712
Iteration 37/1000 | Loss: 0.00002711
Iteration 38/1000 | Loss: 0.00002710
Iteration 39/1000 | Loss: 0.00002710
Iteration 40/1000 | Loss: 0.00002710
Iteration 41/1000 | Loss: 0.00002709
Iteration 42/1000 | Loss: 0.00002709
Iteration 43/1000 | Loss: 0.00002708
Iteration 44/1000 | Loss: 0.00002708
Iteration 45/1000 | Loss: 0.00002708
Iteration 46/1000 | Loss: 0.00002708
Iteration 47/1000 | Loss: 0.00002707
Iteration 48/1000 | Loss: 0.00002707
Iteration 49/1000 | Loss: 0.00002707
Iteration 50/1000 | Loss: 0.00002706
Iteration 51/1000 | Loss: 0.00002706
Iteration 52/1000 | Loss: 0.00002704
Iteration 53/1000 | Loss: 0.00002704
Iteration 54/1000 | Loss: 0.00002704
Iteration 55/1000 | Loss: 0.00002703
Iteration 56/1000 | Loss: 0.00002703
Iteration 57/1000 | Loss: 0.00002703
Iteration 58/1000 | Loss: 0.00002703
Iteration 59/1000 | Loss: 0.00002700
Iteration 60/1000 | Loss: 0.00002699
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002699
Iteration 64/1000 | Loss: 0.00002699
Iteration 65/1000 | Loss: 0.00002699
Iteration 66/1000 | Loss: 0.00002699
Iteration 67/1000 | Loss: 0.00002699
Iteration 68/1000 | Loss: 0.00002698
Iteration 69/1000 | Loss: 0.00002697
Iteration 70/1000 | Loss: 0.00002697
Iteration 71/1000 | Loss: 0.00002697
Iteration 72/1000 | Loss: 0.00002696
Iteration 73/1000 | Loss: 0.00002696
Iteration 74/1000 | Loss: 0.00002696
Iteration 75/1000 | Loss: 0.00002696
Iteration 76/1000 | Loss: 0.00002696
Iteration 77/1000 | Loss: 0.00002696
Iteration 78/1000 | Loss: 0.00002696
Iteration 79/1000 | Loss: 0.00002696
Iteration 80/1000 | Loss: 0.00002695
Iteration 81/1000 | Loss: 0.00002695
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002695
Iteration 84/1000 | Loss: 0.00002695
Iteration 85/1000 | Loss: 0.00002695
Iteration 86/1000 | Loss: 0.00002695
Iteration 87/1000 | Loss: 0.00002695
Iteration 88/1000 | Loss: 0.00002695
Iteration 89/1000 | Loss: 0.00002695
Iteration 90/1000 | Loss: 0.00002695
Iteration 91/1000 | Loss: 0.00002695
Iteration 92/1000 | Loss: 0.00002695
Iteration 93/1000 | Loss: 0.00002694
Iteration 94/1000 | Loss: 0.00002694
Iteration 95/1000 | Loss: 0.00002694
Iteration 96/1000 | Loss: 0.00002693
Iteration 97/1000 | Loss: 0.00002693
Iteration 98/1000 | Loss: 0.00002693
Iteration 99/1000 | Loss: 0.00002693
Iteration 100/1000 | Loss: 0.00002693
Iteration 101/1000 | Loss: 0.00002692
Iteration 102/1000 | Loss: 0.00002692
Iteration 103/1000 | Loss: 0.00002692
Iteration 104/1000 | Loss: 0.00002692
Iteration 105/1000 | Loss: 0.00002692
Iteration 106/1000 | Loss: 0.00002692
Iteration 107/1000 | Loss: 0.00002691
Iteration 108/1000 | Loss: 0.00002691
Iteration 109/1000 | Loss: 0.00002691
Iteration 110/1000 | Loss: 0.00002691
Iteration 111/1000 | Loss: 0.00002691
Iteration 112/1000 | Loss: 0.00002691
Iteration 113/1000 | Loss: 0.00002691
Iteration 114/1000 | Loss: 0.00002691
Iteration 115/1000 | Loss: 0.00002691
Iteration 116/1000 | Loss: 0.00002690
Iteration 117/1000 | Loss: 0.00002690
Iteration 118/1000 | Loss: 0.00002690
Iteration 119/1000 | Loss: 0.00002689
Iteration 120/1000 | Loss: 0.00002689
Iteration 121/1000 | Loss: 0.00002689
Iteration 122/1000 | Loss: 0.00002689
Iteration 123/1000 | Loss: 0.00002689
Iteration 124/1000 | Loss: 0.00002688
Iteration 125/1000 | Loss: 0.00002688
Iteration 126/1000 | Loss: 0.00002688
Iteration 127/1000 | Loss: 0.00002688
Iteration 128/1000 | Loss: 0.00002688
Iteration 129/1000 | Loss: 0.00002688
Iteration 130/1000 | Loss: 0.00002688
Iteration 131/1000 | Loss: 0.00002688
Iteration 132/1000 | Loss: 0.00002688
Iteration 133/1000 | Loss: 0.00002688
Iteration 134/1000 | Loss: 0.00002688
Iteration 135/1000 | Loss: 0.00002688
Iteration 136/1000 | Loss: 0.00002688
Iteration 137/1000 | Loss: 0.00002687
Iteration 138/1000 | Loss: 0.00002687
Iteration 139/1000 | Loss: 0.00002687
Iteration 140/1000 | Loss: 0.00002687
Iteration 141/1000 | Loss: 0.00002687
Iteration 142/1000 | Loss: 0.00002686
Iteration 143/1000 | Loss: 0.00002686
Iteration 144/1000 | Loss: 0.00002686
Iteration 145/1000 | Loss: 0.00002686
Iteration 146/1000 | Loss: 0.00002686
Iteration 147/1000 | Loss: 0.00002686
Iteration 148/1000 | Loss: 0.00002686
Iteration 149/1000 | Loss: 0.00002686
Iteration 150/1000 | Loss: 0.00002686
Iteration 151/1000 | Loss: 0.00002686
Iteration 152/1000 | Loss: 0.00002686
Iteration 153/1000 | Loss: 0.00002686
Iteration 154/1000 | Loss: 0.00002686
Iteration 155/1000 | Loss: 0.00002686
Iteration 156/1000 | Loss: 0.00002686
Iteration 157/1000 | Loss: 0.00002686
Iteration 158/1000 | Loss: 0.00002686
Iteration 159/1000 | Loss: 0.00002686
Iteration 160/1000 | Loss: 0.00002686
Iteration 161/1000 | Loss: 0.00002686
Iteration 162/1000 | Loss: 0.00002686
Iteration 163/1000 | Loss: 0.00002686
Iteration 164/1000 | Loss: 0.00002686
Iteration 165/1000 | Loss: 0.00002686
Iteration 166/1000 | Loss: 0.00002686
Iteration 167/1000 | Loss: 0.00002686
Iteration 168/1000 | Loss: 0.00002686
Iteration 169/1000 | Loss: 0.00002686
Iteration 170/1000 | Loss: 0.00002686
Iteration 171/1000 | Loss: 0.00002686
Iteration 172/1000 | Loss: 0.00002686
Iteration 173/1000 | Loss: 0.00002686
Iteration 174/1000 | Loss: 0.00002686
Iteration 175/1000 | Loss: 0.00002686
Iteration 176/1000 | Loss: 0.00002686
Iteration 177/1000 | Loss: 0.00002686
Iteration 178/1000 | Loss: 0.00002686
Iteration 179/1000 | Loss: 0.00002686
Iteration 180/1000 | Loss: 0.00002686
Iteration 181/1000 | Loss: 0.00002686
Iteration 182/1000 | Loss: 0.00002686
Iteration 183/1000 | Loss: 0.00002686
Iteration 184/1000 | Loss: 0.00002686
Iteration 185/1000 | Loss: 0.00002686
Iteration 186/1000 | Loss: 0.00002686
Iteration 187/1000 | Loss: 0.00002686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.6857725970330648e-05, 2.6857725970330648e-05, 2.6857725970330648e-05, 2.6857725970330648e-05, 2.6857725970330648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6857725970330648e-05

Optimization complete. Final v2v error: 4.449491500854492 mm

Highest mean error: 5.808969497680664 mm for frame 62

Lowest mean error: 3.892486333847046 mm for frame 87

Saving results

Total time: 42.959965467453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601715
Iteration 2/25 | Loss: 0.00184355
Iteration 3/25 | Loss: 0.00156399
Iteration 4/25 | Loss: 0.00153821
Iteration 5/25 | Loss: 0.00152855
Iteration 6/25 | Loss: 0.00152571
Iteration 7/25 | Loss: 0.00152534
Iteration 8/25 | Loss: 0.00152534
Iteration 9/25 | Loss: 0.00152534
Iteration 10/25 | Loss: 0.00152534
Iteration 11/25 | Loss: 0.00152534
Iteration 12/25 | Loss: 0.00152534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015253365272656083, 0.0015253365272656083, 0.0015253365272656083, 0.0015253365272656083, 0.0015253365272656083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015253365272656083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05441332
Iteration 2/25 | Loss: 0.00192542
Iteration 3/25 | Loss: 0.00192538
Iteration 4/25 | Loss: 0.00192537
Iteration 5/25 | Loss: 0.00192537
Iteration 6/25 | Loss: 0.00192537
Iteration 7/25 | Loss: 0.00192537
Iteration 8/25 | Loss: 0.00192537
Iteration 9/25 | Loss: 0.00192537
Iteration 10/25 | Loss: 0.00192537
Iteration 11/25 | Loss: 0.00192537
Iteration 12/25 | Loss: 0.00192537
Iteration 13/25 | Loss: 0.00192537
Iteration 14/25 | Loss: 0.00192537
Iteration 15/25 | Loss: 0.00192537
Iteration 16/25 | Loss: 0.00192537
Iteration 17/25 | Loss: 0.00192537
Iteration 18/25 | Loss: 0.00192537
Iteration 19/25 | Loss: 0.00192537
Iteration 20/25 | Loss: 0.00192537
Iteration 21/25 | Loss: 0.00192537
Iteration 22/25 | Loss: 0.00192537
Iteration 23/25 | Loss: 0.00192537
Iteration 24/25 | Loss: 0.00192537
Iteration 25/25 | Loss: 0.00192537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192537
Iteration 2/1000 | Loss: 0.00006865
Iteration 3/1000 | Loss: 0.00005008
Iteration 4/1000 | Loss: 0.00004396
Iteration 5/1000 | Loss: 0.00004104
Iteration 6/1000 | Loss: 0.00003949
Iteration 7/1000 | Loss: 0.00003805
Iteration 8/1000 | Loss: 0.00003691
Iteration 9/1000 | Loss: 0.00003597
Iteration 10/1000 | Loss: 0.00003533
Iteration 11/1000 | Loss: 0.00003472
Iteration 12/1000 | Loss: 0.00003429
Iteration 13/1000 | Loss: 0.00003401
Iteration 14/1000 | Loss: 0.00003374
Iteration 15/1000 | Loss: 0.00003366
Iteration 16/1000 | Loss: 0.00003350
Iteration 17/1000 | Loss: 0.00003337
Iteration 18/1000 | Loss: 0.00003328
Iteration 19/1000 | Loss: 0.00003328
Iteration 20/1000 | Loss: 0.00003320
Iteration 21/1000 | Loss: 0.00003312
Iteration 22/1000 | Loss: 0.00003299
Iteration 23/1000 | Loss: 0.00003296
Iteration 24/1000 | Loss: 0.00003293
Iteration 25/1000 | Loss: 0.00003293
Iteration 26/1000 | Loss: 0.00003292
Iteration 27/1000 | Loss: 0.00003292
Iteration 28/1000 | Loss: 0.00003291
Iteration 29/1000 | Loss: 0.00003291
Iteration 30/1000 | Loss: 0.00003290
Iteration 31/1000 | Loss: 0.00003290
Iteration 32/1000 | Loss: 0.00003289
Iteration 33/1000 | Loss: 0.00003289
Iteration 34/1000 | Loss: 0.00003289
Iteration 35/1000 | Loss: 0.00003288
Iteration 36/1000 | Loss: 0.00003288
Iteration 37/1000 | Loss: 0.00003288
Iteration 38/1000 | Loss: 0.00003288
Iteration 39/1000 | Loss: 0.00003288
Iteration 40/1000 | Loss: 0.00003288
Iteration 41/1000 | Loss: 0.00003288
Iteration 42/1000 | Loss: 0.00003288
Iteration 43/1000 | Loss: 0.00003288
Iteration 44/1000 | Loss: 0.00003288
Iteration 45/1000 | Loss: 0.00003288
Iteration 46/1000 | Loss: 0.00003288
Iteration 47/1000 | Loss: 0.00003287
Iteration 48/1000 | Loss: 0.00003287
Iteration 49/1000 | Loss: 0.00003287
Iteration 50/1000 | Loss: 0.00003287
Iteration 51/1000 | Loss: 0.00003287
Iteration 52/1000 | Loss: 0.00003285
Iteration 53/1000 | Loss: 0.00003285
Iteration 54/1000 | Loss: 0.00003284
Iteration 55/1000 | Loss: 0.00003284
Iteration 56/1000 | Loss: 0.00003282
Iteration 57/1000 | Loss: 0.00003282
Iteration 58/1000 | Loss: 0.00003282
Iteration 59/1000 | Loss: 0.00003281
Iteration 60/1000 | Loss: 0.00003281
Iteration 61/1000 | Loss: 0.00003280
Iteration 62/1000 | Loss: 0.00003280
Iteration 63/1000 | Loss: 0.00003279
Iteration 64/1000 | Loss: 0.00003278
Iteration 65/1000 | Loss: 0.00003278
Iteration 66/1000 | Loss: 0.00003278
Iteration 67/1000 | Loss: 0.00003277
Iteration 68/1000 | Loss: 0.00003277
Iteration 69/1000 | Loss: 0.00003277
Iteration 70/1000 | Loss: 0.00003277
Iteration 71/1000 | Loss: 0.00003277
Iteration 72/1000 | Loss: 0.00003277
Iteration 73/1000 | Loss: 0.00003277
Iteration 74/1000 | Loss: 0.00003277
Iteration 75/1000 | Loss: 0.00003277
Iteration 76/1000 | Loss: 0.00003276
Iteration 77/1000 | Loss: 0.00003275
Iteration 78/1000 | Loss: 0.00003274
Iteration 79/1000 | Loss: 0.00003274
Iteration 80/1000 | Loss: 0.00003274
Iteration 81/1000 | Loss: 0.00003274
Iteration 82/1000 | Loss: 0.00003273
Iteration 83/1000 | Loss: 0.00003273
Iteration 84/1000 | Loss: 0.00003273
Iteration 85/1000 | Loss: 0.00003273
Iteration 86/1000 | Loss: 0.00003273
Iteration 87/1000 | Loss: 0.00003273
Iteration 88/1000 | Loss: 0.00003273
Iteration 89/1000 | Loss: 0.00003272
Iteration 90/1000 | Loss: 0.00003272
Iteration 91/1000 | Loss: 0.00003272
Iteration 92/1000 | Loss: 0.00003272
Iteration 93/1000 | Loss: 0.00003272
Iteration 94/1000 | Loss: 0.00003272
Iteration 95/1000 | Loss: 0.00003272
Iteration 96/1000 | Loss: 0.00003271
Iteration 97/1000 | Loss: 0.00003271
Iteration 98/1000 | Loss: 0.00003271
Iteration 99/1000 | Loss: 0.00003271
Iteration 100/1000 | Loss: 0.00003270
Iteration 101/1000 | Loss: 0.00003270
Iteration 102/1000 | Loss: 0.00003270
Iteration 103/1000 | Loss: 0.00003269
Iteration 104/1000 | Loss: 0.00003269
Iteration 105/1000 | Loss: 0.00003269
Iteration 106/1000 | Loss: 0.00003269
Iteration 107/1000 | Loss: 0.00003269
Iteration 108/1000 | Loss: 0.00003269
Iteration 109/1000 | Loss: 0.00003269
Iteration 110/1000 | Loss: 0.00003268
Iteration 111/1000 | Loss: 0.00003268
Iteration 112/1000 | Loss: 0.00003268
Iteration 113/1000 | Loss: 0.00003268
Iteration 114/1000 | Loss: 0.00003268
Iteration 115/1000 | Loss: 0.00003267
Iteration 116/1000 | Loss: 0.00003267
Iteration 117/1000 | Loss: 0.00003267
Iteration 118/1000 | Loss: 0.00003267
Iteration 119/1000 | Loss: 0.00003267
Iteration 120/1000 | Loss: 0.00003267
Iteration 121/1000 | Loss: 0.00003267
Iteration 122/1000 | Loss: 0.00003267
Iteration 123/1000 | Loss: 0.00003266
Iteration 124/1000 | Loss: 0.00003266
Iteration 125/1000 | Loss: 0.00003266
Iteration 126/1000 | Loss: 0.00003266
Iteration 127/1000 | Loss: 0.00003266
Iteration 128/1000 | Loss: 0.00003266
Iteration 129/1000 | Loss: 0.00003266
Iteration 130/1000 | Loss: 0.00003266
Iteration 131/1000 | Loss: 0.00003266
Iteration 132/1000 | Loss: 0.00003265
Iteration 133/1000 | Loss: 0.00003265
Iteration 134/1000 | Loss: 0.00003265
Iteration 135/1000 | Loss: 0.00003265
Iteration 136/1000 | Loss: 0.00003265
Iteration 137/1000 | Loss: 0.00003264
Iteration 138/1000 | Loss: 0.00003264
Iteration 139/1000 | Loss: 0.00003264
Iteration 140/1000 | Loss: 0.00003264
Iteration 141/1000 | Loss: 0.00003264
Iteration 142/1000 | Loss: 0.00003264
Iteration 143/1000 | Loss: 0.00003263
Iteration 144/1000 | Loss: 0.00003263
Iteration 145/1000 | Loss: 0.00003263
Iteration 146/1000 | Loss: 0.00003263
Iteration 147/1000 | Loss: 0.00003263
Iteration 148/1000 | Loss: 0.00003263
Iteration 149/1000 | Loss: 0.00003263
Iteration 150/1000 | Loss: 0.00003263
Iteration 151/1000 | Loss: 0.00003263
Iteration 152/1000 | Loss: 0.00003263
Iteration 153/1000 | Loss: 0.00003263
Iteration 154/1000 | Loss: 0.00003263
Iteration 155/1000 | Loss: 0.00003263
Iteration 156/1000 | Loss: 0.00003263
Iteration 157/1000 | Loss: 0.00003263
Iteration 158/1000 | Loss: 0.00003262
Iteration 159/1000 | Loss: 0.00003262
Iteration 160/1000 | Loss: 0.00003262
Iteration 161/1000 | Loss: 0.00003262
Iteration 162/1000 | Loss: 0.00003262
Iteration 163/1000 | Loss: 0.00003262
Iteration 164/1000 | Loss: 0.00003261
Iteration 165/1000 | Loss: 0.00003261
Iteration 166/1000 | Loss: 0.00003261
Iteration 167/1000 | Loss: 0.00003261
Iteration 168/1000 | Loss: 0.00003261
Iteration 169/1000 | Loss: 0.00003261
Iteration 170/1000 | Loss: 0.00003261
Iteration 171/1000 | Loss: 0.00003261
Iteration 172/1000 | Loss: 0.00003261
Iteration 173/1000 | Loss: 0.00003261
Iteration 174/1000 | Loss: 0.00003261
Iteration 175/1000 | Loss: 0.00003261
Iteration 176/1000 | Loss: 0.00003261
Iteration 177/1000 | Loss: 0.00003261
Iteration 178/1000 | Loss: 0.00003260
Iteration 179/1000 | Loss: 0.00003260
Iteration 180/1000 | Loss: 0.00003260
Iteration 181/1000 | Loss: 0.00003260
Iteration 182/1000 | Loss: 0.00003260
Iteration 183/1000 | Loss: 0.00003260
Iteration 184/1000 | Loss: 0.00003260
Iteration 185/1000 | Loss: 0.00003260
Iteration 186/1000 | Loss: 0.00003260
Iteration 187/1000 | Loss: 0.00003260
Iteration 188/1000 | Loss: 0.00003259
Iteration 189/1000 | Loss: 0.00003259
Iteration 190/1000 | Loss: 0.00003259
Iteration 191/1000 | Loss: 0.00003259
Iteration 192/1000 | Loss: 0.00003259
Iteration 193/1000 | Loss: 0.00003259
Iteration 194/1000 | Loss: 0.00003259
Iteration 195/1000 | Loss: 0.00003259
Iteration 196/1000 | Loss: 0.00003259
Iteration 197/1000 | Loss: 0.00003259
Iteration 198/1000 | Loss: 0.00003259
Iteration 199/1000 | Loss: 0.00003259
Iteration 200/1000 | Loss: 0.00003259
Iteration 201/1000 | Loss: 0.00003259
Iteration 202/1000 | Loss: 0.00003258
Iteration 203/1000 | Loss: 0.00003258
Iteration 204/1000 | Loss: 0.00003258
Iteration 205/1000 | Loss: 0.00003258
Iteration 206/1000 | Loss: 0.00003258
Iteration 207/1000 | Loss: 0.00003258
Iteration 208/1000 | Loss: 0.00003258
Iteration 209/1000 | Loss: 0.00003258
Iteration 210/1000 | Loss: 0.00003258
Iteration 211/1000 | Loss: 0.00003258
Iteration 212/1000 | Loss: 0.00003258
Iteration 213/1000 | Loss: 0.00003258
Iteration 214/1000 | Loss: 0.00003258
Iteration 215/1000 | Loss: 0.00003258
Iteration 216/1000 | Loss: 0.00003258
Iteration 217/1000 | Loss: 0.00003258
Iteration 218/1000 | Loss: 0.00003258
Iteration 219/1000 | Loss: 0.00003257
Iteration 220/1000 | Loss: 0.00003257
Iteration 221/1000 | Loss: 0.00003257
Iteration 222/1000 | Loss: 0.00003257
Iteration 223/1000 | Loss: 0.00003257
Iteration 224/1000 | Loss: 0.00003257
Iteration 225/1000 | Loss: 0.00003257
Iteration 226/1000 | Loss: 0.00003257
Iteration 227/1000 | Loss: 0.00003257
Iteration 228/1000 | Loss: 0.00003257
Iteration 229/1000 | Loss: 0.00003257
Iteration 230/1000 | Loss: 0.00003257
Iteration 231/1000 | Loss: 0.00003257
Iteration 232/1000 | Loss: 0.00003257
Iteration 233/1000 | Loss: 0.00003257
Iteration 234/1000 | Loss: 0.00003257
Iteration 235/1000 | Loss: 0.00003257
Iteration 236/1000 | Loss: 0.00003257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [3.256839409004897e-05, 3.256839409004897e-05, 3.256839409004897e-05, 3.256839409004897e-05, 3.256839409004897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.256839409004897e-05

Optimization complete. Final v2v error: 4.695120334625244 mm

Highest mean error: 5.876109600067139 mm for frame 64

Lowest mean error: 3.98153018951416 mm for frame 173

Saving results

Total time: 52.3774311542511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924178
Iteration 2/25 | Loss: 0.00169312
Iteration 3/25 | Loss: 0.00148778
Iteration 4/25 | Loss: 0.00147231
Iteration 5/25 | Loss: 0.00146839
Iteration 6/25 | Loss: 0.00146699
Iteration 7/25 | Loss: 0.00146696
Iteration 8/25 | Loss: 0.00146696
Iteration 9/25 | Loss: 0.00146696
Iteration 10/25 | Loss: 0.00146696
Iteration 11/25 | Loss: 0.00146696
Iteration 12/25 | Loss: 0.00146696
Iteration 13/25 | Loss: 0.00146696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014669576194137335, 0.0014669576194137335, 0.0014669576194137335, 0.0014669576194137335, 0.0014669576194137335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014669576194137335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02158999
Iteration 2/25 | Loss: 0.00171846
Iteration 3/25 | Loss: 0.00171846
Iteration 4/25 | Loss: 0.00171846
Iteration 5/25 | Loss: 0.00171846
Iteration 6/25 | Loss: 0.00171846
Iteration 7/25 | Loss: 0.00171846
Iteration 8/25 | Loss: 0.00171846
Iteration 9/25 | Loss: 0.00171846
Iteration 10/25 | Loss: 0.00171846
Iteration 11/25 | Loss: 0.00171846
Iteration 12/25 | Loss: 0.00171846
Iteration 13/25 | Loss: 0.00171846
Iteration 14/25 | Loss: 0.00171846
Iteration 15/25 | Loss: 0.00171846
Iteration 16/25 | Loss: 0.00171846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017184586031362414, 0.0017184586031362414, 0.0017184586031362414, 0.0017184586031362414, 0.0017184586031362414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017184586031362414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171846
Iteration 2/1000 | Loss: 0.00005151
Iteration 3/1000 | Loss: 0.00004025
Iteration 4/1000 | Loss: 0.00003618
Iteration 5/1000 | Loss: 0.00003427
Iteration 6/1000 | Loss: 0.00003318
Iteration 7/1000 | Loss: 0.00003247
Iteration 8/1000 | Loss: 0.00003207
Iteration 9/1000 | Loss: 0.00003181
Iteration 10/1000 | Loss: 0.00003161
Iteration 11/1000 | Loss: 0.00003160
Iteration 12/1000 | Loss: 0.00003155
Iteration 13/1000 | Loss: 0.00003155
Iteration 14/1000 | Loss: 0.00003155
Iteration 15/1000 | Loss: 0.00003154
Iteration 16/1000 | Loss: 0.00003154
Iteration 17/1000 | Loss: 0.00003154
Iteration 18/1000 | Loss: 0.00003154
Iteration 19/1000 | Loss: 0.00003154
Iteration 20/1000 | Loss: 0.00003154
Iteration 21/1000 | Loss: 0.00003152
Iteration 22/1000 | Loss: 0.00003152
Iteration 23/1000 | Loss: 0.00003152
Iteration 24/1000 | Loss: 0.00003152
Iteration 25/1000 | Loss: 0.00003152
Iteration 26/1000 | Loss: 0.00003152
Iteration 27/1000 | Loss: 0.00003151
Iteration 28/1000 | Loss: 0.00003151
Iteration 29/1000 | Loss: 0.00003151
Iteration 30/1000 | Loss: 0.00003151
Iteration 31/1000 | Loss: 0.00003151
Iteration 32/1000 | Loss: 0.00003150
Iteration 33/1000 | Loss: 0.00003149
Iteration 34/1000 | Loss: 0.00003149
Iteration 35/1000 | Loss: 0.00003148
Iteration 36/1000 | Loss: 0.00003148
Iteration 37/1000 | Loss: 0.00003148
Iteration 38/1000 | Loss: 0.00003148
Iteration 39/1000 | Loss: 0.00003147
Iteration 40/1000 | Loss: 0.00003147
Iteration 41/1000 | Loss: 0.00003147
Iteration 42/1000 | Loss: 0.00003146
Iteration 43/1000 | Loss: 0.00003146
Iteration 44/1000 | Loss: 0.00003146
Iteration 45/1000 | Loss: 0.00003146
Iteration 46/1000 | Loss: 0.00003146
Iteration 47/1000 | Loss: 0.00003146
Iteration 48/1000 | Loss: 0.00003146
Iteration 49/1000 | Loss: 0.00003145
Iteration 50/1000 | Loss: 0.00003145
Iteration 51/1000 | Loss: 0.00003145
Iteration 52/1000 | Loss: 0.00003145
Iteration 53/1000 | Loss: 0.00003145
Iteration 54/1000 | Loss: 0.00003145
Iteration 55/1000 | Loss: 0.00003145
Iteration 56/1000 | Loss: 0.00003144
Iteration 57/1000 | Loss: 0.00003144
Iteration 58/1000 | Loss: 0.00003144
Iteration 59/1000 | Loss: 0.00003144
Iteration 60/1000 | Loss: 0.00003144
Iteration 61/1000 | Loss: 0.00003144
Iteration 62/1000 | Loss: 0.00003144
Iteration 63/1000 | Loss: 0.00003143
Iteration 64/1000 | Loss: 0.00003143
Iteration 65/1000 | Loss: 0.00003142
Iteration 66/1000 | Loss: 0.00003142
Iteration 67/1000 | Loss: 0.00003142
Iteration 68/1000 | Loss: 0.00003142
Iteration 69/1000 | Loss: 0.00003142
Iteration 70/1000 | Loss: 0.00003141
Iteration 71/1000 | Loss: 0.00003141
Iteration 72/1000 | Loss: 0.00003141
Iteration 73/1000 | Loss: 0.00003141
Iteration 74/1000 | Loss: 0.00003141
Iteration 75/1000 | Loss: 0.00003141
Iteration 76/1000 | Loss: 0.00003141
Iteration 77/1000 | Loss: 0.00003141
Iteration 78/1000 | Loss: 0.00003141
Iteration 79/1000 | Loss: 0.00003141
Iteration 80/1000 | Loss: 0.00003140
Iteration 81/1000 | Loss: 0.00003140
Iteration 82/1000 | Loss: 0.00003140
Iteration 83/1000 | Loss: 0.00003140
Iteration 84/1000 | Loss: 0.00003140
Iteration 85/1000 | Loss: 0.00003140
Iteration 86/1000 | Loss: 0.00003140
Iteration 87/1000 | Loss: 0.00003140
Iteration 88/1000 | Loss: 0.00003140
Iteration 89/1000 | Loss: 0.00003140
Iteration 90/1000 | Loss: 0.00003140
Iteration 91/1000 | Loss: 0.00003140
Iteration 92/1000 | Loss: 0.00003140
Iteration 93/1000 | Loss: 0.00003140
Iteration 94/1000 | Loss: 0.00003140
Iteration 95/1000 | Loss: 0.00003140
Iteration 96/1000 | Loss: 0.00003140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.140413173241541e-05, 3.140413173241541e-05, 3.140413173241541e-05, 3.140413173241541e-05, 3.140413173241541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.140413173241541e-05

Optimization complete. Final v2v error: 4.732811450958252 mm

Highest mean error: 4.889347553253174 mm for frame 14

Lowest mean error: 4.560215473175049 mm for frame 98

Saving results

Total time: 28.603667974472046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142559
Iteration 2/25 | Loss: 0.00233965
Iteration 3/25 | Loss: 0.00185432
Iteration 4/25 | Loss: 0.00178525
Iteration 5/25 | Loss: 0.00188514
Iteration 6/25 | Loss: 0.00178873
Iteration 7/25 | Loss: 0.00173310
Iteration 8/25 | Loss: 0.00169431
Iteration 9/25 | Loss: 0.00165366
Iteration 10/25 | Loss: 0.00164085
Iteration 11/25 | Loss: 0.00161930
Iteration 12/25 | Loss: 0.00158741
Iteration 13/25 | Loss: 0.00156951
Iteration 14/25 | Loss: 0.00156435
Iteration 15/25 | Loss: 0.00157144
Iteration 16/25 | Loss: 0.00156992
Iteration 17/25 | Loss: 0.00158050
Iteration 18/25 | Loss: 0.00158063
Iteration 19/25 | Loss: 0.00157774
Iteration 20/25 | Loss: 0.00155889
Iteration 21/25 | Loss: 0.00155111
Iteration 22/25 | Loss: 0.00155155
Iteration 23/25 | Loss: 0.00155533
Iteration 24/25 | Loss: 0.00155000
Iteration 25/25 | Loss: 0.00155260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01864791
Iteration 2/25 | Loss: 0.00272893
Iteration 3/25 | Loss: 0.00272893
Iteration 4/25 | Loss: 0.00272893
Iteration 5/25 | Loss: 0.00272893
Iteration 6/25 | Loss: 0.00272893
Iteration 7/25 | Loss: 0.00272893
Iteration 8/25 | Loss: 0.00272893
Iteration 9/25 | Loss: 0.00272893
Iteration 10/25 | Loss: 0.00272893
Iteration 11/25 | Loss: 0.00272893
Iteration 12/25 | Loss: 0.00272893
Iteration 13/25 | Loss: 0.00272893
Iteration 14/25 | Loss: 0.00272893
Iteration 15/25 | Loss: 0.00272893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002728927880525589, 0.002728927880525589, 0.002728927880525589, 0.002728927880525589, 0.002728927880525589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002728927880525589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272893
Iteration 2/1000 | Loss: 0.00100189
Iteration 3/1000 | Loss: 0.00057343
Iteration 4/1000 | Loss: 0.00160908
Iteration 5/1000 | Loss: 0.00130269
Iteration 6/1000 | Loss: 0.00125256
Iteration 7/1000 | Loss: 0.00212719
Iteration 8/1000 | Loss: 0.00135315
Iteration 9/1000 | Loss: 0.00166938
Iteration 10/1000 | Loss: 0.00211565
Iteration 11/1000 | Loss: 0.00127706
Iteration 12/1000 | Loss: 0.00114309
Iteration 13/1000 | Loss: 0.00189821
Iteration 14/1000 | Loss: 0.00086744
Iteration 15/1000 | Loss: 0.00106873
Iteration 16/1000 | Loss: 0.00060944
Iteration 17/1000 | Loss: 0.00058502
Iteration 18/1000 | Loss: 0.00060960
Iteration 19/1000 | Loss: 0.00052151
Iteration 20/1000 | Loss: 0.00049972
Iteration 21/1000 | Loss: 0.00051179
Iteration 22/1000 | Loss: 0.00038747
Iteration 23/1000 | Loss: 0.00042235
Iteration 24/1000 | Loss: 0.00043621
Iteration 25/1000 | Loss: 0.00035256
Iteration 26/1000 | Loss: 0.00040752
Iteration 27/1000 | Loss: 0.00030733
Iteration 28/1000 | Loss: 0.00018250
Iteration 29/1000 | Loss: 0.00038626
Iteration 30/1000 | Loss: 0.00053452
Iteration 31/1000 | Loss: 0.00050017
Iteration 32/1000 | Loss: 0.00040987
Iteration 33/1000 | Loss: 0.00021746
Iteration 34/1000 | Loss: 0.00049830
Iteration 35/1000 | Loss: 0.00039384
Iteration 36/1000 | Loss: 0.00028631
Iteration 37/1000 | Loss: 0.00034889
Iteration 38/1000 | Loss: 0.00033822
Iteration 39/1000 | Loss: 0.00035221
Iteration 40/1000 | Loss: 0.00028430
Iteration 41/1000 | Loss: 0.00017342
Iteration 42/1000 | Loss: 0.00008474
Iteration 43/1000 | Loss: 0.00027593
Iteration 44/1000 | Loss: 0.00018904
Iteration 45/1000 | Loss: 0.00022438
Iteration 46/1000 | Loss: 0.00033386
Iteration 47/1000 | Loss: 0.00035462
Iteration 48/1000 | Loss: 0.00024546
Iteration 49/1000 | Loss: 0.00031156
Iteration 50/1000 | Loss: 0.00031100
Iteration 51/1000 | Loss: 0.00034576
Iteration 52/1000 | Loss: 0.00020542
Iteration 53/1000 | Loss: 0.00016639
Iteration 54/1000 | Loss: 0.00049204
Iteration 55/1000 | Loss: 0.00017818
Iteration 56/1000 | Loss: 0.00021536
Iteration 57/1000 | Loss: 0.00052125
Iteration 58/1000 | Loss: 0.00049361
Iteration 59/1000 | Loss: 0.00046806
Iteration 60/1000 | Loss: 0.00041083
Iteration 61/1000 | Loss: 0.00022322
Iteration 62/1000 | Loss: 0.00020213
Iteration 63/1000 | Loss: 0.00020984
Iteration 64/1000 | Loss: 0.00034595
Iteration 65/1000 | Loss: 0.00030870
Iteration 66/1000 | Loss: 0.00042909
Iteration 67/1000 | Loss: 0.00018349
Iteration 68/1000 | Loss: 0.00020393
Iteration 69/1000 | Loss: 0.00012452
Iteration 70/1000 | Loss: 0.00014071
Iteration 71/1000 | Loss: 0.00066558
Iteration 72/1000 | Loss: 0.00031926
Iteration 73/1000 | Loss: 0.00021490
Iteration 74/1000 | Loss: 0.00020200
Iteration 75/1000 | Loss: 0.00017744
Iteration 76/1000 | Loss: 0.00043007
Iteration 77/1000 | Loss: 0.00037960
Iteration 78/1000 | Loss: 0.00026254
Iteration 79/1000 | Loss: 0.00022885
Iteration 80/1000 | Loss: 0.00009846
Iteration 81/1000 | Loss: 0.00056969
Iteration 82/1000 | Loss: 0.00043471
Iteration 83/1000 | Loss: 0.00025107
Iteration 84/1000 | Loss: 0.00007573
Iteration 85/1000 | Loss: 0.00021326
Iteration 86/1000 | Loss: 0.00018245
Iteration 87/1000 | Loss: 0.00021666
Iteration 88/1000 | Loss: 0.00016252
Iteration 89/1000 | Loss: 0.00018470
Iteration 90/1000 | Loss: 0.00021286
Iteration 91/1000 | Loss: 0.00017235
Iteration 92/1000 | Loss: 0.00022240
Iteration 93/1000 | Loss: 0.00048675
Iteration 94/1000 | Loss: 0.00039411
Iteration 95/1000 | Loss: 0.00027176
Iteration 96/1000 | Loss: 0.00022679
Iteration 97/1000 | Loss: 0.00017889
Iteration 98/1000 | Loss: 0.00008117
Iteration 99/1000 | Loss: 0.00036544
Iteration 100/1000 | Loss: 0.00042178
Iteration 101/1000 | Loss: 0.00023248
Iteration 102/1000 | Loss: 0.00032839
Iteration 103/1000 | Loss: 0.00044862
Iteration 104/1000 | Loss: 0.00057695
Iteration 105/1000 | Loss: 0.00049966
Iteration 106/1000 | Loss: 0.00023461
Iteration 107/1000 | Loss: 0.00021722
Iteration 108/1000 | Loss: 0.00069993
Iteration 109/1000 | Loss: 0.00059024
Iteration 110/1000 | Loss: 0.00025855
Iteration 111/1000 | Loss: 0.00023386
Iteration 112/1000 | Loss: 0.00021243
Iteration 113/1000 | Loss: 0.00019584
Iteration 114/1000 | Loss: 0.00008256
Iteration 115/1000 | Loss: 0.00006316
Iteration 116/1000 | Loss: 0.00005313
Iteration 117/1000 | Loss: 0.00005172
Iteration 118/1000 | Loss: 0.00092435
Iteration 119/1000 | Loss: 0.00070677
Iteration 120/1000 | Loss: 0.00065845
Iteration 121/1000 | Loss: 0.00006475
Iteration 122/1000 | Loss: 0.00028421
Iteration 123/1000 | Loss: 0.00029763
Iteration 124/1000 | Loss: 0.00027889
Iteration 125/1000 | Loss: 0.00043878
Iteration 126/1000 | Loss: 0.00029904
Iteration 127/1000 | Loss: 0.00059665
Iteration 128/1000 | Loss: 0.00030901
Iteration 129/1000 | Loss: 0.00043879
Iteration 130/1000 | Loss: 0.00019435
Iteration 131/1000 | Loss: 0.00016491
Iteration 132/1000 | Loss: 0.00005441
Iteration 133/1000 | Loss: 0.00018916
Iteration 134/1000 | Loss: 0.00036974
Iteration 135/1000 | Loss: 0.00018429
Iteration 136/1000 | Loss: 0.00022276
Iteration 137/1000 | Loss: 0.00010166
Iteration 138/1000 | Loss: 0.00014252
Iteration 139/1000 | Loss: 0.00015327
Iteration 140/1000 | Loss: 0.00013422
Iteration 141/1000 | Loss: 0.00014749
Iteration 142/1000 | Loss: 0.00005011
Iteration 143/1000 | Loss: 0.00004890
Iteration 144/1000 | Loss: 0.00014271
Iteration 145/1000 | Loss: 0.00005901
Iteration 146/1000 | Loss: 0.00004870
Iteration 147/1000 | Loss: 0.00055907
Iteration 148/1000 | Loss: 0.00092721
Iteration 149/1000 | Loss: 0.00091243
Iteration 150/1000 | Loss: 0.00028025
Iteration 151/1000 | Loss: 0.00016417
Iteration 152/1000 | Loss: 0.00006609
Iteration 153/1000 | Loss: 0.00015753
Iteration 154/1000 | Loss: 0.00005304
Iteration 155/1000 | Loss: 0.00006720
Iteration 156/1000 | Loss: 0.00038590
Iteration 157/1000 | Loss: 0.00052519
Iteration 158/1000 | Loss: 0.00057333
Iteration 159/1000 | Loss: 0.00053562
Iteration 160/1000 | Loss: 0.00032623
Iteration 161/1000 | Loss: 0.00049871
Iteration 162/1000 | Loss: 0.00031099
Iteration 163/1000 | Loss: 0.00018796
Iteration 164/1000 | Loss: 0.00013905
Iteration 165/1000 | Loss: 0.00006626
Iteration 166/1000 | Loss: 0.00035394
Iteration 167/1000 | Loss: 0.00028533
Iteration 168/1000 | Loss: 0.00028450
Iteration 169/1000 | Loss: 0.00023546
Iteration 170/1000 | Loss: 0.00024849
Iteration 171/1000 | Loss: 0.00030762
Iteration 172/1000 | Loss: 0.00024466
Iteration 173/1000 | Loss: 0.00018468
Iteration 174/1000 | Loss: 0.00021179
Iteration 175/1000 | Loss: 0.00018611
Iteration 176/1000 | Loss: 0.00011859
Iteration 177/1000 | Loss: 0.00027785
Iteration 178/1000 | Loss: 0.00031989
Iteration 179/1000 | Loss: 0.00052681
Iteration 180/1000 | Loss: 0.00032477
Iteration 181/1000 | Loss: 0.00006230
Iteration 182/1000 | Loss: 0.00015530
Iteration 183/1000 | Loss: 0.00005151
Iteration 184/1000 | Loss: 0.00004890
Iteration 185/1000 | Loss: 0.00004685
Iteration 186/1000 | Loss: 0.00022995
Iteration 187/1000 | Loss: 0.00007117
Iteration 188/1000 | Loss: 0.00021944
Iteration 189/1000 | Loss: 0.00009330
Iteration 190/1000 | Loss: 0.00004593
Iteration 191/1000 | Loss: 0.00004405
Iteration 192/1000 | Loss: 0.00004350
Iteration 193/1000 | Loss: 0.00030172
Iteration 194/1000 | Loss: 0.00070518
Iteration 195/1000 | Loss: 0.00007847
Iteration 196/1000 | Loss: 0.00005979
Iteration 197/1000 | Loss: 0.00005112
Iteration 198/1000 | Loss: 0.00004780
Iteration 199/1000 | Loss: 0.00004602
Iteration 200/1000 | Loss: 0.00004433
Iteration 201/1000 | Loss: 0.00004328
Iteration 202/1000 | Loss: 0.00004842
Iteration 203/1000 | Loss: 0.00009945
Iteration 204/1000 | Loss: 0.00016158
Iteration 205/1000 | Loss: 0.00005027
Iteration 206/1000 | Loss: 0.00007385
Iteration 207/1000 | Loss: 0.00005410
Iteration 208/1000 | Loss: 0.00014780
Iteration 209/1000 | Loss: 0.00023031
Iteration 210/1000 | Loss: 0.00016612
Iteration 211/1000 | Loss: 0.00019327
Iteration 212/1000 | Loss: 0.00018539
Iteration 213/1000 | Loss: 0.00005749
Iteration 214/1000 | Loss: 0.00013875
Iteration 215/1000 | Loss: 0.00016658
Iteration 216/1000 | Loss: 0.00005274
Iteration 217/1000 | Loss: 0.00004952
Iteration 218/1000 | Loss: 0.00004721
Iteration 219/1000 | Loss: 0.00009378
Iteration 220/1000 | Loss: 0.00013905
Iteration 221/1000 | Loss: 0.00004546
Iteration 222/1000 | Loss: 0.00011827
Iteration 223/1000 | Loss: 0.00017227
Iteration 224/1000 | Loss: 0.00005852
Iteration 225/1000 | Loss: 0.00004946
Iteration 226/1000 | Loss: 0.00004131
Iteration 227/1000 | Loss: 0.00004060
Iteration 228/1000 | Loss: 0.00004023
Iteration 229/1000 | Loss: 0.00003978
Iteration 230/1000 | Loss: 0.00003949
Iteration 231/1000 | Loss: 0.00003920
Iteration 232/1000 | Loss: 0.00003903
Iteration 233/1000 | Loss: 0.00003903
Iteration 234/1000 | Loss: 0.00003902
Iteration 235/1000 | Loss: 0.00003901
Iteration 236/1000 | Loss: 0.00003900
Iteration 237/1000 | Loss: 0.00003890
Iteration 238/1000 | Loss: 0.00003888
Iteration 239/1000 | Loss: 0.00003887
Iteration 240/1000 | Loss: 0.00003886
Iteration 241/1000 | Loss: 0.00003886
Iteration 242/1000 | Loss: 0.00003886
Iteration 243/1000 | Loss: 0.00003885
Iteration 244/1000 | Loss: 0.00003885
Iteration 245/1000 | Loss: 0.00003884
Iteration 246/1000 | Loss: 0.00003883
Iteration 247/1000 | Loss: 0.00003880
Iteration 248/1000 | Loss: 0.00003872
Iteration 249/1000 | Loss: 0.00003868
Iteration 250/1000 | Loss: 0.00003867
Iteration 251/1000 | Loss: 0.00003867
Iteration 252/1000 | Loss: 0.00003864
Iteration 253/1000 | Loss: 0.00003863
Iteration 254/1000 | Loss: 0.00003863
Iteration 255/1000 | Loss: 0.00003862
Iteration 256/1000 | Loss: 0.00003862
Iteration 257/1000 | Loss: 0.00003862
Iteration 258/1000 | Loss: 0.00003862
Iteration 259/1000 | Loss: 0.00003861
Iteration 260/1000 | Loss: 0.00003861
Iteration 261/1000 | Loss: 0.00003861
Iteration 262/1000 | Loss: 0.00003860
Iteration 263/1000 | Loss: 0.00003860
Iteration 264/1000 | Loss: 0.00003860
Iteration 265/1000 | Loss: 0.00003860
Iteration 266/1000 | Loss: 0.00003860
Iteration 267/1000 | Loss: 0.00003860
Iteration 268/1000 | Loss: 0.00003859
Iteration 269/1000 | Loss: 0.00003859
Iteration 270/1000 | Loss: 0.00003859
Iteration 271/1000 | Loss: 0.00003859
Iteration 272/1000 | Loss: 0.00003859
Iteration 273/1000 | Loss: 0.00003859
Iteration 274/1000 | Loss: 0.00003858
Iteration 275/1000 | Loss: 0.00003858
Iteration 276/1000 | Loss: 0.00003858
Iteration 277/1000 | Loss: 0.00003858
Iteration 278/1000 | Loss: 0.00003858
Iteration 279/1000 | Loss: 0.00003858
Iteration 280/1000 | Loss: 0.00003858
Iteration 281/1000 | Loss: 0.00003857
Iteration 282/1000 | Loss: 0.00003857
Iteration 283/1000 | Loss: 0.00003857
Iteration 284/1000 | Loss: 0.00003856
Iteration 285/1000 | Loss: 0.00003856
Iteration 286/1000 | Loss: 0.00003856
Iteration 287/1000 | Loss: 0.00003855
Iteration 288/1000 | Loss: 0.00003855
Iteration 289/1000 | Loss: 0.00003855
Iteration 290/1000 | Loss: 0.00003855
Iteration 291/1000 | Loss: 0.00003855
Iteration 292/1000 | Loss: 0.00003855
Iteration 293/1000 | Loss: 0.00003854
Iteration 294/1000 | Loss: 0.00003854
Iteration 295/1000 | Loss: 0.00003854
Iteration 296/1000 | Loss: 0.00003854
Iteration 297/1000 | Loss: 0.00003854
Iteration 298/1000 | Loss: 0.00003854
Iteration 299/1000 | Loss: 0.00003854
Iteration 300/1000 | Loss: 0.00003854
Iteration 301/1000 | Loss: 0.00003854
Iteration 302/1000 | Loss: 0.00003853
Iteration 303/1000 | Loss: 0.00003853
Iteration 304/1000 | Loss: 0.00003853
Iteration 305/1000 | Loss: 0.00003853
Iteration 306/1000 | Loss: 0.00003853
Iteration 307/1000 | Loss: 0.00003851
Iteration 308/1000 | Loss: 0.00003851
Iteration 309/1000 | Loss: 0.00003851
Iteration 310/1000 | Loss: 0.00003851
Iteration 311/1000 | Loss: 0.00003851
Iteration 312/1000 | Loss: 0.00029202
Iteration 313/1000 | Loss: 0.00004746
Iteration 314/1000 | Loss: 0.00004346
Iteration 315/1000 | Loss: 0.00004172
Iteration 316/1000 | Loss: 0.00004109
Iteration 317/1000 | Loss: 0.00029065
Iteration 318/1000 | Loss: 0.00033038
Iteration 319/1000 | Loss: 0.00008265
Iteration 320/1000 | Loss: 0.00005099
Iteration 321/1000 | Loss: 0.00004570
Iteration 322/1000 | Loss: 0.00004336
Iteration 323/1000 | Loss: 0.00004203
Iteration 324/1000 | Loss: 0.00004110
Iteration 325/1000 | Loss: 0.00004024
Iteration 326/1000 | Loss: 0.00003978
Iteration 327/1000 | Loss: 0.00003941
Iteration 328/1000 | Loss: 0.00003911
Iteration 329/1000 | Loss: 0.00003884
Iteration 330/1000 | Loss: 0.00003858
Iteration 331/1000 | Loss: 0.00003834
Iteration 332/1000 | Loss: 0.00003825
Iteration 333/1000 | Loss: 0.00003820
Iteration 334/1000 | Loss: 0.00003815
Iteration 335/1000 | Loss: 0.00003812
Iteration 336/1000 | Loss: 0.00003808
Iteration 337/1000 | Loss: 0.00003804
Iteration 338/1000 | Loss: 0.00003802
Iteration 339/1000 | Loss: 0.00003796
Iteration 340/1000 | Loss: 0.00003793
Iteration 341/1000 | Loss: 0.00003793
Iteration 342/1000 | Loss: 0.00003793
Iteration 343/1000 | Loss: 0.00003792
Iteration 344/1000 | Loss: 0.00003792
Iteration 345/1000 | Loss: 0.00003792
Iteration 346/1000 | Loss: 0.00003792
Iteration 347/1000 | Loss: 0.00003791
Iteration 348/1000 | Loss: 0.00003791
Iteration 349/1000 | Loss: 0.00003791
Iteration 350/1000 | Loss: 0.00003790
Iteration 351/1000 | Loss: 0.00003790
Iteration 352/1000 | Loss: 0.00003790
Iteration 353/1000 | Loss: 0.00003790
Iteration 354/1000 | Loss: 0.00003789
Iteration 355/1000 | Loss: 0.00003789
Iteration 356/1000 | Loss: 0.00003789
Iteration 357/1000 | Loss: 0.00003789
Iteration 358/1000 | Loss: 0.00003789
Iteration 359/1000 | Loss: 0.00003789
Iteration 360/1000 | Loss: 0.00003789
Iteration 361/1000 | Loss: 0.00003788
Iteration 362/1000 | Loss: 0.00003788
Iteration 363/1000 | Loss: 0.00003788
Iteration 364/1000 | Loss: 0.00003788
Iteration 365/1000 | Loss: 0.00003787
Iteration 366/1000 | Loss: 0.00003787
Iteration 367/1000 | Loss: 0.00003787
Iteration 368/1000 | Loss: 0.00003787
Iteration 369/1000 | Loss: 0.00003786
Iteration 370/1000 | Loss: 0.00003786
Iteration 371/1000 | Loss: 0.00003786
Iteration 372/1000 | Loss: 0.00003786
Iteration 373/1000 | Loss: 0.00003786
Iteration 374/1000 | Loss: 0.00003785
Iteration 375/1000 | Loss: 0.00003785
Iteration 376/1000 | Loss: 0.00003785
Iteration 377/1000 | Loss: 0.00003785
Iteration 378/1000 | Loss: 0.00003785
Iteration 379/1000 | Loss: 0.00003785
Iteration 380/1000 | Loss: 0.00003785
Iteration 381/1000 | Loss: 0.00003785
Iteration 382/1000 | Loss: 0.00003785
Iteration 383/1000 | Loss: 0.00003785
Iteration 384/1000 | Loss: 0.00003785
Iteration 385/1000 | Loss: 0.00003785
Iteration 386/1000 | Loss: 0.00003785
Iteration 387/1000 | Loss: 0.00003785
Iteration 388/1000 | Loss: 0.00003785
Iteration 389/1000 | Loss: 0.00003785
Iteration 390/1000 | Loss: 0.00003784
Iteration 391/1000 | Loss: 0.00003784
Iteration 392/1000 | Loss: 0.00003784
Iteration 393/1000 | Loss: 0.00003784
Iteration 394/1000 | Loss: 0.00003784
Iteration 395/1000 | Loss: 0.00003784
Iteration 396/1000 | Loss: 0.00003784
Iteration 397/1000 | Loss: 0.00003784
Iteration 398/1000 | Loss: 0.00003784
Iteration 399/1000 | Loss: 0.00003784
Iteration 400/1000 | Loss: 0.00003784
Iteration 401/1000 | Loss: 0.00028799
Iteration 402/1000 | Loss: 0.00023993
Iteration 403/1000 | Loss: 0.00004974
Iteration 404/1000 | Loss: 0.00005830
Iteration 405/1000 | Loss: 0.00004305
Iteration 406/1000 | Loss: 0.00004054
Iteration 407/1000 | Loss: 0.00003938
Iteration 408/1000 | Loss: 0.00003896
Iteration 409/1000 | Loss: 0.00003865
Iteration 410/1000 | Loss: 0.00003839
Iteration 411/1000 | Loss: 0.00006051
Iteration 412/1000 | Loss: 0.00003996
Iteration 413/1000 | Loss: 0.00006404
Iteration 414/1000 | Loss: 0.00006403
Iteration 415/1000 | Loss: 0.00004132
Iteration 416/1000 | Loss: 0.00005324
Iteration 417/1000 | Loss: 0.00003907
Iteration 418/1000 | Loss: 0.00003862
Iteration 419/1000 | Loss: 0.00003809
Iteration 420/1000 | Loss: 0.00003768
Iteration 421/1000 | Loss: 0.00003757
Iteration 422/1000 | Loss: 0.00003745
Iteration 423/1000 | Loss: 0.00003742
Iteration 424/1000 | Loss: 0.00003741
Iteration 425/1000 | Loss: 0.00003741
Iteration 426/1000 | Loss: 0.00003741
Iteration 427/1000 | Loss: 0.00003741
Iteration 428/1000 | Loss: 0.00003740
Iteration 429/1000 | Loss: 0.00003738
Iteration 430/1000 | Loss: 0.00003738
Iteration 431/1000 | Loss: 0.00003737
Iteration 432/1000 | Loss: 0.00003736
Iteration 433/1000 | Loss: 0.00003736
Iteration 434/1000 | Loss: 0.00003736
Iteration 435/1000 | Loss: 0.00003736
Iteration 436/1000 | Loss: 0.00003736
Iteration 437/1000 | Loss: 0.00003736
Iteration 438/1000 | Loss: 0.00003736
Iteration 439/1000 | Loss: 0.00003735
Iteration 440/1000 | Loss: 0.00003733
Iteration 441/1000 | Loss: 0.00003733
Iteration 442/1000 | Loss: 0.00003730
Iteration 443/1000 | Loss: 0.00003729
Iteration 444/1000 | Loss: 0.00003729
Iteration 445/1000 | Loss: 0.00003728
Iteration 446/1000 | Loss: 0.00003728
Iteration 447/1000 | Loss: 0.00003728
Iteration 448/1000 | Loss: 0.00003728
Iteration 449/1000 | Loss: 0.00003728
Iteration 450/1000 | Loss: 0.00003727
Iteration 451/1000 | Loss: 0.00003727
Iteration 452/1000 | Loss: 0.00003727
Iteration 453/1000 | Loss: 0.00003726
Iteration 454/1000 | Loss: 0.00003726
Iteration 455/1000 | Loss: 0.00003725
Iteration 456/1000 | Loss: 0.00003725
Iteration 457/1000 | Loss: 0.00003725
Iteration 458/1000 | Loss: 0.00003724
Iteration 459/1000 | Loss: 0.00003724
Iteration 460/1000 | Loss: 0.00003724
Iteration 461/1000 | Loss: 0.00003724
Iteration 462/1000 | Loss: 0.00003724
Iteration 463/1000 | Loss: 0.00003724
Iteration 464/1000 | Loss: 0.00003724
Iteration 465/1000 | Loss: 0.00003724
Iteration 466/1000 | Loss: 0.00003724
Iteration 467/1000 | Loss: 0.00003723
Iteration 468/1000 | Loss: 0.00003723
Iteration 469/1000 | Loss: 0.00003723
Iteration 470/1000 | Loss: 0.00003723
Iteration 471/1000 | Loss: 0.00003723
Iteration 472/1000 | Loss: 0.00003723
Iteration 473/1000 | Loss: 0.00003723
Iteration 474/1000 | Loss: 0.00003723
Iteration 475/1000 | Loss: 0.00003722
Iteration 476/1000 | Loss: 0.00003722
Iteration 477/1000 | Loss: 0.00003722
Iteration 478/1000 | Loss: 0.00003722
Iteration 479/1000 | Loss: 0.00003722
Iteration 480/1000 | Loss: 0.00003722
Iteration 481/1000 | Loss: 0.00003722
Iteration 482/1000 | Loss: 0.00003722
Iteration 483/1000 | Loss: 0.00003722
Iteration 484/1000 | Loss: 0.00003722
Iteration 485/1000 | Loss: 0.00003722
Iteration 486/1000 | Loss: 0.00003722
Iteration 487/1000 | Loss: 0.00003722
Iteration 488/1000 | Loss: 0.00003722
Iteration 489/1000 | Loss: 0.00003722
Iteration 490/1000 | Loss: 0.00003722
Iteration 491/1000 | Loss: 0.00003722
Iteration 492/1000 | Loss: 0.00003722
Iteration 493/1000 | Loss: 0.00003722
Iteration 494/1000 | Loss: 0.00003722
Iteration 495/1000 | Loss: 0.00003722
Iteration 496/1000 | Loss: 0.00003722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 496. Stopping optimization.
Last 5 losses: [3.7222638638922945e-05, 3.7222638638922945e-05, 3.7222638638922945e-05, 3.7222638638922945e-05, 3.7222638638922945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7222638638922945e-05

Optimization complete. Final v2v error: 5.236460208892822 mm

Highest mean error: 6.641210079193115 mm for frame 135

Lowest mean error: 4.801905155181885 mm for frame 64

Saving results

Total time: 463.70770740509033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029618
Iteration 2/25 | Loss: 0.00146688
Iteration 3/25 | Loss: 0.00138985
Iteration 4/25 | Loss: 0.00137397
Iteration 5/25 | Loss: 0.00136797
Iteration 6/25 | Loss: 0.00136760
Iteration 7/25 | Loss: 0.00136760
Iteration 8/25 | Loss: 0.00136760
Iteration 9/25 | Loss: 0.00136760
Iteration 10/25 | Loss: 0.00136760
Iteration 11/25 | Loss: 0.00136760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013676038943231106, 0.0013676038943231106, 0.0013676038943231106, 0.0013676038943231106, 0.0013676038943231106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013676038943231106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55420828
Iteration 2/25 | Loss: 0.00159705
Iteration 3/25 | Loss: 0.00159705
Iteration 4/25 | Loss: 0.00159705
Iteration 5/25 | Loss: 0.00159705
Iteration 6/25 | Loss: 0.00159705
Iteration 7/25 | Loss: 0.00159704
Iteration 8/25 | Loss: 0.00159704
Iteration 9/25 | Loss: 0.00159704
Iteration 10/25 | Loss: 0.00159704
Iteration 11/25 | Loss: 0.00159704
Iteration 12/25 | Loss: 0.00159704
Iteration 13/25 | Loss: 0.00159704
Iteration 14/25 | Loss: 0.00159704
Iteration 15/25 | Loss: 0.00159704
Iteration 16/25 | Loss: 0.00159704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001597044407390058, 0.001597044407390058, 0.001597044407390058, 0.001597044407390058, 0.001597044407390058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001597044407390058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159704
Iteration 2/1000 | Loss: 0.00004829
Iteration 3/1000 | Loss: 0.00003405
Iteration 4/1000 | Loss: 0.00003070
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002824
Iteration 7/1000 | Loss: 0.00002771
Iteration 8/1000 | Loss: 0.00002726
Iteration 9/1000 | Loss: 0.00002699
Iteration 10/1000 | Loss: 0.00002695
Iteration 11/1000 | Loss: 0.00002694
Iteration 12/1000 | Loss: 0.00002694
Iteration 13/1000 | Loss: 0.00002691
Iteration 14/1000 | Loss: 0.00002690
Iteration 15/1000 | Loss: 0.00002689
Iteration 16/1000 | Loss: 0.00002689
Iteration 17/1000 | Loss: 0.00002689
Iteration 18/1000 | Loss: 0.00002688
Iteration 19/1000 | Loss: 0.00002687
Iteration 20/1000 | Loss: 0.00002687
Iteration 21/1000 | Loss: 0.00002687
Iteration 22/1000 | Loss: 0.00002686
Iteration 23/1000 | Loss: 0.00002685
Iteration 24/1000 | Loss: 0.00002685
Iteration 25/1000 | Loss: 0.00002684
Iteration 26/1000 | Loss: 0.00002684
Iteration 27/1000 | Loss: 0.00002684
Iteration 28/1000 | Loss: 0.00002684
Iteration 29/1000 | Loss: 0.00002684
Iteration 30/1000 | Loss: 0.00002684
Iteration 31/1000 | Loss: 0.00002683
Iteration 32/1000 | Loss: 0.00002682
Iteration 33/1000 | Loss: 0.00002682
Iteration 34/1000 | Loss: 0.00002681
Iteration 35/1000 | Loss: 0.00002681
Iteration 36/1000 | Loss: 0.00002681
Iteration 37/1000 | Loss: 0.00002681
Iteration 38/1000 | Loss: 0.00002681
Iteration 39/1000 | Loss: 0.00002681
Iteration 40/1000 | Loss: 0.00002681
Iteration 41/1000 | Loss: 0.00002680
Iteration 42/1000 | Loss: 0.00002680
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002680
Iteration 45/1000 | Loss: 0.00002680
Iteration 46/1000 | Loss: 0.00002680
Iteration 47/1000 | Loss: 0.00002680
Iteration 48/1000 | Loss: 0.00002680
Iteration 49/1000 | Loss: 0.00002679
Iteration 50/1000 | Loss: 0.00002679
Iteration 51/1000 | Loss: 0.00002679
Iteration 52/1000 | Loss: 0.00002678
Iteration 53/1000 | Loss: 0.00002678
Iteration 54/1000 | Loss: 0.00002677
Iteration 55/1000 | Loss: 0.00002677
Iteration 56/1000 | Loss: 0.00002677
Iteration 57/1000 | Loss: 0.00002677
Iteration 58/1000 | Loss: 0.00002676
Iteration 59/1000 | Loss: 0.00002676
Iteration 60/1000 | Loss: 0.00002676
Iteration 61/1000 | Loss: 0.00002676
Iteration 62/1000 | Loss: 0.00002676
Iteration 63/1000 | Loss: 0.00002676
Iteration 64/1000 | Loss: 0.00002675
Iteration 65/1000 | Loss: 0.00002675
Iteration 66/1000 | Loss: 0.00002675
Iteration 67/1000 | Loss: 0.00002674
Iteration 68/1000 | Loss: 0.00002674
Iteration 69/1000 | Loss: 0.00002674
Iteration 70/1000 | Loss: 0.00002673
Iteration 71/1000 | Loss: 0.00002673
Iteration 72/1000 | Loss: 0.00002673
Iteration 73/1000 | Loss: 0.00002673
Iteration 74/1000 | Loss: 0.00002673
Iteration 75/1000 | Loss: 0.00002673
Iteration 76/1000 | Loss: 0.00002673
Iteration 77/1000 | Loss: 0.00002673
Iteration 78/1000 | Loss: 0.00002673
Iteration 79/1000 | Loss: 0.00002673
Iteration 80/1000 | Loss: 0.00002672
Iteration 81/1000 | Loss: 0.00002672
Iteration 82/1000 | Loss: 0.00002672
Iteration 83/1000 | Loss: 0.00002672
Iteration 84/1000 | Loss: 0.00002672
Iteration 85/1000 | Loss: 0.00002672
Iteration 86/1000 | Loss: 0.00002672
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00002671
Iteration 89/1000 | Loss: 0.00002671
Iteration 90/1000 | Loss: 0.00002671
Iteration 91/1000 | Loss: 0.00002671
Iteration 92/1000 | Loss: 0.00002670
Iteration 93/1000 | Loss: 0.00002670
Iteration 94/1000 | Loss: 0.00002670
Iteration 95/1000 | Loss: 0.00002670
Iteration 96/1000 | Loss: 0.00002670
Iteration 97/1000 | Loss: 0.00002670
Iteration 98/1000 | Loss: 0.00002670
Iteration 99/1000 | Loss: 0.00002670
Iteration 100/1000 | Loss: 0.00002670
Iteration 101/1000 | Loss: 0.00002670
Iteration 102/1000 | Loss: 0.00002670
Iteration 103/1000 | Loss: 0.00002670
Iteration 104/1000 | Loss: 0.00002670
Iteration 105/1000 | Loss: 0.00002670
Iteration 106/1000 | Loss: 0.00002669
Iteration 107/1000 | Loss: 0.00002669
Iteration 108/1000 | Loss: 0.00002669
Iteration 109/1000 | Loss: 0.00002669
Iteration 110/1000 | Loss: 0.00002669
Iteration 111/1000 | Loss: 0.00002669
Iteration 112/1000 | Loss: 0.00002669
Iteration 113/1000 | Loss: 0.00002669
Iteration 114/1000 | Loss: 0.00002669
Iteration 115/1000 | Loss: 0.00002669
Iteration 116/1000 | Loss: 0.00002669
Iteration 117/1000 | Loss: 0.00002668
Iteration 118/1000 | Loss: 0.00002668
Iteration 119/1000 | Loss: 0.00002668
Iteration 120/1000 | Loss: 0.00002668
Iteration 121/1000 | Loss: 0.00002668
Iteration 122/1000 | Loss: 0.00002668
Iteration 123/1000 | Loss: 0.00002668
Iteration 124/1000 | Loss: 0.00002668
Iteration 125/1000 | Loss: 0.00002668
Iteration 126/1000 | Loss: 0.00002667
Iteration 127/1000 | Loss: 0.00002667
Iteration 128/1000 | Loss: 0.00002667
Iteration 129/1000 | Loss: 0.00002667
Iteration 130/1000 | Loss: 0.00002667
Iteration 131/1000 | Loss: 0.00002667
Iteration 132/1000 | Loss: 0.00002666
Iteration 133/1000 | Loss: 0.00002666
Iteration 134/1000 | Loss: 0.00002666
Iteration 135/1000 | Loss: 0.00002666
Iteration 136/1000 | Loss: 0.00002666
Iteration 137/1000 | Loss: 0.00002666
Iteration 138/1000 | Loss: 0.00002666
Iteration 139/1000 | Loss: 0.00002666
Iteration 140/1000 | Loss: 0.00002666
Iteration 141/1000 | Loss: 0.00002665
Iteration 142/1000 | Loss: 0.00002665
Iteration 143/1000 | Loss: 0.00002665
Iteration 144/1000 | Loss: 0.00002665
Iteration 145/1000 | Loss: 0.00002665
Iteration 146/1000 | Loss: 0.00002665
Iteration 147/1000 | Loss: 0.00002664
Iteration 148/1000 | Loss: 0.00002664
Iteration 149/1000 | Loss: 0.00002664
Iteration 150/1000 | Loss: 0.00002664
Iteration 151/1000 | Loss: 0.00002664
Iteration 152/1000 | Loss: 0.00002664
Iteration 153/1000 | Loss: 0.00002664
Iteration 154/1000 | Loss: 0.00002664
Iteration 155/1000 | Loss: 0.00002664
Iteration 156/1000 | Loss: 0.00002664
Iteration 157/1000 | Loss: 0.00002663
Iteration 158/1000 | Loss: 0.00002663
Iteration 159/1000 | Loss: 0.00002663
Iteration 160/1000 | Loss: 0.00002663
Iteration 161/1000 | Loss: 0.00002663
Iteration 162/1000 | Loss: 0.00002663
Iteration 163/1000 | Loss: 0.00002663
Iteration 164/1000 | Loss: 0.00002663
Iteration 165/1000 | Loss: 0.00002662
Iteration 166/1000 | Loss: 0.00002662
Iteration 167/1000 | Loss: 0.00002662
Iteration 168/1000 | Loss: 0.00002662
Iteration 169/1000 | Loss: 0.00002662
Iteration 170/1000 | Loss: 0.00002662
Iteration 171/1000 | Loss: 0.00002662
Iteration 172/1000 | Loss: 0.00002662
Iteration 173/1000 | Loss: 0.00002662
Iteration 174/1000 | Loss: 0.00002662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.6623523808666505e-05, 2.6623523808666505e-05, 2.6623523808666505e-05, 2.6623523808666505e-05, 2.6623523808666505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6623523808666505e-05

Optimization complete. Final v2v error: 4.450161457061768 mm

Highest mean error: 4.885260105133057 mm for frame 30

Lowest mean error: 4.0352911949157715 mm for frame 133

Saving results

Total time: 36.42112851142883
