Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=165, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9240-9295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434986
Iteration 2/25 | Loss: 0.00147923
Iteration 3/25 | Loss: 0.00140316
Iteration 4/25 | Loss: 0.00139915
Iteration 5/25 | Loss: 0.00139915
Iteration 6/25 | Loss: 0.00139911
Iteration 7/25 | Loss: 0.00139911
Iteration 8/25 | Loss: 0.00139911
Iteration 9/25 | Loss: 0.00139911
Iteration 10/25 | Loss: 0.00139911
Iteration 11/25 | Loss: 0.00139911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013991062296554446, 0.0013991062296554446, 0.0013991062296554446, 0.0013991062296554446, 0.0013991062296554446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013991062296554446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24293721
Iteration 2/25 | Loss: 0.00201451
Iteration 3/25 | Loss: 0.00201451
Iteration 4/25 | Loss: 0.00201451
Iteration 5/25 | Loss: 0.00201451
Iteration 6/25 | Loss: 0.00201451
Iteration 7/25 | Loss: 0.00201451
Iteration 8/25 | Loss: 0.00201451
Iteration 9/25 | Loss: 0.00201451
Iteration 10/25 | Loss: 0.00201451
Iteration 11/25 | Loss: 0.00201450
Iteration 12/25 | Loss: 0.00201450
Iteration 13/25 | Loss: 0.00201450
Iteration 14/25 | Loss: 0.00201450
Iteration 15/25 | Loss: 0.00201450
Iteration 16/25 | Loss: 0.00201451
Iteration 17/25 | Loss: 0.00201450
Iteration 18/25 | Loss: 0.00201450
Iteration 19/25 | Loss: 0.00201450
Iteration 20/25 | Loss: 0.00201450
Iteration 21/25 | Loss: 0.00201450
Iteration 22/25 | Loss: 0.00201450
Iteration 23/25 | Loss: 0.00201450
Iteration 24/25 | Loss: 0.00201450
Iteration 25/25 | Loss: 0.00201450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0020145049784332514, 0.0020145049784332514, 0.0020145049784332514, 0.0020145049784332514, 0.0020145049784332514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020145049784332514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201450
Iteration 2/1000 | Loss: 0.00002841
Iteration 3/1000 | Loss: 0.00002104
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001824
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001702
Iteration 12/1000 | Loss: 0.00001692
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001663
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001649
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001637
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001615
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00001614
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001612
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001611
Iteration 36/1000 | Loss: 0.00001611
Iteration 37/1000 | Loss: 0.00001611
Iteration 38/1000 | Loss: 0.00001610
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001606
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001596
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001592
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001592
Iteration 75/1000 | Loss: 0.00001592
Iteration 76/1000 | Loss: 0.00001592
Iteration 77/1000 | Loss: 0.00001591
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001590
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001585
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001584
Iteration 129/1000 | Loss: 0.00001584
Iteration 130/1000 | Loss: 0.00001584
Iteration 131/1000 | Loss: 0.00001584
Iteration 132/1000 | Loss: 0.00001584
Iteration 133/1000 | Loss: 0.00001584
Iteration 134/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.5840523701626807e-05, 1.5840523701626807e-05, 1.5840523701626807e-05, 1.5840523701626807e-05, 1.5840523701626807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5840523701626807e-05

Optimization complete. Final v2v error: 3.369628667831421 mm

Highest mean error: 3.4155287742614746 mm for frame 171

Lowest mean error: 3.3309154510498047 mm for frame 80

Saving results

Total time: 37.961153745651245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787035
Iteration 2/25 | Loss: 0.00195190
Iteration 3/25 | Loss: 0.00144006
Iteration 4/25 | Loss: 0.00134821
Iteration 5/25 | Loss: 0.00133156
Iteration 6/25 | Loss: 0.00133565
Iteration 7/25 | Loss: 0.00133451
Iteration 8/25 | Loss: 0.00132812
Iteration 9/25 | Loss: 0.00132396
Iteration 10/25 | Loss: 0.00132109
Iteration 11/25 | Loss: 0.00132439
Iteration 12/25 | Loss: 0.00132013
Iteration 13/25 | Loss: 0.00132008
Iteration 14/25 | Loss: 0.00132008
Iteration 15/25 | Loss: 0.00132006
Iteration 16/25 | Loss: 0.00132005
Iteration 17/25 | Loss: 0.00132005
Iteration 18/25 | Loss: 0.00132005
Iteration 19/25 | Loss: 0.00132005
Iteration 20/25 | Loss: 0.00132005
Iteration 21/25 | Loss: 0.00132005
Iteration 22/25 | Loss: 0.00132005
Iteration 23/25 | Loss: 0.00132005
Iteration 24/25 | Loss: 0.00132005
Iteration 25/25 | Loss: 0.00132004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80696964
Iteration 2/25 | Loss: 0.00211861
Iteration 3/25 | Loss: 0.00201612
Iteration 4/25 | Loss: 0.00201612
Iteration 5/25 | Loss: 0.00201612
Iteration 6/25 | Loss: 0.00201612
Iteration 7/25 | Loss: 0.00201611
Iteration 8/25 | Loss: 0.00201611
Iteration 9/25 | Loss: 0.00201611
Iteration 10/25 | Loss: 0.00201611
Iteration 11/25 | Loss: 0.00201611
Iteration 12/25 | Loss: 0.00201611
Iteration 13/25 | Loss: 0.00201611
Iteration 14/25 | Loss: 0.00201611
Iteration 15/25 | Loss: 0.00201611
Iteration 16/25 | Loss: 0.00201611
Iteration 17/25 | Loss: 0.00201611
Iteration 18/25 | Loss: 0.00201611
Iteration 19/25 | Loss: 0.00201611
Iteration 20/25 | Loss: 0.00201611
Iteration 21/25 | Loss: 0.00201611
Iteration 22/25 | Loss: 0.00201611
Iteration 23/25 | Loss: 0.00201611
Iteration 24/25 | Loss: 0.00201611
Iteration 25/25 | Loss: 0.00201611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201611
Iteration 2/1000 | Loss: 0.00006245
Iteration 3/1000 | Loss: 0.00005876
Iteration 4/1000 | Loss: 0.00002253
Iteration 5/1000 | Loss: 0.00004957
Iteration 6/1000 | Loss: 0.00003710
Iteration 7/1000 | Loss: 0.00001513
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001432
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001368
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001353
Iteration 14/1000 | Loss: 0.00002124
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001318
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001317
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001313
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001287
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001282
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001278
Iteration 83/1000 | Loss: 0.00001278
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001277
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001272
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001272
Iteration 105/1000 | Loss: 0.00001272
Iteration 106/1000 | Loss: 0.00001272
Iteration 107/1000 | Loss: 0.00001272
Iteration 108/1000 | Loss: 0.00001272
Iteration 109/1000 | Loss: 0.00001272
Iteration 110/1000 | Loss: 0.00001272
Iteration 111/1000 | Loss: 0.00001272
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001272
Iteration 131/1000 | Loss: 0.00001272
Iteration 132/1000 | Loss: 0.00001272
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001272
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001272
Iteration 139/1000 | Loss: 0.00001272
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001272
Iteration 142/1000 | Loss: 0.00001272
Iteration 143/1000 | Loss: 0.00001272
Iteration 144/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2720609447569586e-05, 1.2720609447569586e-05, 1.2720609447569586e-05, 1.2720609447569586e-05, 1.2720609447569586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2720609447569586e-05

Optimization complete. Final v2v error: 3.066680431365967 mm

Highest mean error: 3.6063249111175537 mm for frame 117

Lowest mean error: 2.8892626762390137 mm for frame 176

Saving results

Total time: 56.93542432785034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642861
Iteration 2/25 | Loss: 0.00156521
Iteration 3/25 | Loss: 0.00141101
Iteration 4/25 | Loss: 0.00138907
Iteration 5/25 | Loss: 0.00138173
Iteration 6/25 | Loss: 0.00138181
Iteration 7/25 | Loss: 0.00138136
Iteration 8/25 | Loss: 0.00137864
Iteration 9/25 | Loss: 0.00137895
Iteration 10/25 | Loss: 0.00137692
Iteration 11/25 | Loss: 0.00137598
Iteration 12/25 | Loss: 0.00137558
Iteration 13/25 | Loss: 0.00137550
Iteration 14/25 | Loss: 0.00137540
Iteration 15/25 | Loss: 0.00137540
Iteration 16/25 | Loss: 0.00137540
Iteration 17/25 | Loss: 0.00137540
Iteration 18/25 | Loss: 0.00137540
Iteration 19/25 | Loss: 0.00137540
Iteration 20/25 | Loss: 0.00137540
Iteration 21/25 | Loss: 0.00137540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013753966195508838, 0.0013753966195508838, 0.0013753966195508838, 0.0013753966195508838, 0.0013753966195508838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013753966195508838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70220006
Iteration 2/25 | Loss: 0.00309540
Iteration 3/25 | Loss: 0.00309540
Iteration 4/25 | Loss: 0.00309540
Iteration 5/25 | Loss: 0.00309540
Iteration 6/25 | Loss: 0.00309540
Iteration 7/25 | Loss: 0.00309540
Iteration 8/25 | Loss: 0.00309540
Iteration 9/25 | Loss: 0.00309540
Iteration 10/25 | Loss: 0.00309540
Iteration 11/25 | Loss: 0.00309540
Iteration 12/25 | Loss: 0.00309540
Iteration 13/25 | Loss: 0.00309540
Iteration 14/25 | Loss: 0.00309540
Iteration 15/25 | Loss: 0.00309540
Iteration 16/25 | Loss: 0.00309540
Iteration 17/25 | Loss: 0.00309540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0030953988898545504, 0.0030953988898545504, 0.0030953988898545504, 0.0030953988898545504, 0.0030953988898545504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030953988898545504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309540
Iteration 2/1000 | Loss: 0.00008114
Iteration 3/1000 | Loss: 0.00005050
Iteration 4/1000 | Loss: 0.00004052
Iteration 5/1000 | Loss: 0.00003578
Iteration 6/1000 | Loss: 0.00019654
Iteration 7/1000 | Loss: 0.00022346
Iteration 8/1000 | Loss: 0.00003720
Iteration 9/1000 | Loss: 0.00002715
Iteration 10/1000 | Loss: 0.00002397
Iteration 11/1000 | Loss: 0.00002236
Iteration 12/1000 | Loss: 0.00002139
Iteration 13/1000 | Loss: 0.00002080
Iteration 14/1000 | Loss: 0.00002041
Iteration 15/1000 | Loss: 0.00024901
Iteration 16/1000 | Loss: 0.00002876
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00001927
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001771
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001643
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001626
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001620
Iteration 57/1000 | Loss: 0.00001620
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001618
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001617
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001611
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001607
Iteration 91/1000 | Loss: 0.00001607
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Iteration 101/1000 | Loss: 0.00001603
Iteration 102/1000 | Loss: 0.00001603
Iteration 103/1000 | Loss: 0.00001603
Iteration 104/1000 | Loss: 0.00001603
Iteration 105/1000 | Loss: 0.00001603
Iteration 106/1000 | Loss: 0.00001603
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001601
Iteration 121/1000 | Loss: 0.00001601
Iteration 122/1000 | Loss: 0.00001601
Iteration 123/1000 | Loss: 0.00001601
Iteration 124/1000 | Loss: 0.00001601
Iteration 125/1000 | Loss: 0.00001601
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001601
Iteration 128/1000 | Loss: 0.00001600
Iteration 129/1000 | Loss: 0.00001600
Iteration 130/1000 | Loss: 0.00001600
Iteration 131/1000 | Loss: 0.00001600
Iteration 132/1000 | Loss: 0.00001600
Iteration 133/1000 | Loss: 0.00001600
Iteration 134/1000 | Loss: 0.00001600
Iteration 135/1000 | Loss: 0.00001599
Iteration 136/1000 | Loss: 0.00001599
Iteration 137/1000 | Loss: 0.00001599
Iteration 138/1000 | Loss: 0.00001599
Iteration 139/1000 | Loss: 0.00001599
Iteration 140/1000 | Loss: 0.00001599
Iteration 141/1000 | Loss: 0.00001599
Iteration 142/1000 | Loss: 0.00001598
Iteration 143/1000 | Loss: 0.00001598
Iteration 144/1000 | Loss: 0.00001598
Iteration 145/1000 | Loss: 0.00001598
Iteration 146/1000 | Loss: 0.00001598
Iteration 147/1000 | Loss: 0.00001598
Iteration 148/1000 | Loss: 0.00001598
Iteration 149/1000 | Loss: 0.00001598
Iteration 150/1000 | Loss: 0.00001598
Iteration 151/1000 | Loss: 0.00001598
Iteration 152/1000 | Loss: 0.00001598
Iteration 153/1000 | Loss: 0.00001598
Iteration 154/1000 | Loss: 0.00001598
Iteration 155/1000 | Loss: 0.00001597
Iteration 156/1000 | Loss: 0.00001597
Iteration 157/1000 | Loss: 0.00001597
Iteration 158/1000 | Loss: 0.00001597
Iteration 159/1000 | Loss: 0.00001597
Iteration 160/1000 | Loss: 0.00001597
Iteration 161/1000 | Loss: 0.00001597
Iteration 162/1000 | Loss: 0.00001597
Iteration 163/1000 | Loss: 0.00001597
Iteration 164/1000 | Loss: 0.00001597
Iteration 165/1000 | Loss: 0.00001597
Iteration 166/1000 | Loss: 0.00001597
Iteration 167/1000 | Loss: 0.00001597
Iteration 168/1000 | Loss: 0.00001597
Iteration 169/1000 | Loss: 0.00001597
Iteration 170/1000 | Loss: 0.00001597
Iteration 171/1000 | Loss: 0.00001597
Iteration 172/1000 | Loss: 0.00001596
Iteration 173/1000 | Loss: 0.00001596
Iteration 174/1000 | Loss: 0.00001596
Iteration 175/1000 | Loss: 0.00001596
Iteration 176/1000 | Loss: 0.00001596
Iteration 177/1000 | Loss: 0.00001596
Iteration 178/1000 | Loss: 0.00001596
Iteration 179/1000 | Loss: 0.00001596
Iteration 180/1000 | Loss: 0.00001596
Iteration 181/1000 | Loss: 0.00001596
Iteration 182/1000 | Loss: 0.00001596
Iteration 183/1000 | Loss: 0.00001596
Iteration 184/1000 | Loss: 0.00001596
Iteration 185/1000 | Loss: 0.00001596
Iteration 186/1000 | Loss: 0.00001596
Iteration 187/1000 | Loss: 0.00001596
Iteration 188/1000 | Loss: 0.00001596
Iteration 189/1000 | Loss: 0.00001596
Iteration 190/1000 | Loss: 0.00001596
Iteration 191/1000 | Loss: 0.00001596
Iteration 192/1000 | Loss: 0.00001596
Iteration 193/1000 | Loss: 0.00001595
Iteration 194/1000 | Loss: 0.00001595
Iteration 195/1000 | Loss: 0.00001595
Iteration 196/1000 | Loss: 0.00001595
Iteration 197/1000 | Loss: 0.00001595
Iteration 198/1000 | Loss: 0.00001595
Iteration 199/1000 | Loss: 0.00001595
Iteration 200/1000 | Loss: 0.00001595
Iteration 201/1000 | Loss: 0.00001595
Iteration 202/1000 | Loss: 0.00001595
Iteration 203/1000 | Loss: 0.00001595
Iteration 204/1000 | Loss: 0.00001595
Iteration 205/1000 | Loss: 0.00001595
Iteration 206/1000 | Loss: 0.00001595
Iteration 207/1000 | Loss: 0.00001595
Iteration 208/1000 | Loss: 0.00001595
Iteration 209/1000 | Loss: 0.00001595
Iteration 210/1000 | Loss: 0.00001595
Iteration 211/1000 | Loss: 0.00001595
Iteration 212/1000 | Loss: 0.00001595
Iteration 213/1000 | Loss: 0.00001595
Iteration 214/1000 | Loss: 0.00001595
Iteration 215/1000 | Loss: 0.00001594
Iteration 216/1000 | Loss: 0.00001594
Iteration 217/1000 | Loss: 0.00001594
Iteration 218/1000 | Loss: 0.00001594
Iteration 219/1000 | Loss: 0.00001594
Iteration 220/1000 | Loss: 0.00001594
Iteration 221/1000 | Loss: 0.00001594
Iteration 222/1000 | Loss: 0.00001594
Iteration 223/1000 | Loss: 0.00001594
Iteration 224/1000 | Loss: 0.00001594
Iteration 225/1000 | Loss: 0.00001594
Iteration 226/1000 | Loss: 0.00001594
Iteration 227/1000 | Loss: 0.00001594
Iteration 228/1000 | Loss: 0.00001594
Iteration 229/1000 | Loss: 0.00001594
Iteration 230/1000 | Loss: 0.00001594
Iteration 231/1000 | Loss: 0.00001594
Iteration 232/1000 | Loss: 0.00001593
Iteration 233/1000 | Loss: 0.00001593
Iteration 234/1000 | Loss: 0.00001593
Iteration 235/1000 | Loss: 0.00001593
Iteration 236/1000 | Loss: 0.00001593
Iteration 237/1000 | Loss: 0.00001593
Iteration 238/1000 | Loss: 0.00001593
Iteration 239/1000 | Loss: 0.00001593
Iteration 240/1000 | Loss: 0.00001593
Iteration 241/1000 | Loss: 0.00001593
Iteration 242/1000 | Loss: 0.00001592
Iteration 243/1000 | Loss: 0.00001592
Iteration 244/1000 | Loss: 0.00001592
Iteration 245/1000 | Loss: 0.00001592
Iteration 246/1000 | Loss: 0.00001592
Iteration 247/1000 | Loss: 0.00001592
Iteration 248/1000 | Loss: 0.00001592
Iteration 249/1000 | Loss: 0.00001592
Iteration 250/1000 | Loss: 0.00001592
Iteration 251/1000 | Loss: 0.00001592
Iteration 252/1000 | Loss: 0.00001592
Iteration 253/1000 | Loss: 0.00001592
Iteration 254/1000 | Loss: 0.00001592
Iteration 255/1000 | Loss: 0.00001591
Iteration 256/1000 | Loss: 0.00001591
Iteration 257/1000 | Loss: 0.00001591
Iteration 258/1000 | Loss: 0.00001591
Iteration 259/1000 | Loss: 0.00001591
Iteration 260/1000 | Loss: 0.00001591
Iteration 261/1000 | Loss: 0.00001591
Iteration 262/1000 | Loss: 0.00001591
Iteration 263/1000 | Loss: 0.00001591
Iteration 264/1000 | Loss: 0.00001591
Iteration 265/1000 | Loss: 0.00001591
Iteration 266/1000 | Loss: 0.00001591
Iteration 267/1000 | Loss: 0.00001591
Iteration 268/1000 | Loss: 0.00001591
Iteration 269/1000 | Loss: 0.00001591
Iteration 270/1000 | Loss: 0.00001591
Iteration 271/1000 | Loss: 0.00001591
Iteration 272/1000 | Loss: 0.00001591
Iteration 273/1000 | Loss: 0.00001591
Iteration 274/1000 | Loss: 0.00001590
Iteration 275/1000 | Loss: 0.00001590
Iteration 276/1000 | Loss: 0.00001590
Iteration 277/1000 | Loss: 0.00001590
Iteration 278/1000 | Loss: 0.00001590
Iteration 279/1000 | Loss: 0.00001590
Iteration 280/1000 | Loss: 0.00001590
Iteration 281/1000 | Loss: 0.00001590
Iteration 282/1000 | Loss: 0.00001590
Iteration 283/1000 | Loss: 0.00001590
Iteration 284/1000 | Loss: 0.00001590
Iteration 285/1000 | Loss: 0.00001590
Iteration 286/1000 | Loss: 0.00001590
Iteration 287/1000 | Loss: 0.00001590
Iteration 288/1000 | Loss: 0.00001589
Iteration 289/1000 | Loss: 0.00001589
Iteration 290/1000 | Loss: 0.00001589
Iteration 291/1000 | Loss: 0.00001589
Iteration 292/1000 | Loss: 0.00001589
Iteration 293/1000 | Loss: 0.00001589
Iteration 294/1000 | Loss: 0.00001589
Iteration 295/1000 | Loss: 0.00001589
Iteration 296/1000 | Loss: 0.00001589
Iteration 297/1000 | Loss: 0.00001589
Iteration 298/1000 | Loss: 0.00001589
Iteration 299/1000 | Loss: 0.00001589
Iteration 300/1000 | Loss: 0.00001589
Iteration 301/1000 | Loss: 0.00001589
Iteration 302/1000 | Loss: 0.00001589
Iteration 303/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [1.5888230336713605e-05, 1.5888230336713605e-05, 1.5888230336713605e-05, 1.5888230336713605e-05, 1.5888230336713605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5888230336713605e-05

Optimization complete. Final v2v error: 3.3560216426849365 mm

Highest mean error: 4.1887383460998535 mm for frame 176

Lowest mean error: 2.825592279434204 mm for frame 156

Saving results

Total time: 91.60680031776428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386736
Iteration 2/25 | Loss: 0.00147362
Iteration 3/25 | Loss: 0.00134760
Iteration 4/25 | Loss: 0.00132407
Iteration 5/25 | Loss: 0.00131884
Iteration 6/25 | Loss: 0.00131761
Iteration 7/25 | Loss: 0.00131751
Iteration 8/25 | Loss: 0.00131751
Iteration 9/25 | Loss: 0.00131751
Iteration 10/25 | Loss: 0.00131751
Iteration 11/25 | Loss: 0.00131751
Iteration 12/25 | Loss: 0.00131751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001317508053034544, 0.001317508053034544, 0.001317508053034544, 0.001317508053034544, 0.001317508053034544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001317508053034544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19498205
Iteration 2/25 | Loss: 0.00248398
Iteration 3/25 | Loss: 0.00248398
Iteration 4/25 | Loss: 0.00248398
Iteration 5/25 | Loss: 0.00248398
Iteration 6/25 | Loss: 0.00248398
Iteration 7/25 | Loss: 0.00248398
Iteration 8/25 | Loss: 0.00248398
Iteration 9/25 | Loss: 0.00248398
Iteration 10/25 | Loss: 0.00248398
Iteration 11/25 | Loss: 0.00248398
Iteration 12/25 | Loss: 0.00248398
Iteration 13/25 | Loss: 0.00248398
Iteration 14/25 | Loss: 0.00248398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00248397933319211, 0.00248397933319211, 0.00248397933319211, 0.00248397933319211, 0.00248397933319211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00248397933319211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248398
Iteration 2/1000 | Loss: 0.00004711
Iteration 3/1000 | Loss: 0.00003408
Iteration 4/1000 | Loss: 0.00002659
Iteration 5/1000 | Loss: 0.00002355
Iteration 6/1000 | Loss: 0.00002191
Iteration 7/1000 | Loss: 0.00002047
Iteration 8/1000 | Loss: 0.00001968
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001839
Iteration 12/1000 | Loss: 0.00001811
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001754
Iteration 17/1000 | Loss: 0.00001753
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001701
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001695
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001690
Iteration 81/1000 | Loss: 0.00001690
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001689
Iteration 84/1000 | Loss: 0.00001689
Iteration 85/1000 | Loss: 0.00001689
Iteration 86/1000 | Loss: 0.00001688
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001687
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001683
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001683
Iteration 124/1000 | Loss: 0.00001683
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001681
Iteration 134/1000 | Loss: 0.00001681
Iteration 135/1000 | Loss: 0.00001681
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001680
Iteration 141/1000 | Loss: 0.00001680
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001679
Iteration 144/1000 | Loss: 0.00001679
Iteration 145/1000 | Loss: 0.00001678
Iteration 146/1000 | Loss: 0.00001678
Iteration 147/1000 | Loss: 0.00001678
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001677
Iteration 151/1000 | Loss: 0.00001677
Iteration 152/1000 | Loss: 0.00001676
Iteration 153/1000 | Loss: 0.00001676
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001676
Iteration 159/1000 | Loss: 0.00001676
Iteration 160/1000 | Loss: 0.00001676
Iteration 161/1000 | Loss: 0.00001676
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001675
Iteration 164/1000 | Loss: 0.00001675
Iteration 165/1000 | Loss: 0.00001675
Iteration 166/1000 | Loss: 0.00001675
Iteration 167/1000 | Loss: 0.00001675
Iteration 168/1000 | Loss: 0.00001675
Iteration 169/1000 | Loss: 0.00001674
Iteration 170/1000 | Loss: 0.00001674
Iteration 171/1000 | Loss: 0.00001674
Iteration 172/1000 | Loss: 0.00001674
Iteration 173/1000 | Loss: 0.00001674
Iteration 174/1000 | Loss: 0.00001674
Iteration 175/1000 | Loss: 0.00001674
Iteration 176/1000 | Loss: 0.00001674
Iteration 177/1000 | Loss: 0.00001673
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001672
Iteration 182/1000 | Loss: 0.00001672
Iteration 183/1000 | Loss: 0.00001672
Iteration 184/1000 | Loss: 0.00001671
Iteration 185/1000 | Loss: 0.00001671
Iteration 186/1000 | Loss: 0.00001671
Iteration 187/1000 | Loss: 0.00001671
Iteration 188/1000 | Loss: 0.00001670
Iteration 189/1000 | Loss: 0.00001670
Iteration 190/1000 | Loss: 0.00001670
Iteration 191/1000 | Loss: 0.00001670
Iteration 192/1000 | Loss: 0.00001670
Iteration 193/1000 | Loss: 0.00001670
Iteration 194/1000 | Loss: 0.00001669
Iteration 195/1000 | Loss: 0.00001669
Iteration 196/1000 | Loss: 0.00001669
Iteration 197/1000 | Loss: 0.00001669
Iteration 198/1000 | Loss: 0.00001669
Iteration 199/1000 | Loss: 0.00001669
Iteration 200/1000 | Loss: 0.00001669
Iteration 201/1000 | Loss: 0.00001669
Iteration 202/1000 | Loss: 0.00001669
Iteration 203/1000 | Loss: 0.00001669
Iteration 204/1000 | Loss: 0.00001669
Iteration 205/1000 | Loss: 0.00001668
Iteration 206/1000 | Loss: 0.00001668
Iteration 207/1000 | Loss: 0.00001668
Iteration 208/1000 | Loss: 0.00001668
Iteration 209/1000 | Loss: 0.00001668
Iteration 210/1000 | Loss: 0.00001668
Iteration 211/1000 | Loss: 0.00001668
Iteration 212/1000 | Loss: 0.00001668
Iteration 213/1000 | Loss: 0.00001668
Iteration 214/1000 | Loss: 0.00001668
Iteration 215/1000 | Loss: 0.00001668
Iteration 216/1000 | Loss: 0.00001668
Iteration 217/1000 | Loss: 0.00001668
Iteration 218/1000 | Loss: 0.00001667
Iteration 219/1000 | Loss: 0.00001667
Iteration 220/1000 | Loss: 0.00001667
Iteration 221/1000 | Loss: 0.00001667
Iteration 222/1000 | Loss: 0.00001667
Iteration 223/1000 | Loss: 0.00001667
Iteration 224/1000 | Loss: 0.00001667
Iteration 225/1000 | Loss: 0.00001667
Iteration 226/1000 | Loss: 0.00001667
Iteration 227/1000 | Loss: 0.00001667
Iteration 228/1000 | Loss: 0.00001667
Iteration 229/1000 | Loss: 0.00001666
Iteration 230/1000 | Loss: 0.00001666
Iteration 231/1000 | Loss: 0.00001666
Iteration 232/1000 | Loss: 0.00001666
Iteration 233/1000 | Loss: 0.00001666
Iteration 234/1000 | Loss: 0.00001666
Iteration 235/1000 | Loss: 0.00001666
Iteration 236/1000 | Loss: 0.00001666
Iteration 237/1000 | Loss: 0.00001666
Iteration 238/1000 | Loss: 0.00001666
Iteration 239/1000 | Loss: 0.00001666
Iteration 240/1000 | Loss: 0.00001666
Iteration 241/1000 | Loss: 0.00001666
Iteration 242/1000 | Loss: 0.00001666
Iteration 243/1000 | Loss: 0.00001666
Iteration 244/1000 | Loss: 0.00001666
Iteration 245/1000 | Loss: 0.00001666
Iteration 246/1000 | Loss: 0.00001666
Iteration 247/1000 | Loss: 0.00001666
Iteration 248/1000 | Loss: 0.00001666
Iteration 249/1000 | Loss: 0.00001666
Iteration 250/1000 | Loss: 0.00001666
Iteration 251/1000 | Loss: 0.00001666
Iteration 252/1000 | Loss: 0.00001666
Iteration 253/1000 | Loss: 0.00001666
Iteration 254/1000 | Loss: 0.00001666
Iteration 255/1000 | Loss: 0.00001666
Iteration 256/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.665810304984916e-05, 1.665810304984916e-05, 1.665810304984916e-05, 1.665810304984916e-05, 1.665810304984916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.665810304984916e-05

Optimization complete. Final v2v error: 3.493049144744873 mm

Highest mean error: 4.207761287689209 mm for frame 73

Lowest mean error: 2.9080958366394043 mm for frame 83

Saving results

Total time: 51.96652889251709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793709
Iteration 2/25 | Loss: 0.00179524
Iteration 3/25 | Loss: 0.00164615
Iteration 4/25 | Loss: 0.00161220
Iteration 5/25 | Loss: 0.00160480
Iteration 6/25 | Loss: 0.00160273
Iteration 7/25 | Loss: 0.00160389
Iteration 8/25 | Loss: 0.00158497
Iteration 9/25 | Loss: 0.00156684
Iteration 10/25 | Loss: 0.00155625
Iteration 11/25 | Loss: 0.00154616
Iteration 12/25 | Loss: 0.00153452
Iteration 13/25 | Loss: 0.00152843
Iteration 14/25 | Loss: 0.00153213
Iteration 15/25 | Loss: 0.00152542
Iteration 16/25 | Loss: 0.00152345
Iteration 17/25 | Loss: 0.00152313
Iteration 18/25 | Loss: 0.00152299
Iteration 19/25 | Loss: 0.00152250
Iteration 20/25 | Loss: 0.00152097
Iteration 21/25 | Loss: 0.00152035
Iteration 22/25 | Loss: 0.00151981
Iteration 23/25 | Loss: 0.00152427
Iteration 24/25 | Loss: 0.00151464
Iteration 25/25 | Loss: 0.00151221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17630172
Iteration 2/25 | Loss: 0.00497754
Iteration 3/25 | Loss: 0.00497752
Iteration 4/25 | Loss: 0.00497752
Iteration 5/25 | Loss: 0.00497752
Iteration 6/25 | Loss: 0.00497752
Iteration 7/25 | Loss: 0.00497752
Iteration 8/25 | Loss: 0.00497752
Iteration 9/25 | Loss: 0.00497752
Iteration 10/25 | Loss: 0.00497752
Iteration 11/25 | Loss: 0.00497752
Iteration 12/25 | Loss: 0.00497752
Iteration 13/25 | Loss: 0.00497752
Iteration 14/25 | Loss: 0.00497752
Iteration 15/25 | Loss: 0.00497752
Iteration 16/25 | Loss: 0.00497752
Iteration 17/25 | Loss: 0.00497752
Iteration 18/25 | Loss: 0.00497752
Iteration 19/25 | Loss: 0.00497752
Iteration 20/25 | Loss: 0.00497752
Iteration 21/25 | Loss: 0.00497752
Iteration 22/25 | Loss: 0.00497752
Iteration 23/25 | Loss: 0.00497752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004977516829967499, 0.004977516829967499, 0.004977516829967499, 0.004977516829967499, 0.004977516829967499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004977516829967499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00497752
Iteration 2/1000 | Loss: 0.00035393
Iteration 3/1000 | Loss: 0.00020423
Iteration 4/1000 | Loss: 0.00016201
Iteration 5/1000 | Loss: 0.00013910
Iteration 6/1000 | Loss: 0.00013011
Iteration 7/1000 | Loss: 0.00012188
Iteration 8/1000 | Loss: 0.00011803
Iteration 9/1000 | Loss: 0.00011294
Iteration 10/1000 | Loss: 0.00010936
Iteration 11/1000 | Loss: 0.00010712
Iteration 12/1000 | Loss: 0.00010491
Iteration 13/1000 | Loss: 0.00010303
Iteration 14/1000 | Loss: 0.00304954
Iteration 15/1000 | Loss: 0.00173638
Iteration 16/1000 | Loss: 0.00038484
Iteration 17/1000 | Loss: 0.00071169
Iteration 18/1000 | Loss: 0.00022078
Iteration 19/1000 | Loss: 0.00027085
Iteration 20/1000 | Loss: 0.00016904
Iteration 21/1000 | Loss: 0.00020063
Iteration 22/1000 | Loss: 0.00064914
Iteration 23/1000 | Loss: 0.00061646
Iteration 24/1000 | Loss: 0.00047728
Iteration 25/1000 | Loss: 0.00040892
Iteration 26/1000 | Loss: 0.00020485
Iteration 27/1000 | Loss: 0.00019698
Iteration 28/1000 | Loss: 0.00024330
Iteration 29/1000 | Loss: 0.00012036
Iteration 30/1000 | Loss: 0.00075941
Iteration 31/1000 | Loss: 0.00013624
Iteration 32/1000 | Loss: 0.00016525
Iteration 33/1000 | Loss: 0.00075357
Iteration 34/1000 | Loss: 0.00029060
Iteration 35/1000 | Loss: 0.00019796
Iteration 36/1000 | Loss: 0.00008493
Iteration 37/1000 | Loss: 0.00007815
Iteration 38/1000 | Loss: 0.00007431
Iteration 39/1000 | Loss: 0.00007058
Iteration 40/1000 | Loss: 0.00050433
Iteration 41/1000 | Loss: 0.00074876
Iteration 42/1000 | Loss: 0.00008319
Iteration 43/1000 | Loss: 0.00006676
Iteration 44/1000 | Loss: 0.00028156
Iteration 45/1000 | Loss: 0.00077971
Iteration 46/1000 | Loss: 0.00011519
Iteration 47/1000 | Loss: 0.00077206
Iteration 48/1000 | Loss: 0.00009469
Iteration 49/1000 | Loss: 0.00018236
Iteration 50/1000 | Loss: 0.00007922
Iteration 51/1000 | Loss: 0.00016225
Iteration 52/1000 | Loss: 0.00020876
Iteration 53/1000 | Loss: 0.00017769
Iteration 54/1000 | Loss: 0.00006307
Iteration 55/1000 | Loss: 0.00110069
Iteration 56/1000 | Loss: 0.00280543
Iteration 57/1000 | Loss: 0.00017064
Iteration 58/1000 | Loss: 0.00021327
Iteration 59/1000 | Loss: 0.00016864
Iteration 60/1000 | Loss: 0.00006071
Iteration 61/1000 | Loss: 0.00023465
Iteration 62/1000 | Loss: 0.00031678
Iteration 63/1000 | Loss: 0.00028281
Iteration 64/1000 | Loss: 0.00018926
Iteration 65/1000 | Loss: 0.00039893
Iteration 66/1000 | Loss: 0.00016315
Iteration 67/1000 | Loss: 0.00004951
Iteration 68/1000 | Loss: 0.00020268
Iteration 69/1000 | Loss: 0.00016480
Iteration 70/1000 | Loss: 0.00008375
Iteration 71/1000 | Loss: 0.00004454
Iteration 72/1000 | Loss: 0.00004108
Iteration 73/1000 | Loss: 0.00026002
Iteration 74/1000 | Loss: 0.00020428
Iteration 75/1000 | Loss: 0.00003807
Iteration 76/1000 | Loss: 0.00018051
Iteration 77/1000 | Loss: 0.00023383
Iteration 78/1000 | Loss: 0.00017053
Iteration 79/1000 | Loss: 0.00006128
Iteration 80/1000 | Loss: 0.00012288
Iteration 81/1000 | Loss: 0.00021700
Iteration 82/1000 | Loss: 0.00019424
Iteration 83/1000 | Loss: 0.00009668
Iteration 84/1000 | Loss: 0.00021743
Iteration 85/1000 | Loss: 0.00021701
Iteration 86/1000 | Loss: 0.00020286
Iteration 87/1000 | Loss: 0.00020165
Iteration 88/1000 | Loss: 0.00004427
Iteration 89/1000 | Loss: 0.00003876
Iteration 90/1000 | Loss: 0.00003727
Iteration 91/1000 | Loss: 0.00003586
Iteration 92/1000 | Loss: 0.00062566
Iteration 93/1000 | Loss: 0.00110689
Iteration 94/1000 | Loss: 0.00062116
Iteration 95/1000 | Loss: 0.00033675
Iteration 96/1000 | Loss: 0.00046676
Iteration 97/1000 | Loss: 0.00004025
Iteration 98/1000 | Loss: 0.00003363
Iteration 99/1000 | Loss: 0.00003163
Iteration 100/1000 | Loss: 0.00003015
Iteration 101/1000 | Loss: 0.00002948
Iteration 102/1000 | Loss: 0.00061903
Iteration 103/1000 | Loss: 0.00048307
Iteration 104/1000 | Loss: 0.00051477
Iteration 105/1000 | Loss: 0.00006020
Iteration 106/1000 | Loss: 0.00003734
Iteration 107/1000 | Loss: 0.00003015
Iteration 108/1000 | Loss: 0.00002702
Iteration 109/1000 | Loss: 0.00002606
Iteration 110/1000 | Loss: 0.00002513
Iteration 111/1000 | Loss: 0.00002372
Iteration 112/1000 | Loss: 0.00002315
Iteration 113/1000 | Loss: 0.00002247
Iteration 114/1000 | Loss: 0.00049107
Iteration 115/1000 | Loss: 0.00009315
Iteration 116/1000 | Loss: 0.00004172
Iteration 117/1000 | Loss: 0.00003243
Iteration 118/1000 | Loss: 0.00002791
Iteration 119/1000 | Loss: 0.00002538
Iteration 120/1000 | Loss: 0.00002700
Iteration 121/1000 | Loss: 0.00002410
Iteration 122/1000 | Loss: 0.00002133
Iteration 123/1000 | Loss: 0.00002032
Iteration 124/1000 | Loss: 0.00001951
Iteration 125/1000 | Loss: 0.00001881
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001742
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001673
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001666
Iteration 135/1000 | Loss: 0.00001665
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001664
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001663
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001663
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001661
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001660
Iteration 147/1000 | Loss: 0.00001660
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001658
Iteration 151/1000 | Loss: 0.00001658
Iteration 152/1000 | Loss: 0.00001657
Iteration 153/1000 | Loss: 0.00001657
Iteration 154/1000 | Loss: 0.00001657
Iteration 155/1000 | Loss: 0.00001657
Iteration 156/1000 | Loss: 0.00001657
Iteration 157/1000 | Loss: 0.00001657
Iteration 158/1000 | Loss: 0.00001657
Iteration 159/1000 | Loss: 0.00001657
Iteration 160/1000 | Loss: 0.00001657
Iteration 161/1000 | Loss: 0.00001656
Iteration 162/1000 | Loss: 0.00001656
Iteration 163/1000 | Loss: 0.00001656
Iteration 164/1000 | Loss: 0.00001656
Iteration 165/1000 | Loss: 0.00001656
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001656
Iteration 168/1000 | Loss: 0.00001656
Iteration 169/1000 | Loss: 0.00001656
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001655
Iteration 186/1000 | Loss: 0.00001655
Iteration 187/1000 | Loss: 0.00001655
Iteration 188/1000 | Loss: 0.00001655
Iteration 189/1000 | Loss: 0.00001655
Iteration 190/1000 | Loss: 0.00001655
Iteration 191/1000 | Loss: 0.00001655
Iteration 192/1000 | Loss: 0.00001655
Iteration 193/1000 | Loss: 0.00001655
Iteration 194/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.6546482584089972e-05, 1.6546482584089972e-05, 1.6546482584089972e-05, 1.6546482584089972e-05, 1.6546482584089972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6546482584089972e-05

Optimization complete. Final v2v error: 3.3526840209960938 mm

Highest mean error: 7.605639457702637 mm for frame 96

Lowest mean error: 2.7585039138793945 mm for frame 3

Saving results

Total time: 233.32854390144348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832122
Iteration 2/25 | Loss: 0.00167490
Iteration 3/25 | Loss: 0.00147524
Iteration 4/25 | Loss: 0.00145016
Iteration 5/25 | Loss: 0.00144233
Iteration 6/25 | Loss: 0.00145291
Iteration 7/25 | Loss: 0.00139411
Iteration 8/25 | Loss: 0.00137289
Iteration 9/25 | Loss: 0.00137187
Iteration 10/25 | Loss: 0.00135608
Iteration 11/25 | Loss: 0.00135198
Iteration 12/25 | Loss: 0.00135086
Iteration 13/25 | Loss: 0.00134066
Iteration 14/25 | Loss: 0.00133988
Iteration 15/25 | Loss: 0.00133982
Iteration 16/25 | Loss: 0.00133982
Iteration 17/25 | Loss: 0.00133982
Iteration 18/25 | Loss: 0.00133982
Iteration 19/25 | Loss: 0.00133981
Iteration 20/25 | Loss: 0.00133981
Iteration 21/25 | Loss: 0.00133981
Iteration 22/25 | Loss: 0.00133981
Iteration 23/25 | Loss: 0.00133981
Iteration 24/25 | Loss: 0.00133981
Iteration 25/25 | Loss: 0.00133981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83667988
Iteration 2/25 | Loss: 0.00134897
Iteration 3/25 | Loss: 0.00134896
Iteration 4/25 | Loss: 0.00134896
Iteration 5/25 | Loss: 0.00134896
Iteration 6/25 | Loss: 0.00134896
Iteration 7/25 | Loss: 0.00134896
Iteration 8/25 | Loss: 0.00134896
Iteration 9/25 | Loss: 0.00134896
Iteration 10/25 | Loss: 0.00134896
Iteration 11/25 | Loss: 0.00134896
Iteration 12/25 | Loss: 0.00134896
Iteration 13/25 | Loss: 0.00134896
Iteration 14/25 | Loss: 0.00134896
Iteration 15/25 | Loss: 0.00134896
Iteration 16/25 | Loss: 0.00134896
Iteration 17/25 | Loss: 0.00134896
Iteration 18/25 | Loss: 0.00134896
Iteration 19/25 | Loss: 0.00134896
Iteration 20/25 | Loss: 0.00134896
Iteration 21/25 | Loss: 0.00134896
Iteration 22/25 | Loss: 0.00134896
Iteration 23/25 | Loss: 0.00134896
Iteration 24/25 | Loss: 0.00134896
Iteration 25/25 | Loss: 0.00134896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134896
Iteration 2/1000 | Loss: 0.00003379
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002427
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002245
Iteration 7/1000 | Loss: 0.00002178
Iteration 8/1000 | Loss: 0.00002127
Iteration 9/1000 | Loss: 0.00002083
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002008
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001963
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001940
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001937
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001928
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001926
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001925
Iteration 45/1000 | Loss: 0.00001925
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001921
Iteration 74/1000 | Loss: 0.00001921
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001920
Iteration 80/1000 | Loss: 0.00001920
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001919
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001919
Iteration 85/1000 | Loss: 0.00001919
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001917
Iteration 91/1000 | Loss: 0.00001917
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001915
Iteration 97/1000 | Loss: 0.00001915
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001915
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001914
Iteration 107/1000 | Loss: 0.00001914
Iteration 108/1000 | Loss: 0.00001914
Iteration 109/1000 | Loss: 0.00001914
Iteration 110/1000 | Loss: 0.00001914
Iteration 111/1000 | Loss: 0.00001914
Iteration 112/1000 | Loss: 0.00001914
Iteration 113/1000 | Loss: 0.00001914
Iteration 114/1000 | Loss: 0.00001914
Iteration 115/1000 | Loss: 0.00001914
Iteration 116/1000 | Loss: 0.00001914
Iteration 117/1000 | Loss: 0.00001914
Iteration 118/1000 | Loss: 0.00001914
Iteration 119/1000 | Loss: 0.00001914
Iteration 120/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.9135195543640293e-05, 1.9135195543640293e-05, 1.9135195543640293e-05, 1.9135195543640293e-05, 1.9135195543640293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9135195543640293e-05

Optimization complete. Final v2v error: 3.7456202507019043 mm

Highest mean error: 3.9341323375701904 mm for frame 20

Lowest mean error: 3.6255176067352295 mm for frame 0

Saving results

Total time: 56.126168727874756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779443
Iteration 2/25 | Loss: 0.00151458
Iteration 3/25 | Loss: 0.00137729
Iteration 4/25 | Loss: 0.00135578
Iteration 5/25 | Loss: 0.00134785
Iteration 6/25 | Loss: 0.00134877
Iteration 7/25 | Loss: 0.00134596
Iteration 8/25 | Loss: 0.00134394
Iteration 9/25 | Loss: 0.00134342
Iteration 10/25 | Loss: 0.00134331
Iteration 11/25 | Loss: 0.00134330
Iteration 12/25 | Loss: 0.00134329
Iteration 13/25 | Loss: 0.00134329
Iteration 14/25 | Loss: 0.00134329
Iteration 15/25 | Loss: 0.00134329
Iteration 16/25 | Loss: 0.00134329
Iteration 17/25 | Loss: 0.00134329
Iteration 18/25 | Loss: 0.00134329
Iteration 19/25 | Loss: 0.00134329
Iteration 20/25 | Loss: 0.00134329
Iteration 21/25 | Loss: 0.00134329
Iteration 22/25 | Loss: 0.00134329
Iteration 23/25 | Loss: 0.00134328
Iteration 24/25 | Loss: 0.00134328
Iteration 25/25 | Loss: 0.00134328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47949147
Iteration 2/25 | Loss: 0.00186001
Iteration 3/25 | Loss: 0.00186000
Iteration 4/25 | Loss: 0.00186000
Iteration 5/25 | Loss: 0.00186000
Iteration 6/25 | Loss: 0.00186000
Iteration 7/25 | Loss: 0.00186000
Iteration 8/25 | Loss: 0.00186000
Iteration 9/25 | Loss: 0.00186000
Iteration 10/25 | Loss: 0.00186000
Iteration 11/25 | Loss: 0.00186000
Iteration 12/25 | Loss: 0.00186000
Iteration 13/25 | Loss: 0.00186000
Iteration 14/25 | Loss: 0.00186000
Iteration 15/25 | Loss: 0.00186000
Iteration 16/25 | Loss: 0.00186000
Iteration 17/25 | Loss: 0.00186000
Iteration 18/25 | Loss: 0.00186000
Iteration 19/25 | Loss: 0.00186000
Iteration 20/25 | Loss: 0.00186000
Iteration 21/25 | Loss: 0.00186000
Iteration 22/25 | Loss: 0.00186000
Iteration 23/25 | Loss: 0.00186000
Iteration 24/25 | Loss: 0.00186000
Iteration 25/25 | Loss: 0.00186000

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186000
Iteration 2/1000 | Loss: 0.00002956
Iteration 3/1000 | Loss: 0.00002157
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001631
Iteration 10/1000 | Loss: 0.00001599
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001566
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001527
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001506
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001501
Iteration 39/1000 | Loss: 0.00001501
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001501
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001498
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001497
Iteration 62/1000 | Loss: 0.00001497
Iteration 63/1000 | Loss: 0.00001497
Iteration 64/1000 | Loss: 0.00001497
Iteration 65/1000 | Loss: 0.00001497
Iteration 66/1000 | Loss: 0.00001497
Iteration 67/1000 | Loss: 0.00001496
Iteration 68/1000 | Loss: 0.00001496
Iteration 69/1000 | Loss: 0.00001496
Iteration 70/1000 | Loss: 0.00001496
Iteration 71/1000 | Loss: 0.00001496
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001494
Iteration 77/1000 | Loss: 0.00001494
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001494
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001493
Iteration 82/1000 | Loss: 0.00001493
Iteration 83/1000 | Loss: 0.00001493
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001492
Iteration 89/1000 | Loss: 0.00001492
Iteration 90/1000 | Loss: 0.00001491
Iteration 91/1000 | Loss: 0.00001491
Iteration 92/1000 | Loss: 0.00001491
Iteration 93/1000 | Loss: 0.00001491
Iteration 94/1000 | Loss: 0.00001490
Iteration 95/1000 | Loss: 0.00001490
Iteration 96/1000 | Loss: 0.00001490
Iteration 97/1000 | Loss: 0.00001490
Iteration 98/1000 | Loss: 0.00001490
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001490
Iteration 101/1000 | Loss: 0.00001490
Iteration 102/1000 | Loss: 0.00001490
Iteration 103/1000 | Loss: 0.00001490
Iteration 104/1000 | Loss: 0.00001490
Iteration 105/1000 | Loss: 0.00001490
Iteration 106/1000 | Loss: 0.00001490
Iteration 107/1000 | Loss: 0.00001490
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001490
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001490
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001490
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.490095291956095e-05, 1.490095291956095e-05, 1.490095291956095e-05, 1.490095291956095e-05, 1.490095291956095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.490095291956095e-05

Optimization complete. Final v2v error: 3.308880090713501 mm

Highest mean error: 3.932142734527588 mm for frame 173

Lowest mean error: 2.95503568649292 mm for frame 52

Saving results

Total time: 52.27779841423035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569220
Iteration 2/25 | Loss: 0.00138254
Iteration 3/25 | Loss: 0.00131925
Iteration 4/25 | Loss: 0.00131036
Iteration 5/25 | Loss: 0.00130725
Iteration 6/25 | Loss: 0.00130706
Iteration 7/25 | Loss: 0.00130706
Iteration 8/25 | Loss: 0.00130706
Iteration 9/25 | Loss: 0.00130706
Iteration 10/25 | Loss: 0.00130706
Iteration 11/25 | Loss: 0.00130706
Iteration 12/25 | Loss: 0.00130706
Iteration 13/25 | Loss: 0.00130706
Iteration 14/25 | Loss: 0.00130706
Iteration 15/25 | Loss: 0.00130706
Iteration 16/25 | Loss: 0.00130706
Iteration 17/25 | Loss: 0.00130706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001307056867517531, 0.001307056867517531, 0.001307056867517531, 0.001307056867517531, 0.001307056867517531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001307056867517531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65557337
Iteration 2/25 | Loss: 0.00205579
Iteration 3/25 | Loss: 0.00205579
Iteration 4/25 | Loss: 0.00205579
Iteration 5/25 | Loss: 0.00205579
Iteration 6/25 | Loss: 0.00205579
Iteration 7/25 | Loss: 0.00205579
Iteration 8/25 | Loss: 0.00205579
Iteration 9/25 | Loss: 0.00205578
Iteration 10/25 | Loss: 0.00205578
Iteration 11/25 | Loss: 0.00205578
Iteration 12/25 | Loss: 0.00205578
Iteration 13/25 | Loss: 0.00205578
Iteration 14/25 | Loss: 0.00205578
Iteration 15/25 | Loss: 0.00205578
Iteration 16/25 | Loss: 0.00205578
Iteration 17/25 | Loss: 0.00205578
Iteration 18/25 | Loss: 0.00205578
Iteration 19/25 | Loss: 0.00205578
Iteration 20/25 | Loss: 0.00205578
Iteration 21/25 | Loss: 0.00205578
Iteration 22/25 | Loss: 0.00205578
Iteration 23/25 | Loss: 0.00205578
Iteration 24/25 | Loss: 0.00205578
Iteration 25/25 | Loss: 0.00205578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205578
Iteration 2/1000 | Loss: 0.00002121
Iteration 3/1000 | Loss: 0.00001574
Iteration 4/1000 | Loss: 0.00001414
Iteration 5/1000 | Loss: 0.00001338
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001133
Iteration 10/1000 | Loss: 0.00001116
Iteration 11/1000 | Loss: 0.00001112
Iteration 12/1000 | Loss: 0.00001103
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001081
Iteration 16/1000 | Loss: 0.00001080
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001078
Iteration 19/1000 | Loss: 0.00001073
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001045
Iteration 26/1000 | Loss: 0.00001034
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001027
Iteration 29/1000 | Loss: 0.00001026
Iteration 30/1000 | Loss: 0.00001025
Iteration 31/1000 | Loss: 0.00001025
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001023
Iteration 34/1000 | Loss: 0.00001023
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001022
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001021
Iteration 39/1000 | Loss: 0.00001020
Iteration 40/1000 | Loss: 0.00001020
Iteration 41/1000 | Loss: 0.00001020
Iteration 42/1000 | Loss: 0.00001020
Iteration 43/1000 | Loss: 0.00001020
Iteration 44/1000 | Loss: 0.00001020
Iteration 45/1000 | Loss: 0.00001020
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001019
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001018
Iteration 52/1000 | Loss: 0.00001018
Iteration 53/1000 | Loss: 0.00001018
Iteration 54/1000 | Loss: 0.00001017
Iteration 55/1000 | Loss: 0.00001017
Iteration 56/1000 | Loss: 0.00001017
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001016
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001015
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001015
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001014
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001013
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001012
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001009
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001008
Iteration 95/1000 | Loss: 0.00001008
Iteration 96/1000 | Loss: 0.00001008
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001008
Iteration 100/1000 | Loss: 0.00001008
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001008
Iteration 106/1000 | Loss: 0.00001007
Iteration 107/1000 | Loss: 0.00001007
Iteration 108/1000 | Loss: 0.00001007
Iteration 109/1000 | Loss: 0.00001007
Iteration 110/1000 | Loss: 0.00001007
Iteration 111/1000 | Loss: 0.00001007
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001007
Iteration 114/1000 | Loss: 0.00001007
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001006
Iteration 118/1000 | Loss: 0.00001006
Iteration 119/1000 | Loss: 0.00001006
Iteration 120/1000 | Loss: 0.00001006
Iteration 121/1000 | Loss: 0.00001006
Iteration 122/1000 | Loss: 0.00001006
Iteration 123/1000 | Loss: 0.00001006
Iteration 124/1000 | Loss: 0.00001006
Iteration 125/1000 | Loss: 0.00001006
Iteration 126/1000 | Loss: 0.00001006
Iteration 127/1000 | Loss: 0.00001006
Iteration 128/1000 | Loss: 0.00001006
Iteration 129/1000 | Loss: 0.00001006
Iteration 130/1000 | Loss: 0.00001005
Iteration 131/1000 | Loss: 0.00001005
Iteration 132/1000 | Loss: 0.00001005
Iteration 133/1000 | Loss: 0.00001005
Iteration 134/1000 | Loss: 0.00001005
Iteration 135/1000 | Loss: 0.00001005
Iteration 136/1000 | Loss: 0.00001005
Iteration 137/1000 | Loss: 0.00001005
Iteration 138/1000 | Loss: 0.00001005
Iteration 139/1000 | Loss: 0.00001005
Iteration 140/1000 | Loss: 0.00001005
Iteration 141/1000 | Loss: 0.00001005
Iteration 142/1000 | Loss: 0.00001005
Iteration 143/1000 | Loss: 0.00001005
Iteration 144/1000 | Loss: 0.00001005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.005471949611092e-05, 1.005471949611092e-05, 1.005471949611092e-05, 1.005471949611092e-05, 1.005471949611092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.005471949611092e-05

Optimization complete. Final v2v error: 2.7768046855926514 mm

Highest mean error: 3.09084415435791 mm for frame 112

Lowest mean error: 2.636209726333618 mm for frame 38

Saving results

Total time: 37.80215811729431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765063
Iteration 2/25 | Loss: 0.00149448
Iteration 3/25 | Loss: 0.00137283
Iteration 4/25 | Loss: 0.00135866
Iteration 5/25 | Loss: 0.00135797
Iteration 6/25 | Loss: 0.00135797
Iteration 7/25 | Loss: 0.00135797
Iteration 8/25 | Loss: 0.00135797
Iteration 9/25 | Loss: 0.00135797
Iteration 10/25 | Loss: 0.00135797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013579707592725754, 0.0013579707592725754, 0.0013579707592725754, 0.0013579707592725754, 0.0013579707592725754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013579707592725754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20438945
Iteration 2/25 | Loss: 0.00182744
Iteration 3/25 | Loss: 0.00182743
Iteration 4/25 | Loss: 0.00182743
Iteration 5/25 | Loss: 0.00182743
Iteration 6/25 | Loss: 0.00182743
Iteration 7/25 | Loss: 0.00182743
Iteration 8/25 | Loss: 0.00182743
Iteration 9/25 | Loss: 0.00182743
Iteration 10/25 | Loss: 0.00182743
Iteration 11/25 | Loss: 0.00182743
Iteration 12/25 | Loss: 0.00182743
Iteration 13/25 | Loss: 0.00182743
Iteration 14/25 | Loss: 0.00182743
Iteration 15/25 | Loss: 0.00182743
Iteration 16/25 | Loss: 0.00182743
Iteration 17/25 | Loss: 0.00182743
Iteration 18/25 | Loss: 0.00182743
Iteration 19/25 | Loss: 0.00182743
Iteration 20/25 | Loss: 0.00182743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018274253234267235, 0.0018274253234267235, 0.0018274253234267235, 0.0018274253234267235, 0.0018274253234267235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018274253234267235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182743
Iteration 2/1000 | Loss: 0.00002908
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001969
Iteration 5/1000 | Loss: 0.00001849
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001744
Iteration 8/1000 | Loss: 0.00001696
Iteration 9/1000 | Loss: 0.00001648
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001575
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001485
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001466
Iteration 33/1000 | Loss: 0.00001461
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001454
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001447
Iteration 64/1000 | Loss: 0.00001447
Iteration 65/1000 | Loss: 0.00001447
Iteration 66/1000 | Loss: 0.00001447
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001441
Iteration 89/1000 | Loss: 0.00001441
Iteration 90/1000 | Loss: 0.00001441
Iteration 91/1000 | Loss: 0.00001441
Iteration 92/1000 | Loss: 0.00001441
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001440
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001440
Iteration 128/1000 | Loss: 0.00001440
Iteration 129/1000 | Loss: 0.00001440
Iteration 130/1000 | Loss: 0.00001440
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001440
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001440
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4402135093405377e-05, 1.4402135093405377e-05, 1.4402135093405377e-05, 1.4402135093405377e-05, 1.4402135093405377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4402135093405377e-05

Optimization complete. Final v2v error: 3.2209558486938477 mm

Highest mean error: 3.8225457668304443 mm for frame 233

Lowest mean error: 2.6023635864257812 mm for frame 79

Saving results

Total time: 44.94092512130737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356003
Iteration 2/25 | Loss: 0.00133619
Iteration 3/25 | Loss: 0.00128687
Iteration 4/25 | Loss: 0.00128017
Iteration 5/25 | Loss: 0.00127791
Iteration 6/25 | Loss: 0.00127786
Iteration 7/25 | Loss: 0.00127786
Iteration 8/25 | Loss: 0.00127786
Iteration 9/25 | Loss: 0.00127786
Iteration 10/25 | Loss: 0.00127786
Iteration 11/25 | Loss: 0.00127786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012778647942468524, 0.0012778647942468524, 0.0012778647942468524, 0.0012778647942468524, 0.0012778647942468524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012778647942468524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25236189
Iteration 2/25 | Loss: 0.00221474
Iteration 3/25 | Loss: 0.00221474
Iteration 4/25 | Loss: 0.00221474
Iteration 5/25 | Loss: 0.00221474
Iteration 6/25 | Loss: 0.00221474
Iteration 7/25 | Loss: 0.00221474
Iteration 8/25 | Loss: 0.00221474
Iteration 9/25 | Loss: 0.00221474
Iteration 10/25 | Loss: 0.00221474
Iteration 11/25 | Loss: 0.00221474
Iteration 12/25 | Loss: 0.00221474
Iteration 13/25 | Loss: 0.00221473
Iteration 14/25 | Loss: 0.00221473
Iteration 15/25 | Loss: 0.00221473
Iteration 16/25 | Loss: 0.00221473
Iteration 17/25 | Loss: 0.00221473
Iteration 18/25 | Loss: 0.00221473
Iteration 19/25 | Loss: 0.00221473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002214734675362706, 0.002214734675362706, 0.002214734675362706, 0.002214734675362706, 0.002214734675362706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002214734675362706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221473
Iteration 2/1000 | Loss: 0.00001964
Iteration 3/1000 | Loss: 0.00001377
Iteration 4/1000 | Loss: 0.00001212
Iteration 5/1000 | Loss: 0.00001136
Iteration 6/1000 | Loss: 0.00001071
Iteration 7/1000 | Loss: 0.00001030
Iteration 8/1000 | Loss: 0.00000996
Iteration 9/1000 | Loss: 0.00000966
Iteration 10/1000 | Loss: 0.00000951
Iteration 11/1000 | Loss: 0.00000945
Iteration 12/1000 | Loss: 0.00000927
Iteration 13/1000 | Loss: 0.00000914
Iteration 14/1000 | Loss: 0.00000912
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000903
Iteration 17/1000 | Loss: 0.00000893
Iteration 18/1000 | Loss: 0.00000893
Iteration 19/1000 | Loss: 0.00000892
Iteration 20/1000 | Loss: 0.00000891
Iteration 21/1000 | Loss: 0.00000889
Iteration 22/1000 | Loss: 0.00000882
Iteration 23/1000 | Loss: 0.00000881
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000875
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000870
Iteration 28/1000 | Loss: 0.00000869
Iteration 29/1000 | Loss: 0.00000869
Iteration 30/1000 | Loss: 0.00000869
Iteration 31/1000 | Loss: 0.00000869
Iteration 32/1000 | Loss: 0.00000868
Iteration 33/1000 | Loss: 0.00000868
Iteration 34/1000 | Loss: 0.00000868
Iteration 35/1000 | Loss: 0.00000867
Iteration 36/1000 | Loss: 0.00000866
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000865
Iteration 40/1000 | Loss: 0.00000864
Iteration 41/1000 | Loss: 0.00000863
Iteration 42/1000 | Loss: 0.00000862
Iteration 43/1000 | Loss: 0.00000862
Iteration 44/1000 | Loss: 0.00000861
Iteration 45/1000 | Loss: 0.00000861
Iteration 46/1000 | Loss: 0.00000861
Iteration 47/1000 | Loss: 0.00000861
Iteration 48/1000 | Loss: 0.00000860
Iteration 49/1000 | Loss: 0.00000860
Iteration 50/1000 | Loss: 0.00000860
Iteration 51/1000 | Loss: 0.00000860
Iteration 52/1000 | Loss: 0.00000860
Iteration 53/1000 | Loss: 0.00000860
Iteration 54/1000 | Loss: 0.00000860
Iteration 55/1000 | Loss: 0.00000860
Iteration 56/1000 | Loss: 0.00000860
Iteration 57/1000 | Loss: 0.00000860
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000859
Iteration 60/1000 | Loss: 0.00000859
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000859
Iteration 66/1000 | Loss: 0.00000859
Iteration 67/1000 | Loss: 0.00000859
Iteration 68/1000 | Loss: 0.00000858
Iteration 69/1000 | Loss: 0.00000857
Iteration 70/1000 | Loss: 0.00000857
Iteration 71/1000 | Loss: 0.00000857
Iteration 72/1000 | Loss: 0.00000857
Iteration 73/1000 | Loss: 0.00000857
Iteration 74/1000 | Loss: 0.00000857
Iteration 75/1000 | Loss: 0.00000857
Iteration 76/1000 | Loss: 0.00000857
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000855
Iteration 80/1000 | Loss: 0.00000855
Iteration 81/1000 | Loss: 0.00000855
Iteration 82/1000 | Loss: 0.00000854
Iteration 83/1000 | Loss: 0.00000854
Iteration 84/1000 | Loss: 0.00000853
Iteration 85/1000 | Loss: 0.00000853
Iteration 86/1000 | Loss: 0.00000853
Iteration 87/1000 | Loss: 0.00000853
Iteration 88/1000 | Loss: 0.00000853
Iteration 89/1000 | Loss: 0.00000853
Iteration 90/1000 | Loss: 0.00000853
Iteration 91/1000 | Loss: 0.00000853
Iteration 92/1000 | Loss: 0.00000853
Iteration 93/1000 | Loss: 0.00000853
Iteration 94/1000 | Loss: 0.00000853
Iteration 95/1000 | Loss: 0.00000853
Iteration 96/1000 | Loss: 0.00000853
Iteration 97/1000 | Loss: 0.00000852
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000852
Iteration 102/1000 | Loss: 0.00000852
Iteration 103/1000 | Loss: 0.00000852
Iteration 104/1000 | Loss: 0.00000852
Iteration 105/1000 | Loss: 0.00000851
Iteration 106/1000 | Loss: 0.00000851
Iteration 107/1000 | Loss: 0.00000851
Iteration 108/1000 | Loss: 0.00000851
Iteration 109/1000 | Loss: 0.00000851
Iteration 110/1000 | Loss: 0.00000851
Iteration 111/1000 | Loss: 0.00000850
Iteration 112/1000 | Loss: 0.00000850
Iteration 113/1000 | Loss: 0.00000850
Iteration 114/1000 | Loss: 0.00000850
Iteration 115/1000 | Loss: 0.00000850
Iteration 116/1000 | Loss: 0.00000850
Iteration 117/1000 | Loss: 0.00000850
Iteration 118/1000 | Loss: 0.00000850
Iteration 119/1000 | Loss: 0.00000850
Iteration 120/1000 | Loss: 0.00000850
Iteration 121/1000 | Loss: 0.00000850
Iteration 122/1000 | Loss: 0.00000850
Iteration 123/1000 | Loss: 0.00000850
Iteration 124/1000 | Loss: 0.00000849
Iteration 125/1000 | Loss: 0.00000849
Iteration 126/1000 | Loss: 0.00000849
Iteration 127/1000 | Loss: 0.00000849
Iteration 128/1000 | Loss: 0.00000849
Iteration 129/1000 | Loss: 0.00000849
Iteration 130/1000 | Loss: 0.00000849
Iteration 131/1000 | Loss: 0.00000849
Iteration 132/1000 | Loss: 0.00000849
Iteration 133/1000 | Loss: 0.00000849
Iteration 134/1000 | Loss: 0.00000849
Iteration 135/1000 | Loss: 0.00000849
Iteration 136/1000 | Loss: 0.00000849
Iteration 137/1000 | Loss: 0.00000849
Iteration 138/1000 | Loss: 0.00000849
Iteration 139/1000 | Loss: 0.00000848
Iteration 140/1000 | Loss: 0.00000848
Iteration 141/1000 | Loss: 0.00000848
Iteration 142/1000 | Loss: 0.00000848
Iteration 143/1000 | Loss: 0.00000848
Iteration 144/1000 | Loss: 0.00000848
Iteration 145/1000 | Loss: 0.00000848
Iteration 146/1000 | Loss: 0.00000848
Iteration 147/1000 | Loss: 0.00000848
Iteration 148/1000 | Loss: 0.00000848
Iteration 149/1000 | Loss: 0.00000847
Iteration 150/1000 | Loss: 0.00000847
Iteration 151/1000 | Loss: 0.00000847
Iteration 152/1000 | Loss: 0.00000847
Iteration 153/1000 | Loss: 0.00000847
Iteration 154/1000 | Loss: 0.00000846
Iteration 155/1000 | Loss: 0.00000846
Iteration 156/1000 | Loss: 0.00000846
Iteration 157/1000 | Loss: 0.00000846
Iteration 158/1000 | Loss: 0.00000846
Iteration 159/1000 | Loss: 0.00000846
Iteration 160/1000 | Loss: 0.00000846
Iteration 161/1000 | Loss: 0.00000846
Iteration 162/1000 | Loss: 0.00000846
Iteration 163/1000 | Loss: 0.00000846
Iteration 164/1000 | Loss: 0.00000845
Iteration 165/1000 | Loss: 0.00000845
Iteration 166/1000 | Loss: 0.00000845
Iteration 167/1000 | Loss: 0.00000845
Iteration 168/1000 | Loss: 0.00000845
Iteration 169/1000 | Loss: 0.00000845
Iteration 170/1000 | Loss: 0.00000845
Iteration 171/1000 | Loss: 0.00000845
Iteration 172/1000 | Loss: 0.00000844
Iteration 173/1000 | Loss: 0.00000844
Iteration 174/1000 | Loss: 0.00000844
Iteration 175/1000 | Loss: 0.00000843
Iteration 176/1000 | Loss: 0.00000843
Iteration 177/1000 | Loss: 0.00000843
Iteration 178/1000 | Loss: 0.00000842
Iteration 179/1000 | Loss: 0.00000842
Iteration 180/1000 | Loss: 0.00000842
Iteration 181/1000 | Loss: 0.00000841
Iteration 182/1000 | Loss: 0.00000841
Iteration 183/1000 | Loss: 0.00000840
Iteration 184/1000 | Loss: 0.00000840
Iteration 185/1000 | Loss: 0.00000840
Iteration 186/1000 | Loss: 0.00000839
Iteration 187/1000 | Loss: 0.00000839
Iteration 188/1000 | Loss: 0.00000838
Iteration 189/1000 | Loss: 0.00000838
Iteration 190/1000 | Loss: 0.00000838
Iteration 191/1000 | Loss: 0.00000838
Iteration 192/1000 | Loss: 0.00000838
Iteration 193/1000 | Loss: 0.00000838
Iteration 194/1000 | Loss: 0.00000838
Iteration 195/1000 | Loss: 0.00000838
Iteration 196/1000 | Loss: 0.00000838
Iteration 197/1000 | Loss: 0.00000838
Iteration 198/1000 | Loss: 0.00000838
Iteration 199/1000 | Loss: 0.00000838
Iteration 200/1000 | Loss: 0.00000838
Iteration 201/1000 | Loss: 0.00000838
Iteration 202/1000 | Loss: 0.00000838
Iteration 203/1000 | Loss: 0.00000838
Iteration 204/1000 | Loss: 0.00000837
Iteration 205/1000 | Loss: 0.00000837
Iteration 206/1000 | Loss: 0.00000837
Iteration 207/1000 | Loss: 0.00000837
Iteration 208/1000 | Loss: 0.00000837
Iteration 209/1000 | Loss: 0.00000837
Iteration 210/1000 | Loss: 0.00000837
Iteration 211/1000 | Loss: 0.00000837
Iteration 212/1000 | Loss: 0.00000837
Iteration 213/1000 | Loss: 0.00000837
Iteration 214/1000 | Loss: 0.00000837
Iteration 215/1000 | Loss: 0.00000837
Iteration 216/1000 | Loss: 0.00000837
Iteration 217/1000 | Loss: 0.00000837
Iteration 218/1000 | Loss: 0.00000837
Iteration 219/1000 | Loss: 0.00000837
Iteration 220/1000 | Loss: 0.00000837
Iteration 221/1000 | Loss: 0.00000837
Iteration 222/1000 | Loss: 0.00000837
Iteration 223/1000 | Loss: 0.00000837
Iteration 224/1000 | Loss: 0.00000837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [8.373184755328111e-06, 8.373184755328111e-06, 8.373184755328111e-06, 8.373184755328111e-06, 8.373184755328111e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.373184755328111e-06

Optimization complete. Final v2v error: 2.5255446434020996 mm

Highest mean error: 2.607286214828491 mm for frame 131

Lowest mean error: 2.4880058765411377 mm for frame 82

Saving results

Total time: 42.776339292526245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712520
Iteration 2/25 | Loss: 0.00199785
Iteration 3/25 | Loss: 0.00150581
Iteration 4/25 | Loss: 0.00142504
Iteration 5/25 | Loss: 0.00142275
Iteration 6/25 | Loss: 0.00136864
Iteration 7/25 | Loss: 0.00136280
Iteration 8/25 | Loss: 0.00135766
Iteration 9/25 | Loss: 0.00135618
Iteration 10/25 | Loss: 0.00135368
Iteration 11/25 | Loss: 0.00135045
Iteration 12/25 | Loss: 0.00134944
Iteration 13/25 | Loss: 0.00134998
Iteration 14/25 | Loss: 0.00134743
Iteration 15/25 | Loss: 0.00134609
Iteration 16/25 | Loss: 0.00134558
Iteration 17/25 | Loss: 0.00134549
Iteration 18/25 | Loss: 0.00134549
Iteration 19/25 | Loss: 0.00134548
Iteration 20/25 | Loss: 0.00134548
Iteration 21/25 | Loss: 0.00134548
Iteration 22/25 | Loss: 0.00134548
Iteration 23/25 | Loss: 0.00134548
Iteration 24/25 | Loss: 0.00134548
Iteration 25/25 | Loss: 0.00134548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19194949
Iteration 2/25 | Loss: 0.00231734
Iteration 3/25 | Loss: 0.00231733
Iteration 4/25 | Loss: 0.00231733
Iteration 5/25 | Loss: 0.00231733
Iteration 6/25 | Loss: 0.00231733
Iteration 7/25 | Loss: 0.00231733
Iteration 8/25 | Loss: 0.00231733
Iteration 9/25 | Loss: 0.00231733
Iteration 10/25 | Loss: 0.00231733
Iteration 11/25 | Loss: 0.00231733
Iteration 12/25 | Loss: 0.00231733
Iteration 13/25 | Loss: 0.00231733
Iteration 14/25 | Loss: 0.00231733
Iteration 15/25 | Loss: 0.00231733
Iteration 16/25 | Loss: 0.00231733
Iteration 17/25 | Loss: 0.00231733
Iteration 18/25 | Loss: 0.00231733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023173256777226925, 0.0023173256777226925, 0.0023173256777226925, 0.0023173256777226925, 0.0023173256777226925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023173256777226925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231733
Iteration 2/1000 | Loss: 0.00004827
Iteration 3/1000 | Loss: 0.00005423
Iteration 4/1000 | Loss: 0.00002678
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00003456
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00001994
Iteration 9/1000 | Loss: 0.00001924
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001817
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001733
Iteration 14/1000 | Loss: 0.00001697
Iteration 15/1000 | Loss: 0.00001670
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001609
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001575
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001574
Iteration 47/1000 | Loss: 0.00001574
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001573
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001570
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001569
Iteration 65/1000 | Loss: 0.00001569
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001569
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.558595431561116e-05, 1.558595431561116e-05, 1.558595431561116e-05, 1.558595431561116e-05, 1.558595431561116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.558595431561116e-05

Optimization complete. Final v2v error: 3.3854215145111084 mm

Highest mean error: 4.210896968841553 mm for frame 233

Lowest mean error: 3.150585651397705 mm for frame 173

Saving results

Total time: 75.23768854141235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979147
Iteration 2/25 | Loss: 0.00259283
Iteration 3/25 | Loss: 0.00191442
Iteration 4/25 | Loss: 0.00178551
Iteration 5/25 | Loss: 0.00188477
Iteration 6/25 | Loss: 0.00174084
Iteration 7/25 | Loss: 0.00149101
Iteration 8/25 | Loss: 0.00141936
Iteration 9/25 | Loss: 0.00141053
Iteration 10/25 | Loss: 0.00140687
Iteration 11/25 | Loss: 0.00140775
Iteration 12/25 | Loss: 0.00140814
Iteration 13/25 | Loss: 0.00140355
Iteration 14/25 | Loss: 0.00140223
Iteration 15/25 | Loss: 0.00140199
Iteration 16/25 | Loss: 0.00140193
Iteration 17/25 | Loss: 0.00140193
Iteration 18/25 | Loss: 0.00140193
Iteration 19/25 | Loss: 0.00140193
Iteration 20/25 | Loss: 0.00140193
Iteration 21/25 | Loss: 0.00140193
Iteration 22/25 | Loss: 0.00140193
Iteration 23/25 | Loss: 0.00140193
Iteration 24/25 | Loss: 0.00140193
Iteration 25/25 | Loss: 0.00140193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20833588
Iteration 2/25 | Loss: 0.00205597
Iteration 3/25 | Loss: 0.00205597
Iteration 4/25 | Loss: 0.00205597
Iteration 5/25 | Loss: 0.00205596
Iteration 6/25 | Loss: 0.00205596
Iteration 7/25 | Loss: 0.00205596
Iteration 8/25 | Loss: 0.00205596
Iteration 9/25 | Loss: 0.00205596
Iteration 10/25 | Loss: 0.00205596
Iteration 11/25 | Loss: 0.00205596
Iteration 12/25 | Loss: 0.00205596
Iteration 13/25 | Loss: 0.00205596
Iteration 14/25 | Loss: 0.00205596
Iteration 15/25 | Loss: 0.00205596
Iteration 16/25 | Loss: 0.00205596
Iteration 17/25 | Loss: 0.00205596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020559628028422594, 0.0020559628028422594, 0.0020559628028422594, 0.0020559628028422594, 0.0020559628028422594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020559628028422594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205596
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00002800
Iteration 4/1000 | Loss: 0.00002631
Iteration 5/1000 | Loss: 0.00002547
Iteration 6/1000 | Loss: 0.00002464
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002350
Iteration 9/1000 | Loss: 0.00002298
Iteration 10/1000 | Loss: 0.00002269
Iteration 11/1000 | Loss: 0.00002243
Iteration 12/1000 | Loss: 0.00002229
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002212
Iteration 15/1000 | Loss: 0.00002207
Iteration 16/1000 | Loss: 0.00002206
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002201
Iteration 19/1000 | Loss: 0.00002190
Iteration 20/1000 | Loss: 0.00002189
Iteration 21/1000 | Loss: 0.00002189
Iteration 22/1000 | Loss: 0.00002188
Iteration 23/1000 | Loss: 0.00002188
Iteration 24/1000 | Loss: 0.00002187
Iteration 25/1000 | Loss: 0.00002186
Iteration 26/1000 | Loss: 0.00002186
Iteration 27/1000 | Loss: 0.00002185
Iteration 28/1000 | Loss: 0.00002185
Iteration 29/1000 | Loss: 0.00002185
Iteration 30/1000 | Loss: 0.00002185
Iteration 31/1000 | Loss: 0.00002185
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002182
Iteration 38/1000 | Loss: 0.00002182
Iteration 39/1000 | Loss: 0.00002182
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002181
Iteration 52/1000 | Loss: 0.00002181
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002177
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00002176
Iteration 73/1000 | Loss: 0.00002176
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002175
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002174
Iteration 82/1000 | Loss: 0.00002174
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002173
Iteration 87/1000 | Loss: 0.00002172
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002171
Iteration 90/1000 | Loss: 0.00002171
Iteration 91/1000 | Loss: 0.00002171
Iteration 92/1000 | Loss: 0.00002170
Iteration 93/1000 | Loss: 0.00002170
Iteration 94/1000 | Loss: 0.00002170
Iteration 95/1000 | Loss: 0.00002170
Iteration 96/1000 | Loss: 0.00002170
Iteration 97/1000 | Loss: 0.00002170
Iteration 98/1000 | Loss: 0.00002169
Iteration 99/1000 | Loss: 0.00002169
Iteration 100/1000 | Loss: 0.00002168
Iteration 101/1000 | Loss: 0.00002168
Iteration 102/1000 | Loss: 0.00002168
Iteration 103/1000 | Loss: 0.00002168
Iteration 104/1000 | Loss: 0.00002168
Iteration 105/1000 | Loss: 0.00002168
Iteration 106/1000 | Loss: 0.00002167
Iteration 107/1000 | Loss: 0.00002167
Iteration 108/1000 | Loss: 0.00002167
Iteration 109/1000 | Loss: 0.00002166
Iteration 110/1000 | Loss: 0.00002166
Iteration 111/1000 | Loss: 0.00002166
Iteration 112/1000 | Loss: 0.00002165
Iteration 113/1000 | Loss: 0.00002165
Iteration 114/1000 | Loss: 0.00002165
Iteration 115/1000 | Loss: 0.00002165
Iteration 116/1000 | Loss: 0.00002164
Iteration 117/1000 | Loss: 0.00002164
Iteration 118/1000 | Loss: 0.00002163
Iteration 119/1000 | Loss: 0.00002163
Iteration 120/1000 | Loss: 0.00002163
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002162
Iteration 123/1000 | Loss: 0.00002162
Iteration 124/1000 | Loss: 0.00002162
Iteration 125/1000 | Loss: 0.00002162
Iteration 126/1000 | Loss: 0.00002162
Iteration 127/1000 | Loss: 0.00002161
Iteration 128/1000 | Loss: 0.00002161
Iteration 129/1000 | Loss: 0.00002161
Iteration 130/1000 | Loss: 0.00002161
Iteration 131/1000 | Loss: 0.00002161
Iteration 132/1000 | Loss: 0.00002161
Iteration 133/1000 | Loss: 0.00002161
Iteration 134/1000 | Loss: 0.00002161
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002161
Iteration 137/1000 | Loss: 0.00002161
Iteration 138/1000 | Loss: 0.00002161
Iteration 139/1000 | Loss: 0.00002161
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002161
Iteration 142/1000 | Loss: 0.00002161
Iteration 143/1000 | Loss: 0.00002161
Iteration 144/1000 | Loss: 0.00002161
Iteration 145/1000 | Loss: 0.00002161
Iteration 146/1000 | Loss: 0.00002161
Iteration 147/1000 | Loss: 0.00002161
Iteration 148/1000 | Loss: 0.00002161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.1612549971905537e-05, 2.1612549971905537e-05, 2.1612549971905537e-05, 2.1612549971905537e-05, 2.1612549971905537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1612549971905537e-05

Optimization complete. Final v2v error: 3.9843790531158447 mm

Highest mean error: 4.264944076538086 mm for frame 0

Lowest mean error: 3.695770740509033 mm for frame 68

Saving results

Total time: 64.11483979225159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392955
Iteration 2/25 | Loss: 0.00138319
Iteration 3/25 | Loss: 0.00132201
Iteration 4/25 | Loss: 0.00131227
Iteration 5/25 | Loss: 0.00131137
Iteration 6/25 | Loss: 0.00131137
Iteration 7/25 | Loss: 0.00131137
Iteration 8/25 | Loss: 0.00131137
Iteration 9/25 | Loss: 0.00131137
Iteration 10/25 | Loss: 0.00131137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001311371917836368, 0.001311371917836368, 0.001311371917836368, 0.001311371917836368, 0.001311371917836368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001311371917836368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24351764
Iteration 2/25 | Loss: 0.00205902
Iteration 3/25 | Loss: 0.00205902
Iteration 4/25 | Loss: 0.00205902
Iteration 5/25 | Loss: 0.00205902
Iteration 6/25 | Loss: 0.00205902
Iteration 7/25 | Loss: 0.00205902
Iteration 8/25 | Loss: 0.00205902
Iteration 9/25 | Loss: 0.00205902
Iteration 10/25 | Loss: 0.00205902
Iteration 11/25 | Loss: 0.00205902
Iteration 12/25 | Loss: 0.00205902
Iteration 13/25 | Loss: 0.00205902
Iteration 14/25 | Loss: 0.00205902
Iteration 15/25 | Loss: 0.00205902
Iteration 16/25 | Loss: 0.00205902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002059017075225711, 0.002059017075225711, 0.002059017075225711, 0.002059017075225711, 0.002059017075225711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002059017075225711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205902
Iteration 2/1000 | Loss: 0.00002037
Iteration 3/1000 | Loss: 0.00001704
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001280
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001215
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001185
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001178
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001156
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001155
Iteration 131/1000 | Loss: 0.00001155
Iteration 132/1000 | Loss: 0.00001155
Iteration 133/1000 | Loss: 0.00001155
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001154
Iteration 136/1000 | Loss: 0.00001154
Iteration 137/1000 | Loss: 0.00001154
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001152
Iteration 144/1000 | Loss: 0.00001152
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001152
Iteration 150/1000 | Loss: 0.00001152
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001151
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001151
Iteration 156/1000 | Loss: 0.00001151
Iteration 157/1000 | Loss: 0.00001151
Iteration 158/1000 | Loss: 0.00001151
Iteration 159/1000 | Loss: 0.00001151
Iteration 160/1000 | Loss: 0.00001151
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00001150
Iteration 163/1000 | Loss: 0.00001150
Iteration 164/1000 | Loss: 0.00001150
Iteration 165/1000 | Loss: 0.00001150
Iteration 166/1000 | Loss: 0.00001150
Iteration 167/1000 | Loss: 0.00001150
Iteration 168/1000 | Loss: 0.00001150
Iteration 169/1000 | Loss: 0.00001150
Iteration 170/1000 | Loss: 0.00001150
Iteration 171/1000 | Loss: 0.00001150
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001150
Iteration 176/1000 | Loss: 0.00001150
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001149
Iteration 179/1000 | Loss: 0.00001149
Iteration 180/1000 | Loss: 0.00001149
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001149
Iteration 187/1000 | Loss: 0.00001149
Iteration 188/1000 | Loss: 0.00001149
Iteration 189/1000 | Loss: 0.00001149
Iteration 190/1000 | Loss: 0.00001149
Iteration 191/1000 | Loss: 0.00001149
Iteration 192/1000 | Loss: 0.00001149
Iteration 193/1000 | Loss: 0.00001149
Iteration 194/1000 | Loss: 0.00001149
Iteration 195/1000 | Loss: 0.00001149
Iteration 196/1000 | Loss: 0.00001148
Iteration 197/1000 | Loss: 0.00001148
Iteration 198/1000 | Loss: 0.00001148
Iteration 199/1000 | Loss: 0.00001148
Iteration 200/1000 | Loss: 0.00001148
Iteration 201/1000 | Loss: 0.00001148
Iteration 202/1000 | Loss: 0.00001148
Iteration 203/1000 | Loss: 0.00001148
Iteration 204/1000 | Loss: 0.00001148
Iteration 205/1000 | Loss: 0.00001148
Iteration 206/1000 | Loss: 0.00001148
Iteration 207/1000 | Loss: 0.00001148
Iteration 208/1000 | Loss: 0.00001148
Iteration 209/1000 | Loss: 0.00001148
Iteration 210/1000 | Loss: 0.00001148
Iteration 211/1000 | Loss: 0.00001148
Iteration 212/1000 | Loss: 0.00001148
Iteration 213/1000 | Loss: 0.00001148
Iteration 214/1000 | Loss: 0.00001148
Iteration 215/1000 | Loss: 0.00001148
Iteration 216/1000 | Loss: 0.00001148
Iteration 217/1000 | Loss: 0.00001148
Iteration 218/1000 | Loss: 0.00001148
Iteration 219/1000 | Loss: 0.00001148
Iteration 220/1000 | Loss: 0.00001148
Iteration 221/1000 | Loss: 0.00001147
Iteration 222/1000 | Loss: 0.00001147
Iteration 223/1000 | Loss: 0.00001147
Iteration 224/1000 | Loss: 0.00001147
Iteration 225/1000 | Loss: 0.00001147
Iteration 226/1000 | Loss: 0.00001147
Iteration 227/1000 | Loss: 0.00001147
Iteration 228/1000 | Loss: 0.00001147
Iteration 229/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.1473513040982652e-05, 1.1473513040982652e-05, 1.1473513040982652e-05, 1.1473513040982652e-05, 1.1473513040982652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1473513040982652e-05

Optimization complete. Final v2v error: 2.9448604583740234 mm

Highest mean error: 3.0385639667510986 mm for frame 207

Lowest mean error: 2.8663787841796875 mm for frame 231

Saving results

Total time: 51.48916053771973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435238
Iteration 2/25 | Loss: 0.00143366
Iteration 3/25 | Loss: 0.00137542
Iteration 4/25 | Loss: 0.00136575
Iteration 5/25 | Loss: 0.00136242
Iteration 6/25 | Loss: 0.00136196
Iteration 7/25 | Loss: 0.00136196
Iteration 8/25 | Loss: 0.00136196
Iteration 9/25 | Loss: 0.00136196
Iteration 10/25 | Loss: 0.00136196
Iteration 11/25 | Loss: 0.00136196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001361963339149952, 0.001361963339149952, 0.001361963339149952, 0.001361963339149952, 0.001361963339149952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001361963339149952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65394473
Iteration 2/25 | Loss: 0.00219913
Iteration 3/25 | Loss: 0.00219913
Iteration 4/25 | Loss: 0.00219913
Iteration 5/25 | Loss: 0.00219913
Iteration 6/25 | Loss: 0.00219913
Iteration 7/25 | Loss: 0.00219913
Iteration 8/25 | Loss: 0.00219913
Iteration 9/25 | Loss: 0.00219913
Iteration 10/25 | Loss: 0.00219913
Iteration 11/25 | Loss: 0.00219913
Iteration 12/25 | Loss: 0.00219913
Iteration 13/25 | Loss: 0.00219913
Iteration 14/25 | Loss: 0.00219913
Iteration 15/25 | Loss: 0.00219913
Iteration 16/25 | Loss: 0.00219913
Iteration 17/25 | Loss: 0.00219913
Iteration 18/25 | Loss: 0.00219913
Iteration 19/25 | Loss: 0.00219913
Iteration 20/25 | Loss: 0.00219913
Iteration 21/25 | Loss: 0.00219913
Iteration 22/25 | Loss: 0.00219913
Iteration 23/25 | Loss: 0.00219913
Iteration 24/25 | Loss: 0.00219913
Iteration 25/25 | Loss: 0.00219913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219913
Iteration 2/1000 | Loss: 0.00003166
Iteration 3/1000 | Loss: 0.00002305
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001963
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001792
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001749
Iteration 13/1000 | Loss: 0.00001740
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001707
Iteration 20/1000 | Loss: 0.00001707
Iteration 21/1000 | Loss: 0.00001706
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001699
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001698
Iteration 36/1000 | Loss: 0.00001698
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001696
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001684
Iteration 87/1000 | Loss: 0.00001684
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001683
Iteration 94/1000 | Loss: 0.00001683
Iteration 95/1000 | Loss: 0.00001683
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001681
Iteration 103/1000 | Loss: 0.00001681
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001680
Iteration 107/1000 | Loss: 0.00001680
Iteration 108/1000 | Loss: 0.00001680
Iteration 109/1000 | Loss: 0.00001679
Iteration 110/1000 | Loss: 0.00001679
Iteration 111/1000 | Loss: 0.00001679
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001678
Iteration 115/1000 | Loss: 0.00001678
Iteration 116/1000 | Loss: 0.00001678
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001678
Iteration 119/1000 | Loss: 0.00001678
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001676
Iteration 124/1000 | Loss: 0.00001676
Iteration 125/1000 | Loss: 0.00001676
Iteration 126/1000 | Loss: 0.00001676
Iteration 127/1000 | Loss: 0.00001675
Iteration 128/1000 | Loss: 0.00001675
Iteration 129/1000 | Loss: 0.00001675
Iteration 130/1000 | Loss: 0.00001675
Iteration 131/1000 | Loss: 0.00001675
Iteration 132/1000 | Loss: 0.00001674
Iteration 133/1000 | Loss: 0.00001674
Iteration 134/1000 | Loss: 0.00001674
Iteration 135/1000 | Loss: 0.00001674
Iteration 136/1000 | Loss: 0.00001674
Iteration 137/1000 | Loss: 0.00001674
Iteration 138/1000 | Loss: 0.00001673
Iteration 139/1000 | Loss: 0.00001673
Iteration 140/1000 | Loss: 0.00001673
Iteration 141/1000 | Loss: 0.00001673
Iteration 142/1000 | Loss: 0.00001673
Iteration 143/1000 | Loss: 0.00001673
Iteration 144/1000 | Loss: 0.00001673
Iteration 145/1000 | Loss: 0.00001673
Iteration 146/1000 | Loss: 0.00001673
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001673
Iteration 149/1000 | Loss: 0.00001673
Iteration 150/1000 | Loss: 0.00001672
Iteration 151/1000 | Loss: 0.00001672
Iteration 152/1000 | Loss: 0.00001672
Iteration 153/1000 | Loss: 0.00001672
Iteration 154/1000 | Loss: 0.00001672
Iteration 155/1000 | Loss: 0.00001672
Iteration 156/1000 | Loss: 0.00001672
Iteration 157/1000 | Loss: 0.00001672
Iteration 158/1000 | Loss: 0.00001671
Iteration 159/1000 | Loss: 0.00001671
Iteration 160/1000 | Loss: 0.00001671
Iteration 161/1000 | Loss: 0.00001671
Iteration 162/1000 | Loss: 0.00001671
Iteration 163/1000 | Loss: 0.00001671
Iteration 164/1000 | Loss: 0.00001671
Iteration 165/1000 | Loss: 0.00001671
Iteration 166/1000 | Loss: 0.00001670
Iteration 167/1000 | Loss: 0.00001670
Iteration 168/1000 | Loss: 0.00001670
Iteration 169/1000 | Loss: 0.00001670
Iteration 170/1000 | Loss: 0.00001670
Iteration 171/1000 | Loss: 0.00001670
Iteration 172/1000 | Loss: 0.00001669
Iteration 173/1000 | Loss: 0.00001669
Iteration 174/1000 | Loss: 0.00001669
Iteration 175/1000 | Loss: 0.00001669
Iteration 176/1000 | Loss: 0.00001669
Iteration 177/1000 | Loss: 0.00001669
Iteration 178/1000 | Loss: 0.00001669
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001668
Iteration 186/1000 | Loss: 0.00001668
Iteration 187/1000 | Loss: 0.00001668
Iteration 188/1000 | Loss: 0.00001668
Iteration 189/1000 | Loss: 0.00001668
Iteration 190/1000 | Loss: 0.00001668
Iteration 191/1000 | Loss: 0.00001667
Iteration 192/1000 | Loss: 0.00001667
Iteration 193/1000 | Loss: 0.00001667
Iteration 194/1000 | Loss: 0.00001667
Iteration 195/1000 | Loss: 0.00001667
Iteration 196/1000 | Loss: 0.00001667
Iteration 197/1000 | Loss: 0.00001667
Iteration 198/1000 | Loss: 0.00001667
Iteration 199/1000 | Loss: 0.00001667
Iteration 200/1000 | Loss: 0.00001667
Iteration 201/1000 | Loss: 0.00001667
Iteration 202/1000 | Loss: 0.00001667
Iteration 203/1000 | Loss: 0.00001667
Iteration 204/1000 | Loss: 0.00001667
Iteration 205/1000 | Loss: 0.00001666
Iteration 206/1000 | Loss: 0.00001666
Iteration 207/1000 | Loss: 0.00001666
Iteration 208/1000 | Loss: 0.00001666
Iteration 209/1000 | Loss: 0.00001666
Iteration 210/1000 | Loss: 0.00001666
Iteration 211/1000 | Loss: 0.00001666
Iteration 212/1000 | Loss: 0.00001666
Iteration 213/1000 | Loss: 0.00001665
Iteration 214/1000 | Loss: 0.00001665
Iteration 215/1000 | Loss: 0.00001665
Iteration 216/1000 | Loss: 0.00001665
Iteration 217/1000 | Loss: 0.00001665
Iteration 218/1000 | Loss: 0.00001665
Iteration 219/1000 | Loss: 0.00001665
Iteration 220/1000 | Loss: 0.00001665
Iteration 221/1000 | Loss: 0.00001665
Iteration 222/1000 | Loss: 0.00001665
Iteration 223/1000 | Loss: 0.00001665
Iteration 224/1000 | Loss: 0.00001665
Iteration 225/1000 | Loss: 0.00001665
Iteration 226/1000 | Loss: 0.00001665
Iteration 227/1000 | Loss: 0.00001665
Iteration 228/1000 | Loss: 0.00001665
Iteration 229/1000 | Loss: 0.00001665
Iteration 230/1000 | Loss: 0.00001665
Iteration 231/1000 | Loss: 0.00001665
Iteration 232/1000 | Loss: 0.00001665
Iteration 233/1000 | Loss: 0.00001665
Iteration 234/1000 | Loss: 0.00001665
Iteration 235/1000 | Loss: 0.00001665
Iteration 236/1000 | Loss: 0.00001665
Iteration 237/1000 | Loss: 0.00001664
Iteration 238/1000 | Loss: 0.00001664
Iteration 239/1000 | Loss: 0.00001664
Iteration 240/1000 | Loss: 0.00001664
Iteration 241/1000 | Loss: 0.00001664
Iteration 242/1000 | Loss: 0.00001664
Iteration 243/1000 | Loss: 0.00001664
Iteration 244/1000 | Loss: 0.00001664
Iteration 245/1000 | Loss: 0.00001664
Iteration 246/1000 | Loss: 0.00001664
Iteration 247/1000 | Loss: 0.00001664
Iteration 248/1000 | Loss: 0.00001664
Iteration 249/1000 | Loss: 0.00001664
Iteration 250/1000 | Loss: 0.00001664
Iteration 251/1000 | Loss: 0.00001664
Iteration 252/1000 | Loss: 0.00001664
Iteration 253/1000 | Loss: 0.00001664
Iteration 254/1000 | Loss: 0.00001664
Iteration 255/1000 | Loss: 0.00001664
Iteration 256/1000 | Loss: 0.00001664
Iteration 257/1000 | Loss: 0.00001664
Iteration 258/1000 | Loss: 0.00001664
Iteration 259/1000 | Loss: 0.00001664
Iteration 260/1000 | Loss: 0.00001664
Iteration 261/1000 | Loss: 0.00001664
Iteration 262/1000 | Loss: 0.00001664
Iteration 263/1000 | Loss: 0.00001664
Iteration 264/1000 | Loss: 0.00001664
Iteration 265/1000 | Loss: 0.00001664
Iteration 266/1000 | Loss: 0.00001664
Iteration 267/1000 | Loss: 0.00001664
Iteration 268/1000 | Loss: 0.00001664
Iteration 269/1000 | Loss: 0.00001664
Iteration 270/1000 | Loss: 0.00001664
Iteration 271/1000 | Loss: 0.00001664
Iteration 272/1000 | Loss: 0.00001664
Iteration 273/1000 | Loss: 0.00001664
Iteration 274/1000 | Loss: 0.00001664
Iteration 275/1000 | Loss: 0.00001664
Iteration 276/1000 | Loss: 0.00001664
Iteration 277/1000 | Loss: 0.00001664
Iteration 278/1000 | Loss: 0.00001664
Iteration 279/1000 | Loss: 0.00001664
Iteration 280/1000 | Loss: 0.00001664
Iteration 281/1000 | Loss: 0.00001664
Iteration 282/1000 | Loss: 0.00001664
Iteration 283/1000 | Loss: 0.00001664
Iteration 284/1000 | Loss: 0.00001664
Iteration 285/1000 | Loss: 0.00001664
Iteration 286/1000 | Loss: 0.00001664
Iteration 287/1000 | Loss: 0.00001664
Iteration 288/1000 | Loss: 0.00001664
Iteration 289/1000 | Loss: 0.00001664
Iteration 290/1000 | Loss: 0.00001664
Iteration 291/1000 | Loss: 0.00001664
Iteration 292/1000 | Loss: 0.00001664
Iteration 293/1000 | Loss: 0.00001664
Iteration 294/1000 | Loss: 0.00001664
Iteration 295/1000 | Loss: 0.00001664
Iteration 296/1000 | Loss: 0.00001664
Iteration 297/1000 | Loss: 0.00001664
Iteration 298/1000 | Loss: 0.00001664
Iteration 299/1000 | Loss: 0.00001664
Iteration 300/1000 | Loss: 0.00001664
Iteration 301/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.664346200414002e-05, 1.664346200414002e-05, 1.664346200414002e-05, 1.664346200414002e-05, 1.664346200414002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.664346200414002e-05

Optimization complete. Final v2v error: 3.4815785884857178 mm

Highest mean error: 3.968214988708496 mm for frame 157

Lowest mean error: 3.4028217792510986 mm for frame 69

Saving results

Total time: 50.77999997138977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387787
Iteration 2/25 | Loss: 0.00144723
Iteration 3/25 | Loss: 0.00132955
Iteration 4/25 | Loss: 0.00131916
Iteration 5/25 | Loss: 0.00131748
Iteration 6/25 | Loss: 0.00131748
Iteration 7/25 | Loss: 0.00131748
Iteration 8/25 | Loss: 0.00131748
Iteration 9/25 | Loss: 0.00131748
Iteration 10/25 | Loss: 0.00131748
Iteration 11/25 | Loss: 0.00131748
Iteration 12/25 | Loss: 0.00131748
Iteration 13/25 | Loss: 0.00131748
Iteration 14/25 | Loss: 0.00131748
Iteration 15/25 | Loss: 0.00131748
Iteration 16/25 | Loss: 0.00131748
Iteration 17/25 | Loss: 0.00131748
Iteration 18/25 | Loss: 0.00131748
Iteration 19/25 | Loss: 0.00131748
Iteration 20/25 | Loss: 0.00131748
Iteration 21/25 | Loss: 0.00131748
Iteration 22/25 | Loss: 0.00131748
Iteration 23/25 | Loss: 0.00131748
Iteration 24/25 | Loss: 0.00131748
Iteration 25/25 | Loss: 0.00131748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50405109
Iteration 2/25 | Loss: 0.00209644
Iteration 3/25 | Loss: 0.00209643
Iteration 4/25 | Loss: 0.00209643
Iteration 5/25 | Loss: 0.00209643
Iteration 6/25 | Loss: 0.00209643
Iteration 7/25 | Loss: 0.00209643
Iteration 8/25 | Loss: 0.00209643
Iteration 9/25 | Loss: 0.00209643
Iteration 10/25 | Loss: 0.00209643
Iteration 11/25 | Loss: 0.00209643
Iteration 12/25 | Loss: 0.00209643
Iteration 13/25 | Loss: 0.00209643
Iteration 14/25 | Loss: 0.00209643
Iteration 15/25 | Loss: 0.00209643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020964322611689568, 0.0020964322611689568, 0.0020964322611689568, 0.0020964322611689568, 0.0020964322611689568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020964322611689568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209643
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00001964
Iteration 4/1000 | Loss: 0.00001607
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001309
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001095
Iteration 16/1000 | Loss: 0.00001088
Iteration 17/1000 | Loss: 0.00001085
Iteration 18/1000 | Loss: 0.00001084
Iteration 19/1000 | Loss: 0.00001082
Iteration 20/1000 | Loss: 0.00001081
Iteration 21/1000 | Loss: 0.00001080
Iteration 22/1000 | Loss: 0.00001080
Iteration 23/1000 | Loss: 0.00001079
Iteration 24/1000 | Loss: 0.00001078
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001061
Iteration 27/1000 | Loss: 0.00001054
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001051
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001040
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001035
Iteration 44/1000 | Loss: 0.00001035
Iteration 45/1000 | Loss: 0.00001035
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001034
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001029
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001029
Iteration 56/1000 | Loss: 0.00001028
Iteration 57/1000 | Loss: 0.00001028
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001026
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001025
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001024
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001021
Iteration 75/1000 | Loss: 0.00001021
Iteration 76/1000 | Loss: 0.00001020
Iteration 77/1000 | Loss: 0.00001020
Iteration 78/1000 | Loss: 0.00001020
Iteration 79/1000 | Loss: 0.00001020
Iteration 80/1000 | Loss: 0.00001020
Iteration 81/1000 | Loss: 0.00001019
Iteration 82/1000 | Loss: 0.00001019
Iteration 83/1000 | Loss: 0.00001017
Iteration 84/1000 | Loss: 0.00001017
Iteration 85/1000 | Loss: 0.00001016
Iteration 86/1000 | Loss: 0.00001016
Iteration 87/1000 | Loss: 0.00001015
Iteration 88/1000 | Loss: 0.00001015
Iteration 89/1000 | Loss: 0.00001014
Iteration 90/1000 | Loss: 0.00001014
Iteration 91/1000 | Loss: 0.00001013
Iteration 92/1000 | Loss: 0.00001013
Iteration 93/1000 | Loss: 0.00001012
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001009
Iteration 98/1000 | Loss: 0.00001009
Iteration 99/1000 | Loss: 0.00001009
Iteration 100/1000 | Loss: 0.00001009
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001007
Iteration 104/1000 | Loss: 0.00001007
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001006
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001005
Iteration 109/1000 | Loss: 0.00001005
Iteration 110/1000 | Loss: 0.00001005
Iteration 111/1000 | Loss: 0.00001005
Iteration 112/1000 | Loss: 0.00001005
Iteration 113/1000 | Loss: 0.00001005
Iteration 114/1000 | Loss: 0.00001005
Iteration 115/1000 | Loss: 0.00001005
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001004
Iteration 120/1000 | Loss: 0.00001004
Iteration 121/1000 | Loss: 0.00001003
Iteration 122/1000 | Loss: 0.00001003
Iteration 123/1000 | Loss: 0.00001003
Iteration 124/1000 | Loss: 0.00001003
Iteration 125/1000 | Loss: 0.00001003
Iteration 126/1000 | Loss: 0.00001003
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001002
Iteration 129/1000 | Loss: 0.00001002
Iteration 130/1000 | Loss: 0.00001002
Iteration 131/1000 | Loss: 0.00001002
Iteration 132/1000 | Loss: 0.00001002
Iteration 133/1000 | Loss: 0.00001002
Iteration 134/1000 | Loss: 0.00001002
Iteration 135/1000 | Loss: 0.00001001
Iteration 136/1000 | Loss: 0.00001001
Iteration 137/1000 | Loss: 0.00001001
Iteration 138/1000 | Loss: 0.00001001
Iteration 139/1000 | Loss: 0.00001001
Iteration 140/1000 | Loss: 0.00001001
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Iteration 143/1000 | Loss: 0.00001000
Iteration 144/1000 | Loss: 0.00001000
Iteration 145/1000 | Loss: 0.00001000
Iteration 146/1000 | Loss: 0.00001000
Iteration 147/1000 | Loss: 0.00001000
Iteration 148/1000 | Loss: 0.00001000
Iteration 149/1000 | Loss: 0.00001000
Iteration 150/1000 | Loss: 0.00001000
Iteration 151/1000 | Loss: 0.00000999
Iteration 152/1000 | Loss: 0.00000999
Iteration 153/1000 | Loss: 0.00000999
Iteration 154/1000 | Loss: 0.00000999
Iteration 155/1000 | Loss: 0.00000999
Iteration 156/1000 | Loss: 0.00000999
Iteration 157/1000 | Loss: 0.00000999
Iteration 158/1000 | Loss: 0.00000999
Iteration 159/1000 | Loss: 0.00000999
Iteration 160/1000 | Loss: 0.00000999
Iteration 161/1000 | Loss: 0.00000999
Iteration 162/1000 | Loss: 0.00000999
Iteration 163/1000 | Loss: 0.00000999
Iteration 164/1000 | Loss: 0.00000999
Iteration 165/1000 | Loss: 0.00000999
Iteration 166/1000 | Loss: 0.00000999
Iteration 167/1000 | Loss: 0.00000999
Iteration 168/1000 | Loss: 0.00000999
Iteration 169/1000 | Loss: 0.00000998
Iteration 170/1000 | Loss: 0.00000998
Iteration 171/1000 | Loss: 0.00000998
Iteration 172/1000 | Loss: 0.00000998
Iteration 173/1000 | Loss: 0.00000998
Iteration 174/1000 | Loss: 0.00000998
Iteration 175/1000 | Loss: 0.00000998
Iteration 176/1000 | Loss: 0.00000998
Iteration 177/1000 | Loss: 0.00000998
Iteration 178/1000 | Loss: 0.00000998
Iteration 179/1000 | Loss: 0.00000998
Iteration 180/1000 | Loss: 0.00000998
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000997
Iteration 184/1000 | Loss: 0.00000997
Iteration 185/1000 | Loss: 0.00000997
Iteration 186/1000 | Loss: 0.00000997
Iteration 187/1000 | Loss: 0.00000997
Iteration 188/1000 | Loss: 0.00000997
Iteration 189/1000 | Loss: 0.00000997
Iteration 190/1000 | Loss: 0.00000997
Iteration 191/1000 | Loss: 0.00000997
Iteration 192/1000 | Loss: 0.00000997
Iteration 193/1000 | Loss: 0.00000997
Iteration 194/1000 | Loss: 0.00000997
Iteration 195/1000 | Loss: 0.00000997
Iteration 196/1000 | Loss: 0.00000997
Iteration 197/1000 | Loss: 0.00000997
Iteration 198/1000 | Loss: 0.00000997
Iteration 199/1000 | Loss: 0.00000997
Iteration 200/1000 | Loss: 0.00000997
Iteration 201/1000 | Loss: 0.00000997
Iteration 202/1000 | Loss: 0.00000997
Iteration 203/1000 | Loss: 0.00000997
Iteration 204/1000 | Loss: 0.00000997
Iteration 205/1000 | Loss: 0.00000997
Iteration 206/1000 | Loss: 0.00000997
Iteration 207/1000 | Loss: 0.00000997
Iteration 208/1000 | Loss: 0.00000997
Iteration 209/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.965718163584825e-06, 9.965718163584825e-06, 9.965718163584825e-06, 9.965718163584825e-06, 9.965718163584825e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.965718163584825e-06

Optimization complete. Final v2v error: 2.729236602783203 mm

Highest mean error: 2.945436954498291 mm for frame 148

Lowest mean error: 2.5874288082122803 mm for frame 134

Saving results

Total time: 52.17568564414978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963431
Iteration 2/25 | Loss: 0.00963431
Iteration 3/25 | Loss: 0.00327417
Iteration 4/25 | Loss: 0.00260595
Iteration 5/25 | Loss: 0.00246583
Iteration 6/25 | Loss: 0.00250555
Iteration 7/25 | Loss: 0.00224067
Iteration 8/25 | Loss: 0.00199618
Iteration 9/25 | Loss: 0.00182897
Iteration 10/25 | Loss: 0.00176579
Iteration 11/25 | Loss: 0.00173712
Iteration 12/25 | Loss: 0.00169050
Iteration 13/25 | Loss: 0.00166293
Iteration 14/25 | Loss: 0.00164029
Iteration 15/25 | Loss: 0.00161237
Iteration 16/25 | Loss: 0.00159398
Iteration 17/25 | Loss: 0.00160216
Iteration 18/25 | Loss: 0.00158467
Iteration 19/25 | Loss: 0.00156974
Iteration 20/25 | Loss: 0.00156942
Iteration 21/25 | Loss: 0.00155590
Iteration 22/25 | Loss: 0.00154484
Iteration 23/25 | Loss: 0.00155897
Iteration 24/25 | Loss: 0.00155518
Iteration 25/25 | Loss: 0.00155239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22238672
Iteration 2/25 | Loss: 0.00427513
Iteration 3/25 | Loss: 0.00345562
Iteration 4/25 | Loss: 0.00345562
Iteration 5/25 | Loss: 0.00345562
Iteration 6/25 | Loss: 0.00345562
Iteration 7/25 | Loss: 0.00345561
Iteration 8/25 | Loss: 0.00345561
Iteration 9/25 | Loss: 0.00345561
Iteration 10/25 | Loss: 0.00345561
Iteration 11/25 | Loss: 0.00345561
Iteration 12/25 | Loss: 0.00345561
Iteration 13/25 | Loss: 0.00345561
Iteration 14/25 | Loss: 0.00345561
Iteration 15/25 | Loss: 0.00345561
Iteration 16/25 | Loss: 0.00345561
Iteration 17/25 | Loss: 0.00345561
Iteration 18/25 | Loss: 0.00345561
Iteration 19/25 | Loss: 0.00345561
Iteration 20/25 | Loss: 0.00345561
Iteration 21/25 | Loss: 0.00345561
Iteration 22/25 | Loss: 0.00345561
Iteration 23/25 | Loss: 0.00345561
Iteration 24/25 | Loss: 0.00345561
Iteration 25/25 | Loss: 0.00345561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345561
Iteration 2/1000 | Loss: 0.00103326
Iteration 3/1000 | Loss: 0.00027913
Iteration 4/1000 | Loss: 0.00041160
Iteration 5/1000 | Loss: 0.00029784
Iteration 6/1000 | Loss: 0.00014375
Iteration 7/1000 | Loss: 0.00014185
Iteration 8/1000 | Loss: 0.00016467
Iteration 9/1000 | Loss: 0.00016323
Iteration 10/1000 | Loss: 0.00015467
Iteration 11/1000 | Loss: 0.00014346
Iteration 12/1000 | Loss: 0.00016188
Iteration 13/1000 | Loss: 0.00014833
Iteration 14/1000 | Loss: 0.00015253
Iteration 15/1000 | Loss: 0.00016346
Iteration 16/1000 | Loss: 0.00017603
Iteration 17/1000 | Loss: 0.00022925
Iteration 18/1000 | Loss: 0.00019948
Iteration 19/1000 | Loss: 0.00018467
Iteration 20/1000 | Loss: 0.00019336
Iteration 21/1000 | Loss: 0.00019110
Iteration 22/1000 | Loss: 0.00048057
Iteration 23/1000 | Loss: 0.00101840
Iteration 24/1000 | Loss: 0.00034873
Iteration 25/1000 | Loss: 0.00074013
Iteration 26/1000 | Loss: 0.00030115
Iteration 27/1000 | Loss: 0.00026151
Iteration 28/1000 | Loss: 0.00015855
Iteration 29/1000 | Loss: 0.00026582
Iteration 30/1000 | Loss: 0.00029716
Iteration 31/1000 | Loss: 0.00014172
Iteration 32/1000 | Loss: 0.00014030
Iteration 33/1000 | Loss: 0.00017927
Iteration 34/1000 | Loss: 0.00016893
Iteration 35/1000 | Loss: 0.00036750
Iteration 36/1000 | Loss: 0.00031097
Iteration 37/1000 | Loss: 0.00023042
Iteration 38/1000 | Loss: 0.00024624
Iteration 39/1000 | Loss: 0.00024498
Iteration 40/1000 | Loss: 0.00021191
Iteration 41/1000 | Loss: 0.00027046
Iteration 42/1000 | Loss: 0.00023909
Iteration 43/1000 | Loss: 0.00016614
Iteration 44/1000 | Loss: 0.00016465
Iteration 45/1000 | Loss: 0.00030731
Iteration 46/1000 | Loss: 0.00026667
Iteration 47/1000 | Loss: 0.00015973
Iteration 48/1000 | Loss: 0.00016370
Iteration 49/1000 | Loss: 0.00018912
Iteration 50/1000 | Loss: 0.00035288
Iteration 51/1000 | Loss: 0.00026844
Iteration 52/1000 | Loss: 0.00024593
Iteration 53/1000 | Loss: 0.00031472
Iteration 54/1000 | Loss: 0.00027485
Iteration 55/1000 | Loss: 0.00026258
Iteration 56/1000 | Loss: 0.00027188
Iteration 57/1000 | Loss: 0.00039392
Iteration 58/1000 | Loss: 0.00035265
Iteration 59/1000 | Loss: 0.00027796
Iteration 60/1000 | Loss: 0.00027487
Iteration 61/1000 | Loss: 0.00058114
Iteration 62/1000 | Loss: 0.00058847
Iteration 63/1000 | Loss: 0.00037180
Iteration 64/1000 | Loss: 0.00035188
Iteration 65/1000 | Loss: 0.00027807
Iteration 66/1000 | Loss: 0.00026086
Iteration 67/1000 | Loss: 0.00031004
Iteration 68/1000 | Loss: 0.00020146
Iteration 69/1000 | Loss: 0.00022575
Iteration 70/1000 | Loss: 0.00007986
Iteration 71/1000 | Loss: 0.00019525
Iteration 72/1000 | Loss: 0.00015374
Iteration 73/1000 | Loss: 0.00014431
Iteration 74/1000 | Loss: 0.00033447
Iteration 75/1000 | Loss: 0.00009426
Iteration 76/1000 | Loss: 0.00011539
Iteration 77/1000 | Loss: 0.00005807
Iteration 78/1000 | Loss: 0.00006236
Iteration 79/1000 | Loss: 0.00035936
Iteration 80/1000 | Loss: 0.00012792
Iteration 81/1000 | Loss: 0.00025484
Iteration 82/1000 | Loss: 0.00010257
Iteration 83/1000 | Loss: 0.00008064
Iteration 84/1000 | Loss: 0.00005189
Iteration 85/1000 | Loss: 0.00014658
Iteration 86/1000 | Loss: 0.00010265
Iteration 87/1000 | Loss: 0.00023600
Iteration 88/1000 | Loss: 0.00013858
Iteration 89/1000 | Loss: 0.00023299
Iteration 90/1000 | Loss: 0.00023163
Iteration 91/1000 | Loss: 0.00006254
Iteration 92/1000 | Loss: 0.00021914
Iteration 93/1000 | Loss: 0.00017594
Iteration 94/1000 | Loss: 0.00029366
Iteration 95/1000 | Loss: 0.00018959
Iteration 96/1000 | Loss: 0.00034564
Iteration 97/1000 | Loss: 0.00033151
Iteration 98/1000 | Loss: 0.00026162
Iteration 99/1000 | Loss: 0.00008690
Iteration 100/1000 | Loss: 0.00007608
Iteration 101/1000 | Loss: 0.00008109
Iteration 102/1000 | Loss: 0.00007419
Iteration 103/1000 | Loss: 0.00008676
Iteration 104/1000 | Loss: 0.00018121
Iteration 105/1000 | Loss: 0.00015740
Iteration 106/1000 | Loss: 0.00007066
Iteration 107/1000 | Loss: 0.00009395
Iteration 108/1000 | Loss: 0.00013258
Iteration 109/1000 | Loss: 0.00007096
Iteration 110/1000 | Loss: 0.00008078
Iteration 111/1000 | Loss: 0.00007882
Iteration 112/1000 | Loss: 0.00007753
Iteration 113/1000 | Loss: 0.00007892
Iteration 114/1000 | Loss: 0.00007826
Iteration 115/1000 | Loss: 0.00008647
Iteration 116/1000 | Loss: 0.00007588
Iteration 117/1000 | Loss: 0.00008146
Iteration 118/1000 | Loss: 0.00007545
Iteration 119/1000 | Loss: 0.00006986
Iteration 120/1000 | Loss: 0.00023101
Iteration 121/1000 | Loss: 0.00008719
Iteration 122/1000 | Loss: 0.00008370
Iteration 123/1000 | Loss: 0.00008577
Iteration 124/1000 | Loss: 0.00008846
Iteration 125/1000 | Loss: 0.00007725
Iteration 126/1000 | Loss: 0.00008739
Iteration 127/1000 | Loss: 0.00007517
Iteration 128/1000 | Loss: 0.00006358
Iteration 129/1000 | Loss: 0.00007249
Iteration 130/1000 | Loss: 0.00007606
Iteration 131/1000 | Loss: 0.00008508
Iteration 132/1000 | Loss: 0.00008006
Iteration 133/1000 | Loss: 0.00008079
Iteration 134/1000 | Loss: 0.00007113
Iteration 135/1000 | Loss: 0.00006941
Iteration 136/1000 | Loss: 0.00008613
Iteration 137/1000 | Loss: 0.00008287
Iteration 138/1000 | Loss: 0.00006558
Iteration 139/1000 | Loss: 0.00008717
Iteration 140/1000 | Loss: 0.00008256
Iteration 141/1000 | Loss: 0.00008422
Iteration 142/1000 | Loss: 0.00006907
Iteration 143/1000 | Loss: 0.00008072
Iteration 144/1000 | Loss: 0.00008585
Iteration 145/1000 | Loss: 0.00007701
Iteration 146/1000 | Loss: 0.00006859
Iteration 147/1000 | Loss: 0.00008665
Iteration 148/1000 | Loss: 0.00007650
Iteration 149/1000 | Loss: 0.00007924
Iteration 150/1000 | Loss: 0.00006994
Iteration 151/1000 | Loss: 0.00008873
Iteration 152/1000 | Loss: 0.00006465
Iteration 153/1000 | Loss: 0.00008525
Iteration 154/1000 | Loss: 0.00008452
Iteration 155/1000 | Loss: 0.00008544
Iteration 156/1000 | Loss: 0.00005697
Iteration 157/1000 | Loss: 0.00006253
Iteration 158/1000 | Loss: 0.00007241
Iteration 159/1000 | Loss: 0.00008351
Iteration 160/1000 | Loss: 0.00008274
Iteration 161/1000 | Loss: 0.00008328
Iteration 162/1000 | Loss: 0.00016912
Iteration 163/1000 | Loss: 0.00009936
Iteration 164/1000 | Loss: 0.00006552
Iteration 165/1000 | Loss: 0.00005349
Iteration 166/1000 | Loss: 0.00007258
Iteration 167/1000 | Loss: 0.00007065
Iteration 168/1000 | Loss: 0.00006738
Iteration 169/1000 | Loss: 0.00007443
Iteration 170/1000 | Loss: 0.00008628
Iteration 171/1000 | Loss: 0.00007530
Iteration 172/1000 | Loss: 0.00007663
Iteration 173/1000 | Loss: 0.00007755
Iteration 174/1000 | Loss: 0.00008375
Iteration 175/1000 | Loss: 0.00007680
Iteration 176/1000 | Loss: 0.00008255
Iteration 177/1000 | Loss: 0.00006586
Iteration 178/1000 | Loss: 0.00005527
Iteration 179/1000 | Loss: 0.00007108
Iteration 180/1000 | Loss: 0.00006914
Iteration 181/1000 | Loss: 0.00007499
Iteration 182/1000 | Loss: 0.00006515
Iteration 183/1000 | Loss: 0.00007051
Iteration 184/1000 | Loss: 0.00006546
Iteration 185/1000 | Loss: 0.00007432
Iteration 186/1000 | Loss: 0.00006764
Iteration 187/1000 | Loss: 0.00005988
Iteration 188/1000 | Loss: 0.00006287
Iteration 189/1000 | Loss: 0.00006426
Iteration 190/1000 | Loss: 0.00006994
Iteration 191/1000 | Loss: 0.00006450
Iteration 192/1000 | Loss: 0.00006843
Iteration 193/1000 | Loss: 0.00005802
Iteration 194/1000 | Loss: 0.00007084
Iteration 195/1000 | Loss: 0.00007596
Iteration 196/1000 | Loss: 0.00007513
Iteration 197/1000 | Loss: 0.00006082
Iteration 198/1000 | Loss: 0.00007221
Iteration 199/1000 | Loss: 0.00006916
Iteration 200/1000 | Loss: 0.00006959
Iteration 201/1000 | Loss: 0.00006981
Iteration 202/1000 | Loss: 0.00007298
Iteration 203/1000 | Loss: 0.00006903
Iteration 204/1000 | Loss: 0.00007873
Iteration 205/1000 | Loss: 0.00005321
Iteration 206/1000 | Loss: 0.00004316
Iteration 207/1000 | Loss: 0.00004102
Iteration 208/1000 | Loss: 0.00004019
Iteration 209/1000 | Loss: 0.00003935
Iteration 210/1000 | Loss: 0.00003884
Iteration 211/1000 | Loss: 0.00003852
Iteration 212/1000 | Loss: 0.00003846
Iteration 213/1000 | Loss: 0.00003840
Iteration 214/1000 | Loss: 0.00003840
Iteration 215/1000 | Loss: 0.00003840
Iteration 216/1000 | Loss: 0.00003840
Iteration 217/1000 | Loss: 0.00003839
Iteration 218/1000 | Loss: 0.00003839
Iteration 219/1000 | Loss: 0.00003838
Iteration 220/1000 | Loss: 0.00003838
Iteration 221/1000 | Loss: 0.00003837
Iteration 222/1000 | Loss: 0.00003837
Iteration 223/1000 | Loss: 0.00003835
Iteration 224/1000 | Loss: 0.00003832
Iteration 225/1000 | Loss: 0.00003831
Iteration 226/1000 | Loss: 0.00003827
Iteration 227/1000 | Loss: 0.00003818
Iteration 228/1000 | Loss: 0.00003817
Iteration 229/1000 | Loss: 0.00003817
Iteration 230/1000 | Loss: 0.00003815
Iteration 231/1000 | Loss: 0.00003814
Iteration 232/1000 | Loss: 0.00003813
Iteration 233/1000 | Loss: 0.00003813
Iteration 234/1000 | Loss: 0.00003813
Iteration 235/1000 | Loss: 0.00003812
Iteration 236/1000 | Loss: 0.00003812
Iteration 237/1000 | Loss: 0.00003811
Iteration 238/1000 | Loss: 0.00003811
Iteration 239/1000 | Loss: 0.00003809
Iteration 240/1000 | Loss: 0.00003803
Iteration 241/1000 | Loss: 0.00003803
Iteration 242/1000 | Loss: 0.00003792
Iteration 243/1000 | Loss: 0.00003789
Iteration 244/1000 | Loss: 0.00003789
Iteration 245/1000 | Loss: 0.00003788
Iteration 246/1000 | Loss: 0.00003788
Iteration 247/1000 | Loss: 0.00003788
Iteration 248/1000 | Loss: 0.00003788
Iteration 249/1000 | Loss: 0.00003788
Iteration 250/1000 | Loss: 0.00003787
Iteration 251/1000 | Loss: 0.00003787
Iteration 252/1000 | Loss: 0.00003786
Iteration 253/1000 | Loss: 0.00003786
Iteration 254/1000 | Loss: 0.00003785
Iteration 255/1000 | Loss: 0.00003784
Iteration 256/1000 | Loss: 0.00003784
Iteration 257/1000 | Loss: 0.00003783
Iteration 258/1000 | Loss: 0.00003783
Iteration 259/1000 | Loss: 0.00003783
Iteration 260/1000 | Loss: 0.00003782
Iteration 261/1000 | Loss: 0.00003782
Iteration 262/1000 | Loss: 0.00003781
Iteration 263/1000 | Loss: 0.00003781
Iteration 264/1000 | Loss: 0.00003780
Iteration 265/1000 | Loss: 0.00003780
Iteration 266/1000 | Loss: 0.00003780
Iteration 267/1000 | Loss: 0.00003780
Iteration 268/1000 | Loss: 0.00003780
Iteration 269/1000 | Loss: 0.00003780
Iteration 270/1000 | Loss: 0.00003780
Iteration 271/1000 | Loss: 0.00003780
Iteration 272/1000 | Loss: 0.00003780
Iteration 273/1000 | Loss: 0.00003779
Iteration 274/1000 | Loss: 0.00003779
Iteration 275/1000 | Loss: 0.00003779
Iteration 276/1000 | Loss: 0.00003779
Iteration 277/1000 | Loss: 0.00003779
Iteration 278/1000 | Loss: 0.00003779
Iteration 279/1000 | Loss: 0.00003779
Iteration 280/1000 | Loss: 0.00003779
Iteration 281/1000 | Loss: 0.00003778
Iteration 282/1000 | Loss: 0.00003778
Iteration 283/1000 | Loss: 0.00003778
Iteration 284/1000 | Loss: 0.00003778
Iteration 285/1000 | Loss: 0.00003777
Iteration 286/1000 | Loss: 0.00003777
Iteration 287/1000 | Loss: 0.00003777
Iteration 288/1000 | Loss: 0.00003777
Iteration 289/1000 | Loss: 0.00003777
Iteration 290/1000 | Loss: 0.00003777
Iteration 291/1000 | Loss: 0.00003777
Iteration 292/1000 | Loss: 0.00003777
Iteration 293/1000 | Loss: 0.00003776
Iteration 294/1000 | Loss: 0.00003776
Iteration 295/1000 | Loss: 0.00003776
Iteration 296/1000 | Loss: 0.00003776
Iteration 297/1000 | Loss: 0.00003776
Iteration 298/1000 | Loss: 0.00003776
Iteration 299/1000 | Loss: 0.00003775
Iteration 300/1000 | Loss: 0.00003775
Iteration 301/1000 | Loss: 0.00003775
Iteration 302/1000 | Loss: 0.00003774
Iteration 303/1000 | Loss: 0.00003774
Iteration 304/1000 | Loss: 0.00003774
Iteration 305/1000 | Loss: 0.00003774
Iteration 306/1000 | Loss: 0.00003774
Iteration 307/1000 | Loss: 0.00003774
Iteration 308/1000 | Loss: 0.00003774
Iteration 309/1000 | Loss: 0.00003773
Iteration 310/1000 | Loss: 0.00003773
Iteration 311/1000 | Loss: 0.00003773
Iteration 312/1000 | Loss: 0.00003773
Iteration 313/1000 | Loss: 0.00003773
Iteration 314/1000 | Loss: 0.00003773
Iteration 315/1000 | Loss: 0.00003773
Iteration 316/1000 | Loss: 0.00003773
Iteration 317/1000 | Loss: 0.00003773
Iteration 318/1000 | Loss: 0.00003773
Iteration 319/1000 | Loss: 0.00003773
Iteration 320/1000 | Loss: 0.00003773
Iteration 321/1000 | Loss: 0.00003773
Iteration 322/1000 | Loss: 0.00003773
Iteration 323/1000 | Loss: 0.00003773
Iteration 324/1000 | Loss: 0.00003772
Iteration 325/1000 | Loss: 0.00003772
Iteration 326/1000 | Loss: 0.00003772
Iteration 327/1000 | Loss: 0.00003772
Iteration 328/1000 | Loss: 0.00003772
Iteration 329/1000 | Loss: 0.00003772
Iteration 330/1000 | Loss: 0.00003772
Iteration 331/1000 | Loss: 0.00003772
Iteration 332/1000 | Loss: 0.00003771
Iteration 333/1000 | Loss: 0.00003771
Iteration 334/1000 | Loss: 0.00003771
Iteration 335/1000 | Loss: 0.00003771
Iteration 336/1000 | Loss: 0.00003771
Iteration 337/1000 | Loss: 0.00003771
Iteration 338/1000 | Loss: 0.00003771
Iteration 339/1000 | Loss: 0.00003771
Iteration 340/1000 | Loss: 0.00003771
Iteration 341/1000 | Loss: 0.00003771
Iteration 342/1000 | Loss: 0.00003771
Iteration 343/1000 | Loss: 0.00003770
Iteration 344/1000 | Loss: 0.00003770
Iteration 345/1000 | Loss: 0.00003770
Iteration 346/1000 | Loss: 0.00003770
Iteration 347/1000 | Loss: 0.00003770
Iteration 348/1000 | Loss: 0.00003770
Iteration 349/1000 | Loss: 0.00003770
Iteration 350/1000 | Loss: 0.00003770
Iteration 351/1000 | Loss: 0.00003769
Iteration 352/1000 | Loss: 0.00003769
Iteration 353/1000 | Loss: 0.00003769
Iteration 354/1000 | Loss: 0.00003769
Iteration 355/1000 | Loss: 0.00003769
Iteration 356/1000 | Loss: 0.00003769
Iteration 357/1000 | Loss: 0.00003769
Iteration 358/1000 | Loss: 0.00003769
Iteration 359/1000 | Loss: 0.00003769
Iteration 360/1000 | Loss: 0.00003769
Iteration 361/1000 | Loss: 0.00003769
Iteration 362/1000 | Loss: 0.00003769
Iteration 363/1000 | Loss: 0.00003769
Iteration 364/1000 | Loss: 0.00003769
Iteration 365/1000 | Loss: 0.00003769
Iteration 366/1000 | Loss: 0.00003769
Iteration 367/1000 | Loss: 0.00003769
Iteration 368/1000 | Loss: 0.00003769
Iteration 369/1000 | Loss: 0.00003769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 369. Stopping optimization.
Last 5 losses: [3.7685647839680314e-05, 3.7685647839680314e-05, 3.7685647839680314e-05, 3.7685647839680314e-05, 3.7685647839680314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7685647839680314e-05

Optimization complete. Final v2v error: 3.9867806434631348 mm

Highest mean error: 10.642252922058105 mm for frame 199

Lowest mean error: 3.260401725769043 mm for frame 97

Saving results

Total time: 407.86330366134644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00697470
Iteration 2/25 | Loss: 0.00143868
Iteration 3/25 | Loss: 0.00136423
Iteration 4/25 | Loss: 0.00133993
Iteration 5/25 | Loss: 0.00133464
Iteration 6/25 | Loss: 0.00133125
Iteration 7/25 | Loss: 0.00132985
Iteration 8/25 | Loss: 0.00132937
Iteration 9/25 | Loss: 0.00132892
Iteration 10/25 | Loss: 0.00132871
Iteration 11/25 | Loss: 0.00132854
Iteration 12/25 | Loss: 0.00132847
Iteration 13/25 | Loss: 0.00132847
Iteration 14/25 | Loss: 0.00132847
Iteration 15/25 | Loss: 0.00132846
Iteration 16/25 | Loss: 0.00132846
Iteration 17/25 | Loss: 0.00132846
Iteration 18/25 | Loss: 0.00132846
Iteration 19/25 | Loss: 0.00132846
Iteration 20/25 | Loss: 0.00132846
Iteration 21/25 | Loss: 0.00132846
Iteration 22/25 | Loss: 0.00132846
Iteration 23/25 | Loss: 0.00132846
Iteration 24/25 | Loss: 0.00132846
Iteration 25/25 | Loss: 0.00132845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45683718
Iteration 2/25 | Loss: 0.00218953
Iteration 3/25 | Loss: 0.00218953
Iteration 4/25 | Loss: 0.00218953
Iteration 5/25 | Loss: 0.00218953
Iteration 6/25 | Loss: 0.00218953
Iteration 7/25 | Loss: 0.00218953
Iteration 8/25 | Loss: 0.00218953
Iteration 9/25 | Loss: 0.00218953
Iteration 10/25 | Loss: 0.00218953
Iteration 11/25 | Loss: 0.00218953
Iteration 12/25 | Loss: 0.00218953
Iteration 13/25 | Loss: 0.00218953
Iteration 14/25 | Loss: 0.00218953
Iteration 15/25 | Loss: 0.00218953
Iteration 16/25 | Loss: 0.00218953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021895302925258875, 0.0021895302925258875, 0.0021895302925258875, 0.0021895302925258875, 0.0021895302925258875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021895302925258875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218953
Iteration 2/1000 | Loss: 0.00002203
Iteration 3/1000 | Loss: 0.00001848
Iteration 4/1000 | Loss: 0.00001688
Iteration 5/1000 | Loss: 0.00001612
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00023217
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001265
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001212
Iteration 74/1000 | Loss: 0.00001212
Iteration 75/1000 | Loss: 0.00001211
Iteration 76/1000 | Loss: 0.00001211
Iteration 77/1000 | Loss: 0.00001211
Iteration 78/1000 | Loss: 0.00001211
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001208
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001208
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.2056005289196037e-05, 1.2056005289196037e-05, 1.2056005289196037e-05, 1.2056005289196037e-05, 1.2056005289196037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2056005289196037e-05

Optimization complete. Final v2v error: 3.0290307998657227 mm

Highest mean error: 3.9539992809295654 mm for frame 140

Lowest mean error: 2.785921573638916 mm for frame 29

Saving results

Total time: 59.937664270401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860824
Iteration 2/25 | Loss: 0.00192064
Iteration 3/25 | Loss: 0.00146658
Iteration 4/25 | Loss: 0.00139651
Iteration 5/25 | Loss: 0.00139957
Iteration 6/25 | Loss: 0.00138761
Iteration 7/25 | Loss: 0.00136845
Iteration 8/25 | Loss: 0.00135583
Iteration 9/25 | Loss: 0.00135061
Iteration 10/25 | Loss: 0.00134901
Iteration 11/25 | Loss: 0.00134829
Iteration 12/25 | Loss: 0.00134765
Iteration 13/25 | Loss: 0.00134719
Iteration 14/25 | Loss: 0.00134705
Iteration 15/25 | Loss: 0.00134705
Iteration 16/25 | Loss: 0.00134704
Iteration 17/25 | Loss: 0.00134704
Iteration 18/25 | Loss: 0.00134704
Iteration 19/25 | Loss: 0.00134704
Iteration 20/25 | Loss: 0.00134704
Iteration 21/25 | Loss: 0.00134700
Iteration 22/25 | Loss: 0.00134700
Iteration 23/25 | Loss: 0.00134700
Iteration 24/25 | Loss: 0.00134700
Iteration 25/25 | Loss: 0.00134699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10215306
Iteration 2/25 | Loss: 0.00219711
Iteration 3/25 | Loss: 0.00219710
Iteration 4/25 | Loss: 0.00219710
Iteration 5/25 | Loss: 0.00219710
Iteration 6/25 | Loss: 0.00219710
Iteration 7/25 | Loss: 0.00219710
Iteration 8/25 | Loss: 0.00219710
Iteration 9/25 | Loss: 0.00219710
Iteration 10/25 | Loss: 0.00219710
Iteration 11/25 | Loss: 0.00219710
Iteration 12/25 | Loss: 0.00219710
Iteration 13/25 | Loss: 0.00219710
Iteration 14/25 | Loss: 0.00219710
Iteration 15/25 | Loss: 0.00219710
Iteration 16/25 | Loss: 0.00219710
Iteration 17/25 | Loss: 0.00219710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002197099616751075, 0.002197099616751075, 0.002197099616751075, 0.002197099616751075, 0.002197099616751075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002197099616751075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219710
Iteration 2/1000 | Loss: 0.00003049
Iteration 3/1000 | Loss: 0.00002410
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001993
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001916
Iteration 10/1000 | Loss: 0.00001908
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001832
Iteration 17/1000 | Loss: 0.00001831
Iteration 18/1000 | Loss: 0.00001820
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001809
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001807
Iteration 32/1000 | Loss: 0.00001807
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001804
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001803
Iteration 44/1000 | Loss: 0.00001803
Iteration 45/1000 | Loss: 0.00001802
Iteration 46/1000 | Loss: 0.00001802
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001800
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001793
Iteration 56/1000 | Loss: 0.00001792
Iteration 57/1000 | Loss: 0.00001792
Iteration 58/1000 | Loss: 0.00001791
Iteration 59/1000 | Loss: 0.00001791
Iteration 60/1000 | Loss: 0.00001790
Iteration 61/1000 | Loss: 0.00001790
Iteration 62/1000 | Loss: 0.00001790
Iteration 63/1000 | Loss: 0.00001789
Iteration 64/1000 | Loss: 0.00001789
Iteration 65/1000 | Loss: 0.00001789
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001788
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001787
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001787
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001787
Iteration 76/1000 | Loss: 0.00001786
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00001783
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001782
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001781
Iteration 111/1000 | Loss: 0.00001781
Iteration 112/1000 | Loss: 0.00001781
Iteration 113/1000 | Loss: 0.00001781
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001780
Iteration 116/1000 | Loss: 0.00001780
Iteration 117/1000 | Loss: 0.00001780
Iteration 118/1000 | Loss: 0.00001780
Iteration 119/1000 | Loss: 0.00001780
Iteration 120/1000 | Loss: 0.00001780
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001779
Iteration 126/1000 | Loss: 0.00001779
Iteration 127/1000 | Loss: 0.00001779
Iteration 128/1000 | Loss: 0.00001779
Iteration 129/1000 | Loss: 0.00001779
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001779
Iteration 132/1000 | Loss: 0.00001779
Iteration 133/1000 | Loss: 0.00001779
Iteration 134/1000 | Loss: 0.00001779
Iteration 135/1000 | Loss: 0.00001779
Iteration 136/1000 | Loss: 0.00001779
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001777
Iteration 143/1000 | Loss: 0.00001777
Iteration 144/1000 | Loss: 0.00001777
Iteration 145/1000 | Loss: 0.00001777
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001777
Iteration 148/1000 | Loss: 0.00001776
Iteration 149/1000 | Loss: 0.00001776
Iteration 150/1000 | Loss: 0.00001776
Iteration 151/1000 | Loss: 0.00001776
Iteration 152/1000 | Loss: 0.00001776
Iteration 153/1000 | Loss: 0.00001776
Iteration 154/1000 | Loss: 0.00001776
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001776
Iteration 162/1000 | Loss: 0.00001776
Iteration 163/1000 | Loss: 0.00001776
Iteration 164/1000 | Loss: 0.00001776
Iteration 165/1000 | Loss: 0.00001776
Iteration 166/1000 | Loss: 0.00001776
Iteration 167/1000 | Loss: 0.00001776
Iteration 168/1000 | Loss: 0.00001776
Iteration 169/1000 | Loss: 0.00001775
Iteration 170/1000 | Loss: 0.00001775
Iteration 171/1000 | Loss: 0.00001775
Iteration 172/1000 | Loss: 0.00001775
Iteration 173/1000 | Loss: 0.00001775
Iteration 174/1000 | Loss: 0.00001774
Iteration 175/1000 | Loss: 0.00001774
Iteration 176/1000 | Loss: 0.00001774
Iteration 177/1000 | Loss: 0.00001774
Iteration 178/1000 | Loss: 0.00001774
Iteration 179/1000 | Loss: 0.00001774
Iteration 180/1000 | Loss: 0.00001774
Iteration 181/1000 | Loss: 0.00001774
Iteration 182/1000 | Loss: 0.00001774
Iteration 183/1000 | Loss: 0.00001774
Iteration 184/1000 | Loss: 0.00001774
Iteration 185/1000 | Loss: 0.00001774
Iteration 186/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.7739595932653174e-05, 1.7739595932653174e-05, 1.7739595932653174e-05, 1.7739595932653174e-05, 1.7739595932653174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7739595932653174e-05

Optimization complete. Final v2v error: 3.5546875 mm

Highest mean error: 4.127913475036621 mm for frame 112

Lowest mean error: 3.094489812850952 mm for frame 6

Saving results

Total time: 67.4692029953003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889791
Iteration 2/25 | Loss: 0.00146503
Iteration 3/25 | Loss: 0.00137043
Iteration 4/25 | Loss: 0.00135346
Iteration 5/25 | Loss: 0.00134827
Iteration 6/25 | Loss: 0.00134681
Iteration 7/25 | Loss: 0.00134681
Iteration 8/25 | Loss: 0.00134681
Iteration 9/25 | Loss: 0.00134681
Iteration 10/25 | Loss: 0.00134681
Iteration 11/25 | Loss: 0.00134681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001346813514828682, 0.001346813514828682, 0.001346813514828682, 0.001346813514828682, 0.001346813514828682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001346813514828682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26125443
Iteration 2/25 | Loss: 0.00211426
Iteration 3/25 | Loss: 0.00211425
Iteration 4/25 | Loss: 0.00211425
Iteration 5/25 | Loss: 0.00211424
Iteration 6/25 | Loss: 0.00211424
Iteration 7/25 | Loss: 0.00211424
Iteration 8/25 | Loss: 0.00211424
Iteration 9/25 | Loss: 0.00211424
Iteration 10/25 | Loss: 0.00211424
Iteration 11/25 | Loss: 0.00211424
Iteration 12/25 | Loss: 0.00211424
Iteration 13/25 | Loss: 0.00211424
Iteration 14/25 | Loss: 0.00211424
Iteration 15/25 | Loss: 0.00211424
Iteration 16/25 | Loss: 0.00211424
Iteration 17/25 | Loss: 0.00211424
Iteration 18/25 | Loss: 0.00211424
Iteration 19/25 | Loss: 0.00211424
Iteration 20/25 | Loss: 0.00211424
Iteration 21/25 | Loss: 0.00211424
Iteration 22/25 | Loss: 0.00211424
Iteration 23/25 | Loss: 0.00211424
Iteration 24/25 | Loss: 0.00211424
Iteration 25/25 | Loss: 0.00211424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211424
Iteration 2/1000 | Loss: 0.00003337
Iteration 3/1000 | Loss: 0.00002448
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002040
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001829
Iteration 9/1000 | Loss: 0.00001787
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001731
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001661
Iteration 25/1000 | Loss: 0.00001661
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001643
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001640
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001639
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001637
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001634
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001633
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001624
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001619
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001613
Iteration 119/1000 | Loss: 0.00001613
Iteration 120/1000 | Loss: 0.00001612
Iteration 121/1000 | Loss: 0.00001612
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001612
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Iteration 126/1000 | Loss: 0.00001611
Iteration 127/1000 | Loss: 0.00001611
Iteration 128/1000 | Loss: 0.00001611
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001609
Iteration 140/1000 | Loss: 0.00001609
Iteration 141/1000 | Loss: 0.00001609
Iteration 142/1000 | Loss: 0.00001609
Iteration 143/1000 | Loss: 0.00001609
Iteration 144/1000 | Loss: 0.00001609
Iteration 145/1000 | Loss: 0.00001609
Iteration 146/1000 | Loss: 0.00001609
Iteration 147/1000 | Loss: 0.00001609
Iteration 148/1000 | Loss: 0.00001609
Iteration 149/1000 | Loss: 0.00001609
Iteration 150/1000 | Loss: 0.00001609
Iteration 151/1000 | Loss: 0.00001609
Iteration 152/1000 | Loss: 0.00001609
Iteration 153/1000 | Loss: 0.00001609
Iteration 154/1000 | Loss: 0.00001608
Iteration 155/1000 | Loss: 0.00001608
Iteration 156/1000 | Loss: 0.00001608
Iteration 157/1000 | Loss: 0.00001608
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00001608
Iteration 160/1000 | Loss: 0.00001608
Iteration 161/1000 | Loss: 0.00001608
Iteration 162/1000 | Loss: 0.00001608
Iteration 163/1000 | Loss: 0.00001608
Iteration 164/1000 | Loss: 0.00001608
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Iteration 170/1000 | Loss: 0.00001608
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001607
Iteration 173/1000 | Loss: 0.00001607
Iteration 174/1000 | Loss: 0.00001607
Iteration 175/1000 | Loss: 0.00001607
Iteration 176/1000 | Loss: 0.00001607
Iteration 177/1000 | Loss: 0.00001607
Iteration 178/1000 | Loss: 0.00001607
Iteration 179/1000 | Loss: 0.00001607
Iteration 180/1000 | Loss: 0.00001607
Iteration 181/1000 | Loss: 0.00001607
Iteration 182/1000 | Loss: 0.00001607
Iteration 183/1000 | Loss: 0.00001606
Iteration 184/1000 | Loss: 0.00001606
Iteration 185/1000 | Loss: 0.00001606
Iteration 186/1000 | Loss: 0.00001606
Iteration 187/1000 | Loss: 0.00001606
Iteration 188/1000 | Loss: 0.00001606
Iteration 189/1000 | Loss: 0.00001606
Iteration 190/1000 | Loss: 0.00001606
Iteration 191/1000 | Loss: 0.00001606
Iteration 192/1000 | Loss: 0.00001606
Iteration 193/1000 | Loss: 0.00001606
Iteration 194/1000 | Loss: 0.00001606
Iteration 195/1000 | Loss: 0.00001606
Iteration 196/1000 | Loss: 0.00001606
Iteration 197/1000 | Loss: 0.00001605
Iteration 198/1000 | Loss: 0.00001605
Iteration 199/1000 | Loss: 0.00001605
Iteration 200/1000 | Loss: 0.00001605
Iteration 201/1000 | Loss: 0.00001605
Iteration 202/1000 | Loss: 0.00001605
Iteration 203/1000 | Loss: 0.00001605
Iteration 204/1000 | Loss: 0.00001605
Iteration 205/1000 | Loss: 0.00001605
Iteration 206/1000 | Loss: 0.00001605
Iteration 207/1000 | Loss: 0.00001605
Iteration 208/1000 | Loss: 0.00001605
Iteration 209/1000 | Loss: 0.00001605
Iteration 210/1000 | Loss: 0.00001605
Iteration 211/1000 | Loss: 0.00001605
Iteration 212/1000 | Loss: 0.00001605
Iteration 213/1000 | Loss: 0.00001605
Iteration 214/1000 | Loss: 0.00001605
Iteration 215/1000 | Loss: 0.00001605
Iteration 216/1000 | Loss: 0.00001605
Iteration 217/1000 | Loss: 0.00001605
Iteration 218/1000 | Loss: 0.00001605
Iteration 219/1000 | Loss: 0.00001605
Iteration 220/1000 | Loss: 0.00001605
Iteration 221/1000 | Loss: 0.00001604
Iteration 222/1000 | Loss: 0.00001604
Iteration 223/1000 | Loss: 0.00001604
Iteration 224/1000 | Loss: 0.00001604
Iteration 225/1000 | Loss: 0.00001604
Iteration 226/1000 | Loss: 0.00001604
Iteration 227/1000 | Loss: 0.00001604
Iteration 228/1000 | Loss: 0.00001604
Iteration 229/1000 | Loss: 0.00001604
Iteration 230/1000 | Loss: 0.00001604
Iteration 231/1000 | Loss: 0.00001604
Iteration 232/1000 | Loss: 0.00001604
Iteration 233/1000 | Loss: 0.00001604
Iteration 234/1000 | Loss: 0.00001604
Iteration 235/1000 | Loss: 0.00001604
Iteration 236/1000 | Loss: 0.00001604
Iteration 237/1000 | Loss: 0.00001604
Iteration 238/1000 | Loss: 0.00001604
Iteration 239/1000 | Loss: 0.00001604
Iteration 240/1000 | Loss: 0.00001604
Iteration 241/1000 | Loss: 0.00001604
Iteration 242/1000 | Loss: 0.00001604
Iteration 243/1000 | Loss: 0.00001604
Iteration 244/1000 | Loss: 0.00001604
Iteration 245/1000 | Loss: 0.00001604
Iteration 246/1000 | Loss: 0.00001604
Iteration 247/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.604395583854057e-05, 1.604395583854057e-05, 1.604395583854057e-05, 1.604395583854057e-05, 1.604395583854057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.604395583854057e-05

Optimization complete. Final v2v error: 3.4070780277252197 mm

Highest mean error: 5.124694347381592 mm for frame 76

Lowest mean error: 2.938264846801758 mm for frame 102

Saving results

Total time: 47.055044651031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948574
Iteration 2/25 | Loss: 0.00180718
Iteration 3/25 | Loss: 0.00148505
Iteration 4/25 | Loss: 0.00146572
Iteration 5/25 | Loss: 0.00145959
Iteration 6/25 | Loss: 0.00145909
Iteration 7/25 | Loss: 0.00145909
Iteration 8/25 | Loss: 0.00145909
Iteration 9/25 | Loss: 0.00145909
Iteration 10/25 | Loss: 0.00145909
Iteration 11/25 | Loss: 0.00145909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014590892242267728, 0.0014590892242267728, 0.0014590892242267728, 0.0014590892242267728, 0.0014590892242267728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014590892242267728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74144500
Iteration 2/25 | Loss: 0.00206607
Iteration 3/25 | Loss: 0.00206606
Iteration 4/25 | Loss: 0.00206606
Iteration 5/25 | Loss: 0.00206606
Iteration 6/25 | Loss: 0.00206606
Iteration 7/25 | Loss: 0.00206606
Iteration 8/25 | Loss: 0.00206606
Iteration 9/25 | Loss: 0.00206606
Iteration 10/25 | Loss: 0.00206606
Iteration 11/25 | Loss: 0.00206606
Iteration 12/25 | Loss: 0.00206606
Iteration 13/25 | Loss: 0.00206606
Iteration 14/25 | Loss: 0.00206606
Iteration 15/25 | Loss: 0.00206606
Iteration 16/25 | Loss: 0.00206606
Iteration 17/25 | Loss: 0.00206606
Iteration 18/25 | Loss: 0.00206606
Iteration 19/25 | Loss: 0.00206606
Iteration 20/25 | Loss: 0.00206606
Iteration 21/25 | Loss: 0.00206606
Iteration 22/25 | Loss: 0.00206606
Iteration 23/25 | Loss: 0.00206606
Iteration 24/25 | Loss: 0.00206606
Iteration 25/25 | Loss: 0.00206606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206606
Iteration 2/1000 | Loss: 0.00005929
Iteration 3/1000 | Loss: 0.00004284
Iteration 4/1000 | Loss: 0.00003631
Iteration 5/1000 | Loss: 0.00003447
Iteration 6/1000 | Loss: 0.00003275
Iteration 7/1000 | Loss: 0.00003184
Iteration 8/1000 | Loss: 0.00003083
Iteration 9/1000 | Loss: 0.00003030
Iteration 10/1000 | Loss: 0.00002991
Iteration 11/1000 | Loss: 0.00002950
Iteration 12/1000 | Loss: 0.00002918
Iteration 13/1000 | Loss: 0.00002886
Iteration 14/1000 | Loss: 0.00002861
Iteration 15/1000 | Loss: 0.00002834
Iteration 16/1000 | Loss: 0.00002805
Iteration 17/1000 | Loss: 0.00002781
Iteration 18/1000 | Loss: 0.00002762
Iteration 19/1000 | Loss: 0.00002745
Iteration 20/1000 | Loss: 0.00002739
Iteration 21/1000 | Loss: 0.00002725
Iteration 22/1000 | Loss: 0.00002720
Iteration 23/1000 | Loss: 0.00002718
Iteration 24/1000 | Loss: 0.00002714
Iteration 25/1000 | Loss: 0.00002711
Iteration 26/1000 | Loss: 0.00002709
Iteration 27/1000 | Loss: 0.00002709
Iteration 28/1000 | Loss: 0.00002706
Iteration 29/1000 | Loss: 0.00002706
Iteration 30/1000 | Loss: 0.00002706
Iteration 31/1000 | Loss: 0.00002706
Iteration 32/1000 | Loss: 0.00002705
Iteration 33/1000 | Loss: 0.00002705
Iteration 34/1000 | Loss: 0.00002703
Iteration 35/1000 | Loss: 0.00002703
Iteration 36/1000 | Loss: 0.00002703
Iteration 37/1000 | Loss: 0.00002703
Iteration 38/1000 | Loss: 0.00002702
Iteration 39/1000 | Loss: 0.00002702
Iteration 40/1000 | Loss: 0.00002701
Iteration 41/1000 | Loss: 0.00002701
Iteration 42/1000 | Loss: 0.00002701
Iteration 43/1000 | Loss: 0.00002701
Iteration 44/1000 | Loss: 0.00002700
Iteration 45/1000 | Loss: 0.00002700
Iteration 46/1000 | Loss: 0.00002700
Iteration 47/1000 | Loss: 0.00002700
Iteration 48/1000 | Loss: 0.00002700
Iteration 49/1000 | Loss: 0.00002700
Iteration 50/1000 | Loss: 0.00002700
Iteration 51/1000 | Loss: 0.00002700
Iteration 52/1000 | Loss: 0.00002700
Iteration 53/1000 | Loss: 0.00002700
Iteration 54/1000 | Loss: 0.00002700
Iteration 55/1000 | Loss: 0.00002700
Iteration 56/1000 | Loss: 0.00002699
Iteration 57/1000 | Loss: 0.00002699
Iteration 58/1000 | Loss: 0.00002699
Iteration 59/1000 | Loss: 0.00002699
Iteration 60/1000 | Loss: 0.00002699
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002699
Iteration 64/1000 | Loss: 0.00002699
Iteration 65/1000 | Loss: 0.00002699
Iteration 66/1000 | Loss: 0.00002699
Iteration 67/1000 | Loss: 0.00002699
Iteration 68/1000 | Loss: 0.00002699
Iteration 69/1000 | Loss: 0.00002698
Iteration 70/1000 | Loss: 0.00002698
Iteration 71/1000 | Loss: 0.00002698
Iteration 72/1000 | Loss: 0.00002698
Iteration 73/1000 | Loss: 0.00002698
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002697
Iteration 76/1000 | Loss: 0.00002697
Iteration 77/1000 | Loss: 0.00002697
Iteration 78/1000 | Loss: 0.00002696
Iteration 79/1000 | Loss: 0.00002696
Iteration 80/1000 | Loss: 0.00002696
Iteration 81/1000 | Loss: 0.00002696
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002695
Iteration 84/1000 | Loss: 0.00002695
Iteration 85/1000 | Loss: 0.00002695
Iteration 86/1000 | Loss: 0.00002695
Iteration 87/1000 | Loss: 0.00002695
Iteration 88/1000 | Loss: 0.00002695
Iteration 89/1000 | Loss: 0.00002694
Iteration 90/1000 | Loss: 0.00002694
Iteration 91/1000 | Loss: 0.00002694
Iteration 92/1000 | Loss: 0.00002694
Iteration 93/1000 | Loss: 0.00002694
Iteration 94/1000 | Loss: 0.00002693
Iteration 95/1000 | Loss: 0.00002693
Iteration 96/1000 | Loss: 0.00002693
Iteration 97/1000 | Loss: 0.00002692
Iteration 98/1000 | Loss: 0.00002692
Iteration 99/1000 | Loss: 0.00002692
Iteration 100/1000 | Loss: 0.00002692
Iteration 101/1000 | Loss: 0.00002692
Iteration 102/1000 | Loss: 0.00002692
Iteration 103/1000 | Loss: 0.00002692
Iteration 104/1000 | Loss: 0.00002692
Iteration 105/1000 | Loss: 0.00002691
Iteration 106/1000 | Loss: 0.00002691
Iteration 107/1000 | Loss: 0.00002691
Iteration 108/1000 | Loss: 0.00002691
Iteration 109/1000 | Loss: 0.00002691
Iteration 110/1000 | Loss: 0.00002690
Iteration 111/1000 | Loss: 0.00002690
Iteration 112/1000 | Loss: 0.00002690
Iteration 113/1000 | Loss: 0.00002690
Iteration 114/1000 | Loss: 0.00002690
Iteration 115/1000 | Loss: 0.00002690
Iteration 116/1000 | Loss: 0.00002690
Iteration 117/1000 | Loss: 0.00002690
Iteration 118/1000 | Loss: 0.00002690
Iteration 119/1000 | Loss: 0.00002690
Iteration 120/1000 | Loss: 0.00002689
Iteration 121/1000 | Loss: 0.00002689
Iteration 122/1000 | Loss: 0.00002689
Iteration 123/1000 | Loss: 0.00002689
Iteration 124/1000 | Loss: 0.00002689
Iteration 125/1000 | Loss: 0.00002688
Iteration 126/1000 | Loss: 0.00002688
Iteration 127/1000 | Loss: 0.00002688
Iteration 128/1000 | Loss: 0.00002688
Iteration 129/1000 | Loss: 0.00002688
Iteration 130/1000 | Loss: 0.00002688
Iteration 131/1000 | Loss: 0.00002688
Iteration 132/1000 | Loss: 0.00002688
Iteration 133/1000 | Loss: 0.00002688
Iteration 134/1000 | Loss: 0.00002688
Iteration 135/1000 | Loss: 0.00002688
Iteration 136/1000 | Loss: 0.00002687
Iteration 137/1000 | Loss: 0.00002687
Iteration 138/1000 | Loss: 0.00002687
Iteration 139/1000 | Loss: 0.00002687
Iteration 140/1000 | Loss: 0.00002687
Iteration 141/1000 | Loss: 0.00002687
Iteration 142/1000 | Loss: 0.00002687
Iteration 143/1000 | Loss: 0.00002687
Iteration 144/1000 | Loss: 0.00002687
Iteration 145/1000 | Loss: 0.00002687
Iteration 146/1000 | Loss: 0.00002686
Iteration 147/1000 | Loss: 0.00002686
Iteration 148/1000 | Loss: 0.00002686
Iteration 149/1000 | Loss: 0.00002686
Iteration 150/1000 | Loss: 0.00002686
Iteration 151/1000 | Loss: 0.00002686
Iteration 152/1000 | Loss: 0.00002686
Iteration 153/1000 | Loss: 0.00002686
Iteration 154/1000 | Loss: 0.00002686
Iteration 155/1000 | Loss: 0.00002686
Iteration 156/1000 | Loss: 0.00002686
Iteration 157/1000 | Loss: 0.00002686
Iteration 158/1000 | Loss: 0.00002685
Iteration 159/1000 | Loss: 0.00002685
Iteration 160/1000 | Loss: 0.00002685
Iteration 161/1000 | Loss: 0.00002685
Iteration 162/1000 | Loss: 0.00002685
Iteration 163/1000 | Loss: 0.00002685
Iteration 164/1000 | Loss: 0.00002685
Iteration 165/1000 | Loss: 0.00002685
Iteration 166/1000 | Loss: 0.00002685
Iteration 167/1000 | Loss: 0.00002685
Iteration 168/1000 | Loss: 0.00002685
Iteration 169/1000 | Loss: 0.00002685
Iteration 170/1000 | Loss: 0.00002685
Iteration 171/1000 | Loss: 0.00002685
Iteration 172/1000 | Loss: 0.00002685
Iteration 173/1000 | Loss: 0.00002685
Iteration 174/1000 | Loss: 0.00002685
Iteration 175/1000 | Loss: 0.00002684
Iteration 176/1000 | Loss: 0.00002684
Iteration 177/1000 | Loss: 0.00002684
Iteration 178/1000 | Loss: 0.00002684
Iteration 179/1000 | Loss: 0.00002684
Iteration 180/1000 | Loss: 0.00002684
Iteration 181/1000 | Loss: 0.00002684
Iteration 182/1000 | Loss: 0.00002684
Iteration 183/1000 | Loss: 0.00002684
Iteration 184/1000 | Loss: 0.00002684
Iteration 185/1000 | Loss: 0.00002684
Iteration 186/1000 | Loss: 0.00002684
Iteration 187/1000 | Loss: 0.00002684
Iteration 188/1000 | Loss: 0.00002684
Iteration 189/1000 | Loss: 0.00002684
Iteration 190/1000 | Loss: 0.00002684
Iteration 191/1000 | Loss: 0.00002684
Iteration 192/1000 | Loss: 0.00002684
Iteration 193/1000 | Loss: 0.00002684
Iteration 194/1000 | Loss: 0.00002684
Iteration 195/1000 | Loss: 0.00002684
Iteration 196/1000 | Loss: 0.00002684
Iteration 197/1000 | Loss: 0.00002684
Iteration 198/1000 | Loss: 0.00002684
Iteration 199/1000 | Loss: 0.00002684
Iteration 200/1000 | Loss: 0.00002684
Iteration 201/1000 | Loss: 0.00002684
Iteration 202/1000 | Loss: 0.00002684
Iteration 203/1000 | Loss: 0.00002684
Iteration 204/1000 | Loss: 0.00002684
Iteration 205/1000 | Loss: 0.00002684
Iteration 206/1000 | Loss: 0.00002684
Iteration 207/1000 | Loss: 0.00002684
Iteration 208/1000 | Loss: 0.00002684
Iteration 209/1000 | Loss: 0.00002684
Iteration 210/1000 | Loss: 0.00002684
Iteration 211/1000 | Loss: 0.00002684
Iteration 212/1000 | Loss: 0.00002684
Iteration 213/1000 | Loss: 0.00002684
Iteration 214/1000 | Loss: 0.00002684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.683917227841448e-05, 2.683917227841448e-05, 2.683917227841448e-05, 2.683917227841448e-05, 2.683917227841448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.683917227841448e-05

Optimization complete. Final v2v error: 4.427618026733398 mm

Highest mean error: 4.962043762207031 mm for frame 190

Lowest mean error: 3.892624616622925 mm for frame 50

Saving results

Total time: 53.017756938934326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886457
Iteration 2/25 | Loss: 0.00167541
Iteration 3/25 | Loss: 0.00152295
Iteration 4/25 | Loss: 0.00149255
Iteration 5/25 | Loss: 0.00147157
Iteration 6/25 | Loss: 0.00144598
Iteration 7/25 | Loss: 0.00142815
Iteration 8/25 | Loss: 0.00142505
Iteration 9/25 | Loss: 0.00142238
Iteration 10/25 | Loss: 0.00143222
Iteration 11/25 | Loss: 0.00146412
Iteration 12/25 | Loss: 0.00144820
Iteration 13/25 | Loss: 0.00143099
Iteration 14/25 | Loss: 0.00143976
Iteration 15/25 | Loss: 0.00142598
Iteration 16/25 | Loss: 0.00141466
Iteration 17/25 | Loss: 0.00140917
Iteration 18/25 | Loss: 0.00140851
Iteration 19/25 | Loss: 0.00140520
Iteration 20/25 | Loss: 0.00140423
Iteration 21/25 | Loss: 0.00140043
Iteration 22/25 | Loss: 0.00139949
Iteration 23/25 | Loss: 0.00139983
Iteration 24/25 | Loss: 0.00139939
Iteration 25/25 | Loss: 0.00139897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24876368
Iteration 2/25 | Loss: 0.00239140
Iteration 3/25 | Loss: 0.00239139
Iteration 4/25 | Loss: 0.00239139
Iteration 5/25 | Loss: 0.00239139
Iteration 6/25 | Loss: 0.00239139
Iteration 7/25 | Loss: 0.00239139
Iteration 8/25 | Loss: 0.00239139
Iteration 9/25 | Loss: 0.00239139
Iteration 10/25 | Loss: 0.00239139
Iteration 11/25 | Loss: 0.00239139
Iteration 12/25 | Loss: 0.00239139
Iteration 13/25 | Loss: 0.00239139
Iteration 14/25 | Loss: 0.00239139
Iteration 15/25 | Loss: 0.00239139
Iteration 16/25 | Loss: 0.00239139
Iteration 17/25 | Loss: 0.00239139
Iteration 18/25 | Loss: 0.00239139
Iteration 19/25 | Loss: 0.00239139
Iteration 20/25 | Loss: 0.00239139
Iteration 21/25 | Loss: 0.00239139
Iteration 22/25 | Loss: 0.00239139
Iteration 23/25 | Loss: 0.00239139
Iteration 24/25 | Loss: 0.00239139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002391386078670621, 0.002391386078670621, 0.002391386078670621, 0.002391386078670621, 0.002391386078670621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002391386078670621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239139
Iteration 2/1000 | Loss: 0.00006363
Iteration 3/1000 | Loss: 0.00034948
Iteration 4/1000 | Loss: 0.00015229
Iteration 5/1000 | Loss: 0.00007254
Iteration 6/1000 | Loss: 0.00006869
Iteration 7/1000 | Loss: 0.00006869
Iteration 8/1000 | Loss: 0.00004567
Iteration 9/1000 | Loss: 0.00004490
Iteration 10/1000 | Loss: 0.00003918
Iteration 11/1000 | Loss: 0.00003377
Iteration 12/1000 | Loss: 0.00025414
Iteration 13/1000 | Loss: 0.00020081
Iteration 14/1000 | Loss: 0.00005172
Iteration 15/1000 | Loss: 0.00011850
Iteration 16/1000 | Loss: 0.00003148
Iteration 17/1000 | Loss: 0.00004172
Iteration 18/1000 | Loss: 0.00003975
Iteration 19/1000 | Loss: 0.00013768
Iteration 20/1000 | Loss: 0.00045024
Iteration 21/1000 | Loss: 0.00028191
Iteration 22/1000 | Loss: 0.00009509
Iteration 23/1000 | Loss: 0.00009162
Iteration 24/1000 | Loss: 0.00003908
Iteration 25/1000 | Loss: 0.00010430
Iteration 26/1000 | Loss: 0.00009405
Iteration 27/1000 | Loss: 0.00007813
Iteration 28/1000 | Loss: 0.00003673
Iteration 29/1000 | Loss: 0.00003703
Iteration 30/1000 | Loss: 0.00004156
Iteration 31/1000 | Loss: 0.00004356
Iteration 32/1000 | Loss: 0.00003978
Iteration 33/1000 | Loss: 0.00003321
Iteration 34/1000 | Loss: 0.00002847
Iteration 35/1000 | Loss: 0.00026588
Iteration 36/1000 | Loss: 0.00015916
Iteration 37/1000 | Loss: 0.00005465
Iteration 38/1000 | Loss: 0.00005018
Iteration 39/1000 | Loss: 0.00003322
Iteration 40/1000 | Loss: 0.00003067
Iteration 41/1000 | Loss: 0.00003223
Iteration 42/1000 | Loss: 0.00003513
Iteration 43/1000 | Loss: 0.00003604
Iteration 44/1000 | Loss: 0.00003348
Iteration 45/1000 | Loss: 0.00003572
Iteration 46/1000 | Loss: 0.00003745
Iteration 47/1000 | Loss: 0.00003591
Iteration 48/1000 | Loss: 0.00029066
Iteration 49/1000 | Loss: 0.00025011
Iteration 50/1000 | Loss: 0.00024645
Iteration 51/1000 | Loss: 0.00013794
Iteration 52/1000 | Loss: 0.00003123
Iteration 53/1000 | Loss: 0.00002722
Iteration 54/1000 | Loss: 0.00002480
Iteration 55/1000 | Loss: 0.00002397
Iteration 56/1000 | Loss: 0.00025867
Iteration 57/1000 | Loss: 0.00028465
Iteration 58/1000 | Loss: 0.00007407
Iteration 59/1000 | Loss: 0.00012216
Iteration 60/1000 | Loss: 0.00019929
Iteration 61/1000 | Loss: 0.00013287
Iteration 62/1000 | Loss: 0.00008487
Iteration 63/1000 | Loss: 0.00014187
Iteration 64/1000 | Loss: 0.00016556
Iteration 65/1000 | Loss: 0.00014322
Iteration 66/1000 | Loss: 0.00016034
Iteration 67/1000 | Loss: 0.00020441
Iteration 68/1000 | Loss: 0.00025399
Iteration 69/1000 | Loss: 0.00017606
Iteration 70/1000 | Loss: 0.00019552
Iteration 71/1000 | Loss: 0.00013465
Iteration 72/1000 | Loss: 0.00013710
Iteration 73/1000 | Loss: 0.00003017
Iteration 74/1000 | Loss: 0.00021253
Iteration 75/1000 | Loss: 0.00022932
Iteration 76/1000 | Loss: 0.00015843
Iteration 77/1000 | Loss: 0.00002622
Iteration 78/1000 | Loss: 0.00002495
Iteration 79/1000 | Loss: 0.00008505
Iteration 80/1000 | Loss: 0.00008368
Iteration 81/1000 | Loss: 0.00002814
Iteration 82/1000 | Loss: 0.00021940
Iteration 83/1000 | Loss: 0.00018767
Iteration 84/1000 | Loss: 0.00003011
Iteration 85/1000 | Loss: 0.00014274
Iteration 86/1000 | Loss: 0.00009461
Iteration 87/1000 | Loss: 0.00013526
Iteration 88/1000 | Loss: 0.00013159
Iteration 89/1000 | Loss: 0.00013015
Iteration 90/1000 | Loss: 0.00028021
Iteration 91/1000 | Loss: 0.00021297
Iteration 92/1000 | Loss: 0.00020092
Iteration 93/1000 | Loss: 0.00006031
Iteration 94/1000 | Loss: 0.00014056
Iteration 95/1000 | Loss: 0.00009772
Iteration 96/1000 | Loss: 0.00002631
Iteration 97/1000 | Loss: 0.00002501
Iteration 98/1000 | Loss: 0.00002430
Iteration 99/1000 | Loss: 0.00008850
Iteration 100/1000 | Loss: 0.00009744
Iteration 101/1000 | Loss: 0.00008140
Iteration 102/1000 | Loss: 0.00010828
Iteration 103/1000 | Loss: 0.00002386
Iteration 104/1000 | Loss: 0.00002358
Iteration 105/1000 | Loss: 0.00002329
Iteration 106/1000 | Loss: 0.00010544
Iteration 107/1000 | Loss: 0.00010414
Iteration 108/1000 | Loss: 0.00008706
Iteration 109/1000 | Loss: 0.00011380
Iteration 110/1000 | Loss: 0.00008499
Iteration 111/1000 | Loss: 0.00010535
Iteration 112/1000 | Loss: 0.00005293
Iteration 113/1000 | Loss: 0.00002338
Iteration 114/1000 | Loss: 0.00010300
Iteration 115/1000 | Loss: 0.00013612
Iteration 116/1000 | Loss: 0.00005791
Iteration 117/1000 | Loss: 0.00008264
Iteration 118/1000 | Loss: 0.00024048
Iteration 119/1000 | Loss: 0.00048808
Iteration 120/1000 | Loss: 0.00017902
Iteration 121/1000 | Loss: 0.00013174
Iteration 122/1000 | Loss: 0.00028354
Iteration 123/1000 | Loss: 0.00019239
Iteration 124/1000 | Loss: 0.00010293
Iteration 125/1000 | Loss: 0.00008894
Iteration 126/1000 | Loss: 0.00002431
Iteration 127/1000 | Loss: 0.00009656
Iteration 128/1000 | Loss: 0.00009520
Iteration 129/1000 | Loss: 0.00024824
Iteration 130/1000 | Loss: 0.00019123
Iteration 131/1000 | Loss: 0.00016293
Iteration 132/1000 | Loss: 0.00009425
Iteration 133/1000 | Loss: 0.00016518
Iteration 134/1000 | Loss: 0.00015364
Iteration 135/1000 | Loss: 0.00021559
Iteration 136/1000 | Loss: 0.00009927
Iteration 137/1000 | Loss: 0.00011121
Iteration 138/1000 | Loss: 0.00013992
Iteration 139/1000 | Loss: 0.00010886
Iteration 140/1000 | Loss: 0.00006323
Iteration 141/1000 | Loss: 0.00003082
Iteration 142/1000 | Loss: 0.00004286
Iteration 143/1000 | Loss: 0.00005595
Iteration 144/1000 | Loss: 0.00002682
Iteration 145/1000 | Loss: 0.00002530
Iteration 146/1000 | Loss: 0.00013817
Iteration 147/1000 | Loss: 0.00011981
Iteration 148/1000 | Loss: 0.00007680
Iteration 149/1000 | Loss: 0.00009856
Iteration 150/1000 | Loss: 0.00005447
Iteration 151/1000 | Loss: 0.00002601
Iteration 152/1000 | Loss: 0.00009950
Iteration 153/1000 | Loss: 0.00007214
Iteration 154/1000 | Loss: 0.00015691
Iteration 155/1000 | Loss: 0.00028569
Iteration 156/1000 | Loss: 0.00028405
Iteration 157/1000 | Loss: 0.00003647
Iteration 158/1000 | Loss: 0.00003020
Iteration 159/1000 | Loss: 0.00016283
Iteration 160/1000 | Loss: 0.00010056
Iteration 161/1000 | Loss: 0.00015107
Iteration 162/1000 | Loss: 0.00010679
Iteration 163/1000 | Loss: 0.00014647
Iteration 164/1000 | Loss: 0.00012332
Iteration 165/1000 | Loss: 0.00016480
Iteration 166/1000 | Loss: 0.00018252
Iteration 167/1000 | Loss: 0.00013767
Iteration 168/1000 | Loss: 0.00002608
Iteration 169/1000 | Loss: 0.00025817
Iteration 170/1000 | Loss: 0.00017301
Iteration 171/1000 | Loss: 0.00024070
Iteration 172/1000 | Loss: 0.00004826
Iteration 173/1000 | Loss: 0.00013506
Iteration 174/1000 | Loss: 0.00002530
Iteration 175/1000 | Loss: 0.00002392
Iteration 176/1000 | Loss: 0.00002270
Iteration 177/1000 | Loss: 0.00002184
Iteration 178/1000 | Loss: 0.00002140
Iteration 179/1000 | Loss: 0.00002113
Iteration 180/1000 | Loss: 0.00002107
Iteration 181/1000 | Loss: 0.00002093
Iteration 182/1000 | Loss: 0.00002092
Iteration 183/1000 | Loss: 0.00002091
Iteration 184/1000 | Loss: 0.00002089
Iteration 185/1000 | Loss: 0.00002083
Iteration 186/1000 | Loss: 0.00002070
Iteration 187/1000 | Loss: 0.00002068
Iteration 188/1000 | Loss: 0.00002066
Iteration 189/1000 | Loss: 0.00002065
Iteration 190/1000 | Loss: 0.00002064
Iteration 191/1000 | Loss: 0.00002062
Iteration 192/1000 | Loss: 0.00002062
Iteration 193/1000 | Loss: 0.00002061
Iteration 194/1000 | Loss: 0.00002060
Iteration 195/1000 | Loss: 0.00002060
Iteration 196/1000 | Loss: 0.00002059
Iteration 197/1000 | Loss: 0.00002058
Iteration 198/1000 | Loss: 0.00002058
Iteration 199/1000 | Loss: 0.00002057
Iteration 200/1000 | Loss: 0.00002057
Iteration 201/1000 | Loss: 0.00002057
Iteration 202/1000 | Loss: 0.00002056
Iteration 203/1000 | Loss: 0.00002056
Iteration 204/1000 | Loss: 0.00002056
Iteration 205/1000 | Loss: 0.00002056
Iteration 206/1000 | Loss: 0.00002056
Iteration 207/1000 | Loss: 0.00002056
Iteration 208/1000 | Loss: 0.00002056
Iteration 209/1000 | Loss: 0.00002056
Iteration 210/1000 | Loss: 0.00002056
Iteration 211/1000 | Loss: 0.00002056
Iteration 212/1000 | Loss: 0.00002055
Iteration 213/1000 | Loss: 0.00002055
Iteration 214/1000 | Loss: 0.00002055
Iteration 215/1000 | Loss: 0.00002054
Iteration 216/1000 | Loss: 0.00002054
Iteration 217/1000 | Loss: 0.00002053
Iteration 218/1000 | Loss: 0.00002053
Iteration 219/1000 | Loss: 0.00002053
Iteration 220/1000 | Loss: 0.00002052
Iteration 221/1000 | Loss: 0.00002052
Iteration 222/1000 | Loss: 0.00002052
Iteration 223/1000 | Loss: 0.00002051
Iteration 224/1000 | Loss: 0.00002051
Iteration 225/1000 | Loss: 0.00002051
Iteration 226/1000 | Loss: 0.00002050
Iteration 227/1000 | Loss: 0.00002050
Iteration 228/1000 | Loss: 0.00002049
Iteration 229/1000 | Loss: 0.00002049
Iteration 230/1000 | Loss: 0.00002048
Iteration 231/1000 | Loss: 0.00002047
Iteration 232/1000 | Loss: 0.00002047
Iteration 233/1000 | Loss: 0.00002047
Iteration 234/1000 | Loss: 0.00002046
Iteration 235/1000 | Loss: 0.00002046
Iteration 236/1000 | Loss: 0.00002046
Iteration 237/1000 | Loss: 0.00002046
Iteration 238/1000 | Loss: 0.00002045
Iteration 239/1000 | Loss: 0.00002045
Iteration 240/1000 | Loss: 0.00002045
Iteration 241/1000 | Loss: 0.00002045
Iteration 242/1000 | Loss: 0.00002045
Iteration 243/1000 | Loss: 0.00002045
Iteration 244/1000 | Loss: 0.00002044
Iteration 245/1000 | Loss: 0.00002043
Iteration 246/1000 | Loss: 0.00002043
Iteration 247/1000 | Loss: 0.00002042
Iteration 248/1000 | Loss: 0.00002042
Iteration 249/1000 | Loss: 0.00002042
Iteration 250/1000 | Loss: 0.00002042
Iteration 251/1000 | Loss: 0.00002041
Iteration 252/1000 | Loss: 0.00002041
Iteration 253/1000 | Loss: 0.00002041
Iteration 254/1000 | Loss: 0.00002040
Iteration 255/1000 | Loss: 0.00002040
Iteration 256/1000 | Loss: 0.00002040
Iteration 257/1000 | Loss: 0.00002040
Iteration 258/1000 | Loss: 0.00002039
Iteration 259/1000 | Loss: 0.00002038
Iteration 260/1000 | Loss: 0.00002038
Iteration 261/1000 | Loss: 0.00002037
Iteration 262/1000 | Loss: 0.00002037
Iteration 263/1000 | Loss: 0.00002037
Iteration 264/1000 | Loss: 0.00002037
Iteration 265/1000 | Loss: 0.00002037
Iteration 266/1000 | Loss: 0.00002037
Iteration 267/1000 | Loss: 0.00002037
Iteration 268/1000 | Loss: 0.00002037
Iteration 269/1000 | Loss: 0.00002037
Iteration 270/1000 | Loss: 0.00002037
Iteration 271/1000 | Loss: 0.00002037
Iteration 272/1000 | Loss: 0.00002036
Iteration 273/1000 | Loss: 0.00002036
Iteration 274/1000 | Loss: 0.00002036
Iteration 275/1000 | Loss: 0.00002036
Iteration 276/1000 | Loss: 0.00002036
Iteration 277/1000 | Loss: 0.00002035
Iteration 278/1000 | Loss: 0.00002035
Iteration 279/1000 | Loss: 0.00002035
Iteration 280/1000 | Loss: 0.00002035
Iteration 281/1000 | Loss: 0.00002034
Iteration 282/1000 | Loss: 0.00002034
Iteration 283/1000 | Loss: 0.00002034
Iteration 284/1000 | Loss: 0.00002034
Iteration 285/1000 | Loss: 0.00002033
Iteration 286/1000 | Loss: 0.00002033
Iteration 287/1000 | Loss: 0.00002033
Iteration 288/1000 | Loss: 0.00002033
Iteration 289/1000 | Loss: 0.00002033
Iteration 290/1000 | Loss: 0.00002033
Iteration 291/1000 | Loss: 0.00002033
Iteration 292/1000 | Loss: 0.00002033
Iteration 293/1000 | Loss: 0.00002033
Iteration 294/1000 | Loss: 0.00002033
Iteration 295/1000 | Loss: 0.00002033
Iteration 296/1000 | Loss: 0.00002033
Iteration 297/1000 | Loss: 0.00002033
Iteration 298/1000 | Loss: 0.00002033
Iteration 299/1000 | Loss: 0.00002033
Iteration 300/1000 | Loss: 0.00002032
Iteration 301/1000 | Loss: 0.00002032
Iteration 302/1000 | Loss: 0.00002032
Iteration 303/1000 | Loss: 0.00002032
Iteration 304/1000 | Loss: 0.00002032
Iteration 305/1000 | Loss: 0.00002032
Iteration 306/1000 | Loss: 0.00002032
Iteration 307/1000 | Loss: 0.00002032
Iteration 308/1000 | Loss: 0.00002031
Iteration 309/1000 | Loss: 0.00002031
Iteration 310/1000 | Loss: 0.00002031
Iteration 311/1000 | Loss: 0.00002031
Iteration 312/1000 | Loss: 0.00002031
Iteration 313/1000 | Loss: 0.00002031
Iteration 314/1000 | Loss: 0.00002031
Iteration 315/1000 | Loss: 0.00002031
Iteration 316/1000 | Loss: 0.00002031
Iteration 317/1000 | Loss: 0.00002031
Iteration 318/1000 | Loss: 0.00002030
Iteration 319/1000 | Loss: 0.00002030
Iteration 320/1000 | Loss: 0.00002030
Iteration 321/1000 | Loss: 0.00002030
Iteration 322/1000 | Loss: 0.00002030
Iteration 323/1000 | Loss: 0.00002030
Iteration 324/1000 | Loss: 0.00002030
Iteration 325/1000 | Loss: 0.00002030
Iteration 326/1000 | Loss: 0.00002030
Iteration 327/1000 | Loss: 0.00002030
Iteration 328/1000 | Loss: 0.00002030
Iteration 329/1000 | Loss: 0.00002030
Iteration 330/1000 | Loss: 0.00002030
Iteration 331/1000 | Loss: 0.00002030
Iteration 332/1000 | Loss: 0.00002030
Iteration 333/1000 | Loss: 0.00002030
Iteration 334/1000 | Loss: 0.00002030
Iteration 335/1000 | Loss: 0.00002030
Iteration 336/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [2.030077848758083e-05, 2.030077848758083e-05, 2.030077848758083e-05, 2.030077848758083e-05, 2.030077848758083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.030077848758083e-05

Optimization complete. Final v2v error: 3.7619779109954834 mm

Highest mean error: 5.443292617797852 mm for frame 212

Lowest mean error: 3.1145267486572266 mm for frame 22

Saving results

Total time: 355.9935784339905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418382
Iteration 2/25 | Loss: 0.00141025
Iteration 3/25 | Loss: 0.00133091
Iteration 4/25 | Loss: 0.00132458
Iteration 5/25 | Loss: 0.00132255
Iteration 6/25 | Loss: 0.00132236
Iteration 7/25 | Loss: 0.00132236
Iteration 8/25 | Loss: 0.00132236
Iteration 9/25 | Loss: 0.00132236
Iteration 10/25 | Loss: 0.00132236
Iteration 11/25 | Loss: 0.00132236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013223644345998764, 0.0013223644345998764, 0.0013223644345998764, 0.0013223644345998764, 0.0013223644345998764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013223644345998764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.62512159
Iteration 2/25 | Loss: 0.00204042
Iteration 3/25 | Loss: 0.00204040
Iteration 4/25 | Loss: 0.00204040
Iteration 5/25 | Loss: 0.00204040
Iteration 6/25 | Loss: 0.00204040
Iteration 7/25 | Loss: 0.00204040
Iteration 8/25 | Loss: 0.00204040
Iteration 9/25 | Loss: 0.00204040
Iteration 10/25 | Loss: 0.00204040
Iteration 11/25 | Loss: 0.00204040
Iteration 12/25 | Loss: 0.00204040
Iteration 13/25 | Loss: 0.00204040
Iteration 14/25 | Loss: 0.00204040
Iteration 15/25 | Loss: 0.00204040
Iteration 16/25 | Loss: 0.00204040
Iteration 17/25 | Loss: 0.00204040
Iteration 18/25 | Loss: 0.00204040
Iteration 19/25 | Loss: 0.00204040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002040401566773653, 0.002040401566773653, 0.002040401566773653, 0.002040401566773653, 0.002040401566773653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002040401566773653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204040
Iteration 2/1000 | Loss: 0.00002740
Iteration 3/1000 | Loss: 0.00002050
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001603
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001472
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001347
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001262
Iteration 18/1000 | Loss: 0.00001260
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001236
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001235
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001228
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001205
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001197
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.1972831998718902e-05, 1.1972831998718902e-05, 1.1972831998718902e-05, 1.1972831998718902e-05, 1.1972831998718902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1972831998718902e-05

Optimization complete. Final v2v error: 3.001271963119507 mm

Highest mean error: 3.6105759143829346 mm for frame 85

Lowest mean error: 2.7127926349639893 mm for frame 43

Saving results

Total time: 39.16536831855774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399914
Iteration 2/25 | Loss: 0.00141008
Iteration 3/25 | Loss: 0.00130907
Iteration 4/25 | Loss: 0.00130322
Iteration 5/25 | Loss: 0.00130146
Iteration 6/25 | Loss: 0.00130130
Iteration 7/25 | Loss: 0.00130130
Iteration 8/25 | Loss: 0.00130130
Iteration 9/25 | Loss: 0.00130130
Iteration 10/25 | Loss: 0.00130130
Iteration 11/25 | Loss: 0.00130130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013012964045628905, 0.0013012964045628905, 0.0013012964045628905, 0.0013012964045628905, 0.0013012964045628905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013012964045628905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24915791
Iteration 2/25 | Loss: 0.00231151
Iteration 3/25 | Loss: 0.00231151
Iteration 4/25 | Loss: 0.00231151
Iteration 5/25 | Loss: 0.00231151
Iteration 6/25 | Loss: 0.00231151
Iteration 7/25 | Loss: 0.00231150
Iteration 8/25 | Loss: 0.00231150
Iteration 9/25 | Loss: 0.00231150
Iteration 10/25 | Loss: 0.00231150
Iteration 11/25 | Loss: 0.00231150
Iteration 12/25 | Loss: 0.00231150
Iteration 13/25 | Loss: 0.00231150
Iteration 14/25 | Loss: 0.00231150
Iteration 15/25 | Loss: 0.00231150
Iteration 16/25 | Loss: 0.00231150
Iteration 17/25 | Loss: 0.00231150
Iteration 18/25 | Loss: 0.00231150
Iteration 19/25 | Loss: 0.00231150
Iteration 20/25 | Loss: 0.00231150
Iteration 21/25 | Loss: 0.00231150
Iteration 22/25 | Loss: 0.00231150
Iteration 23/25 | Loss: 0.00231150
Iteration 24/25 | Loss: 0.00231150
Iteration 25/25 | Loss: 0.00231150

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231150
Iteration 2/1000 | Loss: 0.00002442
Iteration 3/1000 | Loss: 0.00001533
Iteration 4/1000 | Loss: 0.00001367
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001212
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001093
Iteration 10/1000 | Loss: 0.00001062
Iteration 11/1000 | Loss: 0.00001057
Iteration 12/1000 | Loss: 0.00001055
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001025
Iteration 15/1000 | Loss: 0.00001025
Iteration 16/1000 | Loss: 0.00001024
Iteration 17/1000 | Loss: 0.00001020
Iteration 18/1000 | Loss: 0.00001020
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001009
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000996
Iteration 27/1000 | Loss: 0.00000994
Iteration 28/1000 | Loss: 0.00000993
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000990
Iteration 33/1000 | Loss: 0.00000989
Iteration 34/1000 | Loss: 0.00000986
Iteration 35/1000 | Loss: 0.00000986
Iteration 36/1000 | Loss: 0.00000985
Iteration 37/1000 | Loss: 0.00000984
Iteration 38/1000 | Loss: 0.00000984
Iteration 39/1000 | Loss: 0.00000983
Iteration 40/1000 | Loss: 0.00000983
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000973
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000973
Iteration 60/1000 | Loss: 0.00000973
Iteration 61/1000 | Loss: 0.00000972
Iteration 62/1000 | Loss: 0.00000972
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000972
Iteration 65/1000 | Loss: 0.00000972
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000971
Iteration 69/1000 | Loss: 0.00000971
Iteration 70/1000 | Loss: 0.00000971
Iteration 71/1000 | Loss: 0.00000971
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000969
Iteration 75/1000 | Loss: 0.00000969
Iteration 76/1000 | Loss: 0.00000968
Iteration 77/1000 | Loss: 0.00000968
Iteration 78/1000 | Loss: 0.00000967
Iteration 79/1000 | Loss: 0.00000966
Iteration 80/1000 | Loss: 0.00000966
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000965
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000964
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000962
Iteration 95/1000 | Loss: 0.00000962
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000960
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000958
Iteration 111/1000 | Loss: 0.00000958
Iteration 112/1000 | Loss: 0.00000958
Iteration 113/1000 | Loss: 0.00000958
Iteration 114/1000 | Loss: 0.00000957
Iteration 115/1000 | Loss: 0.00000957
Iteration 116/1000 | Loss: 0.00000957
Iteration 117/1000 | Loss: 0.00000957
Iteration 118/1000 | Loss: 0.00000957
Iteration 119/1000 | Loss: 0.00000957
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000956
Iteration 123/1000 | Loss: 0.00000956
Iteration 124/1000 | Loss: 0.00000956
Iteration 125/1000 | Loss: 0.00000956
Iteration 126/1000 | Loss: 0.00000956
Iteration 127/1000 | Loss: 0.00000956
Iteration 128/1000 | Loss: 0.00000955
Iteration 129/1000 | Loss: 0.00000955
Iteration 130/1000 | Loss: 0.00000955
Iteration 131/1000 | Loss: 0.00000955
Iteration 132/1000 | Loss: 0.00000955
Iteration 133/1000 | Loss: 0.00000955
Iteration 134/1000 | Loss: 0.00000955
Iteration 135/1000 | Loss: 0.00000955
Iteration 136/1000 | Loss: 0.00000954
Iteration 137/1000 | Loss: 0.00000954
Iteration 138/1000 | Loss: 0.00000954
Iteration 139/1000 | Loss: 0.00000954
Iteration 140/1000 | Loss: 0.00000954
Iteration 141/1000 | Loss: 0.00000954
Iteration 142/1000 | Loss: 0.00000954
Iteration 143/1000 | Loss: 0.00000954
Iteration 144/1000 | Loss: 0.00000954
Iteration 145/1000 | Loss: 0.00000954
Iteration 146/1000 | Loss: 0.00000954
Iteration 147/1000 | Loss: 0.00000954
Iteration 148/1000 | Loss: 0.00000954
Iteration 149/1000 | Loss: 0.00000953
Iteration 150/1000 | Loss: 0.00000953
Iteration 151/1000 | Loss: 0.00000953
Iteration 152/1000 | Loss: 0.00000953
Iteration 153/1000 | Loss: 0.00000953
Iteration 154/1000 | Loss: 0.00000953
Iteration 155/1000 | Loss: 0.00000953
Iteration 156/1000 | Loss: 0.00000953
Iteration 157/1000 | Loss: 0.00000953
Iteration 158/1000 | Loss: 0.00000952
Iteration 159/1000 | Loss: 0.00000952
Iteration 160/1000 | Loss: 0.00000952
Iteration 161/1000 | Loss: 0.00000952
Iteration 162/1000 | Loss: 0.00000951
Iteration 163/1000 | Loss: 0.00000951
Iteration 164/1000 | Loss: 0.00000951
Iteration 165/1000 | Loss: 0.00000951
Iteration 166/1000 | Loss: 0.00000951
Iteration 167/1000 | Loss: 0.00000951
Iteration 168/1000 | Loss: 0.00000950
Iteration 169/1000 | Loss: 0.00000950
Iteration 170/1000 | Loss: 0.00000950
Iteration 171/1000 | Loss: 0.00000950
Iteration 172/1000 | Loss: 0.00000949
Iteration 173/1000 | Loss: 0.00000949
Iteration 174/1000 | Loss: 0.00000948
Iteration 175/1000 | Loss: 0.00000948
Iteration 176/1000 | Loss: 0.00000948
Iteration 177/1000 | Loss: 0.00000948
Iteration 178/1000 | Loss: 0.00000948
Iteration 179/1000 | Loss: 0.00000947
Iteration 180/1000 | Loss: 0.00000947
Iteration 181/1000 | Loss: 0.00000946
Iteration 182/1000 | Loss: 0.00000946
Iteration 183/1000 | Loss: 0.00000946
Iteration 184/1000 | Loss: 0.00000945
Iteration 185/1000 | Loss: 0.00000945
Iteration 186/1000 | Loss: 0.00000945
Iteration 187/1000 | Loss: 0.00000945
Iteration 188/1000 | Loss: 0.00000945
Iteration 189/1000 | Loss: 0.00000945
Iteration 190/1000 | Loss: 0.00000945
Iteration 191/1000 | Loss: 0.00000945
Iteration 192/1000 | Loss: 0.00000945
Iteration 193/1000 | Loss: 0.00000944
Iteration 194/1000 | Loss: 0.00000944
Iteration 195/1000 | Loss: 0.00000944
Iteration 196/1000 | Loss: 0.00000944
Iteration 197/1000 | Loss: 0.00000944
Iteration 198/1000 | Loss: 0.00000944
Iteration 199/1000 | Loss: 0.00000944
Iteration 200/1000 | Loss: 0.00000944
Iteration 201/1000 | Loss: 0.00000944
Iteration 202/1000 | Loss: 0.00000944
Iteration 203/1000 | Loss: 0.00000944
Iteration 204/1000 | Loss: 0.00000943
Iteration 205/1000 | Loss: 0.00000943
Iteration 206/1000 | Loss: 0.00000943
Iteration 207/1000 | Loss: 0.00000943
Iteration 208/1000 | Loss: 0.00000943
Iteration 209/1000 | Loss: 0.00000943
Iteration 210/1000 | Loss: 0.00000943
Iteration 211/1000 | Loss: 0.00000943
Iteration 212/1000 | Loss: 0.00000943
Iteration 213/1000 | Loss: 0.00000943
Iteration 214/1000 | Loss: 0.00000943
Iteration 215/1000 | Loss: 0.00000942
Iteration 216/1000 | Loss: 0.00000942
Iteration 217/1000 | Loss: 0.00000942
Iteration 218/1000 | Loss: 0.00000942
Iteration 219/1000 | Loss: 0.00000942
Iteration 220/1000 | Loss: 0.00000942
Iteration 221/1000 | Loss: 0.00000942
Iteration 222/1000 | Loss: 0.00000942
Iteration 223/1000 | Loss: 0.00000942
Iteration 224/1000 | Loss: 0.00000941
Iteration 225/1000 | Loss: 0.00000941
Iteration 226/1000 | Loss: 0.00000941
Iteration 227/1000 | Loss: 0.00000941
Iteration 228/1000 | Loss: 0.00000941
Iteration 229/1000 | Loss: 0.00000941
Iteration 230/1000 | Loss: 0.00000941
Iteration 231/1000 | Loss: 0.00000940
Iteration 232/1000 | Loss: 0.00000940
Iteration 233/1000 | Loss: 0.00000940
Iteration 234/1000 | Loss: 0.00000940
Iteration 235/1000 | Loss: 0.00000940
Iteration 236/1000 | Loss: 0.00000940
Iteration 237/1000 | Loss: 0.00000940
Iteration 238/1000 | Loss: 0.00000940
Iteration 239/1000 | Loss: 0.00000940
Iteration 240/1000 | Loss: 0.00000940
Iteration 241/1000 | Loss: 0.00000940
Iteration 242/1000 | Loss: 0.00000940
Iteration 243/1000 | Loss: 0.00000940
Iteration 244/1000 | Loss: 0.00000940
Iteration 245/1000 | Loss: 0.00000940
Iteration 246/1000 | Loss: 0.00000940
Iteration 247/1000 | Loss: 0.00000940
Iteration 248/1000 | Loss: 0.00000940
Iteration 249/1000 | Loss: 0.00000940
Iteration 250/1000 | Loss: 0.00000940
Iteration 251/1000 | Loss: 0.00000940
Iteration 252/1000 | Loss: 0.00000940
Iteration 253/1000 | Loss: 0.00000940
Iteration 254/1000 | Loss: 0.00000939
Iteration 255/1000 | Loss: 0.00000939
Iteration 256/1000 | Loss: 0.00000939
Iteration 257/1000 | Loss: 0.00000939
Iteration 258/1000 | Loss: 0.00000939
Iteration 259/1000 | Loss: 0.00000939
Iteration 260/1000 | Loss: 0.00000939
Iteration 261/1000 | Loss: 0.00000939
Iteration 262/1000 | Loss: 0.00000939
Iteration 263/1000 | Loss: 0.00000939
Iteration 264/1000 | Loss: 0.00000939
Iteration 265/1000 | Loss: 0.00000939
Iteration 266/1000 | Loss: 0.00000939
Iteration 267/1000 | Loss: 0.00000939
Iteration 268/1000 | Loss: 0.00000939
Iteration 269/1000 | Loss: 0.00000939
Iteration 270/1000 | Loss: 0.00000939
Iteration 271/1000 | Loss: 0.00000939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [9.39354231377365e-06, 9.39354231377365e-06, 9.39354231377365e-06, 9.39354231377365e-06, 9.39354231377365e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.39354231377365e-06

Optimization complete. Final v2v error: 2.639425754547119 mm

Highest mean error: 3.553187608718872 mm for frame 71

Lowest mean error: 2.4679646492004395 mm for frame 156

Saving results

Total time: 47.38382434844971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00683868
Iteration 2/25 | Loss: 0.00163618
Iteration 3/25 | Loss: 0.00141283
Iteration 4/25 | Loss: 0.00140934
Iteration 5/25 | Loss: 0.00140006
Iteration 6/25 | Loss: 0.00138650
Iteration 7/25 | Loss: 0.00137211
Iteration 8/25 | Loss: 0.00137050
Iteration 9/25 | Loss: 0.00137024
Iteration 10/25 | Loss: 0.00137019
Iteration 11/25 | Loss: 0.00137019
Iteration 12/25 | Loss: 0.00137019
Iteration 13/25 | Loss: 0.00137019
Iteration 14/25 | Loss: 0.00137018
Iteration 15/25 | Loss: 0.00137018
Iteration 16/25 | Loss: 0.00137018
Iteration 17/25 | Loss: 0.00137018
Iteration 18/25 | Loss: 0.00137018
Iteration 19/25 | Loss: 0.00137018
Iteration 20/25 | Loss: 0.00137018
Iteration 21/25 | Loss: 0.00137018
Iteration 22/25 | Loss: 0.00137018
Iteration 23/25 | Loss: 0.00137018
Iteration 24/25 | Loss: 0.00137018
Iteration 25/25 | Loss: 0.00137018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16526151
Iteration 2/25 | Loss: 0.00297517
Iteration 3/25 | Loss: 0.00297515
Iteration 4/25 | Loss: 0.00297515
Iteration 5/25 | Loss: 0.00297515
Iteration 6/25 | Loss: 0.00297515
Iteration 7/25 | Loss: 0.00297515
Iteration 8/25 | Loss: 0.00297515
Iteration 9/25 | Loss: 0.00297515
Iteration 10/25 | Loss: 0.00297515
Iteration 11/25 | Loss: 0.00297515
Iteration 12/25 | Loss: 0.00297515
Iteration 13/25 | Loss: 0.00297515
Iteration 14/25 | Loss: 0.00297515
Iteration 15/25 | Loss: 0.00297515
Iteration 16/25 | Loss: 0.00297515
Iteration 17/25 | Loss: 0.00297515
Iteration 18/25 | Loss: 0.00297515
Iteration 19/25 | Loss: 0.00297515
Iteration 20/25 | Loss: 0.00297515
Iteration 21/25 | Loss: 0.00297515
Iteration 22/25 | Loss: 0.00297515
Iteration 23/25 | Loss: 0.00297515
Iteration 24/25 | Loss: 0.00297515
Iteration 25/25 | Loss: 0.00297515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00297515
Iteration 2/1000 | Loss: 0.00011108
Iteration 3/1000 | Loss: 0.00009774
Iteration 4/1000 | Loss: 0.00006048
Iteration 5/1000 | Loss: 0.00003882
Iteration 6/1000 | Loss: 0.00002907
Iteration 7/1000 | Loss: 0.00002376
Iteration 8/1000 | Loss: 0.00002154
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00029713
Iteration 12/1000 | Loss: 0.00002408
Iteration 13/1000 | Loss: 0.00002131
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001815
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001647
Iteration 18/1000 | Loss: 0.00014822
Iteration 19/1000 | Loss: 0.00002660
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002149
Iteration 22/1000 | Loss: 0.00002542
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001523
Iteration 28/1000 | Loss: 0.00001523
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00014192
Iteration 31/1000 | Loss: 0.00001947
Iteration 32/1000 | Loss: 0.00001772
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001476
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00002195
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002141
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001406
Iteration 46/1000 | Loss: 0.00001396
Iteration 47/1000 | Loss: 0.00001395
Iteration 48/1000 | Loss: 0.00001395
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001380
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001333
Iteration 105/1000 | Loss: 0.00001333
Iteration 106/1000 | Loss: 0.00001333
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001332
Iteration 112/1000 | Loss: 0.00001332
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001331
Iteration 117/1000 | Loss: 0.00001331
Iteration 118/1000 | Loss: 0.00001331
Iteration 119/1000 | Loss: 0.00001331
Iteration 120/1000 | Loss: 0.00001330
Iteration 121/1000 | Loss: 0.00001330
Iteration 122/1000 | Loss: 0.00001330
Iteration 123/1000 | Loss: 0.00001330
Iteration 124/1000 | Loss: 0.00001330
Iteration 125/1000 | Loss: 0.00001330
Iteration 126/1000 | Loss: 0.00001330
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001326
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001325
Iteration 167/1000 | Loss: 0.00001325
Iteration 168/1000 | Loss: 0.00001325
Iteration 169/1000 | Loss: 0.00001325
Iteration 170/1000 | Loss: 0.00001325
Iteration 171/1000 | Loss: 0.00001325
Iteration 172/1000 | Loss: 0.00001325
Iteration 173/1000 | Loss: 0.00001325
Iteration 174/1000 | Loss: 0.00001325
Iteration 175/1000 | Loss: 0.00001325
Iteration 176/1000 | Loss: 0.00001325
Iteration 177/1000 | Loss: 0.00001325
Iteration 178/1000 | Loss: 0.00001325
Iteration 179/1000 | Loss: 0.00001325
Iteration 180/1000 | Loss: 0.00001325
Iteration 181/1000 | Loss: 0.00001325
Iteration 182/1000 | Loss: 0.00001325
Iteration 183/1000 | Loss: 0.00001325
Iteration 184/1000 | Loss: 0.00001325
Iteration 185/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.3249255971459206e-05, 1.3249255971459206e-05, 1.3249255971459206e-05, 1.3249255971459206e-05, 1.3249255971459206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3249255971459206e-05

Optimization complete. Final v2v error: 3.121464252471924 mm

Highest mean error: 4.961241245269775 mm for frame 118

Lowest mean error: 2.862398862838745 mm for frame 11

Saving results

Total time: 105.29002165794373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489733
Iteration 2/25 | Loss: 0.00151296
Iteration 3/25 | Loss: 0.00142684
Iteration 4/25 | Loss: 0.00141829
Iteration 5/25 | Loss: 0.00141582
Iteration 6/25 | Loss: 0.00141557
Iteration 7/25 | Loss: 0.00141557
Iteration 8/25 | Loss: 0.00141557
Iteration 9/25 | Loss: 0.00141557
Iteration 10/25 | Loss: 0.00141557
Iteration 11/25 | Loss: 0.00141557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014155677054077387, 0.0014155677054077387, 0.0014155677054077387, 0.0014155677054077387, 0.0014155677054077387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014155677054077387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25086558
Iteration 2/25 | Loss: 0.00209980
Iteration 3/25 | Loss: 0.00209979
Iteration 4/25 | Loss: 0.00209979
Iteration 5/25 | Loss: 0.00209979
Iteration 6/25 | Loss: 0.00209979
Iteration 7/25 | Loss: 0.00209979
Iteration 8/25 | Loss: 0.00209979
Iteration 9/25 | Loss: 0.00209979
Iteration 10/25 | Loss: 0.00209979
Iteration 11/25 | Loss: 0.00209979
Iteration 12/25 | Loss: 0.00209979
Iteration 13/25 | Loss: 0.00209979
Iteration 14/25 | Loss: 0.00209979
Iteration 15/25 | Loss: 0.00209979
Iteration 16/25 | Loss: 0.00209979
Iteration 17/25 | Loss: 0.00209979
Iteration 18/25 | Loss: 0.00209979
Iteration 19/25 | Loss: 0.00209979
Iteration 20/25 | Loss: 0.00209979
Iteration 21/25 | Loss: 0.00209979
Iteration 22/25 | Loss: 0.00209979
Iteration 23/25 | Loss: 0.00209979
Iteration 24/25 | Loss: 0.00209979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020997889805585146, 0.0020997889805585146, 0.0020997889805585146, 0.0020997889805585146, 0.0020997889805585146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020997889805585146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209979
Iteration 2/1000 | Loss: 0.00004194
Iteration 3/1000 | Loss: 0.00002802
Iteration 4/1000 | Loss: 0.00002558
Iteration 5/1000 | Loss: 0.00002397
Iteration 6/1000 | Loss: 0.00002302
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002189
Iteration 9/1000 | Loss: 0.00002145
Iteration 10/1000 | Loss: 0.00002117
Iteration 11/1000 | Loss: 0.00002092
Iteration 12/1000 | Loss: 0.00002072
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002046
Iteration 16/1000 | Loss: 0.00002045
Iteration 17/1000 | Loss: 0.00002043
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002031
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002020
Iteration 25/1000 | Loss: 0.00002019
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002015
Iteration 30/1000 | Loss: 0.00002015
Iteration 31/1000 | Loss: 0.00002014
Iteration 32/1000 | Loss: 0.00002013
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002013
Iteration 35/1000 | Loss: 0.00002012
Iteration 36/1000 | Loss: 0.00002012
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002011
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002008
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002006
Iteration 44/1000 | Loss: 0.00002006
Iteration 45/1000 | Loss: 0.00002005
Iteration 46/1000 | Loss: 0.00002005
Iteration 47/1000 | Loss: 0.00002005
Iteration 48/1000 | Loss: 0.00002005
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002003
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002001
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001997
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001997
Iteration 70/1000 | Loss: 0.00001996
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001996
Iteration 74/1000 | Loss: 0.00001995
Iteration 75/1000 | Loss: 0.00001995
Iteration 76/1000 | Loss: 0.00001995
Iteration 77/1000 | Loss: 0.00001994
Iteration 78/1000 | Loss: 0.00001994
Iteration 79/1000 | Loss: 0.00001994
Iteration 80/1000 | Loss: 0.00001994
Iteration 81/1000 | Loss: 0.00001993
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001993
Iteration 84/1000 | Loss: 0.00001993
Iteration 85/1000 | Loss: 0.00001993
Iteration 86/1000 | Loss: 0.00001993
Iteration 87/1000 | Loss: 0.00001993
Iteration 88/1000 | Loss: 0.00001993
Iteration 89/1000 | Loss: 0.00001993
Iteration 90/1000 | Loss: 0.00001992
Iteration 91/1000 | Loss: 0.00001991
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001991
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001990
Iteration 96/1000 | Loss: 0.00001990
Iteration 97/1000 | Loss: 0.00001990
Iteration 98/1000 | Loss: 0.00001989
Iteration 99/1000 | Loss: 0.00001989
Iteration 100/1000 | Loss: 0.00001989
Iteration 101/1000 | Loss: 0.00001989
Iteration 102/1000 | Loss: 0.00001989
Iteration 103/1000 | Loss: 0.00001988
Iteration 104/1000 | Loss: 0.00001988
Iteration 105/1000 | Loss: 0.00001988
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001985
Iteration 122/1000 | Loss: 0.00001985
Iteration 123/1000 | Loss: 0.00001985
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001983
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001983
Iteration 131/1000 | Loss: 0.00001983
Iteration 132/1000 | Loss: 0.00001983
Iteration 133/1000 | Loss: 0.00001983
Iteration 134/1000 | Loss: 0.00001983
Iteration 135/1000 | Loss: 0.00001983
Iteration 136/1000 | Loss: 0.00001983
Iteration 137/1000 | Loss: 0.00001983
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001981
Iteration 149/1000 | Loss: 0.00001980
Iteration 150/1000 | Loss: 0.00001980
Iteration 151/1000 | Loss: 0.00001980
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Iteration 155/1000 | Loss: 0.00001979
Iteration 156/1000 | Loss: 0.00001979
Iteration 157/1000 | Loss: 0.00001979
Iteration 158/1000 | Loss: 0.00001979
Iteration 159/1000 | Loss: 0.00001978
Iteration 160/1000 | Loss: 0.00001978
Iteration 161/1000 | Loss: 0.00001978
Iteration 162/1000 | Loss: 0.00001978
Iteration 163/1000 | Loss: 0.00001978
Iteration 164/1000 | Loss: 0.00001978
Iteration 165/1000 | Loss: 0.00001978
Iteration 166/1000 | Loss: 0.00001978
Iteration 167/1000 | Loss: 0.00001978
Iteration 168/1000 | Loss: 0.00001978
Iteration 169/1000 | Loss: 0.00001978
Iteration 170/1000 | Loss: 0.00001978
Iteration 171/1000 | Loss: 0.00001978
Iteration 172/1000 | Loss: 0.00001978
Iteration 173/1000 | Loss: 0.00001978
Iteration 174/1000 | Loss: 0.00001978
Iteration 175/1000 | Loss: 0.00001978
Iteration 176/1000 | Loss: 0.00001978
Iteration 177/1000 | Loss: 0.00001978
Iteration 178/1000 | Loss: 0.00001978
Iteration 179/1000 | Loss: 0.00001978
Iteration 180/1000 | Loss: 0.00001978
Iteration 181/1000 | Loss: 0.00001978
Iteration 182/1000 | Loss: 0.00001978
Iteration 183/1000 | Loss: 0.00001978
Iteration 184/1000 | Loss: 0.00001978
Iteration 185/1000 | Loss: 0.00001978
Iteration 186/1000 | Loss: 0.00001978
Iteration 187/1000 | Loss: 0.00001978
Iteration 188/1000 | Loss: 0.00001978
Iteration 189/1000 | Loss: 0.00001978
Iteration 190/1000 | Loss: 0.00001978
Iteration 191/1000 | Loss: 0.00001978
Iteration 192/1000 | Loss: 0.00001978
Iteration 193/1000 | Loss: 0.00001978
Iteration 194/1000 | Loss: 0.00001978
Iteration 195/1000 | Loss: 0.00001978
Iteration 196/1000 | Loss: 0.00001978
Iteration 197/1000 | Loss: 0.00001978
Iteration 198/1000 | Loss: 0.00001978
Iteration 199/1000 | Loss: 0.00001978
Iteration 200/1000 | Loss: 0.00001978
Iteration 201/1000 | Loss: 0.00001978
Iteration 202/1000 | Loss: 0.00001978
Iteration 203/1000 | Loss: 0.00001978
Iteration 204/1000 | Loss: 0.00001978
Iteration 205/1000 | Loss: 0.00001978
Iteration 206/1000 | Loss: 0.00001978
Iteration 207/1000 | Loss: 0.00001978
Iteration 208/1000 | Loss: 0.00001978
Iteration 209/1000 | Loss: 0.00001978
Iteration 210/1000 | Loss: 0.00001978
Iteration 211/1000 | Loss: 0.00001978
Iteration 212/1000 | Loss: 0.00001978
Iteration 213/1000 | Loss: 0.00001978
Iteration 214/1000 | Loss: 0.00001978
Iteration 215/1000 | Loss: 0.00001978
Iteration 216/1000 | Loss: 0.00001978
Iteration 217/1000 | Loss: 0.00001978
Iteration 218/1000 | Loss: 0.00001978
Iteration 219/1000 | Loss: 0.00001978
Iteration 220/1000 | Loss: 0.00001978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.977954525500536e-05, 1.977954525500536e-05, 1.977954525500536e-05, 1.977954525500536e-05, 1.977954525500536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.977954525500536e-05

Optimization complete. Final v2v error: 3.6583690643310547 mm

Highest mean error: 4.225811958312988 mm for frame 132

Lowest mean error: 3.144479274749756 mm for frame 1

Saving results

Total time: 43.756503105163574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463884
Iteration 2/25 | Loss: 0.00144951
Iteration 3/25 | Loss: 0.00137841
Iteration 4/25 | Loss: 0.00136243
Iteration 5/25 | Loss: 0.00135886
Iteration 6/25 | Loss: 0.00135886
Iteration 7/25 | Loss: 0.00135886
Iteration 8/25 | Loss: 0.00135886
Iteration 9/25 | Loss: 0.00135886
Iteration 10/25 | Loss: 0.00135886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013588638976216316, 0.0013588638976216316, 0.0013588638976216316, 0.0013588638976216316, 0.0013588638976216316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013588638976216316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23772144
Iteration 2/25 | Loss: 0.00191884
Iteration 3/25 | Loss: 0.00191883
Iteration 4/25 | Loss: 0.00191883
Iteration 5/25 | Loss: 0.00191883
Iteration 6/25 | Loss: 0.00191883
Iteration 7/25 | Loss: 0.00191883
Iteration 8/25 | Loss: 0.00191883
Iteration 9/25 | Loss: 0.00191883
Iteration 10/25 | Loss: 0.00191883
Iteration 11/25 | Loss: 0.00191883
Iteration 12/25 | Loss: 0.00191883
Iteration 13/25 | Loss: 0.00191883
Iteration 14/25 | Loss: 0.00191883
Iteration 15/25 | Loss: 0.00191883
Iteration 16/25 | Loss: 0.00191883
Iteration 17/25 | Loss: 0.00191883
Iteration 18/25 | Loss: 0.00191883
Iteration 19/25 | Loss: 0.00191883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019188274163752794, 0.0019188274163752794, 0.0019188274163752794, 0.0019188274163752794, 0.0019188274163752794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019188274163752794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191883
Iteration 2/1000 | Loss: 0.00002651
Iteration 3/1000 | Loss: 0.00002036
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001703
Iteration 7/1000 | Loss: 0.00001658
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001521
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001446
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001437
Iteration 29/1000 | Loss: 0.00001437
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001424
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001424
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001418
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001418
Iteration 50/1000 | Loss: 0.00001418
Iteration 51/1000 | Loss: 0.00001418
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001418
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001416
Iteration 58/1000 | Loss: 0.00001415
Iteration 59/1000 | Loss: 0.00001415
Iteration 60/1000 | Loss: 0.00001415
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001411
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001405
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001399
Iteration 121/1000 | Loss: 0.00001399
Iteration 122/1000 | Loss: 0.00001399
Iteration 123/1000 | Loss: 0.00001399
Iteration 124/1000 | Loss: 0.00001399
Iteration 125/1000 | Loss: 0.00001399
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001398
Iteration 129/1000 | Loss: 0.00001398
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001397
Iteration 136/1000 | Loss: 0.00001397
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001397
Iteration 139/1000 | Loss: 0.00001397
Iteration 140/1000 | Loss: 0.00001397
Iteration 141/1000 | Loss: 0.00001397
Iteration 142/1000 | Loss: 0.00001397
Iteration 143/1000 | Loss: 0.00001397
Iteration 144/1000 | Loss: 0.00001397
Iteration 145/1000 | Loss: 0.00001397
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001396
Iteration 149/1000 | Loss: 0.00001396
Iteration 150/1000 | Loss: 0.00001396
Iteration 151/1000 | Loss: 0.00001396
Iteration 152/1000 | Loss: 0.00001395
Iteration 153/1000 | Loss: 0.00001395
Iteration 154/1000 | Loss: 0.00001395
Iteration 155/1000 | Loss: 0.00001395
Iteration 156/1000 | Loss: 0.00001395
Iteration 157/1000 | Loss: 0.00001395
Iteration 158/1000 | Loss: 0.00001395
Iteration 159/1000 | Loss: 0.00001395
Iteration 160/1000 | Loss: 0.00001395
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001393
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001392
Iteration 173/1000 | Loss: 0.00001392
Iteration 174/1000 | Loss: 0.00001392
Iteration 175/1000 | Loss: 0.00001391
Iteration 176/1000 | Loss: 0.00001391
Iteration 177/1000 | Loss: 0.00001390
Iteration 178/1000 | Loss: 0.00001390
Iteration 179/1000 | Loss: 0.00001390
Iteration 180/1000 | Loss: 0.00001390
Iteration 181/1000 | Loss: 0.00001390
Iteration 182/1000 | Loss: 0.00001390
Iteration 183/1000 | Loss: 0.00001390
Iteration 184/1000 | Loss: 0.00001390
Iteration 185/1000 | Loss: 0.00001390
Iteration 186/1000 | Loss: 0.00001390
Iteration 187/1000 | Loss: 0.00001390
Iteration 188/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.3899049918109085e-05, 1.3899049918109085e-05, 1.3899049918109085e-05, 1.3899049918109085e-05, 1.3899049918109085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3899049918109085e-05

Optimization complete. Final v2v error: 3.1484832763671875 mm

Highest mean error: 3.4874281883239746 mm for frame 191

Lowest mean error: 2.8999359607696533 mm for frame 237

Saving results

Total time: 48.80835723876953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443815
Iteration 2/25 | Loss: 0.00139331
Iteration 3/25 | Loss: 0.00133832
Iteration 4/25 | Loss: 0.00132901
Iteration 5/25 | Loss: 0.00132598
Iteration 6/25 | Loss: 0.00132598
Iteration 7/25 | Loss: 0.00132598
Iteration 8/25 | Loss: 0.00132598
Iteration 9/25 | Loss: 0.00132598
Iteration 10/25 | Loss: 0.00132598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00132597831543535, 0.00132597831543535, 0.00132597831543535, 0.00132597831543535, 0.00132597831543535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00132597831543535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24603403
Iteration 2/25 | Loss: 0.00226474
Iteration 3/25 | Loss: 0.00226474
Iteration 4/25 | Loss: 0.00226474
Iteration 5/25 | Loss: 0.00226473
Iteration 6/25 | Loss: 0.00226473
Iteration 7/25 | Loss: 0.00226473
Iteration 8/25 | Loss: 0.00226473
Iteration 9/25 | Loss: 0.00226473
Iteration 10/25 | Loss: 0.00226473
Iteration 11/25 | Loss: 0.00226473
Iteration 12/25 | Loss: 0.00226473
Iteration 13/25 | Loss: 0.00226473
Iteration 14/25 | Loss: 0.00226473
Iteration 15/25 | Loss: 0.00226473
Iteration 16/25 | Loss: 0.00226473
Iteration 17/25 | Loss: 0.00226473
Iteration 18/25 | Loss: 0.00226473
Iteration 19/25 | Loss: 0.00226473
Iteration 20/25 | Loss: 0.00226473
Iteration 21/25 | Loss: 0.00226473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002264732029289007, 0.002264732029289007, 0.002264732029289007, 0.002264732029289007, 0.002264732029289007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002264732029289007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226473
Iteration 2/1000 | Loss: 0.00002255
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001508
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00001420
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001287
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001255
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001211
Iteration 40/1000 | Loss: 0.00001211
Iteration 41/1000 | Loss: 0.00001211
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001207
Iteration 53/1000 | Loss: 0.00001207
Iteration 54/1000 | Loss: 0.00001207
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001206
Iteration 57/1000 | Loss: 0.00001206
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001205
Iteration 60/1000 | Loss: 0.00001205
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001202
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001199
Iteration 105/1000 | Loss: 0.00001199
Iteration 106/1000 | Loss: 0.00001199
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001198
Iteration 112/1000 | Loss: 0.00001198
Iteration 113/1000 | Loss: 0.00001198
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.1981913303316105e-05, 1.1981913303316105e-05, 1.1981913303316105e-05, 1.1981913303316105e-05, 1.1981913303316105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1981913303316105e-05

Optimization complete. Final v2v error: 2.995570421218872 mm

Highest mean error: 3.099879503250122 mm for frame 115

Lowest mean error: 2.877953290939331 mm for frame 190

Saving results

Total time: 39.719459772109985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802184
Iteration 2/25 | Loss: 0.00158356
Iteration 3/25 | Loss: 0.00140831
Iteration 4/25 | Loss: 0.00139603
Iteration 5/25 | Loss: 0.00139565
Iteration 6/25 | Loss: 0.00139565
Iteration 7/25 | Loss: 0.00139565
Iteration 8/25 | Loss: 0.00139565
Iteration 9/25 | Loss: 0.00139565
Iteration 10/25 | Loss: 0.00139565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001395653118379414, 0.001395653118379414, 0.001395653118379414, 0.001395653118379414, 0.001395653118379414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001395653118379414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14220595
Iteration 2/25 | Loss: 0.00168901
Iteration 3/25 | Loss: 0.00168900
Iteration 4/25 | Loss: 0.00168900
Iteration 5/25 | Loss: 0.00168900
Iteration 6/25 | Loss: 0.00168900
Iteration 7/25 | Loss: 0.00168900
Iteration 8/25 | Loss: 0.00168900
Iteration 9/25 | Loss: 0.00168900
Iteration 10/25 | Loss: 0.00168900
Iteration 11/25 | Loss: 0.00168900
Iteration 12/25 | Loss: 0.00168900
Iteration 13/25 | Loss: 0.00168900
Iteration 14/25 | Loss: 0.00168900
Iteration 15/25 | Loss: 0.00168900
Iteration 16/25 | Loss: 0.00168900
Iteration 17/25 | Loss: 0.00168900
Iteration 18/25 | Loss: 0.00168900
Iteration 19/25 | Loss: 0.00168900
Iteration 20/25 | Loss: 0.00168900
Iteration 21/25 | Loss: 0.00168900
Iteration 22/25 | Loss: 0.00168900
Iteration 23/25 | Loss: 0.00168900
Iteration 24/25 | Loss: 0.00168900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016890006372705102, 0.0016890006372705102, 0.0016890006372705102, 0.0016890006372705102, 0.0016890006372705102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016890006372705102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168900
Iteration 2/1000 | Loss: 0.00003457
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002055
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001958
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001895
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001831
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001802
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001787
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001786
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001785
Iteration 28/1000 | Loss: 0.00001784
Iteration 29/1000 | Loss: 0.00001784
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001783
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001772
Iteration 74/1000 | Loss: 0.00001772
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001771
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001769
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001768
Iteration 92/1000 | Loss: 0.00001768
Iteration 93/1000 | Loss: 0.00001768
Iteration 94/1000 | Loss: 0.00001768
Iteration 95/1000 | Loss: 0.00001768
Iteration 96/1000 | Loss: 0.00001768
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001766
Iteration 106/1000 | Loss: 0.00001766
Iteration 107/1000 | Loss: 0.00001766
Iteration 108/1000 | Loss: 0.00001766
Iteration 109/1000 | Loss: 0.00001766
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001766
Iteration 115/1000 | Loss: 0.00001766
Iteration 116/1000 | Loss: 0.00001766
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001766
Iteration 122/1000 | Loss: 0.00001766
Iteration 123/1000 | Loss: 0.00001766
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.766374771250412e-05, 1.766374771250412e-05, 1.766374771250412e-05, 1.766374771250412e-05, 1.766374771250412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.766374771250412e-05

Optimization complete. Final v2v error: 3.568744659423828 mm

Highest mean error: 3.9444425106048584 mm for frame 58

Lowest mean error: 3.3289666175842285 mm for frame 6

Saving results

Total time: 35.143118143081665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998731
Iteration 2/25 | Loss: 0.00261549
Iteration 3/25 | Loss: 0.00303223
Iteration 4/25 | Loss: 0.00163236
Iteration 5/25 | Loss: 0.00163204
Iteration 6/25 | Loss: 0.00155352
Iteration 7/25 | Loss: 0.00149897
Iteration 8/25 | Loss: 0.00144598
Iteration 9/25 | Loss: 0.00142886
Iteration 10/25 | Loss: 0.00141782
Iteration 11/25 | Loss: 0.00139569
Iteration 12/25 | Loss: 0.00140177
Iteration 13/25 | Loss: 0.00140784
Iteration 14/25 | Loss: 0.00140195
Iteration 15/25 | Loss: 0.00141853
Iteration 16/25 | Loss: 0.00140810
Iteration 17/25 | Loss: 0.00139031
Iteration 18/25 | Loss: 0.00137808
Iteration 19/25 | Loss: 0.00136978
Iteration 20/25 | Loss: 0.00137018
Iteration 21/25 | Loss: 0.00136797
Iteration 22/25 | Loss: 0.00136289
Iteration 23/25 | Loss: 0.00136218
Iteration 24/25 | Loss: 0.00136331
Iteration 25/25 | Loss: 0.00136893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34832394
Iteration 2/25 | Loss: 0.00237641
Iteration 3/25 | Loss: 0.00237641
Iteration 4/25 | Loss: 0.00237641
Iteration 5/25 | Loss: 0.00237640
Iteration 6/25 | Loss: 0.00237640
Iteration 7/25 | Loss: 0.00237640
Iteration 8/25 | Loss: 0.00237640
Iteration 9/25 | Loss: 0.00237640
Iteration 10/25 | Loss: 0.00237640
Iteration 11/25 | Loss: 0.00237640
Iteration 12/25 | Loss: 0.00237640
Iteration 13/25 | Loss: 0.00237640
Iteration 14/25 | Loss: 0.00237640
Iteration 15/25 | Loss: 0.00237640
Iteration 16/25 | Loss: 0.00237640
Iteration 17/25 | Loss: 0.00237640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023764027282595634, 0.0023764027282595634, 0.0023764027282595634, 0.0023764027282595634, 0.0023764027282595634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023764027282595634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237640
Iteration 2/1000 | Loss: 0.00041229
Iteration 3/1000 | Loss: 0.00056665
Iteration 4/1000 | Loss: 0.00010070
Iteration 5/1000 | Loss: 0.00004867
Iteration 6/1000 | Loss: 0.00051983
Iteration 7/1000 | Loss: 0.00005324
Iteration 8/1000 | Loss: 0.00011016
Iteration 9/1000 | Loss: 0.00010152
Iteration 10/1000 | Loss: 0.00023096
Iteration 11/1000 | Loss: 0.00014962
Iteration 12/1000 | Loss: 0.00013812
Iteration 13/1000 | Loss: 0.00009434
Iteration 14/1000 | Loss: 0.00019501
Iteration 15/1000 | Loss: 0.00042260
Iteration 16/1000 | Loss: 0.00023810
Iteration 17/1000 | Loss: 0.00019512
Iteration 18/1000 | Loss: 0.00015410
Iteration 19/1000 | Loss: 0.00034690
Iteration 20/1000 | Loss: 0.00048785
Iteration 21/1000 | Loss: 0.00068487
Iteration 22/1000 | Loss: 0.00056163
Iteration 23/1000 | Loss: 0.00072731
Iteration 24/1000 | Loss: 0.00021734
Iteration 25/1000 | Loss: 0.00009047
Iteration 26/1000 | Loss: 0.00003933
Iteration 27/1000 | Loss: 0.00006567
Iteration 28/1000 | Loss: 0.00006025
Iteration 29/1000 | Loss: 0.00013845
Iteration 30/1000 | Loss: 0.00017175
Iteration 31/1000 | Loss: 0.00022127
Iteration 32/1000 | Loss: 0.00013964
Iteration 33/1000 | Loss: 0.00003940
Iteration 34/1000 | Loss: 0.00004112
Iteration 35/1000 | Loss: 0.00019074
Iteration 36/1000 | Loss: 0.00013935
Iteration 37/1000 | Loss: 0.00004355
Iteration 38/1000 | Loss: 0.00006206
Iteration 39/1000 | Loss: 0.00020350
Iteration 40/1000 | Loss: 0.00018461
Iteration 41/1000 | Loss: 0.00013742
Iteration 42/1000 | Loss: 0.00003663
Iteration 43/1000 | Loss: 0.00030029
Iteration 44/1000 | Loss: 0.00022497
Iteration 45/1000 | Loss: 0.00023776
Iteration 46/1000 | Loss: 0.00022457
Iteration 47/1000 | Loss: 0.00018731
Iteration 48/1000 | Loss: 0.00021317
Iteration 49/1000 | Loss: 0.00021096
Iteration 50/1000 | Loss: 0.00016092
Iteration 51/1000 | Loss: 0.00013266
Iteration 52/1000 | Loss: 0.00002693
Iteration 53/1000 | Loss: 0.00002540
Iteration 54/1000 | Loss: 0.00023758
Iteration 55/1000 | Loss: 0.00016292
Iteration 56/1000 | Loss: 0.00002628
Iteration 57/1000 | Loss: 0.00002547
Iteration 58/1000 | Loss: 0.00023942
Iteration 59/1000 | Loss: 0.00016094
Iteration 60/1000 | Loss: 0.00007609
Iteration 61/1000 | Loss: 0.00013387
Iteration 62/1000 | Loss: 0.00006657
Iteration 63/1000 | Loss: 0.00010407
Iteration 64/1000 | Loss: 0.00007902
Iteration 65/1000 | Loss: 0.00007544
Iteration 66/1000 | Loss: 0.00007681
Iteration 67/1000 | Loss: 0.00016769
Iteration 68/1000 | Loss: 0.00002518
Iteration 69/1000 | Loss: 0.00005269
Iteration 70/1000 | Loss: 0.00015544
Iteration 71/1000 | Loss: 0.00019468
Iteration 72/1000 | Loss: 0.00020483
Iteration 73/1000 | Loss: 0.00133761
Iteration 74/1000 | Loss: 0.00054049
Iteration 75/1000 | Loss: 0.00005637
Iteration 76/1000 | Loss: 0.00004067
Iteration 77/1000 | Loss: 0.00003011
Iteration 78/1000 | Loss: 0.00002782
Iteration 79/1000 | Loss: 0.00073105
Iteration 80/1000 | Loss: 0.00026876
Iteration 81/1000 | Loss: 0.00069518
Iteration 82/1000 | Loss: 0.00020512
Iteration 83/1000 | Loss: 0.00005243
Iteration 84/1000 | Loss: 0.00043623
Iteration 85/1000 | Loss: 0.00016815
Iteration 86/1000 | Loss: 0.00062194
Iteration 87/1000 | Loss: 0.00034552
Iteration 88/1000 | Loss: 0.00003507
Iteration 89/1000 | Loss: 0.00037277
Iteration 90/1000 | Loss: 0.00010296
Iteration 91/1000 | Loss: 0.00003376
Iteration 92/1000 | Loss: 0.00002771
Iteration 93/1000 | Loss: 0.00026574
Iteration 94/1000 | Loss: 0.00006183
Iteration 95/1000 | Loss: 0.00015618
Iteration 96/1000 | Loss: 0.00003823
Iteration 97/1000 | Loss: 0.00030775
Iteration 98/1000 | Loss: 0.00004682
Iteration 99/1000 | Loss: 0.00003772
Iteration 100/1000 | Loss: 0.00003163
Iteration 101/1000 | Loss: 0.00002885
Iteration 102/1000 | Loss: 0.00002709
Iteration 103/1000 | Loss: 0.00002695
Iteration 104/1000 | Loss: 0.00002473
Iteration 105/1000 | Loss: 0.00093456
Iteration 106/1000 | Loss: 0.00030600
Iteration 107/1000 | Loss: 0.00005687
Iteration 108/1000 | Loss: 0.00002501
Iteration 109/1000 | Loss: 0.00002307
Iteration 110/1000 | Loss: 0.00002184
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002075
Iteration 113/1000 | Loss: 0.00002040
Iteration 114/1000 | Loss: 0.00056741
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00001898
Iteration 117/1000 | Loss: 0.00001773
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001654
Iteration 121/1000 | Loss: 0.00001634
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001609
Iteration 126/1000 | Loss: 0.00001605
Iteration 127/1000 | Loss: 0.00001593
Iteration 128/1000 | Loss: 0.00001591
Iteration 129/1000 | Loss: 0.00001589
Iteration 130/1000 | Loss: 0.00001589
Iteration 131/1000 | Loss: 0.00001584
Iteration 132/1000 | Loss: 0.00001581
Iteration 133/1000 | Loss: 0.00001581
Iteration 134/1000 | Loss: 0.00001575
Iteration 135/1000 | Loss: 0.00001575
Iteration 136/1000 | Loss: 0.00001574
Iteration 137/1000 | Loss: 0.00001574
Iteration 138/1000 | Loss: 0.00001571
Iteration 139/1000 | Loss: 0.00001565
Iteration 140/1000 | Loss: 0.00001565
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001562
Iteration 144/1000 | Loss: 0.00001561
Iteration 145/1000 | Loss: 0.00001561
Iteration 146/1000 | Loss: 0.00001559
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001559
Iteration 150/1000 | Loss: 0.00001559
Iteration 151/1000 | Loss: 0.00001559
Iteration 152/1000 | Loss: 0.00001559
Iteration 153/1000 | Loss: 0.00001559
Iteration 154/1000 | Loss: 0.00001559
Iteration 155/1000 | Loss: 0.00001559
Iteration 156/1000 | Loss: 0.00001558
Iteration 157/1000 | Loss: 0.00001558
Iteration 158/1000 | Loss: 0.00001558
Iteration 159/1000 | Loss: 0.00001558
Iteration 160/1000 | Loss: 0.00001557
Iteration 161/1000 | Loss: 0.00001557
Iteration 162/1000 | Loss: 0.00001557
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001555
Iteration 167/1000 | Loss: 0.00001555
Iteration 168/1000 | Loss: 0.00001554
Iteration 169/1000 | Loss: 0.00001554
Iteration 170/1000 | Loss: 0.00001554
Iteration 171/1000 | Loss: 0.00001553
Iteration 172/1000 | Loss: 0.00001553
Iteration 173/1000 | Loss: 0.00001553
Iteration 174/1000 | Loss: 0.00001553
Iteration 175/1000 | Loss: 0.00001552
Iteration 176/1000 | Loss: 0.00001552
Iteration 177/1000 | Loss: 0.00001552
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001549
Iteration 181/1000 | Loss: 0.00001546
Iteration 182/1000 | Loss: 0.00001546
Iteration 183/1000 | Loss: 0.00001539
Iteration 184/1000 | Loss: 0.00001521
Iteration 185/1000 | Loss: 0.00001521
Iteration 186/1000 | Loss: 0.00001517
Iteration 187/1000 | Loss: 0.00001883
Iteration 188/1000 | Loss: 0.00029590
Iteration 189/1000 | Loss: 0.00002669
Iteration 190/1000 | Loss: 0.00002013
Iteration 191/1000 | Loss: 0.00001832
Iteration 192/1000 | Loss: 0.00001708
Iteration 193/1000 | Loss: 0.00001642
Iteration 194/1000 | Loss: 0.00001588
Iteration 195/1000 | Loss: 0.00001551
Iteration 196/1000 | Loss: 0.00001526
Iteration 197/1000 | Loss: 0.00001519
Iteration 198/1000 | Loss: 0.00001504
Iteration 199/1000 | Loss: 0.00001495
Iteration 200/1000 | Loss: 0.00001491
Iteration 201/1000 | Loss: 0.00001484
Iteration 202/1000 | Loss: 0.00001483
Iteration 203/1000 | Loss: 0.00001482
Iteration 204/1000 | Loss: 0.00001480
Iteration 205/1000 | Loss: 0.00001480
Iteration 206/1000 | Loss: 0.00001479
Iteration 207/1000 | Loss: 0.00001479
Iteration 208/1000 | Loss: 0.00001478
Iteration 209/1000 | Loss: 0.00001478
Iteration 210/1000 | Loss: 0.00001477
Iteration 211/1000 | Loss: 0.00001477
Iteration 212/1000 | Loss: 0.00001477
Iteration 213/1000 | Loss: 0.00001476
Iteration 214/1000 | Loss: 0.00001475
Iteration 215/1000 | Loss: 0.00001474
Iteration 216/1000 | Loss: 0.00001474
Iteration 217/1000 | Loss: 0.00001474
Iteration 218/1000 | Loss: 0.00001474
Iteration 219/1000 | Loss: 0.00001473
Iteration 220/1000 | Loss: 0.00001473
Iteration 221/1000 | Loss: 0.00001473
Iteration 222/1000 | Loss: 0.00001472
Iteration 223/1000 | Loss: 0.00001472
Iteration 224/1000 | Loss: 0.00001472
Iteration 225/1000 | Loss: 0.00001472
Iteration 226/1000 | Loss: 0.00001472
Iteration 227/1000 | Loss: 0.00001471
Iteration 228/1000 | Loss: 0.00001471
Iteration 229/1000 | Loss: 0.00001471
Iteration 230/1000 | Loss: 0.00001471
Iteration 231/1000 | Loss: 0.00001471
Iteration 232/1000 | Loss: 0.00001471
Iteration 233/1000 | Loss: 0.00001471
Iteration 234/1000 | Loss: 0.00001471
Iteration 235/1000 | Loss: 0.00001471
Iteration 236/1000 | Loss: 0.00001471
Iteration 237/1000 | Loss: 0.00001471
Iteration 238/1000 | Loss: 0.00001471
Iteration 239/1000 | Loss: 0.00001471
Iteration 240/1000 | Loss: 0.00001471
Iteration 241/1000 | Loss: 0.00001471
Iteration 242/1000 | Loss: 0.00001471
Iteration 243/1000 | Loss: 0.00001471
Iteration 244/1000 | Loss: 0.00001471
Iteration 245/1000 | Loss: 0.00001471
Iteration 246/1000 | Loss: 0.00001471
Iteration 247/1000 | Loss: 0.00001471
Iteration 248/1000 | Loss: 0.00001471
Iteration 249/1000 | Loss: 0.00001471
Iteration 250/1000 | Loss: 0.00001471
Iteration 251/1000 | Loss: 0.00001471
Iteration 252/1000 | Loss: 0.00001471
Iteration 253/1000 | Loss: 0.00001471
Iteration 254/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.471020277676871e-05, 1.471020277676871e-05, 1.471020277676871e-05, 1.471020277676871e-05, 1.471020277676871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.471020277676871e-05

Optimization complete. Final v2v error: 3.0911896228790283 mm

Highest mean error: 6.117038726806641 mm for frame 102

Lowest mean error: 2.6818366050720215 mm for frame 124

Saving results

Total time: 246.74397730827332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391539
Iteration 2/25 | Loss: 0.00143093
Iteration 3/25 | Loss: 0.00134743
Iteration 4/25 | Loss: 0.00133100
Iteration 5/25 | Loss: 0.00132414
Iteration 6/25 | Loss: 0.00132303
Iteration 7/25 | Loss: 0.00132303
Iteration 8/25 | Loss: 0.00132303
Iteration 9/25 | Loss: 0.00132303
Iteration 10/25 | Loss: 0.00132303
Iteration 11/25 | Loss: 0.00132303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013230274198576808, 0.0013230274198576808, 0.0013230274198576808, 0.0013230274198576808, 0.0013230274198576808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013230274198576808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20292258
Iteration 2/25 | Loss: 0.00331490
Iteration 3/25 | Loss: 0.00331490
Iteration 4/25 | Loss: 0.00331489
Iteration 5/25 | Loss: 0.00331489
Iteration 6/25 | Loss: 0.00331489
Iteration 7/25 | Loss: 0.00331489
Iteration 8/25 | Loss: 0.00331489
Iteration 9/25 | Loss: 0.00331489
Iteration 10/25 | Loss: 0.00331489
Iteration 11/25 | Loss: 0.00331489
Iteration 12/25 | Loss: 0.00331489
Iteration 13/25 | Loss: 0.00331489
Iteration 14/25 | Loss: 0.00331489
Iteration 15/25 | Loss: 0.00331489
Iteration 16/25 | Loss: 0.00331489
Iteration 17/25 | Loss: 0.00331489
Iteration 18/25 | Loss: 0.00331489
Iteration 19/25 | Loss: 0.00331489
Iteration 20/25 | Loss: 0.00331489
Iteration 21/25 | Loss: 0.00331489
Iteration 22/25 | Loss: 0.00331489
Iteration 23/25 | Loss: 0.00331489
Iteration 24/25 | Loss: 0.00331489
Iteration 25/25 | Loss: 0.00331489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00331489
Iteration 2/1000 | Loss: 0.00004626
Iteration 3/1000 | Loss: 0.00003366
Iteration 4/1000 | Loss: 0.00002877
Iteration 5/1000 | Loss: 0.00002644
Iteration 6/1000 | Loss: 0.00002469
Iteration 7/1000 | Loss: 0.00002356
Iteration 8/1000 | Loss: 0.00002278
Iteration 9/1000 | Loss: 0.00002232
Iteration 10/1000 | Loss: 0.00002189
Iteration 11/1000 | Loss: 0.00002157
Iteration 12/1000 | Loss: 0.00002135
Iteration 13/1000 | Loss: 0.00002127
Iteration 14/1000 | Loss: 0.00002110
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002095
Iteration 17/1000 | Loss: 0.00002094
Iteration 18/1000 | Loss: 0.00002093
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002091
Iteration 21/1000 | Loss: 0.00002090
Iteration 22/1000 | Loss: 0.00002090
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002087
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00002085
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002084
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002083
Iteration 31/1000 | Loss: 0.00002082
Iteration 32/1000 | Loss: 0.00002082
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002080
Iteration 36/1000 | Loss: 0.00002079
Iteration 37/1000 | Loss: 0.00002079
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002078
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002068
Iteration 43/1000 | Loss: 0.00002067
Iteration 44/1000 | Loss: 0.00002065
Iteration 45/1000 | Loss: 0.00002063
Iteration 46/1000 | Loss: 0.00002063
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002063
Iteration 49/1000 | Loss: 0.00002063
Iteration 50/1000 | Loss: 0.00002063
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002062
Iteration 54/1000 | Loss: 0.00002062
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002062
Iteration 57/1000 | Loss: 0.00002061
Iteration 58/1000 | Loss: 0.00002061
Iteration 59/1000 | Loss: 0.00002060
Iteration 60/1000 | Loss: 0.00002060
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002059
Iteration 64/1000 | Loss: 0.00002059
Iteration 65/1000 | Loss: 0.00002059
Iteration 66/1000 | Loss: 0.00002058
Iteration 67/1000 | Loss: 0.00002058
Iteration 68/1000 | Loss: 0.00002057
Iteration 69/1000 | Loss: 0.00002057
Iteration 70/1000 | Loss: 0.00002057
Iteration 71/1000 | Loss: 0.00002057
Iteration 72/1000 | Loss: 0.00002056
Iteration 73/1000 | Loss: 0.00002056
Iteration 74/1000 | Loss: 0.00002056
Iteration 75/1000 | Loss: 0.00002056
Iteration 76/1000 | Loss: 0.00002055
Iteration 77/1000 | Loss: 0.00002055
Iteration 78/1000 | Loss: 0.00002055
Iteration 79/1000 | Loss: 0.00002055
Iteration 80/1000 | Loss: 0.00002054
Iteration 81/1000 | Loss: 0.00002054
Iteration 82/1000 | Loss: 0.00002052
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002051
Iteration 86/1000 | Loss: 0.00002051
Iteration 87/1000 | Loss: 0.00002051
Iteration 88/1000 | Loss: 0.00002051
Iteration 89/1000 | Loss: 0.00002051
Iteration 90/1000 | Loss: 0.00002051
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002051
Iteration 94/1000 | Loss: 0.00002050
Iteration 95/1000 | Loss: 0.00002050
Iteration 96/1000 | Loss: 0.00002050
Iteration 97/1000 | Loss: 0.00002050
Iteration 98/1000 | Loss: 0.00002050
Iteration 99/1000 | Loss: 0.00002050
Iteration 100/1000 | Loss: 0.00002049
Iteration 101/1000 | Loss: 0.00002049
Iteration 102/1000 | Loss: 0.00002049
Iteration 103/1000 | Loss: 0.00002049
Iteration 104/1000 | Loss: 0.00002048
Iteration 105/1000 | Loss: 0.00002048
Iteration 106/1000 | Loss: 0.00002048
Iteration 107/1000 | Loss: 0.00002047
Iteration 108/1000 | Loss: 0.00002047
Iteration 109/1000 | Loss: 0.00002047
Iteration 110/1000 | Loss: 0.00002047
Iteration 111/1000 | Loss: 0.00002047
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002046
Iteration 116/1000 | Loss: 0.00002046
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002046
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002045
Iteration 122/1000 | Loss: 0.00002045
Iteration 123/1000 | Loss: 0.00002045
Iteration 124/1000 | Loss: 0.00002045
Iteration 125/1000 | Loss: 0.00002045
Iteration 126/1000 | Loss: 0.00002045
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00002045
Iteration 129/1000 | Loss: 0.00002045
Iteration 130/1000 | Loss: 0.00002045
Iteration 131/1000 | Loss: 0.00002045
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002043
Iteration 136/1000 | Loss: 0.00002043
Iteration 137/1000 | Loss: 0.00002042
Iteration 138/1000 | Loss: 0.00002042
Iteration 139/1000 | Loss: 0.00002042
Iteration 140/1000 | Loss: 0.00002042
Iteration 141/1000 | Loss: 0.00002041
Iteration 142/1000 | Loss: 0.00002041
Iteration 143/1000 | Loss: 0.00002041
Iteration 144/1000 | Loss: 0.00002041
Iteration 145/1000 | Loss: 0.00002041
Iteration 146/1000 | Loss: 0.00002041
Iteration 147/1000 | Loss: 0.00002041
Iteration 148/1000 | Loss: 0.00002040
Iteration 149/1000 | Loss: 0.00002040
Iteration 150/1000 | Loss: 0.00002040
Iteration 151/1000 | Loss: 0.00002040
Iteration 152/1000 | Loss: 0.00002040
Iteration 153/1000 | Loss: 0.00002040
Iteration 154/1000 | Loss: 0.00002040
Iteration 155/1000 | Loss: 0.00002039
Iteration 156/1000 | Loss: 0.00002039
Iteration 157/1000 | Loss: 0.00002039
Iteration 158/1000 | Loss: 0.00002038
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002038
Iteration 161/1000 | Loss: 0.00002038
Iteration 162/1000 | Loss: 0.00002038
Iteration 163/1000 | Loss: 0.00002038
Iteration 164/1000 | Loss: 0.00002038
Iteration 165/1000 | Loss: 0.00002038
Iteration 166/1000 | Loss: 0.00002037
Iteration 167/1000 | Loss: 0.00002037
Iteration 168/1000 | Loss: 0.00002036
Iteration 169/1000 | Loss: 0.00002036
Iteration 170/1000 | Loss: 0.00002036
Iteration 171/1000 | Loss: 0.00002036
Iteration 172/1000 | Loss: 0.00002036
Iteration 173/1000 | Loss: 0.00002036
Iteration 174/1000 | Loss: 0.00002036
Iteration 175/1000 | Loss: 0.00002036
Iteration 176/1000 | Loss: 0.00002036
Iteration 177/1000 | Loss: 0.00002036
Iteration 178/1000 | Loss: 0.00002035
Iteration 179/1000 | Loss: 0.00002035
Iteration 180/1000 | Loss: 0.00002035
Iteration 181/1000 | Loss: 0.00002035
Iteration 182/1000 | Loss: 0.00002034
Iteration 183/1000 | Loss: 0.00002034
Iteration 184/1000 | Loss: 0.00002034
Iteration 185/1000 | Loss: 0.00002034
Iteration 186/1000 | Loss: 0.00002034
Iteration 187/1000 | Loss: 0.00002033
Iteration 188/1000 | Loss: 0.00002033
Iteration 189/1000 | Loss: 0.00002033
Iteration 190/1000 | Loss: 0.00002033
Iteration 191/1000 | Loss: 0.00002033
Iteration 192/1000 | Loss: 0.00002033
Iteration 193/1000 | Loss: 0.00002033
Iteration 194/1000 | Loss: 0.00002033
Iteration 195/1000 | Loss: 0.00002033
Iteration 196/1000 | Loss: 0.00002033
Iteration 197/1000 | Loss: 0.00002033
Iteration 198/1000 | Loss: 0.00002033
Iteration 199/1000 | Loss: 0.00002033
Iteration 200/1000 | Loss: 0.00002033
Iteration 201/1000 | Loss: 0.00002032
Iteration 202/1000 | Loss: 0.00002032
Iteration 203/1000 | Loss: 0.00002032
Iteration 204/1000 | Loss: 0.00002032
Iteration 205/1000 | Loss: 0.00002032
Iteration 206/1000 | Loss: 0.00002032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.0324760043877177e-05, 2.0324760043877177e-05, 2.0324760043877177e-05, 2.0324760043877177e-05, 2.0324760043877177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0324760043877177e-05

Optimization complete. Final v2v error: 3.757219076156616 mm

Highest mean error: 4.759242534637451 mm for frame 103

Lowest mean error: 2.7867939472198486 mm for frame 0

Saving results

Total time: 50.48749828338623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732394
Iteration 2/25 | Loss: 0.00143355
Iteration 3/25 | Loss: 0.00136990
Iteration 4/25 | Loss: 0.00136227
Iteration 5/25 | Loss: 0.00136055
Iteration 6/25 | Loss: 0.00136040
Iteration 7/25 | Loss: 0.00136040
Iteration 8/25 | Loss: 0.00136040
Iteration 9/25 | Loss: 0.00136040
Iteration 10/25 | Loss: 0.00136040
Iteration 11/25 | Loss: 0.00136040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001360403373837471, 0.001360403373837471, 0.001360403373837471, 0.001360403373837471, 0.001360403373837471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001360403373837471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22599483
Iteration 2/25 | Loss: 0.00192116
Iteration 3/25 | Loss: 0.00192109
Iteration 4/25 | Loss: 0.00192109
Iteration 5/25 | Loss: 0.00192109
Iteration 6/25 | Loss: 0.00192109
Iteration 7/25 | Loss: 0.00192109
Iteration 8/25 | Loss: 0.00192109
Iteration 9/25 | Loss: 0.00192109
Iteration 10/25 | Loss: 0.00192109
Iteration 11/25 | Loss: 0.00192109
Iteration 12/25 | Loss: 0.00192109
Iteration 13/25 | Loss: 0.00192109
Iteration 14/25 | Loss: 0.00192109
Iteration 15/25 | Loss: 0.00192109
Iteration 16/25 | Loss: 0.00192109
Iteration 17/25 | Loss: 0.00192109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019210891332477331, 0.0019210891332477331, 0.0019210891332477331, 0.0019210891332477331, 0.0019210891332477331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019210891332477331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192109
Iteration 2/1000 | Loss: 0.00003925
Iteration 3/1000 | Loss: 0.00003019
Iteration 4/1000 | Loss: 0.00002611
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002149
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002065
Iteration 11/1000 | Loss: 0.00002030
Iteration 12/1000 | Loss: 0.00001990
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001869
Iteration 17/1000 | Loss: 0.00001860
Iteration 18/1000 | Loss: 0.00001854
Iteration 19/1000 | Loss: 0.00001852
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001846
Iteration 22/1000 | Loss: 0.00001842
Iteration 23/1000 | Loss: 0.00001842
Iteration 24/1000 | Loss: 0.00001837
Iteration 25/1000 | Loss: 0.00001834
Iteration 26/1000 | Loss: 0.00001833
Iteration 27/1000 | Loss: 0.00001831
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001829
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001826
Iteration 32/1000 | Loss: 0.00001825
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001824
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001820
Iteration 40/1000 | Loss: 0.00001816
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001803
Iteration 59/1000 | Loss: 0.00001803
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001803
Iteration 62/1000 | Loss: 0.00001803
Iteration 63/1000 | Loss: 0.00001803
Iteration 64/1000 | Loss: 0.00001803
Iteration 65/1000 | Loss: 0.00001802
Iteration 66/1000 | Loss: 0.00001801
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001800
Iteration 69/1000 | Loss: 0.00001800
Iteration 70/1000 | Loss: 0.00001800
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001799
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001798
Iteration 81/1000 | Loss: 0.00001798
Iteration 82/1000 | Loss: 0.00001798
Iteration 83/1000 | Loss: 0.00001798
Iteration 84/1000 | Loss: 0.00001798
Iteration 85/1000 | Loss: 0.00001797
Iteration 86/1000 | Loss: 0.00001797
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001792
Iteration 117/1000 | Loss: 0.00001792
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001792
Iteration 121/1000 | Loss: 0.00001792
Iteration 122/1000 | Loss: 0.00001792
Iteration 123/1000 | Loss: 0.00001792
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001792
Iteration 127/1000 | Loss: 0.00001792
Iteration 128/1000 | Loss: 0.00001792
Iteration 129/1000 | Loss: 0.00001792
Iteration 130/1000 | Loss: 0.00001792
Iteration 131/1000 | Loss: 0.00001792
Iteration 132/1000 | Loss: 0.00001792
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001792
Iteration 135/1000 | Loss: 0.00001792
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7923366613104008e-05, 1.7923366613104008e-05, 1.7923366613104008e-05, 1.7923366613104008e-05, 1.7923366613104008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7923366613104008e-05

Optimization complete. Final v2v error: 3.5981976985931396 mm

Highest mean error: 3.8436145782470703 mm for frame 45

Lowest mean error: 3.4868197441101074 mm for frame 96

Saving results

Total time: 42.80161094665527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438809
Iteration 2/25 | Loss: 0.00138309
Iteration 3/25 | Loss: 0.00131751
Iteration 4/25 | Loss: 0.00130691
Iteration 5/25 | Loss: 0.00130476
Iteration 6/25 | Loss: 0.00130476
Iteration 7/25 | Loss: 0.00130476
Iteration 8/25 | Loss: 0.00130476
Iteration 9/25 | Loss: 0.00130476
Iteration 10/25 | Loss: 0.00130476
Iteration 11/25 | Loss: 0.00130476
Iteration 12/25 | Loss: 0.00130476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013047636020928621, 0.0013047636020928621, 0.0013047636020928621, 0.0013047636020928621, 0.0013047636020928621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013047636020928621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49219847
Iteration 2/25 | Loss: 0.00204708
Iteration 3/25 | Loss: 0.00204708
Iteration 4/25 | Loss: 0.00204708
Iteration 5/25 | Loss: 0.00204708
Iteration 6/25 | Loss: 0.00204708
Iteration 7/25 | Loss: 0.00204708
Iteration 8/25 | Loss: 0.00204708
Iteration 9/25 | Loss: 0.00204708
Iteration 10/25 | Loss: 0.00204708
Iteration 11/25 | Loss: 0.00204708
Iteration 12/25 | Loss: 0.00204708
Iteration 13/25 | Loss: 0.00204708
Iteration 14/25 | Loss: 0.00204708
Iteration 15/25 | Loss: 0.00204708
Iteration 16/25 | Loss: 0.00204708
Iteration 17/25 | Loss: 0.00204708
Iteration 18/25 | Loss: 0.00204708
Iteration 19/25 | Loss: 0.00204708
Iteration 20/25 | Loss: 0.00204708
Iteration 21/25 | Loss: 0.00204708
Iteration 22/25 | Loss: 0.00204708
Iteration 23/25 | Loss: 0.00204708
Iteration 24/25 | Loss: 0.00204708
Iteration 25/25 | Loss: 0.00204708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204708
Iteration 2/1000 | Loss: 0.00001996
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001319
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001249
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001131
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001123
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001121
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001118
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001104
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001094
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001094
Iteration 61/1000 | Loss: 0.00001093
Iteration 62/1000 | Loss: 0.00001093
Iteration 63/1000 | Loss: 0.00001093
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001092
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001091
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001090
Iteration 70/1000 | Loss: 0.00001090
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001088
Iteration 74/1000 | Loss: 0.00001088
Iteration 75/1000 | Loss: 0.00001087
Iteration 76/1000 | Loss: 0.00001087
Iteration 77/1000 | Loss: 0.00001087
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001086
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001086
Iteration 85/1000 | Loss: 0.00001086
Iteration 86/1000 | Loss: 0.00001086
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001086
Iteration 89/1000 | Loss: 0.00001085
Iteration 90/1000 | Loss: 0.00001085
Iteration 91/1000 | Loss: 0.00001085
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001085
Iteration 94/1000 | Loss: 0.00001085
Iteration 95/1000 | Loss: 0.00001084
Iteration 96/1000 | Loss: 0.00001084
Iteration 97/1000 | Loss: 0.00001084
Iteration 98/1000 | Loss: 0.00001084
Iteration 99/1000 | Loss: 0.00001083
Iteration 100/1000 | Loss: 0.00001083
Iteration 101/1000 | Loss: 0.00001083
Iteration 102/1000 | Loss: 0.00001083
Iteration 103/1000 | Loss: 0.00001083
Iteration 104/1000 | Loss: 0.00001083
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001081
Iteration 109/1000 | Loss: 0.00001081
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001081
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001079
Iteration 125/1000 | Loss: 0.00001079
Iteration 126/1000 | Loss: 0.00001079
Iteration 127/1000 | Loss: 0.00001079
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001077
Iteration 132/1000 | Loss: 0.00001077
Iteration 133/1000 | Loss: 0.00001077
Iteration 134/1000 | Loss: 0.00001077
Iteration 135/1000 | Loss: 0.00001077
Iteration 136/1000 | Loss: 0.00001077
Iteration 137/1000 | Loss: 0.00001077
Iteration 138/1000 | Loss: 0.00001077
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0770737389975693e-05, 1.0770737389975693e-05, 1.0770737389975693e-05, 1.0770737389975693e-05, 1.0770737389975693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0770737389975693e-05

Optimization complete. Final v2v error: 2.8824779987335205 mm

Highest mean error: 3.1417737007141113 mm for frame 44

Lowest mean error: 2.6719343662261963 mm for frame 25

Saving results

Total time: 45.20836520195007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831416
Iteration 2/25 | Loss: 0.00140175
Iteration 3/25 | Loss: 0.00133638
Iteration 4/25 | Loss: 0.00132204
Iteration 5/25 | Loss: 0.00131761
Iteration 6/25 | Loss: 0.00131761
Iteration 7/25 | Loss: 0.00131761
Iteration 8/25 | Loss: 0.00131761
Iteration 9/25 | Loss: 0.00131761
Iteration 10/25 | Loss: 0.00131761
Iteration 11/25 | Loss: 0.00131761
Iteration 12/25 | Loss: 0.00131761
Iteration 13/25 | Loss: 0.00131761
Iteration 14/25 | Loss: 0.00131761
Iteration 15/25 | Loss: 0.00131761
Iteration 16/25 | Loss: 0.00131761
Iteration 17/25 | Loss: 0.00131761
Iteration 18/25 | Loss: 0.00131761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001317606889642775, 0.001317606889642775, 0.001317606889642775, 0.001317606889642775, 0.001317606889642775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001317606889642775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25936806
Iteration 2/25 | Loss: 0.00206188
Iteration 3/25 | Loss: 0.00206187
Iteration 4/25 | Loss: 0.00206187
Iteration 5/25 | Loss: 0.00206187
Iteration 6/25 | Loss: 0.00206187
Iteration 7/25 | Loss: 0.00206187
Iteration 8/25 | Loss: 0.00206187
Iteration 9/25 | Loss: 0.00206187
Iteration 10/25 | Loss: 0.00206187
Iteration 11/25 | Loss: 0.00206187
Iteration 12/25 | Loss: 0.00206187
Iteration 13/25 | Loss: 0.00206187
Iteration 14/25 | Loss: 0.00206187
Iteration 15/25 | Loss: 0.00206187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020618659909814596, 0.0020618659909814596, 0.0020618659909814596, 0.0020618659909814596, 0.0020618659909814596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020618659909814596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206187
Iteration 2/1000 | Loss: 0.00002207
Iteration 3/1000 | Loss: 0.00001833
Iteration 4/1000 | Loss: 0.00001715
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001450
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001323
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001260
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001227
Iteration 59/1000 | Loss: 0.00001227
Iteration 60/1000 | Loss: 0.00001226
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001226
Iteration 63/1000 | Loss: 0.00001226
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001225
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001224
Iteration 70/1000 | Loss: 0.00001224
Iteration 71/1000 | Loss: 0.00001224
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001224
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001224
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001222
Iteration 93/1000 | Loss: 0.00001222
Iteration 94/1000 | Loss: 0.00001222
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001219
Iteration 123/1000 | Loss: 0.00001219
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001218
Iteration 133/1000 | Loss: 0.00001218
Iteration 134/1000 | Loss: 0.00001218
Iteration 135/1000 | Loss: 0.00001218
Iteration 136/1000 | Loss: 0.00001218
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001217
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001217
Iteration 144/1000 | Loss: 0.00001217
Iteration 145/1000 | Loss: 0.00001216
Iteration 146/1000 | Loss: 0.00001216
Iteration 147/1000 | Loss: 0.00001216
Iteration 148/1000 | Loss: 0.00001216
Iteration 149/1000 | Loss: 0.00001216
Iteration 150/1000 | Loss: 0.00001216
Iteration 151/1000 | Loss: 0.00001216
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001215
Iteration 155/1000 | Loss: 0.00001215
Iteration 156/1000 | Loss: 0.00001215
Iteration 157/1000 | Loss: 0.00001215
Iteration 158/1000 | Loss: 0.00001215
Iteration 159/1000 | Loss: 0.00001215
Iteration 160/1000 | Loss: 0.00001215
Iteration 161/1000 | Loss: 0.00001215
Iteration 162/1000 | Loss: 0.00001215
Iteration 163/1000 | Loss: 0.00001215
Iteration 164/1000 | Loss: 0.00001215
Iteration 165/1000 | Loss: 0.00001215
Iteration 166/1000 | Loss: 0.00001214
Iteration 167/1000 | Loss: 0.00001214
Iteration 168/1000 | Loss: 0.00001214
Iteration 169/1000 | Loss: 0.00001214
Iteration 170/1000 | Loss: 0.00001214
Iteration 171/1000 | Loss: 0.00001214
Iteration 172/1000 | Loss: 0.00001214
Iteration 173/1000 | Loss: 0.00001214
Iteration 174/1000 | Loss: 0.00001214
Iteration 175/1000 | Loss: 0.00001213
Iteration 176/1000 | Loss: 0.00001213
Iteration 177/1000 | Loss: 0.00001213
Iteration 178/1000 | Loss: 0.00001213
Iteration 179/1000 | Loss: 0.00001213
Iteration 180/1000 | Loss: 0.00001213
Iteration 181/1000 | Loss: 0.00001213
Iteration 182/1000 | Loss: 0.00001213
Iteration 183/1000 | Loss: 0.00001213
Iteration 184/1000 | Loss: 0.00001213
Iteration 185/1000 | Loss: 0.00001213
Iteration 186/1000 | Loss: 0.00001213
Iteration 187/1000 | Loss: 0.00001212
Iteration 188/1000 | Loss: 0.00001212
Iteration 189/1000 | Loss: 0.00001212
Iteration 190/1000 | Loss: 0.00001212
Iteration 191/1000 | Loss: 0.00001212
Iteration 192/1000 | Loss: 0.00001212
Iteration 193/1000 | Loss: 0.00001212
Iteration 194/1000 | Loss: 0.00001212
Iteration 195/1000 | Loss: 0.00001212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.2123359738325235e-05, 1.2123359738325235e-05, 1.2123359738325235e-05, 1.2123359738325235e-05, 1.2123359738325235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2123359738325235e-05

Optimization complete. Final v2v error: 3.03275728225708 mm

Highest mean error: 3.3142311573028564 mm for frame 115

Lowest mean error: 2.9156179428100586 mm for frame 196

Saving results

Total time: 51.620185136795044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062425
Iteration 2/25 | Loss: 0.00268222
Iteration 3/25 | Loss: 0.00205586
Iteration 4/25 | Loss: 0.00227955
Iteration 5/25 | Loss: 0.00213547
Iteration 6/25 | Loss: 0.00209247
Iteration 7/25 | Loss: 0.00190500
Iteration 8/25 | Loss: 0.00177586
Iteration 9/25 | Loss: 0.00171412
Iteration 10/25 | Loss: 0.00170529
Iteration 11/25 | Loss: 0.00170683
Iteration 12/25 | Loss: 0.00168316
Iteration 13/25 | Loss: 0.00164575
Iteration 14/25 | Loss: 0.00162289
Iteration 15/25 | Loss: 0.00161895
Iteration 16/25 | Loss: 0.00162039
Iteration 17/25 | Loss: 0.00161862
Iteration 18/25 | Loss: 0.00161271
Iteration 19/25 | Loss: 0.00160573
Iteration 20/25 | Loss: 0.00160273
Iteration 21/25 | Loss: 0.00159163
Iteration 22/25 | Loss: 0.00158589
Iteration 23/25 | Loss: 0.00159818
Iteration 24/25 | Loss: 0.00160757
Iteration 25/25 | Loss: 0.00159220

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60422283
Iteration 2/25 | Loss: 0.00254971
Iteration 3/25 | Loss: 0.00254970
Iteration 4/25 | Loss: 0.00254970
Iteration 5/25 | Loss: 0.00254970
Iteration 6/25 | Loss: 0.00254970
Iteration 7/25 | Loss: 0.00254970
Iteration 8/25 | Loss: 0.00254970
Iteration 9/25 | Loss: 0.00254970
Iteration 10/25 | Loss: 0.00254970
Iteration 11/25 | Loss: 0.00254970
Iteration 12/25 | Loss: 0.00254970
Iteration 13/25 | Loss: 0.00254970
Iteration 14/25 | Loss: 0.00254970
Iteration 15/25 | Loss: 0.00254970
Iteration 16/25 | Loss: 0.00254970
Iteration 17/25 | Loss: 0.00254970
Iteration 18/25 | Loss: 0.00254970
Iteration 19/25 | Loss: 0.00254970
Iteration 20/25 | Loss: 0.00254970
Iteration 21/25 | Loss: 0.00254970
Iteration 22/25 | Loss: 0.00254970
Iteration 23/25 | Loss: 0.00254970
Iteration 24/25 | Loss: 0.00254970
Iteration 25/25 | Loss: 0.00254970

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254970
Iteration 2/1000 | Loss: 0.00058288
Iteration 3/1000 | Loss: 0.00066227
Iteration 4/1000 | Loss: 0.00077225
Iteration 5/1000 | Loss: 0.00067060
Iteration 6/1000 | Loss: 0.00082147
Iteration 7/1000 | Loss: 0.00068147
Iteration 8/1000 | Loss: 0.00061007
Iteration 9/1000 | Loss: 0.00059681
Iteration 10/1000 | Loss: 0.00041664
Iteration 11/1000 | Loss: 0.00070092
Iteration 12/1000 | Loss: 0.00058406
Iteration 13/1000 | Loss: 0.00061061
Iteration 14/1000 | Loss: 0.00078367
Iteration 15/1000 | Loss: 0.00109832
Iteration 16/1000 | Loss: 0.00143729
Iteration 17/1000 | Loss: 0.00131540
Iteration 18/1000 | Loss: 0.00090163
Iteration 19/1000 | Loss: 0.00069197
Iteration 20/1000 | Loss: 0.00085300
Iteration 21/1000 | Loss: 0.00033818
Iteration 22/1000 | Loss: 0.00018231
Iteration 23/1000 | Loss: 0.00013345
Iteration 24/1000 | Loss: 0.00055033
Iteration 25/1000 | Loss: 0.00039040
Iteration 26/1000 | Loss: 0.00040264
Iteration 27/1000 | Loss: 0.00039882
Iteration 28/1000 | Loss: 0.00033867
Iteration 29/1000 | Loss: 0.00020145
Iteration 30/1000 | Loss: 0.00037056
Iteration 31/1000 | Loss: 0.00063731
Iteration 32/1000 | Loss: 0.00074229
Iteration 33/1000 | Loss: 0.00051936
Iteration 34/1000 | Loss: 0.00038162
Iteration 35/1000 | Loss: 0.00030722
Iteration 36/1000 | Loss: 0.00024831
Iteration 37/1000 | Loss: 0.00021717
Iteration 38/1000 | Loss: 0.00025731
Iteration 39/1000 | Loss: 0.00062409
Iteration 40/1000 | Loss: 0.00040263
Iteration 41/1000 | Loss: 0.00014603
Iteration 42/1000 | Loss: 0.00015375
Iteration 43/1000 | Loss: 0.00011967
Iteration 44/1000 | Loss: 0.00009393
Iteration 45/1000 | Loss: 0.00010509
Iteration 46/1000 | Loss: 0.00015613
Iteration 47/1000 | Loss: 0.00048480
Iteration 48/1000 | Loss: 0.00037961
Iteration 49/1000 | Loss: 0.00039934
Iteration 50/1000 | Loss: 0.00038002
Iteration 51/1000 | Loss: 0.00021435
Iteration 52/1000 | Loss: 0.00031553
Iteration 53/1000 | Loss: 0.00022539
Iteration 54/1000 | Loss: 0.00010810
Iteration 55/1000 | Loss: 0.00017599
Iteration 56/1000 | Loss: 0.00011494
Iteration 57/1000 | Loss: 0.00018822
Iteration 58/1000 | Loss: 0.00008802
Iteration 59/1000 | Loss: 0.00010694
Iteration 60/1000 | Loss: 0.00008913
Iteration 61/1000 | Loss: 0.00017182
Iteration 62/1000 | Loss: 0.00021705
Iteration 63/1000 | Loss: 0.00017595
Iteration 64/1000 | Loss: 0.00011649
Iteration 65/1000 | Loss: 0.00013816
Iteration 66/1000 | Loss: 0.00013939
Iteration 67/1000 | Loss: 0.00017107
Iteration 68/1000 | Loss: 0.00084035
Iteration 69/1000 | Loss: 0.00060206
Iteration 70/1000 | Loss: 0.00015482
Iteration 71/1000 | Loss: 0.00017354
Iteration 72/1000 | Loss: 0.00060210
Iteration 73/1000 | Loss: 0.00030962
Iteration 74/1000 | Loss: 0.00015511
Iteration 75/1000 | Loss: 0.00012183
Iteration 76/1000 | Loss: 0.00030509
Iteration 77/1000 | Loss: 0.00019412
Iteration 78/1000 | Loss: 0.00020081
Iteration 79/1000 | Loss: 0.00021511
Iteration 80/1000 | Loss: 0.00028824
Iteration 81/1000 | Loss: 0.00022415
Iteration 82/1000 | Loss: 0.00042918
Iteration 83/1000 | Loss: 0.00057068
Iteration 84/1000 | Loss: 0.00039395
Iteration 85/1000 | Loss: 0.00044799
Iteration 86/1000 | Loss: 0.00036355
Iteration 87/1000 | Loss: 0.00037696
Iteration 88/1000 | Loss: 0.00036898
Iteration 89/1000 | Loss: 0.00039608
Iteration 90/1000 | Loss: 0.00038452
Iteration 91/1000 | Loss: 0.00062829
Iteration 92/1000 | Loss: 0.00038568
Iteration 93/1000 | Loss: 0.00060386
Iteration 94/1000 | Loss: 0.00072820
Iteration 95/1000 | Loss: 0.00052798
Iteration 96/1000 | Loss: 0.00031662
Iteration 97/1000 | Loss: 0.00034267
Iteration 98/1000 | Loss: 0.00034935
Iteration 99/1000 | Loss: 0.00019622
Iteration 100/1000 | Loss: 0.00031063
Iteration 101/1000 | Loss: 0.00024476
Iteration 102/1000 | Loss: 0.00031914
Iteration 103/1000 | Loss: 0.00018045
Iteration 104/1000 | Loss: 0.00039867
Iteration 105/1000 | Loss: 0.00044867
Iteration 106/1000 | Loss: 0.00010434
Iteration 107/1000 | Loss: 0.00037259
Iteration 108/1000 | Loss: 0.00035958
Iteration 109/1000 | Loss: 0.00011756
Iteration 110/1000 | Loss: 0.00021883
Iteration 111/1000 | Loss: 0.00025915
Iteration 112/1000 | Loss: 0.00021736
Iteration 113/1000 | Loss: 0.00033811
Iteration 114/1000 | Loss: 0.00028763
Iteration 115/1000 | Loss: 0.00034248
Iteration 116/1000 | Loss: 0.00030090
Iteration 117/1000 | Loss: 0.00024285
Iteration 118/1000 | Loss: 0.00019158
Iteration 119/1000 | Loss: 0.00067277
Iteration 120/1000 | Loss: 0.00042117
Iteration 121/1000 | Loss: 0.00032377
Iteration 122/1000 | Loss: 0.00034714
Iteration 123/1000 | Loss: 0.00061859
Iteration 124/1000 | Loss: 0.00025692
Iteration 125/1000 | Loss: 0.00042983
Iteration 126/1000 | Loss: 0.00042961
Iteration 127/1000 | Loss: 0.00048259
Iteration 128/1000 | Loss: 0.00024404
Iteration 129/1000 | Loss: 0.00016608
Iteration 130/1000 | Loss: 0.00042652
Iteration 131/1000 | Loss: 0.00025500
Iteration 132/1000 | Loss: 0.00026638
Iteration 133/1000 | Loss: 0.00036763
Iteration 134/1000 | Loss: 0.00045491
Iteration 135/1000 | Loss: 0.00023093
Iteration 136/1000 | Loss: 0.00029812
Iteration 137/1000 | Loss: 0.00014521
Iteration 138/1000 | Loss: 0.00007806
Iteration 139/1000 | Loss: 0.00024449
Iteration 140/1000 | Loss: 0.00020401
Iteration 141/1000 | Loss: 0.00010294
Iteration 142/1000 | Loss: 0.00014801
Iteration 143/1000 | Loss: 0.00029483
Iteration 144/1000 | Loss: 0.00022675
Iteration 145/1000 | Loss: 0.00021550
Iteration 146/1000 | Loss: 0.00022210
Iteration 147/1000 | Loss: 0.00019032
Iteration 148/1000 | Loss: 0.00019549
Iteration 149/1000 | Loss: 0.00017911
Iteration 150/1000 | Loss: 0.00019837
Iteration 151/1000 | Loss: 0.00022187
Iteration 152/1000 | Loss: 0.00018856
Iteration 153/1000 | Loss: 0.00023986
Iteration 154/1000 | Loss: 0.00022770
Iteration 155/1000 | Loss: 0.00027206
Iteration 156/1000 | Loss: 0.00039956
Iteration 157/1000 | Loss: 0.00032119
Iteration 158/1000 | Loss: 0.00019094
Iteration 159/1000 | Loss: 0.00018947
Iteration 160/1000 | Loss: 0.00041057
Iteration 161/1000 | Loss: 0.00018388
Iteration 162/1000 | Loss: 0.00014101
Iteration 163/1000 | Loss: 0.00013572
Iteration 164/1000 | Loss: 0.00026103
Iteration 165/1000 | Loss: 0.00028737
Iteration 166/1000 | Loss: 0.00029327
Iteration 167/1000 | Loss: 0.00027305
Iteration 168/1000 | Loss: 0.00024277
Iteration 169/1000 | Loss: 0.00012803
Iteration 170/1000 | Loss: 0.00017787
Iteration 171/1000 | Loss: 0.00012306
Iteration 172/1000 | Loss: 0.00034071
Iteration 173/1000 | Loss: 0.00024265
Iteration 174/1000 | Loss: 0.00022984
Iteration 175/1000 | Loss: 0.00022926
Iteration 176/1000 | Loss: 0.00022430
Iteration 177/1000 | Loss: 0.00020875
Iteration 178/1000 | Loss: 0.00019882
Iteration 179/1000 | Loss: 0.00019535
Iteration 180/1000 | Loss: 0.00043224
Iteration 181/1000 | Loss: 0.00030493
Iteration 182/1000 | Loss: 0.00038966
Iteration 183/1000 | Loss: 0.00028572
Iteration 184/1000 | Loss: 0.00029371
Iteration 185/1000 | Loss: 0.00024550
Iteration 186/1000 | Loss: 0.00018100
Iteration 187/1000 | Loss: 0.00024982
Iteration 188/1000 | Loss: 0.00017305
Iteration 189/1000 | Loss: 0.00031620
Iteration 190/1000 | Loss: 0.00028574
Iteration 191/1000 | Loss: 0.00020385
Iteration 192/1000 | Loss: 0.00014651
Iteration 193/1000 | Loss: 0.00037868
Iteration 194/1000 | Loss: 0.00018723
Iteration 195/1000 | Loss: 0.00026961
Iteration 196/1000 | Loss: 0.00026912
Iteration 197/1000 | Loss: 0.00009420
Iteration 198/1000 | Loss: 0.00021755
Iteration 199/1000 | Loss: 0.00020026
Iteration 200/1000 | Loss: 0.00013090
Iteration 201/1000 | Loss: 0.00017929
Iteration 202/1000 | Loss: 0.00018643
Iteration 203/1000 | Loss: 0.00011489
Iteration 204/1000 | Loss: 0.00012757
Iteration 205/1000 | Loss: 0.00027443
Iteration 206/1000 | Loss: 0.00014048
Iteration 207/1000 | Loss: 0.00010833
Iteration 208/1000 | Loss: 0.00008136
Iteration 209/1000 | Loss: 0.00007367
Iteration 210/1000 | Loss: 0.00021386
Iteration 211/1000 | Loss: 0.00049094
Iteration 212/1000 | Loss: 0.00030057
Iteration 213/1000 | Loss: 0.00028670
Iteration 214/1000 | Loss: 0.00024730
Iteration 215/1000 | Loss: 0.00025415
Iteration 216/1000 | Loss: 0.00021721
Iteration 217/1000 | Loss: 0.00033488
Iteration 218/1000 | Loss: 0.00026471
Iteration 219/1000 | Loss: 0.00025423
Iteration 220/1000 | Loss: 0.00028239
Iteration 221/1000 | Loss: 0.00026368
Iteration 222/1000 | Loss: 0.00024542
Iteration 223/1000 | Loss: 0.00023986
Iteration 224/1000 | Loss: 0.00023922
Iteration 225/1000 | Loss: 0.00027230
Iteration 226/1000 | Loss: 0.00025088
Iteration 227/1000 | Loss: 0.00018020
Iteration 228/1000 | Loss: 0.00010347
Iteration 229/1000 | Loss: 0.00010900
Iteration 230/1000 | Loss: 0.00010214
Iteration 231/1000 | Loss: 0.00011128
Iteration 232/1000 | Loss: 0.00011395
Iteration 233/1000 | Loss: 0.00010327
Iteration 234/1000 | Loss: 0.00010440
Iteration 235/1000 | Loss: 0.00010780
Iteration 236/1000 | Loss: 0.00027921
Iteration 237/1000 | Loss: 0.00024402
Iteration 238/1000 | Loss: 0.00008390
Iteration 239/1000 | Loss: 0.00009184
Iteration 240/1000 | Loss: 0.00008491
Iteration 241/1000 | Loss: 0.00032990
Iteration 242/1000 | Loss: 0.00010717
Iteration 243/1000 | Loss: 0.00009583
Iteration 244/1000 | Loss: 0.00009032
Iteration 245/1000 | Loss: 0.00009613
Iteration 246/1000 | Loss: 0.00009846
Iteration 247/1000 | Loss: 0.00009505
Iteration 248/1000 | Loss: 0.00011312
Iteration 249/1000 | Loss: 0.00010737
Iteration 250/1000 | Loss: 0.00010331
Iteration 251/1000 | Loss: 0.00009605
Iteration 252/1000 | Loss: 0.00006912
Iteration 253/1000 | Loss: 0.00005616
Iteration 254/1000 | Loss: 0.00008710
Iteration 255/1000 | Loss: 0.00008831
Iteration 256/1000 | Loss: 0.00009490
Iteration 257/1000 | Loss: 0.00010816
Iteration 258/1000 | Loss: 0.00009074
Iteration 259/1000 | Loss: 0.00009017
Iteration 260/1000 | Loss: 0.00009854
Iteration 261/1000 | Loss: 0.00009928
Iteration 262/1000 | Loss: 0.00009477
Iteration 263/1000 | Loss: 0.00009666
Iteration 264/1000 | Loss: 0.00009402
Iteration 265/1000 | Loss: 0.00010068
Iteration 266/1000 | Loss: 0.00009403
Iteration 267/1000 | Loss: 0.00009751
Iteration 268/1000 | Loss: 0.00009547
Iteration 269/1000 | Loss: 0.00011794
Iteration 270/1000 | Loss: 0.00011689
Iteration 271/1000 | Loss: 0.00009466
Iteration 272/1000 | Loss: 0.00009554
Iteration 273/1000 | Loss: 0.00009970
Iteration 274/1000 | Loss: 0.00009311
Iteration 275/1000 | Loss: 0.00010263
Iteration 276/1000 | Loss: 0.00009176
Iteration 277/1000 | Loss: 0.00009989
Iteration 278/1000 | Loss: 0.00009255
Iteration 279/1000 | Loss: 0.00010261
Iteration 280/1000 | Loss: 0.00010297
Iteration 281/1000 | Loss: 0.00009882
Iteration 282/1000 | Loss: 0.00009645
Iteration 283/1000 | Loss: 0.00010959
Iteration 284/1000 | Loss: 0.00010818
Iteration 285/1000 | Loss: 0.00010802
Iteration 286/1000 | Loss: 0.00009700
Iteration 287/1000 | Loss: 0.00010539
Iteration 288/1000 | Loss: 0.00009456
Iteration 289/1000 | Loss: 0.00006277
Iteration 290/1000 | Loss: 0.00007869
Iteration 291/1000 | Loss: 0.00009998
Iteration 292/1000 | Loss: 0.00009586
Iteration 293/1000 | Loss: 0.00010492
Iteration 294/1000 | Loss: 0.00006171
Iteration 295/1000 | Loss: 0.00011711
Iteration 296/1000 | Loss: 0.00009397
Iteration 297/1000 | Loss: 0.00010301
Iteration 298/1000 | Loss: 0.00009221
Iteration 299/1000 | Loss: 0.00007951
Iteration 300/1000 | Loss: 0.00009624
Iteration 301/1000 | Loss: 0.00006018
Iteration 302/1000 | Loss: 0.00006944
Iteration 303/1000 | Loss: 0.00005441
Iteration 304/1000 | Loss: 0.00006431
Iteration 305/1000 | Loss: 0.00004646
Iteration 306/1000 | Loss: 0.00006628
Iteration 307/1000 | Loss: 0.00006181
Iteration 308/1000 | Loss: 0.00007035
Iteration 309/1000 | Loss: 0.00005516
Iteration 310/1000 | Loss: 0.00005368
Iteration 311/1000 | Loss: 0.00006353
Iteration 312/1000 | Loss: 0.00006793
Iteration 313/1000 | Loss: 0.00006664
Iteration 314/1000 | Loss: 0.00006391
Iteration 315/1000 | Loss: 0.00005803
Iteration 316/1000 | Loss: 0.00007036
Iteration 317/1000 | Loss: 0.00005985
Iteration 318/1000 | Loss: 0.00006505
Iteration 319/1000 | Loss: 0.00005955
Iteration 320/1000 | Loss: 0.00006317
Iteration 321/1000 | Loss: 0.00005907
Iteration 322/1000 | Loss: 0.00006749
Iteration 323/1000 | Loss: 0.00005577
Iteration 324/1000 | Loss: 0.00005559
Iteration 325/1000 | Loss: 0.00006042
Iteration 326/1000 | Loss: 0.00006896
Iteration 327/1000 | Loss: 0.00006329
Iteration 328/1000 | Loss: 0.00007016
Iteration 329/1000 | Loss: 0.00005993
Iteration 330/1000 | Loss: 0.00006410
Iteration 331/1000 | Loss: 0.00005944
Iteration 332/1000 | Loss: 0.00006464
Iteration 333/1000 | Loss: 0.00006193
Iteration 334/1000 | Loss: 0.00006666
Iteration 335/1000 | Loss: 0.00006209
Iteration 336/1000 | Loss: 0.00006828
Iteration 337/1000 | Loss: 0.00006635
Iteration 338/1000 | Loss: 0.00006779
Iteration 339/1000 | Loss: 0.00006824
Iteration 340/1000 | Loss: 0.00006605
Iteration 341/1000 | Loss: 0.00006838
Iteration 342/1000 | Loss: 0.00006610
Iteration 343/1000 | Loss: 0.00006035
Iteration 344/1000 | Loss: 0.00006519
Iteration 345/1000 | Loss: 0.00005939
Iteration 346/1000 | Loss: 0.00006286
Iteration 347/1000 | Loss: 0.00006313
Iteration 348/1000 | Loss: 0.00007232
Iteration 349/1000 | Loss: 0.00006311
Iteration 350/1000 | Loss: 0.00006559
Iteration 351/1000 | Loss: 0.00017949
Iteration 352/1000 | Loss: 0.00014510
Iteration 353/1000 | Loss: 0.00006654
Iteration 354/1000 | Loss: 0.00006689
Iteration 355/1000 | Loss: 0.00006349
Iteration 356/1000 | Loss: 0.00006566
Iteration 357/1000 | Loss: 0.00015368
Iteration 358/1000 | Loss: 0.00014371
Iteration 359/1000 | Loss: 0.00006430
Iteration 360/1000 | Loss: 0.00006550
Iteration 361/1000 | Loss: 0.00006308
Iteration 362/1000 | Loss: 0.00006451
Iteration 363/1000 | Loss: 0.00006570
Iteration 364/1000 | Loss: 0.00006694
Iteration 365/1000 | Loss: 0.00005781
Iteration 366/1000 | Loss: 0.00006201
Iteration 367/1000 | Loss: 0.00006539
Iteration 368/1000 | Loss: 0.00006071
Iteration 369/1000 | Loss: 0.00011846
Iteration 370/1000 | Loss: 0.00014469
Iteration 371/1000 | Loss: 0.00005842
Iteration 372/1000 | Loss: 0.00014478
Iteration 373/1000 | Loss: 0.00014479
Iteration 374/1000 | Loss: 0.00006456
Iteration 375/1000 | Loss: 0.00006335
Iteration 376/1000 | Loss: 0.00006407
Iteration 377/1000 | Loss: 0.00006360
Iteration 378/1000 | Loss: 0.00006445
Iteration 379/1000 | Loss: 0.00005743
Iteration 380/1000 | Loss: 0.00006543
Iteration 381/1000 | Loss: 0.00006675
Iteration 382/1000 | Loss: 0.00006443
Iteration 383/1000 | Loss: 0.00005717
Iteration 384/1000 | Loss: 0.00007428
Iteration 385/1000 | Loss: 0.00007468
Iteration 386/1000 | Loss: 0.00006205
Iteration 387/1000 | Loss: 0.00006439
Iteration 388/1000 | Loss: 0.00006185
Iteration 389/1000 | Loss: 0.00006012
Iteration 390/1000 | Loss: 0.00007088
Iteration 391/1000 | Loss: 0.00006614
Iteration 392/1000 | Loss: 0.00006648
Iteration 393/1000 | Loss: 0.00006658
Iteration 394/1000 | Loss: 0.00007049
Iteration 395/1000 | Loss: 0.00006325
Iteration 396/1000 | Loss: 0.00006964
Iteration 397/1000 | Loss: 0.00005803
Iteration 398/1000 | Loss: 0.00006628
Iteration 399/1000 | Loss: 0.00005663
Iteration 400/1000 | Loss: 0.00006978
Iteration 401/1000 | Loss: 0.00006341
Iteration 402/1000 | Loss: 0.00006359
Iteration 403/1000 | Loss: 0.00006704
Iteration 404/1000 | Loss: 0.00005936
Iteration 405/1000 | Loss: 0.00006353
Iteration 406/1000 | Loss: 0.00006285
Iteration 407/1000 | Loss: 0.00006796
Iteration 408/1000 | Loss: 0.00006314
Iteration 409/1000 | Loss: 0.00005903
Iteration 410/1000 | Loss: 0.00006921
Iteration 411/1000 | Loss: 0.00007089
Iteration 412/1000 | Loss: 0.00006831
Iteration 413/1000 | Loss: 0.00006878
Iteration 414/1000 | Loss: 0.00007004
Iteration 415/1000 | Loss: 0.00005870
Iteration 416/1000 | Loss: 0.00006197
Iteration 417/1000 | Loss: 0.00006548
Iteration 418/1000 | Loss: 0.00006782
Iteration 419/1000 | Loss: 0.00006387
Iteration 420/1000 | Loss: 0.00005555
Iteration 421/1000 | Loss: 0.00005641
Iteration 422/1000 | Loss: 0.00005591
Iteration 423/1000 | Loss: 0.00005797
Iteration 424/1000 | Loss: 0.00006376
Iteration 425/1000 | Loss: 0.00006646
Iteration 426/1000 | Loss: 0.00006201
Iteration 427/1000 | Loss: 0.00006232
Iteration 428/1000 | Loss: 0.00005602
Iteration 429/1000 | Loss: 0.00007185
Iteration 430/1000 | Loss: 0.00005809
Iteration 431/1000 | Loss: 0.00006439
Iteration 432/1000 | Loss: 0.00005978
Iteration 433/1000 | Loss: 0.00006740
Iteration 434/1000 | Loss: 0.00005632
Iteration 435/1000 | Loss: 0.00005858
Iteration 436/1000 | Loss: 0.00005712
Iteration 437/1000 | Loss: 0.00006622
Iteration 438/1000 | Loss: 0.00006010
Iteration 439/1000 | Loss: 0.00005672
Iteration 440/1000 | Loss: 0.00005894
Iteration 441/1000 | Loss: 0.00004721
Iteration 442/1000 | Loss: 0.00005612
Iteration 443/1000 | Loss: 0.00005736
Iteration 444/1000 | Loss: 0.00006429
Iteration 445/1000 | Loss: 0.00005585
Iteration 446/1000 | Loss: 0.00005302
Iteration 447/1000 | Loss: 0.00006336
Iteration 448/1000 | Loss: 0.00005674
Iteration 449/1000 | Loss: 0.00003463
Iteration 450/1000 | Loss: 0.00004857
Iteration 451/1000 | Loss: 0.00006668
Iteration 452/1000 | Loss: 0.00006197
Iteration 453/1000 | Loss: 0.00005236
Iteration 454/1000 | Loss: 0.00005977
Iteration 455/1000 | Loss: 0.00005414
Iteration 456/1000 | Loss: 0.00005927
Iteration 457/1000 | Loss: 0.00006098
Iteration 458/1000 | Loss: 0.00005777
Iteration 459/1000 | Loss: 0.00007277
Iteration 460/1000 | Loss: 0.00006079
Iteration 461/1000 | Loss: 0.00006057
Iteration 462/1000 | Loss: 0.00006144
Iteration 463/1000 | Loss: 0.00006154
Iteration 464/1000 | Loss: 0.00005461
Iteration 465/1000 | Loss: 0.00007757
Iteration 466/1000 | Loss: 0.00005731
Iteration 467/1000 | Loss: 0.00007581
Iteration 468/1000 | Loss: 0.00005925
Iteration 469/1000 | Loss: 0.00005838
Iteration 470/1000 | Loss: 0.00005950
Iteration 471/1000 | Loss: 0.00006288
Iteration 472/1000 | Loss: 0.00005601
Iteration 473/1000 | Loss: 0.00005745
Iteration 474/1000 | Loss: 0.00005402
Iteration 475/1000 | Loss: 0.00006690
Iteration 476/1000 | Loss: 0.00006665
Iteration 477/1000 | Loss: 0.00006056
Iteration 478/1000 | Loss: 0.00006045
Iteration 479/1000 | Loss: 0.00004707
Iteration 480/1000 | Loss: 0.00005880
Iteration 481/1000 | Loss: 0.00003727
Iteration 482/1000 | Loss: 0.00006438
Iteration 483/1000 | Loss: 0.00006026
Iteration 484/1000 | Loss: 0.00004845
Iteration 485/1000 | Loss: 0.00006570
Iteration 486/1000 | Loss: 0.00007336
Iteration 487/1000 | Loss: 0.00005851
Iteration 488/1000 | Loss: 0.00009296
Iteration 489/1000 | Loss: 0.00005843
Iteration 490/1000 | Loss: 0.00008355
Iteration 491/1000 | Loss: 0.00005825
Iteration 492/1000 | Loss: 0.00008215
Iteration 493/1000 | Loss: 0.00005078
Iteration 494/1000 | Loss: 0.00007405
Iteration 495/1000 | Loss: 0.00007521
Iteration 496/1000 | Loss: 0.00006614
Iteration 497/1000 | Loss: 0.00006220
Iteration 498/1000 | Loss: 0.00005773
Iteration 499/1000 | Loss: 0.00006090
Iteration 500/1000 | Loss: 0.00006302
Iteration 501/1000 | Loss: 0.00004948
Iteration 502/1000 | Loss: 0.00004937
Iteration 503/1000 | Loss: 0.00006092
Iteration 504/1000 | Loss: 0.00006818
Iteration 505/1000 | Loss: 0.00006538
Iteration 506/1000 | Loss: 0.00006714
Iteration 507/1000 | Loss: 0.00005183
Iteration 508/1000 | Loss: 0.00006936
Iteration 509/1000 | Loss: 0.00005830
Iteration 510/1000 | Loss: 0.00006625
Iteration 511/1000 | Loss: 0.00006561
Iteration 512/1000 | Loss: 0.00006144
Iteration 513/1000 | Loss: 0.00005734
Iteration 514/1000 | Loss: 0.00007358
Iteration 515/1000 | Loss: 0.00005817
Iteration 516/1000 | Loss: 0.00006633
Iteration 517/1000 | Loss: 0.00005388
Iteration 518/1000 | Loss: 0.00006693
Iteration 519/1000 | Loss: 0.00006488
Iteration 520/1000 | Loss: 0.00005172
Iteration 521/1000 | Loss: 0.00004675
Iteration 522/1000 | Loss: 0.00003815
Iteration 523/1000 | Loss: 0.00005893
Iteration 524/1000 | Loss: 0.00019495
Iteration 525/1000 | Loss: 0.00014958
Iteration 526/1000 | Loss: 0.00006680
Iteration 527/1000 | Loss: 0.00006432
Iteration 528/1000 | Loss: 0.00006244
Iteration 529/1000 | Loss: 0.00006876
Iteration 530/1000 | Loss: 0.00006467
Iteration 531/1000 | Loss: 0.00007792
Iteration 532/1000 | Loss: 0.00006865
Iteration 533/1000 | Loss: 0.00006864
Iteration 534/1000 | Loss: 0.00006754
Iteration 535/1000 | Loss: 0.00007091
Iteration 536/1000 | Loss: 0.00006071
Iteration 537/1000 | Loss: 0.00006911
Iteration 538/1000 | Loss: 0.00005901
Iteration 539/1000 | Loss: 0.00006933
Iteration 540/1000 | Loss: 0.00006742
Iteration 541/1000 | Loss: 0.00006840
Iteration 542/1000 | Loss: 0.00006544
Iteration 543/1000 | Loss: 0.00007006
Iteration 544/1000 | Loss: 0.00006091
Iteration 545/1000 | Loss: 0.00005882
Iteration 546/1000 | Loss: 0.00004064
Iteration 547/1000 | Loss: 0.00004392
Iteration 548/1000 | Loss: 0.00006131
Iteration 549/1000 | Loss: 0.00006609
Iteration 550/1000 | Loss: 0.00004550
Iteration 551/1000 | Loss: 0.00005081
Iteration 552/1000 | Loss: 0.00005520
Iteration 553/1000 | Loss: 0.00007599
Iteration 554/1000 | Loss: 0.00005883
Iteration 555/1000 | Loss: 0.00006672
Iteration 556/1000 | Loss: 0.00006406
Iteration 557/1000 | Loss: 0.00006665
Iteration 558/1000 | Loss: 0.00005860
Iteration 559/1000 | Loss: 0.00006357
Iteration 560/1000 | Loss: 0.00006662
Iteration 561/1000 | Loss: 0.00006672
Iteration 562/1000 | Loss: 0.00006515
Iteration 563/1000 | Loss: 0.00008566
Iteration 564/1000 | Loss: 0.00007344
Iteration 565/1000 | Loss: 0.00006309
Iteration 566/1000 | Loss: 0.00006162
Iteration 567/1000 | Loss: 0.00005572
Iteration 568/1000 | Loss: 0.00004928
Iteration 569/1000 | Loss: 0.00014087
Iteration 570/1000 | Loss: 0.00012376
Iteration 571/1000 | Loss: 0.00002846
Iteration 572/1000 | Loss: 0.00002679
Iteration 573/1000 | Loss: 0.00002586
Iteration 574/1000 | Loss: 0.00002511
Iteration 575/1000 | Loss: 0.00002470
Iteration 576/1000 | Loss: 0.00002429
Iteration 577/1000 | Loss: 0.00002405
Iteration 578/1000 | Loss: 0.00002379
Iteration 579/1000 | Loss: 0.00002353
Iteration 580/1000 | Loss: 0.00002336
Iteration 581/1000 | Loss: 0.00002323
Iteration 582/1000 | Loss: 0.00002320
Iteration 583/1000 | Loss: 0.00002320
Iteration 584/1000 | Loss: 0.00002316
Iteration 585/1000 | Loss: 0.00002316
Iteration 586/1000 | Loss: 0.00002306
Iteration 587/1000 | Loss: 0.00002305
Iteration 588/1000 | Loss: 0.00002304
Iteration 589/1000 | Loss: 0.00002303
Iteration 590/1000 | Loss: 0.00002302
Iteration 591/1000 | Loss: 0.00002302
Iteration 592/1000 | Loss: 0.00002301
Iteration 593/1000 | Loss: 0.00002301
Iteration 594/1000 | Loss: 0.00002301
Iteration 595/1000 | Loss: 0.00002301
Iteration 596/1000 | Loss: 0.00002300
Iteration 597/1000 | Loss: 0.00002300
Iteration 598/1000 | Loss: 0.00002300
Iteration 599/1000 | Loss: 0.00002297
Iteration 600/1000 | Loss: 0.00002297
Iteration 601/1000 | Loss: 0.00002297
Iteration 602/1000 | Loss: 0.00002297
Iteration 603/1000 | Loss: 0.00002296
Iteration 604/1000 | Loss: 0.00002296
Iteration 605/1000 | Loss: 0.00002296
Iteration 606/1000 | Loss: 0.00002295
Iteration 607/1000 | Loss: 0.00002295
Iteration 608/1000 | Loss: 0.00002295
Iteration 609/1000 | Loss: 0.00002295
Iteration 610/1000 | Loss: 0.00002295
Iteration 611/1000 | Loss: 0.00013543
Iteration 612/1000 | Loss: 0.00013543
Iteration 613/1000 | Loss: 0.00013542
Iteration 614/1000 | Loss: 0.00002854
Iteration 615/1000 | Loss: 0.00002597
Iteration 616/1000 | Loss: 0.00002447
Iteration 617/1000 | Loss: 0.00002387
Iteration 618/1000 | Loss: 0.00002360
Iteration 619/1000 | Loss: 0.00002351
Iteration 620/1000 | Loss: 0.00002346
Iteration 621/1000 | Loss: 0.00002346
Iteration 622/1000 | Loss: 0.00002346
Iteration 623/1000 | Loss: 0.00002346
Iteration 624/1000 | Loss: 0.00002345
Iteration 625/1000 | Loss: 0.00002345
Iteration 626/1000 | Loss: 0.00002345
Iteration 627/1000 | Loss: 0.00002344
Iteration 628/1000 | Loss: 0.00002342
Iteration 629/1000 | Loss: 0.00002338
Iteration 630/1000 | Loss: 0.00002337
Iteration 631/1000 | Loss: 0.00002337
Iteration 632/1000 | Loss: 0.00002337
Iteration 633/1000 | Loss: 0.00002336
Iteration 634/1000 | Loss: 0.00002336
Iteration 635/1000 | Loss: 0.00002335
Iteration 636/1000 | Loss: 0.00002335
Iteration 637/1000 | Loss: 0.00002334
Iteration 638/1000 | Loss: 0.00002334
Iteration 639/1000 | Loss: 0.00002334
Iteration 640/1000 | Loss: 0.00002334
Iteration 641/1000 | Loss: 0.00002333
Iteration 642/1000 | Loss: 0.00020924
Iteration 643/1000 | Loss: 0.00019710
Iteration 644/1000 | Loss: 0.00022569
Iteration 645/1000 | Loss: 0.00018880
Iteration 646/1000 | Loss: 0.00011991
Iteration 647/1000 | Loss: 0.00008600
Iteration 648/1000 | Loss: 0.00002494
Iteration 649/1000 | Loss: 0.00002379
Iteration 650/1000 | Loss: 0.00017734
Iteration 651/1000 | Loss: 0.00019407
Iteration 652/1000 | Loss: 0.00012035
Iteration 653/1000 | Loss: 0.00009803
Iteration 654/1000 | Loss: 0.00002979
Iteration 655/1000 | Loss: 0.00002621
Iteration 656/1000 | Loss: 0.00013205
Iteration 657/1000 | Loss: 0.00002516
Iteration 658/1000 | Loss: 0.00002386
Iteration 659/1000 | Loss: 0.00002342
Iteration 660/1000 | Loss: 0.00002317
Iteration 661/1000 | Loss: 0.00002303
Iteration 662/1000 | Loss: 0.00002299
Iteration 663/1000 | Loss: 0.00002299
Iteration 664/1000 | Loss: 0.00002298
Iteration 665/1000 | Loss: 0.00002298
Iteration 666/1000 | Loss: 0.00002298
Iteration 667/1000 | Loss: 0.00002298
Iteration 668/1000 | Loss: 0.00002298
Iteration 669/1000 | Loss: 0.00002298
Iteration 670/1000 | Loss: 0.00002298
Iteration 671/1000 | Loss: 0.00002298
Iteration 672/1000 | Loss: 0.00002298
Iteration 673/1000 | Loss: 0.00002297
Iteration 674/1000 | Loss: 0.00002297
Iteration 675/1000 | Loss: 0.00002296
Iteration 676/1000 | Loss: 0.00002293
Iteration 677/1000 | Loss: 0.00002290
Iteration 678/1000 | Loss: 0.00002290
Iteration 679/1000 | Loss: 0.00002289
Iteration 680/1000 | Loss: 0.00002289
Iteration 681/1000 | Loss: 0.00002289
Iteration 682/1000 | Loss: 0.00002289
Iteration 683/1000 | Loss: 0.00002289
Iteration 684/1000 | Loss: 0.00002288
Iteration 685/1000 | Loss: 0.00002288
Iteration 686/1000 | Loss: 0.00002288
Iteration 687/1000 | Loss: 0.00002288
Iteration 688/1000 | Loss: 0.00002288
Iteration 689/1000 | Loss: 0.00002288
Iteration 690/1000 | Loss: 0.00002288
Iteration 691/1000 | Loss: 0.00002288
Iteration 692/1000 | Loss: 0.00002288
Iteration 693/1000 | Loss: 0.00002288
Iteration 694/1000 | Loss: 0.00002288
Iteration 695/1000 | Loss: 0.00002288
Iteration 696/1000 | Loss: 0.00002288
Iteration 697/1000 | Loss: 0.00002288
Iteration 698/1000 | Loss: 0.00002288
Iteration 699/1000 | Loss: 0.00002288
Iteration 700/1000 | Loss: 0.00002288
Iteration 701/1000 | Loss: 0.00002288
Iteration 702/1000 | Loss: 0.00002288
Iteration 703/1000 | Loss: 0.00002288
Iteration 704/1000 | Loss: 0.00002288
Iteration 705/1000 | Loss: 0.00002288
Iteration 706/1000 | Loss: 0.00002288
Iteration 707/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 707. Stopping optimization.
Last 5 losses: [2.2880511096445844e-05, 2.2880511096445844e-05, 2.2880511096445844e-05, 2.2880511096445844e-05, 2.2880511096445844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2880511096445844e-05

Optimization complete. Final v2v error: 3.9311485290527344 mm

Highest mean error: 9.017196655273438 mm for frame 170

Lowest mean error: 3.460953950881958 mm for frame 38

Saving results

Total time: 920.1649248600006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415666
Iteration 2/25 | Loss: 0.00143496
Iteration 3/25 | Loss: 0.00136065
Iteration 4/25 | Loss: 0.00135201
Iteration 5/25 | Loss: 0.00134949
Iteration 6/25 | Loss: 0.00134949
Iteration 7/25 | Loss: 0.00134949
Iteration 8/25 | Loss: 0.00134949
Iteration 9/25 | Loss: 0.00134949
Iteration 10/25 | Loss: 0.00134949
Iteration 11/25 | Loss: 0.00134949
Iteration 12/25 | Loss: 0.00134949
Iteration 13/25 | Loss: 0.00134949
Iteration 14/25 | Loss: 0.00134949
Iteration 15/25 | Loss: 0.00134949
Iteration 16/25 | Loss: 0.00134949
Iteration 17/25 | Loss: 0.00134949
Iteration 18/25 | Loss: 0.00134949
Iteration 19/25 | Loss: 0.00134949
Iteration 20/25 | Loss: 0.00134949
Iteration 21/25 | Loss: 0.00134949
Iteration 22/25 | Loss: 0.00134949
Iteration 23/25 | Loss: 0.00134949
Iteration 24/25 | Loss: 0.00134949
Iteration 25/25 | Loss: 0.00134949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23836219
Iteration 2/25 | Loss: 0.00203029
Iteration 3/25 | Loss: 0.00203029
Iteration 4/25 | Loss: 0.00203029
Iteration 5/25 | Loss: 0.00203029
Iteration 6/25 | Loss: 0.00203029
Iteration 7/25 | Loss: 0.00203029
Iteration 8/25 | Loss: 0.00203029
Iteration 9/25 | Loss: 0.00203029
Iteration 10/25 | Loss: 0.00203029
Iteration 11/25 | Loss: 0.00203029
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002030289964750409, 0.002030289964750409, 0.002030289964750409, 0.002030289964750409, 0.002030289964750409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002030289964750409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203029
Iteration 2/1000 | Loss: 0.00002759
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00001510
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001387
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001266
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001244
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001208
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001208
Iteration 34/1000 | Loss: 0.00001208
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001190
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001189
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001187
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.1862277460750192e-05, 1.1862277460750192e-05, 1.1862277460750192e-05, 1.1862277460750192e-05, 1.1862277460750192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1862277460750192e-05

Optimization complete. Final v2v error: 2.954943895339966 mm

Highest mean error: 3.361476421356201 mm for frame 5

Lowest mean error: 2.7749226093292236 mm for frame 112

Saving results

Total time: 35.792261600494385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854065
Iteration 2/25 | Loss: 0.00159801
Iteration 3/25 | Loss: 0.00138411
Iteration 4/25 | Loss: 0.00136623
Iteration 5/25 | Loss: 0.00136289
Iteration 6/25 | Loss: 0.00136289
Iteration 7/25 | Loss: 0.00136289
Iteration 8/25 | Loss: 0.00136289
Iteration 9/25 | Loss: 0.00136289
Iteration 10/25 | Loss: 0.00136289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013628903543576598, 0.0013628903543576598, 0.0013628903543576598, 0.0013628903543576598, 0.0013628903543576598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013628903543576598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75232887
Iteration 2/25 | Loss: 0.00173699
Iteration 3/25 | Loss: 0.00173698
Iteration 4/25 | Loss: 0.00173698
Iteration 5/25 | Loss: 0.00173698
Iteration 6/25 | Loss: 0.00173698
Iteration 7/25 | Loss: 0.00173698
Iteration 8/25 | Loss: 0.00173698
Iteration 9/25 | Loss: 0.00173698
Iteration 10/25 | Loss: 0.00173698
Iteration 11/25 | Loss: 0.00173698
Iteration 12/25 | Loss: 0.00173698
Iteration 13/25 | Loss: 0.00173698
Iteration 14/25 | Loss: 0.00173698
Iteration 15/25 | Loss: 0.00173698
Iteration 16/25 | Loss: 0.00173698
Iteration 17/25 | Loss: 0.00173698
Iteration 18/25 | Loss: 0.00173698
Iteration 19/25 | Loss: 0.00173698
Iteration 20/25 | Loss: 0.00173698
Iteration 21/25 | Loss: 0.00173698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017369779525324702, 0.0017369779525324702, 0.0017369779525324702, 0.0017369779525324702, 0.0017369779525324702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017369779525324702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173698
Iteration 2/1000 | Loss: 0.00003079
Iteration 3/1000 | Loss: 0.00002089
Iteration 4/1000 | Loss: 0.00001878
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001654
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001562
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001477
Iteration 13/1000 | Loss: 0.00001476
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001460
Iteration 18/1000 | Loss: 0.00001458
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001454
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001430
Iteration 26/1000 | Loss: 0.00001429
Iteration 27/1000 | Loss: 0.00001429
Iteration 28/1000 | Loss: 0.00001429
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001428
Iteration 31/1000 | Loss: 0.00001427
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001426
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001426
Iteration 36/1000 | Loss: 0.00001425
Iteration 37/1000 | Loss: 0.00001425
Iteration 38/1000 | Loss: 0.00001425
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001424
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001412
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001410
Iteration 81/1000 | Loss: 0.00001409
Iteration 82/1000 | Loss: 0.00001409
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001408
Iteration 87/1000 | Loss: 0.00001408
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001407
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001402
Iteration 109/1000 | Loss: 0.00001402
Iteration 110/1000 | Loss: 0.00001402
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001401
Iteration 119/1000 | Loss: 0.00001401
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001399
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001398
Iteration 129/1000 | Loss: 0.00001398
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001397
Iteration 134/1000 | Loss: 0.00001397
Iteration 135/1000 | Loss: 0.00001397
Iteration 136/1000 | Loss: 0.00001397
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001394
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001393
Iteration 157/1000 | Loss: 0.00001393
Iteration 158/1000 | Loss: 0.00001393
Iteration 159/1000 | Loss: 0.00001393
Iteration 160/1000 | Loss: 0.00001393
Iteration 161/1000 | Loss: 0.00001393
Iteration 162/1000 | Loss: 0.00001393
Iteration 163/1000 | Loss: 0.00001393
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001393
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001392
Iteration 173/1000 | Loss: 0.00001392
Iteration 174/1000 | Loss: 0.00001392
Iteration 175/1000 | Loss: 0.00001392
Iteration 176/1000 | Loss: 0.00001392
Iteration 177/1000 | Loss: 0.00001392
Iteration 178/1000 | Loss: 0.00001392
Iteration 179/1000 | Loss: 0.00001392
Iteration 180/1000 | Loss: 0.00001392
Iteration 181/1000 | Loss: 0.00001392
Iteration 182/1000 | Loss: 0.00001392
Iteration 183/1000 | Loss: 0.00001392
Iteration 184/1000 | Loss: 0.00001392
Iteration 185/1000 | Loss: 0.00001392
Iteration 186/1000 | Loss: 0.00001392
Iteration 187/1000 | Loss: 0.00001392
Iteration 188/1000 | Loss: 0.00001392
Iteration 189/1000 | Loss: 0.00001392
Iteration 190/1000 | Loss: 0.00001392
Iteration 191/1000 | Loss: 0.00001392
Iteration 192/1000 | Loss: 0.00001392
Iteration 193/1000 | Loss: 0.00001392
Iteration 194/1000 | Loss: 0.00001392
Iteration 195/1000 | Loss: 0.00001392
Iteration 196/1000 | Loss: 0.00001392
Iteration 197/1000 | Loss: 0.00001392
Iteration 198/1000 | Loss: 0.00001392
Iteration 199/1000 | Loss: 0.00001392
Iteration 200/1000 | Loss: 0.00001392
Iteration 201/1000 | Loss: 0.00001392
Iteration 202/1000 | Loss: 0.00001392
Iteration 203/1000 | Loss: 0.00001392
Iteration 204/1000 | Loss: 0.00001392
Iteration 205/1000 | Loss: 0.00001392
Iteration 206/1000 | Loss: 0.00001392
Iteration 207/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.3917905562266242e-05, 1.3917905562266242e-05, 1.3917905562266242e-05, 1.3917905562266242e-05, 1.3917905562266242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3917905562266242e-05

Optimization complete. Final v2v error: 3.1862921714782715 mm

Highest mean error: 3.5462212562561035 mm for frame 6

Lowest mean error: 2.88427472114563 mm for frame 61

Saving results

Total time: 47.53211736679077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804300
Iteration 2/25 | Loss: 0.00173657
Iteration 3/25 | Loss: 0.00146497
Iteration 4/25 | Loss: 0.00144248
Iteration 5/25 | Loss: 0.00143891
Iteration 6/25 | Loss: 0.00143825
Iteration 7/25 | Loss: 0.00143825
Iteration 8/25 | Loss: 0.00143825
Iteration 9/25 | Loss: 0.00143825
Iteration 10/25 | Loss: 0.00143825
Iteration 11/25 | Loss: 0.00143825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001438251929357648, 0.001438251929357648, 0.001438251929357648, 0.001438251929357648, 0.001438251929357648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001438251929357648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25715482
Iteration 2/25 | Loss: 0.00191860
Iteration 3/25 | Loss: 0.00191860
Iteration 4/25 | Loss: 0.00191860
Iteration 5/25 | Loss: 0.00191860
Iteration 6/25 | Loss: 0.00191860
Iteration 7/25 | Loss: 0.00191860
Iteration 8/25 | Loss: 0.00191860
Iteration 9/25 | Loss: 0.00191860
Iteration 10/25 | Loss: 0.00191860
Iteration 11/25 | Loss: 0.00191860
Iteration 12/25 | Loss: 0.00191860
Iteration 13/25 | Loss: 0.00191860
Iteration 14/25 | Loss: 0.00191860
Iteration 15/25 | Loss: 0.00191860
Iteration 16/25 | Loss: 0.00191860
Iteration 17/25 | Loss: 0.00191860
Iteration 18/25 | Loss: 0.00191860
Iteration 19/25 | Loss: 0.00191860
Iteration 20/25 | Loss: 0.00191860
Iteration 21/25 | Loss: 0.00191860
Iteration 22/25 | Loss: 0.00191860
Iteration 23/25 | Loss: 0.00191860
Iteration 24/25 | Loss: 0.00191860
Iteration 25/25 | Loss: 0.00191860

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191860
Iteration 2/1000 | Loss: 0.00004422
Iteration 3/1000 | Loss: 0.00002991
Iteration 4/1000 | Loss: 0.00002705
Iteration 5/1000 | Loss: 0.00002542
Iteration 6/1000 | Loss: 0.00002439
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002326
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002247
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002175
Iteration 14/1000 | Loss: 0.00002161
Iteration 15/1000 | Loss: 0.00002155
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002140
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00002120
Iteration 21/1000 | Loss: 0.00002114
Iteration 22/1000 | Loss: 0.00002112
Iteration 23/1000 | Loss: 0.00002111
Iteration 24/1000 | Loss: 0.00002110
Iteration 25/1000 | Loss: 0.00002108
Iteration 26/1000 | Loss: 0.00002104
Iteration 27/1000 | Loss: 0.00002100
Iteration 28/1000 | Loss: 0.00002100
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002096
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002096
Iteration 34/1000 | Loss: 0.00002096
Iteration 35/1000 | Loss: 0.00002095
Iteration 36/1000 | Loss: 0.00002094
Iteration 37/1000 | Loss: 0.00002094
Iteration 38/1000 | Loss: 0.00002093
Iteration 39/1000 | Loss: 0.00002093
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002092
Iteration 42/1000 | Loss: 0.00002092
Iteration 43/1000 | Loss: 0.00002092
Iteration 44/1000 | Loss: 0.00002092
Iteration 45/1000 | Loss: 0.00002091
Iteration 46/1000 | Loss: 0.00002091
Iteration 47/1000 | Loss: 0.00002091
Iteration 48/1000 | Loss: 0.00002090
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002089
Iteration 51/1000 | Loss: 0.00002089
Iteration 52/1000 | Loss: 0.00002089
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002089
Iteration 55/1000 | Loss: 0.00002089
Iteration 56/1000 | Loss: 0.00002089
Iteration 57/1000 | Loss: 0.00002089
Iteration 58/1000 | Loss: 0.00002089
Iteration 59/1000 | Loss: 0.00002089
Iteration 60/1000 | Loss: 0.00002089
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002086
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002086
Iteration 70/1000 | Loss: 0.00002086
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002085
Iteration 73/1000 | Loss: 0.00002085
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002084
Iteration 80/1000 | Loss: 0.00002084
Iteration 81/1000 | Loss: 0.00002084
Iteration 82/1000 | Loss: 0.00002084
Iteration 83/1000 | Loss: 0.00002084
Iteration 84/1000 | Loss: 0.00002084
Iteration 85/1000 | Loss: 0.00002084
Iteration 86/1000 | Loss: 0.00002084
Iteration 87/1000 | Loss: 0.00002084
Iteration 88/1000 | Loss: 0.00002083
Iteration 89/1000 | Loss: 0.00002083
Iteration 90/1000 | Loss: 0.00002083
Iteration 91/1000 | Loss: 0.00002083
Iteration 92/1000 | Loss: 0.00002083
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00002082
Iteration 96/1000 | Loss: 0.00002081
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002080
Iteration 107/1000 | Loss: 0.00002080
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002079
Iteration 111/1000 | Loss: 0.00002079
Iteration 112/1000 | Loss: 0.00002079
Iteration 113/1000 | Loss: 0.00002079
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002079
Iteration 117/1000 | Loss: 0.00002079
Iteration 118/1000 | Loss: 0.00002079
Iteration 119/1000 | Loss: 0.00002079
Iteration 120/1000 | Loss: 0.00002079
Iteration 121/1000 | Loss: 0.00002079
Iteration 122/1000 | Loss: 0.00002078
Iteration 123/1000 | Loss: 0.00002078
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002078
Iteration 128/1000 | Loss: 0.00002078
Iteration 129/1000 | Loss: 0.00002078
Iteration 130/1000 | Loss: 0.00002078
Iteration 131/1000 | Loss: 0.00002078
Iteration 132/1000 | Loss: 0.00002078
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002077
Iteration 136/1000 | Loss: 0.00002077
Iteration 137/1000 | Loss: 0.00002077
Iteration 138/1000 | Loss: 0.00002077
Iteration 139/1000 | Loss: 0.00002077
Iteration 140/1000 | Loss: 0.00002077
Iteration 141/1000 | Loss: 0.00002077
Iteration 142/1000 | Loss: 0.00002077
Iteration 143/1000 | Loss: 0.00002077
Iteration 144/1000 | Loss: 0.00002077
Iteration 145/1000 | Loss: 0.00002077
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002077
Iteration 148/1000 | Loss: 0.00002077
Iteration 149/1000 | Loss: 0.00002076
Iteration 150/1000 | Loss: 0.00002076
Iteration 151/1000 | Loss: 0.00002076
Iteration 152/1000 | Loss: 0.00002076
Iteration 153/1000 | Loss: 0.00002076
Iteration 154/1000 | Loss: 0.00002076
Iteration 155/1000 | Loss: 0.00002076
Iteration 156/1000 | Loss: 0.00002076
Iteration 157/1000 | Loss: 0.00002076
Iteration 158/1000 | Loss: 0.00002076
Iteration 159/1000 | Loss: 0.00002076
Iteration 160/1000 | Loss: 0.00002076
Iteration 161/1000 | Loss: 0.00002075
Iteration 162/1000 | Loss: 0.00002075
Iteration 163/1000 | Loss: 0.00002075
Iteration 164/1000 | Loss: 0.00002075
Iteration 165/1000 | Loss: 0.00002075
Iteration 166/1000 | Loss: 0.00002075
Iteration 167/1000 | Loss: 0.00002075
Iteration 168/1000 | Loss: 0.00002075
Iteration 169/1000 | Loss: 0.00002075
Iteration 170/1000 | Loss: 0.00002075
Iteration 171/1000 | Loss: 0.00002075
Iteration 172/1000 | Loss: 0.00002075
Iteration 173/1000 | Loss: 0.00002075
Iteration 174/1000 | Loss: 0.00002075
Iteration 175/1000 | Loss: 0.00002074
Iteration 176/1000 | Loss: 0.00002074
Iteration 177/1000 | Loss: 0.00002074
Iteration 178/1000 | Loss: 0.00002074
Iteration 179/1000 | Loss: 0.00002074
Iteration 180/1000 | Loss: 0.00002074
Iteration 181/1000 | Loss: 0.00002074
Iteration 182/1000 | Loss: 0.00002074
Iteration 183/1000 | Loss: 0.00002074
Iteration 184/1000 | Loss: 0.00002074
Iteration 185/1000 | Loss: 0.00002074
Iteration 186/1000 | Loss: 0.00002074
Iteration 187/1000 | Loss: 0.00002073
Iteration 188/1000 | Loss: 0.00002073
Iteration 189/1000 | Loss: 0.00002073
Iteration 190/1000 | Loss: 0.00002073
Iteration 191/1000 | Loss: 0.00002073
Iteration 192/1000 | Loss: 0.00002073
Iteration 193/1000 | Loss: 0.00002073
Iteration 194/1000 | Loss: 0.00002073
Iteration 195/1000 | Loss: 0.00002073
Iteration 196/1000 | Loss: 0.00002073
Iteration 197/1000 | Loss: 0.00002073
Iteration 198/1000 | Loss: 0.00002073
Iteration 199/1000 | Loss: 0.00002073
Iteration 200/1000 | Loss: 0.00002073
Iteration 201/1000 | Loss: 0.00002073
Iteration 202/1000 | Loss: 0.00002072
Iteration 203/1000 | Loss: 0.00002072
Iteration 204/1000 | Loss: 0.00002072
Iteration 205/1000 | Loss: 0.00002072
Iteration 206/1000 | Loss: 0.00002072
Iteration 207/1000 | Loss: 0.00002072
Iteration 208/1000 | Loss: 0.00002072
Iteration 209/1000 | Loss: 0.00002072
Iteration 210/1000 | Loss: 0.00002072
Iteration 211/1000 | Loss: 0.00002072
Iteration 212/1000 | Loss: 0.00002072
Iteration 213/1000 | Loss: 0.00002072
Iteration 214/1000 | Loss: 0.00002072
Iteration 215/1000 | Loss: 0.00002072
Iteration 216/1000 | Loss: 0.00002072
Iteration 217/1000 | Loss: 0.00002072
Iteration 218/1000 | Loss: 0.00002072
Iteration 219/1000 | Loss: 0.00002072
Iteration 220/1000 | Loss: 0.00002072
Iteration 221/1000 | Loss: 0.00002072
Iteration 222/1000 | Loss: 0.00002072
Iteration 223/1000 | Loss: 0.00002072
Iteration 224/1000 | Loss: 0.00002072
Iteration 225/1000 | Loss: 0.00002072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.0717108782264404e-05, 2.0717108782264404e-05, 2.0717108782264404e-05, 2.0717108782264404e-05, 2.0717108782264404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0717108782264404e-05

Optimization complete. Final v2v error: 3.758823871612549 mm

Highest mean error: 5.028876781463623 mm for frame 150

Lowest mean error: 2.8978376388549805 mm for frame 8

Saving results

Total time: 46.18938088417053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393018
Iteration 2/25 | Loss: 0.00141126
Iteration 3/25 | Loss: 0.00132848
Iteration 4/25 | Loss: 0.00131948
Iteration 5/25 | Loss: 0.00131820
Iteration 6/25 | Loss: 0.00131820
Iteration 7/25 | Loss: 0.00131820
Iteration 8/25 | Loss: 0.00131820
Iteration 9/25 | Loss: 0.00131820
Iteration 10/25 | Loss: 0.00131820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001318198861554265, 0.001318198861554265, 0.001318198861554265, 0.001318198861554265, 0.001318198861554265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001318198861554265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51200020
Iteration 2/25 | Loss: 0.00203711
Iteration 3/25 | Loss: 0.00203711
Iteration 4/25 | Loss: 0.00203711
Iteration 5/25 | Loss: 0.00203711
Iteration 6/25 | Loss: 0.00203711
Iteration 7/25 | Loss: 0.00203711
Iteration 8/25 | Loss: 0.00203711
Iteration 9/25 | Loss: 0.00203711
Iteration 10/25 | Loss: 0.00203711
Iteration 11/25 | Loss: 0.00203711
Iteration 12/25 | Loss: 0.00203711
Iteration 13/25 | Loss: 0.00203711
Iteration 14/25 | Loss: 0.00203711
Iteration 15/25 | Loss: 0.00203711
Iteration 16/25 | Loss: 0.00203711
Iteration 17/25 | Loss: 0.00203711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002037106780335307, 0.002037106780335307, 0.002037106780335307, 0.002037106780335307, 0.002037106780335307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002037106780335307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203711
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00001907
Iteration 4/1000 | Loss: 0.00001579
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001274
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001194
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001111
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001086
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001076
Iteration 16/1000 | Loss: 0.00001073
Iteration 17/1000 | Loss: 0.00001072
Iteration 18/1000 | Loss: 0.00001071
Iteration 19/1000 | Loss: 0.00001071
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001052
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001042
Iteration 26/1000 | Loss: 0.00001041
Iteration 27/1000 | Loss: 0.00001039
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001032
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001026
Iteration 32/1000 | Loss: 0.00001025
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001024
Iteration 35/1000 | Loss: 0.00001019
Iteration 36/1000 | Loss: 0.00001018
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001015
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001006
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.0059897249448113e-05, 1.0059897249448113e-05, 1.0059897249448113e-05, 1.0059897249448113e-05, 1.0059897249448113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0059897249448113e-05

Optimization complete. Final v2v error: 2.7415945529937744 mm

Highest mean error: 2.8528499603271484 mm for frame 115

Lowest mean error: 2.588367223739624 mm for frame 200

Saving results

Total time: 37.464032888412476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038232
Iteration 2/25 | Loss: 0.01038231
Iteration 3/25 | Loss: 0.01038231
Iteration 4/25 | Loss: 0.01038231
Iteration 5/25 | Loss: 0.01038231
Iteration 6/25 | Loss: 0.01038231
Iteration 7/25 | Loss: 0.01038231
Iteration 8/25 | Loss: 0.01038230
Iteration 9/25 | Loss: 0.01038230
Iteration 10/25 | Loss: 0.01038230
Iteration 11/25 | Loss: 0.01038230
Iteration 12/25 | Loss: 0.01038230
Iteration 13/25 | Loss: 0.01038229
Iteration 14/25 | Loss: 0.01038229
Iteration 15/25 | Loss: 0.01038229
Iteration 16/25 | Loss: 0.01038229
Iteration 17/25 | Loss: 0.01038229
Iteration 18/25 | Loss: 0.01038229
Iteration 19/25 | Loss: 0.01038229
Iteration 20/25 | Loss: 0.01038228
Iteration 21/25 | Loss: 0.01038228
Iteration 22/25 | Loss: 0.01038228
Iteration 23/25 | Loss: 0.01038228
Iteration 24/25 | Loss: 0.01038227
Iteration 25/25 | Loss: 0.01038227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82389712
Iteration 2/25 | Loss: 0.09485637
Iteration 3/25 | Loss: 0.09400499
Iteration 4/25 | Loss: 0.09300135
Iteration 5/25 | Loss: 0.09300133
Iteration 6/25 | Loss: 0.09300133
Iteration 7/25 | Loss: 0.09300133
Iteration 8/25 | Loss: 0.09300133
Iteration 9/25 | Loss: 0.09300133
Iteration 10/25 | Loss: 0.09300133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.09300132840871811, 0.09300132840871811, 0.09300132840871811, 0.09300132840871811, 0.09300132840871811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09300132840871811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09300133
Iteration 2/1000 | Loss: 0.00133782
Iteration 3/1000 | Loss: 0.00219681
Iteration 4/1000 | Loss: 0.00220141
Iteration 5/1000 | Loss: 0.00194309
Iteration 6/1000 | Loss: 0.00213976
Iteration 7/1000 | Loss: 0.00150436
Iteration 8/1000 | Loss: 0.00180725
Iteration 9/1000 | Loss: 0.00164057
Iteration 10/1000 | Loss: 0.00010826
Iteration 11/1000 | Loss: 0.00006972
Iteration 12/1000 | Loss: 0.00009220
Iteration 13/1000 | Loss: 0.00196138
Iteration 14/1000 | Loss: 0.00173197
Iteration 15/1000 | Loss: 0.00011450
Iteration 16/1000 | Loss: 0.00114499
Iteration 17/1000 | Loss: 0.00170871
Iteration 18/1000 | Loss: 0.00168762
Iteration 19/1000 | Loss: 0.00138287
Iteration 20/1000 | Loss: 0.00026267
Iteration 21/1000 | Loss: 0.00071609
Iteration 22/1000 | Loss: 0.00033610
Iteration 23/1000 | Loss: 0.00004239
Iteration 24/1000 | Loss: 0.00009536
Iteration 25/1000 | Loss: 0.00003510
Iteration 26/1000 | Loss: 0.00004946
Iteration 27/1000 | Loss: 0.00026741
Iteration 28/1000 | Loss: 0.00045024
Iteration 29/1000 | Loss: 0.00008385
Iteration 30/1000 | Loss: 0.00013312
Iteration 31/1000 | Loss: 0.00130003
Iteration 32/1000 | Loss: 0.00273213
Iteration 33/1000 | Loss: 0.00006505
Iteration 34/1000 | Loss: 0.00014844
Iteration 35/1000 | Loss: 0.00135336
Iteration 36/1000 | Loss: 0.00016129
Iteration 37/1000 | Loss: 0.00022413
Iteration 38/1000 | Loss: 0.00004938
Iteration 39/1000 | Loss: 0.00110980
Iteration 40/1000 | Loss: 0.00015320
Iteration 41/1000 | Loss: 0.00010442
Iteration 42/1000 | Loss: 0.00017339
Iteration 43/1000 | Loss: 0.00006198
Iteration 44/1000 | Loss: 0.00019661
Iteration 45/1000 | Loss: 0.00003445
Iteration 46/1000 | Loss: 0.00004292
Iteration 47/1000 | Loss: 0.00065604
Iteration 48/1000 | Loss: 0.00003619
Iteration 49/1000 | Loss: 0.00032880
Iteration 50/1000 | Loss: 0.00122752
Iteration 51/1000 | Loss: 0.00112238
Iteration 52/1000 | Loss: 0.00179493
Iteration 53/1000 | Loss: 0.00019283
Iteration 54/1000 | Loss: 0.00066529
Iteration 55/1000 | Loss: 0.00004312
Iteration 56/1000 | Loss: 0.00009602
Iteration 57/1000 | Loss: 0.00038870
Iteration 58/1000 | Loss: 0.00004585
Iteration 59/1000 | Loss: 0.00005813
Iteration 60/1000 | Loss: 0.00004340
Iteration 61/1000 | Loss: 0.00008539
Iteration 62/1000 | Loss: 0.00002380
Iteration 63/1000 | Loss: 0.00019424
Iteration 64/1000 | Loss: 0.00154944
Iteration 65/1000 | Loss: 0.00003561
Iteration 66/1000 | Loss: 0.00027993
Iteration 67/1000 | Loss: 0.00098969
Iteration 68/1000 | Loss: 0.00003906
Iteration 69/1000 | Loss: 0.00002658
Iteration 70/1000 | Loss: 0.00008652
Iteration 71/1000 | Loss: 0.00002843
Iteration 72/1000 | Loss: 0.00002240
Iteration 73/1000 | Loss: 0.00013152
Iteration 74/1000 | Loss: 0.00008667
Iteration 75/1000 | Loss: 0.00009101
Iteration 76/1000 | Loss: 0.00002190
Iteration 77/1000 | Loss: 0.00002838
Iteration 78/1000 | Loss: 0.00002141
Iteration 79/1000 | Loss: 0.00002954
Iteration 80/1000 | Loss: 0.00003226
Iteration 81/1000 | Loss: 0.00038656
Iteration 82/1000 | Loss: 0.00005531
Iteration 83/1000 | Loss: 0.00002857
Iteration 84/1000 | Loss: 0.00002253
Iteration 85/1000 | Loss: 0.00002175
Iteration 86/1000 | Loss: 0.00005622
Iteration 87/1000 | Loss: 0.00002396
Iteration 88/1000 | Loss: 0.00003590
Iteration 89/1000 | Loss: 0.00007245
Iteration 90/1000 | Loss: 0.00003952
Iteration 91/1000 | Loss: 0.00003735
Iteration 92/1000 | Loss: 0.00001917
Iteration 93/1000 | Loss: 0.00004641
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00003479
Iteration 97/1000 | Loss: 0.00002989
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001863
Iteration 101/1000 | Loss: 0.00001863
Iteration 102/1000 | Loss: 0.00001863
Iteration 103/1000 | Loss: 0.00001862
Iteration 104/1000 | Loss: 0.00001978
Iteration 105/1000 | Loss: 0.00002800
Iteration 106/1000 | Loss: 0.00003062
Iteration 107/1000 | Loss: 0.00002536
Iteration 108/1000 | Loss: 0.00004415
Iteration 109/1000 | Loss: 0.00002164
Iteration 110/1000 | Loss: 0.00005056
Iteration 111/1000 | Loss: 0.00019153
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00001830
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00003959
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00013947
Iteration 120/1000 | Loss: 0.00002894
Iteration 121/1000 | Loss: 0.00004400
Iteration 122/1000 | Loss: 0.00002012
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001874
Iteration 125/1000 | Loss: 0.00004530
Iteration 126/1000 | Loss: 0.00002208
Iteration 127/1000 | Loss: 0.00002033
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001800
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001799
Iteration 135/1000 | Loss: 0.00001799
Iteration 136/1000 | Loss: 0.00001799
Iteration 137/1000 | Loss: 0.00001799
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001797
Iteration 145/1000 | Loss: 0.00001797
Iteration 146/1000 | Loss: 0.00001797
Iteration 147/1000 | Loss: 0.00001797
Iteration 148/1000 | Loss: 0.00001797
Iteration 149/1000 | Loss: 0.00001797
Iteration 150/1000 | Loss: 0.00001797
Iteration 151/1000 | Loss: 0.00001797
Iteration 152/1000 | Loss: 0.00001797
Iteration 153/1000 | Loss: 0.00001797
Iteration 154/1000 | Loss: 0.00001796
Iteration 155/1000 | Loss: 0.00001796
Iteration 156/1000 | Loss: 0.00001796
Iteration 157/1000 | Loss: 0.00001796
Iteration 158/1000 | Loss: 0.00001796
Iteration 159/1000 | Loss: 0.00001796
Iteration 160/1000 | Loss: 0.00001796
Iteration 161/1000 | Loss: 0.00001796
Iteration 162/1000 | Loss: 0.00001796
Iteration 163/1000 | Loss: 0.00001796
Iteration 164/1000 | Loss: 0.00001796
Iteration 165/1000 | Loss: 0.00001796
Iteration 166/1000 | Loss: 0.00001796
Iteration 167/1000 | Loss: 0.00001796
Iteration 168/1000 | Loss: 0.00001796
Iteration 169/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.7957616364583373e-05, 1.7957616364583373e-05, 1.7957616364583373e-05, 1.7957616364583373e-05, 1.7957616364583373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7957616364583373e-05

Optimization complete. Final v2v error: 3.6427857875823975 mm

Highest mean error: 5.235137462615967 mm for frame 27

Lowest mean error: 2.8076701164245605 mm for frame 237

Saving results

Total time: 198.1903738975525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603079
Iteration 2/25 | Loss: 0.00152174
Iteration 3/25 | Loss: 0.00137177
Iteration 4/25 | Loss: 0.00135836
Iteration 5/25 | Loss: 0.00135608
Iteration 6/25 | Loss: 0.00135608
Iteration 7/25 | Loss: 0.00135608
Iteration 8/25 | Loss: 0.00135608
Iteration 9/25 | Loss: 0.00135608
Iteration 10/25 | Loss: 0.00135608
Iteration 11/25 | Loss: 0.00135608
Iteration 12/25 | Loss: 0.00135608
Iteration 13/25 | Loss: 0.00135608
Iteration 14/25 | Loss: 0.00135608
Iteration 15/25 | Loss: 0.00135608
Iteration 16/25 | Loss: 0.00135608
Iteration 17/25 | Loss: 0.00135608
Iteration 18/25 | Loss: 0.00135608
Iteration 19/25 | Loss: 0.00135608
Iteration 20/25 | Loss: 0.00135608
Iteration 21/25 | Loss: 0.00135608
Iteration 22/25 | Loss: 0.00135608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013560776133090258, 0.0013560776133090258, 0.0013560776133090258, 0.0013560776133090258, 0.0013560776133090258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013560776133090258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.97354269
Iteration 2/25 | Loss: 0.00214485
Iteration 3/25 | Loss: 0.00214479
Iteration 4/25 | Loss: 0.00214479
Iteration 5/25 | Loss: 0.00214479
Iteration 6/25 | Loss: 0.00214479
Iteration 7/25 | Loss: 0.00214479
Iteration 8/25 | Loss: 0.00214479
Iteration 9/25 | Loss: 0.00214479
Iteration 10/25 | Loss: 0.00214479
Iteration 11/25 | Loss: 0.00214479
Iteration 12/25 | Loss: 0.00214479
Iteration 13/25 | Loss: 0.00214479
Iteration 14/25 | Loss: 0.00214479
Iteration 15/25 | Loss: 0.00214479
Iteration 16/25 | Loss: 0.00214479
Iteration 17/25 | Loss: 0.00214479
Iteration 18/25 | Loss: 0.00214479
Iteration 19/25 | Loss: 0.00214479
Iteration 20/25 | Loss: 0.00214479
Iteration 21/25 | Loss: 0.00214479
Iteration 22/25 | Loss: 0.00214479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0021447883918881416, 0.0021447883918881416, 0.0021447883918881416, 0.0021447883918881416, 0.0021447883918881416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021447883918881416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214479
Iteration 2/1000 | Loss: 0.00002890
Iteration 3/1000 | Loss: 0.00001995
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001642
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001515
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001425
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001384
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001333
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001329
Iteration 32/1000 | Loss: 0.00001329
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001329
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001328
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001324
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001321
Iteration 53/1000 | Loss: 0.00001321
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001315
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001313
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001306
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001305
Iteration 137/1000 | Loss: 0.00001305
Iteration 138/1000 | Loss: 0.00001305
Iteration 139/1000 | Loss: 0.00001305
Iteration 140/1000 | Loss: 0.00001305
Iteration 141/1000 | Loss: 0.00001305
Iteration 142/1000 | Loss: 0.00001305
Iteration 143/1000 | Loss: 0.00001305
Iteration 144/1000 | Loss: 0.00001305
Iteration 145/1000 | Loss: 0.00001305
Iteration 146/1000 | Loss: 0.00001305
Iteration 147/1000 | Loss: 0.00001305
Iteration 148/1000 | Loss: 0.00001305
Iteration 149/1000 | Loss: 0.00001305
Iteration 150/1000 | Loss: 0.00001305
Iteration 151/1000 | Loss: 0.00001305
Iteration 152/1000 | Loss: 0.00001305
Iteration 153/1000 | Loss: 0.00001305
Iteration 154/1000 | Loss: 0.00001305
Iteration 155/1000 | Loss: 0.00001305
Iteration 156/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.3046961612417363e-05, 1.3046961612417363e-05, 1.3046961612417363e-05, 1.3046961612417363e-05, 1.3046961612417363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3046961612417363e-05

Optimization complete. Final v2v error: 3.081723928451538 mm

Highest mean error: 4.047574520111084 mm for frame 33

Lowest mean error: 2.707098960876465 mm for frame 154

Saving results

Total time: 39.499913930892944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558552
Iteration 2/25 | Loss: 0.00137863
Iteration 3/25 | Loss: 0.00131612
Iteration 4/25 | Loss: 0.00130773
Iteration 5/25 | Loss: 0.00130485
Iteration 6/25 | Loss: 0.00130470
Iteration 7/25 | Loss: 0.00130470
Iteration 8/25 | Loss: 0.00130470
Iteration 9/25 | Loss: 0.00130470
Iteration 10/25 | Loss: 0.00130470
Iteration 11/25 | Loss: 0.00130470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013046966632828116, 0.0013046966632828116, 0.0013046966632828116, 0.0013046966632828116, 0.0013046966632828116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013046966632828116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60762262
Iteration 2/25 | Loss: 0.00204359
Iteration 3/25 | Loss: 0.00204359
Iteration 4/25 | Loss: 0.00204359
Iteration 5/25 | Loss: 0.00204359
Iteration 6/25 | Loss: 0.00204359
Iteration 7/25 | Loss: 0.00204359
Iteration 8/25 | Loss: 0.00204359
Iteration 9/25 | Loss: 0.00204359
Iteration 10/25 | Loss: 0.00204359
Iteration 11/25 | Loss: 0.00204359
Iteration 12/25 | Loss: 0.00204359
Iteration 13/25 | Loss: 0.00204359
Iteration 14/25 | Loss: 0.00204359
Iteration 15/25 | Loss: 0.00204359
Iteration 16/25 | Loss: 0.00204359
Iteration 17/25 | Loss: 0.00204359
Iteration 18/25 | Loss: 0.00204359
Iteration 19/25 | Loss: 0.00204359
Iteration 20/25 | Loss: 0.00204359
Iteration 21/25 | Loss: 0.00204359
Iteration 22/25 | Loss: 0.00204359
Iteration 23/25 | Loss: 0.00204359
Iteration 24/25 | Loss: 0.00204359
Iteration 25/25 | Loss: 0.00204359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204359
Iteration 2/1000 | Loss: 0.00002176
Iteration 3/1000 | Loss: 0.00001551
Iteration 4/1000 | Loss: 0.00001396
Iteration 5/1000 | Loss: 0.00001317
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001165
Iteration 9/1000 | Loss: 0.00001124
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001091
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001080
Iteration 14/1000 | Loss: 0.00001079
Iteration 15/1000 | Loss: 0.00001078
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001059
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001055
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001041
Iteration 27/1000 | Loss: 0.00001041
Iteration 28/1000 | Loss: 0.00001040
Iteration 29/1000 | Loss: 0.00001029
Iteration 30/1000 | Loss: 0.00001026
Iteration 31/1000 | Loss: 0.00001025
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001023
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001021
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001020
Iteration 39/1000 | Loss: 0.00001018
Iteration 40/1000 | Loss: 0.00001018
Iteration 41/1000 | Loss: 0.00001017
Iteration 42/1000 | Loss: 0.00001017
Iteration 43/1000 | Loss: 0.00001016
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001016
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001014
Iteration 51/1000 | Loss: 0.00001013
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001012
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001010
Iteration 58/1000 | Loss: 0.00001010
Iteration 59/1000 | Loss: 0.00001010
Iteration 60/1000 | Loss: 0.00001010
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001010
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001010
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001009
Iteration 67/1000 | Loss: 0.00001009
Iteration 68/1000 | Loss: 0.00001008
Iteration 69/1000 | Loss: 0.00001008
Iteration 70/1000 | Loss: 0.00001008
Iteration 71/1000 | Loss: 0.00001008
Iteration 72/1000 | Loss: 0.00001007
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001007
Iteration 75/1000 | Loss: 0.00001007
Iteration 76/1000 | Loss: 0.00001007
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001007
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001006
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Iteration 87/1000 | Loss: 0.00001005
Iteration 88/1000 | Loss: 0.00001005
Iteration 89/1000 | Loss: 0.00001005
Iteration 90/1000 | Loss: 0.00001004
Iteration 91/1000 | Loss: 0.00001004
Iteration 92/1000 | Loss: 0.00001004
Iteration 93/1000 | Loss: 0.00001004
Iteration 94/1000 | Loss: 0.00001004
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001003
Iteration 97/1000 | Loss: 0.00001003
Iteration 98/1000 | Loss: 0.00001003
Iteration 99/1000 | Loss: 0.00001003
Iteration 100/1000 | Loss: 0.00001003
Iteration 101/1000 | Loss: 0.00001003
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001002
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001001
Iteration 113/1000 | Loss: 0.00001000
Iteration 114/1000 | Loss: 0.00001000
Iteration 115/1000 | Loss: 0.00001000
Iteration 116/1000 | Loss: 0.00001000
Iteration 117/1000 | Loss: 0.00001000
Iteration 118/1000 | Loss: 0.00001000
Iteration 119/1000 | Loss: 0.00001000
Iteration 120/1000 | Loss: 0.00001000
Iteration 121/1000 | Loss: 0.00001000
Iteration 122/1000 | Loss: 0.00001000
Iteration 123/1000 | Loss: 0.00001000
Iteration 124/1000 | Loss: 0.00001000
Iteration 125/1000 | Loss: 0.00001000
Iteration 126/1000 | Loss: 0.00000999
Iteration 127/1000 | Loss: 0.00000999
Iteration 128/1000 | Loss: 0.00000999
Iteration 129/1000 | Loss: 0.00000999
Iteration 130/1000 | Loss: 0.00000999
Iteration 131/1000 | Loss: 0.00000999
Iteration 132/1000 | Loss: 0.00000999
Iteration 133/1000 | Loss: 0.00000999
Iteration 134/1000 | Loss: 0.00000999
Iteration 135/1000 | Loss: 0.00000999
Iteration 136/1000 | Loss: 0.00000999
Iteration 137/1000 | Loss: 0.00000998
Iteration 138/1000 | Loss: 0.00000998
Iteration 139/1000 | Loss: 0.00000998
Iteration 140/1000 | Loss: 0.00000998
Iteration 141/1000 | Loss: 0.00000998
Iteration 142/1000 | Loss: 0.00000998
Iteration 143/1000 | Loss: 0.00000998
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000997
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000996
Iteration 155/1000 | Loss: 0.00000996
Iteration 156/1000 | Loss: 0.00000996
Iteration 157/1000 | Loss: 0.00000996
Iteration 158/1000 | Loss: 0.00000996
Iteration 159/1000 | Loss: 0.00000996
Iteration 160/1000 | Loss: 0.00000996
Iteration 161/1000 | Loss: 0.00000996
Iteration 162/1000 | Loss: 0.00000996
Iteration 163/1000 | Loss: 0.00000995
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000995
Iteration 166/1000 | Loss: 0.00000995
Iteration 167/1000 | Loss: 0.00000995
Iteration 168/1000 | Loss: 0.00000995
Iteration 169/1000 | Loss: 0.00000995
Iteration 170/1000 | Loss: 0.00000995
Iteration 171/1000 | Loss: 0.00000994
Iteration 172/1000 | Loss: 0.00000994
Iteration 173/1000 | Loss: 0.00000994
Iteration 174/1000 | Loss: 0.00000993
Iteration 175/1000 | Loss: 0.00000993
Iteration 176/1000 | Loss: 0.00000993
Iteration 177/1000 | Loss: 0.00000992
Iteration 178/1000 | Loss: 0.00000992
Iteration 179/1000 | Loss: 0.00000992
Iteration 180/1000 | Loss: 0.00000992
Iteration 181/1000 | Loss: 0.00000991
Iteration 182/1000 | Loss: 0.00000991
Iteration 183/1000 | Loss: 0.00000990
Iteration 184/1000 | Loss: 0.00000990
Iteration 185/1000 | Loss: 0.00000990
Iteration 186/1000 | Loss: 0.00000990
Iteration 187/1000 | Loss: 0.00000990
Iteration 188/1000 | Loss: 0.00000990
Iteration 189/1000 | Loss: 0.00000990
Iteration 190/1000 | Loss: 0.00000990
Iteration 191/1000 | Loss: 0.00000990
Iteration 192/1000 | Loss: 0.00000990
Iteration 193/1000 | Loss: 0.00000989
Iteration 194/1000 | Loss: 0.00000989
Iteration 195/1000 | Loss: 0.00000989
Iteration 196/1000 | Loss: 0.00000989
Iteration 197/1000 | Loss: 0.00000989
Iteration 198/1000 | Loss: 0.00000989
Iteration 199/1000 | Loss: 0.00000989
Iteration 200/1000 | Loss: 0.00000988
Iteration 201/1000 | Loss: 0.00000988
Iteration 202/1000 | Loss: 0.00000988
Iteration 203/1000 | Loss: 0.00000988
Iteration 204/1000 | Loss: 0.00000988
Iteration 205/1000 | Loss: 0.00000988
Iteration 206/1000 | Loss: 0.00000988
Iteration 207/1000 | Loss: 0.00000988
Iteration 208/1000 | Loss: 0.00000987
Iteration 209/1000 | Loss: 0.00000987
Iteration 210/1000 | Loss: 0.00000987
Iteration 211/1000 | Loss: 0.00000987
Iteration 212/1000 | Loss: 0.00000987
Iteration 213/1000 | Loss: 0.00000987
Iteration 214/1000 | Loss: 0.00000987
Iteration 215/1000 | Loss: 0.00000987
Iteration 216/1000 | Loss: 0.00000987
Iteration 217/1000 | Loss: 0.00000987
Iteration 218/1000 | Loss: 0.00000986
Iteration 219/1000 | Loss: 0.00000986
Iteration 220/1000 | Loss: 0.00000986
Iteration 221/1000 | Loss: 0.00000986
Iteration 222/1000 | Loss: 0.00000986
Iteration 223/1000 | Loss: 0.00000986
Iteration 224/1000 | Loss: 0.00000986
Iteration 225/1000 | Loss: 0.00000986
Iteration 226/1000 | Loss: 0.00000986
Iteration 227/1000 | Loss: 0.00000986
Iteration 228/1000 | Loss: 0.00000986
Iteration 229/1000 | Loss: 0.00000986
Iteration 230/1000 | Loss: 0.00000986
Iteration 231/1000 | Loss: 0.00000985
Iteration 232/1000 | Loss: 0.00000985
Iteration 233/1000 | Loss: 0.00000985
Iteration 234/1000 | Loss: 0.00000985
Iteration 235/1000 | Loss: 0.00000985
Iteration 236/1000 | Loss: 0.00000985
Iteration 237/1000 | Loss: 0.00000985
Iteration 238/1000 | Loss: 0.00000985
Iteration 239/1000 | Loss: 0.00000985
Iteration 240/1000 | Loss: 0.00000985
Iteration 241/1000 | Loss: 0.00000985
Iteration 242/1000 | Loss: 0.00000985
Iteration 243/1000 | Loss: 0.00000985
Iteration 244/1000 | Loss: 0.00000984
Iteration 245/1000 | Loss: 0.00000984
Iteration 246/1000 | Loss: 0.00000984
Iteration 247/1000 | Loss: 0.00000984
Iteration 248/1000 | Loss: 0.00000984
Iteration 249/1000 | Loss: 0.00000984
Iteration 250/1000 | Loss: 0.00000984
Iteration 251/1000 | Loss: 0.00000984
Iteration 252/1000 | Loss: 0.00000984
Iteration 253/1000 | Loss: 0.00000984
Iteration 254/1000 | Loss: 0.00000984
Iteration 255/1000 | Loss: 0.00000984
Iteration 256/1000 | Loss: 0.00000984
Iteration 257/1000 | Loss: 0.00000984
Iteration 258/1000 | Loss: 0.00000984
Iteration 259/1000 | Loss: 0.00000984
Iteration 260/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [9.841784049058333e-06, 9.841784049058333e-06, 9.841784049058333e-06, 9.841784049058333e-06, 9.841784049058333e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.841784049058333e-06

Optimization complete. Final v2v error: 2.745776891708374 mm

Highest mean error: 3.0728635787963867 mm for frame 111

Lowest mean error: 2.6112616062164307 mm for frame 37

Saving results

Total time: 43.937384843826294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968804
Iteration 2/25 | Loss: 0.00201550
Iteration 3/25 | Loss: 0.00161105
Iteration 4/25 | Loss: 0.00155597
Iteration 5/25 | Loss: 0.00154637
Iteration 6/25 | Loss: 0.00153379
Iteration 7/25 | Loss: 0.00148428
Iteration 8/25 | Loss: 0.00144199
Iteration 9/25 | Loss: 0.00142640
Iteration 10/25 | Loss: 0.00141008
Iteration 11/25 | Loss: 0.00140100
Iteration 12/25 | Loss: 0.00139840
Iteration 13/25 | Loss: 0.00139758
Iteration 14/25 | Loss: 0.00139942
Iteration 15/25 | Loss: 0.00139746
Iteration 16/25 | Loss: 0.00139631
Iteration 17/25 | Loss: 0.00139552
Iteration 18/25 | Loss: 0.00139527
Iteration 19/25 | Loss: 0.00139515
Iteration 20/25 | Loss: 0.00139511
Iteration 21/25 | Loss: 0.00139511
Iteration 22/25 | Loss: 0.00139511
Iteration 23/25 | Loss: 0.00139511
Iteration 24/25 | Loss: 0.00139511
Iteration 25/25 | Loss: 0.00139511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013951052678748965, 0.0013951052678748965, 0.0013951052678748965, 0.0013951052678748965, 0.0013951052678748965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013951052678748965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22744846
Iteration 2/25 | Loss: 0.00229463
Iteration 3/25 | Loss: 0.00227252
Iteration 4/25 | Loss: 0.00227252
Iteration 5/25 | Loss: 0.00227252
Iteration 6/25 | Loss: 0.00227252
Iteration 7/25 | Loss: 0.00227252
Iteration 8/25 | Loss: 0.00227252
Iteration 9/25 | Loss: 0.00227252
Iteration 10/25 | Loss: 0.00227252
Iteration 11/25 | Loss: 0.00227252
Iteration 12/25 | Loss: 0.00227252
Iteration 13/25 | Loss: 0.00227252
Iteration 14/25 | Loss: 0.00227252
Iteration 15/25 | Loss: 0.00227252
Iteration 16/25 | Loss: 0.00227252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022725192829966545, 0.0022725192829966545, 0.0022725192829966545, 0.0022725192829966545, 0.0022725192829966545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022725192829966545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227252
Iteration 2/1000 | Loss: 0.00013563
Iteration 3/1000 | Loss: 0.00030257
Iteration 4/1000 | Loss: 0.00008770
Iteration 5/1000 | Loss: 0.00004343
Iteration 6/1000 | Loss: 0.00003811
Iteration 7/1000 | Loss: 0.00006462
Iteration 8/1000 | Loss: 0.00003577
Iteration 9/1000 | Loss: 0.00007059
Iteration 10/1000 | Loss: 0.00003272
Iteration 11/1000 | Loss: 0.00009020
Iteration 12/1000 | Loss: 0.00072775
Iteration 13/1000 | Loss: 0.00004006
Iteration 14/1000 | Loss: 0.00006166
Iteration 15/1000 | Loss: 0.00004065
Iteration 16/1000 | Loss: 0.00002904
Iteration 17/1000 | Loss: 0.00003729
Iteration 18/1000 | Loss: 0.00036038
Iteration 19/1000 | Loss: 0.00017647
Iteration 20/1000 | Loss: 0.00003998
Iteration 21/1000 | Loss: 0.00003538
Iteration 22/1000 | Loss: 0.00003075
Iteration 23/1000 | Loss: 0.00003037
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00004327
Iteration 26/1000 | Loss: 0.00002399
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002339
Iteration 29/1000 | Loss: 0.00002323
Iteration 30/1000 | Loss: 0.00002307
Iteration 31/1000 | Loss: 0.00002290
Iteration 32/1000 | Loss: 0.00002289
Iteration 33/1000 | Loss: 0.00002287
Iteration 34/1000 | Loss: 0.00002287
Iteration 35/1000 | Loss: 0.00002283
Iteration 36/1000 | Loss: 0.00002278
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002274
Iteration 39/1000 | Loss: 0.00002273
Iteration 40/1000 | Loss: 0.00002269
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002260
Iteration 44/1000 | Loss: 0.00002260
Iteration 45/1000 | Loss: 0.00002258
Iteration 46/1000 | Loss: 0.00002257
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00002254
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002253
Iteration 54/1000 | Loss: 0.00002253
Iteration 55/1000 | Loss: 0.00002253
Iteration 56/1000 | Loss: 0.00002252
Iteration 57/1000 | Loss: 0.00002252
Iteration 58/1000 | Loss: 0.00002252
Iteration 59/1000 | Loss: 0.00002252
Iteration 60/1000 | Loss: 0.00002252
Iteration 61/1000 | Loss: 0.00002252
Iteration 62/1000 | Loss: 0.00002252
Iteration 63/1000 | Loss: 0.00002251
Iteration 64/1000 | Loss: 0.00002251
Iteration 65/1000 | Loss: 0.00002251
Iteration 66/1000 | Loss: 0.00002251
Iteration 67/1000 | Loss: 0.00002250
Iteration 68/1000 | Loss: 0.00002250
Iteration 69/1000 | Loss: 0.00002250
Iteration 70/1000 | Loss: 0.00002250
Iteration 71/1000 | Loss: 0.00002249
Iteration 72/1000 | Loss: 0.00002249
Iteration 73/1000 | Loss: 0.00002249
Iteration 74/1000 | Loss: 0.00002249
Iteration 75/1000 | Loss: 0.00002249
Iteration 76/1000 | Loss: 0.00002249
Iteration 77/1000 | Loss: 0.00002249
Iteration 78/1000 | Loss: 0.00002248
Iteration 79/1000 | Loss: 0.00002248
Iteration 80/1000 | Loss: 0.00002247
Iteration 81/1000 | Loss: 0.00002247
Iteration 82/1000 | Loss: 0.00002247
Iteration 83/1000 | Loss: 0.00002247
Iteration 84/1000 | Loss: 0.00002247
Iteration 85/1000 | Loss: 0.00002247
Iteration 86/1000 | Loss: 0.00002247
Iteration 87/1000 | Loss: 0.00002247
Iteration 88/1000 | Loss: 0.00002247
Iteration 89/1000 | Loss: 0.00002247
Iteration 90/1000 | Loss: 0.00002247
Iteration 91/1000 | Loss: 0.00002247
Iteration 92/1000 | Loss: 0.00002246
Iteration 93/1000 | Loss: 0.00002246
Iteration 94/1000 | Loss: 0.00002246
Iteration 95/1000 | Loss: 0.00002246
Iteration 96/1000 | Loss: 0.00002245
Iteration 97/1000 | Loss: 0.00002245
Iteration 98/1000 | Loss: 0.00002245
Iteration 99/1000 | Loss: 0.00002245
Iteration 100/1000 | Loss: 0.00002245
Iteration 101/1000 | Loss: 0.00002245
Iteration 102/1000 | Loss: 0.00002245
Iteration 103/1000 | Loss: 0.00002245
Iteration 104/1000 | Loss: 0.00002245
Iteration 105/1000 | Loss: 0.00002245
Iteration 106/1000 | Loss: 0.00002245
Iteration 107/1000 | Loss: 0.00002245
Iteration 108/1000 | Loss: 0.00002245
Iteration 109/1000 | Loss: 0.00002245
Iteration 110/1000 | Loss: 0.00002245
Iteration 111/1000 | Loss: 0.00002244
Iteration 112/1000 | Loss: 0.00002244
Iteration 113/1000 | Loss: 0.00002244
Iteration 114/1000 | Loss: 0.00002244
Iteration 115/1000 | Loss: 0.00002244
Iteration 116/1000 | Loss: 0.00002244
Iteration 117/1000 | Loss: 0.00002244
Iteration 118/1000 | Loss: 0.00002244
Iteration 119/1000 | Loss: 0.00002244
Iteration 120/1000 | Loss: 0.00002244
Iteration 121/1000 | Loss: 0.00002244
Iteration 122/1000 | Loss: 0.00002243
Iteration 123/1000 | Loss: 0.00002243
Iteration 124/1000 | Loss: 0.00002243
Iteration 125/1000 | Loss: 0.00002243
Iteration 126/1000 | Loss: 0.00002243
Iteration 127/1000 | Loss: 0.00002243
Iteration 128/1000 | Loss: 0.00002243
Iteration 129/1000 | Loss: 0.00002243
Iteration 130/1000 | Loss: 0.00002243
Iteration 131/1000 | Loss: 0.00002243
Iteration 132/1000 | Loss: 0.00002243
Iteration 133/1000 | Loss: 0.00002243
Iteration 134/1000 | Loss: 0.00002243
Iteration 135/1000 | Loss: 0.00002242
Iteration 136/1000 | Loss: 0.00002242
Iteration 137/1000 | Loss: 0.00002242
Iteration 138/1000 | Loss: 0.00002242
Iteration 139/1000 | Loss: 0.00002242
Iteration 140/1000 | Loss: 0.00002242
Iteration 141/1000 | Loss: 0.00002242
Iteration 142/1000 | Loss: 0.00002242
Iteration 143/1000 | Loss: 0.00002242
Iteration 144/1000 | Loss: 0.00002242
Iteration 145/1000 | Loss: 0.00002241
Iteration 146/1000 | Loss: 0.00002241
Iteration 147/1000 | Loss: 0.00002241
Iteration 148/1000 | Loss: 0.00002241
Iteration 149/1000 | Loss: 0.00002241
Iteration 150/1000 | Loss: 0.00002241
Iteration 151/1000 | Loss: 0.00002241
Iteration 152/1000 | Loss: 0.00002241
Iteration 153/1000 | Loss: 0.00002240
Iteration 154/1000 | Loss: 0.00002240
Iteration 155/1000 | Loss: 0.00002240
Iteration 156/1000 | Loss: 0.00002239
Iteration 157/1000 | Loss: 0.00002239
Iteration 158/1000 | Loss: 0.00002239
Iteration 159/1000 | Loss: 0.00002239
Iteration 160/1000 | Loss: 0.00002239
Iteration 161/1000 | Loss: 0.00002239
Iteration 162/1000 | Loss: 0.00002239
Iteration 163/1000 | Loss: 0.00002239
Iteration 164/1000 | Loss: 0.00002239
Iteration 165/1000 | Loss: 0.00002239
Iteration 166/1000 | Loss: 0.00002239
Iteration 167/1000 | Loss: 0.00002239
Iteration 168/1000 | Loss: 0.00002239
Iteration 169/1000 | Loss: 0.00002238
Iteration 170/1000 | Loss: 0.00002238
Iteration 171/1000 | Loss: 0.00002238
Iteration 172/1000 | Loss: 0.00002238
Iteration 173/1000 | Loss: 0.00002238
Iteration 174/1000 | Loss: 0.00002238
Iteration 175/1000 | Loss: 0.00002238
Iteration 176/1000 | Loss: 0.00002238
Iteration 177/1000 | Loss: 0.00002238
Iteration 178/1000 | Loss: 0.00002238
Iteration 179/1000 | Loss: 0.00002238
Iteration 180/1000 | Loss: 0.00002238
Iteration 181/1000 | Loss: 0.00002238
Iteration 182/1000 | Loss: 0.00002238
Iteration 183/1000 | Loss: 0.00002238
Iteration 184/1000 | Loss: 0.00002238
Iteration 185/1000 | Loss: 0.00002238
Iteration 186/1000 | Loss: 0.00002238
Iteration 187/1000 | Loss: 0.00002238
Iteration 188/1000 | Loss: 0.00002238
Iteration 189/1000 | Loss: 0.00002238
Iteration 190/1000 | Loss: 0.00002238
Iteration 191/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.2383987015928142e-05, 2.2383987015928142e-05, 2.2383987015928142e-05, 2.2383987015928142e-05, 2.2383987015928142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2383987015928142e-05

Optimization complete. Final v2v error: 4.053628444671631 mm

Highest mean error: 5.053744792938232 mm for frame 90

Lowest mean error: 3.4192631244659424 mm for frame 24

Saving results

Total time: 102.06484055519104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455248
Iteration 2/25 | Loss: 0.00153174
Iteration 3/25 | Loss: 0.00139094
Iteration 4/25 | Loss: 0.00138100
Iteration 5/25 | Loss: 0.00137937
Iteration 6/25 | Loss: 0.00137933
Iteration 7/25 | Loss: 0.00137933
Iteration 8/25 | Loss: 0.00137933
Iteration 9/25 | Loss: 0.00137933
Iteration 10/25 | Loss: 0.00137933
Iteration 11/25 | Loss: 0.00137933
Iteration 12/25 | Loss: 0.00137933
Iteration 13/25 | Loss: 0.00137933
Iteration 14/25 | Loss: 0.00137933
Iteration 15/25 | Loss: 0.00137933
Iteration 16/25 | Loss: 0.00137933
Iteration 17/25 | Loss: 0.00137933
Iteration 18/25 | Loss: 0.00137933
Iteration 19/25 | Loss: 0.00137933
Iteration 20/25 | Loss: 0.00137933
Iteration 21/25 | Loss: 0.00137933
Iteration 22/25 | Loss: 0.00137933
Iteration 23/25 | Loss: 0.00137933
Iteration 24/25 | Loss: 0.00137933
Iteration 25/25 | Loss: 0.00137933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98245811
Iteration 2/25 | Loss: 0.00161224
Iteration 3/25 | Loss: 0.00161222
Iteration 4/25 | Loss: 0.00161222
Iteration 5/25 | Loss: 0.00161222
Iteration 6/25 | Loss: 0.00161222
Iteration 7/25 | Loss: 0.00161222
Iteration 8/25 | Loss: 0.00161222
Iteration 9/25 | Loss: 0.00161222
Iteration 10/25 | Loss: 0.00161221
Iteration 11/25 | Loss: 0.00161221
Iteration 12/25 | Loss: 0.00161221
Iteration 13/25 | Loss: 0.00161221
Iteration 14/25 | Loss: 0.00161221
Iteration 15/25 | Loss: 0.00161221
Iteration 16/25 | Loss: 0.00161221
Iteration 17/25 | Loss: 0.00161221
Iteration 18/25 | Loss: 0.00161221
Iteration 19/25 | Loss: 0.00161221
Iteration 20/25 | Loss: 0.00161221
Iteration 21/25 | Loss: 0.00161221
Iteration 22/25 | Loss: 0.00161221
Iteration 23/25 | Loss: 0.00161221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016122147208079696, 0.0016122147208079696, 0.0016122147208079696, 0.0016122147208079696, 0.0016122147208079696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016122147208079696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161221
Iteration 2/1000 | Loss: 0.00003450
Iteration 3/1000 | Loss: 0.00002472
Iteration 4/1000 | Loss: 0.00002214
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00001960
Iteration 8/1000 | Loss: 0.00001901
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001797
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001694
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001685
Iteration 94/1000 | Loss: 0.00001685
Iteration 95/1000 | Loss: 0.00001685
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001685
Iteration 101/1000 | Loss: 0.00001685
Iteration 102/1000 | Loss: 0.00001685
Iteration 103/1000 | Loss: 0.00001685
Iteration 104/1000 | Loss: 0.00001684
Iteration 105/1000 | Loss: 0.00001684
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001684
Iteration 108/1000 | Loss: 0.00001684
Iteration 109/1000 | Loss: 0.00001684
Iteration 110/1000 | Loss: 0.00001684
Iteration 111/1000 | Loss: 0.00001684
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001683
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001682
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.6819709344417788e-05, 1.6819709344417788e-05, 1.6819709344417788e-05, 1.6819709344417788e-05, 1.6819709344417788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6819709344417788e-05

Optimization complete. Final v2v error: 3.4720795154571533 mm

Highest mean error: 4.0665283203125 mm for frame 62

Lowest mean error: 3.0913171768188477 mm for frame 2

Saving results

Total time: 39.406845808029175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069933
Iteration 2/25 | Loss: 0.00302610
Iteration 3/25 | Loss: 0.00226526
Iteration 4/25 | Loss: 0.00216070
Iteration 5/25 | Loss: 0.00197881
Iteration 6/25 | Loss: 0.00183388
Iteration 7/25 | Loss: 0.00176134
Iteration 8/25 | Loss: 0.00175173
Iteration 9/25 | Loss: 0.00174918
Iteration 10/25 | Loss: 0.00174697
Iteration 11/25 | Loss: 0.00174638
Iteration 12/25 | Loss: 0.00174421
Iteration 13/25 | Loss: 0.00174402
Iteration 14/25 | Loss: 0.00174391
Iteration 15/25 | Loss: 0.00174384
Iteration 16/25 | Loss: 0.00174381
Iteration 17/25 | Loss: 0.00174381
Iteration 18/25 | Loss: 0.00174381
Iteration 19/25 | Loss: 0.00174381
Iteration 20/25 | Loss: 0.00174381
Iteration 21/25 | Loss: 0.00174380
Iteration 22/25 | Loss: 0.00174380
Iteration 23/25 | Loss: 0.00174380
Iteration 24/25 | Loss: 0.00174380
Iteration 25/25 | Loss: 0.00174380

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10600603
Iteration 2/25 | Loss: 0.00331336
Iteration 3/25 | Loss: 0.00331335
Iteration 4/25 | Loss: 0.00331335
Iteration 5/25 | Loss: 0.00331335
Iteration 6/25 | Loss: 0.00331335
Iteration 7/25 | Loss: 0.00331335
Iteration 8/25 | Loss: 0.00331335
Iteration 9/25 | Loss: 0.00331335
Iteration 10/25 | Loss: 0.00331335
Iteration 11/25 | Loss: 0.00331335
Iteration 12/25 | Loss: 0.00331335
Iteration 13/25 | Loss: 0.00331335
Iteration 14/25 | Loss: 0.00331335
Iteration 15/25 | Loss: 0.00331335
Iteration 16/25 | Loss: 0.00331335
Iteration 17/25 | Loss: 0.00331335
Iteration 18/25 | Loss: 0.00331335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003313349559903145, 0.003313349559903145, 0.003313349559903145, 0.003313349559903145, 0.003313349559903145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003313349559903145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00331335
Iteration 2/1000 | Loss: 0.00029629
Iteration 3/1000 | Loss: 0.00022805
Iteration 4/1000 | Loss: 0.00020287
Iteration 5/1000 | Loss: 0.00019172
Iteration 6/1000 | Loss: 0.00018187
Iteration 7/1000 | Loss: 0.00017369
Iteration 8/1000 | Loss: 0.00016832
Iteration 9/1000 | Loss: 0.00016342
Iteration 10/1000 | Loss: 0.00015960
Iteration 11/1000 | Loss: 0.00015631
Iteration 12/1000 | Loss: 0.00015378
Iteration 13/1000 | Loss: 0.00015214
Iteration 14/1000 | Loss: 0.00015100
Iteration 15/1000 | Loss: 0.00015004
Iteration 16/1000 | Loss: 0.00014883
Iteration 17/1000 | Loss: 0.00014800
Iteration 18/1000 | Loss: 0.00014753
Iteration 19/1000 | Loss: 0.00014711
Iteration 20/1000 | Loss: 0.00014690
Iteration 21/1000 | Loss: 0.00014669
Iteration 22/1000 | Loss: 0.00014645
Iteration 23/1000 | Loss: 0.00014627
Iteration 24/1000 | Loss: 0.00014626
Iteration 25/1000 | Loss: 0.00014620
Iteration 26/1000 | Loss: 0.00014618
Iteration 27/1000 | Loss: 0.00014616
Iteration 28/1000 | Loss: 0.00014613
Iteration 29/1000 | Loss: 0.00014612
Iteration 30/1000 | Loss: 0.00014600
Iteration 31/1000 | Loss: 0.00014599
Iteration 32/1000 | Loss: 0.00014597
Iteration 33/1000 | Loss: 0.00014597
Iteration 34/1000 | Loss: 0.00014596
Iteration 35/1000 | Loss: 0.00014588
Iteration 36/1000 | Loss: 0.00014586
Iteration 37/1000 | Loss: 0.00014585
Iteration 38/1000 | Loss: 0.00014585
Iteration 39/1000 | Loss: 0.00014584
Iteration 40/1000 | Loss: 0.00014584
Iteration 41/1000 | Loss: 0.00014579
Iteration 42/1000 | Loss: 0.00014579
Iteration 43/1000 | Loss: 0.00014578
Iteration 44/1000 | Loss: 0.00014576
Iteration 45/1000 | Loss: 0.00014576
Iteration 46/1000 | Loss: 0.00014575
Iteration 47/1000 | Loss: 0.00014573
Iteration 48/1000 | Loss: 0.00014573
Iteration 49/1000 | Loss: 0.00014573
Iteration 50/1000 | Loss: 0.00014570
Iteration 51/1000 | Loss: 0.00014568
Iteration 52/1000 | Loss: 0.00014568
Iteration 53/1000 | Loss: 0.00014567
Iteration 54/1000 | Loss: 0.00014567
Iteration 55/1000 | Loss: 0.00014567
Iteration 56/1000 | Loss: 0.00014567
Iteration 57/1000 | Loss: 0.00014565
Iteration 58/1000 | Loss: 0.00014565
Iteration 59/1000 | Loss: 0.00014565
Iteration 60/1000 | Loss: 0.00014564
Iteration 61/1000 | Loss: 0.00014564
Iteration 62/1000 | Loss: 0.00014564
Iteration 63/1000 | Loss: 0.00014563
Iteration 64/1000 | Loss: 0.00014563
Iteration 65/1000 | Loss: 0.00014563
Iteration 66/1000 | Loss: 0.00014563
Iteration 67/1000 | Loss: 0.00014562
Iteration 68/1000 | Loss: 0.00014562
Iteration 69/1000 | Loss: 0.00014562
Iteration 70/1000 | Loss: 0.00014561
Iteration 71/1000 | Loss: 0.00014561
Iteration 72/1000 | Loss: 0.00014561
Iteration 73/1000 | Loss: 0.00014561
Iteration 74/1000 | Loss: 0.00014561
Iteration 75/1000 | Loss: 0.00014561
Iteration 76/1000 | Loss: 0.00014561
Iteration 77/1000 | Loss: 0.00014561
Iteration 78/1000 | Loss: 0.00014560
Iteration 79/1000 | Loss: 0.00014560
Iteration 80/1000 | Loss: 0.00014560
Iteration 81/1000 | Loss: 0.00014560
Iteration 82/1000 | Loss: 0.00014560
Iteration 83/1000 | Loss: 0.00014559
Iteration 84/1000 | Loss: 0.00014559
Iteration 85/1000 | Loss: 0.00014558
Iteration 86/1000 | Loss: 0.00014558
Iteration 87/1000 | Loss: 0.00014558
Iteration 88/1000 | Loss: 0.00014558
Iteration 89/1000 | Loss: 0.00014558
Iteration 90/1000 | Loss: 0.00014557
Iteration 91/1000 | Loss: 0.00014557
Iteration 92/1000 | Loss: 0.00014556
Iteration 93/1000 | Loss: 0.00014556
Iteration 94/1000 | Loss: 0.00014556
Iteration 95/1000 | Loss: 0.00014555
Iteration 96/1000 | Loss: 0.00014554
Iteration 97/1000 | Loss: 0.00014554
Iteration 98/1000 | Loss: 0.00014554
Iteration 99/1000 | Loss: 0.00014554
Iteration 100/1000 | Loss: 0.00014554
Iteration 101/1000 | Loss: 0.00014553
Iteration 102/1000 | Loss: 0.00014553
Iteration 103/1000 | Loss: 0.00014553
Iteration 104/1000 | Loss: 0.00014553
Iteration 105/1000 | Loss: 0.00014553
Iteration 106/1000 | Loss: 0.00014553
Iteration 107/1000 | Loss: 0.00014553
Iteration 108/1000 | Loss: 0.00014553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [0.00014553133223671466, 0.00014553133223671466, 0.00014553133223671466, 0.00014553133223671466, 0.00014553133223671466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014553133223671466

Optimization complete. Final v2v error: 7.180283546447754 mm

Highest mean error: 10.957951545715332 mm for frame 77

Lowest mean error: 4.988378524780273 mm for frame 59

Saving results

Total time: 82.4934868812561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001145
Iteration 2/25 | Loss: 0.01001145
Iteration 3/25 | Loss: 0.01001145
Iteration 4/25 | Loss: 0.00249740
Iteration 5/25 | Loss: 0.00172157
Iteration 6/25 | Loss: 0.00158589
Iteration 7/25 | Loss: 0.00150574
Iteration 8/25 | Loss: 0.00144703
Iteration 9/25 | Loss: 0.00154805
Iteration 10/25 | Loss: 0.00147268
Iteration 11/25 | Loss: 0.00140937
Iteration 12/25 | Loss: 0.00136916
Iteration 13/25 | Loss: 0.00135850
Iteration 14/25 | Loss: 0.00134908
Iteration 15/25 | Loss: 0.00134197
Iteration 16/25 | Loss: 0.00134121
Iteration 17/25 | Loss: 0.00134051
Iteration 18/25 | Loss: 0.00133827
Iteration 19/25 | Loss: 0.00133826
Iteration 20/25 | Loss: 0.00133826
Iteration 21/25 | Loss: 0.00133826
Iteration 22/25 | Loss: 0.00133826
Iteration 23/25 | Loss: 0.00133909
Iteration 24/25 | Loss: 0.00133908
Iteration 25/25 | Loss: 0.00133908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22327352
Iteration 2/25 | Loss: 0.00217441
Iteration 3/25 | Loss: 0.00212589
Iteration 4/25 | Loss: 0.00212589
Iteration 5/25 | Loss: 0.00212589
Iteration 6/25 | Loss: 0.00212589
Iteration 7/25 | Loss: 0.00212589
Iteration 8/25 | Loss: 0.00212589
Iteration 9/25 | Loss: 0.00212589
Iteration 10/25 | Loss: 0.00212589
Iteration 11/25 | Loss: 0.00212589
Iteration 12/25 | Loss: 0.00212589
Iteration 13/25 | Loss: 0.00212589
Iteration 14/25 | Loss: 0.00212589
Iteration 15/25 | Loss: 0.00212589
Iteration 16/25 | Loss: 0.00212589
Iteration 17/25 | Loss: 0.00212589
Iteration 18/25 | Loss: 0.00212589
Iteration 19/25 | Loss: 0.00212589
Iteration 20/25 | Loss: 0.00212589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002125886967405677, 0.002125886967405677, 0.002125886967405677, 0.002125886967405677, 0.002125886967405677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002125886967405677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212589
Iteration 2/1000 | Loss: 0.00004394
Iteration 3/1000 | Loss: 0.00006478
Iteration 4/1000 | Loss: 0.00003074
Iteration 5/1000 | Loss: 0.00003879
Iteration 6/1000 | Loss: 0.00003299
Iteration 7/1000 | Loss: 0.00007650
Iteration 8/1000 | Loss: 0.00002234
Iteration 9/1000 | Loss: 0.00029225
Iteration 10/1000 | Loss: 0.00003247
Iteration 11/1000 | Loss: 0.00002736
Iteration 12/1000 | Loss: 0.00002105
Iteration 13/1000 | Loss: 0.00005178
Iteration 14/1000 | Loss: 0.00002484
Iteration 15/1000 | Loss: 0.00061922
Iteration 16/1000 | Loss: 0.00010527
Iteration 17/1000 | Loss: 0.00006252
Iteration 18/1000 | Loss: 0.00003181
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00004447
Iteration 21/1000 | Loss: 0.00002990
Iteration 22/1000 | Loss: 0.00002538
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00002588
Iteration 25/1000 | Loss: 0.00003115
Iteration 26/1000 | Loss: 0.00001916
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001630
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001608
Iteration 32/1000 | Loss: 0.00002079
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001829
Iteration 36/1000 | Loss: 0.00005890
Iteration 37/1000 | Loss: 0.00002250
Iteration 38/1000 | Loss: 0.00001963
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001510
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001478
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001476
Iteration 52/1000 | Loss: 0.00001476
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001474
Iteration 70/1000 | Loss: 0.00001474
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00002806
Iteration 79/1000 | Loss: 0.00007577
Iteration 80/1000 | Loss: 0.00002318
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00002222
Iteration 84/1000 | Loss: 0.00001688
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001464
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001495
Iteration 94/1000 | Loss: 0.00001463
Iteration 95/1000 | Loss: 0.00001463
Iteration 96/1000 | Loss: 0.00001463
Iteration 97/1000 | Loss: 0.00001463
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001463
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001463
Iteration 104/1000 | Loss: 0.00001463
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001463
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001463
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4625915355281904e-05, 1.4625915355281904e-05, 1.4625915355281904e-05, 1.4625915355281904e-05, 1.4625915355281904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4625915355281904e-05

Optimization complete. Final v2v error: 3.3192951679229736 mm

Highest mean error: 3.740635871887207 mm for frame 22

Lowest mean error: 3.0034899711608887 mm for frame 63

Saving results

Total time: 111.79815864562988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742381
Iteration 2/25 | Loss: 0.00146875
Iteration 3/25 | Loss: 0.00135921
Iteration 4/25 | Loss: 0.00133356
Iteration 5/25 | Loss: 0.00132479
Iteration 6/25 | Loss: 0.00132287
Iteration 7/25 | Loss: 0.00132287
Iteration 8/25 | Loss: 0.00132287
Iteration 9/25 | Loss: 0.00132287
Iteration 10/25 | Loss: 0.00132287
Iteration 11/25 | Loss: 0.00132287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001322867930866778, 0.001322867930866778, 0.001322867930866778, 0.001322867930866778, 0.001322867930866778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322867930866778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20029020
Iteration 2/25 | Loss: 0.00349422
Iteration 3/25 | Loss: 0.00349422
Iteration 4/25 | Loss: 0.00349422
Iteration 5/25 | Loss: 0.00349422
Iteration 6/25 | Loss: 0.00349422
Iteration 7/25 | Loss: 0.00349422
Iteration 8/25 | Loss: 0.00349422
Iteration 9/25 | Loss: 0.00349422
Iteration 10/25 | Loss: 0.00349422
Iteration 11/25 | Loss: 0.00349422
Iteration 12/25 | Loss: 0.00349422
Iteration 13/25 | Loss: 0.00349422
Iteration 14/25 | Loss: 0.00349422
Iteration 15/25 | Loss: 0.00349422
Iteration 16/25 | Loss: 0.00349422
Iteration 17/25 | Loss: 0.00349422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0034942184574902058, 0.0034942184574902058, 0.0034942184574902058, 0.0034942184574902058, 0.0034942184574902058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034942184574902058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00349422
Iteration 2/1000 | Loss: 0.00004196
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00002181
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001868
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001743
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001728
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00001726
Iteration 25/1000 | Loss: 0.00001726
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001721
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001712
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001712
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001700
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001695
Iteration 56/1000 | Loss: 0.00001695
Iteration 57/1000 | Loss: 0.00001694
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001687
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001687
Iteration 72/1000 | Loss: 0.00001687
Iteration 73/1000 | Loss: 0.00001686
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001686
Iteration 76/1000 | Loss: 0.00001686
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001679
Iteration 103/1000 | Loss: 0.00001679
Iteration 104/1000 | Loss: 0.00001679
Iteration 105/1000 | Loss: 0.00001679
Iteration 106/1000 | Loss: 0.00001679
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00001679
Iteration 109/1000 | Loss: 0.00001679
Iteration 110/1000 | Loss: 0.00001679
Iteration 111/1000 | Loss: 0.00001678
Iteration 112/1000 | Loss: 0.00001678
Iteration 113/1000 | Loss: 0.00001678
Iteration 114/1000 | Loss: 0.00001678
Iteration 115/1000 | Loss: 0.00001678
Iteration 116/1000 | Loss: 0.00001678
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001677
Iteration 124/1000 | Loss: 0.00001677
Iteration 125/1000 | Loss: 0.00001677
Iteration 126/1000 | Loss: 0.00001677
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001677
Iteration 129/1000 | Loss: 0.00001677
Iteration 130/1000 | Loss: 0.00001677
Iteration 131/1000 | Loss: 0.00001677
Iteration 132/1000 | Loss: 0.00001677
Iteration 133/1000 | Loss: 0.00001677
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001675
Iteration 138/1000 | Loss: 0.00001675
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001674
Iteration 141/1000 | Loss: 0.00001674
Iteration 142/1000 | Loss: 0.00001674
Iteration 143/1000 | Loss: 0.00001674
Iteration 144/1000 | Loss: 0.00001674
Iteration 145/1000 | Loss: 0.00001674
Iteration 146/1000 | Loss: 0.00001674
Iteration 147/1000 | Loss: 0.00001674
Iteration 148/1000 | Loss: 0.00001673
Iteration 149/1000 | Loss: 0.00001673
Iteration 150/1000 | Loss: 0.00001673
Iteration 151/1000 | Loss: 0.00001673
Iteration 152/1000 | Loss: 0.00001673
Iteration 153/1000 | Loss: 0.00001673
Iteration 154/1000 | Loss: 0.00001672
Iteration 155/1000 | Loss: 0.00001672
Iteration 156/1000 | Loss: 0.00001672
Iteration 157/1000 | Loss: 0.00001672
Iteration 158/1000 | Loss: 0.00001672
Iteration 159/1000 | Loss: 0.00001671
Iteration 160/1000 | Loss: 0.00001671
Iteration 161/1000 | Loss: 0.00001671
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001670
Iteration 165/1000 | Loss: 0.00001670
Iteration 166/1000 | Loss: 0.00001670
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001669
Iteration 172/1000 | Loss: 0.00001669
Iteration 173/1000 | Loss: 0.00001669
Iteration 174/1000 | Loss: 0.00001669
Iteration 175/1000 | Loss: 0.00001669
Iteration 176/1000 | Loss: 0.00001668
Iteration 177/1000 | Loss: 0.00001668
Iteration 178/1000 | Loss: 0.00001668
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001668
Iteration 186/1000 | Loss: 0.00001668
Iteration 187/1000 | Loss: 0.00001668
Iteration 188/1000 | Loss: 0.00001667
Iteration 189/1000 | Loss: 0.00001667
Iteration 190/1000 | Loss: 0.00001667
Iteration 191/1000 | Loss: 0.00001666
Iteration 192/1000 | Loss: 0.00001666
Iteration 193/1000 | Loss: 0.00001666
Iteration 194/1000 | Loss: 0.00001666
Iteration 195/1000 | Loss: 0.00001666
Iteration 196/1000 | Loss: 0.00001666
Iteration 197/1000 | Loss: 0.00001666
Iteration 198/1000 | Loss: 0.00001666
Iteration 199/1000 | Loss: 0.00001665
Iteration 200/1000 | Loss: 0.00001665
Iteration 201/1000 | Loss: 0.00001665
Iteration 202/1000 | Loss: 0.00001665
Iteration 203/1000 | Loss: 0.00001665
Iteration 204/1000 | Loss: 0.00001665
Iteration 205/1000 | Loss: 0.00001665
Iteration 206/1000 | Loss: 0.00001665
Iteration 207/1000 | Loss: 0.00001665
Iteration 208/1000 | Loss: 0.00001665
Iteration 209/1000 | Loss: 0.00001665
Iteration 210/1000 | Loss: 0.00001665
Iteration 211/1000 | Loss: 0.00001665
Iteration 212/1000 | Loss: 0.00001665
Iteration 213/1000 | Loss: 0.00001665
Iteration 214/1000 | Loss: 0.00001665
Iteration 215/1000 | Loss: 0.00001665
Iteration 216/1000 | Loss: 0.00001664
Iteration 217/1000 | Loss: 0.00001664
Iteration 218/1000 | Loss: 0.00001664
Iteration 219/1000 | Loss: 0.00001664
Iteration 220/1000 | Loss: 0.00001664
Iteration 221/1000 | Loss: 0.00001664
Iteration 222/1000 | Loss: 0.00001664
Iteration 223/1000 | Loss: 0.00001664
Iteration 224/1000 | Loss: 0.00001664
Iteration 225/1000 | Loss: 0.00001664
Iteration 226/1000 | Loss: 0.00001664
Iteration 227/1000 | Loss: 0.00001664
Iteration 228/1000 | Loss: 0.00001664
Iteration 229/1000 | Loss: 0.00001664
Iteration 230/1000 | Loss: 0.00001664
Iteration 231/1000 | Loss: 0.00001664
Iteration 232/1000 | Loss: 0.00001664
Iteration 233/1000 | Loss: 0.00001664
Iteration 234/1000 | Loss: 0.00001664
Iteration 235/1000 | Loss: 0.00001664
Iteration 236/1000 | Loss: 0.00001664
Iteration 237/1000 | Loss: 0.00001664
Iteration 238/1000 | Loss: 0.00001664
Iteration 239/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.663682996877469e-05, 1.663682996877469e-05, 1.663682996877469e-05, 1.663682996877469e-05, 1.663682996877469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.663682996877469e-05

Optimization complete. Final v2v error: 3.4655933380126953 mm

Highest mean error: 4.308220863342285 mm for frame 59

Lowest mean error: 2.7872471809387207 mm for frame 121

Saving results

Total time: 53.54969501495361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426309
Iteration 2/25 | Loss: 0.00144182
Iteration 3/25 | Loss: 0.00136429
Iteration 4/25 | Loss: 0.00135773
Iteration 5/25 | Loss: 0.00135581
Iteration 6/25 | Loss: 0.00135579
Iteration 7/25 | Loss: 0.00135579
Iteration 8/25 | Loss: 0.00135579
Iteration 9/25 | Loss: 0.00135579
Iteration 10/25 | Loss: 0.00135579
Iteration 11/25 | Loss: 0.00135579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001355786225758493, 0.001355786225758493, 0.001355786225758493, 0.001355786225758493, 0.001355786225758493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355786225758493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24446678
Iteration 2/25 | Loss: 0.00185878
Iteration 3/25 | Loss: 0.00185878
Iteration 4/25 | Loss: 0.00185878
Iteration 5/25 | Loss: 0.00185878
Iteration 6/25 | Loss: 0.00185878
Iteration 7/25 | Loss: 0.00185878
Iteration 8/25 | Loss: 0.00185878
Iteration 9/25 | Loss: 0.00185878
Iteration 10/25 | Loss: 0.00185878
Iteration 11/25 | Loss: 0.00185878
Iteration 12/25 | Loss: 0.00185878
Iteration 13/25 | Loss: 0.00185878
Iteration 14/25 | Loss: 0.00185878
Iteration 15/25 | Loss: 0.00185878
Iteration 16/25 | Loss: 0.00185878
Iteration 17/25 | Loss: 0.00185878
Iteration 18/25 | Loss: 0.00185878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018587785307317972, 0.0018587785307317972, 0.0018587785307317972, 0.0018587785307317972, 0.0018587785307317972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018587785307317972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185878
Iteration 2/1000 | Loss: 0.00003904
Iteration 3/1000 | Loss: 0.00002283
Iteration 4/1000 | Loss: 0.00001998
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001617
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001528
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001488
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001458
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001433
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001431
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001430
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001428
Iteration 29/1000 | Loss: 0.00001427
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001421
Iteration 40/1000 | Loss: 0.00001421
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001418
Iteration 50/1000 | Loss: 0.00001418
Iteration 51/1000 | Loss: 0.00001418
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001414
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001413
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001412
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001411
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001406
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001405
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001404
Iteration 112/1000 | Loss: 0.00001404
Iteration 113/1000 | Loss: 0.00001404
Iteration 114/1000 | Loss: 0.00001404
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001404
Iteration 117/1000 | Loss: 0.00001404
Iteration 118/1000 | Loss: 0.00001404
Iteration 119/1000 | Loss: 0.00001404
Iteration 120/1000 | Loss: 0.00001403
Iteration 121/1000 | Loss: 0.00001403
Iteration 122/1000 | Loss: 0.00001403
Iteration 123/1000 | Loss: 0.00001403
Iteration 124/1000 | Loss: 0.00001403
Iteration 125/1000 | Loss: 0.00001403
Iteration 126/1000 | Loss: 0.00001403
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001403
Iteration 129/1000 | Loss: 0.00001403
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001402
Iteration 143/1000 | Loss: 0.00001402
Iteration 144/1000 | Loss: 0.00001402
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001401
Iteration 147/1000 | Loss: 0.00001401
Iteration 148/1000 | Loss: 0.00001401
Iteration 149/1000 | Loss: 0.00001401
Iteration 150/1000 | Loss: 0.00001401
Iteration 151/1000 | Loss: 0.00001401
Iteration 152/1000 | Loss: 0.00001401
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Iteration 160/1000 | Loss: 0.00001400
Iteration 161/1000 | Loss: 0.00001400
Iteration 162/1000 | Loss: 0.00001399
Iteration 163/1000 | Loss: 0.00001399
Iteration 164/1000 | Loss: 0.00001399
Iteration 165/1000 | Loss: 0.00001399
Iteration 166/1000 | Loss: 0.00001398
Iteration 167/1000 | Loss: 0.00001398
Iteration 168/1000 | Loss: 0.00001398
Iteration 169/1000 | Loss: 0.00001398
Iteration 170/1000 | Loss: 0.00001398
Iteration 171/1000 | Loss: 0.00001398
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001397
Iteration 175/1000 | Loss: 0.00001397
Iteration 176/1000 | Loss: 0.00001397
Iteration 177/1000 | Loss: 0.00001397
Iteration 178/1000 | Loss: 0.00001397
Iteration 179/1000 | Loss: 0.00001396
Iteration 180/1000 | Loss: 0.00001396
Iteration 181/1000 | Loss: 0.00001396
Iteration 182/1000 | Loss: 0.00001396
Iteration 183/1000 | Loss: 0.00001396
Iteration 184/1000 | Loss: 0.00001396
Iteration 185/1000 | Loss: 0.00001396
Iteration 186/1000 | Loss: 0.00001396
Iteration 187/1000 | Loss: 0.00001396
Iteration 188/1000 | Loss: 0.00001395
Iteration 189/1000 | Loss: 0.00001395
Iteration 190/1000 | Loss: 0.00001395
Iteration 191/1000 | Loss: 0.00001395
Iteration 192/1000 | Loss: 0.00001394
Iteration 193/1000 | Loss: 0.00001394
Iteration 194/1000 | Loss: 0.00001394
Iteration 195/1000 | Loss: 0.00001394
Iteration 196/1000 | Loss: 0.00001393
Iteration 197/1000 | Loss: 0.00001393
Iteration 198/1000 | Loss: 0.00001393
Iteration 199/1000 | Loss: 0.00001393
Iteration 200/1000 | Loss: 0.00001393
Iteration 201/1000 | Loss: 0.00001393
Iteration 202/1000 | Loss: 0.00001393
Iteration 203/1000 | Loss: 0.00001393
Iteration 204/1000 | Loss: 0.00001393
Iteration 205/1000 | Loss: 0.00001393
Iteration 206/1000 | Loss: 0.00001393
Iteration 207/1000 | Loss: 0.00001393
Iteration 208/1000 | Loss: 0.00001393
Iteration 209/1000 | Loss: 0.00001393
Iteration 210/1000 | Loss: 0.00001393
Iteration 211/1000 | Loss: 0.00001393
Iteration 212/1000 | Loss: 0.00001393
Iteration 213/1000 | Loss: 0.00001392
Iteration 214/1000 | Loss: 0.00001392
Iteration 215/1000 | Loss: 0.00001392
Iteration 216/1000 | Loss: 0.00001391
Iteration 217/1000 | Loss: 0.00001391
Iteration 218/1000 | Loss: 0.00001391
Iteration 219/1000 | Loss: 0.00001391
Iteration 220/1000 | Loss: 0.00001391
Iteration 221/1000 | Loss: 0.00001391
Iteration 222/1000 | Loss: 0.00001391
Iteration 223/1000 | Loss: 0.00001391
Iteration 224/1000 | Loss: 0.00001391
Iteration 225/1000 | Loss: 0.00001390
Iteration 226/1000 | Loss: 0.00001390
Iteration 227/1000 | Loss: 0.00001390
Iteration 228/1000 | Loss: 0.00001390
Iteration 229/1000 | Loss: 0.00001390
Iteration 230/1000 | Loss: 0.00001389
Iteration 231/1000 | Loss: 0.00001389
Iteration 232/1000 | Loss: 0.00001389
Iteration 233/1000 | Loss: 0.00001389
Iteration 234/1000 | Loss: 0.00001388
Iteration 235/1000 | Loss: 0.00001388
Iteration 236/1000 | Loss: 0.00001388
Iteration 237/1000 | Loss: 0.00001388
Iteration 238/1000 | Loss: 0.00001388
Iteration 239/1000 | Loss: 0.00001388
Iteration 240/1000 | Loss: 0.00001388
Iteration 241/1000 | Loss: 0.00001388
Iteration 242/1000 | Loss: 0.00001388
Iteration 243/1000 | Loss: 0.00001388
Iteration 244/1000 | Loss: 0.00001387
Iteration 245/1000 | Loss: 0.00001387
Iteration 246/1000 | Loss: 0.00001387
Iteration 247/1000 | Loss: 0.00001387
Iteration 248/1000 | Loss: 0.00001387
Iteration 249/1000 | Loss: 0.00001387
Iteration 250/1000 | Loss: 0.00001387
Iteration 251/1000 | Loss: 0.00001387
Iteration 252/1000 | Loss: 0.00001387
Iteration 253/1000 | Loss: 0.00001387
Iteration 254/1000 | Loss: 0.00001387
Iteration 255/1000 | Loss: 0.00001387
Iteration 256/1000 | Loss: 0.00001387
Iteration 257/1000 | Loss: 0.00001387
Iteration 258/1000 | Loss: 0.00001387
Iteration 259/1000 | Loss: 0.00001387
Iteration 260/1000 | Loss: 0.00001386
Iteration 261/1000 | Loss: 0.00001386
Iteration 262/1000 | Loss: 0.00001386
Iteration 263/1000 | Loss: 0.00001386
Iteration 264/1000 | Loss: 0.00001386
Iteration 265/1000 | Loss: 0.00001386
Iteration 266/1000 | Loss: 0.00001386
Iteration 267/1000 | Loss: 0.00001386
Iteration 268/1000 | Loss: 0.00001386
Iteration 269/1000 | Loss: 0.00001386
Iteration 270/1000 | Loss: 0.00001386
Iteration 271/1000 | Loss: 0.00001386
Iteration 272/1000 | Loss: 0.00001386
Iteration 273/1000 | Loss: 0.00001385
Iteration 274/1000 | Loss: 0.00001385
Iteration 275/1000 | Loss: 0.00001385
Iteration 276/1000 | Loss: 0.00001385
Iteration 277/1000 | Loss: 0.00001385
Iteration 278/1000 | Loss: 0.00001385
Iteration 279/1000 | Loss: 0.00001385
Iteration 280/1000 | Loss: 0.00001385
Iteration 281/1000 | Loss: 0.00001385
Iteration 282/1000 | Loss: 0.00001385
Iteration 283/1000 | Loss: 0.00001384
Iteration 284/1000 | Loss: 0.00001384
Iteration 285/1000 | Loss: 0.00001384
Iteration 286/1000 | Loss: 0.00001384
Iteration 287/1000 | Loss: 0.00001384
Iteration 288/1000 | Loss: 0.00001384
Iteration 289/1000 | Loss: 0.00001384
Iteration 290/1000 | Loss: 0.00001384
Iteration 291/1000 | Loss: 0.00001384
Iteration 292/1000 | Loss: 0.00001384
Iteration 293/1000 | Loss: 0.00001384
Iteration 294/1000 | Loss: 0.00001384
Iteration 295/1000 | Loss: 0.00001383
Iteration 296/1000 | Loss: 0.00001383
Iteration 297/1000 | Loss: 0.00001383
Iteration 298/1000 | Loss: 0.00001383
Iteration 299/1000 | Loss: 0.00001383
Iteration 300/1000 | Loss: 0.00001383
Iteration 301/1000 | Loss: 0.00001383
Iteration 302/1000 | Loss: 0.00001383
Iteration 303/1000 | Loss: 0.00001383
Iteration 304/1000 | Loss: 0.00001383
Iteration 305/1000 | Loss: 0.00001383
Iteration 306/1000 | Loss: 0.00001383
Iteration 307/1000 | Loss: 0.00001383
Iteration 308/1000 | Loss: 0.00001383
Iteration 309/1000 | Loss: 0.00001383
Iteration 310/1000 | Loss: 0.00001383
Iteration 311/1000 | Loss: 0.00001383
Iteration 312/1000 | Loss: 0.00001383
Iteration 313/1000 | Loss: 0.00001383
Iteration 314/1000 | Loss: 0.00001383
Iteration 315/1000 | Loss: 0.00001383
Iteration 316/1000 | Loss: 0.00001383
Iteration 317/1000 | Loss: 0.00001382
Iteration 318/1000 | Loss: 0.00001382
Iteration 319/1000 | Loss: 0.00001382
Iteration 320/1000 | Loss: 0.00001382
Iteration 321/1000 | Loss: 0.00001382
Iteration 322/1000 | Loss: 0.00001382
Iteration 323/1000 | Loss: 0.00001381
Iteration 324/1000 | Loss: 0.00001381
Iteration 325/1000 | Loss: 0.00001381
Iteration 326/1000 | Loss: 0.00001381
Iteration 327/1000 | Loss: 0.00001380
Iteration 328/1000 | Loss: 0.00001380
Iteration 329/1000 | Loss: 0.00001380
Iteration 330/1000 | Loss: 0.00001380
Iteration 331/1000 | Loss: 0.00001379
Iteration 332/1000 | Loss: 0.00001379
Iteration 333/1000 | Loss: 0.00001379
Iteration 334/1000 | Loss: 0.00001379
Iteration 335/1000 | Loss: 0.00001379
Iteration 336/1000 | Loss: 0.00001378
Iteration 337/1000 | Loss: 0.00001378
Iteration 338/1000 | Loss: 0.00001378
Iteration 339/1000 | Loss: 0.00001378
Iteration 340/1000 | Loss: 0.00001378
Iteration 341/1000 | Loss: 0.00001378
Iteration 342/1000 | Loss: 0.00001378
Iteration 343/1000 | Loss: 0.00001378
Iteration 344/1000 | Loss: 0.00001378
Iteration 345/1000 | Loss: 0.00001378
Iteration 346/1000 | Loss: 0.00001378
Iteration 347/1000 | Loss: 0.00001378
Iteration 348/1000 | Loss: 0.00001378
Iteration 349/1000 | Loss: 0.00001378
Iteration 350/1000 | Loss: 0.00001378
Iteration 351/1000 | Loss: 0.00001378
Iteration 352/1000 | Loss: 0.00001378
Iteration 353/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.37808128783945e-05, 1.37808128783945e-05, 1.37808128783945e-05, 1.37808128783945e-05, 1.37808128783945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.37808128783945e-05

Optimization complete. Final v2v error: 3.236725330352783 mm

Highest mean error: 3.5119996070861816 mm for frame 30

Lowest mean error: 3.059804677963257 mm for frame 8

Saving results

Total time: 47.81878304481506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785425
Iteration 2/25 | Loss: 0.00151026
Iteration 3/25 | Loss: 0.00139023
Iteration 4/25 | Loss: 0.00137774
Iteration 5/25 | Loss: 0.00137381
Iteration 6/25 | Loss: 0.00137301
Iteration 7/25 | Loss: 0.00137301
Iteration 8/25 | Loss: 0.00137301
Iteration 9/25 | Loss: 0.00137301
Iteration 10/25 | Loss: 0.00137301
Iteration 11/25 | Loss: 0.00137301
Iteration 12/25 | Loss: 0.00137301
Iteration 13/25 | Loss: 0.00137301
Iteration 14/25 | Loss: 0.00137301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013730113860219717, 0.0013730113860219717, 0.0013730113860219717, 0.0013730113860219717, 0.0013730113860219717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013730113860219717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16136086
Iteration 2/25 | Loss: 0.00160610
Iteration 3/25 | Loss: 0.00160606
Iteration 4/25 | Loss: 0.00160606
Iteration 5/25 | Loss: 0.00160606
Iteration 6/25 | Loss: 0.00160606
Iteration 7/25 | Loss: 0.00160606
Iteration 8/25 | Loss: 0.00160606
Iteration 9/25 | Loss: 0.00160606
Iteration 10/25 | Loss: 0.00160606
Iteration 11/25 | Loss: 0.00160606
Iteration 12/25 | Loss: 0.00160606
Iteration 13/25 | Loss: 0.00160606
Iteration 14/25 | Loss: 0.00160606
Iteration 15/25 | Loss: 0.00160606
Iteration 16/25 | Loss: 0.00160606
Iteration 17/25 | Loss: 0.00160606
Iteration 18/25 | Loss: 0.00160606
Iteration 19/25 | Loss: 0.00160606
Iteration 20/25 | Loss: 0.00160606
Iteration 21/25 | Loss: 0.00160606
Iteration 22/25 | Loss: 0.00160606
Iteration 23/25 | Loss: 0.00160606
Iteration 24/25 | Loss: 0.00160606
Iteration 25/25 | Loss: 0.00160606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160606
Iteration 2/1000 | Loss: 0.00004505
Iteration 3/1000 | Loss: 0.00003090
Iteration 4/1000 | Loss: 0.00002447
Iteration 5/1000 | Loss: 0.00002191
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00001960
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001840
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001739
Iteration 14/1000 | Loss: 0.00001721
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001701
Iteration 22/1000 | Loss: 0.00001699
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001677
Iteration 34/1000 | Loss: 0.00001676
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001667
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001667
Iteration 52/1000 | Loss: 0.00001666
Iteration 53/1000 | Loss: 0.00001666
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001664
Iteration 57/1000 | Loss: 0.00001664
Iteration 58/1000 | Loss: 0.00001664
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001663
Iteration 63/1000 | Loss: 0.00001663
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001654
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00001654
Iteration 94/1000 | Loss: 0.00001654
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001653
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001652
Iteration 101/1000 | Loss: 0.00001652
Iteration 102/1000 | Loss: 0.00001652
Iteration 103/1000 | Loss: 0.00001652
Iteration 104/1000 | Loss: 0.00001652
Iteration 105/1000 | Loss: 0.00001652
Iteration 106/1000 | Loss: 0.00001652
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001649
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001648
Iteration 127/1000 | Loss: 0.00001648
Iteration 128/1000 | Loss: 0.00001648
Iteration 129/1000 | Loss: 0.00001648
Iteration 130/1000 | Loss: 0.00001648
Iteration 131/1000 | Loss: 0.00001648
Iteration 132/1000 | Loss: 0.00001648
Iteration 133/1000 | Loss: 0.00001648
Iteration 134/1000 | Loss: 0.00001648
Iteration 135/1000 | Loss: 0.00001648
Iteration 136/1000 | Loss: 0.00001648
Iteration 137/1000 | Loss: 0.00001648
Iteration 138/1000 | Loss: 0.00001647
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001647
Iteration 141/1000 | Loss: 0.00001647
Iteration 142/1000 | Loss: 0.00001647
Iteration 143/1000 | Loss: 0.00001647
Iteration 144/1000 | Loss: 0.00001647
Iteration 145/1000 | Loss: 0.00001647
Iteration 146/1000 | Loss: 0.00001647
Iteration 147/1000 | Loss: 0.00001647
Iteration 148/1000 | Loss: 0.00001647
Iteration 149/1000 | Loss: 0.00001646
Iteration 150/1000 | Loss: 0.00001646
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001646
Iteration 153/1000 | Loss: 0.00001646
Iteration 154/1000 | Loss: 0.00001646
Iteration 155/1000 | Loss: 0.00001646
Iteration 156/1000 | Loss: 0.00001645
Iteration 157/1000 | Loss: 0.00001645
Iteration 158/1000 | Loss: 0.00001645
Iteration 159/1000 | Loss: 0.00001645
Iteration 160/1000 | Loss: 0.00001645
Iteration 161/1000 | Loss: 0.00001644
Iteration 162/1000 | Loss: 0.00001644
Iteration 163/1000 | Loss: 0.00001644
Iteration 164/1000 | Loss: 0.00001644
Iteration 165/1000 | Loss: 0.00001644
Iteration 166/1000 | Loss: 0.00001644
Iteration 167/1000 | Loss: 0.00001644
Iteration 168/1000 | Loss: 0.00001644
Iteration 169/1000 | Loss: 0.00001644
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001644
Iteration 172/1000 | Loss: 0.00001644
Iteration 173/1000 | Loss: 0.00001644
Iteration 174/1000 | Loss: 0.00001644
Iteration 175/1000 | Loss: 0.00001644
Iteration 176/1000 | Loss: 0.00001644
Iteration 177/1000 | Loss: 0.00001644
Iteration 178/1000 | Loss: 0.00001644
Iteration 179/1000 | Loss: 0.00001644
Iteration 180/1000 | Loss: 0.00001644
Iteration 181/1000 | Loss: 0.00001644
Iteration 182/1000 | Loss: 0.00001644
Iteration 183/1000 | Loss: 0.00001644
Iteration 184/1000 | Loss: 0.00001644
Iteration 185/1000 | Loss: 0.00001644
Iteration 186/1000 | Loss: 0.00001644
Iteration 187/1000 | Loss: 0.00001644
Iteration 188/1000 | Loss: 0.00001644
Iteration 189/1000 | Loss: 0.00001644
Iteration 190/1000 | Loss: 0.00001644
Iteration 191/1000 | Loss: 0.00001644
Iteration 192/1000 | Loss: 0.00001644
Iteration 193/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.6435107681900263e-05, 1.6435107681900263e-05, 1.6435107681900263e-05, 1.6435107681900263e-05, 1.6435107681900263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6435107681900263e-05

Optimization complete. Final v2v error: 3.4175500869750977 mm

Highest mean error: 4.638040542602539 mm for frame 52

Lowest mean error: 2.7734107971191406 mm for frame 82

Saving results

Total time: 43.766433000564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783793
Iteration 2/25 | Loss: 0.00151019
Iteration 3/25 | Loss: 0.00133498
Iteration 4/25 | Loss: 0.00132099
Iteration 5/25 | Loss: 0.00131669
Iteration 6/25 | Loss: 0.00131551
Iteration 7/25 | Loss: 0.00131551
Iteration 8/25 | Loss: 0.00131551
Iteration 9/25 | Loss: 0.00131551
Iteration 10/25 | Loss: 0.00131551
Iteration 11/25 | Loss: 0.00131551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013155069900676608, 0.0013155069900676608, 0.0013155069900676608, 0.0013155069900676608, 0.0013155069900676608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013155069900676608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11021626
Iteration 2/25 | Loss: 0.00224622
Iteration 3/25 | Loss: 0.00224622
Iteration 4/25 | Loss: 0.00224622
Iteration 5/25 | Loss: 0.00224622
Iteration 6/25 | Loss: 0.00224621
Iteration 7/25 | Loss: 0.00224621
Iteration 8/25 | Loss: 0.00224621
Iteration 9/25 | Loss: 0.00224621
Iteration 10/25 | Loss: 0.00224621
Iteration 11/25 | Loss: 0.00224621
Iteration 12/25 | Loss: 0.00224621
Iteration 13/25 | Loss: 0.00224621
Iteration 14/25 | Loss: 0.00224621
Iteration 15/25 | Loss: 0.00224621
Iteration 16/25 | Loss: 0.00224621
Iteration 17/25 | Loss: 0.00224621
Iteration 18/25 | Loss: 0.00224621
Iteration 19/25 | Loss: 0.00224621
Iteration 20/25 | Loss: 0.00224621
Iteration 21/25 | Loss: 0.00224621
Iteration 22/25 | Loss: 0.00224621
Iteration 23/25 | Loss: 0.00224621
Iteration 24/25 | Loss: 0.00224621
Iteration 25/25 | Loss: 0.00224621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224621
Iteration 2/1000 | Loss: 0.00003551
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00001865
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001511
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001334
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001329
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001313
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001313
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001309
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001309
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001308
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001307
Iteration 75/1000 | Loss: 0.00001307
Iteration 76/1000 | Loss: 0.00001307
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001306
Iteration 79/1000 | Loss: 0.00001306
Iteration 80/1000 | Loss: 0.00001306
Iteration 81/1000 | Loss: 0.00001306
Iteration 82/1000 | Loss: 0.00001305
Iteration 83/1000 | Loss: 0.00001305
Iteration 84/1000 | Loss: 0.00001305
Iteration 85/1000 | Loss: 0.00001305
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001304
Iteration 92/1000 | Loss: 0.00001303
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001302
Iteration 95/1000 | Loss: 0.00001302
Iteration 96/1000 | Loss: 0.00001302
Iteration 97/1000 | Loss: 0.00001301
Iteration 98/1000 | Loss: 0.00001301
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001299
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001298
Iteration 111/1000 | Loss: 0.00001298
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001296
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001296
Iteration 127/1000 | Loss: 0.00001295
Iteration 128/1000 | Loss: 0.00001295
Iteration 129/1000 | Loss: 0.00001295
Iteration 130/1000 | Loss: 0.00001295
Iteration 131/1000 | Loss: 0.00001295
Iteration 132/1000 | Loss: 0.00001295
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001293
Iteration 137/1000 | Loss: 0.00001293
Iteration 138/1000 | Loss: 0.00001293
Iteration 139/1000 | Loss: 0.00001293
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001292
Iteration 146/1000 | Loss: 0.00001292
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001292
Iteration 149/1000 | Loss: 0.00001292
Iteration 150/1000 | Loss: 0.00001292
Iteration 151/1000 | Loss: 0.00001292
Iteration 152/1000 | Loss: 0.00001292
Iteration 153/1000 | Loss: 0.00001292
Iteration 154/1000 | Loss: 0.00001292
Iteration 155/1000 | Loss: 0.00001292
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001292
Iteration 163/1000 | Loss: 0.00001292
Iteration 164/1000 | Loss: 0.00001292
Iteration 165/1000 | Loss: 0.00001292
Iteration 166/1000 | Loss: 0.00001292
Iteration 167/1000 | Loss: 0.00001292
Iteration 168/1000 | Loss: 0.00001292
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001292
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001292
Iteration 176/1000 | Loss: 0.00001292
Iteration 177/1000 | Loss: 0.00001292
Iteration 178/1000 | Loss: 0.00001292
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001292
Iteration 181/1000 | Loss: 0.00001292
Iteration 182/1000 | Loss: 0.00001292
Iteration 183/1000 | Loss: 0.00001292
Iteration 184/1000 | Loss: 0.00001292
Iteration 185/1000 | Loss: 0.00001292
Iteration 186/1000 | Loss: 0.00001292
Iteration 187/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.2919636901642662e-05, 1.2919636901642662e-05, 1.2919636901642662e-05, 1.2919636901642662e-05, 1.2919636901642662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2919636901642662e-05

Optimization complete. Final v2v error: 3.0299365520477295 mm

Highest mean error: 4.27946662902832 mm for frame 76

Lowest mean error: 2.581096649169922 mm for frame 109

Saving results

Total time: 44.93246078491211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752923
Iteration 2/25 | Loss: 0.00168060
Iteration 3/25 | Loss: 0.00149672
Iteration 4/25 | Loss: 0.00152536
Iteration 5/25 | Loss: 0.00146044
Iteration 6/25 | Loss: 0.00145248
Iteration 7/25 | Loss: 0.00145581
Iteration 8/25 | Loss: 0.00146223
Iteration 9/25 | Loss: 0.00146851
Iteration 10/25 | Loss: 0.00145492
Iteration 11/25 | Loss: 0.00144653
Iteration 12/25 | Loss: 0.00144536
Iteration 13/25 | Loss: 0.00144497
Iteration 14/25 | Loss: 0.00144492
Iteration 15/25 | Loss: 0.00144492
Iteration 16/25 | Loss: 0.00144491
Iteration 17/25 | Loss: 0.00144491
Iteration 18/25 | Loss: 0.00144491
Iteration 19/25 | Loss: 0.00144491
Iteration 20/25 | Loss: 0.00144491
Iteration 21/25 | Loss: 0.00144491
Iteration 22/25 | Loss: 0.00144491
Iteration 23/25 | Loss: 0.00144491
Iteration 24/25 | Loss: 0.00144490
Iteration 25/25 | Loss: 0.00144490

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37816858
Iteration 2/25 | Loss: 0.00196224
Iteration 3/25 | Loss: 0.00196222
Iteration 4/25 | Loss: 0.00196222
Iteration 5/25 | Loss: 0.00196222
Iteration 6/25 | Loss: 0.00196222
Iteration 7/25 | Loss: 0.00196221
Iteration 8/25 | Loss: 0.00196221
Iteration 9/25 | Loss: 0.00196221
Iteration 10/25 | Loss: 0.00196221
Iteration 11/25 | Loss: 0.00196221
Iteration 12/25 | Loss: 0.00196221
Iteration 13/25 | Loss: 0.00196221
Iteration 14/25 | Loss: 0.00196221
Iteration 15/25 | Loss: 0.00196221
Iteration 16/25 | Loss: 0.00196221
Iteration 17/25 | Loss: 0.00196221
Iteration 18/25 | Loss: 0.00196221
Iteration 19/25 | Loss: 0.00196221
Iteration 20/25 | Loss: 0.00196221
Iteration 21/25 | Loss: 0.00196221
Iteration 22/25 | Loss: 0.00196221
Iteration 23/25 | Loss: 0.00196221
Iteration 24/25 | Loss: 0.00196221
Iteration 25/25 | Loss: 0.00196221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0019622123800218105, 0.0019622123800218105, 0.0019622123800218105, 0.0019622123800218105, 0.0019622123800218105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019622123800218105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196221
Iteration 2/1000 | Loss: 0.00003232
Iteration 3/1000 | Loss: 0.00002628
Iteration 4/1000 | Loss: 0.00002465
Iteration 5/1000 | Loss: 0.00002381
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002259
Iteration 8/1000 | Loss: 0.00002190
Iteration 9/1000 | Loss: 0.00002135
Iteration 10/1000 | Loss: 0.00002103
Iteration 11/1000 | Loss: 0.00002074
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002031
Iteration 14/1000 | Loss: 0.00002018
Iteration 15/1000 | Loss: 0.00002017
Iteration 16/1000 | Loss: 0.00002017
Iteration 17/1000 | Loss: 0.00002017
Iteration 18/1000 | Loss: 0.00002017
Iteration 19/1000 | Loss: 0.00002014
Iteration 20/1000 | Loss: 0.00002014
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002010
Iteration 24/1000 | Loss: 0.00002010
Iteration 25/1000 | Loss: 0.00002009
Iteration 26/1000 | Loss: 0.00002008
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001992
Iteration 32/1000 | Loss: 0.00001992
Iteration 33/1000 | Loss: 0.00001988
Iteration 34/1000 | Loss: 0.00001987
Iteration 35/1000 | Loss: 0.00001987
Iteration 36/1000 | Loss: 0.00001986
Iteration 37/1000 | Loss: 0.00001986
Iteration 38/1000 | Loss: 0.00001985
Iteration 39/1000 | Loss: 0.00001985
Iteration 40/1000 | Loss: 0.00001985
Iteration 41/1000 | Loss: 0.00001984
Iteration 42/1000 | Loss: 0.00001984
Iteration 43/1000 | Loss: 0.00001984
Iteration 44/1000 | Loss: 0.00001984
Iteration 45/1000 | Loss: 0.00001984
Iteration 46/1000 | Loss: 0.00001983
Iteration 47/1000 | Loss: 0.00001983
Iteration 48/1000 | Loss: 0.00001982
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001982
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00001981
Iteration 55/1000 | Loss: 0.00001981
Iteration 56/1000 | Loss: 0.00001981
Iteration 57/1000 | Loss: 0.00001981
Iteration 58/1000 | Loss: 0.00001981
Iteration 59/1000 | Loss: 0.00001981
Iteration 60/1000 | Loss: 0.00001981
Iteration 61/1000 | Loss: 0.00001981
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001980
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001979
Iteration 67/1000 | Loss: 0.00001979
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001978
Iteration 70/1000 | Loss: 0.00001978
Iteration 71/1000 | Loss: 0.00001978
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001977
Iteration 74/1000 | Loss: 0.00001977
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001976
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00001976
Iteration 80/1000 | Loss: 0.00001976
Iteration 81/1000 | Loss: 0.00001975
Iteration 82/1000 | Loss: 0.00001975
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001973
Iteration 95/1000 | Loss: 0.00001973
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001972
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001971
Iteration 103/1000 | Loss: 0.00001971
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001969
Iteration 118/1000 | Loss: 0.00001969
Iteration 119/1000 | Loss: 0.00001969
Iteration 120/1000 | Loss: 0.00001969
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001967
Iteration 126/1000 | Loss: 0.00001967
Iteration 127/1000 | Loss: 0.00001967
Iteration 128/1000 | Loss: 0.00001967
Iteration 129/1000 | Loss: 0.00001967
Iteration 130/1000 | Loss: 0.00001967
Iteration 131/1000 | Loss: 0.00001967
Iteration 132/1000 | Loss: 0.00001967
Iteration 133/1000 | Loss: 0.00001967
Iteration 134/1000 | Loss: 0.00001967
Iteration 135/1000 | Loss: 0.00001967
Iteration 136/1000 | Loss: 0.00001966
Iteration 137/1000 | Loss: 0.00001966
Iteration 138/1000 | Loss: 0.00001966
Iteration 139/1000 | Loss: 0.00001966
Iteration 140/1000 | Loss: 0.00001966
Iteration 141/1000 | Loss: 0.00001966
Iteration 142/1000 | Loss: 0.00001966
Iteration 143/1000 | Loss: 0.00001966
Iteration 144/1000 | Loss: 0.00001966
Iteration 145/1000 | Loss: 0.00001966
Iteration 146/1000 | Loss: 0.00001966
Iteration 147/1000 | Loss: 0.00001966
Iteration 148/1000 | Loss: 0.00001966
Iteration 149/1000 | Loss: 0.00001966
Iteration 150/1000 | Loss: 0.00001966
Iteration 151/1000 | Loss: 0.00001966
Iteration 152/1000 | Loss: 0.00001965
Iteration 153/1000 | Loss: 0.00001965
Iteration 154/1000 | Loss: 0.00001965
Iteration 155/1000 | Loss: 0.00001965
Iteration 156/1000 | Loss: 0.00001965
Iteration 157/1000 | Loss: 0.00001964
Iteration 158/1000 | Loss: 0.00001964
Iteration 159/1000 | Loss: 0.00001964
Iteration 160/1000 | Loss: 0.00001964
Iteration 161/1000 | Loss: 0.00001964
Iteration 162/1000 | Loss: 0.00001964
Iteration 163/1000 | Loss: 0.00001964
Iteration 164/1000 | Loss: 0.00001964
Iteration 165/1000 | Loss: 0.00001963
Iteration 166/1000 | Loss: 0.00001963
Iteration 167/1000 | Loss: 0.00001963
Iteration 168/1000 | Loss: 0.00001963
Iteration 169/1000 | Loss: 0.00001963
Iteration 170/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.9634546333691105e-05, 1.9634546333691105e-05, 1.9634546333691105e-05, 1.9634546333691105e-05, 1.9634546333691105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9634546333691105e-05

Optimization complete. Final v2v error: 3.6896471977233887 mm

Highest mean error: 3.9907116889953613 mm for frame 220

Lowest mean error: 3.354252815246582 mm for frame 147

Saving results

Total time: 65.98363208770752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969451
Iteration 2/25 | Loss: 0.00969451
Iteration 3/25 | Loss: 0.00291897
Iteration 4/25 | Loss: 0.00188124
Iteration 5/25 | Loss: 0.00177128
Iteration 6/25 | Loss: 0.00164768
Iteration 7/25 | Loss: 0.00167213
Iteration 8/25 | Loss: 0.00174602
Iteration 9/25 | Loss: 0.00173429
Iteration 10/25 | Loss: 0.00164806
Iteration 11/25 | Loss: 0.00154117
Iteration 12/25 | Loss: 0.00149718
Iteration 13/25 | Loss: 0.00146453
Iteration 14/25 | Loss: 0.00146322
Iteration 15/25 | Loss: 0.00143838
Iteration 16/25 | Loss: 0.00141472
Iteration 17/25 | Loss: 0.00140866
Iteration 18/25 | Loss: 0.00140202
Iteration 19/25 | Loss: 0.00140141
Iteration 20/25 | Loss: 0.00139914
Iteration 21/25 | Loss: 0.00139390
Iteration 22/25 | Loss: 0.00139517
Iteration 23/25 | Loss: 0.00139151
Iteration 24/25 | Loss: 0.00139137
Iteration 25/25 | Loss: 0.00139002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25479376
Iteration 2/25 | Loss: 0.00200890
Iteration 3/25 | Loss: 0.00200890
Iteration 4/25 | Loss: 0.00200890
Iteration 5/25 | Loss: 0.00200890
Iteration 6/25 | Loss: 0.00200890
Iteration 7/25 | Loss: 0.00200890
Iteration 8/25 | Loss: 0.00200890
Iteration 9/25 | Loss: 0.00200890
Iteration 10/25 | Loss: 0.00200890
Iteration 11/25 | Loss: 0.00200890
Iteration 12/25 | Loss: 0.00200890
Iteration 13/25 | Loss: 0.00200890
Iteration 14/25 | Loss: 0.00200890
Iteration 15/25 | Loss: 0.00200890
Iteration 16/25 | Loss: 0.00200890
Iteration 17/25 | Loss: 0.00200890
Iteration 18/25 | Loss: 0.00200890
Iteration 19/25 | Loss: 0.00200890
Iteration 20/25 | Loss: 0.00200890
Iteration 21/25 | Loss: 0.00200890
Iteration 22/25 | Loss: 0.00200890
Iteration 23/25 | Loss: 0.00200890
Iteration 24/25 | Loss: 0.00200890
Iteration 25/25 | Loss: 0.00200890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200890
Iteration 2/1000 | Loss: 0.00004632
Iteration 3/1000 | Loss: 0.00003444
Iteration 4/1000 | Loss: 0.00002690
Iteration 5/1000 | Loss: 0.00003150
Iteration 6/1000 | Loss: 0.00010787
Iteration 7/1000 | Loss: 0.00011218
Iteration 8/1000 | Loss: 0.00006069
Iteration 9/1000 | Loss: 0.00002563
Iteration 10/1000 | Loss: 0.00005808
Iteration 11/1000 | Loss: 0.00004354
Iteration 12/1000 | Loss: 0.00011156
Iteration 13/1000 | Loss: 0.00004973
Iteration 14/1000 | Loss: 0.00003605
Iteration 15/1000 | Loss: 0.00004077
Iteration 16/1000 | Loss: 0.00010220
Iteration 17/1000 | Loss: 0.00012366
Iteration 18/1000 | Loss: 0.00005401
Iteration 19/1000 | Loss: 0.00004126
Iteration 20/1000 | Loss: 0.00004242
Iteration 21/1000 | Loss: 0.00004157
Iteration 22/1000 | Loss: 0.00004118
Iteration 23/1000 | Loss: 0.00011412
Iteration 24/1000 | Loss: 0.00015883
Iteration 25/1000 | Loss: 0.00004076
Iteration 26/1000 | Loss: 0.00004658
Iteration 27/1000 | Loss: 0.00003638
Iteration 28/1000 | Loss: 0.00003317
Iteration 29/1000 | Loss: 0.00003922
Iteration 30/1000 | Loss: 0.00003915
Iteration 31/1000 | Loss: 0.00010212
Iteration 32/1000 | Loss: 0.00011330
Iteration 33/1000 | Loss: 0.00010202
Iteration 34/1000 | Loss: 0.00005171
Iteration 35/1000 | Loss: 0.00004646
Iteration 36/1000 | Loss: 0.00011684
Iteration 37/1000 | Loss: 0.00002299
Iteration 38/1000 | Loss: 0.00004668
Iteration 39/1000 | Loss: 0.00006164
Iteration 40/1000 | Loss: 0.00003098
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00006612
Iteration 45/1000 | Loss: 0.00020070
Iteration 46/1000 | Loss: 0.00003802
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002026
Iteration 49/1000 | Loss: 0.00004756
Iteration 50/1000 | Loss: 0.00006817
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002867
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001700
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001693
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001679
Iteration 69/1000 | Loss: 0.00001679
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001678
Iteration 72/1000 | Loss: 0.00001678
Iteration 73/1000 | Loss: 0.00001678
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00004115
Iteration 92/1000 | Loss: 0.00018770
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00002789
Iteration 95/1000 | Loss: 0.00002586
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00001810
Iteration 98/1000 | Loss: 0.00002284
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001661
Iteration 102/1000 | Loss: 0.00001661
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001657
Iteration 110/1000 | Loss: 0.00001657
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001656
Iteration 114/1000 | Loss: 0.00001656
Iteration 115/1000 | Loss: 0.00001656
Iteration 116/1000 | Loss: 0.00001656
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001656
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001655
Iteration 122/1000 | Loss: 0.00001655
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00004406
Iteration 128/1000 | Loss: 0.00004406
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001655
Iteration 131/1000 | Loss: 0.00001652
Iteration 132/1000 | Loss: 0.00001652
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001652
Iteration 136/1000 | Loss: 0.00001652
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00001651
Iteration 139/1000 | Loss: 0.00001651
Iteration 140/1000 | Loss: 0.00001651
Iteration 141/1000 | Loss: 0.00001651
Iteration 142/1000 | Loss: 0.00001650
Iteration 143/1000 | Loss: 0.00001650
Iteration 144/1000 | Loss: 0.00001650
Iteration 145/1000 | Loss: 0.00001650
Iteration 146/1000 | Loss: 0.00001650
Iteration 147/1000 | Loss: 0.00001650
Iteration 148/1000 | Loss: 0.00001650
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Iteration 162/1000 | Loss: 0.00001649
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001649
Iteration 165/1000 | Loss: 0.00001649
Iteration 166/1000 | Loss: 0.00001649
Iteration 167/1000 | Loss: 0.00001649
Iteration 168/1000 | Loss: 0.00001649
Iteration 169/1000 | Loss: 0.00001649
Iteration 170/1000 | Loss: 0.00001649
Iteration 171/1000 | Loss: 0.00001648
Iteration 172/1000 | Loss: 0.00001648
Iteration 173/1000 | Loss: 0.00001648
Iteration 174/1000 | Loss: 0.00001648
Iteration 175/1000 | Loss: 0.00001648
Iteration 176/1000 | Loss: 0.00001648
Iteration 177/1000 | Loss: 0.00001648
Iteration 178/1000 | Loss: 0.00001648
Iteration 179/1000 | Loss: 0.00001648
Iteration 180/1000 | Loss: 0.00001648
Iteration 181/1000 | Loss: 0.00001648
Iteration 182/1000 | Loss: 0.00001648
Iteration 183/1000 | Loss: 0.00001648
Iteration 184/1000 | Loss: 0.00001648
Iteration 185/1000 | Loss: 0.00001648
Iteration 186/1000 | Loss: 0.00001648
Iteration 187/1000 | Loss: 0.00001648
Iteration 188/1000 | Loss: 0.00001648
Iteration 189/1000 | Loss: 0.00001648
Iteration 190/1000 | Loss: 0.00001648
Iteration 191/1000 | Loss: 0.00001648
Iteration 192/1000 | Loss: 0.00001648
Iteration 193/1000 | Loss: 0.00001648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.647668977966532e-05, 1.647668977966532e-05, 1.647668977966532e-05, 1.647668977966532e-05, 1.647668977966532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.647668977966532e-05

Optimization complete. Final v2v error: 3.517035961151123 mm

Highest mean error: 4.0989508628845215 mm for frame 226

Lowest mean error: 3.091362953186035 mm for frame 5

Saving results

Total time: 162.3851249217987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402150
Iteration 2/25 | Loss: 0.00138759
Iteration 3/25 | Loss: 0.00132920
Iteration 4/25 | Loss: 0.00132366
Iteration 5/25 | Loss: 0.00132330
Iteration 6/25 | Loss: 0.00132330
Iteration 7/25 | Loss: 0.00132330
Iteration 8/25 | Loss: 0.00132330
Iteration 9/25 | Loss: 0.00132330
Iteration 10/25 | Loss: 0.00132330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013233033241704106, 0.0013233033241704106, 0.0013233033241704106, 0.0013233033241704106, 0.0013233033241704106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013233033241704106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40569103
Iteration 2/25 | Loss: 0.00200671
Iteration 3/25 | Loss: 0.00200671
Iteration 4/25 | Loss: 0.00200671
Iteration 5/25 | Loss: 0.00200671
Iteration 6/25 | Loss: 0.00200671
Iteration 7/25 | Loss: 0.00200671
Iteration 8/25 | Loss: 0.00200671
Iteration 9/25 | Loss: 0.00200671
Iteration 10/25 | Loss: 0.00200671
Iteration 11/25 | Loss: 0.00200671
Iteration 12/25 | Loss: 0.00200671
Iteration 13/25 | Loss: 0.00200671
Iteration 14/25 | Loss: 0.00200671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0020067112054675817, 0.0020067112054675817, 0.0020067112054675817, 0.0020067112054675817, 0.0020067112054675817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020067112054675817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200671
Iteration 2/1000 | Loss: 0.00002954
Iteration 3/1000 | Loss: 0.00001989
Iteration 4/1000 | Loss: 0.00001597
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001208
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001158
Iteration 13/1000 | Loss: 0.00001139
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001117
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001089
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001072
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001071
Iteration 43/1000 | Loss: 0.00001071
Iteration 44/1000 | Loss: 0.00001070
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001066
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001065
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001054
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001049
Iteration 70/1000 | Loss: 0.00001049
Iteration 71/1000 | Loss: 0.00001045
Iteration 72/1000 | Loss: 0.00001045
Iteration 73/1000 | Loss: 0.00001045
Iteration 74/1000 | Loss: 0.00001044
Iteration 75/1000 | Loss: 0.00001044
Iteration 76/1000 | Loss: 0.00001044
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001043
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001039
Iteration 89/1000 | Loss: 0.00001039
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001036
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001034
Iteration 102/1000 | Loss: 0.00001034
Iteration 103/1000 | Loss: 0.00001034
Iteration 104/1000 | Loss: 0.00001034
Iteration 105/1000 | Loss: 0.00001034
Iteration 106/1000 | Loss: 0.00001033
Iteration 107/1000 | Loss: 0.00001033
Iteration 108/1000 | Loss: 0.00001033
Iteration 109/1000 | Loss: 0.00001033
Iteration 110/1000 | Loss: 0.00001033
Iteration 111/1000 | Loss: 0.00001032
Iteration 112/1000 | Loss: 0.00001032
Iteration 113/1000 | Loss: 0.00001032
Iteration 114/1000 | Loss: 0.00001032
Iteration 115/1000 | Loss: 0.00001032
Iteration 116/1000 | Loss: 0.00001032
Iteration 117/1000 | Loss: 0.00001032
Iteration 118/1000 | Loss: 0.00001032
Iteration 119/1000 | Loss: 0.00001032
Iteration 120/1000 | Loss: 0.00001032
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001032
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001031
Iteration 126/1000 | Loss: 0.00001031
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001031
Iteration 129/1000 | Loss: 0.00001031
Iteration 130/1000 | Loss: 0.00001031
Iteration 131/1000 | Loss: 0.00001031
Iteration 132/1000 | Loss: 0.00001031
Iteration 133/1000 | Loss: 0.00001031
Iteration 134/1000 | Loss: 0.00001031
Iteration 135/1000 | Loss: 0.00001031
Iteration 136/1000 | Loss: 0.00001031
Iteration 137/1000 | Loss: 0.00001031
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001030
Iteration 141/1000 | Loss: 0.00001030
Iteration 142/1000 | Loss: 0.00001030
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001030
Iteration 148/1000 | Loss: 0.00001030
Iteration 149/1000 | Loss: 0.00001030
Iteration 150/1000 | Loss: 0.00001030
Iteration 151/1000 | Loss: 0.00001030
Iteration 152/1000 | Loss: 0.00001030
Iteration 153/1000 | Loss: 0.00001030
Iteration 154/1000 | Loss: 0.00001030
Iteration 155/1000 | Loss: 0.00001029
Iteration 156/1000 | Loss: 0.00001029
Iteration 157/1000 | Loss: 0.00001029
Iteration 158/1000 | Loss: 0.00001029
Iteration 159/1000 | Loss: 0.00001029
Iteration 160/1000 | Loss: 0.00001029
Iteration 161/1000 | Loss: 0.00001028
Iteration 162/1000 | Loss: 0.00001028
Iteration 163/1000 | Loss: 0.00001028
Iteration 164/1000 | Loss: 0.00001028
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Iteration 167/1000 | Loss: 0.00001028
Iteration 168/1000 | Loss: 0.00001028
Iteration 169/1000 | Loss: 0.00001028
Iteration 170/1000 | Loss: 0.00001028
Iteration 171/1000 | Loss: 0.00001028
Iteration 172/1000 | Loss: 0.00001028
Iteration 173/1000 | Loss: 0.00001028
Iteration 174/1000 | Loss: 0.00001027
Iteration 175/1000 | Loss: 0.00001027
Iteration 176/1000 | Loss: 0.00001027
Iteration 177/1000 | Loss: 0.00001027
Iteration 178/1000 | Loss: 0.00001027
Iteration 179/1000 | Loss: 0.00001027
Iteration 180/1000 | Loss: 0.00001027
Iteration 181/1000 | Loss: 0.00001027
Iteration 182/1000 | Loss: 0.00001027
Iteration 183/1000 | Loss: 0.00001027
Iteration 184/1000 | Loss: 0.00001027
Iteration 185/1000 | Loss: 0.00001027
Iteration 186/1000 | Loss: 0.00001027
Iteration 187/1000 | Loss: 0.00001027
Iteration 188/1000 | Loss: 0.00001027
Iteration 189/1000 | Loss: 0.00001027
Iteration 190/1000 | Loss: 0.00001027
Iteration 191/1000 | Loss: 0.00001027
Iteration 192/1000 | Loss: 0.00001027
Iteration 193/1000 | Loss: 0.00001026
Iteration 194/1000 | Loss: 0.00001026
Iteration 195/1000 | Loss: 0.00001026
Iteration 196/1000 | Loss: 0.00001026
Iteration 197/1000 | Loss: 0.00001026
Iteration 198/1000 | Loss: 0.00001026
Iteration 199/1000 | Loss: 0.00001026
Iteration 200/1000 | Loss: 0.00001026
Iteration 201/1000 | Loss: 0.00001026
Iteration 202/1000 | Loss: 0.00001026
Iteration 203/1000 | Loss: 0.00001026
Iteration 204/1000 | Loss: 0.00001026
Iteration 205/1000 | Loss: 0.00001026
Iteration 206/1000 | Loss: 0.00001026
Iteration 207/1000 | Loss: 0.00001026
Iteration 208/1000 | Loss: 0.00001026
Iteration 209/1000 | Loss: 0.00001026
Iteration 210/1000 | Loss: 0.00001026
Iteration 211/1000 | Loss: 0.00001026
Iteration 212/1000 | Loss: 0.00001025
Iteration 213/1000 | Loss: 0.00001025
Iteration 214/1000 | Loss: 0.00001025
Iteration 215/1000 | Loss: 0.00001025
Iteration 216/1000 | Loss: 0.00001025
Iteration 217/1000 | Loss: 0.00001025
Iteration 218/1000 | Loss: 0.00001025
Iteration 219/1000 | Loss: 0.00001025
Iteration 220/1000 | Loss: 0.00001025
Iteration 221/1000 | Loss: 0.00001025
Iteration 222/1000 | Loss: 0.00001025
Iteration 223/1000 | Loss: 0.00001025
Iteration 224/1000 | Loss: 0.00001025
Iteration 225/1000 | Loss: 0.00001025
Iteration 226/1000 | Loss: 0.00001025
Iteration 227/1000 | Loss: 0.00001025
Iteration 228/1000 | Loss: 0.00001025
Iteration 229/1000 | Loss: 0.00001025
Iteration 230/1000 | Loss: 0.00001025
Iteration 231/1000 | Loss: 0.00001025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.0252047104586381e-05, 1.0252047104586381e-05, 1.0252047104586381e-05, 1.0252047104586381e-05, 1.0252047104586381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0252047104586381e-05

Optimization complete. Final v2v error: 2.7631900310516357 mm

Highest mean error: 2.9420604705810547 mm for frame 152

Lowest mean error: 2.6279726028442383 mm for frame 245

Saving results

Total time: 53.32575273513794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975169
Iteration 2/25 | Loss: 0.00301644
Iteration 3/25 | Loss: 0.00182091
Iteration 4/25 | Loss: 0.00168782
Iteration 5/25 | Loss: 0.00163613
Iteration 6/25 | Loss: 0.00158751
Iteration 7/25 | Loss: 0.00154871
Iteration 8/25 | Loss: 0.00151841
Iteration 9/25 | Loss: 0.00150403
Iteration 10/25 | Loss: 0.00149789
Iteration 11/25 | Loss: 0.00149777
Iteration 12/25 | Loss: 0.00149317
Iteration 13/25 | Loss: 0.00149157
Iteration 14/25 | Loss: 0.00148769
Iteration 15/25 | Loss: 0.00149111
Iteration 16/25 | Loss: 0.00149054
Iteration 17/25 | Loss: 0.00148993
Iteration 18/25 | Loss: 0.00149067
Iteration 19/25 | Loss: 0.00149140
Iteration 20/25 | Loss: 0.00149248
Iteration 21/25 | Loss: 0.00149725
Iteration 22/25 | Loss: 0.00148988
Iteration 23/25 | Loss: 0.00149301
Iteration 24/25 | Loss: 0.00149110
Iteration 25/25 | Loss: 0.00149144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22313976
Iteration 2/25 | Loss: 0.00188516
Iteration 3/25 | Loss: 0.00188516
Iteration 4/25 | Loss: 0.00188516
Iteration 5/25 | Loss: 0.00188516
Iteration 6/25 | Loss: 0.00188516
Iteration 7/25 | Loss: 0.00188515
Iteration 8/25 | Loss: 0.00188515
Iteration 9/25 | Loss: 0.00188515
Iteration 10/25 | Loss: 0.00188515
Iteration 11/25 | Loss: 0.00188515
Iteration 12/25 | Loss: 0.00188515
Iteration 13/25 | Loss: 0.00188515
Iteration 14/25 | Loss: 0.00188515
Iteration 15/25 | Loss: 0.00188515
Iteration 16/25 | Loss: 0.00188515
Iteration 17/25 | Loss: 0.00188515
Iteration 18/25 | Loss: 0.00188515
Iteration 19/25 | Loss: 0.00188515
Iteration 20/25 | Loss: 0.00188515
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018851520726457238, 0.0018851520726457238, 0.0018851520726457238, 0.0018851520726457238, 0.0018851520726457238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018851520726457238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188515
Iteration 2/1000 | Loss: 0.00007848
Iteration 3/1000 | Loss: 0.00007055
Iteration 4/1000 | Loss: 0.00007868
Iteration 5/1000 | Loss: 0.00009163
Iteration 6/1000 | Loss: 0.00007801
Iteration 7/1000 | Loss: 0.00006898
Iteration 8/1000 | Loss: 0.00007625
Iteration 9/1000 | Loss: 0.00007573
Iteration 10/1000 | Loss: 0.00006543
Iteration 11/1000 | Loss: 0.00007276
Iteration 12/1000 | Loss: 0.00006918
Iteration 13/1000 | Loss: 0.00009210
Iteration 14/1000 | Loss: 0.00008096
Iteration 15/1000 | Loss: 0.00007246
Iteration 16/1000 | Loss: 0.00007374
Iteration 17/1000 | Loss: 0.00007255
Iteration 18/1000 | Loss: 0.00007629
Iteration 19/1000 | Loss: 0.00006657
Iteration 20/1000 | Loss: 0.00016236
Iteration 21/1000 | Loss: 0.00007567
Iteration 22/1000 | Loss: 0.00003817
Iteration 23/1000 | Loss: 0.00003505
Iteration 24/1000 | Loss: 0.00020019
Iteration 25/1000 | Loss: 0.00003371
Iteration 26/1000 | Loss: 0.00003318
Iteration 27/1000 | Loss: 0.00003240
Iteration 28/1000 | Loss: 0.00003178
Iteration 29/1000 | Loss: 0.00003127
Iteration 30/1000 | Loss: 0.00003078
Iteration 31/1000 | Loss: 0.00003058
Iteration 32/1000 | Loss: 0.00003037
Iteration 33/1000 | Loss: 0.00003008
Iteration 34/1000 | Loss: 0.00002991
Iteration 35/1000 | Loss: 0.00002974
Iteration 36/1000 | Loss: 0.00002967
Iteration 37/1000 | Loss: 0.00002957
Iteration 38/1000 | Loss: 0.00002957
Iteration 39/1000 | Loss: 0.00002957
Iteration 40/1000 | Loss: 0.00002957
Iteration 41/1000 | Loss: 0.00002957
Iteration 42/1000 | Loss: 0.00002956
Iteration 43/1000 | Loss: 0.00002956
Iteration 44/1000 | Loss: 0.00002953
Iteration 45/1000 | Loss: 0.00002953
Iteration 46/1000 | Loss: 0.00002953
Iteration 47/1000 | Loss: 0.00002953
Iteration 48/1000 | Loss: 0.00002953
Iteration 49/1000 | Loss: 0.00002953
Iteration 50/1000 | Loss: 0.00002953
Iteration 51/1000 | Loss: 0.00002952
Iteration 52/1000 | Loss: 0.00002952
Iteration 53/1000 | Loss: 0.00002952
Iteration 54/1000 | Loss: 0.00002952
Iteration 55/1000 | Loss: 0.00002952
Iteration 56/1000 | Loss: 0.00002951
Iteration 57/1000 | Loss: 0.00002951
Iteration 58/1000 | Loss: 0.00002951
Iteration 59/1000 | Loss: 0.00002950
Iteration 60/1000 | Loss: 0.00002950
Iteration 61/1000 | Loss: 0.00002950
Iteration 62/1000 | Loss: 0.00002949
Iteration 63/1000 | Loss: 0.00002949
Iteration 64/1000 | Loss: 0.00002949
Iteration 65/1000 | Loss: 0.00002949
Iteration 66/1000 | Loss: 0.00002949
Iteration 67/1000 | Loss: 0.00002949
Iteration 68/1000 | Loss: 0.00002949
Iteration 69/1000 | Loss: 0.00002948
Iteration 70/1000 | Loss: 0.00002948
Iteration 71/1000 | Loss: 0.00002948
Iteration 72/1000 | Loss: 0.00002948
Iteration 73/1000 | Loss: 0.00002947
Iteration 74/1000 | Loss: 0.00002947
Iteration 75/1000 | Loss: 0.00002947
Iteration 76/1000 | Loss: 0.00002947
Iteration 77/1000 | Loss: 0.00002947
Iteration 78/1000 | Loss: 0.00002947
Iteration 79/1000 | Loss: 0.00002946
Iteration 80/1000 | Loss: 0.00002946
Iteration 81/1000 | Loss: 0.00002946
Iteration 82/1000 | Loss: 0.00002946
Iteration 83/1000 | Loss: 0.00002946
Iteration 84/1000 | Loss: 0.00002946
Iteration 85/1000 | Loss: 0.00002946
Iteration 86/1000 | Loss: 0.00002946
Iteration 87/1000 | Loss: 0.00002946
Iteration 88/1000 | Loss: 0.00002946
Iteration 89/1000 | Loss: 0.00002946
Iteration 90/1000 | Loss: 0.00002946
Iteration 91/1000 | Loss: 0.00002946
Iteration 92/1000 | Loss: 0.00002946
Iteration 93/1000 | Loss: 0.00002946
Iteration 94/1000 | Loss: 0.00002945
Iteration 95/1000 | Loss: 0.00002945
Iteration 96/1000 | Loss: 0.00002945
Iteration 97/1000 | Loss: 0.00002945
Iteration 98/1000 | Loss: 0.00002945
Iteration 99/1000 | Loss: 0.00002944
Iteration 100/1000 | Loss: 0.00002944
Iteration 101/1000 | Loss: 0.00002944
Iteration 102/1000 | Loss: 0.00002944
Iteration 103/1000 | Loss: 0.00002944
Iteration 104/1000 | Loss: 0.00002944
Iteration 105/1000 | Loss: 0.00002944
Iteration 106/1000 | Loss: 0.00002943
Iteration 107/1000 | Loss: 0.00002943
Iteration 108/1000 | Loss: 0.00002943
Iteration 109/1000 | Loss: 0.00002943
Iteration 110/1000 | Loss: 0.00002943
Iteration 111/1000 | Loss: 0.00002943
Iteration 112/1000 | Loss: 0.00002943
Iteration 113/1000 | Loss: 0.00002943
Iteration 114/1000 | Loss: 0.00002942
Iteration 115/1000 | Loss: 0.00016274
Iteration 116/1000 | Loss: 0.00003190
Iteration 117/1000 | Loss: 0.00003064
Iteration 118/1000 | Loss: 0.00002984
Iteration 119/1000 | Loss: 0.00002910
Iteration 120/1000 | Loss: 0.00002854
Iteration 121/1000 | Loss: 0.00002835
Iteration 122/1000 | Loss: 0.00002830
Iteration 123/1000 | Loss: 0.00002829
Iteration 124/1000 | Loss: 0.00002822
Iteration 125/1000 | Loss: 0.00002816
Iteration 126/1000 | Loss: 0.00002810
Iteration 127/1000 | Loss: 0.00002809
Iteration 128/1000 | Loss: 0.00002809
Iteration 129/1000 | Loss: 0.00002808
Iteration 130/1000 | Loss: 0.00002808
Iteration 131/1000 | Loss: 0.00002808
Iteration 132/1000 | Loss: 0.00002807
Iteration 133/1000 | Loss: 0.00002807
Iteration 134/1000 | Loss: 0.00002805
Iteration 135/1000 | Loss: 0.00002805
Iteration 136/1000 | Loss: 0.00002805
Iteration 137/1000 | Loss: 0.00002805
Iteration 138/1000 | Loss: 0.00002805
Iteration 139/1000 | Loss: 0.00002805
Iteration 140/1000 | Loss: 0.00002805
Iteration 141/1000 | Loss: 0.00002804
Iteration 142/1000 | Loss: 0.00002804
Iteration 143/1000 | Loss: 0.00002804
Iteration 144/1000 | Loss: 0.00002804
Iteration 145/1000 | Loss: 0.00002802
Iteration 146/1000 | Loss: 0.00002800
Iteration 147/1000 | Loss: 0.00002800
Iteration 148/1000 | Loss: 0.00002800
Iteration 149/1000 | Loss: 0.00002800
Iteration 150/1000 | Loss: 0.00002800
Iteration 151/1000 | Loss: 0.00002800
Iteration 152/1000 | Loss: 0.00002800
Iteration 153/1000 | Loss: 0.00002800
Iteration 154/1000 | Loss: 0.00002799
Iteration 155/1000 | Loss: 0.00002799
Iteration 156/1000 | Loss: 0.00002799
Iteration 157/1000 | Loss: 0.00002799
Iteration 158/1000 | Loss: 0.00002799
Iteration 159/1000 | Loss: 0.00002799
Iteration 160/1000 | Loss: 0.00002798
Iteration 161/1000 | Loss: 0.00002798
Iteration 162/1000 | Loss: 0.00002798
Iteration 163/1000 | Loss: 0.00002797
Iteration 164/1000 | Loss: 0.00002797
Iteration 165/1000 | Loss: 0.00002797
Iteration 166/1000 | Loss: 0.00002796
Iteration 167/1000 | Loss: 0.00002796
Iteration 168/1000 | Loss: 0.00002796
Iteration 169/1000 | Loss: 0.00002796
Iteration 170/1000 | Loss: 0.00002795
Iteration 171/1000 | Loss: 0.00002795
Iteration 172/1000 | Loss: 0.00002795
Iteration 173/1000 | Loss: 0.00002794
Iteration 174/1000 | Loss: 0.00002794
Iteration 175/1000 | Loss: 0.00002794
Iteration 176/1000 | Loss: 0.00002794
Iteration 177/1000 | Loss: 0.00002793
Iteration 178/1000 | Loss: 0.00002793
Iteration 179/1000 | Loss: 0.00002793
Iteration 180/1000 | Loss: 0.00002793
Iteration 181/1000 | Loss: 0.00002792
Iteration 182/1000 | Loss: 0.00002792
Iteration 183/1000 | Loss: 0.00002792
Iteration 184/1000 | Loss: 0.00002792
Iteration 185/1000 | Loss: 0.00002792
Iteration 186/1000 | Loss: 0.00002792
Iteration 187/1000 | Loss: 0.00002792
Iteration 188/1000 | Loss: 0.00002792
Iteration 189/1000 | Loss: 0.00002792
Iteration 190/1000 | Loss: 0.00002791
Iteration 191/1000 | Loss: 0.00002791
Iteration 192/1000 | Loss: 0.00002791
Iteration 193/1000 | Loss: 0.00002791
Iteration 194/1000 | Loss: 0.00002790
Iteration 195/1000 | Loss: 0.00002790
Iteration 196/1000 | Loss: 0.00002790
Iteration 197/1000 | Loss: 0.00002790
Iteration 198/1000 | Loss: 0.00002790
Iteration 199/1000 | Loss: 0.00002790
Iteration 200/1000 | Loss: 0.00002790
Iteration 201/1000 | Loss: 0.00002789
Iteration 202/1000 | Loss: 0.00002789
Iteration 203/1000 | Loss: 0.00002789
Iteration 204/1000 | Loss: 0.00002789
Iteration 205/1000 | Loss: 0.00002789
Iteration 206/1000 | Loss: 0.00002789
Iteration 207/1000 | Loss: 0.00002789
Iteration 208/1000 | Loss: 0.00002789
Iteration 209/1000 | Loss: 0.00002789
Iteration 210/1000 | Loss: 0.00002789
Iteration 211/1000 | Loss: 0.00002789
Iteration 212/1000 | Loss: 0.00002789
Iteration 213/1000 | Loss: 0.00002789
Iteration 214/1000 | Loss: 0.00002789
Iteration 215/1000 | Loss: 0.00002789
Iteration 216/1000 | Loss: 0.00002789
Iteration 217/1000 | Loss: 0.00002789
Iteration 218/1000 | Loss: 0.00002789
Iteration 219/1000 | Loss: 0.00002788
Iteration 220/1000 | Loss: 0.00002788
Iteration 221/1000 | Loss: 0.00002788
Iteration 222/1000 | Loss: 0.00002788
Iteration 223/1000 | Loss: 0.00002788
Iteration 224/1000 | Loss: 0.00002788
Iteration 225/1000 | Loss: 0.00002788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.7884818337042816e-05, 2.7884818337042816e-05, 2.7884818337042816e-05, 2.7884818337042816e-05, 2.7884818337042816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7884818337042816e-05

Optimization complete. Final v2v error: 4.463517189025879 mm

Highest mean error: 5.585583686828613 mm for frame 220

Lowest mean error: 3.930279016494751 mm for frame 205

Saving results

Total time: 137.46152567863464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398295
Iteration 2/25 | Loss: 0.00143003
Iteration 3/25 | Loss: 0.00133668
Iteration 4/25 | Loss: 0.00132645
Iteration 5/25 | Loss: 0.00132324
Iteration 6/25 | Loss: 0.00132214
Iteration 7/25 | Loss: 0.00132214
Iteration 8/25 | Loss: 0.00132214
Iteration 9/25 | Loss: 0.00132214
Iteration 10/25 | Loss: 0.00132214
Iteration 11/25 | Loss: 0.00132214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013221418485045433, 0.0013221418485045433, 0.0013221418485045433, 0.0013221418485045433, 0.0013221418485045433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013221418485045433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47895551
Iteration 2/25 | Loss: 0.00207729
Iteration 3/25 | Loss: 0.00207729
Iteration 4/25 | Loss: 0.00207729
Iteration 5/25 | Loss: 0.00207729
Iteration 6/25 | Loss: 0.00207729
Iteration 7/25 | Loss: 0.00207729
Iteration 8/25 | Loss: 0.00207729
Iteration 9/25 | Loss: 0.00207729
Iteration 10/25 | Loss: 0.00207729
Iteration 11/25 | Loss: 0.00207729
Iteration 12/25 | Loss: 0.00207729
Iteration 13/25 | Loss: 0.00207729
Iteration 14/25 | Loss: 0.00207729
Iteration 15/25 | Loss: 0.00207729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002077290089800954, 0.002077290089800954, 0.002077290089800954, 0.002077290089800954, 0.002077290089800954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002077290089800954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207729
Iteration 2/1000 | Loss: 0.00002874
Iteration 3/1000 | Loss: 0.00001802
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001292
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001200
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001134
Iteration 11/1000 | Loss: 0.00001108
Iteration 12/1000 | Loss: 0.00001089
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001045
Iteration 20/1000 | Loss: 0.00001044
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001043
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001041
Iteration 28/1000 | Loss: 0.00001037
Iteration 29/1000 | Loss: 0.00001037
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001034
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001031
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001031
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001031
Iteration 40/1000 | Loss: 0.00001031
Iteration 41/1000 | Loss: 0.00001031
Iteration 42/1000 | Loss: 0.00001031
Iteration 43/1000 | Loss: 0.00001031
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001030
Iteration 46/1000 | Loss: 0.00001030
Iteration 47/1000 | Loss: 0.00001030
Iteration 48/1000 | Loss: 0.00001030
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001028
Iteration 55/1000 | Loss: 0.00001028
Iteration 56/1000 | Loss: 0.00001027
Iteration 57/1000 | Loss: 0.00001027
Iteration 58/1000 | Loss: 0.00001027
Iteration 59/1000 | Loss: 0.00001027
Iteration 60/1000 | Loss: 0.00001027
Iteration 61/1000 | Loss: 0.00001027
Iteration 62/1000 | Loss: 0.00001026
Iteration 63/1000 | Loss: 0.00001026
Iteration 64/1000 | Loss: 0.00001026
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001024
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001021
Iteration 70/1000 | Loss: 0.00001018
Iteration 71/1000 | Loss: 0.00001018
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001017
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001015
Iteration 77/1000 | Loss: 0.00001015
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001013
Iteration 84/1000 | Loss: 0.00001013
Iteration 85/1000 | Loss: 0.00001013
Iteration 86/1000 | Loss: 0.00001013
Iteration 87/1000 | Loss: 0.00001013
Iteration 88/1000 | Loss: 0.00001013
Iteration 89/1000 | Loss: 0.00001013
Iteration 90/1000 | Loss: 0.00001013
Iteration 91/1000 | Loss: 0.00001012
Iteration 92/1000 | Loss: 0.00001012
Iteration 93/1000 | Loss: 0.00001012
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001011
Iteration 96/1000 | Loss: 0.00001011
Iteration 97/1000 | Loss: 0.00001010
Iteration 98/1000 | Loss: 0.00001010
Iteration 99/1000 | Loss: 0.00001010
Iteration 100/1000 | Loss: 0.00001009
Iteration 101/1000 | Loss: 0.00001009
Iteration 102/1000 | Loss: 0.00001009
Iteration 103/1000 | Loss: 0.00001009
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001008
Iteration 106/1000 | Loss: 0.00001008
Iteration 107/1000 | Loss: 0.00001008
Iteration 108/1000 | Loss: 0.00001008
Iteration 109/1000 | Loss: 0.00001007
Iteration 110/1000 | Loss: 0.00001007
Iteration 111/1000 | Loss: 0.00001007
Iteration 112/1000 | Loss: 0.00001006
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001005
Iteration 115/1000 | Loss: 0.00001005
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001004
Iteration 119/1000 | Loss: 0.00001004
Iteration 120/1000 | Loss: 0.00001004
Iteration 121/1000 | Loss: 0.00001004
Iteration 122/1000 | Loss: 0.00001003
Iteration 123/1000 | Loss: 0.00001003
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001002
Iteration 127/1000 | Loss: 0.00001002
Iteration 128/1000 | Loss: 0.00001002
Iteration 129/1000 | Loss: 0.00001001
Iteration 130/1000 | Loss: 0.00001001
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00000999
Iteration 137/1000 | Loss: 0.00000999
Iteration 138/1000 | Loss: 0.00000999
Iteration 139/1000 | Loss: 0.00000998
Iteration 140/1000 | Loss: 0.00000998
Iteration 141/1000 | Loss: 0.00000998
Iteration 142/1000 | Loss: 0.00000998
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000997
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000996
Iteration 155/1000 | Loss: 0.00000996
Iteration 156/1000 | Loss: 0.00000996
Iteration 157/1000 | Loss: 0.00000996
Iteration 158/1000 | Loss: 0.00000996
Iteration 159/1000 | Loss: 0.00000996
Iteration 160/1000 | Loss: 0.00000996
Iteration 161/1000 | Loss: 0.00000996
Iteration 162/1000 | Loss: 0.00000996
Iteration 163/1000 | Loss: 0.00000996
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000995
Iteration 166/1000 | Loss: 0.00000995
Iteration 167/1000 | Loss: 0.00000995
Iteration 168/1000 | Loss: 0.00000995
Iteration 169/1000 | Loss: 0.00000995
Iteration 170/1000 | Loss: 0.00000994
Iteration 171/1000 | Loss: 0.00000994
Iteration 172/1000 | Loss: 0.00000994
Iteration 173/1000 | Loss: 0.00000994
Iteration 174/1000 | Loss: 0.00000994
Iteration 175/1000 | Loss: 0.00000994
Iteration 176/1000 | Loss: 0.00000994
Iteration 177/1000 | Loss: 0.00000994
Iteration 178/1000 | Loss: 0.00000993
Iteration 179/1000 | Loss: 0.00000993
Iteration 180/1000 | Loss: 0.00000993
Iteration 181/1000 | Loss: 0.00000993
Iteration 182/1000 | Loss: 0.00000993
Iteration 183/1000 | Loss: 0.00000993
Iteration 184/1000 | Loss: 0.00000993
Iteration 185/1000 | Loss: 0.00000993
Iteration 186/1000 | Loss: 0.00000993
Iteration 187/1000 | Loss: 0.00000993
Iteration 188/1000 | Loss: 0.00000993
Iteration 189/1000 | Loss: 0.00000992
Iteration 190/1000 | Loss: 0.00000992
Iteration 191/1000 | Loss: 0.00000992
Iteration 192/1000 | Loss: 0.00000992
Iteration 193/1000 | Loss: 0.00000992
Iteration 194/1000 | Loss: 0.00000992
Iteration 195/1000 | Loss: 0.00000992
Iteration 196/1000 | Loss: 0.00000992
Iteration 197/1000 | Loss: 0.00000992
Iteration 198/1000 | Loss: 0.00000992
Iteration 199/1000 | Loss: 0.00000992
Iteration 200/1000 | Loss: 0.00000992
Iteration 201/1000 | Loss: 0.00000992
Iteration 202/1000 | Loss: 0.00000992
Iteration 203/1000 | Loss: 0.00000992
Iteration 204/1000 | Loss: 0.00000992
Iteration 205/1000 | Loss: 0.00000992
Iteration 206/1000 | Loss: 0.00000992
Iteration 207/1000 | Loss: 0.00000992
Iteration 208/1000 | Loss: 0.00000992
Iteration 209/1000 | Loss: 0.00000992
Iteration 210/1000 | Loss: 0.00000992
Iteration 211/1000 | Loss: 0.00000992
Iteration 212/1000 | Loss: 0.00000992
Iteration 213/1000 | Loss: 0.00000992
Iteration 214/1000 | Loss: 0.00000992
Iteration 215/1000 | Loss: 0.00000992
Iteration 216/1000 | Loss: 0.00000992
Iteration 217/1000 | Loss: 0.00000992
Iteration 218/1000 | Loss: 0.00000992
Iteration 219/1000 | Loss: 0.00000992
Iteration 220/1000 | Loss: 0.00000992
Iteration 221/1000 | Loss: 0.00000992
Iteration 222/1000 | Loss: 0.00000992
Iteration 223/1000 | Loss: 0.00000992
Iteration 224/1000 | Loss: 0.00000992
Iteration 225/1000 | Loss: 0.00000992
Iteration 226/1000 | Loss: 0.00000992
Iteration 227/1000 | Loss: 0.00000992
Iteration 228/1000 | Loss: 0.00000992
Iteration 229/1000 | Loss: 0.00000992
Iteration 230/1000 | Loss: 0.00000992
Iteration 231/1000 | Loss: 0.00000992
Iteration 232/1000 | Loss: 0.00000992
Iteration 233/1000 | Loss: 0.00000992
Iteration 234/1000 | Loss: 0.00000992
Iteration 235/1000 | Loss: 0.00000992
Iteration 236/1000 | Loss: 0.00000992
Iteration 237/1000 | Loss: 0.00000992
Iteration 238/1000 | Loss: 0.00000992
Iteration 239/1000 | Loss: 0.00000992
Iteration 240/1000 | Loss: 0.00000992
Iteration 241/1000 | Loss: 0.00000992
Iteration 242/1000 | Loss: 0.00000992
Iteration 243/1000 | Loss: 0.00000992
Iteration 244/1000 | Loss: 0.00000992
Iteration 245/1000 | Loss: 0.00000992
Iteration 246/1000 | Loss: 0.00000992
Iteration 247/1000 | Loss: 0.00000992
Iteration 248/1000 | Loss: 0.00000992
Iteration 249/1000 | Loss: 0.00000992
Iteration 250/1000 | Loss: 0.00000992
Iteration 251/1000 | Loss: 0.00000992
Iteration 252/1000 | Loss: 0.00000992
Iteration 253/1000 | Loss: 0.00000992
Iteration 254/1000 | Loss: 0.00000992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [9.918039722833782e-06, 9.918039722833782e-06, 9.918039722833782e-06, 9.918039722833782e-06, 9.918039722833782e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.918039722833782e-06

Optimization complete. Final v2v error: 2.702440023422241 mm

Highest mean error: 3.0402488708496094 mm for frame 109

Lowest mean error: 2.505037307739258 mm for frame 19

Saving results

Total time: 45.160669565200806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923869
Iteration 2/25 | Loss: 0.00262371
Iteration 3/25 | Loss: 0.00214459
Iteration 4/25 | Loss: 0.00207807
Iteration 5/25 | Loss: 0.00193035
Iteration 6/25 | Loss: 0.00174120
Iteration 7/25 | Loss: 0.00163740
Iteration 8/25 | Loss: 0.00159612
Iteration 9/25 | Loss: 0.00161249
Iteration 10/25 | Loss: 0.00155562
Iteration 11/25 | Loss: 0.00153017
Iteration 12/25 | Loss: 0.00151603
Iteration 13/25 | Loss: 0.00150947
Iteration 14/25 | Loss: 0.00150617
Iteration 15/25 | Loss: 0.00150501
Iteration 16/25 | Loss: 0.00150455
Iteration 17/25 | Loss: 0.00150443
Iteration 18/25 | Loss: 0.00150442
Iteration 19/25 | Loss: 0.00150441
Iteration 20/25 | Loss: 0.00150441
Iteration 21/25 | Loss: 0.00150441
Iteration 22/25 | Loss: 0.00150439
Iteration 23/25 | Loss: 0.00150439
Iteration 24/25 | Loss: 0.00150439
Iteration 25/25 | Loss: 0.00150439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20592988
Iteration 2/25 | Loss: 0.00208438
Iteration 3/25 | Loss: 0.00208438
Iteration 4/25 | Loss: 0.00208438
Iteration 5/25 | Loss: 0.00208438
Iteration 6/25 | Loss: 0.00208438
Iteration 7/25 | Loss: 0.00208438
Iteration 8/25 | Loss: 0.00208438
Iteration 9/25 | Loss: 0.00208438
Iteration 10/25 | Loss: 0.00208438
Iteration 11/25 | Loss: 0.00208438
Iteration 12/25 | Loss: 0.00208438
Iteration 13/25 | Loss: 0.00208438
Iteration 14/25 | Loss: 0.00208438
Iteration 15/25 | Loss: 0.00208438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020843814127147198, 0.0020843814127147198, 0.0020843814127147198, 0.0020843814127147198, 0.0020843814127147198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020843814127147198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208438
Iteration 2/1000 | Loss: 0.00528874
Iteration 3/1000 | Loss: 0.00255024
Iteration 4/1000 | Loss: 0.00119894
Iteration 5/1000 | Loss: 0.00068420
Iteration 6/1000 | Loss: 0.00262108
Iteration 7/1000 | Loss: 0.00310942
Iteration 8/1000 | Loss: 0.00094745
Iteration 9/1000 | Loss: 0.00046017
Iteration 10/1000 | Loss: 0.00029944
Iteration 11/1000 | Loss: 0.00015129
Iteration 12/1000 | Loss: 0.00012464
Iteration 13/1000 | Loss: 0.00027717
Iteration 14/1000 | Loss: 0.00011143
Iteration 15/1000 | Loss: 0.00035064
Iteration 16/1000 | Loss: 0.00103370
Iteration 17/1000 | Loss: 0.00298579
Iteration 18/1000 | Loss: 0.00033425
Iteration 19/1000 | Loss: 0.00064635
Iteration 20/1000 | Loss: 0.00202176
Iteration 21/1000 | Loss: 0.00030350
Iteration 22/1000 | Loss: 0.00021976
Iteration 23/1000 | Loss: 0.00025290
Iteration 24/1000 | Loss: 0.00021670
Iteration 25/1000 | Loss: 0.00015479
Iteration 26/1000 | Loss: 0.00019760
Iteration 27/1000 | Loss: 0.00077673
Iteration 28/1000 | Loss: 0.00074499
Iteration 29/1000 | Loss: 0.00025997
Iteration 30/1000 | Loss: 0.00050212
Iteration 31/1000 | Loss: 0.00018083
Iteration 32/1000 | Loss: 0.00020292
Iteration 33/1000 | Loss: 0.00010054
Iteration 34/1000 | Loss: 0.00067783
Iteration 35/1000 | Loss: 0.00011760
Iteration 36/1000 | Loss: 0.00017246
Iteration 37/1000 | Loss: 0.00028090
Iteration 38/1000 | Loss: 0.00009175
Iteration 39/1000 | Loss: 0.00008643
Iteration 40/1000 | Loss: 0.00028782
Iteration 41/1000 | Loss: 0.00054365
Iteration 42/1000 | Loss: 0.00010483
Iteration 43/1000 | Loss: 0.00007979
Iteration 44/1000 | Loss: 0.00027407
Iteration 45/1000 | Loss: 0.00029104
Iteration 46/1000 | Loss: 0.00007282
Iteration 47/1000 | Loss: 0.00006550
Iteration 48/1000 | Loss: 0.00027219
Iteration 49/1000 | Loss: 0.00022358
Iteration 50/1000 | Loss: 0.00006646
Iteration 51/1000 | Loss: 0.00006032
Iteration 52/1000 | Loss: 0.00005671
Iteration 53/1000 | Loss: 0.00021205
Iteration 54/1000 | Loss: 0.00052639
Iteration 55/1000 | Loss: 0.00025550
Iteration 56/1000 | Loss: 0.00010235
Iteration 57/1000 | Loss: 0.00016915
Iteration 58/1000 | Loss: 0.00010204
Iteration 59/1000 | Loss: 0.00009344
Iteration 60/1000 | Loss: 0.00009495
Iteration 61/1000 | Loss: 0.00010016
Iteration 62/1000 | Loss: 0.00006076
Iteration 63/1000 | Loss: 0.00007615
Iteration 64/1000 | Loss: 0.00006124
Iteration 65/1000 | Loss: 0.00005933
Iteration 66/1000 | Loss: 0.00005104
Iteration 67/1000 | Loss: 0.00005578
Iteration 68/1000 | Loss: 0.00005630
Iteration 69/1000 | Loss: 0.00005178
Iteration 70/1000 | Loss: 0.00006898
Iteration 71/1000 | Loss: 0.00006205
Iteration 72/1000 | Loss: 0.00006649
Iteration 73/1000 | Loss: 0.00017414
Iteration 74/1000 | Loss: 0.00010106
Iteration 75/1000 | Loss: 0.00004554
Iteration 76/1000 | Loss: 0.00004452
Iteration 77/1000 | Loss: 0.00004341
Iteration 78/1000 | Loss: 0.00005943
Iteration 79/1000 | Loss: 0.00006893
Iteration 80/1000 | Loss: 0.00006307
Iteration 81/1000 | Loss: 0.00006626
Iteration 82/1000 | Loss: 0.00006148
Iteration 83/1000 | Loss: 0.00003965
Iteration 84/1000 | Loss: 0.00003877
Iteration 85/1000 | Loss: 0.00003800
Iteration 86/1000 | Loss: 0.00004656
Iteration 87/1000 | Loss: 0.00004276
Iteration 88/1000 | Loss: 0.00003721
Iteration 89/1000 | Loss: 0.00003677
Iteration 90/1000 | Loss: 0.00003630
Iteration 91/1000 | Loss: 0.00003600
Iteration 92/1000 | Loss: 0.00003578
Iteration 93/1000 | Loss: 0.00003575
Iteration 94/1000 | Loss: 0.00003566
Iteration 95/1000 | Loss: 0.00003561
Iteration 96/1000 | Loss: 0.00003560
Iteration 97/1000 | Loss: 0.00003557
Iteration 98/1000 | Loss: 0.00003556
Iteration 99/1000 | Loss: 0.00003556
Iteration 100/1000 | Loss: 0.00003555
Iteration 101/1000 | Loss: 0.00003554
Iteration 102/1000 | Loss: 0.00003553
Iteration 103/1000 | Loss: 0.00003553
Iteration 104/1000 | Loss: 0.00003553
Iteration 105/1000 | Loss: 0.00003553
Iteration 106/1000 | Loss: 0.00003553
Iteration 107/1000 | Loss: 0.00003552
Iteration 108/1000 | Loss: 0.00003552
Iteration 109/1000 | Loss: 0.00003552
Iteration 110/1000 | Loss: 0.00003552
Iteration 111/1000 | Loss: 0.00003552
Iteration 112/1000 | Loss: 0.00003551
Iteration 113/1000 | Loss: 0.00003551
Iteration 114/1000 | Loss: 0.00003547
Iteration 115/1000 | Loss: 0.00003547
Iteration 116/1000 | Loss: 0.00003544
Iteration 117/1000 | Loss: 0.00003544
Iteration 118/1000 | Loss: 0.00003543
Iteration 119/1000 | Loss: 0.00003543
Iteration 120/1000 | Loss: 0.00003542
Iteration 121/1000 | Loss: 0.00003542
Iteration 122/1000 | Loss: 0.00003542
Iteration 123/1000 | Loss: 0.00003537
Iteration 124/1000 | Loss: 0.00003537
Iteration 125/1000 | Loss: 0.00003537
Iteration 126/1000 | Loss: 0.00003537
Iteration 127/1000 | Loss: 0.00003537
Iteration 128/1000 | Loss: 0.00003537
Iteration 129/1000 | Loss: 0.00003537
Iteration 130/1000 | Loss: 0.00003537
Iteration 131/1000 | Loss: 0.00003537
Iteration 132/1000 | Loss: 0.00003537
Iteration 133/1000 | Loss: 0.00003536
Iteration 134/1000 | Loss: 0.00003536
Iteration 135/1000 | Loss: 0.00003536
Iteration 136/1000 | Loss: 0.00003536
Iteration 137/1000 | Loss: 0.00003536
Iteration 138/1000 | Loss: 0.00003536
Iteration 139/1000 | Loss: 0.00003536
Iteration 140/1000 | Loss: 0.00003536
Iteration 141/1000 | Loss: 0.00003535
Iteration 142/1000 | Loss: 0.00003535
Iteration 143/1000 | Loss: 0.00003534
Iteration 144/1000 | Loss: 0.00003534
Iteration 145/1000 | Loss: 0.00003534
Iteration 146/1000 | Loss: 0.00003534
Iteration 147/1000 | Loss: 0.00003533
Iteration 148/1000 | Loss: 0.00003533
Iteration 149/1000 | Loss: 0.00003532
Iteration 150/1000 | Loss: 0.00003532
Iteration 151/1000 | Loss: 0.00003531
Iteration 152/1000 | Loss: 0.00003531
Iteration 153/1000 | Loss: 0.00003531
Iteration 154/1000 | Loss: 0.00003531
Iteration 155/1000 | Loss: 0.00003531
Iteration 156/1000 | Loss: 0.00003531
Iteration 157/1000 | Loss: 0.00006185
Iteration 158/1000 | Loss: 0.00004475
Iteration 159/1000 | Loss: 0.00003538
Iteration 160/1000 | Loss: 0.00006098
Iteration 161/1000 | Loss: 0.00004791
Iteration 162/1000 | Loss: 0.00006055
Iteration 163/1000 | Loss: 0.00004534
Iteration 164/1000 | Loss: 0.00006104
Iteration 165/1000 | Loss: 0.00004483
Iteration 166/1000 | Loss: 0.00006042
Iteration 167/1000 | Loss: 0.00005167
Iteration 168/1000 | Loss: 0.00005988
Iteration 169/1000 | Loss: 0.00004402
Iteration 170/1000 | Loss: 0.00005970
Iteration 171/1000 | Loss: 0.00004501
Iteration 172/1000 | Loss: 0.00005977
Iteration 173/1000 | Loss: 0.00004766
Iteration 174/1000 | Loss: 0.00005913
Iteration 175/1000 | Loss: 0.00004698
Iteration 176/1000 | Loss: 0.00005836
Iteration 177/1000 | Loss: 0.00006862
Iteration 178/1000 | Loss: 0.00003742
Iteration 179/1000 | Loss: 0.00003584
Iteration 180/1000 | Loss: 0.00005973
Iteration 181/1000 | Loss: 0.00004085
Iteration 182/1000 | Loss: 0.00006224
Iteration 183/1000 | Loss: 0.00005328
Iteration 184/1000 | Loss: 0.00005579
Iteration 185/1000 | Loss: 0.00004057
Iteration 186/1000 | Loss: 0.00006039
Iteration 187/1000 | Loss: 0.00003598
Iteration 188/1000 | Loss: 0.00003581
Iteration 189/1000 | Loss: 0.00003575
Iteration 190/1000 | Loss: 0.00003566
Iteration 191/1000 | Loss: 0.00003558
Iteration 192/1000 | Loss: 0.00003554
Iteration 193/1000 | Loss: 0.00003554
Iteration 194/1000 | Loss: 0.00003553
Iteration 195/1000 | Loss: 0.00003552
Iteration 196/1000 | Loss: 0.00003552
Iteration 197/1000 | Loss: 0.00003552
Iteration 198/1000 | Loss: 0.00003551
Iteration 199/1000 | Loss: 0.00003548
Iteration 200/1000 | Loss: 0.00003544
Iteration 201/1000 | Loss: 0.00003543
Iteration 202/1000 | Loss: 0.00003542
Iteration 203/1000 | Loss: 0.00003542
Iteration 204/1000 | Loss: 0.00003541
Iteration 205/1000 | Loss: 0.00003541
Iteration 206/1000 | Loss: 0.00003541
Iteration 207/1000 | Loss: 0.00003541
Iteration 208/1000 | Loss: 0.00003541
Iteration 209/1000 | Loss: 0.00003541
Iteration 210/1000 | Loss: 0.00003541
Iteration 211/1000 | Loss: 0.00003540
Iteration 212/1000 | Loss: 0.00003539
Iteration 213/1000 | Loss: 0.00003539
Iteration 214/1000 | Loss: 0.00003539
Iteration 215/1000 | Loss: 0.00003532
Iteration 216/1000 | Loss: 0.00003509
Iteration 217/1000 | Loss: 0.00003472
Iteration 218/1000 | Loss: 0.00003466
Iteration 219/1000 | Loss: 0.00003439
Iteration 220/1000 | Loss: 0.00003424
Iteration 221/1000 | Loss: 0.00003420
Iteration 222/1000 | Loss: 0.00003412
Iteration 223/1000 | Loss: 0.00003411
Iteration 224/1000 | Loss: 0.00003410
Iteration 225/1000 | Loss: 0.00003410
Iteration 226/1000 | Loss: 0.00003409
Iteration 227/1000 | Loss: 0.00003408
Iteration 228/1000 | Loss: 0.00003408
Iteration 229/1000 | Loss: 0.00003407
Iteration 230/1000 | Loss: 0.00003407
Iteration 231/1000 | Loss: 0.00003407
Iteration 232/1000 | Loss: 0.00003406
Iteration 233/1000 | Loss: 0.00003406
Iteration 234/1000 | Loss: 0.00003405
Iteration 235/1000 | Loss: 0.00003403
Iteration 236/1000 | Loss: 0.00003403
Iteration 237/1000 | Loss: 0.00003403
Iteration 238/1000 | Loss: 0.00003403
Iteration 239/1000 | Loss: 0.00003403
Iteration 240/1000 | Loss: 0.00003403
Iteration 241/1000 | Loss: 0.00003403
Iteration 242/1000 | Loss: 0.00003403
Iteration 243/1000 | Loss: 0.00003403
Iteration 244/1000 | Loss: 0.00003403
Iteration 245/1000 | Loss: 0.00003403
Iteration 246/1000 | Loss: 0.00003403
Iteration 247/1000 | Loss: 0.00003403
Iteration 248/1000 | Loss: 0.00003403
Iteration 249/1000 | Loss: 0.00003403
Iteration 250/1000 | Loss: 0.00003402
Iteration 251/1000 | Loss: 0.00003402
Iteration 252/1000 | Loss: 0.00003402
Iteration 253/1000 | Loss: 0.00003402
Iteration 254/1000 | Loss: 0.00003402
Iteration 255/1000 | Loss: 0.00003402
Iteration 256/1000 | Loss: 0.00003402
Iteration 257/1000 | Loss: 0.00003402
Iteration 258/1000 | Loss: 0.00003402
Iteration 259/1000 | Loss: 0.00003401
Iteration 260/1000 | Loss: 0.00003401
Iteration 261/1000 | Loss: 0.00003401
Iteration 262/1000 | Loss: 0.00003401
Iteration 263/1000 | Loss: 0.00003401
Iteration 264/1000 | Loss: 0.00003401
Iteration 265/1000 | Loss: 0.00003401
Iteration 266/1000 | Loss: 0.00003401
Iteration 267/1000 | Loss: 0.00003401
Iteration 268/1000 | Loss: 0.00003400
Iteration 269/1000 | Loss: 0.00003400
Iteration 270/1000 | Loss: 0.00003400
Iteration 271/1000 | Loss: 0.00003400
Iteration 272/1000 | Loss: 0.00003400
Iteration 273/1000 | Loss: 0.00003400
Iteration 274/1000 | Loss: 0.00003400
Iteration 275/1000 | Loss: 0.00003400
Iteration 276/1000 | Loss: 0.00003400
Iteration 277/1000 | Loss: 0.00003400
Iteration 278/1000 | Loss: 0.00003400
Iteration 279/1000 | Loss: 0.00003400
Iteration 280/1000 | Loss: 0.00003400
Iteration 281/1000 | Loss: 0.00003400
Iteration 282/1000 | Loss: 0.00003400
Iteration 283/1000 | Loss: 0.00003400
Iteration 284/1000 | Loss: 0.00003400
Iteration 285/1000 | Loss: 0.00003399
Iteration 286/1000 | Loss: 0.00003399
Iteration 287/1000 | Loss: 0.00003399
Iteration 288/1000 | Loss: 0.00003399
Iteration 289/1000 | Loss: 0.00003399
Iteration 290/1000 | Loss: 0.00003399
Iteration 291/1000 | Loss: 0.00003399
Iteration 292/1000 | Loss: 0.00003399
Iteration 293/1000 | Loss: 0.00003399
Iteration 294/1000 | Loss: 0.00003399
Iteration 295/1000 | Loss: 0.00003399
Iteration 296/1000 | Loss: 0.00003399
Iteration 297/1000 | Loss: 0.00003399
Iteration 298/1000 | Loss: 0.00003399
Iteration 299/1000 | Loss: 0.00003399
Iteration 300/1000 | Loss: 0.00003399
Iteration 301/1000 | Loss: 0.00003399
Iteration 302/1000 | Loss: 0.00003399
Iteration 303/1000 | Loss: 0.00003399
Iteration 304/1000 | Loss: 0.00003399
Iteration 305/1000 | Loss: 0.00003399
Iteration 306/1000 | Loss: 0.00003399
Iteration 307/1000 | Loss: 0.00003399
Iteration 308/1000 | Loss: 0.00003399
Iteration 309/1000 | Loss: 0.00003399
Iteration 310/1000 | Loss: 0.00003399
Iteration 311/1000 | Loss: 0.00003399
Iteration 312/1000 | Loss: 0.00003399
Iteration 313/1000 | Loss: 0.00003399
Iteration 314/1000 | Loss: 0.00003399
Iteration 315/1000 | Loss: 0.00003399
Iteration 316/1000 | Loss: 0.00003399
Iteration 317/1000 | Loss: 0.00003399
Iteration 318/1000 | Loss: 0.00003399
Iteration 319/1000 | Loss: 0.00003399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 319. Stopping optimization.
Last 5 losses: [3.398900662432425e-05, 3.398900662432425e-05, 3.398900662432425e-05, 3.398900662432425e-05, 3.398900662432425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.398900662432425e-05

Optimization complete. Final v2v error: 4.401051044464111 mm

Highest mean error: 6.947650909423828 mm for frame 70

Lowest mean error: 3.471428871154785 mm for frame 124

Saving results

Total time: 220.41888165473938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436328
Iteration 2/25 | Loss: 0.00153361
Iteration 3/25 | Loss: 0.00137874
Iteration 4/25 | Loss: 0.00136466
Iteration 5/25 | Loss: 0.00136262
Iteration 6/25 | Loss: 0.00136222
Iteration 7/25 | Loss: 0.00136222
Iteration 8/25 | Loss: 0.00136222
Iteration 9/25 | Loss: 0.00136222
Iteration 10/25 | Loss: 0.00136222
Iteration 11/25 | Loss: 0.00136222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013622237602248788, 0.0013622237602248788, 0.0013622237602248788, 0.0013622237602248788, 0.0013622237602248788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013622237602248788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.15883923
Iteration 2/25 | Loss: 0.00190362
Iteration 3/25 | Loss: 0.00190359
Iteration 4/25 | Loss: 0.00190359
Iteration 5/25 | Loss: 0.00190359
Iteration 6/25 | Loss: 0.00190359
Iteration 7/25 | Loss: 0.00190359
Iteration 8/25 | Loss: 0.00190359
Iteration 9/25 | Loss: 0.00190359
Iteration 10/25 | Loss: 0.00190358
Iteration 11/25 | Loss: 0.00190358
Iteration 12/25 | Loss: 0.00190358
Iteration 13/25 | Loss: 0.00190358
Iteration 14/25 | Loss: 0.00190358
Iteration 15/25 | Loss: 0.00190358
Iteration 16/25 | Loss: 0.00190358
Iteration 17/25 | Loss: 0.00190358
Iteration 18/25 | Loss: 0.00190358
Iteration 19/25 | Loss: 0.00190358
Iteration 20/25 | Loss: 0.00190358
Iteration 21/25 | Loss: 0.00190358
Iteration 22/25 | Loss: 0.00190358
Iteration 23/25 | Loss: 0.00190358
Iteration 24/25 | Loss: 0.00190358
Iteration 25/25 | Loss: 0.00190358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190358
Iteration 2/1000 | Loss: 0.00002851
Iteration 3/1000 | Loss: 0.00002021
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001616
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001525
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001447
Iteration 13/1000 | Loss: 0.00001445
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001409
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001405
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001380
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001366
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001362
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001361
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001360
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001356
Iteration 76/1000 | Loss: 0.00001356
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001356
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.3558858881879132e-05, 1.3558858881879132e-05, 1.3558858881879132e-05, 1.3558858881879132e-05, 1.3558858881879132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3558858881879132e-05

Optimization complete. Final v2v error: 3.146106481552124 mm

Highest mean error: 3.8021750450134277 mm for frame 72

Lowest mean error: 2.7995243072509766 mm for frame 100

Saving results

Total time: 36.77193236351013
