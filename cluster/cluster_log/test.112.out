Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=112, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6272-6327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075674
Iteration 2/25 | Loss: 0.00193130
Iteration 3/25 | Loss: 0.00170864
Iteration 4/25 | Loss: 0.00132704
Iteration 5/25 | Loss: 0.00101390
Iteration 6/25 | Loss: 0.00089118
Iteration 7/25 | Loss: 0.00084439
Iteration 8/25 | Loss: 0.00081041
Iteration 9/25 | Loss: 0.00078662
Iteration 10/25 | Loss: 0.00075382
Iteration 11/25 | Loss: 0.00074302
Iteration 12/25 | Loss: 0.00073598
Iteration 13/25 | Loss: 0.00071640
Iteration 14/25 | Loss: 0.00070877
Iteration 15/25 | Loss: 0.00069007
Iteration 16/25 | Loss: 0.00069079
Iteration 17/25 | Loss: 0.00068810
Iteration 18/25 | Loss: 0.00068783
Iteration 19/25 | Loss: 0.00068617
Iteration 20/25 | Loss: 0.00068811
Iteration 21/25 | Loss: 0.00069079
Iteration 22/25 | Loss: 0.00068719
Iteration 23/25 | Loss: 0.00068280
Iteration 24/25 | Loss: 0.00068126
Iteration 25/25 | Loss: 0.00068766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50202966
Iteration 2/25 | Loss: 0.00075199
Iteration 3/25 | Loss: 0.00066363
Iteration 4/25 | Loss: 0.00066363
Iteration 5/25 | Loss: 0.00066363
Iteration 6/25 | Loss: 0.00066363
Iteration 7/25 | Loss: 0.00066363
Iteration 8/25 | Loss: 0.00066363
Iteration 9/25 | Loss: 0.00066363
Iteration 10/25 | Loss: 0.00066363
Iteration 11/25 | Loss: 0.00066363
Iteration 12/25 | Loss: 0.00066363
Iteration 13/25 | Loss: 0.00066363
Iteration 14/25 | Loss: 0.00066363
Iteration 15/25 | Loss: 0.00066363
Iteration 16/25 | Loss: 0.00066363
Iteration 17/25 | Loss: 0.00066363
Iteration 18/25 | Loss: 0.00066363
Iteration 19/25 | Loss: 0.00066363
Iteration 20/25 | Loss: 0.00066363
Iteration 21/25 | Loss: 0.00066363
Iteration 22/25 | Loss: 0.00066363
Iteration 23/25 | Loss: 0.00066363
Iteration 24/25 | Loss: 0.00066363
Iteration 25/25 | Loss: 0.00066363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066363
Iteration 2/1000 | Loss: 0.00069880
Iteration 3/1000 | Loss: 0.00057779
Iteration 4/1000 | Loss: 0.00023271
Iteration 5/1000 | Loss: 0.00046592
Iteration 6/1000 | Loss: 0.00053979
Iteration 7/1000 | Loss: 0.00040231
Iteration 8/1000 | Loss: 0.00041393
Iteration 9/1000 | Loss: 0.00038354
Iteration 10/1000 | Loss: 0.00040144
Iteration 11/1000 | Loss: 0.00040130
Iteration 12/1000 | Loss: 0.00062266
Iteration 13/1000 | Loss: 0.00050832
Iteration 14/1000 | Loss: 0.00057234
Iteration 15/1000 | Loss: 0.00065599
Iteration 16/1000 | Loss: 0.00051937
Iteration 17/1000 | Loss: 0.00091870
Iteration 18/1000 | Loss: 0.00049805
Iteration 19/1000 | Loss: 0.00042552
Iteration 20/1000 | Loss: 0.00117222
Iteration 21/1000 | Loss: 0.00116419
Iteration 22/1000 | Loss: 0.00090025
Iteration 23/1000 | Loss: 0.00048684
Iteration 24/1000 | Loss: 0.00027340
Iteration 25/1000 | Loss: 0.00018284
Iteration 26/1000 | Loss: 0.00025556
Iteration 27/1000 | Loss: 0.00046455
Iteration 28/1000 | Loss: 0.00054601
Iteration 29/1000 | Loss: 0.00033622
Iteration 30/1000 | Loss: 0.00049661
Iteration 31/1000 | Loss: 0.00056610
Iteration 32/1000 | Loss: 0.00028136
Iteration 33/1000 | Loss: 0.00035812
Iteration 34/1000 | Loss: 0.00026031
Iteration 35/1000 | Loss: 0.00008929
Iteration 36/1000 | Loss: 0.00017263
Iteration 37/1000 | Loss: 0.00046207
Iteration 38/1000 | Loss: 0.00039445
Iteration 39/1000 | Loss: 0.00038276
Iteration 40/1000 | Loss: 0.00051753
Iteration 41/1000 | Loss: 0.00038829
Iteration 42/1000 | Loss: 0.00039456
Iteration 43/1000 | Loss: 0.00030254
Iteration 44/1000 | Loss: 0.00036589
Iteration 45/1000 | Loss: 0.00005045
Iteration 46/1000 | Loss: 0.00032349
Iteration 47/1000 | Loss: 0.00032989
Iteration 48/1000 | Loss: 0.00006164
Iteration 49/1000 | Loss: 0.00011007
Iteration 50/1000 | Loss: 0.00014004
Iteration 51/1000 | Loss: 0.00003546
Iteration 52/1000 | Loss: 0.00003741
Iteration 53/1000 | Loss: 0.00002986
Iteration 54/1000 | Loss: 0.00021734
Iteration 55/1000 | Loss: 0.00005471
Iteration 56/1000 | Loss: 0.00078123
Iteration 57/1000 | Loss: 0.00002685
Iteration 58/1000 | Loss: 0.00002345
Iteration 59/1000 | Loss: 0.00002228
Iteration 60/1000 | Loss: 0.00002028
Iteration 61/1000 | Loss: 0.00042096
Iteration 62/1000 | Loss: 0.00032810
Iteration 63/1000 | Loss: 0.00006975
Iteration 64/1000 | Loss: 0.00027316
Iteration 65/1000 | Loss: 0.00042477
Iteration 66/1000 | Loss: 0.00010887
Iteration 67/1000 | Loss: 0.00001997
Iteration 68/1000 | Loss: 0.00013277
Iteration 69/1000 | Loss: 0.00077939
Iteration 70/1000 | Loss: 0.00027676
Iteration 71/1000 | Loss: 0.00024624
Iteration 72/1000 | Loss: 0.00068206
Iteration 73/1000 | Loss: 0.00012587
Iteration 74/1000 | Loss: 0.00027394
Iteration 75/1000 | Loss: 0.00017709
Iteration 76/1000 | Loss: 0.00016694
Iteration 77/1000 | Loss: 0.00003801
Iteration 78/1000 | Loss: 0.00021313
Iteration 79/1000 | Loss: 0.00009652
Iteration 80/1000 | Loss: 0.00030786
Iteration 81/1000 | Loss: 0.00030624
Iteration 82/1000 | Loss: 0.00007586
Iteration 83/1000 | Loss: 0.00046891
Iteration 84/1000 | Loss: 0.00064467
Iteration 85/1000 | Loss: 0.00004417
Iteration 86/1000 | Loss: 0.00002088
Iteration 87/1000 | Loss: 0.00001803
Iteration 88/1000 | Loss: 0.00001627
Iteration 89/1000 | Loss: 0.00032502
Iteration 90/1000 | Loss: 0.00002889
Iteration 91/1000 | Loss: 0.00002202
Iteration 92/1000 | Loss: 0.00029240
Iteration 93/1000 | Loss: 0.00020846
Iteration 94/1000 | Loss: 0.00002763
Iteration 95/1000 | Loss: 0.00038196
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00002437
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001342
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001243
Iteration 104/1000 | Loss: 0.00001225
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00025412
Iteration 107/1000 | Loss: 0.00089362
Iteration 108/1000 | Loss: 0.00071291
Iteration 109/1000 | Loss: 0.00045556
Iteration 110/1000 | Loss: 0.00002988
Iteration 111/1000 | Loss: 0.00007655
Iteration 112/1000 | Loss: 0.00001952
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00006546
Iteration 116/1000 | Loss: 0.00031429
Iteration 117/1000 | Loss: 0.00013259
Iteration 118/1000 | Loss: 0.00020370
Iteration 119/1000 | Loss: 0.00027282
Iteration 120/1000 | Loss: 0.00039495
Iteration 121/1000 | Loss: 0.00068137
Iteration 122/1000 | Loss: 0.00048245
Iteration 123/1000 | Loss: 0.00010904
Iteration 124/1000 | Loss: 0.00127146
Iteration 125/1000 | Loss: 0.00118615
Iteration 126/1000 | Loss: 0.00014326
Iteration 127/1000 | Loss: 0.00002275
Iteration 128/1000 | Loss: 0.00016430
Iteration 129/1000 | Loss: 0.00007455
Iteration 130/1000 | Loss: 0.00031596
Iteration 131/1000 | Loss: 0.00006840
Iteration 132/1000 | Loss: 0.00037129
Iteration 133/1000 | Loss: 0.00006419
Iteration 134/1000 | Loss: 0.00044871
Iteration 135/1000 | Loss: 0.00003885
Iteration 136/1000 | Loss: 0.00016008
Iteration 137/1000 | Loss: 0.00023062
Iteration 138/1000 | Loss: 0.00025511
Iteration 139/1000 | Loss: 0.00022509
Iteration 140/1000 | Loss: 0.00023344
Iteration 141/1000 | Loss: 0.00029322
Iteration 142/1000 | Loss: 0.00002714
Iteration 143/1000 | Loss: 0.00041873
Iteration 144/1000 | Loss: 0.00045295
Iteration 145/1000 | Loss: 0.00002156
Iteration 146/1000 | Loss: 0.00030506
Iteration 147/1000 | Loss: 0.00006526
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00006266
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00001535
Iteration 153/1000 | Loss: 0.00012197
Iteration 154/1000 | Loss: 0.00001591
Iteration 155/1000 | Loss: 0.00001522
Iteration 156/1000 | Loss: 0.00001512
Iteration 157/1000 | Loss: 0.00001509
Iteration 158/1000 | Loss: 0.00001509
Iteration 159/1000 | Loss: 0.00001504
Iteration 160/1000 | Loss: 0.00001496
Iteration 161/1000 | Loss: 0.00001493
Iteration 162/1000 | Loss: 0.00001493
Iteration 163/1000 | Loss: 0.00001489
Iteration 164/1000 | Loss: 0.00001487
Iteration 165/1000 | Loss: 0.00001486
Iteration 166/1000 | Loss: 0.00001486
Iteration 167/1000 | Loss: 0.00001485
Iteration 168/1000 | Loss: 0.00001484
Iteration 169/1000 | Loss: 0.00001484
Iteration 170/1000 | Loss: 0.00001482
Iteration 171/1000 | Loss: 0.00001482
Iteration 172/1000 | Loss: 0.00001482
Iteration 173/1000 | Loss: 0.00001481
Iteration 174/1000 | Loss: 0.00001481
Iteration 175/1000 | Loss: 0.00001481
Iteration 176/1000 | Loss: 0.00001480
Iteration 177/1000 | Loss: 0.00001479
Iteration 178/1000 | Loss: 0.00001478
Iteration 179/1000 | Loss: 0.00001476
Iteration 180/1000 | Loss: 0.00001473
Iteration 181/1000 | Loss: 0.00001473
Iteration 182/1000 | Loss: 0.00001473
Iteration 183/1000 | Loss: 0.00001472
Iteration 184/1000 | Loss: 0.00001472
Iteration 185/1000 | Loss: 0.00001472
Iteration 186/1000 | Loss: 0.00001472
Iteration 187/1000 | Loss: 0.00001472
Iteration 188/1000 | Loss: 0.00001472
Iteration 189/1000 | Loss: 0.00001470
Iteration 190/1000 | Loss: 0.00001470
Iteration 191/1000 | Loss: 0.00001470
Iteration 192/1000 | Loss: 0.00001470
Iteration 193/1000 | Loss: 0.00001470
Iteration 194/1000 | Loss: 0.00001470
Iteration 195/1000 | Loss: 0.00001469
Iteration 196/1000 | Loss: 0.00001469
Iteration 197/1000 | Loss: 0.00001469
Iteration 198/1000 | Loss: 0.00001469
Iteration 199/1000 | Loss: 0.00001469
Iteration 200/1000 | Loss: 0.00001469
Iteration 201/1000 | Loss: 0.00001469
Iteration 202/1000 | Loss: 0.00001469
Iteration 203/1000 | Loss: 0.00001468
Iteration 204/1000 | Loss: 0.00001468
Iteration 205/1000 | Loss: 0.00001467
Iteration 206/1000 | Loss: 0.00001466
Iteration 207/1000 | Loss: 0.00001466
Iteration 208/1000 | Loss: 0.00001466
Iteration 209/1000 | Loss: 0.00001466
Iteration 210/1000 | Loss: 0.00001465
Iteration 211/1000 | Loss: 0.00001465
Iteration 212/1000 | Loss: 0.00001465
Iteration 213/1000 | Loss: 0.00001464
Iteration 214/1000 | Loss: 0.00001464
Iteration 215/1000 | Loss: 0.00001464
Iteration 216/1000 | Loss: 0.00001464
Iteration 217/1000 | Loss: 0.00001463
Iteration 218/1000 | Loss: 0.00001463
Iteration 219/1000 | Loss: 0.00001463
Iteration 220/1000 | Loss: 0.00001463
Iteration 221/1000 | Loss: 0.00001463
Iteration 222/1000 | Loss: 0.00001463
Iteration 223/1000 | Loss: 0.00001462
Iteration 224/1000 | Loss: 0.00001462
Iteration 225/1000 | Loss: 0.00001462
Iteration 226/1000 | Loss: 0.00001462
Iteration 227/1000 | Loss: 0.00001461
Iteration 228/1000 | Loss: 0.00001461
Iteration 229/1000 | Loss: 0.00001461
Iteration 230/1000 | Loss: 0.00001461
Iteration 231/1000 | Loss: 0.00001461
Iteration 232/1000 | Loss: 0.00001461
Iteration 233/1000 | Loss: 0.00001461
Iteration 234/1000 | Loss: 0.00001461
Iteration 235/1000 | Loss: 0.00001461
Iteration 236/1000 | Loss: 0.00001461
Iteration 237/1000 | Loss: 0.00001460
Iteration 238/1000 | Loss: 0.00001460
Iteration 239/1000 | Loss: 0.00001460
Iteration 240/1000 | Loss: 0.00001460
Iteration 241/1000 | Loss: 0.00001460
Iteration 242/1000 | Loss: 0.00001459
Iteration 243/1000 | Loss: 0.00001459
Iteration 244/1000 | Loss: 0.00001459
Iteration 245/1000 | Loss: 0.00001459
Iteration 246/1000 | Loss: 0.00001459
Iteration 247/1000 | Loss: 0.00001459
Iteration 248/1000 | Loss: 0.00001458
Iteration 249/1000 | Loss: 0.00001458
Iteration 250/1000 | Loss: 0.00001458
Iteration 251/1000 | Loss: 0.00001458
Iteration 252/1000 | Loss: 0.00001458
Iteration 253/1000 | Loss: 0.00001458
Iteration 254/1000 | Loss: 0.00001458
Iteration 255/1000 | Loss: 0.00001458
Iteration 256/1000 | Loss: 0.00001458
Iteration 257/1000 | Loss: 0.00001458
Iteration 258/1000 | Loss: 0.00001458
Iteration 259/1000 | Loss: 0.00001458
Iteration 260/1000 | Loss: 0.00001458
Iteration 261/1000 | Loss: 0.00001458
Iteration 262/1000 | Loss: 0.00001458
Iteration 263/1000 | Loss: 0.00001458
Iteration 264/1000 | Loss: 0.00001458
Iteration 265/1000 | Loss: 0.00001458
Iteration 266/1000 | Loss: 0.00001458
Iteration 267/1000 | Loss: 0.00001458
Iteration 268/1000 | Loss: 0.00001458
Iteration 269/1000 | Loss: 0.00001458
Iteration 270/1000 | Loss: 0.00001458
Iteration 271/1000 | Loss: 0.00001458
Iteration 272/1000 | Loss: 0.00001458
Iteration 273/1000 | Loss: 0.00001458
Iteration 274/1000 | Loss: 0.00001458
Iteration 275/1000 | Loss: 0.00001458
Iteration 276/1000 | Loss: 0.00001458
Iteration 277/1000 | Loss: 0.00001458
Iteration 278/1000 | Loss: 0.00001458
Iteration 279/1000 | Loss: 0.00001458
Iteration 280/1000 | Loss: 0.00001458
Iteration 281/1000 | Loss: 0.00001458
Iteration 282/1000 | Loss: 0.00001458
Iteration 283/1000 | Loss: 0.00001458
Iteration 284/1000 | Loss: 0.00001458
Iteration 285/1000 | Loss: 0.00001458
Iteration 286/1000 | Loss: 0.00001458
Iteration 287/1000 | Loss: 0.00001458
Iteration 288/1000 | Loss: 0.00001458
Iteration 289/1000 | Loss: 0.00001458
Iteration 290/1000 | Loss: 0.00001458
Iteration 291/1000 | Loss: 0.00001458
Iteration 292/1000 | Loss: 0.00001458
Iteration 293/1000 | Loss: 0.00001458
Iteration 294/1000 | Loss: 0.00001458
Iteration 295/1000 | Loss: 0.00001458
Iteration 296/1000 | Loss: 0.00001458
Iteration 297/1000 | Loss: 0.00001458
Iteration 298/1000 | Loss: 0.00001458
Iteration 299/1000 | Loss: 0.00001458
Iteration 300/1000 | Loss: 0.00001458
Iteration 301/1000 | Loss: 0.00001458
Iteration 302/1000 | Loss: 0.00001458
Iteration 303/1000 | Loss: 0.00001458
Iteration 304/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [1.4581685718439985e-05, 1.4581685718439985e-05, 1.4581685718439985e-05, 1.4581685718439985e-05, 1.4581685718439985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4581685718439985e-05

Optimization complete. Final v2v error: 2.836167573928833 mm

Highest mean error: 11.03747272491455 mm for frame 100

Lowest mean error: 2.415018320083618 mm for frame 93

Saving results

Total time: 269.3330488204956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898624
Iteration 2/25 | Loss: 0.00102676
Iteration 3/25 | Loss: 0.00074572
Iteration 4/25 | Loss: 0.00071092
Iteration 5/25 | Loss: 0.00070080
Iteration 6/25 | Loss: 0.00069882
Iteration 7/25 | Loss: 0.00069823
Iteration 8/25 | Loss: 0.00069821
Iteration 9/25 | Loss: 0.00069821
Iteration 10/25 | Loss: 0.00069821
Iteration 11/25 | Loss: 0.00069821
Iteration 12/25 | Loss: 0.00069821
Iteration 13/25 | Loss: 0.00069821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006982149207033217, 0.0006982149207033217, 0.0006982149207033217, 0.0006982149207033217, 0.0006982149207033217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006982149207033217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54543889
Iteration 2/25 | Loss: 0.00034541
Iteration 3/25 | Loss: 0.00034541
Iteration 4/25 | Loss: 0.00034541
Iteration 5/25 | Loss: 0.00034541
Iteration 6/25 | Loss: 0.00034541
Iteration 7/25 | Loss: 0.00034541
Iteration 8/25 | Loss: 0.00034541
Iteration 9/25 | Loss: 0.00034541
Iteration 10/25 | Loss: 0.00034541
Iteration 11/25 | Loss: 0.00034541
Iteration 12/25 | Loss: 0.00034541
Iteration 13/25 | Loss: 0.00034541
Iteration 14/25 | Loss: 0.00034541
Iteration 15/25 | Loss: 0.00034541
Iteration 16/25 | Loss: 0.00034541
Iteration 17/25 | Loss: 0.00034541
Iteration 18/25 | Loss: 0.00034541
Iteration 19/25 | Loss: 0.00034541
Iteration 20/25 | Loss: 0.00034541
Iteration 21/25 | Loss: 0.00034541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00034541069180704653, 0.00034541069180704653, 0.00034541069180704653, 0.00034541069180704653, 0.00034541069180704653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034541069180704653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034541
Iteration 2/1000 | Loss: 0.00003238
Iteration 3/1000 | Loss: 0.00002362
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002108
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001932
Iteration 11/1000 | Loss: 0.00001911
Iteration 12/1000 | Loss: 0.00001895
Iteration 13/1000 | Loss: 0.00001882
Iteration 14/1000 | Loss: 0.00001873
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001872
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001870
Iteration 21/1000 | Loss: 0.00001870
Iteration 22/1000 | Loss: 0.00001870
Iteration 23/1000 | Loss: 0.00001868
Iteration 24/1000 | Loss: 0.00001866
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001862
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001854
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001850
Iteration 38/1000 | Loss: 0.00001850
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001846
Iteration 45/1000 | Loss: 0.00001846
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001844
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001842
Iteration 73/1000 | Loss: 0.00001842
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001842
Iteration 87/1000 | Loss: 0.00001842
Iteration 88/1000 | Loss: 0.00001842
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00001842
Iteration 91/1000 | Loss: 0.00001842
Iteration 92/1000 | Loss: 0.00001842
Iteration 93/1000 | Loss: 0.00001842
Iteration 94/1000 | Loss: 0.00001842
Iteration 95/1000 | Loss: 0.00001842
Iteration 96/1000 | Loss: 0.00001842
Iteration 97/1000 | Loss: 0.00001842
Iteration 98/1000 | Loss: 0.00001842
Iteration 99/1000 | Loss: 0.00001842
Iteration 100/1000 | Loss: 0.00001842
Iteration 101/1000 | Loss: 0.00001842
Iteration 102/1000 | Loss: 0.00001842
Iteration 103/1000 | Loss: 0.00001842
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.8423088476993144e-05, 1.8423088476993144e-05, 1.8423088476993144e-05, 1.8423088476993144e-05, 1.8423088476993144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8423088476993144e-05

Optimization complete. Final v2v error: 3.5137546062469482 mm

Highest mean error: 4.960780620574951 mm for frame 56

Lowest mean error: 3.0655012130737305 mm for frame 32

Saving results

Total time: 35.75024890899658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926850
Iteration 2/25 | Loss: 0.00140867
Iteration 3/25 | Loss: 0.00087308
Iteration 4/25 | Loss: 0.00080389
Iteration 5/25 | Loss: 0.00080087
Iteration 6/25 | Loss: 0.00080043
Iteration 7/25 | Loss: 0.00080043
Iteration 8/25 | Loss: 0.00080043
Iteration 9/25 | Loss: 0.00080043
Iteration 10/25 | Loss: 0.00080043
Iteration 11/25 | Loss: 0.00080043
Iteration 12/25 | Loss: 0.00080043
Iteration 13/25 | Loss: 0.00080043
Iteration 14/25 | Loss: 0.00080043
Iteration 15/25 | Loss: 0.00080043
Iteration 16/25 | Loss: 0.00080043
Iteration 17/25 | Loss: 0.00080043
Iteration 18/25 | Loss: 0.00080043
Iteration 19/25 | Loss: 0.00080043
Iteration 20/25 | Loss: 0.00080043
Iteration 21/25 | Loss: 0.00080043
Iteration 22/25 | Loss: 0.00080043
Iteration 23/25 | Loss: 0.00080043
Iteration 24/25 | Loss: 0.00080043
Iteration 25/25 | Loss: 0.00080043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48247623
Iteration 2/25 | Loss: 0.00039876
Iteration 3/25 | Loss: 0.00039870
Iteration 4/25 | Loss: 0.00039870
Iteration 5/25 | Loss: 0.00039870
Iteration 6/25 | Loss: 0.00039870
Iteration 7/25 | Loss: 0.00039870
Iteration 8/25 | Loss: 0.00039870
Iteration 9/25 | Loss: 0.00039870
Iteration 10/25 | Loss: 0.00039870
Iteration 11/25 | Loss: 0.00039870
Iteration 12/25 | Loss: 0.00039870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00039869555621407926, 0.00039869555621407926, 0.00039869555621407926, 0.00039869555621407926, 0.00039869555621407926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039869555621407926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039870
Iteration 2/1000 | Loss: 0.00003959
Iteration 3/1000 | Loss: 0.00002984
Iteration 4/1000 | Loss: 0.00002536
Iteration 5/1000 | Loss: 0.00002360
Iteration 6/1000 | Loss: 0.00002266
Iteration 7/1000 | Loss: 0.00002218
Iteration 8/1000 | Loss: 0.00002174
Iteration 9/1000 | Loss: 0.00002133
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002040
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002019
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002012
Iteration 18/1000 | Loss: 0.00002012
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002009
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002008
Iteration 23/1000 | Loss: 0.00002008
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002008
Iteration 27/1000 | Loss: 0.00002008
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00002008
Iteration 30/1000 | Loss: 0.00002007
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001997
Iteration 37/1000 | Loss: 0.00001996
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001994
Iteration 43/1000 | Loss: 0.00001994
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001990
Iteration 51/1000 | Loss: 0.00001990
Iteration 52/1000 | Loss: 0.00001990
Iteration 53/1000 | Loss: 0.00001990
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001990
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001989
Iteration 66/1000 | Loss: 0.00001989
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001986
Iteration 76/1000 | Loss: 0.00001986
Iteration 77/1000 | Loss: 0.00001986
Iteration 78/1000 | Loss: 0.00001986
Iteration 79/1000 | Loss: 0.00001986
Iteration 80/1000 | Loss: 0.00001986
Iteration 81/1000 | Loss: 0.00001986
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001986
Iteration 85/1000 | Loss: 0.00001986
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001985
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001984
Iteration 109/1000 | Loss: 0.00001984
Iteration 110/1000 | Loss: 0.00001984
Iteration 111/1000 | Loss: 0.00001984
Iteration 112/1000 | Loss: 0.00001984
Iteration 113/1000 | Loss: 0.00001984
Iteration 114/1000 | Loss: 0.00001984
Iteration 115/1000 | Loss: 0.00001984
Iteration 116/1000 | Loss: 0.00001984
Iteration 117/1000 | Loss: 0.00001984
Iteration 118/1000 | Loss: 0.00001984
Iteration 119/1000 | Loss: 0.00001984
Iteration 120/1000 | Loss: 0.00001984
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001984
Iteration 129/1000 | Loss: 0.00001984
Iteration 130/1000 | Loss: 0.00001984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.9835313651128672e-05, 1.9835313651128672e-05, 1.9835313651128672e-05, 1.9835313651128672e-05, 1.9835313651128672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9835313651128672e-05

Optimization complete. Final v2v error: 3.7291200160980225 mm

Highest mean error: 3.89557147026062 mm for frame 118

Lowest mean error: 3.5557281970977783 mm for frame 71

Saving results

Total time: 36.47363257408142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427102
Iteration 2/25 | Loss: 0.00098402
Iteration 3/25 | Loss: 0.00073603
Iteration 4/25 | Loss: 0.00070080
Iteration 5/25 | Loss: 0.00069224
Iteration 6/25 | Loss: 0.00069100
Iteration 7/25 | Loss: 0.00069075
Iteration 8/25 | Loss: 0.00069075
Iteration 9/25 | Loss: 0.00069075
Iteration 10/25 | Loss: 0.00069075
Iteration 11/25 | Loss: 0.00069075
Iteration 12/25 | Loss: 0.00069075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000690753513481468, 0.000690753513481468, 0.000690753513481468, 0.000690753513481468, 0.000690753513481468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000690753513481468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25653446
Iteration 2/25 | Loss: 0.00031799
Iteration 3/25 | Loss: 0.00031799
Iteration 4/25 | Loss: 0.00031799
Iteration 5/25 | Loss: 0.00031799
Iteration 6/25 | Loss: 0.00031799
Iteration 7/25 | Loss: 0.00031798
Iteration 8/25 | Loss: 0.00031798
Iteration 9/25 | Loss: 0.00031798
Iteration 10/25 | Loss: 0.00031798
Iteration 11/25 | Loss: 0.00031798
Iteration 12/25 | Loss: 0.00031798
Iteration 13/25 | Loss: 0.00031798
Iteration 14/25 | Loss: 0.00031798
Iteration 15/25 | Loss: 0.00031798
Iteration 16/25 | Loss: 0.00031798
Iteration 17/25 | Loss: 0.00031798
Iteration 18/25 | Loss: 0.00031798
Iteration 19/25 | Loss: 0.00031798
Iteration 20/25 | Loss: 0.00031798
Iteration 21/25 | Loss: 0.00031798
Iteration 22/25 | Loss: 0.00031798
Iteration 23/25 | Loss: 0.00031798
Iteration 24/25 | Loss: 0.00031798
Iteration 25/25 | Loss: 0.00031798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00031798399868421257, 0.00031798399868421257, 0.00031798399868421257, 0.00031798399868421257, 0.00031798399868421257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031798399868421257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031798
Iteration 2/1000 | Loss: 0.00002616
Iteration 3/1000 | Loss: 0.00001874
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001576
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001456
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001433
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001394
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001393
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001392
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001390
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.3859884347766638e-05, 1.3859884347766638e-05, 1.3859884347766638e-05, 1.3859884347766638e-05, 1.3859884347766638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3859884347766638e-05

Optimization complete. Final v2v error: 3.1836116313934326 mm

Highest mean error: 3.218289852142334 mm for frame 110

Lowest mean error: 3.161189079284668 mm for frame 8

Saving results

Total time: 28.642903089523315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409664
Iteration 2/25 | Loss: 0.00079573
Iteration 3/25 | Loss: 0.00069063
Iteration 4/25 | Loss: 0.00067312
Iteration 5/25 | Loss: 0.00066594
Iteration 6/25 | Loss: 0.00066480
Iteration 7/25 | Loss: 0.00066480
Iteration 8/25 | Loss: 0.00066480
Iteration 9/25 | Loss: 0.00066480
Iteration 10/25 | Loss: 0.00066480
Iteration 11/25 | Loss: 0.00066480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006647996488027275, 0.0006647996488027275, 0.0006647996488027275, 0.0006647996488027275, 0.0006647996488027275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006647996488027275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48653924
Iteration 2/25 | Loss: 0.00028683
Iteration 3/25 | Loss: 0.00028683
Iteration 4/25 | Loss: 0.00028683
Iteration 5/25 | Loss: 0.00028683
Iteration 6/25 | Loss: 0.00028683
Iteration 7/25 | Loss: 0.00028683
Iteration 8/25 | Loss: 0.00028683
Iteration 9/25 | Loss: 0.00028683
Iteration 10/25 | Loss: 0.00028683
Iteration 11/25 | Loss: 0.00028683
Iteration 12/25 | Loss: 0.00028683
Iteration 13/25 | Loss: 0.00028683
Iteration 14/25 | Loss: 0.00028683
Iteration 15/25 | Loss: 0.00028683
Iteration 16/25 | Loss: 0.00028683
Iteration 17/25 | Loss: 0.00028683
Iteration 18/25 | Loss: 0.00028683
Iteration 19/25 | Loss: 0.00028683
Iteration 20/25 | Loss: 0.00028683
Iteration 21/25 | Loss: 0.00028683
Iteration 22/25 | Loss: 0.00028683
Iteration 23/25 | Loss: 0.00028683
Iteration 24/25 | Loss: 0.00028683
Iteration 25/25 | Loss: 0.00028683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028683
Iteration 2/1000 | Loss: 0.00002114
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001278
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001204
Iteration 9/1000 | Loss: 0.00001194
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001140
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001139
Iteration 35/1000 | Loss: 0.00001139
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001138
Iteration 38/1000 | Loss: 0.00001138
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001125
Iteration 63/1000 | Loss: 0.00001125
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001124
Iteration 66/1000 | Loss: 0.00001124
Iteration 67/1000 | Loss: 0.00001123
Iteration 68/1000 | Loss: 0.00001123
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001116
Iteration 116/1000 | Loss: 0.00001116
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001115
Iteration 134/1000 | Loss: 0.00001115
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001114
Iteration 138/1000 | Loss: 0.00001114
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001114
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001114
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001114
Iteration 145/1000 | Loss: 0.00001114
Iteration 146/1000 | Loss: 0.00001114
Iteration 147/1000 | Loss: 0.00001114
Iteration 148/1000 | Loss: 0.00001114
Iteration 149/1000 | Loss: 0.00001114
Iteration 150/1000 | Loss: 0.00001114
Iteration 151/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1141317372675985e-05, 1.1141317372675985e-05, 1.1141317372675985e-05, 1.1141317372675985e-05, 1.1141317372675985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1141317372675985e-05

Optimization complete. Final v2v error: 2.869035482406616 mm

Highest mean error: 3.017655849456787 mm for frame 179

Lowest mean error: 2.785731315612793 mm for frame 17

Saving results

Total time: 38.374181032180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825838
Iteration 2/25 | Loss: 0.00101630
Iteration 3/25 | Loss: 0.00088500
Iteration 4/25 | Loss: 0.00084872
Iteration 5/25 | Loss: 0.00084076
Iteration 6/25 | Loss: 0.00083734
Iteration 7/25 | Loss: 0.00083694
Iteration 8/25 | Loss: 0.00083694
Iteration 9/25 | Loss: 0.00083694
Iteration 10/25 | Loss: 0.00083694
Iteration 11/25 | Loss: 0.00083694
Iteration 12/25 | Loss: 0.00083694
Iteration 13/25 | Loss: 0.00083694
Iteration 14/25 | Loss: 0.00083694
Iteration 15/25 | Loss: 0.00083694
Iteration 16/25 | Loss: 0.00083694
Iteration 17/25 | Loss: 0.00083694
Iteration 18/25 | Loss: 0.00083694
Iteration 19/25 | Loss: 0.00083694
Iteration 20/25 | Loss: 0.00083694
Iteration 21/25 | Loss: 0.00083694
Iteration 22/25 | Loss: 0.00083694
Iteration 23/25 | Loss: 0.00083694
Iteration 24/25 | Loss: 0.00083694
Iteration 25/25 | Loss: 0.00083694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10811305
Iteration 2/25 | Loss: 0.00033553
Iteration 3/25 | Loss: 0.00033553
Iteration 4/25 | Loss: 0.00033553
Iteration 5/25 | Loss: 0.00033553
Iteration 6/25 | Loss: 0.00033553
Iteration 7/25 | Loss: 0.00033553
Iteration 8/25 | Loss: 0.00033553
Iteration 9/25 | Loss: 0.00033553
Iteration 10/25 | Loss: 0.00033553
Iteration 11/25 | Loss: 0.00033553
Iteration 12/25 | Loss: 0.00033553
Iteration 13/25 | Loss: 0.00033553
Iteration 14/25 | Loss: 0.00033553
Iteration 15/25 | Loss: 0.00033553
Iteration 16/25 | Loss: 0.00033553
Iteration 17/25 | Loss: 0.00033553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003355267981532961, 0.0003355267981532961, 0.0003355267981532961, 0.0003355267981532961, 0.0003355267981532961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003355267981532961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033553
Iteration 2/1000 | Loss: 0.00003801
Iteration 3/1000 | Loss: 0.00003262
Iteration 4/1000 | Loss: 0.00003120
Iteration 5/1000 | Loss: 0.00003046
Iteration 6/1000 | Loss: 0.00003000
Iteration 7/1000 | Loss: 0.00002971
Iteration 8/1000 | Loss: 0.00002943
Iteration 9/1000 | Loss: 0.00002922
Iteration 10/1000 | Loss: 0.00002911
Iteration 11/1000 | Loss: 0.00002901
Iteration 12/1000 | Loss: 0.00002894
Iteration 13/1000 | Loss: 0.00002894
Iteration 14/1000 | Loss: 0.00002893
Iteration 15/1000 | Loss: 0.00002890
Iteration 16/1000 | Loss: 0.00002887
Iteration 17/1000 | Loss: 0.00002886
Iteration 18/1000 | Loss: 0.00002886
Iteration 19/1000 | Loss: 0.00002886
Iteration 20/1000 | Loss: 0.00002886
Iteration 21/1000 | Loss: 0.00002885
Iteration 22/1000 | Loss: 0.00002885
Iteration 23/1000 | Loss: 0.00002884
Iteration 24/1000 | Loss: 0.00002884
Iteration 25/1000 | Loss: 0.00002883
Iteration 26/1000 | Loss: 0.00002883
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00002883
Iteration 29/1000 | Loss: 0.00002882
Iteration 30/1000 | Loss: 0.00002882
Iteration 31/1000 | Loss: 0.00002882
Iteration 32/1000 | Loss: 0.00002882
Iteration 33/1000 | Loss: 0.00002882
Iteration 34/1000 | Loss: 0.00002882
Iteration 35/1000 | Loss: 0.00002882
Iteration 36/1000 | Loss: 0.00002881
Iteration 37/1000 | Loss: 0.00002881
Iteration 38/1000 | Loss: 0.00002881
Iteration 39/1000 | Loss: 0.00002880
Iteration 40/1000 | Loss: 0.00002880
Iteration 41/1000 | Loss: 0.00002879
Iteration 42/1000 | Loss: 0.00002879
Iteration 43/1000 | Loss: 0.00002879
Iteration 44/1000 | Loss: 0.00002878
Iteration 45/1000 | Loss: 0.00002878
Iteration 46/1000 | Loss: 0.00002878
Iteration 47/1000 | Loss: 0.00002878
Iteration 48/1000 | Loss: 0.00002878
Iteration 49/1000 | Loss: 0.00002878
Iteration 50/1000 | Loss: 0.00002877
Iteration 51/1000 | Loss: 0.00002877
Iteration 52/1000 | Loss: 0.00002877
Iteration 53/1000 | Loss: 0.00002874
Iteration 54/1000 | Loss: 0.00002873
Iteration 55/1000 | Loss: 0.00002873
Iteration 56/1000 | Loss: 0.00002873
Iteration 57/1000 | Loss: 0.00002873
Iteration 58/1000 | Loss: 0.00002873
Iteration 59/1000 | Loss: 0.00002872
Iteration 60/1000 | Loss: 0.00002872
Iteration 61/1000 | Loss: 0.00002871
Iteration 62/1000 | Loss: 0.00002871
Iteration 63/1000 | Loss: 0.00002870
Iteration 64/1000 | Loss: 0.00002870
Iteration 65/1000 | Loss: 0.00002870
Iteration 66/1000 | Loss: 0.00002870
Iteration 67/1000 | Loss: 0.00002870
Iteration 68/1000 | Loss: 0.00002870
Iteration 69/1000 | Loss: 0.00002870
Iteration 70/1000 | Loss: 0.00002870
Iteration 71/1000 | Loss: 0.00002870
Iteration 72/1000 | Loss: 0.00002870
Iteration 73/1000 | Loss: 0.00002870
Iteration 74/1000 | Loss: 0.00002870
Iteration 75/1000 | Loss: 0.00002870
Iteration 76/1000 | Loss: 0.00002869
Iteration 77/1000 | Loss: 0.00002869
Iteration 78/1000 | Loss: 0.00002869
Iteration 79/1000 | Loss: 0.00002868
Iteration 80/1000 | Loss: 0.00002868
Iteration 81/1000 | Loss: 0.00002868
Iteration 82/1000 | Loss: 0.00002867
Iteration 83/1000 | Loss: 0.00002867
Iteration 84/1000 | Loss: 0.00002867
Iteration 85/1000 | Loss: 0.00002867
Iteration 86/1000 | Loss: 0.00002866
Iteration 87/1000 | Loss: 0.00002866
Iteration 88/1000 | Loss: 0.00002866
Iteration 89/1000 | Loss: 0.00002866
Iteration 90/1000 | Loss: 0.00002866
Iteration 91/1000 | Loss: 0.00002866
Iteration 92/1000 | Loss: 0.00002866
Iteration 93/1000 | Loss: 0.00002865
Iteration 94/1000 | Loss: 0.00002865
Iteration 95/1000 | Loss: 0.00002865
Iteration 96/1000 | Loss: 0.00002865
Iteration 97/1000 | Loss: 0.00002865
Iteration 98/1000 | Loss: 0.00002865
Iteration 99/1000 | Loss: 0.00002865
Iteration 100/1000 | Loss: 0.00002864
Iteration 101/1000 | Loss: 0.00002864
Iteration 102/1000 | Loss: 0.00002864
Iteration 103/1000 | Loss: 0.00002864
Iteration 104/1000 | Loss: 0.00002864
Iteration 105/1000 | Loss: 0.00002864
Iteration 106/1000 | Loss: 0.00002864
Iteration 107/1000 | Loss: 0.00002864
Iteration 108/1000 | Loss: 0.00002863
Iteration 109/1000 | Loss: 0.00002863
Iteration 110/1000 | Loss: 0.00002863
Iteration 111/1000 | Loss: 0.00002863
Iteration 112/1000 | Loss: 0.00002863
Iteration 113/1000 | Loss: 0.00002863
Iteration 114/1000 | Loss: 0.00002863
Iteration 115/1000 | Loss: 0.00002863
Iteration 116/1000 | Loss: 0.00002863
Iteration 117/1000 | Loss: 0.00002863
Iteration 118/1000 | Loss: 0.00002863
Iteration 119/1000 | Loss: 0.00002863
Iteration 120/1000 | Loss: 0.00002863
Iteration 121/1000 | Loss: 0.00002863
Iteration 122/1000 | Loss: 0.00002863
Iteration 123/1000 | Loss: 0.00002863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.8632968678721227e-05, 2.8632968678721227e-05, 2.8632968678721227e-05, 2.8632968678721227e-05, 2.8632968678721227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8632968678721227e-05

Optimization complete. Final v2v error: 4.391859531402588 mm

Highest mean error: 4.719557762145996 mm for frame 144

Lowest mean error: 4.058408260345459 mm for frame 121

Saving results

Total time: 40.0748291015625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497641
Iteration 2/25 | Loss: 0.00092610
Iteration 3/25 | Loss: 0.00074726
Iteration 4/25 | Loss: 0.00070344
Iteration 5/25 | Loss: 0.00069291
Iteration 6/25 | Loss: 0.00069129
Iteration 7/25 | Loss: 0.00069129
Iteration 8/25 | Loss: 0.00069129
Iteration 9/25 | Loss: 0.00069129
Iteration 10/25 | Loss: 0.00069129
Iteration 11/25 | Loss: 0.00069129
Iteration 12/25 | Loss: 0.00069129
Iteration 13/25 | Loss: 0.00069129
Iteration 14/25 | Loss: 0.00069129
Iteration 15/25 | Loss: 0.00069129
Iteration 16/25 | Loss: 0.00069129
Iteration 17/25 | Loss: 0.00069129
Iteration 18/25 | Loss: 0.00069129
Iteration 19/25 | Loss: 0.00069129
Iteration 20/25 | Loss: 0.00069129
Iteration 21/25 | Loss: 0.00069129
Iteration 22/25 | Loss: 0.00069129
Iteration 23/25 | Loss: 0.00069129
Iteration 24/25 | Loss: 0.00069129
Iteration 25/25 | Loss: 0.00069129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82792944
Iteration 2/25 | Loss: 0.00027425
Iteration 3/25 | Loss: 0.00027425
Iteration 4/25 | Loss: 0.00027425
Iteration 5/25 | Loss: 0.00027425
Iteration 6/25 | Loss: 0.00027425
Iteration 7/25 | Loss: 0.00027425
Iteration 8/25 | Loss: 0.00027425
Iteration 9/25 | Loss: 0.00027425
Iteration 10/25 | Loss: 0.00027425
Iteration 11/25 | Loss: 0.00027425
Iteration 12/25 | Loss: 0.00027425
Iteration 13/25 | Loss: 0.00027425
Iteration 14/25 | Loss: 0.00027425
Iteration 15/25 | Loss: 0.00027425
Iteration 16/25 | Loss: 0.00027425
Iteration 17/25 | Loss: 0.00027425
Iteration 18/25 | Loss: 0.00027425
Iteration 19/25 | Loss: 0.00027425
Iteration 20/25 | Loss: 0.00027425
Iteration 21/25 | Loss: 0.00027425
Iteration 22/25 | Loss: 0.00027425
Iteration 23/25 | Loss: 0.00027425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002742450451478362, 0.0002742450451478362, 0.0002742450451478362, 0.0002742450451478362, 0.0002742450451478362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002742450451478362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027425
Iteration 2/1000 | Loss: 0.00003162
Iteration 3/1000 | Loss: 0.00002295
Iteration 4/1000 | Loss: 0.00002165
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001947
Iteration 8/1000 | Loss: 0.00001902
Iteration 9/1000 | Loss: 0.00001875
Iteration 10/1000 | Loss: 0.00001851
Iteration 11/1000 | Loss: 0.00001832
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001777
Iteration 17/1000 | Loss: 0.00001777
Iteration 18/1000 | Loss: 0.00001767
Iteration 19/1000 | Loss: 0.00001765
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001757
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001756
Iteration 28/1000 | Loss: 0.00001756
Iteration 29/1000 | Loss: 0.00001756
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001754
Iteration 35/1000 | Loss: 0.00001754
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001754
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001753
Iteration 44/1000 | Loss: 0.00001751
Iteration 45/1000 | Loss: 0.00001751
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001750
Iteration 48/1000 | Loss: 0.00001750
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001749
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001748
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001746
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001741
Iteration 77/1000 | Loss: 0.00001741
Iteration 78/1000 | Loss: 0.00001741
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001740
Iteration 82/1000 | Loss: 0.00001740
Iteration 83/1000 | Loss: 0.00001739
Iteration 84/1000 | Loss: 0.00001739
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001739
Iteration 87/1000 | Loss: 0.00001739
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001739
Iteration 90/1000 | Loss: 0.00001738
Iteration 91/1000 | Loss: 0.00001737
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001737
Iteration 94/1000 | Loss: 0.00001737
Iteration 95/1000 | Loss: 0.00001737
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001735
Iteration 99/1000 | Loss: 0.00001735
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001734
Iteration 103/1000 | Loss: 0.00001734
Iteration 104/1000 | Loss: 0.00001734
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001733
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001733
Iteration 111/1000 | Loss: 0.00001733
Iteration 112/1000 | Loss: 0.00001733
Iteration 113/1000 | Loss: 0.00001733
Iteration 114/1000 | Loss: 0.00001733
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001731
Iteration 130/1000 | Loss: 0.00001730
Iteration 131/1000 | Loss: 0.00001730
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001729
Iteration 137/1000 | Loss: 0.00001729
Iteration 138/1000 | Loss: 0.00001729
Iteration 139/1000 | Loss: 0.00001729
Iteration 140/1000 | Loss: 0.00001729
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001728
Iteration 144/1000 | Loss: 0.00001728
Iteration 145/1000 | Loss: 0.00001728
Iteration 146/1000 | Loss: 0.00001728
Iteration 147/1000 | Loss: 0.00001728
Iteration 148/1000 | Loss: 0.00001728
Iteration 149/1000 | Loss: 0.00001728
Iteration 150/1000 | Loss: 0.00001728
Iteration 151/1000 | Loss: 0.00001728
Iteration 152/1000 | Loss: 0.00001728
Iteration 153/1000 | Loss: 0.00001728
Iteration 154/1000 | Loss: 0.00001728
Iteration 155/1000 | Loss: 0.00001728
Iteration 156/1000 | Loss: 0.00001728
Iteration 157/1000 | Loss: 0.00001728
Iteration 158/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.727633753034752e-05, 1.727633753034752e-05, 1.727633753034752e-05, 1.727633753034752e-05, 1.727633753034752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.727633753034752e-05

Optimization complete. Final v2v error: 3.5022132396698 mm

Highest mean error: 4.414142608642578 mm for frame 2

Lowest mean error: 3.237175226211548 mm for frame 35

Saving results

Total time: 50.29022216796875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997424
Iteration 2/25 | Loss: 0.00278213
Iteration 3/25 | Loss: 0.00195051
Iteration 4/25 | Loss: 0.00164953
Iteration 5/25 | Loss: 0.00144765
Iteration 6/25 | Loss: 0.00132177
Iteration 7/25 | Loss: 0.00119012
Iteration 8/25 | Loss: 0.00106436
Iteration 9/25 | Loss: 0.00105430
Iteration 10/25 | Loss: 0.00098518
Iteration 11/25 | Loss: 0.00096577
Iteration 12/25 | Loss: 0.00095771
Iteration 13/25 | Loss: 0.00096060
Iteration 14/25 | Loss: 0.00095871
Iteration 15/25 | Loss: 0.00095049
Iteration 16/25 | Loss: 0.00094706
Iteration 17/25 | Loss: 0.00094581
Iteration 18/25 | Loss: 0.00095238
Iteration 19/25 | Loss: 0.00094523
Iteration 20/25 | Loss: 0.00094507
Iteration 21/25 | Loss: 0.00094488
Iteration 22/25 | Loss: 0.00094475
Iteration 23/25 | Loss: 0.00094465
Iteration 24/25 | Loss: 0.00094463
Iteration 25/25 | Loss: 0.00094463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48567271
Iteration 2/25 | Loss: 0.00112391
Iteration 3/25 | Loss: 0.00075518
Iteration 4/25 | Loss: 0.00075518
Iteration 5/25 | Loss: 0.00075518
Iteration 6/25 | Loss: 0.00075518
Iteration 7/25 | Loss: 0.00075518
Iteration 8/25 | Loss: 0.00075518
Iteration 9/25 | Loss: 0.00075518
Iteration 10/25 | Loss: 0.00075518
Iteration 11/25 | Loss: 0.00075518
Iteration 12/25 | Loss: 0.00075518
Iteration 13/25 | Loss: 0.00075518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007551810122095048, 0.0007551810122095048, 0.0007551810122095048, 0.0007551810122095048, 0.0007551810122095048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007551810122095048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075518
Iteration 2/1000 | Loss: 0.00023015
Iteration 3/1000 | Loss: 0.00088808
Iteration 4/1000 | Loss: 0.00024485
Iteration 5/1000 | Loss: 0.00004894
Iteration 6/1000 | Loss: 0.00004444
Iteration 7/1000 | Loss: 0.00005943
Iteration 8/1000 | Loss: 0.00004047
Iteration 9/1000 | Loss: 0.00003480
Iteration 10/1000 | Loss: 0.00003390
Iteration 11/1000 | Loss: 0.00003302
Iteration 12/1000 | Loss: 0.00005145
Iteration 13/1000 | Loss: 0.00003237
Iteration 14/1000 | Loss: 0.00003204
Iteration 15/1000 | Loss: 0.00005839
Iteration 16/1000 | Loss: 0.00016866
Iteration 17/1000 | Loss: 0.00004956
Iteration 18/1000 | Loss: 0.00003495
Iteration 19/1000 | Loss: 0.00003620
Iteration 20/1000 | Loss: 0.00010169
Iteration 21/1000 | Loss: 0.00005475
Iteration 22/1000 | Loss: 0.00004198
Iteration 23/1000 | Loss: 0.00033542
Iteration 24/1000 | Loss: 0.00007961
Iteration 25/1000 | Loss: 0.00003278
Iteration 26/1000 | Loss: 0.00003181
Iteration 27/1000 | Loss: 0.00003161
Iteration 28/1000 | Loss: 0.00018685
Iteration 29/1000 | Loss: 0.00003640
Iteration 30/1000 | Loss: 0.00006692
Iteration 31/1000 | Loss: 0.00003156
Iteration 32/1000 | Loss: 0.00003656
Iteration 33/1000 | Loss: 0.00003656
Iteration 34/1000 | Loss: 0.00003136
Iteration 35/1000 | Loss: 0.00003134
Iteration 36/1000 | Loss: 0.00003134
Iteration 37/1000 | Loss: 0.00003133
Iteration 38/1000 | Loss: 0.00003133
Iteration 39/1000 | Loss: 0.00003133
Iteration 40/1000 | Loss: 0.00003132
Iteration 41/1000 | Loss: 0.00003132
Iteration 42/1000 | Loss: 0.00003130
Iteration 43/1000 | Loss: 0.00003130
Iteration 44/1000 | Loss: 0.00003130
Iteration 45/1000 | Loss: 0.00003129
Iteration 46/1000 | Loss: 0.00003129
Iteration 47/1000 | Loss: 0.00003129
Iteration 48/1000 | Loss: 0.00003129
Iteration 49/1000 | Loss: 0.00003129
Iteration 50/1000 | Loss: 0.00003129
Iteration 51/1000 | Loss: 0.00003129
Iteration 52/1000 | Loss: 0.00003129
Iteration 53/1000 | Loss: 0.00003129
Iteration 54/1000 | Loss: 0.00003129
Iteration 55/1000 | Loss: 0.00003129
Iteration 56/1000 | Loss: 0.00003129
Iteration 57/1000 | Loss: 0.00003129
Iteration 58/1000 | Loss: 0.00003128
Iteration 59/1000 | Loss: 0.00003128
Iteration 60/1000 | Loss: 0.00003128
Iteration 61/1000 | Loss: 0.00003128
Iteration 62/1000 | Loss: 0.00003128
Iteration 63/1000 | Loss: 0.00003128
Iteration 64/1000 | Loss: 0.00003128
Iteration 65/1000 | Loss: 0.00003128
Iteration 66/1000 | Loss: 0.00003128
Iteration 67/1000 | Loss: 0.00003127
Iteration 68/1000 | Loss: 0.00003127
Iteration 69/1000 | Loss: 0.00015121
Iteration 70/1000 | Loss: 0.00007651
Iteration 71/1000 | Loss: 0.00004251
Iteration 72/1000 | Loss: 0.00004052
Iteration 73/1000 | Loss: 0.00019745
Iteration 74/1000 | Loss: 0.00003154
Iteration 75/1000 | Loss: 0.00003131
Iteration 76/1000 | Loss: 0.00003130
Iteration 77/1000 | Loss: 0.00003125
Iteration 78/1000 | Loss: 0.00003124
Iteration 79/1000 | Loss: 0.00003124
Iteration 80/1000 | Loss: 0.00003124
Iteration 81/1000 | Loss: 0.00003124
Iteration 82/1000 | Loss: 0.00003123
Iteration 83/1000 | Loss: 0.00003122
Iteration 84/1000 | Loss: 0.00003122
Iteration 85/1000 | Loss: 0.00003122
Iteration 86/1000 | Loss: 0.00003122
Iteration 87/1000 | Loss: 0.00003122
Iteration 88/1000 | Loss: 0.00003121
Iteration 89/1000 | Loss: 0.00003121
Iteration 90/1000 | Loss: 0.00003121
Iteration 91/1000 | Loss: 0.00003121
Iteration 92/1000 | Loss: 0.00003121
Iteration 93/1000 | Loss: 0.00003121
Iteration 94/1000 | Loss: 0.00003121
Iteration 95/1000 | Loss: 0.00003121
Iteration 96/1000 | Loss: 0.00003121
Iteration 97/1000 | Loss: 0.00003121
Iteration 98/1000 | Loss: 0.00003121
Iteration 99/1000 | Loss: 0.00003121
Iteration 100/1000 | Loss: 0.00003120
Iteration 101/1000 | Loss: 0.00003120
Iteration 102/1000 | Loss: 0.00003120
Iteration 103/1000 | Loss: 0.00003120
Iteration 104/1000 | Loss: 0.00017752
Iteration 105/1000 | Loss: 0.00003545
Iteration 106/1000 | Loss: 0.00003293
Iteration 107/1000 | Loss: 0.00004478
Iteration 108/1000 | Loss: 0.00017149
Iteration 109/1000 | Loss: 0.00003152
Iteration 110/1000 | Loss: 0.00003129
Iteration 111/1000 | Loss: 0.00003125
Iteration 112/1000 | Loss: 0.00011777
Iteration 113/1000 | Loss: 0.00012168
Iteration 114/1000 | Loss: 0.00008442
Iteration 115/1000 | Loss: 0.00005537
Iteration 116/1000 | Loss: 0.00003137
Iteration 117/1000 | Loss: 0.00003124
Iteration 118/1000 | Loss: 0.00008110
Iteration 119/1000 | Loss: 0.00004192
Iteration 120/1000 | Loss: 0.00007821
Iteration 121/1000 | Loss: 0.00004261
Iteration 122/1000 | Loss: 0.00003138
Iteration 123/1000 | Loss: 0.00003422
Iteration 124/1000 | Loss: 0.00003124
Iteration 125/1000 | Loss: 0.00003123
Iteration 126/1000 | Loss: 0.00003123
Iteration 127/1000 | Loss: 0.00003123
Iteration 128/1000 | Loss: 0.00003123
Iteration 129/1000 | Loss: 0.00003123
Iteration 130/1000 | Loss: 0.00003123
Iteration 131/1000 | Loss: 0.00003123
Iteration 132/1000 | Loss: 0.00003123
Iteration 133/1000 | Loss: 0.00003123
Iteration 134/1000 | Loss: 0.00003123
Iteration 135/1000 | Loss: 0.00003123
Iteration 136/1000 | Loss: 0.00003123
Iteration 137/1000 | Loss: 0.00003123
Iteration 138/1000 | Loss: 0.00003122
Iteration 139/1000 | Loss: 0.00003122
Iteration 140/1000 | Loss: 0.00003122
Iteration 141/1000 | Loss: 0.00003122
Iteration 142/1000 | Loss: 0.00003122
Iteration 143/1000 | Loss: 0.00003122
Iteration 144/1000 | Loss: 0.00003121
Iteration 145/1000 | Loss: 0.00003121
Iteration 146/1000 | Loss: 0.00003121
Iteration 147/1000 | Loss: 0.00003121
Iteration 148/1000 | Loss: 0.00003121
Iteration 149/1000 | Loss: 0.00003121
Iteration 150/1000 | Loss: 0.00003121
Iteration 151/1000 | Loss: 0.00003347
Iteration 152/1000 | Loss: 0.00003126
Iteration 153/1000 | Loss: 0.00003126
Iteration 154/1000 | Loss: 0.00003126
Iteration 155/1000 | Loss: 0.00003133
Iteration 156/1000 | Loss: 0.00003123
Iteration 157/1000 | Loss: 0.00003122
Iteration 158/1000 | Loss: 0.00003124
Iteration 159/1000 | Loss: 0.00003124
Iteration 160/1000 | Loss: 0.00003120
Iteration 161/1000 | Loss: 0.00003120
Iteration 162/1000 | Loss: 0.00003120
Iteration 163/1000 | Loss: 0.00003120
Iteration 164/1000 | Loss: 0.00003120
Iteration 165/1000 | Loss: 0.00003120
Iteration 166/1000 | Loss: 0.00003120
Iteration 167/1000 | Loss: 0.00003120
Iteration 168/1000 | Loss: 0.00003120
Iteration 169/1000 | Loss: 0.00003120
Iteration 170/1000 | Loss: 0.00003120
Iteration 171/1000 | Loss: 0.00003120
Iteration 172/1000 | Loss: 0.00003120
Iteration 173/1000 | Loss: 0.00003120
Iteration 174/1000 | Loss: 0.00003120
Iteration 175/1000 | Loss: 0.00003120
Iteration 176/1000 | Loss: 0.00003120
Iteration 177/1000 | Loss: 0.00003120
Iteration 178/1000 | Loss: 0.00003120
Iteration 179/1000 | Loss: 0.00003120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.119675238849595e-05, 3.119675238849595e-05, 3.119675238849595e-05, 3.119675238849595e-05, 3.119675238849595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.119675238849595e-05

Optimization complete. Final v2v error: 4.532074451446533 mm

Highest mean error: 12.386017799377441 mm for frame 232

Lowest mean error: 3.527996778488159 mm for frame 154

Saving results

Total time: 149.96487498283386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00635210
Iteration 2/25 | Loss: 0.00111686
Iteration 3/25 | Loss: 0.00082555
Iteration 4/25 | Loss: 0.00079253
Iteration 5/25 | Loss: 0.00078326
Iteration 6/25 | Loss: 0.00078212
Iteration 7/25 | Loss: 0.00078212
Iteration 8/25 | Loss: 0.00078212
Iteration 9/25 | Loss: 0.00078212
Iteration 10/25 | Loss: 0.00078212
Iteration 11/25 | Loss: 0.00078212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007821247563697398, 0.0007821247563697398, 0.0007821247563697398, 0.0007821247563697398, 0.0007821247563697398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007821247563697398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.70206451
Iteration 2/25 | Loss: 0.00034046
Iteration 3/25 | Loss: 0.00034028
Iteration 4/25 | Loss: 0.00034028
Iteration 5/25 | Loss: 0.00034028
Iteration 6/25 | Loss: 0.00034028
Iteration 7/25 | Loss: 0.00034028
Iteration 8/25 | Loss: 0.00034028
Iteration 9/25 | Loss: 0.00034028
Iteration 10/25 | Loss: 0.00034028
Iteration 11/25 | Loss: 0.00034028
Iteration 12/25 | Loss: 0.00034028
Iteration 13/25 | Loss: 0.00034028
Iteration 14/25 | Loss: 0.00034028
Iteration 15/25 | Loss: 0.00034028
Iteration 16/25 | Loss: 0.00034028
Iteration 17/25 | Loss: 0.00034028
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00034027567016892135, 0.00034027567016892135, 0.00034027567016892135, 0.00034027567016892135, 0.00034027567016892135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034027567016892135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034028
Iteration 2/1000 | Loss: 0.00002739
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001620
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001618
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001610
Iteration 19/1000 | Loss: 0.00001609
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001607
Iteration 23/1000 | Loss: 0.00001605
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001598
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001593
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001592
Iteration 36/1000 | Loss: 0.00001592
Iteration 37/1000 | Loss: 0.00001592
Iteration 38/1000 | Loss: 0.00001591
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001587
Iteration 77/1000 | Loss: 0.00001587
Iteration 78/1000 | Loss: 0.00001587
Iteration 79/1000 | Loss: 0.00001587
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001587
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001585
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001585
Iteration 124/1000 | Loss: 0.00001585
Iteration 125/1000 | Loss: 0.00001585
Iteration 126/1000 | Loss: 0.00001585
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Iteration 129/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.5851743228267878e-05, 1.5851743228267878e-05, 1.5851743228267878e-05, 1.5851743228267878e-05, 1.5851743228267878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5851743228267878e-05

Optimization complete. Final v2v error: 3.301234245300293 mm

Highest mean error: 3.736203193664551 mm for frame 169

Lowest mean error: 2.8547778129577637 mm for frame 209

Saving results

Total time: 35.92683744430542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815374
Iteration 2/25 | Loss: 0.00098803
Iteration 3/25 | Loss: 0.00084530
Iteration 4/25 | Loss: 0.00080489
Iteration 5/25 | Loss: 0.00079605
Iteration 6/25 | Loss: 0.00079481
Iteration 7/25 | Loss: 0.00079480
Iteration 8/25 | Loss: 0.00079480
Iteration 9/25 | Loss: 0.00079480
Iteration 10/25 | Loss: 0.00079480
Iteration 11/25 | Loss: 0.00079480
Iteration 12/25 | Loss: 0.00079480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007947983685880899, 0.0007947983685880899, 0.0007947983685880899, 0.0007947983685880899, 0.0007947983685880899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007947983685880899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62679434
Iteration 2/25 | Loss: 0.00033787
Iteration 3/25 | Loss: 0.00033787
Iteration 4/25 | Loss: 0.00033787
Iteration 5/25 | Loss: 0.00033787
Iteration 6/25 | Loss: 0.00033787
Iteration 7/25 | Loss: 0.00033787
Iteration 8/25 | Loss: 0.00033787
Iteration 9/25 | Loss: 0.00033787
Iteration 10/25 | Loss: 0.00033787
Iteration 11/25 | Loss: 0.00033787
Iteration 12/25 | Loss: 0.00033787
Iteration 13/25 | Loss: 0.00033787
Iteration 14/25 | Loss: 0.00033787
Iteration 15/25 | Loss: 0.00033787
Iteration 16/25 | Loss: 0.00033787
Iteration 17/25 | Loss: 0.00033787
Iteration 18/25 | Loss: 0.00033787
Iteration 19/25 | Loss: 0.00033787
Iteration 20/25 | Loss: 0.00033787
Iteration 21/25 | Loss: 0.00033787
Iteration 22/25 | Loss: 0.00033787
Iteration 23/25 | Loss: 0.00033787
Iteration 24/25 | Loss: 0.00033787
Iteration 25/25 | Loss: 0.00033787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033787
Iteration 2/1000 | Loss: 0.00003995
Iteration 3/1000 | Loss: 0.00003008
Iteration 4/1000 | Loss: 0.00002818
Iteration 5/1000 | Loss: 0.00002690
Iteration 6/1000 | Loss: 0.00002616
Iteration 7/1000 | Loss: 0.00002564
Iteration 8/1000 | Loss: 0.00002519
Iteration 9/1000 | Loss: 0.00002495
Iteration 10/1000 | Loss: 0.00002474
Iteration 11/1000 | Loss: 0.00002470
Iteration 12/1000 | Loss: 0.00002469
Iteration 13/1000 | Loss: 0.00002469
Iteration 14/1000 | Loss: 0.00002463
Iteration 15/1000 | Loss: 0.00002459
Iteration 16/1000 | Loss: 0.00002458
Iteration 17/1000 | Loss: 0.00002451
Iteration 18/1000 | Loss: 0.00002449
Iteration 19/1000 | Loss: 0.00002449
Iteration 20/1000 | Loss: 0.00002447
Iteration 21/1000 | Loss: 0.00002445
Iteration 22/1000 | Loss: 0.00002445
Iteration 23/1000 | Loss: 0.00002445
Iteration 24/1000 | Loss: 0.00002445
Iteration 25/1000 | Loss: 0.00002444
Iteration 26/1000 | Loss: 0.00002444
Iteration 27/1000 | Loss: 0.00002444
Iteration 28/1000 | Loss: 0.00002444
Iteration 29/1000 | Loss: 0.00002443
Iteration 30/1000 | Loss: 0.00002443
Iteration 31/1000 | Loss: 0.00002443
Iteration 32/1000 | Loss: 0.00002443
Iteration 33/1000 | Loss: 0.00002443
Iteration 34/1000 | Loss: 0.00002442
Iteration 35/1000 | Loss: 0.00002442
Iteration 36/1000 | Loss: 0.00002442
Iteration 37/1000 | Loss: 0.00002441
Iteration 38/1000 | Loss: 0.00002441
Iteration 39/1000 | Loss: 0.00002441
Iteration 40/1000 | Loss: 0.00002441
Iteration 41/1000 | Loss: 0.00002441
Iteration 42/1000 | Loss: 0.00002440
Iteration 43/1000 | Loss: 0.00002440
Iteration 44/1000 | Loss: 0.00002440
Iteration 45/1000 | Loss: 0.00002440
Iteration 46/1000 | Loss: 0.00002440
Iteration 47/1000 | Loss: 0.00002440
Iteration 48/1000 | Loss: 0.00002440
Iteration 49/1000 | Loss: 0.00002439
Iteration 50/1000 | Loss: 0.00002439
Iteration 51/1000 | Loss: 0.00002439
Iteration 52/1000 | Loss: 0.00002439
Iteration 53/1000 | Loss: 0.00002439
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002439
Iteration 56/1000 | Loss: 0.00002439
Iteration 57/1000 | Loss: 0.00002439
Iteration 58/1000 | Loss: 0.00002439
Iteration 59/1000 | Loss: 0.00002439
Iteration 60/1000 | Loss: 0.00002439
Iteration 61/1000 | Loss: 0.00002439
Iteration 62/1000 | Loss: 0.00002439
Iteration 63/1000 | Loss: 0.00002439
Iteration 64/1000 | Loss: 0.00002439
Iteration 65/1000 | Loss: 0.00002439
Iteration 66/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.43938611674821e-05, 2.43938611674821e-05, 2.43938611674821e-05, 2.43938611674821e-05, 2.43938611674821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.43938611674821e-05

Optimization complete. Final v2v error: 4.181741714477539 mm

Highest mean error: 4.677244186401367 mm for frame 60

Lowest mean error: 3.6975841522216797 mm for frame 172

Saving results

Total time: 30.931464195251465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074551
Iteration 2/25 | Loss: 0.00148512
Iteration 3/25 | Loss: 0.00089582
Iteration 4/25 | Loss: 0.00084571
Iteration 5/25 | Loss: 0.00083077
Iteration 6/25 | Loss: 0.00082703
Iteration 7/25 | Loss: 0.00082599
Iteration 8/25 | Loss: 0.00082588
Iteration 9/25 | Loss: 0.00082588
Iteration 10/25 | Loss: 0.00082588
Iteration 11/25 | Loss: 0.00082588
Iteration 12/25 | Loss: 0.00082588
Iteration 13/25 | Loss: 0.00082588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008258775342255831, 0.0008258775342255831, 0.0008258775342255831, 0.0008258775342255831, 0.0008258775342255831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008258775342255831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39851367
Iteration 2/25 | Loss: 0.00038279
Iteration 3/25 | Loss: 0.00038279
Iteration 4/25 | Loss: 0.00038279
Iteration 5/25 | Loss: 0.00038279
Iteration 6/25 | Loss: 0.00038279
Iteration 7/25 | Loss: 0.00038279
Iteration 8/25 | Loss: 0.00038279
Iteration 9/25 | Loss: 0.00038279
Iteration 10/25 | Loss: 0.00038279
Iteration 11/25 | Loss: 0.00038279
Iteration 12/25 | Loss: 0.00038279
Iteration 13/25 | Loss: 0.00038279
Iteration 14/25 | Loss: 0.00038279
Iteration 15/25 | Loss: 0.00038279
Iteration 16/25 | Loss: 0.00038279
Iteration 17/25 | Loss: 0.00038279
Iteration 18/25 | Loss: 0.00038279
Iteration 19/25 | Loss: 0.00038279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003827902255579829, 0.0003827902255579829, 0.0003827902255579829, 0.0003827902255579829, 0.0003827902255579829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003827902255579829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038279
Iteration 2/1000 | Loss: 0.00004500
Iteration 3/1000 | Loss: 0.00003166
Iteration 4/1000 | Loss: 0.00002893
Iteration 5/1000 | Loss: 0.00002727
Iteration 6/1000 | Loss: 0.00002586
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00002390
Iteration 9/1000 | Loss: 0.00002338
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002263
Iteration 13/1000 | Loss: 0.00002247
Iteration 14/1000 | Loss: 0.00002236
Iteration 15/1000 | Loss: 0.00002236
Iteration 16/1000 | Loss: 0.00002234
Iteration 17/1000 | Loss: 0.00002233
Iteration 18/1000 | Loss: 0.00002233
Iteration 19/1000 | Loss: 0.00002229
Iteration 20/1000 | Loss: 0.00002229
Iteration 21/1000 | Loss: 0.00002228
Iteration 22/1000 | Loss: 0.00002227
Iteration 23/1000 | Loss: 0.00002225
Iteration 24/1000 | Loss: 0.00002225
Iteration 25/1000 | Loss: 0.00002225
Iteration 26/1000 | Loss: 0.00002224
Iteration 27/1000 | Loss: 0.00002224
Iteration 28/1000 | Loss: 0.00002224
Iteration 29/1000 | Loss: 0.00002224
Iteration 30/1000 | Loss: 0.00002223
Iteration 31/1000 | Loss: 0.00002223
Iteration 32/1000 | Loss: 0.00002223
Iteration 33/1000 | Loss: 0.00002222
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002221
Iteration 39/1000 | Loss: 0.00002221
Iteration 40/1000 | Loss: 0.00002221
Iteration 41/1000 | Loss: 0.00002221
Iteration 42/1000 | Loss: 0.00002221
Iteration 43/1000 | Loss: 0.00002221
Iteration 44/1000 | Loss: 0.00002221
Iteration 45/1000 | Loss: 0.00002221
Iteration 46/1000 | Loss: 0.00002221
Iteration 47/1000 | Loss: 0.00002221
Iteration 48/1000 | Loss: 0.00002221
Iteration 49/1000 | Loss: 0.00002220
Iteration 50/1000 | Loss: 0.00002220
Iteration 51/1000 | Loss: 0.00002220
Iteration 52/1000 | Loss: 0.00002220
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002219
Iteration 55/1000 | Loss: 0.00002219
Iteration 56/1000 | Loss: 0.00002218
Iteration 57/1000 | Loss: 0.00002218
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002218
Iteration 60/1000 | Loss: 0.00002217
Iteration 61/1000 | Loss: 0.00002217
Iteration 62/1000 | Loss: 0.00002217
Iteration 63/1000 | Loss: 0.00002217
Iteration 64/1000 | Loss: 0.00002217
Iteration 65/1000 | Loss: 0.00002217
Iteration 66/1000 | Loss: 0.00002217
Iteration 67/1000 | Loss: 0.00002216
Iteration 68/1000 | Loss: 0.00002216
Iteration 69/1000 | Loss: 0.00002216
Iteration 70/1000 | Loss: 0.00002215
Iteration 71/1000 | Loss: 0.00002215
Iteration 72/1000 | Loss: 0.00002215
Iteration 73/1000 | Loss: 0.00002214
Iteration 74/1000 | Loss: 0.00002214
Iteration 75/1000 | Loss: 0.00002214
Iteration 76/1000 | Loss: 0.00002214
Iteration 77/1000 | Loss: 0.00002214
Iteration 78/1000 | Loss: 0.00002214
Iteration 79/1000 | Loss: 0.00002214
Iteration 80/1000 | Loss: 0.00002214
Iteration 81/1000 | Loss: 0.00002213
Iteration 82/1000 | Loss: 0.00002213
Iteration 83/1000 | Loss: 0.00002213
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002213
Iteration 86/1000 | Loss: 0.00002213
Iteration 87/1000 | Loss: 0.00002213
Iteration 88/1000 | Loss: 0.00002212
Iteration 89/1000 | Loss: 0.00002212
Iteration 90/1000 | Loss: 0.00002212
Iteration 91/1000 | Loss: 0.00002212
Iteration 92/1000 | Loss: 0.00002212
Iteration 93/1000 | Loss: 0.00002212
Iteration 94/1000 | Loss: 0.00002212
Iteration 95/1000 | Loss: 0.00002211
Iteration 96/1000 | Loss: 0.00002211
Iteration 97/1000 | Loss: 0.00002211
Iteration 98/1000 | Loss: 0.00002211
Iteration 99/1000 | Loss: 0.00002211
Iteration 100/1000 | Loss: 0.00002211
Iteration 101/1000 | Loss: 0.00002210
Iteration 102/1000 | Loss: 0.00002210
Iteration 103/1000 | Loss: 0.00002210
Iteration 104/1000 | Loss: 0.00002210
Iteration 105/1000 | Loss: 0.00002210
Iteration 106/1000 | Loss: 0.00002210
Iteration 107/1000 | Loss: 0.00002210
Iteration 108/1000 | Loss: 0.00002210
Iteration 109/1000 | Loss: 0.00002210
Iteration 110/1000 | Loss: 0.00002210
Iteration 111/1000 | Loss: 0.00002210
Iteration 112/1000 | Loss: 0.00002210
Iteration 113/1000 | Loss: 0.00002210
Iteration 114/1000 | Loss: 0.00002210
Iteration 115/1000 | Loss: 0.00002210
Iteration 116/1000 | Loss: 0.00002210
Iteration 117/1000 | Loss: 0.00002210
Iteration 118/1000 | Loss: 0.00002210
Iteration 119/1000 | Loss: 0.00002210
Iteration 120/1000 | Loss: 0.00002210
Iteration 121/1000 | Loss: 0.00002210
Iteration 122/1000 | Loss: 0.00002210
Iteration 123/1000 | Loss: 0.00002210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.210138518421445e-05, 2.210138518421445e-05, 2.210138518421445e-05, 2.210138518421445e-05, 2.210138518421445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.210138518421445e-05

Optimization complete. Final v2v error: 3.8076798915863037 mm

Highest mean error: 4.841171741485596 mm for frame 27

Lowest mean error: 2.926124095916748 mm for frame 0

Saving results

Total time: 40.724626779556274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558142
Iteration 2/25 | Loss: 0.00110824
Iteration 3/25 | Loss: 0.00081033
Iteration 4/25 | Loss: 0.00072075
Iteration 5/25 | Loss: 0.00070393
Iteration 6/25 | Loss: 0.00070553
Iteration 7/25 | Loss: 0.00070762
Iteration 8/25 | Loss: 0.00069766
Iteration 9/25 | Loss: 0.00070018
Iteration 10/25 | Loss: 0.00069624
Iteration 11/25 | Loss: 0.00069244
Iteration 12/25 | Loss: 0.00069097
Iteration 13/25 | Loss: 0.00069040
Iteration 14/25 | Loss: 0.00069004
Iteration 15/25 | Loss: 0.00068992
Iteration 16/25 | Loss: 0.00068991
Iteration 17/25 | Loss: 0.00068991
Iteration 18/25 | Loss: 0.00068991
Iteration 19/25 | Loss: 0.00068990
Iteration 20/25 | Loss: 0.00068990
Iteration 21/25 | Loss: 0.00068990
Iteration 22/25 | Loss: 0.00068990
Iteration 23/25 | Loss: 0.00068990
Iteration 24/25 | Loss: 0.00068990
Iteration 25/25 | Loss: 0.00068990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74735904
Iteration 2/25 | Loss: 0.00031020
Iteration 3/25 | Loss: 0.00031020
Iteration 4/25 | Loss: 0.00031020
Iteration 5/25 | Loss: 0.00031020
Iteration 6/25 | Loss: 0.00031020
Iteration 7/25 | Loss: 0.00031020
Iteration 8/25 | Loss: 0.00031020
Iteration 9/25 | Loss: 0.00031020
Iteration 10/25 | Loss: 0.00031020
Iteration 11/25 | Loss: 0.00031020
Iteration 12/25 | Loss: 0.00031020
Iteration 13/25 | Loss: 0.00031020
Iteration 14/25 | Loss: 0.00031020
Iteration 15/25 | Loss: 0.00031020
Iteration 16/25 | Loss: 0.00031020
Iteration 17/25 | Loss: 0.00031020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031019552261568606, 0.00031019552261568606, 0.00031019552261568606, 0.00031019552261568606, 0.00031019552261568606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031019552261568606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031020
Iteration 2/1000 | Loss: 0.00002866
Iteration 3/1000 | Loss: 0.00002245
Iteration 4/1000 | Loss: 0.00002125
Iteration 5/1000 | Loss: 0.00002008
Iteration 6/1000 | Loss: 0.00001953
Iteration 7/1000 | Loss: 0.00001875
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001794
Iteration 11/1000 | Loss: 0.00001782
Iteration 12/1000 | Loss: 0.00001773
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001761
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001759
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001758
Iteration 26/1000 | Loss: 0.00001757
Iteration 27/1000 | Loss: 0.00001757
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001753
Iteration 34/1000 | Loss: 0.00001753
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00001748
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001732
Iteration 54/1000 | Loss: 0.00001732
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001731
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.729692485241685e-05, 1.729692485241685e-05, 1.729692485241685e-05, 1.729692485241685e-05, 1.729692485241685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.729692485241685e-05

Optimization complete. Final v2v error: 3.5116043090820312 mm

Highest mean error: 4.224607944488525 mm for frame 186

Lowest mean error: 3.2143189907073975 mm for frame 213

Saving results

Total time: 57.48887085914612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032110
Iteration 2/25 | Loss: 0.00098514
Iteration 3/25 | Loss: 0.00074938
Iteration 4/25 | Loss: 0.00071529
Iteration 5/25 | Loss: 0.00070797
Iteration 6/25 | Loss: 0.00070686
Iteration 7/25 | Loss: 0.00070686
Iteration 8/25 | Loss: 0.00070686
Iteration 9/25 | Loss: 0.00070686
Iteration 10/25 | Loss: 0.00070686
Iteration 11/25 | Loss: 0.00070686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007068589329719543, 0.0007068589329719543, 0.0007068589329719543, 0.0007068589329719543, 0.0007068589329719543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007068589329719543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46330142
Iteration 2/25 | Loss: 0.00035307
Iteration 3/25 | Loss: 0.00035307
Iteration 4/25 | Loss: 0.00035307
Iteration 5/25 | Loss: 0.00035307
Iteration 6/25 | Loss: 0.00035307
Iteration 7/25 | Loss: 0.00035307
Iteration 8/25 | Loss: 0.00035307
Iteration 9/25 | Loss: 0.00035307
Iteration 10/25 | Loss: 0.00035307
Iteration 11/25 | Loss: 0.00035307
Iteration 12/25 | Loss: 0.00035307
Iteration 13/25 | Loss: 0.00035307
Iteration 14/25 | Loss: 0.00035307
Iteration 15/25 | Loss: 0.00035307
Iteration 16/25 | Loss: 0.00035307
Iteration 17/25 | Loss: 0.00035307
Iteration 18/25 | Loss: 0.00035307
Iteration 19/25 | Loss: 0.00035307
Iteration 20/25 | Loss: 0.00035307
Iteration 21/25 | Loss: 0.00035307
Iteration 22/25 | Loss: 0.00035307
Iteration 23/25 | Loss: 0.00035307
Iteration 24/25 | Loss: 0.00035307
Iteration 25/25 | Loss: 0.00035307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035307
Iteration 2/1000 | Loss: 0.00001889
Iteration 3/1000 | Loss: 0.00001446
Iteration 4/1000 | Loss: 0.00001369
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001249
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001237
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001229
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001228
Iteration 77/1000 | Loss: 0.00001228
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001226
Iteration 93/1000 | Loss: 0.00001226
Iteration 94/1000 | Loss: 0.00001226
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001225
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001224
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001220
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.2203186997794546e-05, 1.2203186997794546e-05, 1.2203186997794546e-05, 1.2203186997794546e-05, 1.2203186997794546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2203186997794546e-05

Optimization complete. Final v2v error: 2.976861000061035 mm

Highest mean error: 3.2375829219818115 mm for frame 181

Lowest mean error: 2.748764991760254 mm for frame 133

Saving results

Total time: 29.72439217567444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386679
Iteration 2/25 | Loss: 0.00082372
Iteration 3/25 | Loss: 0.00071407
Iteration 4/25 | Loss: 0.00068281
Iteration 5/25 | Loss: 0.00067915
Iteration 6/25 | Loss: 0.00067849
Iteration 7/25 | Loss: 0.00067849
Iteration 8/25 | Loss: 0.00067849
Iteration 9/25 | Loss: 0.00067849
Iteration 10/25 | Loss: 0.00067849
Iteration 11/25 | Loss: 0.00067849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006784879951737821, 0.0006784879951737821, 0.0006784879951737821, 0.0006784879951737821, 0.0006784879951737821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006784879951737821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59659469
Iteration 2/25 | Loss: 0.00030674
Iteration 3/25 | Loss: 0.00030674
Iteration 4/25 | Loss: 0.00030674
Iteration 5/25 | Loss: 0.00030674
Iteration 6/25 | Loss: 0.00030674
Iteration 7/25 | Loss: 0.00030674
Iteration 8/25 | Loss: 0.00030674
Iteration 9/25 | Loss: 0.00030674
Iteration 10/25 | Loss: 0.00030674
Iteration 11/25 | Loss: 0.00030674
Iteration 12/25 | Loss: 0.00030674
Iteration 13/25 | Loss: 0.00030674
Iteration 14/25 | Loss: 0.00030674
Iteration 15/25 | Loss: 0.00030674
Iteration 16/25 | Loss: 0.00030674
Iteration 17/25 | Loss: 0.00030674
Iteration 18/25 | Loss: 0.00030674
Iteration 19/25 | Loss: 0.00030674
Iteration 20/25 | Loss: 0.00030674
Iteration 21/25 | Loss: 0.00030674
Iteration 22/25 | Loss: 0.00030674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00030673679430037737, 0.00030673679430037737, 0.00030673679430037737, 0.00030673679430037737, 0.00030673679430037737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030673679430037737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030674
Iteration 2/1000 | Loss: 0.00002488
Iteration 3/1000 | Loss: 0.00001940
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001657
Iteration 7/1000 | Loss: 0.00001621
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001586
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001572
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001566
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001558
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001556
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001556
Iteration 44/1000 | Loss: 0.00001556
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001555
Iteration 47/1000 | Loss: 0.00001555
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001551
Iteration 64/1000 | Loss: 0.00001551
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001538
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001536
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001534
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001533
Iteration 120/1000 | Loss: 0.00001533
Iteration 121/1000 | Loss: 0.00001533
Iteration 122/1000 | Loss: 0.00001533
Iteration 123/1000 | Loss: 0.00001533
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001532
Iteration 129/1000 | Loss: 0.00001532
Iteration 130/1000 | Loss: 0.00001532
Iteration 131/1000 | Loss: 0.00001532
Iteration 132/1000 | Loss: 0.00001532
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001532
Iteration 135/1000 | Loss: 0.00001532
Iteration 136/1000 | Loss: 0.00001532
Iteration 137/1000 | Loss: 0.00001532
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001531
Iteration 140/1000 | Loss: 0.00001531
Iteration 141/1000 | Loss: 0.00001531
Iteration 142/1000 | Loss: 0.00001531
Iteration 143/1000 | Loss: 0.00001531
Iteration 144/1000 | Loss: 0.00001531
Iteration 145/1000 | Loss: 0.00001531
Iteration 146/1000 | Loss: 0.00001530
Iteration 147/1000 | Loss: 0.00001530
Iteration 148/1000 | Loss: 0.00001530
Iteration 149/1000 | Loss: 0.00001530
Iteration 150/1000 | Loss: 0.00001530
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001530
Iteration 154/1000 | Loss: 0.00001530
Iteration 155/1000 | Loss: 0.00001530
Iteration 156/1000 | Loss: 0.00001530
Iteration 157/1000 | Loss: 0.00001530
Iteration 158/1000 | Loss: 0.00001530
Iteration 159/1000 | Loss: 0.00001530
Iteration 160/1000 | Loss: 0.00001530
Iteration 161/1000 | Loss: 0.00001530
Iteration 162/1000 | Loss: 0.00001529
Iteration 163/1000 | Loss: 0.00001529
Iteration 164/1000 | Loss: 0.00001529
Iteration 165/1000 | Loss: 0.00001529
Iteration 166/1000 | Loss: 0.00001529
Iteration 167/1000 | Loss: 0.00001528
Iteration 168/1000 | Loss: 0.00001528
Iteration 169/1000 | Loss: 0.00001528
Iteration 170/1000 | Loss: 0.00001528
Iteration 171/1000 | Loss: 0.00001528
Iteration 172/1000 | Loss: 0.00001528
Iteration 173/1000 | Loss: 0.00001528
Iteration 174/1000 | Loss: 0.00001528
Iteration 175/1000 | Loss: 0.00001528
Iteration 176/1000 | Loss: 0.00001528
Iteration 177/1000 | Loss: 0.00001527
Iteration 178/1000 | Loss: 0.00001527
Iteration 179/1000 | Loss: 0.00001527
Iteration 180/1000 | Loss: 0.00001527
Iteration 181/1000 | Loss: 0.00001527
Iteration 182/1000 | Loss: 0.00001527
Iteration 183/1000 | Loss: 0.00001527
Iteration 184/1000 | Loss: 0.00001527
Iteration 185/1000 | Loss: 0.00001527
Iteration 186/1000 | Loss: 0.00001527
Iteration 187/1000 | Loss: 0.00001527
Iteration 188/1000 | Loss: 0.00001527
Iteration 189/1000 | Loss: 0.00001527
Iteration 190/1000 | Loss: 0.00001527
Iteration 191/1000 | Loss: 0.00001527
Iteration 192/1000 | Loss: 0.00001527
Iteration 193/1000 | Loss: 0.00001527
Iteration 194/1000 | Loss: 0.00001527
Iteration 195/1000 | Loss: 0.00001527
Iteration 196/1000 | Loss: 0.00001527
Iteration 197/1000 | Loss: 0.00001527
Iteration 198/1000 | Loss: 0.00001527
Iteration 199/1000 | Loss: 0.00001527
Iteration 200/1000 | Loss: 0.00001527
Iteration 201/1000 | Loss: 0.00001527
Iteration 202/1000 | Loss: 0.00001527
Iteration 203/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.526952655694913e-05, 1.526952655694913e-05, 1.526952655694913e-05, 1.526952655694913e-05, 1.526952655694913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.526952655694913e-05

Optimization complete. Final v2v error: 3.2874176502227783 mm

Highest mean error: 3.526848077774048 mm for frame 169

Lowest mean error: 3.101680040359497 mm for frame 210

Saving results

Total time: 40.26758694648743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372226
Iteration 2/25 | Loss: 0.00079436
Iteration 3/25 | Loss: 0.00066060
Iteration 4/25 | Loss: 0.00063739
Iteration 5/25 | Loss: 0.00063223
Iteration 6/25 | Loss: 0.00063063
Iteration 7/25 | Loss: 0.00063019
Iteration 8/25 | Loss: 0.00063019
Iteration 9/25 | Loss: 0.00063019
Iteration 10/25 | Loss: 0.00063019
Iteration 11/25 | Loss: 0.00063019
Iteration 12/25 | Loss: 0.00063019
Iteration 13/25 | Loss: 0.00063019
Iteration 14/25 | Loss: 0.00063019
Iteration 15/25 | Loss: 0.00063019
Iteration 16/25 | Loss: 0.00063019
Iteration 17/25 | Loss: 0.00063019
Iteration 18/25 | Loss: 0.00063019
Iteration 19/25 | Loss: 0.00063019
Iteration 20/25 | Loss: 0.00063019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006301889661699533, 0.0006301889661699533, 0.0006301889661699533, 0.0006301889661699533, 0.0006301889661699533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006301889661699533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72608066
Iteration 2/25 | Loss: 0.00027972
Iteration 3/25 | Loss: 0.00027971
Iteration 4/25 | Loss: 0.00027971
Iteration 5/25 | Loss: 0.00027971
Iteration 6/25 | Loss: 0.00027970
Iteration 7/25 | Loss: 0.00027970
Iteration 8/25 | Loss: 0.00027970
Iteration 9/25 | Loss: 0.00027970
Iteration 10/25 | Loss: 0.00027970
Iteration 11/25 | Loss: 0.00027970
Iteration 12/25 | Loss: 0.00027970
Iteration 13/25 | Loss: 0.00027970
Iteration 14/25 | Loss: 0.00027970
Iteration 15/25 | Loss: 0.00027970
Iteration 16/25 | Loss: 0.00027970
Iteration 17/25 | Loss: 0.00027970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002797038177959621, 0.0002797038177959621, 0.0002797038177959621, 0.0002797038177959621, 0.0002797038177959621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002797038177959621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027970
Iteration 2/1000 | Loss: 0.00002031
Iteration 3/1000 | Loss: 0.00001131
Iteration 4/1000 | Loss: 0.00001016
Iteration 5/1000 | Loss: 0.00000964
Iteration 6/1000 | Loss: 0.00000930
Iteration 7/1000 | Loss: 0.00000902
Iteration 8/1000 | Loss: 0.00000898
Iteration 9/1000 | Loss: 0.00000897
Iteration 10/1000 | Loss: 0.00000894
Iteration 11/1000 | Loss: 0.00000893
Iteration 12/1000 | Loss: 0.00000893
Iteration 13/1000 | Loss: 0.00000888
Iteration 14/1000 | Loss: 0.00000888
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000884
Iteration 17/1000 | Loss: 0.00000884
Iteration 18/1000 | Loss: 0.00000884
Iteration 19/1000 | Loss: 0.00000883
Iteration 20/1000 | Loss: 0.00000883
Iteration 21/1000 | Loss: 0.00000883
Iteration 22/1000 | Loss: 0.00000883
Iteration 23/1000 | Loss: 0.00000883
Iteration 24/1000 | Loss: 0.00000883
Iteration 25/1000 | Loss: 0.00000883
Iteration 26/1000 | Loss: 0.00000883
Iteration 27/1000 | Loss: 0.00000883
Iteration 28/1000 | Loss: 0.00000883
Iteration 29/1000 | Loss: 0.00000883
Iteration 30/1000 | Loss: 0.00000883
Iteration 31/1000 | Loss: 0.00000882
Iteration 32/1000 | Loss: 0.00000882
Iteration 33/1000 | Loss: 0.00000882
Iteration 34/1000 | Loss: 0.00000882
Iteration 35/1000 | Loss: 0.00000882
Iteration 36/1000 | Loss: 0.00000881
Iteration 37/1000 | Loss: 0.00000880
Iteration 38/1000 | Loss: 0.00000880
Iteration 39/1000 | Loss: 0.00000880
Iteration 40/1000 | Loss: 0.00000879
Iteration 41/1000 | Loss: 0.00000879
Iteration 42/1000 | Loss: 0.00000879
Iteration 43/1000 | Loss: 0.00000878
Iteration 44/1000 | Loss: 0.00000878
Iteration 45/1000 | Loss: 0.00000877
Iteration 46/1000 | Loss: 0.00000877
Iteration 47/1000 | Loss: 0.00000877
Iteration 48/1000 | Loss: 0.00000877
Iteration 49/1000 | Loss: 0.00000875
Iteration 50/1000 | Loss: 0.00000875
Iteration 51/1000 | Loss: 0.00000875
Iteration 52/1000 | Loss: 0.00000874
Iteration 53/1000 | Loss: 0.00000874
Iteration 54/1000 | Loss: 0.00000873
Iteration 55/1000 | Loss: 0.00000873
Iteration 56/1000 | Loss: 0.00000872
Iteration 57/1000 | Loss: 0.00000871
Iteration 58/1000 | Loss: 0.00000871
Iteration 59/1000 | Loss: 0.00000871
Iteration 60/1000 | Loss: 0.00000870
Iteration 61/1000 | Loss: 0.00000870
Iteration 62/1000 | Loss: 0.00000869
Iteration 63/1000 | Loss: 0.00000869
Iteration 64/1000 | Loss: 0.00000869
Iteration 65/1000 | Loss: 0.00000869
Iteration 66/1000 | Loss: 0.00000868
Iteration 67/1000 | Loss: 0.00000868
Iteration 68/1000 | Loss: 0.00000868
Iteration 69/1000 | Loss: 0.00000868
Iteration 70/1000 | Loss: 0.00000867
Iteration 71/1000 | Loss: 0.00000867
Iteration 72/1000 | Loss: 0.00000867
Iteration 73/1000 | Loss: 0.00000867
Iteration 74/1000 | Loss: 0.00000867
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000867
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000866
Iteration 80/1000 | Loss: 0.00000866
Iteration 81/1000 | Loss: 0.00000866
Iteration 82/1000 | Loss: 0.00000866
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000863
Iteration 92/1000 | Loss: 0.00000863
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000862
Iteration 95/1000 | Loss: 0.00000861
Iteration 96/1000 | Loss: 0.00000861
Iteration 97/1000 | Loss: 0.00000861
Iteration 98/1000 | Loss: 0.00000861
Iteration 99/1000 | Loss: 0.00000860
Iteration 100/1000 | Loss: 0.00000860
Iteration 101/1000 | Loss: 0.00000860
Iteration 102/1000 | Loss: 0.00000860
Iteration 103/1000 | Loss: 0.00000860
Iteration 104/1000 | Loss: 0.00000860
Iteration 105/1000 | Loss: 0.00000860
Iteration 106/1000 | Loss: 0.00000860
Iteration 107/1000 | Loss: 0.00000859
Iteration 108/1000 | Loss: 0.00000859
Iteration 109/1000 | Loss: 0.00000859
Iteration 110/1000 | Loss: 0.00000858
Iteration 111/1000 | Loss: 0.00000858
Iteration 112/1000 | Loss: 0.00000857
Iteration 113/1000 | Loss: 0.00000857
Iteration 114/1000 | Loss: 0.00000856
Iteration 115/1000 | Loss: 0.00000856
Iteration 116/1000 | Loss: 0.00000856
Iteration 117/1000 | Loss: 0.00000856
Iteration 118/1000 | Loss: 0.00000856
Iteration 119/1000 | Loss: 0.00000856
Iteration 120/1000 | Loss: 0.00000856
Iteration 121/1000 | Loss: 0.00000855
Iteration 122/1000 | Loss: 0.00000855
Iteration 123/1000 | Loss: 0.00000855
Iteration 124/1000 | Loss: 0.00000855
Iteration 125/1000 | Loss: 0.00000854
Iteration 126/1000 | Loss: 0.00000854
Iteration 127/1000 | Loss: 0.00000853
Iteration 128/1000 | Loss: 0.00000853
Iteration 129/1000 | Loss: 0.00000853
Iteration 130/1000 | Loss: 0.00000853
Iteration 131/1000 | Loss: 0.00000853
Iteration 132/1000 | Loss: 0.00000853
Iteration 133/1000 | Loss: 0.00000853
Iteration 134/1000 | Loss: 0.00000853
Iteration 135/1000 | Loss: 0.00000852
Iteration 136/1000 | Loss: 0.00000852
Iteration 137/1000 | Loss: 0.00000852
Iteration 138/1000 | Loss: 0.00000852
Iteration 139/1000 | Loss: 0.00000852
Iteration 140/1000 | Loss: 0.00000852
Iteration 141/1000 | Loss: 0.00000851
Iteration 142/1000 | Loss: 0.00000851
Iteration 143/1000 | Loss: 0.00000851
Iteration 144/1000 | Loss: 0.00000851
Iteration 145/1000 | Loss: 0.00000851
Iteration 146/1000 | Loss: 0.00000851
Iteration 147/1000 | Loss: 0.00000850
Iteration 148/1000 | Loss: 0.00000850
Iteration 149/1000 | Loss: 0.00000850
Iteration 150/1000 | Loss: 0.00000850
Iteration 151/1000 | Loss: 0.00000850
Iteration 152/1000 | Loss: 0.00000850
Iteration 153/1000 | Loss: 0.00000850
Iteration 154/1000 | Loss: 0.00000850
Iteration 155/1000 | Loss: 0.00000850
Iteration 156/1000 | Loss: 0.00000850
Iteration 157/1000 | Loss: 0.00000850
Iteration 158/1000 | Loss: 0.00000850
Iteration 159/1000 | Loss: 0.00000850
Iteration 160/1000 | Loss: 0.00000850
Iteration 161/1000 | Loss: 0.00000850
Iteration 162/1000 | Loss: 0.00000850
Iteration 163/1000 | Loss: 0.00000849
Iteration 164/1000 | Loss: 0.00000849
Iteration 165/1000 | Loss: 0.00000849
Iteration 166/1000 | Loss: 0.00000849
Iteration 167/1000 | Loss: 0.00000849
Iteration 168/1000 | Loss: 0.00000849
Iteration 169/1000 | Loss: 0.00000849
Iteration 170/1000 | Loss: 0.00000849
Iteration 171/1000 | Loss: 0.00000849
Iteration 172/1000 | Loss: 0.00000848
Iteration 173/1000 | Loss: 0.00000848
Iteration 174/1000 | Loss: 0.00000848
Iteration 175/1000 | Loss: 0.00000848
Iteration 176/1000 | Loss: 0.00000848
Iteration 177/1000 | Loss: 0.00000848
Iteration 178/1000 | Loss: 0.00000848
Iteration 179/1000 | Loss: 0.00000848
Iteration 180/1000 | Loss: 0.00000847
Iteration 181/1000 | Loss: 0.00000847
Iteration 182/1000 | Loss: 0.00000847
Iteration 183/1000 | Loss: 0.00000847
Iteration 184/1000 | Loss: 0.00000846
Iteration 185/1000 | Loss: 0.00000846
Iteration 186/1000 | Loss: 0.00000846
Iteration 187/1000 | Loss: 0.00000846
Iteration 188/1000 | Loss: 0.00000846
Iteration 189/1000 | Loss: 0.00000846
Iteration 190/1000 | Loss: 0.00000846
Iteration 191/1000 | Loss: 0.00000846
Iteration 192/1000 | Loss: 0.00000846
Iteration 193/1000 | Loss: 0.00000845
Iteration 194/1000 | Loss: 0.00000845
Iteration 195/1000 | Loss: 0.00000845
Iteration 196/1000 | Loss: 0.00000845
Iteration 197/1000 | Loss: 0.00000845
Iteration 198/1000 | Loss: 0.00000845
Iteration 199/1000 | Loss: 0.00000844
Iteration 200/1000 | Loss: 0.00000844
Iteration 201/1000 | Loss: 0.00000844
Iteration 202/1000 | Loss: 0.00000844
Iteration 203/1000 | Loss: 0.00000843
Iteration 204/1000 | Loss: 0.00000843
Iteration 205/1000 | Loss: 0.00000843
Iteration 206/1000 | Loss: 0.00000843
Iteration 207/1000 | Loss: 0.00000843
Iteration 208/1000 | Loss: 0.00000843
Iteration 209/1000 | Loss: 0.00000843
Iteration 210/1000 | Loss: 0.00000843
Iteration 211/1000 | Loss: 0.00000843
Iteration 212/1000 | Loss: 0.00000843
Iteration 213/1000 | Loss: 0.00000843
Iteration 214/1000 | Loss: 0.00000843
Iteration 215/1000 | Loss: 0.00000843
Iteration 216/1000 | Loss: 0.00000843
Iteration 217/1000 | Loss: 0.00000843
Iteration 218/1000 | Loss: 0.00000843
Iteration 219/1000 | Loss: 0.00000842
Iteration 220/1000 | Loss: 0.00000842
Iteration 221/1000 | Loss: 0.00000842
Iteration 222/1000 | Loss: 0.00000842
Iteration 223/1000 | Loss: 0.00000842
Iteration 224/1000 | Loss: 0.00000842
Iteration 225/1000 | Loss: 0.00000842
Iteration 226/1000 | Loss: 0.00000842
Iteration 227/1000 | Loss: 0.00000842
Iteration 228/1000 | Loss: 0.00000842
Iteration 229/1000 | Loss: 0.00000842
Iteration 230/1000 | Loss: 0.00000842
Iteration 231/1000 | Loss: 0.00000842
Iteration 232/1000 | Loss: 0.00000842
Iteration 233/1000 | Loss: 0.00000842
Iteration 234/1000 | Loss: 0.00000842
Iteration 235/1000 | Loss: 0.00000842
Iteration 236/1000 | Loss: 0.00000842
Iteration 237/1000 | Loss: 0.00000842
Iteration 238/1000 | Loss: 0.00000842
Iteration 239/1000 | Loss: 0.00000841
Iteration 240/1000 | Loss: 0.00000841
Iteration 241/1000 | Loss: 0.00000841
Iteration 242/1000 | Loss: 0.00000841
Iteration 243/1000 | Loss: 0.00000841
Iteration 244/1000 | Loss: 0.00000841
Iteration 245/1000 | Loss: 0.00000841
Iteration 246/1000 | Loss: 0.00000841
Iteration 247/1000 | Loss: 0.00000841
Iteration 248/1000 | Loss: 0.00000841
Iteration 249/1000 | Loss: 0.00000841
Iteration 250/1000 | Loss: 0.00000841
Iteration 251/1000 | Loss: 0.00000841
Iteration 252/1000 | Loss: 0.00000841
Iteration 253/1000 | Loss: 0.00000841
Iteration 254/1000 | Loss: 0.00000841
Iteration 255/1000 | Loss: 0.00000841
Iteration 256/1000 | Loss: 0.00000841
Iteration 257/1000 | Loss: 0.00000841
Iteration 258/1000 | Loss: 0.00000841
Iteration 259/1000 | Loss: 0.00000841
Iteration 260/1000 | Loss: 0.00000841
Iteration 261/1000 | Loss: 0.00000841
Iteration 262/1000 | Loss: 0.00000841
Iteration 263/1000 | Loss: 0.00000841
Iteration 264/1000 | Loss: 0.00000841
Iteration 265/1000 | Loss: 0.00000841
Iteration 266/1000 | Loss: 0.00000841
Iteration 267/1000 | Loss: 0.00000841
Iteration 268/1000 | Loss: 0.00000841
Iteration 269/1000 | Loss: 0.00000841
Iteration 270/1000 | Loss: 0.00000841
Iteration 271/1000 | Loss: 0.00000841
Iteration 272/1000 | Loss: 0.00000841
Iteration 273/1000 | Loss: 0.00000841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [8.4144749052939e-06, 8.4144749052939e-06, 8.4144749052939e-06, 8.4144749052939e-06, 8.4144749052939e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.4144749052939e-06

Optimization complete. Final v2v error: 2.4805095195770264 mm

Highest mean error: 2.9383881092071533 mm for frame 67

Lowest mean error: 2.3776493072509766 mm for frame 101

Saving results

Total time: 38.680630683898926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858416
Iteration 2/25 | Loss: 0.00106839
Iteration 3/25 | Loss: 0.00076010
Iteration 4/25 | Loss: 0.00069610
Iteration 5/25 | Loss: 0.00068539
Iteration 6/25 | Loss: 0.00068288
Iteration 7/25 | Loss: 0.00068229
Iteration 8/25 | Loss: 0.00068229
Iteration 9/25 | Loss: 0.00068229
Iteration 10/25 | Loss: 0.00068229
Iteration 11/25 | Loss: 0.00068229
Iteration 12/25 | Loss: 0.00068229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006822901195846498, 0.0006822901195846498, 0.0006822901195846498, 0.0006822901195846498, 0.0006822901195846498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006822901195846498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50119054
Iteration 2/25 | Loss: 0.00030567
Iteration 3/25 | Loss: 0.00030567
Iteration 4/25 | Loss: 0.00030567
Iteration 5/25 | Loss: 0.00030567
Iteration 6/25 | Loss: 0.00030567
Iteration 7/25 | Loss: 0.00030567
Iteration 8/25 | Loss: 0.00030567
Iteration 9/25 | Loss: 0.00030567
Iteration 10/25 | Loss: 0.00030567
Iteration 11/25 | Loss: 0.00030567
Iteration 12/25 | Loss: 0.00030567
Iteration 13/25 | Loss: 0.00030567
Iteration 14/25 | Loss: 0.00030567
Iteration 15/25 | Loss: 0.00030567
Iteration 16/25 | Loss: 0.00030567
Iteration 17/25 | Loss: 0.00030567
Iteration 18/25 | Loss: 0.00030567
Iteration 19/25 | Loss: 0.00030567
Iteration 20/25 | Loss: 0.00030567
Iteration 21/25 | Loss: 0.00030567
Iteration 22/25 | Loss: 0.00030567
Iteration 23/25 | Loss: 0.00030567
Iteration 24/25 | Loss: 0.00030567
Iteration 25/25 | Loss: 0.00030567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030567
Iteration 2/1000 | Loss: 0.00002217
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001383
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001254
Iteration 7/1000 | Loss: 0.00001224
Iteration 8/1000 | Loss: 0.00001200
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001168
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001163
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001162
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001161
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001154
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001153
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001152
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001152
Iteration 41/1000 | Loss: 0.00001152
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001151
Iteration 45/1000 | Loss: 0.00001150
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001150
Iteration 48/1000 | Loss: 0.00001149
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001148
Iteration 51/1000 | Loss: 0.00001148
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001147
Iteration 54/1000 | Loss: 0.00001147
Iteration 55/1000 | Loss: 0.00001147
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001145
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.1445124073361512e-05, 1.1445124073361512e-05, 1.1445124073361512e-05, 1.1445124073361512e-05, 1.1445124073361512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1445124073361512e-05

Optimization complete. Final v2v error: 2.8809711933135986 mm

Highest mean error: 3.3239011764526367 mm for frame 113

Lowest mean error: 2.672015905380249 mm for frame 25

Saving results

Total time: 34.190486907958984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839892
Iteration 2/25 | Loss: 0.00090594
Iteration 3/25 | Loss: 0.00070580
Iteration 4/25 | Loss: 0.00067237
Iteration 5/25 | Loss: 0.00066202
Iteration 6/25 | Loss: 0.00065975
Iteration 7/25 | Loss: 0.00065916
Iteration 8/25 | Loss: 0.00065916
Iteration 9/25 | Loss: 0.00065916
Iteration 10/25 | Loss: 0.00065916
Iteration 11/25 | Loss: 0.00065916
Iteration 12/25 | Loss: 0.00065916
Iteration 13/25 | Loss: 0.00065916
Iteration 14/25 | Loss: 0.00065916
Iteration 15/25 | Loss: 0.00065916
Iteration 16/25 | Loss: 0.00065916
Iteration 17/25 | Loss: 0.00065916
Iteration 18/25 | Loss: 0.00065916
Iteration 19/25 | Loss: 0.00065916
Iteration 20/25 | Loss: 0.00065916
Iteration 21/25 | Loss: 0.00065916
Iteration 22/25 | Loss: 0.00065916
Iteration 23/25 | Loss: 0.00065916
Iteration 24/25 | Loss: 0.00065916
Iteration 25/25 | Loss: 0.00065916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.49109435
Iteration 2/25 | Loss: 0.00029578
Iteration 3/25 | Loss: 0.00029578
Iteration 4/25 | Loss: 0.00029578
Iteration 5/25 | Loss: 0.00029578
Iteration 6/25 | Loss: 0.00029578
Iteration 7/25 | Loss: 0.00029578
Iteration 8/25 | Loss: 0.00029578
Iteration 9/25 | Loss: 0.00029578
Iteration 10/25 | Loss: 0.00029577
Iteration 11/25 | Loss: 0.00029577
Iteration 12/25 | Loss: 0.00029577
Iteration 13/25 | Loss: 0.00029577
Iteration 14/25 | Loss: 0.00029577
Iteration 15/25 | Loss: 0.00029577
Iteration 16/25 | Loss: 0.00029577
Iteration 17/25 | Loss: 0.00029577
Iteration 18/25 | Loss: 0.00029577
Iteration 19/25 | Loss: 0.00029577
Iteration 20/25 | Loss: 0.00029577
Iteration 21/25 | Loss: 0.00029577
Iteration 22/25 | Loss: 0.00029577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00029577454552054405, 0.00029577454552054405, 0.00029577454552054405, 0.00029577454552054405, 0.00029577454552054405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029577454552054405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029577
Iteration 2/1000 | Loss: 0.00002096
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001230
Iteration 10/1000 | Loss: 0.00001219
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001164
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001154
Iteration 102/1000 | Loss: 0.00001154
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.1530446499818936e-05, 1.1530446499818936e-05, 1.1530446499818936e-05, 1.1530446499818936e-05, 1.1530446499818936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1530446499818936e-05

Optimization complete. Final v2v error: 2.8586764335632324 mm

Highest mean error: 3.860677480697632 mm for frame 72

Lowest mean error: 2.512923240661621 mm for frame 115

Saving results

Total time: 35.938782691955566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405912
Iteration 2/25 | Loss: 0.00082956
Iteration 3/25 | Loss: 0.00068447
Iteration 4/25 | Loss: 0.00066132
Iteration 5/25 | Loss: 0.00065528
Iteration 6/25 | Loss: 0.00065368
Iteration 7/25 | Loss: 0.00065346
Iteration 8/25 | Loss: 0.00065346
Iteration 9/25 | Loss: 0.00065346
Iteration 10/25 | Loss: 0.00065346
Iteration 11/25 | Loss: 0.00065346
Iteration 12/25 | Loss: 0.00065346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006534592248499393, 0.0006534592248499393, 0.0006534592248499393, 0.0006534592248499393, 0.0006534592248499393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006534592248499393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49210024
Iteration 2/25 | Loss: 0.00031245
Iteration 3/25 | Loss: 0.00031245
Iteration 4/25 | Loss: 0.00031245
Iteration 5/25 | Loss: 0.00031245
Iteration 6/25 | Loss: 0.00031245
Iteration 7/25 | Loss: 0.00031245
Iteration 8/25 | Loss: 0.00031245
Iteration 9/25 | Loss: 0.00031245
Iteration 10/25 | Loss: 0.00031245
Iteration 11/25 | Loss: 0.00031245
Iteration 12/25 | Loss: 0.00031245
Iteration 13/25 | Loss: 0.00031245
Iteration 14/25 | Loss: 0.00031245
Iteration 15/25 | Loss: 0.00031245
Iteration 16/25 | Loss: 0.00031245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003124481299892068, 0.0003124481299892068, 0.0003124481299892068, 0.0003124481299892068, 0.0003124481299892068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003124481299892068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031245
Iteration 2/1000 | Loss: 0.00002173
Iteration 3/1000 | Loss: 0.00001586
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001325
Iteration 6/1000 | Loss: 0.00001275
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001239
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001231
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001192
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001188
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001186
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001184
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001184
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001176
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001175
Iteration 142/1000 | Loss: 0.00001175
Iteration 143/1000 | Loss: 0.00001175
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001175
Iteration 147/1000 | Loss: 0.00001175
Iteration 148/1000 | Loss: 0.00001175
Iteration 149/1000 | Loss: 0.00001175
Iteration 150/1000 | Loss: 0.00001175
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001175
Iteration 157/1000 | Loss: 0.00001175
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001175
Iteration 166/1000 | Loss: 0.00001175
Iteration 167/1000 | Loss: 0.00001175
Iteration 168/1000 | Loss: 0.00001175
Iteration 169/1000 | Loss: 0.00001175
Iteration 170/1000 | Loss: 0.00001175
Iteration 171/1000 | Loss: 0.00001175
Iteration 172/1000 | Loss: 0.00001175
Iteration 173/1000 | Loss: 0.00001175
Iteration 174/1000 | Loss: 0.00001175
Iteration 175/1000 | Loss: 0.00001175
Iteration 176/1000 | Loss: 0.00001175
Iteration 177/1000 | Loss: 0.00001175
Iteration 178/1000 | Loss: 0.00001175
Iteration 179/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1749028999474831e-05, 1.1749028999474831e-05, 1.1749028999474831e-05, 1.1749028999474831e-05, 1.1749028999474831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1749028999474831e-05

Optimization complete. Final v2v error: 2.8621790409088135 mm

Highest mean error: 3.2010321617126465 mm for frame 58

Lowest mean error: 2.539767265319824 mm for frame 97

Saving results

Total time: 35.646400690078735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067816
Iteration 2/25 | Loss: 0.00160188
Iteration 3/25 | Loss: 0.00111751
Iteration 4/25 | Loss: 0.00096216
Iteration 5/25 | Loss: 0.00092918
Iteration 6/25 | Loss: 0.00084356
Iteration 7/25 | Loss: 0.00082661
Iteration 8/25 | Loss: 0.00083600
Iteration 9/25 | Loss: 0.00081791
Iteration 10/25 | Loss: 0.00079612
Iteration 11/25 | Loss: 0.00078355
Iteration 12/25 | Loss: 0.00078371
Iteration 13/25 | Loss: 0.00078525
Iteration 14/25 | Loss: 0.00078205
Iteration 15/25 | Loss: 0.00077211
Iteration 16/25 | Loss: 0.00076834
Iteration 17/25 | Loss: 0.00076742
Iteration 18/25 | Loss: 0.00076714
Iteration 19/25 | Loss: 0.00076707
Iteration 20/25 | Loss: 0.00076705
Iteration 21/25 | Loss: 0.00076705
Iteration 22/25 | Loss: 0.00076704
Iteration 23/25 | Loss: 0.00076704
Iteration 24/25 | Loss: 0.00076704
Iteration 25/25 | Loss: 0.00076704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49953270
Iteration 2/25 | Loss: 0.00039857
Iteration 3/25 | Loss: 0.00039857
Iteration 4/25 | Loss: 0.00039857
Iteration 5/25 | Loss: 0.00039857
Iteration 6/25 | Loss: 0.00039857
Iteration 7/25 | Loss: 0.00039857
Iteration 8/25 | Loss: 0.00039857
Iteration 9/25 | Loss: 0.00039857
Iteration 10/25 | Loss: 0.00039857
Iteration 11/25 | Loss: 0.00039857
Iteration 12/25 | Loss: 0.00039857
Iteration 13/25 | Loss: 0.00039857
Iteration 14/25 | Loss: 0.00039857
Iteration 15/25 | Loss: 0.00039857
Iteration 16/25 | Loss: 0.00039857
Iteration 17/25 | Loss: 0.00039857
Iteration 18/25 | Loss: 0.00039857
Iteration 19/25 | Loss: 0.00039857
Iteration 20/25 | Loss: 0.00039857
Iteration 21/25 | Loss: 0.00039857
Iteration 22/25 | Loss: 0.00039857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003985674702562392, 0.0003985674702562392, 0.0003985674702562392, 0.0003985674702562392, 0.0003985674702562392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003985674702562392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039857
Iteration 2/1000 | Loss: 0.00005237
Iteration 3/1000 | Loss: 0.00003583
Iteration 4/1000 | Loss: 0.00003217
Iteration 5/1000 | Loss: 0.00002934
Iteration 6/1000 | Loss: 0.00002774
Iteration 7/1000 | Loss: 0.00002671
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00085838
Iteration 10/1000 | Loss: 0.00018610
Iteration 11/1000 | Loss: 0.00002792
Iteration 12/1000 | Loss: 0.00002424
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001877
Iteration 18/1000 | Loss: 0.00001860
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001850
Iteration 21/1000 | Loss: 0.00001834
Iteration 22/1000 | Loss: 0.00001827
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001826
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001825
Iteration 28/1000 | Loss: 0.00001825
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001824
Iteration 31/1000 | Loss: 0.00001823
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001823
Iteration 34/1000 | Loss: 0.00001823
Iteration 35/1000 | Loss: 0.00001819
Iteration 36/1000 | Loss: 0.00001818
Iteration 37/1000 | Loss: 0.00001818
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00001817
Iteration 40/1000 | Loss: 0.00001813
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00001807
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001804
Iteration 49/1000 | Loss: 0.00001803
Iteration 50/1000 | Loss: 0.00001803
Iteration 51/1000 | Loss: 0.00001803
Iteration 52/1000 | Loss: 0.00001803
Iteration 53/1000 | Loss: 0.00001803
Iteration 54/1000 | Loss: 0.00001802
Iteration 55/1000 | Loss: 0.00001802
Iteration 56/1000 | Loss: 0.00001802
Iteration 57/1000 | Loss: 0.00001802
Iteration 58/1000 | Loss: 0.00001802
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001801
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001801
Iteration 63/1000 | Loss: 0.00001801
Iteration 64/1000 | Loss: 0.00001800
Iteration 65/1000 | Loss: 0.00001800
Iteration 66/1000 | Loss: 0.00001800
Iteration 67/1000 | Loss: 0.00001800
Iteration 68/1000 | Loss: 0.00001799
Iteration 69/1000 | Loss: 0.00001799
Iteration 70/1000 | Loss: 0.00001799
Iteration 71/1000 | Loss: 0.00001799
Iteration 72/1000 | Loss: 0.00001799
Iteration 73/1000 | Loss: 0.00001798
Iteration 74/1000 | Loss: 0.00001798
Iteration 75/1000 | Loss: 0.00001798
Iteration 76/1000 | Loss: 0.00001798
Iteration 77/1000 | Loss: 0.00001798
Iteration 78/1000 | Loss: 0.00001797
Iteration 79/1000 | Loss: 0.00001797
Iteration 80/1000 | Loss: 0.00001797
Iteration 81/1000 | Loss: 0.00001797
Iteration 82/1000 | Loss: 0.00001797
Iteration 83/1000 | Loss: 0.00001797
Iteration 84/1000 | Loss: 0.00001797
Iteration 85/1000 | Loss: 0.00001797
Iteration 86/1000 | Loss: 0.00001797
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001795
Iteration 108/1000 | Loss: 0.00001795
Iteration 109/1000 | Loss: 0.00001795
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001794
Iteration 120/1000 | Loss: 0.00001794
Iteration 121/1000 | Loss: 0.00001794
Iteration 122/1000 | Loss: 0.00001794
Iteration 123/1000 | Loss: 0.00001794
Iteration 124/1000 | Loss: 0.00001794
Iteration 125/1000 | Loss: 0.00001794
Iteration 126/1000 | Loss: 0.00001794
Iteration 127/1000 | Loss: 0.00001794
Iteration 128/1000 | Loss: 0.00001794
Iteration 129/1000 | Loss: 0.00001794
Iteration 130/1000 | Loss: 0.00001794
Iteration 131/1000 | Loss: 0.00001794
Iteration 132/1000 | Loss: 0.00001794
Iteration 133/1000 | Loss: 0.00001794
Iteration 134/1000 | Loss: 0.00001794
Iteration 135/1000 | Loss: 0.00001794
Iteration 136/1000 | Loss: 0.00001794
Iteration 137/1000 | Loss: 0.00001794
Iteration 138/1000 | Loss: 0.00001794
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001794
Iteration 142/1000 | Loss: 0.00001794
Iteration 143/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.793533556337934e-05, 1.793533556337934e-05, 1.793533556337934e-05, 1.793533556337934e-05, 1.793533556337934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.793533556337934e-05

Optimization complete. Final v2v error: 3.5457420349121094 mm

Highest mean error: 3.9969546794891357 mm for frame 184

Lowest mean error: 3.2174062728881836 mm for frame 37

Saving results

Total time: 71.96508741378784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01129269
Iteration 2/25 | Loss: 0.01129269
Iteration 3/25 | Loss: 0.00308968
Iteration 4/25 | Loss: 0.00179748
Iteration 5/25 | Loss: 0.00156586
Iteration 6/25 | Loss: 0.00161238
Iteration 7/25 | Loss: 0.00091415
Iteration 8/25 | Loss: 0.00086383
Iteration 9/25 | Loss: 0.00084759
Iteration 10/25 | Loss: 0.00083943
Iteration 11/25 | Loss: 0.00083480
Iteration 12/25 | Loss: 0.00083273
Iteration 13/25 | Loss: 0.00082951
Iteration 14/25 | Loss: 0.00082865
Iteration 15/25 | Loss: 0.00082849
Iteration 16/25 | Loss: 0.00082848
Iteration 17/25 | Loss: 0.00082848
Iteration 18/25 | Loss: 0.00082848
Iteration 19/25 | Loss: 0.00082848
Iteration 20/25 | Loss: 0.00082848
Iteration 21/25 | Loss: 0.00082848
Iteration 22/25 | Loss: 0.00082848
Iteration 23/25 | Loss: 0.00082848
Iteration 24/25 | Loss: 0.00082848
Iteration 25/25 | Loss: 0.00082848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69818854
Iteration 2/25 | Loss: 0.00037954
Iteration 3/25 | Loss: 0.00037954
Iteration 4/25 | Loss: 0.00037954
Iteration 5/25 | Loss: 0.00037954
Iteration 6/25 | Loss: 0.00037954
Iteration 7/25 | Loss: 0.00037954
Iteration 8/25 | Loss: 0.00037954
Iteration 9/25 | Loss: 0.00037954
Iteration 10/25 | Loss: 0.00037954
Iteration 11/25 | Loss: 0.00037954
Iteration 12/25 | Loss: 0.00037954
Iteration 13/25 | Loss: 0.00037954
Iteration 14/25 | Loss: 0.00037954
Iteration 15/25 | Loss: 0.00037954
Iteration 16/25 | Loss: 0.00037954
Iteration 17/25 | Loss: 0.00037954
Iteration 18/25 | Loss: 0.00037954
Iteration 19/25 | Loss: 0.00037954
Iteration 20/25 | Loss: 0.00037954
Iteration 21/25 | Loss: 0.00037954
Iteration 22/25 | Loss: 0.00037954
Iteration 23/25 | Loss: 0.00037954
Iteration 24/25 | Loss: 0.00037954
Iteration 25/25 | Loss: 0.00037954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037954
Iteration 2/1000 | Loss: 0.00005746
Iteration 3/1000 | Loss: 0.00004661
Iteration 4/1000 | Loss: 0.00005671
Iteration 5/1000 | Loss: 0.00002505
Iteration 6/1000 | Loss: 0.00002364
Iteration 7/1000 | Loss: 0.00002293
Iteration 8/1000 | Loss: 0.00006177
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002278
Iteration 11/1000 | Loss: 0.00002220
Iteration 12/1000 | Loss: 0.00002213
Iteration 13/1000 | Loss: 0.00002193
Iteration 14/1000 | Loss: 0.00002191
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002183
Iteration 17/1000 | Loss: 0.00002182
Iteration 18/1000 | Loss: 0.00002181
Iteration 19/1000 | Loss: 0.00002165
Iteration 20/1000 | Loss: 0.00002161
Iteration 21/1000 | Loss: 0.00002161
Iteration 22/1000 | Loss: 0.00002160
Iteration 23/1000 | Loss: 0.00002160
Iteration 24/1000 | Loss: 0.00002156
Iteration 25/1000 | Loss: 0.00002154
Iteration 26/1000 | Loss: 0.00002153
Iteration 27/1000 | Loss: 0.00002152
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002147
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002146
Iteration 32/1000 | Loss: 0.00002146
Iteration 33/1000 | Loss: 0.00002144
Iteration 34/1000 | Loss: 0.00002143
Iteration 35/1000 | Loss: 0.00002143
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002143
Iteration 38/1000 | Loss: 0.00002143
Iteration 39/1000 | Loss: 0.00002143
Iteration 40/1000 | Loss: 0.00002143
Iteration 41/1000 | Loss: 0.00002143
Iteration 42/1000 | Loss: 0.00002143
Iteration 43/1000 | Loss: 0.00002143
Iteration 44/1000 | Loss: 0.00002143
Iteration 45/1000 | Loss: 0.00002143
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002143
Iteration 48/1000 | Loss: 0.00002143
Iteration 49/1000 | Loss: 0.00002143
Iteration 50/1000 | Loss: 0.00002142
Iteration 51/1000 | Loss: 0.00002142
Iteration 52/1000 | Loss: 0.00002142
Iteration 53/1000 | Loss: 0.00006044
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00003471
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00002963
Iteration 58/1000 | Loss: 0.00002139
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002136
Iteration 62/1000 | Loss: 0.00002136
Iteration 63/1000 | Loss: 0.00002136
Iteration 64/1000 | Loss: 0.00002136
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002215
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002135
Iteration 73/1000 | Loss: 0.00002135
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002135
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002135
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.1350673705455847e-05, 2.1350673705455847e-05, 2.1350673705455847e-05, 2.1350673705455847e-05, 2.1350673705455847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1350673705455847e-05

Optimization complete. Final v2v error: 3.730935573577881 mm

Highest mean error: 4.276231288909912 mm for frame 27

Lowest mean error: 3.2157437801361084 mm for frame 148

Saving results

Total time: 57.25077486038208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812891
Iteration 2/25 | Loss: 0.00113026
Iteration 3/25 | Loss: 0.00087824
Iteration 4/25 | Loss: 0.00080542
Iteration 5/25 | Loss: 0.00077917
Iteration 6/25 | Loss: 0.00077142
Iteration 7/25 | Loss: 0.00075748
Iteration 8/25 | Loss: 0.00074409
Iteration 9/25 | Loss: 0.00074545
Iteration 10/25 | Loss: 0.00075160
Iteration 11/25 | Loss: 0.00078198
Iteration 12/25 | Loss: 0.00077840
Iteration 13/25 | Loss: 0.00077279
Iteration 14/25 | Loss: 0.00076078
Iteration 15/25 | Loss: 0.00074531
Iteration 16/25 | Loss: 0.00073692
Iteration 17/25 | Loss: 0.00072385
Iteration 18/25 | Loss: 0.00071716
Iteration 19/25 | Loss: 0.00071418
Iteration 20/25 | Loss: 0.00071309
Iteration 21/25 | Loss: 0.00071257
Iteration 22/25 | Loss: 0.00071239
Iteration 23/25 | Loss: 0.00071239
Iteration 24/25 | Loss: 0.00071239
Iteration 25/25 | Loss: 0.00071239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52833486
Iteration 2/25 | Loss: 0.00033082
Iteration 3/25 | Loss: 0.00033081
Iteration 4/25 | Loss: 0.00033081
Iteration 5/25 | Loss: 0.00033081
Iteration 6/25 | Loss: 0.00033081
Iteration 7/25 | Loss: 0.00033081
Iteration 8/25 | Loss: 0.00033081
Iteration 9/25 | Loss: 0.00033081
Iteration 10/25 | Loss: 0.00033080
Iteration 11/25 | Loss: 0.00033080
Iteration 12/25 | Loss: 0.00033080
Iteration 13/25 | Loss: 0.00033080
Iteration 14/25 | Loss: 0.00033080
Iteration 15/25 | Loss: 0.00033080
Iteration 16/25 | Loss: 0.00033080
Iteration 17/25 | Loss: 0.00033080
Iteration 18/25 | Loss: 0.00033080
Iteration 19/25 | Loss: 0.00033080
Iteration 20/25 | Loss: 0.00033080
Iteration 21/25 | Loss: 0.00033080
Iteration 22/25 | Loss: 0.00033080
Iteration 23/25 | Loss: 0.00033080
Iteration 24/25 | Loss: 0.00033080
Iteration 25/25 | Loss: 0.00033080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033080
Iteration 2/1000 | Loss: 0.00003378
Iteration 3/1000 | Loss: 0.00002514
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002126
Iteration 7/1000 | Loss: 0.00002062
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00001980
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001932
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001925
Iteration 14/1000 | Loss: 0.00001910
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001974
Iteration 17/1000 | Loss: 0.00001896
Iteration 18/1000 | Loss: 0.00001870
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001851
Iteration 21/1000 | Loss: 0.00001850
Iteration 22/1000 | Loss: 0.00001850
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001849
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001846
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001841
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001840
Iteration 43/1000 | Loss: 0.00001840
Iteration 44/1000 | Loss: 0.00001840
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001840
Iteration 47/1000 | Loss: 0.00001840
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001832
Iteration 62/1000 | Loss: 0.00001832
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001828
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00001827
Iteration 79/1000 | Loss: 0.00001827
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001826
Iteration 82/1000 | Loss: 0.00001826
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001820
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001819
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001818
Iteration 127/1000 | Loss: 0.00001818
Iteration 128/1000 | Loss: 0.00001818
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001818
Iteration 131/1000 | Loss: 0.00001818
Iteration 132/1000 | Loss: 0.00001818
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001817
Iteration 136/1000 | Loss: 0.00001817
Iteration 137/1000 | Loss: 0.00001817
Iteration 138/1000 | Loss: 0.00001817
Iteration 139/1000 | Loss: 0.00001817
Iteration 140/1000 | Loss: 0.00001817
Iteration 141/1000 | Loss: 0.00001817
Iteration 142/1000 | Loss: 0.00001817
Iteration 143/1000 | Loss: 0.00001817
Iteration 144/1000 | Loss: 0.00001817
Iteration 145/1000 | Loss: 0.00001817
Iteration 146/1000 | Loss: 0.00001817
Iteration 147/1000 | Loss: 0.00001817
Iteration 148/1000 | Loss: 0.00001817
Iteration 149/1000 | Loss: 0.00001817
Iteration 150/1000 | Loss: 0.00001817
Iteration 151/1000 | Loss: 0.00001817
Iteration 152/1000 | Loss: 0.00001816
Iteration 153/1000 | Loss: 0.00001816
Iteration 154/1000 | Loss: 0.00001816
Iteration 155/1000 | Loss: 0.00001816
Iteration 156/1000 | Loss: 0.00001816
Iteration 157/1000 | Loss: 0.00001816
Iteration 158/1000 | Loss: 0.00001816
Iteration 159/1000 | Loss: 0.00001816
Iteration 160/1000 | Loss: 0.00001816
Iteration 161/1000 | Loss: 0.00001816
Iteration 162/1000 | Loss: 0.00001816
Iteration 163/1000 | Loss: 0.00001816
Iteration 164/1000 | Loss: 0.00001816
Iteration 165/1000 | Loss: 0.00001816
Iteration 166/1000 | Loss: 0.00001816
Iteration 167/1000 | Loss: 0.00001816
Iteration 168/1000 | Loss: 0.00001816
Iteration 169/1000 | Loss: 0.00001816
Iteration 170/1000 | Loss: 0.00001816
Iteration 171/1000 | Loss: 0.00001816
Iteration 172/1000 | Loss: 0.00001816
Iteration 173/1000 | Loss: 0.00001816
Iteration 174/1000 | Loss: 0.00001816
Iteration 175/1000 | Loss: 0.00001816
Iteration 176/1000 | Loss: 0.00001816
Iteration 177/1000 | Loss: 0.00001816
Iteration 178/1000 | Loss: 0.00001816
Iteration 179/1000 | Loss: 0.00001816
Iteration 180/1000 | Loss: 0.00001816
Iteration 181/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.8156975784222595e-05, 1.8156975784222595e-05, 1.8156975784222595e-05, 1.8156975784222595e-05, 1.8156975784222595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8156975784222595e-05

Optimization complete. Final v2v error: 3.5070414543151855 mm

Highest mean error: 4.591152667999268 mm for frame 14

Lowest mean error: 2.922659158706665 mm for frame 55

Saving results

Total time: 83.7220447063446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046347
Iteration 2/25 | Loss: 0.01046347
Iteration 3/25 | Loss: 0.00173588
Iteration 4/25 | Loss: 0.00100718
Iteration 5/25 | Loss: 0.00094072
Iteration 6/25 | Loss: 0.00086888
Iteration 7/25 | Loss: 0.00083392
Iteration 8/25 | Loss: 0.00080008
Iteration 9/25 | Loss: 0.00078837
Iteration 10/25 | Loss: 0.00078840
Iteration 11/25 | Loss: 0.00078184
Iteration 12/25 | Loss: 0.00077352
Iteration 13/25 | Loss: 0.00076591
Iteration 14/25 | Loss: 0.00076638
Iteration 15/25 | Loss: 0.00076259
Iteration 16/25 | Loss: 0.00075719
Iteration 17/25 | Loss: 0.00075966
Iteration 18/25 | Loss: 0.00075184
Iteration 19/25 | Loss: 0.00075608
Iteration 20/25 | Loss: 0.00075413
Iteration 21/25 | Loss: 0.00075309
Iteration 22/25 | Loss: 0.00075379
Iteration 23/25 | Loss: 0.00075329
Iteration 24/25 | Loss: 0.00075329
Iteration 25/25 | Loss: 0.00075313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46912611
Iteration 2/25 | Loss: 0.00174190
Iteration 3/25 | Loss: 0.00136565
Iteration 4/25 | Loss: 0.00136565
Iteration 5/25 | Loss: 0.00136565
Iteration 6/25 | Loss: 0.00136565
Iteration 7/25 | Loss: 0.00136565
Iteration 8/25 | Loss: 0.00136565
Iteration 9/25 | Loss: 0.00136565
Iteration 10/25 | Loss: 0.00136565
Iteration 11/25 | Loss: 0.00136565
Iteration 12/25 | Loss: 0.00136565
Iteration 13/25 | Loss: 0.00136565
Iteration 14/25 | Loss: 0.00136565
Iteration 15/25 | Loss: 0.00136565
Iteration 16/25 | Loss: 0.00136565
Iteration 17/25 | Loss: 0.00136565
Iteration 18/25 | Loss: 0.00136565
Iteration 19/25 | Loss: 0.00136565
Iteration 20/25 | Loss: 0.00136565
Iteration 21/25 | Loss: 0.00136565
Iteration 22/25 | Loss: 0.00136565
Iteration 23/25 | Loss: 0.00136565
Iteration 24/25 | Loss: 0.00136565
Iteration 25/25 | Loss: 0.00136565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136565
Iteration 2/1000 | Loss: 0.00102304
Iteration 3/1000 | Loss: 0.00167856
Iteration 4/1000 | Loss: 0.00040003
Iteration 5/1000 | Loss: 0.00256230
Iteration 6/1000 | Loss: 0.00042961
Iteration 7/1000 | Loss: 0.00040636
Iteration 8/1000 | Loss: 0.00036749
Iteration 9/1000 | Loss: 0.00032833
Iteration 10/1000 | Loss: 0.00031654
Iteration 11/1000 | Loss: 0.00019779
Iteration 12/1000 | Loss: 0.00010714
Iteration 13/1000 | Loss: 0.00063277
Iteration 14/1000 | Loss: 0.00024361
Iteration 15/1000 | Loss: 0.00109418
Iteration 16/1000 | Loss: 0.00023578
Iteration 17/1000 | Loss: 0.00036973
Iteration 18/1000 | Loss: 0.00085229
Iteration 19/1000 | Loss: 0.00039911
Iteration 20/1000 | Loss: 0.00018856
Iteration 21/1000 | Loss: 0.00099070
Iteration 22/1000 | Loss: 0.00129657
Iteration 23/1000 | Loss: 0.00016867
Iteration 24/1000 | Loss: 0.00067816
Iteration 25/1000 | Loss: 0.00057128
Iteration 26/1000 | Loss: 0.00060884
Iteration 27/1000 | Loss: 0.00103272
Iteration 28/1000 | Loss: 0.00052503
Iteration 29/1000 | Loss: 0.00027049
Iteration 30/1000 | Loss: 0.00044535
Iteration 31/1000 | Loss: 0.00043532
Iteration 32/1000 | Loss: 0.00124417
Iteration 33/1000 | Loss: 0.00068208
Iteration 34/1000 | Loss: 0.00067486
Iteration 35/1000 | Loss: 0.00040680
Iteration 36/1000 | Loss: 0.00066927
Iteration 37/1000 | Loss: 0.00032407
Iteration 38/1000 | Loss: 0.00041371
Iteration 39/1000 | Loss: 0.00085567
Iteration 40/1000 | Loss: 0.00103705
Iteration 41/1000 | Loss: 0.00085476
Iteration 42/1000 | Loss: 0.00098337
Iteration 43/1000 | Loss: 0.00034476
Iteration 44/1000 | Loss: 0.00045711
Iteration 45/1000 | Loss: 0.00039968
Iteration 46/1000 | Loss: 0.00020637
Iteration 47/1000 | Loss: 0.00054917
Iteration 48/1000 | Loss: 0.00070200
Iteration 49/1000 | Loss: 0.00039894
Iteration 50/1000 | Loss: 0.00055699
Iteration 51/1000 | Loss: 0.00068223
Iteration 52/1000 | Loss: 0.00008874
Iteration 53/1000 | Loss: 0.00004738
Iteration 54/1000 | Loss: 0.00010188
Iteration 55/1000 | Loss: 0.00004656
Iteration 56/1000 | Loss: 0.00005175
Iteration 57/1000 | Loss: 0.00004096
Iteration 58/1000 | Loss: 0.00048659
Iteration 59/1000 | Loss: 0.00025137
Iteration 60/1000 | Loss: 0.00266490
Iteration 61/1000 | Loss: 0.00062443
Iteration 62/1000 | Loss: 0.00016312
Iteration 63/1000 | Loss: 0.00022749
Iteration 64/1000 | Loss: 0.00020057
Iteration 65/1000 | Loss: 0.00008763
Iteration 66/1000 | Loss: 0.00005121
Iteration 67/1000 | Loss: 0.00003964
Iteration 68/1000 | Loss: 0.00003727
Iteration 69/1000 | Loss: 0.00005573
Iteration 70/1000 | Loss: 0.00002369
Iteration 71/1000 | Loss: 0.00004424
Iteration 72/1000 | Loss: 0.00002295
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00006677
Iteration 75/1000 | Loss: 0.00003896
Iteration 76/1000 | Loss: 0.00002390
Iteration 77/1000 | Loss: 0.00002149
Iteration 78/1000 | Loss: 0.00005870
Iteration 79/1000 | Loss: 0.00006620
Iteration 80/1000 | Loss: 0.00002119
Iteration 81/1000 | Loss: 0.00004540
Iteration 82/1000 | Loss: 0.00002124
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002089
Iteration 85/1000 | Loss: 0.00002088
Iteration 86/1000 | Loss: 0.00002088
Iteration 87/1000 | Loss: 0.00002088
Iteration 88/1000 | Loss: 0.00002088
Iteration 89/1000 | Loss: 0.00002088
Iteration 90/1000 | Loss: 0.00002088
Iteration 91/1000 | Loss: 0.00002087
Iteration 92/1000 | Loss: 0.00002086
Iteration 93/1000 | Loss: 0.00002259
Iteration 94/1000 | Loss: 0.00004270
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002077
Iteration 98/1000 | Loss: 0.00002075
Iteration 99/1000 | Loss: 0.00002075
Iteration 100/1000 | Loss: 0.00002075
Iteration 101/1000 | Loss: 0.00002074
Iteration 102/1000 | Loss: 0.00002074
Iteration 103/1000 | Loss: 0.00002074
Iteration 104/1000 | Loss: 0.00002074
Iteration 105/1000 | Loss: 0.00002074
Iteration 106/1000 | Loss: 0.00003231
Iteration 107/1000 | Loss: 0.00002070
Iteration 108/1000 | Loss: 0.00002069
Iteration 109/1000 | Loss: 0.00002069
Iteration 110/1000 | Loss: 0.00002069
Iteration 111/1000 | Loss: 0.00002069
Iteration 112/1000 | Loss: 0.00002069
Iteration 113/1000 | Loss: 0.00002069
Iteration 114/1000 | Loss: 0.00002069
Iteration 115/1000 | Loss: 0.00002069
Iteration 116/1000 | Loss: 0.00002069
Iteration 117/1000 | Loss: 0.00002069
Iteration 118/1000 | Loss: 0.00002068
Iteration 119/1000 | Loss: 0.00050420
Iteration 120/1000 | Loss: 0.00037739
Iteration 121/1000 | Loss: 0.00003423
Iteration 122/1000 | Loss: 0.00003792
Iteration 123/1000 | Loss: 0.00005652
Iteration 124/1000 | Loss: 0.00003469
Iteration 125/1000 | Loss: 0.00005654
Iteration 126/1000 | Loss: 0.00010981
Iteration 127/1000 | Loss: 0.00003524
Iteration 128/1000 | Loss: 0.00003295
Iteration 129/1000 | Loss: 0.00004484
Iteration 130/1000 | Loss: 0.00003563
Iteration 131/1000 | Loss: 0.00003958
Iteration 132/1000 | Loss: 0.00009884
Iteration 133/1000 | Loss: 0.00001876
Iteration 134/1000 | Loss: 0.00001864
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001856
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001848
Iteration 139/1000 | Loss: 0.00001847
Iteration 140/1000 | Loss: 0.00001847
Iteration 141/1000 | Loss: 0.00001846
Iteration 142/1000 | Loss: 0.00001844
Iteration 143/1000 | Loss: 0.00001844
Iteration 144/1000 | Loss: 0.00001844
Iteration 145/1000 | Loss: 0.00001844
Iteration 146/1000 | Loss: 0.00001844
Iteration 147/1000 | Loss: 0.00001843
Iteration 148/1000 | Loss: 0.00001843
Iteration 149/1000 | Loss: 0.00001842
Iteration 150/1000 | Loss: 0.00001842
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001841
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001840
Iteration 156/1000 | Loss: 0.00001840
Iteration 157/1000 | Loss: 0.00001840
Iteration 158/1000 | Loss: 0.00001840
Iteration 159/1000 | Loss: 0.00001839
Iteration 160/1000 | Loss: 0.00001839
Iteration 161/1000 | Loss: 0.00001839
Iteration 162/1000 | Loss: 0.00001839
Iteration 163/1000 | Loss: 0.00001839
Iteration 164/1000 | Loss: 0.00001839
Iteration 165/1000 | Loss: 0.00001839
Iteration 166/1000 | Loss: 0.00001839
Iteration 167/1000 | Loss: 0.00001839
Iteration 168/1000 | Loss: 0.00001839
Iteration 169/1000 | Loss: 0.00001839
Iteration 170/1000 | Loss: 0.00001839
Iteration 171/1000 | Loss: 0.00001838
Iteration 172/1000 | Loss: 0.00001838
Iteration 173/1000 | Loss: 0.00001838
Iteration 174/1000 | Loss: 0.00001838
Iteration 175/1000 | Loss: 0.00001837
Iteration 176/1000 | Loss: 0.00001837
Iteration 177/1000 | Loss: 0.00001836
Iteration 178/1000 | Loss: 0.00001836
Iteration 179/1000 | Loss: 0.00001835
Iteration 180/1000 | Loss: 0.00001835
Iteration 181/1000 | Loss: 0.00001835
Iteration 182/1000 | Loss: 0.00001834
Iteration 183/1000 | Loss: 0.00001833
Iteration 184/1000 | Loss: 0.00001833
Iteration 185/1000 | Loss: 0.00001833
Iteration 186/1000 | Loss: 0.00001832
Iteration 187/1000 | Loss: 0.00004919
Iteration 188/1000 | Loss: 0.00001834
Iteration 189/1000 | Loss: 0.00001832
Iteration 190/1000 | Loss: 0.00001831
Iteration 191/1000 | Loss: 0.00001831
Iteration 192/1000 | Loss: 0.00001830
Iteration 193/1000 | Loss: 0.00001830
Iteration 194/1000 | Loss: 0.00001830
Iteration 195/1000 | Loss: 0.00001830
Iteration 196/1000 | Loss: 0.00001830
Iteration 197/1000 | Loss: 0.00001830
Iteration 198/1000 | Loss: 0.00001830
Iteration 199/1000 | Loss: 0.00001830
Iteration 200/1000 | Loss: 0.00001830
Iteration 201/1000 | Loss: 0.00001830
Iteration 202/1000 | Loss: 0.00001830
Iteration 203/1000 | Loss: 0.00001830
Iteration 204/1000 | Loss: 0.00001830
Iteration 205/1000 | Loss: 0.00001829
Iteration 206/1000 | Loss: 0.00001829
Iteration 207/1000 | Loss: 0.00001828
Iteration 208/1000 | Loss: 0.00001828
Iteration 209/1000 | Loss: 0.00001827
Iteration 210/1000 | Loss: 0.00001827
Iteration 211/1000 | Loss: 0.00001827
Iteration 212/1000 | Loss: 0.00001827
Iteration 213/1000 | Loss: 0.00001827
Iteration 214/1000 | Loss: 0.00001827
Iteration 215/1000 | Loss: 0.00001827
Iteration 216/1000 | Loss: 0.00001827
Iteration 217/1000 | Loss: 0.00001827
Iteration 218/1000 | Loss: 0.00001827
Iteration 219/1000 | Loss: 0.00001827
Iteration 220/1000 | Loss: 0.00001827
Iteration 221/1000 | Loss: 0.00001827
Iteration 222/1000 | Loss: 0.00001827
Iteration 223/1000 | Loss: 0.00001827
Iteration 224/1000 | Loss: 0.00001827
Iteration 225/1000 | Loss: 0.00001827
Iteration 226/1000 | Loss: 0.00001827
Iteration 227/1000 | Loss: 0.00001827
Iteration 228/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.826735751819797e-05, 1.826735751819797e-05, 1.826735751819797e-05, 1.826735751819797e-05, 1.826735751819797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826735751819797e-05

Optimization complete. Final v2v error: 2.8903443813323975 mm

Highest mean error: 11.85962200164795 mm for frame 8

Lowest mean error: 2.2859039306640625 mm for frame 110

Saving results

Total time: 227.2428708076477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860186
Iteration 2/25 | Loss: 0.00155593
Iteration 3/25 | Loss: 0.00100752
Iteration 4/25 | Loss: 0.00088498
Iteration 5/25 | Loss: 0.00087272
Iteration 6/25 | Loss: 0.00088063
Iteration 7/25 | Loss: 0.00084620
Iteration 8/25 | Loss: 0.00082278
Iteration 9/25 | Loss: 0.00081368
Iteration 10/25 | Loss: 0.00080585
Iteration 11/25 | Loss: 0.00080150
Iteration 12/25 | Loss: 0.00080921
Iteration 13/25 | Loss: 0.00079833
Iteration 14/25 | Loss: 0.00078454
Iteration 15/25 | Loss: 0.00078068
Iteration 16/25 | Loss: 0.00078002
Iteration 17/25 | Loss: 0.00077917
Iteration 18/25 | Loss: 0.00077833
Iteration 19/25 | Loss: 0.00077753
Iteration 20/25 | Loss: 0.00077687
Iteration 21/25 | Loss: 0.00077672
Iteration 22/25 | Loss: 0.00077666
Iteration 23/25 | Loss: 0.00077666
Iteration 24/25 | Loss: 0.00077666
Iteration 25/25 | Loss: 0.00077666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.78204966
Iteration 2/25 | Loss: 0.00044746
Iteration 3/25 | Loss: 0.00044744
Iteration 4/25 | Loss: 0.00044744
Iteration 5/25 | Loss: 0.00044744
Iteration 6/25 | Loss: 0.00044744
Iteration 7/25 | Loss: 0.00044744
Iteration 8/25 | Loss: 0.00044744
Iteration 9/25 | Loss: 0.00044744
Iteration 10/25 | Loss: 0.00044744
Iteration 11/25 | Loss: 0.00044744
Iteration 12/25 | Loss: 0.00044744
Iteration 13/25 | Loss: 0.00044744
Iteration 14/25 | Loss: 0.00044744
Iteration 15/25 | Loss: 0.00044744
Iteration 16/25 | Loss: 0.00044744
Iteration 17/25 | Loss: 0.00044744
Iteration 18/25 | Loss: 0.00044744
Iteration 19/25 | Loss: 0.00044744
Iteration 20/25 | Loss: 0.00044744
Iteration 21/25 | Loss: 0.00044744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004474385641515255, 0.0004474385641515255, 0.0004474385641515255, 0.0004474385641515255, 0.0004474385641515255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004474385641515255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044744
Iteration 2/1000 | Loss: 0.00003838
Iteration 3/1000 | Loss: 0.00003128
Iteration 4/1000 | Loss: 0.00007268
Iteration 5/1000 | Loss: 0.00004794
Iteration 6/1000 | Loss: 0.00002792
Iteration 7/1000 | Loss: 0.00002629
Iteration 8/1000 | Loss: 0.00014644
Iteration 9/1000 | Loss: 0.00006412
Iteration 10/1000 | Loss: 0.00009914
Iteration 11/1000 | Loss: 0.00002528
Iteration 12/1000 | Loss: 0.00002250
Iteration 13/1000 | Loss: 0.00002053
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00005632
Iteration 16/1000 | Loss: 0.00001917
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00004517
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001854
Iteration 24/1000 | Loss: 0.00001854
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001850
Iteration 28/1000 | Loss: 0.00001846
Iteration 29/1000 | Loss: 0.00001841
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00005425
Iteration 32/1000 | Loss: 0.00001844
Iteration 33/1000 | Loss: 0.00001837
Iteration 34/1000 | Loss: 0.00001836
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001833
Iteration 40/1000 | Loss: 0.00001833
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001832
Iteration 45/1000 | Loss: 0.00001832
Iteration 46/1000 | Loss: 0.00001832
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001829
Iteration 56/1000 | Loss: 0.00001829
Iteration 57/1000 | Loss: 0.00001828
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001828
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001827
Iteration 66/1000 | Loss: 0.00001827
Iteration 67/1000 | Loss: 0.00001827
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001826
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001825
Iteration 85/1000 | Loss: 0.00001825
Iteration 86/1000 | Loss: 0.00001825
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001825
Iteration 95/1000 | Loss: 0.00001825
Iteration 96/1000 | Loss: 0.00001825
Iteration 97/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.825405706767924e-05, 1.825405706767924e-05, 1.825405706767924e-05, 1.825405706767924e-05, 1.825405706767924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.825405706767924e-05

Optimization complete. Final v2v error: 3.6308646202087402 mm

Highest mean error: 4.214170932769775 mm for frame 192

Lowest mean error: 3.2274398803710938 mm for frame 0

Saving results

Total time: 83.84458112716675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481709
Iteration 2/25 | Loss: 0.00110750
Iteration 3/25 | Loss: 0.00075705
Iteration 4/25 | Loss: 0.00069903
Iteration 5/25 | Loss: 0.00068663
Iteration 6/25 | Loss: 0.00068341
Iteration 7/25 | Loss: 0.00068197
Iteration 8/25 | Loss: 0.00068167
Iteration 9/25 | Loss: 0.00068167
Iteration 10/25 | Loss: 0.00068167
Iteration 11/25 | Loss: 0.00068167
Iteration 12/25 | Loss: 0.00068167
Iteration 13/25 | Loss: 0.00068167
Iteration 14/25 | Loss: 0.00068167
Iteration 15/25 | Loss: 0.00068167
Iteration 16/25 | Loss: 0.00068167
Iteration 17/25 | Loss: 0.00068167
Iteration 18/25 | Loss: 0.00068167
Iteration 19/25 | Loss: 0.00068167
Iteration 20/25 | Loss: 0.00068167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006816706736572087, 0.0006816706736572087, 0.0006816706736572087, 0.0006816706736572087, 0.0006816706736572087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006816706736572087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49615467
Iteration 2/25 | Loss: 0.00030700
Iteration 3/25 | Loss: 0.00030700
Iteration 4/25 | Loss: 0.00030700
Iteration 5/25 | Loss: 0.00030700
Iteration 6/25 | Loss: 0.00030700
Iteration 7/25 | Loss: 0.00030700
Iteration 8/25 | Loss: 0.00030700
Iteration 9/25 | Loss: 0.00030700
Iteration 10/25 | Loss: 0.00030700
Iteration 11/25 | Loss: 0.00030700
Iteration 12/25 | Loss: 0.00030700
Iteration 13/25 | Loss: 0.00030700
Iteration 14/25 | Loss: 0.00030700
Iteration 15/25 | Loss: 0.00030700
Iteration 16/25 | Loss: 0.00030700
Iteration 17/25 | Loss: 0.00030700
Iteration 18/25 | Loss: 0.00030700
Iteration 19/25 | Loss: 0.00030700
Iteration 20/25 | Loss: 0.00030700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003069958765991032, 0.0003069958765991032, 0.0003069958765991032, 0.0003069958765991032, 0.0003069958765991032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003069958765991032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030700
Iteration 2/1000 | Loss: 0.00002570
Iteration 3/1000 | Loss: 0.00001694
Iteration 4/1000 | Loss: 0.00001550
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001321
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001320
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001314
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001311
Iteration 34/1000 | Loss: 0.00001311
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001310
Iteration 37/1000 | Loss: 0.00001310
Iteration 38/1000 | Loss: 0.00001310
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001310
Iteration 43/1000 | Loss: 0.00001310
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001309
Iteration 46/1000 | Loss: 0.00001309
Iteration 47/1000 | Loss: 0.00001309
Iteration 48/1000 | Loss: 0.00001309
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001308
Iteration 60/1000 | Loss: 0.00001308
Iteration 61/1000 | Loss: 0.00001308
Iteration 62/1000 | Loss: 0.00001308
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001308
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001308
Iteration 73/1000 | Loss: 0.00001308
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.3076548384560738e-05, 1.3076548384560738e-05, 1.3076548384560738e-05, 1.3076548384560738e-05, 1.3076548384560738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3076548384560738e-05

Optimization complete. Final v2v error: 2.8610026836395264 mm

Highest mean error: 4.437636375427246 mm for frame 83

Lowest mean error: 2.370426893234253 mm for frame 154

Saving results

Total time: 34.01917815208435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388512
Iteration 2/25 | Loss: 0.00083376
Iteration 3/25 | Loss: 0.00067418
Iteration 4/25 | Loss: 0.00064543
Iteration 5/25 | Loss: 0.00063999
Iteration 6/25 | Loss: 0.00063821
Iteration 7/25 | Loss: 0.00063782
Iteration 8/25 | Loss: 0.00063782
Iteration 9/25 | Loss: 0.00063782
Iteration 10/25 | Loss: 0.00063782
Iteration 11/25 | Loss: 0.00063782
Iteration 12/25 | Loss: 0.00063782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006378227262757719, 0.0006378227262757719, 0.0006378227262757719, 0.0006378227262757719, 0.0006378227262757719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006378227262757719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48661828
Iteration 2/25 | Loss: 0.00025670
Iteration 3/25 | Loss: 0.00025669
Iteration 4/25 | Loss: 0.00025669
Iteration 5/25 | Loss: 0.00025669
Iteration 6/25 | Loss: 0.00025669
Iteration 7/25 | Loss: 0.00025669
Iteration 8/25 | Loss: 0.00025669
Iteration 9/25 | Loss: 0.00025669
Iteration 10/25 | Loss: 0.00025669
Iteration 11/25 | Loss: 0.00025669
Iteration 12/25 | Loss: 0.00025669
Iteration 13/25 | Loss: 0.00025669
Iteration 14/25 | Loss: 0.00025669
Iteration 15/25 | Loss: 0.00025669
Iteration 16/25 | Loss: 0.00025669
Iteration 17/25 | Loss: 0.00025669
Iteration 18/25 | Loss: 0.00025669
Iteration 19/25 | Loss: 0.00025669
Iteration 20/25 | Loss: 0.00025669
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00025668664602562785, 0.00025668664602562785, 0.00025668664602562785, 0.00025668664602562785, 0.00025668664602562785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025668664602562785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025669
Iteration 2/1000 | Loss: 0.00002111
Iteration 3/1000 | Loss: 0.00001264
Iteration 4/1000 | Loss: 0.00001139
Iteration 5/1000 | Loss: 0.00001073
Iteration 6/1000 | Loss: 0.00001031
Iteration 7/1000 | Loss: 0.00001002
Iteration 8/1000 | Loss: 0.00000993
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000993
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000991
Iteration 13/1000 | Loss: 0.00000991
Iteration 14/1000 | Loss: 0.00000990
Iteration 15/1000 | Loss: 0.00000989
Iteration 16/1000 | Loss: 0.00000988
Iteration 17/1000 | Loss: 0.00000988
Iteration 18/1000 | Loss: 0.00000987
Iteration 19/1000 | Loss: 0.00000987
Iteration 20/1000 | Loss: 0.00000986
Iteration 21/1000 | Loss: 0.00000986
Iteration 22/1000 | Loss: 0.00000982
Iteration 23/1000 | Loss: 0.00000981
Iteration 24/1000 | Loss: 0.00000980
Iteration 25/1000 | Loss: 0.00000979
Iteration 26/1000 | Loss: 0.00000978
Iteration 27/1000 | Loss: 0.00000977
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000970
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000969
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000968
Iteration 35/1000 | Loss: 0.00000968
Iteration 36/1000 | Loss: 0.00000967
Iteration 37/1000 | Loss: 0.00000967
Iteration 38/1000 | Loss: 0.00000966
Iteration 39/1000 | Loss: 0.00000966
Iteration 40/1000 | Loss: 0.00000966
Iteration 41/1000 | Loss: 0.00000966
Iteration 42/1000 | Loss: 0.00000966
Iteration 43/1000 | Loss: 0.00000965
Iteration 44/1000 | Loss: 0.00000965
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000965
Iteration 47/1000 | Loss: 0.00000965
Iteration 48/1000 | Loss: 0.00000964
Iteration 49/1000 | Loss: 0.00000964
Iteration 50/1000 | Loss: 0.00000964
Iteration 51/1000 | Loss: 0.00000963
Iteration 52/1000 | Loss: 0.00000962
Iteration 53/1000 | Loss: 0.00000962
Iteration 54/1000 | Loss: 0.00000962
Iteration 55/1000 | Loss: 0.00000961
Iteration 56/1000 | Loss: 0.00000961
Iteration 57/1000 | Loss: 0.00000961
Iteration 58/1000 | Loss: 0.00000960
Iteration 59/1000 | Loss: 0.00000960
Iteration 60/1000 | Loss: 0.00000960
Iteration 61/1000 | Loss: 0.00000960
Iteration 62/1000 | Loss: 0.00000959
Iteration 63/1000 | Loss: 0.00000959
Iteration 64/1000 | Loss: 0.00000959
Iteration 65/1000 | Loss: 0.00000959
Iteration 66/1000 | Loss: 0.00000959
Iteration 67/1000 | Loss: 0.00000958
Iteration 68/1000 | Loss: 0.00000958
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000957
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000957
Iteration 73/1000 | Loss: 0.00000957
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000954
Iteration 80/1000 | Loss: 0.00000954
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000953
Iteration 88/1000 | Loss: 0.00000953
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000950
Iteration 100/1000 | Loss: 0.00000950
Iteration 101/1000 | Loss: 0.00000950
Iteration 102/1000 | Loss: 0.00000949
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000949
Iteration 110/1000 | Loss: 0.00000949
Iteration 111/1000 | Loss: 0.00000949
Iteration 112/1000 | Loss: 0.00000949
Iteration 113/1000 | Loss: 0.00000949
Iteration 114/1000 | Loss: 0.00000949
Iteration 115/1000 | Loss: 0.00000948
Iteration 116/1000 | Loss: 0.00000948
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000948
Iteration 119/1000 | Loss: 0.00000948
Iteration 120/1000 | Loss: 0.00000948
Iteration 121/1000 | Loss: 0.00000947
Iteration 122/1000 | Loss: 0.00000947
Iteration 123/1000 | Loss: 0.00000947
Iteration 124/1000 | Loss: 0.00000947
Iteration 125/1000 | Loss: 0.00000947
Iteration 126/1000 | Loss: 0.00000946
Iteration 127/1000 | Loss: 0.00000946
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000945
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000944
Iteration 139/1000 | Loss: 0.00000944
Iteration 140/1000 | Loss: 0.00000944
Iteration 141/1000 | Loss: 0.00000944
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000942
Iteration 146/1000 | Loss: 0.00000942
Iteration 147/1000 | Loss: 0.00000942
Iteration 148/1000 | Loss: 0.00000941
Iteration 149/1000 | Loss: 0.00000940
Iteration 150/1000 | Loss: 0.00000940
Iteration 151/1000 | Loss: 0.00000940
Iteration 152/1000 | Loss: 0.00000939
Iteration 153/1000 | Loss: 0.00000939
Iteration 154/1000 | Loss: 0.00000939
Iteration 155/1000 | Loss: 0.00000939
Iteration 156/1000 | Loss: 0.00000939
Iteration 157/1000 | Loss: 0.00000939
Iteration 158/1000 | Loss: 0.00000939
Iteration 159/1000 | Loss: 0.00000939
Iteration 160/1000 | Loss: 0.00000939
Iteration 161/1000 | Loss: 0.00000939
Iteration 162/1000 | Loss: 0.00000938
Iteration 163/1000 | Loss: 0.00000938
Iteration 164/1000 | Loss: 0.00000938
Iteration 165/1000 | Loss: 0.00000938
Iteration 166/1000 | Loss: 0.00000938
Iteration 167/1000 | Loss: 0.00000938
Iteration 168/1000 | Loss: 0.00000938
Iteration 169/1000 | Loss: 0.00000938
Iteration 170/1000 | Loss: 0.00000938
Iteration 171/1000 | Loss: 0.00000937
Iteration 172/1000 | Loss: 0.00000937
Iteration 173/1000 | Loss: 0.00000937
Iteration 174/1000 | Loss: 0.00000937
Iteration 175/1000 | Loss: 0.00000937
Iteration 176/1000 | Loss: 0.00000937
Iteration 177/1000 | Loss: 0.00000937
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000937
Iteration 181/1000 | Loss: 0.00000937
Iteration 182/1000 | Loss: 0.00000937
Iteration 183/1000 | Loss: 0.00000937
Iteration 184/1000 | Loss: 0.00000937
Iteration 185/1000 | Loss: 0.00000936
Iteration 186/1000 | Loss: 0.00000936
Iteration 187/1000 | Loss: 0.00000936
Iteration 188/1000 | Loss: 0.00000936
Iteration 189/1000 | Loss: 0.00000936
Iteration 190/1000 | Loss: 0.00000936
Iteration 191/1000 | Loss: 0.00000936
Iteration 192/1000 | Loss: 0.00000936
Iteration 193/1000 | Loss: 0.00000936
Iteration 194/1000 | Loss: 0.00000936
Iteration 195/1000 | Loss: 0.00000936
Iteration 196/1000 | Loss: 0.00000936
Iteration 197/1000 | Loss: 0.00000936
Iteration 198/1000 | Loss: 0.00000936
Iteration 199/1000 | Loss: 0.00000935
Iteration 200/1000 | Loss: 0.00000935
Iteration 201/1000 | Loss: 0.00000935
Iteration 202/1000 | Loss: 0.00000935
Iteration 203/1000 | Loss: 0.00000935
Iteration 204/1000 | Loss: 0.00000935
Iteration 205/1000 | Loss: 0.00000935
Iteration 206/1000 | Loss: 0.00000935
Iteration 207/1000 | Loss: 0.00000935
Iteration 208/1000 | Loss: 0.00000935
Iteration 209/1000 | Loss: 0.00000935
Iteration 210/1000 | Loss: 0.00000935
Iteration 211/1000 | Loss: 0.00000935
Iteration 212/1000 | Loss: 0.00000935
Iteration 213/1000 | Loss: 0.00000934
Iteration 214/1000 | Loss: 0.00000934
Iteration 215/1000 | Loss: 0.00000934
Iteration 216/1000 | Loss: 0.00000934
Iteration 217/1000 | Loss: 0.00000934
Iteration 218/1000 | Loss: 0.00000934
Iteration 219/1000 | Loss: 0.00000934
Iteration 220/1000 | Loss: 0.00000934
Iteration 221/1000 | Loss: 0.00000934
Iteration 222/1000 | Loss: 0.00000934
Iteration 223/1000 | Loss: 0.00000934
Iteration 224/1000 | Loss: 0.00000934
Iteration 225/1000 | Loss: 0.00000934
Iteration 226/1000 | Loss: 0.00000934
Iteration 227/1000 | Loss: 0.00000934
Iteration 228/1000 | Loss: 0.00000934
Iteration 229/1000 | Loss: 0.00000934
Iteration 230/1000 | Loss: 0.00000934
Iteration 231/1000 | Loss: 0.00000933
Iteration 232/1000 | Loss: 0.00000933
Iteration 233/1000 | Loss: 0.00000933
Iteration 234/1000 | Loss: 0.00000933
Iteration 235/1000 | Loss: 0.00000933
Iteration 236/1000 | Loss: 0.00000933
Iteration 237/1000 | Loss: 0.00000933
Iteration 238/1000 | Loss: 0.00000933
Iteration 239/1000 | Loss: 0.00000933
Iteration 240/1000 | Loss: 0.00000933
Iteration 241/1000 | Loss: 0.00000933
Iteration 242/1000 | Loss: 0.00000933
Iteration 243/1000 | Loss: 0.00000933
Iteration 244/1000 | Loss: 0.00000933
Iteration 245/1000 | Loss: 0.00000933
Iteration 246/1000 | Loss: 0.00000933
Iteration 247/1000 | Loss: 0.00000933
Iteration 248/1000 | Loss: 0.00000933
Iteration 249/1000 | Loss: 0.00000933
Iteration 250/1000 | Loss: 0.00000932
Iteration 251/1000 | Loss: 0.00000932
Iteration 252/1000 | Loss: 0.00000932
Iteration 253/1000 | Loss: 0.00000932
Iteration 254/1000 | Loss: 0.00000932
Iteration 255/1000 | Loss: 0.00000932
Iteration 256/1000 | Loss: 0.00000932
Iteration 257/1000 | Loss: 0.00000932
Iteration 258/1000 | Loss: 0.00000932
Iteration 259/1000 | Loss: 0.00000932
Iteration 260/1000 | Loss: 0.00000932
Iteration 261/1000 | Loss: 0.00000932
Iteration 262/1000 | Loss: 0.00000932
Iteration 263/1000 | Loss: 0.00000932
Iteration 264/1000 | Loss: 0.00000932
Iteration 265/1000 | Loss: 0.00000932
Iteration 266/1000 | Loss: 0.00000932
Iteration 267/1000 | Loss: 0.00000932
Iteration 268/1000 | Loss: 0.00000932
Iteration 269/1000 | Loss: 0.00000932
Iteration 270/1000 | Loss: 0.00000932
Iteration 271/1000 | Loss: 0.00000932
Iteration 272/1000 | Loss: 0.00000932
Iteration 273/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [9.318669071944896e-06, 9.318669071944896e-06, 9.318669071944896e-06, 9.318669071944896e-06, 9.318669071944896e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.318669071944896e-06

Optimization complete. Final v2v error: 2.586747407913208 mm

Highest mean error: 3.434152126312256 mm for frame 71

Lowest mean error: 2.4375877380371094 mm for frame 165

Saving results

Total time: 40.623631954193115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830249
Iteration 2/25 | Loss: 0.00105373
Iteration 3/25 | Loss: 0.00072095
Iteration 4/25 | Loss: 0.00068972
Iteration 5/25 | Loss: 0.00068216
Iteration 6/25 | Loss: 0.00068048
Iteration 7/25 | Loss: 0.00068030
Iteration 8/25 | Loss: 0.00068030
Iteration 9/25 | Loss: 0.00068030
Iteration 10/25 | Loss: 0.00068030
Iteration 11/25 | Loss: 0.00068030
Iteration 12/25 | Loss: 0.00068030
Iteration 13/25 | Loss: 0.00068030
Iteration 14/25 | Loss: 0.00068030
Iteration 15/25 | Loss: 0.00068030
Iteration 16/25 | Loss: 0.00068030
Iteration 17/25 | Loss: 0.00068030
Iteration 18/25 | Loss: 0.00068030
Iteration 19/25 | Loss: 0.00068030
Iteration 20/25 | Loss: 0.00068030
Iteration 21/25 | Loss: 0.00068030
Iteration 22/25 | Loss: 0.00068030
Iteration 23/25 | Loss: 0.00068030
Iteration 24/25 | Loss: 0.00068030
Iteration 25/25 | Loss: 0.00068030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27226138
Iteration 2/25 | Loss: 0.00027768
Iteration 3/25 | Loss: 0.00027767
Iteration 4/25 | Loss: 0.00027767
Iteration 5/25 | Loss: 0.00027767
Iteration 6/25 | Loss: 0.00027767
Iteration 7/25 | Loss: 0.00027767
Iteration 8/25 | Loss: 0.00027767
Iteration 9/25 | Loss: 0.00027767
Iteration 10/25 | Loss: 0.00027767
Iteration 11/25 | Loss: 0.00027767
Iteration 12/25 | Loss: 0.00027767
Iteration 13/25 | Loss: 0.00027767
Iteration 14/25 | Loss: 0.00027767
Iteration 15/25 | Loss: 0.00027767
Iteration 16/25 | Loss: 0.00027767
Iteration 17/25 | Loss: 0.00027767
Iteration 18/25 | Loss: 0.00027767
Iteration 19/25 | Loss: 0.00027767
Iteration 20/25 | Loss: 0.00027767
Iteration 21/25 | Loss: 0.00027767
Iteration 22/25 | Loss: 0.00027767
Iteration 23/25 | Loss: 0.00027767
Iteration 24/25 | Loss: 0.00027767
Iteration 25/25 | Loss: 0.00027767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027767
Iteration 2/1000 | Loss: 0.00003637
Iteration 3/1000 | Loss: 0.00002410
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001976
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001828
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001695
Iteration 14/1000 | Loss: 0.00001694
Iteration 15/1000 | Loss: 0.00001685
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001656
Iteration 29/1000 | Loss: 0.00001655
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001654
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001645
Iteration 54/1000 | Loss: 0.00001645
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001642
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00001642
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001641
Iteration 71/1000 | Loss: 0.00001641
Iteration 72/1000 | Loss: 0.00001641
Iteration 73/1000 | Loss: 0.00001641
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001640
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001640
Iteration 79/1000 | Loss: 0.00001640
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001639
Iteration 82/1000 | Loss: 0.00001639
Iteration 83/1000 | Loss: 0.00001639
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001638
Iteration 93/1000 | Loss: 0.00001637
Iteration 94/1000 | Loss: 0.00001637
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001634
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001633
Iteration 105/1000 | Loss: 0.00001633
Iteration 106/1000 | Loss: 0.00001633
Iteration 107/1000 | Loss: 0.00001632
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001630
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001627
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001626
Iteration 117/1000 | Loss: 0.00001626
Iteration 118/1000 | Loss: 0.00001626
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001625
Iteration 121/1000 | Loss: 0.00001625
Iteration 122/1000 | Loss: 0.00001625
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001622
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001621
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001621
Iteration 149/1000 | Loss: 0.00001621
Iteration 150/1000 | Loss: 0.00001621
Iteration 151/1000 | Loss: 0.00001621
Iteration 152/1000 | Loss: 0.00001621
Iteration 153/1000 | Loss: 0.00001621
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001620
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001620
Iteration 161/1000 | Loss: 0.00001620
Iteration 162/1000 | Loss: 0.00001620
Iteration 163/1000 | Loss: 0.00001620
Iteration 164/1000 | Loss: 0.00001620
Iteration 165/1000 | Loss: 0.00001620
Iteration 166/1000 | Loss: 0.00001620
Iteration 167/1000 | Loss: 0.00001620
Iteration 168/1000 | Loss: 0.00001620
Iteration 169/1000 | Loss: 0.00001620
Iteration 170/1000 | Loss: 0.00001620
Iteration 171/1000 | Loss: 0.00001620
Iteration 172/1000 | Loss: 0.00001620
Iteration 173/1000 | Loss: 0.00001619
Iteration 174/1000 | Loss: 0.00001619
Iteration 175/1000 | Loss: 0.00001619
Iteration 176/1000 | Loss: 0.00001619
Iteration 177/1000 | Loss: 0.00001619
Iteration 178/1000 | Loss: 0.00001619
Iteration 179/1000 | Loss: 0.00001619
Iteration 180/1000 | Loss: 0.00001619
Iteration 181/1000 | Loss: 0.00001619
Iteration 182/1000 | Loss: 0.00001619
Iteration 183/1000 | Loss: 0.00001619
Iteration 184/1000 | Loss: 0.00001619
Iteration 185/1000 | Loss: 0.00001619
Iteration 186/1000 | Loss: 0.00001619
Iteration 187/1000 | Loss: 0.00001619
Iteration 188/1000 | Loss: 0.00001619
Iteration 189/1000 | Loss: 0.00001619
Iteration 190/1000 | Loss: 0.00001619
Iteration 191/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6187483197427355e-05, 1.6187483197427355e-05, 1.6187483197427355e-05, 1.6187483197427355e-05, 1.6187483197427355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6187483197427355e-05

Optimization complete. Final v2v error: 3.3368940353393555 mm

Highest mean error: 4.907613754272461 mm for frame 63

Lowest mean error: 2.7526512145996094 mm for frame 89

Saving results

Total time: 42.35091018676758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054216
Iteration 2/25 | Loss: 0.00216032
Iteration 3/25 | Loss: 0.00167184
Iteration 4/25 | Loss: 0.00151421
Iteration 5/25 | Loss: 0.00145328
Iteration 6/25 | Loss: 0.00112097
Iteration 7/25 | Loss: 0.00089826
Iteration 8/25 | Loss: 0.00086321
Iteration 9/25 | Loss: 0.00085511
Iteration 10/25 | Loss: 0.00085446
Iteration 11/25 | Loss: 0.00085446
Iteration 12/25 | Loss: 0.00085446
Iteration 13/25 | Loss: 0.00085446
Iteration 14/25 | Loss: 0.00085446
Iteration 15/25 | Loss: 0.00085446
Iteration 16/25 | Loss: 0.00085446
Iteration 17/25 | Loss: 0.00085446
Iteration 18/25 | Loss: 0.00085446
Iteration 19/25 | Loss: 0.00085446
Iteration 20/25 | Loss: 0.00085446
Iteration 21/25 | Loss: 0.00085446
Iteration 22/25 | Loss: 0.00085446
Iteration 23/25 | Loss: 0.00085446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008544569718651474, 0.0008544569718651474, 0.0008544569718651474, 0.0008544569718651474, 0.0008544569718651474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008544569718651474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47178602
Iteration 2/25 | Loss: 0.00029334
Iteration 3/25 | Loss: 0.00029333
Iteration 4/25 | Loss: 0.00029333
Iteration 5/25 | Loss: 0.00029333
Iteration 6/25 | Loss: 0.00029333
Iteration 7/25 | Loss: 0.00029333
Iteration 8/25 | Loss: 0.00029333
Iteration 9/25 | Loss: 0.00029333
Iteration 10/25 | Loss: 0.00029333
Iteration 11/25 | Loss: 0.00029333
Iteration 12/25 | Loss: 0.00029333
Iteration 13/25 | Loss: 0.00029333
Iteration 14/25 | Loss: 0.00029333
Iteration 15/25 | Loss: 0.00029333
Iteration 16/25 | Loss: 0.00029333
Iteration 17/25 | Loss: 0.00029333
Iteration 18/25 | Loss: 0.00029333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00029333046404644847, 0.00029333046404644847, 0.00029333046404644847, 0.00029333046404644847, 0.00029333046404644847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029333046404644847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029333
Iteration 2/1000 | Loss: 0.00003206
Iteration 3/1000 | Loss: 0.00002690
Iteration 4/1000 | Loss: 0.00002559
Iteration 5/1000 | Loss: 0.00002459
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002379
Iteration 8/1000 | Loss: 0.00002358
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00002358
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002349
Iteration 14/1000 | Loss: 0.00002348
Iteration 15/1000 | Loss: 0.00002348
Iteration 16/1000 | Loss: 0.00002346
Iteration 17/1000 | Loss: 0.00002346
Iteration 18/1000 | Loss: 0.00002346
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00002345
Iteration 23/1000 | Loss: 0.00002345
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002345
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002344
Iteration 28/1000 | Loss: 0.00002344
Iteration 29/1000 | Loss: 0.00002344
Iteration 30/1000 | Loss: 0.00002343
Iteration 31/1000 | Loss: 0.00002343
Iteration 32/1000 | Loss: 0.00002343
Iteration 33/1000 | Loss: 0.00002342
Iteration 34/1000 | Loss: 0.00002341
Iteration 35/1000 | Loss: 0.00002340
Iteration 36/1000 | Loss: 0.00002340
Iteration 37/1000 | Loss: 0.00002340
Iteration 38/1000 | Loss: 0.00002340
Iteration 39/1000 | Loss: 0.00002340
Iteration 40/1000 | Loss: 0.00002339
Iteration 41/1000 | Loss: 0.00002339
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002339
Iteration 45/1000 | Loss: 0.00002339
Iteration 46/1000 | Loss: 0.00002339
Iteration 47/1000 | Loss: 0.00002339
Iteration 48/1000 | Loss: 0.00002338
Iteration 49/1000 | Loss: 0.00002338
Iteration 50/1000 | Loss: 0.00002338
Iteration 51/1000 | Loss: 0.00002338
Iteration 52/1000 | Loss: 0.00002338
Iteration 53/1000 | Loss: 0.00002338
Iteration 54/1000 | Loss: 0.00002338
Iteration 55/1000 | Loss: 0.00002338
Iteration 56/1000 | Loss: 0.00002338
Iteration 57/1000 | Loss: 0.00002338
Iteration 58/1000 | Loss: 0.00002337
Iteration 59/1000 | Loss: 0.00002337
Iteration 60/1000 | Loss: 0.00002337
Iteration 61/1000 | Loss: 0.00002337
Iteration 62/1000 | Loss: 0.00002337
Iteration 63/1000 | Loss: 0.00002337
Iteration 64/1000 | Loss: 0.00002337
Iteration 65/1000 | Loss: 0.00002337
Iteration 66/1000 | Loss: 0.00002337
Iteration 67/1000 | Loss: 0.00002337
Iteration 68/1000 | Loss: 0.00002336
Iteration 69/1000 | Loss: 0.00002336
Iteration 70/1000 | Loss: 0.00002336
Iteration 71/1000 | Loss: 0.00002336
Iteration 72/1000 | Loss: 0.00002336
Iteration 73/1000 | Loss: 0.00002336
Iteration 74/1000 | Loss: 0.00002336
Iteration 75/1000 | Loss: 0.00002336
Iteration 76/1000 | Loss: 0.00002336
Iteration 77/1000 | Loss: 0.00002335
Iteration 78/1000 | Loss: 0.00002335
Iteration 79/1000 | Loss: 0.00002335
Iteration 80/1000 | Loss: 0.00002335
Iteration 81/1000 | Loss: 0.00002335
Iteration 82/1000 | Loss: 0.00002335
Iteration 83/1000 | Loss: 0.00002335
Iteration 84/1000 | Loss: 0.00002335
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002335
Iteration 87/1000 | Loss: 0.00002335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.335269164177589e-05, 2.335269164177589e-05, 2.335269164177589e-05, 2.335269164177589e-05, 2.335269164177589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.335269164177589e-05

Optimization complete. Final v2v error: 4.035077095031738 mm

Highest mean error: 4.375504970550537 mm for frame 27

Lowest mean error: 3.8778235912323 mm for frame 8

Saving results

Total time: 37.81552577018738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419837
Iteration 2/25 | Loss: 0.00085718
Iteration 3/25 | Loss: 0.00070188
Iteration 4/25 | Loss: 0.00067251
Iteration 5/25 | Loss: 0.00066200
Iteration 6/25 | Loss: 0.00065886
Iteration 7/25 | Loss: 0.00065796
Iteration 8/25 | Loss: 0.00065794
Iteration 9/25 | Loss: 0.00065794
Iteration 10/25 | Loss: 0.00065794
Iteration 11/25 | Loss: 0.00065794
Iteration 12/25 | Loss: 0.00065794
Iteration 13/25 | Loss: 0.00065794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006579402252100408, 0.0006579402252100408, 0.0006579402252100408, 0.0006579402252100408, 0.0006579402252100408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006579402252100408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46752083
Iteration 2/25 | Loss: 0.00032307
Iteration 3/25 | Loss: 0.00032306
Iteration 4/25 | Loss: 0.00032306
Iteration 5/25 | Loss: 0.00032306
Iteration 6/25 | Loss: 0.00032306
Iteration 7/25 | Loss: 0.00032306
Iteration 8/25 | Loss: 0.00032306
Iteration 9/25 | Loss: 0.00032306
Iteration 10/25 | Loss: 0.00032306
Iteration 11/25 | Loss: 0.00032306
Iteration 12/25 | Loss: 0.00032306
Iteration 13/25 | Loss: 0.00032306
Iteration 14/25 | Loss: 0.00032306
Iteration 15/25 | Loss: 0.00032306
Iteration 16/25 | Loss: 0.00032306
Iteration 17/25 | Loss: 0.00032306
Iteration 18/25 | Loss: 0.00032306
Iteration 19/25 | Loss: 0.00032306
Iteration 20/25 | Loss: 0.00032306
Iteration 21/25 | Loss: 0.00032306
Iteration 22/25 | Loss: 0.00032306
Iteration 23/25 | Loss: 0.00032306
Iteration 24/25 | Loss: 0.00032306
Iteration 25/25 | Loss: 0.00032306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032306
Iteration 2/1000 | Loss: 0.00002394
Iteration 3/1000 | Loss: 0.00001594
Iteration 4/1000 | Loss: 0.00001442
Iteration 5/1000 | Loss: 0.00001355
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001224
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001188
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001178
Iteration 19/1000 | Loss: 0.00001177
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001176
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001170
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001166
Iteration 36/1000 | Loss: 0.00001165
Iteration 37/1000 | Loss: 0.00001165
Iteration 38/1000 | Loss: 0.00001165
Iteration 39/1000 | Loss: 0.00001164
Iteration 40/1000 | Loss: 0.00001164
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001159
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001158
Iteration 59/1000 | Loss: 0.00001158
Iteration 60/1000 | Loss: 0.00001158
Iteration 61/1000 | Loss: 0.00001158
Iteration 62/1000 | Loss: 0.00001158
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001157
Iteration 66/1000 | Loss: 0.00001157
Iteration 67/1000 | Loss: 0.00001157
Iteration 68/1000 | Loss: 0.00001157
Iteration 69/1000 | Loss: 0.00001156
Iteration 70/1000 | Loss: 0.00001156
Iteration 71/1000 | Loss: 0.00001156
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001155
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001155
Iteration 89/1000 | Loss: 0.00001155
Iteration 90/1000 | Loss: 0.00001155
Iteration 91/1000 | Loss: 0.00001155
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001154
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001153
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001153
Iteration 146/1000 | Loss: 0.00001153
Iteration 147/1000 | Loss: 0.00001153
Iteration 148/1000 | Loss: 0.00001153
Iteration 149/1000 | Loss: 0.00001153
Iteration 150/1000 | Loss: 0.00001153
Iteration 151/1000 | Loss: 0.00001153
Iteration 152/1000 | Loss: 0.00001153
Iteration 153/1000 | Loss: 0.00001153
Iteration 154/1000 | Loss: 0.00001153
Iteration 155/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.1526353773660958e-05, 1.1526353773660958e-05, 1.1526353773660958e-05, 1.1526353773660958e-05, 1.1526353773660958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1526353773660958e-05

Optimization complete. Final v2v error: 2.9018967151641846 mm

Highest mean error: 3.786940097808838 mm for frame 61

Lowest mean error: 2.632822275161743 mm for frame 92

Saving results

Total time: 39.62864851951599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461356
Iteration 2/25 | Loss: 0.00105439
Iteration 3/25 | Loss: 0.00070971
Iteration 4/25 | Loss: 0.00068144
Iteration 5/25 | Loss: 0.00067178
Iteration 6/25 | Loss: 0.00066911
Iteration 7/25 | Loss: 0.00066832
Iteration 8/25 | Loss: 0.00066824
Iteration 9/25 | Loss: 0.00066824
Iteration 10/25 | Loss: 0.00066824
Iteration 11/25 | Loss: 0.00066824
Iteration 12/25 | Loss: 0.00066824
Iteration 13/25 | Loss: 0.00066824
Iteration 14/25 | Loss: 0.00066824
Iteration 15/25 | Loss: 0.00066824
Iteration 16/25 | Loss: 0.00066824
Iteration 17/25 | Loss: 0.00066824
Iteration 18/25 | Loss: 0.00066824
Iteration 19/25 | Loss: 0.00066824
Iteration 20/25 | Loss: 0.00066824
Iteration 21/25 | Loss: 0.00066824
Iteration 22/25 | Loss: 0.00066824
Iteration 23/25 | Loss: 0.00066824
Iteration 24/25 | Loss: 0.00066824
Iteration 25/25 | Loss: 0.00066824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56521857
Iteration 2/25 | Loss: 0.00026721
Iteration 3/25 | Loss: 0.00026721
Iteration 4/25 | Loss: 0.00026721
Iteration 5/25 | Loss: 0.00026721
Iteration 6/25 | Loss: 0.00026721
Iteration 7/25 | Loss: 0.00026721
Iteration 8/25 | Loss: 0.00026721
Iteration 9/25 | Loss: 0.00026721
Iteration 10/25 | Loss: 0.00026721
Iteration 11/25 | Loss: 0.00026721
Iteration 12/25 | Loss: 0.00026721
Iteration 13/25 | Loss: 0.00026721
Iteration 14/25 | Loss: 0.00026721
Iteration 15/25 | Loss: 0.00026721
Iteration 16/25 | Loss: 0.00026721
Iteration 17/25 | Loss: 0.00026721
Iteration 18/25 | Loss: 0.00026721
Iteration 19/25 | Loss: 0.00026721
Iteration 20/25 | Loss: 0.00026721
Iteration 21/25 | Loss: 0.00026721
Iteration 22/25 | Loss: 0.00026721
Iteration 23/25 | Loss: 0.00026721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000267207738943398, 0.000267207738943398, 0.000267207738943398, 0.000267207738943398, 0.000267207738943398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000267207738943398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026721
Iteration 2/1000 | Loss: 0.00002762
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001531
Iteration 6/1000 | Loss: 0.00001479
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001412
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001371
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001367
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001354
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001337
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001337
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001334
Iteration 93/1000 | Loss: 0.00001334
Iteration 94/1000 | Loss: 0.00001334
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001333
Iteration 99/1000 | Loss: 0.00001333
Iteration 100/1000 | Loss: 0.00001333
Iteration 101/1000 | Loss: 0.00001333
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001331
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001331
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001324
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00001324
Iteration 174/1000 | Loss: 0.00001323
Iteration 175/1000 | Loss: 0.00001323
Iteration 176/1000 | Loss: 0.00001323
Iteration 177/1000 | Loss: 0.00001323
Iteration 178/1000 | Loss: 0.00001323
Iteration 179/1000 | Loss: 0.00001323
Iteration 180/1000 | Loss: 0.00001323
Iteration 181/1000 | Loss: 0.00001323
Iteration 182/1000 | Loss: 0.00001323
Iteration 183/1000 | Loss: 0.00001323
Iteration 184/1000 | Loss: 0.00001323
Iteration 185/1000 | Loss: 0.00001323
Iteration 186/1000 | Loss: 0.00001323
Iteration 187/1000 | Loss: 0.00001323
Iteration 188/1000 | Loss: 0.00001323
Iteration 189/1000 | Loss: 0.00001323
Iteration 190/1000 | Loss: 0.00001322
Iteration 191/1000 | Loss: 0.00001322
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001322
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001322
Iteration 196/1000 | Loss: 0.00001322
Iteration 197/1000 | Loss: 0.00001322
Iteration 198/1000 | Loss: 0.00001322
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001322
Iteration 201/1000 | Loss: 0.00001322
Iteration 202/1000 | Loss: 0.00001322
Iteration 203/1000 | Loss: 0.00001322
Iteration 204/1000 | Loss: 0.00001322
Iteration 205/1000 | Loss: 0.00001322
Iteration 206/1000 | Loss: 0.00001322
Iteration 207/1000 | Loss: 0.00001322
Iteration 208/1000 | Loss: 0.00001322
Iteration 209/1000 | Loss: 0.00001322
Iteration 210/1000 | Loss: 0.00001322
Iteration 211/1000 | Loss: 0.00001322
Iteration 212/1000 | Loss: 0.00001322
Iteration 213/1000 | Loss: 0.00001322
Iteration 214/1000 | Loss: 0.00001322
Iteration 215/1000 | Loss: 0.00001322
Iteration 216/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.3215849321568385e-05, 1.3215849321568385e-05, 1.3215849321568385e-05, 1.3215849321568385e-05, 1.3215849321568385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3215849321568385e-05

Optimization complete. Final v2v error: 2.9955646991729736 mm

Highest mean error: 3.6968846321105957 mm for frame 22

Lowest mean error: 2.4411463737487793 mm for frame 107

Saving results

Total time: 40.61464071273804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794648
Iteration 2/25 | Loss: 0.00143075
Iteration 3/25 | Loss: 0.00093034
Iteration 4/25 | Loss: 0.00088082
Iteration 5/25 | Loss: 0.00081518
Iteration 6/25 | Loss: 0.00093459
Iteration 7/25 | Loss: 0.00083164
Iteration 8/25 | Loss: 0.00083990
Iteration 9/25 | Loss: 0.00077031
Iteration 10/25 | Loss: 0.00074841
Iteration 11/25 | Loss: 0.00074417
Iteration 12/25 | Loss: 0.00073552
Iteration 13/25 | Loss: 0.00072998
Iteration 14/25 | Loss: 0.00072307
Iteration 15/25 | Loss: 0.00071674
Iteration 16/25 | Loss: 0.00071229
Iteration 17/25 | Loss: 0.00071086
Iteration 18/25 | Loss: 0.00070992
Iteration 19/25 | Loss: 0.00071195
Iteration 20/25 | Loss: 0.00071324
Iteration 21/25 | Loss: 0.00070641
Iteration 22/25 | Loss: 0.00070120
Iteration 23/25 | Loss: 0.00069933
Iteration 24/25 | Loss: 0.00069812
Iteration 25/25 | Loss: 0.00069743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73042274
Iteration 2/25 | Loss: 0.00040014
Iteration 3/25 | Loss: 0.00040014
Iteration 4/25 | Loss: 0.00040014
Iteration 5/25 | Loss: 0.00040014
Iteration 6/25 | Loss: 0.00040014
Iteration 7/25 | Loss: 0.00040014
Iteration 8/25 | Loss: 0.00040014
Iteration 9/25 | Loss: 0.00040014
Iteration 10/25 | Loss: 0.00040014
Iteration 11/25 | Loss: 0.00040014
Iteration 12/25 | Loss: 0.00040014
Iteration 13/25 | Loss: 0.00040014
Iteration 14/25 | Loss: 0.00040014
Iteration 15/25 | Loss: 0.00040014
Iteration 16/25 | Loss: 0.00040014
Iteration 17/25 | Loss: 0.00040014
Iteration 18/25 | Loss: 0.00040014
Iteration 19/25 | Loss: 0.00040014
Iteration 20/25 | Loss: 0.00040014
Iteration 21/25 | Loss: 0.00040014
Iteration 22/25 | Loss: 0.00040014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004001357883680612, 0.0004001357883680612, 0.0004001357883680612, 0.0004001357883680612, 0.0004001357883680612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004001357883680612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040014
Iteration 2/1000 | Loss: 0.00003326
Iteration 3/1000 | Loss: 0.00020258
Iteration 4/1000 | Loss: 0.00010302
Iteration 5/1000 | Loss: 0.00015625
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001643
Iteration 10/1000 | Loss: 0.00001581
Iteration 11/1000 | Loss: 0.00001541
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001434
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001429
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001428
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001412
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001402
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001399
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001395
Iteration 64/1000 | Loss: 0.00001395
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001395
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001394
Iteration 70/1000 | Loss: 0.00001393
Iteration 71/1000 | Loss: 0.00001393
Iteration 72/1000 | Loss: 0.00001393
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001388
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001387
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001386
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001385
Iteration 119/1000 | Loss: 0.00001385
Iteration 120/1000 | Loss: 0.00001385
Iteration 121/1000 | Loss: 0.00001385
Iteration 122/1000 | Loss: 0.00001385
Iteration 123/1000 | Loss: 0.00001384
Iteration 124/1000 | Loss: 0.00001384
Iteration 125/1000 | Loss: 0.00001384
Iteration 126/1000 | Loss: 0.00001384
Iteration 127/1000 | Loss: 0.00001383
Iteration 128/1000 | Loss: 0.00001383
Iteration 129/1000 | Loss: 0.00001383
Iteration 130/1000 | Loss: 0.00001383
Iteration 131/1000 | Loss: 0.00001383
Iteration 132/1000 | Loss: 0.00001382
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001382
Iteration 136/1000 | Loss: 0.00001382
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001380
Iteration 160/1000 | Loss: 0.00001380
Iteration 161/1000 | Loss: 0.00001380
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001379
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001377
Iteration 177/1000 | Loss: 0.00001377
Iteration 178/1000 | Loss: 0.00001377
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001376
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001375
Iteration 183/1000 | Loss: 0.00001375
Iteration 184/1000 | Loss: 0.00001375
Iteration 185/1000 | Loss: 0.00001375
Iteration 186/1000 | Loss: 0.00001375
Iteration 187/1000 | Loss: 0.00001374
Iteration 188/1000 | Loss: 0.00001374
Iteration 189/1000 | Loss: 0.00001374
Iteration 190/1000 | Loss: 0.00001374
Iteration 191/1000 | Loss: 0.00001374
Iteration 192/1000 | Loss: 0.00001374
Iteration 193/1000 | Loss: 0.00001373
Iteration 194/1000 | Loss: 0.00001373
Iteration 195/1000 | Loss: 0.00001373
Iteration 196/1000 | Loss: 0.00001373
Iteration 197/1000 | Loss: 0.00001373
Iteration 198/1000 | Loss: 0.00001373
Iteration 199/1000 | Loss: 0.00001373
Iteration 200/1000 | Loss: 0.00001373
Iteration 201/1000 | Loss: 0.00001373
Iteration 202/1000 | Loss: 0.00001373
Iteration 203/1000 | Loss: 0.00001373
Iteration 204/1000 | Loss: 0.00001372
Iteration 205/1000 | Loss: 0.00001372
Iteration 206/1000 | Loss: 0.00001372
Iteration 207/1000 | Loss: 0.00001372
Iteration 208/1000 | Loss: 0.00001372
Iteration 209/1000 | Loss: 0.00001372
Iteration 210/1000 | Loss: 0.00001372
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001372
Iteration 213/1000 | Loss: 0.00001372
Iteration 214/1000 | Loss: 0.00001372
Iteration 215/1000 | Loss: 0.00001372
Iteration 216/1000 | Loss: 0.00001372
Iteration 217/1000 | Loss: 0.00001372
Iteration 218/1000 | Loss: 0.00001372
Iteration 219/1000 | Loss: 0.00001372
Iteration 220/1000 | Loss: 0.00001372
Iteration 221/1000 | Loss: 0.00001372
Iteration 222/1000 | Loss: 0.00001372
Iteration 223/1000 | Loss: 0.00001372
Iteration 224/1000 | Loss: 0.00001372
Iteration 225/1000 | Loss: 0.00001371
Iteration 226/1000 | Loss: 0.00001371
Iteration 227/1000 | Loss: 0.00001371
Iteration 228/1000 | Loss: 0.00001371
Iteration 229/1000 | Loss: 0.00001371
Iteration 230/1000 | Loss: 0.00001371
Iteration 231/1000 | Loss: 0.00001371
Iteration 232/1000 | Loss: 0.00001371
Iteration 233/1000 | Loss: 0.00001371
Iteration 234/1000 | Loss: 0.00001371
Iteration 235/1000 | Loss: 0.00001371
Iteration 236/1000 | Loss: 0.00001371
Iteration 237/1000 | Loss: 0.00001371
Iteration 238/1000 | Loss: 0.00001371
Iteration 239/1000 | Loss: 0.00001371
Iteration 240/1000 | Loss: 0.00001371
Iteration 241/1000 | Loss: 0.00001371
Iteration 242/1000 | Loss: 0.00001371
Iteration 243/1000 | Loss: 0.00001371
Iteration 244/1000 | Loss: 0.00001371
Iteration 245/1000 | Loss: 0.00001371
Iteration 246/1000 | Loss: 0.00001371
Iteration 247/1000 | Loss: 0.00001371
Iteration 248/1000 | Loss: 0.00001371
Iteration 249/1000 | Loss: 0.00001371
Iteration 250/1000 | Loss: 0.00001371
Iteration 251/1000 | Loss: 0.00001371
Iteration 252/1000 | Loss: 0.00001371
Iteration 253/1000 | Loss: 0.00001371
Iteration 254/1000 | Loss: 0.00001371
Iteration 255/1000 | Loss: 0.00001371
Iteration 256/1000 | Loss: 0.00001371
Iteration 257/1000 | Loss: 0.00001371
Iteration 258/1000 | Loss: 0.00001371
Iteration 259/1000 | Loss: 0.00001371
Iteration 260/1000 | Loss: 0.00001371
Iteration 261/1000 | Loss: 0.00001371
Iteration 262/1000 | Loss: 0.00001371
Iteration 263/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.3708197911910247e-05, 1.3708197911910247e-05, 1.3708197911910247e-05, 1.3708197911910247e-05, 1.3708197911910247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3708197911910247e-05

Optimization complete. Final v2v error: 3.0599193572998047 mm

Highest mean error: 4.5033087730407715 mm for frame 72

Lowest mean error: 2.41494083404541 mm for frame 235

Saving results

Total time: 96.95086669921875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093828
Iteration 2/25 | Loss: 0.01093828
Iteration 3/25 | Loss: 0.01093827
Iteration 4/25 | Loss: 0.01093827
Iteration 5/25 | Loss: 0.01093827
Iteration 6/25 | Loss: 0.01093827
Iteration 7/25 | Loss: 0.01093827
Iteration 8/25 | Loss: 0.01093827
Iteration 9/25 | Loss: 0.01093827
Iteration 10/25 | Loss: 0.01093827
Iteration 11/25 | Loss: 0.01093826
Iteration 12/25 | Loss: 0.01093826
Iteration 13/25 | Loss: 0.01093826
Iteration 14/25 | Loss: 0.01093826
Iteration 15/25 | Loss: 0.01093826
Iteration 16/25 | Loss: 0.01093826
Iteration 17/25 | Loss: 0.01093826
Iteration 18/25 | Loss: 0.01093826
Iteration 19/25 | Loss: 0.01093826
Iteration 20/25 | Loss: 0.01093825
Iteration 21/25 | Loss: 0.01093825
Iteration 22/25 | Loss: 0.01093825
Iteration 23/25 | Loss: 0.01093825
Iteration 24/25 | Loss: 0.01093825
Iteration 25/25 | Loss: 0.01093825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82180166
Iteration 2/25 | Loss: 0.08483318
Iteration 3/25 | Loss: 0.08430769
Iteration 4/25 | Loss: 0.08426225
Iteration 5/25 | Loss: 0.08423068
Iteration 6/25 | Loss: 0.08423065
Iteration 7/25 | Loss: 0.08423064
Iteration 8/25 | Loss: 0.08423064
Iteration 9/25 | Loss: 0.08423064
Iteration 10/25 | Loss: 0.08423064
Iteration 11/25 | Loss: 0.08423064
Iteration 12/25 | Loss: 0.08423064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08423063904047012, 0.08423063904047012, 0.08423063904047012, 0.08423063904047012, 0.08423063904047012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08423063904047012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08423064
Iteration 2/1000 | Loss: 0.00430161
Iteration 3/1000 | Loss: 0.00556588
Iteration 4/1000 | Loss: 0.00178509
Iteration 5/1000 | Loss: 0.00292540
Iteration 6/1000 | Loss: 0.00058465
Iteration 7/1000 | Loss: 0.00023497
Iteration 8/1000 | Loss: 0.00049448
Iteration 9/1000 | Loss: 0.00006837
Iteration 10/1000 | Loss: 0.00008715
Iteration 11/1000 | Loss: 0.00037959
Iteration 12/1000 | Loss: 0.00088595
Iteration 13/1000 | Loss: 0.00006318
Iteration 14/1000 | Loss: 0.00004452
Iteration 15/1000 | Loss: 0.00004896
Iteration 16/1000 | Loss: 0.00008775
Iteration 17/1000 | Loss: 0.00037151
Iteration 18/1000 | Loss: 0.00002922
Iteration 19/1000 | Loss: 0.00017523
Iteration 20/1000 | Loss: 0.00085169
Iteration 21/1000 | Loss: 0.00053490
Iteration 22/1000 | Loss: 0.00007763
Iteration 23/1000 | Loss: 0.00023553
Iteration 24/1000 | Loss: 0.00010993
Iteration 25/1000 | Loss: 0.00014956
Iteration 26/1000 | Loss: 0.00043188
Iteration 27/1000 | Loss: 0.00086080
Iteration 28/1000 | Loss: 0.00041859
Iteration 29/1000 | Loss: 0.00004698
Iteration 30/1000 | Loss: 0.00003943
Iteration 31/1000 | Loss: 0.00002687
Iteration 32/1000 | Loss: 0.00002430
Iteration 33/1000 | Loss: 0.00004765
Iteration 34/1000 | Loss: 0.00002775
Iteration 35/1000 | Loss: 0.00001813
Iteration 36/1000 | Loss: 0.00019876
Iteration 37/1000 | Loss: 0.00003337
Iteration 38/1000 | Loss: 0.00005540
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00009675
Iteration 41/1000 | Loss: 0.00003958
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00028172
Iteration 45/1000 | Loss: 0.00003268
Iteration 46/1000 | Loss: 0.00004124
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00003003
Iteration 49/1000 | Loss: 0.00002395
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00010188
Iteration 52/1000 | Loss: 0.00067526
Iteration 53/1000 | Loss: 0.00003108
Iteration 54/1000 | Loss: 0.00011443
Iteration 55/1000 | Loss: 0.00003426
Iteration 56/1000 | Loss: 0.00002068
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00002717
Iteration 59/1000 | Loss: 0.00014302
Iteration 60/1000 | Loss: 0.00002148
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00016245
Iteration 69/1000 | Loss: 0.00018370
Iteration 70/1000 | Loss: 0.00002171
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001254
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001254
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001250
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00002923
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001246
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001245
Iteration 98/1000 | Loss: 0.00001245
Iteration 99/1000 | Loss: 0.00001245
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001244
Iteration 105/1000 | Loss: 0.00001244
Iteration 106/1000 | Loss: 0.00001244
Iteration 107/1000 | Loss: 0.00001244
Iteration 108/1000 | Loss: 0.00001244
Iteration 109/1000 | Loss: 0.00001244
Iteration 110/1000 | Loss: 0.00001244
Iteration 111/1000 | Loss: 0.00001244
Iteration 112/1000 | Loss: 0.00001244
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001243
Iteration 119/1000 | Loss: 0.00001826
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00010133
Iteration 122/1000 | Loss: 0.00001268
Iteration 123/1000 | Loss: 0.00001243
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001238
Iteration 138/1000 | Loss: 0.00001238
Iteration 139/1000 | Loss: 0.00001238
Iteration 140/1000 | Loss: 0.00001745
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001237
Iteration 143/1000 | Loss: 0.00001237
Iteration 144/1000 | Loss: 0.00001237
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001237
Iteration 147/1000 | Loss: 0.00001237
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001235
Iteration 151/1000 | Loss: 0.00001235
Iteration 152/1000 | Loss: 0.00001235
Iteration 153/1000 | Loss: 0.00001235
Iteration 154/1000 | Loss: 0.00001235
Iteration 155/1000 | Loss: 0.00001235
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001235
Iteration 160/1000 | Loss: 0.00001235
Iteration 161/1000 | Loss: 0.00001235
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001234
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001234
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.2342729860392865e-05, 1.2342729860392865e-05, 1.2342729860392865e-05, 1.2342729860392865e-05, 1.2342729860392865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2342729860392865e-05

Optimization complete. Final v2v error: 3.0041980743408203 mm

Highest mean error: 3.8459575176239014 mm for frame 12

Lowest mean error: 2.6080143451690674 mm for frame 239

Saving results

Total time: 131.7425708770752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866651
Iteration 2/25 | Loss: 0.00081709
Iteration 3/25 | Loss: 0.00066340
Iteration 4/25 | Loss: 0.00064055
Iteration 5/25 | Loss: 0.00063693
Iteration 6/25 | Loss: 0.00063682
Iteration 7/25 | Loss: 0.00063682
Iteration 8/25 | Loss: 0.00063682
Iteration 9/25 | Loss: 0.00063682
Iteration 10/25 | Loss: 0.00063682
Iteration 11/25 | Loss: 0.00063682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006368217873387039, 0.0006368217873387039, 0.0006368217873387039, 0.0006368217873387039, 0.0006368217873387039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006368217873387039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.08471584
Iteration 2/25 | Loss: 0.00025852
Iteration 3/25 | Loss: 0.00025852
Iteration 4/25 | Loss: 0.00025852
Iteration 5/25 | Loss: 0.00025852
Iteration 6/25 | Loss: 0.00025851
Iteration 7/25 | Loss: 0.00025851
Iteration 8/25 | Loss: 0.00025851
Iteration 9/25 | Loss: 0.00025851
Iteration 10/25 | Loss: 0.00025851
Iteration 11/25 | Loss: 0.00025851
Iteration 12/25 | Loss: 0.00025851
Iteration 13/25 | Loss: 0.00025851
Iteration 14/25 | Loss: 0.00025851
Iteration 15/25 | Loss: 0.00025851
Iteration 16/25 | Loss: 0.00025851
Iteration 17/25 | Loss: 0.00025851
Iteration 18/25 | Loss: 0.00025851
Iteration 19/25 | Loss: 0.00025851
Iteration 20/25 | Loss: 0.00025851
Iteration 21/25 | Loss: 0.00025851
Iteration 22/25 | Loss: 0.00025851
Iteration 23/25 | Loss: 0.00025851
Iteration 24/25 | Loss: 0.00025851
Iteration 25/25 | Loss: 0.00025851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025851
Iteration 2/1000 | Loss: 0.00001928
Iteration 3/1000 | Loss: 0.00001357
Iteration 4/1000 | Loss: 0.00001215
Iteration 5/1000 | Loss: 0.00001129
Iteration 6/1000 | Loss: 0.00001095
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001051
Iteration 11/1000 | Loss: 0.00001050
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001039
Iteration 14/1000 | Loss: 0.00001034
Iteration 15/1000 | Loss: 0.00001034
Iteration 16/1000 | Loss: 0.00001034
Iteration 17/1000 | Loss: 0.00001034
Iteration 18/1000 | Loss: 0.00001033
Iteration 19/1000 | Loss: 0.00001033
Iteration 20/1000 | Loss: 0.00001033
Iteration 21/1000 | Loss: 0.00001033
Iteration 22/1000 | Loss: 0.00001033
Iteration 23/1000 | Loss: 0.00001033
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001032
Iteration 27/1000 | Loss: 0.00001028
Iteration 28/1000 | Loss: 0.00001028
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001027
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001026
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001025
Iteration 35/1000 | Loss: 0.00001025
Iteration 36/1000 | Loss: 0.00001024
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001023
Iteration 39/1000 | Loss: 0.00001023
Iteration 40/1000 | Loss: 0.00001022
Iteration 41/1000 | Loss: 0.00001022
Iteration 42/1000 | Loss: 0.00001022
Iteration 43/1000 | Loss: 0.00001022
Iteration 44/1000 | Loss: 0.00001022
Iteration 45/1000 | Loss: 0.00001022
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001017
Iteration 53/1000 | Loss: 0.00001017
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001016
Iteration 60/1000 | Loss: 0.00001016
Iteration 61/1000 | Loss: 0.00001016
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001013
Iteration 68/1000 | Loss: 0.00001013
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001010
Iteration 83/1000 | Loss: 0.00001009
Iteration 84/1000 | Loss: 0.00001008
Iteration 85/1000 | Loss: 0.00001008
Iteration 86/1000 | Loss: 0.00001008
Iteration 87/1000 | Loss: 0.00001008
Iteration 88/1000 | Loss: 0.00001007
Iteration 89/1000 | Loss: 0.00001007
Iteration 90/1000 | Loss: 0.00001007
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001006
Iteration 93/1000 | Loss: 0.00001006
Iteration 94/1000 | Loss: 0.00001006
Iteration 95/1000 | Loss: 0.00001006
Iteration 96/1000 | Loss: 0.00001006
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001005
Iteration 101/1000 | Loss: 0.00001005
Iteration 102/1000 | Loss: 0.00001005
Iteration 103/1000 | Loss: 0.00001005
Iteration 104/1000 | Loss: 0.00001004
Iteration 105/1000 | Loss: 0.00001004
Iteration 106/1000 | Loss: 0.00001004
Iteration 107/1000 | Loss: 0.00001004
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001002
Iteration 113/1000 | Loss: 0.00001002
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001002
Iteration 116/1000 | Loss: 0.00001001
Iteration 117/1000 | Loss: 0.00001001
Iteration 118/1000 | Loss: 0.00001001
Iteration 119/1000 | Loss: 0.00001000
Iteration 120/1000 | Loss: 0.00001000
Iteration 121/1000 | Loss: 0.00001000
Iteration 122/1000 | Loss: 0.00000999
Iteration 123/1000 | Loss: 0.00000999
Iteration 124/1000 | Loss: 0.00000999
Iteration 125/1000 | Loss: 0.00000999
Iteration 126/1000 | Loss: 0.00000999
Iteration 127/1000 | Loss: 0.00000999
Iteration 128/1000 | Loss: 0.00000999
Iteration 129/1000 | Loss: 0.00000999
Iteration 130/1000 | Loss: 0.00000998
Iteration 131/1000 | Loss: 0.00000998
Iteration 132/1000 | Loss: 0.00000998
Iteration 133/1000 | Loss: 0.00000998
Iteration 134/1000 | Loss: 0.00000998
Iteration 135/1000 | Loss: 0.00000998
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000997
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000996
Iteration 152/1000 | Loss: 0.00000996
Iteration 153/1000 | Loss: 0.00000996
Iteration 154/1000 | Loss: 0.00000996
Iteration 155/1000 | Loss: 0.00000996
Iteration 156/1000 | Loss: 0.00000996
Iteration 157/1000 | Loss: 0.00000996
Iteration 158/1000 | Loss: 0.00000996
Iteration 159/1000 | Loss: 0.00000996
Iteration 160/1000 | Loss: 0.00000996
Iteration 161/1000 | Loss: 0.00000995
Iteration 162/1000 | Loss: 0.00000995
Iteration 163/1000 | Loss: 0.00000995
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000995
Iteration 166/1000 | Loss: 0.00000995
Iteration 167/1000 | Loss: 0.00000995
Iteration 168/1000 | Loss: 0.00000995
Iteration 169/1000 | Loss: 0.00000995
Iteration 170/1000 | Loss: 0.00000995
Iteration 171/1000 | Loss: 0.00000995
Iteration 172/1000 | Loss: 0.00000995
Iteration 173/1000 | Loss: 0.00000995
Iteration 174/1000 | Loss: 0.00000995
Iteration 175/1000 | Loss: 0.00000995
Iteration 176/1000 | Loss: 0.00000995
Iteration 177/1000 | Loss: 0.00000995
Iteration 178/1000 | Loss: 0.00000995
Iteration 179/1000 | Loss: 0.00000995
Iteration 180/1000 | Loss: 0.00000995
Iteration 181/1000 | Loss: 0.00000995
Iteration 182/1000 | Loss: 0.00000995
Iteration 183/1000 | Loss: 0.00000995
Iteration 184/1000 | Loss: 0.00000995
Iteration 185/1000 | Loss: 0.00000995
Iteration 186/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [9.949958439392503e-06, 9.949958439392503e-06, 9.949958439392503e-06, 9.949958439392503e-06, 9.949958439392503e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.949958439392503e-06

Optimization complete. Final v2v error: 2.689754009246826 mm

Highest mean error: 2.8992760181427 mm for frame 235

Lowest mean error: 2.576524019241333 mm for frame 35

Saving results

Total time: 37.848713397979736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810647
Iteration 2/25 | Loss: 0.00113960
Iteration 3/25 | Loss: 0.00079505
Iteration 4/25 | Loss: 0.00073219
Iteration 5/25 | Loss: 0.00071569
Iteration 6/25 | Loss: 0.00071586
Iteration 7/25 | Loss: 0.00071135
Iteration 8/25 | Loss: 0.00071042
Iteration 9/25 | Loss: 0.00071016
Iteration 10/25 | Loss: 0.00071357
Iteration 11/25 | Loss: 0.00071643
Iteration 12/25 | Loss: 0.00071338
Iteration 13/25 | Loss: 0.00071862
Iteration 14/25 | Loss: 0.00071347
Iteration 15/25 | Loss: 0.00071029
Iteration 16/25 | Loss: 0.00070938
Iteration 17/25 | Loss: 0.00071261
Iteration 18/25 | Loss: 0.00071236
Iteration 19/25 | Loss: 0.00071164
Iteration 20/25 | Loss: 0.00070965
Iteration 21/25 | Loss: 0.00070920
Iteration 22/25 | Loss: 0.00070833
Iteration 23/25 | Loss: 0.00070693
Iteration 24/25 | Loss: 0.00070675
Iteration 25/25 | Loss: 0.00070662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74105048
Iteration 2/25 | Loss: 0.00033163
Iteration 3/25 | Loss: 0.00033162
Iteration 4/25 | Loss: 0.00033162
Iteration 5/25 | Loss: 0.00033162
Iteration 6/25 | Loss: 0.00033162
Iteration 7/25 | Loss: 0.00033162
Iteration 8/25 | Loss: 0.00033162
Iteration 9/25 | Loss: 0.00033162
Iteration 10/25 | Loss: 0.00033162
Iteration 11/25 | Loss: 0.00033162
Iteration 12/25 | Loss: 0.00033162
Iteration 13/25 | Loss: 0.00033162
Iteration 14/25 | Loss: 0.00033162
Iteration 15/25 | Loss: 0.00033162
Iteration 16/25 | Loss: 0.00033162
Iteration 17/25 | Loss: 0.00033162
Iteration 18/25 | Loss: 0.00033162
Iteration 19/25 | Loss: 0.00033162
Iteration 20/25 | Loss: 0.00033162
Iteration 21/25 | Loss: 0.00033162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00033161870669573545, 0.00033161870669573545, 0.00033161870669573545, 0.00033161870669573545, 0.00033161870669573545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033161870669573545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033162
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001846
Iteration 5/1000 | Loss: 0.00001758
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001607
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001594
Iteration 15/1000 | Loss: 0.00001593
Iteration 16/1000 | Loss: 0.00001593
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001588
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001581
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001573
Iteration 55/1000 | Loss: 0.00001573
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001570
Iteration 63/1000 | Loss: 0.00001570
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001569
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001569
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001568
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001565
Iteration 91/1000 | Loss: 0.00001565
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001564
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001564
Iteration 111/1000 | Loss: 0.00001564
Iteration 112/1000 | Loss: 0.00001564
Iteration 113/1000 | Loss: 0.00001564
Iteration 114/1000 | Loss: 0.00001564
Iteration 115/1000 | Loss: 0.00001564
Iteration 116/1000 | Loss: 0.00001564
Iteration 117/1000 | Loss: 0.00001564
Iteration 118/1000 | Loss: 0.00001564
Iteration 119/1000 | Loss: 0.00001564
Iteration 120/1000 | Loss: 0.00001564
Iteration 121/1000 | Loss: 0.00001564
Iteration 122/1000 | Loss: 0.00001564
Iteration 123/1000 | Loss: 0.00001564
Iteration 124/1000 | Loss: 0.00001564
Iteration 125/1000 | Loss: 0.00001564
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5642328435205854e-05, 1.5642328435205854e-05, 1.5642328435205854e-05, 1.5642328435205854e-05, 1.5642328435205854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5642328435205854e-05

Optimization complete. Final v2v error: 3.3044610023498535 mm

Highest mean error: 3.6561412811279297 mm for frame 24

Lowest mean error: 2.964820384979248 mm for frame 161

Saving results

Total time: 77.18576407432556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768667
Iteration 2/25 | Loss: 0.00121857
Iteration 3/25 | Loss: 0.00084538
Iteration 4/25 | Loss: 0.00078243
Iteration 5/25 | Loss: 0.00076251
Iteration 6/25 | Loss: 0.00075394
Iteration 7/25 | Loss: 0.00075351
Iteration 8/25 | Loss: 0.00075254
Iteration 9/25 | Loss: 0.00075366
Iteration 10/25 | Loss: 0.00075218
Iteration 11/25 | Loss: 0.00075342
Iteration 12/25 | Loss: 0.00075084
Iteration 13/25 | Loss: 0.00075021
Iteration 14/25 | Loss: 0.00074865
Iteration 15/25 | Loss: 0.00074634
Iteration 16/25 | Loss: 0.00074572
Iteration 17/25 | Loss: 0.00074526
Iteration 18/25 | Loss: 0.00074493
Iteration 19/25 | Loss: 0.00074479
Iteration 20/25 | Loss: 0.00074470
Iteration 21/25 | Loss: 0.00074467
Iteration 22/25 | Loss: 0.00074467
Iteration 23/25 | Loss: 0.00074467
Iteration 24/25 | Loss: 0.00074467
Iteration 25/25 | Loss: 0.00074467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.80245686
Iteration 2/25 | Loss: 0.00044655
Iteration 3/25 | Loss: 0.00044643
Iteration 4/25 | Loss: 0.00044642
Iteration 5/25 | Loss: 0.00044642
Iteration 6/25 | Loss: 0.00044642
Iteration 7/25 | Loss: 0.00044642
Iteration 8/25 | Loss: 0.00044642
Iteration 9/25 | Loss: 0.00044642
Iteration 10/25 | Loss: 0.00044642
Iteration 11/25 | Loss: 0.00044642
Iteration 12/25 | Loss: 0.00044642
Iteration 13/25 | Loss: 0.00044642
Iteration 14/25 | Loss: 0.00044642
Iteration 15/25 | Loss: 0.00044642
Iteration 16/25 | Loss: 0.00044642
Iteration 17/25 | Loss: 0.00044642
Iteration 18/25 | Loss: 0.00044642
Iteration 19/25 | Loss: 0.00044642
Iteration 20/25 | Loss: 0.00044642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00044642240391112864, 0.00044642240391112864, 0.00044642240391112864, 0.00044642240391112864, 0.00044642240391112864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044642240391112864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044642
Iteration 2/1000 | Loss: 0.00009622
Iteration 3/1000 | Loss: 0.00003312
Iteration 4/1000 | Loss: 0.00002765
Iteration 5/1000 | Loss: 0.00002416
Iteration 6/1000 | Loss: 0.00002318
Iteration 7/1000 | Loss: 0.00008965
Iteration 8/1000 | Loss: 0.00004464
Iteration 9/1000 | Loss: 0.00008677
Iteration 10/1000 | Loss: 0.00002858
Iteration 11/1000 | Loss: 0.00007249
Iteration 12/1000 | Loss: 0.00002515
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002851
Iteration 15/1000 | Loss: 0.00002389
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002270
Iteration 18/1000 | Loss: 0.00020514
Iteration 19/1000 | Loss: 0.00015857
Iteration 20/1000 | Loss: 0.00021352
Iteration 21/1000 | Loss: 0.00024146
Iteration 22/1000 | Loss: 0.00030335
Iteration 23/1000 | Loss: 0.00023524
Iteration 24/1000 | Loss: 0.00021356
Iteration 25/1000 | Loss: 0.00019890
Iteration 26/1000 | Loss: 0.00020629
Iteration 27/1000 | Loss: 0.00017177
Iteration 28/1000 | Loss: 0.00011564
Iteration 29/1000 | Loss: 0.00015822
Iteration 30/1000 | Loss: 0.00018235
Iteration 31/1000 | Loss: 0.00002733
Iteration 32/1000 | Loss: 0.00002444
Iteration 33/1000 | Loss: 0.00003566
Iteration 34/1000 | Loss: 0.00002847
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00004008
Iteration 38/1000 | Loss: 0.00002176
Iteration 39/1000 | Loss: 0.00002132
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002066
Iteration 42/1000 | Loss: 0.00003957
Iteration 43/1000 | Loss: 0.00002531
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00002845
Iteration 48/1000 | Loss: 0.00004120
Iteration 49/1000 | Loss: 0.00001805
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00002785
Iteration 58/1000 | Loss: 0.00016426
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00002783
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001770
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001769
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001768
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001764
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001764
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001763
Iteration 87/1000 | Loss: 0.00001763
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001762
Iteration 94/1000 | Loss: 0.00001762
Iteration 95/1000 | Loss: 0.00001762
Iteration 96/1000 | Loss: 0.00001761
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001761
Iteration 99/1000 | Loss: 0.00001761
Iteration 100/1000 | Loss: 0.00001761
Iteration 101/1000 | Loss: 0.00001761
Iteration 102/1000 | Loss: 0.00001760
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001759
Iteration 107/1000 | Loss: 0.00001759
Iteration 108/1000 | Loss: 0.00001759
Iteration 109/1000 | Loss: 0.00001758
Iteration 110/1000 | Loss: 0.00001758
Iteration 111/1000 | Loss: 0.00001758
Iteration 112/1000 | Loss: 0.00001758
Iteration 113/1000 | Loss: 0.00001758
Iteration 114/1000 | Loss: 0.00001758
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001757
Iteration 117/1000 | Loss: 0.00001757
Iteration 118/1000 | Loss: 0.00001757
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001756
Iteration 124/1000 | Loss: 0.00001756
Iteration 125/1000 | Loss: 0.00001756
Iteration 126/1000 | Loss: 0.00001756
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001756
Iteration 129/1000 | Loss: 0.00001756
Iteration 130/1000 | Loss: 0.00001756
Iteration 131/1000 | Loss: 0.00001756
Iteration 132/1000 | Loss: 0.00001756
Iteration 133/1000 | Loss: 0.00001756
Iteration 134/1000 | Loss: 0.00001756
Iteration 135/1000 | Loss: 0.00001756
Iteration 136/1000 | Loss: 0.00001756
Iteration 137/1000 | Loss: 0.00001755
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001755
Iteration 140/1000 | Loss: 0.00001755
Iteration 141/1000 | Loss: 0.00001755
Iteration 142/1000 | Loss: 0.00001755
Iteration 143/1000 | Loss: 0.00001755
Iteration 144/1000 | Loss: 0.00001755
Iteration 145/1000 | Loss: 0.00001755
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001755
Iteration 148/1000 | Loss: 0.00001755
Iteration 149/1000 | Loss: 0.00001755
Iteration 150/1000 | Loss: 0.00001755
Iteration 151/1000 | Loss: 0.00001755
Iteration 152/1000 | Loss: 0.00001755
Iteration 153/1000 | Loss: 0.00001755
Iteration 154/1000 | Loss: 0.00001755
Iteration 155/1000 | Loss: 0.00001755
Iteration 156/1000 | Loss: 0.00001755
Iteration 157/1000 | Loss: 0.00001755
Iteration 158/1000 | Loss: 0.00001755
Iteration 159/1000 | Loss: 0.00001755
Iteration 160/1000 | Loss: 0.00001755
Iteration 161/1000 | Loss: 0.00001755
Iteration 162/1000 | Loss: 0.00001755
Iteration 163/1000 | Loss: 0.00001755
Iteration 164/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7546224626130424e-05, 1.7546224626130424e-05, 1.7546224626130424e-05, 1.7546224626130424e-05, 1.7546224626130424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7546224626130424e-05

Optimization complete. Final v2v error: 3.449110507965088 mm

Highest mean error: 4.9286346435546875 mm for frame 170

Lowest mean error: 2.611922264099121 mm for frame 94

Saving results

Total time: 133.13149881362915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040282
Iteration 2/25 | Loss: 0.00254466
Iteration 3/25 | Loss: 0.00184555
Iteration 4/25 | Loss: 0.00137765
Iteration 5/25 | Loss: 0.00141324
Iteration 6/25 | Loss: 0.00134318
Iteration 7/25 | Loss: 0.00115027
Iteration 8/25 | Loss: 0.00107448
Iteration 9/25 | Loss: 0.00103929
Iteration 10/25 | Loss: 0.00103203
Iteration 11/25 | Loss: 0.00099394
Iteration 12/25 | Loss: 0.00097450
Iteration 13/25 | Loss: 0.00093711
Iteration 14/25 | Loss: 0.00091576
Iteration 15/25 | Loss: 0.00090748
Iteration 16/25 | Loss: 0.00090572
Iteration 17/25 | Loss: 0.00090521
Iteration 18/25 | Loss: 0.00090503
Iteration 19/25 | Loss: 0.00090497
Iteration 20/25 | Loss: 0.00090497
Iteration 21/25 | Loss: 0.00090497
Iteration 22/25 | Loss: 0.00090497
Iteration 23/25 | Loss: 0.00090497
Iteration 24/25 | Loss: 0.00090497
Iteration 25/25 | Loss: 0.00090497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42472994
Iteration 2/25 | Loss: 0.00038003
Iteration 3/25 | Loss: 0.00038003
Iteration 4/25 | Loss: 0.00038003
Iteration 5/25 | Loss: 0.00038003
Iteration 6/25 | Loss: 0.00038003
Iteration 7/25 | Loss: 0.00038003
Iteration 8/25 | Loss: 0.00038003
Iteration 9/25 | Loss: 0.00038003
Iteration 10/25 | Loss: 0.00038003
Iteration 11/25 | Loss: 0.00038003
Iteration 12/25 | Loss: 0.00038003
Iteration 13/25 | Loss: 0.00038003
Iteration 14/25 | Loss: 0.00038003
Iteration 15/25 | Loss: 0.00038003
Iteration 16/25 | Loss: 0.00038003
Iteration 17/25 | Loss: 0.00038003
Iteration 18/25 | Loss: 0.00038003
Iteration 19/25 | Loss: 0.00038003
Iteration 20/25 | Loss: 0.00038003
Iteration 21/25 | Loss: 0.00038003
Iteration 22/25 | Loss: 0.00038003
Iteration 23/25 | Loss: 0.00038003
Iteration 24/25 | Loss: 0.00038003
Iteration 25/25 | Loss: 0.00038003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038003
Iteration 2/1000 | Loss: 0.00011354
Iteration 3/1000 | Loss: 0.00016838
Iteration 4/1000 | Loss: 0.00016921
Iteration 5/1000 | Loss: 0.00014263
Iteration 6/1000 | Loss: 0.00014544
Iteration 7/1000 | Loss: 0.00013743
Iteration 8/1000 | Loss: 0.00031113
Iteration 9/1000 | Loss: 0.00014602
Iteration 10/1000 | Loss: 0.00006723
Iteration 11/1000 | Loss: 0.00003522
Iteration 12/1000 | Loss: 0.00003369
Iteration 13/1000 | Loss: 0.00020890
Iteration 14/1000 | Loss: 0.00003321
Iteration 15/1000 | Loss: 0.00012652
Iteration 16/1000 | Loss: 0.00003181
Iteration 17/1000 | Loss: 0.00003007
Iteration 18/1000 | Loss: 0.00002953
Iteration 19/1000 | Loss: 0.00002923
Iteration 20/1000 | Loss: 0.00002899
Iteration 21/1000 | Loss: 0.00002877
Iteration 22/1000 | Loss: 0.00002860
Iteration 23/1000 | Loss: 0.00002858
Iteration 24/1000 | Loss: 0.00002856
Iteration 25/1000 | Loss: 0.00002846
Iteration 26/1000 | Loss: 0.00002844
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002842
Iteration 29/1000 | Loss: 0.00002840
Iteration 30/1000 | Loss: 0.00002839
Iteration 31/1000 | Loss: 0.00002839
Iteration 32/1000 | Loss: 0.00002838
Iteration 33/1000 | Loss: 0.00002838
Iteration 34/1000 | Loss: 0.00002832
Iteration 35/1000 | Loss: 0.00002830
Iteration 36/1000 | Loss: 0.00002829
Iteration 37/1000 | Loss: 0.00002828
Iteration 38/1000 | Loss: 0.00002827
Iteration 39/1000 | Loss: 0.00002827
Iteration 40/1000 | Loss: 0.00002827
Iteration 41/1000 | Loss: 0.00002827
Iteration 42/1000 | Loss: 0.00002827
Iteration 43/1000 | Loss: 0.00002826
Iteration 44/1000 | Loss: 0.00002826
Iteration 45/1000 | Loss: 0.00002826
Iteration 46/1000 | Loss: 0.00002826
Iteration 47/1000 | Loss: 0.00002826
Iteration 48/1000 | Loss: 0.00002825
Iteration 49/1000 | Loss: 0.00002825
Iteration 50/1000 | Loss: 0.00002824
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002823
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00002823
Iteration 55/1000 | Loss: 0.00002822
Iteration 56/1000 | Loss: 0.00002822
Iteration 57/1000 | Loss: 0.00002822
Iteration 58/1000 | Loss: 0.00002821
Iteration 59/1000 | Loss: 0.00002820
Iteration 60/1000 | Loss: 0.00002820
Iteration 61/1000 | Loss: 0.00002820
Iteration 62/1000 | Loss: 0.00002820
Iteration 63/1000 | Loss: 0.00002820
Iteration 64/1000 | Loss: 0.00002820
Iteration 65/1000 | Loss: 0.00002820
Iteration 66/1000 | Loss: 0.00002820
Iteration 67/1000 | Loss: 0.00002820
Iteration 68/1000 | Loss: 0.00002820
Iteration 69/1000 | Loss: 0.00002820
Iteration 70/1000 | Loss: 0.00002820
Iteration 71/1000 | Loss: 0.00002819
Iteration 72/1000 | Loss: 0.00002819
Iteration 73/1000 | Loss: 0.00002819
Iteration 74/1000 | Loss: 0.00002819
Iteration 75/1000 | Loss: 0.00002819
Iteration 76/1000 | Loss: 0.00002819
Iteration 77/1000 | Loss: 0.00002819
Iteration 78/1000 | Loss: 0.00002819
Iteration 79/1000 | Loss: 0.00002818
Iteration 80/1000 | Loss: 0.00002816
Iteration 81/1000 | Loss: 0.00002815
Iteration 82/1000 | Loss: 0.00002815
Iteration 83/1000 | Loss: 0.00002815
Iteration 84/1000 | Loss: 0.00002815
Iteration 85/1000 | Loss: 0.00002815
Iteration 86/1000 | Loss: 0.00002814
Iteration 87/1000 | Loss: 0.00002814
Iteration 88/1000 | Loss: 0.00002814
Iteration 89/1000 | Loss: 0.00002814
Iteration 90/1000 | Loss: 0.00002814
Iteration 91/1000 | Loss: 0.00002814
Iteration 92/1000 | Loss: 0.00002814
Iteration 93/1000 | Loss: 0.00002814
Iteration 94/1000 | Loss: 0.00002814
Iteration 95/1000 | Loss: 0.00002814
Iteration 96/1000 | Loss: 0.00002813
Iteration 97/1000 | Loss: 0.00002813
Iteration 98/1000 | Loss: 0.00002813
Iteration 99/1000 | Loss: 0.00002813
Iteration 100/1000 | Loss: 0.00002813
Iteration 101/1000 | Loss: 0.00002813
Iteration 102/1000 | Loss: 0.00002813
Iteration 103/1000 | Loss: 0.00002813
Iteration 104/1000 | Loss: 0.00002813
Iteration 105/1000 | Loss: 0.00002813
Iteration 106/1000 | Loss: 0.00002813
Iteration 107/1000 | Loss: 0.00002813
Iteration 108/1000 | Loss: 0.00002813
Iteration 109/1000 | Loss: 0.00002813
Iteration 110/1000 | Loss: 0.00002813
Iteration 111/1000 | Loss: 0.00002813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.813359424180817e-05, 2.813359424180817e-05, 2.813359424180817e-05, 2.813359424180817e-05, 2.813359424180817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.813359424180817e-05

Optimization complete. Final v2v error: 4.4841132164001465 mm

Highest mean error: 5.135526657104492 mm for frame 239

Lowest mean error: 4.243109226226807 mm for frame 10

Saving results

Total time: 80.48365521430969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436528
Iteration 2/25 | Loss: 0.00091111
Iteration 3/25 | Loss: 0.00075331
Iteration 4/25 | Loss: 0.00072600
Iteration 5/25 | Loss: 0.00071517
Iteration 6/25 | Loss: 0.00071236
Iteration 7/25 | Loss: 0.00071154
Iteration 8/25 | Loss: 0.00071154
Iteration 9/25 | Loss: 0.00071154
Iteration 10/25 | Loss: 0.00071154
Iteration 11/25 | Loss: 0.00071154
Iteration 12/25 | Loss: 0.00071154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007115367334336042, 0.0007115367334336042, 0.0007115367334336042, 0.0007115367334336042, 0.0007115367334336042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007115367334336042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48869967
Iteration 2/25 | Loss: 0.00040575
Iteration 3/25 | Loss: 0.00040574
Iteration 4/25 | Loss: 0.00040574
Iteration 5/25 | Loss: 0.00040574
Iteration 6/25 | Loss: 0.00040574
Iteration 7/25 | Loss: 0.00040574
Iteration 8/25 | Loss: 0.00040574
Iteration 9/25 | Loss: 0.00040574
Iteration 10/25 | Loss: 0.00040574
Iteration 11/25 | Loss: 0.00040574
Iteration 12/25 | Loss: 0.00040574
Iteration 13/25 | Loss: 0.00040574
Iteration 14/25 | Loss: 0.00040574
Iteration 15/25 | Loss: 0.00040574
Iteration 16/25 | Loss: 0.00040574
Iteration 17/25 | Loss: 0.00040574
Iteration 18/25 | Loss: 0.00040574
Iteration 19/25 | Loss: 0.00040574
Iteration 20/25 | Loss: 0.00040574
Iteration 21/25 | Loss: 0.00040574
Iteration 22/25 | Loss: 0.00040574
Iteration 23/25 | Loss: 0.00040574
Iteration 24/25 | Loss: 0.00040574
Iteration 25/25 | Loss: 0.00040574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040574
Iteration 2/1000 | Loss: 0.00003070
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001818
Iteration 5/1000 | Loss: 0.00001718
Iteration 6/1000 | Loss: 0.00001647
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001551
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001541
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001492
Iteration 18/1000 | Loss: 0.00001488
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001485
Iteration 22/1000 | Loss: 0.00001484
Iteration 23/1000 | Loss: 0.00001484
Iteration 24/1000 | Loss: 0.00001483
Iteration 25/1000 | Loss: 0.00001483
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001480
Iteration 32/1000 | Loss: 0.00001480
Iteration 33/1000 | Loss: 0.00001479
Iteration 34/1000 | Loss: 0.00001479
Iteration 35/1000 | Loss: 0.00001478
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001476
Iteration 40/1000 | Loss: 0.00001476
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001473
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001471
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001468
Iteration 67/1000 | Loss: 0.00001468
Iteration 68/1000 | Loss: 0.00001468
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001467
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001467
Iteration 81/1000 | Loss: 0.00001467
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001466
Iteration 88/1000 | Loss: 0.00001466
Iteration 89/1000 | Loss: 0.00001466
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001465
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001464
Iteration 98/1000 | Loss: 0.00001464
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001463
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001462
Iteration 111/1000 | Loss: 0.00001462
Iteration 112/1000 | Loss: 0.00001462
Iteration 113/1000 | Loss: 0.00001462
Iteration 114/1000 | Loss: 0.00001461
Iteration 115/1000 | Loss: 0.00001461
Iteration 116/1000 | Loss: 0.00001461
Iteration 117/1000 | Loss: 0.00001461
Iteration 118/1000 | Loss: 0.00001461
Iteration 119/1000 | Loss: 0.00001461
Iteration 120/1000 | Loss: 0.00001461
Iteration 121/1000 | Loss: 0.00001461
Iteration 122/1000 | Loss: 0.00001460
Iteration 123/1000 | Loss: 0.00001460
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001460
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001459
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001459
Iteration 139/1000 | Loss: 0.00001459
Iteration 140/1000 | Loss: 0.00001459
Iteration 141/1000 | Loss: 0.00001459
Iteration 142/1000 | Loss: 0.00001459
Iteration 143/1000 | Loss: 0.00001459
Iteration 144/1000 | Loss: 0.00001459
Iteration 145/1000 | Loss: 0.00001459
Iteration 146/1000 | Loss: 0.00001458
Iteration 147/1000 | Loss: 0.00001458
Iteration 148/1000 | Loss: 0.00001458
Iteration 149/1000 | Loss: 0.00001458
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001458
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001458
Iteration 154/1000 | Loss: 0.00001458
Iteration 155/1000 | Loss: 0.00001458
Iteration 156/1000 | Loss: 0.00001458
Iteration 157/1000 | Loss: 0.00001458
Iteration 158/1000 | Loss: 0.00001458
Iteration 159/1000 | Loss: 0.00001458
Iteration 160/1000 | Loss: 0.00001458
Iteration 161/1000 | Loss: 0.00001458
Iteration 162/1000 | Loss: 0.00001458
Iteration 163/1000 | Loss: 0.00001458
Iteration 164/1000 | Loss: 0.00001458
Iteration 165/1000 | Loss: 0.00001458
Iteration 166/1000 | Loss: 0.00001458
Iteration 167/1000 | Loss: 0.00001458
Iteration 168/1000 | Loss: 0.00001458
Iteration 169/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4584983546228614e-05, 1.4584983546228614e-05, 1.4584983546228614e-05, 1.4584983546228614e-05, 1.4584983546228614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4584983546228614e-05

Optimization complete. Final v2v error: 3.1947710514068604 mm

Highest mean error: 3.789637565612793 mm for frame 144

Lowest mean error: 2.5541932582855225 mm for frame 161

Saving results

Total time: 45.41033864021301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394694
Iteration 2/25 | Loss: 0.00079384
Iteration 3/25 | Loss: 0.00069343
Iteration 4/25 | Loss: 0.00067041
Iteration 5/25 | Loss: 0.00066333
Iteration 6/25 | Loss: 0.00066192
Iteration 7/25 | Loss: 0.00066150
Iteration 8/25 | Loss: 0.00066150
Iteration 9/25 | Loss: 0.00066150
Iteration 10/25 | Loss: 0.00066150
Iteration 11/25 | Loss: 0.00066150
Iteration 12/25 | Loss: 0.00066150
Iteration 13/25 | Loss: 0.00066150
Iteration 14/25 | Loss: 0.00066150
Iteration 15/25 | Loss: 0.00066150
Iteration 16/25 | Loss: 0.00066150
Iteration 17/25 | Loss: 0.00066150
Iteration 18/25 | Loss: 0.00066150
Iteration 19/25 | Loss: 0.00066150
Iteration 20/25 | Loss: 0.00066150
Iteration 21/25 | Loss: 0.00066150
Iteration 22/25 | Loss: 0.00066150
Iteration 23/25 | Loss: 0.00066150
Iteration 24/25 | Loss: 0.00066150
Iteration 25/25 | Loss: 0.00066150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.93541002
Iteration 2/25 | Loss: 0.00031182
Iteration 3/25 | Loss: 0.00031181
Iteration 4/25 | Loss: 0.00031181
Iteration 5/25 | Loss: 0.00031181
Iteration 6/25 | Loss: 0.00031181
Iteration 7/25 | Loss: 0.00031181
Iteration 8/25 | Loss: 0.00031181
Iteration 9/25 | Loss: 0.00031181
Iteration 10/25 | Loss: 0.00031181
Iteration 11/25 | Loss: 0.00031181
Iteration 12/25 | Loss: 0.00031181
Iteration 13/25 | Loss: 0.00031181
Iteration 14/25 | Loss: 0.00031181
Iteration 15/25 | Loss: 0.00031181
Iteration 16/25 | Loss: 0.00031181
Iteration 17/25 | Loss: 0.00031181
Iteration 18/25 | Loss: 0.00031181
Iteration 19/25 | Loss: 0.00031181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003118101158179343, 0.0003118101158179343, 0.0003118101158179343, 0.0003118101158179343, 0.0003118101158179343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003118101158179343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031181
Iteration 2/1000 | Loss: 0.00002678
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001437
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001356
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001344
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001325
Iteration 31/1000 | Loss: 0.00001325
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001319
Iteration 45/1000 | Loss: 0.00001319
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001305
Iteration 104/1000 | Loss: 0.00001305
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001304
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001304
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001302
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001301
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001300
Iteration 142/1000 | Loss: 0.00001300
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001300
Iteration 164/1000 | Loss: 0.00001300
Iteration 165/1000 | Loss: 0.00001300
Iteration 166/1000 | Loss: 0.00001300
Iteration 167/1000 | Loss: 0.00001300
Iteration 168/1000 | Loss: 0.00001300
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001300
Iteration 179/1000 | Loss: 0.00001300
Iteration 180/1000 | Loss: 0.00001300
Iteration 181/1000 | Loss: 0.00001300
Iteration 182/1000 | Loss: 0.00001300
Iteration 183/1000 | Loss: 0.00001300
Iteration 184/1000 | Loss: 0.00001300
Iteration 185/1000 | Loss: 0.00001300
Iteration 186/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2997320482099894e-05, 1.2997320482099894e-05, 1.2997320482099894e-05, 1.2997320482099894e-05, 1.2997320482099894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2997320482099894e-05

Optimization complete. Final v2v error: 3.0925912857055664 mm

Highest mean error: 3.3602545261383057 mm for frame 46

Lowest mean error: 2.8660268783569336 mm for frame 88

Saving results

Total time: 37.07335638999939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408462
Iteration 2/25 | Loss: 0.00092497
Iteration 3/25 | Loss: 0.00074417
Iteration 4/25 | Loss: 0.00070358
Iteration 5/25 | Loss: 0.00069189
Iteration 6/25 | Loss: 0.00068877
Iteration 7/25 | Loss: 0.00068793
Iteration 8/25 | Loss: 0.00068770
Iteration 9/25 | Loss: 0.00068767
Iteration 10/25 | Loss: 0.00068767
Iteration 11/25 | Loss: 0.00068767
Iteration 12/25 | Loss: 0.00068767
Iteration 13/25 | Loss: 0.00068767
Iteration 14/25 | Loss: 0.00068767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006876701954752207, 0.0006876701954752207, 0.0006876701954752207, 0.0006876701954752207, 0.0006876701954752207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006876701954752207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53136110
Iteration 2/25 | Loss: 0.00032999
Iteration 3/25 | Loss: 0.00032999
Iteration 4/25 | Loss: 0.00032999
Iteration 5/25 | Loss: 0.00032998
Iteration 6/25 | Loss: 0.00032998
Iteration 7/25 | Loss: 0.00032998
Iteration 8/25 | Loss: 0.00032998
Iteration 9/25 | Loss: 0.00032998
Iteration 10/25 | Loss: 0.00032998
Iteration 11/25 | Loss: 0.00032998
Iteration 12/25 | Loss: 0.00032998
Iteration 13/25 | Loss: 0.00032998
Iteration 14/25 | Loss: 0.00032998
Iteration 15/25 | Loss: 0.00032998
Iteration 16/25 | Loss: 0.00032998
Iteration 17/25 | Loss: 0.00032998
Iteration 18/25 | Loss: 0.00032998
Iteration 19/25 | Loss: 0.00032998
Iteration 20/25 | Loss: 0.00032998
Iteration 21/25 | Loss: 0.00032998
Iteration 22/25 | Loss: 0.00032998
Iteration 23/25 | Loss: 0.00032998
Iteration 24/25 | Loss: 0.00032998
Iteration 25/25 | Loss: 0.00032998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032998
Iteration 2/1000 | Loss: 0.00003097
Iteration 3/1000 | Loss: 0.00002160
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001670
Iteration 9/1000 | Loss: 0.00001646
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001629
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001628
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001618
Iteration 20/1000 | Loss: 0.00001617
Iteration 21/1000 | Loss: 0.00001617
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001607
Iteration 28/1000 | Loss: 0.00001606
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001605
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001604
Iteration 33/1000 | Loss: 0.00001604
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001601
Iteration 36/1000 | Loss: 0.00001600
Iteration 37/1000 | Loss: 0.00001600
Iteration 38/1000 | Loss: 0.00001599
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001599
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001597
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001596
Iteration 50/1000 | Loss: 0.00001596
Iteration 51/1000 | Loss: 0.00001596
Iteration 52/1000 | Loss: 0.00001596
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001591
Iteration 67/1000 | Loss: 0.00001591
Iteration 68/1000 | Loss: 0.00001591
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001587
Iteration 78/1000 | Loss: 0.00001587
Iteration 79/1000 | Loss: 0.00001587
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001585
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001584
Iteration 100/1000 | Loss: 0.00001584
Iteration 101/1000 | Loss: 0.00001584
Iteration 102/1000 | Loss: 0.00001584
Iteration 103/1000 | Loss: 0.00001584
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001583
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001582
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001582
Iteration 116/1000 | Loss: 0.00001582
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001581
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001578
Iteration 141/1000 | Loss: 0.00001578
Iteration 142/1000 | Loss: 0.00001578
Iteration 143/1000 | Loss: 0.00001578
Iteration 144/1000 | Loss: 0.00001578
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001577
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001576
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001576
Iteration 154/1000 | Loss: 0.00001576
Iteration 155/1000 | Loss: 0.00001576
Iteration 156/1000 | Loss: 0.00001575
Iteration 157/1000 | Loss: 0.00001575
Iteration 158/1000 | Loss: 0.00001575
Iteration 159/1000 | Loss: 0.00001575
Iteration 160/1000 | Loss: 0.00001575
Iteration 161/1000 | Loss: 0.00001575
Iteration 162/1000 | Loss: 0.00001575
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001575
Iteration 165/1000 | Loss: 0.00001575
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001574
Iteration 168/1000 | Loss: 0.00001574
Iteration 169/1000 | Loss: 0.00001574
Iteration 170/1000 | Loss: 0.00001574
Iteration 171/1000 | Loss: 0.00001574
Iteration 172/1000 | Loss: 0.00001574
Iteration 173/1000 | Loss: 0.00001574
Iteration 174/1000 | Loss: 0.00001574
Iteration 175/1000 | Loss: 0.00001574
Iteration 176/1000 | Loss: 0.00001574
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001573
Iteration 182/1000 | Loss: 0.00001573
Iteration 183/1000 | Loss: 0.00001573
Iteration 184/1000 | Loss: 0.00001573
Iteration 185/1000 | Loss: 0.00001573
Iteration 186/1000 | Loss: 0.00001573
Iteration 187/1000 | Loss: 0.00001573
Iteration 188/1000 | Loss: 0.00001573
Iteration 189/1000 | Loss: 0.00001573
Iteration 190/1000 | Loss: 0.00001573
Iteration 191/1000 | Loss: 0.00001573
Iteration 192/1000 | Loss: 0.00001573
Iteration 193/1000 | Loss: 0.00001572
Iteration 194/1000 | Loss: 0.00001572
Iteration 195/1000 | Loss: 0.00001572
Iteration 196/1000 | Loss: 0.00001572
Iteration 197/1000 | Loss: 0.00001572
Iteration 198/1000 | Loss: 0.00001572
Iteration 199/1000 | Loss: 0.00001572
Iteration 200/1000 | Loss: 0.00001572
Iteration 201/1000 | Loss: 0.00001572
Iteration 202/1000 | Loss: 0.00001572
Iteration 203/1000 | Loss: 0.00001572
Iteration 204/1000 | Loss: 0.00001572
Iteration 205/1000 | Loss: 0.00001572
Iteration 206/1000 | Loss: 0.00001572
Iteration 207/1000 | Loss: 0.00001572
Iteration 208/1000 | Loss: 0.00001572
Iteration 209/1000 | Loss: 0.00001572
Iteration 210/1000 | Loss: 0.00001572
Iteration 211/1000 | Loss: 0.00001572
Iteration 212/1000 | Loss: 0.00001571
Iteration 213/1000 | Loss: 0.00001571
Iteration 214/1000 | Loss: 0.00001571
Iteration 215/1000 | Loss: 0.00001571
Iteration 216/1000 | Loss: 0.00001571
Iteration 217/1000 | Loss: 0.00001571
Iteration 218/1000 | Loss: 0.00001571
Iteration 219/1000 | Loss: 0.00001571
Iteration 220/1000 | Loss: 0.00001571
Iteration 221/1000 | Loss: 0.00001571
Iteration 222/1000 | Loss: 0.00001571
Iteration 223/1000 | Loss: 0.00001571
Iteration 224/1000 | Loss: 0.00001571
Iteration 225/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.5710367733845487e-05, 1.5710367733845487e-05, 1.5710367733845487e-05, 1.5710367733845487e-05, 1.5710367733845487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5710367733845487e-05

Optimization complete. Final v2v error: 3.3368914127349854 mm

Highest mean error: 4.356414318084717 mm for frame 46

Lowest mean error: 2.921149492263794 mm for frame 92

Saving results

Total time: 43.108787298202515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720627
Iteration 2/25 | Loss: 0.00122524
Iteration 3/25 | Loss: 0.00081889
Iteration 4/25 | Loss: 0.00073033
Iteration 5/25 | Loss: 0.00071681
Iteration 6/25 | Loss: 0.00071365
Iteration 7/25 | Loss: 0.00071350
Iteration 8/25 | Loss: 0.00071350
Iteration 9/25 | Loss: 0.00071350
Iteration 10/25 | Loss: 0.00071350
Iteration 11/25 | Loss: 0.00071350
Iteration 12/25 | Loss: 0.00071350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007135026389732957, 0.0007135026389732957, 0.0007135026389732957, 0.0007135026389732957, 0.0007135026389732957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007135026389732957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50178182
Iteration 2/25 | Loss: 0.00031252
Iteration 3/25 | Loss: 0.00031251
Iteration 4/25 | Loss: 0.00031251
Iteration 5/25 | Loss: 0.00031251
Iteration 6/25 | Loss: 0.00031251
Iteration 7/25 | Loss: 0.00031250
Iteration 8/25 | Loss: 0.00031250
Iteration 9/25 | Loss: 0.00031250
Iteration 10/25 | Loss: 0.00031250
Iteration 11/25 | Loss: 0.00031250
Iteration 12/25 | Loss: 0.00031250
Iteration 13/25 | Loss: 0.00031250
Iteration 14/25 | Loss: 0.00031250
Iteration 15/25 | Loss: 0.00031250
Iteration 16/25 | Loss: 0.00031250
Iteration 17/25 | Loss: 0.00031250
Iteration 18/25 | Loss: 0.00031250
Iteration 19/25 | Loss: 0.00031250
Iteration 20/25 | Loss: 0.00031250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003125039511360228, 0.0003125039511360228, 0.0003125039511360228, 0.0003125039511360228, 0.0003125039511360228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003125039511360228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031250
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00002152
Iteration 4/1000 | Loss: 0.00001960
Iteration 5/1000 | Loss: 0.00001848
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001655
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001620
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001616
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001613
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001612
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001609
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001604
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001593
Iteration 34/1000 | Loss: 0.00001592
Iteration 35/1000 | Loss: 0.00001592
Iteration 36/1000 | Loss: 0.00001592
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001591
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001590
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001589
Iteration 44/1000 | Loss: 0.00001589
Iteration 45/1000 | Loss: 0.00001589
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001588
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001588
Iteration 55/1000 | Loss: 0.00001588
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001587
Iteration 61/1000 | Loss: 0.00001587
Iteration 62/1000 | Loss: 0.00001587
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001586
Iteration 65/1000 | Loss: 0.00001586
Iteration 66/1000 | Loss: 0.00001586
Iteration 67/1000 | Loss: 0.00001586
Iteration 68/1000 | Loss: 0.00001585
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001585
Iteration 71/1000 | Loss: 0.00001584
Iteration 72/1000 | Loss: 0.00001584
Iteration 73/1000 | Loss: 0.00001584
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001581
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001578
Iteration 100/1000 | Loss: 0.00001578
Iteration 101/1000 | Loss: 0.00001578
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001577
Iteration 106/1000 | Loss: 0.00001577
Iteration 107/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.577300463395659e-05, 1.577300463395659e-05, 1.577300463395659e-05, 1.577300463395659e-05, 1.577300463395659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.577300463395659e-05

Optimization complete. Final v2v error: 3.4113247394561768 mm

Highest mean error: 3.7582619190216064 mm for frame 201

Lowest mean error: 3.1575348377227783 mm for frame 212

Saving results

Total time: 39.666950941085815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115747
Iteration 2/25 | Loss: 0.00497906
Iteration 3/25 | Loss: 0.00296836
Iteration 4/25 | Loss: 0.00262871
Iteration 5/25 | Loss: 0.00233957
Iteration 6/25 | Loss: 0.00223967
Iteration 7/25 | Loss: 0.00203950
Iteration 8/25 | Loss: 0.00202258
Iteration 9/25 | Loss: 0.00195054
Iteration 10/25 | Loss: 0.00190522
Iteration 11/25 | Loss: 0.00187579
Iteration 12/25 | Loss: 0.00188211
Iteration 13/25 | Loss: 0.00186763
Iteration 14/25 | Loss: 0.00182634
Iteration 15/25 | Loss: 0.00181420
Iteration 16/25 | Loss: 0.00181258
Iteration 17/25 | Loss: 0.00180603
Iteration 18/25 | Loss: 0.00179551
Iteration 19/25 | Loss: 0.00175671
Iteration 20/25 | Loss: 0.00166862
Iteration 21/25 | Loss: 0.00153637
Iteration 22/25 | Loss: 0.00149562
Iteration 23/25 | Loss: 0.00148042
Iteration 24/25 | Loss: 0.00147716
Iteration 25/25 | Loss: 0.00146920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98737168
Iteration 2/25 | Loss: 0.00670864
Iteration 3/25 | Loss: 0.00544163
Iteration 4/25 | Loss: 0.00544163
Iteration 5/25 | Loss: 0.00544162
Iteration 6/25 | Loss: 0.00544162
Iteration 7/25 | Loss: 0.00544162
Iteration 8/25 | Loss: 0.00544162
Iteration 9/25 | Loss: 0.00544162
Iteration 10/25 | Loss: 0.00544162
Iteration 11/25 | Loss: 0.00544162
Iteration 12/25 | Loss: 0.00544162
Iteration 13/25 | Loss: 0.00544162
Iteration 14/25 | Loss: 0.00544162
Iteration 15/25 | Loss: 0.00544162
Iteration 16/25 | Loss: 0.00544162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005441621411591768, 0.005441621411591768, 0.005441621411591768, 0.005441621411591768, 0.005441621411591768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005441621411591768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00544162
Iteration 2/1000 | Loss: 0.00452205
Iteration 3/1000 | Loss: 0.00189204
Iteration 4/1000 | Loss: 0.00259624
Iteration 5/1000 | Loss: 0.00127508
Iteration 6/1000 | Loss: 0.00164053
Iteration 7/1000 | Loss: 0.00086921
Iteration 8/1000 | Loss: 0.00083305
Iteration 9/1000 | Loss: 0.00070897
Iteration 10/1000 | Loss: 0.00088827
Iteration 11/1000 | Loss: 0.00117210
Iteration 12/1000 | Loss: 0.00078569
Iteration 13/1000 | Loss: 0.00136144
Iteration 14/1000 | Loss: 0.00061500
Iteration 15/1000 | Loss: 0.00059004
Iteration 16/1000 | Loss: 0.00160897
Iteration 17/1000 | Loss: 0.00044595
Iteration 18/1000 | Loss: 0.00069189
Iteration 19/1000 | Loss: 0.00139110
Iteration 20/1000 | Loss: 0.00403595
Iteration 21/1000 | Loss: 0.00130555
Iteration 22/1000 | Loss: 0.00110850
Iteration 23/1000 | Loss: 0.00060044
Iteration 24/1000 | Loss: 0.00052057
Iteration 25/1000 | Loss: 0.00142370
Iteration 26/1000 | Loss: 0.00053066
Iteration 27/1000 | Loss: 0.00031455
Iteration 28/1000 | Loss: 0.00030334
Iteration 29/1000 | Loss: 0.00324712
Iteration 30/1000 | Loss: 0.00079557
Iteration 31/1000 | Loss: 0.00033511
Iteration 32/1000 | Loss: 0.00075190
Iteration 33/1000 | Loss: 0.00258951
Iteration 34/1000 | Loss: 0.00046424
Iteration 35/1000 | Loss: 0.00030011
Iteration 36/1000 | Loss: 0.00093959
Iteration 37/1000 | Loss: 0.00097371
Iteration 38/1000 | Loss: 0.00221369
Iteration 39/1000 | Loss: 0.00096425
Iteration 40/1000 | Loss: 0.00059371
Iteration 41/1000 | Loss: 0.00029451
Iteration 42/1000 | Loss: 0.00027016
Iteration 43/1000 | Loss: 0.00168821
Iteration 44/1000 | Loss: 0.00121986
Iteration 45/1000 | Loss: 0.00173470
Iteration 46/1000 | Loss: 0.00050539
Iteration 47/1000 | Loss: 0.00029926
Iteration 48/1000 | Loss: 0.00094662
Iteration 49/1000 | Loss: 0.00094753
Iteration 50/1000 | Loss: 0.00079486
Iteration 51/1000 | Loss: 0.00218565
Iteration 52/1000 | Loss: 0.00182876
Iteration 53/1000 | Loss: 0.00082591
Iteration 54/1000 | Loss: 0.00122712
Iteration 55/1000 | Loss: 0.00076197
Iteration 56/1000 | Loss: 0.00135418
Iteration 57/1000 | Loss: 0.00035478
Iteration 58/1000 | Loss: 0.00025683
Iteration 59/1000 | Loss: 0.00024173
Iteration 60/1000 | Loss: 0.00136295
Iteration 61/1000 | Loss: 0.00027879
Iteration 62/1000 | Loss: 0.00118189
Iteration 63/1000 | Loss: 0.00176187
Iteration 64/1000 | Loss: 0.00185919
Iteration 65/1000 | Loss: 0.00029987
Iteration 66/1000 | Loss: 0.00053248
Iteration 67/1000 | Loss: 0.00022242
Iteration 68/1000 | Loss: 0.00127482
Iteration 69/1000 | Loss: 0.00508936
Iteration 70/1000 | Loss: 0.00248221
Iteration 71/1000 | Loss: 0.00095878
Iteration 72/1000 | Loss: 0.00100190
Iteration 73/1000 | Loss: 0.00119817
Iteration 74/1000 | Loss: 0.00036138
Iteration 75/1000 | Loss: 0.00125430
Iteration 76/1000 | Loss: 0.00139372
Iteration 77/1000 | Loss: 0.00031068
Iteration 78/1000 | Loss: 0.00190894
Iteration 79/1000 | Loss: 0.00208183
Iteration 80/1000 | Loss: 0.00068217
Iteration 81/1000 | Loss: 0.00041956
Iteration 82/1000 | Loss: 0.00351671
Iteration 83/1000 | Loss: 0.00029609
Iteration 84/1000 | Loss: 0.00023919
Iteration 85/1000 | Loss: 0.00305073
Iteration 86/1000 | Loss: 0.00217471
Iteration 87/1000 | Loss: 0.00026703
Iteration 88/1000 | Loss: 0.00024188
Iteration 89/1000 | Loss: 0.00022179
Iteration 90/1000 | Loss: 0.00289583
Iteration 91/1000 | Loss: 0.00133617
Iteration 92/1000 | Loss: 0.00193595
Iteration 93/1000 | Loss: 0.00089360
Iteration 94/1000 | Loss: 0.00023809
Iteration 95/1000 | Loss: 0.00022902
Iteration 96/1000 | Loss: 0.00021044
Iteration 97/1000 | Loss: 0.00104540
Iteration 98/1000 | Loss: 0.01111711
Iteration 99/1000 | Loss: 0.00479718
Iteration 100/1000 | Loss: 0.00203707
Iteration 101/1000 | Loss: 0.00210885
Iteration 102/1000 | Loss: 0.00110031
Iteration 103/1000 | Loss: 0.00041803
Iteration 104/1000 | Loss: 0.00018822
Iteration 105/1000 | Loss: 0.00029310
Iteration 106/1000 | Loss: 0.00119292
Iteration 107/1000 | Loss: 0.00025209
Iteration 108/1000 | Loss: 0.00017412
Iteration 109/1000 | Loss: 0.00126343
Iteration 110/1000 | Loss: 0.00080669
Iteration 111/1000 | Loss: 0.00034944
Iteration 112/1000 | Loss: 0.00030704
Iteration 113/1000 | Loss: 0.00042997
Iteration 114/1000 | Loss: 0.00215068
Iteration 115/1000 | Loss: 0.00050815
Iteration 116/1000 | Loss: 0.00047680
Iteration 117/1000 | Loss: 0.00343294
Iteration 118/1000 | Loss: 0.00146706
Iteration 119/1000 | Loss: 0.00061899
Iteration 120/1000 | Loss: 0.00034637
Iteration 121/1000 | Loss: 0.00022713
Iteration 122/1000 | Loss: 0.00183117
Iteration 123/1000 | Loss: 0.00032524
Iteration 124/1000 | Loss: 0.00019774
Iteration 125/1000 | Loss: 0.00033326
Iteration 126/1000 | Loss: 0.00030256
Iteration 127/1000 | Loss: 0.00024644
Iteration 128/1000 | Loss: 0.00511954
Iteration 129/1000 | Loss: 0.00058275
Iteration 130/1000 | Loss: 0.00026229
Iteration 131/1000 | Loss: 0.00022081
Iteration 132/1000 | Loss: 0.00027783
Iteration 133/1000 | Loss: 0.00121743
Iteration 134/1000 | Loss: 0.00034763
Iteration 135/1000 | Loss: 0.00019989
Iteration 136/1000 | Loss: 0.00015130
Iteration 137/1000 | Loss: 0.00115439
Iteration 138/1000 | Loss: 0.00253251
Iteration 139/1000 | Loss: 0.00070884
Iteration 140/1000 | Loss: 0.00023463
Iteration 141/1000 | Loss: 0.00013570
Iteration 142/1000 | Loss: 0.00230763
Iteration 143/1000 | Loss: 0.00056284
Iteration 144/1000 | Loss: 0.00072592
Iteration 145/1000 | Loss: 0.00233966
Iteration 146/1000 | Loss: 0.00096754
Iteration 147/1000 | Loss: 0.00092325
Iteration 148/1000 | Loss: 0.00025255
Iteration 149/1000 | Loss: 0.00024615
Iteration 150/1000 | Loss: 0.00024611
Iteration 151/1000 | Loss: 0.00105952
Iteration 152/1000 | Loss: 0.00264987
Iteration 153/1000 | Loss: 0.00108073
Iteration 154/1000 | Loss: 0.00017432
Iteration 155/1000 | Loss: 0.00055832
Iteration 156/1000 | Loss: 0.00038933
Iteration 157/1000 | Loss: 0.00024281
Iteration 158/1000 | Loss: 0.00013256
Iteration 159/1000 | Loss: 0.00029578
Iteration 160/1000 | Loss: 0.00022555
Iteration 161/1000 | Loss: 0.00039209
Iteration 162/1000 | Loss: 0.00136931
Iteration 163/1000 | Loss: 0.00212354
Iteration 164/1000 | Loss: 0.00033334
Iteration 165/1000 | Loss: 0.00013502
Iteration 166/1000 | Loss: 0.00011816
Iteration 167/1000 | Loss: 0.00011047
Iteration 168/1000 | Loss: 0.00010452
Iteration 169/1000 | Loss: 0.00137312
Iteration 170/1000 | Loss: 0.00040656
Iteration 171/1000 | Loss: 0.00027895
Iteration 172/1000 | Loss: 0.00233189
Iteration 173/1000 | Loss: 0.00120973
Iteration 174/1000 | Loss: 0.00015723
Iteration 175/1000 | Loss: 0.00079194
Iteration 176/1000 | Loss: 0.00024584
Iteration 177/1000 | Loss: 0.00012081
Iteration 178/1000 | Loss: 0.00010426
Iteration 179/1000 | Loss: 0.00197952
Iteration 180/1000 | Loss: 0.00022594
Iteration 181/1000 | Loss: 0.00010576
Iteration 182/1000 | Loss: 0.00108626
Iteration 183/1000 | Loss: 0.00139484
Iteration 184/1000 | Loss: 0.00019287
Iteration 185/1000 | Loss: 0.00009560
Iteration 186/1000 | Loss: 0.00025553
Iteration 187/1000 | Loss: 0.00061828
Iteration 188/1000 | Loss: 0.00010752
Iteration 189/1000 | Loss: 0.00023657
Iteration 190/1000 | Loss: 0.00012654
Iteration 191/1000 | Loss: 0.00123699
Iteration 192/1000 | Loss: 0.00012676
Iteration 193/1000 | Loss: 0.00009296
Iteration 194/1000 | Loss: 0.00009000
Iteration 195/1000 | Loss: 0.00037345
Iteration 196/1000 | Loss: 0.00269531
Iteration 197/1000 | Loss: 0.00325482
Iteration 198/1000 | Loss: 0.00029756
Iteration 199/1000 | Loss: 0.00199993
Iteration 200/1000 | Loss: 0.00081143
Iteration 201/1000 | Loss: 0.00082248
Iteration 202/1000 | Loss: 0.00057467
Iteration 203/1000 | Loss: 0.00027710
Iteration 204/1000 | Loss: 0.00133833
Iteration 205/1000 | Loss: 0.00025504
Iteration 206/1000 | Loss: 0.00133664
Iteration 207/1000 | Loss: 0.00148607
Iteration 208/1000 | Loss: 0.00098778
Iteration 209/1000 | Loss: 0.00074965
Iteration 210/1000 | Loss: 0.00086631
Iteration 211/1000 | Loss: 0.00035099
Iteration 212/1000 | Loss: 0.00051083
Iteration 213/1000 | Loss: 0.00009705
Iteration 214/1000 | Loss: 0.00071859
Iteration 215/1000 | Loss: 0.00059860
Iteration 216/1000 | Loss: 0.00175229
Iteration 217/1000 | Loss: 0.00087169
Iteration 218/1000 | Loss: 0.00100536
Iteration 219/1000 | Loss: 0.00115428
Iteration 220/1000 | Loss: 0.00025527
Iteration 221/1000 | Loss: 0.00211983
Iteration 222/1000 | Loss: 0.00037243
Iteration 223/1000 | Loss: 0.00088026
Iteration 224/1000 | Loss: 0.00023118
Iteration 225/1000 | Loss: 0.00009565
Iteration 226/1000 | Loss: 0.00017452
Iteration 227/1000 | Loss: 0.00008133
Iteration 228/1000 | Loss: 0.00007715
Iteration 229/1000 | Loss: 0.00025830
Iteration 230/1000 | Loss: 0.00035627
Iteration 231/1000 | Loss: 0.00026013
Iteration 232/1000 | Loss: 0.00026409
Iteration 233/1000 | Loss: 0.00016295
Iteration 234/1000 | Loss: 0.00016156
Iteration 235/1000 | Loss: 0.00022566
Iteration 236/1000 | Loss: 0.00045096
Iteration 237/1000 | Loss: 0.00021676
Iteration 238/1000 | Loss: 0.00030702
Iteration 239/1000 | Loss: 0.00030830
Iteration 240/1000 | Loss: 0.00021101
Iteration 241/1000 | Loss: 0.00019995
Iteration 242/1000 | Loss: 0.00020149
Iteration 243/1000 | Loss: 0.00019949
Iteration 244/1000 | Loss: 0.00150480
Iteration 245/1000 | Loss: 0.00090425
Iteration 246/1000 | Loss: 0.00047672
Iteration 247/1000 | Loss: 0.00062599
Iteration 248/1000 | Loss: 0.00160287
Iteration 249/1000 | Loss: 0.00051661
Iteration 250/1000 | Loss: 0.00009385
Iteration 251/1000 | Loss: 0.00008123
Iteration 252/1000 | Loss: 0.00022814
Iteration 253/1000 | Loss: 0.00016505
Iteration 254/1000 | Loss: 0.00014539
Iteration 255/1000 | Loss: 0.00008584
Iteration 256/1000 | Loss: 0.00007683
Iteration 257/1000 | Loss: 0.00007249
Iteration 258/1000 | Loss: 0.00007006
Iteration 259/1000 | Loss: 0.00006869
Iteration 260/1000 | Loss: 0.00006757
Iteration 261/1000 | Loss: 0.00116289
Iteration 262/1000 | Loss: 0.00143871
Iteration 263/1000 | Loss: 0.00142252
Iteration 264/1000 | Loss: 0.00073655
Iteration 265/1000 | Loss: 0.00007213
Iteration 266/1000 | Loss: 0.00006508
Iteration 267/1000 | Loss: 0.00006272
Iteration 268/1000 | Loss: 0.00115258
Iteration 269/1000 | Loss: 0.00049659
Iteration 270/1000 | Loss: 0.00006362
Iteration 271/1000 | Loss: 0.00006080
Iteration 272/1000 | Loss: 0.00005995
Iteration 273/1000 | Loss: 0.00005953
Iteration 274/1000 | Loss: 0.00115659
Iteration 275/1000 | Loss: 0.00084759
Iteration 276/1000 | Loss: 0.00009829
Iteration 277/1000 | Loss: 0.00007428
Iteration 278/1000 | Loss: 0.00039978
Iteration 279/1000 | Loss: 0.00009374
Iteration 280/1000 | Loss: 0.00006394
Iteration 281/1000 | Loss: 0.00006217
Iteration 282/1000 | Loss: 0.00006091
Iteration 283/1000 | Loss: 0.00015736
Iteration 284/1000 | Loss: 0.00005747
Iteration 285/1000 | Loss: 0.00005635
Iteration 286/1000 | Loss: 0.00033748
Iteration 287/1000 | Loss: 0.00005515
Iteration 288/1000 | Loss: 0.00010362
Iteration 289/1000 | Loss: 0.00005315
Iteration 290/1000 | Loss: 0.00005212
Iteration 291/1000 | Loss: 0.00005103
Iteration 292/1000 | Loss: 0.00005030
Iteration 293/1000 | Loss: 0.00004977
Iteration 294/1000 | Loss: 0.00004952
Iteration 295/1000 | Loss: 0.00004942
Iteration 296/1000 | Loss: 0.00004927
Iteration 297/1000 | Loss: 0.00004921
Iteration 298/1000 | Loss: 0.00004916
Iteration 299/1000 | Loss: 0.00004915
Iteration 300/1000 | Loss: 0.00004915
Iteration 301/1000 | Loss: 0.00004915
Iteration 302/1000 | Loss: 0.00004914
Iteration 303/1000 | Loss: 0.00004912
Iteration 304/1000 | Loss: 0.00004910
Iteration 305/1000 | Loss: 0.00004909
Iteration 306/1000 | Loss: 0.00004909
Iteration 307/1000 | Loss: 0.00004908
Iteration 308/1000 | Loss: 0.00004908
Iteration 309/1000 | Loss: 0.00004908
Iteration 310/1000 | Loss: 0.00004908
Iteration 311/1000 | Loss: 0.00004907
Iteration 312/1000 | Loss: 0.00004907
Iteration 313/1000 | Loss: 0.00004907
Iteration 314/1000 | Loss: 0.00004907
Iteration 315/1000 | Loss: 0.00004906
Iteration 316/1000 | Loss: 0.00004906
Iteration 317/1000 | Loss: 0.00004905
Iteration 318/1000 | Loss: 0.00004905
Iteration 319/1000 | Loss: 0.00004904
Iteration 320/1000 | Loss: 0.00004904
Iteration 321/1000 | Loss: 0.00004904
Iteration 322/1000 | Loss: 0.00004903
Iteration 323/1000 | Loss: 0.00004900
Iteration 324/1000 | Loss: 0.00004900
Iteration 325/1000 | Loss: 0.00004900
Iteration 326/1000 | Loss: 0.00004899
Iteration 327/1000 | Loss: 0.00004899
Iteration 328/1000 | Loss: 0.00004899
Iteration 329/1000 | Loss: 0.00004899
Iteration 330/1000 | Loss: 0.00004898
Iteration 331/1000 | Loss: 0.00004898
Iteration 332/1000 | Loss: 0.00004898
Iteration 333/1000 | Loss: 0.00004897
Iteration 334/1000 | Loss: 0.00004897
Iteration 335/1000 | Loss: 0.00004897
Iteration 336/1000 | Loss: 0.00004896
Iteration 337/1000 | Loss: 0.00004896
Iteration 338/1000 | Loss: 0.00004896
Iteration 339/1000 | Loss: 0.00004896
Iteration 340/1000 | Loss: 0.00004895
Iteration 341/1000 | Loss: 0.00004895
Iteration 342/1000 | Loss: 0.00004895
Iteration 343/1000 | Loss: 0.00004894
Iteration 344/1000 | Loss: 0.00004894
Iteration 345/1000 | Loss: 0.00004894
Iteration 346/1000 | Loss: 0.00004893
Iteration 347/1000 | Loss: 0.00004893
Iteration 348/1000 | Loss: 0.00004893
Iteration 349/1000 | Loss: 0.00004892
Iteration 350/1000 | Loss: 0.00004892
Iteration 351/1000 | Loss: 0.00004892
Iteration 352/1000 | Loss: 0.00004891
Iteration 353/1000 | Loss: 0.00004891
Iteration 354/1000 | Loss: 0.00004890
Iteration 355/1000 | Loss: 0.00004890
Iteration 356/1000 | Loss: 0.00004890
Iteration 357/1000 | Loss: 0.00004890
Iteration 358/1000 | Loss: 0.00004890
Iteration 359/1000 | Loss: 0.00004890
Iteration 360/1000 | Loss: 0.00004889
Iteration 361/1000 | Loss: 0.00004889
Iteration 362/1000 | Loss: 0.00004889
Iteration 363/1000 | Loss: 0.00004889
Iteration 364/1000 | Loss: 0.00004889
Iteration 365/1000 | Loss: 0.00004889
Iteration 366/1000 | Loss: 0.00004889
Iteration 367/1000 | Loss: 0.00004889
Iteration 368/1000 | Loss: 0.00004889
Iteration 369/1000 | Loss: 0.00004888
Iteration 370/1000 | Loss: 0.00004888
Iteration 371/1000 | Loss: 0.00004888
Iteration 372/1000 | Loss: 0.00004888
Iteration 373/1000 | Loss: 0.00004887
Iteration 374/1000 | Loss: 0.00004887
Iteration 375/1000 | Loss: 0.00004887
Iteration 376/1000 | Loss: 0.00004887
Iteration 377/1000 | Loss: 0.00004886
Iteration 378/1000 | Loss: 0.00004886
Iteration 379/1000 | Loss: 0.00004886
Iteration 380/1000 | Loss: 0.00004886
Iteration 381/1000 | Loss: 0.00004886
Iteration 382/1000 | Loss: 0.00004886
Iteration 383/1000 | Loss: 0.00004886
Iteration 384/1000 | Loss: 0.00004886
Iteration 385/1000 | Loss: 0.00004886
Iteration 386/1000 | Loss: 0.00004886
Iteration 387/1000 | Loss: 0.00004886
Iteration 388/1000 | Loss: 0.00004886
Iteration 389/1000 | Loss: 0.00004885
Iteration 390/1000 | Loss: 0.00004885
Iteration 391/1000 | Loss: 0.00004885
Iteration 392/1000 | Loss: 0.00004885
Iteration 393/1000 | Loss: 0.00004885
Iteration 394/1000 | Loss: 0.00004885
Iteration 395/1000 | Loss: 0.00004885
Iteration 396/1000 | Loss: 0.00004885
Iteration 397/1000 | Loss: 0.00004885
Iteration 398/1000 | Loss: 0.00004885
Iteration 399/1000 | Loss: 0.00004885
Iteration 400/1000 | Loss: 0.00004885
Iteration 401/1000 | Loss: 0.00004885
Iteration 402/1000 | Loss: 0.00004885
Iteration 403/1000 | Loss: 0.00004885
Iteration 404/1000 | Loss: 0.00004884
Iteration 405/1000 | Loss: 0.00004884
Iteration 406/1000 | Loss: 0.00004884
Iteration 407/1000 | Loss: 0.00004884
Iteration 408/1000 | Loss: 0.00004884
Iteration 409/1000 | Loss: 0.00004884
Iteration 410/1000 | Loss: 0.00004884
Iteration 411/1000 | Loss: 0.00004884
Iteration 412/1000 | Loss: 0.00004884
Iteration 413/1000 | Loss: 0.00004884
Iteration 414/1000 | Loss: 0.00004884
Iteration 415/1000 | Loss: 0.00004884
Iteration 416/1000 | Loss: 0.00004884
Iteration 417/1000 | Loss: 0.00004884
Iteration 418/1000 | Loss: 0.00004884
Iteration 419/1000 | Loss: 0.00004884
Iteration 420/1000 | Loss: 0.00004884
Iteration 421/1000 | Loss: 0.00004883
Iteration 422/1000 | Loss: 0.00004883
Iteration 423/1000 | Loss: 0.00004883
Iteration 424/1000 | Loss: 0.00004883
Iteration 425/1000 | Loss: 0.00004883
Iteration 426/1000 | Loss: 0.00004883
Iteration 427/1000 | Loss: 0.00004883
Iteration 428/1000 | Loss: 0.00004883
Iteration 429/1000 | Loss: 0.00004883
Iteration 430/1000 | Loss: 0.00004883
Iteration 431/1000 | Loss: 0.00004882
Iteration 432/1000 | Loss: 0.00004882
Iteration 433/1000 | Loss: 0.00004882
Iteration 434/1000 | Loss: 0.00004882
Iteration 435/1000 | Loss: 0.00004882
Iteration 436/1000 | Loss: 0.00004882
Iteration 437/1000 | Loss: 0.00004881
Iteration 438/1000 | Loss: 0.00004881
Iteration 439/1000 | Loss: 0.00004881
Iteration 440/1000 | Loss: 0.00004881
Iteration 441/1000 | Loss: 0.00004881
Iteration 442/1000 | Loss: 0.00004881
Iteration 443/1000 | Loss: 0.00004881
Iteration 444/1000 | Loss: 0.00004881
Iteration 445/1000 | Loss: 0.00004881
Iteration 446/1000 | Loss: 0.00004881
Iteration 447/1000 | Loss: 0.00004881
Iteration 448/1000 | Loss: 0.00004881
Iteration 449/1000 | Loss: 0.00004881
Iteration 450/1000 | Loss: 0.00004881
Iteration 451/1000 | Loss: 0.00004881
Iteration 452/1000 | Loss: 0.00004880
Iteration 453/1000 | Loss: 0.00004880
Iteration 454/1000 | Loss: 0.00004880
Iteration 455/1000 | Loss: 0.00004880
Iteration 456/1000 | Loss: 0.00004880
Iteration 457/1000 | Loss: 0.00004880
Iteration 458/1000 | Loss: 0.00004880
Iteration 459/1000 | Loss: 0.00004880
Iteration 460/1000 | Loss: 0.00004880
Iteration 461/1000 | Loss: 0.00004880
Iteration 462/1000 | Loss: 0.00004880
Iteration 463/1000 | Loss: 0.00004880
Iteration 464/1000 | Loss: 0.00004880
Iteration 465/1000 | Loss: 0.00004880
Iteration 466/1000 | Loss: 0.00004880
Iteration 467/1000 | Loss: 0.00004880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 467. Stopping optimization.
Last 5 losses: [4.879996049567126e-05, 4.879996049567126e-05, 4.879996049567126e-05, 4.879996049567126e-05, 4.879996049567126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.879996049567126e-05

Optimization complete. Final v2v error: 4.700928688049316 mm

Highest mean error: 12.835845947265625 mm for frame 2

Lowest mean error: 3.3276731967926025 mm for frame 188

Saving results

Total time: 534.3523194789886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082156
Iteration 2/25 | Loss: 0.00143904
Iteration 3/25 | Loss: 0.00114699
Iteration 4/25 | Loss: 0.00089973
Iteration 5/25 | Loss: 0.00082381
Iteration 6/25 | Loss: 0.00076635
Iteration 7/25 | Loss: 0.00079285
Iteration 8/25 | Loss: 0.00078951
Iteration 9/25 | Loss: 0.00075661
Iteration 10/25 | Loss: 0.00078012
Iteration 11/25 | Loss: 0.00075841
Iteration 12/25 | Loss: 0.00073240
Iteration 13/25 | Loss: 0.00072927
Iteration 14/25 | Loss: 0.00072374
Iteration 15/25 | Loss: 0.00072174
Iteration 16/25 | Loss: 0.00072233
Iteration 17/25 | Loss: 0.00072022
Iteration 18/25 | Loss: 0.00072164
Iteration 19/25 | Loss: 0.00072022
Iteration 20/25 | Loss: 0.00072067
Iteration 21/25 | Loss: 0.00073140
Iteration 22/25 | Loss: 0.00072735
Iteration 23/25 | Loss: 0.00072151
Iteration 24/25 | Loss: 0.00071497
Iteration 25/25 | Loss: 0.00071545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49734783
Iteration 2/25 | Loss: 0.00057388
Iteration 3/25 | Loss: 0.00057388
Iteration 4/25 | Loss: 0.00057388
Iteration 5/25 | Loss: 0.00057388
Iteration 6/25 | Loss: 0.00057388
Iteration 7/25 | Loss: 0.00057388
Iteration 8/25 | Loss: 0.00057388
Iteration 9/25 | Loss: 0.00057388
Iteration 10/25 | Loss: 0.00057388
Iteration 11/25 | Loss: 0.00057388
Iteration 12/25 | Loss: 0.00057388
Iteration 13/25 | Loss: 0.00057388
Iteration 14/25 | Loss: 0.00057388
Iteration 15/25 | Loss: 0.00057388
Iteration 16/25 | Loss: 0.00057388
Iteration 17/25 | Loss: 0.00057388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005738804466091096, 0.0005738804466091096, 0.0005738804466091096, 0.0005738804466091096, 0.0005738804466091096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005738804466091096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057388
Iteration 2/1000 | Loss: 0.00015174
Iteration 3/1000 | Loss: 0.00014999
Iteration 4/1000 | Loss: 0.00004104
Iteration 5/1000 | Loss: 0.00003449
Iteration 6/1000 | Loss: 0.00005540
Iteration 7/1000 | Loss: 0.00024574
Iteration 8/1000 | Loss: 0.00015199
Iteration 9/1000 | Loss: 0.00003634
Iteration 10/1000 | Loss: 0.00024894
Iteration 11/1000 | Loss: 0.00024559
Iteration 12/1000 | Loss: 0.00009570
Iteration 13/1000 | Loss: 0.00031729
Iteration 14/1000 | Loss: 0.00014911
Iteration 15/1000 | Loss: 0.00003293
Iteration 16/1000 | Loss: 0.00003557
Iteration 17/1000 | Loss: 0.00002408
Iteration 18/1000 | Loss: 0.00002268
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00056388
Iteration 22/1000 | Loss: 0.00077062
Iteration 23/1000 | Loss: 0.00018544
Iteration 24/1000 | Loss: 0.00026578
Iteration 25/1000 | Loss: 0.00057203
Iteration 26/1000 | Loss: 0.00007381
Iteration 27/1000 | Loss: 0.00018542
Iteration 28/1000 | Loss: 0.00034441
Iteration 29/1000 | Loss: 0.00030145
Iteration 30/1000 | Loss: 0.00034373
Iteration 31/1000 | Loss: 0.00022133
Iteration 32/1000 | Loss: 0.00003561
Iteration 33/1000 | Loss: 0.00036829
Iteration 34/1000 | Loss: 0.00006282
Iteration 35/1000 | Loss: 0.00002716
Iteration 36/1000 | Loss: 0.00002969
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00004688
Iteration 42/1000 | Loss: 0.00007587
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00007723
Iteration 47/1000 | Loss: 0.00002299
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00003321
Iteration 56/1000 | Loss: 0.00002240
Iteration 57/1000 | Loss: 0.00001509
Iteration 58/1000 | Loss: 0.00001509
Iteration 59/1000 | Loss: 0.00001508
Iteration 60/1000 | Loss: 0.00001508
Iteration 61/1000 | Loss: 0.00001508
Iteration 62/1000 | Loss: 0.00001508
Iteration 63/1000 | Loss: 0.00001508
Iteration 64/1000 | Loss: 0.00001508
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00002843
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4982294487708714e-05, 1.4982294487708714e-05, 1.4982294487708714e-05, 1.4982294487708714e-05, 1.4982294487708714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4982294487708714e-05

Optimization complete. Final v2v error: 3.2640535831451416 mm

Highest mean error: 4.5445332527160645 mm for frame 91

Lowest mean error: 3.0509963035583496 mm for frame 136

Saving results

Total time: 119.64941930770874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861450
Iteration 2/25 | Loss: 0.00118490
Iteration 3/25 | Loss: 0.00086570
Iteration 4/25 | Loss: 0.00080294
Iteration 5/25 | Loss: 0.00078915
Iteration 6/25 | Loss: 0.00078671
Iteration 7/25 | Loss: 0.00078613
Iteration 8/25 | Loss: 0.00078613
Iteration 9/25 | Loss: 0.00078613
Iteration 10/25 | Loss: 0.00078611
Iteration 11/25 | Loss: 0.00078611
Iteration 12/25 | Loss: 0.00078611
Iteration 13/25 | Loss: 0.00078611
Iteration 14/25 | Loss: 0.00078611
Iteration 15/25 | Loss: 0.00078611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007861139602027833, 0.0007861139602027833, 0.0007861139602027833, 0.0007861139602027833, 0.0007861139602027833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007861139602027833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05064249
Iteration 2/25 | Loss: 0.00039214
Iteration 3/25 | Loss: 0.00039213
Iteration 4/25 | Loss: 0.00039213
Iteration 5/25 | Loss: 0.00039212
Iteration 6/25 | Loss: 0.00039212
Iteration 7/25 | Loss: 0.00039212
Iteration 8/25 | Loss: 0.00039212
Iteration 9/25 | Loss: 0.00039212
Iteration 10/25 | Loss: 0.00039212
Iteration 11/25 | Loss: 0.00039212
Iteration 12/25 | Loss: 0.00039212
Iteration 13/25 | Loss: 0.00039212
Iteration 14/25 | Loss: 0.00039212
Iteration 15/25 | Loss: 0.00039212
Iteration 16/25 | Loss: 0.00039212
Iteration 17/25 | Loss: 0.00039212
Iteration 18/25 | Loss: 0.00039212
Iteration 19/25 | Loss: 0.00039212
Iteration 20/25 | Loss: 0.00039212
Iteration 21/25 | Loss: 0.00039212
Iteration 22/25 | Loss: 0.00039212
Iteration 23/25 | Loss: 0.00039212
Iteration 24/25 | Loss: 0.00039212
Iteration 25/25 | Loss: 0.00039212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039212
Iteration 2/1000 | Loss: 0.00006974
Iteration 3/1000 | Loss: 0.00004626
Iteration 4/1000 | Loss: 0.00003803
Iteration 5/1000 | Loss: 0.00003521
Iteration 6/1000 | Loss: 0.00003256
Iteration 7/1000 | Loss: 0.00003119
Iteration 8/1000 | Loss: 0.00003022
Iteration 9/1000 | Loss: 0.00002963
Iteration 10/1000 | Loss: 0.00002927
Iteration 11/1000 | Loss: 0.00002904
Iteration 12/1000 | Loss: 0.00002879
Iteration 13/1000 | Loss: 0.00002861
Iteration 14/1000 | Loss: 0.00002855
Iteration 15/1000 | Loss: 0.00002855
Iteration 16/1000 | Loss: 0.00002851
Iteration 17/1000 | Loss: 0.00002850
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002848
Iteration 20/1000 | Loss: 0.00002848
Iteration 21/1000 | Loss: 0.00002848
Iteration 22/1000 | Loss: 0.00002847
Iteration 23/1000 | Loss: 0.00002847
Iteration 24/1000 | Loss: 0.00002846
Iteration 25/1000 | Loss: 0.00002846
Iteration 26/1000 | Loss: 0.00002845
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002844
Iteration 29/1000 | Loss: 0.00002843
Iteration 30/1000 | Loss: 0.00002843
Iteration 31/1000 | Loss: 0.00002843
Iteration 32/1000 | Loss: 0.00002842
Iteration 33/1000 | Loss: 0.00002841
Iteration 34/1000 | Loss: 0.00002841
Iteration 35/1000 | Loss: 0.00002840
Iteration 36/1000 | Loss: 0.00002840
Iteration 37/1000 | Loss: 0.00002840
Iteration 38/1000 | Loss: 0.00002840
Iteration 39/1000 | Loss: 0.00002840
Iteration 40/1000 | Loss: 0.00002840
Iteration 41/1000 | Loss: 0.00002840
Iteration 42/1000 | Loss: 0.00002840
Iteration 43/1000 | Loss: 0.00002839
Iteration 44/1000 | Loss: 0.00002838
Iteration 45/1000 | Loss: 0.00002838
Iteration 46/1000 | Loss: 0.00002838
Iteration 47/1000 | Loss: 0.00002838
Iteration 48/1000 | Loss: 0.00002838
Iteration 49/1000 | Loss: 0.00002838
Iteration 50/1000 | Loss: 0.00002838
Iteration 51/1000 | Loss: 0.00002837
Iteration 52/1000 | Loss: 0.00002837
Iteration 53/1000 | Loss: 0.00002837
Iteration 54/1000 | Loss: 0.00002837
Iteration 55/1000 | Loss: 0.00002837
Iteration 56/1000 | Loss: 0.00002837
Iteration 57/1000 | Loss: 0.00002837
Iteration 58/1000 | Loss: 0.00002837
Iteration 59/1000 | Loss: 0.00002837
Iteration 60/1000 | Loss: 0.00002837
Iteration 61/1000 | Loss: 0.00002837
Iteration 62/1000 | Loss: 0.00002837
Iteration 63/1000 | Loss: 0.00002837
Iteration 64/1000 | Loss: 0.00002836
Iteration 65/1000 | Loss: 0.00002836
Iteration 66/1000 | Loss: 0.00002836
Iteration 67/1000 | Loss: 0.00002836
Iteration 68/1000 | Loss: 0.00002836
Iteration 69/1000 | Loss: 0.00002835
Iteration 70/1000 | Loss: 0.00002835
Iteration 71/1000 | Loss: 0.00002835
Iteration 72/1000 | Loss: 0.00002835
Iteration 73/1000 | Loss: 0.00002835
Iteration 74/1000 | Loss: 0.00002835
Iteration 75/1000 | Loss: 0.00002835
Iteration 76/1000 | Loss: 0.00002835
Iteration 77/1000 | Loss: 0.00002835
Iteration 78/1000 | Loss: 0.00002835
Iteration 79/1000 | Loss: 0.00002835
Iteration 80/1000 | Loss: 0.00002835
Iteration 81/1000 | Loss: 0.00002835
Iteration 82/1000 | Loss: 0.00002834
Iteration 83/1000 | Loss: 0.00002834
Iteration 84/1000 | Loss: 0.00002834
Iteration 85/1000 | Loss: 0.00002834
Iteration 86/1000 | Loss: 0.00002834
Iteration 87/1000 | Loss: 0.00002834
Iteration 88/1000 | Loss: 0.00002834
Iteration 89/1000 | Loss: 0.00002834
Iteration 90/1000 | Loss: 0.00002834
Iteration 91/1000 | Loss: 0.00002834
Iteration 92/1000 | Loss: 0.00002834
Iteration 93/1000 | Loss: 0.00002834
Iteration 94/1000 | Loss: 0.00002834
Iteration 95/1000 | Loss: 0.00002834
Iteration 96/1000 | Loss: 0.00002834
Iteration 97/1000 | Loss: 0.00002834
Iteration 98/1000 | Loss: 0.00002833
Iteration 99/1000 | Loss: 0.00002833
Iteration 100/1000 | Loss: 0.00002833
Iteration 101/1000 | Loss: 0.00002833
Iteration 102/1000 | Loss: 0.00002833
Iteration 103/1000 | Loss: 0.00002833
Iteration 104/1000 | Loss: 0.00002833
Iteration 105/1000 | Loss: 0.00002833
Iteration 106/1000 | Loss: 0.00002833
Iteration 107/1000 | Loss: 0.00002832
Iteration 108/1000 | Loss: 0.00002832
Iteration 109/1000 | Loss: 0.00002832
Iteration 110/1000 | Loss: 0.00002832
Iteration 111/1000 | Loss: 0.00002832
Iteration 112/1000 | Loss: 0.00002832
Iteration 113/1000 | Loss: 0.00002832
Iteration 114/1000 | Loss: 0.00002832
Iteration 115/1000 | Loss: 0.00002832
Iteration 116/1000 | Loss: 0.00002832
Iteration 117/1000 | Loss: 0.00002832
Iteration 118/1000 | Loss: 0.00002832
Iteration 119/1000 | Loss: 0.00002832
Iteration 120/1000 | Loss: 0.00002832
Iteration 121/1000 | Loss: 0.00002832
Iteration 122/1000 | Loss: 0.00002832
Iteration 123/1000 | Loss: 0.00002832
Iteration 124/1000 | Loss: 0.00002832
Iteration 125/1000 | Loss: 0.00002832
Iteration 126/1000 | Loss: 0.00002832
Iteration 127/1000 | Loss: 0.00002832
Iteration 128/1000 | Loss: 0.00002832
Iteration 129/1000 | Loss: 0.00002832
Iteration 130/1000 | Loss: 0.00002832
Iteration 131/1000 | Loss: 0.00002832
Iteration 132/1000 | Loss: 0.00002832
Iteration 133/1000 | Loss: 0.00002832
Iteration 134/1000 | Loss: 0.00002832
Iteration 135/1000 | Loss: 0.00002832
Iteration 136/1000 | Loss: 0.00002832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.8323525839368813e-05, 2.8323525839368813e-05, 2.8323525839368813e-05, 2.8323525839368813e-05, 2.8323525839368813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8323525839368813e-05

Optimization complete. Final v2v error: 4.431039333343506 mm

Highest mean error: 5.016737461090088 mm for frame 131

Lowest mean error: 4.089684009552002 mm for frame 31

Saving results

Total time: 36.001646757125854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428639
Iteration 2/25 | Loss: 0.00093796
Iteration 3/25 | Loss: 0.00072112
Iteration 4/25 | Loss: 0.00070824
Iteration 5/25 | Loss: 0.00070366
Iteration 6/25 | Loss: 0.00070249
Iteration 7/25 | Loss: 0.00070249
Iteration 8/25 | Loss: 0.00070249
Iteration 9/25 | Loss: 0.00070249
Iteration 10/25 | Loss: 0.00070249
Iteration 11/25 | Loss: 0.00070249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007024854421615601, 0.0007024854421615601, 0.0007024854421615601, 0.0007024854421615601, 0.0007024854421615601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007024854421615601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50989628
Iteration 2/25 | Loss: 0.00032609
Iteration 3/25 | Loss: 0.00032609
Iteration 4/25 | Loss: 0.00032609
Iteration 5/25 | Loss: 0.00032609
Iteration 6/25 | Loss: 0.00032609
Iteration 7/25 | Loss: 0.00032609
Iteration 8/25 | Loss: 0.00032609
Iteration 9/25 | Loss: 0.00032609
Iteration 10/25 | Loss: 0.00032609
Iteration 11/25 | Loss: 0.00032609
Iteration 12/25 | Loss: 0.00032609
Iteration 13/25 | Loss: 0.00032609
Iteration 14/25 | Loss: 0.00032609
Iteration 15/25 | Loss: 0.00032609
Iteration 16/25 | Loss: 0.00032609
Iteration 17/25 | Loss: 0.00032609
Iteration 18/25 | Loss: 0.00032609
Iteration 19/25 | Loss: 0.00032609
Iteration 20/25 | Loss: 0.00032609
Iteration 21/25 | Loss: 0.00032609
Iteration 22/25 | Loss: 0.00032609
Iteration 23/25 | Loss: 0.00032609
Iteration 24/25 | Loss: 0.00032609
Iteration 25/25 | Loss: 0.00032609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032609
Iteration 2/1000 | Loss: 0.00001934
Iteration 3/1000 | Loss: 0.00001579
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001381
Iteration 8/1000 | Loss: 0.00001368
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001353
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001351
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001347
Iteration 19/1000 | Loss: 0.00001347
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001339
Iteration 63/1000 | Loss: 0.00001339
Iteration 64/1000 | Loss: 0.00001339
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001338
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001338
Iteration 72/1000 | Loss: 0.00001338
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001336
Iteration 108/1000 | Loss: 0.00001336
Iteration 109/1000 | Loss: 0.00001336
Iteration 110/1000 | Loss: 0.00001336
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001336
Iteration 116/1000 | Loss: 0.00001336
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.3364010555960704e-05, 1.3364010555960704e-05, 1.3364010555960704e-05, 1.3364010555960704e-05, 1.3364010555960704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3364010555960704e-05

Optimization complete. Final v2v error: 3.0809597969055176 mm

Highest mean error: 3.166463851928711 mm for frame 171

Lowest mean error: 3.034287691116333 mm for frame 66

Saving results

Total time: 28.430598735809326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794046
Iteration 2/25 | Loss: 0.00193513
Iteration 3/25 | Loss: 0.00103551
Iteration 4/25 | Loss: 0.00084853
Iteration 5/25 | Loss: 0.00076994
Iteration 6/25 | Loss: 0.00075236
Iteration 7/25 | Loss: 0.00075716
Iteration 8/25 | Loss: 0.00074609
Iteration 9/25 | Loss: 0.00074778
Iteration 10/25 | Loss: 0.00075043
Iteration 11/25 | Loss: 0.00075084
Iteration 12/25 | Loss: 0.00074555
Iteration 13/25 | Loss: 0.00074311
Iteration 14/25 | Loss: 0.00074467
Iteration 15/25 | Loss: 0.00074374
Iteration 16/25 | Loss: 0.00074243
Iteration 17/25 | Loss: 0.00074146
Iteration 18/25 | Loss: 0.00074109
Iteration 19/25 | Loss: 0.00074143
Iteration 20/25 | Loss: 0.00074020
Iteration 21/25 | Loss: 0.00073950
Iteration 22/25 | Loss: 0.00073925
Iteration 23/25 | Loss: 0.00073910
Iteration 24/25 | Loss: 0.00073902
Iteration 25/25 | Loss: 0.00073897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18728113
Iteration 2/25 | Loss: 0.00042933
Iteration 3/25 | Loss: 0.00042931
Iteration 4/25 | Loss: 0.00042931
Iteration 5/25 | Loss: 0.00042930
Iteration 6/25 | Loss: 0.00042930
Iteration 7/25 | Loss: 0.00042930
Iteration 8/25 | Loss: 0.00042930
Iteration 9/25 | Loss: 0.00042930
Iteration 10/25 | Loss: 0.00042930
Iteration 11/25 | Loss: 0.00042930
Iteration 12/25 | Loss: 0.00042930
Iteration 13/25 | Loss: 0.00042930
Iteration 14/25 | Loss: 0.00042930
Iteration 15/25 | Loss: 0.00042930
Iteration 16/25 | Loss: 0.00042930
Iteration 17/25 | Loss: 0.00042930
Iteration 18/25 | Loss: 0.00042930
Iteration 19/25 | Loss: 0.00042930
Iteration 20/25 | Loss: 0.00042930
Iteration 21/25 | Loss: 0.00042930
Iteration 22/25 | Loss: 0.00042930
Iteration 23/25 | Loss: 0.00042930
Iteration 24/25 | Loss: 0.00042930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00042930286144837737, 0.00042930286144837737, 0.00042930286144837737, 0.00042930286144837737, 0.00042930286144837737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042930286144837737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042930
Iteration 2/1000 | Loss: 0.00002609
Iteration 3/1000 | Loss: 0.00001749
Iteration 4/1000 | Loss: 0.00001600
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001457
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00011109
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001227
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001226
Iteration 29/1000 | Loss: 0.00001226
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001223
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001206
Iteration 85/1000 | Loss: 0.00001206
Iteration 86/1000 | Loss: 0.00001206
Iteration 87/1000 | Loss: 0.00001206
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001205
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001201
Iteration 118/1000 | Loss: 0.00001201
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001200
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001200
Iteration 126/1000 | Loss: 0.00001200
Iteration 127/1000 | Loss: 0.00001200
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001196
Iteration 177/1000 | Loss: 0.00001196
Iteration 178/1000 | Loss: 0.00001196
Iteration 179/1000 | Loss: 0.00001196
Iteration 180/1000 | Loss: 0.00001196
Iteration 181/1000 | Loss: 0.00001196
Iteration 182/1000 | Loss: 0.00001196
Iteration 183/1000 | Loss: 0.00001196
Iteration 184/1000 | Loss: 0.00001196
Iteration 185/1000 | Loss: 0.00001196
Iteration 186/1000 | Loss: 0.00001196
Iteration 187/1000 | Loss: 0.00001196
Iteration 188/1000 | Loss: 0.00001196
Iteration 189/1000 | Loss: 0.00001196
Iteration 190/1000 | Loss: 0.00001196
Iteration 191/1000 | Loss: 0.00001196
Iteration 192/1000 | Loss: 0.00001196
Iteration 193/1000 | Loss: 0.00001196
Iteration 194/1000 | Loss: 0.00001196
Iteration 195/1000 | Loss: 0.00001196
Iteration 196/1000 | Loss: 0.00001196
Iteration 197/1000 | Loss: 0.00001196
Iteration 198/1000 | Loss: 0.00001196
Iteration 199/1000 | Loss: 0.00001196
Iteration 200/1000 | Loss: 0.00001196
Iteration 201/1000 | Loss: 0.00001196
Iteration 202/1000 | Loss: 0.00001196
Iteration 203/1000 | Loss: 0.00001196
Iteration 204/1000 | Loss: 0.00001196
Iteration 205/1000 | Loss: 0.00001196
Iteration 206/1000 | Loss: 0.00001196
Iteration 207/1000 | Loss: 0.00001196
Iteration 208/1000 | Loss: 0.00001196
Iteration 209/1000 | Loss: 0.00001196
Iteration 210/1000 | Loss: 0.00001196
Iteration 211/1000 | Loss: 0.00001196
Iteration 212/1000 | Loss: 0.00001196
Iteration 213/1000 | Loss: 0.00001196
Iteration 214/1000 | Loss: 0.00001196
Iteration 215/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.1955652553297114e-05, 1.1955652553297114e-05, 1.1955652553297114e-05, 1.1955652553297114e-05, 1.1955652553297114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1955652553297114e-05

Optimization complete. Final v2v error: 2.939213275909424 mm

Highest mean error: 3.845639944076538 mm for frame 209

Lowest mean error: 2.693361520767212 mm for frame 18

Saving results

Total time: 87.10512471199036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627604
Iteration 2/25 | Loss: 0.00132176
Iteration 3/25 | Loss: 0.00101101
Iteration 4/25 | Loss: 0.00093409
Iteration 5/25 | Loss: 0.00091301
Iteration 6/25 | Loss: 0.00092625
Iteration 7/25 | Loss: 0.00089665
Iteration 8/25 | Loss: 0.00085781
Iteration 9/25 | Loss: 0.00084192
Iteration 10/25 | Loss: 0.00083356
Iteration 11/25 | Loss: 0.00083078
Iteration 12/25 | Loss: 0.00083008
Iteration 13/25 | Loss: 0.00082995
Iteration 14/25 | Loss: 0.00082995
Iteration 15/25 | Loss: 0.00082995
Iteration 16/25 | Loss: 0.00082995
Iteration 17/25 | Loss: 0.00082995
Iteration 18/25 | Loss: 0.00082995
Iteration 19/25 | Loss: 0.00082995
Iteration 20/25 | Loss: 0.00082995
Iteration 21/25 | Loss: 0.00082995
Iteration 22/25 | Loss: 0.00082995
Iteration 23/25 | Loss: 0.00082995
Iteration 24/25 | Loss: 0.00082995
Iteration 25/25 | Loss: 0.00082994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38144243
Iteration 2/25 | Loss: 0.00050945
Iteration 3/25 | Loss: 0.00050936
Iteration 4/25 | Loss: 0.00050936
Iteration 5/25 | Loss: 0.00050936
Iteration 6/25 | Loss: 0.00050936
Iteration 7/25 | Loss: 0.00050936
Iteration 8/25 | Loss: 0.00050936
Iteration 9/25 | Loss: 0.00050936
Iteration 10/25 | Loss: 0.00050936
Iteration 11/25 | Loss: 0.00050936
Iteration 12/25 | Loss: 0.00050936
Iteration 13/25 | Loss: 0.00050936
Iteration 14/25 | Loss: 0.00050936
Iteration 15/25 | Loss: 0.00050936
Iteration 16/25 | Loss: 0.00050936
Iteration 17/25 | Loss: 0.00050936
Iteration 18/25 | Loss: 0.00050936
Iteration 19/25 | Loss: 0.00050936
Iteration 20/25 | Loss: 0.00050936
Iteration 21/25 | Loss: 0.00050936
Iteration 22/25 | Loss: 0.00050936
Iteration 23/25 | Loss: 0.00050936
Iteration 24/25 | Loss: 0.00050936
Iteration 25/25 | Loss: 0.00050936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050936
Iteration 2/1000 | Loss: 0.00008723
Iteration 3/1000 | Loss: 0.00004739
Iteration 4/1000 | Loss: 0.00003556
Iteration 5/1000 | Loss: 0.00003050
Iteration 6/1000 | Loss: 0.00002874
Iteration 7/1000 | Loss: 0.00002805
Iteration 8/1000 | Loss: 0.00002735
Iteration 9/1000 | Loss: 0.00002676
Iteration 10/1000 | Loss: 0.00002643
Iteration 11/1000 | Loss: 0.00002616
Iteration 12/1000 | Loss: 0.00002596
Iteration 13/1000 | Loss: 0.00002575
Iteration 14/1000 | Loss: 0.00002569
Iteration 15/1000 | Loss: 0.00002566
Iteration 16/1000 | Loss: 0.00002552
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002538
Iteration 19/1000 | Loss: 0.00002538
Iteration 20/1000 | Loss: 0.00002537
Iteration 21/1000 | Loss: 0.00002533
Iteration 22/1000 | Loss: 0.00002533
Iteration 23/1000 | Loss: 0.00002529
Iteration 24/1000 | Loss: 0.00002528
Iteration 25/1000 | Loss: 0.00002528
Iteration 26/1000 | Loss: 0.00002527
Iteration 27/1000 | Loss: 0.00002527
Iteration 28/1000 | Loss: 0.00002526
Iteration 29/1000 | Loss: 0.00002526
Iteration 30/1000 | Loss: 0.00002525
Iteration 31/1000 | Loss: 0.00002525
Iteration 32/1000 | Loss: 0.00002525
Iteration 33/1000 | Loss: 0.00002524
Iteration 34/1000 | Loss: 0.00002523
Iteration 35/1000 | Loss: 0.00002522
Iteration 36/1000 | Loss: 0.00002522
Iteration 37/1000 | Loss: 0.00002522
Iteration 38/1000 | Loss: 0.00002521
Iteration 39/1000 | Loss: 0.00002521
Iteration 40/1000 | Loss: 0.00002521
Iteration 41/1000 | Loss: 0.00002519
Iteration 42/1000 | Loss: 0.00002519
Iteration 43/1000 | Loss: 0.00002519
Iteration 44/1000 | Loss: 0.00002519
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00002519
Iteration 48/1000 | Loss: 0.00002519
Iteration 49/1000 | Loss: 0.00002519
Iteration 50/1000 | Loss: 0.00002518
Iteration 51/1000 | Loss: 0.00002518
Iteration 52/1000 | Loss: 0.00002518
Iteration 53/1000 | Loss: 0.00002518
Iteration 54/1000 | Loss: 0.00002518
Iteration 55/1000 | Loss: 0.00002518
Iteration 56/1000 | Loss: 0.00002518
Iteration 57/1000 | Loss: 0.00002518
Iteration 58/1000 | Loss: 0.00002518
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002517
Iteration 62/1000 | Loss: 0.00002517
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002515
Iteration 65/1000 | Loss: 0.00002515
Iteration 66/1000 | Loss: 0.00002514
Iteration 67/1000 | Loss: 0.00002514
Iteration 68/1000 | Loss: 0.00002514
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002512
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002511
Iteration 76/1000 | Loss: 0.00002511
Iteration 77/1000 | Loss: 0.00002511
Iteration 78/1000 | Loss: 0.00002511
Iteration 79/1000 | Loss: 0.00002510
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002509
Iteration 82/1000 | Loss: 0.00002509
Iteration 83/1000 | Loss: 0.00002507
Iteration 84/1000 | Loss: 0.00002507
Iteration 85/1000 | Loss: 0.00002507
Iteration 86/1000 | Loss: 0.00002507
Iteration 87/1000 | Loss: 0.00002507
Iteration 88/1000 | Loss: 0.00002507
Iteration 89/1000 | Loss: 0.00002506
Iteration 90/1000 | Loss: 0.00002506
Iteration 91/1000 | Loss: 0.00002505
Iteration 92/1000 | Loss: 0.00002505
Iteration 93/1000 | Loss: 0.00002505
Iteration 94/1000 | Loss: 0.00002504
Iteration 95/1000 | Loss: 0.00002504
Iteration 96/1000 | Loss: 0.00002503
Iteration 97/1000 | Loss: 0.00002503
Iteration 98/1000 | Loss: 0.00002502
Iteration 99/1000 | Loss: 0.00002502
Iteration 100/1000 | Loss: 0.00002502
Iteration 101/1000 | Loss: 0.00002501
Iteration 102/1000 | Loss: 0.00002501
Iteration 103/1000 | Loss: 0.00002501
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002500
Iteration 107/1000 | Loss: 0.00002500
Iteration 108/1000 | Loss: 0.00002499
Iteration 109/1000 | Loss: 0.00002499
Iteration 110/1000 | Loss: 0.00002499
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002498
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002498
Iteration 115/1000 | Loss: 0.00002498
Iteration 116/1000 | Loss: 0.00002498
Iteration 117/1000 | Loss: 0.00002498
Iteration 118/1000 | Loss: 0.00002498
Iteration 119/1000 | Loss: 0.00002498
Iteration 120/1000 | Loss: 0.00002498
Iteration 121/1000 | Loss: 0.00002497
Iteration 122/1000 | Loss: 0.00002497
Iteration 123/1000 | Loss: 0.00002497
Iteration 124/1000 | Loss: 0.00002497
Iteration 125/1000 | Loss: 0.00002497
Iteration 126/1000 | Loss: 0.00002497
Iteration 127/1000 | Loss: 0.00002497
Iteration 128/1000 | Loss: 0.00002497
Iteration 129/1000 | Loss: 0.00002497
Iteration 130/1000 | Loss: 0.00002496
Iteration 131/1000 | Loss: 0.00002496
Iteration 132/1000 | Loss: 0.00002496
Iteration 133/1000 | Loss: 0.00002496
Iteration 134/1000 | Loss: 0.00002496
Iteration 135/1000 | Loss: 0.00002496
Iteration 136/1000 | Loss: 0.00002496
Iteration 137/1000 | Loss: 0.00002496
Iteration 138/1000 | Loss: 0.00002496
Iteration 139/1000 | Loss: 0.00002496
Iteration 140/1000 | Loss: 0.00002495
Iteration 141/1000 | Loss: 0.00002495
Iteration 142/1000 | Loss: 0.00002495
Iteration 143/1000 | Loss: 0.00002495
Iteration 144/1000 | Loss: 0.00002495
Iteration 145/1000 | Loss: 0.00002495
Iteration 146/1000 | Loss: 0.00002495
Iteration 147/1000 | Loss: 0.00002495
Iteration 148/1000 | Loss: 0.00002495
Iteration 149/1000 | Loss: 0.00002495
Iteration 150/1000 | Loss: 0.00002495
Iteration 151/1000 | Loss: 0.00002495
Iteration 152/1000 | Loss: 0.00002495
Iteration 153/1000 | Loss: 0.00002495
Iteration 154/1000 | Loss: 0.00002495
Iteration 155/1000 | Loss: 0.00002495
Iteration 156/1000 | Loss: 0.00002495
Iteration 157/1000 | Loss: 0.00002495
Iteration 158/1000 | Loss: 0.00002494
Iteration 159/1000 | Loss: 0.00002494
Iteration 160/1000 | Loss: 0.00002494
Iteration 161/1000 | Loss: 0.00002494
Iteration 162/1000 | Loss: 0.00002494
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00002494
Iteration 165/1000 | Loss: 0.00002494
Iteration 166/1000 | Loss: 0.00002494
Iteration 167/1000 | Loss: 0.00002494
Iteration 168/1000 | Loss: 0.00002494
Iteration 169/1000 | Loss: 0.00002494
Iteration 170/1000 | Loss: 0.00002494
Iteration 171/1000 | Loss: 0.00002494
Iteration 172/1000 | Loss: 0.00002493
Iteration 173/1000 | Loss: 0.00002493
Iteration 174/1000 | Loss: 0.00002493
Iteration 175/1000 | Loss: 0.00002493
Iteration 176/1000 | Loss: 0.00002493
Iteration 177/1000 | Loss: 0.00002493
Iteration 178/1000 | Loss: 0.00002493
Iteration 179/1000 | Loss: 0.00002493
Iteration 180/1000 | Loss: 0.00002493
Iteration 181/1000 | Loss: 0.00002493
Iteration 182/1000 | Loss: 0.00002493
Iteration 183/1000 | Loss: 0.00002493
Iteration 184/1000 | Loss: 0.00002493
Iteration 185/1000 | Loss: 0.00002492
Iteration 186/1000 | Loss: 0.00002492
Iteration 187/1000 | Loss: 0.00002492
Iteration 188/1000 | Loss: 0.00002492
Iteration 189/1000 | Loss: 0.00002492
Iteration 190/1000 | Loss: 0.00002492
Iteration 191/1000 | Loss: 0.00002492
Iteration 192/1000 | Loss: 0.00002492
Iteration 193/1000 | Loss: 0.00002491
Iteration 194/1000 | Loss: 0.00002491
Iteration 195/1000 | Loss: 0.00002491
Iteration 196/1000 | Loss: 0.00002491
Iteration 197/1000 | Loss: 0.00002491
Iteration 198/1000 | Loss: 0.00002491
Iteration 199/1000 | Loss: 0.00002491
Iteration 200/1000 | Loss: 0.00002491
Iteration 201/1000 | Loss: 0.00002490
Iteration 202/1000 | Loss: 0.00002490
Iteration 203/1000 | Loss: 0.00002490
Iteration 204/1000 | Loss: 0.00002490
Iteration 205/1000 | Loss: 0.00002490
Iteration 206/1000 | Loss: 0.00002490
Iteration 207/1000 | Loss: 0.00002490
Iteration 208/1000 | Loss: 0.00002490
Iteration 209/1000 | Loss: 0.00002490
Iteration 210/1000 | Loss: 0.00002490
Iteration 211/1000 | Loss: 0.00002490
Iteration 212/1000 | Loss: 0.00002490
Iteration 213/1000 | Loss: 0.00002490
Iteration 214/1000 | Loss: 0.00002489
Iteration 215/1000 | Loss: 0.00002489
Iteration 216/1000 | Loss: 0.00002489
Iteration 217/1000 | Loss: 0.00002489
Iteration 218/1000 | Loss: 0.00002489
Iteration 219/1000 | Loss: 0.00002489
Iteration 220/1000 | Loss: 0.00002489
Iteration 221/1000 | Loss: 0.00002489
Iteration 222/1000 | Loss: 0.00002489
Iteration 223/1000 | Loss: 0.00002489
Iteration 224/1000 | Loss: 0.00002489
Iteration 225/1000 | Loss: 0.00002489
Iteration 226/1000 | Loss: 0.00002489
Iteration 227/1000 | Loss: 0.00002489
Iteration 228/1000 | Loss: 0.00002489
Iteration 229/1000 | Loss: 0.00002489
Iteration 230/1000 | Loss: 0.00002489
Iteration 231/1000 | Loss: 0.00002489
Iteration 232/1000 | Loss: 0.00002489
Iteration 233/1000 | Loss: 0.00002489
Iteration 234/1000 | Loss: 0.00002489
Iteration 235/1000 | Loss: 0.00002488
Iteration 236/1000 | Loss: 0.00002488
Iteration 237/1000 | Loss: 0.00002488
Iteration 238/1000 | Loss: 0.00002488
Iteration 239/1000 | Loss: 0.00002488
Iteration 240/1000 | Loss: 0.00002488
Iteration 241/1000 | Loss: 0.00002488
Iteration 242/1000 | Loss: 0.00002488
Iteration 243/1000 | Loss: 0.00002488
Iteration 244/1000 | Loss: 0.00002488
Iteration 245/1000 | Loss: 0.00002488
Iteration 246/1000 | Loss: 0.00002488
Iteration 247/1000 | Loss: 0.00002488
Iteration 248/1000 | Loss: 0.00002488
Iteration 249/1000 | Loss: 0.00002488
Iteration 250/1000 | Loss: 0.00002488
Iteration 251/1000 | Loss: 0.00002488
Iteration 252/1000 | Loss: 0.00002488
Iteration 253/1000 | Loss: 0.00002488
Iteration 254/1000 | Loss: 0.00002488
Iteration 255/1000 | Loss: 0.00002488
Iteration 256/1000 | Loss: 0.00002488
Iteration 257/1000 | Loss: 0.00002487
Iteration 258/1000 | Loss: 0.00002487
Iteration 259/1000 | Loss: 0.00002487
Iteration 260/1000 | Loss: 0.00002487
Iteration 261/1000 | Loss: 0.00002487
Iteration 262/1000 | Loss: 0.00002487
Iteration 263/1000 | Loss: 0.00002487
Iteration 264/1000 | Loss: 0.00002487
Iteration 265/1000 | Loss: 0.00002487
Iteration 266/1000 | Loss: 0.00002487
Iteration 267/1000 | Loss: 0.00002487
Iteration 268/1000 | Loss: 0.00002487
Iteration 269/1000 | Loss: 0.00002487
Iteration 270/1000 | Loss: 0.00002487
Iteration 271/1000 | Loss: 0.00002487
Iteration 272/1000 | Loss: 0.00002487
Iteration 273/1000 | Loss: 0.00002487
Iteration 274/1000 | Loss: 0.00002487
Iteration 275/1000 | Loss: 0.00002487
Iteration 276/1000 | Loss: 0.00002487
Iteration 277/1000 | Loss: 0.00002487
Iteration 278/1000 | Loss: 0.00002487
Iteration 279/1000 | Loss: 0.00002487
Iteration 280/1000 | Loss: 0.00002486
Iteration 281/1000 | Loss: 0.00002486
Iteration 282/1000 | Loss: 0.00002486
Iteration 283/1000 | Loss: 0.00002486
Iteration 284/1000 | Loss: 0.00002486
Iteration 285/1000 | Loss: 0.00002486
Iteration 286/1000 | Loss: 0.00002486
Iteration 287/1000 | Loss: 0.00002486
Iteration 288/1000 | Loss: 0.00002486
Iteration 289/1000 | Loss: 0.00002486
Iteration 290/1000 | Loss: 0.00002486
Iteration 291/1000 | Loss: 0.00002486
Iteration 292/1000 | Loss: 0.00002486
Iteration 293/1000 | Loss: 0.00002486
Iteration 294/1000 | Loss: 0.00002486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.4862423742888495e-05, 2.4862423742888495e-05, 2.4862423742888495e-05, 2.4862423742888495e-05, 2.4862423742888495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4862423742888495e-05

Optimization complete. Final v2v error: 3.772587537765503 mm

Highest mean error: 5.710775375366211 mm for frame 118

Lowest mean error: 2.858729600906372 mm for frame 135

Saving results

Total time: 64.96549677848816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784168
Iteration 2/25 | Loss: 0.00131661
Iteration 3/25 | Loss: 0.00082283
Iteration 4/25 | Loss: 0.00074912
Iteration 5/25 | Loss: 0.00073028
Iteration 6/25 | Loss: 0.00072531
Iteration 7/25 | Loss: 0.00072446
Iteration 8/25 | Loss: 0.00072436
Iteration 9/25 | Loss: 0.00072436
Iteration 10/25 | Loss: 0.00072436
Iteration 11/25 | Loss: 0.00072436
Iteration 12/25 | Loss: 0.00072436
Iteration 13/25 | Loss: 0.00072436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007243597065098584, 0.0007243597065098584, 0.0007243597065098584, 0.0007243597065098584, 0.0007243597065098584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007243597065098584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.20814657
Iteration 2/25 | Loss: 0.00034972
Iteration 3/25 | Loss: 0.00034963
Iteration 4/25 | Loss: 0.00034963
Iteration 5/25 | Loss: 0.00034962
Iteration 6/25 | Loss: 0.00034962
Iteration 7/25 | Loss: 0.00034962
Iteration 8/25 | Loss: 0.00034962
Iteration 9/25 | Loss: 0.00034962
Iteration 10/25 | Loss: 0.00034962
Iteration 11/25 | Loss: 0.00034962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0003496243734844029, 0.0003496243734844029, 0.0003496243734844029, 0.0003496243734844029, 0.0003496243734844029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003496243734844029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034962
Iteration 2/1000 | Loss: 0.00003123
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00001960
Iteration 5/1000 | Loss: 0.00001842
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001720
Iteration 8/1000 | Loss: 0.00001689
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001635
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001618
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001603
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001589
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001582
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001581
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001578
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001577
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001572
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001572
Iteration 98/1000 | Loss: 0.00001572
Iteration 99/1000 | Loss: 0.00001572
Iteration 100/1000 | Loss: 0.00001572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.5715777408331633e-05, 1.5715777408331633e-05, 1.5715777408331633e-05, 1.5715777408331633e-05, 1.5715777408331633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5715777408331633e-05

Optimization complete. Final v2v error: 3.291140079498291 mm

Highest mean error: 4.668299198150635 mm for frame 112

Lowest mean error: 2.63012957572937 mm for frame 116

Saving results

Total time: 38.12441802024841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007500
Iteration 2/25 | Loss: 0.01007499
Iteration 3/25 | Loss: 0.01007499
Iteration 4/25 | Loss: 0.01007499
Iteration 5/25 | Loss: 0.01007499
Iteration 6/25 | Loss: 0.01007499
Iteration 7/25 | Loss: 0.01007499
Iteration 8/25 | Loss: 0.00285563
Iteration 9/25 | Loss: 0.00185764
Iteration 10/25 | Loss: 0.00171696
Iteration 11/25 | Loss: 0.00168786
Iteration 12/25 | Loss: 0.00165373
Iteration 13/25 | Loss: 0.00163990
Iteration 14/25 | Loss: 0.00163266
Iteration 15/25 | Loss: 0.00163187
Iteration 16/25 | Loss: 0.00162874
Iteration 17/25 | Loss: 0.00162689
Iteration 18/25 | Loss: 0.00162669
Iteration 19/25 | Loss: 0.00162634
Iteration 20/25 | Loss: 0.00162561
Iteration 21/25 | Loss: 0.00162498
Iteration 22/25 | Loss: 0.00162482
Iteration 23/25 | Loss: 0.00162479
Iteration 24/25 | Loss: 0.00162478
Iteration 25/25 | Loss: 0.00162478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50720668
Iteration 2/25 | Loss: 0.00861016
Iteration 3/25 | Loss: 0.00816333
Iteration 4/25 | Loss: 0.00815569
Iteration 5/25 | Loss: 0.00815569
Iteration 6/25 | Loss: 0.00815569
Iteration 7/25 | Loss: 0.00815569
Iteration 8/25 | Loss: 0.00815569
Iteration 9/25 | Loss: 0.00815569
Iteration 10/25 | Loss: 0.00815569
Iteration 11/25 | Loss: 0.00815569
Iteration 12/25 | Loss: 0.00815569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.008155686780810356, 0.008155686780810356, 0.008155686780810356, 0.008155686780810356, 0.008155686780810356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008155686780810356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00815569
Iteration 2/1000 | Loss: 0.01115848
Iteration 3/1000 | Loss: 0.00573836
Iteration 4/1000 | Loss: 0.00113089
Iteration 5/1000 | Loss: 0.00087235
Iteration 6/1000 | Loss: 0.00109563
Iteration 7/1000 | Loss: 0.00760880
Iteration 8/1000 | Loss: 0.00245982
Iteration 9/1000 | Loss: 0.00081567
Iteration 10/1000 | Loss: 0.00057821
Iteration 11/1000 | Loss: 0.00201546
Iteration 12/1000 | Loss: 0.00062872
Iteration 13/1000 | Loss: 0.00120485
Iteration 14/1000 | Loss: 0.00093721
Iteration 15/1000 | Loss: 0.00066255
Iteration 16/1000 | Loss: 0.00350340
Iteration 17/1000 | Loss: 0.00177606
Iteration 18/1000 | Loss: 0.00157571
Iteration 19/1000 | Loss: 0.00134872
Iteration 20/1000 | Loss: 0.00084197
Iteration 21/1000 | Loss: 0.00076478
Iteration 22/1000 | Loss: 0.00025472
Iteration 23/1000 | Loss: 0.00038038
Iteration 24/1000 | Loss: 0.00089738
Iteration 25/1000 | Loss: 0.00436198
Iteration 26/1000 | Loss: 0.00130701
Iteration 27/1000 | Loss: 0.00155771
Iteration 28/1000 | Loss: 0.00107556
Iteration 29/1000 | Loss: 0.00064073
Iteration 30/1000 | Loss: 0.00084814
Iteration 31/1000 | Loss: 0.00058634
Iteration 32/1000 | Loss: 0.00042463
Iteration 33/1000 | Loss: 0.00189290
Iteration 34/1000 | Loss: 0.00061497
Iteration 35/1000 | Loss: 0.00041407
Iteration 36/1000 | Loss: 0.00055947
Iteration 37/1000 | Loss: 0.00093098
Iteration 38/1000 | Loss: 0.00018001
Iteration 39/1000 | Loss: 0.00029580
Iteration 40/1000 | Loss: 0.00062261
Iteration 41/1000 | Loss: 0.00154474
Iteration 42/1000 | Loss: 0.00321238
Iteration 43/1000 | Loss: 0.00254265
Iteration 44/1000 | Loss: 0.00285546
Iteration 45/1000 | Loss: 0.00234763
Iteration 46/1000 | Loss: 0.00411206
Iteration 47/1000 | Loss: 0.00056342
Iteration 48/1000 | Loss: 0.00011924
Iteration 49/1000 | Loss: 0.00037448
Iteration 50/1000 | Loss: 0.00022245
Iteration 51/1000 | Loss: 0.00048321
Iteration 52/1000 | Loss: 0.00063201
Iteration 53/1000 | Loss: 0.00007151
Iteration 54/1000 | Loss: 0.00012277
Iteration 55/1000 | Loss: 0.00005946
Iteration 56/1000 | Loss: 0.00032890
Iteration 57/1000 | Loss: 0.00029527
Iteration 58/1000 | Loss: 0.00005981
Iteration 59/1000 | Loss: 0.00009081
Iteration 60/1000 | Loss: 0.00009756
Iteration 61/1000 | Loss: 0.00004279
Iteration 62/1000 | Loss: 0.00008244
Iteration 63/1000 | Loss: 0.00004042
Iteration 64/1000 | Loss: 0.00018551
Iteration 65/1000 | Loss: 0.00033779
Iteration 66/1000 | Loss: 0.00050129
Iteration 67/1000 | Loss: 0.00028185
Iteration 68/1000 | Loss: 0.00045946
Iteration 69/1000 | Loss: 0.00022626
Iteration 70/1000 | Loss: 0.00016268
Iteration 71/1000 | Loss: 0.00013136
Iteration 72/1000 | Loss: 0.00017555
Iteration 73/1000 | Loss: 0.00005594
Iteration 74/1000 | Loss: 0.00013156
Iteration 75/1000 | Loss: 0.00021710
Iteration 76/1000 | Loss: 0.00009616
Iteration 77/1000 | Loss: 0.00005687
Iteration 78/1000 | Loss: 0.00010337
Iteration 79/1000 | Loss: 0.00011572
Iteration 80/1000 | Loss: 0.00016465
Iteration 81/1000 | Loss: 0.00044996
Iteration 82/1000 | Loss: 0.00004559
Iteration 83/1000 | Loss: 0.00018243
Iteration 84/1000 | Loss: 0.00004401
Iteration 85/1000 | Loss: 0.00003500
Iteration 86/1000 | Loss: 0.00012030
Iteration 87/1000 | Loss: 0.00003087
Iteration 88/1000 | Loss: 0.00002955
Iteration 89/1000 | Loss: 0.00021477
Iteration 90/1000 | Loss: 0.00003211
Iteration 91/1000 | Loss: 0.00002914
Iteration 92/1000 | Loss: 0.00002839
Iteration 93/1000 | Loss: 0.00002786
Iteration 94/1000 | Loss: 0.00012661
Iteration 95/1000 | Loss: 0.00003189
Iteration 96/1000 | Loss: 0.00002815
Iteration 97/1000 | Loss: 0.00002664
Iteration 98/1000 | Loss: 0.00002570
Iteration 99/1000 | Loss: 0.00002513
Iteration 100/1000 | Loss: 0.00002488
Iteration 101/1000 | Loss: 0.00002468
Iteration 102/1000 | Loss: 0.00012329
Iteration 103/1000 | Loss: 0.00004127
Iteration 104/1000 | Loss: 0.00002879
Iteration 105/1000 | Loss: 0.00002480
Iteration 106/1000 | Loss: 0.00002377
Iteration 107/1000 | Loss: 0.00002308
Iteration 108/1000 | Loss: 0.00002280
Iteration 109/1000 | Loss: 0.00002271
Iteration 110/1000 | Loss: 0.00002267
Iteration 111/1000 | Loss: 0.00002266
Iteration 112/1000 | Loss: 0.00002259
Iteration 113/1000 | Loss: 0.00002257
Iteration 114/1000 | Loss: 0.00002256
Iteration 115/1000 | Loss: 0.00002256
Iteration 116/1000 | Loss: 0.00002256
Iteration 117/1000 | Loss: 0.00002256
Iteration 118/1000 | Loss: 0.00002256
Iteration 119/1000 | Loss: 0.00002256
Iteration 120/1000 | Loss: 0.00002256
Iteration 121/1000 | Loss: 0.00002256
Iteration 122/1000 | Loss: 0.00002256
Iteration 123/1000 | Loss: 0.00002255
Iteration 124/1000 | Loss: 0.00002254
Iteration 125/1000 | Loss: 0.00002253
Iteration 126/1000 | Loss: 0.00002253
Iteration 127/1000 | Loss: 0.00002249
Iteration 128/1000 | Loss: 0.00002249
Iteration 129/1000 | Loss: 0.00002249
Iteration 130/1000 | Loss: 0.00002247
Iteration 131/1000 | Loss: 0.00002247
Iteration 132/1000 | Loss: 0.00002247
Iteration 133/1000 | Loss: 0.00002247
Iteration 134/1000 | Loss: 0.00002247
Iteration 135/1000 | Loss: 0.00002247
Iteration 136/1000 | Loss: 0.00002247
Iteration 137/1000 | Loss: 0.00002247
Iteration 138/1000 | Loss: 0.00002246
Iteration 139/1000 | Loss: 0.00002246
Iteration 140/1000 | Loss: 0.00002246
Iteration 141/1000 | Loss: 0.00002245
Iteration 142/1000 | Loss: 0.00002245
Iteration 143/1000 | Loss: 0.00002245
Iteration 144/1000 | Loss: 0.00002245
Iteration 145/1000 | Loss: 0.00002245
Iteration 146/1000 | Loss: 0.00002244
Iteration 147/1000 | Loss: 0.00002244
Iteration 148/1000 | Loss: 0.00002244
Iteration 149/1000 | Loss: 0.00002244
Iteration 150/1000 | Loss: 0.00002244
Iteration 151/1000 | Loss: 0.00002244
Iteration 152/1000 | Loss: 0.00002244
Iteration 153/1000 | Loss: 0.00002243
Iteration 154/1000 | Loss: 0.00002243
Iteration 155/1000 | Loss: 0.00002243
Iteration 156/1000 | Loss: 0.00002243
Iteration 157/1000 | Loss: 0.00002243
Iteration 158/1000 | Loss: 0.00002243
Iteration 159/1000 | Loss: 0.00002243
Iteration 160/1000 | Loss: 0.00002243
Iteration 161/1000 | Loss: 0.00002243
Iteration 162/1000 | Loss: 0.00002243
Iteration 163/1000 | Loss: 0.00002242
Iteration 164/1000 | Loss: 0.00002242
Iteration 165/1000 | Loss: 0.00002242
Iteration 166/1000 | Loss: 0.00002242
Iteration 167/1000 | Loss: 0.00002242
Iteration 168/1000 | Loss: 0.00002242
Iteration 169/1000 | Loss: 0.00002242
Iteration 170/1000 | Loss: 0.00002242
Iteration 171/1000 | Loss: 0.00002241
Iteration 172/1000 | Loss: 0.00002241
Iteration 173/1000 | Loss: 0.00002241
Iteration 174/1000 | Loss: 0.00002241
Iteration 175/1000 | Loss: 0.00002240
Iteration 176/1000 | Loss: 0.00002240
Iteration 177/1000 | Loss: 0.00002240
Iteration 178/1000 | Loss: 0.00002240
Iteration 179/1000 | Loss: 0.00002240
Iteration 180/1000 | Loss: 0.00002240
Iteration 181/1000 | Loss: 0.00002239
Iteration 182/1000 | Loss: 0.00002239
Iteration 183/1000 | Loss: 0.00002239
Iteration 184/1000 | Loss: 0.00002239
Iteration 185/1000 | Loss: 0.00002239
Iteration 186/1000 | Loss: 0.00002239
Iteration 187/1000 | Loss: 0.00002239
Iteration 188/1000 | Loss: 0.00002239
Iteration 189/1000 | Loss: 0.00002239
Iteration 190/1000 | Loss: 0.00002239
Iteration 191/1000 | Loss: 0.00002239
Iteration 192/1000 | Loss: 0.00002239
Iteration 193/1000 | Loss: 0.00002239
Iteration 194/1000 | Loss: 0.00002239
Iteration 195/1000 | Loss: 0.00002239
Iteration 196/1000 | Loss: 0.00002239
Iteration 197/1000 | Loss: 0.00002239
Iteration 198/1000 | Loss: 0.00002239
Iteration 199/1000 | Loss: 0.00002239
Iteration 200/1000 | Loss: 0.00002239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.239127570646815e-05, 2.239127570646815e-05, 2.239127570646815e-05, 2.239127570646815e-05, 2.239127570646815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.239127570646815e-05

Optimization complete. Final v2v error: 3.8115010261535645 mm

Highest mean error: 11.9346284866333 mm for frame 183

Lowest mean error: 3.5528433322906494 mm for frame 199

Saving results

Total time: 216.3347465991974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796516
Iteration 2/25 | Loss: 0.00120608
Iteration 3/25 | Loss: 0.00080835
Iteration 4/25 | Loss: 0.00073336
Iteration 5/25 | Loss: 0.00071673
Iteration 6/25 | Loss: 0.00071239
Iteration 7/25 | Loss: 0.00071180
Iteration 8/25 | Loss: 0.00071180
Iteration 9/25 | Loss: 0.00071180
Iteration 10/25 | Loss: 0.00071180
Iteration 11/25 | Loss: 0.00071180
Iteration 12/25 | Loss: 0.00071180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007118031498976052, 0.0007118031498976052, 0.0007118031498976052, 0.0007118031498976052, 0.0007118031498976052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007118031498976052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45987535
Iteration 2/25 | Loss: 0.00037050
Iteration 3/25 | Loss: 0.00037045
Iteration 4/25 | Loss: 0.00037045
Iteration 5/25 | Loss: 0.00037045
Iteration 6/25 | Loss: 0.00037045
Iteration 7/25 | Loss: 0.00037045
Iteration 8/25 | Loss: 0.00037045
Iteration 9/25 | Loss: 0.00037045
Iteration 10/25 | Loss: 0.00037045
Iteration 11/25 | Loss: 0.00037045
Iteration 12/25 | Loss: 0.00037045
Iteration 13/25 | Loss: 0.00037045
Iteration 14/25 | Loss: 0.00037045
Iteration 15/25 | Loss: 0.00037045
Iteration 16/25 | Loss: 0.00037045
Iteration 17/25 | Loss: 0.00037045
Iteration 18/25 | Loss: 0.00037045
Iteration 19/25 | Loss: 0.00037045
Iteration 20/25 | Loss: 0.00037045
Iteration 21/25 | Loss: 0.00037045
Iteration 22/25 | Loss: 0.00037045
Iteration 23/25 | Loss: 0.00037045
Iteration 24/25 | Loss: 0.00037045
Iteration 25/25 | Loss: 0.00037045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037045
Iteration 2/1000 | Loss: 0.00003447
Iteration 3/1000 | Loss: 0.00002435
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001938
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001774
Iteration 8/1000 | Loss: 0.00001726
Iteration 9/1000 | Loss: 0.00001688
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001629
Iteration 12/1000 | Loss: 0.00001616
Iteration 13/1000 | Loss: 0.00001607
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001589
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00001580
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001577
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001576
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001575
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001574
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001574
Iteration 51/1000 | Loss: 0.00001573
Iteration 52/1000 | Loss: 0.00001573
Iteration 53/1000 | Loss: 0.00001573
Iteration 54/1000 | Loss: 0.00001573
Iteration 55/1000 | Loss: 0.00001573
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001571
Iteration 68/1000 | Loss: 0.00001571
Iteration 69/1000 | Loss: 0.00001571
Iteration 70/1000 | Loss: 0.00001571
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00001571
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001571
Iteration 77/1000 | Loss: 0.00001571
Iteration 78/1000 | Loss: 0.00001571
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001571
Iteration 102/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.570972199260723e-05, 1.570972199260723e-05, 1.570972199260723e-05, 1.570972199260723e-05, 1.570972199260723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.570972199260723e-05

Optimization complete. Final v2v error: 3.344193458557129 mm

Highest mean error: 4.265103816986084 mm for frame 21

Lowest mean error: 2.893608331680298 mm for frame 64

Saving results

Total time: 39.463271617889404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00169317
Iteration 2/25 | Loss: 0.00079082
Iteration 3/25 | Loss: 0.00070176
Iteration 4/25 | Loss: 0.00067845
Iteration 5/25 | Loss: 0.00067024
Iteration 6/25 | Loss: 0.00066853
Iteration 7/25 | Loss: 0.00066765
Iteration 8/25 | Loss: 0.00066765
Iteration 9/25 | Loss: 0.00066765
Iteration 10/25 | Loss: 0.00066765
Iteration 11/25 | Loss: 0.00066765
Iteration 12/25 | Loss: 0.00066765
Iteration 13/25 | Loss: 0.00066765
Iteration 14/25 | Loss: 0.00066765
Iteration 15/25 | Loss: 0.00066765
Iteration 16/25 | Loss: 0.00066765
Iteration 17/25 | Loss: 0.00066765
Iteration 18/25 | Loss: 0.00066765
Iteration 19/25 | Loss: 0.00066765
Iteration 20/25 | Loss: 0.00066765
Iteration 21/25 | Loss: 0.00066765
Iteration 22/25 | Loss: 0.00066765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006676519988104701, 0.0006676519988104701, 0.0006676519988104701, 0.0006676519988104701, 0.0006676519988104701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006676519988104701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51983166
Iteration 2/25 | Loss: 0.00034049
Iteration 3/25 | Loss: 0.00034049
Iteration 4/25 | Loss: 0.00034049
Iteration 5/25 | Loss: 0.00034049
Iteration 6/25 | Loss: 0.00034049
Iteration 7/25 | Loss: 0.00034049
Iteration 8/25 | Loss: 0.00034049
Iteration 9/25 | Loss: 0.00034049
Iteration 10/25 | Loss: 0.00034049
Iteration 11/25 | Loss: 0.00034049
Iteration 12/25 | Loss: 0.00034049
Iteration 13/25 | Loss: 0.00034049
Iteration 14/25 | Loss: 0.00034049
Iteration 15/25 | Loss: 0.00034049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00034049118403345346, 0.00034049118403345346, 0.00034049118403345346, 0.00034049118403345346, 0.00034049118403345346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034049118403345346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034049
Iteration 2/1000 | Loss: 0.00003132
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001351
Iteration 6/1000 | Loss: 0.00001295
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001204
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001162
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001162
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001157
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001144
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001137
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001135
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.135268757934682e-05, 1.135268757934682e-05, 1.135268757934682e-05, 1.135268757934682e-05, 1.135268757934682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.135268757934682e-05

Optimization complete. Final v2v error: 2.921799421310425 mm

Highest mean error: 3.209343433380127 mm for frame 21

Lowest mean error: 2.7215218544006348 mm for frame 47

Saving results

Total time: 36.04863381385803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836977
Iteration 2/25 | Loss: 0.00104765
Iteration 3/25 | Loss: 0.00082556
Iteration 4/25 | Loss: 0.00076477
Iteration 5/25 | Loss: 0.00074964
Iteration 6/25 | Loss: 0.00074635
Iteration 7/25 | Loss: 0.00074537
Iteration 8/25 | Loss: 0.00074528
Iteration 9/25 | Loss: 0.00074528
Iteration 10/25 | Loss: 0.00074528
Iteration 11/25 | Loss: 0.00074528
Iteration 12/25 | Loss: 0.00074528
Iteration 13/25 | Loss: 0.00074528
Iteration 14/25 | Loss: 0.00074528
Iteration 15/25 | Loss: 0.00074528
Iteration 16/25 | Loss: 0.00074528
Iteration 17/25 | Loss: 0.00074528
Iteration 18/25 | Loss: 0.00074528
Iteration 19/25 | Loss: 0.00074528
Iteration 20/25 | Loss: 0.00074528
Iteration 21/25 | Loss: 0.00074528
Iteration 22/25 | Loss: 0.00074528
Iteration 23/25 | Loss: 0.00074528
Iteration 24/25 | Loss: 0.00074528
Iteration 25/25 | Loss: 0.00074528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46202290
Iteration 2/25 | Loss: 0.00036082
Iteration 3/25 | Loss: 0.00036082
Iteration 4/25 | Loss: 0.00036082
Iteration 5/25 | Loss: 0.00036082
Iteration 6/25 | Loss: 0.00036081
Iteration 7/25 | Loss: 0.00036081
Iteration 8/25 | Loss: 0.00036081
Iteration 9/25 | Loss: 0.00036081
Iteration 10/25 | Loss: 0.00036081
Iteration 11/25 | Loss: 0.00036081
Iteration 12/25 | Loss: 0.00036081
Iteration 13/25 | Loss: 0.00036081
Iteration 14/25 | Loss: 0.00036081
Iteration 15/25 | Loss: 0.00036081
Iteration 16/25 | Loss: 0.00036081
Iteration 17/25 | Loss: 0.00036081
Iteration 18/25 | Loss: 0.00036081
Iteration 19/25 | Loss: 0.00036081
Iteration 20/25 | Loss: 0.00036081
Iteration 21/25 | Loss: 0.00036081
Iteration 22/25 | Loss: 0.00036081
Iteration 23/25 | Loss: 0.00036081
Iteration 24/25 | Loss: 0.00036081
Iteration 25/25 | Loss: 0.00036081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036081
Iteration 2/1000 | Loss: 0.00006051
Iteration 3/1000 | Loss: 0.00004128
Iteration 4/1000 | Loss: 0.00003562
Iteration 5/1000 | Loss: 0.00003351
Iteration 6/1000 | Loss: 0.00003159
Iteration 7/1000 | Loss: 0.00003059
Iteration 8/1000 | Loss: 0.00002959
Iteration 9/1000 | Loss: 0.00002893
Iteration 10/1000 | Loss: 0.00002853
Iteration 11/1000 | Loss: 0.00002819
Iteration 12/1000 | Loss: 0.00002806
Iteration 13/1000 | Loss: 0.00002782
Iteration 14/1000 | Loss: 0.00002765
Iteration 15/1000 | Loss: 0.00002759
Iteration 16/1000 | Loss: 0.00002749
Iteration 17/1000 | Loss: 0.00002746
Iteration 18/1000 | Loss: 0.00002743
Iteration 19/1000 | Loss: 0.00002742
Iteration 20/1000 | Loss: 0.00002741
Iteration 21/1000 | Loss: 0.00002741
Iteration 22/1000 | Loss: 0.00002741
Iteration 23/1000 | Loss: 0.00002739
Iteration 24/1000 | Loss: 0.00002739
Iteration 25/1000 | Loss: 0.00002738
Iteration 26/1000 | Loss: 0.00002738
Iteration 27/1000 | Loss: 0.00002737
Iteration 28/1000 | Loss: 0.00002737
Iteration 29/1000 | Loss: 0.00002736
Iteration 30/1000 | Loss: 0.00002736
Iteration 31/1000 | Loss: 0.00002735
Iteration 32/1000 | Loss: 0.00002734
Iteration 33/1000 | Loss: 0.00002734
Iteration 34/1000 | Loss: 0.00002733
Iteration 35/1000 | Loss: 0.00002733
Iteration 36/1000 | Loss: 0.00002732
Iteration 37/1000 | Loss: 0.00002732
Iteration 38/1000 | Loss: 0.00002731
Iteration 39/1000 | Loss: 0.00002731
Iteration 40/1000 | Loss: 0.00002731
Iteration 41/1000 | Loss: 0.00002730
Iteration 42/1000 | Loss: 0.00002730
Iteration 43/1000 | Loss: 0.00002730
Iteration 44/1000 | Loss: 0.00002730
Iteration 45/1000 | Loss: 0.00002729
Iteration 46/1000 | Loss: 0.00002729
Iteration 47/1000 | Loss: 0.00002729
Iteration 48/1000 | Loss: 0.00002729
Iteration 49/1000 | Loss: 0.00002729
Iteration 50/1000 | Loss: 0.00002729
Iteration 51/1000 | Loss: 0.00002728
Iteration 52/1000 | Loss: 0.00002728
Iteration 53/1000 | Loss: 0.00002728
Iteration 54/1000 | Loss: 0.00002728
Iteration 55/1000 | Loss: 0.00002728
Iteration 56/1000 | Loss: 0.00002727
Iteration 57/1000 | Loss: 0.00002727
Iteration 58/1000 | Loss: 0.00002727
Iteration 59/1000 | Loss: 0.00002727
Iteration 60/1000 | Loss: 0.00002727
Iteration 61/1000 | Loss: 0.00002726
Iteration 62/1000 | Loss: 0.00002725
Iteration 63/1000 | Loss: 0.00002725
Iteration 64/1000 | Loss: 0.00002724
Iteration 65/1000 | Loss: 0.00002724
Iteration 66/1000 | Loss: 0.00002724
Iteration 67/1000 | Loss: 0.00002724
Iteration 68/1000 | Loss: 0.00002723
Iteration 69/1000 | Loss: 0.00002723
Iteration 70/1000 | Loss: 0.00002723
Iteration 71/1000 | Loss: 0.00002723
Iteration 72/1000 | Loss: 0.00002722
Iteration 73/1000 | Loss: 0.00002722
Iteration 74/1000 | Loss: 0.00002722
Iteration 75/1000 | Loss: 0.00002721
Iteration 76/1000 | Loss: 0.00002721
Iteration 77/1000 | Loss: 0.00002721
Iteration 78/1000 | Loss: 0.00002721
Iteration 79/1000 | Loss: 0.00002721
Iteration 80/1000 | Loss: 0.00002721
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00002721
Iteration 83/1000 | Loss: 0.00002720
Iteration 84/1000 | Loss: 0.00002720
Iteration 85/1000 | Loss: 0.00002720
Iteration 86/1000 | Loss: 0.00002720
Iteration 87/1000 | Loss: 0.00002720
Iteration 88/1000 | Loss: 0.00002720
Iteration 89/1000 | Loss: 0.00002719
Iteration 90/1000 | Loss: 0.00002719
Iteration 91/1000 | Loss: 0.00002719
Iteration 92/1000 | Loss: 0.00002719
Iteration 93/1000 | Loss: 0.00002718
Iteration 94/1000 | Loss: 0.00002718
Iteration 95/1000 | Loss: 0.00002718
Iteration 96/1000 | Loss: 0.00002717
Iteration 97/1000 | Loss: 0.00002717
Iteration 98/1000 | Loss: 0.00002717
Iteration 99/1000 | Loss: 0.00002717
Iteration 100/1000 | Loss: 0.00002716
Iteration 101/1000 | Loss: 0.00002716
Iteration 102/1000 | Loss: 0.00002716
Iteration 103/1000 | Loss: 0.00002716
Iteration 104/1000 | Loss: 0.00002716
Iteration 105/1000 | Loss: 0.00002715
Iteration 106/1000 | Loss: 0.00002715
Iteration 107/1000 | Loss: 0.00002715
Iteration 108/1000 | Loss: 0.00002715
Iteration 109/1000 | Loss: 0.00002715
Iteration 110/1000 | Loss: 0.00002715
Iteration 111/1000 | Loss: 0.00002715
Iteration 112/1000 | Loss: 0.00002714
Iteration 113/1000 | Loss: 0.00002714
Iteration 114/1000 | Loss: 0.00002714
Iteration 115/1000 | Loss: 0.00002714
Iteration 116/1000 | Loss: 0.00002714
Iteration 117/1000 | Loss: 0.00002713
Iteration 118/1000 | Loss: 0.00002713
Iteration 119/1000 | Loss: 0.00002713
Iteration 120/1000 | Loss: 0.00002713
Iteration 121/1000 | Loss: 0.00002712
Iteration 122/1000 | Loss: 0.00002712
Iteration 123/1000 | Loss: 0.00002712
Iteration 124/1000 | Loss: 0.00002712
Iteration 125/1000 | Loss: 0.00002712
Iteration 126/1000 | Loss: 0.00002712
Iteration 127/1000 | Loss: 0.00002712
Iteration 128/1000 | Loss: 0.00002712
Iteration 129/1000 | Loss: 0.00002711
Iteration 130/1000 | Loss: 0.00002711
Iteration 131/1000 | Loss: 0.00002711
Iteration 132/1000 | Loss: 0.00002711
Iteration 133/1000 | Loss: 0.00002711
Iteration 134/1000 | Loss: 0.00002711
Iteration 135/1000 | Loss: 0.00002711
Iteration 136/1000 | Loss: 0.00002711
Iteration 137/1000 | Loss: 0.00002711
Iteration 138/1000 | Loss: 0.00002711
Iteration 139/1000 | Loss: 0.00002711
Iteration 140/1000 | Loss: 0.00002711
Iteration 141/1000 | Loss: 0.00002711
Iteration 142/1000 | Loss: 0.00002711
Iteration 143/1000 | Loss: 0.00002711
Iteration 144/1000 | Loss: 0.00002711
Iteration 145/1000 | Loss: 0.00002711
Iteration 146/1000 | Loss: 0.00002711
Iteration 147/1000 | Loss: 0.00002711
Iteration 148/1000 | Loss: 0.00002711
Iteration 149/1000 | Loss: 0.00002711
Iteration 150/1000 | Loss: 0.00002711
Iteration 151/1000 | Loss: 0.00002711
Iteration 152/1000 | Loss: 0.00002711
Iteration 153/1000 | Loss: 0.00002711
Iteration 154/1000 | Loss: 0.00002711
Iteration 155/1000 | Loss: 0.00002711
Iteration 156/1000 | Loss: 0.00002711
Iteration 157/1000 | Loss: 0.00002711
Iteration 158/1000 | Loss: 0.00002711
Iteration 159/1000 | Loss: 0.00002711
Iteration 160/1000 | Loss: 0.00002711
Iteration 161/1000 | Loss: 0.00002711
Iteration 162/1000 | Loss: 0.00002711
Iteration 163/1000 | Loss: 0.00002711
Iteration 164/1000 | Loss: 0.00002711
Iteration 165/1000 | Loss: 0.00002711
Iteration 166/1000 | Loss: 0.00002711
Iteration 167/1000 | Loss: 0.00002711
Iteration 168/1000 | Loss: 0.00002711
Iteration 169/1000 | Loss: 0.00002711
Iteration 170/1000 | Loss: 0.00002711
Iteration 171/1000 | Loss: 0.00002711
Iteration 172/1000 | Loss: 0.00002711
Iteration 173/1000 | Loss: 0.00002711
Iteration 174/1000 | Loss: 0.00002711
Iteration 175/1000 | Loss: 0.00002711
Iteration 176/1000 | Loss: 0.00002711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.7113410396850668e-05, 2.7113410396850668e-05, 2.7113410396850668e-05, 2.7113410396850668e-05, 2.7113410396850668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7113410396850668e-05

Optimization complete. Final v2v error: 4.334153652191162 mm

Highest mean error: 4.992159843444824 mm for frame 26

Lowest mean error: 3.9521706104278564 mm for frame 54

Saving results

Total time: 42.23134207725525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857048
Iteration 2/25 | Loss: 0.00129247
Iteration 3/25 | Loss: 0.00082697
Iteration 4/25 | Loss: 0.00077525
Iteration 5/25 | Loss: 0.00076637
Iteration 6/25 | Loss: 0.00076340
Iteration 7/25 | Loss: 0.00076291
Iteration 8/25 | Loss: 0.00076291
Iteration 9/25 | Loss: 0.00076291
Iteration 10/25 | Loss: 0.00076291
Iteration 11/25 | Loss: 0.00076291
Iteration 12/25 | Loss: 0.00076291
Iteration 13/25 | Loss: 0.00076291
Iteration 14/25 | Loss: 0.00076291
Iteration 15/25 | Loss: 0.00076291
Iteration 16/25 | Loss: 0.00076291
Iteration 17/25 | Loss: 0.00076291
Iteration 18/25 | Loss: 0.00076291
Iteration 19/25 | Loss: 0.00076291
Iteration 20/25 | Loss: 0.00076291
Iteration 21/25 | Loss: 0.00076291
Iteration 22/25 | Loss: 0.00076291
Iteration 23/25 | Loss: 0.00076291
Iteration 24/25 | Loss: 0.00076291
Iteration 25/25 | Loss: 0.00076291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73247111
Iteration 2/25 | Loss: 0.00048122
Iteration 3/25 | Loss: 0.00048121
Iteration 4/25 | Loss: 0.00048121
Iteration 5/25 | Loss: 0.00048121
Iteration 6/25 | Loss: 0.00048121
Iteration 7/25 | Loss: 0.00048121
Iteration 8/25 | Loss: 0.00048121
Iteration 9/25 | Loss: 0.00048121
Iteration 10/25 | Loss: 0.00048121
Iteration 11/25 | Loss: 0.00048121
Iteration 12/25 | Loss: 0.00048121
Iteration 13/25 | Loss: 0.00048121
Iteration 14/25 | Loss: 0.00048121
Iteration 15/25 | Loss: 0.00048121
Iteration 16/25 | Loss: 0.00048121
Iteration 17/25 | Loss: 0.00048121
Iteration 18/25 | Loss: 0.00048121
Iteration 19/25 | Loss: 0.00048121
Iteration 20/25 | Loss: 0.00048121
Iteration 21/25 | Loss: 0.00048121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004812079423572868, 0.0004812079423572868, 0.0004812079423572868, 0.0004812079423572868, 0.0004812079423572868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004812079423572868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048121
Iteration 2/1000 | Loss: 0.00002585
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001472
Iteration 7/1000 | Loss: 0.00001436
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001373
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001350
Iteration 45/1000 | Loss: 0.00001350
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001349
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001349
Iteration 54/1000 | Loss: 0.00001349
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001349
Iteration 67/1000 | Loss: 0.00001349
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001349
Iteration 71/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.34881011035759e-05, 1.34881011035759e-05, 1.34881011035759e-05, 1.34881011035759e-05, 1.34881011035759e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.34881011035759e-05

Optimization complete. Final v2v error: 3.119884967803955 mm

Highest mean error: 3.471557855606079 mm for frame 95

Lowest mean error: 2.732727289199829 mm for frame 175

Saving results

Total time: 34.69063115119934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771503
Iteration 2/25 | Loss: 0.00157877
Iteration 3/25 | Loss: 0.00083675
Iteration 4/25 | Loss: 0.00088928
Iteration 5/25 | Loss: 0.00071955
Iteration 6/25 | Loss: 0.00070229
Iteration 7/25 | Loss: 0.00069217
Iteration 8/25 | Loss: 0.00068645
Iteration 9/25 | Loss: 0.00068485
Iteration 10/25 | Loss: 0.00068337
Iteration 11/25 | Loss: 0.00068236
Iteration 12/25 | Loss: 0.00068173
Iteration 13/25 | Loss: 0.00068145
Iteration 14/25 | Loss: 0.00068134
Iteration 15/25 | Loss: 0.00068134
Iteration 16/25 | Loss: 0.00068134
Iteration 17/25 | Loss: 0.00068133
Iteration 18/25 | Loss: 0.00068133
Iteration 19/25 | Loss: 0.00068133
Iteration 20/25 | Loss: 0.00068133
Iteration 21/25 | Loss: 0.00068133
Iteration 22/25 | Loss: 0.00068133
Iteration 23/25 | Loss: 0.00068133
Iteration 24/25 | Loss: 0.00068133
Iteration 25/25 | Loss: 0.00068133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97055006
Iteration 2/25 | Loss: 0.00043470
Iteration 3/25 | Loss: 0.00043469
Iteration 4/25 | Loss: 0.00043469
Iteration 5/25 | Loss: 0.00049417
Iteration 6/25 | Loss: 0.00049417
Iteration 7/25 | Loss: 0.00039847
Iteration 8/25 | Loss: 0.00039846
Iteration 9/25 | Loss: 0.00039846
Iteration 10/25 | Loss: 0.00039846
Iteration 11/25 | Loss: 0.00039846
Iteration 12/25 | Loss: 0.00039846
Iteration 13/25 | Loss: 0.00039846
Iteration 14/25 | Loss: 0.00039846
Iteration 15/25 | Loss: 0.00039846
Iteration 16/25 | Loss: 0.00039846
Iteration 17/25 | Loss: 0.00039846
Iteration 18/25 | Loss: 0.00039846
Iteration 19/25 | Loss: 0.00039846
Iteration 20/25 | Loss: 0.00039846
Iteration 21/25 | Loss: 0.00039846
Iteration 22/25 | Loss: 0.00039846
Iteration 23/25 | Loss: 0.00039846
Iteration 24/25 | Loss: 0.00039846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00039845905848778784, 0.00039845905848778784, 0.00039845905848778784, 0.00039845905848778784, 0.00039845905848778784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039845905848778784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039846
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00007506
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001458
Iteration 8/1000 | Loss: 0.00012770
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00009209
Iteration 12/1000 | Loss: 0.00007298
Iteration 13/1000 | Loss: 0.00001419
Iteration 14/1000 | Loss: 0.00003392
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001366
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001353
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00011631
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001357
Iteration 39/1000 | Loss: 0.00001356
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001345
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001345
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001342
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001341
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001336
Iteration 71/1000 | Loss: 0.00001336
Iteration 72/1000 | Loss: 0.00001336
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001329
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001329
Iteration 152/1000 | Loss: 0.00001329
Iteration 153/1000 | Loss: 0.00001329
Iteration 154/1000 | Loss: 0.00001329
Iteration 155/1000 | Loss: 0.00001329
Iteration 156/1000 | Loss: 0.00001329
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001329
Iteration 159/1000 | Loss: 0.00001329
Iteration 160/1000 | Loss: 0.00001329
Iteration 161/1000 | Loss: 0.00001329
Iteration 162/1000 | Loss: 0.00001329
Iteration 163/1000 | Loss: 0.00001329
Iteration 164/1000 | Loss: 0.00001329
Iteration 165/1000 | Loss: 0.00001329
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001329
Iteration 171/1000 | Loss: 0.00001329
Iteration 172/1000 | Loss: 0.00001329
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3293033589434344e-05, 1.3293033589434344e-05, 1.3293033589434344e-05, 1.3293033589434344e-05, 1.3293033589434344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3293033589434344e-05

Optimization complete. Final v2v error: 3.0447616577148438 mm

Highest mean error: 3.457056760787964 mm for frame 160

Lowest mean error: 2.772165536880493 mm for frame 24

Saving results

Total time: 63.24228525161743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855431
Iteration 2/25 | Loss: 0.00142722
Iteration 3/25 | Loss: 0.00085549
Iteration 4/25 | Loss: 0.00079251
Iteration 5/25 | Loss: 0.00078252
Iteration 6/25 | Loss: 0.00077920
Iteration 7/25 | Loss: 0.00077690
Iteration 8/25 | Loss: 0.00076946
Iteration 9/25 | Loss: 0.00076862
Iteration 10/25 | Loss: 0.00076839
Iteration 11/25 | Loss: 0.00076838
Iteration 12/25 | Loss: 0.00076838
Iteration 13/25 | Loss: 0.00076838
Iteration 14/25 | Loss: 0.00076838
Iteration 15/25 | Loss: 0.00076838
Iteration 16/25 | Loss: 0.00076838
Iteration 17/25 | Loss: 0.00076838
Iteration 18/25 | Loss: 0.00076838
Iteration 19/25 | Loss: 0.00076838
Iteration 20/25 | Loss: 0.00076838
Iteration 21/25 | Loss: 0.00076838
Iteration 22/25 | Loss: 0.00076837
Iteration 23/25 | Loss: 0.00076837
Iteration 24/25 | Loss: 0.00076837
Iteration 25/25 | Loss: 0.00076837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29067338
Iteration 2/25 | Loss: 0.00043309
Iteration 3/25 | Loss: 0.00043309
Iteration 4/25 | Loss: 0.00043309
Iteration 5/25 | Loss: 0.00043309
Iteration 6/25 | Loss: 0.00043309
Iteration 7/25 | Loss: 0.00043309
Iteration 8/25 | Loss: 0.00043309
Iteration 9/25 | Loss: 0.00043309
Iteration 10/25 | Loss: 0.00043309
Iteration 11/25 | Loss: 0.00043309
Iteration 12/25 | Loss: 0.00043309
Iteration 13/25 | Loss: 0.00043309
Iteration 14/25 | Loss: 0.00043309
Iteration 15/25 | Loss: 0.00043309
Iteration 16/25 | Loss: 0.00043309
Iteration 17/25 | Loss: 0.00043309
Iteration 18/25 | Loss: 0.00043309
Iteration 19/25 | Loss: 0.00043309
Iteration 20/25 | Loss: 0.00043309
Iteration 21/25 | Loss: 0.00043309
Iteration 22/25 | Loss: 0.00043309
Iteration 23/25 | Loss: 0.00043309
Iteration 24/25 | Loss: 0.00043309
Iteration 25/25 | Loss: 0.00043309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043309
Iteration 2/1000 | Loss: 0.00002224
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001385
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001379
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001376
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001374
Iteration 47/1000 | Loss: 0.00001374
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001372
Iteration 54/1000 | Loss: 0.00001372
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001371
Iteration 70/1000 | Loss: 0.00001371
Iteration 71/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.3708119695365895e-05, 1.3708119695365895e-05, 1.3708119695365895e-05, 1.3708119695365895e-05, 1.3708119695365895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3708119695365895e-05

Optimization complete. Final v2v error: 3.1634628772735596 mm

Highest mean error: 3.9371917247772217 mm for frame 107

Lowest mean error: 2.81219482421875 mm for frame 156

Saving results

Total time: 40.71272945404053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944458
Iteration 2/25 | Loss: 0.00113005
Iteration 3/25 | Loss: 0.00080395
Iteration 4/25 | Loss: 0.00076120
Iteration 5/25 | Loss: 0.00074176
Iteration 6/25 | Loss: 0.00073657
Iteration 7/25 | Loss: 0.00073559
Iteration 8/25 | Loss: 0.00073544
Iteration 9/25 | Loss: 0.00073544
Iteration 10/25 | Loss: 0.00073544
Iteration 11/25 | Loss: 0.00073544
Iteration 12/25 | Loss: 0.00073544
Iteration 13/25 | Loss: 0.00073544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007354441331699491, 0.0007354441331699491, 0.0007354441331699491, 0.0007354441331699491, 0.0007354441331699491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007354441331699491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42148292
Iteration 2/25 | Loss: 0.00015499
Iteration 3/25 | Loss: 0.00015497
Iteration 4/25 | Loss: 0.00015497
Iteration 5/25 | Loss: 0.00015497
Iteration 6/25 | Loss: 0.00015497
Iteration 7/25 | Loss: 0.00015497
Iteration 8/25 | Loss: 0.00015497
Iteration 9/25 | Loss: 0.00015497
Iteration 10/25 | Loss: 0.00015497
Iteration 11/25 | Loss: 0.00015497
Iteration 12/25 | Loss: 0.00015497
Iteration 13/25 | Loss: 0.00015497
Iteration 14/25 | Loss: 0.00015497
Iteration 15/25 | Loss: 0.00015497
Iteration 16/25 | Loss: 0.00015497
Iteration 17/25 | Loss: 0.00015497
Iteration 18/25 | Loss: 0.00015497
Iteration 19/25 | Loss: 0.00015497
Iteration 20/25 | Loss: 0.00015497
Iteration 21/25 | Loss: 0.00015497
Iteration 22/25 | Loss: 0.00015497
Iteration 23/25 | Loss: 0.00015497
Iteration 24/25 | Loss: 0.00015497
Iteration 25/25 | Loss: 0.00015497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015497
Iteration 2/1000 | Loss: 0.00004330
Iteration 3/1000 | Loss: 0.00003715
Iteration 4/1000 | Loss: 0.00003443
Iteration 5/1000 | Loss: 0.00003294
Iteration 6/1000 | Loss: 0.00003173
Iteration 7/1000 | Loss: 0.00003071
Iteration 8/1000 | Loss: 0.00002997
Iteration 9/1000 | Loss: 0.00002961
Iteration 10/1000 | Loss: 0.00002933
Iteration 11/1000 | Loss: 0.00002912
Iteration 12/1000 | Loss: 0.00002900
Iteration 13/1000 | Loss: 0.00002898
Iteration 14/1000 | Loss: 0.00002894
Iteration 15/1000 | Loss: 0.00002892
Iteration 16/1000 | Loss: 0.00002890
Iteration 17/1000 | Loss: 0.00002890
Iteration 18/1000 | Loss: 0.00002890
Iteration 19/1000 | Loss: 0.00002888
Iteration 20/1000 | Loss: 0.00002887
Iteration 21/1000 | Loss: 0.00002886
Iteration 22/1000 | Loss: 0.00002880
Iteration 23/1000 | Loss: 0.00002879
Iteration 24/1000 | Loss: 0.00002878
Iteration 25/1000 | Loss: 0.00002877
Iteration 26/1000 | Loss: 0.00002876
Iteration 27/1000 | Loss: 0.00002875
Iteration 28/1000 | Loss: 0.00002875
Iteration 29/1000 | Loss: 0.00002875
Iteration 30/1000 | Loss: 0.00002874
Iteration 31/1000 | Loss: 0.00002874
Iteration 32/1000 | Loss: 0.00002874
Iteration 33/1000 | Loss: 0.00002873
Iteration 34/1000 | Loss: 0.00002873
Iteration 35/1000 | Loss: 0.00002873
Iteration 36/1000 | Loss: 0.00002872
Iteration 37/1000 | Loss: 0.00002871
Iteration 38/1000 | Loss: 0.00002871
Iteration 39/1000 | Loss: 0.00002867
Iteration 40/1000 | Loss: 0.00002867
Iteration 41/1000 | Loss: 0.00002867
Iteration 42/1000 | Loss: 0.00002867
Iteration 43/1000 | Loss: 0.00002867
Iteration 44/1000 | Loss: 0.00002867
Iteration 45/1000 | Loss: 0.00002867
Iteration 46/1000 | Loss: 0.00002866
Iteration 47/1000 | Loss: 0.00002866
Iteration 48/1000 | Loss: 0.00002866
Iteration 49/1000 | Loss: 0.00002865
Iteration 50/1000 | Loss: 0.00002865
Iteration 51/1000 | Loss: 0.00002864
Iteration 52/1000 | Loss: 0.00002864
Iteration 53/1000 | Loss: 0.00002864
Iteration 54/1000 | Loss: 0.00002863
Iteration 55/1000 | Loss: 0.00002863
Iteration 56/1000 | Loss: 0.00002863
Iteration 57/1000 | Loss: 0.00002863
Iteration 58/1000 | Loss: 0.00002863
Iteration 59/1000 | Loss: 0.00002863
Iteration 60/1000 | Loss: 0.00002863
Iteration 61/1000 | Loss: 0.00002863
Iteration 62/1000 | Loss: 0.00002863
Iteration 63/1000 | Loss: 0.00002863
Iteration 64/1000 | Loss: 0.00002862
Iteration 65/1000 | Loss: 0.00002862
Iteration 66/1000 | Loss: 0.00002862
Iteration 67/1000 | Loss: 0.00002862
Iteration 68/1000 | Loss: 0.00002862
Iteration 69/1000 | Loss: 0.00002862
Iteration 70/1000 | Loss: 0.00002861
Iteration 71/1000 | Loss: 0.00002861
Iteration 72/1000 | Loss: 0.00002861
Iteration 73/1000 | Loss: 0.00002861
Iteration 74/1000 | Loss: 0.00002861
Iteration 75/1000 | Loss: 0.00002860
Iteration 76/1000 | Loss: 0.00002860
Iteration 77/1000 | Loss: 0.00002860
Iteration 78/1000 | Loss: 0.00002860
Iteration 79/1000 | Loss: 0.00002860
Iteration 80/1000 | Loss: 0.00002860
Iteration 81/1000 | Loss: 0.00002859
Iteration 82/1000 | Loss: 0.00002859
Iteration 83/1000 | Loss: 0.00002859
Iteration 84/1000 | Loss: 0.00002859
Iteration 85/1000 | Loss: 0.00002859
Iteration 86/1000 | Loss: 0.00002859
Iteration 87/1000 | Loss: 0.00002859
Iteration 88/1000 | Loss: 0.00002858
Iteration 89/1000 | Loss: 0.00002858
Iteration 90/1000 | Loss: 0.00002857
Iteration 91/1000 | Loss: 0.00002857
Iteration 92/1000 | Loss: 0.00002857
Iteration 93/1000 | Loss: 0.00002857
Iteration 94/1000 | Loss: 0.00002856
Iteration 95/1000 | Loss: 0.00002856
Iteration 96/1000 | Loss: 0.00002856
Iteration 97/1000 | Loss: 0.00002856
Iteration 98/1000 | Loss: 0.00002856
Iteration 99/1000 | Loss: 0.00002856
Iteration 100/1000 | Loss: 0.00002856
Iteration 101/1000 | Loss: 0.00002856
Iteration 102/1000 | Loss: 0.00002856
Iteration 103/1000 | Loss: 0.00002856
Iteration 104/1000 | Loss: 0.00002855
Iteration 105/1000 | Loss: 0.00002855
Iteration 106/1000 | Loss: 0.00002855
Iteration 107/1000 | Loss: 0.00002855
Iteration 108/1000 | Loss: 0.00002855
Iteration 109/1000 | Loss: 0.00002855
Iteration 110/1000 | Loss: 0.00002855
Iteration 111/1000 | Loss: 0.00002855
Iteration 112/1000 | Loss: 0.00002855
Iteration 113/1000 | Loss: 0.00002854
Iteration 114/1000 | Loss: 0.00002854
Iteration 115/1000 | Loss: 0.00002854
Iteration 116/1000 | Loss: 0.00002854
Iteration 117/1000 | Loss: 0.00002854
Iteration 118/1000 | Loss: 0.00002854
Iteration 119/1000 | Loss: 0.00002854
Iteration 120/1000 | Loss: 0.00002854
Iteration 121/1000 | Loss: 0.00002854
Iteration 122/1000 | Loss: 0.00002854
Iteration 123/1000 | Loss: 0.00002854
Iteration 124/1000 | Loss: 0.00002854
Iteration 125/1000 | Loss: 0.00002854
Iteration 126/1000 | Loss: 0.00002854
Iteration 127/1000 | Loss: 0.00002854
Iteration 128/1000 | Loss: 0.00002854
Iteration 129/1000 | Loss: 0.00002854
Iteration 130/1000 | Loss: 0.00002853
Iteration 131/1000 | Loss: 0.00002853
Iteration 132/1000 | Loss: 0.00002853
Iteration 133/1000 | Loss: 0.00002853
Iteration 134/1000 | Loss: 0.00002853
Iteration 135/1000 | Loss: 0.00002853
Iteration 136/1000 | Loss: 0.00002853
Iteration 137/1000 | Loss: 0.00002853
Iteration 138/1000 | Loss: 0.00002853
Iteration 139/1000 | Loss: 0.00002853
Iteration 140/1000 | Loss: 0.00002852
Iteration 141/1000 | Loss: 0.00002852
Iteration 142/1000 | Loss: 0.00002852
Iteration 143/1000 | Loss: 0.00002852
Iteration 144/1000 | Loss: 0.00002852
Iteration 145/1000 | Loss: 0.00002852
Iteration 146/1000 | Loss: 0.00002852
Iteration 147/1000 | Loss: 0.00002852
Iteration 148/1000 | Loss: 0.00002851
Iteration 149/1000 | Loss: 0.00002851
Iteration 150/1000 | Loss: 0.00002851
Iteration 151/1000 | Loss: 0.00002851
Iteration 152/1000 | Loss: 0.00002851
Iteration 153/1000 | Loss: 0.00002851
Iteration 154/1000 | Loss: 0.00002851
Iteration 155/1000 | Loss: 0.00002851
Iteration 156/1000 | Loss: 0.00002851
Iteration 157/1000 | Loss: 0.00002851
Iteration 158/1000 | Loss: 0.00002851
Iteration 159/1000 | Loss: 0.00002851
Iteration 160/1000 | Loss: 0.00002851
Iteration 161/1000 | Loss: 0.00002851
Iteration 162/1000 | Loss: 0.00002851
Iteration 163/1000 | Loss: 0.00002851
Iteration 164/1000 | Loss: 0.00002850
Iteration 165/1000 | Loss: 0.00002850
Iteration 166/1000 | Loss: 0.00002850
Iteration 167/1000 | Loss: 0.00002850
Iteration 168/1000 | Loss: 0.00002850
Iteration 169/1000 | Loss: 0.00002850
Iteration 170/1000 | Loss: 0.00002850
Iteration 171/1000 | Loss: 0.00002850
Iteration 172/1000 | Loss: 0.00002850
Iteration 173/1000 | Loss: 0.00002850
Iteration 174/1000 | Loss: 0.00002850
Iteration 175/1000 | Loss: 0.00002850
Iteration 176/1000 | Loss: 0.00002850
Iteration 177/1000 | Loss: 0.00002850
Iteration 178/1000 | Loss: 0.00002850
Iteration 179/1000 | Loss: 0.00002850
Iteration 180/1000 | Loss: 0.00002850
Iteration 181/1000 | Loss: 0.00002850
Iteration 182/1000 | Loss: 0.00002850
Iteration 183/1000 | Loss: 0.00002850
Iteration 184/1000 | Loss: 0.00002849
Iteration 185/1000 | Loss: 0.00002849
Iteration 186/1000 | Loss: 0.00002849
Iteration 187/1000 | Loss: 0.00002849
Iteration 188/1000 | Loss: 0.00002849
Iteration 189/1000 | Loss: 0.00002849
Iteration 190/1000 | Loss: 0.00002849
Iteration 191/1000 | Loss: 0.00002849
Iteration 192/1000 | Loss: 0.00002849
Iteration 193/1000 | Loss: 0.00002849
Iteration 194/1000 | Loss: 0.00002849
Iteration 195/1000 | Loss: 0.00002849
Iteration 196/1000 | Loss: 0.00002849
Iteration 197/1000 | Loss: 0.00002849
Iteration 198/1000 | Loss: 0.00002849
Iteration 199/1000 | Loss: 0.00002849
Iteration 200/1000 | Loss: 0.00002849
Iteration 201/1000 | Loss: 0.00002849
Iteration 202/1000 | Loss: 0.00002849
Iteration 203/1000 | Loss: 0.00002849
Iteration 204/1000 | Loss: 0.00002848
Iteration 205/1000 | Loss: 0.00002848
Iteration 206/1000 | Loss: 0.00002848
Iteration 207/1000 | Loss: 0.00002848
Iteration 208/1000 | Loss: 0.00002848
Iteration 209/1000 | Loss: 0.00002848
Iteration 210/1000 | Loss: 0.00002848
Iteration 211/1000 | Loss: 0.00002848
Iteration 212/1000 | Loss: 0.00002848
Iteration 213/1000 | Loss: 0.00002848
Iteration 214/1000 | Loss: 0.00002848
Iteration 215/1000 | Loss: 0.00002848
Iteration 216/1000 | Loss: 0.00002848
Iteration 217/1000 | Loss: 0.00002848
Iteration 218/1000 | Loss: 0.00002848
Iteration 219/1000 | Loss: 0.00002848
Iteration 220/1000 | Loss: 0.00002848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.8483682399382815e-05, 2.8483682399382815e-05, 2.8483682399382815e-05, 2.8483682399382815e-05, 2.8483682399382815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8483682399382815e-05

Optimization complete. Final v2v error: 4.480435848236084 mm

Highest mean error: 6.074554920196533 mm for frame 66

Lowest mean error: 3.7219948768615723 mm for frame 42

Saving results

Total time: 41.571200132369995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002827
Iteration 2/25 | Loss: 0.00150455
Iteration 3/25 | Loss: 0.00092538
Iteration 4/25 | Loss: 0.00088035
Iteration 5/25 | Loss: 0.00086391
Iteration 6/25 | Loss: 0.00086022
Iteration 7/25 | Loss: 0.00085966
Iteration 8/25 | Loss: 0.00085966
Iteration 9/25 | Loss: 0.00085966
Iteration 10/25 | Loss: 0.00085966
Iteration 11/25 | Loss: 0.00085966
Iteration 12/25 | Loss: 0.00085966
Iteration 13/25 | Loss: 0.00085966
Iteration 14/25 | Loss: 0.00085966
Iteration 15/25 | Loss: 0.00085966
Iteration 16/25 | Loss: 0.00085966
Iteration 17/25 | Loss: 0.00085966
Iteration 18/25 | Loss: 0.00085966
Iteration 19/25 | Loss: 0.00085966
Iteration 20/25 | Loss: 0.00085966
Iteration 21/25 | Loss: 0.00085966
Iteration 22/25 | Loss: 0.00085966
Iteration 23/25 | Loss: 0.00085966
Iteration 24/25 | Loss: 0.00085966
Iteration 25/25 | Loss: 0.00085966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92611855
Iteration 2/25 | Loss: 0.00021707
Iteration 3/25 | Loss: 0.00021706
Iteration 4/25 | Loss: 0.00021706
Iteration 5/25 | Loss: 0.00021706
Iteration 6/25 | Loss: 0.00021706
Iteration 7/25 | Loss: 0.00021706
Iteration 8/25 | Loss: 0.00021706
Iteration 9/25 | Loss: 0.00021706
Iteration 10/25 | Loss: 0.00021706
Iteration 11/25 | Loss: 0.00021706
Iteration 12/25 | Loss: 0.00021706
Iteration 13/25 | Loss: 0.00021706
Iteration 14/25 | Loss: 0.00021706
Iteration 15/25 | Loss: 0.00021706
Iteration 16/25 | Loss: 0.00021706
Iteration 17/25 | Loss: 0.00021706
Iteration 18/25 | Loss: 0.00021706
Iteration 19/25 | Loss: 0.00021706
Iteration 20/25 | Loss: 0.00021706
Iteration 21/25 | Loss: 0.00021706
Iteration 22/25 | Loss: 0.00021706
Iteration 23/25 | Loss: 0.00021706
Iteration 24/25 | Loss: 0.00021706
Iteration 25/25 | Loss: 0.00021706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021706
Iteration 2/1000 | Loss: 0.00006227
Iteration 3/1000 | Loss: 0.00004809
Iteration 4/1000 | Loss: 0.00004511
Iteration 5/1000 | Loss: 0.00004318
Iteration 6/1000 | Loss: 0.00004184
Iteration 7/1000 | Loss: 0.00004089
Iteration 8/1000 | Loss: 0.00004040
Iteration 9/1000 | Loss: 0.00004000
Iteration 10/1000 | Loss: 0.00003971
Iteration 11/1000 | Loss: 0.00003952
Iteration 12/1000 | Loss: 0.00003930
Iteration 13/1000 | Loss: 0.00003915
Iteration 14/1000 | Loss: 0.00003901
Iteration 15/1000 | Loss: 0.00003900
Iteration 16/1000 | Loss: 0.00003895
Iteration 17/1000 | Loss: 0.00003887
Iteration 18/1000 | Loss: 0.00003884
Iteration 19/1000 | Loss: 0.00003879
Iteration 20/1000 | Loss: 0.00003877
Iteration 21/1000 | Loss: 0.00003877
Iteration 22/1000 | Loss: 0.00003876
Iteration 23/1000 | Loss: 0.00003876
Iteration 24/1000 | Loss: 0.00003873
Iteration 25/1000 | Loss: 0.00003873
Iteration 26/1000 | Loss: 0.00003873
Iteration 27/1000 | Loss: 0.00003872
Iteration 28/1000 | Loss: 0.00003872
Iteration 29/1000 | Loss: 0.00003871
Iteration 30/1000 | Loss: 0.00003870
Iteration 31/1000 | Loss: 0.00003870
Iteration 32/1000 | Loss: 0.00003869
Iteration 33/1000 | Loss: 0.00003869
Iteration 34/1000 | Loss: 0.00003869
Iteration 35/1000 | Loss: 0.00003869
Iteration 36/1000 | Loss: 0.00003868
Iteration 37/1000 | Loss: 0.00003868
Iteration 38/1000 | Loss: 0.00003868
Iteration 39/1000 | Loss: 0.00003868
Iteration 40/1000 | Loss: 0.00003868
Iteration 41/1000 | Loss: 0.00003868
Iteration 42/1000 | Loss: 0.00003867
Iteration 43/1000 | Loss: 0.00003867
Iteration 44/1000 | Loss: 0.00003867
Iteration 45/1000 | Loss: 0.00003867
Iteration 46/1000 | Loss: 0.00003866
Iteration 47/1000 | Loss: 0.00003866
Iteration 48/1000 | Loss: 0.00003866
Iteration 49/1000 | Loss: 0.00003866
Iteration 50/1000 | Loss: 0.00003866
Iteration 51/1000 | Loss: 0.00003866
Iteration 52/1000 | Loss: 0.00003866
Iteration 53/1000 | Loss: 0.00003866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [3.8657221011817455e-05, 3.8657221011817455e-05, 3.8657221011817455e-05, 3.8657221011817455e-05, 3.8657221011817455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8657221011817455e-05

Optimization complete. Final v2v error: 5.170557498931885 mm

Highest mean error: 5.77587366104126 mm for frame 94

Lowest mean error: 4.599893569946289 mm for frame 122

Saving results

Total time: 41.92044687271118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111228
Iteration 2/25 | Loss: 0.00349471
Iteration 3/25 | Loss: 0.00158175
Iteration 4/25 | Loss: 0.00128985
Iteration 5/25 | Loss: 0.00141763
Iteration 6/25 | Loss: 0.00177866
Iteration 7/25 | Loss: 0.00141727
Iteration 8/25 | Loss: 0.00117705
Iteration 9/25 | Loss: 0.00105290
Iteration 10/25 | Loss: 0.00102824
Iteration 11/25 | Loss: 0.00100095
Iteration 12/25 | Loss: 0.00095813
Iteration 13/25 | Loss: 0.00092409
Iteration 14/25 | Loss: 0.00089112
Iteration 15/25 | Loss: 0.00087330
Iteration 16/25 | Loss: 0.00085628
Iteration 17/25 | Loss: 0.00083357
Iteration 18/25 | Loss: 0.00084285
Iteration 19/25 | Loss: 0.00082997
Iteration 20/25 | Loss: 0.00081573
Iteration 21/25 | Loss: 0.00081769
Iteration 22/25 | Loss: 0.00080750
Iteration 23/25 | Loss: 0.00080759
Iteration 24/25 | Loss: 0.00080929
Iteration 25/25 | Loss: 0.00079872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02910280
Iteration 2/25 | Loss: 0.00243555
Iteration 3/25 | Loss: 0.00149073
Iteration 4/25 | Loss: 0.00149073
Iteration 5/25 | Loss: 0.00149073
Iteration 6/25 | Loss: 0.00149073
Iteration 7/25 | Loss: 0.00149073
Iteration 8/25 | Loss: 0.00149073
Iteration 9/25 | Loss: 0.00149073
Iteration 10/25 | Loss: 0.00149073
Iteration 11/25 | Loss: 0.00149073
Iteration 12/25 | Loss: 0.00149073
Iteration 13/25 | Loss: 0.00149073
Iteration 14/25 | Loss: 0.00149073
Iteration 15/25 | Loss: 0.00149073
Iteration 16/25 | Loss: 0.00149073
Iteration 17/25 | Loss: 0.00149073
Iteration 18/25 | Loss: 0.00149073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001490728696808219, 0.001490728696808219, 0.001490728696808219, 0.001490728696808219, 0.001490728696808219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001490728696808219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149073
Iteration 2/1000 | Loss: 0.00162272
Iteration 3/1000 | Loss: 0.00057954
Iteration 4/1000 | Loss: 0.00075409
Iteration 5/1000 | Loss: 0.00039672
Iteration 6/1000 | Loss: 0.00080176
Iteration 7/1000 | Loss: 0.00033332
Iteration 8/1000 | Loss: 0.00026696
Iteration 9/1000 | Loss: 0.00034492
Iteration 10/1000 | Loss: 0.00023230
Iteration 11/1000 | Loss: 0.00058676
Iteration 12/1000 | Loss: 0.00051270
Iteration 13/1000 | Loss: 0.00019354
Iteration 14/1000 | Loss: 0.00034169
Iteration 15/1000 | Loss: 0.00022126
Iteration 16/1000 | Loss: 0.00016159
Iteration 17/1000 | Loss: 0.00024450
Iteration 18/1000 | Loss: 0.00011540
Iteration 19/1000 | Loss: 0.00035501
Iteration 20/1000 | Loss: 0.00063682
Iteration 21/1000 | Loss: 0.00015580
Iteration 22/1000 | Loss: 0.00011739
Iteration 23/1000 | Loss: 0.00012773
Iteration 24/1000 | Loss: 0.00080571
Iteration 25/1000 | Loss: 0.00063577
Iteration 26/1000 | Loss: 0.00049533
Iteration 27/1000 | Loss: 0.00040187
Iteration 28/1000 | Loss: 0.00031363
Iteration 29/1000 | Loss: 0.00031207
Iteration 30/1000 | Loss: 0.00018352
Iteration 31/1000 | Loss: 0.00078000
Iteration 32/1000 | Loss: 0.00046837
Iteration 33/1000 | Loss: 0.00036313
Iteration 34/1000 | Loss: 0.00092366
Iteration 35/1000 | Loss: 0.00050361
Iteration 36/1000 | Loss: 0.00070075
Iteration 37/1000 | Loss: 0.00042343
Iteration 38/1000 | Loss: 0.00046267
Iteration 39/1000 | Loss: 0.00025268
Iteration 40/1000 | Loss: 0.00028591
Iteration 41/1000 | Loss: 0.00008216
Iteration 42/1000 | Loss: 0.00007670
Iteration 43/1000 | Loss: 0.00059247
Iteration 44/1000 | Loss: 0.00041086
Iteration 45/1000 | Loss: 0.00093638
Iteration 46/1000 | Loss: 0.00029862
Iteration 47/1000 | Loss: 0.00032027
Iteration 48/1000 | Loss: 0.00016101
Iteration 49/1000 | Loss: 0.00013592
Iteration 50/1000 | Loss: 0.00005555
Iteration 51/1000 | Loss: 0.00004854
Iteration 52/1000 | Loss: 0.00042659
Iteration 53/1000 | Loss: 0.00038946
Iteration 54/1000 | Loss: 0.00027037
Iteration 55/1000 | Loss: 0.00030125
Iteration 56/1000 | Loss: 0.00013339
Iteration 57/1000 | Loss: 0.00016281
Iteration 58/1000 | Loss: 0.00004999
Iteration 59/1000 | Loss: 0.00017986
Iteration 60/1000 | Loss: 0.00009505
Iteration 61/1000 | Loss: 0.00023329
Iteration 62/1000 | Loss: 0.00005419
Iteration 63/1000 | Loss: 0.00006556
Iteration 64/1000 | Loss: 0.00005222
Iteration 65/1000 | Loss: 0.00004651
Iteration 66/1000 | Loss: 0.00047787
Iteration 67/1000 | Loss: 0.00080635
Iteration 68/1000 | Loss: 0.00051973
Iteration 69/1000 | Loss: 0.00053280
Iteration 70/1000 | Loss: 0.00046807
Iteration 71/1000 | Loss: 0.00013227
Iteration 72/1000 | Loss: 0.00005436
Iteration 73/1000 | Loss: 0.00023142
Iteration 74/1000 | Loss: 0.00030171
Iteration 75/1000 | Loss: 0.00030704
Iteration 76/1000 | Loss: 0.00007727
Iteration 77/1000 | Loss: 0.00005094
Iteration 78/1000 | Loss: 0.00005610
Iteration 79/1000 | Loss: 0.00007724
Iteration 80/1000 | Loss: 0.00047767
Iteration 81/1000 | Loss: 0.00026656
Iteration 82/1000 | Loss: 0.00006237
Iteration 83/1000 | Loss: 0.00005474
Iteration 84/1000 | Loss: 0.00003871
Iteration 85/1000 | Loss: 0.00069668
Iteration 86/1000 | Loss: 0.00034915
Iteration 87/1000 | Loss: 0.00012921
Iteration 88/1000 | Loss: 0.00006521
Iteration 89/1000 | Loss: 0.00004124
Iteration 90/1000 | Loss: 0.00004520
Iteration 91/1000 | Loss: 0.00002697
Iteration 92/1000 | Loss: 0.00056355
Iteration 93/1000 | Loss: 0.00055304
Iteration 94/1000 | Loss: 0.00004346
Iteration 95/1000 | Loss: 0.00056229
Iteration 96/1000 | Loss: 0.00061015
Iteration 97/1000 | Loss: 0.00031605
Iteration 98/1000 | Loss: 0.00042444
Iteration 99/1000 | Loss: 0.00053672
Iteration 100/1000 | Loss: 0.00051172
Iteration 101/1000 | Loss: 0.00046689
Iteration 102/1000 | Loss: 0.00086181
Iteration 103/1000 | Loss: 0.00035053
Iteration 104/1000 | Loss: 0.00056477
Iteration 105/1000 | Loss: 0.00026060
Iteration 106/1000 | Loss: 0.00021701
Iteration 107/1000 | Loss: 0.00014255
Iteration 108/1000 | Loss: 0.00011672
Iteration 109/1000 | Loss: 0.00022835
Iteration 110/1000 | Loss: 0.00021908
Iteration 111/1000 | Loss: 0.00086167
Iteration 112/1000 | Loss: 0.00039561
Iteration 113/1000 | Loss: 0.00064259
Iteration 114/1000 | Loss: 0.00033575
Iteration 115/1000 | Loss: 0.00043799
Iteration 116/1000 | Loss: 0.00030831
Iteration 117/1000 | Loss: 0.00007021
Iteration 118/1000 | Loss: 0.00045932
Iteration 119/1000 | Loss: 0.00009862
Iteration 120/1000 | Loss: 0.00027844
Iteration 121/1000 | Loss: 0.00019076
Iteration 122/1000 | Loss: 0.00014712
Iteration 123/1000 | Loss: 0.00015833
Iteration 124/1000 | Loss: 0.00013954
Iteration 125/1000 | Loss: 0.00004100
Iteration 126/1000 | Loss: 0.00003719
Iteration 127/1000 | Loss: 0.00003417
Iteration 128/1000 | Loss: 0.00014212
Iteration 129/1000 | Loss: 0.00044417
Iteration 130/1000 | Loss: 0.00006708
Iteration 131/1000 | Loss: 0.00009555
Iteration 132/1000 | Loss: 0.00021595
Iteration 133/1000 | Loss: 0.00043876
Iteration 134/1000 | Loss: 0.00050459
Iteration 135/1000 | Loss: 0.00020074
Iteration 136/1000 | Loss: 0.00030392
Iteration 137/1000 | Loss: 0.00025127
Iteration 138/1000 | Loss: 0.00034161
Iteration 139/1000 | Loss: 0.00023301
Iteration 140/1000 | Loss: 0.00009593
Iteration 141/1000 | Loss: 0.00019794
Iteration 142/1000 | Loss: 0.00016159
Iteration 143/1000 | Loss: 0.00003216
Iteration 144/1000 | Loss: 0.00002741
Iteration 145/1000 | Loss: 0.00018100
Iteration 146/1000 | Loss: 0.00042000
Iteration 147/1000 | Loss: 0.00038994
Iteration 148/1000 | Loss: 0.00044129
Iteration 149/1000 | Loss: 0.00034505
Iteration 150/1000 | Loss: 0.00021401
Iteration 151/1000 | Loss: 0.00005661
Iteration 152/1000 | Loss: 0.00006548
Iteration 153/1000 | Loss: 0.00003113
Iteration 154/1000 | Loss: 0.00002839
Iteration 155/1000 | Loss: 0.00069382
Iteration 156/1000 | Loss: 0.00005305
Iteration 157/1000 | Loss: 0.00006600
Iteration 158/1000 | Loss: 0.00004648
Iteration 159/1000 | Loss: 0.00004471
Iteration 160/1000 | Loss: 0.00002861
Iteration 161/1000 | Loss: 0.00002469
Iteration 162/1000 | Loss: 0.00004811
Iteration 163/1000 | Loss: 0.00002765
Iteration 164/1000 | Loss: 0.00002306
Iteration 165/1000 | Loss: 0.00002218
Iteration 166/1000 | Loss: 0.00002166
Iteration 167/1000 | Loss: 0.00002119
Iteration 168/1000 | Loss: 0.00002089
Iteration 169/1000 | Loss: 0.00002047
Iteration 170/1000 | Loss: 0.00037582
Iteration 171/1000 | Loss: 0.00015951
Iteration 172/1000 | Loss: 0.00023365
Iteration 173/1000 | Loss: 0.00016900
Iteration 174/1000 | Loss: 0.00017699
Iteration 175/1000 | Loss: 0.00018291
Iteration 176/1000 | Loss: 0.00019599
Iteration 177/1000 | Loss: 0.00020785
Iteration 178/1000 | Loss: 0.00014668
Iteration 179/1000 | Loss: 0.00017490
Iteration 180/1000 | Loss: 0.00039707
Iteration 181/1000 | Loss: 0.00004286
Iteration 182/1000 | Loss: 0.00002962
Iteration 183/1000 | Loss: 0.00002382
Iteration 184/1000 | Loss: 0.00002141
Iteration 185/1000 | Loss: 0.00002039
Iteration 186/1000 | Loss: 0.00001936
Iteration 187/1000 | Loss: 0.00001855
Iteration 188/1000 | Loss: 0.00001788
Iteration 189/1000 | Loss: 0.00001739
Iteration 190/1000 | Loss: 0.00001710
Iteration 191/1000 | Loss: 0.00001691
Iteration 192/1000 | Loss: 0.00001691
Iteration 193/1000 | Loss: 0.00001687
Iteration 194/1000 | Loss: 0.00001684
Iteration 195/1000 | Loss: 0.00001681
Iteration 196/1000 | Loss: 0.00001671
Iteration 197/1000 | Loss: 0.00001659
Iteration 198/1000 | Loss: 0.00001657
Iteration 199/1000 | Loss: 0.00001653
Iteration 200/1000 | Loss: 0.00001646
Iteration 201/1000 | Loss: 0.00001645
Iteration 202/1000 | Loss: 0.00001639
Iteration 203/1000 | Loss: 0.00001634
Iteration 204/1000 | Loss: 0.00017622
Iteration 205/1000 | Loss: 0.00002307
Iteration 206/1000 | Loss: 0.00002073
Iteration 207/1000 | Loss: 0.00001873
Iteration 208/1000 | Loss: 0.00019522
Iteration 209/1000 | Loss: 0.00033576
Iteration 210/1000 | Loss: 0.00003773
Iteration 211/1000 | Loss: 0.00020503
Iteration 212/1000 | Loss: 0.00002690
Iteration 213/1000 | Loss: 0.00019547
Iteration 214/1000 | Loss: 0.00027096
Iteration 215/1000 | Loss: 0.00042980
Iteration 216/1000 | Loss: 0.00003365
Iteration 217/1000 | Loss: 0.00018890
Iteration 218/1000 | Loss: 0.00010074
Iteration 219/1000 | Loss: 0.00051618
Iteration 220/1000 | Loss: 0.00036668
Iteration 221/1000 | Loss: 0.00022659
Iteration 222/1000 | Loss: 0.00003770
Iteration 223/1000 | Loss: 0.00048573
Iteration 224/1000 | Loss: 0.00051763
Iteration 225/1000 | Loss: 0.00009018
Iteration 226/1000 | Loss: 0.00003197
Iteration 227/1000 | Loss: 0.00040196
Iteration 228/1000 | Loss: 0.00004665
Iteration 229/1000 | Loss: 0.00002804
Iteration 230/1000 | Loss: 0.00002217
Iteration 231/1000 | Loss: 0.00003214
Iteration 232/1000 | Loss: 0.00002614
Iteration 233/1000 | Loss: 0.00001872
Iteration 234/1000 | Loss: 0.00001799
Iteration 235/1000 | Loss: 0.00006550
Iteration 236/1000 | Loss: 0.00001837
Iteration 237/1000 | Loss: 0.00001641
Iteration 238/1000 | Loss: 0.00001543
Iteration 239/1000 | Loss: 0.00001518
Iteration 240/1000 | Loss: 0.00001506
Iteration 241/1000 | Loss: 0.00001494
Iteration 242/1000 | Loss: 0.00001491
Iteration 243/1000 | Loss: 0.00001491
Iteration 244/1000 | Loss: 0.00001490
Iteration 245/1000 | Loss: 0.00001485
Iteration 246/1000 | Loss: 0.00001485
Iteration 247/1000 | Loss: 0.00001484
Iteration 248/1000 | Loss: 0.00001484
Iteration 249/1000 | Loss: 0.00001483
Iteration 250/1000 | Loss: 0.00001483
Iteration 251/1000 | Loss: 0.00001483
Iteration 252/1000 | Loss: 0.00001483
Iteration 253/1000 | Loss: 0.00001483
Iteration 254/1000 | Loss: 0.00001482
Iteration 255/1000 | Loss: 0.00001482
Iteration 256/1000 | Loss: 0.00001481
Iteration 257/1000 | Loss: 0.00001481
Iteration 258/1000 | Loss: 0.00001481
Iteration 259/1000 | Loss: 0.00001480
Iteration 260/1000 | Loss: 0.00001480
Iteration 261/1000 | Loss: 0.00001479
Iteration 262/1000 | Loss: 0.00001479
Iteration 263/1000 | Loss: 0.00001478
Iteration 264/1000 | Loss: 0.00001478
Iteration 265/1000 | Loss: 0.00001478
Iteration 266/1000 | Loss: 0.00001478
Iteration 267/1000 | Loss: 0.00001477
Iteration 268/1000 | Loss: 0.00001477
Iteration 269/1000 | Loss: 0.00001477
Iteration 270/1000 | Loss: 0.00001477
Iteration 271/1000 | Loss: 0.00001476
Iteration 272/1000 | Loss: 0.00001476
Iteration 273/1000 | Loss: 0.00001476
Iteration 274/1000 | Loss: 0.00001476
Iteration 275/1000 | Loss: 0.00001475
Iteration 276/1000 | Loss: 0.00001475
Iteration 277/1000 | Loss: 0.00001475
Iteration 278/1000 | Loss: 0.00001475
Iteration 279/1000 | Loss: 0.00001474
Iteration 280/1000 | Loss: 0.00001474
Iteration 281/1000 | Loss: 0.00001474
Iteration 282/1000 | Loss: 0.00001474
Iteration 283/1000 | Loss: 0.00001474
Iteration 284/1000 | Loss: 0.00001474
Iteration 285/1000 | Loss: 0.00001474
Iteration 286/1000 | Loss: 0.00001474
Iteration 287/1000 | Loss: 0.00001474
Iteration 288/1000 | Loss: 0.00001474
Iteration 289/1000 | Loss: 0.00001474
Iteration 290/1000 | Loss: 0.00001474
Iteration 291/1000 | Loss: 0.00001474
Iteration 292/1000 | Loss: 0.00001474
Iteration 293/1000 | Loss: 0.00001473
Iteration 294/1000 | Loss: 0.00001473
Iteration 295/1000 | Loss: 0.00001473
Iteration 296/1000 | Loss: 0.00001473
Iteration 297/1000 | Loss: 0.00001473
Iteration 298/1000 | Loss: 0.00001473
Iteration 299/1000 | Loss: 0.00001473
Iteration 300/1000 | Loss: 0.00001473
Iteration 301/1000 | Loss: 0.00001473
Iteration 302/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.4733891475771088e-05, 1.4733891475771088e-05, 1.4733891475771088e-05, 1.4733891475771088e-05, 1.4733891475771088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4733891475771088e-05

Optimization complete. Final v2v error: 3.2372865676879883 mm

Highest mean error: 6.1692214012146 mm for frame 16

Lowest mean error: 2.971212863922119 mm for frame 47

Saving results

Total time: 384.42050862312317
