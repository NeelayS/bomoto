Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=28, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1568-1623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992212
Iteration 2/25 | Loss: 0.00992212
Iteration 3/25 | Loss: 0.00992212
Iteration 4/25 | Loss: 0.00992212
Iteration 5/25 | Loss: 0.00992212
Iteration 6/25 | Loss: 0.00992211
Iteration 7/25 | Loss: 0.00992211
Iteration 8/25 | Loss: 0.00992211
Iteration 9/25 | Loss: 0.00275244
Iteration 10/25 | Loss: 0.00205240
Iteration 11/25 | Loss: 0.00191470
Iteration 12/25 | Loss: 0.00204189
Iteration 13/25 | Loss: 0.00178385
Iteration 14/25 | Loss: 0.00149611
Iteration 15/25 | Loss: 0.00140405
Iteration 16/25 | Loss: 0.00137729
Iteration 17/25 | Loss: 0.00137577
Iteration 18/25 | Loss: 0.00137513
Iteration 19/25 | Loss: 0.00137072
Iteration 20/25 | Loss: 0.00136413
Iteration 21/25 | Loss: 0.00136235
Iteration 22/25 | Loss: 0.00136203
Iteration 23/25 | Loss: 0.00135986
Iteration 24/25 | Loss: 0.00135859
Iteration 25/25 | Loss: 0.00136060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43875861
Iteration 2/25 | Loss: 0.00116383
Iteration 3/25 | Loss: 0.00090980
Iteration 4/25 | Loss: 0.00090980
Iteration 5/25 | Loss: 0.00090980
Iteration 6/25 | Loss: 0.00090980
Iteration 7/25 | Loss: 0.00090980
Iteration 8/25 | Loss: 0.00090979
Iteration 9/25 | Loss: 0.00090979
Iteration 10/25 | Loss: 0.00090979
Iteration 11/25 | Loss: 0.00090979
Iteration 12/25 | Loss: 0.00090979
Iteration 13/25 | Loss: 0.00090979
Iteration 14/25 | Loss: 0.00090979
Iteration 15/25 | Loss: 0.00090979
Iteration 16/25 | Loss: 0.00090979
Iteration 17/25 | Loss: 0.00090979
Iteration 18/25 | Loss: 0.00090979
Iteration 19/25 | Loss: 0.00090979
Iteration 20/25 | Loss: 0.00090979
Iteration 21/25 | Loss: 0.00090979
Iteration 22/25 | Loss: 0.00090979
Iteration 23/25 | Loss: 0.00090979
Iteration 24/25 | Loss: 0.00090979
Iteration 25/25 | Loss: 0.00090979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090979
Iteration 2/1000 | Loss: 0.00110038
Iteration 3/1000 | Loss: 0.00054848
Iteration 4/1000 | Loss: 0.00012886
Iteration 5/1000 | Loss: 0.00046225
Iteration 6/1000 | Loss: 0.00022199
Iteration 7/1000 | Loss: 0.00014142
Iteration 8/1000 | Loss: 0.00007214
Iteration 9/1000 | Loss: 0.00014167
Iteration 10/1000 | Loss: 0.00008936
Iteration 11/1000 | Loss: 0.00007814
Iteration 12/1000 | Loss: 0.00015338
Iteration 13/1000 | Loss: 0.00013996
Iteration 14/1000 | Loss: 0.00020100
Iteration 15/1000 | Loss: 0.00016937
Iteration 16/1000 | Loss: 0.00020955
Iteration 17/1000 | Loss: 0.00026521
Iteration 18/1000 | Loss: 0.00023647
Iteration 19/1000 | Loss: 0.00143312
Iteration 20/1000 | Loss: 0.00254671
Iteration 21/1000 | Loss: 0.00168527
Iteration 22/1000 | Loss: 0.00243057
Iteration 23/1000 | Loss: 0.00163485
Iteration 24/1000 | Loss: 0.00087958
Iteration 25/1000 | Loss: 0.00012658
Iteration 26/1000 | Loss: 0.00077857
Iteration 27/1000 | Loss: 0.00005837
Iteration 28/1000 | Loss: 0.00004549
Iteration 29/1000 | Loss: 0.00005084
Iteration 30/1000 | Loss: 0.00017256
Iteration 31/1000 | Loss: 0.00004568
Iteration 32/1000 | Loss: 0.00004483
Iteration 33/1000 | Loss: 0.00063144
Iteration 34/1000 | Loss: 0.00014768
Iteration 35/1000 | Loss: 0.00007597
Iteration 36/1000 | Loss: 0.00006451
Iteration 37/1000 | Loss: 0.00006010
Iteration 38/1000 | Loss: 0.00004086
Iteration 39/1000 | Loss: 0.00004658
Iteration 40/1000 | Loss: 0.00003916
Iteration 41/1000 | Loss: 0.00003704
Iteration 42/1000 | Loss: 0.00022993
Iteration 43/1000 | Loss: 0.00004588
Iteration 44/1000 | Loss: 0.00004531
Iteration 45/1000 | Loss: 0.00003961
Iteration 46/1000 | Loss: 0.00004551
Iteration 47/1000 | Loss: 0.00003881
Iteration 48/1000 | Loss: 0.00005060
Iteration 49/1000 | Loss: 0.00017021
Iteration 50/1000 | Loss: 0.00004901
Iteration 51/1000 | Loss: 0.00004268
Iteration 52/1000 | Loss: 0.00003778
Iteration 53/1000 | Loss: 0.00003520
Iteration 54/1000 | Loss: 0.00020100
Iteration 55/1000 | Loss: 0.00004544
Iteration 56/1000 | Loss: 0.00004613
Iteration 57/1000 | Loss: 0.00013415
Iteration 58/1000 | Loss: 0.00004661
Iteration 59/1000 | Loss: 0.00004432
Iteration 60/1000 | Loss: 0.00004016
Iteration 61/1000 | Loss: 0.00003690
Iteration 62/1000 | Loss: 0.00003754
Iteration 63/1000 | Loss: 0.00003872
Iteration 64/1000 | Loss: 0.00003733
Iteration 65/1000 | Loss: 0.00010024
Iteration 66/1000 | Loss: 0.00022252
Iteration 67/1000 | Loss: 0.00031713
Iteration 68/1000 | Loss: 0.00038447
Iteration 69/1000 | Loss: 0.00021066
Iteration 70/1000 | Loss: 0.00009045
Iteration 71/1000 | Loss: 0.00004442
Iteration 72/1000 | Loss: 0.00005489
Iteration 73/1000 | Loss: 0.00008501
Iteration 74/1000 | Loss: 0.00003915
Iteration 75/1000 | Loss: 0.00003566
Iteration 76/1000 | Loss: 0.00005057
Iteration 77/1000 | Loss: 0.00004477
Iteration 78/1000 | Loss: 0.00004334
Iteration 79/1000 | Loss: 0.00003416
Iteration 80/1000 | Loss: 0.00004306
Iteration 81/1000 | Loss: 0.00003655
Iteration 82/1000 | Loss: 0.00012770
Iteration 83/1000 | Loss: 0.00006916
Iteration 84/1000 | Loss: 0.00013300
Iteration 85/1000 | Loss: 0.00006922
Iteration 86/1000 | Loss: 0.00003659
Iteration 87/1000 | Loss: 0.00003626
Iteration 88/1000 | Loss: 0.00003283
Iteration 89/1000 | Loss: 0.00017923
Iteration 90/1000 | Loss: 0.00014437
Iteration 91/1000 | Loss: 0.00008892
Iteration 92/1000 | Loss: 0.00003519
Iteration 93/1000 | Loss: 0.00006993
Iteration 94/1000 | Loss: 0.00003371
Iteration 95/1000 | Loss: 0.00003290
Iteration 96/1000 | Loss: 0.00003251
Iteration 97/1000 | Loss: 0.00007273
Iteration 98/1000 | Loss: 0.00003183
Iteration 99/1000 | Loss: 0.00011693
Iteration 100/1000 | Loss: 0.00003350
Iteration 101/1000 | Loss: 0.00003654
Iteration 102/1000 | Loss: 0.00003153
Iteration 103/1000 | Loss: 0.00004208
Iteration 104/1000 | Loss: 0.00002963
Iteration 105/1000 | Loss: 0.00002913
Iteration 106/1000 | Loss: 0.00003769
Iteration 107/1000 | Loss: 0.00002866
Iteration 108/1000 | Loss: 0.00002831
Iteration 109/1000 | Loss: 0.00002786
Iteration 110/1000 | Loss: 0.00002724
Iteration 111/1000 | Loss: 0.00002701
Iteration 112/1000 | Loss: 0.00005242
Iteration 113/1000 | Loss: 0.00002681
Iteration 114/1000 | Loss: 0.00002612
Iteration 115/1000 | Loss: 0.00002582
Iteration 116/1000 | Loss: 0.00002535
Iteration 117/1000 | Loss: 0.00002507
Iteration 118/1000 | Loss: 0.00003044
Iteration 119/1000 | Loss: 0.00002486
Iteration 120/1000 | Loss: 0.00002484
Iteration 121/1000 | Loss: 0.00002484
Iteration 122/1000 | Loss: 0.00002477
Iteration 123/1000 | Loss: 0.00002477
Iteration 124/1000 | Loss: 0.00002474
Iteration 125/1000 | Loss: 0.00002472
Iteration 126/1000 | Loss: 0.00002470
Iteration 127/1000 | Loss: 0.00002470
Iteration 128/1000 | Loss: 0.00002469
Iteration 129/1000 | Loss: 0.00002469
Iteration 130/1000 | Loss: 0.00002469
Iteration 131/1000 | Loss: 0.00002887
Iteration 132/1000 | Loss: 0.00002465
Iteration 133/1000 | Loss: 0.00002464
Iteration 134/1000 | Loss: 0.00002463
Iteration 135/1000 | Loss: 0.00002463
Iteration 136/1000 | Loss: 0.00002463
Iteration 137/1000 | Loss: 0.00002463
Iteration 138/1000 | Loss: 0.00002463
Iteration 139/1000 | Loss: 0.00002463
Iteration 140/1000 | Loss: 0.00002463
Iteration 141/1000 | Loss: 0.00002463
Iteration 142/1000 | Loss: 0.00002463
Iteration 143/1000 | Loss: 0.00002463
Iteration 144/1000 | Loss: 0.00002462
Iteration 145/1000 | Loss: 0.00002462
Iteration 146/1000 | Loss: 0.00002462
Iteration 147/1000 | Loss: 0.00002462
Iteration 148/1000 | Loss: 0.00002461
Iteration 149/1000 | Loss: 0.00002461
Iteration 150/1000 | Loss: 0.00002461
Iteration 151/1000 | Loss: 0.00002461
Iteration 152/1000 | Loss: 0.00002460
Iteration 153/1000 | Loss: 0.00002460
Iteration 154/1000 | Loss: 0.00002460
Iteration 155/1000 | Loss: 0.00002460
Iteration 156/1000 | Loss: 0.00002460
Iteration 157/1000 | Loss: 0.00002460
Iteration 158/1000 | Loss: 0.00002460
Iteration 159/1000 | Loss: 0.00002460
Iteration 160/1000 | Loss: 0.00002460
Iteration 161/1000 | Loss: 0.00002460
Iteration 162/1000 | Loss: 0.00002460
Iteration 163/1000 | Loss: 0.00002460
Iteration 164/1000 | Loss: 0.00002459
Iteration 165/1000 | Loss: 0.00002459
Iteration 166/1000 | Loss: 0.00002459
Iteration 167/1000 | Loss: 0.00002459
Iteration 168/1000 | Loss: 0.00002459
Iteration 169/1000 | Loss: 0.00002459
Iteration 170/1000 | Loss: 0.00002459
Iteration 171/1000 | Loss: 0.00002459
Iteration 172/1000 | Loss: 0.00002459
Iteration 173/1000 | Loss: 0.00002459
Iteration 174/1000 | Loss: 0.00002459
Iteration 175/1000 | Loss: 0.00002459
Iteration 176/1000 | Loss: 0.00002458
Iteration 177/1000 | Loss: 0.00002458
Iteration 178/1000 | Loss: 0.00002458
Iteration 179/1000 | Loss: 0.00002458
Iteration 180/1000 | Loss: 0.00002458
Iteration 181/1000 | Loss: 0.00002458
Iteration 182/1000 | Loss: 0.00002457
Iteration 183/1000 | Loss: 0.00002457
Iteration 184/1000 | Loss: 0.00002457
Iteration 185/1000 | Loss: 0.00002457
Iteration 186/1000 | Loss: 0.00002457
Iteration 187/1000 | Loss: 0.00002457
Iteration 188/1000 | Loss: 0.00002457
Iteration 189/1000 | Loss: 0.00002457
Iteration 190/1000 | Loss: 0.00002457
Iteration 191/1000 | Loss: 0.00002456
Iteration 192/1000 | Loss: 0.00002456
Iteration 193/1000 | Loss: 0.00002456
Iteration 194/1000 | Loss: 0.00002456
Iteration 195/1000 | Loss: 0.00002456
Iteration 196/1000 | Loss: 0.00002456
Iteration 197/1000 | Loss: 0.00002456
Iteration 198/1000 | Loss: 0.00002456
Iteration 199/1000 | Loss: 0.00002456
Iteration 200/1000 | Loss: 0.00002456
Iteration 201/1000 | Loss: 0.00002456
Iteration 202/1000 | Loss: 0.00002456
Iteration 203/1000 | Loss: 0.00002456
Iteration 204/1000 | Loss: 0.00002456
Iteration 205/1000 | Loss: 0.00002456
Iteration 206/1000 | Loss: 0.00002456
Iteration 207/1000 | Loss: 0.00002456
Iteration 208/1000 | Loss: 0.00002456
Iteration 209/1000 | Loss: 0.00002455
Iteration 210/1000 | Loss: 0.00002455
Iteration 211/1000 | Loss: 0.00002455
Iteration 212/1000 | Loss: 0.00002455
Iteration 213/1000 | Loss: 0.00002455
Iteration 214/1000 | Loss: 0.00002455
Iteration 215/1000 | Loss: 0.00002455
Iteration 216/1000 | Loss: 0.00002455
Iteration 217/1000 | Loss: 0.00002455
Iteration 218/1000 | Loss: 0.00002455
Iteration 219/1000 | Loss: 0.00002455
Iteration 220/1000 | Loss: 0.00002455
Iteration 221/1000 | Loss: 0.00002455
Iteration 222/1000 | Loss: 0.00002455
Iteration 223/1000 | Loss: 0.00002455
Iteration 224/1000 | Loss: 0.00002455
Iteration 225/1000 | Loss: 0.00002455
Iteration 226/1000 | Loss: 0.00002455
Iteration 227/1000 | Loss: 0.00002455
Iteration 228/1000 | Loss: 0.00002455
Iteration 229/1000 | Loss: 0.00002455
Iteration 230/1000 | Loss: 0.00002455
Iteration 231/1000 | Loss: 0.00002455
Iteration 232/1000 | Loss: 0.00002455
Iteration 233/1000 | Loss: 0.00002455
Iteration 234/1000 | Loss: 0.00002455
Iteration 235/1000 | Loss: 0.00002455
Iteration 236/1000 | Loss: 0.00002455
Iteration 237/1000 | Loss: 0.00002455
Iteration 238/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.4552593458793126e-05, 2.4552593458793126e-05, 2.4552593458793126e-05, 2.4552593458793126e-05, 2.4552593458793126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4552593458793126e-05

Optimization complete. Final v2v error: 4.074302673339844 mm

Highest mean error: 6.062354564666748 mm for frame 129

Lowest mean error: 3.448997974395752 mm for frame 65

Saving results

Total time: 239.9320514202118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913194
Iteration 2/25 | Loss: 0.00160419
Iteration 3/25 | Loss: 0.00145981
Iteration 4/25 | Loss: 0.00128526
Iteration 5/25 | Loss: 0.00127619
Iteration 6/25 | Loss: 0.00127787
Iteration 7/25 | Loss: 0.00127466
Iteration 8/25 | Loss: 0.00127211
Iteration 9/25 | Loss: 0.00128038
Iteration 10/25 | Loss: 0.00127695
Iteration 11/25 | Loss: 0.00127127
Iteration 12/25 | Loss: 0.00126914
Iteration 13/25 | Loss: 0.00126898
Iteration 14/25 | Loss: 0.00126897
Iteration 15/25 | Loss: 0.00126896
Iteration 16/25 | Loss: 0.00126896
Iteration 17/25 | Loss: 0.00126896
Iteration 18/25 | Loss: 0.00126896
Iteration 19/25 | Loss: 0.00126896
Iteration 20/25 | Loss: 0.00126896
Iteration 21/25 | Loss: 0.00126896
Iteration 22/25 | Loss: 0.00126896
Iteration 23/25 | Loss: 0.00126896
Iteration 24/25 | Loss: 0.00126896
Iteration 25/25 | Loss: 0.00126896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56049418
Iteration 2/25 | Loss: 0.00085381
Iteration 3/25 | Loss: 0.00073800
Iteration 4/25 | Loss: 0.00073800
Iteration 5/25 | Loss: 0.00073800
Iteration 6/25 | Loss: 0.00073800
Iteration 7/25 | Loss: 0.00073800
Iteration 8/25 | Loss: 0.00073800
Iteration 9/25 | Loss: 0.00073800
Iteration 10/25 | Loss: 0.00073800
Iteration 11/25 | Loss: 0.00073800
Iteration 12/25 | Loss: 0.00073800
Iteration 13/25 | Loss: 0.00073800
Iteration 14/25 | Loss: 0.00073800
Iteration 15/25 | Loss: 0.00073800
Iteration 16/25 | Loss: 0.00073800
Iteration 17/25 | Loss: 0.00073800
Iteration 18/25 | Loss: 0.00073800
Iteration 19/25 | Loss: 0.00073800
Iteration 20/25 | Loss: 0.00073800
Iteration 21/25 | Loss: 0.00073800
Iteration 22/25 | Loss: 0.00073800
Iteration 23/25 | Loss: 0.00073800
Iteration 24/25 | Loss: 0.00073800
Iteration 25/25 | Loss: 0.00073800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007379997405223548, 0.0007379997405223548, 0.0007379997405223548, 0.0007379997405223548, 0.0007379997405223548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007379997405223548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073800
Iteration 2/1000 | Loss: 0.00012830
Iteration 3/1000 | Loss: 0.00019406
Iteration 4/1000 | Loss: 0.00002344
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002096
Iteration 7/1000 | Loss: 0.00002032
Iteration 8/1000 | Loss: 0.00001977
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001879
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001819
Iteration 22/1000 | Loss: 0.00001819
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001817
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001815
Iteration 31/1000 | Loss: 0.00001815
Iteration 32/1000 | Loss: 0.00001815
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001814
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001813
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001813
Iteration 41/1000 | Loss: 0.00001813
Iteration 42/1000 | Loss: 0.00001812
Iteration 43/1000 | Loss: 0.00001812
Iteration 44/1000 | Loss: 0.00001812
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001811
Iteration 47/1000 | Loss: 0.00001811
Iteration 48/1000 | Loss: 0.00001811
Iteration 49/1000 | Loss: 0.00001810
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001807
Iteration 61/1000 | Loss: 0.00001807
Iteration 62/1000 | Loss: 0.00001807
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001806
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001804
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001803
Iteration 83/1000 | Loss: 0.00001803
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001801
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001801
Iteration 99/1000 | Loss: 0.00001801
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001801
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001800
Iteration 105/1000 | Loss: 0.00001800
Iteration 106/1000 | Loss: 0.00001800
Iteration 107/1000 | Loss: 0.00001800
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001799
Iteration 110/1000 | Loss: 0.00001799
Iteration 111/1000 | Loss: 0.00001799
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001798
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001797
Iteration 119/1000 | Loss: 0.00001797
Iteration 120/1000 | Loss: 0.00001797
Iteration 121/1000 | Loss: 0.00001797
Iteration 122/1000 | Loss: 0.00001796
Iteration 123/1000 | Loss: 0.00001796
Iteration 124/1000 | Loss: 0.00001796
Iteration 125/1000 | Loss: 0.00001796
Iteration 126/1000 | Loss: 0.00001796
Iteration 127/1000 | Loss: 0.00001796
Iteration 128/1000 | Loss: 0.00001796
Iteration 129/1000 | Loss: 0.00001796
Iteration 130/1000 | Loss: 0.00001796
Iteration 131/1000 | Loss: 0.00001796
Iteration 132/1000 | Loss: 0.00001795
Iteration 133/1000 | Loss: 0.00001795
Iteration 134/1000 | Loss: 0.00001795
Iteration 135/1000 | Loss: 0.00001795
Iteration 136/1000 | Loss: 0.00001795
Iteration 137/1000 | Loss: 0.00001795
Iteration 138/1000 | Loss: 0.00001795
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001794
Iteration 142/1000 | Loss: 0.00001794
Iteration 143/1000 | Loss: 0.00001794
Iteration 144/1000 | Loss: 0.00001794
Iteration 145/1000 | Loss: 0.00001793
Iteration 146/1000 | Loss: 0.00001793
Iteration 147/1000 | Loss: 0.00001793
Iteration 148/1000 | Loss: 0.00001793
Iteration 149/1000 | Loss: 0.00001793
Iteration 150/1000 | Loss: 0.00001793
Iteration 151/1000 | Loss: 0.00001793
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001793
Iteration 155/1000 | Loss: 0.00001793
Iteration 156/1000 | Loss: 0.00001793
Iteration 157/1000 | Loss: 0.00001793
Iteration 158/1000 | Loss: 0.00001793
Iteration 159/1000 | Loss: 0.00001793
Iteration 160/1000 | Loss: 0.00001793
Iteration 161/1000 | Loss: 0.00001793
Iteration 162/1000 | Loss: 0.00001793
Iteration 163/1000 | Loss: 0.00001792
Iteration 164/1000 | Loss: 0.00001792
Iteration 165/1000 | Loss: 0.00001792
Iteration 166/1000 | Loss: 0.00001792
Iteration 167/1000 | Loss: 0.00001792
Iteration 168/1000 | Loss: 0.00001792
Iteration 169/1000 | Loss: 0.00001792
Iteration 170/1000 | Loss: 0.00001792
Iteration 171/1000 | Loss: 0.00001792
Iteration 172/1000 | Loss: 0.00001792
Iteration 173/1000 | Loss: 0.00001792
Iteration 174/1000 | Loss: 0.00001792
Iteration 175/1000 | Loss: 0.00001792
Iteration 176/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.791758586477954e-05, 1.791758586477954e-05, 1.791758586477954e-05, 1.791758586477954e-05, 1.791758586477954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.791758586477954e-05

Optimization complete. Final v2v error: 3.5700700283050537 mm

Highest mean error: 4.249602794647217 mm for frame 39

Lowest mean error: 3.1263906955718994 mm for frame 121

Saving results

Total time: 55.62009000778198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796365
Iteration 2/25 | Loss: 0.00144754
Iteration 3/25 | Loss: 0.00129467
Iteration 4/25 | Loss: 0.00127435
Iteration 5/25 | Loss: 0.00126969
Iteration 6/25 | Loss: 0.00126901
Iteration 7/25 | Loss: 0.00126901
Iteration 8/25 | Loss: 0.00126901
Iteration 9/25 | Loss: 0.00126901
Iteration 10/25 | Loss: 0.00126901
Iteration 11/25 | Loss: 0.00126901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012690089643001556, 0.0012690089643001556, 0.0012690089643001556, 0.0012690089643001556, 0.0012690089643001556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012690089643001556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39551914
Iteration 2/25 | Loss: 0.00072593
Iteration 3/25 | Loss: 0.00072588
Iteration 4/25 | Loss: 0.00072588
Iteration 5/25 | Loss: 0.00072588
Iteration 6/25 | Loss: 0.00072588
Iteration 7/25 | Loss: 0.00072588
Iteration 8/25 | Loss: 0.00072588
Iteration 9/25 | Loss: 0.00072588
Iteration 10/25 | Loss: 0.00072588
Iteration 11/25 | Loss: 0.00072588
Iteration 12/25 | Loss: 0.00072588
Iteration 13/25 | Loss: 0.00072588
Iteration 14/25 | Loss: 0.00072588
Iteration 15/25 | Loss: 0.00072588
Iteration 16/25 | Loss: 0.00072588
Iteration 17/25 | Loss: 0.00072588
Iteration 18/25 | Loss: 0.00072588
Iteration 19/25 | Loss: 0.00072588
Iteration 20/25 | Loss: 0.00072588
Iteration 21/25 | Loss: 0.00072588
Iteration 22/25 | Loss: 0.00072588
Iteration 23/25 | Loss: 0.00072588
Iteration 24/25 | Loss: 0.00072588
Iteration 25/25 | Loss: 0.00072588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072588
Iteration 2/1000 | Loss: 0.00003688
Iteration 3/1000 | Loss: 0.00002577
Iteration 4/1000 | Loss: 0.00002337
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002011
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001872
Iteration 12/1000 | Loss: 0.00001871
Iteration 13/1000 | Loss: 0.00001868
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001838
Iteration 22/1000 | Loss: 0.00001835
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001820
Iteration 32/1000 | Loss: 0.00001820
Iteration 33/1000 | Loss: 0.00001820
Iteration 34/1000 | Loss: 0.00001820
Iteration 35/1000 | Loss: 0.00001820
Iteration 36/1000 | Loss: 0.00001819
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001818
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001817
Iteration 42/1000 | Loss: 0.00001817
Iteration 43/1000 | Loss: 0.00001817
Iteration 44/1000 | Loss: 0.00001816
Iteration 45/1000 | Loss: 0.00001816
Iteration 46/1000 | Loss: 0.00001815
Iteration 47/1000 | Loss: 0.00001815
Iteration 48/1000 | Loss: 0.00001815
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001814
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001812
Iteration 57/1000 | Loss: 0.00001811
Iteration 58/1000 | Loss: 0.00001811
Iteration 59/1000 | Loss: 0.00001811
Iteration 60/1000 | Loss: 0.00001811
Iteration 61/1000 | Loss: 0.00001810
Iteration 62/1000 | Loss: 0.00001810
Iteration 63/1000 | Loss: 0.00001810
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001809
Iteration 68/1000 | Loss: 0.00001809
Iteration 69/1000 | Loss: 0.00001809
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001808
Iteration 74/1000 | Loss: 0.00001808
Iteration 75/1000 | Loss: 0.00001807
Iteration 76/1000 | Loss: 0.00001807
Iteration 77/1000 | Loss: 0.00001807
Iteration 78/1000 | Loss: 0.00001807
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001806
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001806
Iteration 101/1000 | Loss: 0.00001806
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001806
Iteration 110/1000 | Loss: 0.00001806
Iteration 111/1000 | Loss: 0.00001806
Iteration 112/1000 | Loss: 0.00001806
Iteration 113/1000 | Loss: 0.00001806
Iteration 114/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.805932515708264e-05, 1.805932515708264e-05, 1.805932515708264e-05, 1.805932515708264e-05, 1.805932515708264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.805932515708264e-05

Optimization complete. Final v2v error: 3.5212082862854004 mm

Highest mean error: 4.870004653930664 mm for frame 23

Lowest mean error: 2.9942214488983154 mm for frame 36

Saving results

Total time: 40.155550956726074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083530
Iteration 2/25 | Loss: 0.01083529
Iteration 3/25 | Loss: 0.00404766
Iteration 4/25 | Loss: 0.00238417
Iteration 5/25 | Loss: 0.00224459
Iteration 6/25 | Loss: 0.00214513
Iteration 7/25 | Loss: 0.00185294
Iteration 8/25 | Loss: 0.00169799
Iteration 9/25 | Loss: 0.00168087
Iteration 10/25 | Loss: 0.00167522
Iteration 11/25 | Loss: 0.00167237
Iteration 12/25 | Loss: 0.00167174
Iteration 13/25 | Loss: 0.00167134
Iteration 14/25 | Loss: 0.00167098
Iteration 15/25 | Loss: 0.00167082
Iteration 16/25 | Loss: 0.00167070
Iteration 17/25 | Loss: 0.00167066
Iteration 18/25 | Loss: 0.00167066
Iteration 19/25 | Loss: 0.00167066
Iteration 20/25 | Loss: 0.00167066
Iteration 21/25 | Loss: 0.00167066
Iteration 22/25 | Loss: 0.00167066
Iteration 23/25 | Loss: 0.00167066
Iteration 24/25 | Loss: 0.00167066
Iteration 25/25 | Loss: 0.00167066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27947199
Iteration 2/25 | Loss: 0.00248377
Iteration 3/25 | Loss: 0.00248375
Iteration 4/25 | Loss: 0.00248375
Iteration 5/25 | Loss: 0.00248375
Iteration 6/25 | Loss: 0.00248375
Iteration 7/25 | Loss: 0.00248375
Iteration 8/25 | Loss: 0.00248375
Iteration 9/25 | Loss: 0.00248375
Iteration 10/25 | Loss: 0.00248375
Iteration 11/25 | Loss: 0.00248375
Iteration 12/25 | Loss: 0.00248375
Iteration 13/25 | Loss: 0.00248375
Iteration 14/25 | Loss: 0.00248375
Iteration 15/25 | Loss: 0.00248375
Iteration 16/25 | Loss: 0.00248375
Iteration 17/25 | Loss: 0.00248375
Iteration 18/25 | Loss: 0.00248375
Iteration 19/25 | Loss: 0.00248375
Iteration 20/25 | Loss: 0.00248375
Iteration 21/25 | Loss: 0.00248375
Iteration 22/25 | Loss: 0.00248375
Iteration 23/25 | Loss: 0.00248375
Iteration 24/25 | Loss: 0.00248375
Iteration 25/25 | Loss: 0.00248375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248375
Iteration 2/1000 | Loss: 0.00029599
Iteration 3/1000 | Loss: 0.00024253
Iteration 4/1000 | Loss: 0.00021930
Iteration 5/1000 | Loss: 0.00020342
Iteration 6/1000 | Loss: 0.00019376
Iteration 7/1000 | Loss: 0.00018441
Iteration 8/1000 | Loss: 0.00017646
Iteration 9/1000 | Loss: 0.00017258
Iteration 10/1000 | Loss: 0.00016717
Iteration 11/1000 | Loss: 0.00016412
Iteration 12/1000 | Loss: 0.00016111
Iteration 13/1000 | Loss: 0.00015927
Iteration 14/1000 | Loss: 0.00015812
Iteration 15/1000 | Loss: 0.00015701
Iteration 16/1000 | Loss: 0.00015550
Iteration 17/1000 | Loss: 0.00015469
Iteration 18/1000 | Loss: 0.00015409
Iteration 19/1000 | Loss: 0.00015356
Iteration 20/1000 | Loss: 0.00015324
Iteration 21/1000 | Loss: 0.00015291
Iteration 22/1000 | Loss: 0.00015256
Iteration 23/1000 | Loss: 0.00015228
Iteration 24/1000 | Loss: 0.00015219
Iteration 25/1000 | Loss: 0.00015205
Iteration 26/1000 | Loss: 0.00015195
Iteration 27/1000 | Loss: 0.00015180
Iteration 28/1000 | Loss: 0.00015177
Iteration 29/1000 | Loss: 0.00015173
Iteration 30/1000 | Loss: 0.00015171
Iteration 31/1000 | Loss: 0.00015165
Iteration 32/1000 | Loss: 0.00015158
Iteration 33/1000 | Loss: 0.00015153
Iteration 34/1000 | Loss: 0.00015152
Iteration 35/1000 | Loss: 0.00015146
Iteration 36/1000 | Loss: 0.00015146
Iteration 37/1000 | Loss: 0.00015145
Iteration 38/1000 | Loss: 0.00015142
Iteration 39/1000 | Loss: 0.00015141
Iteration 40/1000 | Loss: 0.00015141
Iteration 41/1000 | Loss: 0.00015133
Iteration 42/1000 | Loss: 0.00015133
Iteration 43/1000 | Loss: 0.00015133
Iteration 44/1000 | Loss: 0.00015133
Iteration 45/1000 | Loss: 0.00015133
Iteration 46/1000 | Loss: 0.00015133
Iteration 47/1000 | Loss: 0.00015133
Iteration 48/1000 | Loss: 0.00015132
Iteration 49/1000 | Loss: 0.00015132
Iteration 50/1000 | Loss: 0.00015132
Iteration 51/1000 | Loss: 0.00015131
Iteration 52/1000 | Loss: 0.00015131
Iteration 53/1000 | Loss: 0.00015130
Iteration 54/1000 | Loss: 0.00015130
Iteration 55/1000 | Loss: 0.00015130
Iteration 56/1000 | Loss: 0.00015130
Iteration 57/1000 | Loss: 0.00015129
Iteration 58/1000 | Loss: 0.00015129
Iteration 59/1000 | Loss: 0.00015129
Iteration 60/1000 | Loss: 0.00015129
Iteration 61/1000 | Loss: 0.00015129
Iteration 62/1000 | Loss: 0.00015128
Iteration 63/1000 | Loss: 0.00015128
Iteration 64/1000 | Loss: 0.00015128
Iteration 65/1000 | Loss: 0.00015128
Iteration 66/1000 | Loss: 0.00015127
Iteration 67/1000 | Loss: 0.00015127
Iteration 68/1000 | Loss: 0.00015127
Iteration 69/1000 | Loss: 0.00015127
Iteration 70/1000 | Loss: 0.00015127
Iteration 71/1000 | Loss: 0.00015127
Iteration 72/1000 | Loss: 0.00015126
Iteration 73/1000 | Loss: 0.00015126
Iteration 74/1000 | Loss: 0.00015126
Iteration 75/1000 | Loss: 0.00015125
Iteration 76/1000 | Loss: 0.00015125
Iteration 77/1000 | Loss: 0.00015125
Iteration 78/1000 | Loss: 0.00015124
Iteration 79/1000 | Loss: 0.00015124
Iteration 80/1000 | Loss: 0.00015124
Iteration 81/1000 | Loss: 0.00015124
Iteration 82/1000 | Loss: 0.00015124
Iteration 83/1000 | Loss: 0.00015124
Iteration 84/1000 | Loss: 0.00015124
Iteration 85/1000 | Loss: 0.00015124
Iteration 86/1000 | Loss: 0.00015124
Iteration 87/1000 | Loss: 0.00015123
Iteration 88/1000 | Loss: 0.00015123
Iteration 89/1000 | Loss: 0.00015123
Iteration 90/1000 | Loss: 0.00015123
Iteration 91/1000 | Loss: 0.00015122
Iteration 92/1000 | Loss: 0.00015122
Iteration 93/1000 | Loss: 0.00015122
Iteration 94/1000 | Loss: 0.00015122
Iteration 95/1000 | Loss: 0.00015122
Iteration 96/1000 | Loss: 0.00015122
Iteration 97/1000 | Loss: 0.00015121
Iteration 98/1000 | Loss: 0.00015121
Iteration 99/1000 | Loss: 0.00015121
Iteration 100/1000 | Loss: 0.00015121
Iteration 101/1000 | Loss: 0.00015121
Iteration 102/1000 | Loss: 0.00015121
Iteration 103/1000 | Loss: 0.00015121
Iteration 104/1000 | Loss: 0.00015120
Iteration 105/1000 | Loss: 0.00015120
Iteration 106/1000 | Loss: 0.00015120
Iteration 107/1000 | Loss: 0.00015120
Iteration 108/1000 | Loss: 0.00015120
Iteration 109/1000 | Loss: 0.00015120
Iteration 110/1000 | Loss: 0.00015120
Iteration 111/1000 | Loss: 0.00015120
Iteration 112/1000 | Loss: 0.00015120
Iteration 113/1000 | Loss: 0.00015120
Iteration 114/1000 | Loss: 0.00015120
Iteration 115/1000 | Loss: 0.00015120
Iteration 116/1000 | Loss: 0.00015119
Iteration 117/1000 | Loss: 0.00015119
Iteration 118/1000 | Loss: 0.00015119
Iteration 119/1000 | Loss: 0.00015119
Iteration 120/1000 | Loss: 0.00015119
Iteration 121/1000 | Loss: 0.00015119
Iteration 122/1000 | Loss: 0.00015119
Iteration 123/1000 | Loss: 0.00015118
Iteration 124/1000 | Loss: 0.00015118
Iteration 125/1000 | Loss: 0.00015118
Iteration 126/1000 | Loss: 0.00015118
Iteration 127/1000 | Loss: 0.00015118
Iteration 128/1000 | Loss: 0.00015118
Iteration 129/1000 | Loss: 0.00015118
Iteration 130/1000 | Loss: 0.00015118
Iteration 131/1000 | Loss: 0.00015118
Iteration 132/1000 | Loss: 0.00015118
Iteration 133/1000 | Loss: 0.00015118
Iteration 134/1000 | Loss: 0.00015118
Iteration 135/1000 | Loss: 0.00015118
Iteration 136/1000 | Loss: 0.00015118
Iteration 137/1000 | Loss: 0.00015117
Iteration 138/1000 | Loss: 0.00015117
Iteration 139/1000 | Loss: 0.00015117
Iteration 140/1000 | Loss: 0.00015117
Iteration 141/1000 | Loss: 0.00015117
Iteration 142/1000 | Loss: 0.00015116
Iteration 143/1000 | Loss: 0.00015116
Iteration 144/1000 | Loss: 0.00015116
Iteration 145/1000 | Loss: 0.00015116
Iteration 146/1000 | Loss: 0.00015116
Iteration 147/1000 | Loss: 0.00015116
Iteration 148/1000 | Loss: 0.00015116
Iteration 149/1000 | Loss: 0.00015116
Iteration 150/1000 | Loss: 0.00015116
Iteration 151/1000 | Loss: 0.00015116
Iteration 152/1000 | Loss: 0.00015116
Iteration 153/1000 | Loss: 0.00015116
Iteration 154/1000 | Loss: 0.00015116
Iteration 155/1000 | Loss: 0.00015116
Iteration 156/1000 | Loss: 0.00015116
Iteration 157/1000 | Loss: 0.00015116
Iteration 158/1000 | Loss: 0.00015116
Iteration 159/1000 | Loss: 0.00015116
Iteration 160/1000 | Loss: 0.00015116
Iteration 161/1000 | Loss: 0.00015116
Iteration 162/1000 | Loss: 0.00015116
Iteration 163/1000 | Loss: 0.00015116
Iteration 164/1000 | Loss: 0.00015116
Iteration 165/1000 | Loss: 0.00015116
Iteration 166/1000 | Loss: 0.00015116
Iteration 167/1000 | Loss: 0.00015116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [0.0001511611044406891, 0.0001511611044406891, 0.0001511611044406891, 0.0001511611044406891, 0.0001511611044406891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001511611044406891

Optimization complete. Final v2v error: 7.2913384437561035 mm

Highest mean error: 11.128535270690918 mm for frame 71

Lowest mean error: 5.076210975646973 mm for frame 61

Saving results

Total time: 85.64329075813293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051611
Iteration 2/25 | Loss: 0.00421823
Iteration 3/25 | Loss: 0.00336094
Iteration 4/25 | Loss: 0.00324265
Iteration 5/25 | Loss: 0.00330180
Iteration 6/25 | Loss: 0.00300472
Iteration 7/25 | Loss: 0.00282525
Iteration 8/25 | Loss: 0.00287105
Iteration 9/25 | Loss: 0.00262579
Iteration 10/25 | Loss: 0.00246689
Iteration 11/25 | Loss: 0.00236709
Iteration 12/25 | Loss: 0.00239520
Iteration 13/25 | Loss: 0.00232641
Iteration 14/25 | Loss: 0.00221802
Iteration 15/25 | Loss: 0.00208104
Iteration 16/25 | Loss: 0.00209388
Iteration 17/25 | Loss: 0.00190480
Iteration 18/25 | Loss: 0.00189759
Iteration 19/25 | Loss: 0.00181207
Iteration 20/25 | Loss: 0.00181570
Iteration 21/25 | Loss: 0.00174239
Iteration 22/25 | Loss: 0.00174960
Iteration 23/25 | Loss: 0.00174043
Iteration 24/25 | Loss: 0.00172941
Iteration 25/25 | Loss: 0.00172758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13231766
Iteration 2/25 | Loss: 0.00332520
Iteration 3/25 | Loss: 0.00332520
Iteration 4/25 | Loss: 0.00332520
Iteration 5/25 | Loss: 0.00332520
Iteration 6/25 | Loss: 0.00332520
Iteration 7/25 | Loss: 0.00332520
Iteration 8/25 | Loss: 0.00332519
Iteration 9/25 | Loss: 0.00332519
Iteration 10/25 | Loss: 0.00332519
Iteration 11/25 | Loss: 0.00332519
Iteration 12/25 | Loss: 0.00332519
Iteration 13/25 | Loss: 0.00332519
Iteration 14/25 | Loss: 0.00332519
Iteration 15/25 | Loss: 0.00332519
Iteration 16/25 | Loss: 0.00332519
Iteration 17/25 | Loss: 0.00332519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003325193887576461, 0.003325193887576461, 0.003325193887576461, 0.003325193887576461, 0.003325193887576461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003325193887576461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332519
Iteration 2/1000 | Loss: 0.00108175
Iteration 3/1000 | Loss: 0.00055736
Iteration 4/1000 | Loss: 0.00023079
Iteration 5/1000 | Loss: 0.00020005
Iteration 6/1000 | Loss: 0.00039349
Iteration 7/1000 | Loss: 0.00018835
Iteration 8/1000 | Loss: 0.00087034
Iteration 9/1000 | Loss: 0.00046408
Iteration 10/1000 | Loss: 0.00041674
Iteration 11/1000 | Loss: 0.00098657
Iteration 12/1000 | Loss: 0.00059278
Iteration 13/1000 | Loss: 0.00333603
Iteration 14/1000 | Loss: 0.00706810
Iteration 15/1000 | Loss: 0.00232191
Iteration 16/1000 | Loss: 0.00051721
Iteration 17/1000 | Loss: 0.00069870
Iteration 18/1000 | Loss: 0.00041590
Iteration 19/1000 | Loss: 0.00025231
Iteration 20/1000 | Loss: 0.00017672
Iteration 21/1000 | Loss: 0.00014615
Iteration 22/1000 | Loss: 0.00177298
Iteration 23/1000 | Loss: 0.00048962
Iteration 24/1000 | Loss: 0.00089297
Iteration 25/1000 | Loss: 0.00135773
Iteration 26/1000 | Loss: 0.00096879
Iteration 27/1000 | Loss: 0.00119968
Iteration 28/1000 | Loss: 0.00180605
Iteration 29/1000 | Loss: 0.00032848
Iteration 30/1000 | Loss: 0.00025750
Iteration 31/1000 | Loss: 0.00084226
Iteration 32/1000 | Loss: 0.00013829
Iteration 33/1000 | Loss: 0.00080054
Iteration 34/1000 | Loss: 0.00049168
Iteration 35/1000 | Loss: 0.00079776
Iteration 36/1000 | Loss: 0.00044478
Iteration 37/1000 | Loss: 0.00076425
Iteration 38/1000 | Loss: 0.00037061
Iteration 39/1000 | Loss: 0.00034000
Iteration 40/1000 | Loss: 0.00012667
Iteration 41/1000 | Loss: 0.00027637
Iteration 42/1000 | Loss: 0.00031265
Iteration 43/1000 | Loss: 0.00010154
Iteration 44/1000 | Loss: 0.00011059
Iteration 45/1000 | Loss: 0.00010128
Iteration 46/1000 | Loss: 0.00034373
Iteration 47/1000 | Loss: 0.00074505
Iteration 48/1000 | Loss: 0.00018686
Iteration 49/1000 | Loss: 0.00010778
Iteration 50/1000 | Loss: 0.00009277
Iteration 51/1000 | Loss: 0.00009062
Iteration 52/1000 | Loss: 0.00008227
Iteration 53/1000 | Loss: 0.00064913
Iteration 54/1000 | Loss: 0.00010874
Iteration 55/1000 | Loss: 0.00009840
Iteration 56/1000 | Loss: 0.00007549
Iteration 57/1000 | Loss: 0.00007282
Iteration 58/1000 | Loss: 0.00007073
Iteration 59/1000 | Loss: 0.00006898
Iteration 60/1000 | Loss: 0.00010071
Iteration 61/1000 | Loss: 0.00007026
Iteration 62/1000 | Loss: 0.00006743
Iteration 63/1000 | Loss: 0.00083565
Iteration 64/1000 | Loss: 0.00069768
Iteration 65/1000 | Loss: 0.00062592
Iteration 66/1000 | Loss: 0.00022610
Iteration 67/1000 | Loss: 0.00024501
Iteration 68/1000 | Loss: 0.00024296
Iteration 69/1000 | Loss: 0.00015062
Iteration 70/1000 | Loss: 0.00017825
Iteration 71/1000 | Loss: 0.00026884
Iteration 72/1000 | Loss: 0.00009836
Iteration 73/1000 | Loss: 0.00007114
Iteration 74/1000 | Loss: 0.00007841
Iteration 75/1000 | Loss: 0.00020898
Iteration 76/1000 | Loss: 0.00006701
Iteration 77/1000 | Loss: 0.00006086
Iteration 78/1000 | Loss: 0.00005874
Iteration 79/1000 | Loss: 0.00005875
Iteration 80/1000 | Loss: 0.00025427
Iteration 81/1000 | Loss: 0.00005746
Iteration 82/1000 | Loss: 0.00047676
Iteration 83/1000 | Loss: 0.00005868
Iteration 84/1000 | Loss: 0.00005408
Iteration 85/1000 | Loss: 0.00005113
Iteration 86/1000 | Loss: 0.00004919
Iteration 87/1000 | Loss: 0.00004779
Iteration 88/1000 | Loss: 0.00004699
Iteration 89/1000 | Loss: 0.00004633
Iteration 90/1000 | Loss: 0.00004589
Iteration 91/1000 | Loss: 0.00004559
Iteration 92/1000 | Loss: 0.00030167
Iteration 93/1000 | Loss: 0.00005487
Iteration 94/1000 | Loss: 0.00004970
Iteration 95/1000 | Loss: 0.00004787
Iteration 96/1000 | Loss: 0.00004731
Iteration 97/1000 | Loss: 0.00004697
Iteration 98/1000 | Loss: 0.00004646
Iteration 99/1000 | Loss: 0.00004620
Iteration 100/1000 | Loss: 0.00004604
Iteration 101/1000 | Loss: 0.00030975
Iteration 102/1000 | Loss: 0.00021165
Iteration 103/1000 | Loss: 0.00030827
Iteration 104/1000 | Loss: 0.00009321
Iteration 105/1000 | Loss: 0.00006146
Iteration 106/1000 | Loss: 0.00012939
Iteration 107/1000 | Loss: 0.00005363
Iteration 108/1000 | Loss: 0.00005062
Iteration 109/1000 | Loss: 0.00023947
Iteration 110/1000 | Loss: 0.00016173
Iteration 111/1000 | Loss: 0.00023857
Iteration 112/1000 | Loss: 0.00005894
Iteration 113/1000 | Loss: 0.00004997
Iteration 114/1000 | Loss: 0.00004803
Iteration 115/1000 | Loss: 0.00007091
Iteration 116/1000 | Loss: 0.00005479
Iteration 117/1000 | Loss: 0.00004946
Iteration 118/1000 | Loss: 0.00006010
Iteration 119/1000 | Loss: 0.00007005
Iteration 120/1000 | Loss: 0.00005604
Iteration 121/1000 | Loss: 0.00006943
Iteration 122/1000 | Loss: 0.00004563
Iteration 123/1000 | Loss: 0.00004527
Iteration 124/1000 | Loss: 0.00019904
Iteration 125/1000 | Loss: 0.00024677
Iteration 126/1000 | Loss: 0.00013874
Iteration 127/1000 | Loss: 0.00005021
Iteration 128/1000 | Loss: 0.00004692
Iteration 129/1000 | Loss: 0.00004552
Iteration 130/1000 | Loss: 0.00024111
Iteration 131/1000 | Loss: 0.00044396
Iteration 132/1000 | Loss: 0.00012067
Iteration 133/1000 | Loss: 0.00005435
Iteration 134/1000 | Loss: 0.00004722
Iteration 135/1000 | Loss: 0.00023958
Iteration 136/1000 | Loss: 0.00024475
Iteration 137/1000 | Loss: 0.00012665
Iteration 138/1000 | Loss: 0.00004530
Iteration 139/1000 | Loss: 0.00004292
Iteration 140/1000 | Loss: 0.00004130
Iteration 141/1000 | Loss: 0.00003960
Iteration 142/1000 | Loss: 0.00003849
Iteration 143/1000 | Loss: 0.00003768
Iteration 144/1000 | Loss: 0.00003728
Iteration 145/1000 | Loss: 0.00003703
Iteration 146/1000 | Loss: 0.00003683
Iteration 147/1000 | Loss: 0.00003680
Iteration 148/1000 | Loss: 0.00003680
Iteration 149/1000 | Loss: 0.00003679
Iteration 150/1000 | Loss: 0.00003678
Iteration 151/1000 | Loss: 0.00029133
Iteration 152/1000 | Loss: 0.00021261
Iteration 153/1000 | Loss: 0.00004049
Iteration 154/1000 | Loss: 0.00003744
Iteration 155/1000 | Loss: 0.00003679
Iteration 156/1000 | Loss: 0.00003666
Iteration 157/1000 | Loss: 0.00003664
Iteration 158/1000 | Loss: 0.00003663
Iteration 159/1000 | Loss: 0.00003663
Iteration 160/1000 | Loss: 0.00028274
Iteration 161/1000 | Loss: 0.00037664
Iteration 162/1000 | Loss: 0.00037646
Iteration 163/1000 | Loss: 0.00005239
Iteration 164/1000 | Loss: 0.00004420
Iteration 165/1000 | Loss: 0.00004055
Iteration 166/1000 | Loss: 0.00003890
Iteration 167/1000 | Loss: 0.00003803
Iteration 168/1000 | Loss: 0.00003751
Iteration 169/1000 | Loss: 0.00003698
Iteration 170/1000 | Loss: 0.00003657
Iteration 171/1000 | Loss: 0.00003651
Iteration 172/1000 | Loss: 0.00003646
Iteration 173/1000 | Loss: 0.00003639
Iteration 174/1000 | Loss: 0.00003636
Iteration 175/1000 | Loss: 0.00003636
Iteration 176/1000 | Loss: 0.00003635
Iteration 177/1000 | Loss: 0.00003635
Iteration 178/1000 | Loss: 0.00003634
Iteration 179/1000 | Loss: 0.00003634
Iteration 180/1000 | Loss: 0.00003634
Iteration 181/1000 | Loss: 0.00003633
Iteration 182/1000 | Loss: 0.00003629
Iteration 183/1000 | Loss: 0.00003629
Iteration 184/1000 | Loss: 0.00003628
Iteration 185/1000 | Loss: 0.00003628
Iteration 186/1000 | Loss: 0.00003628
Iteration 187/1000 | Loss: 0.00003628
Iteration 188/1000 | Loss: 0.00003628
Iteration 189/1000 | Loss: 0.00003628
Iteration 190/1000 | Loss: 0.00003628
Iteration 191/1000 | Loss: 0.00003628
Iteration 192/1000 | Loss: 0.00003628
Iteration 193/1000 | Loss: 0.00003628
Iteration 194/1000 | Loss: 0.00003627
Iteration 195/1000 | Loss: 0.00003627
Iteration 196/1000 | Loss: 0.00003627
Iteration 197/1000 | Loss: 0.00003626
Iteration 198/1000 | Loss: 0.00003626
Iteration 199/1000 | Loss: 0.00003625
Iteration 200/1000 | Loss: 0.00003625
Iteration 201/1000 | Loss: 0.00003624
Iteration 202/1000 | Loss: 0.00003622
Iteration 203/1000 | Loss: 0.00003622
Iteration 204/1000 | Loss: 0.00003622
Iteration 205/1000 | Loss: 0.00003622
Iteration 206/1000 | Loss: 0.00003622
Iteration 207/1000 | Loss: 0.00003622
Iteration 208/1000 | Loss: 0.00003622
Iteration 209/1000 | Loss: 0.00003622
Iteration 210/1000 | Loss: 0.00003621
Iteration 211/1000 | Loss: 0.00003621
Iteration 212/1000 | Loss: 0.00003618
Iteration 213/1000 | Loss: 0.00003616
Iteration 214/1000 | Loss: 0.00003616
Iteration 215/1000 | Loss: 0.00003616
Iteration 216/1000 | Loss: 0.00003616
Iteration 217/1000 | Loss: 0.00003616
Iteration 218/1000 | Loss: 0.00003616
Iteration 219/1000 | Loss: 0.00003616
Iteration 220/1000 | Loss: 0.00003616
Iteration 221/1000 | Loss: 0.00003616
Iteration 222/1000 | Loss: 0.00003616
Iteration 223/1000 | Loss: 0.00003616
Iteration 224/1000 | Loss: 0.00003616
Iteration 225/1000 | Loss: 0.00003615
Iteration 226/1000 | Loss: 0.00003615
Iteration 227/1000 | Loss: 0.00003615
Iteration 228/1000 | Loss: 0.00003614
Iteration 229/1000 | Loss: 0.00003613
Iteration 230/1000 | Loss: 0.00003612
Iteration 231/1000 | Loss: 0.00003608
Iteration 232/1000 | Loss: 0.00003607
Iteration 233/1000 | Loss: 0.00003606
Iteration 234/1000 | Loss: 0.00003606
Iteration 235/1000 | Loss: 0.00003606
Iteration 236/1000 | Loss: 0.00003606
Iteration 237/1000 | Loss: 0.00003605
Iteration 238/1000 | Loss: 0.00003605
Iteration 239/1000 | Loss: 0.00003605
Iteration 240/1000 | Loss: 0.00003605
Iteration 241/1000 | Loss: 0.00003605
Iteration 242/1000 | Loss: 0.00003605
Iteration 243/1000 | Loss: 0.00003605
Iteration 244/1000 | Loss: 0.00003605
Iteration 245/1000 | Loss: 0.00003605
Iteration 246/1000 | Loss: 0.00003605
Iteration 247/1000 | Loss: 0.00003604
Iteration 248/1000 | Loss: 0.00003604
Iteration 249/1000 | Loss: 0.00003604
Iteration 250/1000 | Loss: 0.00003604
Iteration 251/1000 | Loss: 0.00003604
Iteration 252/1000 | Loss: 0.00003604
Iteration 253/1000 | Loss: 0.00003604
Iteration 254/1000 | Loss: 0.00003604
Iteration 255/1000 | Loss: 0.00003603
Iteration 256/1000 | Loss: 0.00003603
Iteration 257/1000 | Loss: 0.00003603
Iteration 258/1000 | Loss: 0.00003603
Iteration 259/1000 | Loss: 0.00003602
Iteration 260/1000 | Loss: 0.00003602
Iteration 261/1000 | Loss: 0.00003602
Iteration 262/1000 | Loss: 0.00003602
Iteration 263/1000 | Loss: 0.00003602
Iteration 264/1000 | Loss: 0.00003602
Iteration 265/1000 | Loss: 0.00003602
Iteration 266/1000 | Loss: 0.00003602
Iteration 267/1000 | Loss: 0.00003602
Iteration 268/1000 | Loss: 0.00003602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [3.60218073183205e-05, 3.60218073183205e-05, 3.60218073183205e-05, 3.60218073183205e-05, 3.60218073183205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.60218073183205e-05

Optimization complete. Final v2v error: 4.580700397491455 mm

Highest mean error: 6.550252437591553 mm for frame 60

Lowest mean error: 4.133265495300293 mm for frame 163

Saving results

Total time: 284.8275954723358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876187
Iteration 2/25 | Loss: 0.00162233
Iteration 3/25 | Loss: 0.00127099
Iteration 4/25 | Loss: 0.00123563
Iteration 5/25 | Loss: 0.00122339
Iteration 6/25 | Loss: 0.00121360
Iteration 7/25 | Loss: 0.00121084
Iteration 8/25 | Loss: 0.00121071
Iteration 9/25 | Loss: 0.00120713
Iteration 10/25 | Loss: 0.00120833
Iteration 11/25 | Loss: 0.00120565
Iteration 12/25 | Loss: 0.00120512
Iteration 13/25 | Loss: 0.00120475
Iteration 14/25 | Loss: 0.00120464
Iteration 15/25 | Loss: 0.00120464
Iteration 16/25 | Loss: 0.00120464
Iteration 17/25 | Loss: 0.00120464
Iteration 18/25 | Loss: 0.00120463
Iteration 19/25 | Loss: 0.00120463
Iteration 20/25 | Loss: 0.00120463
Iteration 21/25 | Loss: 0.00120463
Iteration 22/25 | Loss: 0.00120463
Iteration 23/25 | Loss: 0.00120463
Iteration 24/25 | Loss: 0.00120463
Iteration 25/25 | Loss: 0.00120463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13549161
Iteration 2/25 | Loss: 0.00078328
Iteration 3/25 | Loss: 0.00078328
Iteration 4/25 | Loss: 0.00076450
Iteration 5/25 | Loss: 0.00076450
Iteration 6/25 | Loss: 0.00076450
Iteration 7/25 | Loss: 0.00076450
Iteration 8/25 | Loss: 0.00076450
Iteration 9/25 | Loss: 0.00076450
Iteration 10/25 | Loss: 0.00076450
Iteration 11/25 | Loss: 0.00076450
Iteration 12/25 | Loss: 0.00076450
Iteration 13/25 | Loss: 0.00076450
Iteration 14/25 | Loss: 0.00076450
Iteration 15/25 | Loss: 0.00076450
Iteration 16/25 | Loss: 0.00076450
Iteration 17/25 | Loss: 0.00076450
Iteration 18/25 | Loss: 0.00076450
Iteration 19/25 | Loss: 0.00076450
Iteration 20/25 | Loss: 0.00076450
Iteration 21/25 | Loss: 0.00076450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007644956931471825, 0.0007644956931471825, 0.0007644956931471825, 0.0007644956931471825, 0.0007644956931471825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007644956931471825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076450
Iteration 2/1000 | Loss: 0.00005340
Iteration 3/1000 | Loss: 0.00005087
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00004787
Iteration 6/1000 | Loss: 0.00009856
Iteration 7/1000 | Loss: 0.00022012
Iteration 8/1000 | Loss: 0.00001539
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00004089
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00005527
Iteration 16/1000 | Loss: 0.00005527
Iteration 17/1000 | Loss: 0.00009522
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00003474
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00002476
Iteration 32/1000 | Loss: 0.00002404
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00001320
Iteration 40/1000 | Loss: 0.00001320
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001319
Iteration 44/1000 | Loss: 0.00001319
Iteration 45/1000 | Loss: 0.00001319
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001318
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001308
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001307
Iteration 67/1000 | Loss: 0.00001307
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001306
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001306
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00005644
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001296
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001295
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001295
Iteration 128/1000 | Loss: 0.00001295
Iteration 129/1000 | Loss: 0.00001295
Iteration 130/1000 | Loss: 0.00001295
Iteration 131/1000 | Loss: 0.00001295
Iteration 132/1000 | Loss: 0.00001295
Iteration 133/1000 | Loss: 0.00001295
Iteration 134/1000 | Loss: 0.00001295
Iteration 135/1000 | Loss: 0.00001295
Iteration 136/1000 | Loss: 0.00001295
Iteration 137/1000 | Loss: 0.00001295
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001294
Iteration 141/1000 | Loss: 0.00001294
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001293
Iteration 146/1000 | Loss: 0.00004411
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001290
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001290
Iteration 158/1000 | Loss: 0.00001290
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001289
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001289
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001289
Iteration 175/1000 | Loss: 0.00001289
Iteration 176/1000 | Loss: 0.00001289
Iteration 177/1000 | Loss: 0.00001289
Iteration 178/1000 | Loss: 0.00001289
Iteration 179/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.2890599464299157e-05, 1.2890599464299157e-05, 1.2890599464299157e-05, 1.2890599464299157e-05, 1.2890599464299157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2890599464299157e-05

Optimization complete. Final v2v error: 3.061652183532715 mm

Highest mean error: 3.4118971824645996 mm for frame 38

Lowest mean error: 2.8309006690979004 mm for frame 224

Saving results

Total time: 79.10635328292847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773133
Iteration 2/25 | Loss: 0.00139135
Iteration 3/25 | Loss: 0.00123626
Iteration 4/25 | Loss: 0.00122058
Iteration 5/25 | Loss: 0.00121806
Iteration 6/25 | Loss: 0.00121806
Iteration 7/25 | Loss: 0.00121806
Iteration 8/25 | Loss: 0.00121806
Iteration 9/25 | Loss: 0.00121806
Iteration 10/25 | Loss: 0.00121806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012180627090856433, 0.0012180627090856433, 0.0012180627090856433, 0.0012180627090856433, 0.0012180627090856433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012180627090856433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43836141
Iteration 2/25 | Loss: 0.00072766
Iteration 3/25 | Loss: 0.00072766
Iteration 4/25 | Loss: 0.00072766
Iteration 5/25 | Loss: 0.00072765
Iteration 6/25 | Loss: 0.00072765
Iteration 7/25 | Loss: 0.00072765
Iteration 8/25 | Loss: 0.00072765
Iteration 9/25 | Loss: 0.00072765
Iteration 10/25 | Loss: 0.00072765
Iteration 11/25 | Loss: 0.00072765
Iteration 12/25 | Loss: 0.00072765
Iteration 13/25 | Loss: 0.00072765
Iteration 14/25 | Loss: 0.00072765
Iteration 15/25 | Loss: 0.00072765
Iteration 16/25 | Loss: 0.00072765
Iteration 17/25 | Loss: 0.00072765
Iteration 18/25 | Loss: 0.00072765
Iteration 19/25 | Loss: 0.00072765
Iteration 20/25 | Loss: 0.00072765
Iteration 21/25 | Loss: 0.00072765
Iteration 22/25 | Loss: 0.00072765
Iteration 23/25 | Loss: 0.00072765
Iteration 24/25 | Loss: 0.00072765
Iteration 25/25 | Loss: 0.00072765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072765
Iteration 2/1000 | Loss: 0.00002997
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001248
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001208
Iteration 37/1000 | Loss: 0.00001208
Iteration 38/1000 | Loss: 0.00001208
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001206
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001205
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001197
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Iteration 101/1000 | Loss: 0.00001188
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001183
Iteration 121/1000 | Loss: 0.00001183
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001183
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001182
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001181
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001180
Iteration 137/1000 | Loss: 0.00001180
Iteration 138/1000 | Loss: 0.00001180
Iteration 139/1000 | Loss: 0.00001179
Iteration 140/1000 | Loss: 0.00001179
Iteration 141/1000 | Loss: 0.00001179
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001178
Iteration 144/1000 | Loss: 0.00001178
Iteration 145/1000 | Loss: 0.00001178
Iteration 146/1000 | Loss: 0.00001178
Iteration 147/1000 | Loss: 0.00001178
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001177
Iteration 152/1000 | Loss: 0.00001177
Iteration 153/1000 | Loss: 0.00001177
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001175
Iteration 166/1000 | Loss: 0.00001175
Iteration 167/1000 | Loss: 0.00001175
Iteration 168/1000 | Loss: 0.00001175
Iteration 169/1000 | Loss: 0.00001175
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001174
Iteration 172/1000 | Loss: 0.00001174
Iteration 173/1000 | Loss: 0.00001174
Iteration 174/1000 | Loss: 0.00001173
Iteration 175/1000 | Loss: 0.00001173
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001172
Iteration 180/1000 | Loss: 0.00001172
Iteration 181/1000 | Loss: 0.00001172
Iteration 182/1000 | Loss: 0.00001172
Iteration 183/1000 | Loss: 0.00001172
Iteration 184/1000 | Loss: 0.00001172
Iteration 185/1000 | Loss: 0.00001171
Iteration 186/1000 | Loss: 0.00001171
Iteration 187/1000 | Loss: 0.00001171
Iteration 188/1000 | Loss: 0.00001171
Iteration 189/1000 | Loss: 0.00001171
Iteration 190/1000 | Loss: 0.00001171
Iteration 191/1000 | Loss: 0.00001171
Iteration 192/1000 | Loss: 0.00001171
Iteration 193/1000 | Loss: 0.00001171
Iteration 194/1000 | Loss: 0.00001171
Iteration 195/1000 | Loss: 0.00001171
Iteration 196/1000 | Loss: 0.00001171
Iteration 197/1000 | Loss: 0.00001171
Iteration 198/1000 | Loss: 0.00001171
Iteration 199/1000 | Loss: 0.00001171
Iteration 200/1000 | Loss: 0.00001171
Iteration 201/1000 | Loss: 0.00001171
Iteration 202/1000 | Loss: 0.00001171
Iteration 203/1000 | Loss: 0.00001171
Iteration 204/1000 | Loss: 0.00001171
Iteration 205/1000 | Loss: 0.00001171
Iteration 206/1000 | Loss: 0.00001171
Iteration 207/1000 | Loss: 0.00001171
Iteration 208/1000 | Loss: 0.00001171
Iteration 209/1000 | Loss: 0.00001171
Iteration 210/1000 | Loss: 0.00001171
Iteration 211/1000 | Loss: 0.00001171
Iteration 212/1000 | Loss: 0.00001171
Iteration 213/1000 | Loss: 0.00001171
Iteration 214/1000 | Loss: 0.00001171
Iteration 215/1000 | Loss: 0.00001171
Iteration 216/1000 | Loss: 0.00001171
Iteration 217/1000 | Loss: 0.00001171
Iteration 218/1000 | Loss: 0.00001171
Iteration 219/1000 | Loss: 0.00001171
Iteration 220/1000 | Loss: 0.00001171
Iteration 221/1000 | Loss: 0.00001171
Iteration 222/1000 | Loss: 0.00001171
Iteration 223/1000 | Loss: 0.00001171
Iteration 224/1000 | Loss: 0.00001171
Iteration 225/1000 | Loss: 0.00001171
Iteration 226/1000 | Loss: 0.00001171
Iteration 227/1000 | Loss: 0.00001171
Iteration 228/1000 | Loss: 0.00001171
Iteration 229/1000 | Loss: 0.00001171
Iteration 230/1000 | Loss: 0.00001171
Iteration 231/1000 | Loss: 0.00001171
Iteration 232/1000 | Loss: 0.00001171
Iteration 233/1000 | Loss: 0.00001171
Iteration 234/1000 | Loss: 0.00001171
Iteration 235/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.1709741556842346e-05, 1.1709741556842346e-05, 1.1709741556842346e-05, 1.1709741556842346e-05, 1.1709741556842346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1709741556842346e-05

Optimization complete. Final v2v error: 2.92221999168396 mm

Highest mean error: 3.310551404953003 mm for frame 124

Lowest mean error: 2.7497763633728027 mm for frame 174

Saving results

Total time: 45.259103536605835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998498
Iteration 2/25 | Loss: 0.00179361
Iteration 3/25 | Loss: 0.00142427
Iteration 4/25 | Loss: 0.00133824
Iteration 5/25 | Loss: 0.00131874
Iteration 6/25 | Loss: 0.00132070
Iteration 7/25 | Loss: 0.00130210
Iteration 8/25 | Loss: 0.00129729
Iteration 9/25 | Loss: 0.00129627
Iteration 10/25 | Loss: 0.00129615
Iteration 11/25 | Loss: 0.00129615
Iteration 12/25 | Loss: 0.00129615
Iteration 13/25 | Loss: 0.00129615
Iteration 14/25 | Loss: 0.00129615
Iteration 15/25 | Loss: 0.00129615
Iteration 16/25 | Loss: 0.00129614
Iteration 17/25 | Loss: 0.00129614
Iteration 18/25 | Loss: 0.00129614
Iteration 19/25 | Loss: 0.00129614
Iteration 20/25 | Loss: 0.00129614
Iteration 21/25 | Loss: 0.00129614
Iteration 22/25 | Loss: 0.00129614
Iteration 23/25 | Loss: 0.00129614
Iteration 24/25 | Loss: 0.00129614
Iteration 25/25 | Loss: 0.00129614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012961415341123939, 0.0012961415341123939, 0.0012961415341123939, 0.0012961415341123939, 0.0012961415341123939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012961415341123939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44526100
Iteration 2/25 | Loss: 0.00079011
Iteration 3/25 | Loss: 0.00079011
Iteration 4/25 | Loss: 0.00079011
Iteration 5/25 | Loss: 0.00079011
Iteration 6/25 | Loss: 0.00079011
Iteration 7/25 | Loss: 0.00079011
Iteration 8/25 | Loss: 0.00079011
Iteration 9/25 | Loss: 0.00079011
Iteration 10/25 | Loss: 0.00079011
Iteration 11/25 | Loss: 0.00079011
Iteration 12/25 | Loss: 0.00079011
Iteration 13/25 | Loss: 0.00079011
Iteration 14/25 | Loss: 0.00079011
Iteration 15/25 | Loss: 0.00079011
Iteration 16/25 | Loss: 0.00079011
Iteration 17/25 | Loss: 0.00079011
Iteration 18/25 | Loss: 0.00079011
Iteration 19/25 | Loss: 0.00079011
Iteration 20/25 | Loss: 0.00079011
Iteration 21/25 | Loss: 0.00079011
Iteration 22/25 | Loss: 0.00079011
Iteration 23/25 | Loss: 0.00079011
Iteration 24/25 | Loss: 0.00079011
Iteration 25/25 | Loss: 0.00079011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079011
Iteration 2/1000 | Loss: 0.00003460
Iteration 3/1000 | Loss: 0.00002348
Iteration 4/1000 | Loss: 0.00002157
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00002048
Iteration 7/1000 | Loss: 0.00002001
Iteration 8/1000 | Loss: 0.00001962
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001891
Iteration 14/1000 | Loss: 0.00001886
Iteration 15/1000 | Loss: 0.00001879
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00001866
Iteration 22/1000 | Loss: 0.00001859
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001858
Iteration 25/1000 | Loss: 0.00001858
Iteration 26/1000 | Loss: 0.00001858
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001857
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001855
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001854
Iteration 38/1000 | Loss: 0.00001854
Iteration 39/1000 | Loss: 0.00001854
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001854
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00001854
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001851
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001850
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001850
Iteration 54/1000 | Loss: 0.00001850
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001849
Iteration 61/1000 | Loss: 0.00001849
Iteration 62/1000 | Loss: 0.00001849
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001847
Iteration 67/1000 | Loss: 0.00001846
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001845
Iteration 70/1000 | Loss: 0.00001845
Iteration 71/1000 | Loss: 0.00001845
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001844
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001837
Iteration 133/1000 | Loss: 0.00001837
Iteration 134/1000 | Loss: 0.00001837
Iteration 135/1000 | Loss: 0.00001837
Iteration 136/1000 | Loss: 0.00001837
Iteration 137/1000 | Loss: 0.00001837
Iteration 138/1000 | Loss: 0.00001837
Iteration 139/1000 | Loss: 0.00001837
Iteration 140/1000 | Loss: 0.00001837
Iteration 141/1000 | Loss: 0.00001837
Iteration 142/1000 | Loss: 0.00001837
Iteration 143/1000 | Loss: 0.00001837
Iteration 144/1000 | Loss: 0.00001837
Iteration 145/1000 | Loss: 0.00001837
Iteration 146/1000 | Loss: 0.00001837
Iteration 147/1000 | Loss: 0.00001837
Iteration 148/1000 | Loss: 0.00001837
Iteration 149/1000 | Loss: 0.00001837
Iteration 150/1000 | Loss: 0.00001837
Iteration 151/1000 | Loss: 0.00001837
Iteration 152/1000 | Loss: 0.00001837
Iteration 153/1000 | Loss: 0.00001837
Iteration 154/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.837007948779501e-05, 1.837007948779501e-05, 1.837007948779501e-05, 1.837007948779501e-05, 1.837007948779501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.837007948779501e-05

Optimization complete. Final v2v error: 3.6940524578094482 mm

Highest mean error: 3.769340991973877 mm for frame 56

Lowest mean error: 3.6077208518981934 mm for frame 0

Saving results

Total time: 43.44617438316345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008772
Iteration 2/25 | Loss: 0.00175929
Iteration 3/25 | Loss: 0.00142870
Iteration 4/25 | Loss: 0.00135654
Iteration 5/25 | Loss: 0.00135612
Iteration 6/25 | Loss: 0.00133515
Iteration 7/25 | Loss: 0.00134966
Iteration 8/25 | Loss: 0.00131261
Iteration 9/25 | Loss: 0.00130015
Iteration 10/25 | Loss: 0.00129481
Iteration 11/25 | Loss: 0.00128802
Iteration 12/25 | Loss: 0.00128655
Iteration 13/25 | Loss: 0.00128965
Iteration 14/25 | Loss: 0.00129450
Iteration 15/25 | Loss: 0.00128376
Iteration 16/25 | Loss: 0.00129175
Iteration 17/25 | Loss: 0.00127086
Iteration 18/25 | Loss: 0.00127787
Iteration 19/25 | Loss: 0.00126373
Iteration 20/25 | Loss: 0.00126588
Iteration 21/25 | Loss: 0.00126126
Iteration 22/25 | Loss: 0.00126128
Iteration 23/25 | Loss: 0.00125940
Iteration 24/25 | Loss: 0.00125830
Iteration 25/25 | Loss: 0.00125654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.03664207
Iteration 2/25 | Loss: 0.00103813
Iteration 3/25 | Loss: 0.00098213
Iteration 4/25 | Loss: 0.00098213
Iteration 5/25 | Loss: 0.00098213
Iteration 6/25 | Loss: 0.00098213
Iteration 7/25 | Loss: 0.00098213
Iteration 8/25 | Loss: 0.00098213
Iteration 9/25 | Loss: 0.00098213
Iteration 10/25 | Loss: 0.00098213
Iteration 11/25 | Loss: 0.00098213
Iteration 12/25 | Loss: 0.00098213
Iteration 13/25 | Loss: 0.00098213
Iteration 14/25 | Loss: 0.00098213
Iteration 15/25 | Loss: 0.00098213
Iteration 16/25 | Loss: 0.00098213
Iteration 17/25 | Loss: 0.00098213
Iteration 18/25 | Loss: 0.00098213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009821263374760747, 0.0009821263374760747, 0.0009821263374760747, 0.0009821263374760747, 0.0009821263374760747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009821263374760747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098213
Iteration 2/1000 | Loss: 0.00096636
Iteration 3/1000 | Loss: 0.00006282
Iteration 4/1000 | Loss: 0.00002749
Iteration 5/1000 | Loss: 0.00003731
Iteration 6/1000 | Loss: 0.00012286
Iteration 7/1000 | Loss: 0.00005922
Iteration 8/1000 | Loss: 0.00003147
Iteration 9/1000 | Loss: 0.00003981
Iteration 10/1000 | Loss: 0.00002550
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00003058
Iteration 13/1000 | Loss: 0.00002219
Iteration 14/1000 | Loss: 0.00003201
Iteration 15/1000 | Loss: 0.00005802
Iteration 16/1000 | Loss: 0.00003004
Iteration 17/1000 | Loss: 0.00003255
Iteration 18/1000 | Loss: 0.00002938
Iteration 19/1000 | Loss: 0.00003544
Iteration 20/1000 | Loss: 0.00002905
Iteration 21/1000 | Loss: 0.00003304
Iteration 22/1000 | Loss: 0.00003083
Iteration 23/1000 | Loss: 0.00002940
Iteration 24/1000 | Loss: 0.00002845
Iteration 25/1000 | Loss: 0.00014417
Iteration 26/1000 | Loss: 0.00002525
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00006617
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00009128
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00005237
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001489
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001476
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001457
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001452
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001450
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001447
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.4466157153947279e-05, 1.4466157153947279e-05, 1.4466157153947279e-05, 1.4466157153947279e-05, 1.4466157153947279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4466157153947279e-05

Optimization complete. Final v2v error: 3.1731767654418945 mm

Highest mean error: 6.0855278968811035 mm for frame 142

Lowest mean error: 2.869814395904541 mm for frame 73

Saving results

Total time: 117.6207857131958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983287
Iteration 2/25 | Loss: 0.00983287
Iteration 3/25 | Loss: 0.00983287
Iteration 4/25 | Loss: 0.00329391
Iteration 5/25 | Loss: 0.00206556
Iteration 6/25 | Loss: 0.00169929
Iteration 7/25 | Loss: 0.00161681
Iteration 8/25 | Loss: 0.00159427
Iteration 9/25 | Loss: 0.00157690
Iteration 10/25 | Loss: 0.00158780
Iteration 11/25 | Loss: 0.00151487
Iteration 12/25 | Loss: 0.00149461
Iteration 13/25 | Loss: 0.00148311
Iteration 14/25 | Loss: 0.00147970
Iteration 15/25 | Loss: 0.00148174
Iteration 16/25 | Loss: 0.00148191
Iteration 17/25 | Loss: 0.00146835
Iteration 18/25 | Loss: 0.00146662
Iteration 19/25 | Loss: 0.00146656
Iteration 20/25 | Loss: 0.00145055
Iteration 21/25 | Loss: 0.00144324
Iteration 22/25 | Loss: 0.00144325
Iteration 23/25 | Loss: 0.00144376
Iteration 24/25 | Loss: 0.00144759
Iteration 25/25 | Loss: 0.00144358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42082000
Iteration 2/25 | Loss: 0.00239492
Iteration 3/25 | Loss: 0.00194539
Iteration 4/25 | Loss: 0.00194539
Iteration 5/25 | Loss: 0.00194539
Iteration 6/25 | Loss: 0.00194539
Iteration 7/25 | Loss: 0.00194539
Iteration 8/25 | Loss: 0.00194539
Iteration 9/25 | Loss: 0.00194539
Iteration 10/25 | Loss: 0.00194539
Iteration 11/25 | Loss: 0.00194539
Iteration 12/25 | Loss: 0.00194539
Iteration 13/25 | Loss: 0.00194539
Iteration 14/25 | Loss: 0.00194539
Iteration 15/25 | Loss: 0.00194539
Iteration 16/25 | Loss: 0.00194539
Iteration 17/25 | Loss: 0.00194539
Iteration 18/25 | Loss: 0.00194539
Iteration 19/25 | Loss: 0.00194539
Iteration 20/25 | Loss: 0.00194539
Iteration 21/25 | Loss: 0.00194539
Iteration 22/25 | Loss: 0.00194539
Iteration 23/25 | Loss: 0.00194539
Iteration 24/25 | Loss: 0.00194539
Iteration 25/25 | Loss: 0.00194539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194539
Iteration 2/1000 | Loss: 0.00055217
Iteration 3/1000 | Loss: 0.00080201
Iteration 4/1000 | Loss: 0.00091669
Iteration 5/1000 | Loss: 0.00020571
Iteration 6/1000 | Loss: 0.00015686
Iteration 7/1000 | Loss: 0.00025535
Iteration 8/1000 | Loss: 0.00020247
Iteration 9/1000 | Loss: 0.00027588
Iteration 10/1000 | Loss: 0.00014529
Iteration 11/1000 | Loss: 0.00047342
Iteration 12/1000 | Loss: 0.00062710
Iteration 13/1000 | Loss: 0.00009135
Iteration 14/1000 | Loss: 0.00010806
Iteration 15/1000 | Loss: 0.00012885
Iteration 16/1000 | Loss: 0.00020421
Iteration 17/1000 | Loss: 0.00018144
Iteration 18/1000 | Loss: 0.00014621
Iteration 19/1000 | Loss: 0.00034130
Iteration 20/1000 | Loss: 0.00032272
Iteration 21/1000 | Loss: 0.00008745
Iteration 22/1000 | Loss: 0.00008623
Iteration 23/1000 | Loss: 0.00010678
Iteration 24/1000 | Loss: 0.00008569
Iteration 25/1000 | Loss: 0.00231760
Iteration 26/1000 | Loss: 0.00272211
Iteration 27/1000 | Loss: 0.00771503
Iteration 28/1000 | Loss: 0.00198433
Iteration 29/1000 | Loss: 0.00099919
Iteration 30/1000 | Loss: 0.00121012
Iteration 31/1000 | Loss: 0.00036747
Iteration 32/1000 | Loss: 0.00015162
Iteration 33/1000 | Loss: 0.00021843
Iteration 34/1000 | Loss: 0.00009242
Iteration 35/1000 | Loss: 0.00016886
Iteration 36/1000 | Loss: 0.00014446
Iteration 37/1000 | Loss: 0.00022438
Iteration 38/1000 | Loss: 0.00019073
Iteration 39/1000 | Loss: 0.00006059
Iteration 40/1000 | Loss: 0.00004867
Iteration 41/1000 | Loss: 0.00014790
Iteration 42/1000 | Loss: 0.00003978
Iteration 43/1000 | Loss: 0.00008528
Iteration 44/1000 | Loss: 0.00003185
Iteration 45/1000 | Loss: 0.00010470
Iteration 46/1000 | Loss: 0.00006152
Iteration 47/1000 | Loss: 0.00010110
Iteration 48/1000 | Loss: 0.00002753
Iteration 49/1000 | Loss: 0.00006058
Iteration 50/1000 | Loss: 0.00003052
Iteration 51/1000 | Loss: 0.00006344
Iteration 52/1000 | Loss: 0.00003202
Iteration 53/1000 | Loss: 0.00004130
Iteration 54/1000 | Loss: 0.00002416
Iteration 55/1000 | Loss: 0.00002690
Iteration 56/1000 | Loss: 0.00004417
Iteration 57/1000 | Loss: 0.00002064
Iteration 58/1000 | Loss: 0.00004382
Iteration 59/1000 | Loss: 0.00006105
Iteration 60/1000 | Loss: 0.00002507
Iteration 61/1000 | Loss: 0.00002283
Iteration 62/1000 | Loss: 0.00002355
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00003145
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002921
Iteration 67/1000 | Loss: 0.00003131
Iteration 68/1000 | Loss: 0.00002702
Iteration 69/1000 | Loss: 0.00002854
Iteration 70/1000 | Loss: 0.00002168
Iteration 71/1000 | Loss: 0.00002381
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00002191
Iteration 74/1000 | Loss: 0.00002004
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001984
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001984
Iteration 81/1000 | Loss: 0.00001984
Iteration 82/1000 | Loss: 0.00001984
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001984
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001984
Iteration 88/1000 | Loss: 0.00001984
Iteration 89/1000 | Loss: 0.00001984
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001984
Iteration 109/1000 | Loss: 0.00001984
Iteration 110/1000 | Loss: 0.00001984
Iteration 111/1000 | Loss: 0.00001984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.9839771994156763e-05, 1.9839771994156763e-05, 1.9839771994156763e-05, 1.9839771994156763e-05, 1.9839771994156763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9839771994156763e-05

Optimization complete. Final v2v error: 3.5485475063323975 mm

Highest mean error: 5.778955459594727 mm for frame 225

Lowest mean error: 3.025456190109253 mm for frame 135

Saving results

Total time: 160.30907559394836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828918
Iteration 2/25 | Loss: 0.00126187
Iteration 3/25 | Loss: 0.00119877
Iteration 4/25 | Loss: 0.00119038
Iteration 5/25 | Loss: 0.00118734
Iteration 6/25 | Loss: 0.00118695
Iteration 7/25 | Loss: 0.00118695
Iteration 8/25 | Loss: 0.00118695
Iteration 9/25 | Loss: 0.00118695
Iteration 10/25 | Loss: 0.00118695
Iteration 11/25 | Loss: 0.00118695
Iteration 12/25 | Loss: 0.00118695
Iteration 13/25 | Loss: 0.00118695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001186948036774993, 0.001186948036774993, 0.001186948036774993, 0.001186948036774993, 0.001186948036774993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001186948036774993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52109122
Iteration 2/25 | Loss: 0.00077970
Iteration 3/25 | Loss: 0.00077969
Iteration 4/25 | Loss: 0.00077969
Iteration 5/25 | Loss: 0.00077969
Iteration 6/25 | Loss: 0.00077969
Iteration 7/25 | Loss: 0.00077969
Iteration 8/25 | Loss: 0.00077969
Iteration 9/25 | Loss: 0.00077969
Iteration 10/25 | Loss: 0.00077969
Iteration 11/25 | Loss: 0.00077969
Iteration 12/25 | Loss: 0.00077969
Iteration 13/25 | Loss: 0.00077969
Iteration 14/25 | Loss: 0.00077969
Iteration 15/25 | Loss: 0.00077969
Iteration 16/25 | Loss: 0.00077969
Iteration 17/25 | Loss: 0.00077969
Iteration 18/25 | Loss: 0.00077969
Iteration 19/25 | Loss: 0.00077969
Iteration 20/25 | Loss: 0.00077969
Iteration 21/25 | Loss: 0.00077969
Iteration 22/25 | Loss: 0.00077969
Iteration 23/25 | Loss: 0.00077969
Iteration 24/25 | Loss: 0.00077969
Iteration 25/25 | Loss: 0.00077969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077969
Iteration 2/1000 | Loss: 0.00002302
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001166
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001111
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001098
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001089
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001089
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001085
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001081
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001079
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001078
Iteration 89/1000 | Loss: 0.00001078
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001073
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001072
Iteration 126/1000 | Loss: 0.00001072
Iteration 127/1000 | Loss: 0.00001072
Iteration 128/1000 | Loss: 0.00001072
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001072
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001071
Iteration 142/1000 | Loss: 0.00001071
Iteration 143/1000 | Loss: 0.00001071
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001071
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001071
Iteration 151/1000 | Loss: 0.00001071
Iteration 152/1000 | Loss: 0.00001071
Iteration 153/1000 | Loss: 0.00001071
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001071
Iteration 156/1000 | Loss: 0.00001071
Iteration 157/1000 | Loss: 0.00001071
Iteration 158/1000 | Loss: 0.00001071
Iteration 159/1000 | Loss: 0.00001071
Iteration 160/1000 | Loss: 0.00001071
Iteration 161/1000 | Loss: 0.00001071
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001071
Iteration 168/1000 | Loss: 0.00001071
Iteration 169/1000 | Loss: 0.00001071
Iteration 170/1000 | Loss: 0.00001071
Iteration 171/1000 | Loss: 0.00001071
Iteration 172/1000 | Loss: 0.00001071
Iteration 173/1000 | Loss: 0.00001071
Iteration 174/1000 | Loss: 0.00001071
Iteration 175/1000 | Loss: 0.00001071
Iteration 176/1000 | Loss: 0.00001071
Iteration 177/1000 | Loss: 0.00001071
Iteration 178/1000 | Loss: 0.00001071
Iteration 179/1000 | Loss: 0.00001071
Iteration 180/1000 | Loss: 0.00001071
Iteration 181/1000 | Loss: 0.00001071
Iteration 182/1000 | Loss: 0.00001071
Iteration 183/1000 | Loss: 0.00001071
Iteration 184/1000 | Loss: 0.00001071
Iteration 185/1000 | Loss: 0.00001071
Iteration 186/1000 | Loss: 0.00001071
Iteration 187/1000 | Loss: 0.00001071
Iteration 188/1000 | Loss: 0.00001071
Iteration 189/1000 | Loss: 0.00001071
Iteration 190/1000 | Loss: 0.00001071
Iteration 191/1000 | Loss: 0.00001071
Iteration 192/1000 | Loss: 0.00001071
Iteration 193/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.0713260962802451e-05, 1.0713260962802451e-05, 1.0713260962802451e-05, 1.0713260962802451e-05, 1.0713260962802451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0713260962802451e-05

Optimization complete. Final v2v error: 2.8004961013793945 mm

Highest mean error: 3.3047895431518555 mm for frame 89

Lowest mean error: 2.6293702125549316 mm for frame 13

Saving results

Total time: 36.84664344787598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059322
Iteration 2/25 | Loss: 0.00170983
Iteration 3/25 | Loss: 0.00145482
Iteration 4/25 | Loss: 0.00137414
Iteration 5/25 | Loss: 0.00135888
Iteration 6/25 | Loss: 0.00134345
Iteration 7/25 | Loss: 0.00133145
Iteration 8/25 | Loss: 0.00132576
Iteration 9/25 | Loss: 0.00132396
Iteration 10/25 | Loss: 0.00132337
Iteration 11/25 | Loss: 0.00132330
Iteration 12/25 | Loss: 0.00132329
Iteration 13/25 | Loss: 0.00132329
Iteration 14/25 | Loss: 0.00132329
Iteration 15/25 | Loss: 0.00132329
Iteration 16/25 | Loss: 0.00132329
Iteration 17/25 | Loss: 0.00132329
Iteration 18/25 | Loss: 0.00132328
Iteration 19/25 | Loss: 0.00132328
Iteration 20/25 | Loss: 0.00132328
Iteration 21/25 | Loss: 0.00132328
Iteration 22/25 | Loss: 0.00132328
Iteration 23/25 | Loss: 0.00132328
Iteration 24/25 | Loss: 0.00132328
Iteration 25/25 | Loss: 0.00132328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.12755871
Iteration 2/25 | Loss: 0.00096021
Iteration 3/25 | Loss: 0.00095999
Iteration 4/25 | Loss: 0.00095999
Iteration 5/25 | Loss: 0.00095998
Iteration 6/25 | Loss: 0.00095998
Iteration 7/25 | Loss: 0.00095998
Iteration 8/25 | Loss: 0.00095998
Iteration 9/25 | Loss: 0.00095998
Iteration 10/25 | Loss: 0.00095998
Iteration 11/25 | Loss: 0.00095998
Iteration 12/25 | Loss: 0.00095998
Iteration 13/25 | Loss: 0.00095998
Iteration 14/25 | Loss: 0.00095998
Iteration 15/25 | Loss: 0.00095998
Iteration 16/25 | Loss: 0.00095998
Iteration 17/25 | Loss: 0.00095998
Iteration 18/25 | Loss: 0.00095998
Iteration 19/25 | Loss: 0.00095998
Iteration 20/25 | Loss: 0.00095998
Iteration 21/25 | Loss: 0.00095998
Iteration 22/25 | Loss: 0.00095998
Iteration 23/25 | Loss: 0.00095998
Iteration 24/25 | Loss: 0.00095998
Iteration 25/25 | Loss: 0.00095998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095998
Iteration 2/1000 | Loss: 0.00008373
Iteration 3/1000 | Loss: 0.00012326
Iteration 4/1000 | Loss: 0.00005101
Iteration 5/1000 | Loss: 0.00004771
Iteration 6/1000 | Loss: 0.00004545
Iteration 7/1000 | Loss: 0.00004383
Iteration 8/1000 | Loss: 0.00004278
Iteration 9/1000 | Loss: 0.00004192
Iteration 10/1000 | Loss: 0.00004126
Iteration 11/1000 | Loss: 0.00004072
Iteration 12/1000 | Loss: 0.00004029
Iteration 13/1000 | Loss: 0.00003996
Iteration 14/1000 | Loss: 0.00003965
Iteration 15/1000 | Loss: 0.00003937
Iteration 16/1000 | Loss: 0.00003917
Iteration 17/1000 | Loss: 0.00003900
Iteration 18/1000 | Loss: 0.00003891
Iteration 19/1000 | Loss: 0.00003877
Iteration 20/1000 | Loss: 0.00003869
Iteration 21/1000 | Loss: 0.00003865
Iteration 22/1000 | Loss: 0.00003863
Iteration 23/1000 | Loss: 0.00003861
Iteration 24/1000 | Loss: 0.00003857
Iteration 25/1000 | Loss: 0.00003853
Iteration 26/1000 | Loss: 0.00003852
Iteration 27/1000 | Loss: 0.00003848
Iteration 28/1000 | Loss: 0.00003848
Iteration 29/1000 | Loss: 0.00003847
Iteration 30/1000 | Loss: 0.00003847
Iteration 31/1000 | Loss: 0.00003846
Iteration 32/1000 | Loss: 0.00003843
Iteration 33/1000 | Loss: 0.00003843
Iteration 34/1000 | Loss: 0.00003841
Iteration 35/1000 | Loss: 0.00003841
Iteration 36/1000 | Loss: 0.00003838
Iteration 37/1000 | Loss: 0.00003838
Iteration 38/1000 | Loss: 0.00003837
Iteration 39/1000 | Loss: 0.00003837
Iteration 40/1000 | Loss: 0.00003835
Iteration 41/1000 | Loss: 0.00003835
Iteration 42/1000 | Loss: 0.00003835
Iteration 43/1000 | Loss: 0.00003834
Iteration 44/1000 | Loss: 0.00003832
Iteration 45/1000 | Loss: 0.00003832
Iteration 46/1000 | Loss: 0.00003832
Iteration 47/1000 | Loss: 0.00003830
Iteration 48/1000 | Loss: 0.00003829
Iteration 49/1000 | Loss: 0.00003829
Iteration 50/1000 | Loss: 0.00003829
Iteration 51/1000 | Loss: 0.00003828
Iteration 52/1000 | Loss: 0.00003828
Iteration 53/1000 | Loss: 0.00003828
Iteration 54/1000 | Loss: 0.00003827
Iteration 55/1000 | Loss: 0.00003827
Iteration 56/1000 | Loss: 0.00003826
Iteration 57/1000 | Loss: 0.00003826
Iteration 58/1000 | Loss: 0.00003825
Iteration 59/1000 | Loss: 0.00003825
Iteration 60/1000 | Loss: 0.00003824
Iteration 61/1000 | Loss: 0.00003824
Iteration 62/1000 | Loss: 0.00003824
Iteration 63/1000 | Loss: 0.00003823
Iteration 64/1000 | Loss: 0.00003823
Iteration 65/1000 | Loss: 0.00003822
Iteration 66/1000 | Loss: 0.00003822
Iteration 67/1000 | Loss: 0.00003821
Iteration 68/1000 | Loss: 0.00003821
Iteration 69/1000 | Loss: 0.00003821
Iteration 70/1000 | Loss: 0.00003820
Iteration 71/1000 | Loss: 0.00003820
Iteration 72/1000 | Loss: 0.00003820
Iteration 73/1000 | Loss: 0.00003819
Iteration 74/1000 | Loss: 0.00003819
Iteration 75/1000 | Loss: 0.00003819
Iteration 76/1000 | Loss: 0.00003819
Iteration 77/1000 | Loss: 0.00003818
Iteration 78/1000 | Loss: 0.00003818
Iteration 79/1000 | Loss: 0.00003817
Iteration 80/1000 | Loss: 0.00003817
Iteration 81/1000 | Loss: 0.00003817
Iteration 82/1000 | Loss: 0.00003817
Iteration 83/1000 | Loss: 0.00003817
Iteration 84/1000 | Loss: 0.00003816
Iteration 85/1000 | Loss: 0.00003816
Iteration 86/1000 | Loss: 0.00003816
Iteration 87/1000 | Loss: 0.00003815
Iteration 88/1000 | Loss: 0.00003815
Iteration 89/1000 | Loss: 0.00003815
Iteration 90/1000 | Loss: 0.00003814
Iteration 91/1000 | Loss: 0.00003814
Iteration 92/1000 | Loss: 0.00003814
Iteration 93/1000 | Loss: 0.00003814
Iteration 94/1000 | Loss: 0.00003814
Iteration 95/1000 | Loss: 0.00003814
Iteration 96/1000 | Loss: 0.00003814
Iteration 97/1000 | Loss: 0.00003814
Iteration 98/1000 | Loss: 0.00003814
Iteration 99/1000 | Loss: 0.00003813
Iteration 100/1000 | Loss: 0.00003813
Iteration 101/1000 | Loss: 0.00003813
Iteration 102/1000 | Loss: 0.00003812
Iteration 103/1000 | Loss: 0.00003812
Iteration 104/1000 | Loss: 0.00003812
Iteration 105/1000 | Loss: 0.00003811
Iteration 106/1000 | Loss: 0.00003811
Iteration 107/1000 | Loss: 0.00003811
Iteration 108/1000 | Loss: 0.00003810
Iteration 109/1000 | Loss: 0.00003810
Iteration 110/1000 | Loss: 0.00003810
Iteration 111/1000 | Loss: 0.00003810
Iteration 112/1000 | Loss: 0.00003809
Iteration 113/1000 | Loss: 0.00003809
Iteration 114/1000 | Loss: 0.00003809
Iteration 115/1000 | Loss: 0.00003809
Iteration 116/1000 | Loss: 0.00003808
Iteration 117/1000 | Loss: 0.00003808
Iteration 118/1000 | Loss: 0.00003808
Iteration 119/1000 | Loss: 0.00003808
Iteration 120/1000 | Loss: 0.00003807
Iteration 121/1000 | Loss: 0.00003807
Iteration 122/1000 | Loss: 0.00003807
Iteration 123/1000 | Loss: 0.00003807
Iteration 124/1000 | Loss: 0.00003807
Iteration 125/1000 | Loss: 0.00003807
Iteration 126/1000 | Loss: 0.00003807
Iteration 127/1000 | Loss: 0.00003807
Iteration 128/1000 | Loss: 0.00003807
Iteration 129/1000 | Loss: 0.00003806
Iteration 130/1000 | Loss: 0.00003806
Iteration 131/1000 | Loss: 0.00003806
Iteration 132/1000 | Loss: 0.00003806
Iteration 133/1000 | Loss: 0.00003806
Iteration 134/1000 | Loss: 0.00003805
Iteration 135/1000 | Loss: 0.00003805
Iteration 136/1000 | Loss: 0.00003805
Iteration 137/1000 | Loss: 0.00003805
Iteration 138/1000 | Loss: 0.00003804
Iteration 139/1000 | Loss: 0.00003804
Iteration 140/1000 | Loss: 0.00003804
Iteration 141/1000 | Loss: 0.00003804
Iteration 142/1000 | Loss: 0.00003803
Iteration 143/1000 | Loss: 0.00003803
Iteration 144/1000 | Loss: 0.00003803
Iteration 145/1000 | Loss: 0.00003803
Iteration 146/1000 | Loss: 0.00003802
Iteration 147/1000 | Loss: 0.00003802
Iteration 148/1000 | Loss: 0.00003802
Iteration 149/1000 | Loss: 0.00003802
Iteration 150/1000 | Loss: 0.00003802
Iteration 151/1000 | Loss: 0.00003802
Iteration 152/1000 | Loss: 0.00003802
Iteration 153/1000 | Loss: 0.00003801
Iteration 154/1000 | Loss: 0.00003801
Iteration 155/1000 | Loss: 0.00003801
Iteration 156/1000 | Loss: 0.00003801
Iteration 157/1000 | Loss: 0.00003801
Iteration 158/1000 | Loss: 0.00003800
Iteration 159/1000 | Loss: 0.00003800
Iteration 160/1000 | Loss: 0.00003800
Iteration 161/1000 | Loss: 0.00003800
Iteration 162/1000 | Loss: 0.00003800
Iteration 163/1000 | Loss: 0.00003800
Iteration 164/1000 | Loss: 0.00003800
Iteration 165/1000 | Loss: 0.00003800
Iteration 166/1000 | Loss: 0.00003800
Iteration 167/1000 | Loss: 0.00003800
Iteration 168/1000 | Loss: 0.00003799
Iteration 169/1000 | Loss: 0.00003799
Iteration 170/1000 | Loss: 0.00003799
Iteration 171/1000 | Loss: 0.00003799
Iteration 172/1000 | Loss: 0.00003799
Iteration 173/1000 | Loss: 0.00003799
Iteration 174/1000 | Loss: 0.00003799
Iteration 175/1000 | Loss: 0.00003799
Iteration 176/1000 | Loss: 0.00003799
Iteration 177/1000 | Loss: 0.00003799
Iteration 178/1000 | Loss: 0.00003799
Iteration 179/1000 | Loss: 0.00003799
Iteration 180/1000 | Loss: 0.00003799
Iteration 181/1000 | Loss: 0.00003799
Iteration 182/1000 | Loss: 0.00003799
Iteration 183/1000 | Loss: 0.00003798
Iteration 184/1000 | Loss: 0.00003798
Iteration 185/1000 | Loss: 0.00003798
Iteration 186/1000 | Loss: 0.00003798
Iteration 187/1000 | Loss: 0.00003798
Iteration 188/1000 | Loss: 0.00003798
Iteration 189/1000 | Loss: 0.00003798
Iteration 190/1000 | Loss: 0.00003798
Iteration 191/1000 | Loss: 0.00003798
Iteration 192/1000 | Loss: 0.00003798
Iteration 193/1000 | Loss: 0.00003798
Iteration 194/1000 | Loss: 0.00003798
Iteration 195/1000 | Loss: 0.00003798
Iteration 196/1000 | Loss: 0.00003798
Iteration 197/1000 | Loss: 0.00003798
Iteration 198/1000 | Loss: 0.00003798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [3.7979159969836473e-05, 3.7979159969836473e-05, 3.7979159969836473e-05, 3.7979159969836473e-05, 3.7979159969836473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7979159969836473e-05

Optimization complete. Final v2v error: 4.94083309173584 mm

Highest mean error: 7.461886405944824 mm for frame 99

Lowest mean error: 3.495410680770874 mm for frame 139

Saving results

Total time: 61.086018085479736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056183
Iteration 2/25 | Loss: 0.00167495
Iteration 3/25 | Loss: 0.00146623
Iteration 4/25 | Loss: 0.00144508
Iteration 5/25 | Loss: 0.00143413
Iteration 6/25 | Loss: 0.00145998
Iteration 7/25 | Loss: 0.00141661
Iteration 8/25 | Loss: 0.00150317
Iteration 9/25 | Loss: 0.00136571
Iteration 10/25 | Loss: 0.00133752
Iteration 11/25 | Loss: 0.00133133
Iteration 12/25 | Loss: 0.00133012
Iteration 13/25 | Loss: 0.00132963
Iteration 14/25 | Loss: 0.00132923
Iteration 15/25 | Loss: 0.00132903
Iteration 16/25 | Loss: 0.00132890
Iteration 17/25 | Loss: 0.00132888
Iteration 18/25 | Loss: 0.00132888
Iteration 19/25 | Loss: 0.00132888
Iteration 20/25 | Loss: 0.00132887
Iteration 21/25 | Loss: 0.00132887
Iteration 22/25 | Loss: 0.00132887
Iteration 23/25 | Loss: 0.00132887
Iteration 24/25 | Loss: 0.00132887
Iteration 25/25 | Loss: 0.00132887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57603908
Iteration 2/25 | Loss: 0.00076068
Iteration 3/25 | Loss: 0.00076065
Iteration 4/25 | Loss: 0.00076065
Iteration 5/25 | Loss: 0.00076065
Iteration 6/25 | Loss: 0.00076065
Iteration 7/25 | Loss: 0.00076065
Iteration 8/25 | Loss: 0.00076065
Iteration 9/25 | Loss: 0.00076065
Iteration 10/25 | Loss: 0.00076065
Iteration 11/25 | Loss: 0.00076065
Iteration 12/25 | Loss: 0.00076065
Iteration 13/25 | Loss: 0.00076065
Iteration 14/25 | Loss: 0.00076065
Iteration 15/25 | Loss: 0.00076065
Iteration 16/25 | Loss: 0.00076065
Iteration 17/25 | Loss: 0.00076065
Iteration 18/25 | Loss: 0.00076065
Iteration 19/25 | Loss: 0.00076065
Iteration 20/25 | Loss: 0.00076065
Iteration 21/25 | Loss: 0.00076065
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007606515428051353, 0.0007606515428051353, 0.0007606515428051353, 0.0007606515428051353, 0.0007606515428051353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007606515428051353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076065
Iteration 2/1000 | Loss: 0.00005034
Iteration 3/1000 | Loss: 0.00003585
Iteration 4/1000 | Loss: 0.00003084
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002764
Iteration 7/1000 | Loss: 0.00002698
Iteration 8/1000 | Loss: 0.00002640
Iteration 9/1000 | Loss: 0.00031658
Iteration 10/1000 | Loss: 0.00002798
Iteration 11/1000 | Loss: 0.00002543
Iteration 12/1000 | Loss: 0.00002421
Iteration 13/1000 | Loss: 0.00002324
Iteration 14/1000 | Loss: 0.00002270
Iteration 15/1000 | Loss: 0.00002247
Iteration 16/1000 | Loss: 0.00002244
Iteration 17/1000 | Loss: 0.00002227
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00002198
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00002163
Iteration 23/1000 | Loss: 0.00002162
Iteration 24/1000 | Loss: 0.00002161
Iteration 25/1000 | Loss: 0.00002155
Iteration 26/1000 | Loss: 0.00002155
Iteration 27/1000 | Loss: 0.00002153
Iteration 28/1000 | Loss: 0.00002153
Iteration 29/1000 | Loss: 0.00002152
Iteration 30/1000 | Loss: 0.00002152
Iteration 31/1000 | Loss: 0.00002152
Iteration 32/1000 | Loss: 0.00002152
Iteration 33/1000 | Loss: 0.00002152
Iteration 34/1000 | Loss: 0.00002152
Iteration 35/1000 | Loss: 0.00002152
Iteration 36/1000 | Loss: 0.00002152
Iteration 37/1000 | Loss: 0.00002152
Iteration 38/1000 | Loss: 0.00002151
Iteration 39/1000 | Loss: 0.00002151
Iteration 40/1000 | Loss: 0.00002151
Iteration 41/1000 | Loss: 0.00002151
Iteration 42/1000 | Loss: 0.00002150
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002149
Iteration 46/1000 | Loss: 0.00002149
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002148
Iteration 49/1000 | Loss: 0.00002148
Iteration 50/1000 | Loss: 0.00002148
Iteration 51/1000 | Loss: 0.00002148
Iteration 52/1000 | Loss: 0.00002148
Iteration 53/1000 | Loss: 0.00002148
Iteration 54/1000 | Loss: 0.00002148
Iteration 55/1000 | Loss: 0.00002148
Iteration 56/1000 | Loss: 0.00002147
Iteration 57/1000 | Loss: 0.00002147
Iteration 58/1000 | Loss: 0.00002147
Iteration 59/1000 | Loss: 0.00002146
Iteration 60/1000 | Loss: 0.00002146
Iteration 61/1000 | Loss: 0.00002146
Iteration 62/1000 | Loss: 0.00002146
Iteration 63/1000 | Loss: 0.00002146
Iteration 64/1000 | Loss: 0.00002146
Iteration 65/1000 | Loss: 0.00002146
Iteration 66/1000 | Loss: 0.00002146
Iteration 67/1000 | Loss: 0.00002145
Iteration 68/1000 | Loss: 0.00002145
Iteration 69/1000 | Loss: 0.00002145
Iteration 70/1000 | Loss: 0.00002145
Iteration 71/1000 | Loss: 0.00002145
Iteration 72/1000 | Loss: 0.00002145
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00002145
Iteration 77/1000 | Loss: 0.00002145
Iteration 78/1000 | Loss: 0.00002145
Iteration 79/1000 | Loss: 0.00002145
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002145
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.144777317880653e-05, 2.144777317880653e-05, 2.144777317880653e-05, 2.144777317880653e-05, 2.144777317880653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.144777317880653e-05

Optimization complete. Final v2v error: 3.8193681240081787 mm

Highest mean error: 4.290289402008057 mm for frame 115

Lowest mean error: 3.5576207637786865 mm for frame 51

Saving results

Total time: 57.56162071228027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812292
Iteration 2/25 | Loss: 0.00138313
Iteration 3/25 | Loss: 0.00126211
Iteration 4/25 | Loss: 0.00125180
Iteration 5/25 | Loss: 0.00124857
Iteration 6/25 | Loss: 0.00124857
Iteration 7/25 | Loss: 0.00124857
Iteration 8/25 | Loss: 0.00124857
Iteration 9/25 | Loss: 0.00124857
Iteration 10/25 | Loss: 0.00124857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012485728366300464, 0.0012485728366300464, 0.0012485728366300464, 0.0012485728366300464, 0.0012485728366300464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012485728366300464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.86713004
Iteration 2/25 | Loss: 0.00072782
Iteration 3/25 | Loss: 0.00072778
Iteration 4/25 | Loss: 0.00072778
Iteration 5/25 | Loss: 0.00072778
Iteration 6/25 | Loss: 0.00072778
Iteration 7/25 | Loss: 0.00072778
Iteration 8/25 | Loss: 0.00072778
Iteration 9/25 | Loss: 0.00072778
Iteration 10/25 | Loss: 0.00072778
Iteration 11/25 | Loss: 0.00072778
Iteration 12/25 | Loss: 0.00072778
Iteration 13/25 | Loss: 0.00072778
Iteration 14/25 | Loss: 0.00072778
Iteration 15/25 | Loss: 0.00072778
Iteration 16/25 | Loss: 0.00072778
Iteration 17/25 | Loss: 0.00072778
Iteration 18/25 | Loss: 0.00072778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00072777911555022, 0.00072777911555022, 0.00072777911555022, 0.00072777911555022, 0.00072777911555022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00072777911555022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072778
Iteration 2/1000 | Loss: 0.00003936
Iteration 3/1000 | Loss: 0.00002666
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002231
Iteration 7/1000 | Loss: 0.00002166
Iteration 8/1000 | Loss: 0.00002120
Iteration 9/1000 | Loss: 0.00002072
Iteration 10/1000 | Loss: 0.00002039
Iteration 11/1000 | Loss: 0.00002020
Iteration 12/1000 | Loss: 0.00001996
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001981
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001978
Iteration 18/1000 | Loss: 0.00001974
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001946
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001944
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001936
Iteration 28/1000 | Loss: 0.00001936
Iteration 29/1000 | Loss: 0.00001934
Iteration 30/1000 | Loss: 0.00001934
Iteration 31/1000 | Loss: 0.00001933
Iteration 32/1000 | Loss: 0.00001933
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001914
Iteration 65/1000 | Loss: 0.00001914
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001913
Iteration 72/1000 | Loss: 0.00001913
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001911
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001911
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001911
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001910
Iteration 91/1000 | Loss: 0.00001910
Iteration 92/1000 | Loss: 0.00001910
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.9103541490039788e-05, 1.9103541490039788e-05, 1.9103541490039788e-05, 1.9103541490039788e-05, 1.9103541490039788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9103541490039788e-05

Optimization complete. Final v2v error: 3.6960835456848145 mm

Highest mean error: 4.608703136444092 mm for frame 2

Lowest mean error: 3.1375744342803955 mm for frame 109

Saving results

Total time: 42.723087310791016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769174
Iteration 2/25 | Loss: 0.00192665
Iteration 3/25 | Loss: 0.00140449
Iteration 4/25 | Loss: 0.00135052
Iteration 5/25 | Loss: 0.00127869
Iteration 6/25 | Loss: 0.00126197
Iteration 7/25 | Loss: 0.00122339
Iteration 8/25 | Loss: 0.00120926
Iteration 9/25 | Loss: 0.00119999
Iteration 10/25 | Loss: 0.00119446
Iteration 11/25 | Loss: 0.00119786
Iteration 12/25 | Loss: 0.00119044
Iteration 13/25 | Loss: 0.00119108
Iteration 14/25 | Loss: 0.00119036
Iteration 15/25 | Loss: 0.00119026
Iteration 16/25 | Loss: 0.00119026
Iteration 17/25 | Loss: 0.00119026
Iteration 18/25 | Loss: 0.00119026
Iteration 19/25 | Loss: 0.00119026
Iteration 20/25 | Loss: 0.00119026
Iteration 21/25 | Loss: 0.00119026
Iteration 22/25 | Loss: 0.00119026
Iteration 23/25 | Loss: 0.00119026
Iteration 24/25 | Loss: 0.00119026
Iteration 25/25 | Loss: 0.00119026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11722088
Iteration 2/25 | Loss: 0.00087638
Iteration 3/25 | Loss: 0.00087637
Iteration 4/25 | Loss: 0.00081768
Iteration 5/25 | Loss: 0.00081768
Iteration 6/25 | Loss: 0.00081768
Iteration 7/25 | Loss: 0.00081768
Iteration 8/25 | Loss: 0.00081768
Iteration 9/25 | Loss: 0.00081768
Iteration 10/25 | Loss: 0.00081768
Iteration 11/25 | Loss: 0.00081768
Iteration 12/25 | Loss: 0.00081768
Iteration 13/25 | Loss: 0.00081768
Iteration 14/25 | Loss: 0.00081768
Iteration 15/25 | Loss: 0.00081768
Iteration 16/25 | Loss: 0.00081768
Iteration 17/25 | Loss: 0.00081768
Iteration 18/25 | Loss: 0.00081768
Iteration 19/25 | Loss: 0.00081768
Iteration 20/25 | Loss: 0.00081768
Iteration 21/25 | Loss: 0.00081768
Iteration 22/25 | Loss: 0.00081768
Iteration 23/25 | Loss: 0.00081768
Iteration 24/25 | Loss: 0.00081768
Iteration 25/25 | Loss: 0.00081768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081768
Iteration 2/1000 | Loss: 0.00009822
Iteration 3/1000 | Loss: 0.00008536
Iteration 4/1000 | Loss: 0.00035704
Iteration 5/1000 | Loss: 0.00001634
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00002443
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00012827
Iteration 10/1000 | Loss: 0.00010670
Iteration 11/1000 | Loss: 0.00028733
Iteration 12/1000 | Loss: 0.00004301
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001221
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001220
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001949
Iteration 32/1000 | Loss: 0.00002405
Iteration 33/1000 | Loss: 0.00016381
Iteration 34/1000 | Loss: 0.00006647
Iteration 35/1000 | Loss: 0.00002129
Iteration 36/1000 | Loss: 0.00002182
Iteration 37/1000 | Loss: 0.00003499
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00002516
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00003967
Iteration 43/1000 | Loss: 0.00053824
Iteration 44/1000 | Loss: 0.00011419
Iteration 45/1000 | Loss: 0.00007848
Iteration 46/1000 | Loss: 0.00001758
Iteration 47/1000 | Loss: 0.00006701
Iteration 48/1000 | Loss: 0.00001813
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00003617
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00002981
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00002567
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001224
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001160
Iteration 72/1000 | Loss: 0.00001160
Iteration 73/1000 | Loss: 0.00001160
Iteration 74/1000 | Loss: 0.00001160
Iteration 75/1000 | Loss: 0.00001160
Iteration 76/1000 | Loss: 0.00001160
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00002955
Iteration 131/1000 | Loss: 0.00001996
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00015573
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00001623
Iteration 136/1000 | Loss: 0.00006039
Iteration 137/1000 | Loss: 0.00005149
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001508
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00003332
Iteration 143/1000 | Loss: 0.00001232
Iteration 144/1000 | Loss: 0.00001712
Iteration 145/1000 | Loss: 0.00002274
Iteration 146/1000 | Loss: 0.00003377
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00003982
Iteration 149/1000 | Loss: 0.00001167
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001141
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001141
Iteration 156/1000 | Loss: 0.00001141
Iteration 157/1000 | Loss: 0.00001141
Iteration 158/1000 | Loss: 0.00001141
Iteration 159/1000 | Loss: 0.00001141
Iteration 160/1000 | Loss: 0.00001141
Iteration 161/1000 | Loss: 0.00001141
Iteration 162/1000 | Loss: 0.00001141
Iteration 163/1000 | Loss: 0.00001141
Iteration 164/1000 | Loss: 0.00001141
Iteration 165/1000 | Loss: 0.00001141
Iteration 166/1000 | Loss: 0.00001141
Iteration 167/1000 | Loss: 0.00001141
Iteration 168/1000 | Loss: 0.00001140
Iteration 169/1000 | Loss: 0.00001140
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001140
Iteration 173/1000 | Loss: 0.00001140
Iteration 174/1000 | Loss: 0.00001140
Iteration 175/1000 | Loss: 0.00001140
Iteration 176/1000 | Loss: 0.00001140
Iteration 177/1000 | Loss: 0.00001140
Iteration 178/1000 | Loss: 0.00001139
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001139
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001139
Iteration 191/1000 | Loss: 0.00001139
Iteration 192/1000 | Loss: 0.00001139
Iteration 193/1000 | Loss: 0.00001139
Iteration 194/1000 | Loss: 0.00001139
Iteration 195/1000 | Loss: 0.00001139
Iteration 196/1000 | Loss: 0.00001139
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.138861171057215e-05, 1.138861171057215e-05, 1.138861171057215e-05, 1.138861171057215e-05, 1.138861171057215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.138861171057215e-05

Optimization complete. Final v2v error: 2.9019713401794434 mm

Highest mean error: 3.2406227588653564 mm for frame 61

Lowest mean error: 2.7690765857696533 mm for frame 147

Saving results

Total time: 120.75240302085876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999217
Iteration 2/25 | Loss: 0.00196981
Iteration 3/25 | Loss: 0.00156566
Iteration 4/25 | Loss: 0.00150035
Iteration 5/25 | Loss: 0.00151084
Iteration 6/25 | Loss: 0.00151440
Iteration 7/25 | Loss: 0.00148722
Iteration 8/25 | Loss: 0.00138826
Iteration 9/25 | Loss: 0.00140280
Iteration 10/25 | Loss: 0.00138694
Iteration 11/25 | Loss: 0.00135527
Iteration 12/25 | Loss: 0.00134728
Iteration 13/25 | Loss: 0.00133842
Iteration 14/25 | Loss: 0.00133346
Iteration 15/25 | Loss: 0.00133475
Iteration 16/25 | Loss: 0.00133280
Iteration 17/25 | Loss: 0.00133002
Iteration 18/25 | Loss: 0.00132144
Iteration 19/25 | Loss: 0.00131537
Iteration 20/25 | Loss: 0.00132932
Iteration 21/25 | Loss: 0.00131898
Iteration 22/25 | Loss: 0.00131691
Iteration 23/25 | Loss: 0.00131880
Iteration 24/25 | Loss: 0.00132520
Iteration 25/25 | Loss: 0.00132319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55149543
Iteration 2/25 | Loss: 0.00148736
Iteration 3/25 | Loss: 0.00147691
Iteration 4/25 | Loss: 0.00147691
Iteration 5/25 | Loss: 0.00147691
Iteration 6/25 | Loss: 0.00147691
Iteration 7/25 | Loss: 0.00147691
Iteration 8/25 | Loss: 0.00147691
Iteration 9/25 | Loss: 0.00147691
Iteration 10/25 | Loss: 0.00147691
Iteration 11/25 | Loss: 0.00147691
Iteration 12/25 | Loss: 0.00147691
Iteration 13/25 | Loss: 0.00147691
Iteration 14/25 | Loss: 0.00147691
Iteration 15/25 | Loss: 0.00147691
Iteration 16/25 | Loss: 0.00147691
Iteration 17/25 | Loss: 0.00147691
Iteration 18/25 | Loss: 0.00147691
Iteration 19/25 | Loss: 0.00147691
Iteration 20/25 | Loss: 0.00147691
Iteration 21/25 | Loss: 0.00147691
Iteration 22/25 | Loss: 0.00147691
Iteration 23/25 | Loss: 0.00147691
Iteration 24/25 | Loss: 0.00147691
Iteration 25/25 | Loss: 0.00147691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147691
Iteration 2/1000 | Loss: 0.00082766
Iteration 3/1000 | Loss: 0.00071328
Iteration 4/1000 | Loss: 0.00088621
Iteration 5/1000 | Loss: 0.00089835
Iteration 6/1000 | Loss: 0.00102050
Iteration 7/1000 | Loss: 0.00115258
Iteration 8/1000 | Loss: 0.00103234
Iteration 9/1000 | Loss: 0.00084585
Iteration 10/1000 | Loss: 0.00101025
Iteration 11/1000 | Loss: 0.00100077
Iteration 12/1000 | Loss: 0.00088428
Iteration 13/1000 | Loss: 0.00078845
Iteration 14/1000 | Loss: 0.00092954
Iteration 15/1000 | Loss: 0.00093783
Iteration 16/1000 | Loss: 0.00044959
Iteration 17/1000 | Loss: 0.00061885
Iteration 18/1000 | Loss: 0.00046906
Iteration 19/1000 | Loss: 0.00044799
Iteration 20/1000 | Loss: 0.00083571
Iteration 21/1000 | Loss: 0.00047140
Iteration 22/1000 | Loss: 0.00051353
Iteration 23/1000 | Loss: 0.00129225
Iteration 24/1000 | Loss: 0.00066789
Iteration 25/1000 | Loss: 0.00029954
Iteration 26/1000 | Loss: 0.00112232
Iteration 27/1000 | Loss: 0.00059067
Iteration 28/1000 | Loss: 0.00058284
Iteration 29/1000 | Loss: 0.00068407
Iteration 30/1000 | Loss: 0.00063292
Iteration 31/1000 | Loss: 0.00091926
Iteration 32/1000 | Loss: 0.00045995
Iteration 33/1000 | Loss: 0.00094831
Iteration 34/1000 | Loss: 0.00120866
Iteration 35/1000 | Loss: 0.00064699
Iteration 36/1000 | Loss: 0.00092633
Iteration 37/1000 | Loss: 0.00064291
Iteration 38/1000 | Loss: 0.00044377
Iteration 39/1000 | Loss: 0.00030875
Iteration 40/1000 | Loss: 0.00008367
Iteration 41/1000 | Loss: 0.00028070
Iteration 42/1000 | Loss: 0.00019930
Iteration 43/1000 | Loss: 0.00024926
Iteration 44/1000 | Loss: 0.00013349
Iteration 45/1000 | Loss: 0.00036198
Iteration 46/1000 | Loss: 0.00013893
Iteration 47/1000 | Loss: 0.00052358
Iteration 48/1000 | Loss: 0.00018337
Iteration 49/1000 | Loss: 0.00016163
Iteration 50/1000 | Loss: 0.00013342
Iteration 51/1000 | Loss: 0.00036690
Iteration 52/1000 | Loss: 0.00025740
Iteration 53/1000 | Loss: 0.00028473
Iteration 54/1000 | Loss: 0.00030865
Iteration 55/1000 | Loss: 0.00006884
Iteration 56/1000 | Loss: 0.00044984
Iteration 57/1000 | Loss: 0.00041992
Iteration 58/1000 | Loss: 0.00008629
Iteration 59/1000 | Loss: 0.00013442
Iteration 60/1000 | Loss: 0.00011084
Iteration 61/1000 | Loss: 0.00019960
Iteration 62/1000 | Loss: 0.00026121
Iteration 63/1000 | Loss: 0.00035280
Iteration 64/1000 | Loss: 0.00011896
Iteration 65/1000 | Loss: 0.00030141
Iteration 66/1000 | Loss: 0.00027413
Iteration 67/1000 | Loss: 0.00010576
Iteration 68/1000 | Loss: 0.00018855
Iteration 69/1000 | Loss: 0.00020794
Iteration 70/1000 | Loss: 0.00024573
Iteration 71/1000 | Loss: 0.00012447
Iteration 72/1000 | Loss: 0.00027038
Iteration 73/1000 | Loss: 0.00005152
Iteration 74/1000 | Loss: 0.00012418
Iteration 75/1000 | Loss: 0.00014831
Iteration 76/1000 | Loss: 0.00007109
Iteration 77/1000 | Loss: 0.00021982
Iteration 78/1000 | Loss: 0.00019054
Iteration 79/1000 | Loss: 0.00021643
Iteration 80/1000 | Loss: 0.00005336
Iteration 81/1000 | Loss: 0.00032306
Iteration 82/1000 | Loss: 0.00026112
Iteration 83/1000 | Loss: 0.00005511
Iteration 84/1000 | Loss: 0.00009634
Iteration 85/1000 | Loss: 0.00021837
Iteration 86/1000 | Loss: 0.00032101
Iteration 87/1000 | Loss: 0.00005286
Iteration 88/1000 | Loss: 0.00085242
Iteration 89/1000 | Loss: 0.00085554
Iteration 90/1000 | Loss: 0.00030327
Iteration 91/1000 | Loss: 0.00054562
Iteration 92/1000 | Loss: 0.00028796
Iteration 93/1000 | Loss: 0.00025877
Iteration 94/1000 | Loss: 0.00022562
Iteration 95/1000 | Loss: 0.00003955
Iteration 96/1000 | Loss: 0.00005542
Iteration 97/1000 | Loss: 0.00004985
Iteration 98/1000 | Loss: 0.00005044
Iteration 99/1000 | Loss: 0.00019037
Iteration 100/1000 | Loss: 0.00029953
Iteration 101/1000 | Loss: 0.00029127
Iteration 102/1000 | Loss: 0.00027213
Iteration 103/1000 | Loss: 0.00004886
Iteration 104/1000 | Loss: 0.00005466
Iteration 105/1000 | Loss: 0.00003357
Iteration 106/1000 | Loss: 0.00016368
Iteration 107/1000 | Loss: 0.00011666
Iteration 108/1000 | Loss: 0.00004828
Iteration 109/1000 | Loss: 0.00004593
Iteration 110/1000 | Loss: 0.00017809
Iteration 111/1000 | Loss: 0.00010655
Iteration 112/1000 | Loss: 0.00002678
Iteration 113/1000 | Loss: 0.00003125
Iteration 114/1000 | Loss: 0.00030146
Iteration 115/1000 | Loss: 0.00011604
Iteration 116/1000 | Loss: 0.00033507
Iteration 117/1000 | Loss: 0.00018122
Iteration 118/1000 | Loss: 0.00022005
Iteration 119/1000 | Loss: 0.00028915
Iteration 120/1000 | Loss: 0.00035560
Iteration 121/1000 | Loss: 0.00006582
Iteration 122/1000 | Loss: 0.00013610
Iteration 123/1000 | Loss: 0.00004104
Iteration 124/1000 | Loss: 0.00004846
Iteration 125/1000 | Loss: 0.00004267
Iteration 126/1000 | Loss: 0.00014140
Iteration 127/1000 | Loss: 0.00004548
Iteration 128/1000 | Loss: 0.00016658
Iteration 129/1000 | Loss: 0.00005938
Iteration 130/1000 | Loss: 0.00010318
Iteration 131/1000 | Loss: 0.00007026
Iteration 132/1000 | Loss: 0.00020377
Iteration 133/1000 | Loss: 0.00027619
Iteration 134/1000 | Loss: 0.00018924
Iteration 135/1000 | Loss: 0.00008639
Iteration 136/1000 | Loss: 0.00015590
Iteration 137/1000 | Loss: 0.00018120
Iteration 138/1000 | Loss: 0.00003365
Iteration 139/1000 | Loss: 0.00002893
Iteration 140/1000 | Loss: 0.00017323
Iteration 141/1000 | Loss: 0.00017974
Iteration 142/1000 | Loss: 0.00002699
Iteration 143/1000 | Loss: 0.00002568
Iteration 144/1000 | Loss: 0.00016206
Iteration 145/1000 | Loss: 0.00018643
Iteration 146/1000 | Loss: 0.00002483
Iteration 147/1000 | Loss: 0.00002248
Iteration 148/1000 | Loss: 0.00002147
Iteration 149/1000 | Loss: 0.00018574
Iteration 150/1000 | Loss: 0.00017240
Iteration 151/1000 | Loss: 0.00016754
Iteration 152/1000 | Loss: 0.00003044
Iteration 153/1000 | Loss: 0.00014492
Iteration 154/1000 | Loss: 0.00014337
Iteration 155/1000 | Loss: 0.00007562
Iteration 156/1000 | Loss: 0.00002844
Iteration 157/1000 | Loss: 0.00013888
Iteration 158/1000 | Loss: 0.00003669
Iteration 159/1000 | Loss: 0.00003010
Iteration 160/1000 | Loss: 0.00002559
Iteration 161/1000 | Loss: 0.00002234
Iteration 162/1000 | Loss: 0.00002147
Iteration 163/1000 | Loss: 0.00002091
Iteration 164/1000 | Loss: 0.00027331
Iteration 165/1000 | Loss: 0.00025610
Iteration 166/1000 | Loss: 0.00003058
Iteration 167/1000 | Loss: 0.00002591
Iteration 168/1000 | Loss: 0.00002340
Iteration 169/1000 | Loss: 0.00002082
Iteration 170/1000 | Loss: 0.00001893
Iteration 171/1000 | Loss: 0.00001818
Iteration 172/1000 | Loss: 0.00001772
Iteration 173/1000 | Loss: 0.00001708
Iteration 174/1000 | Loss: 0.00001675
Iteration 175/1000 | Loss: 0.00001671
Iteration 176/1000 | Loss: 0.00001659
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001656
Iteration 179/1000 | Loss: 0.00001656
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001654
Iteration 182/1000 | Loss: 0.00001654
Iteration 183/1000 | Loss: 0.00001653
Iteration 184/1000 | Loss: 0.00001653
Iteration 185/1000 | Loss: 0.00001653
Iteration 186/1000 | Loss: 0.00001650
Iteration 187/1000 | Loss: 0.00001647
Iteration 188/1000 | Loss: 0.00001646
Iteration 189/1000 | Loss: 0.00001646
Iteration 190/1000 | Loss: 0.00001646
Iteration 191/1000 | Loss: 0.00001645
Iteration 192/1000 | Loss: 0.00001644
Iteration 193/1000 | Loss: 0.00001643
Iteration 194/1000 | Loss: 0.00001643
Iteration 195/1000 | Loss: 0.00001642
Iteration 196/1000 | Loss: 0.00001642
Iteration 197/1000 | Loss: 0.00001641
Iteration 198/1000 | Loss: 0.00001641
Iteration 199/1000 | Loss: 0.00001641
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001640
Iteration 202/1000 | Loss: 0.00001640
Iteration 203/1000 | Loss: 0.00001639
Iteration 204/1000 | Loss: 0.00001639
Iteration 205/1000 | Loss: 0.00001639
Iteration 206/1000 | Loss: 0.00001638
Iteration 207/1000 | Loss: 0.00001638
Iteration 208/1000 | Loss: 0.00001638
Iteration 209/1000 | Loss: 0.00001638
Iteration 210/1000 | Loss: 0.00001638
Iteration 211/1000 | Loss: 0.00001638
Iteration 212/1000 | Loss: 0.00001638
Iteration 213/1000 | Loss: 0.00001637
Iteration 214/1000 | Loss: 0.00001637
Iteration 215/1000 | Loss: 0.00001637
Iteration 216/1000 | Loss: 0.00001637
Iteration 217/1000 | Loss: 0.00001637
Iteration 218/1000 | Loss: 0.00001636
Iteration 219/1000 | Loss: 0.00001636
Iteration 220/1000 | Loss: 0.00001636
Iteration 221/1000 | Loss: 0.00001636
Iteration 222/1000 | Loss: 0.00001636
Iteration 223/1000 | Loss: 0.00001635
Iteration 224/1000 | Loss: 0.00001635
Iteration 225/1000 | Loss: 0.00001635
Iteration 226/1000 | Loss: 0.00001635
Iteration 227/1000 | Loss: 0.00001635
Iteration 228/1000 | Loss: 0.00001635
Iteration 229/1000 | Loss: 0.00001635
Iteration 230/1000 | Loss: 0.00001634
Iteration 231/1000 | Loss: 0.00001634
Iteration 232/1000 | Loss: 0.00001634
Iteration 233/1000 | Loss: 0.00001634
Iteration 234/1000 | Loss: 0.00001633
Iteration 235/1000 | Loss: 0.00001633
Iteration 236/1000 | Loss: 0.00001633
Iteration 237/1000 | Loss: 0.00001633
Iteration 238/1000 | Loss: 0.00001633
Iteration 239/1000 | Loss: 0.00001632
Iteration 240/1000 | Loss: 0.00001632
Iteration 241/1000 | Loss: 0.00001632
Iteration 242/1000 | Loss: 0.00001632
Iteration 243/1000 | Loss: 0.00001632
Iteration 244/1000 | Loss: 0.00001632
Iteration 245/1000 | Loss: 0.00001632
Iteration 246/1000 | Loss: 0.00001632
Iteration 247/1000 | Loss: 0.00001632
Iteration 248/1000 | Loss: 0.00001632
Iteration 249/1000 | Loss: 0.00001632
Iteration 250/1000 | Loss: 0.00001632
Iteration 251/1000 | Loss: 0.00001632
Iteration 252/1000 | Loss: 0.00001632
Iteration 253/1000 | Loss: 0.00001632
Iteration 254/1000 | Loss: 0.00001632
Iteration 255/1000 | Loss: 0.00001632
Iteration 256/1000 | Loss: 0.00001632
Iteration 257/1000 | Loss: 0.00001632
Iteration 258/1000 | Loss: 0.00001632
Iteration 259/1000 | Loss: 0.00001631
Iteration 260/1000 | Loss: 0.00001631
Iteration 261/1000 | Loss: 0.00001631
Iteration 262/1000 | Loss: 0.00001631
Iteration 263/1000 | Loss: 0.00001631
Iteration 264/1000 | Loss: 0.00001631
Iteration 265/1000 | Loss: 0.00001631
Iteration 266/1000 | Loss: 0.00001631
Iteration 267/1000 | Loss: 0.00001631
Iteration 268/1000 | Loss: 0.00001630
Iteration 269/1000 | Loss: 0.00001630
Iteration 270/1000 | Loss: 0.00001630
Iteration 271/1000 | Loss: 0.00001630
Iteration 272/1000 | Loss: 0.00001630
Iteration 273/1000 | Loss: 0.00001630
Iteration 274/1000 | Loss: 0.00001630
Iteration 275/1000 | Loss: 0.00001630
Iteration 276/1000 | Loss: 0.00001630
Iteration 277/1000 | Loss: 0.00001629
Iteration 278/1000 | Loss: 0.00001629
Iteration 279/1000 | Loss: 0.00001629
Iteration 280/1000 | Loss: 0.00001629
Iteration 281/1000 | Loss: 0.00001629
Iteration 282/1000 | Loss: 0.00001629
Iteration 283/1000 | Loss: 0.00001629
Iteration 284/1000 | Loss: 0.00001629
Iteration 285/1000 | Loss: 0.00001628
Iteration 286/1000 | Loss: 0.00001628
Iteration 287/1000 | Loss: 0.00001628
Iteration 288/1000 | Loss: 0.00001628
Iteration 289/1000 | Loss: 0.00001628
Iteration 290/1000 | Loss: 0.00001628
Iteration 291/1000 | Loss: 0.00001628
Iteration 292/1000 | Loss: 0.00001628
Iteration 293/1000 | Loss: 0.00001628
Iteration 294/1000 | Loss: 0.00001628
Iteration 295/1000 | Loss: 0.00001628
Iteration 296/1000 | Loss: 0.00001628
Iteration 297/1000 | Loss: 0.00001628
Iteration 298/1000 | Loss: 0.00001628
Iteration 299/1000 | Loss: 0.00001628
Iteration 300/1000 | Loss: 0.00001628
Iteration 301/1000 | Loss: 0.00001628
Iteration 302/1000 | Loss: 0.00001628
Iteration 303/1000 | Loss: 0.00001628
Iteration 304/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [1.6275460438919254e-05, 1.6275460438919254e-05, 1.6275460438919254e-05, 1.6275460438919254e-05, 1.6275460438919254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6275460438919254e-05

Optimization complete. Final v2v error: 3.3991689682006836 mm

Highest mean error: 4.49733304977417 mm for frame 68

Lowest mean error: 2.9654176235198975 mm for frame 21

Saving results

Total time: 294.1527681350708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530832
Iteration 2/25 | Loss: 0.00149893
Iteration 3/25 | Loss: 0.00130556
Iteration 4/25 | Loss: 0.00129108
Iteration 5/25 | Loss: 0.00128862
Iteration 6/25 | Loss: 0.00128849
Iteration 7/25 | Loss: 0.00128849
Iteration 8/25 | Loss: 0.00128849
Iteration 9/25 | Loss: 0.00128849
Iteration 10/25 | Loss: 0.00128849
Iteration 11/25 | Loss: 0.00128849
Iteration 12/25 | Loss: 0.00128849
Iteration 13/25 | Loss: 0.00128849
Iteration 14/25 | Loss: 0.00128849
Iteration 15/25 | Loss: 0.00128849
Iteration 16/25 | Loss: 0.00128849
Iteration 17/25 | Loss: 0.00128849
Iteration 18/25 | Loss: 0.00128849
Iteration 19/25 | Loss: 0.00128849
Iteration 20/25 | Loss: 0.00128849
Iteration 21/25 | Loss: 0.00128849
Iteration 22/25 | Loss: 0.00128849
Iteration 23/25 | Loss: 0.00128849
Iteration 24/25 | Loss: 0.00128849
Iteration 25/25 | Loss: 0.00128849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43682075
Iteration 2/25 | Loss: 0.00063713
Iteration 3/25 | Loss: 0.00063713
Iteration 4/25 | Loss: 0.00063713
Iteration 5/25 | Loss: 0.00063713
Iteration 6/25 | Loss: 0.00063713
Iteration 7/25 | Loss: 0.00063713
Iteration 8/25 | Loss: 0.00063713
Iteration 9/25 | Loss: 0.00063713
Iteration 10/25 | Loss: 0.00063713
Iteration 11/25 | Loss: 0.00063713
Iteration 12/25 | Loss: 0.00063713
Iteration 13/25 | Loss: 0.00063713
Iteration 14/25 | Loss: 0.00063713
Iteration 15/25 | Loss: 0.00063713
Iteration 16/25 | Loss: 0.00063713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000637129123788327, 0.000637129123788327, 0.000637129123788327, 0.000637129123788327, 0.000637129123788327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000637129123788327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063713
Iteration 2/1000 | Loss: 0.00003780
Iteration 3/1000 | Loss: 0.00002668
Iteration 4/1000 | Loss: 0.00002459
Iteration 5/1000 | Loss: 0.00002346
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002207
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002116
Iteration 11/1000 | Loss: 0.00002108
Iteration 12/1000 | Loss: 0.00002105
Iteration 13/1000 | Loss: 0.00002098
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002087
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002083
Iteration 18/1000 | Loss: 0.00002082
Iteration 19/1000 | Loss: 0.00002080
Iteration 20/1000 | Loss: 0.00002079
Iteration 21/1000 | Loss: 0.00002079
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002079
Iteration 24/1000 | Loss: 0.00002079
Iteration 25/1000 | Loss: 0.00002079
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002078
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002078
Iteration 31/1000 | Loss: 0.00002077
Iteration 32/1000 | Loss: 0.00002077
Iteration 33/1000 | Loss: 0.00002077
Iteration 34/1000 | Loss: 0.00002077
Iteration 35/1000 | Loss: 0.00002077
Iteration 36/1000 | Loss: 0.00002077
Iteration 37/1000 | Loss: 0.00002077
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00002076
Iteration 40/1000 | Loss: 0.00002075
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002074
Iteration 44/1000 | Loss: 0.00002074
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002073
Iteration 47/1000 | Loss: 0.00002073
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002073
Iteration 50/1000 | Loss: 0.00002072
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002071
Iteration 53/1000 | Loss: 0.00002071
Iteration 54/1000 | Loss: 0.00002071
Iteration 55/1000 | Loss: 0.00002071
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002066
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002064
Iteration 94/1000 | Loss: 0.00002064
Iteration 95/1000 | Loss: 0.00002064
Iteration 96/1000 | Loss: 0.00002064
Iteration 97/1000 | Loss: 0.00002064
Iteration 98/1000 | Loss: 0.00002064
Iteration 99/1000 | Loss: 0.00002064
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002064
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002063
Iteration 109/1000 | Loss: 0.00002063
Iteration 110/1000 | Loss: 0.00002063
Iteration 111/1000 | Loss: 0.00002063
Iteration 112/1000 | Loss: 0.00002063
Iteration 113/1000 | Loss: 0.00002063
Iteration 114/1000 | Loss: 0.00002063
Iteration 115/1000 | Loss: 0.00002063
Iteration 116/1000 | Loss: 0.00002063
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002062
Iteration 120/1000 | Loss: 0.00002062
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002062
Iteration 123/1000 | Loss: 0.00002062
Iteration 124/1000 | Loss: 0.00002062
Iteration 125/1000 | Loss: 0.00002062
Iteration 126/1000 | Loss: 0.00002062
Iteration 127/1000 | Loss: 0.00002062
Iteration 128/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.062325074803084e-05, 2.062325074803084e-05, 2.062325074803084e-05, 2.062325074803084e-05, 2.062325074803084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.062325074803084e-05

Optimization complete. Final v2v error: 3.8119242191314697 mm

Highest mean error: 4.180169105529785 mm for frame 163

Lowest mean error: 3.605123996734619 mm for frame 205

Saving results

Total time: 34.212791204452515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880839
Iteration 2/25 | Loss: 0.00151853
Iteration 3/25 | Loss: 0.00133849
Iteration 4/25 | Loss: 0.00132164
Iteration 5/25 | Loss: 0.00132252
Iteration 6/25 | Loss: 0.00132412
Iteration 7/25 | Loss: 0.00131632
Iteration 8/25 | Loss: 0.00132042
Iteration 9/25 | Loss: 0.00131599
Iteration 10/25 | Loss: 0.00131579
Iteration 11/25 | Loss: 0.00131578
Iteration 12/25 | Loss: 0.00131578
Iteration 13/25 | Loss: 0.00131578
Iteration 14/25 | Loss: 0.00131578
Iteration 15/25 | Loss: 0.00131578
Iteration 16/25 | Loss: 0.00131578
Iteration 17/25 | Loss: 0.00131578
Iteration 18/25 | Loss: 0.00131578
Iteration 19/25 | Loss: 0.00131578
Iteration 20/25 | Loss: 0.00131578
Iteration 21/25 | Loss: 0.00131578
Iteration 22/25 | Loss: 0.00131578
Iteration 23/25 | Loss: 0.00131578
Iteration 24/25 | Loss: 0.00131578
Iteration 25/25 | Loss: 0.00131578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.43330956
Iteration 2/25 | Loss: 0.00076961
Iteration 3/25 | Loss: 0.00076949
Iteration 4/25 | Loss: 0.00076948
Iteration 5/25 | Loss: 0.00076948
Iteration 6/25 | Loss: 0.00076948
Iteration 7/25 | Loss: 0.00076948
Iteration 8/25 | Loss: 0.00076948
Iteration 9/25 | Loss: 0.00076948
Iteration 10/25 | Loss: 0.00076948
Iteration 11/25 | Loss: 0.00076948
Iteration 12/25 | Loss: 0.00076948
Iteration 13/25 | Loss: 0.00076948
Iteration 14/25 | Loss: 0.00076948
Iteration 15/25 | Loss: 0.00076948
Iteration 16/25 | Loss: 0.00076948
Iteration 17/25 | Loss: 0.00076948
Iteration 18/25 | Loss: 0.00076948
Iteration 19/25 | Loss: 0.00076948
Iteration 20/25 | Loss: 0.00076948
Iteration 21/25 | Loss: 0.00076948
Iteration 22/25 | Loss: 0.00076948
Iteration 23/25 | Loss: 0.00076948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007694822270423174, 0.0007694822270423174, 0.0007694822270423174, 0.0007694822270423174, 0.0007694822270423174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007694822270423174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076948
Iteration 2/1000 | Loss: 0.00004832
Iteration 3/1000 | Loss: 0.00013880
Iteration 4/1000 | Loss: 0.00007077
Iteration 5/1000 | Loss: 0.00005749
Iteration 6/1000 | Loss: 0.00002871
Iteration 7/1000 | Loss: 0.00002759
Iteration 8/1000 | Loss: 0.00002666
Iteration 9/1000 | Loss: 0.00002604
Iteration 10/1000 | Loss: 0.00002553
Iteration 11/1000 | Loss: 0.00002519
Iteration 12/1000 | Loss: 0.00002495
Iteration 13/1000 | Loss: 0.00002479
Iteration 14/1000 | Loss: 0.00002477
Iteration 15/1000 | Loss: 0.00002473
Iteration 16/1000 | Loss: 0.00002470
Iteration 17/1000 | Loss: 0.00002465
Iteration 18/1000 | Loss: 0.00002465
Iteration 19/1000 | Loss: 0.00002463
Iteration 20/1000 | Loss: 0.00002462
Iteration 21/1000 | Loss: 0.00002462
Iteration 22/1000 | Loss: 0.00002462
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002457
Iteration 27/1000 | Loss: 0.00002456
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002454
Iteration 30/1000 | Loss: 0.00002454
Iteration 31/1000 | Loss: 0.00002453
Iteration 32/1000 | Loss: 0.00002448
Iteration 33/1000 | Loss: 0.00002448
Iteration 34/1000 | Loss: 0.00002448
Iteration 35/1000 | Loss: 0.00002448
Iteration 36/1000 | Loss: 0.00002448
Iteration 37/1000 | Loss: 0.00002448
Iteration 38/1000 | Loss: 0.00002448
Iteration 39/1000 | Loss: 0.00002448
Iteration 40/1000 | Loss: 0.00002448
Iteration 41/1000 | Loss: 0.00002448
Iteration 42/1000 | Loss: 0.00002448
Iteration 43/1000 | Loss: 0.00002448
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002448
Iteration 47/1000 | Loss: 0.00002447
Iteration 48/1000 | Loss: 0.00002447
Iteration 49/1000 | Loss: 0.00002447
Iteration 50/1000 | Loss: 0.00002447
Iteration 51/1000 | Loss: 0.00002447
Iteration 52/1000 | Loss: 0.00002447
Iteration 53/1000 | Loss: 0.00002447
Iteration 54/1000 | Loss: 0.00002447
Iteration 55/1000 | Loss: 0.00002447
Iteration 56/1000 | Loss: 0.00002447
Iteration 57/1000 | Loss: 0.00002447
Iteration 58/1000 | Loss: 0.00002447
Iteration 59/1000 | Loss: 0.00002447
Iteration 60/1000 | Loss: 0.00002447
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002447
Iteration 64/1000 | Loss: 0.00002447
Iteration 65/1000 | Loss: 0.00002447
Iteration 66/1000 | Loss: 0.00002447
Iteration 67/1000 | Loss: 0.00002447
Iteration 68/1000 | Loss: 0.00002447
Iteration 69/1000 | Loss: 0.00002447
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002447
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002447
Iteration 78/1000 | Loss: 0.00002447
Iteration 79/1000 | Loss: 0.00002447
Iteration 80/1000 | Loss: 0.00002447
Iteration 81/1000 | Loss: 0.00002447
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00002447
Iteration 84/1000 | Loss: 0.00002447
Iteration 85/1000 | Loss: 0.00002447
Iteration 86/1000 | Loss: 0.00002447
Iteration 87/1000 | Loss: 0.00002447
Iteration 88/1000 | Loss: 0.00002447
Iteration 89/1000 | Loss: 0.00002447
Iteration 90/1000 | Loss: 0.00002447
Iteration 91/1000 | Loss: 0.00002447
Iteration 92/1000 | Loss: 0.00002447
Iteration 93/1000 | Loss: 0.00002447
Iteration 94/1000 | Loss: 0.00002447
Iteration 95/1000 | Loss: 0.00002447
Iteration 96/1000 | Loss: 0.00002447
Iteration 97/1000 | Loss: 0.00002447
Iteration 98/1000 | Loss: 0.00002447
Iteration 99/1000 | Loss: 0.00002447
Iteration 100/1000 | Loss: 0.00002447
Iteration 101/1000 | Loss: 0.00002447
Iteration 102/1000 | Loss: 0.00002447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.4467517505399883e-05, 2.4467517505399883e-05, 2.4467517505399883e-05, 2.4467517505399883e-05, 2.4467517505399883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4467517505399883e-05

Optimization complete. Final v2v error: 4.1062822341918945 mm

Highest mean error: 6.284419059753418 mm for frame 112

Lowest mean error: 3.5103917121887207 mm for frame 206

Saving results

Total time: 47.471696615219116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799678
Iteration 2/25 | Loss: 0.00128799
Iteration 3/25 | Loss: 0.00119211
Iteration 4/25 | Loss: 0.00118615
Iteration 5/25 | Loss: 0.00118416
Iteration 6/25 | Loss: 0.00118416
Iteration 7/25 | Loss: 0.00118416
Iteration 8/25 | Loss: 0.00118416
Iteration 9/25 | Loss: 0.00118416
Iteration 10/25 | Loss: 0.00118416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011841553496196866, 0.0011841553496196866, 0.0011841553496196866, 0.0011841553496196866, 0.0011841553496196866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011841553496196866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44526780
Iteration 2/25 | Loss: 0.00071436
Iteration 3/25 | Loss: 0.00071436
Iteration 4/25 | Loss: 0.00071436
Iteration 5/25 | Loss: 0.00071436
Iteration 6/25 | Loss: 0.00071435
Iteration 7/25 | Loss: 0.00071435
Iteration 8/25 | Loss: 0.00071435
Iteration 9/25 | Loss: 0.00071435
Iteration 10/25 | Loss: 0.00071435
Iteration 11/25 | Loss: 0.00071435
Iteration 12/25 | Loss: 0.00071435
Iteration 13/25 | Loss: 0.00071435
Iteration 14/25 | Loss: 0.00071435
Iteration 15/25 | Loss: 0.00071435
Iteration 16/25 | Loss: 0.00071435
Iteration 17/25 | Loss: 0.00071435
Iteration 18/25 | Loss: 0.00071435
Iteration 19/25 | Loss: 0.00071435
Iteration 20/25 | Loss: 0.00071435
Iteration 21/25 | Loss: 0.00071435
Iteration 22/25 | Loss: 0.00071435
Iteration 23/25 | Loss: 0.00071435
Iteration 24/25 | Loss: 0.00071435
Iteration 25/25 | Loss: 0.00071435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071435
Iteration 2/1000 | Loss: 0.00002534
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001236
Iteration 8/1000 | Loss: 0.00001205
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001136
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001131
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001130
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001129
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001127
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001124
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001123
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001122
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001122
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001118
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001113
Iteration 117/1000 | Loss: 0.00001113
Iteration 118/1000 | Loss: 0.00001113
Iteration 119/1000 | Loss: 0.00001113
Iteration 120/1000 | Loss: 0.00001113
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001112
Iteration 125/1000 | Loss: 0.00001112
Iteration 126/1000 | Loss: 0.00001112
Iteration 127/1000 | Loss: 0.00001112
Iteration 128/1000 | Loss: 0.00001112
Iteration 129/1000 | Loss: 0.00001111
Iteration 130/1000 | Loss: 0.00001111
Iteration 131/1000 | Loss: 0.00001111
Iteration 132/1000 | Loss: 0.00001110
Iteration 133/1000 | Loss: 0.00001110
Iteration 134/1000 | Loss: 0.00001110
Iteration 135/1000 | Loss: 0.00001110
Iteration 136/1000 | Loss: 0.00001110
Iteration 137/1000 | Loss: 0.00001110
Iteration 138/1000 | Loss: 0.00001110
Iteration 139/1000 | Loss: 0.00001110
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001108
Iteration 150/1000 | Loss: 0.00001108
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001106
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001105
Iteration 179/1000 | Loss: 0.00001105
Iteration 180/1000 | Loss: 0.00001105
Iteration 181/1000 | Loss: 0.00001105
Iteration 182/1000 | Loss: 0.00001105
Iteration 183/1000 | Loss: 0.00001105
Iteration 184/1000 | Loss: 0.00001105
Iteration 185/1000 | Loss: 0.00001105
Iteration 186/1000 | Loss: 0.00001104
Iteration 187/1000 | Loss: 0.00001104
Iteration 188/1000 | Loss: 0.00001104
Iteration 189/1000 | Loss: 0.00001104
Iteration 190/1000 | Loss: 0.00001104
Iteration 191/1000 | Loss: 0.00001104
Iteration 192/1000 | Loss: 0.00001104
Iteration 193/1000 | Loss: 0.00001104
Iteration 194/1000 | Loss: 0.00001104
Iteration 195/1000 | Loss: 0.00001103
Iteration 196/1000 | Loss: 0.00001103
Iteration 197/1000 | Loss: 0.00001103
Iteration 198/1000 | Loss: 0.00001103
Iteration 199/1000 | Loss: 0.00001103
Iteration 200/1000 | Loss: 0.00001103
Iteration 201/1000 | Loss: 0.00001103
Iteration 202/1000 | Loss: 0.00001103
Iteration 203/1000 | Loss: 0.00001103
Iteration 204/1000 | Loss: 0.00001103
Iteration 205/1000 | Loss: 0.00001102
Iteration 206/1000 | Loss: 0.00001102
Iteration 207/1000 | Loss: 0.00001102
Iteration 208/1000 | Loss: 0.00001102
Iteration 209/1000 | Loss: 0.00001102
Iteration 210/1000 | Loss: 0.00001102
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Iteration 219/1000 | Loss: 0.00001101
Iteration 220/1000 | Loss: 0.00001101
Iteration 221/1000 | Loss: 0.00001101
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001101
Iteration 225/1000 | Loss: 0.00001101
Iteration 226/1000 | Loss: 0.00001101
Iteration 227/1000 | Loss: 0.00001101
Iteration 228/1000 | Loss: 0.00001101
Iteration 229/1000 | Loss: 0.00001101
Iteration 230/1000 | Loss: 0.00001101
Iteration 231/1000 | Loss: 0.00001101
Iteration 232/1000 | Loss: 0.00001101
Iteration 233/1000 | Loss: 0.00001101
Iteration 234/1000 | Loss: 0.00001101
Iteration 235/1000 | Loss: 0.00001101
Iteration 236/1000 | Loss: 0.00001101
Iteration 237/1000 | Loss: 0.00001101
Iteration 238/1000 | Loss: 0.00001101
Iteration 239/1000 | Loss: 0.00001101
Iteration 240/1000 | Loss: 0.00001101
Iteration 241/1000 | Loss: 0.00001101
Iteration 242/1000 | Loss: 0.00001101
Iteration 243/1000 | Loss: 0.00001101
Iteration 244/1000 | Loss: 0.00001101
Iteration 245/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.1007955436070915e-05, 1.1007955436070915e-05, 1.1007955436070915e-05, 1.1007955436070915e-05, 1.1007955436070915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1007955436070915e-05

Optimization complete. Final v2v error: 2.828364610671997 mm

Highest mean error: 2.985473871231079 mm for frame 27

Lowest mean error: 2.6875803470611572 mm for frame 6

Saving results

Total time: 39.22564482688904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041986
Iteration 2/25 | Loss: 0.01041986
Iteration 3/25 | Loss: 0.01041985
Iteration 4/25 | Loss: 0.00280821
Iteration 5/25 | Loss: 0.00214287
Iteration 6/25 | Loss: 0.00215483
Iteration 7/25 | Loss: 0.00198999
Iteration 8/25 | Loss: 0.00166826
Iteration 9/25 | Loss: 0.00159047
Iteration 10/25 | Loss: 0.00150823
Iteration 11/25 | Loss: 0.00146545
Iteration 12/25 | Loss: 0.00146150
Iteration 13/25 | Loss: 0.00145061
Iteration 14/25 | Loss: 0.00142419
Iteration 15/25 | Loss: 0.00142182
Iteration 16/25 | Loss: 0.00141425
Iteration 17/25 | Loss: 0.00138969
Iteration 18/25 | Loss: 0.00136765
Iteration 19/25 | Loss: 0.00137017
Iteration 20/25 | Loss: 0.00136238
Iteration 21/25 | Loss: 0.00136318
Iteration 22/25 | Loss: 0.00135293
Iteration 23/25 | Loss: 0.00134681
Iteration 24/25 | Loss: 0.00133975
Iteration 25/25 | Loss: 0.00133594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47891891
Iteration 2/25 | Loss: 0.00194731
Iteration 3/25 | Loss: 0.00190425
Iteration 4/25 | Loss: 0.00190425
Iteration 5/25 | Loss: 0.00190425
Iteration 6/25 | Loss: 0.00190425
Iteration 7/25 | Loss: 0.00190425
Iteration 8/25 | Loss: 0.00190425
Iteration 9/25 | Loss: 0.00190425
Iteration 10/25 | Loss: 0.00190425
Iteration 11/25 | Loss: 0.00190425
Iteration 12/25 | Loss: 0.00190425
Iteration 13/25 | Loss: 0.00190425
Iteration 14/25 | Loss: 0.00190425
Iteration 15/25 | Loss: 0.00190425
Iteration 16/25 | Loss: 0.00190425
Iteration 17/25 | Loss: 0.00190425
Iteration 18/25 | Loss: 0.00190425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019042456988245249, 0.0019042456988245249, 0.0019042456988245249, 0.0019042456988245249, 0.0019042456988245249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019042456988245249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190425
Iteration 2/1000 | Loss: 0.00330595
Iteration 3/1000 | Loss: 0.00316551
Iteration 4/1000 | Loss: 0.00313184
Iteration 5/1000 | Loss: 0.00415407
Iteration 6/1000 | Loss: 0.00332924
Iteration 7/1000 | Loss: 0.00331117
Iteration 8/1000 | Loss: 0.00330037
Iteration 9/1000 | Loss: 0.00362623
Iteration 10/1000 | Loss: 0.00466524
Iteration 11/1000 | Loss: 0.00435990
Iteration 12/1000 | Loss: 0.00599791
Iteration 13/1000 | Loss: 0.00443329
Iteration 14/1000 | Loss: 0.00459575
Iteration 15/1000 | Loss: 0.00420475
Iteration 16/1000 | Loss: 0.00382938
Iteration 17/1000 | Loss: 0.00379392
Iteration 18/1000 | Loss: 0.00329023
Iteration 19/1000 | Loss: 0.00424860
Iteration 20/1000 | Loss: 0.00457604
Iteration 21/1000 | Loss: 0.00431369
Iteration 22/1000 | Loss: 0.00450871
Iteration 23/1000 | Loss: 0.00466792
Iteration 24/1000 | Loss: 0.00401569
Iteration 25/1000 | Loss: 0.00420643
Iteration 26/1000 | Loss: 0.00476894
Iteration 27/1000 | Loss: 0.00419767
Iteration 28/1000 | Loss: 0.00453962
Iteration 29/1000 | Loss: 0.00441314
Iteration 30/1000 | Loss: 0.00554332
Iteration 31/1000 | Loss: 0.00459249
Iteration 32/1000 | Loss: 0.00381819
Iteration 33/1000 | Loss: 0.00362417
Iteration 34/1000 | Loss: 0.00420137
Iteration 35/1000 | Loss: 0.00427368
Iteration 36/1000 | Loss: 0.00591746
Iteration 37/1000 | Loss: 0.00452070
Iteration 38/1000 | Loss: 0.00303611
Iteration 39/1000 | Loss: 0.00336359
Iteration 40/1000 | Loss: 0.00383678
Iteration 41/1000 | Loss: 0.00433415
Iteration 42/1000 | Loss: 0.00467258
Iteration 43/1000 | Loss: 0.00461596
Iteration 44/1000 | Loss: 0.00420174
Iteration 45/1000 | Loss: 0.00427215
Iteration 46/1000 | Loss: 0.00371635
Iteration 47/1000 | Loss: 0.00392795
Iteration 48/1000 | Loss: 0.00372024
Iteration 49/1000 | Loss: 0.00350059
Iteration 50/1000 | Loss: 0.00391259
Iteration 51/1000 | Loss: 0.00366338
Iteration 52/1000 | Loss: 0.00386466
Iteration 53/1000 | Loss: 0.00421796
Iteration 54/1000 | Loss: 0.00421722
Iteration 55/1000 | Loss: 0.00394993
Iteration 56/1000 | Loss: 0.00412435
Iteration 57/1000 | Loss: 0.00394556
Iteration 58/1000 | Loss: 0.00480687
Iteration 59/1000 | Loss: 0.00415958
Iteration 60/1000 | Loss: 0.00457204
Iteration 61/1000 | Loss: 0.00424163
Iteration 62/1000 | Loss: 0.00522282
Iteration 63/1000 | Loss: 0.00385290
Iteration 64/1000 | Loss: 0.00435159
Iteration 65/1000 | Loss: 0.00531593
Iteration 66/1000 | Loss: 0.00484950
Iteration 67/1000 | Loss: 0.00416179
Iteration 68/1000 | Loss: 0.00462669
Iteration 69/1000 | Loss: 0.00563601
Iteration 70/1000 | Loss: 0.00381044
Iteration 71/1000 | Loss: 0.00316473
Iteration 72/1000 | Loss: 0.00451403
Iteration 73/1000 | Loss: 0.00356800
Iteration 74/1000 | Loss: 0.00565263
Iteration 75/1000 | Loss: 0.00497698
Iteration 76/1000 | Loss: 0.00599806
Iteration 77/1000 | Loss: 0.00421085
Iteration 78/1000 | Loss: 0.00419262
Iteration 79/1000 | Loss: 0.00426958
Iteration 80/1000 | Loss: 0.00272653
Iteration 81/1000 | Loss: 0.00309721
Iteration 82/1000 | Loss: 0.00328496
Iteration 83/1000 | Loss: 0.00336572
Iteration 84/1000 | Loss: 0.00267582
Iteration 85/1000 | Loss: 0.00336527
Iteration 86/1000 | Loss: 0.00296651
Iteration 87/1000 | Loss: 0.00242868
Iteration 88/1000 | Loss: 0.00239319
Iteration 89/1000 | Loss: 0.00295386
Iteration 90/1000 | Loss: 0.00287759
Iteration 91/1000 | Loss: 0.00256608
Iteration 92/1000 | Loss: 0.00285690
Iteration 93/1000 | Loss: 0.00319113
Iteration 94/1000 | Loss: 0.00300173
Iteration 95/1000 | Loss: 0.00314466
Iteration 96/1000 | Loss: 0.00264856
Iteration 97/1000 | Loss: 0.00264636
Iteration 98/1000 | Loss: 0.00302086
Iteration 99/1000 | Loss: 0.00249745
Iteration 100/1000 | Loss: 0.00295059
Iteration 101/1000 | Loss: 0.00224423
Iteration 102/1000 | Loss: 0.00315806
Iteration 103/1000 | Loss: 0.00220072
Iteration 104/1000 | Loss: 0.00334414
Iteration 105/1000 | Loss: 0.00274199
Iteration 106/1000 | Loss: 0.00280322
Iteration 107/1000 | Loss: 0.00306955
Iteration 108/1000 | Loss: 0.00355873
Iteration 109/1000 | Loss: 0.00352860
Iteration 110/1000 | Loss: 0.00328136
Iteration 111/1000 | Loss: 0.00271933
Iteration 112/1000 | Loss: 0.00252791
Iteration 113/1000 | Loss: 0.00271630
Iteration 114/1000 | Loss: 0.00248724
Iteration 115/1000 | Loss: 0.00251854
Iteration 116/1000 | Loss: 0.00252054
Iteration 117/1000 | Loss: 0.00287184
Iteration 118/1000 | Loss: 0.00303934
Iteration 119/1000 | Loss: 0.00288174
Iteration 120/1000 | Loss: 0.00296937
Iteration 121/1000 | Loss: 0.00276003
Iteration 122/1000 | Loss: 0.00474276
Iteration 123/1000 | Loss: 0.00319349
Iteration 124/1000 | Loss: 0.00291554
Iteration 125/1000 | Loss: 0.00295333
Iteration 126/1000 | Loss: 0.00316692
Iteration 127/1000 | Loss: 0.00267487
Iteration 128/1000 | Loss: 0.00303629
Iteration 129/1000 | Loss: 0.00214827
Iteration 130/1000 | Loss: 0.00259044
Iteration 131/1000 | Loss: 0.00222754
Iteration 132/1000 | Loss: 0.00221525
Iteration 133/1000 | Loss: 0.00310723
Iteration 134/1000 | Loss: 0.00238511
Iteration 135/1000 | Loss: 0.00147145
Iteration 136/1000 | Loss: 0.00218149
Iteration 137/1000 | Loss: 0.00288268
Iteration 138/1000 | Loss: 0.00286044
Iteration 139/1000 | Loss: 0.00289040
Iteration 140/1000 | Loss: 0.00331331
Iteration 141/1000 | Loss: 0.00269914
Iteration 142/1000 | Loss: 0.00256256
Iteration 143/1000 | Loss: 0.00304556
Iteration 144/1000 | Loss: 0.00284726
Iteration 145/1000 | Loss: 0.00206341
Iteration 146/1000 | Loss: 0.00226410
Iteration 147/1000 | Loss: 0.00266639
Iteration 148/1000 | Loss: 0.00408612
Iteration 149/1000 | Loss: 0.00349834
Iteration 150/1000 | Loss: 0.00245145
Iteration 151/1000 | Loss: 0.00188342
Iteration 152/1000 | Loss: 0.00269169
Iteration 153/1000 | Loss: 0.00285897
Iteration 154/1000 | Loss: 0.00270325
Iteration 155/1000 | Loss: 0.00243634
Iteration 156/1000 | Loss: 0.00214643
Iteration 157/1000 | Loss: 0.00223352
Iteration 158/1000 | Loss: 0.00224260
Iteration 159/1000 | Loss: 0.00274360
Iteration 160/1000 | Loss: 0.00254710
Iteration 161/1000 | Loss: 0.00282928
Iteration 162/1000 | Loss: 0.00274690
Iteration 163/1000 | Loss: 0.00273080
Iteration 164/1000 | Loss: 0.00236559
Iteration 165/1000 | Loss: 0.00299330
Iteration 166/1000 | Loss: 0.00281951
Iteration 167/1000 | Loss: 0.00216799
Iteration 168/1000 | Loss: 0.00371446
Iteration 169/1000 | Loss: 0.00281405
Iteration 170/1000 | Loss: 0.00263159
Iteration 171/1000 | Loss: 0.00237216
Iteration 172/1000 | Loss: 0.00299661
Iteration 173/1000 | Loss: 0.00273366
Iteration 174/1000 | Loss: 0.00247945
Iteration 175/1000 | Loss: 0.00376362
Iteration 176/1000 | Loss: 0.00248631
Iteration 177/1000 | Loss: 0.00324263
Iteration 178/1000 | Loss: 0.00282112
Iteration 179/1000 | Loss: 0.00299249
Iteration 180/1000 | Loss: 0.00312692
Iteration 181/1000 | Loss: 0.00275171
Iteration 182/1000 | Loss: 0.00299192
Iteration 183/1000 | Loss: 0.00263382
Iteration 184/1000 | Loss: 0.00274258
Iteration 185/1000 | Loss: 0.00462690
Iteration 186/1000 | Loss: 0.00241187
Iteration 187/1000 | Loss: 0.00235815
Iteration 188/1000 | Loss: 0.00288934
Iteration 189/1000 | Loss: 0.00239896
Iteration 190/1000 | Loss: 0.00262085
Iteration 191/1000 | Loss: 0.00345247
Iteration 192/1000 | Loss: 0.00363025
Iteration 193/1000 | Loss: 0.00372458
Iteration 194/1000 | Loss: 0.00261650
Iteration 195/1000 | Loss: 0.00297690
Iteration 196/1000 | Loss: 0.00330682
Iteration 197/1000 | Loss: 0.00241953
Iteration 198/1000 | Loss: 0.00350982
Iteration 199/1000 | Loss: 0.00252443
Iteration 200/1000 | Loss: 0.00298915
Iteration 201/1000 | Loss: 0.00313080
Iteration 202/1000 | Loss: 0.00339288
Iteration 203/1000 | Loss: 0.00293561
Iteration 204/1000 | Loss: 0.00268183
Iteration 205/1000 | Loss: 0.00240669
Iteration 206/1000 | Loss: 0.00223360
Iteration 207/1000 | Loss: 0.00260404
Iteration 208/1000 | Loss: 0.00263288
Iteration 209/1000 | Loss: 0.00256087
Iteration 210/1000 | Loss: 0.00328494
Iteration 211/1000 | Loss: 0.00252099
Iteration 212/1000 | Loss: 0.00228636
Iteration 213/1000 | Loss: 0.00256763
Iteration 214/1000 | Loss: 0.00228545
Iteration 215/1000 | Loss: 0.00233463
Iteration 216/1000 | Loss: 0.00351151
Iteration 217/1000 | Loss: 0.00256157
Iteration 218/1000 | Loss: 0.00276927
Iteration 219/1000 | Loss: 0.00243818
Iteration 220/1000 | Loss: 0.00205316
Iteration 221/1000 | Loss: 0.00225411
Iteration 222/1000 | Loss: 0.00275099
Iteration 223/1000 | Loss: 0.00236501
Iteration 224/1000 | Loss: 0.00244877
Iteration 225/1000 | Loss: 0.00223909
Iteration 226/1000 | Loss: 0.00204906
Iteration 227/1000 | Loss: 0.00223492
Iteration 228/1000 | Loss: 0.00229714
Iteration 229/1000 | Loss: 0.00315967
Iteration 230/1000 | Loss: 0.00367453
Iteration 231/1000 | Loss: 0.00276210
Iteration 232/1000 | Loss: 0.00299642
Iteration 233/1000 | Loss: 0.00200637
Iteration 234/1000 | Loss: 0.00189244
Iteration 235/1000 | Loss: 0.00117629
Iteration 236/1000 | Loss: 0.00186374
Iteration 237/1000 | Loss: 0.00195000
Iteration 238/1000 | Loss: 0.00203677
Iteration 239/1000 | Loss: 0.00227172
Iteration 240/1000 | Loss: 0.00199580
Iteration 241/1000 | Loss: 0.00219573
Iteration 242/1000 | Loss: 0.00243044
Iteration 243/1000 | Loss: 0.00248246
Iteration 244/1000 | Loss: 0.00228161
Iteration 245/1000 | Loss: 0.00238579
Iteration 246/1000 | Loss: 0.00301899
Iteration 247/1000 | Loss: 0.00232472
Iteration 248/1000 | Loss: 0.00255242
Iteration 249/1000 | Loss: 0.00271234
Iteration 250/1000 | Loss: 0.00210870
Iteration 251/1000 | Loss: 0.00213379
Iteration 252/1000 | Loss: 0.00180119
Iteration 253/1000 | Loss: 0.00232038
Iteration 254/1000 | Loss: 0.00231932
Iteration 255/1000 | Loss: 0.00242469
Iteration 256/1000 | Loss: 0.00173693
Iteration 257/1000 | Loss: 0.00192764
Iteration 258/1000 | Loss: 0.00183182
Iteration 259/1000 | Loss: 0.00235625
Iteration 260/1000 | Loss: 0.00211669
Iteration 261/1000 | Loss: 0.00268210
Iteration 262/1000 | Loss: 0.00253220
Iteration 263/1000 | Loss: 0.00288803
Iteration 264/1000 | Loss: 0.00182915
Iteration 265/1000 | Loss: 0.00211630
Iteration 266/1000 | Loss: 0.00253912
Iteration 267/1000 | Loss: 0.00228438
Iteration 268/1000 | Loss: 0.00312754
Iteration 269/1000 | Loss: 0.00344359
Iteration 270/1000 | Loss: 0.00298415
Iteration 271/1000 | Loss: 0.00242927
Iteration 272/1000 | Loss: 0.00179586
Iteration 273/1000 | Loss: 0.00270249
Iteration 274/1000 | Loss: 0.00330423
Iteration 275/1000 | Loss: 0.00359597
Iteration 276/1000 | Loss: 0.00152929
Iteration 277/1000 | Loss: 0.00196896
Iteration 278/1000 | Loss: 0.00204849
Iteration 279/1000 | Loss: 0.00191544
Iteration 280/1000 | Loss: 0.00199065
Iteration 281/1000 | Loss: 0.00165581
Iteration 282/1000 | Loss: 0.00119368
Iteration 283/1000 | Loss: 0.00151516
Iteration 284/1000 | Loss: 0.00165219
Iteration 285/1000 | Loss: 0.00165992
Iteration 286/1000 | Loss: 0.00196161
Iteration 287/1000 | Loss: 0.00162620
Iteration 288/1000 | Loss: 0.00138654
Iteration 289/1000 | Loss: 0.00138960
Iteration 290/1000 | Loss: 0.00178637
Iteration 291/1000 | Loss: 0.00218476
Iteration 292/1000 | Loss: 0.00322544
Iteration 293/1000 | Loss: 0.00130437
Iteration 294/1000 | Loss: 0.00125211
Iteration 295/1000 | Loss: 0.00138332
Iteration 296/1000 | Loss: 0.00159586
Iteration 297/1000 | Loss: 0.00178310
Iteration 298/1000 | Loss: 0.00201508
Iteration 299/1000 | Loss: 0.00208996
Iteration 300/1000 | Loss: 0.00380673
Iteration 301/1000 | Loss: 0.00238701
Iteration 302/1000 | Loss: 0.00163909
Iteration 303/1000 | Loss: 0.00117179
Iteration 304/1000 | Loss: 0.00154726
Iteration 305/1000 | Loss: 0.00121100
Iteration 306/1000 | Loss: 0.00100439
Iteration 307/1000 | Loss: 0.00124110
Iteration 308/1000 | Loss: 0.00142939
Iteration 309/1000 | Loss: 0.00121176
Iteration 310/1000 | Loss: 0.00116459
Iteration 311/1000 | Loss: 0.00169938
Iteration 312/1000 | Loss: 0.00141188
Iteration 313/1000 | Loss: 0.00144520
Iteration 314/1000 | Loss: 0.00189819
Iteration 315/1000 | Loss: 0.00365459
Iteration 316/1000 | Loss: 0.00152188
Iteration 317/1000 | Loss: 0.00139366
Iteration 318/1000 | Loss: 0.00143223
Iteration 319/1000 | Loss: 0.00152167
Iteration 320/1000 | Loss: 0.00122083
Iteration 321/1000 | Loss: 0.00144439
Iteration 322/1000 | Loss: 0.00139667
Iteration 323/1000 | Loss: 0.00109843
Iteration 324/1000 | Loss: 0.00109838
Iteration 325/1000 | Loss: 0.00148650
Iteration 326/1000 | Loss: 0.00149783
Iteration 327/1000 | Loss: 0.00153947
Iteration 328/1000 | Loss: 0.00158966
Iteration 329/1000 | Loss: 0.00150629
Iteration 330/1000 | Loss: 0.00148082
Iteration 331/1000 | Loss: 0.00140477
Iteration 332/1000 | Loss: 0.00107800
Iteration 333/1000 | Loss: 0.00108731
Iteration 334/1000 | Loss: 0.00100269
Iteration 335/1000 | Loss: 0.00108128
Iteration 336/1000 | Loss: 0.00117498
Iteration 337/1000 | Loss: 0.00116702
Iteration 338/1000 | Loss: 0.00106213
Iteration 339/1000 | Loss: 0.00124418
Iteration 340/1000 | Loss: 0.00131777
Iteration 341/1000 | Loss: 0.00120686
Iteration 342/1000 | Loss: 0.00148298
Iteration 343/1000 | Loss: 0.00153922
Iteration 344/1000 | Loss: 0.00148633
Iteration 345/1000 | Loss: 0.00146427
Iteration 346/1000 | Loss: 0.00208509
Iteration 347/1000 | Loss: 0.00131696
Iteration 348/1000 | Loss: 0.00135429
Iteration 349/1000 | Loss: 0.00145473
Iteration 350/1000 | Loss: 0.00122929
Iteration 351/1000 | Loss: 0.00121191
Iteration 352/1000 | Loss: 0.00138455
Iteration 353/1000 | Loss: 0.00152823
Iteration 354/1000 | Loss: 0.00136077
Iteration 355/1000 | Loss: 0.00109310
Iteration 356/1000 | Loss: 0.00120162
Iteration 357/1000 | Loss: 0.00119232
Iteration 358/1000 | Loss: 0.00241559
Iteration 359/1000 | Loss: 0.00162860
Iteration 360/1000 | Loss: 0.00144091
Iteration 361/1000 | Loss: 0.00138377
Iteration 362/1000 | Loss: 0.00145202
Iteration 363/1000 | Loss: 0.00129535
Iteration 364/1000 | Loss: 0.00122248
Iteration 365/1000 | Loss: 0.00116086
Iteration 366/1000 | Loss: 0.00165330
Iteration 367/1000 | Loss: 0.00123538
Iteration 368/1000 | Loss: 0.00110314
Iteration 369/1000 | Loss: 0.00112082
Iteration 370/1000 | Loss: 0.00107794
Iteration 371/1000 | Loss: 0.00177069
Iteration 372/1000 | Loss: 0.00193466
Iteration 373/1000 | Loss: 0.00136804
Iteration 374/1000 | Loss: 0.00095044
Iteration 375/1000 | Loss: 0.00113674
Iteration 376/1000 | Loss: 0.00155327
Iteration 377/1000 | Loss: 0.00145900
Iteration 378/1000 | Loss: 0.00106843
Iteration 379/1000 | Loss: 0.00151634
Iteration 380/1000 | Loss: 0.00115601
Iteration 381/1000 | Loss: 0.00134654
Iteration 382/1000 | Loss: 0.00108196
Iteration 383/1000 | Loss: 0.00120621
Iteration 384/1000 | Loss: 0.00075699
Iteration 385/1000 | Loss: 0.00083428
Iteration 386/1000 | Loss: 0.00098227
Iteration 387/1000 | Loss: 0.00116837
Iteration 388/1000 | Loss: 0.00145377
Iteration 389/1000 | Loss: 0.00171638
Iteration 390/1000 | Loss: 0.00134325
Iteration 391/1000 | Loss: 0.00158912
Iteration 392/1000 | Loss: 0.00123732
Iteration 393/1000 | Loss: 0.00147151
Iteration 394/1000 | Loss: 0.00127549
Iteration 395/1000 | Loss: 0.00100620
Iteration 396/1000 | Loss: 0.00152042
Iteration 397/1000 | Loss: 0.00113540
Iteration 398/1000 | Loss: 0.00117260
Iteration 399/1000 | Loss: 0.00131143
Iteration 400/1000 | Loss: 0.00125090
Iteration 401/1000 | Loss: 0.00172168
Iteration 402/1000 | Loss: 0.00133435
Iteration 403/1000 | Loss: 0.00137897
Iteration 404/1000 | Loss: 0.00134947
Iteration 405/1000 | Loss: 0.00126253
Iteration 406/1000 | Loss: 0.00128541
Iteration 407/1000 | Loss: 0.00134278
Iteration 408/1000 | Loss: 0.00124940
Iteration 409/1000 | Loss: 0.00138788
Iteration 410/1000 | Loss: 0.00136495
Iteration 411/1000 | Loss: 0.00160936
Iteration 412/1000 | Loss: 0.00146911
Iteration 413/1000 | Loss: 0.00134550
Iteration 414/1000 | Loss: 0.00127253
Iteration 415/1000 | Loss: 0.00143340
Iteration 416/1000 | Loss: 0.00127205
Iteration 417/1000 | Loss: 0.00103914
Iteration 418/1000 | Loss: 0.00137474
Iteration 419/1000 | Loss: 0.00104927
Iteration 420/1000 | Loss: 0.00109968
Iteration 421/1000 | Loss: 0.00142869
Iteration 422/1000 | Loss: 0.00147194
Iteration 423/1000 | Loss: 0.00121053
Iteration 424/1000 | Loss: 0.00110944
Iteration 425/1000 | Loss: 0.00134838
Iteration 426/1000 | Loss: 0.00113262
Iteration 427/1000 | Loss: 0.00077664
Iteration 428/1000 | Loss: 0.00096576
Iteration 429/1000 | Loss: 0.00098100
Iteration 430/1000 | Loss: 0.00198222
Iteration 431/1000 | Loss: 0.00056539
Iteration 432/1000 | Loss: 0.00099682
Iteration 433/1000 | Loss: 0.00077166
Iteration 434/1000 | Loss: 0.00100480
Iteration 435/1000 | Loss: 0.00112975
Iteration 436/1000 | Loss: 0.00079173
Iteration 437/1000 | Loss: 0.00077416
Iteration 438/1000 | Loss: 0.00105386
Iteration 439/1000 | Loss: 0.00085162
Iteration 440/1000 | Loss: 0.00092559
Iteration 441/1000 | Loss: 0.00112234
Iteration 442/1000 | Loss: 0.00128292
Iteration 443/1000 | Loss: 0.00108978
Iteration 444/1000 | Loss: 0.00104482
Iteration 445/1000 | Loss: 0.00112499
Iteration 446/1000 | Loss: 0.00074548
Iteration 447/1000 | Loss: 0.00064137
Iteration 448/1000 | Loss: 0.00060473
Iteration 449/1000 | Loss: 0.00083628
Iteration 450/1000 | Loss: 0.00083543
Iteration 451/1000 | Loss: 0.00093783
Iteration 452/1000 | Loss: 0.00097130
Iteration 453/1000 | Loss: 0.00095995
Iteration 454/1000 | Loss: 0.00128452
Iteration 455/1000 | Loss: 0.00139433
Iteration 456/1000 | Loss: 0.00090367
Iteration 457/1000 | Loss: 0.00076991
Iteration 458/1000 | Loss: 0.00080232
Iteration 459/1000 | Loss: 0.00083279
Iteration 460/1000 | Loss: 0.00098531
Iteration 461/1000 | Loss: 0.00096529
Iteration 462/1000 | Loss: 0.00184596
Iteration 463/1000 | Loss: 0.00109939
Iteration 464/1000 | Loss: 0.00177691
Iteration 465/1000 | Loss: 0.00145696
Iteration 466/1000 | Loss: 0.00154562
Iteration 467/1000 | Loss: 0.00108282
Iteration 468/1000 | Loss: 0.00087449
Iteration 469/1000 | Loss: 0.00095426
Iteration 470/1000 | Loss: 0.00060341
Iteration 471/1000 | Loss: 0.00089658
Iteration 472/1000 | Loss: 0.00153004
Iteration 473/1000 | Loss: 0.00112947
Iteration 474/1000 | Loss: 0.00089190
Iteration 475/1000 | Loss: 0.00085005
Iteration 476/1000 | Loss: 0.00149131
Iteration 477/1000 | Loss: 0.00163816
Iteration 478/1000 | Loss: 0.00111261
Iteration 479/1000 | Loss: 0.00093691
Iteration 480/1000 | Loss: 0.00087551
Iteration 481/1000 | Loss: 0.00119967
Iteration 482/1000 | Loss: 0.00101650
Iteration 483/1000 | Loss: 0.00096181
Iteration 484/1000 | Loss: 0.00097398
Iteration 485/1000 | Loss: 0.00098671
Iteration 486/1000 | Loss: 0.00108557
Iteration 487/1000 | Loss: 0.00094440
Iteration 488/1000 | Loss: 0.00117291
Iteration 489/1000 | Loss: 0.00098636
Iteration 490/1000 | Loss: 0.00106664
Iteration 491/1000 | Loss: 0.00104282
Iteration 492/1000 | Loss: 0.00106306
Iteration 493/1000 | Loss: 0.00101152
Iteration 494/1000 | Loss: 0.00097767
Iteration 495/1000 | Loss: 0.00099147
Iteration 496/1000 | Loss: 0.00109389
Iteration 497/1000 | Loss: 0.00098846
Iteration 498/1000 | Loss: 0.00099244
Iteration 499/1000 | Loss: 0.00107924
Iteration 500/1000 | Loss: 0.00085519
Iteration 501/1000 | Loss: 0.00095290
Iteration 502/1000 | Loss: 0.00080548
Iteration 503/1000 | Loss: 0.00091480
Iteration 504/1000 | Loss: 0.00083377
Iteration 505/1000 | Loss: 0.00097508
Iteration 506/1000 | Loss: 0.00095439
Iteration 507/1000 | Loss: 0.00093070
Iteration 508/1000 | Loss: 0.00105700
Iteration 509/1000 | Loss: 0.00088759
Iteration 510/1000 | Loss: 0.00095597
Iteration 511/1000 | Loss: 0.00129348
Iteration 512/1000 | Loss: 0.00092333
Iteration 513/1000 | Loss: 0.00105804
Iteration 514/1000 | Loss: 0.00097125
Iteration 515/1000 | Loss: 0.00132452
Iteration 516/1000 | Loss: 0.00095981
Iteration 517/1000 | Loss: 0.00113649
Iteration 518/1000 | Loss: 0.00088368
Iteration 519/1000 | Loss: 0.00095009
Iteration 520/1000 | Loss: 0.00133070
Iteration 521/1000 | Loss: 0.00088528
Iteration 522/1000 | Loss: 0.00087306
Iteration 523/1000 | Loss: 0.00110767
Iteration 524/1000 | Loss: 0.00100230
Iteration 525/1000 | Loss: 0.00129798
Iteration 526/1000 | Loss: 0.00139577
Iteration 527/1000 | Loss: 0.00091384
Iteration 528/1000 | Loss: 0.00129254
Iteration 529/1000 | Loss: 0.00096645
Iteration 530/1000 | Loss: 0.00099882
Iteration 531/1000 | Loss: 0.00110775
Iteration 532/1000 | Loss: 0.00101654
Iteration 533/1000 | Loss: 0.00096230
Iteration 534/1000 | Loss: 0.00099503
Iteration 535/1000 | Loss: 0.00107025
Iteration 536/1000 | Loss: 0.00077975
Iteration 537/1000 | Loss: 0.00066949
Iteration 538/1000 | Loss: 0.00064306
Iteration 539/1000 | Loss: 0.00090845
Iteration 540/1000 | Loss: 0.00436025
Iteration 541/1000 | Loss: 0.00097292
Iteration 542/1000 | Loss: 0.00091885
Iteration 543/1000 | Loss: 0.00094782
Iteration 544/1000 | Loss: 0.00104007
Iteration 545/1000 | Loss: 0.00100870
Iteration 546/1000 | Loss: 0.00120464
Iteration 547/1000 | Loss: 0.00101396
Iteration 548/1000 | Loss: 0.00109429
Iteration 549/1000 | Loss: 0.00103405
Iteration 550/1000 | Loss: 0.00103133
Iteration 551/1000 | Loss: 0.00094976
Iteration 552/1000 | Loss: 0.00103961
Iteration 553/1000 | Loss: 0.00086594
Iteration 554/1000 | Loss: 0.00150417
Iteration 555/1000 | Loss: 0.00106489
Iteration 556/1000 | Loss: 0.00099303
Iteration 557/1000 | Loss: 0.00115461
Iteration 558/1000 | Loss: 0.00086978
Iteration 559/1000 | Loss: 0.00108895
Iteration 560/1000 | Loss: 0.00177613
Iteration 561/1000 | Loss: 0.00213748
Iteration 562/1000 | Loss: 0.00157450
Iteration 563/1000 | Loss: 0.00121398
Iteration 564/1000 | Loss: 0.00125298
Iteration 565/1000 | Loss: 0.00148587
Iteration 566/1000 | Loss: 0.00126061
Iteration 567/1000 | Loss: 0.00132439
Iteration 568/1000 | Loss: 0.00245623
Iteration 569/1000 | Loss: 0.00179326
Iteration 570/1000 | Loss: 0.00296728
Iteration 571/1000 | Loss: 0.00180300
Iteration 572/1000 | Loss: 0.00221599
Iteration 573/1000 | Loss: 0.00125548
Iteration 574/1000 | Loss: 0.00169367
Iteration 575/1000 | Loss: 0.00079740
Iteration 576/1000 | Loss: 0.00086635
Iteration 577/1000 | Loss: 0.00094672
Iteration 578/1000 | Loss: 0.00109806
Iteration 579/1000 | Loss: 0.00110876
Iteration 580/1000 | Loss: 0.00089554
Iteration 581/1000 | Loss: 0.00073646
Iteration 582/1000 | Loss: 0.00093344
Iteration 583/1000 | Loss: 0.00105378
Iteration 584/1000 | Loss: 0.00079372
Iteration 585/1000 | Loss: 0.00076552
Iteration 586/1000 | Loss: 0.00101064
Iteration 587/1000 | Loss: 0.00122273
Iteration 588/1000 | Loss: 0.00101344
Iteration 589/1000 | Loss: 0.00086762
Iteration 590/1000 | Loss: 0.00100744
Iteration 591/1000 | Loss: 0.00062628
Iteration 592/1000 | Loss: 0.00092929
Iteration 593/1000 | Loss: 0.00090944
Iteration 594/1000 | Loss: 0.00097968
Iteration 595/1000 | Loss: 0.00091736
Iteration 596/1000 | Loss: 0.00092428
Iteration 597/1000 | Loss: 0.00112709
Iteration 598/1000 | Loss: 0.00101900
Iteration 599/1000 | Loss: 0.00096496
Iteration 600/1000 | Loss: 0.00119814
Iteration 601/1000 | Loss: 0.00112343
Iteration 602/1000 | Loss: 0.00102238
Iteration 603/1000 | Loss: 0.00096068
Iteration 604/1000 | Loss: 0.00099007
Iteration 605/1000 | Loss: 0.00092902
Iteration 606/1000 | Loss: 0.00089349
Iteration 607/1000 | Loss: 0.00094044
Iteration 608/1000 | Loss: 0.00114047
Iteration 609/1000 | Loss: 0.00103020
Iteration 610/1000 | Loss: 0.00111833
Iteration 611/1000 | Loss: 0.00105694
Iteration 612/1000 | Loss: 0.00105609
Iteration 613/1000 | Loss: 0.00104320
Iteration 614/1000 | Loss: 0.00136349
Iteration 615/1000 | Loss: 0.00094354
Iteration 616/1000 | Loss: 0.00103225
Iteration 617/1000 | Loss: 0.00102657
Iteration 618/1000 | Loss: 0.00090064
Iteration 619/1000 | Loss: 0.00089099
Iteration 620/1000 | Loss: 0.00120976
Iteration 621/1000 | Loss: 0.00099409
Iteration 622/1000 | Loss: 0.00101201
Iteration 623/1000 | Loss: 0.00099505
Iteration 624/1000 | Loss: 0.00106292
Iteration 625/1000 | Loss: 0.00129256
Iteration 626/1000 | Loss: 0.00113564
Iteration 627/1000 | Loss: 0.00134123
Iteration 628/1000 | Loss: 0.00126246
Iteration 629/1000 | Loss: 0.00128661
Iteration 630/1000 | Loss: 0.00055494
Iteration 631/1000 | Loss: 0.00062879
Iteration 632/1000 | Loss: 0.00070449
Iteration 633/1000 | Loss: 0.00076389
Iteration 634/1000 | Loss: 0.00112756
Iteration 635/1000 | Loss: 0.00100902
Iteration 636/1000 | Loss: 0.00105209
Iteration 637/1000 | Loss: 0.00097277
Iteration 638/1000 | Loss: 0.00102021
Iteration 639/1000 | Loss: 0.00159374
Iteration 640/1000 | Loss: 0.00103886
Iteration 641/1000 | Loss: 0.00083357
Iteration 642/1000 | Loss: 0.00099637
Iteration 643/1000 | Loss: 0.00105747
Iteration 644/1000 | Loss: 0.00056449
Iteration 645/1000 | Loss: 0.00073980
Iteration 646/1000 | Loss: 0.00047099
Iteration 647/1000 | Loss: 0.00054379
Iteration 648/1000 | Loss: 0.00067357
Iteration 649/1000 | Loss: 0.00132149
Iteration 650/1000 | Loss: 0.00076510
Iteration 651/1000 | Loss: 0.00106664
Iteration 652/1000 | Loss: 0.00111939
Iteration 653/1000 | Loss: 0.00074844
Iteration 654/1000 | Loss: 0.00092459
Iteration 655/1000 | Loss: 0.00089747
Iteration 656/1000 | Loss: 0.00082817
Iteration 657/1000 | Loss: 0.00128573
Iteration 658/1000 | Loss: 0.00093168
Iteration 659/1000 | Loss: 0.00080609
Iteration 660/1000 | Loss: 0.00082120
Iteration 661/1000 | Loss: 0.00123276
Iteration 662/1000 | Loss: 0.00096585
Iteration 663/1000 | Loss: 0.00086515
Iteration 664/1000 | Loss: 0.00090823
Iteration 665/1000 | Loss: 0.00101765
Iteration 666/1000 | Loss: 0.00097709
Iteration 667/1000 | Loss: 0.00153279
Iteration 668/1000 | Loss: 0.00095315
Iteration 669/1000 | Loss: 0.00132237
Iteration 670/1000 | Loss: 0.00104922
Iteration 671/1000 | Loss: 0.00102248
Iteration 672/1000 | Loss: 0.00092503
Iteration 673/1000 | Loss: 0.00120941
Iteration 674/1000 | Loss: 0.00155334
Iteration 675/1000 | Loss: 0.00122504
Iteration 676/1000 | Loss: 0.00104157
Iteration 677/1000 | Loss: 0.00088448
Iteration 678/1000 | Loss: 0.00101716
Iteration 679/1000 | Loss: 0.00086359
Iteration 680/1000 | Loss: 0.00085407
Iteration 681/1000 | Loss: 0.00121382
Iteration 682/1000 | Loss: 0.00091616
Iteration 683/1000 | Loss: 0.00078554
Iteration 684/1000 | Loss: 0.00180992
Iteration 685/1000 | Loss: 0.00112981
Iteration 686/1000 | Loss: 0.00092126
Iteration 687/1000 | Loss: 0.00095071
Iteration 688/1000 | Loss: 0.00084821
Iteration 689/1000 | Loss: 0.00061813
Iteration 690/1000 | Loss: 0.00077610
Iteration 691/1000 | Loss: 0.00070180
Iteration 692/1000 | Loss: 0.00116890
Iteration 693/1000 | Loss: 0.00095248
Iteration 694/1000 | Loss: 0.00163443
Iteration 695/1000 | Loss: 0.00106139
Iteration 696/1000 | Loss: 0.00130868
Iteration 697/1000 | Loss: 0.00134453
Iteration 698/1000 | Loss: 0.00124253
Iteration 699/1000 | Loss: 0.00121479
Iteration 700/1000 | Loss: 0.00128962
Iteration 701/1000 | Loss: 0.00101921
Iteration 702/1000 | Loss: 0.00093221
Iteration 703/1000 | Loss: 0.00080961
Iteration 704/1000 | Loss: 0.00088679
Iteration 705/1000 | Loss: 0.00073975
Iteration 706/1000 | Loss: 0.00093068
Iteration 707/1000 | Loss: 0.00087456
Iteration 708/1000 | Loss: 0.00092392
Iteration 709/1000 | Loss: 0.00112583
Iteration 710/1000 | Loss: 0.00095346
Iteration 711/1000 | Loss: 0.00090390
Iteration 712/1000 | Loss: 0.00100000
Iteration 713/1000 | Loss: 0.00089297
Iteration 714/1000 | Loss: 0.00095634
Iteration 715/1000 | Loss: 0.00085338
Iteration 716/1000 | Loss: 0.00088348
Iteration 717/1000 | Loss: 0.00104607
Iteration 718/1000 | Loss: 0.00096290
Iteration 719/1000 | Loss: 0.00072724
Iteration 720/1000 | Loss: 0.00071564
Iteration 721/1000 | Loss: 0.00092139
Iteration 722/1000 | Loss: 0.00123364
Iteration 723/1000 | Loss: 0.00083660
Iteration 724/1000 | Loss: 0.00105125
Iteration 725/1000 | Loss: 0.00143237
Iteration 726/1000 | Loss: 0.00105266
Iteration 727/1000 | Loss: 0.00185506
Iteration 728/1000 | Loss: 0.00096120
Iteration 729/1000 | Loss: 0.00056190
Iteration 730/1000 | Loss: 0.00080394
Iteration 731/1000 | Loss: 0.00075694
Iteration 732/1000 | Loss: 0.00084946
Iteration 733/1000 | Loss: 0.00096946
Iteration 734/1000 | Loss: 0.00121303
Iteration 735/1000 | Loss: 0.00076283
Iteration 736/1000 | Loss: 0.00077501
Iteration 737/1000 | Loss: 0.00074735
Iteration 738/1000 | Loss: 0.00081866
Iteration 739/1000 | Loss: 0.00124196
Iteration 740/1000 | Loss: 0.00105511
Iteration 741/1000 | Loss: 0.00079294
Iteration 742/1000 | Loss: 0.00059714
Iteration 743/1000 | Loss: 0.00057381
Iteration 744/1000 | Loss: 0.00096287
Iteration 745/1000 | Loss: 0.00080382
Iteration 746/1000 | Loss: 0.00073752
Iteration 747/1000 | Loss: 0.00079397
Iteration 748/1000 | Loss: 0.00059789
Iteration 749/1000 | Loss: 0.00081606
Iteration 750/1000 | Loss: 0.00117104
Iteration 751/1000 | Loss: 0.00148425
Iteration 752/1000 | Loss: 0.00082010
Iteration 753/1000 | Loss: 0.00077734
Iteration 754/1000 | Loss: 0.00079747
Iteration 755/1000 | Loss: 0.00073998
Iteration 756/1000 | Loss: 0.00084263
Iteration 757/1000 | Loss: 0.00082180
Iteration 758/1000 | Loss: 0.00108631
Iteration 759/1000 | Loss: 0.00107097
Iteration 760/1000 | Loss: 0.00086383
Iteration 761/1000 | Loss: 0.00097524
Iteration 762/1000 | Loss: 0.00080551
Iteration 763/1000 | Loss: 0.00104548
Iteration 764/1000 | Loss: 0.00099893
Iteration 765/1000 | Loss: 0.00095220
Iteration 766/1000 | Loss: 0.00095868
Iteration 767/1000 | Loss: 0.00109669
Iteration 768/1000 | Loss: 0.00097604
Iteration 769/1000 | Loss: 0.00155310
Iteration 770/1000 | Loss: 0.00116935
Iteration 771/1000 | Loss: 0.00128474
Iteration 772/1000 | Loss: 0.00134727
Iteration 773/1000 | Loss: 0.00099402
Iteration 774/1000 | Loss: 0.00129018
Iteration 775/1000 | Loss: 0.00148152
Iteration 776/1000 | Loss: 0.00104687
Iteration 777/1000 | Loss: 0.00123746
Iteration 778/1000 | Loss: 0.00112114
Iteration 779/1000 | Loss: 0.00145101
Iteration 780/1000 | Loss: 0.00084532
Iteration 781/1000 | Loss: 0.00083755
Iteration 782/1000 | Loss: 0.00098511
Iteration 783/1000 | Loss: 0.00091844
Iteration 784/1000 | Loss: 0.00087019
Iteration 785/1000 | Loss: 0.00099158
Iteration 786/1000 | Loss: 0.00086464
Iteration 787/1000 | Loss: 0.00092258
Iteration 788/1000 | Loss: 0.00085754
Iteration 789/1000 | Loss: 0.00110892
Iteration 790/1000 | Loss: 0.00087916
Iteration 791/1000 | Loss: 0.00092343
Iteration 792/1000 | Loss: 0.00084880
Iteration 793/1000 | Loss: 0.00079078
Iteration 794/1000 | Loss: 0.00064562
Iteration 795/1000 | Loss: 0.00067740
Iteration 796/1000 | Loss: 0.00073831
Iteration 797/1000 | Loss: 0.00075105
Iteration 798/1000 | Loss: 0.00073711
Iteration 799/1000 | Loss: 0.00108278
Iteration 800/1000 | Loss: 0.00088569
Iteration 801/1000 | Loss: 0.00089722
Iteration 802/1000 | Loss: 0.00106565
Iteration 803/1000 | Loss: 0.00088959
Iteration 804/1000 | Loss: 0.00114416
Iteration 805/1000 | Loss: 0.00093258
Iteration 806/1000 | Loss: 0.00108631
Iteration 807/1000 | Loss: 0.00090282
Iteration 808/1000 | Loss: 0.00116490
Iteration 809/1000 | Loss: 0.00088683
Iteration 810/1000 | Loss: 0.00060668
Iteration 811/1000 | Loss: 0.00073062
Iteration 812/1000 | Loss: 0.00062971
Iteration 813/1000 | Loss: 0.00102877
Iteration 814/1000 | Loss: 0.00161616
Iteration 815/1000 | Loss: 0.00080307
Iteration 816/1000 | Loss: 0.00062300
Iteration 817/1000 | Loss: 0.00107535
Iteration 818/1000 | Loss: 0.00148078
Iteration 819/1000 | Loss: 0.00109969
Iteration 820/1000 | Loss: 0.00063861
Iteration 821/1000 | Loss: 0.00069020
Iteration 822/1000 | Loss: 0.00061777
Iteration 823/1000 | Loss: 0.00050448
Iteration 824/1000 | Loss: 0.00078249
Iteration 825/1000 | Loss: 0.00066368
Iteration 826/1000 | Loss: 0.00123539
Iteration 827/1000 | Loss: 0.00063172
Iteration 828/1000 | Loss: 0.00043847
Iteration 829/1000 | Loss: 0.00046762
Iteration 830/1000 | Loss: 0.00046652
Iteration 831/1000 | Loss: 0.00051533
Iteration 832/1000 | Loss: 0.00098849
Iteration 833/1000 | Loss: 0.00061409
Iteration 834/1000 | Loss: 0.00055654
Iteration 835/1000 | Loss: 0.00065237
Iteration 836/1000 | Loss: 0.00057932
Iteration 837/1000 | Loss: 0.00058104
Iteration 838/1000 | Loss: 0.00070928
Iteration 839/1000 | Loss: 0.00055805
Iteration 840/1000 | Loss: 0.00051569
Iteration 841/1000 | Loss: 0.00074109
Iteration 842/1000 | Loss: 0.00068270
Iteration 843/1000 | Loss: 0.00053217
Iteration 844/1000 | Loss: 0.00075901
Iteration 845/1000 | Loss: 0.00058606
Iteration 846/1000 | Loss: 0.00062186
Iteration 847/1000 | Loss: 0.00056666
Iteration 848/1000 | Loss: 0.00061386
Iteration 849/1000 | Loss: 0.00072606
Iteration 850/1000 | Loss: 0.00074361
Iteration 851/1000 | Loss: 0.00076950
Iteration 852/1000 | Loss: 0.00084919
Iteration 853/1000 | Loss: 0.00080171
Iteration 854/1000 | Loss: 0.00078283
Iteration 855/1000 | Loss: 0.00074115
Iteration 856/1000 | Loss: 0.00089526
Iteration 857/1000 | Loss: 0.00094418
Iteration 858/1000 | Loss: 0.00071865
Iteration 859/1000 | Loss: 0.00046596
Iteration 860/1000 | Loss: 0.00063272
Iteration 861/1000 | Loss: 0.00089120
Iteration 862/1000 | Loss: 0.00125662
Iteration 863/1000 | Loss: 0.00124112
Iteration 864/1000 | Loss: 0.00095163
Iteration 865/1000 | Loss: 0.00102628
Iteration 866/1000 | Loss: 0.00078565
Iteration 867/1000 | Loss: 0.00104815
Iteration 868/1000 | Loss: 0.00086765
Iteration 869/1000 | Loss: 0.00112760
Iteration 870/1000 | Loss: 0.00112527
Iteration 871/1000 | Loss: 0.00082464
Iteration 872/1000 | Loss: 0.00114824
Iteration 873/1000 | Loss: 0.00092895
Iteration 874/1000 | Loss: 0.00116838
Iteration 875/1000 | Loss: 0.00069703
Iteration 876/1000 | Loss: 0.00051571
Iteration 877/1000 | Loss: 0.00066827
Iteration 878/1000 | Loss: 0.00078268
Iteration 879/1000 | Loss: 0.00069801
Iteration 880/1000 | Loss: 0.00071617
Iteration 881/1000 | Loss: 0.00060807
Iteration 882/1000 | Loss: 0.00085554
Iteration 883/1000 | Loss: 0.00137959
Iteration 884/1000 | Loss: 0.00138657
Iteration 885/1000 | Loss: 0.00074014
Iteration 886/1000 | Loss: 0.00066251
Iteration 887/1000 | Loss: 0.00088586
Iteration 888/1000 | Loss: 0.00043183
Iteration 889/1000 | Loss: 0.00061259
Iteration 890/1000 | Loss: 0.00068586
Iteration 891/1000 | Loss: 0.00072657
Iteration 892/1000 | Loss: 0.00068291
Iteration 893/1000 | Loss: 0.00075325
Iteration 894/1000 | Loss: 0.00083664
Iteration 895/1000 | Loss: 0.00158967
Iteration 896/1000 | Loss: 0.00086307
Iteration 897/1000 | Loss: 0.00095997
Iteration 898/1000 | Loss: 0.00085896
Iteration 899/1000 | Loss: 0.00184739
Iteration 900/1000 | Loss: 0.00133556
Iteration 901/1000 | Loss: 0.00158519
Iteration 902/1000 | Loss: 0.00124997
Iteration 903/1000 | Loss: 0.00161427
Iteration 904/1000 | Loss: 0.00113963
Iteration 905/1000 | Loss: 0.00127528
Iteration 906/1000 | Loss: 0.00072511
Iteration 907/1000 | Loss: 0.00062905
Iteration 908/1000 | Loss: 0.00052445
Iteration 909/1000 | Loss: 0.00065183
Iteration 910/1000 | Loss: 0.00062518
Iteration 911/1000 | Loss: 0.00060513
Iteration 912/1000 | Loss: 0.00066933
Iteration 913/1000 | Loss: 0.00065417
Iteration 914/1000 | Loss: 0.00077507
Iteration 915/1000 | Loss: 0.00068942
Iteration 916/1000 | Loss: 0.00063152
Iteration 917/1000 | Loss: 0.00075046
Iteration 918/1000 | Loss: 0.00073374
Iteration 919/1000 | Loss: 0.00052470
Iteration 920/1000 | Loss: 0.00058792
Iteration 921/1000 | Loss: 0.00067532
Iteration 922/1000 | Loss: 0.00119643
Iteration 923/1000 | Loss: 0.00071438
Iteration 924/1000 | Loss: 0.00110716
Iteration 925/1000 | Loss: 0.00097997
Iteration 926/1000 | Loss: 0.00092947
Iteration 927/1000 | Loss: 0.00087270
Iteration 928/1000 | Loss: 0.00062878
Iteration 929/1000 | Loss: 0.00053905
Iteration 930/1000 | Loss: 0.00059827
Iteration 931/1000 | Loss: 0.00083556
Iteration 932/1000 | Loss: 0.00066844
Iteration 933/1000 | Loss: 0.00097369
Iteration 934/1000 | Loss: 0.00081168
Iteration 935/1000 | Loss: 0.00071584
Iteration 936/1000 | Loss: 0.00098469
Iteration 937/1000 | Loss: 0.00062134
Iteration 938/1000 | Loss: 0.00040960
Iteration 939/1000 | Loss: 0.00044609
Iteration 940/1000 | Loss: 0.00049865
Iteration 941/1000 | Loss: 0.00063347
Iteration 942/1000 | Loss: 0.00077154
Iteration 943/1000 | Loss: 0.00034987
Iteration 944/1000 | Loss: 0.00055432
Iteration 945/1000 | Loss: 0.00050439
Iteration 946/1000 | Loss: 0.00057743
Iteration 947/1000 | Loss: 0.00067395
Iteration 948/1000 | Loss: 0.00053580
Iteration 949/1000 | Loss: 0.00059842
Iteration 950/1000 | Loss: 0.00058108
Iteration 951/1000 | Loss: 0.00058128
Iteration 952/1000 | Loss: 0.00064865
Iteration 953/1000 | Loss: 0.00059770
Iteration 954/1000 | Loss: 0.00059653
Iteration 955/1000 | Loss: 0.00057487
Iteration 956/1000 | Loss: 0.00057388
Iteration 957/1000 | Loss: 0.00060759
Iteration 958/1000 | Loss: 0.00056126
Iteration 959/1000 | Loss: 0.00059241
Iteration 960/1000 | Loss: 0.00056295
Iteration 961/1000 | Loss: 0.00057515
Iteration 962/1000 | Loss: 0.00060637
Iteration 963/1000 | Loss: 0.00055659
Iteration 964/1000 | Loss: 0.00059319
Iteration 965/1000 | Loss: 0.00063402
Iteration 966/1000 | Loss: 0.00058390
Iteration 967/1000 | Loss: 0.00062015
Iteration 968/1000 | Loss: 0.00078888
Iteration 969/1000 | Loss: 0.00064472
Iteration 970/1000 | Loss: 0.00064999
Iteration 971/1000 | Loss: 0.00026651
Iteration 972/1000 | Loss: 0.00041397
Iteration 973/1000 | Loss: 0.00047857
Iteration 974/1000 | Loss: 0.00059615
Iteration 975/1000 | Loss: 0.00062176
Iteration 976/1000 | Loss: 0.00042669
Iteration 977/1000 | Loss: 0.00066512
Iteration 978/1000 | Loss: 0.00071324
Iteration 979/1000 | Loss: 0.00056884
Iteration 980/1000 | Loss: 0.00066435
Iteration 981/1000 | Loss: 0.00065599
Iteration 982/1000 | Loss: 0.00055893
Iteration 983/1000 | Loss: 0.00063059
Iteration 984/1000 | Loss: 0.00053080
Iteration 985/1000 | Loss: 0.00060959
Iteration 986/1000 | Loss: 0.00069769
Iteration 987/1000 | Loss: 0.00065591
Iteration 988/1000 | Loss: 0.00075019
Iteration 989/1000 | Loss: 0.00069671
Iteration 990/1000 | Loss: 0.00076373
Iteration 991/1000 | Loss: 0.00059948
Iteration 992/1000 | Loss: 0.00057163
Iteration 993/1000 | Loss: 0.00067508
Iteration 994/1000 | Loss: 0.00060162
Iteration 995/1000 | Loss: 0.00070353
Iteration 996/1000 | Loss: 0.00060904
Iteration 997/1000 | Loss: 0.00072464
Iteration 998/1000 | Loss: 0.00074440
Iteration 999/1000 | Loss: 0.00070138
Iteration 1000/1000 | Loss: 0.00046823

Optimization complete. Final v2v error: 4.373283386230469 mm

Highest mean error: 86.51990509033203 mm for frame 168

Lowest mean error: 2.5632524490356445 mm for frame 9

Saving results

Total time: 1493.4643847942352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691121
Iteration 2/25 | Loss: 0.00159612
Iteration 3/25 | Loss: 0.00133840
Iteration 4/25 | Loss: 0.00130757
Iteration 5/25 | Loss: 0.00128760
Iteration 6/25 | Loss: 0.00128337
Iteration 7/25 | Loss: 0.00127367
Iteration 8/25 | Loss: 0.00126669
Iteration 9/25 | Loss: 0.00126241
Iteration 10/25 | Loss: 0.00126222
Iteration 11/25 | Loss: 0.00126222
Iteration 12/25 | Loss: 0.00126222
Iteration 13/25 | Loss: 0.00126222
Iteration 14/25 | Loss: 0.00126221
Iteration 15/25 | Loss: 0.00126221
Iteration 16/25 | Loss: 0.00126221
Iteration 17/25 | Loss: 0.00126221
Iteration 18/25 | Loss: 0.00126221
Iteration 19/25 | Loss: 0.00126221
Iteration 20/25 | Loss: 0.00126221
Iteration 21/25 | Loss: 0.00126221
Iteration 22/25 | Loss: 0.00126221
Iteration 23/25 | Loss: 0.00126221
Iteration 24/25 | Loss: 0.00126221
Iteration 25/25 | Loss: 0.00126221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12647486
Iteration 2/25 | Loss: 0.00088277
Iteration 3/25 | Loss: 0.00088246
Iteration 4/25 | Loss: 0.00088246
Iteration 5/25 | Loss: 0.00088246
Iteration 6/25 | Loss: 0.00088246
Iteration 7/25 | Loss: 0.00088246
Iteration 8/25 | Loss: 0.00088246
Iteration 9/25 | Loss: 0.00088246
Iteration 10/25 | Loss: 0.00088246
Iteration 11/25 | Loss: 0.00088246
Iteration 12/25 | Loss: 0.00088246
Iteration 13/25 | Loss: 0.00088246
Iteration 14/25 | Loss: 0.00088246
Iteration 15/25 | Loss: 0.00088246
Iteration 16/25 | Loss: 0.00088246
Iteration 17/25 | Loss: 0.00088246
Iteration 18/25 | Loss: 0.00088246
Iteration 19/25 | Loss: 0.00088246
Iteration 20/25 | Loss: 0.00088246
Iteration 21/25 | Loss: 0.00088245
Iteration 22/25 | Loss: 0.00088246
Iteration 23/25 | Loss: 0.00088246
Iteration 24/25 | Loss: 0.00088245
Iteration 25/25 | Loss: 0.00088245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088245
Iteration 2/1000 | Loss: 0.00007549
Iteration 3/1000 | Loss: 0.00004213
Iteration 4/1000 | Loss: 0.00003420
Iteration 5/1000 | Loss: 0.00003078
Iteration 6/1000 | Loss: 0.00002896
Iteration 7/1000 | Loss: 0.00002755
Iteration 8/1000 | Loss: 0.00002678
Iteration 9/1000 | Loss: 0.00002606
Iteration 10/1000 | Loss: 0.00002553
Iteration 11/1000 | Loss: 0.00002511
Iteration 12/1000 | Loss: 0.00002480
Iteration 13/1000 | Loss: 0.00002453
Iteration 14/1000 | Loss: 0.00002448
Iteration 15/1000 | Loss: 0.00002434
Iteration 16/1000 | Loss: 0.00002433
Iteration 17/1000 | Loss: 0.00002422
Iteration 18/1000 | Loss: 0.00002409
Iteration 19/1000 | Loss: 0.00002409
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002402
Iteration 22/1000 | Loss: 0.00002400
Iteration 23/1000 | Loss: 0.00002396
Iteration 24/1000 | Loss: 0.00002395
Iteration 25/1000 | Loss: 0.00002394
Iteration 26/1000 | Loss: 0.00002394
Iteration 27/1000 | Loss: 0.00002392
Iteration 28/1000 | Loss: 0.00002391
Iteration 29/1000 | Loss: 0.00002391
Iteration 30/1000 | Loss: 0.00002390
Iteration 31/1000 | Loss: 0.00002389
Iteration 32/1000 | Loss: 0.00002387
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002386
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00002384
Iteration 37/1000 | Loss: 0.00002384
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00002383
Iteration 40/1000 | Loss: 0.00002382
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002379
Iteration 43/1000 | Loss: 0.00002379
Iteration 44/1000 | Loss: 0.00002379
Iteration 45/1000 | Loss: 0.00002377
Iteration 46/1000 | Loss: 0.00002374
Iteration 47/1000 | Loss: 0.00002372
Iteration 48/1000 | Loss: 0.00002371
Iteration 49/1000 | Loss: 0.00002371
Iteration 50/1000 | Loss: 0.00002370
Iteration 51/1000 | Loss: 0.00002370
Iteration 52/1000 | Loss: 0.00002368
Iteration 53/1000 | Loss: 0.00002366
Iteration 54/1000 | Loss: 0.00002366
Iteration 55/1000 | Loss: 0.00002366
Iteration 56/1000 | Loss: 0.00002365
Iteration 57/1000 | Loss: 0.00002365
Iteration 58/1000 | Loss: 0.00002364
Iteration 59/1000 | Loss: 0.00002364
Iteration 60/1000 | Loss: 0.00002364
Iteration 61/1000 | Loss: 0.00002363
Iteration 62/1000 | Loss: 0.00002363
Iteration 63/1000 | Loss: 0.00002362
Iteration 64/1000 | Loss: 0.00002362
Iteration 65/1000 | Loss: 0.00002362
Iteration 66/1000 | Loss: 0.00002362
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002360
Iteration 69/1000 | Loss: 0.00002360
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002359
Iteration 73/1000 | Loss: 0.00002357
Iteration 74/1000 | Loss: 0.00002357
Iteration 75/1000 | Loss: 0.00002357
Iteration 76/1000 | Loss: 0.00002357
Iteration 77/1000 | Loss: 0.00002356
Iteration 78/1000 | Loss: 0.00002356
Iteration 79/1000 | Loss: 0.00002356
Iteration 80/1000 | Loss: 0.00002355
Iteration 81/1000 | Loss: 0.00002355
Iteration 82/1000 | Loss: 0.00002355
Iteration 83/1000 | Loss: 0.00002354
Iteration 84/1000 | Loss: 0.00002354
Iteration 85/1000 | Loss: 0.00002354
Iteration 86/1000 | Loss: 0.00002353
Iteration 87/1000 | Loss: 0.00002353
Iteration 88/1000 | Loss: 0.00002353
Iteration 89/1000 | Loss: 0.00002352
Iteration 90/1000 | Loss: 0.00002352
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002351
Iteration 93/1000 | Loss: 0.00002351
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002351
Iteration 96/1000 | Loss: 0.00002351
Iteration 97/1000 | Loss: 0.00002350
Iteration 98/1000 | Loss: 0.00002350
Iteration 99/1000 | Loss: 0.00002350
Iteration 100/1000 | Loss: 0.00002350
Iteration 101/1000 | Loss: 0.00002349
Iteration 102/1000 | Loss: 0.00002349
Iteration 103/1000 | Loss: 0.00002348
Iteration 104/1000 | Loss: 0.00002348
Iteration 105/1000 | Loss: 0.00002348
Iteration 106/1000 | Loss: 0.00002348
Iteration 107/1000 | Loss: 0.00002347
Iteration 108/1000 | Loss: 0.00002347
Iteration 109/1000 | Loss: 0.00002347
Iteration 110/1000 | Loss: 0.00002347
Iteration 111/1000 | Loss: 0.00002347
Iteration 112/1000 | Loss: 0.00002347
Iteration 113/1000 | Loss: 0.00002347
Iteration 114/1000 | Loss: 0.00002347
Iteration 115/1000 | Loss: 0.00002347
Iteration 116/1000 | Loss: 0.00002347
Iteration 117/1000 | Loss: 0.00002346
Iteration 118/1000 | Loss: 0.00002346
Iteration 119/1000 | Loss: 0.00002346
Iteration 120/1000 | Loss: 0.00002345
Iteration 121/1000 | Loss: 0.00002345
Iteration 122/1000 | Loss: 0.00002345
Iteration 123/1000 | Loss: 0.00002344
Iteration 124/1000 | Loss: 0.00002344
Iteration 125/1000 | Loss: 0.00002344
Iteration 126/1000 | Loss: 0.00002344
Iteration 127/1000 | Loss: 0.00002343
Iteration 128/1000 | Loss: 0.00002343
Iteration 129/1000 | Loss: 0.00002343
Iteration 130/1000 | Loss: 0.00002342
Iteration 131/1000 | Loss: 0.00002342
Iteration 132/1000 | Loss: 0.00002342
Iteration 133/1000 | Loss: 0.00002342
Iteration 134/1000 | Loss: 0.00002341
Iteration 135/1000 | Loss: 0.00002341
Iteration 136/1000 | Loss: 0.00002341
Iteration 137/1000 | Loss: 0.00002341
Iteration 138/1000 | Loss: 0.00002341
Iteration 139/1000 | Loss: 0.00002341
Iteration 140/1000 | Loss: 0.00002341
Iteration 141/1000 | Loss: 0.00002341
Iteration 142/1000 | Loss: 0.00002341
Iteration 143/1000 | Loss: 0.00002341
Iteration 144/1000 | Loss: 0.00002341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.3410564608639106e-05, 2.3410564608639106e-05, 2.3410564608639106e-05, 2.3410564608639106e-05, 2.3410564608639106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3410564608639106e-05

Optimization complete. Final v2v error: 3.9061098098754883 mm

Highest mean error: 6.1460723876953125 mm for frame 132

Lowest mean error: 3.0064122676849365 mm for frame 205

Saving results

Total time: 59.86986970901489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893627
Iteration 2/25 | Loss: 0.00185467
Iteration 3/25 | Loss: 0.00140798
Iteration 4/25 | Loss: 0.00134440
Iteration 5/25 | Loss: 0.00134177
Iteration 6/25 | Loss: 0.00134157
Iteration 7/25 | Loss: 0.00134157
Iteration 8/25 | Loss: 0.00134157
Iteration 9/25 | Loss: 0.00134157
Iteration 10/25 | Loss: 0.00134157
Iteration 11/25 | Loss: 0.00134157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001341565977782011, 0.001341565977782011, 0.001341565977782011, 0.001341565977782011, 0.001341565977782011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001341565977782011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40826011
Iteration 2/25 | Loss: 0.00073814
Iteration 3/25 | Loss: 0.00073809
Iteration 4/25 | Loss: 0.00073809
Iteration 5/25 | Loss: 0.00073809
Iteration 6/25 | Loss: 0.00073809
Iteration 7/25 | Loss: 0.00073809
Iteration 8/25 | Loss: 0.00073809
Iteration 9/25 | Loss: 0.00073809
Iteration 10/25 | Loss: 0.00073809
Iteration 11/25 | Loss: 0.00073809
Iteration 12/25 | Loss: 0.00073809
Iteration 13/25 | Loss: 0.00073809
Iteration 14/25 | Loss: 0.00073809
Iteration 15/25 | Loss: 0.00073809
Iteration 16/25 | Loss: 0.00073809
Iteration 17/25 | Loss: 0.00073809
Iteration 18/25 | Loss: 0.00073809
Iteration 19/25 | Loss: 0.00073809
Iteration 20/25 | Loss: 0.00073809
Iteration 21/25 | Loss: 0.00073809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007380882743746042, 0.0007380882743746042, 0.0007380882743746042, 0.0007380882743746042, 0.0007380882743746042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007380882743746042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073809
Iteration 2/1000 | Loss: 0.00006139
Iteration 3/1000 | Loss: 0.00004662
Iteration 4/1000 | Loss: 0.00003771
Iteration 5/1000 | Loss: 0.00003496
Iteration 6/1000 | Loss: 0.00003365
Iteration 7/1000 | Loss: 0.00003275
Iteration 8/1000 | Loss: 0.00003224
Iteration 9/1000 | Loss: 0.00003159
Iteration 10/1000 | Loss: 0.00003114
Iteration 11/1000 | Loss: 0.00003071
Iteration 12/1000 | Loss: 0.00003035
Iteration 13/1000 | Loss: 0.00003002
Iteration 14/1000 | Loss: 0.00002995
Iteration 15/1000 | Loss: 0.00002990
Iteration 16/1000 | Loss: 0.00002983
Iteration 17/1000 | Loss: 0.00002970
Iteration 18/1000 | Loss: 0.00002964
Iteration 19/1000 | Loss: 0.00002961
Iteration 20/1000 | Loss: 0.00002960
Iteration 21/1000 | Loss: 0.00002960
Iteration 22/1000 | Loss: 0.00002959
Iteration 23/1000 | Loss: 0.00002959
Iteration 24/1000 | Loss: 0.00002959
Iteration 25/1000 | Loss: 0.00002958
Iteration 26/1000 | Loss: 0.00002958
Iteration 27/1000 | Loss: 0.00002958
Iteration 28/1000 | Loss: 0.00002957
Iteration 29/1000 | Loss: 0.00002951
Iteration 30/1000 | Loss: 0.00002950
Iteration 31/1000 | Loss: 0.00002950
Iteration 32/1000 | Loss: 0.00002946
Iteration 33/1000 | Loss: 0.00002946
Iteration 34/1000 | Loss: 0.00002946
Iteration 35/1000 | Loss: 0.00002946
Iteration 36/1000 | Loss: 0.00002945
Iteration 37/1000 | Loss: 0.00002945
Iteration 38/1000 | Loss: 0.00002945
Iteration 39/1000 | Loss: 0.00002945
Iteration 40/1000 | Loss: 0.00002945
Iteration 41/1000 | Loss: 0.00002945
Iteration 42/1000 | Loss: 0.00002945
Iteration 43/1000 | Loss: 0.00002945
Iteration 44/1000 | Loss: 0.00002944
Iteration 45/1000 | Loss: 0.00002944
Iteration 46/1000 | Loss: 0.00002942
Iteration 47/1000 | Loss: 0.00002942
Iteration 48/1000 | Loss: 0.00002942
Iteration 49/1000 | Loss: 0.00002942
Iteration 50/1000 | Loss: 0.00002942
Iteration 51/1000 | Loss: 0.00002942
Iteration 52/1000 | Loss: 0.00002941
Iteration 53/1000 | Loss: 0.00002941
Iteration 54/1000 | Loss: 0.00002941
Iteration 55/1000 | Loss: 0.00002941
Iteration 56/1000 | Loss: 0.00002941
Iteration 57/1000 | Loss: 0.00002941
Iteration 58/1000 | Loss: 0.00002941
Iteration 59/1000 | Loss: 0.00002941
Iteration 60/1000 | Loss: 0.00002940
Iteration 61/1000 | Loss: 0.00002940
Iteration 62/1000 | Loss: 0.00002940
Iteration 63/1000 | Loss: 0.00002939
Iteration 64/1000 | Loss: 0.00002939
Iteration 65/1000 | Loss: 0.00002939
Iteration 66/1000 | Loss: 0.00002939
Iteration 67/1000 | Loss: 0.00002939
Iteration 68/1000 | Loss: 0.00002939
Iteration 69/1000 | Loss: 0.00002938
Iteration 70/1000 | Loss: 0.00002938
Iteration 71/1000 | Loss: 0.00002938
Iteration 72/1000 | Loss: 0.00002938
Iteration 73/1000 | Loss: 0.00002938
Iteration 74/1000 | Loss: 0.00002937
Iteration 75/1000 | Loss: 0.00002937
Iteration 76/1000 | Loss: 0.00002937
Iteration 77/1000 | Loss: 0.00002937
Iteration 78/1000 | Loss: 0.00002937
Iteration 79/1000 | Loss: 0.00002937
Iteration 80/1000 | Loss: 0.00002937
Iteration 81/1000 | Loss: 0.00002937
Iteration 82/1000 | Loss: 0.00002937
Iteration 83/1000 | Loss: 0.00002937
Iteration 84/1000 | Loss: 0.00002937
Iteration 85/1000 | Loss: 0.00002937
Iteration 86/1000 | Loss: 0.00002937
Iteration 87/1000 | Loss: 0.00002937
Iteration 88/1000 | Loss: 0.00002936
Iteration 89/1000 | Loss: 0.00002936
Iteration 90/1000 | Loss: 0.00002936
Iteration 91/1000 | Loss: 0.00002936
Iteration 92/1000 | Loss: 0.00002936
Iteration 93/1000 | Loss: 0.00002936
Iteration 94/1000 | Loss: 0.00002936
Iteration 95/1000 | Loss: 0.00002936
Iteration 96/1000 | Loss: 0.00002936
Iteration 97/1000 | Loss: 0.00002936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.936356031568721e-05, 2.936356031568721e-05, 2.936356031568721e-05, 2.936356031568721e-05, 2.936356031568721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.936356031568721e-05

Optimization complete. Final v2v error: 4.513479709625244 mm

Highest mean error: 4.739374160766602 mm for frame 0

Lowest mean error: 4.243216037750244 mm for frame 67

Saving results

Total time: 34.8265380859375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079291
Iteration 2/25 | Loss: 0.00183608
Iteration 3/25 | Loss: 0.00141086
Iteration 4/25 | Loss: 0.00139198
Iteration 5/25 | Loss: 0.00138996
Iteration 6/25 | Loss: 0.00137551
Iteration 7/25 | Loss: 0.00136592
Iteration 8/25 | Loss: 0.00133969
Iteration 9/25 | Loss: 0.00134237
Iteration 10/25 | Loss: 0.00133954
Iteration 11/25 | Loss: 0.00133441
Iteration 12/25 | Loss: 0.00133959
Iteration 13/25 | Loss: 0.00134023
Iteration 14/25 | Loss: 0.00133708
Iteration 15/25 | Loss: 0.00133948
Iteration 16/25 | Loss: 0.00134198
Iteration 17/25 | Loss: 0.00133673
Iteration 18/25 | Loss: 0.00134190
Iteration 19/25 | Loss: 0.00133587
Iteration 20/25 | Loss: 0.00133196
Iteration 21/25 | Loss: 0.00132991
Iteration 22/25 | Loss: 0.00132780
Iteration 23/25 | Loss: 0.00132770
Iteration 24/25 | Loss: 0.00133032
Iteration 25/25 | Loss: 0.00133370

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12382007
Iteration 2/25 | Loss: 0.00087608
Iteration 3/25 | Loss: 0.00087608
Iteration 4/25 | Loss: 0.00087608
Iteration 5/25 | Loss: 0.00087608
Iteration 6/25 | Loss: 0.00087608
Iteration 7/25 | Loss: 0.00087608
Iteration 8/25 | Loss: 0.00087608
Iteration 9/25 | Loss: 0.00087608
Iteration 10/25 | Loss: 0.00087608
Iteration 11/25 | Loss: 0.00087608
Iteration 12/25 | Loss: 0.00087608
Iteration 13/25 | Loss: 0.00087607
Iteration 14/25 | Loss: 0.00087608
Iteration 15/25 | Loss: 0.00087608
Iteration 16/25 | Loss: 0.00087608
Iteration 17/25 | Loss: 0.00087608
Iteration 18/25 | Loss: 0.00087608
Iteration 19/25 | Loss: 0.00087608
Iteration 20/25 | Loss: 0.00087608
Iteration 21/25 | Loss: 0.00087608
Iteration 22/25 | Loss: 0.00087607
Iteration 23/25 | Loss: 0.00087608
Iteration 24/25 | Loss: 0.00087608
Iteration 25/25 | Loss: 0.00087608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087607
Iteration 2/1000 | Loss: 0.00028648
Iteration 3/1000 | Loss: 0.00126936
Iteration 4/1000 | Loss: 0.00023049
Iteration 5/1000 | Loss: 0.00022599
Iteration 6/1000 | Loss: 0.00014464
Iteration 7/1000 | Loss: 0.00013531
Iteration 8/1000 | Loss: 0.00003848
Iteration 9/1000 | Loss: 0.00003536
Iteration 10/1000 | Loss: 0.00003317
Iteration 11/1000 | Loss: 0.00003153
Iteration 12/1000 | Loss: 0.00002988
Iteration 13/1000 | Loss: 0.00002907
Iteration 14/1000 | Loss: 0.00002846
Iteration 15/1000 | Loss: 0.00022959
Iteration 16/1000 | Loss: 0.00006291
Iteration 17/1000 | Loss: 0.00002862
Iteration 18/1000 | Loss: 0.00002685
Iteration 19/1000 | Loss: 0.00003547
Iteration 20/1000 | Loss: 0.00002675
Iteration 21/1000 | Loss: 0.00002822
Iteration 22/1000 | Loss: 0.00002445
Iteration 23/1000 | Loss: 0.00002411
Iteration 24/1000 | Loss: 0.00003895
Iteration 25/1000 | Loss: 0.00002382
Iteration 26/1000 | Loss: 0.00002374
Iteration 27/1000 | Loss: 0.00021208
Iteration 28/1000 | Loss: 0.00003563
Iteration 29/1000 | Loss: 0.00004378
Iteration 30/1000 | Loss: 0.00002295
Iteration 31/1000 | Loss: 0.00002242
Iteration 32/1000 | Loss: 0.00003717
Iteration 33/1000 | Loss: 0.00002410
Iteration 34/1000 | Loss: 0.00002150
Iteration 35/1000 | Loss: 0.00002410
Iteration 36/1000 | Loss: 0.00002134
Iteration 37/1000 | Loss: 0.00003823
Iteration 38/1000 | Loss: 0.00002126
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002123
Iteration 41/1000 | Loss: 0.00002122
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002119
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002113
Iteration 54/1000 | Loss: 0.00002113
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002686
Iteration 57/1000 | Loss: 0.00002109
Iteration 58/1000 | Loss: 0.00002106
Iteration 59/1000 | Loss: 0.00002105
Iteration 60/1000 | Loss: 0.00002104
Iteration 61/1000 | Loss: 0.00002104
Iteration 62/1000 | Loss: 0.00002103
Iteration 63/1000 | Loss: 0.00002103
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002102
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002102
Iteration 68/1000 | Loss: 0.00002101
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00002101
Iteration 73/1000 | Loss: 0.00002100
Iteration 74/1000 | Loss: 0.00002100
Iteration 75/1000 | Loss: 0.00002100
Iteration 76/1000 | Loss: 0.00002100
Iteration 77/1000 | Loss: 0.00002100
Iteration 78/1000 | Loss: 0.00002100
Iteration 79/1000 | Loss: 0.00002100
Iteration 80/1000 | Loss: 0.00002099
Iteration 81/1000 | Loss: 0.00002099
Iteration 82/1000 | Loss: 0.00002099
Iteration 83/1000 | Loss: 0.00002099
Iteration 84/1000 | Loss: 0.00002099
Iteration 85/1000 | Loss: 0.00002099
Iteration 86/1000 | Loss: 0.00002099
Iteration 87/1000 | Loss: 0.00002099
Iteration 88/1000 | Loss: 0.00002098
Iteration 89/1000 | Loss: 0.00002098
Iteration 90/1000 | Loss: 0.00002098
Iteration 91/1000 | Loss: 0.00002097
Iteration 92/1000 | Loss: 0.00002096
Iteration 93/1000 | Loss: 0.00002094
Iteration 94/1000 | Loss: 0.00002094
Iteration 95/1000 | Loss: 0.00002092
Iteration 96/1000 | Loss: 0.00002092
Iteration 97/1000 | Loss: 0.00002092
Iteration 98/1000 | Loss: 0.00002091
Iteration 99/1000 | Loss: 0.00002091
Iteration 100/1000 | Loss: 0.00002090
Iteration 101/1000 | Loss: 0.00002090
Iteration 102/1000 | Loss: 0.00002090
Iteration 103/1000 | Loss: 0.00002090
Iteration 104/1000 | Loss: 0.00002090
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002090
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00002090
Iteration 112/1000 | Loss: 0.00002089
Iteration 113/1000 | Loss: 0.00002089
Iteration 114/1000 | Loss: 0.00002089
Iteration 115/1000 | Loss: 0.00002089
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002088
Iteration 118/1000 | Loss: 0.00002088
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002088
Iteration 122/1000 | Loss: 0.00002087
Iteration 123/1000 | Loss: 0.00002087
Iteration 124/1000 | Loss: 0.00002087
Iteration 125/1000 | Loss: 0.00002087
Iteration 126/1000 | Loss: 0.00002087
Iteration 127/1000 | Loss: 0.00002087
Iteration 128/1000 | Loss: 0.00002087
Iteration 129/1000 | Loss: 0.00002087
Iteration 130/1000 | Loss: 0.00002087
Iteration 131/1000 | Loss: 0.00002087
Iteration 132/1000 | Loss: 0.00002086
Iteration 133/1000 | Loss: 0.00002086
Iteration 134/1000 | Loss: 0.00002086
Iteration 135/1000 | Loss: 0.00002086
Iteration 136/1000 | Loss: 0.00002086
Iteration 137/1000 | Loss: 0.00002086
Iteration 138/1000 | Loss: 0.00002086
Iteration 139/1000 | Loss: 0.00002086
Iteration 140/1000 | Loss: 0.00002086
Iteration 141/1000 | Loss: 0.00002086
Iteration 142/1000 | Loss: 0.00002086
Iteration 143/1000 | Loss: 0.00002086
Iteration 144/1000 | Loss: 0.00002086
Iteration 145/1000 | Loss: 0.00002086
Iteration 146/1000 | Loss: 0.00002086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.0859983123955317e-05, 2.0859983123955317e-05, 2.0859983123955317e-05, 2.0859983123955317e-05, 2.0859983123955317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0859983123955317e-05

Optimization complete. Final v2v error: 3.787914514541626 mm

Highest mean error: 4.297476291656494 mm for frame 21

Lowest mean error: 3.5839085578918457 mm for frame 1

Saving results

Total time: 104.6024181842804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813352
Iteration 2/25 | Loss: 0.00171232
Iteration 3/25 | Loss: 0.00145513
Iteration 4/25 | Loss: 0.00140457
Iteration 5/25 | Loss: 0.00138228
Iteration 6/25 | Loss: 0.00138189
Iteration 7/25 | Loss: 0.00136334
Iteration 8/25 | Loss: 0.00135459
Iteration 9/25 | Loss: 0.00135306
Iteration 10/25 | Loss: 0.00135277
Iteration 11/25 | Loss: 0.00135236
Iteration 12/25 | Loss: 0.00135096
Iteration 13/25 | Loss: 0.00134657
Iteration 14/25 | Loss: 0.00134366
Iteration 15/25 | Loss: 0.00134209
Iteration 16/25 | Loss: 0.00134606
Iteration 17/25 | Loss: 0.00134620
Iteration 18/25 | Loss: 0.00134433
Iteration 19/25 | Loss: 0.00134812
Iteration 20/25 | Loss: 0.00134390
Iteration 21/25 | Loss: 0.00134247
Iteration 22/25 | Loss: 0.00132903
Iteration 23/25 | Loss: 0.00131603
Iteration 24/25 | Loss: 0.00131115
Iteration 25/25 | Loss: 0.00131007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.52788401
Iteration 2/25 | Loss: 0.00122591
Iteration 3/25 | Loss: 0.00122568
Iteration 4/25 | Loss: 0.00122568
Iteration 5/25 | Loss: 0.00122567
Iteration 6/25 | Loss: 0.00122567
Iteration 7/25 | Loss: 0.00122567
Iteration 8/25 | Loss: 0.00122567
Iteration 9/25 | Loss: 0.00122567
Iteration 10/25 | Loss: 0.00122567
Iteration 11/25 | Loss: 0.00122567
Iteration 12/25 | Loss: 0.00122567
Iteration 13/25 | Loss: 0.00122567
Iteration 14/25 | Loss: 0.00122567
Iteration 15/25 | Loss: 0.00122567
Iteration 16/25 | Loss: 0.00122567
Iteration 17/25 | Loss: 0.00122567
Iteration 18/25 | Loss: 0.00122567
Iteration 19/25 | Loss: 0.00122567
Iteration 20/25 | Loss: 0.00122567
Iteration 21/25 | Loss: 0.00122567
Iteration 22/25 | Loss: 0.00122567
Iteration 23/25 | Loss: 0.00122567
Iteration 24/25 | Loss: 0.00122567
Iteration 25/25 | Loss: 0.00122567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122567
Iteration 2/1000 | Loss: 0.00014719
Iteration 3/1000 | Loss: 0.00009341
Iteration 4/1000 | Loss: 0.00007302
Iteration 5/1000 | Loss: 0.00006580
Iteration 6/1000 | Loss: 0.00006190
Iteration 7/1000 | Loss: 0.00005928
Iteration 8/1000 | Loss: 0.00005738
Iteration 9/1000 | Loss: 0.00005582
Iteration 10/1000 | Loss: 0.00005390
Iteration 11/1000 | Loss: 0.00005257
Iteration 12/1000 | Loss: 0.00005161
Iteration 13/1000 | Loss: 0.00105832
Iteration 14/1000 | Loss: 0.00241555
Iteration 15/1000 | Loss: 0.00011396
Iteration 16/1000 | Loss: 0.00006422
Iteration 17/1000 | Loss: 0.00004901
Iteration 18/1000 | Loss: 0.00004321
Iteration 19/1000 | Loss: 0.00003941
Iteration 20/1000 | Loss: 0.00003664
Iteration 21/1000 | Loss: 0.00003474
Iteration 22/1000 | Loss: 0.00003376
Iteration 23/1000 | Loss: 0.00003306
Iteration 24/1000 | Loss: 0.00003257
Iteration 25/1000 | Loss: 0.00003212
Iteration 26/1000 | Loss: 0.00003164
Iteration 27/1000 | Loss: 0.00003132
Iteration 28/1000 | Loss: 0.00003110
Iteration 29/1000 | Loss: 0.00003102
Iteration 30/1000 | Loss: 0.00003101
Iteration 31/1000 | Loss: 0.00003099
Iteration 32/1000 | Loss: 0.00003084
Iteration 33/1000 | Loss: 0.00003083
Iteration 34/1000 | Loss: 0.00003073
Iteration 35/1000 | Loss: 0.00003073
Iteration 36/1000 | Loss: 0.00003069
Iteration 37/1000 | Loss: 0.00003069
Iteration 38/1000 | Loss: 0.00003068
Iteration 39/1000 | Loss: 0.00003064
Iteration 40/1000 | Loss: 0.00003062
Iteration 41/1000 | Loss: 0.00003061
Iteration 42/1000 | Loss: 0.00003060
Iteration 43/1000 | Loss: 0.00003060
Iteration 44/1000 | Loss: 0.00003060
Iteration 45/1000 | Loss: 0.00003060
Iteration 46/1000 | Loss: 0.00003060
Iteration 47/1000 | Loss: 0.00003060
Iteration 48/1000 | Loss: 0.00003060
Iteration 49/1000 | Loss: 0.00003060
Iteration 50/1000 | Loss: 0.00003058
Iteration 51/1000 | Loss: 0.00003058
Iteration 52/1000 | Loss: 0.00003058
Iteration 53/1000 | Loss: 0.00003058
Iteration 54/1000 | Loss: 0.00003058
Iteration 55/1000 | Loss: 0.00003058
Iteration 56/1000 | Loss: 0.00003057
Iteration 57/1000 | Loss: 0.00003057
Iteration 58/1000 | Loss: 0.00003056
Iteration 59/1000 | Loss: 0.00003056
Iteration 60/1000 | Loss: 0.00003055
Iteration 61/1000 | Loss: 0.00003055
Iteration 62/1000 | Loss: 0.00003054
Iteration 63/1000 | Loss: 0.00003053
Iteration 64/1000 | Loss: 0.00003053
Iteration 65/1000 | Loss: 0.00003053
Iteration 66/1000 | Loss: 0.00003053
Iteration 67/1000 | Loss: 0.00003053
Iteration 68/1000 | Loss: 0.00003053
Iteration 69/1000 | Loss: 0.00003053
Iteration 70/1000 | Loss: 0.00003053
Iteration 71/1000 | Loss: 0.00003053
Iteration 72/1000 | Loss: 0.00003052
Iteration 73/1000 | Loss: 0.00003052
Iteration 74/1000 | Loss: 0.00003052
Iteration 75/1000 | Loss: 0.00003052
Iteration 76/1000 | Loss: 0.00003052
Iteration 77/1000 | Loss: 0.00003052
Iteration 78/1000 | Loss: 0.00003052
Iteration 79/1000 | Loss: 0.00003050
Iteration 80/1000 | Loss: 0.00003049
Iteration 81/1000 | Loss: 0.00003048
Iteration 82/1000 | Loss: 0.00003048
Iteration 83/1000 | Loss: 0.00003048
Iteration 84/1000 | Loss: 0.00003047
Iteration 85/1000 | Loss: 0.00003045
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003044
Iteration 90/1000 | Loss: 0.00003044
Iteration 91/1000 | Loss: 0.00003044
Iteration 92/1000 | Loss: 0.00003043
Iteration 93/1000 | Loss: 0.00003043
Iteration 94/1000 | Loss: 0.00003043
Iteration 95/1000 | Loss: 0.00003042
Iteration 96/1000 | Loss: 0.00003042
Iteration 97/1000 | Loss: 0.00003042
Iteration 98/1000 | Loss: 0.00003041
Iteration 99/1000 | Loss: 0.00003041
Iteration 100/1000 | Loss: 0.00003041
Iteration 101/1000 | Loss: 0.00003041
Iteration 102/1000 | Loss: 0.00003040
Iteration 103/1000 | Loss: 0.00003040
Iteration 104/1000 | Loss: 0.00003040
Iteration 105/1000 | Loss: 0.00003039
Iteration 106/1000 | Loss: 0.00003039
Iteration 107/1000 | Loss: 0.00003039
Iteration 108/1000 | Loss: 0.00003038
Iteration 109/1000 | Loss: 0.00003038
Iteration 110/1000 | Loss: 0.00003038
Iteration 111/1000 | Loss: 0.00003038
Iteration 112/1000 | Loss: 0.00003037
Iteration 113/1000 | Loss: 0.00003037
Iteration 114/1000 | Loss: 0.00003037
Iteration 115/1000 | Loss: 0.00003037
Iteration 116/1000 | Loss: 0.00003037
Iteration 117/1000 | Loss: 0.00003037
Iteration 118/1000 | Loss: 0.00003037
Iteration 119/1000 | Loss: 0.00003037
Iteration 120/1000 | Loss: 0.00003037
Iteration 121/1000 | Loss: 0.00003036
Iteration 122/1000 | Loss: 0.00003036
Iteration 123/1000 | Loss: 0.00003036
Iteration 124/1000 | Loss: 0.00003036
Iteration 125/1000 | Loss: 0.00003036
Iteration 126/1000 | Loss: 0.00003036
Iteration 127/1000 | Loss: 0.00003036
Iteration 128/1000 | Loss: 0.00003036
Iteration 129/1000 | Loss: 0.00003036
Iteration 130/1000 | Loss: 0.00003036
Iteration 131/1000 | Loss: 0.00003036
Iteration 132/1000 | Loss: 0.00003036
Iteration 133/1000 | Loss: 0.00003036
Iteration 134/1000 | Loss: 0.00003035
Iteration 135/1000 | Loss: 0.00003035
Iteration 136/1000 | Loss: 0.00003035
Iteration 137/1000 | Loss: 0.00003035
Iteration 138/1000 | Loss: 0.00003035
Iteration 139/1000 | Loss: 0.00003035
Iteration 140/1000 | Loss: 0.00003035
Iteration 141/1000 | Loss: 0.00003035
Iteration 142/1000 | Loss: 0.00003035
Iteration 143/1000 | Loss: 0.00003035
Iteration 144/1000 | Loss: 0.00003035
Iteration 145/1000 | Loss: 0.00003034
Iteration 146/1000 | Loss: 0.00003034
Iteration 147/1000 | Loss: 0.00003034
Iteration 148/1000 | Loss: 0.00003034
Iteration 149/1000 | Loss: 0.00003034
Iteration 150/1000 | Loss: 0.00003034
Iteration 151/1000 | Loss: 0.00003034
Iteration 152/1000 | Loss: 0.00003034
Iteration 153/1000 | Loss: 0.00003034
Iteration 154/1000 | Loss: 0.00003034
Iteration 155/1000 | Loss: 0.00003034
Iteration 156/1000 | Loss: 0.00003034
Iteration 157/1000 | Loss: 0.00003033
Iteration 158/1000 | Loss: 0.00003033
Iteration 159/1000 | Loss: 0.00003033
Iteration 160/1000 | Loss: 0.00003033
Iteration 161/1000 | Loss: 0.00003033
Iteration 162/1000 | Loss: 0.00003033
Iteration 163/1000 | Loss: 0.00003033
Iteration 164/1000 | Loss: 0.00003032
Iteration 165/1000 | Loss: 0.00003032
Iteration 166/1000 | Loss: 0.00003032
Iteration 167/1000 | Loss: 0.00003032
Iteration 168/1000 | Loss: 0.00003032
Iteration 169/1000 | Loss: 0.00003032
Iteration 170/1000 | Loss: 0.00003032
Iteration 171/1000 | Loss: 0.00003032
Iteration 172/1000 | Loss: 0.00003031
Iteration 173/1000 | Loss: 0.00003031
Iteration 174/1000 | Loss: 0.00003031
Iteration 175/1000 | Loss: 0.00003031
Iteration 176/1000 | Loss: 0.00003031
Iteration 177/1000 | Loss: 0.00003031
Iteration 178/1000 | Loss: 0.00003031
Iteration 179/1000 | Loss: 0.00003031
Iteration 180/1000 | Loss: 0.00003030
Iteration 181/1000 | Loss: 0.00003030
Iteration 182/1000 | Loss: 0.00003030
Iteration 183/1000 | Loss: 0.00003030
Iteration 184/1000 | Loss: 0.00003030
Iteration 185/1000 | Loss: 0.00003029
Iteration 186/1000 | Loss: 0.00003029
Iteration 187/1000 | Loss: 0.00003029
Iteration 188/1000 | Loss: 0.00003029
Iteration 189/1000 | Loss: 0.00003028
Iteration 190/1000 | Loss: 0.00003028
Iteration 191/1000 | Loss: 0.00003028
Iteration 192/1000 | Loss: 0.00003028
Iteration 193/1000 | Loss: 0.00003028
Iteration 194/1000 | Loss: 0.00003028
Iteration 195/1000 | Loss: 0.00003028
Iteration 196/1000 | Loss: 0.00003028
Iteration 197/1000 | Loss: 0.00003027
Iteration 198/1000 | Loss: 0.00003027
Iteration 199/1000 | Loss: 0.00003027
Iteration 200/1000 | Loss: 0.00003027
Iteration 201/1000 | Loss: 0.00003027
Iteration 202/1000 | Loss: 0.00003027
Iteration 203/1000 | Loss: 0.00003027
Iteration 204/1000 | Loss: 0.00003026
Iteration 205/1000 | Loss: 0.00003026
Iteration 206/1000 | Loss: 0.00003026
Iteration 207/1000 | Loss: 0.00003026
Iteration 208/1000 | Loss: 0.00003026
Iteration 209/1000 | Loss: 0.00003026
Iteration 210/1000 | Loss: 0.00003026
Iteration 211/1000 | Loss: 0.00003026
Iteration 212/1000 | Loss: 0.00003026
Iteration 213/1000 | Loss: 0.00003026
Iteration 214/1000 | Loss: 0.00003026
Iteration 215/1000 | Loss: 0.00003026
Iteration 216/1000 | Loss: 0.00003026
Iteration 217/1000 | Loss: 0.00003026
Iteration 218/1000 | Loss: 0.00003026
Iteration 219/1000 | Loss: 0.00003026
Iteration 220/1000 | Loss: 0.00003025
Iteration 221/1000 | Loss: 0.00003025
Iteration 222/1000 | Loss: 0.00003025
Iteration 223/1000 | Loss: 0.00003025
Iteration 224/1000 | Loss: 0.00003025
Iteration 225/1000 | Loss: 0.00003025
Iteration 226/1000 | Loss: 0.00003025
Iteration 227/1000 | Loss: 0.00003025
Iteration 228/1000 | Loss: 0.00003025
Iteration 229/1000 | Loss: 0.00003025
Iteration 230/1000 | Loss: 0.00003025
Iteration 231/1000 | Loss: 0.00003025
Iteration 232/1000 | Loss: 0.00003025
Iteration 233/1000 | Loss: 0.00003025
Iteration 234/1000 | Loss: 0.00003025
Iteration 235/1000 | Loss: 0.00003025
Iteration 236/1000 | Loss: 0.00003025
Iteration 237/1000 | Loss: 0.00003025
Iteration 238/1000 | Loss: 0.00003025
Iteration 239/1000 | Loss: 0.00003025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [3.0253873774199747e-05, 3.0253873774199747e-05, 3.0253873774199747e-05, 3.0253873774199747e-05, 3.0253873774199747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0253873774199747e-05

Optimization complete. Final v2v error: 4.40269136428833 mm

Highest mean error: 7.1773295402526855 mm for frame 55

Lowest mean error: 3.0233075618743896 mm for frame 104

Saving results

Total time: 99.15971827507019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739415
Iteration 2/25 | Loss: 0.00135475
Iteration 3/25 | Loss: 0.00127379
Iteration 4/25 | Loss: 0.00126228
Iteration 5/25 | Loss: 0.00125880
Iteration 6/25 | Loss: 0.00125812
Iteration 7/25 | Loss: 0.00125812
Iteration 8/25 | Loss: 0.00125812
Iteration 9/25 | Loss: 0.00125812
Iteration 10/25 | Loss: 0.00125812
Iteration 11/25 | Loss: 0.00125812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012581206392496824, 0.0012581206392496824, 0.0012581206392496824, 0.0012581206392496824, 0.0012581206392496824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012581206392496824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43148625
Iteration 2/25 | Loss: 0.00060225
Iteration 3/25 | Loss: 0.00060217
Iteration 4/25 | Loss: 0.00060217
Iteration 5/25 | Loss: 0.00060217
Iteration 6/25 | Loss: 0.00060217
Iteration 7/25 | Loss: 0.00060217
Iteration 8/25 | Loss: 0.00060217
Iteration 9/25 | Loss: 0.00060217
Iteration 10/25 | Loss: 0.00060217
Iteration 11/25 | Loss: 0.00060216
Iteration 12/25 | Loss: 0.00060216
Iteration 13/25 | Loss: 0.00060216
Iteration 14/25 | Loss: 0.00060216
Iteration 15/25 | Loss: 0.00060216
Iteration 16/25 | Loss: 0.00060216
Iteration 17/25 | Loss: 0.00060216
Iteration 18/25 | Loss: 0.00060216
Iteration 19/25 | Loss: 0.00060216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006021646549925208, 0.0006021646549925208, 0.0006021646549925208, 0.0006021646549925208, 0.0006021646549925208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006021646549925208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060216
Iteration 2/1000 | Loss: 0.00003869
Iteration 3/1000 | Loss: 0.00002958
Iteration 4/1000 | Loss: 0.00002645
Iteration 5/1000 | Loss: 0.00002525
Iteration 6/1000 | Loss: 0.00002431
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002243
Iteration 10/1000 | Loss: 0.00002210
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002118
Iteration 15/1000 | Loss: 0.00002105
Iteration 16/1000 | Loss: 0.00002102
Iteration 17/1000 | Loss: 0.00002091
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002088
Iteration 21/1000 | Loss: 0.00002087
Iteration 22/1000 | Loss: 0.00002087
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002081
Iteration 26/1000 | Loss: 0.00002080
Iteration 27/1000 | Loss: 0.00002079
Iteration 28/1000 | Loss: 0.00002079
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002078
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002077
Iteration 33/1000 | Loss: 0.00002077
Iteration 34/1000 | Loss: 0.00002077
Iteration 35/1000 | Loss: 0.00002077
Iteration 36/1000 | Loss: 0.00002076
Iteration 37/1000 | Loss: 0.00002076
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00002075
Iteration 40/1000 | Loss: 0.00002075
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002074
Iteration 43/1000 | Loss: 0.00002074
Iteration 44/1000 | Loss: 0.00002074
Iteration 45/1000 | Loss: 0.00002073
Iteration 46/1000 | Loss: 0.00002073
Iteration 47/1000 | Loss: 0.00002072
Iteration 48/1000 | Loss: 0.00002071
Iteration 49/1000 | Loss: 0.00002071
Iteration 50/1000 | Loss: 0.00002070
Iteration 51/1000 | Loss: 0.00002070
Iteration 52/1000 | Loss: 0.00002070
Iteration 53/1000 | Loss: 0.00002069
Iteration 54/1000 | Loss: 0.00002069
Iteration 55/1000 | Loss: 0.00002069
Iteration 56/1000 | Loss: 0.00002069
Iteration 57/1000 | Loss: 0.00002069
Iteration 58/1000 | Loss: 0.00002068
Iteration 59/1000 | Loss: 0.00002068
Iteration 60/1000 | Loss: 0.00002068
Iteration 61/1000 | Loss: 0.00002067
Iteration 62/1000 | Loss: 0.00002067
Iteration 63/1000 | Loss: 0.00002067
Iteration 64/1000 | Loss: 0.00002067
Iteration 65/1000 | Loss: 0.00002067
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.0664840121753514e-05, 2.0664840121753514e-05, 2.0664840121753514e-05, 2.0664840121753514e-05, 2.0664840121753514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0664840121753514e-05

Optimization complete. Final v2v error: 3.7855799198150635 mm

Highest mean error: 4.013648986816406 mm for frame 41

Lowest mean error: 3.663299798965454 mm for frame 98

Saving results

Total time: 34.59520435333252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803974
Iteration 2/25 | Loss: 0.00164283
Iteration 3/25 | Loss: 0.00136555
Iteration 4/25 | Loss: 0.00131632
Iteration 5/25 | Loss: 0.00130395
Iteration 6/25 | Loss: 0.00130136
Iteration 7/25 | Loss: 0.00130136
Iteration 8/25 | Loss: 0.00130136
Iteration 9/25 | Loss: 0.00130136
Iteration 10/25 | Loss: 0.00130136
Iteration 11/25 | Loss: 0.00130136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013013631105422974, 0.0013013631105422974, 0.0013013631105422974, 0.0013013631105422974, 0.0013013631105422974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013013631105422974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43213975
Iteration 2/25 | Loss: 0.00070537
Iteration 3/25 | Loss: 0.00070536
Iteration 4/25 | Loss: 0.00070536
Iteration 5/25 | Loss: 0.00070536
Iteration 6/25 | Loss: 0.00070536
Iteration 7/25 | Loss: 0.00070536
Iteration 8/25 | Loss: 0.00070536
Iteration 9/25 | Loss: 0.00070536
Iteration 10/25 | Loss: 0.00070536
Iteration 11/25 | Loss: 0.00070536
Iteration 12/25 | Loss: 0.00070536
Iteration 13/25 | Loss: 0.00070536
Iteration 14/25 | Loss: 0.00070536
Iteration 15/25 | Loss: 0.00070536
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007053621811792254, 0.0007053621811792254, 0.0007053621811792254, 0.0007053621811792254, 0.0007053621811792254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007053621811792254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070536
Iteration 2/1000 | Loss: 0.00003648
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002451
Iteration 5/1000 | Loss: 0.00002348
Iteration 6/1000 | Loss: 0.00002285
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002208
Iteration 9/1000 | Loss: 0.00002172
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002147
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002146
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002139
Iteration 17/1000 | Loss: 0.00002130
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002122
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002120
Iteration 25/1000 | Loss: 0.00002120
Iteration 26/1000 | Loss: 0.00002120
Iteration 27/1000 | Loss: 0.00002119
Iteration 28/1000 | Loss: 0.00002119
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002117
Iteration 34/1000 | Loss: 0.00002117
Iteration 35/1000 | Loss: 0.00002116
Iteration 36/1000 | Loss: 0.00002116
Iteration 37/1000 | Loss: 0.00002116
Iteration 38/1000 | Loss: 0.00002115
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002111
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002109
Iteration 49/1000 | Loss: 0.00002109
Iteration 50/1000 | Loss: 0.00002108
Iteration 51/1000 | Loss: 0.00002107
Iteration 52/1000 | Loss: 0.00002107
Iteration 53/1000 | Loss: 0.00002107
Iteration 54/1000 | Loss: 0.00002107
Iteration 55/1000 | Loss: 0.00002106
Iteration 56/1000 | Loss: 0.00002106
Iteration 57/1000 | Loss: 0.00002106
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002105
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00002105
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002104
Iteration 67/1000 | Loss: 0.00002104
Iteration 68/1000 | Loss: 0.00002104
Iteration 69/1000 | Loss: 0.00002104
Iteration 70/1000 | Loss: 0.00002104
Iteration 71/1000 | Loss: 0.00002104
Iteration 72/1000 | Loss: 0.00002104
Iteration 73/1000 | Loss: 0.00002104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.1037523765699007e-05, 2.1037523765699007e-05, 2.1037523765699007e-05, 2.1037523765699007e-05, 2.1037523765699007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1037523765699007e-05

Optimization complete. Final v2v error: 3.846372127532959 mm

Highest mean error: 4.423540115356445 mm for frame 191

Lowest mean error: 3.5227458477020264 mm for frame 0

Saving results

Total time: 34.10604166984558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00673700
Iteration 2/25 | Loss: 0.00156316
Iteration 3/25 | Loss: 0.00138840
Iteration 4/25 | Loss: 0.00128115
Iteration 5/25 | Loss: 0.00126978
Iteration 6/25 | Loss: 0.00126090
Iteration 7/25 | Loss: 0.00125770
Iteration 8/25 | Loss: 0.00125570
Iteration 9/25 | Loss: 0.00125477
Iteration 10/25 | Loss: 0.00125427
Iteration 11/25 | Loss: 0.00125389
Iteration 12/25 | Loss: 0.00125670
Iteration 13/25 | Loss: 0.00125259
Iteration 14/25 | Loss: 0.00125154
Iteration 15/25 | Loss: 0.00125107
Iteration 16/25 | Loss: 0.00125098
Iteration 17/25 | Loss: 0.00125098
Iteration 18/25 | Loss: 0.00125097
Iteration 19/25 | Loss: 0.00125097
Iteration 20/25 | Loss: 0.00125097
Iteration 21/25 | Loss: 0.00125097
Iteration 22/25 | Loss: 0.00125097
Iteration 23/25 | Loss: 0.00125097
Iteration 24/25 | Loss: 0.00125097
Iteration 25/25 | Loss: 0.00125097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.61855650
Iteration 2/25 | Loss: 0.00073859
Iteration 3/25 | Loss: 0.00073807
Iteration 4/25 | Loss: 0.00073807
Iteration 5/25 | Loss: 0.00073807
Iteration 6/25 | Loss: 0.00073807
Iteration 7/25 | Loss: 0.00073807
Iteration 8/25 | Loss: 0.00073807
Iteration 9/25 | Loss: 0.00073807
Iteration 10/25 | Loss: 0.00073807
Iteration 11/25 | Loss: 0.00073807
Iteration 12/25 | Loss: 0.00073807
Iteration 13/25 | Loss: 0.00073807
Iteration 14/25 | Loss: 0.00073807
Iteration 15/25 | Loss: 0.00073807
Iteration 16/25 | Loss: 0.00073807
Iteration 17/25 | Loss: 0.00073807
Iteration 18/25 | Loss: 0.00073807
Iteration 19/25 | Loss: 0.00073807
Iteration 20/25 | Loss: 0.00073807
Iteration 21/25 | Loss: 0.00073807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007380701717920601, 0.0007380701717920601, 0.0007380701717920601, 0.0007380701717920601, 0.0007380701717920601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007380701717920601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073807
Iteration 2/1000 | Loss: 0.00003362
Iteration 3/1000 | Loss: 0.00013819
Iteration 4/1000 | Loss: 0.00005760
Iteration 5/1000 | Loss: 0.00002456
Iteration 6/1000 | Loss: 0.00002167
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00004348
Iteration 9/1000 | Loss: 0.00008618
Iteration 10/1000 | Loss: 0.00001970
Iteration 11/1000 | Loss: 0.00001894
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001826
Iteration 14/1000 | Loss: 0.00010340
Iteration 15/1000 | Loss: 0.00003932
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00005977
Iteration 18/1000 | Loss: 0.00006457
Iteration 19/1000 | Loss: 0.00004996
Iteration 20/1000 | Loss: 0.00003442
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001743
Iteration 24/1000 | Loss: 0.00006236
Iteration 25/1000 | Loss: 0.00002197
Iteration 26/1000 | Loss: 0.00002375
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00008144
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00002409
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001701
Iteration 39/1000 | Loss: 0.00001700
Iteration 40/1000 | Loss: 0.00001700
Iteration 41/1000 | Loss: 0.00001700
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001698
Iteration 44/1000 | Loss: 0.00001698
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001696
Iteration 49/1000 | Loss: 0.00001696
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001694
Iteration 52/1000 | Loss: 0.00001694
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001687
Iteration 60/1000 | Loss: 0.00001687
Iteration 61/1000 | Loss: 0.00001687
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001683
Iteration 88/1000 | Loss: 0.00001683
Iteration 89/1000 | Loss: 0.00001683
Iteration 90/1000 | Loss: 0.00001683
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001683
Iteration 94/1000 | Loss: 0.00001683
Iteration 95/1000 | Loss: 0.00001683
Iteration 96/1000 | Loss: 0.00001683
Iteration 97/1000 | Loss: 0.00001683
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001682
Iteration 105/1000 | Loss: 0.00001682
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001681
Iteration 112/1000 | Loss: 0.00001681
Iteration 113/1000 | Loss: 0.00001681
Iteration 114/1000 | Loss: 0.00001681
Iteration 115/1000 | Loss: 0.00001681
Iteration 116/1000 | Loss: 0.00001681
Iteration 117/1000 | Loss: 0.00001681
Iteration 118/1000 | Loss: 0.00001681
Iteration 119/1000 | Loss: 0.00001681
Iteration 120/1000 | Loss: 0.00001680
Iteration 121/1000 | Loss: 0.00001680
Iteration 122/1000 | Loss: 0.00001680
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001680
Iteration 125/1000 | Loss: 0.00001680
Iteration 126/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.6804657207103446e-05, 1.6804657207103446e-05, 1.6804657207103446e-05, 1.6804657207103446e-05, 1.6804657207103446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6804657207103446e-05

Optimization complete. Final v2v error: 3.4657859802246094 mm

Highest mean error: 3.8486905097961426 mm for frame 88

Lowest mean error: 3.077097177505493 mm for frame 57

Saving results

Total time: 87.80734777450562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060369
Iteration 2/25 | Loss: 0.00280862
Iteration 3/25 | Loss: 0.00199502
Iteration 4/25 | Loss: 0.00168050
Iteration 5/25 | Loss: 0.00152417
Iteration 6/25 | Loss: 0.00147007
Iteration 7/25 | Loss: 0.00144495
Iteration 8/25 | Loss: 0.00142804
Iteration 9/25 | Loss: 0.00135711
Iteration 10/25 | Loss: 0.00135120
Iteration 11/25 | Loss: 0.00130162
Iteration 12/25 | Loss: 0.00129479
Iteration 13/25 | Loss: 0.00129243
Iteration 14/25 | Loss: 0.00129399
Iteration 15/25 | Loss: 0.00129230
Iteration 16/25 | Loss: 0.00129338
Iteration 17/25 | Loss: 0.00128782
Iteration 18/25 | Loss: 0.00128571
Iteration 19/25 | Loss: 0.00128530
Iteration 20/25 | Loss: 0.00128513
Iteration 21/25 | Loss: 0.00128513
Iteration 22/25 | Loss: 0.00128512
Iteration 23/25 | Loss: 0.00128512
Iteration 24/25 | Loss: 0.00128512
Iteration 25/25 | Loss: 0.00128512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96694994
Iteration 2/25 | Loss: 0.00107378
Iteration 3/25 | Loss: 0.00107378
Iteration 4/25 | Loss: 0.00106174
Iteration 5/25 | Loss: 0.00106174
Iteration 6/25 | Loss: 0.00106174
Iteration 7/25 | Loss: 0.00106173
Iteration 8/25 | Loss: 0.00106173
Iteration 9/25 | Loss: 0.00106173
Iteration 10/25 | Loss: 0.00106173
Iteration 11/25 | Loss: 0.00106173
Iteration 12/25 | Loss: 0.00106173
Iteration 13/25 | Loss: 0.00106173
Iteration 14/25 | Loss: 0.00106173
Iteration 15/25 | Loss: 0.00106173
Iteration 16/25 | Loss: 0.00106173
Iteration 17/25 | Loss: 0.00106173
Iteration 18/25 | Loss: 0.00106173
Iteration 19/25 | Loss: 0.00106173
Iteration 20/25 | Loss: 0.00106173
Iteration 21/25 | Loss: 0.00106173
Iteration 22/25 | Loss: 0.00106173
Iteration 23/25 | Loss: 0.00106173
Iteration 24/25 | Loss: 0.00106173
Iteration 25/25 | Loss: 0.00106173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106173
Iteration 2/1000 | Loss: 0.00004084
Iteration 3/1000 | Loss: 0.00013951
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00005354
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00004443
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001979
Iteration 10/1000 | Loss: 0.00002887
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00002900
Iteration 13/1000 | Loss: 0.00045909
Iteration 14/1000 | Loss: 0.00076895
Iteration 15/1000 | Loss: 0.00011137
Iteration 16/1000 | Loss: 0.00004661
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001912
Iteration 20/1000 | Loss: 0.00001911
Iteration 21/1000 | Loss: 0.00004589
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001903
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001902
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001902
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00038991
Iteration 35/1000 | Loss: 0.00031721
Iteration 36/1000 | Loss: 0.00003679
Iteration 37/1000 | Loss: 0.00003909
Iteration 38/1000 | Loss: 0.00003798
Iteration 39/1000 | Loss: 0.00002151
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00001915
Iteration 42/1000 | Loss: 0.00001914
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00039506
Iteration 54/1000 | Loss: 0.00002612
Iteration 55/1000 | Loss: 0.00002934
Iteration 56/1000 | Loss: 0.00002324
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00003144
Iteration 59/1000 | Loss: 0.00001922
Iteration 60/1000 | Loss: 0.00005192
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00003052
Iteration 63/1000 | Loss: 0.00001738
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00002494
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00006168
Iteration 80/1000 | Loss: 0.00001686
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001679
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001677
Iteration 108/1000 | Loss: 0.00001677
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00003833
Iteration 111/1000 | Loss: 0.00001678
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001675
Iteration 114/1000 | Loss: 0.00001675
Iteration 115/1000 | Loss: 0.00001675
Iteration 116/1000 | Loss: 0.00001674
Iteration 117/1000 | Loss: 0.00001674
Iteration 118/1000 | Loss: 0.00001674
Iteration 119/1000 | Loss: 0.00001674
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001673
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001672
Iteration 127/1000 | Loss: 0.00001672
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001672
Iteration 134/1000 | Loss: 0.00001671
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001669
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001669
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001669
Iteration 144/1000 | Loss: 0.00001669
Iteration 145/1000 | Loss: 0.00001669
Iteration 146/1000 | Loss: 0.00001669
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001669
Iteration 152/1000 | Loss: 0.00001668
Iteration 153/1000 | Loss: 0.00001668
Iteration 154/1000 | Loss: 0.00001668
Iteration 155/1000 | Loss: 0.00001668
Iteration 156/1000 | Loss: 0.00001668
Iteration 157/1000 | Loss: 0.00001668
Iteration 158/1000 | Loss: 0.00001668
Iteration 159/1000 | Loss: 0.00001668
Iteration 160/1000 | Loss: 0.00001668
Iteration 161/1000 | Loss: 0.00001668
Iteration 162/1000 | Loss: 0.00001668
Iteration 163/1000 | Loss: 0.00001668
Iteration 164/1000 | Loss: 0.00001668
Iteration 165/1000 | Loss: 0.00001668
Iteration 166/1000 | Loss: 0.00001668
Iteration 167/1000 | Loss: 0.00001668
Iteration 168/1000 | Loss: 0.00001668
Iteration 169/1000 | Loss: 0.00001668
Iteration 170/1000 | Loss: 0.00001668
Iteration 171/1000 | Loss: 0.00001668
Iteration 172/1000 | Loss: 0.00001667
Iteration 173/1000 | Loss: 0.00001667
Iteration 174/1000 | Loss: 0.00001667
Iteration 175/1000 | Loss: 0.00001667
Iteration 176/1000 | Loss: 0.00001667
Iteration 177/1000 | Loss: 0.00001667
Iteration 178/1000 | Loss: 0.00001667
Iteration 179/1000 | Loss: 0.00001667
Iteration 180/1000 | Loss: 0.00001667
Iteration 181/1000 | Loss: 0.00001667
Iteration 182/1000 | Loss: 0.00001667
Iteration 183/1000 | Loss: 0.00001666
Iteration 184/1000 | Loss: 0.00001666
Iteration 185/1000 | Loss: 0.00001666
Iteration 186/1000 | Loss: 0.00001666
Iteration 187/1000 | Loss: 0.00001666
Iteration 188/1000 | Loss: 0.00001666
Iteration 189/1000 | Loss: 0.00001666
Iteration 190/1000 | Loss: 0.00001666
Iteration 191/1000 | Loss: 0.00001666
Iteration 192/1000 | Loss: 0.00001666
Iteration 193/1000 | Loss: 0.00001666
Iteration 194/1000 | Loss: 0.00001666
Iteration 195/1000 | Loss: 0.00001666
Iteration 196/1000 | Loss: 0.00001666
Iteration 197/1000 | Loss: 0.00001666
Iteration 198/1000 | Loss: 0.00001666
Iteration 199/1000 | Loss: 0.00001666
Iteration 200/1000 | Loss: 0.00001666
Iteration 201/1000 | Loss: 0.00001665
Iteration 202/1000 | Loss: 0.00001665
Iteration 203/1000 | Loss: 0.00001665
Iteration 204/1000 | Loss: 0.00001665
Iteration 205/1000 | Loss: 0.00001665
Iteration 206/1000 | Loss: 0.00001665
Iteration 207/1000 | Loss: 0.00001665
Iteration 208/1000 | Loss: 0.00001665
Iteration 209/1000 | Loss: 0.00001665
Iteration 210/1000 | Loss: 0.00001665
Iteration 211/1000 | Loss: 0.00001665
Iteration 212/1000 | Loss: 0.00001665
Iteration 213/1000 | Loss: 0.00001665
Iteration 214/1000 | Loss: 0.00001665
Iteration 215/1000 | Loss: 0.00001665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.665365562075749e-05, 1.665365562075749e-05, 1.665365562075749e-05, 1.665365562075749e-05, 1.665365562075749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.665365562075749e-05

Optimization complete. Final v2v error: 3.3836007118225098 mm

Highest mean error: 4.450742721557617 mm for frame 150

Lowest mean error: 3.14141583442688 mm for frame 26

Saving results

Total time: 105.39447331428528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597609
Iteration 2/25 | Loss: 0.00154246
Iteration 3/25 | Loss: 0.00132168
Iteration 4/25 | Loss: 0.00129679
Iteration 5/25 | Loss: 0.00129262
Iteration 6/25 | Loss: 0.00129100
Iteration 7/25 | Loss: 0.00129095
Iteration 8/25 | Loss: 0.00129095
Iteration 9/25 | Loss: 0.00129095
Iteration 10/25 | Loss: 0.00129095
Iteration 11/25 | Loss: 0.00129095
Iteration 12/25 | Loss: 0.00129095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012909461511299014, 0.0012909461511299014, 0.0012909461511299014, 0.0012909461511299014, 0.0012909461511299014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012909461511299014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23282564
Iteration 2/25 | Loss: 0.00073340
Iteration 3/25 | Loss: 0.00073340
Iteration 4/25 | Loss: 0.00073340
Iteration 5/25 | Loss: 0.00073340
Iteration 6/25 | Loss: 0.00073340
Iteration 7/25 | Loss: 0.00073340
Iteration 8/25 | Loss: 0.00073340
Iteration 9/25 | Loss: 0.00073340
Iteration 10/25 | Loss: 0.00073340
Iteration 11/25 | Loss: 0.00073340
Iteration 12/25 | Loss: 0.00073340
Iteration 13/25 | Loss: 0.00073340
Iteration 14/25 | Loss: 0.00073340
Iteration 15/25 | Loss: 0.00073340
Iteration 16/25 | Loss: 0.00073340
Iteration 17/25 | Loss: 0.00073340
Iteration 18/25 | Loss: 0.00073340
Iteration 19/25 | Loss: 0.00073340
Iteration 20/25 | Loss: 0.00073340
Iteration 21/25 | Loss: 0.00073340
Iteration 22/25 | Loss: 0.00073340
Iteration 23/25 | Loss: 0.00073340
Iteration 24/25 | Loss: 0.00073340
Iteration 25/25 | Loss: 0.00073340

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073340
Iteration 2/1000 | Loss: 0.00005061
Iteration 3/1000 | Loss: 0.00003180
Iteration 4/1000 | Loss: 0.00002320
Iteration 5/1000 | Loss: 0.00002132
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001891
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001835
Iteration 14/1000 | Loss: 0.00001835
Iteration 15/1000 | Loss: 0.00001834
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001831
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001813
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001811
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001809
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001807
Iteration 40/1000 | Loss: 0.00001807
Iteration 41/1000 | Loss: 0.00001806
Iteration 42/1000 | Loss: 0.00001806
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001805
Iteration 48/1000 | Loss: 0.00001805
Iteration 49/1000 | Loss: 0.00001805
Iteration 50/1000 | Loss: 0.00001805
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001804
Iteration 53/1000 | Loss: 0.00001804
Iteration 54/1000 | Loss: 0.00001804
Iteration 55/1000 | Loss: 0.00001803
Iteration 56/1000 | Loss: 0.00001803
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001803
Iteration 59/1000 | Loss: 0.00001803
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001802
Iteration 62/1000 | Loss: 0.00001802
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001802
Iteration 65/1000 | Loss: 0.00001802
Iteration 66/1000 | Loss: 0.00001802
Iteration 67/1000 | Loss: 0.00001802
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001801
Iteration 70/1000 | Loss: 0.00001801
Iteration 71/1000 | Loss: 0.00001801
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001799
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001795
Iteration 87/1000 | Loss: 0.00001795
Iteration 88/1000 | Loss: 0.00001795
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00001795
Iteration 91/1000 | Loss: 0.00001795
Iteration 92/1000 | Loss: 0.00001795
Iteration 93/1000 | Loss: 0.00001795
Iteration 94/1000 | Loss: 0.00001794
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001793
Iteration 99/1000 | Loss: 0.00001793
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001793
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001793
Iteration 104/1000 | Loss: 0.00001793
Iteration 105/1000 | Loss: 0.00001793
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001792
Iteration 108/1000 | Loss: 0.00001792
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001791
Iteration 111/1000 | Loss: 0.00001791
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001791
Iteration 114/1000 | Loss: 0.00001791
Iteration 115/1000 | Loss: 0.00001791
Iteration 116/1000 | Loss: 0.00001791
Iteration 117/1000 | Loss: 0.00001790
Iteration 118/1000 | Loss: 0.00001790
Iteration 119/1000 | Loss: 0.00001790
Iteration 120/1000 | Loss: 0.00001790
Iteration 121/1000 | Loss: 0.00001790
Iteration 122/1000 | Loss: 0.00001790
Iteration 123/1000 | Loss: 0.00001789
Iteration 124/1000 | Loss: 0.00001789
Iteration 125/1000 | Loss: 0.00001789
Iteration 126/1000 | Loss: 0.00001789
Iteration 127/1000 | Loss: 0.00001789
Iteration 128/1000 | Loss: 0.00001789
Iteration 129/1000 | Loss: 0.00001788
Iteration 130/1000 | Loss: 0.00001788
Iteration 131/1000 | Loss: 0.00001788
Iteration 132/1000 | Loss: 0.00001788
Iteration 133/1000 | Loss: 0.00001788
Iteration 134/1000 | Loss: 0.00001787
Iteration 135/1000 | Loss: 0.00001787
Iteration 136/1000 | Loss: 0.00001787
Iteration 137/1000 | Loss: 0.00001787
Iteration 138/1000 | Loss: 0.00001787
Iteration 139/1000 | Loss: 0.00001787
Iteration 140/1000 | Loss: 0.00001787
Iteration 141/1000 | Loss: 0.00001787
Iteration 142/1000 | Loss: 0.00001787
Iteration 143/1000 | Loss: 0.00001787
Iteration 144/1000 | Loss: 0.00001787
Iteration 145/1000 | Loss: 0.00001787
Iteration 146/1000 | Loss: 0.00001787
Iteration 147/1000 | Loss: 0.00001787
Iteration 148/1000 | Loss: 0.00001786
Iteration 149/1000 | Loss: 0.00001786
Iteration 150/1000 | Loss: 0.00001786
Iteration 151/1000 | Loss: 0.00001786
Iteration 152/1000 | Loss: 0.00001786
Iteration 153/1000 | Loss: 0.00001786
Iteration 154/1000 | Loss: 0.00001786
Iteration 155/1000 | Loss: 0.00001786
Iteration 156/1000 | Loss: 0.00001786
Iteration 157/1000 | Loss: 0.00001786
Iteration 158/1000 | Loss: 0.00001786
Iteration 159/1000 | Loss: 0.00001786
Iteration 160/1000 | Loss: 0.00001786
Iteration 161/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7863547327579e-05, 1.7863547327579e-05, 1.7863547327579e-05, 1.7863547327579e-05, 1.7863547327579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7863547327579e-05

Optimization complete. Final v2v error: 3.4201138019561768 mm

Highest mean error: 4.971380710601807 mm for frame 56

Lowest mean error: 3.064934015274048 mm for frame 143

Saving results

Total time: 35.80129408836365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025012
Iteration 2/25 | Loss: 0.00150924
Iteration 3/25 | Loss: 0.00130445
Iteration 4/25 | Loss: 0.00123283
Iteration 5/25 | Loss: 0.00122730
Iteration 6/25 | Loss: 0.00122047
Iteration 7/25 | Loss: 0.00121950
Iteration 8/25 | Loss: 0.00121909
Iteration 9/25 | Loss: 0.00121908
Iteration 10/25 | Loss: 0.00121850
Iteration 11/25 | Loss: 0.00121807
Iteration 12/25 | Loss: 0.00121798
Iteration 13/25 | Loss: 0.00121796
Iteration 14/25 | Loss: 0.00121796
Iteration 15/25 | Loss: 0.00121796
Iteration 16/25 | Loss: 0.00121796
Iteration 17/25 | Loss: 0.00121796
Iteration 18/25 | Loss: 0.00121796
Iteration 19/25 | Loss: 0.00121796
Iteration 20/25 | Loss: 0.00121796
Iteration 21/25 | Loss: 0.00121796
Iteration 22/25 | Loss: 0.00121796
Iteration 23/25 | Loss: 0.00121796
Iteration 24/25 | Loss: 0.00121795
Iteration 25/25 | Loss: 0.00121795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63937664
Iteration 2/25 | Loss: 0.00100789
Iteration 3/25 | Loss: 0.00078022
Iteration 4/25 | Loss: 0.00078022
Iteration 5/25 | Loss: 0.00078022
Iteration 6/25 | Loss: 0.00078022
Iteration 7/25 | Loss: 0.00078022
Iteration 8/25 | Loss: 0.00078022
Iteration 9/25 | Loss: 0.00078022
Iteration 10/25 | Loss: 0.00078022
Iteration 11/25 | Loss: 0.00078022
Iteration 12/25 | Loss: 0.00078022
Iteration 13/25 | Loss: 0.00078022
Iteration 14/25 | Loss: 0.00078022
Iteration 15/25 | Loss: 0.00078022
Iteration 16/25 | Loss: 0.00078022
Iteration 17/25 | Loss: 0.00078022
Iteration 18/25 | Loss: 0.00078022
Iteration 19/25 | Loss: 0.00078022
Iteration 20/25 | Loss: 0.00078022
Iteration 21/25 | Loss: 0.00078022
Iteration 22/25 | Loss: 0.00078022
Iteration 23/25 | Loss: 0.00078022
Iteration 24/25 | Loss: 0.00078022
Iteration 25/25 | Loss: 0.00078022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078022
Iteration 2/1000 | Loss: 0.00002816
Iteration 3/1000 | Loss: 0.00001938
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001655
Iteration 6/1000 | Loss: 0.00001570
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001429
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001363
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001350
Iteration 81/1000 | Loss: 0.00001350
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.3499506167136133e-05, 1.3499506167136133e-05, 1.3499506167136133e-05, 1.3499506167136133e-05, 1.3499506167136133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3499506167136133e-05

Optimization complete. Final v2v error: 3.138740062713623 mm

Highest mean error: 3.6416893005371094 mm for frame 222

Lowest mean error: 2.7568790912628174 mm for frame 258

Saving results

Total time: 53.524848222732544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971383
Iteration 2/25 | Loss: 0.00164418
Iteration 3/25 | Loss: 0.00145892
Iteration 4/25 | Loss: 0.00143011
Iteration 5/25 | Loss: 0.00144428
Iteration 6/25 | Loss: 0.00145092
Iteration 7/25 | Loss: 0.00138498
Iteration 8/25 | Loss: 0.00133734
Iteration 9/25 | Loss: 0.00132092
Iteration 10/25 | Loss: 0.00131359
Iteration 11/25 | Loss: 0.00133881
Iteration 12/25 | Loss: 0.00128133
Iteration 13/25 | Loss: 0.00127240
Iteration 14/25 | Loss: 0.00128111
Iteration 15/25 | Loss: 0.00128762
Iteration 16/25 | Loss: 0.00128166
Iteration 17/25 | Loss: 0.00127839
Iteration 18/25 | Loss: 0.00128042
Iteration 19/25 | Loss: 0.00127469
Iteration 20/25 | Loss: 0.00127390
Iteration 21/25 | Loss: 0.00127471
Iteration 22/25 | Loss: 0.00127429
Iteration 23/25 | Loss: 0.00127898
Iteration 24/25 | Loss: 0.00127900
Iteration 25/25 | Loss: 0.00127333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.05847502
Iteration 2/25 | Loss: 0.00088001
Iteration 3/25 | Loss: 0.00088000
Iteration 4/25 | Loss: 0.00088000
Iteration 5/25 | Loss: 0.00088000
Iteration 6/25 | Loss: 0.00088000
Iteration 7/25 | Loss: 0.00087999
Iteration 8/25 | Loss: 0.00087999
Iteration 9/25 | Loss: 0.00087999
Iteration 10/25 | Loss: 0.00087999
Iteration 11/25 | Loss: 0.00087999
Iteration 12/25 | Loss: 0.00087999
Iteration 13/25 | Loss: 0.00087999
Iteration 14/25 | Loss: 0.00087999
Iteration 15/25 | Loss: 0.00087999
Iteration 16/25 | Loss: 0.00087999
Iteration 17/25 | Loss: 0.00087999
Iteration 18/25 | Loss: 0.00087999
Iteration 19/25 | Loss: 0.00087999
Iteration 20/25 | Loss: 0.00087999
Iteration 21/25 | Loss: 0.00087999
Iteration 22/25 | Loss: 0.00087999
Iteration 23/25 | Loss: 0.00087999
Iteration 24/25 | Loss: 0.00087999
Iteration 25/25 | Loss: 0.00087999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087999
Iteration 2/1000 | Loss: 0.00009951
Iteration 3/1000 | Loss: 0.00160230
Iteration 4/1000 | Loss: 0.00007263
Iteration 5/1000 | Loss: 0.00018205
Iteration 6/1000 | Loss: 0.00009474
Iteration 7/1000 | Loss: 0.00004144
Iteration 8/1000 | Loss: 0.00011610
Iteration 9/1000 | Loss: 0.00067849
Iteration 10/1000 | Loss: 0.00046093
Iteration 11/1000 | Loss: 0.00052682
Iteration 12/1000 | Loss: 0.00024453
Iteration 13/1000 | Loss: 0.00042195
Iteration 14/1000 | Loss: 0.00043063
Iteration 15/1000 | Loss: 0.00015663
Iteration 16/1000 | Loss: 0.00049267
Iteration 17/1000 | Loss: 0.00053729
Iteration 18/1000 | Loss: 0.00051244
Iteration 19/1000 | Loss: 0.00029228
Iteration 20/1000 | Loss: 0.00014577
Iteration 21/1000 | Loss: 0.00007712
Iteration 22/1000 | Loss: 0.00010711
Iteration 23/1000 | Loss: 0.00007334
Iteration 24/1000 | Loss: 0.00003237
Iteration 25/1000 | Loss: 0.00004118
Iteration 26/1000 | Loss: 0.00008416
Iteration 27/1000 | Loss: 0.00003047
Iteration 28/1000 | Loss: 0.00005046
Iteration 29/1000 | Loss: 0.00010704
Iteration 30/1000 | Loss: 0.00003317
Iteration 31/1000 | Loss: 0.00025508
Iteration 32/1000 | Loss: 0.00016857
Iteration 33/1000 | Loss: 0.00026176
Iteration 34/1000 | Loss: 0.00011119
Iteration 35/1000 | Loss: 0.00009090
Iteration 36/1000 | Loss: 0.00007732
Iteration 37/1000 | Loss: 0.00007803
Iteration 38/1000 | Loss: 0.00002768
Iteration 39/1000 | Loss: 0.00006266
Iteration 40/1000 | Loss: 0.00004922
Iteration 41/1000 | Loss: 0.00005399
Iteration 42/1000 | Loss: 0.00004792
Iteration 43/1000 | Loss: 0.00002543
Iteration 44/1000 | Loss: 0.00002485
Iteration 45/1000 | Loss: 0.00002438
Iteration 46/1000 | Loss: 0.00002391
Iteration 47/1000 | Loss: 0.00002339
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002045
Iteration 54/1000 | Loss: 0.00002010
Iteration 55/1000 | Loss: 0.00001996
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001979
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001962
Iteration 60/1000 | Loss: 0.00001959
Iteration 61/1000 | Loss: 0.00001957
Iteration 62/1000 | Loss: 0.00001957
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001953
Iteration 68/1000 | Loss: 0.00001952
Iteration 69/1000 | Loss: 0.00001952
Iteration 70/1000 | Loss: 0.00001952
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001950
Iteration 76/1000 | Loss: 0.00001950
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001948
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001947
Iteration 86/1000 | Loss: 0.00001947
Iteration 87/1000 | Loss: 0.00001947
Iteration 88/1000 | Loss: 0.00001947
Iteration 89/1000 | Loss: 0.00001946
Iteration 90/1000 | Loss: 0.00001946
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001946
Iteration 93/1000 | Loss: 0.00001946
Iteration 94/1000 | Loss: 0.00001946
Iteration 95/1000 | Loss: 0.00001946
Iteration 96/1000 | Loss: 0.00001946
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001946
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001945
Iteration 107/1000 | Loss: 0.00001945
Iteration 108/1000 | Loss: 0.00001945
Iteration 109/1000 | Loss: 0.00001945
Iteration 110/1000 | Loss: 0.00001945
Iteration 111/1000 | Loss: 0.00001944
Iteration 112/1000 | Loss: 0.00001944
Iteration 113/1000 | Loss: 0.00001944
Iteration 114/1000 | Loss: 0.00001944
Iteration 115/1000 | Loss: 0.00001944
Iteration 116/1000 | Loss: 0.00001944
Iteration 117/1000 | Loss: 0.00001944
Iteration 118/1000 | Loss: 0.00001943
Iteration 119/1000 | Loss: 0.00001943
Iteration 120/1000 | Loss: 0.00001943
Iteration 121/1000 | Loss: 0.00001943
Iteration 122/1000 | Loss: 0.00001943
Iteration 123/1000 | Loss: 0.00001943
Iteration 124/1000 | Loss: 0.00001943
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001943
Iteration 127/1000 | Loss: 0.00001942
Iteration 128/1000 | Loss: 0.00001942
Iteration 129/1000 | Loss: 0.00001942
Iteration 130/1000 | Loss: 0.00001942
Iteration 131/1000 | Loss: 0.00001942
Iteration 132/1000 | Loss: 0.00001942
Iteration 133/1000 | Loss: 0.00001942
Iteration 134/1000 | Loss: 0.00001942
Iteration 135/1000 | Loss: 0.00001942
Iteration 136/1000 | Loss: 0.00001942
Iteration 137/1000 | Loss: 0.00001942
Iteration 138/1000 | Loss: 0.00001942
Iteration 139/1000 | Loss: 0.00001942
Iteration 140/1000 | Loss: 0.00001941
Iteration 141/1000 | Loss: 0.00001941
Iteration 142/1000 | Loss: 0.00001941
Iteration 143/1000 | Loss: 0.00001941
Iteration 144/1000 | Loss: 0.00001941
Iteration 145/1000 | Loss: 0.00001941
Iteration 146/1000 | Loss: 0.00001941
Iteration 147/1000 | Loss: 0.00001941
Iteration 148/1000 | Loss: 0.00001941
Iteration 149/1000 | Loss: 0.00001941
Iteration 150/1000 | Loss: 0.00001941
Iteration 151/1000 | Loss: 0.00001941
Iteration 152/1000 | Loss: 0.00001941
Iteration 153/1000 | Loss: 0.00001941
Iteration 154/1000 | Loss: 0.00001941
Iteration 155/1000 | Loss: 0.00001941
Iteration 156/1000 | Loss: 0.00001941
Iteration 157/1000 | Loss: 0.00001940
Iteration 158/1000 | Loss: 0.00001940
Iteration 159/1000 | Loss: 0.00001940
Iteration 160/1000 | Loss: 0.00001940
Iteration 161/1000 | Loss: 0.00001940
Iteration 162/1000 | Loss: 0.00001940
Iteration 163/1000 | Loss: 0.00001940
Iteration 164/1000 | Loss: 0.00001940
Iteration 165/1000 | Loss: 0.00001940
Iteration 166/1000 | Loss: 0.00001940
Iteration 167/1000 | Loss: 0.00001940
Iteration 168/1000 | Loss: 0.00001940
Iteration 169/1000 | Loss: 0.00001940
Iteration 170/1000 | Loss: 0.00001940
Iteration 171/1000 | Loss: 0.00001940
Iteration 172/1000 | Loss: 0.00001940
Iteration 173/1000 | Loss: 0.00001940
Iteration 174/1000 | Loss: 0.00001940
Iteration 175/1000 | Loss: 0.00001940
Iteration 176/1000 | Loss: 0.00001940
Iteration 177/1000 | Loss: 0.00001940
Iteration 178/1000 | Loss: 0.00001940
Iteration 179/1000 | Loss: 0.00001940
Iteration 180/1000 | Loss: 0.00001940
Iteration 181/1000 | Loss: 0.00001940
Iteration 182/1000 | Loss: 0.00001940
Iteration 183/1000 | Loss: 0.00001940
Iteration 184/1000 | Loss: 0.00001940
Iteration 185/1000 | Loss: 0.00001940
Iteration 186/1000 | Loss: 0.00001940
Iteration 187/1000 | Loss: 0.00001940
Iteration 188/1000 | Loss: 0.00001940
Iteration 189/1000 | Loss: 0.00001940
Iteration 190/1000 | Loss: 0.00001940
Iteration 191/1000 | Loss: 0.00001940
Iteration 192/1000 | Loss: 0.00001940
Iteration 193/1000 | Loss: 0.00001940
Iteration 194/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.9403756596148014e-05, 1.9403756596148014e-05, 1.9403756596148014e-05, 1.9403756596148014e-05, 1.9403756596148014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9403756596148014e-05

Optimization complete. Final v2v error: 3.4522764682769775 mm

Highest mean error: 12.114686965942383 mm for frame 88

Lowest mean error: 2.889575481414795 mm for frame 32

Saving results

Total time: 132.26890301704407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964822
Iteration 2/25 | Loss: 0.00265975
Iteration 3/25 | Loss: 0.00190653
Iteration 4/25 | Loss: 0.00171611
Iteration 5/25 | Loss: 0.00160208
Iteration 6/25 | Loss: 0.00158235
Iteration 7/25 | Loss: 0.00140612
Iteration 8/25 | Loss: 0.00136618
Iteration 9/25 | Loss: 0.00132551
Iteration 10/25 | Loss: 0.00131322
Iteration 11/25 | Loss: 0.00132399
Iteration 12/25 | Loss: 0.00129982
Iteration 13/25 | Loss: 0.00128149
Iteration 14/25 | Loss: 0.00127416
Iteration 15/25 | Loss: 0.00126915
Iteration 16/25 | Loss: 0.00126552
Iteration 17/25 | Loss: 0.00125996
Iteration 18/25 | Loss: 0.00126175
Iteration 19/25 | Loss: 0.00126102
Iteration 20/25 | Loss: 0.00125536
Iteration 21/25 | Loss: 0.00125433
Iteration 22/25 | Loss: 0.00125423
Iteration 23/25 | Loss: 0.00125422
Iteration 24/25 | Loss: 0.00125420
Iteration 25/25 | Loss: 0.00125420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41657805
Iteration 2/25 | Loss: 0.00073702
Iteration 3/25 | Loss: 0.00068527
Iteration 4/25 | Loss: 0.00068527
Iteration 5/25 | Loss: 0.00068527
Iteration 6/25 | Loss: 0.00068527
Iteration 7/25 | Loss: 0.00068527
Iteration 8/25 | Loss: 0.00068527
Iteration 9/25 | Loss: 0.00068527
Iteration 10/25 | Loss: 0.00068527
Iteration 11/25 | Loss: 0.00068527
Iteration 12/25 | Loss: 0.00068527
Iteration 13/25 | Loss: 0.00068527
Iteration 14/25 | Loss: 0.00068527
Iteration 15/25 | Loss: 0.00068527
Iteration 16/25 | Loss: 0.00068527
Iteration 17/25 | Loss: 0.00068527
Iteration 18/25 | Loss: 0.00068527
Iteration 19/25 | Loss: 0.00068527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006852693040855229, 0.0006852693040855229, 0.0006852693040855229, 0.0006852693040855229, 0.0006852693040855229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006852693040855229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068527
Iteration 2/1000 | Loss: 0.00023317
Iteration 3/1000 | Loss: 0.00019495
Iteration 4/1000 | Loss: 0.00004107
Iteration 5/1000 | Loss: 0.00037576
Iteration 6/1000 | Loss: 0.00025310
Iteration 7/1000 | Loss: 0.00005510
Iteration 8/1000 | Loss: 0.00005253
Iteration 9/1000 | Loss: 0.00003374
Iteration 10/1000 | Loss: 0.00004326
Iteration 11/1000 | Loss: 0.00002952
Iteration 12/1000 | Loss: 0.00006828
Iteration 13/1000 | Loss: 0.00003312
Iteration 14/1000 | Loss: 0.00002740
Iteration 15/1000 | Loss: 0.00002622
Iteration 16/1000 | Loss: 0.00017611
Iteration 17/1000 | Loss: 0.00002740
Iteration 18/1000 | Loss: 0.00005899
Iteration 19/1000 | Loss: 0.00006004
Iteration 20/1000 | Loss: 0.00029437
Iteration 21/1000 | Loss: 0.00002578
Iteration 22/1000 | Loss: 0.00002383
Iteration 23/1000 | Loss: 0.00003174
Iteration 24/1000 | Loss: 0.00002294
Iteration 25/1000 | Loss: 0.00003454
Iteration 26/1000 | Loss: 0.00002248
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002177
Iteration 31/1000 | Loss: 0.00003607
Iteration 32/1000 | Loss: 0.00002155
Iteration 33/1000 | Loss: 0.00002143
Iteration 34/1000 | Loss: 0.00002143
Iteration 35/1000 | Loss: 0.00002143
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002142
Iteration 38/1000 | Loss: 0.00002142
Iteration 39/1000 | Loss: 0.00002142
Iteration 40/1000 | Loss: 0.00002142
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002142
Iteration 43/1000 | Loss: 0.00002142
Iteration 44/1000 | Loss: 0.00002142
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002141
Iteration 47/1000 | Loss: 0.00002141
Iteration 48/1000 | Loss: 0.00002140
Iteration 49/1000 | Loss: 0.00002139
Iteration 50/1000 | Loss: 0.00002138
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00003306
Iteration 54/1000 | Loss: 0.00002130
Iteration 55/1000 | Loss: 0.00002130
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00002519
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002127
Iteration 60/1000 | Loss: 0.00002126
Iteration 61/1000 | Loss: 0.00002126
Iteration 62/1000 | Loss: 0.00002126
Iteration 63/1000 | Loss: 0.00002126
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002126
Iteration 69/1000 | Loss: 0.00002126
Iteration 70/1000 | Loss: 0.00002125
Iteration 71/1000 | Loss: 0.00002124
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002122
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002120
Iteration 84/1000 | Loss: 0.00002120
Iteration 85/1000 | Loss: 0.00002120
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00002120
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002119
Iteration 90/1000 | Loss: 0.00002119
Iteration 91/1000 | Loss: 0.00002118
Iteration 92/1000 | Loss: 0.00002558
Iteration 93/1000 | Loss: 0.00002116
Iteration 94/1000 | Loss: 0.00002116
Iteration 95/1000 | Loss: 0.00002116
Iteration 96/1000 | Loss: 0.00002116
Iteration 97/1000 | Loss: 0.00002116
Iteration 98/1000 | Loss: 0.00002116
Iteration 99/1000 | Loss: 0.00002116
Iteration 100/1000 | Loss: 0.00002116
Iteration 101/1000 | Loss: 0.00002115
Iteration 102/1000 | Loss: 0.00002115
Iteration 103/1000 | Loss: 0.00002115
Iteration 104/1000 | Loss: 0.00002115
Iteration 105/1000 | Loss: 0.00002115
Iteration 106/1000 | Loss: 0.00002115
Iteration 107/1000 | Loss: 0.00002115
Iteration 108/1000 | Loss: 0.00002115
Iteration 109/1000 | Loss: 0.00002114
Iteration 110/1000 | Loss: 0.00002114
Iteration 111/1000 | Loss: 0.00002114
Iteration 112/1000 | Loss: 0.00002114
Iteration 113/1000 | Loss: 0.00002114
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002114
Iteration 119/1000 | Loss: 0.00002114
Iteration 120/1000 | Loss: 0.00002114
Iteration 121/1000 | Loss: 0.00002114
Iteration 122/1000 | Loss: 0.00002114
Iteration 123/1000 | Loss: 0.00002114
Iteration 124/1000 | Loss: 0.00002114
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002113
Iteration 129/1000 | Loss: 0.00002113
Iteration 130/1000 | Loss: 0.00002113
Iteration 131/1000 | Loss: 0.00002113
Iteration 132/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.113496339006815e-05, 2.113496339006815e-05, 2.113496339006815e-05, 2.113496339006815e-05, 2.113496339006815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.113496339006815e-05

Optimization complete. Final v2v error: 3.830606698989868 mm

Highest mean error: 4.755703449249268 mm for frame 150

Lowest mean error: 3.333629846572876 mm for frame 56

Saving results

Total time: 105.66094541549683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494175
Iteration 2/25 | Loss: 0.00142798
Iteration 3/25 | Loss: 0.00127602
Iteration 4/25 | Loss: 0.00126210
Iteration 5/25 | Loss: 0.00125840
Iteration 6/25 | Loss: 0.00125776
Iteration 7/25 | Loss: 0.00125776
Iteration 8/25 | Loss: 0.00125776
Iteration 9/25 | Loss: 0.00125776
Iteration 10/25 | Loss: 0.00125776
Iteration 11/25 | Loss: 0.00125776
Iteration 12/25 | Loss: 0.00125776
Iteration 13/25 | Loss: 0.00125776
Iteration 14/25 | Loss: 0.00125776
Iteration 15/25 | Loss: 0.00125776
Iteration 16/25 | Loss: 0.00125776
Iteration 17/25 | Loss: 0.00125776
Iteration 18/25 | Loss: 0.00125776
Iteration 19/25 | Loss: 0.00125776
Iteration 20/25 | Loss: 0.00125776
Iteration 21/25 | Loss: 0.00125776
Iteration 22/25 | Loss: 0.00125776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012577618472278118, 0.0012577618472278118, 0.0012577618472278118, 0.0012577618472278118, 0.0012577618472278118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012577618472278118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54116023
Iteration 2/25 | Loss: 0.00074097
Iteration 3/25 | Loss: 0.00074096
Iteration 4/25 | Loss: 0.00074096
Iteration 5/25 | Loss: 0.00074096
Iteration 6/25 | Loss: 0.00074095
Iteration 7/25 | Loss: 0.00074095
Iteration 8/25 | Loss: 0.00074095
Iteration 9/25 | Loss: 0.00074095
Iteration 10/25 | Loss: 0.00074095
Iteration 11/25 | Loss: 0.00074095
Iteration 12/25 | Loss: 0.00074095
Iteration 13/25 | Loss: 0.00074095
Iteration 14/25 | Loss: 0.00074095
Iteration 15/25 | Loss: 0.00074095
Iteration 16/25 | Loss: 0.00074095
Iteration 17/25 | Loss: 0.00074095
Iteration 18/25 | Loss: 0.00074095
Iteration 19/25 | Loss: 0.00074095
Iteration 20/25 | Loss: 0.00074095
Iteration 21/25 | Loss: 0.00074095
Iteration 22/25 | Loss: 0.00074095
Iteration 23/25 | Loss: 0.00074095
Iteration 24/25 | Loss: 0.00074095
Iteration 25/25 | Loss: 0.00074095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074095
Iteration 2/1000 | Loss: 0.00003782
Iteration 3/1000 | Loss: 0.00002677
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001825
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001723
Iteration 13/1000 | Loss: 0.00001707
Iteration 14/1000 | Loss: 0.00001704
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001698
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001696
Iteration 21/1000 | Loss: 0.00001696
Iteration 22/1000 | Loss: 0.00001695
Iteration 23/1000 | Loss: 0.00001695
Iteration 24/1000 | Loss: 0.00001695
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001694
Iteration 27/1000 | Loss: 0.00001694
Iteration 28/1000 | Loss: 0.00001694
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001691
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001690
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001688
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001688
Iteration 49/1000 | Loss: 0.00001687
Iteration 50/1000 | Loss: 0.00001687
Iteration 51/1000 | Loss: 0.00001687
Iteration 52/1000 | Loss: 0.00001687
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001686
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001684
Iteration 63/1000 | Loss: 0.00001684
Iteration 64/1000 | Loss: 0.00001684
Iteration 65/1000 | Loss: 0.00001684
Iteration 66/1000 | Loss: 0.00001684
Iteration 67/1000 | Loss: 0.00001683
Iteration 68/1000 | Loss: 0.00001683
Iteration 69/1000 | Loss: 0.00001683
Iteration 70/1000 | Loss: 0.00001683
Iteration 71/1000 | Loss: 0.00001682
Iteration 72/1000 | Loss: 0.00001682
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00001682
Iteration 75/1000 | Loss: 0.00001681
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001679
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001671
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001667
Iteration 111/1000 | Loss: 0.00001667
Iteration 112/1000 | Loss: 0.00001667
Iteration 113/1000 | Loss: 0.00001667
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001666
Iteration 116/1000 | Loss: 0.00001666
Iteration 117/1000 | Loss: 0.00001666
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001665
Iteration 122/1000 | Loss: 0.00001665
Iteration 123/1000 | Loss: 0.00001665
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Iteration 126/1000 | Loss: 0.00001664
Iteration 127/1000 | Loss: 0.00001664
Iteration 128/1000 | Loss: 0.00001664
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001662
Iteration 134/1000 | Loss: 0.00001662
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001661
Iteration 138/1000 | Loss: 0.00001661
Iteration 139/1000 | Loss: 0.00001661
Iteration 140/1000 | Loss: 0.00001661
Iteration 141/1000 | Loss: 0.00001661
Iteration 142/1000 | Loss: 0.00001661
Iteration 143/1000 | Loss: 0.00001661
Iteration 144/1000 | Loss: 0.00001661
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001661
Iteration 147/1000 | Loss: 0.00001661
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001661
Iteration 155/1000 | Loss: 0.00001661
Iteration 156/1000 | Loss: 0.00001661
Iteration 157/1000 | Loss: 0.00001661
Iteration 158/1000 | Loss: 0.00001661
Iteration 159/1000 | Loss: 0.00001661
Iteration 160/1000 | Loss: 0.00001661
Iteration 161/1000 | Loss: 0.00001661
Iteration 162/1000 | Loss: 0.00001661
Iteration 163/1000 | Loss: 0.00001661
Iteration 164/1000 | Loss: 0.00001661
Iteration 165/1000 | Loss: 0.00001661
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Iteration 168/1000 | Loss: 0.00001661
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001661
Iteration 171/1000 | Loss: 0.00001661
Iteration 172/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.6608817531960085e-05, 1.6608817531960085e-05, 1.6608817531960085e-05, 1.6608817531960085e-05, 1.6608817531960085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6608817531960085e-05

Optimization complete. Final v2v error: 3.3572070598602295 mm

Highest mean error: 4.492419719696045 mm for frame 110

Lowest mean error: 2.767289400100708 mm for frame 166

Saving results

Total time: 39.51580286026001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992249
Iteration 2/25 | Loss: 0.00307800
Iteration 3/25 | Loss: 0.00192015
Iteration 4/25 | Loss: 0.00169338
Iteration 5/25 | Loss: 0.00197813
Iteration 6/25 | Loss: 0.00180295
Iteration 7/25 | Loss: 0.00141168
Iteration 8/25 | Loss: 0.00133383
Iteration 9/25 | Loss: 0.00132538
Iteration 10/25 | Loss: 0.00132896
Iteration 11/25 | Loss: 0.00131796
Iteration 12/25 | Loss: 0.00131834
Iteration 13/25 | Loss: 0.00132134
Iteration 14/25 | Loss: 0.00131705
Iteration 15/25 | Loss: 0.00131704
Iteration 16/25 | Loss: 0.00131704
Iteration 17/25 | Loss: 0.00131704
Iteration 18/25 | Loss: 0.00131704
Iteration 19/25 | Loss: 0.00131704
Iteration 20/25 | Loss: 0.00131704
Iteration 21/25 | Loss: 0.00131704
Iteration 22/25 | Loss: 0.00131704
Iteration 23/25 | Loss: 0.00131704
Iteration 24/25 | Loss: 0.00131703
Iteration 25/25 | Loss: 0.00131703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42831409
Iteration 2/25 | Loss: 0.00079382
Iteration 3/25 | Loss: 0.00067631
Iteration 4/25 | Loss: 0.00067631
Iteration 5/25 | Loss: 0.00067631
Iteration 6/25 | Loss: 0.00067631
Iteration 7/25 | Loss: 0.00067631
Iteration 8/25 | Loss: 0.00067631
Iteration 9/25 | Loss: 0.00067631
Iteration 10/25 | Loss: 0.00067631
Iteration 11/25 | Loss: 0.00067631
Iteration 12/25 | Loss: 0.00067631
Iteration 13/25 | Loss: 0.00067631
Iteration 14/25 | Loss: 0.00067631
Iteration 15/25 | Loss: 0.00067631
Iteration 16/25 | Loss: 0.00067631
Iteration 17/25 | Loss: 0.00067631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006763114361092448, 0.0006763114361092448, 0.0006763114361092448, 0.0006763114361092448, 0.0006763114361092448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006763114361092448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067631
Iteration 2/1000 | Loss: 0.00078278
Iteration 3/1000 | Loss: 0.00004208
Iteration 4/1000 | Loss: 0.00030035
Iteration 5/1000 | Loss: 0.00003940
Iteration 6/1000 | Loss: 0.00020735
Iteration 7/1000 | Loss: 0.00020143
Iteration 8/1000 | Loss: 0.00011413
Iteration 9/1000 | Loss: 0.00008025
Iteration 10/1000 | Loss: 0.00003810
Iteration 11/1000 | Loss: 0.00006582
Iteration 12/1000 | Loss: 0.00034485
Iteration 13/1000 | Loss: 0.00009166
Iteration 14/1000 | Loss: 0.00007002
Iteration 15/1000 | Loss: 0.00045638
Iteration 16/1000 | Loss: 0.00005066
Iteration 17/1000 | Loss: 0.00003343
Iteration 18/1000 | Loss: 0.00008704
Iteration 19/1000 | Loss: 0.00003214
Iteration 20/1000 | Loss: 0.00019296
Iteration 21/1000 | Loss: 0.00004526
Iteration 22/1000 | Loss: 0.00005609
Iteration 23/1000 | Loss: 0.00003499
Iteration 24/1000 | Loss: 0.00011304
Iteration 25/1000 | Loss: 0.00013836
Iteration 26/1000 | Loss: 0.00005751
Iteration 27/1000 | Loss: 0.00003380
Iteration 28/1000 | Loss: 0.00003267
Iteration 29/1000 | Loss: 0.00003300
Iteration 30/1000 | Loss: 0.00010954
Iteration 31/1000 | Loss: 0.00004760
Iteration 32/1000 | Loss: 0.00004255
Iteration 33/1000 | Loss: 0.00003848
Iteration 34/1000 | Loss: 0.00004490
Iteration 35/1000 | Loss: 0.00004287
Iteration 36/1000 | Loss: 0.00004278
Iteration 37/1000 | Loss: 0.00003244
Iteration 38/1000 | Loss: 0.00003153
Iteration 39/1000 | Loss: 0.00004941
Iteration 40/1000 | Loss: 0.00003858
Iteration 41/1000 | Loss: 0.00003282
Iteration 42/1000 | Loss: 0.00003153
Iteration 43/1000 | Loss: 0.00003153
Iteration 44/1000 | Loss: 0.00003152
Iteration 45/1000 | Loss: 0.00003154
Iteration 46/1000 | Loss: 0.00003813
Iteration 47/1000 | Loss: 0.00006003
Iteration 48/1000 | Loss: 0.00011178
Iteration 49/1000 | Loss: 0.00064658
Iteration 50/1000 | Loss: 0.00003702
Iteration 51/1000 | Loss: 0.00005259
Iteration 52/1000 | Loss: 0.00003343
Iteration 53/1000 | Loss: 0.00008883
Iteration 54/1000 | Loss: 0.00005620
Iteration 55/1000 | Loss: 0.00003279
Iteration 56/1000 | Loss: 0.00003875
Iteration 57/1000 | Loss: 0.00005722
Iteration 58/1000 | Loss: 0.00005625
Iteration 59/1000 | Loss: 0.00003453
Iteration 60/1000 | Loss: 0.00004832
Iteration 61/1000 | Loss: 0.00003252
Iteration 62/1000 | Loss: 0.00003683
Iteration 63/1000 | Loss: 0.00003151
Iteration 64/1000 | Loss: 0.00003461
Iteration 65/1000 | Loss: 0.00004230
Iteration 66/1000 | Loss: 0.00003414
Iteration 67/1000 | Loss: 0.00003149
Iteration 68/1000 | Loss: 0.00003148
Iteration 69/1000 | Loss: 0.00003147
Iteration 70/1000 | Loss: 0.00003147
Iteration 71/1000 | Loss: 0.00003147
Iteration 72/1000 | Loss: 0.00003147
Iteration 73/1000 | Loss: 0.00003147
Iteration 74/1000 | Loss: 0.00003147
Iteration 75/1000 | Loss: 0.00003147
Iteration 76/1000 | Loss: 0.00003147
Iteration 77/1000 | Loss: 0.00003147
Iteration 78/1000 | Loss: 0.00003147
Iteration 79/1000 | Loss: 0.00003146
Iteration 80/1000 | Loss: 0.00003145
Iteration 81/1000 | Loss: 0.00003145
Iteration 82/1000 | Loss: 0.00003144
Iteration 83/1000 | Loss: 0.00003144
Iteration 84/1000 | Loss: 0.00003144
Iteration 85/1000 | Loss: 0.00003144
Iteration 86/1000 | Loss: 0.00003144
Iteration 87/1000 | Loss: 0.00003143
Iteration 88/1000 | Loss: 0.00005469
Iteration 89/1000 | Loss: 0.00005690
Iteration 90/1000 | Loss: 0.00003223
Iteration 91/1000 | Loss: 0.00008157
Iteration 92/1000 | Loss: 0.00003475
Iteration 93/1000 | Loss: 0.00005491
Iteration 94/1000 | Loss: 0.00003265
Iteration 95/1000 | Loss: 0.00003234
Iteration 96/1000 | Loss: 0.00008347
Iteration 97/1000 | Loss: 0.00003284
Iteration 98/1000 | Loss: 0.00003509
Iteration 99/1000 | Loss: 0.00003931
Iteration 100/1000 | Loss: 0.00003683
Iteration 101/1000 | Loss: 0.00004086
Iteration 102/1000 | Loss: 0.00006096
Iteration 103/1000 | Loss: 0.00003888
Iteration 104/1000 | Loss: 0.00004957
Iteration 105/1000 | Loss: 0.00003497
Iteration 106/1000 | Loss: 0.00003157
Iteration 107/1000 | Loss: 0.00003228
Iteration 108/1000 | Loss: 0.00003358
Iteration 109/1000 | Loss: 0.00003146
Iteration 110/1000 | Loss: 0.00003146
Iteration 111/1000 | Loss: 0.00003146
Iteration 112/1000 | Loss: 0.00003145
Iteration 113/1000 | Loss: 0.00003145
Iteration 114/1000 | Loss: 0.00003145
Iteration 115/1000 | Loss: 0.00003145
Iteration 116/1000 | Loss: 0.00003145
Iteration 117/1000 | Loss: 0.00003145
Iteration 118/1000 | Loss: 0.00003145
Iteration 119/1000 | Loss: 0.00003143
Iteration 120/1000 | Loss: 0.00003143
Iteration 121/1000 | Loss: 0.00003142
Iteration 122/1000 | Loss: 0.00003142
Iteration 123/1000 | Loss: 0.00003142
Iteration 124/1000 | Loss: 0.00003142
Iteration 125/1000 | Loss: 0.00003142
Iteration 126/1000 | Loss: 0.00003141
Iteration 127/1000 | Loss: 0.00003141
Iteration 128/1000 | Loss: 0.00003141
Iteration 129/1000 | Loss: 0.00003141
Iteration 130/1000 | Loss: 0.00003496
Iteration 131/1000 | Loss: 0.00003222
Iteration 132/1000 | Loss: 0.00003141
Iteration 133/1000 | Loss: 0.00003141
Iteration 134/1000 | Loss: 0.00003141
Iteration 135/1000 | Loss: 0.00003141
Iteration 136/1000 | Loss: 0.00003141
Iteration 137/1000 | Loss: 0.00003141
Iteration 138/1000 | Loss: 0.00003141
Iteration 139/1000 | Loss: 0.00003140
Iteration 140/1000 | Loss: 0.00003140
Iteration 141/1000 | Loss: 0.00003140
Iteration 142/1000 | Loss: 0.00003140
Iteration 143/1000 | Loss: 0.00003140
Iteration 144/1000 | Loss: 0.00003140
Iteration 145/1000 | Loss: 0.00003139
Iteration 146/1000 | Loss: 0.00003139
Iteration 147/1000 | Loss: 0.00003139
Iteration 148/1000 | Loss: 0.00003139
Iteration 149/1000 | Loss: 0.00003139
Iteration 150/1000 | Loss: 0.00003139
Iteration 151/1000 | Loss: 0.00003139
Iteration 152/1000 | Loss: 0.00003138
Iteration 153/1000 | Loss: 0.00003138
Iteration 154/1000 | Loss: 0.00003138
Iteration 155/1000 | Loss: 0.00003138
Iteration 156/1000 | Loss: 0.00003137
Iteration 157/1000 | Loss: 0.00003137
Iteration 158/1000 | Loss: 0.00003137
Iteration 159/1000 | Loss: 0.00003137
Iteration 160/1000 | Loss: 0.00003137
Iteration 161/1000 | Loss: 0.00003137
Iteration 162/1000 | Loss: 0.00004683
Iteration 163/1000 | Loss: 0.00003142
Iteration 164/1000 | Loss: 0.00003138
Iteration 165/1000 | Loss: 0.00003138
Iteration 166/1000 | Loss: 0.00003138
Iteration 167/1000 | Loss: 0.00003138
Iteration 168/1000 | Loss: 0.00003792
Iteration 169/1000 | Loss: 0.00003227
Iteration 170/1000 | Loss: 0.00003230
Iteration 171/1000 | Loss: 0.00009183
Iteration 172/1000 | Loss: 0.00005623
Iteration 173/1000 | Loss: 0.00004202
Iteration 174/1000 | Loss: 0.00003316
Iteration 175/1000 | Loss: 0.00005559
Iteration 176/1000 | Loss: 0.00003226
Iteration 177/1000 | Loss: 0.00006350
Iteration 178/1000 | Loss: 0.00003218
Iteration 179/1000 | Loss: 0.00003253
Iteration 180/1000 | Loss: 0.00003856
Iteration 181/1000 | Loss: 0.00004720
Iteration 182/1000 | Loss: 0.00003437
Iteration 183/1000 | Loss: 0.00003665
Iteration 184/1000 | Loss: 0.00003155
Iteration 185/1000 | Loss: 0.00012480
Iteration 186/1000 | Loss: 0.00003597
Iteration 187/1000 | Loss: 0.00003163
Iteration 188/1000 | Loss: 0.00007782
Iteration 189/1000 | Loss: 0.00003223
Iteration 190/1000 | Loss: 0.00007729
Iteration 191/1000 | Loss: 0.00003631
Iteration 192/1000 | Loss: 0.00005468
Iteration 193/1000 | Loss: 0.00003402
Iteration 194/1000 | Loss: 0.00005626
Iteration 195/1000 | Loss: 0.00003364
Iteration 196/1000 | Loss: 0.00004004
Iteration 197/1000 | Loss: 0.00003619
Iteration 198/1000 | Loss: 0.00003271
Iteration 199/1000 | Loss: 0.00003446
Iteration 200/1000 | Loss: 0.00003166
Iteration 201/1000 | Loss: 0.00003142
Iteration 202/1000 | Loss: 0.00003142
Iteration 203/1000 | Loss: 0.00003142
Iteration 204/1000 | Loss: 0.00003142
Iteration 205/1000 | Loss: 0.00003142
Iteration 206/1000 | Loss: 0.00003142
Iteration 207/1000 | Loss: 0.00003142
Iteration 208/1000 | Loss: 0.00003141
Iteration 209/1000 | Loss: 0.00003139
Iteration 210/1000 | Loss: 0.00003139
Iteration 211/1000 | Loss: 0.00007746
Iteration 212/1000 | Loss: 0.00003360
Iteration 213/1000 | Loss: 0.00004867
Iteration 214/1000 | Loss: 0.00003218
Iteration 215/1000 | Loss: 0.00005168
Iteration 216/1000 | Loss: 0.00003142
Iteration 217/1000 | Loss: 0.00003153
Iteration 218/1000 | Loss: 0.00004703
Iteration 219/1000 | Loss: 0.00011369
Iteration 220/1000 | Loss: 0.00006429
Iteration 221/1000 | Loss: 0.00003185
Iteration 222/1000 | Loss: 0.00003443
Iteration 223/1000 | Loss: 0.00004380
Iteration 224/1000 | Loss: 0.00003170
Iteration 225/1000 | Loss: 0.00004127
Iteration 226/1000 | Loss: 0.00003210
Iteration 227/1000 | Loss: 0.00003153
Iteration 228/1000 | Loss: 0.00006214
Iteration 229/1000 | Loss: 0.00003144
Iteration 230/1000 | Loss: 0.00004058
Iteration 231/1000 | Loss: 0.00005884
Iteration 232/1000 | Loss: 0.00005011
Iteration 233/1000 | Loss: 0.00005658
Iteration 234/1000 | Loss: 0.00003327
Iteration 235/1000 | Loss: 0.00004675
Iteration 236/1000 | Loss: 0.00003191
Iteration 237/1000 | Loss: 0.00008119
Iteration 238/1000 | Loss: 0.00003308
Iteration 239/1000 | Loss: 0.00003409
Iteration 240/1000 | Loss: 0.00003198
Iteration 241/1000 | Loss: 0.00004291
Iteration 242/1000 | Loss: 0.00003375
Iteration 243/1000 | Loss: 0.00004950
Iteration 244/1000 | Loss: 0.00003211
Iteration 245/1000 | Loss: 0.00003658
Iteration 246/1000 | Loss: 0.00004002
Iteration 247/1000 | Loss: 0.00004445
Iteration 248/1000 | Loss: 0.00003207
Iteration 249/1000 | Loss: 0.00003880
Iteration 250/1000 | Loss: 0.00003165
Iteration 251/1000 | Loss: 0.00003142
Iteration 252/1000 | Loss: 0.00003776
Iteration 253/1000 | Loss: 0.00003771
Iteration 254/1000 | Loss: 0.00003169
Iteration 255/1000 | Loss: 0.00003177
Iteration 256/1000 | Loss: 0.00003140
Iteration 257/1000 | Loss: 0.00003140
Iteration 258/1000 | Loss: 0.00003140
Iteration 259/1000 | Loss: 0.00003140
Iteration 260/1000 | Loss: 0.00003140
Iteration 261/1000 | Loss: 0.00003139
Iteration 262/1000 | Loss: 0.00003139
Iteration 263/1000 | Loss: 0.00003139
Iteration 264/1000 | Loss: 0.00003139
Iteration 265/1000 | Loss: 0.00003139
Iteration 266/1000 | Loss: 0.00003139
Iteration 267/1000 | Loss: 0.00003139
Iteration 268/1000 | Loss: 0.00003139
Iteration 269/1000 | Loss: 0.00003139
Iteration 270/1000 | Loss: 0.00004641
Iteration 271/1000 | Loss: 0.00003201
Iteration 272/1000 | Loss: 0.00003224
Iteration 273/1000 | Loss: 0.00003143
Iteration 274/1000 | Loss: 0.00006709
Iteration 275/1000 | Loss: 0.00003189
Iteration 276/1000 | Loss: 0.00004316
Iteration 277/1000 | Loss: 0.00003798
Iteration 278/1000 | Loss: 0.00003150
Iteration 279/1000 | Loss: 0.00003149
Iteration 280/1000 | Loss: 0.00003134
Iteration 281/1000 | Loss: 0.00003132
Iteration 282/1000 | Loss: 0.00003132
Iteration 283/1000 | Loss: 0.00003132
Iteration 284/1000 | Loss: 0.00003132
Iteration 285/1000 | Loss: 0.00003132
Iteration 286/1000 | Loss: 0.00003132
Iteration 287/1000 | Loss: 0.00003132
Iteration 288/1000 | Loss: 0.00003132
Iteration 289/1000 | Loss: 0.00003132
Iteration 290/1000 | Loss: 0.00003132
Iteration 291/1000 | Loss: 0.00003132
Iteration 292/1000 | Loss: 0.00003132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [3.131955963908695e-05, 3.131955963908695e-05, 3.131955963908695e-05, 3.131955963908695e-05, 3.131955963908695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.131955963908695e-05

Optimization complete. Final v2v error: 4.681659698486328 mm

Highest mean error: 4.950159072875977 mm for frame 48

Lowest mean error: 4.427367210388184 mm for frame 102

Saving results

Total time: 284.04013991355896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899633
Iteration 2/25 | Loss: 0.00136253
Iteration 3/25 | Loss: 0.00126409
Iteration 4/25 | Loss: 0.00124746
Iteration 5/25 | Loss: 0.00124097
Iteration 6/25 | Loss: 0.00123961
Iteration 7/25 | Loss: 0.00123961
Iteration 8/25 | Loss: 0.00123961
Iteration 9/25 | Loss: 0.00123961
Iteration 10/25 | Loss: 0.00123961
Iteration 11/25 | Loss: 0.00123961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001239608391188085, 0.001239608391188085, 0.001239608391188085, 0.001239608391188085, 0.001239608391188085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001239608391188085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43991852
Iteration 2/25 | Loss: 0.00076491
Iteration 3/25 | Loss: 0.00076490
Iteration 4/25 | Loss: 0.00076490
Iteration 5/25 | Loss: 0.00076490
Iteration 6/25 | Loss: 0.00076490
Iteration 7/25 | Loss: 0.00076490
Iteration 8/25 | Loss: 0.00076489
Iteration 9/25 | Loss: 0.00076489
Iteration 10/25 | Loss: 0.00076489
Iteration 11/25 | Loss: 0.00076489
Iteration 12/25 | Loss: 0.00076489
Iteration 13/25 | Loss: 0.00076489
Iteration 14/25 | Loss: 0.00076489
Iteration 15/25 | Loss: 0.00076489
Iteration 16/25 | Loss: 0.00076489
Iteration 17/25 | Loss: 0.00076489
Iteration 18/25 | Loss: 0.00076489
Iteration 19/25 | Loss: 0.00076489
Iteration 20/25 | Loss: 0.00076489
Iteration 21/25 | Loss: 0.00076489
Iteration 22/25 | Loss: 0.00076489
Iteration 23/25 | Loss: 0.00076489
Iteration 24/25 | Loss: 0.00076489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007648941827937961, 0.0007648941827937961, 0.0007648941827937961, 0.0007648941827937961, 0.0007648941827937961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007648941827937961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076489
Iteration 2/1000 | Loss: 0.00003976
Iteration 3/1000 | Loss: 0.00002693
Iteration 4/1000 | Loss: 0.00002302
Iteration 5/1000 | Loss: 0.00002189
Iteration 6/1000 | Loss: 0.00002082
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001954
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001897
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001870
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001844
Iteration 15/1000 | Loss: 0.00001844
Iteration 16/1000 | Loss: 0.00001843
Iteration 17/1000 | Loss: 0.00001843
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001819
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001805
Iteration 31/1000 | Loss: 0.00001805
Iteration 32/1000 | Loss: 0.00001803
Iteration 33/1000 | Loss: 0.00001800
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001794
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001793
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001791
Iteration 50/1000 | Loss: 0.00001790
Iteration 51/1000 | Loss: 0.00001790
Iteration 52/1000 | Loss: 0.00001789
Iteration 53/1000 | Loss: 0.00001789
Iteration 54/1000 | Loss: 0.00001789
Iteration 55/1000 | Loss: 0.00001789
Iteration 56/1000 | Loss: 0.00001789
Iteration 57/1000 | Loss: 0.00001788
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001788
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001787
Iteration 62/1000 | Loss: 0.00001787
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001784
Iteration 65/1000 | Loss: 0.00001783
Iteration 66/1000 | Loss: 0.00001783
Iteration 67/1000 | Loss: 0.00001783
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001770
Iteration 104/1000 | Loss: 0.00001770
Iteration 105/1000 | Loss: 0.00001770
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001766
Iteration 133/1000 | Loss: 0.00001766
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001764
Iteration 140/1000 | Loss: 0.00001764
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001763
Iteration 144/1000 | Loss: 0.00001763
Iteration 145/1000 | Loss: 0.00001763
Iteration 146/1000 | Loss: 0.00001763
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001762
Iteration 149/1000 | Loss: 0.00001762
Iteration 150/1000 | Loss: 0.00001762
Iteration 151/1000 | Loss: 0.00001762
Iteration 152/1000 | Loss: 0.00001762
Iteration 153/1000 | Loss: 0.00001761
Iteration 154/1000 | Loss: 0.00001761
Iteration 155/1000 | Loss: 0.00001761
Iteration 156/1000 | Loss: 0.00001761
Iteration 157/1000 | Loss: 0.00001761
Iteration 158/1000 | Loss: 0.00001761
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001759
Iteration 168/1000 | Loss: 0.00001759
Iteration 169/1000 | Loss: 0.00001759
Iteration 170/1000 | Loss: 0.00001759
Iteration 171/1000 | Loss: 0.00001759
Iteration 172/1000 | Loss: 0.00001759
Iteration 173/1000 | Loss: 0.00001759
Iteration 174/1000 | Loss: 0.00001759
Iteration 175/1000 | Loss: 0.00001759
Iteration 176/1000 | Loss: 0.00001759
Iteration 177/1000 | Loss: 0.00001758
Iteration 178/1000 | Loss: 0.00001758
Iteration 179/1000 | Loss: 0.00001758
Iteration 180/1000 | Loss: 0.00001758
Iteration 181/1000 | Loss: 0.00001758
Iteration 182/1000 | Loss: 0.00001758
Iteration 183/1000 | Loss: 0.00001758
Iteration 184/1000 | Loss: 0.00001758
Iteration 185/1000 | Loss: 0.00001758
Iteration 186/1000 | Loss: 0.00001758
Iteration 187/1000 | Loss: 0.00001758
Iteration 188/1000 | Loss: 0.00001758
Iteration 189/1000 | Loss: 0.00001757
Iteration 190/1000 | Loss: 0.00001757
Iteration 191/1000 | Loss: 0.00001757
Iteration 192/1000 | Loss: 0.00001757
Iteration 193/1000 | Loss: 0.00001757
Iteration 194/1000 | Loss: 0.00001757
Iteration 195/1000 | Loss: 0.00001757
Iteration 196/1000 | Loss: 0.00001757
Iteration 197/1000 | Loss: 0.00001757
Iteration 198/1000 | Loss: 0.00001757
Iteration 199/1000 | Loss: 0.00001757
Iteration 200/1000 | Loss: 0.00001757
Iteration 201/1000 | Loss: 0.00001757
Iteration 202/1000 | Loss: 0.00001757
Iteration 203/1000 | Loss: 0.00001757
Iteration 204/1000 | Loss: 0.00001757
Iteration 205/1000 | Loss: 0.00001757
Iteration 206/1000 | Loss: 0.00001757
Iteration 207/1000 | Loss: 0.00001757
Iteration 208/1000 | Loss: 0.00001757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.756598248903174e-05, 1.756598248903174e-05, 1.756598248903174e-05, 1.756598248903174e-05, 1.756598248903174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.756598248903174e-05

Optimization complete. Final v2v error: 3.527311086654663 mm

Highest mean error: 5.166719913482666 mm for frame 77

Lowest mean error: 3.054302453994751 mm for frame 102

Saving results

Total time: 43.557778120040894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411902
Iteration 2/25 | Loss: 0.00127818
Iteration 3/25 | Loss: 0.00120333
Iteration 4/25 | Loss: 0.00119473
Iteration 5/25 | Loss: 0.00119237
Iteration 6/25 | Loss: 0.00119196
Iteration 7/25 | Loss: 0.00119196
Iteration 8/25 | Loss: 0.00119196
Iteration 9/25 | Loss: 0.00119196
Iteration 10/25 | Loss: 0.00119196
Iteration 11/25 | Loss: 0.00119196
Iteration 12/25 | Loss: 0.00119196
Iteration 13/25 | Loss: 0.00119196
Iteration 14/25 | Loss: 0.00119196
Iteration 15/25 | Loss: 0.00119196
Iteration 16/25 | Loss: 0.00119196
Iteration 17/25 | Loss: 0.00119196
Iteration 18/25 | Loss: 0.00119196
Iteration 19/25 | Loss: 0.00119196
Iteration 20/25 | Loss: 0.00119196
Iteration 21/25 | Loss: 0.00119196
Iteration 22/25 | Loss: 0.00119196
Iteration 23/25 | Loss: 0.00119196
Iteration 24/25 | Loss: 0.00119196
Iteration 25/25 | Loss: 0.00119196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53068101
Iteration 2/25 | Loss: 0.00073456
Iteration 3/25 | Loss: 0.00073456
Iteration 4/25 | Loss: 0.00073456
Iteration 5/25 | Loss: 0.00073455
Iteration 6/25 | Loss: 0.00073455
Iteration 7/25 | Loss: 0.00073455
Iteration 8/25 | Loss: 0.00073455
Iteration 9/25 | Loss: 0.00073455
Iteration 10/25 | Loss: 0.00073455
Iteration 11/25 | Loss: 0.00073455
Iteration 12/25 | Loss: 0.00073455
Iteration 13/25 | Loss: 0.00073455
Iteration 14/25 | Loss: 0.00073455
Iteration 15/25 | Loss: 0.00073455
Iteration 16/25 | Loss: 0.00073455
Iteration 17/25 | Loss: 0.00073455
Iteration 18/25 | Loss: 0.00073455
Iteration 19/25 | Loss: 0.00073455
Iteration 20/25 | Loss: 0.00073455
Iteration 21/25 | Loss: 0.00073455
Iteration 22/25 | Loss: 0.00073455
Iteration 23/25 | Loss: 0.00073455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007345530902966857, 0.0007345530902966857, 0.0007345530902966857, 0.0007345530902966857, 0.0007345530902966857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007345530902966857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073455
Iteration 2/1000 | Loss: 0.00002285
Iteration 3/1000 | Loss: 0.00001545
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001189
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001180
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001141
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001130
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001127
Iteration 48/1000 | Loss: 0.00001127
Iteration 49/1000 | Loss: 0.00001127
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001120
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001118
Iteration 104/1000 | Loss: 0.00001118
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001118
Iteration 113/1000 | Loss: 0.00001118
Iteration 114/1000 | Loss: 0.00001118
Iteration 115/1000 | Loss: 0.00001118
Iteration 116/1000 | Loss: 0.00001118
Iteration 117/1000 | Loss: 0.00001118
Iteration 118/1000 | Loss: 0.00001118
Iteration 119/1000 | Loss: 0.00001118
Iteration 120/1000 | Loss: 0.00001118
Iteration 121/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.1181345143995713e-05, 1.1181345143995713e-05, 1.1181345143995713e-05, 1.1181345143995713e-05, 1.1181345143995713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1181345143995713e-05

Optimization complete. Final v2v error: 2.8851990699768066 mm

Highest mean error: 3.631051778793335 mm for frame 59

Lowest mean error: 2.5816757678985596 mm for frame 84

Saving results

Total time: 29.943516492843628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835586
Iteration 2/25 | Loss: 0.00135660
Iteration 3/25 | Loss: 0.00128185
Iteration 4/25 | Loss: 0.00126077
Iteration 5/25 | Loss: 0.00125442
Iteration 6/25 | Loss: 0.00125373
Iteration 7/25 | Loss: 0.00125373
Iteration 8/25 | Loss: 0.00125373
Iteration 9/25 | Loss: 0.00125373
Iteration 10/25 | Loss: 0.00125373
Iteration 11/25 | Loss: 0.00125373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001253729802556336, 0.001253729802556336, 0.001253729802556336, 0.001253729802556336, 0.001253729802556336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001253729802556336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42215776
Iteration 2/25 | Loss: 0.00088991
Iteration 3/25 | Loss: 0.00088991
Iteration 4/25 | Loss: 0.00088991
Iteration 5/25 | Loss: 0.00088991
Iteration 6/25 | Loss: 0.00088991
Iteration 7/25 | Loss: 0.00088991
Iteration 8/25 | Loss: 0.00088991
Iteration 9/25 | Loss: 0.00088991
Iteration 10/25 | Loss: 0.00088991
Iteration 11/25 | Loss: 0.00088991
Iteration 12/25 | Loss: 0.00088991
Iteration 13/25 | Loss: 0.00088991
Iteration 14/25 | Loss: 0.00088991
Iteration 15/25 | Loss: 0.00088991
Iteration 16/25 | Loss: 0.00088991
Iteration 17/25 | Loss: 0.00088991
Iteration 18/25 | Loss: 0.00088991
Iteration 19/25 | Loss: 0.00088991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008899081731215119, 0.0008899081731215119, 0.0008899081731215119, 0.0008899081731215119, 0.0008899081731215119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008899081731215119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088991
Iteration 2/1000 | Loss: 0.00002766
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002161
Iteration 6/1000 | Loss: 0.00002108
Iteration 7/1000 | Loss: 0.00002102
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00002041
Iteration 10/1000 | Loss: 0.00002034
Iteration 11/1000 | Loss: 0.00002032
Iteration 12/1000 | Loss: 0.00002022
Iteration 13/1000 | Loss: 0.00002021
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00002004
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001989
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001989
Iteration 29/1000 | Loss: 0.00001989
Iteration 30/1000 | Loss: 0.00001988
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001988
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001987
Iteration 37/1000 | Loss: 0.00001987
Iteration 38/1000 | Loss: 0.00001987
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001980
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001980
Iteration 56/1000 | Loss: 0.00001979
Iteration 57/1000 | Loss: 0.00001979
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001978
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001976
Iteration 71/1000 | Loss: 0.00001976
Iteration 72/1000 | Loss: 0.00001976
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001973
Iteration 79/1000 | Loss: 0.00001973
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001972
Iteration 82/1000 | Loss: 0.00001972
Iteration 83/1000 | Loss: 0.00001972
Iteration 84/1000 | Loss: 0.00001972
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001971
Iteration 88/1000 | Loss: 0.00001971
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001970
Iteration 91/1000 | Loss: 0.00001970
Iteration 92/1000 | Loss: 0.00001970
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.9700388293131255e-05, 1.9700388293131255e-05, 1.9700388293131255e-05, 1.9700388293131255e-05, 1.9700388293131255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9700388293131255e-05

Optimization complete. Final v2v error: 3.7492728233337402 mm

Highest mean error: 3.8405094146728516 mm for frame 166

Lowest mean error: 3.553955078125 mm for frame 5

Saving results

Total time: 34.541096210479736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536381
Iteration 2/25 | Loss: 0.00144517
Iteration 3/25 | Loss: 0.00136244
Iteration 4/25 | Loss: 0.00134941
Iteration 5/25 | Loss: 0.00134468
Iteration 6/25 | Loss: 0.00134468
Iteration 7/25 | Loss: 0.00134468
Iteration 8/25 | Loss: 0.00134468
Iteration 9/25 | Loss: 0.00134468
Iteration 10/25 | Loss: 0.00134468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013446762459352612, 0.0013446762459352612, 0.0013446762459352612, 0.0013446762459352612, 0.0013446762459352612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013446762459352612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80047864
Iteration 2/25 | Loss: 0.00064513
Iteration 3/25 | Loss: 0.00064513
Iteration 4/25 | Loss: 0.00064513
Iteration 5/25 | Loss: 0.00064513
Iteration 6/25 | Loss: 0.00064513
Iteration 7/25 | Loss: 0.00064513
Iteration 8/25 | Loss: 0.00064512
Iteration 9/25 | Loss: 0.00064512
Iteration 10/25 | Loss: 0.00064512
Iteration 11/25 | Loss: 0.00064512
Iteration 12/25 | Loss: 0.00064512
Iteration 13/25 | Loss: 0.00064512
Iteration 14/25 | Loss: 0.00064512
Iteration 15/25 | Loss: 0.00064512
Iteration 16/25 | Loss: 0.00064512
Iteration 17/25 | Loss: 0.00064512
Iteration 18/25 | Loss: 0.00064512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006451243534684181, 0.0006451243534684181, 0.0006451243534684181, 0.0006451243534684181, 0.0006451243534684181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006451243534684181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064512
Iteration 2/1000 | Loss: 0.00005009
Iteration 3/1000 | Loss: 0.00004339
Iteration 4/1000 | Loss: 0.00004096
Iteration 5/1000 | Loss: 0.00003964
Iteration 6/1000 | Loss: 0.00003915
Iteration 7/1000 | Loss: 0.00003851
Iteration 8/1000 | Loss: 0.00003805
Iteration 9/1000 | Loss: 0.00003745
Iteration 10/1000 | Loss: 0.00003697
Iteration 11/1000 | Loss: 0.00003669
Iteration 12/1000 | Loss: 0.00003628
Iteration 13/1000 | Loss: 0.00003596
Iteration 14/1000 | Loss: 0.00003570
Iteration 15/1000 | Loss: 0.00003551
Iteration 16/1000 | Loss: 0.00003550
Iteration 17/1000 | Loss: 0.00003549
Iteration 18/1000 | Loss: 0.00003540
Iteration 19/1000 | Loss: 0.00003534
Iteration 20/1000 | Loss: 0.00003533
Iteration 21/1000 | Loss: 0.00003531
Iteration 22/1000 | Loss: 0.00003530
Iteration 23/1000 | Loss: 0.00003530
Iteration 24/1000 | Loss: 0.00003529
Iteration 25/1000 | Loss: 0.00003528
Iteration 26/1000 | Loss: 0.00003524
Iteration 27/1000 | Loss: 0.00003523
Iteration 28/1000 | Loss: 0.00003523
Iteration 29/1000 | Loss: 0.00003523
Iteration 30/1000 | Loss: 0.00003523
Iteration 31/1000 | Loss: 0.00003523
Iteration 32/1000 | Loss: 0.00003523
Iteration 33/1000 | Loss: 0.00003523
Iteration 34/1000 | Loss: 0.00003523
Iteration 35/1000 | Loss: 0.00003523
Iteration 36/1000 | Loss: 0.00003523
Iteration 37/1000 | Loss: 0.00003523
Iteration 38/1000 | Loss: 0.00003523
Iteration 39/1000 | Loss: 0.00003522
Iteration 40/1000 | Loss: 0.00003522
Iteration 41/1000 | Loss: 0.00003522
Iteration 42/1000 | Loss: 0.00003521
Iteration 43/1000 | Loss: 0.00003520
Iteration 44/1000 | Loss: 0.00003520
Iteration 45/1000 | Loss: 0.00003520
Iteration 46/1000 | Loss: 0.00003519
Iteration 47/1000 | Loss: 0.00003519
Iteration 48/1000 | Loss: 0.00003519
Iteration 49/1000 | Loss: 0.00003519
Iteration 50/1000 | Loss: 0.00003517
Iteration 51/1000 | Loss: 0.00003517
Iteration 52/1000 | Loss: 0.00003517
Iteration 53/1000 | Loss: 0.00003517
Iteration 54/1000 | Loss: 0.00003517
Iteration 55/1000 | Loss: 0.00003517
Iteration 56/1000 | Loss: 0.00003517
Iteration 57/1000 | Loss: 0.00003517
Iteration 58/1000 | Loss: 0.00003517
Iteration 59/1000 | Loss: 0.00003517
Iteration 60/1000 | Loss: 0.00003516
Iteration 61/1000 | Loss: 0.00003516
Iteration 62/1000 | Loss: 0.00003516
Iteration 63/1000 | Loss: 0.00003516
Iteration 64/1000 | Loss: 0.00003516
Iteration 65/1000 | Loss: 0.00003516
Iteration 66/1000 | Loss: 0.00003516
Iteration 67/1000 | Loss: 0.00003515
Iteration 68/1000 | Loss: 0.00003515
Iteration 69/1000 | Loss: 0.00003515
Iteration 70/1000 | Loss: 0.00003515
Iteration 71/1000 | Loss: 0.00003515
Iteration 72/1000 | Loss: 0.00003515
Iteration 73/1000 | Loss: 0.00003515
Iteration 74/1000 | Loss: 0.00003515
Iteration 75/1000 | Loss: 0.00003515
Iteration 76/1000 | Loss: 0.00003515
Iteration 77/1000 | Loss: 0.00003515
Iteration 78/1000 | Loss: 0.00003515
Iteration 79/1000 | Loss: 0.00003515
Iteration 80/1000 | Loss: 0.00003514
Iteration 81/1000 | Loss: 0.00003514
Iteration 82/1000 | Loss: 0.00003514
Iteration 83/1000 | Loss: 0.00003514
Iteration 84/1000 | Loss: 0.00003514
Iteration 85/1000 | Loss: 0.00003514
Iteration 86/1000 | Loss: 0.00003514
Iteration 87/1000 | Loss: 0.00003514
Iteration 88/1000 | Loss: 0.00003514
Iteration 89/1000 | Loss: 0.00003514
Iteration 90/1000 | Loss: 0.00003514
Iteration 91/1000 | Loss: 0.00003514
Iteration 92/1000 | Loss: 0.00003514
Iteration 93/1000 | Loss: 0.00003514
Iteration 94/1000 | Loss: 0.00003514
Iteration 95/1000 | Loss: 0.00003514
Iteration 96/1000 | Loss: 0.00003514
Iteration 97/1000 | Loss: 0.00003514
Iteration 98/1000 | Loss: 0.00003514
Iteration 99/1000 | Loss: 0.00003514
Iteration 100/1000 | Loss: 0.00003514
Iteration 101/1000 | Loss: 0.00003514
Iteration 102/1000 | Loss: 0.00003514
Iteration 103/1000 | Loss: 0.00003514
Iteration 104/1000 | Loss: 0.00003514
Iteration 105/1000 | Loss: 0.00003514
Iteration 106/1000 | Loss: 0.00003514
Iteration 107/1000 | Loss: 0.00003514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [3.514345007715747e-05, 3.514345007715747e-05, 3.514345007715747e-05, 3.514345007715747e-05, 3.514345007715747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.514345007715747e-05

Optimization complete. Final v2v error: 4.699112892150879 mm

Highest mean error: 4.766479969024658 mm for frame 262

Lowest mean error: 4.6578168869018555 mm for frame 199

Saving results

Total time: 41.423523902893066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040922
Iteration 2/25 | Loss: 0.01040922
Iteration 3/25 | Loss: 0.01040922
Iteration 4/25 | Loss: 0.01040922
Iteration 5/25 | Loss: 0.01040922
Iteration 6/25 | Loss: 0.01040922
Iteration 7/25 | Loss: 0.01040921
Iteration 8/25 | Loss: 0.01040921
Iteration 9/25 | Loss: 0.01040921
Iteration 10/25 | Loss: 0.01040921
Iteration 11/25 | Loss: 0.01040921
Iteration 12/25 | Loss: 0.01040921
Iteration 13/25 | Loss: 0.01040921
Iteration 14/25 | Loss: 0.01040921
Iteration 15/25 | Loss: 0.01040921
Iteration 16/25 | Loss: 0.01040920
Iteration 17/25 | Loss: 0.01040920
Iteration 18/25 | Loss: 0.01040920
Iteration 19/25 | Loss: 0.01040920
Iteration 20/25 | Loss: 0.01040920
Iteration 21/25 | Loss: 0.01040920
Iteration 22/25 | Loss: 0.01040920
Iteration 23/25 | Loss: 0.01040920
Iteration 24/25 | Loss: 0.01040919
Iteration 25/25 | Loss: 0.01040919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76830792
Iteration 2/25 | Loss: 0.08490774
Iteration 3/25 | Loss: 0.08484326
Iteration 4/25 | Loss: 0.08484322
Iteration 5/25 | Loss: 0.08484320
Iteration 6/25 | Loss: 0.08484320
Iteration 7/25 | Loss: 0.08484318
Iteration 8/25 | Loss: 0.08484319
Iteration 9/25 | Loss: 0.08484318
Iteration 10/25 | Loss: 0.08484318
Iteration 11/25 | Loss: 0.08484318
Iteration 12/25 | Loss: 0.08484318
Iteration 13/25 | Loss: 0.08484318
Iteration 14/25 | Loss: 0.08484318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.08484318107366562, 0.08484318107366562, 0.08484318107366562, 0.08484318107366562, 0.08484318107366562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08484318107366562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08484318
Iteration 2/1000 | Loss: 0.00824925
Iteration 3/1000 | Loss: 0.00310314
Iteration 4/1000 | Loss: 0.00123807
Iteration 5/1000 | Loss: 0.00134838
Iteration 6/1000 | Loss: 0.00071209
Iteration 7/1000 | Loss: 0.00071194
Iteration 8/1000 | Loss: 0.00272892
Iteration 9/1000 | Loss: 0.00124429
Iteration 10/1000 | Loss: 0.00281447
Iteration 11/1000 | Loss: 0.00022533
Iteration 12/1000 | Loss: 0.00069540
Iteration 13/1000 | Loss: 0.00015897
Iteration 14/1000 | Loss: 0.00032467
Iteration 15/1000 | Loss: 0.00040385
Iteration 16/1000 | Loss: 0.00254221
Iteration 17/1000 | Loss: 0.00012382
Iteration 18/1000 | Loss: 0.00125715
Iteration 19/1000 | Loss: 0.00015603
Iteration 20/1000 | Loss: 0.00158843
Iteration 21/1000 | Loss: 0.00059125
Iteration 22/1000 | Loss: 0.00154039
Iteration 23/1000 | Loss: 0.00583616
Iteration 24/1000 | Loss: 0.00616501
Iteration 25/1000 | Loss: 0.00617920
Iteration 26/1000 | Loss: 0.00095612
Iteration 27/1000 | Loss: 0.00212752
Iteration 28/1000 | Loss: 0.00081990
Iteration 29/1000 | Loss: 0.00294590
Iteration 30/1000 | Loss: 0.00022748
Iteration 31/1000 | Loss: 0.00177125
Iteration 32/1000 | Loss: 0.00096675
Iteration 33/1000 | Loss: 0.00079915
Iteration 34/1000 | Loss: 0.00130145
Iteration 35/1000 | Loss: 0.00098395
Iteration 36/1000 | Loss: 0.00209207
Iteration 37/1000 | Loss: 0.00219778
Iteration 38/1000 | Loss: 0.00200671
Iteration 39/1000 | Loss: 0.00383667
Iteration 40/1000 | Loss: 0.00124293
Iteration 41/1000 | Loss: 0.00102547
Iteration 42/1000 | Loss: 0.00169339
Iteration 43/1000 | Loss: 0.00237719
Iteration 44/1000 | Loss: 0.00119504
Iteration 45/1000 | Loss: 0.00059601
Iteration 46/1000 | Loss: 0.00047075
Iteration 47/1000 | Loss: 0.00093251
Iteration 48/1000 | Loss: 0.00097496
Iteration 49/1000 | Loss: 0.00021259
Iteration 50/1000 | Loss: 0.00023460
Iteration 51/1000 | Loss: 0.00016621
Iteration 52/1000 | Loss: 0.00019604
Iteration 53/1000 | Loss: 0.00015328
Iteration 54/1000 | Loss: 0.00011244
Iteration 55/1000 | Loss: 0.00020979
Iteration 56/1000 | Loss: 0.00018291
Iteration 57/1000 | Loss: 0.00016074
Iteration 58/1000 | Loss: 0.00029997
Iteration 59/1000 | Loss: 0.00014416
Iteration 60/1000 | Loss: 0.00040328
Iteration 61/1000 | Loss: 0.00077047
Iteration 62/1000 | Loss: 0.00046711
Iteration 63/1000 | Loss: 0.00049690
Iteration 64/1000 | Loss: 0.00049070
Iteration 65/1000 | Loss: 0.00044983
Iteration 66/1000 | Loss: 0.00054232
Iteration 67/1000 | Loss: 0.00038816
Iteration 68/1000 | Loss: 0.00046701
Iteration 69/1000 | Loss: 0.00017511
Iteration 70/1000 | Loss: 0.00018237
Iteration 71/1000 | Loss: 0.00058822
Iteration 72/1000 | Loss: 0.00021387
Iteration 73/1000 | Loss: 0.00014074
Iteration 74/1000 | Loss: 0.00043868
Iteration 75/1000 | Loss: 0.00012233
Iteration 76/1000 | Loss: 0.00017681
Iteration 77/1000 | Loss: 0.00015767
Iteration 78/1000 | Loss: 0.00032824
Iteration 79/1000 | Loss: 0.00017553
Iteration 80/1000 | Loss: 0.00058083
Iteration 81/1000 | Loss: 0.00015186
Iteration 82/1000 | Loss: 0.00026500
Iteration 83/1000 | Loss: 0.00018759
Iteration 84/1000 | Loss: 0.00033714
Iteration 85/1000 | Loss: 0.00143834
Iteration 86/1000 | Loss: 0.00230977
Iteration 87/1000 | Loss: 0.00131475
Iteration 88/1000 | Loss: 0.00039147
Iteration 89/1000 | Loss: 0.00009242
Iteration 90/1000 | Loss: 0.00008825
Iteration 91/1000 | Loss: 0.00005229
Iteration 92/1000 | Loss: 0.00042956
Iteration 93/1000 | Loss: 0.00004399
Iteration 94/1000 | Loss: 0.00004161
Iteration 95/1000 | Loss: 0.00010382
Iteration 96/1000 | Loss: 0.00004978
Iteration 97/1000 | Loss: 0.00003836
Iteration 98/1000 | Loss: 0.00006065
Iteration 99/1000 | Loss: 0.00015976
Iteration 100/1000 | Loss: 0.00003695
Iteration 101/1000 | Loss: 0.00004419
Iteration 102/1000 | Loss: 0.00003984
Iteration 103/1000 | Loss: 0.00004801
Iteration 104/1000 | Loss: 0.00004008
Iteration 105/1000 | Loss: 0.00004636
Iteration 106/1000 | Loss: 0.00004871
Iteration 107/1000 | Loss: 0.00007890
Iteration 108/1000 | Loss: 0.00042606
Iteration 109/1000 | Loss: 0.00009970
Iteration 110/1000 | Loss: 0.00023106
Iteration 111/1000 | Loss: 0.00037042
Iteration 112/1000 | Loss: 0.00004660
Iteration 113/1000 | Loss: 0.00005808
Iteration 114/1000 | Loss: 0.00004908
Iteration 115/1000 | Loss: 0.00014088
Iteration 116/1000 | Loss: 0.00011375
Iteration 117/1000 | Loss: 0.00013338
Iteration 118/1000 | Loss: 0.00007862
Iteration 119/1000 | Loss: 0.00009825
Iteration 120/1000 | Loss: 0.00026859
Iteration 121/1000 | Loss: 0.00007928
Iteration 122/1000 | Loss: 0.00004921
Iteration 123/1000 | Loss: 0.00005420
Iteration 124/1000 | Loss: 0.00011613
Iteration 125/1000 | Loss: 0.00004428
Iteration 126/1000 | Loss: 0.00004580
Iteration 127/1000 | Loss: 0.00004247
Iteration 128/1000 | Loss: 0.00004683
Iteration 129/1000 | Loss: 0.00016711
Iteration 130/1000 | Loss: 0.00005388
Iteration 131/1000 | Loss: 0.00004252
Iteration 132/1000 | Loss: 0.00004014
Iteration 133/1000 | Loss: 0.00004488
Iteration 134/1000 | Loss: 0.00004502
Iteration 135/1000 | Loss: 0.00004408
Iteration 136/1000 | Loss: 0.00004728
Iteration 137/1000 | Loss: 0.00004142
Iteration 138/1000 | Loss: 0.00010639
Iteration 139/1000 | Loss: 0.00004004
Iteration 140/1000 | Loss: 0.00004051
Iteration 141/1000 | Loss: 0.00015554
Iteration 142/1000 | Loss: 0.00004189
Iteration 143/1000 | Loss: 0.00003654
Iteration 144/1000 | Loss: 0.00003396
Iteration 145/1000 | Loss: 0.00003306
Iteration 146/1000 | Loss: 0.00012150
Iteration 147/1000 | Loss: 0.00003238
Iteration 148/1000 | Loss: 0.00003217
Iteration 149/1000 | Loss: 0.00003194
Iteration 150/1000 | Loss: 0.00003174
Iteration 151/1000 | Loss: 0.00003154
Iteration 152/1000 | Loss: 0.00003130
Iteration 153/1000 | Loss: 0.00012171
Iteration 154/1000 | Loss: 0.00003104
Iteration 155/1000 | Loss: 0.00003073
Iteration 156/1000 | Loss: 0.00003066
Iteration 157/1000 | Loss: 0.00003066
Iteration 158/1000 | Loss: 0.00003065
Iteration 159/1000 | Loss: 0.00003057
Iteration 160/1000 | Loss: 0.00003057
Iteration 161/1000 | Loss: 0.00003055
Iteration 162/1000 | Loss: 0.00003055
Iteration 163/1000 | Loss: 0.00003054
Iteration 164/1000 | Loss: 0.00003054
Iteration 165/1000 | Loss: 0.00003053
Iteration 166/1000 | Loss: 0.00003052
Iteration 167/1000 | Loss: 0.00003052
Iteration 168/1000 | Loss: 0.00003051
Iteration 169/1000 | Loss: 0.00003050
Iteration 170/1000 | Loss: 0.00003050
Iteration 171/1000 | Loss: 0.00003050
Iteration 172/1000 | Loss: 0.00003050
Iteration 173/1000 | Loss: 0.00003050
Iteration 174/1000 | Loss: 0.00003049
Iteration 175/1000 | Loss: 0.00003049
Iteration 176/1000 | Loss: 0.00003049
Iteration 177/1000 | Loss: 0.00003049
Iteration 178/1000 | Loss: 0.00003049
Iteration 179/1000 | Loss: 0.00003049
Iteration 180/1000 | Loss: 0.00003049
Iteration 181/1000 | Loss: 0.00003049
Iteration 182/1000 | Loss: 0.00003049
Iteration 183/1000 | Loss: 0.00003049
Iteration 184/1000 | Loss: 0.00003049
Iteration 185/1000 | Loss: 0.00003049
Iteration 186/1000 | Loss: 0.00003048
Iteration 187/1000 | Loss: 0.00003048
Iteration 188/1000 | Loss: 0.00003048
Iteration 189/1000 | Loss: 0.00003048
Iteration 190/1000 | Loss: 0.00011443
Iteration 191/1000 | Loss: 0.00010008
Iteration 192/1000 | Loss: 0.00003051
Iteration 193/1000 | Loss: 0.00004333
Iteration 194/1000 | Loss: 0.00003048
Iteration 195/1000 | Loss: 0.00003045
Iteration 196/1000 | Loss: 0.00003045
Iteration 197/1000 | Loss: 0.00003044
Iteration 198/1000 | Loss: 0.00003043
Iteration 199/1000 | Loss: 0.00003043
Iteration 200/1000 | Loss: 0.00003043
Iteration 201/1000 | Loss: 0.00005307
Iteration 202/1000 | Loss: 0.00003042
Iteration 203/1000 | Loss: 0.00003042
Iteration 204/1000 | Loss: 0.00003041
Iteration 205/1000 | Loss: 0.00003040
Iteration 206/1000 | Loss: 0.00003039
Iteration 207/1000 | Loss: 0.00003039
Iteration 208/1000 | Loss: 0.00003037
Iteration 209/1000 | Loss: 0.00003037
Iteration 210/1000 | Loss: 0.00003037
Iteration 211/1000 | Loss: 0.00003037
Iteration 212/1000 | Loss: 0.00003037
Iteration 213/1000 | Loss: 0.00003037
Iteration 214/1000 | Loss: 0.00003037
Iteration 215/1000 | Loss: 0.00003037
Iteration 216/1000 | Loss: 0.00003037
Iteration 217/1000 | Loss: 0.00003037
Iteration 218/1000 | Loss: 0.00003036
Iteration 219/1000 | Loss: 0.00003036
Iteration 220/1000 | Loss: 0.00003036
Iteration 221/1000 | Loss: 0.00003036
Iteration 222/1000 | Loss: 0.00003036
Iteration 223/1000 | Loss: 0.00003036
Iteration 224/1000 | Loss: 0.00003036
Iteration 225/1000 | Loss: 0.00003036
Iteration 226/1000 | Loss: 0.00003035
Iteration 227/1000 | Loss: 0.00003035
Iteration 228/1000 | Loss: 0.00003031
Iteration 229/1000 | Loss: 0.00003029
Iteration 230/1000 | Loss: 0.00003029
Iteration 231/1000 | Loss: 0.00003029
Iteration 232/1000 | Loss: 0.00003029
Iteration 233/1000 | Loss: 0.00003029
Iteration 234/1000 | Loss: 0.00003029
Iteration 235/1000 | Loss: 0.00003029
Iteration 236/1000 | Loss: 0.00003029
Iteration 237/1000 | Loss: 0.00003028
Iteration 238/1000 | Loss: 0.00003028
Iteration 239/1000 | Loss: 0.00003027
Iteration 240/1000 | Loss: 0.00003027
Iteration 241/1000 | Loss: 0.00003027
Iteration 242/1000 | Loss: 0.00003026
Iteration 243/1000 | Loss: 0.00003026
Iteration 244/1000 | Loss: 0.00003026
Iteration 245/1000 | Loss: 0.00003026
Iteration 246/1000 | Loss: 0.00003026
Iteration 247/1000 | Loss: 0.00003025
Iteration 248/1000 | Loss: 0.00003025
Iteration 249/1000 | Loss: 0.00003025
Iteration 250/1000 | Loss: 0.00003025
Iteration 251/1000 | Loss: 0.00003024
Iteration 252/1000 | Loss: 0.00003024
Iteration 253/1000 | Loss: 0.00003024
Iteration 254/1000 | Loss: 0.00003024
Iteration 255/1000 | Loss: 0.00003023
Iteration 256/1000 | Loss: 0.00003023
Iteration 257/1000 | Loss: 0.00003023
Iteration 258/1000 | Loss: 0.00003023
Iteration 259/1000 | Loss: 0.00003022
Iteration 260/1000 | Loss: 0.00003022
Iteration 261/1000 | Loss: 0.00003021
Iteration 262/1000 | Loss: 0.00003021
Iteration 263/1000 | Loss: 0.00003021
Iteration 264/1000 | Loss: 0.00003020
Iteration 265/1000 | Loss: 0.00003020
Iteration 266/1000 | Loss: 0.00003019
Iteration 267/1000 | Loss: 0.00003019
Iteration 268/1000 | Loss: 0.00003019
Iteration 269/1000 | Loss: 0.00003019
Iteration 270/1000 | Loss: 0.00003018
Iteration 271/1000 | Loss: 0.00003018
Iteration 272/1000 | Loss: 0.00003018
Iteration 273/1000 | Loss: 0.00003018
Iteration 274/1000 | Loss: 0.00003018
Iteration 275/1000 | Loss: 0.00003018
Iteration 276/1000 | Loss: 0.00003018
Iteration 277/1000 | Loss: 0.00003017
Iteration 278/1000 | Loss: 0.00003017
Iteration 279/1000 | Loss: 0.00003017
Iteration 280/1000 | Loss: 0.00003016
Iteration 281/1000 | Loss: 0.00003016
Iteration 282/1000 | Loss: 0.00003015
Iteration 283/1000 | Loss: 0.00003011
Iteration 284/1000 | Loss: 0.00003011
Iteration 285/1000 | Loss: 0.00003008
Iteration 286/1000 | Loss: 0.00003007
Iteration 287/1000 | Loss: 0.00003007
Iteration 288/1000 | Loss: 0.00003006
Iteration 289/1000 | Loss: 0.00003006
Iteration 290/1000 | Loss: 0.00003006
Iteration 291/1000 | Loss: 0.00003006
Iteration 292/1000 | Loss: 0.00003006
Iteration 293/1000 | Loss: 0.00003006
Iteration 294/1000 | Loss: 0.00003005
Iteration 295/1000 | Loss: 0.00003005
Iteration 296/1000 | Loss: 0.00003005
Iteration 297/1000 | Loss: 0.00003004
Iteration 298/1000 | Loss: 0.00003004
Iteration 299/1000 | Loss: 0.00003004
Iteration 300/1000 | Loss: 0.00003003
Iteration 301/1000 | Loss: 0.00003003
Iteration 302/1000 | Loss: 0.00003003
Iteration 303/1000 | Loss: 0.00003003
Iteration 304/1000 | Loss: 0.00003002
Iteration 305/1000 | Loss: 0.00003002
Iteration 306/1000 | Loss: 0.00003002
Iteration 307/1000 | Loss: 0.00003001
Iteration 308/1000 | Loss: 0.00003001
Iteration 309/1000 | Loss: 0.00003001
Iteration 310/1000 | Loss: 0.00003000
Iteration 311/1000 | Loss: 0.00003000
Iteration 312/1000 | Loss: 0.00003000
Iteration 313/1000 | Loss: 0.00016074
Iteration 314/1000 | Loss: 0.00003032
Iteration 315/1000 | Loss: 0.00003007
Iteration 316/1000 | Loss: 0.00003006
Iteration 317/1000 | Loss: 0.00003006
Iteration 318/1000 | Loss: 0.00003004
Iteration 319/1000 | Loss: 0.00003004
Iteration 320/1000 | Loss: 0.00003003
Iteration 321/1000 | Loss: 0.00003001
Iteration 322/1000 | Loss: 0.00003001
Iteration 323/1000 | Loss: 0.00003000
Iteration 324/1000 | Loss: 0.00003000
Iteration 325/1000 | Loss: 0.00002999
Iteration 326/1000 | Loss: 0.00002997
Iteration 327/1000 | Loss: 0.00002996
Iteration 328/1000 | Loss: 0.00002996
Iteration 329/1000 | Loss: 0.00002996
Iteration 330/1000 | Loss: 0.00002995
Iteration 331/1000 | Loss: 0.00002995
Iteration 332/1000 | Loss: 0.00002995
Iteration 333/1000 | Loss: 0.00002995
Iteration 334/1000 | Loss: 0.00002995
Iteration 335/1000 | Loss: 0.00002995
Iteration 336/1000 | Loss: 0.00002994
Iteration 337/1000 | Loss: 0.00002994
Iteration 338/1000 | Loss: 0.00002994
Iteration 339/1000 | Loss: 0.00002994
Iteration 340/1000 | Loss: 0.00002994
Iteration 341/1000 | Loss: 0.00002994
Iteration 342/1000 | Loss: 0.00002994
Iteration 343/1000 | Loss: 0.00002994
Iteration 344/1000 | Loss: 0.00002994
Iteration 345/1000 | Loss: 0.00002994
Iteration 346/1000 | Loss: 0.00002994
Iteration 347/1000 | Loss: 0.00002994
Iteration 348/1000 | Loss: 0.00002994
Iteration 349/1000 | Loss: 0.00002994
Iteration 350/1000 | Loss: 0.00002994
Iteration 351/1000 | Loss: 0.00002994
Iteration 352/1000 | Loss: 0.00002994
Iteration 353/1000 | Loss: 0.00002994
Iteration 354/1000 | Loss: 0.00002994
Iteration 355/1000 | Loss: 0.00002994
Iteration 356/1000 | Loss: 0.00002993
Iteration 357/1000 | Loss: 0.00002993
Iteration 358/1000 | Loss: 0.00002993
Iteration 359/1000 | Loss: 0.00002993
Iteration 360/1000 | Loss: 0.00002993
Iteration 361/1000 | Loss: 0.00002993
Iteration 362/1000 | Loss: 0.00002993
Iteration 363/1000 | Loss: 0.00002993
Iteration 364/1000 | Loss: 0.00002993
Iteration 365/1000 | Loss: 0.00002993
Iteration 366/1000 | Loss: 0.00002993
Iteration 367/1000 | Loss: 0.00002993
Iteration 368/1000 | Loss: 0.00002993
Iteration 369/1000 | Loss: 0.00002993
Iteration 370/1000 | Loss: 0.00002993
Iteration 371/1000 | Loss: 0.00002993
Iteration 372/1000 | Loss: 0.00002993
Iteration 373/1000 | Loss: 0.00002993
Iteration 374/1000 | Loss: 0.00002993
Iteration 375/1000 | Loss: 0.00002993
Iteration 376/1000 | Loss: 0.00002993
Iteration 377/1000 | Loss: 0.00002993
Iteration 378/1000 | Loss: 0.00002993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 378. Stopping optimization.
Last 5 losses: [2.9934761187178083e-05, 2.9934761187178083e-05, 2.9934761187178083e-05, 2.9934761187178083e-05, 2.9934761187178083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9934761187178083e-05

Optimization complete. Final v2v error: 4.175156116485596 mm

Highest mean error: 6.523195266723633 mm for frame 30

Lowest mean error: 3.2283878326416016 mm for frame 91

Saving results

Total time: 281.99240159988403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792628
Iteration 2/25 | Loss: 0.00149322
Iteration 3/25 | Loss: 0.00138872
Iteration 4/25 | Loss: 0.00127896
Iteration 5/25 | Loss: 0.00125991
Iteration 6/25 | Loss: 0.00124014
Iteration 7/25 | Loss: 0.00123426
Iteration 8/25 | Loss: 0.00122362
Iteration 9/25 | Loss: 0.00122221
Iteration 10/25 | Loss: 0.00122205
Iteration 11/25 | Loss: 0.00122203
Iteration 12/25 | Loss: 0.00122203
Iteration 13/25 | Loss: 0.00122203
Iteration 14/25 | Loss: 0.00122203
Iteration 15/25 | Loss: 0.00122203
Iteration 16/25 | Loss: 0.00122202
Iteration 17/25 | Loss: 0.00122202
Iteration 18/25 | Loss: 0.00122202
Iteration 19/25 | Loss: 0.00122202
Iteration 20/25 | Loss: 0.00122202
Iteration 21/25 | Loss: 0.00122202
Iteration 22/25 | Loss: 0.00122202
Iteration 23/25 | Loss: 0.00122202
Iteration 24/25 | Loss: 0.00122202
Iteration 25/25 | Loss: 0.00122202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02971816
Iteration 2/25 | Loss: 0.00078691
Iteration 3/25 | Loss: 0.00078690
Iteration 4/25 | Loss: 0.00078690
Iteration 5/25 | Loss: 0.00078690
Iteration 6/25 | Loss: 0.00078690
Iteration 7/25 | Loss: 0.00078690
Iteration 8/25 | Loss: 0.00078690
Iteration 9/25 | Loss: 0.00078690
Iteration 10/25 | Loss: 0.00078690
Iteration 11/25 | Loss: 0.00078690
Iteration 12/25 | Loss: 0.00078690
Iteration 13/25 | Loss: 0.00078690
Iteration 14/25 | Loss: 0.00078690
Iteration 15/25 | Loss: 0.00078690
Iteration 16/25 | Loss: 0.00078690
Iteration 17/25 | Loss: 0.00078690
Iteration 18/25 | Loss: 0.00078690
Iteration 19/25 | Loss: 0.00078690
Iteration 20/25 | Loss: 0.00078690
Iteration 21/25 | Loss: 0.00078690
Iteration 22/25 | Loss: 0.00078690
Iteration 23/25 | Loss: 0.00078690
Iteration 24/25 | Loss: 0.00078690
Iteration 25/25 | Loss: 0.00078690

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078690
Iteration 2/1000 | Loss: 0.00002748
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001877
Iteration 6/1000 | Loss: 0.00013257
Iteration 7/1000 | Loss: 0.00003164
Iteration 8/1000 | Loss: 0.00002058
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001714
Iteration 14/1000 | Loss: 0.00001705
Iteration 15/1000 | Loss: 0.00001703
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001685
Iteration 18/1000 | Loss: 0.00001685
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001679
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001676
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001675
Iteration 30/1000 | Loss: 0.00001675
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001671
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001665
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001663
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001663
Iteration 41/1000 | Loss: 0.00001663
Iteration 42/1000 | Loss: 0.00001663
Iteration 43/1000 | Loss: 0.00001660
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001659
Iteration 48/1000 | Loss: 0.00001658
Iteration 49/1000 | Loss: 0.00001656
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001656
Iteration 52/1000 | Loss: 0.00001656
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001652
Iteration 60/1000 | Loss: 0.00001651
Iteration 61/1000 | Loss: 0.00001651
Iteration 62/1000 | Loss: 0.00001651
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001649
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001646
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001643
Iteration 82/1000 | Loss: 0.00001643
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001640
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001639
Iteration 99/1000 | Loss: 0.00001639
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001638
Iteration 102/1000 | Loss: 0.00001638
Iteration 103/1000 | Loss: 0.00001637
Iteration 104/1000 | Loss: 0.00001637
Iteration 105/1000 | Loss: 0.00001637
Iteration 106/1000 | Loss: 0.00001637
Iteration 107/1000 | Loss: 0.00001637
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001636
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001635
Iteration 116/1000 | Loss: 0.00001635
Iteration 117/1000 | Loss: 0.00001635
Iteration 118/1000 | Loss: 0.00001635
Iteration 119/1000 | Loss: 0.00001635
Iteration 120/1000 | Loss: 0.00001635
Iteration 121/1000 | Loss: 0.00001635
Iteration 122/1000 | Loss: 0.00001634
Iteration 123/1000 | Loss: 0.00001634
Iteration 124/1000 | Loss: 0.00001634
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001633
Iteration 128/1000 | Loss: 0.00001633
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001632
Iteration 131/1000 | Loss: 0.00001632
Iteration 132/1000 | Loss: 0.00001632
Iteration 133/1000 | Loss: 0.00001632
Iteration 134/1000 | Loss: 0.00001631
Iteration 135/1000 | Loss: 0.00001631
Iteration 136/1000 | Loss: 0.00001631
Iteration 137/1000 | Loss: 0.00001631
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001630
Iteration 144/1000 | Loss: 0.00001630
Iteration 145/1000 | Loss: 0.00001630
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001628
Iteration 163/1000 | Loss: 0.00001628
Iteration 164/1000 | Loss: 0.00001628
Iteration 165/1000 | Loss: 0.00001628
Iteration 166/1000 | Loss: 0.00001628
Iteration 167/1000 | Loss: 0.00001628
Iteration 168/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.6283500372082926e-05, 1.6283500372082926e-05, 1.6283500372082926e-05, 1.6283500372082926e-05, 1.6283500372082926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6283500372082926e-05

Optimization complete. Final v2v error: 3.378743886947632 mm

Highest mean error: 4.010701656341553 mm for frame 107

Lowest mean error: 2.954693078994751 mm for frame 0

Saving results

Total time: 52.50636076927185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392254
Iteration 2/25 | Loss: 0.00133600
Iteration 3/25 | Loss: 0.00121473
Iteration 4/25 | Loss: 0.00120275
Iteration 5/25 | Loss: 0.00119919
Iteration 6/25 | Loss: 0.00119784
Iteration 7/25 | Loss: 0.00119755
Iteration 8/25 | Loss: 0.00119755
Iteration 9/25 | Loss: 0.00119755
Iteration 10/25 | Loss: 0.00119755
Iteration 11/25 | Loss: 0.00119755
Iteration 12/25 | Loss: 0.00119755
Iteration 13/25 | Loss: 0.00119755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011975530069321394, 0.0011975530069321394, 0.0011975530069321394, 0.0011975530069321394, 0.0011975530069321394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011975530069321394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38341868
Iteration 2/25 | Loss: 0.00068475
Iteration 3/25 | Loss: 0.00068475
Iteration 4/25 | Loss: 0.00068475
Iteration 5/25 | Loss: 0.00068475
Iteration 6/25 | Loss: 0.00068475
Iteration 7/25 | Loss: 0.00068475
Iteration 8/25 | Loss: 0.00068475
Iteration 9/25 | Loss: 0.00068475
Iteration 10/25 | Loss: 0.00068475
Iteration 11/25 | Loss: 0.00068475
Iteration 12/25 | Loss: 0.00068475
Iteration 13/25 | Loss: 0.00068475
Iteration 14/25 | Loss: 0.00068475
Iteration 15/25 | Loss: 0.00068475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006847511394880712, 0.0006847511394880712, 0.0006847511394880712, 0.0006847511394880712, 0.0006847511394880712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006847511394880712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068475
Iteration 2/1000 | Loss: 0.00004336
Iteration 3/1000 | Loss: 0.00002494
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001564
Iteration 7/1000 | Loss: 0.00001477
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001288
Iteration 14/1000 | Loss: 0.00001285
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001238
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001229
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001228
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001227
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001217
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001217
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001217
Iteration 78/1000 | Loss: 0.00001217
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001216
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001216
Iteration 85/1000 | Loss: 0.00001216
Iteration 86/1000 | Loss: 0.00001216
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00001212
Iteration 146/1000 | Loss: 0.00001212
Iteration 147/1000 | Loss: 0.00001212
Iteration 148/1000 | Loss: 0.00001212
Iteration 149/1000 | Loss: 0.00001212
Iteration 150/1000 | Loss: 0.00001211
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001211
Iteration 155/1000 | Loss: 0.00001211
Iteration 156/1000 | Loss: 0.00001211
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001210
Iteration 159/1000 | Loss: 0.00001210
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001210
Iteration 169/1000 | Loss: 0.00001210
Iteration 170/1000 | Loss: 0.00001210
Iteration 171/1000 | Loss: 0.00001210
Iteration 172/1000 | Loss: 0.00001210
Iteration 173/1000 | Loss: 0.00001210
Iteration 174/1000 | Loss: 0.00001210
Iteration 175/1000 | Loss: 0.00001210
Iteration 176/1000 | Loss: 0.00001210
Iteration 177/1000 | Loss: 0.00001210
Iteration 178/1000 | Loss: 0.00001210
Iteration 179/1000 | Loss: 0.00001210
Iteration 180/1000 | Loss: 0.00001210
Iteration 181/1000 | Loss: 0.00001210
Iteration 182/1000 | Loss: 0.00001210
Iteration 183/1000 | Loss: 0.00001210
Iteration 184/1000 | Loss: 0.00001210
Iteration 185/1000 | Loss: 0.00001210
Iteration 186/1000 | Loss: 0.00001210
Iteration 187/1000 | Loss: 0.00001210
Iteration 188/1000 | Loss: 0.00001210
Iteration 189/1000 | Loss: 0.00001210
Iteration 190/1000 | Loss: 0.00001210
Iteration 191/1000 | Loss: 0.00001210
Iteration 192/1000 | Loss: 0.00001210
Iteration 193/1000 | Loss: 0.00001210
Iteration 194/1000 | Loss: 0.00001210
Iteration 195/1000 | Loss: 0.00001210
Iteration 196/1000 | Loss: 0.00001210
Iteration 197/1000 | Loss: 0.00001210
Iteration 198/1000 | Loss: 0.00001210
Iteration 199/1000 | Loss: 0.00001210
Iteration 200/1000 | Loss: 0.00001210
Iteration 201/1000 | Loss: 0.00001210
Iteration 202/1000 | Loss: 0.00001210
Iteration 203/1000 | Loss: 0.00001210
Iteration 204/1000 | Loss: 0.00001210
Iteration 205/1000 | Loss: 0.00001210
Iteration 206/1000 | Loss: 0.00001210
Iteration 207/1000 | Loss: 0.00001210
Iteration 208/1000 | Loss: 0.00001210
Iteration 209/1000 | Loss: 0.00001210
Iteration 210/1000 | Loss: 0.00001210
Iteration 211/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.2095782039978076e-05, 1.2095782039978076e-05, 1.2095782039978076e-05, 1.2095782039978076e-05, 1.2095782039978076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2095782039978076e-05

Optimization complete. Final v2v error: 2.950674057006836 mm

Highest mean error: 3.643895149230957 mm for frame 104

Lowest mean error: 2.649496078491211 mm for frame 12

Saving results

Total time: 39.72284936904907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769412
Iteration 2/25 | Loss: 0.00175052
Iteration 3/25 | Loss: 0.00140902
Iteration 4/25 | Loss: 0.00138215
Iteration 5/25 | Loss: 0.00138093
Iteration 6/25 | Loss: 0.00138093
Iteration 7/25 | Loss: 0.00138093
Iteration 8/25 | Loss: 0.00138093
Iteration 9/25 | Loss: 0.00138093
Iteration 10/25 | Loss: 0.00138093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013809322845190763, 0.0013809322845190763, 0.0013809322845190763, 0.0013809322845190763, 0.0013809322845190763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013809322845190763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44176865
Iteration 2/25 | Loss: 0.00087551
Iteration 3/25 | Loss: 0.00087549
Iteration 4/25 | Loss: 0.00087549
Iteration 5/25 | Loss: 0.00087549
Iteration 6/25 | Loss: 0.00087549
Iteration 7/25 | Loss: 0.00087549
Iteration 8/25 | Loss: 0.00087549
Iteration 9/25 | Loss: 0.00087549
Iteration 10/25 | Loss: 0.00087549
Iteration 11/25 | Loss: 0.00087549
Iteration 12/25 | Loss: 0.00087549
Iteration 13/25 | Loss: 0.00087549
Iteration 14/25 | Loss: 0.00087549
Iteration 15/25 | Loss: 0.00087549
Iteration 16/25 | Loss: 0.00087549
Iteration 17/25 | Loss: 0.00087549
Iteration 18/25 | Loss: 0.00087549
Iteration 19/25 | Loss: 0.00087549
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008754873997531831, 0.0008754873997531831, 0.0008754873997531831, 0.0008754873997531831, 0.0008754873997531831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008754873997531831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087549
Iteration 2/1000 | Loss: 0.00005110
Iteration 3/1000 | Loss: 0.00003390
Iteration 4/1000 | Loss: 0.00003059
Iteration 5/1000 | Loss: 0.00002909
Iteration 6/1000 | Loss: 0.00002826
Iteration 7/1000 | Loss: 0.00002778
Iteration 8/1000 | Loss: 0.00002741
Iteration 9/1000 | Loss: 0.00002710
Iteration 10/1000 | Loss: 0.00002685
Iteration 11/1000 | Loss: 0.00002685
Iteration 12/1000 | Loss: 0.00002674
Iteration 13/1000 | Loss: 0.00002665
Iteration 14/1000 | Loss: 0.00002660
Iteration 15/1000 | Loss: 0.00002660
Iteration 16/1000 | Loss: 0.00002659
Iteration 17/1000 | Loss: 0.00002659
Iteration 18/1000 | Loss: 0.00002658
Iteration 19/1000 | Loss: 0.00002656
Iteration 20/1000 | Loss: 0.00002656
Iteration 21/1000 | Loss: 0.00002655
Iteration 22/1000 | Loss: 0.00002655
Iteration 23/1000 | Loss: 0.00002654
Iteration 24/1000 | Loss: 0.00002650
Iteration 25/1000 | Loss: 0.00002649
Iteration 26/1000 | Loss: 0.00002649
Iteration 27/1000 | Loss: 0.00002649
Iteration 28/1000 | Loss: 0.00002638
Iteration 29/1000 | Loss: 0.00002638
Iteration 30/1000 | Loss: 0.00002636
Iteration 31/1000 | Loss: 0.00002635
Iteration 32/1000 | Loss: 0.00002635
Iteration 33/1000 | Loss: 0.00002635
Iteration 34/1000 | Loss: 0.00002635
Iteration 35/1000 | Loss: 0.00002634
Iteration 36/1000 | Loss: 0.00002634
Iteration 37/1000 | Loss: 0.00002634
Iteration 38/1000 | Loss: 0.00002631
Iteration 39/1000 | Loss: 0.00002630
Iteration 40/1000 | Loss: 0.00002629
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002628
Iteration 43/1000 | Loss: 0.00002628
Iteration 44/1000 | Loss: 0.00002628
Iteration 45/1000 | Loss: 0.00002628
Iteration 46/1000 | Loss: 0.00002627
Iteration 47/1000 | Loss: 0.00002627
Iteration 48/1000 | Loss: 0.00002627
Iteration 49/1000 | Loss: 0.00002626
Iteration 50/1000 | Loss: 0.00002626
Iteration 51/1000 | Loss: 0.00002626
Iteration 52/1000 | Loss: 0.00002626
Iteration 53/1000 | Loss: 0.00002626
Iteration 54/1000 | Loss: 0.00002626
Iteration 55/1000 | Loss: 0.00002626
Iteration 56/1000 | Loss: 0.00002626
Iteration 57/1000 | Loss: 0.00002626
Iteration 58/1000 | Loss: 0.00002626
Iteration 59/1000 | Loss: 0.00002626
Iteration 60/1000 | Loss: 0.00002626
Iteration 61/1000 | Loss: 0.00002626
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002626
Iteration 65/1000 | Loss: 0.00002626
Iteration 66/1000 | Loss: 0.00002626
Iteration 67/1000 | Loss: 0.00002626
Iteration 68/1000 | Loss: 0.00002626
Iteration 69/1000 | Loss: 0.00002626
Iteration 70/1000 | Loss: 0.00002626
Iteration 71/1000 | Loss: 0.00002626
Iteration 72/1000 | Loss: 0.00002626
Iteration 73/1000 | Loss: 0.00002626
Iteration 74/1000 | Loss: 0.00002626
Iteration 75/1000 | Loss: 0.00002626
Iteration 76/1000 | Loss: 0.00002626
Iteration 77/1000 | Loss: 0.00002626
Iteration 78/1000 | Loss: 0.00002626
Iteration 79/1000 | Loss: 0.00002626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.6255151169607416e-05, 2.6255151169607416e-05, 2.6255151169607416e-05, 2.6255151169607416e-05, 2.6255151169607416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6255151169607416e-05

Optimization complete. Final v2v error: 4.287338733673096 mm

Highest mean error: 4.495131492614746 mm for frame 52

Lowest mean error: 4.110187530517578 mm for frame 109

Saving results

Total time: 28.676593780517578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793637
Iteration 2/25 | Loss: 0.00127158
Iteration 3/25 | Loss: 0.00119739
Iteration 4/25 | Loss: 0.00119212
Iteration 5/25 | Loss: 0.00119094
Iteration 6/25 | Loss: 0.00119094
Iteration 7/25 | Loss: 0.00119094
Iteration 8/25 | Loss: 0.00119094
Iteration 9/25 | Loss: 0.00119094
Iteration 10/25 | Loss: 0.00119094
Iteration 11/25 | Loss: 0.00119094
Iteration 12/25 | Loss: 0.00119094
Iteration 13/25 | Loss: 0.00119094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011909441091120243, 0.0011909441091120243, 0.0011909441091120243, 0.0011909441091120243, 0.0011909441091120243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011909441091120243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44317579
Iteration 2/25 | Loss: 0.00073833
Iteration 3/25 | Loss: 0.00073833
Iteration 4/25 | Loss: 0.00073833
Iteration 5/25 | Loss: 0.00073833
Iteration 6/25 | Loss: 0.00073833
Iteration 7/25 | Loss: 0.00073833
Iteration 8/25 | Loss: 0.00073833
Iteration 9/25 | Loss: 0.00073833
Iteration 10/25 | Loss: 0.00073833
Iteration 11/25 | Loss: 0.00073833
Iteration 12/25 | Loss: 0.00073833
Iteration 13/25 | Loss: 0.00073833
Iteration 14/25 | Loss: 0.00073832
Iteration 15/25 | Loss: 0.00073832
Iteration 16/25 | Loss: 0.00073832
Iteration 17/25 | Loss: 0.00073832
Iteration 18/25 | Loss: 0.00073832
Iteration 19/25 | Loss: 0.00073832
Iteration 20/25 | Loss: 0.00073832
Iteration 21/25 | Loss: 0.00073832
Iteration 22/25 | Loss: 0.00073832
Iteration 23/25 | Loss: 0.00073832
Iteration 24/25 | Loss: 0.00073832
Iteration 25/25 | Loss: 0.00073832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073832
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001653
Iteration 4/1000 | Loss: 0.00001459
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001295
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001169
Iteration 11/1000 | Loss: 0.00001168
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001159
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001147
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001137
Iteration 32/1000 | Loss: 0.00001137
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001136
Iteration 35/1000 | Loss: 0.00001136
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001132
Iteration 50/1000 | Loss: 0.00001132
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001129
Iteration 59/1000 | Loss: 0.00001129
Iteration 60/1000 | Loss: 0.00001129
Iteration 61/1000 | Loss: 0.00001129
Iteration 62/1000 | Loss: 0.00001129
Iteration 63/1000 | Loss: 0.00001129
Iteration 64/1000 | Loss: 0.00001129
Iteration 65/1000 | Loss: 0.00001128
Iteration 66/1000 | Loss: 0.00001128
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001126
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001125
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001119
Iteration 93/1000 | Loss: 0.00001119
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001116
Iteration 97/1000 | Loss: 0.00001116
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001115
Iteration 100/1000 | Loss: 0.00001115
Iteration 101/1000 | Loss: 0.00001115
Iteration 102/1000 | Loss: 0.00001115
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001114
Iteration 107/1000 | Loss: 0.00001114
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001112
Iteration 116/1000 | Loss: 0.00001112
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001111
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001108
Iteration 134/1000 | Loss: 0.00001108
Iteration 135/1000 | Loss: 0.00001108
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001106
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1057246410928201e-05, 1.1057246410928201e-05, 1.1057246410928201e-05, 1.1057246410928201e-05, 1.1057246410928201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1057246410928201e-05

Optimization complete. Final v2v error: 2.832618474960327 mm

Highest mean error: 3.1018764972686768 mm for frame 55

Lowest mean error: 2.6920440196990967 mm for frame 11

Saving results

Total time: 35.31127119064331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013756
Iteration 2/25 | Loss: 0.00292507
Iteration 3/25 | Loss: 0.00219052
Iteration 4/25 | Loss: 0.00191801
Iteration 5/25 | Loss: 0.00159825
Iteration 6/25 | Loss: 0.00150445
Iteration 7/25 | Loss: 0.00143879
Iteration 8/25 | Loss: 0.00136905
Iteration 9/25 | Loss: 0.00133819
Iteration 10/25 | Loss: 0.00130757
Iteration 11/25 | Loss: 0.00130886
Iteration 12/25 | Loss: 0.00129288
Iteration 13/25 | Loss: 0.00128716
Iteration 14/25 | Loss: 0.00128598
Iteration 15/25 | Loss: 0.00128568
Iteration 16/25 | Loss: 0.00128559
Iteration 17/25 | Loss: 0.00128553
Iteration 18/25 | Loss: 0.00128551
Iteration 19/25 | Loss: 0.00128551
Iteration 20/25 | Loss: 0.00128551
Iteration 21/25 | Loss: 0.00128551
Iteration 22/25 | Loss: 0.00128551
Iteration 23/25 | Loss: 0.00128551
Iteration 24/25 | Loss: 0.00128550
Iteration 25/25 | Loss: 0.00128550

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51089263
Iteration 2/25 | Loss: 0.00094949
Iteration 3/25 | Loss: 0.00089223
Iteration 4/25 | Loss: 0.00089223
Iteration 5/25 | Loss: 0.00089223
Iteration 6/25 | Loss: 0.00089223
Iteration 7/25 | Loss: 0.00089223
Iteration 8/25 | Loss: 0.00089223
Iteration 9/25 | Loss: 0.00089223
Iteration 10/25 | Loss: 0.00089223
Iteration 11/25 | Loss: 0.00089223
Iteration 12/25 | Loss: 0.00089223
Iteration 13/25 | Loss: 0.00089223
Iteration 14/25 | Loss: 0.00089223
Iteration 15/25 | Loss: 0.00089223
Iteration 16/25 | Loss: 0.00089223
Iteration 17/25 | Loss: 0.00089223
Iteration 18/25 | Loss: 0.00089223
Iteration 19/25 | Loss: 0.00089223
Iteration 20/25 | Loss: 0.00089223
Iteration 21/25 | Loss: 0.00089223
Iteration 22/25 | Loss: 0.00089223
Iteration 23/25 | Loss: 0.00089223
Iteration 24/25 | Loss: 0.00089223
Iteration 25/25 | Loss: 0.00089223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089223
Iteration 2/1000 | Loss: 0.00016276
Iteration 3/1000 | Loss: 0.00005075
Iteration 4/1000 | Loss: 0.00004258
Iteration 5/1000 | Loss: 0.00030618
Iteration 6/1000 | Loss: 0.00013676
Iteration 7/1000 | Loss: 0.00029343
Iteration 8/1000 | Loss: 0.00005202
Iteration 9/1000 | Loss: 0.00007513
Iteration 10/1000 | Loss: 0.00009708
Iteration 11/1000 | Loss: 0.00003695
Iteration 12/1000 | Loss: 0.00003613
Iteration 13/1000 | Loss: 0.00003528
Iteration 14/1000 | Loss: 0.00024580
Iteration 15/1000 | Loss: 0.00115105
Iteration 16/1000 | Loss: 0.00053222
Iteration 17/1000 | Loss: 0.00005169
Iteration 18/1000 | Loss: 0.00004465
Iteration 19/1000 | Loss: 0.00039453
Iteration 20/1000 | Loss: 0.00002995
Iteration 21/1000 | Loss: 0.00010238
Iteration 22/1000 | Loss: 0.00010633
Iteration 23/1000 | Loss: 0.00005115
Iteration 24/1000 | Loss: 0.00002268
Iteration 25/1000 | Loss: 0.00004313
Iteration 26/1000 | Loss: 0.00002161
Iteration 27/1000 | Loss: 0.00009062
Iteration 28/1000 | Loss: 0.00040653
Iteration 29/1000 | Loss: 0.00002364
Iteration 30/1000 | Loss: 0.00003351
Iteration 31/1000 | Loss: 0.00001989
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00010787
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00001852
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00013181
Iteration 44/1000 | Loss: 0.00003166
Iteration 45/1000 | Loss: 0.00003318
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001815
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001811
Iteration 51/1000 | Loss: 0.00001811
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001807
Iteration 60/1000 | Loss: 0.00001807
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001805
Iteration 74/1000 | Loss: 0.00001805
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001804
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001801
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001801
Iteration 87/1000 | Loss: 0.00001801
Iteration 88/1000 | Loss: 0.00001800
Iteration 89/1000 | Loss: 0.00001800
Iteration 90/1000 | Loss: 0.00001800
Iteration 91/1000 | Loss: 0.00001800
Iteration 92/1000 | Loss: 0.00001800
Iteration 93/1000 | Loss: 0.00001800
Iteration 94/1000 | Loss: 0.00001800
Iteration 95/1000 | Loss: 0.00001799
Iteration 96/1000 | Loss: 0.00001799
Iteration 97/1000 | Loss: 0.00001799
Iteration 98/1000 | Loss: 0.00001799
Iteration 99/1000 | Loss: 0.00001799
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001798
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001797
Iteration 113/1000 | Loss: 0.00001797
Iteration 114/1000 | Loss: 0.00001797
Iteration 115/1000 | Loss: 0.00001797
Iteration 116/1000 | Loss: 0.00001797
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001796
Iteration 121/1000 | Loss: 0.00001796
Iteration 122/1000 | Loss: 0.00001796
Iteration 123/1000 | Loss: 0.00001796
Iteration 124/1000 | Loss: 0.00001796
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001795
Iteration 128/1000 | Loss: 0.00001795
Iteration 129/1000 | Loss: 0.00001795
Iteration 130/1000 | Loss: 0.00001795
Iteration 131/1000 | Loss: 0.00001795
Iteration 132/1000 | Loss: 0.00001795
Iteration 133/1000 | Loss: 0.00001795
Iteration 134/1000 | Loss: 0.00001795
Iteration 135/1000 | Loss: 0.00001795
Iteration 136/1000 | Loss: 0.00001795
Iteration 137/1000 | Loss: 0.00001795
Iteration 138/1000 | Loss: 0.00001795
Iteration 139/1000 | Loss: 0.00001795
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001795
Iteration 142/1000 | Loss: 0.00001795
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00001795
Iteration 145/1000 | Loss: 0.00001795
Iteration 146/1000 | Loss: 0.00001795
Iteration 147/1000 | Loss: 0.00001795
Iteration 148/1000 | Loss: 0.00001795
Iteration 149/1000 | Loss: 0.00001795
Iteration 150/1000 | Loss: 0.00001795
Iteration 151/1000 | Loss: 0.00001795
Iteration 152/1000 | Loss: 0.00001795
Iteration 153/1000 | Loss: 0.00001795
Iteration 154/1000 | Loss: 0.00001795
Iteration 155/1000 | Loss: 0.00001795
Iteration 156/1000 | Loss: 0.00001795
Iteration 157/1000 | Loss: 0.00001795
Iteration 158/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.795082789612934e-05, 1.795082789612934e-05, 1.795082789612934e-05, 1.795082789612934e-05, 1.795082789612934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.795082789612934e-05

Optimization complete. Final v2v error: 3.58558988571167 mm

Highest mean error: 4.231502532958984 mm for frame 234

Lowest mean error: 3.3017499446868896 mm for frame 130

Saving results

Total time: 106.37594199180603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018607
Iteration 2/25 | Loss: 0.00268441
Iteration 3/25 | Loss: 0.00180163
Iteration 4/25 | Loss: 0.00165879
Iteration 5/25 | Loss: 0.00154845
Iteration 6/25 | Loss: 0.00148754
Iteration 7/25 | Loss: 0.00142319
Iteration 8/25 | Loss: 0.00139647
Iteration 9/25 | Loss: 0.00138999
Iteration 10/25 | Loss: 0.00134947
Iteration 11/25 | Loss: 0.00132048
Iteration 12/25 | Loss: 0.00130294
Iteration 13/25 | Loss: 0.00130095
Iteration 14/25 | Loss: 0.00131909
Iteration 15/25 | Loss: 0.00136296
Iteration 16/25 | Loss: 0.00135031
Iteration 17/25 | Loss: 0.00135482
Iteration 18/25 | Loss: 0.00129523
Iteration 19/25 | Loss: 0.00124879
Iteration 20/25 | Loss: 0.00124622
Iteration 21/25 | Loss: 0.00123162
Iteration 22/25 | Loss: 0.00123104
Iteration 23/25 | Loss: 0.00122632
Iteration 24/25 | Loss: 0.00122904
Iteration 25/25 | Loss: 0.00121816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50157499
Iteration 2/25 | Loss: 0.00101015
Iteration 3/25 | Loss: 0.00100886
Iteration 4/25 | Loss: 0.00100886
Iteration 5/25 | Loss: 0.00100886
Iteration 6/25 | Loss: 0.00100886
Iteration 7/25 | Loss: 0.00100886
Iteration 8/25 | Loss: 0.00100886
Iteration 9/25 | Loss: 0.00100886
Iteration 10/25 | Loss: 0.00100886
Iteration 11/25 | Loss: 0.00100886
Iteration 12/25 | Loss: 0.00100886
Iteration 13/25 | Loss: 0.00100886
Iteration 14/25 | Loss: 0.00100886
Iteration 15/25 | Loss: 0.00100886
Iteration 16/25 | Loss: 0.00100886
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010088608833029866, 0.0010088608833029866, 0.0010088608833029866, 0.0010088608833029866, 0.0010088608833029866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010088608833029866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100886
Iteration 2/1000 | Loss: 0.00012419
Iteration 3/1000 | Loss: 0.00051863
Iteration 4/1000 | Loss: 0.00032460
Iteration 5/1000 | Loss: 0.00027476
Iteration 6/1000 | Loss: 0.00017563
Iteration 7/1000 | Loss: 0.00037799
Iteration 8/1000 | Loss: 0.00078050
Iteration 9/1000 | Loss: 0.00025013
Iteration 10/1000 | Loss: 0.00023151
Iteration 11/1000 | Loss: 0.00024372
Iteration 12/1000 | Loss: 0.00020547
Iteration 13/1000 | Loss: 0.00024486
Iteration 14/1000 | Loss: 0.00036386
Iteration 15/1000 | Loss: 0.00024707
Iteration 16/1000 | Loss: 0.00026259
Iteration 17/1000 | Loss: 0.00022488
Iteration 18/1000 | Loss: 0.00014098
Iteration 19/1000 | Loss: 0.00015250
Iteration 20/1000 | Loss: 0.00048157
Iteration 21/1000 | Loss: 0.00031667
Iteration 22/1000 | Loss: 0.00040097
Iteration 23/1000 | Loss: 0.00024664
Iteration 24/1000 | Loss: 0.00062175
Iteration 25/1000 | Loss: 0.00014240
Iteration 26/1000 | Loss: 0.00014141
Iteration 27/1000 | Loss: 0.00045992
Iteration 28/1000 | Loss: 0.00046010
Iteration 29/1000 | Loss: 0.00031014
Iteration 30/1000 | Loss: 0.00022745
Iteration 31/1000 | Loss: 0.00048338
Iteration 32/1000 | Loss: 0.00047576
Iteration 33/1000 | Loss: 0.00024983
Iteration 34/1000 | Loss: 0.00026489
Iteration 35/1000 | Loss: 0.00025391
Iteration 36/1000 | Loss: 0.00027160
Iteration 37/1000 | Loss: 0.00044717
Iteration 38/1000 | Loss: 0.00017637
Iteration 39/1000 | Loss: 0.00019469
Iteration 40/1000 | Loss: 0.00041790
Iteration 41/1000 | Loss: 0.00028231
Iteration 42/1000 | Loss: 0.00026867
Iteration 43/1000 | Loss: 0.00028381
Iteration 44/1000 | Loss: 0.00034246
Iteration 45/1000 | Loss: 0.00026276
Iteration 46/1000 | Loss: 0.00027539
Iteration 47/1000 | Loss: 0.00040844
Iteration 48/1000 | Loss: 0.00022223
Iteration 49/1000 | Loss: 0.00034980
Iteration 50/1000 | Loss: 0.00035820
Iteration 51/1000 | Loss: 0.00032061
Iteration 52/1000 | Loss: 0.00086845
Iteration 53/1000 | Loss: 0.00022046
Iteration 54/1000 | Loss: 0.00029565
Iteration 55/1000 | Loss: 0.00038951
Iteration 56/1000 | Loss: 0.00027683
Iteration 57/1000 | Loss: 0.00024602
Iteration 58/1000 | Loss: 0.00020125
Iteration 59/1000 | Loss: 0.00020181
Iteration 60/1000 | Loss: 0.00019961
Iteration 61/1000 | Loss: 0.00103228
Iteration 62/1000 | Loss: 0.00036001
Iteration 63/1000 | Loss: 0.00040913
Iteration 64/1000 | Loss: 0.00031851
Iteration 65/1000 | Loss: 0.00052807
Iteration 66/1000 | Loss: 0.00012823
Iteration 67/1000 | Loss: 0.00017267
Iteration 68/1000 | Loss: 0.00028657
Iteration 69/1000 | Loss: 0.00041249
Iteration 70/1000 | Loss: 0.00025865
Iteration 71/1000 | Loss: 0.00023241
Iteration 72/1000 | Loss: 0.00023447
Iteration 73/1000 | Loss: 0.00016187
Iteration 74/1000 | Loss: 0.00013353
Iteration 75/1000 | Loss: 0.00031540
Iteration 76/1000 | Loss: 0.00049075
Iteration 77/1000 | Loss: 0.00030853
Iteration 78/1000 | Loss: 0.00046099
Iteration 79/1000 | Loss: 0.00051289
Iteration 80/1000 | Loss: 0.00050575
Iteration 81/1000 | Loss: 0.00039018
Iteration 82/1000 | Loss: 0.00020597
Iteration 83/1000 | Loss: 0.00020690
Iteration 84/1000 | Loss: 0.00060848
Iteration 85/1000 | Loss: 0.00013527
Iteration 86/1000 | Loss: 0.00023558
Iteration 87/1000 | Loss: 0.00016399
Iteration 88/1000 | Loss: 0.00022934
Iteration 89/1000 | Loss: 0.00012208
Iteration 90/1000 | Loss: 0.00013089
Iteration 91/1000 | Loss: 0.00036102
Iteration 92/1000 | Loss: 0.00034971
Iteration 93/1000 | Loss: 0.00016603
Iteration 94/1000 | Loss: 0.00030445
Iteration 95/1000 | Loss: 0.00013062
Iteration 96/1000 | Loss: 0.00012831
Iteration 97/1000 | Loss: 0.00016746
Iteration 98/1000 | Loss: 0.00014029
Iteration 99/1000 | Loss: 0.00015617
Iteration 100/1000 | Loss: 0.00009379
Iteration 101/1000 | Loss: 0.00025124
Iteration 102/1000 | Loss: 0.00004933
Iteration 103/1000 | Loss: 0.00004892
Iteration 104/1000 | Loss: 0.00008903
Iteration 105/1000 | Loss: 0.00026939
Iteration 106/1000 | Loss: 0.00011927
Iteration 107/1000 | Loss: 0.00010966
Iteration 108/1000 | Loss: 0.00016404
Iteration 109/1000 | Loss: 0.00016537
Iteration 110/1000 | Loss: 0.00017111
Iteration 111/1000 | Loss: 0.00018233
Iteration 112/1000 | Loss: 0.00017322
Iteration 113/1000 | Loss: 0.00027535
Iteration 114/1000 | Loss: 0.00027492
Iteration 115/1000 | Loss: 0.00028841
Iteration 116/1000 | Loss: 0.00042371
Iteration 117/1000 | Loss: 0.00024575
Iteration 118/1000 | Loss: 0.00026457
Iteration 119/1000 | Loss: 0.00027266
Iteration 120/1000 | Loss: 0.00029354
Iteration 121/1000 | Loss: 0.00012515
Iteration 122/1000 | Loss: 0.00017283
Iteration 123/1000 | Loss: 0.00018593
Iteration 124/1000 | Loss: 0.00024191
Iteration 125/1000 | Loss: 0.00003928
Iteration 126/1000 | Loss: 0.00003173
Iteration 127/1000 | Loss: 0.00008427
Iteration 128/1000 | Loss: 0.00002619
Iteration 129/1000 | Loss: 0.00002482
Iteration 130/1000 | Loss: 0.00004268
Iteration 131/1000 | Loss: 0.00002376
Iteration 132/1000 | Loss: 0.00007732
Iteration 133/1000 | Loss: 0.00002232
Iteration 134/1000 | Loss: 0.00007076
Iteration 135/1000 | Loss: 0.00002553
Iteration 136/1000 | Loss: 0.00001977
Iteration 137/1000 | Loss: 0.00022174
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001846
Iteration 140/1000 | Loss: 0.00002253
Iteration 141/1000 | Loss: 0.00001803
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001760
Iteration 144/1000 | Loss: 0.00003917
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001736
Iteration 147/1000 | Loss: 0.00001734
Iteration 148/1000 | Loss: 0.00001733
Iteration 149/1000 | Loss: 0.00006492
Iteration 150/1000 | Loss: 0.00001727
Iteration 151/1000 | Loss: 0.00001724
Iteration 152/1000 | Loss: 0.00001724
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001720
Iteration 157/1000 | Loss: 0.00001719
Iteration 158/1000 | Loss: 0.00001719
Iteration 159/1000 | Loss: 0.00001718
Iteration 160/1000 | Loss: 0.00001718
Iteration 161/1000 | Loss: 0.00001718
Iteration 162/1000 | Loss: 0.00001718
Iteration 163/1000 | Loss: 0.00001718
Iteration 164/1000 | Loss: 0.00001717
Iteration 165/1000 | Loss: 0.00001717
Iteration 166/1000 | Loss: 0.00001717
Iteration 167/1000 | Loss: 0.00001717
Iteration 168/1000 | Loss: 0.00001716
Iteration 169/1000 | Loss: 0.00001716
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001714
Iteration 175/1000 | Loss: 0.00001714
Iteration 176/1000 | Loss: 0.00001714
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001712
Iteration 180/1000 | Loss: 0.00001712
Iteration 181/1000 | Loss: 0.00001711
Iteration 182/1000 | Loss: 0.00001711
Iteration 183/1000 | Loss: 0.00001710
Iteration 184/1000 | Loss: 0.00001710
Iteration 185/1000 | Loss: 0.00001709
Iteration 186/1000 | Loss: 0.00001708
Iteration 187/1000 | Loss: 0.00001707
Iteration 188/1000 | Loss: 0.00006258
Iteration 189/1000 | Loss: 0.00004547
Iteration 190/1000 | Loss: 0.00001704
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001704
Iteration 196/1000 | Loss: 0.00001704
Iteration 197/1000 | Loss: 0.00001704
Iteration 198/1000 | Loss: 0.00001704
Iteration 199/1000 | Loss: 0.00001704
Iteration 200/1000 | Loss: 0.00001704
Iteration 201/1000 | Loss: 0.00001704
Iteration 202/1000 | Loss: 0.00001704
Iteration 203/1000 | Loss: 0.00001704
Iteration 204/1000 | Loss: 0.00001704
Iteration 205/1000 | Loss: 0.00001704
Iteration 206/1000 | Loss: 0.00001704
Iteration 207/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.7035639757523313e-05, 1.7035639757523313e-05, 1.7035639757523313e-05, 1.7035639757523313e-05, 1.7035639757523313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7035639757523313e-05

Optimization complete. Final v2v error: 3.4042274951934814 mm

Highest mean error: 7.717698574066162 mm for frame 72

Lowest mean error: 2.683969020843506 mm for frame 164

Saving results

Total time: 290.5440990924835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809013
Iteration 2/25 | Loss: 0.00128876
Iteration 3/25 | Loss: 0.00119590
Iteration 4/25 | Loss: 0.00118463
Iteration 5/25 | Loss: 0.00118194
Iteration 6/25 | Loss: 0.00118192
Iteration 7/25 | Loss: 0.00118192
Iteration 8/25 | Loss: 0.00118192
Iteration 9/25 | Loss: 0.00118192
Iteration 10/25 | Loss: 0.00118192
Iteration 11/25 | Loss: 0.00118192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011819156352430582, 0.0011819156352430582, 0.0011819156352430582, 0.0011819156352430582, 0.0011819156352430582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819156352430582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44265723
Iteration 2/25 | Loss: 0.00067578
Iteration 3/25 | Loss: 0.00067578
Iteration 4/25 | Loss: 0.00067578
Iteration 5/25 | Loss: 0.00067578
Iteration 6/25 | Loss: 0.00067577
Iteration 7/25 | Loss: 0.00067577
Iteration 8/25 | Loss: 0.00067577
Iteration 9/25 | Loss: 0.00067577
Iteration 10/25 | Loss: 0.00067577
Iteration 11/25 | Loss: 0.00067577
Iteration 12/25 | Loss: 0.00067577
Iteration 13/25 | Loss: 0.00067577
Iteration 14/25 | Loss: 0.00067577
Iteration 15/25 | Loss: 0.00067577
Iteration 16/25 | Loss: 0.00067577
Iteration 17/25 | Loss: 0.00067577
Iteration 18/25 | Loss: 0.00067577
Iteration 19/25 | Loss: 0.00067577
Iteration 20/25 | Loss: 0.00067577
Iteration 21/25 | Loss: 0.00067577
Iteration 22/25 | Loss: 0.00067577
Iteration 23/25 | Loss: 0.00067577
Iteration 24/25 | Loss: 0.00067577
Iteration 25/25 | Loss: 0.00067577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067577
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00001675
Iteration 4/1000 | Loss: 0.00001521
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001338
Iteration 7/1000 | Loss: 0.00001282
Iteration 8/1000 | Loss: 0.00001235
Iteration 9/1000 | Loss: 0.00001224
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001169
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001157
Iteration 31/1000 | Loss: 0.00001156
Iteration 32/1000 | Loss: 0.00001155
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001152
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001151
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001150
Iteration 48/1000 | Loss: 0.00001149
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001149
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001149
Iteration 54/1000 | Loss: 0.00001148
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001147
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001144
Iteration 71/1000 | Loss: 0.00001144
Iteration 72/1000 | Loss: 0.00001144
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001140
Iteration 79/1000 | Loss: 0.00001140
Iteration 80/1000 | Loss: 0.00001140
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001132
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001131
Iteration 106/1000 | Loss: 0.00001131
Iteration 107/1000 | Loss: 0.00001131
Iteration 108/1000 | Loss: 0.00001130
Iteration 109/1000 | Loss: 0.00001130
Iteration 110/1000 | Loss: 0.00001130
Iteration 111/1000 | Loss: 0.00001129
Iteration 112/1000 | Loss: 0.00001129
Iteration 113/1000 | Loss: 0.00001129
Iteration 114/1000 | Loss: 0.00001129
Iteration 115/1000 | Loss: 0.00001129
Iteration 116/1000 | Loss: 0.00001129
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001128
Iteration 120/1000 | Loss: 0.00001126
Iteration 121/1000 | Loss: 0.00001126
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001123
Iteration 125/1000 | Loss: 0.00001123
Iteration 126/1000 | Loss: 0.00001122
Iteration 127/1000 | Loss: 0.00001122
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001122
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001121
Iteration 132/1000 | Loss: 0.00001121
Iteration 133/1000 | Loss: 0.00001121
Iteration 134/1000 | Loss: 0.00001121
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001119
Iteration 142/1000 | Loss: 0.00001119
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001119
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001119
Iteration 149/1000 | Loss: 0.00001119
Iteration 150/1000 | Loss: 0.00001119
Iteration 151/1000 | Loss: 0.00001119
Iteration 152/1000 | Loss: 0.00001119
Iteration 153/1000 | Loss: 0.00001119
Iteration 154/1000 | Loss: 0.00001119
Iteration 155/1000 | Loss: 0.00001119
Iteration 156/1000 | Loss: 0.00001119
Iteration 157/1000 | Loss: 0.00001118
Iteration 158/1000 | Loss: 0.00001118
Iteration 159/1000 | Loss: 0.00001118
Iteration 160/1000 | Loss: 0.00001118
Iteration 161/1000 | Loss: 0.00001118
Iteration 162/1000 | Loss: 0.00001118
Iteration 163/1000 | Loss: 0.00001118
Iteration 164/1000 | Loss: 0.00001118
Iteration 165/1000 | Loss: 0.00001118
Iteration 166/1000 | Loss: 0.00001117
Iteration 167/1000 | Loss: 0.00001117
Iteration 168/1000 | Loss: 0.00001117
Iteration 169/1000 | Loss: 0.00001117
Iteration 170/1000 | Loss: 0.00001117
Iteration 171/1000 | Loss: 0.00001117
Iteration 172/1000 | Loss: 0.00001117
Iteration 173/1000 | Loss: 0.00001117
Iteration 174/1000 | Loss: 0.00001117
Iteration 175/1000 | Loss: 0.00001117
Iteration 176/1000 | Loss: 0.00001117
Iteration 177/1000 | Loss: 0.00001117
Iteration 178/1000 | Loss: 0.00001117
Iteration 179/1000 | Loss: 0.00001117
Iteration 180/1000 | Loss: 0.00001117
Iteration 181/1000 | Loss: 0.00001117
Iteration 182/1000 | Loss: 0.00001117
Iteration 183/1000 | Loss: 0.00001117
Iteration 184/1000 | Loss: 0.00001116
Iteration 185/1000 | Loss: 0.00001116
Iteration 186/1000 | Loss: 0.00001116
Iteration 187/1000 | Loss: 0.00001116
Iteration 188/1000 | Loss: 0.00001116
Iteration 189/1000 | Loss: 0.00001116
Iteration 190/1000 | Loss: 0.00001116
Iteration 191/1000 | Loss: 0.00001116
Iteration 192/1000 | Loss: 0.00001116
Iteration 193/1000 | Loss: 0.00001115
Iteration 194/1000 | Loss: 0.00001115
Iteration 195/1000 | Loss: 0.00001115
Iteration 196/1000 | Loss: 0.00001115
Iteration 197/1000 | Loss: 0.00001115
Iteration 198/1000 | Loss: 0.00001115
Iteration 199/1000 | Loss: 0.00001115
Iteration 200/1000 | Loss: 0.00001115
Iteration 201/1000 | Loss: 0.00001115
Iteration 202/1000 | Loss: 0.00001114
Iteration 203/1000 | Loss: 0.00001114
Iteration 204/1000 | Loss: 0.00001114
Iteration 205/1000 | Loss: 0.00001114
Iteration 206/1000 | Loss: 0.00001114
Iteration 207/1000 | Loss: 0.00001114
Iteration 208/1000 | Loss: 0.00001114
Iteration 209/1000 | Loss: 0.00001114
Iteration 210/1000 | Loss: 0.00001114
Iteration 211/1000 | Loss: 0.00001114
Iteration 212/1000 | Loss: 0.00001113
Iteration 213/1000 | Loss: 0.00001113
Iteration 214/1000 | Loss: 0.00001113
Iteration 215/1000 | Loss: 0.00001113
Iteration 216/1000 | Loss: 0.00001113
Iteration 217/1000 | Loss: 0.00001113
Iteration 218/1000 | Loss: 0.00001113
Iteration 219/1000 | Loss: 0.00001113
Iteration 220/1000 | Loss: 0.00001113
Iteration 221/1000 | Loss: 0.00001112
Iteration 222/1000 | Loss: 0.00001112
Iteration 223/1000 | Loss: 0.00001112
Iteration 224/1000 | Loss: 0.00001112
Iteration 225/1000 | Loss: 0.00001112
Iteration 226/1000 | Loss: 0.00001112
Iteration 227/1000 | Loss: 0.00001112
Iteration 228/1000 | Loss: 0.00001112
Iteration 229/1000 | Loss: 0.00001112
Iteration 230/1000 | Loss: 0.00001112
Iteration 231/1000 | Loss: 0.00001112
Iteration 232/1000 | Loss: 0.00001111
Iteration 233/1000 | Loss: 0.00001111
Iteration 234/1000 | Loss: 0.00001111
Iteration 235/1000 | Loss: 0.00001111
Iteration 236/1000 | Loss: 0.00001111
Iteration 237/1000 | Loss: 0.00001111
Iteration 238/1000 | Loss: 0.00001111
Iteration 239/1000 | Loss: 0.00001111
Iteration 240/1000 | Loss: 0.00001111
Iteration 241/1000 | Loss: 0.00001111
Iteration 242/1000 | Loss: 0.00001111
Iteration 243/1000 | Loss: 0.00001111
Iteration 244/1000 | Loss: 0.00001111
Iteration 245/1000 | Loss: 0.00001111
Iteration 246/1000 | Loss: 0.00001111
Iteration 247/1000 | Loss: 0.00001111
Iteration 248/1000 | Loss: 0.00001111
Iteration 249/1000 | Loss: 0.00001110
Iteration 250/1000 | Loss: 0.00001110
Iteration 251/1000 | Loss: 0.00001110
Iteration 252/1000 | Loss: 0.00001110
Iteration 253/1000 | Loss: 0.00001110
Iteration 254/1000 | Loss: 0.00001110
Iteration 255/1000 | Loss: 0.00001110
Iteration 256/1000 | Loss: 0.00001110
Iteration 257/1000 | Loss: 0.00001110
Iteration 258/1000 | Loss: 0.00001110
Iteration 259/1000 | Loss: 0.00001110
Iteration 260/1000 | Loss: 0.00001110
Iteration 261/1000 | Loss: 0.00001110
Iteration 262/1000 | Loss: 0.00001110
Iteration 263/1000 | Loss: 0.00001110
Iteration 264/1000 | Loss: 0.00001110
Iteration 265/1000 | Loss: 0.00001109
Iteration 266/1000 | Loss: 0.00001109
Iteration 267/1000 | Loss: 0.00001109
Iteration 268/1000 | Loss: 0.00001109
Iteration 269/1000 | Loss: 0.00001109
Iteration 270/1000 | Loss: 0.00001109
Iteration 271/1000 | Loss: 0.00001109
Iteration 272/1000 | Loss: 0.00001109
Iteration 273/1000 | Loss: 0.00001109
Iteration 274/1000 | Loss: 0.00001109
Iteration 275/1000 | Loss: 0.00001109
Iteration 276/1000 | Loss: 0.00001109
Iteration 277/1000 | Loss: 0.00001109
Iteration 278/1000 | Loss: 0.00001109
Iteration 279/1000 | Loss: 0.00001109
Iteration 280/1000 | Loss: 0.00001108
Iteration 281/1000 | Loss: 0.00001108
Iteration 282/1000 | Loss: 0.00001108
Iteration 283/1000 | Loss: 0.00001108
Iteration 284/1000 | Loss: 0.00001108
Iteration 285/1000 | Loss: 0.00001108
Iteration 286/1000 | Loss: 0.00001108
Iteration 287/1000 | Loss: 0.00001108
Iteration 288/1000 | Loss: 0.00001108
Iteration 289/1000 | Loss: 0.00001108
Iteration 290/1000 | Loss: 0.00001108
Iteration 291/1000 | Loss: 0.00001108
Iteration 292/1000 | Loss: 0.00001108
Iteration 293/1000 | Loss: 0.00001108
Iteration 294/1000 | Loss: 0.00001108
Iteration 295/1000 | Loss: 0.00001107
Iteration 296/1000 | Loss: 0.00001107
Iteration 297/1000 | Loss: 0.00001107
Iteration 298/1000 | Loss: 0.00001107
Iteration 299/1000 | Loss: 0.00001107
Iteration 300/1000 | Loss: 0.00001107
Iteration 301/1000 | Loss: 0.00001107
Iteration 302/1000 | Loss: 0.00001107
Iteration 303/1000 | Loss: 0.00001107
Iteration 304/1000 | Loss: 0.00001107
Iteration 305/1000 | Loss: 0.00001107
Iteration 306/1000 | Loss: 0.00001107
Iteration 307/1000 | Loss: 0.00001107
Iteration 308/1000 | Loss: 0.00001107
Iteration 309/1000 | Loss: 0.00001107
Iteration 310/1000 | Loss: 0.00001107
Iteration 311/1000 | Loss: 0.00001107
Iteration 312/1000 | Loss: 0.00001107
Iteration 313/1000 | Loss: 0.00001106
Iteration 314/1000 | Loss: 0.00001106
Iteration 315/1000 | Loss: 0.00001106
Iteration 316/1000 | Loss: 0.00001106
Iteration 317/1000 | Loss: 0.00001106
Iteration 318/1000 | Loss: 0.00001106
Iteration 319/1000 | Loss: 0.00001106
Iteration 320/1000 | Loss: 0.00001106
Iteration 321/1000 | Loss: 0.00001106
Iteration 322/1000 | Loss: 0.00001106
Iteration 323/1000 | Loss: 0.00001106
Iteration 324/1000 | Loss: 0.00001106
Iteration 325/1000 | Loss: 0.00001106
Iteration 326/1000 | Loss: 0.00001106
Iteration 327/1000 | Loss: 0.00001106
Iteration 328/1000 | Loss: 0.00001106
Iteration 329/1000 | Loss: 0.00001106
Iteration 330/1000 | Loss: 0.00001106
Iteration 331/1000 | Loss: 0.00001106
Iteration 332/1000 | Loss: 0.00001106
Iteration 333/1000 | Loss: 0.00001106
Iteration 334/1000 | Loss: 0.00001106
Iteration 335/1000 | Loss: 0.00001106
Iteration 336/1000 | Loss: 0.00001106
Iteration 337/1000 | Loss: 0.00001106
Iteration 338/1000 | Loss: 0.00001106
Iteration 339/1000 | Loss: 0.00001106
Iteration 340/1000 | Loss: 0.00001106
Iteration 341/1000 | Loss: 0.00001106
Iteration 342/1000 | Loss: 0.00001106
Iteration 343/1000 | Loss: 0.00001106
Iteration 344/1000 | Loss: 0.00001106
Iteration 345/1000 | Loss: 0.00001106
Iteration 346/1000 | Loss: 0.00001106
Iteration 347/1000 | Loss: 0.00001106
Iteration 348/1000 | Loss: 0.00001106
Iteration 349/1000 | Loss: 0.00001106
Iteration 350/1000 | Loss: 0.00001106
Iteration 351/1000 | Loss: 0.00001106
Iteration 352/1000 | Loss: 0.00001106
Iteration 353/1000 | Loss: 0.00001106
Iteration 354/1000 | Loss: 0.00001106
Iteration 355/1000 | Loss: 0.00001106
Iteration 356/1000 | Loss: 0.00001106
Iteration 357/1000 | Loss: 0.00001106
Iteration 358/1000 | Loss: 0.00001106
Iteration 359/1000 | Loss: 0.00001106
Iteration 360/1000 | Loss: 0.00001106
Iteration 361/1000 | Loss: 0.00001106
Iteration 362/1000 | Loss: 0.00001106
Iteration 363/1000 | Loss: 0.00001106
Iteration 364/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 364. Stopping optimization.
Last 5 losses: [1.1058631571359001e-05, 1.1058631571359001e-05, 1.1058631571359001e-05, 1.1058631571359001e-05, 1.1058631571359001e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1058631571359001e-05

Optimization complete. Final v2v error: 2.8301329612731934 mm

Highest mean error: 3.062688112258911 mm for frame 44

Lowest mean error: 2.6452112197875977 mm for frame 0

Saving results

Total time: 46.65887713432312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904747
Iteration 2/25 | Loss: 0.00167898
Iteration 3/25 | Loss: 0.00139271
Iteration 4/25 | Loss: 0.00136418
Iteration 5/25 | Loss: 0.00135505
Iteration 6/25 | Loss: 0.00135341
Iteration 7/25 | Loss: 0.00135341
Iteration 8/25 | Loss: 0.00135341
Iteration 9/25 | Loss: 0.00135341
Iteration 10/25 | Loss: 0.00135341
Iteration 11/25 | Loss: 0.00135341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013534104218706489, 0.0013534104218706489, 0.0013534104218706489, 0.0013534104218706489, 0.0013534104218706489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013534104218706489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07949674
Iteration 2/25 | Loss: 0.00089087
Iteration 3/25 | Loss: 0.00089087
Iteration 4/25 | Loss: 0.00089087
Iteration 5/25 | Loss: 0.00089087
Iteration 6/25 | Loss: 0.00089087
Iteration 7/25 | Loss: 0.00089087
Iteration 8/25 | Loss: 0.00089087
Iteration 9/25 | Loss: 0.00089087
Iteration 10/25 | Loss: 0.00089087
Iteration 11/25 | Loss: 0.00089086
Iteration 12/25 | Loss: 0.00089086
Iteration 13/25 | Loss: 0.00089086
Iteration 14/25 | Loss: 0.00089086
Iteration 15/25 | Loss: 0.00089086
Iteration 16/25 | Loss: 0.00089086
Iteration 17/25 | Loss: 0.00089086
Iteration 18/25 | Loss: 0.00089086
Iteration 19/25 | Loss: 0.00089086
Iteration 20/25 | Loss: 0.00089086
Iteration 21/25 | Loss: 0.00089086
Iteration 22/25 | Loss: 0.00089086
Iteration 23/25 | Loss: 0.00089086
Iteration 24/25 | Loss: 0.00089086
Iteration 25/25 | Loss: 0.00089086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089086
Iteration 2/1000 | Loss: 0.00006750
Iteration 3/1000 | Loss: 0.00004439
Iteration 4/1000 | Loss: 0.00004066
Iteration 5/1000 | Loss: 0.00003842
Iteration 6/1000 | Loss: 0.00003712
Iteration 7/1000 | Loss: 0.00003599
Iteration 8/1000 | Loss: 0.00003523
Iteration 9/1000 | Loss: 0.00003472
Iteration 10/1000 | Loss: 0.00003433
Iteration 11/1000 | Loss: 0.00003403
Iteration 12/1000 | Loss: 0.00003369
Iteration 13/1000 | Loss: 0.00003339
Iteration 14/1000 | Loss: 0.00003308
Iteration 15/1000 | Loss: 0.00003288
Iteration 16/1000 | Loss: 0.00003263
Iteration 17/1000 | Loss: 0.00003242
Iteration 18/1000 | Loss: 0.00003223
Iteration 19/1000 | Loss: 0.00003208
Iteration 20/1000 | Loss: 0.00003201
Iteration 21/1000 | Loss: 0.00003199
Iteration 22/1000 | Loss: 0.00003199
Iteration 23/1000 | Loss: 0.00003196
Iteration 24/1000 | Loss: 0.00003194
Iteration 25/1000 | Loss: 0.00003193
Iteration 26/1000 | Loss: 0.00003189
Iteration 27/1000 | Loss: 0.00003186
Iteration 28/1000 | Loss: 0.00003186
Iteration 29/1000 | Loss: 0.00003185
Iteration 30/1000 | Loss: 0.00003185
Iteration 31/1000 | Loss: 0.00003185
Iteration 32/1000 | Loss: 0.00003185
Iteration 33/1000 | Loss: 0.00003185
Iteration 34/1000 | Loss: 0.00003185
Iteration 35/1000 | Loss: 0.00003185
Iteration 36/1000 | Loss: 0.00003185
Iteration 37/1000 | Loss: 0.00003185
Iteration 38/1000 | Loss: 0.00003184
Iteration 39/1000 | Loss: 0.00003184
Iteration 40/1000 | Loss: 0.00003184
Iteration 41/1000 | Loss: 0.00003184
Iteration 42/1000 | Loss: 0.00003181
Iteration 43/1000 | Loss: 0.00003181
Iteration 44/1000 | Loss: 0.00003181
Iteration 45/1000 | Loss: 0.00003180
Iteration 46/1000 | Loss: 0.00003180
Iteration 47/1000 | Loss: 0.00003179
Iteration 48/1000 | Loss: 0.00003179
Iteration 49/1000 | Loss: 0.00003179
Iteration 50/1000 | Loss: 0.00003178
Iteration 51/1000 | Loss: 0.00003176
Iteration 52/1000 | Loss: 0.00003175
Iteration 53/1000 | Loss: 0.00003174
Iteration 54/1000 | Loss: 0.00003174
Iteration 55/1000 | Loss: 0.00003173
Iteration 56/1000 | Loss: 0.00003173
Iteration 57/1000 | Loss: 0.00003173
Iteration 58/1000 | Loss: 0.00003172
Iteration 59/1000 | Loss: 0.00003172
Iteration 60/1000 | Loss: 0.00003172
Iteration 61/1000 | Loss: 0.00003172
Iteration 62/1000 | Loss: 0.00003171
Iteration 63/1000 | Loss: 0.00003171
Iteration 64/1000 | Loss: 0.00003171
Iteration 65/1000 | Loss: 0.00003171
Iteration 66/1000 | Loss: 0.00003171
Iteration 67/1000 | Loss: 0.00003171
Iteration 68/1000 | Loss: 0.00003170
Iteration 69/1000 | Loss: 0.00003170
Iteration 70/1000 | Loss: 0.00003170
Iteration 71/1000 | Loss: 0.00003170
Iteration 72/1000 | Loss: 0.00003170
Iteration 73/1000 | Loss: 0.00003170
Iteration 74/1000 | Loss: 0.00003169
Iteration 75/1000 | Loss: 0.00003169
Iteration 76/1000 | Loss: 0.00003169
Iteration 77/1000 | Loss: 0.00003169
Iteration 78/1000 | Loss: 0.00003168
Iteration 79/1000 | Loss: 0.00003168
Iteration 80/1000 | Loss: 0.00003168
Iteration 81/1000 | Loss: 0.00003168
Iteration 82/1000 | Loss: 0.00003168
Iteration 83/1000 | Loss: 0.00003168
Iteration 84/1000 | Loss: 0.00003168
Iteration 85/1000 | Loss: 0.00003168
Iteration 86/1000 | Loss: 0.00003167
Iteration 87/1000 | Loss: 0.00003167
Iteration 88/1000 | Loss: 0.00003167
Iteration 89/1000 | Loss: 0.00003167
Iteration 90/1000 | Loss: 0.00003167
Iteration 91/1000 | Loss: 0.00003167
Iteration 92/1000 | Loss: 0.00003166
Iteration 93/1000 | Loss: 0.00003166
Iteration 94/1000 | Loss: 0.00003166
Iteration 95/1000 | Loss: 0.00003166
Iteration 96/1000 | Loss: 0.00003166
Iteration 97/1000 | Loss: 0.00003166
Iteration 98/1000 | Loss: 0.00003165
Iteration 99/1000 | Loss: 0.00003165
Iteration 100/1000 | Loss: 0.00003165
Iteration 101/1000 | Loss: 0.00003165
Iteration 102/1000 | Loss: 0.00003165
Iteration 103/1000 | Loss: 0.00003165
Iteration 104/1000 | Loss: 0.00003164
Iteration 105/1000 | Loss: 0.00003164
Iteration 106/1000 | Loss: 0.00003164
Iteration 107/1000 | Loss: 0.00003164
Iteration 108/1000 | Loss: 0.00003164
Iteration 109/1000 | Loss: 0.00003164
Iteration 110/1000 | Loss: 0.00003164
Iteration 111/1000 | Loss: 0.00003164
Iteration 112/1000 | Loss: 0.00003164
Iteration 113/1000 | Loss: 0.00003164
Iteration 114/1000 | Loss: 0.00003164
Iteration 115/1000 | Loss: 0.00003164
Iteration 116/1000 | Loss: 0.00003164
Iteration 117/1000 | Loss: 0.00003164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [3.163571818731725e-05, 3.163571818731725e-05, 3.163571818731725e-05, 3.163571818731725e-05, 3.163571818731725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.163571818731725e-05

Optimization complete. Final v2v error: 4.610860824584961 mm

Highest mean error: 5.790206432342529 mm for frame 144

Lowest mean error: 3.8746204376220703 mm for frame 45

Saving results

Total time: 52.98083448410034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443044
Iteration 2/25 | Loss: 0.00138255
Iteration 3/25 | Loss: 0.00125891
Iteration 4/25 | Loss: 0.00124005
Iteration 5/25 | Loss: 0.00123444
Iteration 6/25 | Loss: 0.00123352
Iteration 7/25 | Loss: 0.00123352
Iteration 8/25 | Loss: 0.00123352
Iteration 9/25 | Loss: 0.00123352
Iteration 10/25 | Loss: 0.00123352
Iteration 11/25 | Loss: 0.00123352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012335183564573526, 0.0012335183564573526, 0.0012335183564573526, 0.0012335183564573526, 0.0012335183564573526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012335183564573526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41558528
Iteration 2/25 | Loss: 0.00065348
Iteration 3/25 | Loss: 0.00065348
Iteration 4/25 | Loss: 0.00065348
Iteration 5/25 | Loss: 0.00065348
Iteration 6/25 | Loss: 0.00065348
Iteration 7/25 | Loss: 0.00065348
Iteration 8/25 | Loss: 0.00065348
Iteration 9/25 | Loss: 0.00065347
Iteration 10/25 | Loss: 0.00065347
Iteration 11/25 | Loss: 0.00065347
Iteration 12/25 | Loss: 0.00065347
Iteration 13/25 | Loss: 0.00065347
Iteration 14/25 | Loss: 0.00065347
Iteration 15/25 | Loss: 0.00065347
Iteration 16/25 | Loss: 0.00065347
Iteration 17/25 | Loss: 0.00065347
Iteration 18/25 | Loss: 0.00065347
Iteration 19/25 | Loss: 0.00065347
Iteration 20/25 | Loss: 0.00065347
Iteration 21/25 | Loss: 0.00065347
Iteration 22/25 | Loss: 0.00065347
Iteration 23/25 | Loss: 0.00065347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006534741260111332, 0.0006534741260111332, 0.0006534741260111332, 0.0006534741260111332, 0.0006534741260111332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006534741260111332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065347
Iteration 2/1000 | Loss: 0.00002883
Iteration 3/1000 | Loss: 0.00002280
Iteration 4/1000 | Loss: 0.00002098
Iteration 5/1000 | Loss: 0.00002018
Iteration 6/1000 | Loss: 0.00001948
Iteration 7/1000 | Loss: 0.00001914
Iteration 8/1000 | Loss: 0.00001891
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001805
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001790
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001774
Iteration 19/1000 | Loss: 0.00001772
Iteration 20/1000 | Loss: 0.00001771
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001767
Iteration 23/1000 | Loss: 0.00001766
Iteration 24/1000 | Loss: 0.00001763
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001763
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001762
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001757
Iteration 37/1000 | Loss: 0.00001757
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001753
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001746
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001744
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001743
Iteration 100/1000 | Loss: 0.00001743
Iteration 101/1000 | Loss: 0.00001743
Iteration 102/1000 | Loss: 0.00001743
Iteration 103/1000 | Loss: 0.00001743
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001736
Iteration 124/1000 | Loss: 0.00001736
Iteration 125/1000 | Loss: 0.00001735
Iteration 126/1000 | Loss: 0.00001735
Iteration 127/1000 | Loss: 0.00001735
Iteration 128/1000 | Loss: 0.00001735
Iteration 129/1000 | Loss: 0.00001735
Iteration 130/1000 | Loss: 0.00001735
Iteration 131/1000 | Loss: 0.00001735
Iteration 132/1000 | Loss: 0.00001735
Iteration 133/1000 | Loss: 0.00001735
Iteration 134/1000 | Loss: 0.00001735
Iteration 135/1000 | Loss: 0.00001735
Iteration 136/1000 | Loss: 0.00001735
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001734
Iteration 141/1000 | Loss: 0.00001734
Iteration 142/1000 | Loss: 0.00001733
Iteration 143/1000 | Loss: 0.00001733
Iteration 144/1000 | Loss: 0.00001733
Iteration 145/1000 | Loss: 0.00001733
Iteration 146/1000 | Loss: 0.00001733
Iteration 147/1000 | Loss: 0.00001733
Iteration 148/1000 | Loss: 0.00001733
Iteration 149/1000 | Loss: 0.00001733
Iteration 150/1000 | Loss: 0.00001733
Iteration 151/1000 | Loss: 0.00001733
Iteration 152/1000 | Loss: 0.00001733
Iteration 153/1000 | Loss: 0.00001732
Iteration 154/1000 | Loss: 0.00001732
Iteration 155/1000 | Loss: 0.00001732
Iteration 156/1000 | Loss: 0.00001732
Iteration 157/1000 | Loss: 0.00001731
Iteration 158/1000 | Loss: 0.00001731
Iteration 159/1000 | Loss: 0.00001731
Iteration 160/1000 | Loss: 0.00001731
Iteration 161/1000 | Loss: 0.00001731
Iteration 162/1000 | Loss: 0.00001730
Iteration 163/1000 | Loss: 0.00001730
Iteration 164/1000 | Loss: 0.00001730
Iteration 165/1000 | Loss: 0.00001730
Iteration 166/1000 | Loss: 0.00001730
Iteration 167/1000 | Loss: 0.00001730
Iteration 168/1000 | Loss: 0.00001730
Iteration 169/1000 | Loss: 0.00001730
Iteration 170/1000 | Loss: 0.00001730
Iteration 171/1000 | Loss: 0.00001730
Iteration 172/1000 | Loss: 0.00001730
Iteration 173/1000 | Loss: 0.00001730
Iteration 174/1000 | Loss: 0.00001730
Iteration 175/1000 | Loss: 0.00001730
Iteration 176/1000 | Loss: 0.00001730
Iteration 177/1000 | Loss: 0.00001729
Iteration 178/1000 | Loss: 0.00001729
Iteration 179/1000 | Loss: 0.00001729
Iteration 180/1000 | Loss: 0.00001729
Iteration 181/1000 | Loss: 0.00001729
Iteration 182/1000 | Loss: 0.00001729
Iteration 183/1000 | Loss: 0.00001729
Iteration 184/1000 | Loss: 0.00001729
Iteration 185/1000 | Loss: 0.00001729
Iteration 186/1000 | Loss: 0.00001729
Iteration 187/1000 | Loss: 0.00001729
Iteration 188/1000 | Loss: 0.00001729
Iteration 189/1000 | Loss: 0.00001728
Iteration 190/1000 | Loss: 0.00001728
Iteration 191/1000 | Loss: 0.00001728
Iteration 192/1000 | Loss: 0.00001728
Iteration 193/1000 | Loss: 0.00001728
Iteration 194/1000 | Loss: 0.00001728
Iteration 195/1000 | Loss: 0.00001728
Iteration 196/1000 | Loss: 0.00001728
Iteration 197/1000 | Loss: 0.00001728
Iteration 198/1000 | Loss: 0.00001728
Iteration 199/1000 | Loss: 0.00001728
Iteration 200/1000 | Loss: 0.00001728
Iteration 201/1000 | Loss: 0.00001728
Iteration 202/1000 | Loss: 0.00001728
Iteration 203/1000 | Loss: 0.00001728
Iteration 204/1000 | Loss: 0.00001728
Iteration 205/1000 | Loss: 0.00001728
Iteration 206/1000 | Loss: 0.00001728
Iteration 207/1000 | Loss: 0.00001728
Iteration 208/1000 | Loss: 0.00001728
Iteration 209/1000 | Loss: 0.00001728
Iteration 210/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.7275482605327852e-05, 1.7275482605327852e-05, 1.7275482605327852e-05, 1.7275482605327852e-05, 1.7275482605327852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7275482605327852e-05

Optimization complete. Final v2v error: 3.513437271118164 mm

Highest mean error: 3.751342535018921 mm for frame 178

Lowest mean error: 3.159688949584961 mm for frame 190

Saving results

Total time: 47.19791102409363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989072
Iteration 2/25 | Loss: 0.00330274
Iteration 3/25 | Loss: 0.00248282
Iteration 4/25 | Loss: 0.00215056
Iteration 5/25 | Loss: 0.00238444
Iteration 6/25 | Loss: 0.00216799
Iteration 7/25 | Loss: 0.00211161
Iteration 8/25 | Loss: 0.00197686
Iteration 9/25 | Loss: 0.00184675
Iteration 10/25 | Loss: 0.00178874
Iteration 11/25 | Loss: 0.00176309
Iteration 12/25 | Loss: 0.00175267
Iteration 13/25 | Loss: 0.00171084
Iteration 14/25 | Loss: 0.00169964
Iteration 15/25 | Loss: 0.00168940
Iteration 16/25 | Loss: 0.00168706
Iteration 17/25 | Loss: 0.00171330
Iteration 18/25 | Loss: 0.00171883
Iteration 19/25 | Loss: 0.00169371
Iteration 20/25 | Loss: 0.00166914
Iteration 21/25 | Loss: 0.00167522
Iteration 22/25 | Loss: 0.00167309
Iteration 23/25 | Loss: 0.00167393
Iteration 24/25 | Loss: 0.00167313
Iteration 25/25 | Loss: 0.00166810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45730376
Iteration 2/25 | Loss: 0.00458033
Iteration 3/25 | Loss: 0.00458032
Iteration 4/25 | Loss: 0.00458032
Iteration 5/25 | Loss: 0.00458032
Iteration 6/25 | Loss: 0.00458032
Iteration 7/25 | Loss: 0.00458032
Iteration 8/25 | Loss: 0.00458032
Iteration 9/25 | Loss: 0.00458032
Iteration 10/25 | Loss: 0.00458032
Iteration 11/25 | Loss: 0.00458032
Iteration 12/25 | Loss: 0.00458032
Iteration 13/25 | Loss: 0.00458032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004580323584377766, 0.004580323584377766, 0.004580323584377766, 0.004580323584377766, 0.004580323584377766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004580323584377766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00458032
Iteration 2/1000 | Loss: 0.00088016
Iteration 3/1000 | Loss: 0.00046345
Iteration 4/1000 | Loss: 0.00142293
Iteration 5/1000 | Loss: 0.00491755
Iteration 6/1000 | Loss: 0.00096545
Iteration 7/1000 | Loss: 0.00151628
Iteration 8/1000 | Loss: 0.00143273
Iteration 9/1000 | Loss: 0.00274678
Iteration 10/1000 | Loss: 0.00057577
Iteration 11/1000 | Loss: 0.00084626
Iteration 12/1000 | Loss: 0.00138977
Iteration 13/1000 | Loss: 0.00351934
Iteration 14/1000 | Loss: 0.00557739
Iteration 15/1000 | Loss: 0.00025729
Iteration 16/1000 | Loss: 0.00017753
Iteration 17/1000 | Loss: 0.00014808
Iteration 18/1000 | Loss: 0.00040878
Iteration 19/1000 | Loss: 0.00012856
Iteration 20/1000 | Loss: 0.00273454
Iteration 21/1000 | Loss: 0.00030197
Iteration 22/1000 | Loss: 0.00060080
Iteration 23/1000 | Loss: 0.00011905
Iteration 24/1000 | Loss: 0.00024725
Iteration 25/1000 | Loss: 0.00114670
Iteration 26/1000 | Loss: 0.00044319
Iteration 27/1000 | Loss: 0.00082567
Iteration 28/1000 | Loss: 0.00066005
Iteration 29/1000 | Loss: 0.00110822
Iteration 30/1000 | Loss: 0.00049471
Iteration 31/1000 | Loss: 0.00052421
Iteration 32/1000 | Loss: 0.00035373
Iteration 33/1000 | Loss: 0.00061501
Iteration 34/1000 | Loss: 0.00009045
Iteration 35/1000 | Loss: 0.00022631
Iteration 36/1000 | Loss: 0.00008411
Iteration 37/1000 | Loss: 0.00008071
Iteration 38/1000 | Loss: 0.00007778
Iteration 39/1000 | Loss: 0.00007530
Iteration 40/1000 | Loss: 0.00034265
Iteration 41/1000 | Loss: 0.00007405
Iteration 42/1000 | Loss: 0.00007011
Iteration 43/1000 | Loss: 0.00112134
Iteration 44/1000 | Loss: 0.00088860
Iteration 45/1000 | Loss: 0.00129624
Iteration 46/1000 | Loss: 0.00115033
Iteration 47/1000 | Loss: 0.00032097
Iteration 48/1000 | Loss: 0.00007377
Iteration 49/1000 | Loss: 0.00006612
Iteration 50/1000 | Loss: 0.00006236
Iteration 51/1000 | Loss: 0.00005937
Iteration 52/1000 | Loss: 0.00005735
Iteration 53/1000 | Loss: 0.00005607
Iteration 54/1000 | Loss: 0.00005527
Iteration 55/1000 | Loss: 0.00015982
Iteration 56/1000 | Loss: 0.00061978
Iteration 57/1000 | Loss: 0.00007268
Iteration 58/1000 | Loss: 0.00005808
Iteration 59/1000 | Loss: 0.00005378
Iteration 60/1000 | Loss: 0.00005214
Iteration 61/1000 | Loss: 0.00005085
Iteration 62/1000 | Loss: 0.00004991
Iteration 63/1000 | Loss: 0.00004938
Iteration 64/1000 | Loss: 0.00004893
Iteration 65/1000 | Loss: 0.00004851
Iteration 66/1000 | Loss: 0.00019344
Iteration 67/1000 | Loss: 0.00005506
Iteration 68/1000 | Loss: 0.00005089
Iteration 69/1000 | Loss: 0.00024643
Iteration 70/1000 | Loss: 0.00006248
Iteration 71/1000 | Loss: 0.00005283
Iteration 72/1000 | Loss: 0.00005022
Iteration 73/1000 | Loss: 0.00004926
Iteration 74/1000 | Loss: 0.00004794
Iteration 75/1000 | Loss: 0.00004730
Iteration 76/1000 | Loss: 0.00004692
Iteration 77/1000 | Loss: 0.00004665
Iteration 78/1000 | Loss: 0.00004630
Iteration 79/1000 | Loss: 0.00021890
Iteration 80/1000 | Loss: 0.00027578
Iteration 81/1000 | Loss: 0.00004783
Iteration 82/1000 | Loss: 0.00004472
Iteration 83/1000 | Loss: 0.00004355
Iteration 84/1000 | Loss: 0.00004307
Iteration 85/1000 | Loss: 0.00004247
Iteration 86/1000 | Loss: 0.00004224
Iteration 87/1000 | Loss: 0.00004219
Iteration 88/1000 | Loss: 0.00004217
Iteration 89/1000 | Loss: 0.00004206
Iteration 90/1000 | Loss: 0.00004195
Iteration 91/1000 | Loss: 0.00004193
Iteration 92/1000 | Loss: 0.00004183
Iteration 93/1000 | Loss: 0.00004167
Iteration 94/1000 | Loss: 0.00004162
Iteration 95/1000 | Loss: 0.00004162
Iteration 96/1000 | Loss: 0.00004162
Iteration 97/1000 | Loss: 0.00004160
Iteration 98/1000 | Loss: 0.00004157
Iteration 99/1000 | Loss: 0.00004157
Iteration 100/1000 | Loss: 0.00004157
Iteration 101/1000 | Loss: 0.00004154
Iteration 102/1000 | Loss: 0.00004153
Iteration 103/1000 | Loss: 0.00004153
Iteration 104/1000 | Loss: 0.00004153
Iteration 105/1000 | Loss: 0.00004152
Iteration 106/1000 | Loss: 0.00004152
Iteration 107/1000 | Loss: 0.00004152
Iteration 108/1000 | Loss: 0.00004151
Iteration 109/1000 | Loss: 0.00004151
Iteration 110/1000 | Loss: 0.00004151
Iteration 111/1000 | Loss: 0.00004151
Iteration 112/1000 | Loss: 0.00004151
Iteration 113/1000 | Loss: 0.00004151
Iteration 114/1000 | Loss: 0.00004150
Iteration 115/1000 | Loss: 0.00004150
Iteration 116/1000 | Loss: 0.00004150
Iteration 117/1000 | Loss: 0.00004150
Iteration 118/1000 | Loss: 0.00004150
Iteration 119/1000 | Loss: 0.00004149
Iteration 120/1000 | Loss: 0.00004149
Iteration 121/1000 | Loss: 0.00004149
Iteration 122/1000 | Loss: 0.00004148
Iteration 123/1000 | Loss: 0.00004148
Iteration 124/1000 | Loss: 0.00004148
Iteration 125/1000 | Loss: 0.00004148
Iteration 126/1000 | Loss: 0.00004148
Iteration 127/1000 | Loss: 0.00004147
Iteration 128/1000 | Loss: 0.00004147
Iteration 129/1000 | Loss: 0.00004147
Iteration 130/1000 | Loss: 0.00004147
Iteration 131/1000 | Loss: 0.00004146
Iteration 132/1000 | Loss: 0.00004146
Iteration 133/1000 | Loss: 0.00004146
Iteration 134/1000 | Loss: 0.00004146
Iteration 135/1000 | Loss: 0.00004146
Iteration 136/1000 | Loss: 0.00004146
Iteration 137/1000 | Loss: 0.00004146
Iteration 138/1000 | Loss: 0.00004146
Iteration 139/1000 | Loss: 0.00004146
Iteration 140/1000 | Loss: 0.00004146
Iteration 141/1000 | Loss: 0.00004145
Iteration 142/1000 | Loss: 0.00004145
Iteration 143/1000 | Loss: 0.00004145
Iteration 144/1000 | Loss: 0.00004145
Iteration 145/1000 | Loss: 0.00004145
Iteration 146/1000 | Loss: 0.00004144
Iteration 147/1000 | Loss: 0.00004144
Iteration 148/1000 | Loss: 0.00004144
Iteration 149/1000 | Loss: 0.00004143
Iteration 150/1000 | Loss: 0.00004143
Iteration 151/1000 | Loss: 0.00004143
Iteration 152/1000 | Loss: 0.00004142
Iteration 153/1000 | Loss: 0.00004142
Iteration 154/1000 | Loss: 0.00004142
Iteration 155/1000 | Loss: 0.00004142
Iteration 156/1000 | Loss: 0.00004141
Iteration 157/1000 | Loss: 0.00004141
Iteration 158/1000 | Loss: 0.00004141
Iteration 159/1000 | Loss: 0.00004141
Iteration 160/1000 | Loss: 0.00004141
Iteration 161/1000 | Loss: 0.00004141
Iteration 162/1000 | Loss: 0.00004141
Iteration 163/1000 | Loss: 0.00004141
Iteration 164/1000 | Loss: 0.00004141
Iteration 165/1000 | Loss: 0.00004141
Iteration 166/1000 | Loss: 0.00004141
Iteration 167/1000 | Loss: 0.00004140
Iteration 168/1000 | Loss: 0.00004140
Iteration 169/1000 | Loss: 0.00004140
Iteration 170/1000 | Loss: 0.00004139
Iteration 171/1000 | Loss: 0.00004139
Iteration 172/1000 | Loss: 0.00004139
Iteration 173/1000 | Loss: 0.00004139
Iteration 174/1000 | Loss: 0.00004139
Iteration 175/1000 | Loss: 0.00004139
Iteration 176/1000 | Loss: 0.00004139
Iteration 177/1000 | Loss: 0.00004139
Iteration 178/1000 | Loss: 0.00004139
Iteration 179/1000 | Loss: 0.00004139
Iteration 180/1000 | Loss: 0.00004139
Iteration 181/1000 | Loss: 0.00004139
Iteration 182/1000 | Loss: 0.00004139
Iteration 183/1000 | Loss: 0.00004138
Iteration 184/1000 | Loss: 0.00004138
Iteration 185/1000 | Loss: 0.00004138
Iteration 186/1000 | Loss: 0.00004138
Iteration 187/1000 | Loss: 0.00004138
Iteration 188/1000 | Loss: 0.00004138
Iteration 189/1000 | Loss: 0.00004138
Iteration 190/1000 | Loss: 0.00004138
Iteration 191/1000 | Loss: 0.00004138
Iteration 192/1000 | Loss: 0.00004138
Iteration 193/1000 | Loss: 0.00004138
Iteration 194/1000 | Loss: 0.00004138
Iteration 195/1000 | Loss: 0.00004138
Iteration 196/1000 | Loss: 0.00004138
Iteration 197/1000 | Loss: 0.00004137
Iteration 198/1000 | Loss: 0.00004137
Iteration 199/1000 | Loss: 0.00004137
Iteration 200/1000 | Loss: 0.00004137
Iteration 201/1000 | Loss: 0.00004137
Iteration 202/1000 | Loss: 0.00004137
Iteration 203/1000 | Loss: 0.00004137
Iteration 204/1000 | Loss: 0.00004137
Iteration 205/1000 | Loss: 0.00004136
Iteration 206/1000 | Loss: 0.00004136
Iteration 207/1000 | Loss: 0.00004136
Iteration 208/1000 | Loss: 0.00004136
Iteration 209/1000 | Loss: 0.00004136
Iteration 210/1000 | Loss: 0.00004136
Iteration 211/1000 | Loss: 0.00004136
Iteration 212/1000 | Loss: 0.00004136
Iteration 213/1000 | Loss: 0.00004136
Iteration 214/1000 | Loss: 0.00004136
Iteration 215/1000 | Loss: 0.00004136
Iteration 216/1000 | Loss: 0.00004136
Iteration 217/1000 | Loss: 0.00004136
Iteration 218/1000 | Loss: 0.00004136
Iteration 219/1000 | Loss: 0.00004136
Iteration 220/1000 | Loss: 0.00004136
Iteration 221/1000 | Loss: 0.00004136
Iteration 222/1000 | Loss: 0.00004136
Iteration 223/1000 | Loss: 0.00004136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [4.135656126891263e-05, 4.135656126891263e-05, 4.135656126891263e-05, 4.135656126891263e-05, 4.135656126891263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.135656126891263e-05

Optimization complete. Final v2v error: 4.0909810066223145 mm

Highest mean error: 12.910496711730957 mm for frame 48

Lowest mean error: 3.0453715324401855 mm for frame 200

Saving results

Total time: 189.7165596485138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078007
Iteration 2/25 | Loss: 0.01078007
Iteration 3/25 | Loss: 0.01078007
Iteration 4/25 | Loss: 0.01078007
Iteration 5/25 | Loss: 0.01078006
Iteration 6/25 | Loss: 0.01078006
Iteration 7/25 | Loss: 0.00393447
Iteration 8/25 | Loss: 0.00304870
Iteration 9/25 | Loss: 0.00281726
Iteration 10/25 | Loss: 0.00247315
Iteration 11/25 | Loss: 0.00237810
Iteration 12/25 | Loss: 0.00216808
Iteration 13/25 | Loss: 0.00205090
Iteration 14/25 | Loss: 0.00195284
Iteration 15/25 | Loss: 0.00186163
Iteration 16/25 | Loss: 0.00180393
Iteration 17/25 | Loss: 0.00176152
Iteration 18/25 | Loss: 0.00171584
Iteration 19/25 | Loss: 0.00167789
Iteration 20/25 | Loss: 0.00164792
Iteration 21/25 | Loss: 0.00163298
Iteration 22/25 | Loss: 0.00161589
Iteration 23/25 | Loss: 0.00159119
Iteration 24/25 | Loss: 0.00158301
Iteration 25/25 | Loss: 0.00157589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23197615
Iteration 2/25 | Loss: 0.00227179
Iteration 3/25 | Loss: 0.00227179
Iteration 4/25 | Loss: 0.00227179
Iteration 5/25 | Loss: 0.00227179
Iteration 6/25 | Loss: 0.00227179
Iteration 7/25 | Loss: 0.00227179
Iteration 8/25 | Loss: 0.00227179
Iteration 9/25 | Loss: 0.00227179
Iteration 10/25 | Loss: 0.00227179
Iteration 11/25 | Loss: 0.00227179
Iteration 12/25 | Loss: 0.00227179
Iteration 13/25 | Loss: 0.00227179
Iteration 14/25 | Loss: 0.00227179
Iteration 15/25 | Loss: 0.00227179
Iteration 16/25 | Loss: 0.00227179
Iteration 17/25 | Loss: 0.00227179
Iteration 18/25 | Loss: 0.00227179
Iteration 19/25 | Loss: 0.00227179
Iteration 20/25 | Loss: 0.00227179
Iteration 21/25 | Loss: 0.00227178
Iteration 22/25 | Loss: 0.00227178
Iteration 23/25 | Loss: 0.00227178
Iteration 24/25 | Loss: 0.00227178
Iteration 25/25 | Loss: 0.00227178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227178
Iteration 2/1000 | Loss: 0.00055946
Iteration 3/1000 | Loss: 0.00031040
Iteration 4/1000 | Loss: 0.00024997
Iteration 5/1000 | Loss: 0.00045590
Iteration 6/1000 | Loss: 0.00047786
Iteration 7/1000 | Loss: 0.00029529
Iteration 8/1000 | Loss: 0.00035490
Iteration 9/1000 | Loss: 0.00048575
Iteration 10/1000 | Loss: 0.00050563
Iteration 11/1000 | Loss: 0.00032913
Iteration 12/1000 | Loss: 0.00071469
Iteration 13/1000 | Loss: 0.00059411
Iteration 14/1000 | Loss: 0.00075623
Iteration 15/1000 | Loss: 0.00047652
Iteration 16/1000 | Loss: 0.00036713
Iteration 17/1000 | Loss: 0.00039319
Iteration 18/1000 | Loss: 0.00041216
Iteration 19/1000 | Loss: 0.00027916
Iteration 20/1000 | Loss: 0.00034945
Iteration 21/1000 | Loss: 0.00053706
Iteration 22/1000 | Loss: 0.00058826
Iteration 23/1000 | Loss: 0.00095759
Iteration 24/1000 | Loss: 0.00041385
Iteration 25/1000 | Loss: 0.00020172
Iteration 26/1000 | Loss: 0.00017411
Iteration 27/1000 | Loss: 0.00032116
Iteration 28/1000 | Loss: 0.00028590
Iteration 29/1000 | Loss: 0.00040157
Iteration 30/1000 | Loss: 0.00021024
Iteration 31/1000 | Loss: 0.00022028
Iteration 32/1000 | Loss: 0.00015042
Iteration 33/1000 | Loss: 0.00022084
Iteration 34/1000 | Loss: 0.00036534
Iteration 35/1000 | Loss: 0.00032892
Iteration 36/1000 | Loss: 0.00043898
Iteration 37/1000 | Loss: 0.00042712
Iteration 38/1000 | Loss: 0.00030366
Iteration 39/1000 | Loss: 0.00015886
Iteration 40/1000 | Loss: 0.00016330
Iteration 41/1000 | Loss: 0.00071787
Iteration 42/1000 | Loss: 0.00014157
Iteration 43/1000 | Loss: 0.00012870
Iteration 44/1000 | Loss: 0.00013570
Iteration 45/1000 | Loss: 0.00028478
Iteration 46/1000 | Loss: 0.00024446
Iteration 47/1000 | Loss: 0.00028778
Iteration 48/1000 | Loss: 0.00026652
Iteration 49/1000 | Loss: 0.00022211
Iteration 50/1000 | Loss: 0.00024714
Iteration 51/1000 | Loss: 0.00055556
Iteration 52/1000 | Loss: 0.00080740
Iteration 53/1000 | Loss: 0.00081689
Iteration 54/1000 | Loss: 0.00067332
Iteration 55/1000 | Loss: 0.00065144
Iteration 56/1000 | Loss: 0.00016651
Iteration 57/1000 | Loss: 0.00017343
Iteration 58/1000 | Loss: 0.00099386
Iteration 59/1000 | Loss: 0.00041042
Iteration 60/1000 | Loss: 0.00059959
Iteration 61/1000 | Loss: 0.00033068
Iteration 62/1000 | Loss: 0.00154120
Iteration 63/1000 | Loss: 0.00206420
Iteration 64/1000 | Loss: 0.00039596
Iteration 65/1000 | Loss: 0.00042644
Iteration 66/1000 | Loss: 0.00054329
Iteration 67/1000 | Loss: 0.00029068
Iteration 68/1000 | Loss: 0.00059977
Iteration 69/1000 | Loss: 0.00089978
Iteration 70/1000 | Loss: 0.00018665
Iteration 71/1000 | Loss: 0.00076009
Iteration 72/1000 | Loss: 0.00010929
Iteration 73/1000 | Loss: 0.00037239
Iteration 74/1000 | Loss: 0.00024536
Iteration 75/1000 | Loss: 0.00039394
Iteration 76/1000 | Loss: 0.00015111
Iteration 77/1000 | Loss: 0.00023487
Iteration 78/1000 | Loss: 0.00009172
Iteration 79/1000 | Loss: 0.00009503
Iteration 80/1000 | Loss: 0.00024842
Iteration 81/1000 | Loss: 0.00009003
Iteration 82/1000 | Loss: 0.00021176
Iteration 83/1000 | Loss: 0.00007786
Iteration 84/1000 | Loss: 0.00008796
Iteration 85/1000 | Loss: 0.00024528
Iteration 86/1000 | Loss: 0.00009768
Iteration 87/1000 | Loss: 0.00009717
Iteration 88/1000 | Loss: 0.00026769
Iteration 89/1000 | Loss: 0.00009637
Iteration 90/1000 | Loss: 0.00008576
Iteration 91/1000 | Loss: 0.00008977
Iteration 92/1000 | Loss: 0.00009238
Iteration 93/1000 | Loss: 0.00053095
Iteration 94/1000 | Loss: 0.00023064
Iteration 95/1000 | Loss: 0.00017711
Iteration 96/1000 | Loss: 0.00028952
Iteration 97/1000 | Loss: 0.00048166
Iteration 98/1000 | Loss: 0.00024660
Iteration 99/1000 | Loss: 0.00023192
Iteration 100/1000 | Loss: 0.00011958
Iteration 101/1000 | Loss: 0.00008649
Iteration 102/1000 | Loss: 0.00019102
Iteration 103/1000 | Loss: 0.00028071
Iteration 104/1000 | Loss: 0.00031128
Iteration 105/1000 | Loss: 0.00019249
Iteration 106/1000 | Loss: 0.00021501
Iteration 107/1000 | Loss: 0.00024033
Iteration 108/1000 | Loss: 0.00009981
Iteration 109/1000 | Loss: 0.00035360
Iteration 110/1000 | Loss: 0.00012507
Iteration 111/1000 | Loss: 0.00011250
Iteration 112/1000 | Loss: 0.00011595
Iteration 113/1000 | Loss: 0.00031704
Iteration 114/1000 | Loss: 0.00030323
Iteration 115/1000 | Loss: 0.00022305
Iteration 116/1000 | Loss: 0.00025949
Iteration 117/1000 | Loss: 0.00019642
Iteration 118/1000 | Loss: 0.00049119
Iteration 119/1000 | Loss: 0.00021846
Iteration 120/1000 | Loss: 0.00010453
Iteration 121/1000 | Loss: 0.00009954
Iteration 122/1000 | Loss: 0.00024273
Iteration 123/1000 | Loss: 0.00019075
Iteration 124/1000 | Loss: 0.00018200
Iteration 125/1000 | Loss: 0.00009275
Iteration 126/1000 | Loss: 0.00010499
Iteration 127/1000 | Loss: 0.00037316
Iteration 128/1000 | Loss: 0.00019423
Iteration 129/1000 | Loss: 0.00010060
Iteration 130/1000 | Loss: 0.00042305
Iteration 131/1000 | Loss: 0.00009249
Iteration 132/1000 | Loss: 0.00007605
Iteration 133/1000 | Loss: 0.00008219
Iteration 134/1000 | Loss: 0.00024511
Iteration 135/1000 | Loss: 0.00018841
Iteration 136/1000 | Loss: 0.00032686
Iteration 137/1000 | Loss: 0.00038987
Iteration 138/1000 | Loss: 0.00022851
Iteration 139/1000 | Loss: 0.00009580
Iteration 140/1000 | Loss: 0.00021990
Iteration 141/1000 | Loss: 0.00020935
Iteration 142/1000 | Loss: 0.00010889
Iteration 143/1000 | Loss: 0.00008069
Iteration 144/1000 | Loss: 0.00007451
Iteration 145/1000 | Loss: 0.00009150
Iteration 146/1000 | Loss: 0.00008900
Iteration 147/1000 | Loss: 0.00008796
Iteration 148/1000 | Loss: 0.00051093
Iteration 149/1000 | Loss: 0.00038865
Iteration 150/1000 | Loss: 0.00061139
Iteration 151/1000 | Loss: 0.00036429
Iteration 152/1000 | Loss: 0.00049625
Iteration 153/1000 | Loss: 0.00019571
Iteration 154/1000 | Loss: 0.00014523
Iteration 155/1000 | Loss: 0.00006258
Iteration 156/1000 | Loss: 0.00008684
Iteration 157/1000 | Loss: 0.00008197
Iteration 158/1000 | Loss: 0.00009618
Iteration 159/1000 | Loss: 0.00008904
Iteration 160/1000 | Loss: 0.00007892
Iteration 161/1000 | Loss: 0.00007195
Iteration 162/1000 | Loss: 0.00005888
Iteration 163/1000 | Loss: 0.00007735
Iteration 164/1000 | Loss: 0.00008171
Iteration 165/1000 | Loss: 0.00008426
Iteration 166/1000 | Loss: 0.00011015
Iteration 167/1000 | Loss: 0.00009049
Iteration 168/1000 | Loss: 0.00007792
Iteration 169/1000 | Loss: 0.00006993
Iteration 170/1000 | Loss: 0.00027045
Iteration 171/1000 | Loss: 0.00022537
Iteration 172/1000 | Loss: 0.00008090
Iteration 173/1000 | Loss: 0.00008715
Iteration 174/1000 | Loss: 0.00007209
Iteration 175/1000 | Loss: 0.00007902
Iteration 176/1000 | Loss: 0.00007766
Iteration 177/1000 | Loss: 0.00008246
Iteration 178/1000 | Loss: 0.00008325
Iteration 179/1000 | Loss: 0.00027675
Iteration 180/1000 | Loss: 0.00022788
Iteration 181/1000 | Loss: 0.00008581
Iteration 182/1000 | Loss: 0.00023315
Iteration 183/1000 | Loss: 0.00020322
Iteration 184/1000 | Loss: 0.00013660
Iteration 185/1000 | Loss: 0.00011226
Iteration 186/1000 | Loss: 0.00011517
Iteration 187/1000 | Loss: 0.00011475
Iteration 188/1000 | Loss: 0.00008151
Iteration 189/1000 | Loss: 0.00006130
Iteration 190/1000 | Loss: 0.00005297
Iteration 191/1000 | Loss: 0.00008140
Iteration 192/1000 | Loss: 0.00007550
Iteration 193/1000 | Loss: 0.00007474
Iteration 194/1000 | Loss: 0.00007152
Iteration 195/1000 | Loss: 0.00006062
Iteration 196/1000 | Loss: 0.00006956
Iteration 197/1000 | Loss: 0.00007514
Iteration 198/1000 | Loss: 0.00007710
Iteration 199/1000 | Loss: 0.00025671
Iteration 200/1000 | Loss: 0.00012998
Iteration 201/1000 | Loss: 0.00022926
Iteration 202/1000 | Loss: 0.00016937
Iteration 203/1000 | Loss: 0.00010682
Iteration 204/1000 | Loss: 0.00007188
Iteration 205/1000 | Loss: 0.00006324
Iteration 206/1000 | Loss: 0.00006145
Iteration 207/1000 | Loss: 0.00008185
Iteration 208/1000 | Loss: 0.00007921
Iteration 209/1000 | Loss: 0.00008463
Iteration 210/1000 | Loss: 0.00008404
Iteration 211/1000 | Loss: 0.00019243
Iteration 212/1000 | Loss: 0.00077101
Iteration 213/1000 | Loss: 0.00049237
Iteration 214/1000 | Loss: 0.00019387
Iteration 215/1000 | Loss: 0.00008630
Iteration 216/1000 | Loss: 0.00019468
Iteration 217/1000 | Loss: 0.00035879
Iteration 218/1000 | Loss: 0.00009569
Iteration 219/1000 | Loss: 0.00008308
Iteration 220/1000 | Loss: 0.00008276
Iteration 221/1000 | Loss: 0.00008773
Iteration 222/1000 | Loss: 0.00006669
Iteration 223/1000 | Loss: 0.00006023
Iteration 224/1000 | Loss: 0.00006757
Iteration 225/1000 | Loss: 0.00007205
Iteration 226/1000 | Loss: 0.00009935
Iteration 227/1000 | Loss: 0.00007582
Iteration 228/1000 | Loss: 0.00018328
Iteration 229/1000 | Loss: 0.00018772
Iteration 230/1000 | Loss: 0.00007855
Iteration 231/1000 | Loss: 0.00018289
Iteration 232/1000 | Loss: 0.00007033
Iteration 233/1000 | Loss: 0.00008014
Iteration 234/1000 | Loss: 0.00007694
Iteration 235/1000 | Loss: 0.00008678
Iteration 236/1000 | Loss: 0.00008698
Iteration 237/1000 | Loss: 0.00008342
Iteration 238/1000 | Loss: 0.00008008
Iteration 239/1000 | Loss: 0.00007866
Iteration 240/1000 | Loss: 0.00007549
Iteration 241/1000 | Loss: 0.00020594
Iteration 242/1000 | Loss: 0.00007228
Iteration 243/1000 | Loss: 0.00017206
Iteration 244/1000 | Loss: 0.00019906
Iteration 245/1000 | Loss: 0.00018014
Iteration 246/1000 | Loss: 0.00013224
Iteration 247/1000 | Loss: 0.00013959
Iteration 248/1000 | Loss: 0.00007090
Iteration 249/1000 | Loss: 0.00016088
Iteration 250/1000 | Loss: 0.00007919
Iteration 251/1000 | Loss: 0.00008343
Iteration 252/1000 | Loss: 0.00024226
Iteration 253/1000 | Loss: 0.00018476
Iteration 254/1000 | Loss: 0.00020824
Iteration 255/1000 | Loss: 0.00014279
Iteration 256/1000 | Loss: 0.00007802
Iteration 257/1000 | Loss: 0.00007824
Iteration 258/1000 | Loss: 0.00005571
Iteration 259/1000 | Loss: 0.00006954
Iteration 260/1000 | Loss: 0.00005909
Iteration 261/1000 | Loss: 0.00005928
Iteration 262/1000 | Loss: 0.00006439
Iteration 263/1000 | Loss: 0.00006094
Iteration 264/1000 | Loss: 0.00005160
Iteration 265/1000 | Loss: 0.00006444
Iteration 266/1000 | Loss: 0.00007063
Iteration 267/1000 | Loss: 0.00005968
Iteration 268/1000 | Loss: 0.00006856
Iteration 269/1000 | Loss: 0.00006274
Iteration 270/1000 | Loss: 0.00005558
Iteration 271/1000 | Loss: 0.00007591
Iteration 272/1000 | Loss: 0.00007029
Iteration 273/1000 | Loss: 0.00006022
Iteration 274/1000 | Loss: 0.00005407
Iteration 275/1000 | Loss: 0.00005758
Iteration 276/1000 | Loss: 0.00006058
Iteration 277/1000 | Loss: 0.00006015
Iteration 278/1000 | Loss: 0.00005187
Iteration 279/1000 | Loss: 0.00005813
Iteration 280/1000 | Loss: 0.00005872
Iteration 281/1000 | Loss: 0.00005761
Iteration 282/1000 | Loss: 0.00006352
Iteration 283/1000 | Loss: 0.00005781
Iteration 284/1000 | Loss: 0.00006292
Iteration 285/1000 | Loss: 0.00006530
Iteration 286/1000 | Loss: 0.00025808
Iteration 287/1000 | Loss: 0.00010517
Iteration 288/1000 | Loss: 0.00005326
Iteration 289/1000 | Loss: 0.00006295
Iteration 290/1000 | Loss: 0.00025722
Iteration 291/1000 | Loss: 0.00007230
Iteration 292/1000 | Loss: 0.00006017
Iteration 293/1000 | Loss: 0.00006020
Iteration 294/1000 | Loss: 0.00007094
Iteration 295/1000 | Loss: 0.00006228
Iteration 296/1000 | Loss: 0.00006891
Iteration 297/1000 | Loss: 0.00006876
Iteration 298/1000 | Loss: 0.00006283
Iteration 299/1000 | Loss: 0.00006823
Iteration 300/1000 | Loss: 0.00006790
Iteration 301/1000 | Loss: 0.00006582
Iteration 302/1000 | Loss: 0.00007915
Iteration 303/1000 | Loss: 0.00006274
Iteration 304/1000 | Loss: 0.00006515
Iteration 305/1000 | Loss: 0.00007227
Iteration 306/1000 | Loss: 0.00005907
Iteration 307/1000 | Loss: 0.00006007
Iteration 308/1000 | Loss: 0.00005495
Iteration 309/1000 | Loss: 0.00005150
Iteration 310/1000 | Loss: 0.00006549
Iteration 311/1000 | Loss: 0.00005972
Iteration 312/1000 | Loss: 0.00006133
Iteration 313/1000 | Loss: 0.00005943
Iteration 314/1000 | Loss: 0.00005762
Iteration 315/1000 | Loss: 0.00005672
Iteration 316/1000 | Loss: 0.00007670
Iteration 317/1000 | Loss: 0.00006442
Iteration 318/1000 | Loss: 0.00005148
Iteration 319/1000 | Loss: 0.00005201
Iteration 320/1000 | Loss: 0.00005796
Iteration 321/1000 | Loss: 0.00006004
Iteration 322/1000 | Loss: 0.00005975
Iteration 323/1000 | Loss: 0.00006737
Iteration 324/1000 | Loss: 0.00006537
Iteration 325/1000 | Loss: 0.00005991
Iteration 326/1000 | Loss: 0.00005892
Iteration 327/1000 | Loss: 0.00005064
Iteration 328/1000 | Loss: 0.00005672
Iteration 329/1000 | Loss: 0.00005971
Iteration 330/1000 | Loss: 0.00006318
Iteration 331/1000 | Loss: 0.00006698
Iteration 332/1000 | Loss: 0.00006065
Iteration 333/1000 | Loss: 0.00005828
Iteration 334/1000 | Loss: 0.00006802
Iteration 335/1000 | Loss: 0.00006219
Iteration 336/1000 | Loss: 0.00006396
Iteration 337/1000 | Loss: 0.00006617
Iteration 338/1000 | Loss: 0.00008042
Iteration 339/1000 | Loss: 0.00005577
Iteration 340/1000 | Loss: 0.00005137
Iteration 341/1000 | Loss: 0.00004917
Iteration 342/1000 | Loss: 0.00004769
Iteration 343/1000 | Loss: 0.00004662
Iteration 344/1000 | Loss: 0.00004970
Iteration 345/1000 | Loss: 0.00004539
Iteration 346/1000 | Loss: 0.00004358
Iteration 347/1000 | Loss: 0.00004304
Iteration 348/1000 | Loss: 0.00004277
Iteration 349/1000 | Loss: 0.00004256
Iteration 350/1000 | Loss: 0.00004255
Iteration 351/1000 | Loss: 0.00004252
Iteration 352/1000 | Loss: 0.00004250
Iteration 353/1000 | Loss: 0.00004250
Iteration 354/1000 | Loss: 0.00004243
Iteration 355/1000 | Loss: 0.00004237
Iteration 356/1000 | Loss: 0.00004236
Iteration 357/1000 | Loss: 0.00004236
Iteration 358/1000 | Loss: 0.00004235
Iteration 359/1000 | Loss: 0.00004234
Iteration 360/1000 | Loss: 0.00004230
Iteration 361/1000 | Loss: 0.00004228
Iteration 362/1000 | Loss: 0.00004227
Iteration 363/1000 | Loss: 0.00004227
Iteration 364/1000 | Loss: 0.00004227
Iteration 365/1000 | Loss: 0.00004226
Iteration 366/1000 | Loss: 0.00004226
Iteration 367/1000 | Loss: 0.00004226
Iteration 368/1000 | Loss: 0.00004226
Iteration 369/1000 | Loss: 0.00004226
Iteration 370/1000 | Loss: 0.00004226
Iteration 371/1000 | Loss: 0.00004226
Iteration 372/1000 | Loss: 0.00004226
Iteration 373/1000 | Loss: 0.00004226
Iteration 374/1000 | Loss: 0.00004226
Iteration 375/1000 | Loss: 0.00004225
Iteration 376/1000 | Loss: 0.00004225
Iteration 377/1000 | Loss: 0.00004225
Iteration 378/1000 | Loss: 0.00004225
Iteration 379/1000 | Loss: 0.00004225
Iteration 380/1000 | Loss: 0.00004225
Iteration 381/1000 | Loss: 0.00004225
Iteration 382/1000 | Loss: 0.00004225
Iteration 383/1000 | Loss: 0.00004224
Iteration 384/1000 | Loss: 0.00004224
Iteration 385/1000 | Loss: 0.00004224
Iteration 386/1000 | Loss: 0.00004224
Iteration 387/1000 | Loss: 0.00004224
Iteration 388/1000 | Loss: 0.00004224
Iteration 389/1000 | Loss: 0.00004224
Iteration 390/1000 | Loss: 0.00004224
Iteration 391/1000 | Loss: 0.00004223
Iteration 392/1000 | Loss: 0.00004223
Iteration 393/1000 | Loss: 0.00004223
Iteration 394/1000 | Loss: 0.00004223
Iteration 395/1000 | Loss: 0.00004223
Iteration 396/1000 | Loss: 0.00004222
Iteration 397/1000 | Loss: 0.00004222
Iteration 398/1000 | Loss: 0.00004222
Iteration 399/1000 | Loss: 0.00004222
Iteration 400/1000 | Loss: 0.00004222
Iteration 401/1000 | Loss: 0.00004222
Iteration 402/1000 | Loss: 0.00004222
Iteration 403/1000 | Loss: 0.00004222
Iteration 404/1000 | Loss: 0.00004221
Iteration 405/1000 | Loss: 0.00004221
Iteration 406/1000 | Loss: 0.00004221
Iteration 407/1000 | Loss: 0.00004221
Iteration 408/1000 | Loss: 0.00004221
Iteration 409/1000 | Loss: 0.00004221
Iteration 410/1000 | Loss: 0.00004221
Iteration 411/1000 | Loss: 0.00004221
Iteration 412/1000 | Loss: 0.00004221
Iteration 413/1000 | Loss: 0.00004221
Iteration 414/1000 | Loss: 0.00004221
Iteration 415/1000 | Loss: 0.00004221
Iteration 416/1000 | Loss: 0.00004221
Iteration 417/1000 | Loss: 0.00004221
Iteration 418/1000 | Loss: 0.00004220
Iteration 419/1000 | Loss: 0.00004220
Iteration 420/1000 | Loss: 0.00004220
Iteration 421/1000 | Loss: 0.00004220
Iteration 422/1000 | Loss: 0.00004220
Iteration 423/1000 | Loss: 0.00004220
Iteration 424/1000 | Loss: 0.00004220
Iteration 425/1000 | Loss: 0.00004220
Iteration 426/1000 | Loss: 0.00004220
Iteration 427/1000 | Loss: 0.00004220
Iteration 428/1000 | Loss: 0.00004220
Iteration 429/1000 | Loss: 0.00004220
Iteration 430/1000 | Loss: 0.00004220
Iteration 431/1000 | Loss: 0.00004220
Iteration 432/1000 | Loss: 0.00004220
Iteration 433/1000 | Loss: 0.00004220
Iteration 434/1000 | Loss: 0.00004219
Iteration 435/1000 | Loss: 0.00004219
Iteration 436/1000 | Loss: 0.00004219
Iteration 437/1000 | Loss: 0.00004219
Iteration 438/1000 | Loss: 0.00004219
Iteration 439/1000 | Loss: 0.00004219
Iteration 440/1000 | Loss: 0.00004219
Iteration 441/1000 | Loss: 0.00004219
Iteration 442/1000 | Loss: 0.00004219
Iteration 443/1000 | Loss: 0.00004219
Iteration 444/1000 | Loss: 0.00004219
Iteration 445/1000 | Loss: 0.00004219
Iteration 446/1000 | Loss: 0.00004218
Iteration 447/1000 | Loss: 0.00004218
Iteration 448/1000 | Loss: 0.00004218
Iteration 449/1000 | Loss: 0.00004218
Iteration 450/1000 | Loss: 0.00004218
Iteration 451/1000 | Loss: 0.00004218
Iteration 452/1000 | Loss: 0.00004218
Iteration 453/1000 | Loss: 0.00004218
Iteration 454/1000 | Loss: 0.00004218
Iteration 455/1000 | Loss: 0.00004218
Iteration 456/1000 | Loss: 0.00004218
Iteration 457/1000 | Loss: 0.00004218
Iteration 458/1000 | Loss: 0.00004218
Iteration 459/1000 | Loss: 0.00004218
Iteration 460/1000 | Loss: 0.00004218
Iteration 461/1000 | Loss: 0.00004218
Iteration 462/1000 | Loss: 0.00004218
Iteration 463/1000 | Loss: 0.00004218
Iteration 464/1000 | Loss: 0.00004218
Iteration 465/1000 | Loss: 0.00004218
Iteration 466/1000 | Loss: 0.00004218
Iteration 467/1000 | Loss: 0.00004218
Iteration 468/1000 | Loss: 0.00004218
Iteration 469/1000 | Loss: 0.00004218
Iteration 470/1000 | Loss: 0.00004218
Iteration 471/1000 | Loss: 0.00004218
Iteration 472/1000 | Loss: 0.00004218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 472. Stopping optimization.
Last 5 losses: [4.217826062813401e-05, 4.217826062813401e-05, 4.217826062813401e-05, 4.217826062813401e-05, 4.217826062813401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.217826062813401e-05

Optimization complete. Final v2v error: 4.402331352233887 mm

Highest mean error: 12.551264762878418 mm for frame 176

Lowest mean error: 3.626664638519287 mm for frame 21

Saving results

Total time: 604.8355493545532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453062
Iteration 2/25 | Loss: 0.00131813
Iteration 3/25 | Loss: 0.00124338
Iteration 4/25 | Loss: 0.00122879
Iteration 5/25 | Loss: 0.00122489
Iteration 6/25 | Loss: 0.00122486
Iteration 7/25 | Loss: 0.00122486
Iteration 8/25 | Loss: 0.00122486
Iteration 9/25 | Loss: 0.00122486
Iteration 10/25 | Loss: 0.00122486
Iteration 11/25 | Loss: 0.00122486
Iteration 12/25 | Loss: 0.00122486
Iteration 13/25 | Loss: 0.00122486
Iteration 14/25 | Loss: 0.00122486
Iteration 15/25 | Loss: 0.00122486
Iteration 16/25 | Loss: 0.00122486
Iteration 17/25 | Loss: 0.00122486
Iteration 18/25 | Loss: 0.00122486
Iteration 19/25 | Loss: 0.00122486
Iteration 20/25 | Loss: 0.00122486
Iteration 21/25 | Loss: 0.00122486
Iteration 22/25 | Loss: 0.00122486
Iteration 23/25 | Loss: 0.00122486
Iteration 24/25 | Loss: 0.00122486
Iteration 25/25 | Loss: 0.00122486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45529997
Iteration 2/25 | Loss: 0.00082131
Iteration 3/25 | Loss: 0.00082131
Iteration 4/25 | Loss: 0.00082131
Iteration 5/25 | Loss: 0.00082131
Iteration 6/25 | Loss: 0.00082131
Iteration 7/25 | Loss: 0.00082131
Iteration 8/25 | Loss: 0.00082131
Iteration 9/25 | Loss: 0.00082131
Iteration 10/25 | Loss: 0.00082131
Iteration 11/25 | Loss: 0.00082131
Iteration 12/25 | Loss: 0.00082131
Iteration 13/25 | Loss: 0.00082131
Iteration 14/25 | Loss: 0.00082131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008213084656745195, 0.0008213084656745195, 0.0008213084656745195, 0.0008213084656745195, 0.0008213084656745195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008213084656745195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082131
Iteration 2/1000 | Loss: 0.00002290
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00001558
Iteration 8/1000 | Loss: 0.00001550
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001462
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001454
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001452
Iteration 52/1000 | Loss: 0.00001452
Iteration 53/1000 | Loss: 0.00001452
Iteration 54/1000 | Loss: 0.00001451
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001450
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001446
Iteration 73/1000 | Loss: 0.00001446
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001444
Iteration 82/1000 | Loss: 0.00001444
Iteration 83/1000 | Loss: 0.00001444
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001443
Iteration 88/1000 | Loss: 0.00001443
Iteration 89/1000 | Loss: 0.00001443
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001441
Iteration 92/1000 | Loss: 0.00001441
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001439
Iteration 100/1000 | Loss: 0.00001439
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001438
Iteration 105/1000 | Loss: 0.00001438
Iteration 106/1000 | Loss: 0.00001438
Iteration 107/1000 | Loss: 0.00001438
Iteration 108/1000 | Loss: 0.00001438
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001437
Iteration 111/1000 | Loss: 0.00001437
Iteration 112/1000 | Loss: 0.00001437
Iteration 113/1000 | Loss: 0.00001437
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001437
Iteration 117/1000 | Loss: 0.00001436
Iteration 118/1000 | Loss: 0.00001436
Iteration 119/1000 | Loss: 0.00001436
Iteration 120/1000 | Loss: 0.00001436
Iteration 121/1000 | Loss: 0.00001436
Iteration 122/1000 | Loss: 0.00001436
Iteration 123/1000 | Loss: 0.00001436
Iteration 124/1000 | Loss: 0.00001436
Iteration 125/1000 | Loss: 0.00001436
Iteration 126/1000 | Loss: 0.00001436
Iteration 127/1000 | Loss: 0.00001436
Iteration 128/1000 | Loss: 0.00001435
Iteration 129/1000 | Loss: 0.00001435
Iteration 130/1000 | Loss: 0.00001435
Iteration 131/1000 | Loss: 0.00001434
Iteration 132/1000 | Loss: 0.00001434
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001434
Iteration 135/1000 | Loss: 0.00001434
Iteration 136/1000 | Loss: 0.00001434
Iteration 137/1000 | Loss: 0.00001434
Iteration 138/1000 | Loss: 0.00001434
Iteration 139/1000 | Loss: 0.00001434
Iteration 140/1000 | Loss: 0.00001434
Iteration 141/1000 | Loss: 0.00001434
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001433
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001432
Iteration 149/1000 | Loss: 0.00001432
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001431
Iteration 153/1000 | Loss: 0.00001431
Iteration 154/1000 | Loss: 0.00001431
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001431
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001429
Iteration 163/1000 | Loss: 0.00001429
Iteration 164/1000 | Loss: 0.00001429
Iteration 165/1000 | Loss: 0.00001429
Iteration 166/1000 | Loss: 0.00001429
Iteration 167/1000 | Loss: 0.00001429
Iteration 168/1000 | Loss: 0.00001429
Iteration 169/1000 | Loss: 0.00001429
Iteration 170/1000 | Loss: 0.00001429
Iteration 171/1000 | Loss: 0.00001429
Iteration 172/1000 | Loss: 0.00001429
Iteration 173/1000 | Loss: 0.00001429
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001429
Iteration 176/1000 | Loss: 0.00001429
Iteration 177/1000 | Loss: 0.00001429
Iteration 178/1000 | Loss: 0.00001429
Iteration 179/1000 | Loss: 0.00001429
Iteration 180/1000 | Loss: 0.00001429
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001429
Iteration 184/1000 | Loss: 0.00001429
Iteration 185/1000 | Loss: 0.00001429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.4286557416198775e-05, 1.4286557416198775e-05, 1.4286557416198775e-05, 1.4286557416198775e-05, 1.4286557416198775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4286557416198775e-05

Optimization complete. Final v2v error: 3.1645922660827637 mm

Highest mean error: 3.4653947353363037 mm for frame 97

Lowest mean error: 2.9422688484191895 mm for frame 1

Saving results

Total time: 42.40784001350403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415944
Iteration 2/25 | Loss: 0.00152662
Iteration 3/25 | Loss: 0.00126788
Iteration 4/25 | Loss: 0.00123450
Iteration 5/25 | Loss: 0.00122966
Iteration 6/25 | Loss: 0.00122837
Iteration 7/25 | Loss: 0.00122837
Iteration 8/25 | Loss: 0.00122837
Iteration 9/25 | Loss: 0.00122837
Iteration 10/25 | Loss: 0.00122837
Iteration 11/25 | Loss: 0.00122837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012283683754503727, 0.0012283683754503727, 0.0012283683754503727, 0.0012283683754503727, 0.0012283683754503727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012283683754503727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44402993
Iteration 2/25 | Loss: 0.00077792
Iteration 3/25 | Loss: 0.00077792
Iteration 4/25 | Loss: 0.00077792
Iteration 5/25 | Loss: 0.00077792
Iteration 6/25 | Loss: 0.00077792
Iteration 7/25 | Loss: 0.00077792
Iteration 8/25 | Loss: 0.00077792
Iteration 9/25 | Loss: 0.00077792
Iteration 10/25 | Loss: 0.00077792
Iteration 11/25 | Loss: 0.00077792
Iteration 12/25 | Loss: 0.00077792
Iteration 13/25 | Loss: 0.00077792
Iteration 14/25 | Loss: 0.00077792
Iteration 15/25 | Loss: 0.00077792
Iteration 16/25 | Loss: 0.00077792
Iteration 17/25 | Loss: 0.00077792
Iteration 18/25 | Loss: 0.00077792
Iteration 19/25 | Loss: 0.00077792
Iteration 20/25 | Loss: 0.00077792
Iteration 21/25 | Loss: 0.00077792
Iteration 22/25 | Loss: 0.00077792
Iteration 23/25 | Loss: 0.00077792
Iteration 24/25 | Loss: 0.00077792
Iteration 25/25 | Loss: 0.00077792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077792
Iteration 2/1000 | Loss: 0.00003828
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001782
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001337
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001335
Iteration 20/1000 | Loss: 0.00001334
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001332
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001325
Iteration 30/1000 | Loss: 0.00001325
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001322
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001316
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001315
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001315
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001312
Iteration 66/1000 | Loss: 0.00001312
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001300
Iteration 123/1000 | Loss: 0.00001300
Iteration 124/1000 | Loss: 0.00001300
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001299
Iteration 128/1000 | Loss: 0.00001299
Iteration 129/1000 | Loss: 0.00001299
Iteration 130/1000 | Loss: 0.00001299
Iteration 131/1000 | Loss: 0.00001299
Iteration 132/1000 | Loss: 0.00001299
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001297
Iteration 138/1000 | Loss: 0.00001297
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001296
Iteration 144/1000 | Loss: 0.00001296
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001296
Iteration 147/1000 | Loss: 0.00001295
Iteration 148/1000 | Loss: 0.00001295
Iteration 149/1000 | Loss: 0.00001295
Iteration 150/1000 | Loss: 0.00001295
Iteration 151/1000 | Loss: 0.00001295
Iteration 152/1000 | Loss: 0.00001295
Iteration 153/1000 | Loss: 0.00001295
Iteration 154/1000 | Loss: 0.00001294
Iteration 155/1000 | Loss: 0.00001294
Iteration 156/1000 | Loss: 0.00001294
Iteration 157/1000 | Loss: 0.00001294
Iteration 158/1000 | Loss: 0.00001294
Iteration 159/1000 | Loss: 0.00001294
Iteration 160/1000 | Loss: 0.00001294
Iteration 161/1000 | Loss: 0.00001294
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001293
Iteration 164/1000 | Loss: 0.00001293
Iteration 165/1000 | Loss: 0.00001293
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001293
Iteration 171/1000 | Loss: 0.00001293
Iteration 172/1000 | Loss: 0.00001293
Iteration 173/1000 | Loss: 0.00001293
Iteration 174/1000 | Loss: 0.00001293
Iteration 175/1000 | Loss: 0.00001293
Iteration 176/1000 | Loss: 0.00001293
Iteration 177/1000 | Loss: 0.00001292
Iteration 178/1000 | Loss: 0.00001292
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001292
Iteration 181/1000 | Loss: 0.00001292
Iteration 182/1000 | Loss: 0.00001292
Iteration 183/1000 | Loss: 0.00001292
Iteration 184/1000 | Loss: 0.00001292
Iteration 185/1000 | Loss: 0.00001292
Iteration 186/1000 | Loss: 0.00001292
Iteration 187/1000 | Loss: 0.00001292
Iteration 188/1000 | Loss: 0.00001291
Iteration 189/1000 | Loss: 0.00001291
Iteration 190/1000 | Loss: 0.00001291
Iteration 191/1000 | Loss: 0.00001291
Iteration 192/1000 | Loss: 0.00001291
Iteration 193/1000 | Loss: 0.00001290
Iteration 194/1000 | Loss: 0.00001290
Iteration 195/1000 | Loss: 0.00001290
Iteration 196/1000 | Loss: 0.00001290
Iteration 197/1000 | Loss: 0.00001290
Iteration 198/1000 | Loss: 0.00001290
Iteration 199/1000 | Loss: 0.00001290
Iteration 200/1000 | Loss: 0.00001290
Iteration 201/1000 | Loss: 0.00001290
Iteration 202/1000 | Loss: 0.00001289
Iteration 203/1000 | Loss: 0.00001289
Iteration 204/1000 | Loss: 0.00001289
Iteration 205/1000 | Loss: 0.00001289
Iteration 206/1000 | Loss: 0.00001289
Iteration 207/1000 | Loss: 0.00001289
Iteration 208/1000 | Loss: 0.00001288
Iteration 209/1000 | Loss: 0.00001288
Iteration 210/1000 | Loss: 0.00001288
Iteration 211/1000 | Loss: 0.00001288
Iteration 212/1000 | Loss: 0.00001288
Iteration 213/1000 | Loss: 0.00001288
Iteration 214/1000 | Loss: 0.00001288
Iteration 215/1000 | Loss: 0.00001288
Iteration 216/1000 | Loss: 0.00001288
Iteration 217/1000 | Loss: 0.00001288
Iteration 218/1000 | Loss: 0.00001287
Iteration 219/1000 | Loss: 0.00001287
Iteration 220/1000 | Loss: 0.00001287
Iteration 221/1000 | Loss: 0.00001287
Iteration 222/1000 | Loss: 0.00001287
Iteration 223/1000 | Loss: 0.00001287
Iteration 224/1000 | Loss: 0.00001287
Iteration 225/1000 | Loss: 0.00001287
Iteration 226/1000 | Loss: 0.00001286
Iteration 227/1000 | Loss: 0.00001286
Iteration 228/1000 | Loss: 0.00001286
Iteration 229/1000 | Loss: 0.00001286
Iteration 230/1000 | Loss: 0.00001286
Iteration 231/1000 | Loss: 0.00001286
Iteration 232/1000 | Loss: 0.00001286
Iteration 233/1000 | Loss: 0.00001286
Iteration 234/1000 | Loss: 0.00001286
Iteration 235/1000 | Loss: 0.00001286
Iteration 236/1000 | Loss: 0.00001286
Iteration 237/1000 | Loss: 0.00001286
Iteration 238/1000 | Loss: 0.00001286
Iteration 239/1000 | Loss: 0.00001286
Iteration 240/1000 | Loss: 0.00001286
Iteration 241/1000 | Loss: 0.00001286
Iteration 242/1000 | Loss: 0.00001286
Iteration 243/1000 | Loss: 0.00001286
Iteration 244/1000 | Loss: 0.00001286
Iteration 245/1000 | Loss: 0.00001286
Iteration 246/1000 | Loss: 0.00001286
Iteration 247/1000 | Loss: 0.00001286
Iteration 248/1000 | Loss: 0.00001286
Iteration 249/1000 | Loss: 0.00001286
Iteration 250/1000 | Loss: 0.00001286
Iteration 251/1000 | Loss: 0.00001286
Iteration 252/1000 | Loss: 0.00001286
Iteration 253/1000 | Loss: 0.00001286
Iteration 254/1000 | Loss: 0.00001286
Iteration 255/1000 | Loss: 0.00001286
Iteration 256/1000 | Loss: 0.00001286
Iteration 257/1000 | Loss: 0.00001286
Iteration 258/1000 | Loss: 0.00001286
Iteration 259/1000 | Loss: 0.00001286
Iteration 260/1000 | Loss: 0.00001286
Iteration 261/1000 | Loss: 0.00001286
Iteration 262/1000 | Loss: 0.00001286
Iteration 263/1000 | Loss: 0.00001286
Iteration 264/1000 | Loss: 0.00001286
Iteration 265/1000 | Loss: 0.00001286
Iteration 266/1000 | Loss: 0.00001286
Iteration 267/1000 | Loss: 0.00001286
Iteration 268/1000 | Loss: 0.00001286
Iteration 269/1000 | Loss: 0.00001286
Iteration 270/1000 | Loss: 0.00001286
Iteration 271/1000 | Loss: 0.00001286
Iteration 272/1000 | Loss: 0.00001286
Iteration 273/1000 | Loss: 0.00001286
Iteration 274/1000 | Loss: 0.00001286
Iteration 275/1000 | Loss: 0.00001286
Iteration 276/1000 | Loss: 0.00001286
Iteration 277/1000 | Loss: 0.00001286
Iteration 278/1000 | Loss: 0.00001286
Iteration 279/1000 | Loss: 0.00001286
Iteration 280/1000 | Loss: 0.00001286
Iteration 281/1000 | Loss: 0.00001286
Iteration 282/1000 | Loss: 0.00001286
Iteration 283/1000 | Loss: 0.00001286
Iteration 284/1000 | Loss: 0.00001286
Iteration 285/1000 | Loss: 0.00001286
Iteration 286/1000 | Loss: 0.00001286
Iteration 287/1000 | Loss: 0.00001286
Iteration 288/1000 | Loss: 0.00001286
Iteration 289/1000 | Loss: 0.00001286
Iteration 290/1000 | Loss: 0.00001286
Iteration 291/1000 | Loss: 0.00001286
Iteration 292/1000 | Loss: 0.00001286
Iteration 293/1000 | Loss: 0.00001286
Iteration 294/1000 | Loss: 0.00001286
Iteration 295/1000 | Loss: 0.00001286
Iteration 296/1000 | Loss: 0.00001286
Iteration 297/1000 | Loss: 0.00001286
Iteration 298/1000 | Loss: 0.00001286
Iteration 299/1000 | Loss: 0.00001286
Iteration 300/1000 | Loss: 0.00001286
Iteration 301/1000 | Loss: 0.00001286
Iteration 302/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.2856325156462844e-05, 1.2856325156462844e-05, 1.2856325156462844e-05, 1.2856325156462844e-05, 1.2856325156462844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2856325156462844e-05

Optimization complete. Final v2v error: 3.0717899799346924 mm

Highest mean error: 3.641913652420044 mm for frame 76

Lowest mean error: 2.8367362022399902 mm for frame 165

Saving results

Total time: 45.19514536857605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796258
Iteration 2/25 | Loss: 0.00135580
Iteration 3/25 | Loss: 0.00125880
Iteration 4/25 | Loss: 0.00124442
Iteration 5/25 | Loss: 0.00124065
Iteration 6/25 | Loss: 0.00124007
Iteration 7/25 | Loss: 0.00124007
Iteration 8/25 | Loss: 0.00124007
Iteration 9/25 | Loss: 0.00124007
Iteration 10/25 | Loss: 0.00124007
Iteration 11/25 | Loss: 0.00124007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012400663690641522, 0.0012400663690641522, 0.0012400663690641522, 0.0012400663690641522, 0.0012400663690641522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012400663690641522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48351121
Iteration 2/25 | Loss: 0.00077805
Iteration 3/25 | Loss: 0.00077805
Iteration 4/25 | Loss: 0.00077805
Iteration 5/25 | Loss: 0.00077805
Iteration 6/25 | Loss: 0.00077805
Iteration 7/25 | Loss: 0.00077805
Iteration 8/25 | Loss: 0.00077805
Iteration 9/25 | Loss: 0.00077805
Iteration 10/25 | Loss: 0.00077805
Iteration 11/25 | Loss: 0.00077805
Iteration 12/25 | Loss: 0.00077805
Iteration 13/25 | Loss: 0.00077805
Iteration 14/25 | Loss: 0.00077805
Iteration 15/25 | Loss: 0.00077805
Iteration 16/25 | Loss: 0.00077805
Iteration 17/25 | Loss: 0.00077805
Iteration 18/25 | Loss: 0.00077805
Iteration 19/25 | Loss: 0.00077805
Iteration 20/25 | Loss: 0.00077805
Iteration 21/25 | Loss: 0.00077805
Iteration 22/25 | Loss: 0.00077805
Iteration 23/25 | Loss: 0.00077805
Iteration 24/25 | Loss: 0.00077805
Iteration 25/25 | Loss: 0.00077805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077805
Iteration 2/1000 | Loss: 0.00003144
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001915
Iteration 5/1000 | Loss: 0.00001842
Iteration 6/1000 | Loss: 0.00001799
Iteration 7/1000 | Loss: 0.00001746
Iteration 8/1000 | Loss: 0.00001720
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001708
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001664
Iteration 13/1000 | Loss: 0.00001648
Iteration 14/1000 | Loss: 0.00001644
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001626
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001618
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001610
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001606
Iteration 43/1000 | Loss: 0.00001606
Iteration 44/1000 | Loss: 0.00001606
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001605
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001605
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001604
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001603
Iteration 59/1000 | Loss: 0.00001602
Iteration 60/1000 | Loss: 0.00001602
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001601
Iteration 63/1000 | Loss: 0.00001601
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001600
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001599
Iteration 74/1000 | Loss: 0.00001599
Iteration 75/1000 | Loss: 0.00001598
Iteration 76/1000 | Loss: 0.00001598
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001598
Iteration 79/1000 | Loss: 0.00001598
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001597
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001594
Iteration 87/1000 | Loss: 0.00001594
Iteration 88/1000 | Loss: 0.00001594
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001592
Iteration 103/1000 | Loss: 0.00001592
Iteration 104/1000 | Loss: 0.00001592
Iteration 105/1000 | Loss: 0.00001591
Iteration 106/1000 | Loss: 0.00001591
Iteration 107/1000 | Loss: 0.00001591
Iteration 108/1000 | Loss: 0.00001591
Iteration 109/1000 | Loss: 0.00001591
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001590
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001590
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001589
Iteration 116/1000 | Loss: 0.00001589
Iteration 117/1000 | Loss: 0.00001588
Iteration 118/1000 | Loss: 0.00001588
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001587
Iteration 121/1000 | Loss: 0.00001587
Iteration 122/1000 | Loss: 0.00001586
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001586
Iteration 125/1000 | Loss: 0.00001585
Iteration 126/1000 | Loss: 0.00001585
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Iteration 129/1000 | Loss: 0.00001585
Iteration 130/1000 | Loss: 0.00001585
Iteration 131/1000 | Loss: 0.00001584
Iteration 132/1000 | Loss: 0.00001584
Iteration 133/1000 | Loss: 0.00001584
Iteration 134/1000 | Loss: 0.00001584
Iteration 135/1000 | Loss: 0.00001584
Iteration 136/1000 | Loss: 0.00001584
Iteration 137/1000 | Loss: 0.00001584
Iteration 138/1000 | Loss: 0.00001584
Iteration 139/1000 | Loss: 0.00001584
Iteration 140/1000 | Loss: 0.00001584
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001583
Iteration 146/1000 | Loss: 0.00001583
Iteration 147/1000 | Loss: 0.00001583
Iteration 148/1000 | Loss: 0.00001583
Iteration 149/1000 | Loss: 0.00001583
Iteration 150/1000 | Loss: 0.00001583
Iteration 151/1000 | Loss: 0.00001583
Iteration 152/1000 | Loss: 0.00001583
Iteration 153/1000 | Loss: 0.00001583
Iteration 154/1000 | Loss: 0.00001583
Iteration 155/1000 | Loss: 0.00001583
Iteration 156/1000 | Loss: 0.00001583
Iteration 157/1000 | Loss: 0.00001583
Iteration 158/1000 | Loss: 0.00001582
Iteration 159/1000 | Loss: 0.00001582
Iteration 160/1000 | Loss: 0.00001582
Iteration 161/1000 | Loss: 0.00001582
Iteration 162/1000 | Loss: 0.00001582
Iteration 163/1000 | Loss: 0.00001582
Iteration 164/1000 | Loss: 0.00001582
Iteration 165/1000 | Loss: 0.00001582
Iteration 166/1000 | Loss: 0.00001582
Iteration 167/1000 | Loss: 0.00001582
Iteration 168/1000 | Loss: 0.00001582
Iteration 169/1000 | Loss: 0.00001582
Iteration 170/1000 | Loss: 0.00001582
Iteration 171/1000 | Loss: 0.00001582
Iteration 172/1000 | Loss: 0.00001582
Iteration 173/1000 | Loss: 0.00001582
Iteration 174/1000 | Loss: 0.00001582
Iteration 175/1000 | Loss: 0.00001582
Iteration 176/1000 | Loss: 0.00001582
Iteration 177/1000 | Loss: 0.00001582
Iteration 178/1000 | Loss: 0.00001582
Iteration 179/1000 | Loss: 0.00001582
Iteration 180/1000 | Loss: 0.00001582
Iteration 181/1000 | Loss: 0.00001582
Iteration 182/1000 | Loss: 0.00001582
Iteration 183/1000 | Loss: 0.00001582
Iteration 184/1000 | Loss: 0.00001582
Iteration 185/1000 | Loss: 0.00001582
Iteration 186/1000 | Loss: 0.00001582
Iteration 187/1000 | Loss: 0.00001582
Iteration 188/1000 | Loss: 0.00001582
Iteration 189/1000 | Loss: 0.00001582
Iteration 190/1000 | Loss: 0.00001582
Iteration 191/1000 | Loss: 0.00001582
Iteration 192/1000 | Loss: 0.00001582
Iteration 193/1000 | Loss: 0.00001582
Iteration 194/1000 | Loss: 0.00001582
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001582
Iteration 198/1000 | Loss: 0.00001582
Iteration 199/1000 | Loss: 0.00001582
Iteration 200/1000 | Loss: 0.00001582
Iteration 201/1000 | Loss: 0.00001582
Iteration 202/1000 | Loss: 0.00001582
Iteration 203/1000 | Loss: 0.00001582
Iteration 204/1000 | Loss: 0.00001582
Iteration 205/1000 | Loss: 0.00001582
Iteration 206/1000 | Loss: 0.00001582
Iteration 207/1000 | Loss: 0.00001582
Iteration 208/1000 | Loss: 0.00001582
Iteration 209/1000 | Loss: 0.00001582
Iteration 210/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.5816345694474876e-05, 1.5816345694474876e-05, 1.5816345694474876e-05, 1.5816345694474876e-05, 1.5816345694474876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5816345694474876e-05

Optimization complete. Final v2v error: 3.3915655612945557 mm

Highest mean error: 4.083564281463623 mm for frame 87

Lowest mean error: 3.2062742710113525 mm for frame 138

Saving results

Total time: 38.81754493713379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783004
Iteration 2/25 | Loss: 0.00139795
Iteration 3/25 | Loss: 0.00133124
Iteration 4/25 | Loss: 0.00132056
Iteration 5/25 | Loss: 0.00131785
Iteration 6/25 | Loss: 0.00131785
Iteration 7/25 | Loss: 0.00131785
Iteration 8/25 | Loss: 0.00131785
Iteration 9/25 | Loss: 0.00131785
Iteration 10/25 | Loss: 0.00131785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013178535737097263, 0.0013178535737097263, 0.0013178535737097263, 0.0013178535737097263, 0.0013178535737097263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013178535737097263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65443039
Iteration 2/25 | Loss: 0.00073335
Iteration 3/25 | Loss: 0.00073335
Iteration 4/25 | Loss: 0.00073335
Iteration 5/25 | Loss: 0.00073335
Iteration 6/25 | Loss: 0.00073335
Iteration 7/25 | Loss: 0.00073335
Iteration 8/25 | Loss: 0.00073335
Iteration 9/25 | Loss: 0.00073335
Iteration 10/25 | Loss: 0.00073335
Iteration 11/25 | Loss: 0.00073335
Iteration 12/25 | Loss: 0.00073335
Iteration 13/25 | Loss: 0.00073335
Iteration 14/25 | Loss: 0.00073335
Iteration 15/25 | Loss: 0.00073335
Iteration 16/25 | Loss: 0.00073335
Iteration 17/25 | Loss: 0.00073335
Iteration 18/25 | Loss: 0.00073335
Iteration 19/25 | Loss: 0.00073335
Iteration 20/25 | Loss: 0.00073335
Iteration 21/25 | Loss: 0.00073335
Iteration 22/25 | Loss: 0.00073335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007333470857702196, 0.0007333470857702196, 0.0007333470857702196, 0.0007333470857702196, 0.0007333470857702196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007333470857702196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073335
Iteration 2/1000 | Loss: 0.00004535
Iteration 3/1000 | Loss: 0.00002862
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002422
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002304
Iteration 8/1000 | Loss: 0.00002252
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002165
Iteration 12/1000 | Loss: 0.00002153
Iteration 13/1000 | Loss: 0.00002134
Iteration 14/1000 | Loss: 0.00002118
Iteration 15/1000 | Loss: 0.00002117
Iteration 16/1000 | Loss: 0.00002115
Iteration 17/1000 | Loss: 0.00002115
Iteration 18/1000 | Loss: 0.00002114
Iteration 19/1000 | Loss: 0.00002113
Iteration 20/1000 | Loss: 0.00002112
Iteration 21/1000 | Loss: 0.00002112
Iteration 22/1000 | Loss: 0.00002109
Iteration 23/1000 | Loss: 0.00002107
Iteration 24/1000 | Loss: 0.00002107
Iteration 25/1000 | Loss: 0.00002105
Iteration 26/1000 | Loss: 0.00002105
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002104
Iteration 30/1000 | Loss: 0.00002104
Iteration 31/1000 | Loss: 0.00002104
Iteration 32/1000 | Loss: 0.00002104
Iteration 33/1000 | Loss: 0.00002104
Iteration 34/1000 | Loss: 0.00002104
Iteration 35/1000 | Loss: 0.00002104
Iteration 36/1000 | Loss: 0.00002104
Iteration 37/1000 | Loss: 0.00002104
Iteration 38/1000 | Loss: 0.00002103
Iteration 39/1000 | Loss: 0.00002103
Iteration 40/1000 | Loss: 0.00002103
Iteration 41/1000 | Loss: 0.00002103
Iteration 42/1000 | Loss: 0.00002102
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002102
Iteration 45/1000 | Loss: 0.00002102
Iteration 46/1000 | Loss: 0.00002101
Iteration 47/1000 | Loss: 0.00002101
Iteration 48/1000 | Loss: 0.00002101
Iteration 49/1000 | Loss: 0.00002101
Iteration 50/1000 | Loss: 0.00002100
Iteration 51/1000 | Loss: 0.00002100
Iteration 52/1000 | Loss: 0.00002099
Iteration 53/1000 | Loss: 0.00002099
Iteration 54/1000 | Loss: 0.00002099
Iteration 55/1000 | Loss: 0.00002098
Iteration 56/1000 | Loss: 0.00002098
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002097
Iteration 63/1000 | Loss: 0.00002096
Iteration 64/1000 | Loss: 0.00002096
Iteration 65/1000 | Loss: 0.00002096
Iteration 66/1000 | Loss: 0.00002095
Iteration 67/1000 | Loss: 0.00002095
Iteration 68/1000 | Loss: 0.00002095
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00002094
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002094
Iteration 73/1000 | Loss: 0.00002094
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002094
Iteration 80/1000 | Loss: 0.00002094
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002092
Iteration 86/1000 | Loss: 0.00002092
Iteration 87/1000 | Loss: 0.00002092
Iteration 88/1000 | Loss: 0.00002092
Iteration 89/1000 | Loss: 0.00002092
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002091
Iteration 92/1000 | Loss: 0.00002091
Iteration 93/1000 | Loss: 0.00002091
Iteration 94/1000 | Loss: 0.00002091
Iteration 95/1000 | Loss: 0.00002091
Iteration 96/1000 | Loss: 0.00002091
Iteration 97/1000 | Loss: 0.00002091
Iteration 98/1000 | Loss: 0.00002090
Iteration 99/1000 | Loss: 0.00002090
Iteration 100/1000 | Loss: 0.00002090
Iteration 101/1000 | Loss: 0.00002090
Iteration 102/1000 | Loss: 0.00002089
Iteration 103/1000 | Loss: 0.00002089
Iteration 104/1000 | Loss: 0.00002089
Iteration 105/1000 | Loss: 0.00002088
Iteration 106/1000 | Loss: 0.00002088
Iteration 107/1000 | Loss: 0.00002088
Iteration 108/1000 | Loss: 0.00002088
Iteration 109/1000 | Loss: 0.00002088
Iteration 110/1000 | Loss: 0.00002088
Iteration 111/1000 | Loss: 0.00002088
Iteration 112/1000 | Loss: 0.00002088
Iteration 113/1000 | Loss: 0.00002088
Iteration 114/1000 | Loss: 0.00002087
Iteration 115/1000 | Loss: 0.00002087
Iteration 116/1000 | Loss: 0.00002087
Iteration 117/1000 | Loss: 0.00002087
Iteration 118/1000 | Loss: 0.00002087
Iteration 119/1000 | Loss: 0.00002087
Iteration 120/1000 | Loss: 0.00002087
Iteration 121/1000 | Loss: 0.00002087
Iteration 122/1000 | Loss: 0.00002087
Iteration 123/1000 | Loss: 0.00002087
Iteration 124/1000 | Loss: 0.00002087
Iteration 125/1000 | Loss: 0.00002087
Iteration 126/1000 | Loss: 0.00002086
Iteration 127/1000 | Loss: 0.00002086
Iteration 128/1000 | Loss: 0.00002086
Iteration 129/1000 | Loss: 0.00002086
Iteration 130/1000 | Loss: 0.00002086
Iteration 131/1000 | Loss: 0.00002086
Iteration 132/1000 | Loss: 0.00002086
Iteration 133/1000 | Loss: 0.00002086
Iteration 134/1000 | Loss: 0.00002086
Iteration 135/1000 | Loss: 0.00002086
Iteration 136/1000 | Loss: 0.00002086
Iteration 137/1000 | Loss: 0.00002086
Iteration 138/1000 | Loss: 0.00002086
Iteration 139/1000 | Loss: 0.00002086
Iteration 140/1000 | Loss: 0.00002086
Iteration 141/1000 | Loss: 0.00002086
Iteration 142/1000 | Loss: 0.00002086
Iteration 143/1000 | Loss: 0.00002086
Iteration 144/1000 | Loss: 0.00002086
Iteration 145/1000 | Loss: 0.00002085
Iteration 146/1000 | Loss: 0.00002085
Iteration 147/1000 | Loss: 0.00002085
Iteration 148/1000 | Loss: 0.00002085
Iteration 149/1000 | Loss: 0.00002085
Iteration 150/1000 | Loss: 0.00002085
Iteration 151/1000 | Loss: 0.00002085
Iteration 152/1000 | Loss: 0.00002085
Iteration 153/1000 | Loss: 0.00002085
Iteration 154/1000 | Loss: 0.00002085
Iteration 155/1000 | Loss: 0.00002085
Iteration 156/1000 | Loss: 0.00002085
Iteration 157/1000 | Loss: 0.00002085
Iteration 158/1000 | Loss: 0.00002085
Iteration 159/1000 | Loss: 0.00002084
Iteration 160/1000 | Loss: 0.00002084
Iteration 161/1000 | Loss: 0.00002084
Iteration 162/1000 | Loss: 0.00002084
Iteration 163/1000 | Loss: 0.00002083
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Iteration 166/1000 | Loss: 0.00002083
Iteration 167/1000 | Loss: 0.00002083
Iteration 168/1000 | Loss: 0.00002083
Iteration 169/1000 | Loss: 0.00002083
Iteration 170/1000 | Loss: 0.00002083
Iteration 171/1000 | Loss: 0.00002083
Iteration 172/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.0833082089666277e-05, 2.0833082089666277e-05, 2.0833082089666277e-05, 2.0833082089666277e-05, 2.0833082089666277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0833082089666277e-05

Optimization complete. Final v2v error: 3.804537057876587 mm

Highest mean error: 4.242500305175781 mm for frame 60

Lowest mean error: 3.483097791671753 mm for frame 175

Saving results

Total time: 39.46389889717102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00231171
Iteration 2/25 | Loss: 0.00128945
Iteration 3/25 | Loss: 0.00119709
Iteration 4/25 | Loss: 0.00117561
Iteration 5/25 | Loss: 0.00117025
Iteration 6/25 | Loss: 0.00116878
Iteration 7/25 | Loss: 0.00116878
Iteration 8/25 | Loss: 0.00116878
Iteration 9/25 | Loss: 0.00116878
Iteration 10/25 | Loss: 0.00116878
Iteration 11/25 | Loss: 0.00116878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011687803780660033, 0.0011687803780660033, 0.0011687803780660033, 0.0011687803780660033, 0.0011687803780660033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011687803780660033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41038978
Iteration 2/25 | Loss: 0.00075155
Iteration 3/25 | Loss: 0.00075155
Iteration 4/25 | Loss: 0.00075155
Iteration 5/25 | Loss: 0.00075155
Iteration 6/25 | Loss: 0.00075155
Iteration 7/25 | Loss: 0.00075155
Iteration 8/25 | Loss: 0.00075155
Iteration 9/25 | Loss: 0.00075155
Iteration 10/25 | Loss: 0.00075155
Iteration 11/25 | Loss: 0.00075155
Iteration 12/25 | Loss: 0.00075155
Iteration 13/25 | Loss: 0.00075155
Iteration 14/25 | Loss: 0.00075155
Iteration 15/25 | Loss: 0.00075155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007515474571846426, 0.0007515474571846426, 0.0007515474571846426, 0.0007515474571846426, 0.0007515474571846426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007515474571846426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075155
Iteration 2/1000 | Loss: 0.00003510
Iteration 3/1000 | Loss: 0.00002260
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001526
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001470
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001458
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001452
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001452
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001451
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001450
Iteration 58/1000 | Loss: 0.00001450
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.4423388165596407e-05, 1.4423388165596407e-05, 1.4423388165596407e-05, 1.4423388165596407e-05, 1.4423388165596407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4423388165596407e-05

Optimization complete. Final v2v error: 3.271602153778076 mm

Highest mean error: 3.672640085220337 mm for frame 43

Lowest mean error: 3.0513978004455566 mm for frame 188

Saving results

Total time: 36.50847792625427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568698
Iteration 2/25 | Loss: 0.00125415
Iteration 3/25 | Loss: 0.00119555
Iteration 4/25 | Loss: 0.00118720
Iteration 5/25 | Loss: 0.00118437
Iteration 6/25 | Loss: 0.00118429
Iteration 7/25 | Loss: 0.00118429
Iteration 8/25 | Loss: 0.00118429
Iteration 9/25 | Loss: 0.00118429
Iteration 10/25 | Loss: 0.00118429
Iteration 11/25 | Loss: 0.00118429
Iteration 12/25 | Loss: 0.00118429
Iteration 13/25 | Loss: 0.00118429
Iteration 14/25 | Loss: 0.00118429
Iteration 15/25 | Loss: 0.00118429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011842860840260983, 0.0011842860840260983, 0.0011842860840260983, 0.0011842860840260983, 0.0011842860840260983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011842860840260983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93014312
Iteration 2/25 | Loss: 0.00077969
Iteration 3/25 | Loss: 0.00077968
Iteration 4/25 | Loss: 0.00077968
Iteration 5/25 | Loss: 0.00077968
Iteration 6/25 | Loss: 0.00077968
Iteration 7/25 | Loss: 0.00077968
Iteration 8/25 | Loss: 0.00077968
Iteration 9/25 | Loss: 0.00077968
Iteration 10/25 | Loss: 0.00077968
Iteration 11/25 | Loss: 0.00077968
Iteration 12/25 | Loss: 0.00077968
Iteration 13/25 | Loss: 0.00077968
Iteration 14/25 | Loss: 0.00077968
Iteration 15/25 | Loss: 0.00077968
Iteration 16/25 | Loss: 0.00077968
Iteration 17/25 | Loss: 0.00077968
Iteration 18/25 | Loss: 0.00077968
Iteration 19/25 | Loss: 0.00077968
Iteration 20/25 | Loss: 0.00077968
Iteration 21/25 | Loss: 0.00077968
Iteration 22/25 | Loss: 0.00077968
Iteration 23/25 | Loss: 0.00077968
Iteration 24/25 | Loss: 0.00077968
Iteration 25/25 | Loss: 0.00077968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077968
Iteration 2/1000 | Loss: 0.00002141
Iteration 3/1000 | Loss: 0.00001443
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001164
Iteration 9/1000 | Loss: 0.00001148
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001122
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001117
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001113
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001111
Iteration 26/1000 | Loss: 0.00001111
Iteration 27/1000 | Loss: 0.00001111
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001108
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001107
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001097
Iteration 60/1000 | Loss: 0.00001097
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001091
Iteration 70/1000 | Loss: 0.00001091
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001087
Iteration 74/1000 | Loss: 0.00001087
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001086
Iteration 77/1000 | Loss: 0.00001086
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001085
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001084
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001084
Iteration 88/1000 | Loss: 0.00001084
Iteration 89/1000 | Loss: 0.00001084
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001078
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001075
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001073
Iteration 135/1000 | Loss: 0.00001073
Iteration 136/1000 | Loss: 0.00001073
Iteration 137/1000 | Loss: 0.00001073
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001071
Iteration 151/1000 | Loss: 0.00001071
Iteration 152/1000 | Loss: 0.00001071
Iteration 153/1000 | Loss: 0.00001071
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001071
Iteration 156/1000 | Loss: 0.00001071
Iteration 157/1000 | Loss: 0.00001071
Iteration 158/1000 | Loss: 0.00001071
Iteration 159/1000 | Loss: 0.00001071
Iteration 160/1000 | Loss: 0.00001071
Iteration 161/1000 | Loss: 0.00001071
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001071
Iteration 168/1000 | Loss: 0.00001071
Iteration 169/1000 | Loss: 0.00001071
Iteration 170/1000 | Loss: 0.00001071
Iteration 171/1000 | Loss: 0.00001071
Iteration 172/1000 | Loss: 0.00001071
Iteration 173/1000 | Loss: 0.00001071
Iteration 174/1000 | Loss: 0.00001071
Iteration 175/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.0706758075684775e-05, 1.0706758075684775e-05, 1.0706758075684775e-05, 1.0706758075684775e-05, 1.0706758075684775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0706758075684775e-05

Optimization complete. Final v2v error: 2.8214550018310547 mm

Highest mean error: 3.2081398963928223 mm for frame 119

Lowest mean error: 2.7042598724365234 mm for frame 2

Saving results

Total time: 35.8414146900177
