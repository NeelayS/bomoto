Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=73, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4088-4143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105966
Iteration 2/25 | Loss: 0.00237812
Iteration 3/25 | Loss: 0.00163480
Iteration 4/25 | Loss: 0.00149128
Iteration 5/25 | Loss: 0.00144540
Iteration 6/25 | Loss: 0.00145376
Iteration 7/25 | Loss: 0.00142081
Iteration 8/25 | Loss: 0.00141522
Iteration 9/25 | Loss: 0.00141529
Iteration 10/25 | Loss: 0.00141391
Iteration 11/25 | Loss: 0.00141711
Iteration 12/25 | Loss: 0.00140993
Iteration 13/25 | Loss: 0.00140878
Iteration 14/25 | Loss: 0.00140850
Iteration 15/25 | Loss: 0.00140830
Iteration 16/25 | Loss: 0.00140817
Iteration 17/25 | Loss: 0.00140808
Iteration 18/25 | Loss: 0.00140806
Iteration 19/25 | Loss: 0.00140806
Iteration 20/25 | Loss: 0.00140806
Iteration 21/25 | Loss: 0.00140806
Iteration 22/25 | Loss: 0.00140806
Iteration 23/25 | Loss: 0.00140806
Iteration 24/25 | Loss: 0.00140805
Iteration 25/25 | Loss: 0.00140805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59695840
Iteration 2/25 | Loss: 0.00526337
Iteration 3/25 | Loss: 0.00526337
Iteration 4/25 | Loss: 0.00526337
Iteration 5/25 | Loss: 0.00526337
Iteration 6/25 | Loss: 0.00526337
Iteration 7/25 | Loss: 0.00526337
Iteration 8/25 | Loss: 0.00526337
Iteration 9/25 | Loss: 0.00526337
Iteration 10/25 | Loss: 0.00526337
Iteration 11/25 | Loss: 0.00526337
Iteration 12/25 | Loss: 0.00526337
Iteration 13/25 | Loss: 0.00526337
Iteration 14/25 | Loss: 0.00526337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.005263369530439377, 0.005263369530439377, 0.005263369530439377, 0.005263369530439377, 0.005263369530439377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005263369530439377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00526337
Iteration 2/1000 | Loss: 0.00075646
Iteration 3/1000 | Loss: 0.00057422
Iteration 4/1000 | Loss: 0.00045981
Iteration 5/1000 | Loss: 0.00068257
Iteration 6/1000 | Loss: 0.00037258
Iteration 7/1000 | Loss: 0.00076809
Iteration 8/1000 | Loss: 0.00030163
Iteration 9/1000 | Loss: 0.00027922
Iteration 10/1000 | Loss: 0.00078763
Iteration 11/1000 | Loss: 0.00354254
Iteration 12/1000 | Loss: 0.01340657
Iteration 13/1000 | Loss: 0.00421779
Iteration 14/1000 | Loss: 0.00085344
Iteration 15/1000 | Loss: 0.00063001
Iteration 16/1000 | Loss: 0.00040956
Iteration 17/1000 | Loss: 0.00063265
Iteration 18/1000 | Loss: 0.00055505
Iteration 19/1000 | Loss: 0.00016891
Iteration 20/1000 | Loss: 0.00016661
Iteration 21/1000 | Loss: 0.00008785
Iteration 22/1000 | Loss: 0.00007129
Iteration 23/1000 | Loss: 0.00012696
Iteration 24/1000 | Loss: 0.00004496
Iteration 25/1000 | Loss: 0.00005832
Iteration 26/1000 | Loss: 0.00003598
Iteration 27/1000 | Loss: 0.00003106
Iteration 28/1000 | Loss: 0.00002734
Iteration 29/1000 | Loss: 0.00002559
Iteration 30/1000 | Loss: 0.00002372
Iteration 31/1000 | Loss: 0.00002227
Iteration 32/1000 | Loss: 0.00002131
Iteration 33/1000 | Loss: 0.00002060
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00001981
Iteration 36/1000 | Loss: 0.00001956
Iteration 37/1000 | Loss: 0.00001955
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001950
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001940
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001935
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001932
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001930
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001926
Iteration 94/1000 | Loss: 0.00001926
Iteration 95/1000 | Loss: 0.00001926
Iteration 96/1000 | Loss: 0.00001926
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001925
Iteration 117/1000 | Loss: 0.00001925
Iteration 118/1000 | Loss: 0.00001925
Iteration 119/1000 | Loss: 0.00001925
Iteration 120/1000 | Loss: 0.00001925
Iteration 121/1000 | Loss: 0.00001925
Iteration 122/1000 | Loss: 0.00001925
Iteration 123/1000 | Loss: 0.00001925
Iteration 124/1000 | Loss: 0.00001925
Iteration 125/1000 | Loss: 0.00001925
Iteration 126/1000 | Loss: 0.00001925
Iteration 127/1000 | Loss: 0.00001925
Iteration 128/1000 | Loss: 0.00001925
Iteration 129/1000 | Loss: 0.00001925
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00001925
Iteration 132/1000 | Loss: 0.00001925
Iteration 133/1000 | Loss: 0.00001925
Iteration 134/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.9252251149737276e-05, 1.9252251149737276e-05, 1.9252251149737276e-05, 1.9252251149737276e-05, 1.9252251149737276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9252251149737276e-05

Optimization complete. Final v2v error: 3.7601840496063232 mm

Highest mean error: 10.1957426071167 mm for frame 215

Lowest mean error: 3.3593432903289795 mm for frame 0

Saving results

Total time: 99.35623931884766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456975
Iteration 2/25 | Loss: 0.00118425
Iteration 3/25 | Loss: 0.00102082
Iteration 4/25 | Loss: 0.00097939
Iteration 5/25 | Loss: 0.00097059
Iteration 6/25 | Loss: 0.00096911
Iteration 7/25 | Loss: 0.00096875
Iteration 8/25 | Loss: 0.00096875
Iteration 9/25 | Loss: 0.00096875
Iteration 10/25 | Loss: 0.00096875
Iteration 11/25 | Loss: 0.00096875
Iteration 12/25 | Loss: 0.00096875
Iteration 13/25 | Loss: 0.00096872
Iteration 14/25 | Loss: 0.00096872
Iteration 15/25 | Loss: 0.00096872
Iteration 16/25 | Loss: 0.00096872
Iteration 17/25 | Loss: 0.00096872
Iteration 18/25 | Loss: 0.00096872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009687222773209214, 0.0009687222773209214, 0.0009687222773209214, 0.0009687222773209214, 0.0009687222773209214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009687222773209214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62900937
Iteration 2/25 | Loss: 0.00170743
Iteration 3/25 | Loss: 0.00170742
Iteration 4/25 | Loss: 0.00170742
Iteration 5/25 | Loss: 0.00170742
Iteration 6/25 | Loss: 0.00170742
Iteration 7/25 | Loss: 0.00170742
Iteration 8/25 | Loss: 0.00170742
Iteration 9/25 | Loss: 0.00170742
Iteration 10/25 | Loss: 0.00170742
Iteration 11/25 | Loss: 0.00170742
Iteration 12/25 | Loss: 0.00170742
Iteration 13/25 | Loss: 0.00170742
Iteration 14/25 | Loss: 0.00170742
Iteration 15/25 | Loss: 0.00170742
Iteration 16/25 | Loss: 0.00170742
Iteration 17/25 | Loss: 0.00170742
Iteration 18/25 | Loss: 0.00170742
Iteration 19/25 | Loss: 0.00170742
Iteration 20/25 | Loss: 0.00170742
Iteration 21/25 | Loss: 0.00170742
Iteration 22/25 | Loss: 0.00170742
Iteration 23/25 | Loss: 0.00170742
Iteration 24/25 | Loss: 0.00170742
Iteration 25/25 | Loss: 0.00170742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170742
Iteration 2/1000 | Loss: 0.00002605
Iteration 3/1000 | Loss: 0.00002191
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001721
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00001715
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001713
Iteration 18/1000 | Loss: 0.00001713
Iteration 19/1000 | Loss: 0.00001710
Iteration 20/1000 | Loss: 0.00001710
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001708
Iteration 23/1000 | Loss: 0.00001708
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001703
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001701
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001700
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001695
Iteration 57/1000 | Loss: 0.00001695
Iteration 58/1000 | Loss: 0.00001695
Iteration 59/1000 | Loss: 0.00001695
Iteration 60/1000 | Loss: 0.00001695
Iteration 61/1000 | Loss: 0.00001694
Iteration 62/1000 | Loss: 0.00001694
Iteration 63/1000 | Loss: 0.00001694
Iteration 64/1000 | Loss: 0.00001694
Iteration 65/1000 | Loss: 0.00001694
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001690
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001690
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.6900996342883445e-05, 1.6900996342883445e-05, 1.6900996342883445e-05, 1.6900996342883445e-05, 1.6900996342883445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6900996342883445e-05

Optimization complete. Final v2v error: 3.5364773273468018 mm

Highest mean error: 4.1373138427734375 mm for frame 23

Lowest mean error: 2.8949217796325684 mm for frame 82

Saving results

Total time: 29.724693775177002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569993
Iteration 2/25 | Loss: 0.00124147
Iteration 3/25 | Loss: 0.00099269
Iteration 4/25 | Loss: 0.00097683
Iteration 5/25 | Loss: 0.00097250
Iteration 6/25 | Loss: 0.00097109
Iteration 7/25 | Loss: 0.00097074
Iteration 8/25 | Loss: 0.00097074
Iteration 9/25 | Loss: 0.00097074
Iteration 10/25 | Loss: 0.00097074
Iteration 11/25 | Loss: 0.00097074
Iteration 12/25 | Loss: 0.00097074
Iteration 13/25 | Loss: 0.00097074
Iteration 14/25 | Loss: 0.00097074
Iteration 15/25 | Loss: 0.00097074
Iteration 16/25 | Loss: 0.00097074
Iteration 17/25 | Loss: 0.00097074
Iteration 18/25 | Loss: 0.00097074
Iteration 19/25 | Loss: 0.00097074
Iteration 20/25 | Loss: 0.00097074
Iteration 21/25 | Loss: 0.00097074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009707356221042573, 0.0009707356221042573, 0.0009707356221042573, 0.0009707356221042573, 0.0009707356221042573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009707356221042573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90598577
Iteration 2/25 | Loss: 0.00149070
Iteration 3/25 | Loss: 0.00149069
Iteration 4/25 | Loss: 0.00149069
Iteration 5/25 | Loss: 0.00149069
Iteration 6/25 | Loss: 0.00149069
Iteration 7/25 | Loss: 0.00149069
Iteration 8/25 | Loss: 0.00149069
Iteration 9/25 | Loss: 0.00149069
Iteration 10/25 | Loss: 0.00149069
Iteration 11/25 | Loss: 0.00149069
Iteration 12/25 | Loss: 0.00149069
Iteration 13/25 | Loss: 0.00149069
Iteration 14/25 | Loss: 0.00149069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014906912110745907, 0.0014906912110745907, 0.0014906912110745907, 0.0014906912110745907, 0.0014906912110745907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014906912110745907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149069
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00002122
Iteration 4/1000 | Loss: 0.00001967
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001703
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001681
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001672
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001669
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001659
Iteration 38/1000 | Loss: 0.00001658
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001654
Iteration 48/1000 | Loss: 0.00001654
Iteration 49/1000 | Loss: 0.00001654
Iteration 50/1000 | Loss: 0.00001654
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001653
Iteration 53/1000 | Loss: 0.00001653
Iteration 54/1000 | Loss: 0.00001653
Iteration 55/1000 | Loss: 0.00001653
Iteration 56/1000 | Loss: 0.00001653
Iteration 57/1000 | Loss: 0.00001653
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001653
Iteration 62/1000 | Loss: 0.00001653
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001651
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00001650
Iteration 67/1000 | Loss: 0.00001650
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001646
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001645
Iteration 85/1000 | Loss: 0.00001645
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001634
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001633
Iteration 105/1000 | Loss: 0.00001632
Iteration 106/1000 | Loss: 0.00001632
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001626
Iteration 116/1000 | Loss: 0.00001626
Iteration 117/1000 | Loss: 0.00001626
Iteration 118/1000 | Loss: 0.00001626
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001626
Iteration 121/1000 | Loss: 0.00001626
Iteration 122/1000 | Loss: 0.00001626
Iteration 123/1000 | Loss: 0.00001626
Iteration 124/1000 | Loss: 0.00001626
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001625
Iteration 128/1000 | Loss: 0.00001625
Iteration 129/1000 | Loss: 0.00001625
Iteration 130/1000 | Loss: 0.00001625
Iteration 131/1000 | Loss: 0.00001625
Iteration 132/1000 | Loss: 0.00001624
Iteration 133/1000 | Loss: 0.00001624
Iteration 134/1000 | Loss: 0.00001624
Iteration 135/1000 | Loss: 0.00001624
Iteration 136/1000 | Loss: 0.00001624
Iteration 137/1000 | Loss: 0.00001623
Iteration 138/1000 | Loss: 0.00001623
Iteration 139/1000 | Loss: 0.00001623
Iteration 140/1000 | Loss: 0.00001623
Iteration 141/1000 | Loss: 0.00001623
Iteration 142/1000 | Loss: 0.00001623
Iteration 143/1000 | Loss: 0.00001623
Iteration 144/1000 | Loss: 0.00001623
Iteration 145/1000 | Loss: 0.00001623
Iteration 146/1000 | Loss: 0.00001623
Iteration 147/1000 | Loss: 0.00001623
Iteration 148/1000 | Loss: 0.00001622
Iteration 149/1000 | Loss: 0.00001622
Iteration 150/1000 | Loss: 0.00001622
Iteration 151/1000 | Loss: 0.00001622
Iteration 152/1000 | Loss: 0.00001622
Iteration 153/1000 | Loss: 0.00001622
Iteration 154/1000 | Loss: 0.00001622
Iteration 155/1000 | Loss: 0.00001622
Iteration 156/1000 | Loss: 0.00001622
Iteration 157/1000 | Loss: 0.00001622
Iteration 158/1000 | Loss: 0.00001622
Iteration 159/1000 | Loss: 0.00001621
Iteration 160/1000 | Loss: 0.00001621
Iteration 161/1000 | Loss: 0.00001621
Iteration 162/1000 | Loss: 0.00001621
Iteration 163/1000 | Loss: 0.00001621
Iteration 164/1000 | Loss: 0.00001621
Iteration 165/1000 | Loss: 0.00001621
Iteration 166/1000 | Loss: 0.00001621
Iteration 167/1000 | Loss: 0.00001621
Iteration 168/1000 | Loss: 0.00001621
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00001621
Iteration 172/1000 | Loss: 0.00001621
Iteration 173/1000 | Loss: 0.00001620
Iteration 174/1000 | Loss: 0.00001620
Iteration 175/1000 | Loss: 0.00001620
Iteration 176/1000 | Loss: 0.00001620
Iteration 177/1000 | Loss: 0.00001620
Iteration 178/1000 | Loss: 0.00001620
Iteration 179/1000 | Loss: 0.00001620
Iteration 180/1000 | Loss: 0.00001620
Iteration 181/1000 | Loss: 0.00001620
Iteration 182/1000 | Loss: 0.00001619
Iteration 183/1000 | Loss: 0.00001619
Iteration 184/1000 | Loss: 0.00001619
Iteration 185/1000 | Loss: 0.00001619
Iteration 186/1000 | Loss: 0.00001619
Iteration 187/1000 | Loss: 0.00001619
Iteration 188/1000 | Loss: 0.00001619
Iteration 189/1000 | Loss: 0.00001619
Iteration 190/1000 | Loss: 0.00001619
Iteration 191/1000 | Loss: 0.00001619
Iteration 192/1000 | Loss: 0.00001619
Iteration 193/1000 | Loss: 0.00001618
Iteration 194/1000 | Loss: 0.00001618
Iteration 195/1000 | Loss: 0.00001618
Iteration 196/1000 | Loss: 0.00001618
Iteration 197/1000 | Loss: 0.00001618
Iteration 198/1000 | Loss: 0.00001618
Iteration 199/1000 | Loss: 0.00001618
Iteration 200/1000 | Loss: 0.00001618
Iteration 201/1000 | Loss: 0.00001618
Iteration 202/1000 | Loss: 0.00001618
Iteration 203/1000 | Loss: 0.00001618
Iteration 204/1000 | Loss: 0.00001618
Iteration 205/1000 | Loss: 0.00001618
Iteration 206/1000 | Loss: 0.00001618
Iteration 207/1000 | Loss: 0.00001617
Iteration 208/1000 | Loss: 0.00001617
Iteration 209/1000 | Loss: 0.00001617
Iteration 210/1000 | Loss: 0.00001617
Iteration 211/1000 | Loss: 0.00001617
Iteration 212/1000 | Loss: 0.00001616
Iteration 213/1000 | Loss: 0.00001616
Iteration 214/1000 | Loss: 0.00001616
Iteration 215/1000 | Loss: 0.00001615
Iteration 216/1000 | Loss: 0.00001615
Iteration 217/1000 | Loss: 0.00001615
Iteration 218/1000 | Loss: 0.00001615
Iteration 219/1000 | Loss: 0.00001615
Iteration 220/1000 | Loss: 0.00001615
Iteration 221/1000 | Loss: 0.00001615
Iteration 222/1000 | Loss: 0.00001615
Iteration 223/1000 | Loss: 0.00001615
Iteration 224/1000 | Loss: 0.00001615
Iteration 225/1000 | Loss: 0.00001615
Iteration 226/1000 | Loss: 0.00001614
Iteration 227/1000 | Loss: 0.00001614
Iteration 228/1000 | Loss: 0.00001614
Iteration 229/1000 | Loss: 0.00001614
Iteration 230/1000 | Loss: 0.00001614
Iteration 231/1000 | Loss: 0.00001614
Iteration 232/1000 | Loss: 0.00001614
Iteration 233/1000 | Loss: 0.00001614
Iteration 234/1000 | Loss: 0.00001614
Iteration 235/1000 | Loss: 0.00001614
Iteration 236/1000 | Loss: 0.00001614
Iteration 237/1000 | Loss: 0.00001614
Iteration 238/1000 | Loss: 0.00001614
Iteration 239/1000 | Loss: 0.00001614
Iteration 240/1000 | Loss: 0.00001614
Iteration 241/1000 | Loss: 0.00001614
Iteration 242/1000 | Loss: 0.00001614
Iteration 243/1000 | Loss: 0.00001614
Iteration 244/1000 | Loss: 0.00001613
Iteration 245/1000 | Loss: 0.00001613
Iteration 246/1000 | Loss: 0.00001613
Iteration 247/1000 | Loss: 0.00001613
Iteration 248/1000 | Loss: 0.00001613
Iteration 249/1000 | Loss: 0.00001613
Iteration 250/1000 | Loss: 0.00001613
Iteration 251/1000 | Loss: 0.00001613
Iteration 252/1000 | Loss: 0.00001613
Iteration 253/1000 | Loss: 0.00001613
Iteration 254/1000 | Loss: 0.00001612
Iteration 255/1000 | Loss: 0.00001612
Iteration 256/1000 | Loss: 0.00001612
Iteration 257/1000 | Loss: 0.00001612
Iteration 258/1000 | Loss: 0.00001612
Iteration 259/1000 | Loss: 0.00001612
Iteration 260/1000 | Loss: 0.00001612
Iteration 261/1000 | Loss: 0.00001612
Iteration 262/1000 | Loss: 0.00001612
Iteration 263/1000 | Loss: 0.00001612
Iteration 264/1000 | Loss: 0.00001612
Iteration 265/1000 | Loss: 0.00001612
Iteration 266/1000 | Loss: 0.00001612
Iteration 267/1000 | Loss: 0.00001611
Iteration 268/1000 | Loss: 0.00001611
Iteration 269/1000 | Loss: 0.00001611
Iteration 270/1000 | Loss: 0.00001611
Iteration 271/1000 | Loss: 0.00001611
Iteration 272/1000 | Loss: 0.00001611
Iteration 273/1000 | Loss: 0.00001611
Iteration 274/1000 | Loss: 0.00001611
Iteration 275/1000 | Loss: 0.00001611
Iteration 276/1000 | Loss: 0.00001611
Iteration 277/1000 | Loss: 0.00001611
Iteration 278/1000 | Loss: 0.00001611
Iteration 279/1000 | Loss: 0.00001611
Iteration 280/1000 | Loss: 0.00001611
Iteration 281/1000 | Loss: 0.00001611
Iteration 282/1000 | Loss: 0.00001611
Iteration 283/1000 | Loss: 0.00001611
Iteration 284/1000 | Loss: 0.00001611
Iteration 285/1000 | Loss: 0.00001610
Iteration 286/1000 | Loss: 0.00001610
Iteration 287/1000 | Loss: 0.00001610
Iteration 288/1000 | Loss: 0.00001610
Iteration 289/1000 | Loss: 0.00001610
Iteration 290/1000 | Loss: 0.00001610
Iteration 291/1000 | Loss: 0.00001610
Iteration 292/1000 | Loss: 0.00001610
Iteration 293/1000 | Loss: 0.00001610
Iteration 294/1000 | Loss: 0.00001610
Iteration 295/1000 | Loss: 0.00001610
Iteration 296/1000 | Loss: 0.00001610
Iteration 297/1000 | Loss: 0.00001610
Iteration 298/1000 | Loss: 0.00001610
Iteration 299/1000 | Loss: 0.00001610
Iteration 300/1000 | Loss: 0.00001610
Iteration 301/1000 | Loss: 0.00001610
Iteration 302/1000 | Loss: 0.00001610
Iteration 303/1000 | Loss: 0.00001610
Iteration 304/1000 | Loss: 0.00001610
Iteration 305/1000 | Loss: 0.00001610
Iteration 306/1000 | Loss: 0.00001610
Iteration 307/1000 | Loss: 0.00001610
Iteration 308/1000 | Loss: 0.00001610
Iteration 309/1000 | Loss: 0.00001610
Iteration 310/1000 | Loss: 0.00001610
Iteration 311/1000 | Loss: 0.00001610
Iteration 312/1000 | Loss: 0.00001610
Iteration 313/1000 | Loss: 0.00001610
Iteration 314/1000 | Loss: 0.00001610
Iteration 315/1000 | Loss: 0.00001610
Iteration 316/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [1.609858009032905e-05, 1.609858009032905e-05, 1.609858009032905e-05, 1.609858009032905e-05, 1.609858009032905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.609858009032905e-05

Optimization complete. Final v2v error: 3.428879976272583 mm

Highest mean error: 4.342830181121826 mm for frame 16

Lowest mean error: 3.114389657974243 mm for frame 60

Saving results

Total time: 47.4357008934021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559009
Iteration 2/25 | Loss: 0.00118290
Iteration 3/25 | Loss: 0.00102926
Iteration 4/25 | Loss: 0.00099753
Iteration 5/25 | Loss: 0.00098810
Iteration 6/25 | Loss: 0.00098580
Iteration 7/25 | Loss: 0.00098524
Iteration 8/25 | Loss: 0.00098524
Iteration 9/25 | Loss: 0.00098524
Iteration 10/25 | Loss: 0.00098524
Iteration 11/25 | Loss: 0.00098524
Iteration 12/25 | Loss: 0.00098524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009852390503510833, 0.0009852390503510833, 0.0009852390503510833, 0.0009852390503510833, 0.0009852390503510833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009852390503510833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37347162
Iteration 2/25 | Loss: 0.00190456
Iteration 3/25 | Loss: 0.00190456
Iteration 4/25 | Loss: 0.00190456
Iteration 5/25 | Loss: 0.00190456
Iteration 6/25 | Loss: 0.00190456
Iteration 7/25 | Loss: 0.00190455
Iteration 8/25 | Loss: 0.00190455
Iteration 9/25 | Loss: 0.00190455
Iteration 10/25 | Loss: 0.00190455
Iteration 11/25 | Loss: 0.00190455
Iteration 12/25 | Loss: 0.00190455
Iteration 13/25 | Loss: 0.00190455
Iteration 14/25 | Loss: 0.00190455
Iteration 15/25 | Loss: 0.00190455
Iteration 16/25 | Loss: 0.00190455
Iteration 17/25 | Loss: 0.00190455
Iteration 18/25 | Loss: 0.00190455
Iteration 19/25 | Loss: 0.00190455
Iteration 20/25 | Loss: 0.00190455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00190455443225801, 0.00190455443225801, 0.00190455443225801, 0.00190455443225801, 0.00190455443225801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00190455443225801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190455
Iteration 2/1000 | Loss: 0.00002427
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001611
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001514
Iteration 9/1000 | Loss: 0.00001509
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001498
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001489
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001487
Iteration 19/1000 | Loss: 0.00001487
Iteration 20/1000 | Loss: 0.00001487
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001484
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001467
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001467
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001466
Iteration 68/1000 | Loss: 0.00001466
Iteration 69/1000 | Loss: 0.00001466
Iteration 70/1000 | Loss: 0.00001466
Iteration 71/1000 | Loss: 0.00001466
Iteration 72/1000 | Loss: 0.00001466
Iteration 73/1000 | Loss: 0.00001466
Iteration 74/1000 | Loss: 0.00001466
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001465
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001465
Iteration 90/1000 | Loss: 0.00001465
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001464
Iteration 93/1000 | Loss: 0.00001464
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001464
Iteration 98/1000 | Loss: 0.00001464
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.4639816981798504e-05, 1.4639816981798504e-05, 1.4639816981798504e-05, 1.4639816981798504e-05, 1.4639816981798504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4639816981798504e-05

Optimization complete. Final v2v error: 3.3182578086853027 mm

Highest mean error: 3.869588851928711 mm for frame 0

Lowest mean error: 3.062812089920044 mm for frame 119

Saving results

Total time: 28.960927724838257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002970
Iteration 2/25 | Loss: 0.00159384
Iteration 3/25 | Loss: 0.00122526
Iteration 4/25 | Loss: 0.00105683
Iteration 5/25 | Loss: 0.00103684
Iteration 6/25 | Loss: 0.00102600
Iteration 7/25 | Loss: 0.00103555
Iteration 8/25 | Loss: 0.00101963
Iteration 9/25 | Loss: 0.00101761
Iteration 10/25 | Loss: 0.00101647
Iteration 11/25 | Loss: 0.00101585
Iteration 12/25 | Loss: 0.00101929
Iteration 13/25 | Loss: 0.00101605
Iteration 14/25 | Loss: 0.00101344
Iteration 15/25 | Loss: 0.00101262
Iteration 16/25 | Loss: 0.00101237
Iteration 17/25 | Loss: 0.00101219
Iteration 18/25 | Loss: 0.00101208
Iteration 19/25 | Loss: 0.00101208
Iteration 20/25 | Loss: 0.00101208
Iteration 21/25 | Loss: 0.00101208
Iteration 22/25 | Loss: 0.00101208
Iteration 23/25 | Loss: 0.00101208
Iteration 24/25 | Loss: 0.00101208
Iteration 25/25 | Loss: 0.00101208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59760857
Iteration 2/25 | Loss: 0.00189908
Iteration 3/25 | Loss: 0.00189908
Iteration 4/25 | Loss: 0.00189908
Iteration 5/25 | Loss: 0.00189908
Iteration 6/25 | Loss: 0.00189908
Iteration 7/25 | Loss: 0.00189908
Iteration 8/25 | Loss: 0.00189907
Iteration 9/25 | Loss: 0.00189908
Iteration 10/25 | Loss: 0.00189907
Iteration 11/25 | Loss: 0.00189907
Iteration 12/25 | Loss: 0.00189907
Iteration 13/25 | Loss: 0.00189907
Iteration 14/25 | Loss: 0.00189907
Iteration 15/25 | Loss: 0.00189907
Iteration 16/25 | Loss: 0.00189907
Iteration 17/25 | Loss: 0.00189907
Iteration 18/25 | Loss: 0.00189907
Iteration 19/25 | Loss: 0.00189907
Iteration 20/25 | Loss: 0.00189907
Iteration 21/25 | Loss: 0.00189907
Iteration 22/25 | Loss: 0.00189907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018990749958902597, 0.0018990749958902597, 0.0018990749958902597, 0.0018990749958902597, 0.0018990749958902597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018990749958902597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189908
Iteration 2/1000 | Loss: 0.00003501
Iteration 3/1000 | Loss: 0.00002872
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002470
Iteration 6/1000 | Loss: 0.00002381
Iteration 7/1000 | Loss: 0.00002333
Iteration 8/1000 | Loss: 0.00002292
Iteration 9/1000 | Loss: 0.00002271
Iteration 10/1000 | Loss: 0.00002265
Iteration 11/1000 | Loss: 0.00002256
Iteration 12/1000 | Loss: 0.00002255
Iteration 13/1000 | Loss: 0.00002254
Iteration 14/1000 | Loss: 0.00002254
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002248
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002231
Iteration 19/1000 | Loss: 0.00002226
Iteration 20/1000 | Loss: 0.00002225
Iteration 21/1000 | Loss: 0.00002225
Iteration 22/1000 | Loss: 0.00002224
Iteration 23/1000 | Loss: 0.00002221
Iteration 24/1000 | Loss: 0.00002221
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002219
Iteration 27/1000 | Loss: 0.00002219
Iteration 28/1000 | Loss: 0.00002218
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002217
Iteration 32/1000 | Loss: 0.00002216
Iteration 33/1000 | Loss: 0.00002216
Iteration 34/1000 | Loss: 0.00002216
Iteration 35/1000 | Loss: 0.00002215
Iteration 36/1000 | Loss: 0.00002215
Iteration 37/1000 | Loss: 0.00002215
Iteration 38/1000 | Loss: 0.00002214
Iteration 39/1000 | Loss: 0.00002214
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002213
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002212
Iteration 44/1000 | Loss: 0.00002212
Iteration 45/1000 | Loss: 0.00002212
Iteration 46/1000 | Loss: 0.00002212
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002211
Iteration 49/1000 | Loss: 0.00002210
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002210
Iteration 52/1000 | Loss: 0.00002210
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002209
Iteration 55/1000 | Loss: 0.00002209
Iteration 56/1000 | Loss: 0.00002209
Iteration 57/1000 | Loss: 0.00002209
Iteration 58/1000 | Loss: 0.00002209
Iteration 59/1000 | Loss: 0.00002209
Iteration 60/1000 | Loss: 0.00002208
Iteration 61/1000 | Loss: 0.00002208
Iteration 62/1000 | Loss: 0.00002208
Iteration 63/1000 | Loss: 0.00002208
Iteration 64/1000 | Loss: 0.00002208
Iteration 65/1000 | Loss: 0.00002208
Iteration 66/1000 | Loss: 0.00002208
Iteration 67/1000 | Loss: 0.00002208
Iteration 68/1000 | Loss: 0.00002208
Iteration 69/1000 | Loss: 0.00002208
Iteration 70/1000 | Loss: 0.00002208
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002208
Iteration 73/1000 | Loss: 0.00002207
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002207
Iteration 77/1000 | Loss: 0.00002207
Iteration 78/1000 | Loss: 0.00002207
Iteration 79/1000 | Loss: 0.00002207
Iteration 80/1000 | Loss: 0.00002207
Iteration 81/1000 | Loss: 0.00002207
Iteration 82/1000 | Loss: 0.00002207
Iteration 83/1000 | Loss: 0.00002207
Iteration 84/1000 | Loss: 0.00002207
Iteration 85/1000 | Loss: 0.00002207
Iteration 86/1000 | Loss: 0.00002206
Iteration 87/1000 | Loss: 0.00002206
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002206
Iteration 90/1000 | Loss: 0.00002206
Iteration 91/1000 | Loss: 0.00002205
Iteration 92/1000 | Loss: 0.00002205
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002205
Iteration 96/1000 | Loss: 0.00002205
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002205
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002205
Iteration 102/1000 | Loss: 0.00002205
Iteration 103/1000 | Loss: 0.00002205
Iteration 104/1000 | Loss: 0.00002205
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002205
Iteration 108/1000 | Loss: 0.00002205
Iteration 109/1000 | Loss: 0.00002205
Iteration 110/1000 | Loss: 0.00002205
Iteration 111/1000 | Loss: 0.00002205
Iteration 112/1000 | Loss: 0.00002205
Iteration 113/1000 | Loss: 0.00002205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.205191231041681e-05, 2.205191231041681e-05, 2.205191231041681e-05, 2.205191231041681e-05, 2.205191231041681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.205191231041681e-05

Optimization complete. Final v2v error: 4.060000419616699 mm

Highest mean error: 4.563016414642334 mm for frame 217

Lowest mean error: 3.6843719482421875 mm for frame 78

Saving results

Total time: 61.3362193107605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867249
Iteration 2/25 | Loss: 0.00107741
Iteration 3/25 | Loss: 0.00095215
Iteration 4/25 | Loss: 0.00093612
Iteration 5/25 | Loss: 0.00093187
Iteration 6/25 | Loss: 0.00093092
Iteration 7/25 | Loss: 0.00093092
Iteration 8/25 | Loss: 0.00093092
Iteration 9/25 | Loss: 0.00093092
Iteration 10/25 | Loss: 0.00093092
Iteration 11/25 | Loss: 0.00093092
Iteration 12/25 | Loss: 0.00093092
Iteration 13/25 | Loss: 0.00093092
Iteration 14/25 | Loss: 0.00093092
Iteration 15/25 | Loss: 0.00093092
Iteration 16/25 | Loss: 0.00093092
Iteration 17/25 | Loss: 0.00093092
Iteration 18/25 | Loss: 0.00093092
Iteration 19/25 | Loss: 0.00093092
Iteration 20/25 | Loss: 0.00093092
Iteration 21/25 | Loss: 0.00093092
Iteration 22/25 | Loss: 0.00093092
Iteration 23/25 | Loss: 0.00093092
Iteration 24/25 | Loss: 0.00093092
Iteration 25/25 | Loss: 0.00093092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61993408
Iteration 2/25 | Loss: 0.00177528
Iteration 3/25 | Loss: 0.00177528
Iteration 4/25 | Loss: 0.00177528
Iteration 5/25 | Loss: 0.00177528
Iteration 6/25 | Loss: 0.00177528
Iteration 7/25 | Loss: 0.00177528
Iteration 8/25 | Loss: 0.00177528
Iteration 9/25 | Loss: 0.00177528
Iteration 10/25 | Loss: 0.00177528
Iteration 11/25 | Loss: 0.00177527
Iteration 12/25 | Loss: 0.00177527
Iteration 13/25 | Loss: 0.00177527
Iteration 14/25 | Loss: 0.00177527
Iteration 15/25 | Loss: 0.00177527
Iteration 16/25 | Loss: 0.00177527
Iteration 17/25 | Loss: 0.00177527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017752745188772678, 0.0017752745188772678, 0.0017752745188772678, 0.0017752745188772678, 0.0017752745188772678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017752745188772678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177527
Iteration 2/1000 | Loss: 0.00001916
Iteration 3/1000 | Loss: 0.00001587
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001342
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001336
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001334
Iteration 17/1000 | Loss: 0.00001331
Iteration 18/1000 | Loss: 0.00001331
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001326
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001326
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001324
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001324
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001323
Iteration 48/1000 | Loss: 0.00001323
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001320
Iteration 61/1000 | Loss: 0.00001320
Iteration 62/1000 | Loss: 0.00001320
Iteration 63/1000 | Loss: 0.00001320
Iteration 64/1000 | Loss: 0.00001320
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001319
Iteration 67/1000 | Loss: 0.00001319
Iteration 68/1000 | Loss: 0.00001319
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001314
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.3098176168568898e-05, 1.3098176168568898e-05, 1.3098176168568898e-05, 1.3098176168568898e-05, 1.3098176168568898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3098176168568898e-05

Optimization complete. Final v2v error: 3.1145572662353516 mm

Highest mean error: 3.3594071865081787 mm for frame 85

Lowest mean error: 2.75075101852417 mm for frame 192

Saving results

Total time: 30.764230728149414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764192
Iteration 2/25 | Loss: 0.00144688
Iteration 3/25 | Loss: 0.00110365
Iteration 4/25 | Loss: 0.00106323
Iteration 5/25 | Loss: 0.00103818
Iteration 6/25 | Loss: 0.00102661
Iteration 7/25 | Loss: 0.00104225
Iteration 8/25 | Loss: 0.00101927
Iteration 9/25 | Loss: 0.00102354
Iteration 10/25 | Loss: 0.00101730
Iteration 11/25 | Loss: 0.00101694
Iteration 12/25 | Loss: 0.00101670
Iteration 13/25 | Loss: 0.00101630
Iteration 14/25 | Loss: 0.00101612
Iteration 15/25 | Loss: 0.00101610
Iteration 16/25 | Loss: 0.00101602
Iteration 17/25 | Loss: 0.00101602
Iteration 18/25 | Loss: 0.00101601
Iteration 19/25 | Loss: 0.00101601
Iteration 20/25 | Loss: 0.00101601
Iteration 21/25 | Loss: 0.00101601
Iteration 22/25 | Loss: 0.00101601
Iteration 23/25 | Loss: 0.00101601
Iteration 24/25 | Loss: 0.00101601
Iteration 25/25 | Loss: 0.00101594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.77407598
Iteration 2/25 | Loss: 0.00230965
Iteration 3/25 | Loss: 0.00230700
Iteration 4/25 | Loss: 0.00230700
Iteration 5/25 | Loss: 0.00230700
Iteration 6/25 | Loss: 0.00230700
Iteration 7/25 | Loss: 0.00230700
Iteration 8/25 | Loss: 0.00230700
Iteration 9/25 | Loss: 0.00230700
Iteration 10/25 | Loss: 0.00230700
Iteration 11/25 | Loss: 0.00230700
Iteration 12/25 | Loss: 0.00230700
Iteration 13/25 | Loss: 0.00230700
Iteration 14/25 | Loss: 0.00230700
Iteration 15/25 | Loss: 0.00230700
Iteration 16/25 | Loss: 0.00230700
Iteration 17/25 | Loss: 0.00230700
Iteration 18/25 | Loss: 0.00230700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00230700196698308, 0.00230700196698308, 0.00230700196698308, 0.00230700196698308, 0.00230700196698308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00230700196698308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230700
Iteration 2/1000 | Loss: 0.00003850
Iteration 3/1000 | Loss: 0.00013027
Iteration 4/1000 | Loss: 0.00009179
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002348
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00020747
Iteration 10/1000 | Loss: 0.00021078
Iteration 11/1000 | Loss: 0.00026600
Iteration 12/1000 | Loss: 0.00021058
Iteration 13/1000 | Loss: 0.00004349
Iteration 14/1000 | Loss: 0.00038159
Iteration 15/1000 | Loss: 0.00005778
Iteration 16/1000 | Loss: 0.00003549
Iteration 17/1000 | Loss: 0.00002429
Iteration 18/1000 | Loss: 0.00002251
Iteration 19/1000 | Loss: 0.00002332
Iteration 20/1000 | Loss: 0.00002826
Iteration 21/1000 | Loss: 0.00002304
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00022177
Iteration 24/1000 | Loss: 0.00020549
Iteration 25/1000 | Loss: 0.00003253
Iteration 26/1000 | Loss: 0.00021034
Iteration 27/1000 | Loss: 0.00020698
Iteration 28/1000 | Loss: 0.00017425
Iteration 29/1000 | Loss: 0.00020726
Iteration 30/1000 | Loss: 0.00002870
Iteration 31/1000 | Loss: 0.00002625
Iteration 32/1000 | Loss: 0.00020187
Iteration 33/1000 | Loss: 0.00017799
Iteration 34/1000 | Loss: 0.00025504
Iteration 35/1000 | Loss: 0.00018795
Iteration 36/1000 | Loss: 0.00022795
Iteration 37/1000 | Loss: 0.00018351
Iteration 38/1000 | Loss: 0.00021192
Iteration 39/1000 | Loss: 0.00018391
Iteration 40/1000 | Loss: 0.00017932
Iteration 41/1000 | Loss: 0.00013573
Iteration 42/1000 | Loss: 0.00016826
Iteration 43/1000 | Loss: 0.00018122
Iteration 44/1000 | Loss: 0.00016741
Iteration 45/1000 | Loss: 0.00020514
Iteration 46/1000 | Loss: 0.00016647
Iteration 47/1000 | Loss: 0.00023118
Iteration 48/1000 | Loss: 0.00016277
Iteration 49/1000 | Loss: 0.00020608
Iteration 50/1000 | Loss: 0.00016268
Iteration 51/1000 | Loss: 0.00016300
Iteration 52/1000 | Loss: 0.00002956
Iteration 53/1000 | Loss: 0.00002259
Iteration 54/1000 | Loss: 0.00002234
Iteration 55/1000 | Loss: 0.00022210
Iteration 56/1000 | Loss: 0.00017793
Iteration 57/1000 | Loss: 0.00002972
Iteration 58/1000 | Loss: 0.00002572
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00020121
Iteration 61/1000 | Loss: 0.00027060
Iteration 62/1000 | Loss: 0.00003090
Iteration 63/1000 | Loss: 0.00002496
Iteration 64/1000 | Loss: 0.00002055
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001958
Iteration 67/1000 | Loss: 0.00001927
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00004018
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001871
Iteration 75/1000 | Loss: 0.00001871
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001869
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001868
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00001868
Iteration 93/1000 | Loss: 0.00001868
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001868
Iteration 96/1000 | Loss: 0.00001867
Iteration 97/1000 | Loss: 0.00001867
Iteration 98/1000 | Loss: 0.00001867
Iteration 99/1000 | Loss: 0.00001867
Iteration 100/1000 | Loss: 0.00001867
Iteration 101/1000 | Loss: 0.00001867
Iteration 102/1000 | Loss: 0.00001867
Iteration 103/1000 | Loss: 0.00001867
Iteration 104/1000 | Loss: 0.00001867
Iteration 105/1000 | Loss: 0.00001867
Iteration 106/1000 | Loss: 0.00001867
Iteration 107/1000 | Loss: 0.00001866
Iteration 108/1000 | Loss: 0.00001866
Iteration 109/1000 | Loss: 0.00001866
Iteration 110/1000 | Loss: 0.00001866
Iteration 111/1000 | Loss: 0.00001866
Iteration 112/1000 | Loss: 0.00001866
Iteration 113/1000 | Loss: 0.00001866
Iteration 114/1000 | Loss: 0.00001866
Iteration 115/1000 | Loss: 0.00001866
Iteration 116/1000 | Loss: 0.00001866
Iteration 117/1000 | Loss: 0.00001866
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001865
Iteration 120/1000 | Loss: 0.00001865
Iteration 121/1000 | Loss: 0.00001865
Iteration 122/1000 | Loss: 0.00001865
Iteration 123/1000 | Loss: 0.00001865
Iteration 124/1000 | Loss: 0.00001865
Iteration 125/1000 | Loss: 0.00001865
Iteration 126/1000 | Loss: 0.00001865
Iteration 127/1000 | Loss: 0.00001865
Iteration 128/1000 | Loss: 0.00001865
Iteration 129/1000 | Loss: 0.00001864
Iteration 130/1000 | Loss: 0.00001864
Iteration 131/1000 | Loss: 0.00001864
Iteration 132/1000 | Loss: 0.00001864
Iteration 133/1000 | Loss: 0.00001864
Iteration 134/1000 | Loss: 0.00001864
Iteration 135/1000 | Loss: 0.00001864
Iteration 136/1000 | Loss: 0.00001864
Iteration 137/1000 | Loss: 0.00001864
Iteration 138/1000 | Loss: 0.00001864
Iteration 139/1000 | Loss: 0.00001863
Iteration 140/1000 | Loss: 0.00001863
Iteration 141/1000 | Loss: 0.00001863
Iteration 142/1000 | Loss: 0.00001863
Iteration 143/1000 | Loss: 0.00001863
Iteration 144/1000 | Loss: 0.00001863
Iteration 145/1000 | Loss: 0.00001863
Iteration 146/1000 | Loss: 0.00001863
Iteration 147/1000 | Loss: 0.00001862
Iteration 148/1000 | Loss: 0.00001862
Iteration 149/1000 | Loss: 0.00001862
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001861
Iteration 152/1000 | Loss: 0.00001861
Iteration 153/1000 | Loss: 0.00002752
Iteration 154/1000 | Loss: 0.00001889
Iteration 155/1000 | Loss: 0.00001859
Iteration 156/1000 | Loss: 0.00001858
Iteration 157/1000 | Loss: 0.00001858
Iteration 158/1000 | Loss: 0.00001858
Iteration 159/1000 | Loss: 0.00001858
Iteration 160/1000 | Loss: 0.00001858
Iteration 161/1000 | Loss: 0.00001857
Iteration 162/1000 | Loss: 0.00002060
Iteration 163/1000 | Loss: 0.00001980
Iteration 164/1000 | Loss: 0.00001858
Iteration 165/1000 | Loss: 0.00001909
Iteration 166/1000 | Loss: 0.00001855
Iteration 167/1000 | Loss: 0.00001855
Iteration 168/1000 | Loss: 0.00001854
Iteration 169/1000 | Loss: 0.00001854
Iteration 170/1000 | Loss: 0.00001854
Iteration 171/1000 | Loss: 0.00001854
Iteration 172/1000 | Loss: 0.00001854
Iteration 173/1000 | Loss: 0.00001854
Iteration 174/1000 | Loss: 0.00001854
Iteration 175/1000 | Loss: 0.00001854
Iteration 176/1000 | Loss: 0.00001854
Iteration 177/1000 | Loss: 0.00001854
Iteration 178/1000 | Loss: 0.00001854
Iteration 179/1000 | Loss: 0.00001854
Iteration 180/1000 | Loss: 0.00001854
Iteration 181/1000 | Loss: 0.00001854
Iteration 182/1000 | Loss: 0.00001854
Iteration 183/1000 | Loss: 0.00001854
Iteration 184/1000 | Loss: 0.00001854
Iteration 185/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.8543933038017713e-05, 1.8543933038017713e-05, 1.8543933038017713e-05, 1.8543933038017713e-05, 1.8543933038017713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8543933038017713e-05

Optimization complete. Final v2v error: 3.648954153060913 mm

Highest mean error: 5.953650951385498 mm for frame 88

Lowest mean error: 3.0822947025299072 mm for frame 120

Saving results

Total time: 155.60143566131592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886925
Iteration 2/25 | Loss: 0.00147669
Iteration 3/25 | Loss: 0.00118099
Iteration 4/25 | Loss: 0.00112314
Iteration 5/25 | Loss: 0.00109202
Iteration 6/25 | Loss: 0.00108442
Iteration 7/25 | Loss: 0.00107554
Iteration 8/25 | Loss: 0.00107157
Iteration 9/25 | Loss: 0.00107053
Iteration 10/25 | Loss: 0.00106985
Iteration 11/25 | Loss: 0.00106925
Iteration 12/25 | Loss: 0.00106882
Iteration 13/25 | Loss: 0.00106866
Iteration 14/25 | Loss: 0.00106848
Iteration 15/25 | Loss: 0.00106831
Iteration 16/25 | Loss: 0.00106816
Iteration 17/25 | Loss: 0.00106800
Iteration 18/25 | Loss: 0.00106787
Iteration 19/25 | Loss: 0.00106774
Iteration 20/25 | Loss: 0.00106770
Iteration 21/25 | Loss: 0.00106770
Iteration 22/25 | Loss: 0.00106770
Iteration 23/25 | Loss: 0.00106769
Iteration 24/25 | Loss: 0.00106769
Iteration 25/25 | Loss: 0.00106769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05446482
Iteration 2/25 | Loss: 0.00220849
Iteration 3/25 | Loss: 0.00212386
Iteration 4/25 | Loss: 0.00212386
Iteration 5/25 | Loss: 0.00212386
Iteration 6/25 | Loss: 0.00212386
Iteration 7/25 | Loss: 0.00212386
Iteration 8/25 | Loss: 0.00212386
Iteration 9/25 | Loss: 0.00212386
Iteration 10/25 | Loss: 0.00212386
Iteration 11/25 | Loss: 0.00212386
Iteration 12/25 | Loss: 0.00212386
Iteration 13/25 | Loss: 0.00212386
Iteration 14/25 | Loss: 0.00212386
Iteration 15/25 | Loss: 0.00212386
Iteration 16/25 | Loss: 0.00212386
Iteration 17/25 | Loss: 0.00212386
Iteration 18/25 | Loss: 0.00212386
Iteration 19/25 | Loss: 0.00212386
Iteration 20/25 | Loss: 0.00212386
Iteration 21/25 | Loss: 0.00212386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021238578483462334, 0.0021238578483462334, 0.0021238578483462334, 0.0021238578483462334, 0.0021238578483462334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021238578483462334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212386
Iteration 2/1000 | Loss: 0.00011401
Iteration 3/1000 | Loss: 0.00027797
Iteration 4/1000 | Loss: 0.00005735
Iteration 5/1000 | Loss: 0.00013318
Iteration 6/1000 | Loss: 0.00190436
Iteration 7/1000 | Loss: 0.00103246
Iteration 8/1000 | Loss: 0.00016998
Iteration 9/1000 | Loss: 0.00009230
Iteration 10/1000 | Loss: 0.00175020
Iteration 11/1000 | Loss: 0.00056635
Iteration 12/1000 | Loss: 0.00134909
Iteration 13/1000 | Loss: 0.00084232
Iteration 14/1000 | Loss: 0.00050582
Iteration 15/1000 | Loss: 0.00007331
Iteration 16/1000 | Loss: 0.00006378
Iteration 17/1000 | Loss: 0.00004241
Iteration 18/1000 | Loss: 0.00003655
Iteration 19/1000 | Loss: 0.00004743
Iteration 20/1000 | Loss: 0.00003799
Iteration 21/1000 | Loss: 0.00005205
Iteration 22/1000 | Loss: 0.00006524
Iteration 23/1000 | Loss: 0.00002444
Iteration 24/1000 | Loss: 0.00003938
Iteration 25/1000 | Loss: 0.00002296
Iteration 26/1000 | Loss: 0.00002160
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001751
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001713
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001709
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.7094525901484303e-05, 1.7094525901484303e-05, 1.7094525901484303e-05, 1.7094525901484303e-05, 1.7094525901484303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7094525901484303e-05

Optimization complete. Final v2v error: 3.6149184703826904 mm

Highest mean error: 4.508461952209473 mm for frame 49

Lowest mean error: 3.0958659648895264 mm for frame 88

Saving results

Total time: 91.21808910369873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128338
Iteration 2/25 | Loss: 0.00189648
Iteration 3/25 | Loss: 0.00152773
Iteration 4/25 | Loss: 0.00132567
Iteration 5/25 | Loss: 0.00154747
Iteration 6/25 | Loss: 0.00128466
Iteration 7/25 | Loss: 0.00119267
Iteration 8/25 | Loss: 0.00115979
Iteration 9/25 | Loss: 0.00116835
Iteration 10/25 | Loss: 0.00108206
Iteration 11/25 | Loss: 0.00105228
Iteration 12/25 | Loss: 0.00104373
Iteration 13/25 | Loss: 0.00102733
Iteration 14/25 | Loss: 0.00102662
Iteration 15/25 | Loss: 0.00101991
Iteration 16/25 | Loss: 0.00104606
Iteration 17/25 | Loss: 0.00100570
Iteration 18/25 | Loss: 0.00099818
Iteration 19/25 | Loss: 0.00099520
Iteration 20/25 | Loss: 0.00100058
Iteration 21/25 | Loss: 0.00099996
Iteration 22/25 | Loss: 0.00100184
Iteration 23/25 | Loss: 0.00099907
Iteration 24/25 | Loss: 0.00098936
Iteration 25/25 | Loss: 0.00100232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67186594
Iteration 2/25 | Loss: 0.00640024
Iteration 3/25 | Loss: 0.00309816
Iteration 4/25 | Loss: 0.00309592
Iteration 5/25 | Loss: 0.00309592
Iteration 6/25 | Loss: 0.00309592
Iteration 7/25 | Loss: 0.00309592
Iteration 8/25 | Loss: 0.00309592
Iteration 9/25 | Loss: 0.00309592
Iteration 10/25 | Loss: 0.00309592
Iteration 11/25 | Loss: 0.00309592
Iteration 12/25 | Loss: 0.00309592
Iteration 13/25 | Loss: 0.00309592
Iteration 14/25 | Loss: 0.00309592
Iteration 15/25 | Loss: 0.00309592
Iteration 16/25 | Loss: 0.00309592
Iteration 17/25 | Loss: 0.00309592
Iteration 18/25 | Loss: 0.00309592
Iteration 19/25 | Loss: 0.00309592
Iteration 20/25 | Loss: 0.00309592
Iteration 21/25 | Loss: 0.00309592
Iteration 22/25 | Loss: 0.00309592
Iteration 23/25 | Loss: 0.00309592
Iteration 24/25 | Loss: 0.00309592
Iteration 25/25 | Loss: 0.00309592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309592
Iteration 2/1000 | Loss: 0.00058188
Iteration 3/1000 | Loss: 0.00016835
Iteration 4/1000 | Loss: 0.00033293
Iteration 5/1000 | Loss: 0.00117869
Iteration 6/1000 | Loss: 0.00062760
Iteration 7/1000 | Loss: 0.00093173
Iteration 8/1000 | Loss: 0.00012825
Iteration 9/1000 | Loss: 0.00056363
Iteration 10/1000 | Loss: 0.00077842
Iteration 11/1000 | Loss: 0.00055790
Iteration 12/1000 | Loss: 0.00035537
Iteration 13/1000 | Loss: 0.00023500
Iteration 14/1000 | Loss: 0.00024199
Iteration 15/1000 | Loss: 0.00020615
Iteration 16/1000 | Loss: 0.00034480
Iteration 17/1000 | Loss: 0.00017757
Iteration 18/1000 | Loss: 0.00021192
Iteration 19/1000 | Loss: 0.00012078
Iteration 20/1000 | Loss: 0.00022098
Iteration 21/1000 | Loss: 0.00010886
Iteration 22/1000 | Loss: 0.00009573
Iteration 23/1000 | Loss: 0.00040871
Iteration 24/1000 | Loss: 0.00036489
Iteration 25/1000 | Loss: 0.00030029
Iteration 26/1000 | Loss: 0.00025311
Iteration 27/1000 | Loss: 0.00028859
Iteration 28/1000 | Loss: 0.00038102
Iteration 29/1000 | Loss: 0.00043220
Iteration 30/1000 | Loss: 0.00041620
Iteration 31/1000 | Loss: 0.00041933
Iteration 32/1000 | Loss: 0.00037870
Iteration 33/1000 | Loss: 0.00029104
Iteration 34/1000 | Loss: 0.00044675
Iteration 35/1000 | Loss: 0.00041265
Iteration 36/1000 | Loss: 0.00026296
Iteration 37/1000 | Loss: 0.00030300
Iteration 38/1000 | Loss: 0.00024720
Iteration 39/1000 | Loss: 0.00032738
Iteration 40/1000 | Loss: 0.00022356
Iteration 41/1000 | Loss: 0.00035396
Iteration 42/1000 | Loss: 0.00024489
Iteration 43/1000 | Loss: 0.00013846
Iteration 44/1000 | Loss: 0.00018257
Iteration 45/1000 | Loss: 0.00007231
Iteration 46/1000 | Loss: 0.00028290
Iteration 47/1000 | Loss: 0.00016064
Iteration 48/1000 | Loss: 0.00042176
Iteration 49/1000 | Loss: 0.00044326
Iteration 50/1000 | Loss: 0.00039656
Iteration 51/1000 | Loss: 0.00024739
Iteration 52/1000 | Loss: 0.00017353
Iteration 53/1000 | Loss: 0.00037583
Iteration 54/1000 | Loss: 0.00037625
Iteration 55/1000 | Loss: 0.00010885
Iteration 56/1000 | Loss: 0.00008729
Iteration 57/1000 | Loss: 0.00020496
Iteration 58/1000 | Loss: 0.00020267
Iteration 59/1000 | Loss: 0.00020227
Iteration 60/1000 | Loss: 0.00027395
Iteration 61/1000 | Loss: 0.00007340
Iteration 62/1000 | Loss: 0.00023469
Iteration 63/1000 | Loss: 0.00026055
Iteration 64/1000 | Loss: 0.00011220
Iteration 65/1000 | Loss: 0.00007577
Iteration 66/1000 | Loss: 0.00009566
Iteration 67/1000 | Loss: 0.00018965
Iteration 68/1000 | Loss: 0.00024681
Iteration 69/1000 | Loss: 0.00019661
Iteration 70/1000 | Loss: 0.00028056
Iteration 71/1000 | Loss: 0.00025878
Iteration 72/1000 | Loss: 0.00017554
Iteration 73/1000 | Loss: 0.00006745
Iteration 74/1000 | Loss: 0.00021003
Iteration 75/1000 | Loss: 0.00020767
Iteration 76/1000 | Loss: 0.00025276
Iteration 77/1000 | Loss: 0.00015346
Iteration 78/1000 | Loss: 0.00008487
Iteration 79/1000 | Loss: 0.00004734
Iteration 80/1000 | Loss: 0.00007113
Iteration 81/1000 | Loss: 0.00019756
Iteration 82/1000 | Loss: 0.00015010
Iteration 83/1000 | Loss: 0.00022891
Iteration 84/1000 | Loss: 0.00109765
Iteration 85/1000 | Loss: 0.00081652
Iteration 86/1000 | Loss: 0.00006407
Iteration 87/1000 | Loss: 0.00109577
Iteration 88/1000 | Loss: 0.00085387
Iteration 89/1000 | Loss: 0.00037572
Iteration 90/1000 | Loss: 0.00022941
Iteration 91/1000 | Loss: 0.00020462
Iteration 92/1000 | Loss: 0.00014194
Iteration 93/1000 | Loss: 0.00014099
Iteration 94/1000 | Loss: 0.00011412
Iteration 95/1000 | Loss: 0.00040538
Iteration 96/1000 | Loss: 0.00017453
Iteration 97/1000 | Loss: 0.00015523
Iteration 98/1000 | Loss: 0.00010954
Iteration 99/1000 | Loss: 0.00013663
Iteration 100/1000 | Loss: 0.00020393
Iteration 101/1000 | Loss: 0.00020117
Iteration 102/1000 | Loss: 0.00017538
Iteration 103/1000 | Loss: 0.00017007
Iteration 104/1000 | Loss: 0.00009297
Iteration 105/1000 | Loss: 0.00023731
Iteration 106/1000 | Loss: 0.00014109
Iteration 107/1000 | Loss: 0.00006467
Iteration 108/1000 | Loss: 0.00023521
Iteration 109/1000 | Loss: 0.00018587
Iteration 110/1000 | Loss: 0.00007287
Iteration 111/1000 | Loss: 0.00002974
Iteration 112/1000 | Loss: 0.00002894
Iteration 113/1000 | Loss: 0.00029273
Iteration 114/1000 | Loss: 0.00018163
Iteration 115/1000 | Loss: 0.00009186
Iteration 116/1000 | Loss: 0.00020035
Iteration 117/1000 | Loss: 0.00027758
Iteration 118/1000 | Loss: 0.00021592
Iteration 119/1000 | Loss: 0.00022337
Iteration 120/1000 | Loss: 0.00013821
Iteration 121/1000 | Loss: 0.00018928
Iteration 122/1000 | Loss: 0.00010871
Iteration 123/1000 | Loss: 0.00020263
Iteration 124/1000 | Loss: 0.00019737
Iteration 125/1000 | Loss: 0.00018495
Iteration 126/1000 | Loss: 0.00016853
Iteration 127/1000 | Loss: 0.00007344
Iteration 128/1000 | Loss: 0.00021860
Iteration 129/1000 | Loss: 0.00023416
Iteration 130/1000 | Loss: 0.00018172
Iteration 131/1000 | Loss: 0.00011372
Iteration 132/1000 | Loss: 0.00019538
Iteration 133/1000 | Loss: 0.00018119
Iteration 134/1000 | Loss: 0.00002837
Iteration 135/1000 | Loss: 0.00020924
Iteration 136/1000 | Loss: 0.00006612
Iteration 137/1000 | Loss: 0.00015447
Iteration 138/1000 | Loss: 0.00004352
Iteration 139/1000 | Loss: 0.00006167
Iteration 140/1000 | Loss: 0.00017630
Iteration 141/1000 | Loss: 0.00038124
Iteration 142/1000 | Loss: 0.00026962
Iteration 143/1000 | Loss: 0.00003655
Iteration 144/1000 | Loss: 0.00003253
Iteration 145/1000 | Loss: 0.00033728
Iteration 146/1000 | Loss: 0.00023950
Iteration 147/1000 | Loss: 0.00013801
Iteration 148/1000 | Loss: 0.00042544
Iteration 149/1000 | Loss: 0.00003778
Iteration 150/1000 | Loss: 0.00026887
Iteration 151/1000 | Loss: 0.00011668
Iteration 152/1000 | Loss: 0.00003585
Iteration 153/1000 | Loss: 0.00003390
Iteration 154/1000 | Loss: 0.00003303
Iteration 155/1000 | Loss: 0.00011429
Iteration 156/1000 | Loss: 0.00011747
Iteration 157/1000 | Loss: 0.00046027
Iteration 158/1000 | Loss: 0.00010358
Iteration 159/1000 | Loss: 0.00028518
Iteration 160/1000 | Loss: 0.00017575
Iteration 161/1000 | Loss: 0.00006396
Iteration 162/1000 | Loss: 0.00027888
Iteration 163/1000 | Loss: 0.00049733
Iteration 164/1000 | Loss: 0.00023279
Iteration 165/1000 | Loss: 0.00023500
Iteration 166/1000 | Loss: 0.00020078
Iteration 167/1000 | Loss: 0.00026634
Iteration 168/1000 | Loss: 0.00018053
Iteration 169/1000 | Loss: 0.00022462
Iteration 170/1000 | Loss: 0.00011181
Iteration 171/1000 | Loss: 0.00020396
Iteration 172/1000 | Loss: 0.00009915
Iteration 173/1000 | Loss: 0.00016870
Iteration 174/1000 | Loss: 0.00030260
Iteration 175/1000 | Loss: 0.00013270
Iteration 176/1000 | Loss: 0.00048159
Iteration 177/1000 | Loss: 0.00013162
Iteration 178/1000 | Loss: 0.00040469
Iteration 179/1000 | Loss: 0.00013977
Iteration 180/1000 | Loss: 0.00015244
Iteration 181/1000 | Loss: 0.00012269
Iteration 182/1000 | Loss: 0.00011343
Iteration 183/1000 | Loss: 0.00021308
Iteration 184/1000 | Loss: 0.00044245
Iteration 185/1000 | Loss: 0.00039229
Iteration 186/1000 | Loss: 0.00038185
Iteration 187/1000 | Loss: 0.00017593
Iteration 188/1000 | Loss: 0.00031029
Iteration 189/1000 | Loss: 0.00004353
Iteration 190/1000 | Loss: 0.00029479
Iteration 191/1000 | Loss: 0.00021318
Iteration 192/1000 | Loss: 0.00029095
Iteration 193/1000 | Loss: 0.00040411
Iteration 194/1000 | Loss: 0.00004679
Iteration 195/1000 | Loss: 0.00003594
Iteration 196/1000 | Loss: 0.00003299
Iteration 197/1000 | Loss: 0.00003124
Iteration 198/1000 | Loss: 0.00004088
Iteration 199/1000 | Loss: 0.00003612
Iteration 200/1000 | Loss: 0.00004230
Iteration 201/1000 | Loss: 0.00002934
Iteration 202/1000 | Loss: 0.00002707
Iteration 203/1000 | Loss: 0.00002647
Iteration 204/1000 | Loss: 0.00002612
Iteration 205/1000 | Loss: 0.00002603
Iteration 206/1000 | Loss: 0.00002578
Iteration 207/1000 | Loss: 0.00002575
Iteration 208/1000 | Loss: 0.00002575
Iteration 209/1000 | Loss: 0.00002574
Iteration 210/1000 | Loss: 0.00002574
Iteration 211/1000 | Loss: 0.00002573
Iteration 212/1000 | Loss: 0.00002572
Iteration 213/1000 | Loss: 0.00002566
Iteration 214/1000 | Loss: 0.00002560
Iteration 215/1000 | Loss: 0.00002552
Iteration 216/1000 | Loss: 0.00002544
Iteration 217/1000 | Loss: 0.00002544
Iteration 218/1000 | Loss: 0.00002543
Iteration 219/1000 | Loss: 0.00002542
Iteration 220/1000 | Loss: 0.00002542
Iteration 221/1000 | Loss: 0.00002541
Iteration 222/1000 | Loss: 0.00002541
Iteration 223/1000 | Loss: 0.00002540
Iteration 224/1000 | Loss: 0.00002539
Iteration 225/1000 | Loss: 0.00002539
Iteration 226/1000 | Loss: 0.00002539
Iteration 227/1000 | Loss: 0.00002538
Iteration 228/1000 | Loss: 0.00002538
Iteration 229/1000 | Loss: 0.00002537
Iteration 230/1000 | Loss: 0.00002533
Iteration 231/1000 | Loss: 0.00002524
Iteration 232/1000 | Loss: 0.00002519
Iteration 233/1000 | Loss: 0.00002518
Iteration 234/1000 | Loss: 0.00002517
Iteration 235/1000 | Loss: 0.00002517
Iteration 236/1000 | Loss: 0.00002517
Iteration 237/1000 | Loss: 0.00002516
Iteration 238/1000 | Loss: 0.00002515
Iteration 239/1000 | Loss: 0.00002514
Iteration 240/1000 | Loss: 0.00002514
Iteration 241/1000 | Loss: 0.00002514
Iteration 242/1000 | Loss: 0.00002514
Iteration 243/1000 | Loss: 0.00002514
Iteration 244/1000 | Loss: 0.00002514
Iteration 245/1000 | Loss: 0.00002514
Iteration 246/1000 | Loss: 0.00002514
Iteration 247/1000 | Loss: 0.00002514
Iteration 248/1000 | Loss: 0.00002514
Iteration 249/1000 | Loss: 0.00002514
Iteration 250/1000 | Loss: 0.00002514
Iteration 251/1000 | Loss: 0.00002514
Iteration 252/1000 | Loss: 0.00002513
Iteration 253/1000 | Loss: 0.00002513
Iteration 254/1000 | Loss: 0.00002513
Iteration 255/1000 | Loss: 0.00002513
Iteration 256/1000 | Loss: 0.00002513
Iteration 257/1000 | Loss: 0.00002513
Iteration 258/1000 | Loss: 0.00002513
Iteration 259/1000 | Loss: 0.00002513
Iteration 260/1000 | Loss: 0.00002513
Iteration 261/1000 | Loss: 0.00002513
Iteration 262/1000 | Loss: 0.00002513
Iteration 263/1000 | Loss: 0.00002512
Iteration 264/1000 | Loss: 0.00002512
Iteration 265/1000 | Loss: 0.00002512
Iteration 266/1000 | Loss: 0.00002512
Iteration 267/1000 | Loss: 0.00002512
Iteration 268/1000 | Loss: 0.00002512
Iteration 269/1000 | Loss: 0.00002512
Iteration 270/1000 | Loss: 0.00002512
Iteration 271/1000 | Loss: 0.00002511
Iteration 272/1000 | Loss: 0.00002511
Iteration 273/1000 | Loss: 0.00002511
Iteration 274/1000 | Loss: 0.00002511
Iteration 275/1000 | Loss: 0.00002510
Iteration 276/1000 | Loss: 0.00002510
Iteration 277/1000 | Loss: 0.00002510
Iteration 278/1000 | Loss: 0.00002510
Iteration 279/1000 | Loss: 0.00002510
Iteration 280/1000 | Loss: 0.00002510
Iteration 281/1000 | Loss: 0.00002510
Iteration 282/1000 | Loss: 0.00002510
Iteration 283/1000 | Loss: 0.00002510
Iteration 284/1000 | Loss: 0.00002510
Iteration 285/1000 | Loss: 0.00002510
Iteration 286/1000 | Loss: 0.00002510
Iteration 287/1000 | Loss: 0.00002510
Iteration 288/1000 | Loss: 0.00002510
Iteration 289/1000 | Loss: 0.00002510
Iteration 290/1000 | Loss: 0.00002510
Iteration 291/1000 | Loss: 0.00002510
Iteration 292/1000 | Loss: 0.00002510
Iteration 293/1000 | Loss: 0.00002510
Iteration 294/1000 | Loss: 0.00002510
Iteration 295/1000 | Loss: 0.00002510
Iteration 296/1000 | Loss: 0.00002510
Iteration 297/1000 | Loss: 0.00002510
Iteration 298/1000 | Loss: 0.00002510
Iteration 299/1000 | Loss: 0.00002510
Iteration 300/1000 | Loss: 0.00002509
Iteration 301/1000 | Loss: 0.00002509
Iteration 302/1000 | Loss: 0.00002509
Iteration 303/1000 | Loss: 0.00002509
Iteration 304/1000 | Loss: 0.00002509
Iteration 305/1000 | Loss: 0.00002509
Iteration 306/1000 | Loss: 0.00002509
Iteration 307/1000 | Loss: 0.00002509
Iteration 308/1000 | Loss: 0.00002509
Iteration 309/1000 | Loss: 0.00002509
Iteration 310/1000 | Loss: 0.00002509
Iteration 311/1000 | Loss: 0.00002509
Iteration 312/1000 | Loss: 0.00002509
Iteration 313/1000 | Loss: 0.00002509
Iteration 314/1000 | Loss: 0.00002509
Iteration 315/1000 | Loss: 0.00002509
Iteration 316/1000 | Loss: 0.00002509
Iteration 317/1000 | Loss: 0.00002509
Iteration 318/1000 | Loss: 0.00002509
Iteration 319/1000 | Loss: 0.00002509
Iteration 320/1000 | Loss: 0.00002509
Iteration 321/1000 | Loss: 0.00002509
Iteration 322/1000 | Loss: 0.00002509
Iteration 323/1000 | Loss: 0.00002509
Iteration 324/1000 | Loss: 0.00002509
Iteration 325/1000 | Loss: 0.00002509
Iteration 326/1000 | Loss: 0.00002509
Iteration 327/1000 | Loss: 0.00002509
Iteration 328/1000 | Loss: 0.00002509
Iteration 329/1000 | Loss: 0.00002509
Iteration 330/1000 | Loss: 0.00002509
Iteration 331/1000 | Loss: 0.00002509
Iteration 332/1000 | Loss: 0.00002509
Iteration 333/1000 | Loss: 0.00002509
Iteration 334/1000 | Loss: 0.00002509
Iteration 335/1000 | Loss: 0.00002509
Iteration 336/1000 | Loss: 0.00002509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [2.50926041189814e-05, 2.50926041189814e-05, 2.50926041189814e-05, 2.50926041189814e-05, 2.50926041189814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.50926041189814e-05

Optimization complete. Final v2v error: 3.593532085418701 mm

Highest mean error: 12.651177406311035 mm for frame 91

Lowest mean error: 2.864320993423462 mm for frame 82

Saving results

Total time: 339.76611137390137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962132
Iteration 2/25 | Loss: 0.00178851
Iteration 3/25 | Loss: 0.00116513
Iteration 4/25 | Loss: 0.00112605
Iteration 5/25 | Loss: 0.00107055
Iteration 6/25 | Loss: 0.00104374
Iteration 7/25 | Loss: 0.00102981
Iteration 8/25 | Loss: 0.00102289
Iteration 9/25 | Loss: 0.00100357
Iteration 10/25 | Loss: 0.00099280
Iteration 11/25 | Loss: 0.00098365
Iteration 12/25 | Loss: 0.00098607
Iteration 13/25 | Loss: 0.00098350
Iteration 14/25 | Loss: 0.00097948
Iteration 15/25 | Loss: 0.00097723
Iteration 16/25 | Loss: 0.00097651
Iteration 17/25 | Loss: 0.00097634
Iteration 18/25 | Loss: 0.00097631
Iteration 19/25 | Loss: 0.00097631
Iteration 20/25 | Loss: 0.00097631
Iteration 21/25 | Loss: 0.00097631
Iteration 22/25 | Loss: 0.00097631
Iteration 23/25 | Loss: 0.00097631
Iteration 24/25 | Loss: 0.00097631
Iteration 25/25 | Loss: 0.00097630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42613888
Iteration 2/25 | Loss: 0.00184301
Iteration 3/25 | Loss: 0.00180206
Iteration 4/25 | Loss: 0.00180206
Iteration 5/25 | Loss: 0.00180206
Iteration 6/25 | Loss: 0.00180206
Iteration 7/25 | Loss: 0.00180205
Iteration 8/25 | Loss: 0.00180205
Iteration 9/25 | Loss: 0.00180205
Iteration 10/25 | Loss: 0.00180205
Iteration 11/25 | Loss: 0.00180205
Iteration 12/25 | Loss: 0.00180205
Iteration 13/25 | Loss: 0.00180205
Iteration 14/25 | Loss: 0.00180205
Iteration 15/25 | Loss: 0.00180205
Iteration 16/25 | Loss: 0.00180205
Iteration 17/25 | Loss: 0.00180205
Iteration 18/25 | Loss: 0.00180205
Iteration 19/25 | Loss: 0.00180205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018020541174337268, 0.0018020541174337268, 0.0018020541174337268, 0.0018020541174337268, 0.0018020541174337268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018020541174337268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180205
Iteration 2/1000 | Loss: 0.00002572
Iteration 3/1000 | Loss: 0.00002074
Iteration 4/1000 | Loss: 0.00007054
Iteration 5/1000 | Loss: 0.00001851
Iteration 6/1000 | Loss: 0.00001807
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001739
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001725
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001710
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001703
Iteration 28/1000 | Loss: 0.00001702
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001701
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001700
Iteration 40/1000 | Loss: 0.00001700
Iteration 41/1000 | Loss: 0.00001700
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001700
Iteration 45/1000 | Loss: 0.00001700
Iteration 46/1000 | Loss: 0.00001700
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001700
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001698
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001698
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001696
Iteration 104/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.6959016647888348e-05, 1.6959016647888348e-05, 1.6959016647888348e-05, 1.6959016647888348e-05, 1.6959016647888348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6959016647888348e-05

Optimization complete. Final v2v error: 3.485165596008301 mm

Highest mean error: 3.804023027420044 mm for frame 38

Lowest mean error: 3.171725273132324 mm for frame 65

Saving results

Total time: 57.23737359046936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369651
Iteration 2/25 | Loss: 0.00113946
Iteration 3/25 | Loss: 0.00102306
Iteration 4/25 | Loss: 0.00100215
Iteration 5/25 | Loss: 0.00099701
Iteration 6/25 | Loss: 0.00099547
Iteration 7/25 | Loss: 0.00099547
Iteration 8/25 | Loss: 0.00099547
Iteration 9/25 | Loss: 0.00099547
Iteration 10/25 | Loss: 0.00099547
Iteration 11/25 | Loss: 0.00099547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000995471840724349, 0.000995471840724349, 0.000995471840724349, 0.000995471840724349, 0.000995471840724349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000995471840724349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63066006
Iteration 2/25 | Loss: 0.00210795
Iteration 3/25 | Loss: 0.00210794
Iteration 4/25 | Loss: 0.00210794
Iteration 5/25 | Loss: 0.00210794
Iteration 6/25 | Loss: 0.00210794
Iteration 7/25 | Loss: 0.00210794
Iteration 8/25 | Loss: 0.00210794
Iteration 9/25 | Loss: 0.00210794
Iteration 10/25 | Loss: 0.00210794
Iteration 11/25 | Loss: 0.00210794
Iteration 12/25 | Loss: 0.00210794
Iteration 13/25 | Loss: 0.00210794
Iteration 14/25 | Loss: 0.00210794
Iteration 15/25 | Loss: 0.00210794
Iteration 16/25 | Loss: 0.00210794
Iteration 17/25 | Loss: 0.00210794
Iteration 18/25 | Loss: 0.00210794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002107939450070262, 0.002107939450070262, 0.002107939450070262, 0.002107939450070262, 0.002107939450070262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002107939450070262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210794
Iteration 2/1000 | Loss: 0.00003669
Iteration 3/1000 | Loss: 0.00002490
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001563
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001501
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001492
Iteration 17/1000 | Loss: 0.00001491
Iteration 18/1000 | Loss: 0.00001486
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001484
Iteration 22/1000 | Loss: 0.00001484
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001481
Iteration 25/1000 | Loss: 0.00001481
Iteration 26/1000 | Loss: 0.00001481
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001480
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001479
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001477
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001466
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001466
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001465
Iteration 72/1000 | Loss: 0.00001465
Iteration 73/1000 | Loss: 0.00001465
Iteration 74/1000 | Loss: 0.00001465
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001464
Iteration 78/1000 | Loss: 0.00001464
Iteration 79/1000 | Loss: 0.00001464
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001462
Iteration 83/1000 | Loss: 0.00001462
Iteration 84/1000 | Loss: 0.00001462
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001461
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00001460
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001459
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001459
Iteration 99/1000 | Loss: 0.00001459
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001459
Iteration 106/1000 | Loss: 0.00001459
Iteration 107/1000 | Loss: 0.00001459
Iteration 108/1000 | Loss: 0.00001459
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001458
Iteration 114/1000 | Loss: 0.00001458
Iteration 115/1000 | Loss: 0.00001458
Iteration 116/1000 | Loss: 0.00001458
Iteration 117/1000 | Loss: 0.00001458
Iteration 118/1000 | Loss: 0.00001458
Iteration 119/1000 | Loss: 0.00001458
Iteration 120/1000 | Loss: 0.00001458
Iteration 121/1000 | Loss: 0.00001458
Iteration 122/1000 | Loss: 0.00001458
Iteration 123/1000 | Loss: 0.00001458
Iteration 124/1000 | Loss: 0.00001458
Iteration 125/1000 | Loss: 0.00001458
Iteration 126/1000 | Loss: 0.00001458
Iteration 127/1000 | Loss: 0.00001457
Iteration 128/1000 | Loss: 0.00001457
Iteration 129/1000 | Loss: 0.00001457
Iteration 130/1000 | Loss: 0.00001457
Iteration 131/1000 | Loss: 0.00001457
Iteration 132/1000 | Loss: 0.00001457
Iteration 133/1000 | Loss: 0.00001457
Iteration 134/1000 | Loss: 0.00001457
Iteration 135/1000 | Loss: 0.00001457
Iteration 136/1000 | Loss: 0.00001457
Iteration 137/1000 | Loss: 0.00001457
Iteration 138/1000 | Loss: 0.00001457
Iteration 139/1000 | Loss: 0.00001457
Iteration 140/1000 | Loss: 0.00001457
Iteration 141/1000 | Loss: 0.00001457
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001457
Iteration 157/1000 | Loss: 0.00001457
Iteration 158/1000 | Loss: 0.00001457
Iteration 159/1000 | Loss: 0.00001457
Iteration 160/1000 | Loss: 0.00001457
Iteration 161/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.4565024684998207e-05, 1.4565024684998207e-05, 1.4565024684998207e-05, 1.4565024684998207e-05, 1.4565024684998207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4565024684998207e-05

Optimization complete. Final v2v error: 3.2693898677825928 mm

Highest mean error: 3.4521641731262207 mm for frame 204

Lowest mean error: 3.1003530025482178 mm for frame 103

Saving results

Total time: 36.64502763748169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0164/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0164/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149753
Iteration 2/25 | Loss: 0.01149753
Iteration 3/25 | Loss: 0.01149753
Iteration 4/25 | Loss: 0.01149753
Iteration 5/25 | Loss: 0.01149753
Iteration 6/25 | Loss: 0.01149753
Iteration 7/25 | Loss: 0.01149753
Iteration 8/25 | Loss: 0.01149753
Iteration 9/25 | Loss: 0.01149752
Iteration 10/25 | Loss: 0.01149752
Iteration 11/25 | Loss: 0.01149752
Iteration 12/25 | Loss: 0.01149752
Iteration 13/25 | Loss: 0.01149752
Iteration 14/25 | Loss: 0.01149752
Iteration 15/25 | Loss: 0.01149752
Iteration 16/25 | Loss: 0.01149752
Iteration 17/25 | Loss: 0.01149752
Iteration 18/25 | Loss: 0.01149752
Iteration 19/25 | Loss: 0.01149751
Iteration 20/25 | Loss: 0.01149751
Iteration 21/25 | Loss: 0.01149751
Iteration 22/25 | Loss: 0.01149751
Iteration 23/25 | Loss: 0.01149751
Iteration 24/25 | Loss: 0.01149751
Iteration 25/25 | Loss: 0.01149751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92946029
Iteration 2/25 | Loss: 0.07592818
Iteration 3/25 | Loss: 0.07591625
Iteration 4/25 | Loss: 0.07591625
Iteration 5/25 | Loss: 0.07591624
Iteration 6/25 | Loss: 0.07591623
Iteration 7/25 | Loss: 0.07591623
Iteration 8/25 | Loss: 0.07591623
Iteration 9/25 | Loss: 0.07591623
Iteration 10/25 | Loss: 0.07591623
Iteration 11/25 | Loss: 0.07591623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.07591623067855835, 0.07591623067855835, 0.07591623067855835, 0.07591623067855835, 0.07591623067855835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07591623067855835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07591623
Iteration 2/1000 | Loss: 0.00069805
Iteration 3/1000 | Loss: 0.00025979
Iteration 4/1000 | Loss: 0.00012315
Iteration 5/1000 | Loss: 0.00007121
Iteration 6/1000 | Loss: 0.00004959
Iteration 7/1000 | Loss: 0.00004149
Iteration 8/1000 | Loss: 0.00003590
Iteration 9/1000 | Loss: 0.00003263
Iteration 10/1000 | Loss: 0.00002942
Iteration 11/1000 | Loss: 0.00002670
Iteration 12/1000 | Loss: 0.00002458
Iteration 13/1000 | Loss: 0.00002258
Iteration 14/1000 | Loss: 0.00002063
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001899
Iteration 20/1000 | Loss: 0.00012049
Iteration 21/1000 | Loss: 0.00015878
Iteration 22/1000 | Loss: 0.00018477
Iteration 23/1000 | Loss: 0.00005629
Iteration 24/1000 | Loss: 0.00016956
Iteration 25/1000 | Loss: 0.00017723
Iteration 26/1000 | Loss: 0.00011450
Iteration 27/1000 | Loss: 0.00012081
Iteration 28/1000 | Loss: 0.00012425
Iteration 29/1000 | Loss: 0.00013796
Iteration 30/1000 | Loss: 0.00005523
Iteration 31/1000 | Loss: 0.00005736
Iteration 32/1000 | Loss: 0.00002270
Iteration 33/1000 | Loss: 0.00004659
Iteration 34/1000 | Loss: 0.00003571
Iteration 35/1000 | Loss: 0.00002518
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00001858
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001742
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00003943
Iteration 53/1000 | Loss: 0.00012476
Iteration 54/1000 | Loss: 0.00011276
Iteration 55/1000 | Loss: 0.00013031
Iteration 56/1000 | Loss: 0.00009393
Iteration 57/1000 | Loss: 0.00005058
Iteration 58/1000 | Loss: 0.00006032
Iteration 59/1000 | Loss: 0.00005216
Iteration 60/1000 | Loss: 0.00002654
Iteration 61/1000 | Loss: 0.00005468
Iteration 62/1000 | Loss: 0.00005934
Iteration 63/1000 | Loss: 0.00005311
Iteration 64/1000 | Loss: 0.00005332
Iteration 65/1000 | Loss: 0.00005292
Iteration 66/1000 | Loss: 0.00009044
Iteration 67/1000 | Loss: 0.00006766
Iteration 68/1000 | Loss: 0.00002567
Iteration 69/1000 | Loss: 0.00001871
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00003954
Iteration 82/1000 | Loss: 0.00002226
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001575
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00004049
Iteration 88/1000 | Loss: 0.00007800
Iteration 89/1000 | Loss: 0.00003563
Iteration 90/1000 | Loss: 0.00001844
Iteration 91/1000 | Loss: 0.00001627
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001463
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001455
Iteration 96/1000 | Loss: 0.00001433
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001418
Iteration 109/1000 | Loss: 0.00001418
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001435
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001419
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001414
Iteration 127/1000 | Loss: 0.00001414
Iteration 128/1000 | Loss: 0.00001414
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001414
Iteration 131/1000 | Loss: 0.00001414
Iteration 132/1000 | Loss: 0.00001413
Iteration 133/1000 | Loss: 0.00001413
Iteration 134/1000 | Loss: 0.00001413
Iteration 135/1000 | Loss: 0.00001413
Iteration 136/1000 | Loss: 0.00001413
Iteration 137/1000 | Loss: 0.00001413
Iteration 138/1000 | Loss: 0.00001413
Iteration 139/1000 | Loss: 0.00001413
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001416
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001413
Iteration 148/1000 | Loss: 0.00001413
Iteration 149/1000 | Loss: 0.00001413
Iteration 150/1000 | Loss: 0.00001413
Iteration 151/1000 | Loss: 0.00001412
Iteration 152/1000 | Loss: 0.00001412
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001437
Iteration 155/1000 | Loss: 0.00001432
Iteration 156/1000 | Loss: 0.00001417
Iteration 157/1000 | Loss: 0.00001417
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001433
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001446
Iteration 162/1000 | Loss: 0.00001446
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001412
Iteration 165/1000 | Loss: 0.00001412
Iteration 166/1000 | Loss: 0.00001412
Iteration 167/1000 | Loss: 0.00001412
Iteration 168/1000 | Loss: 0.00001412
Iteration 169/1000 | Loss: 0.00001412
Iteration 170/1000 | Loss: 0.00001412
Iteration 171/1000 | Loss: 0.00001412
Iteration 172/1000 | Loss: 0.00001412
Iteration 173/1000 | Loss: 0.00001412
Iteration 174/1000 | Loss: 0.00001412
Iteration 175/1000 | Loss: 0.00001412
Iteration 176/1000 | Loss: 0.00001412
Iteration 177/1000 | Loss: 0.00001412
Iteration 178/1000 | Loss: 0.00001422
Iteration 179/1000 | Loss: 0.00001422
Iteration 180/1000 | Loss: 0.00001422
Iteration 181/1000 | Loss: 0.00001443
Iteration 182/1000 | Loss: 0.00001450
Iteration 183/1000 | Loss: 0.00001474
Iteration 184/1000 | Loss: 0.00001479
Iteration 185/1000 | Loss: 0.00001479
Iteration 186/1000 | Loss: 0.00001474
Iteration 187/1000 | Loss: 0.00001478
Iteration 188/1000 | Loss: 0.00001473
Iteration 189/1000 | Loss: 0.00001464
Iteration 190/1000 | Loss: 0.00001471
Iteration 191/1000 | Loss: 0.00003899
Iteration 192/1000 | Loss: 0.00003302
Iteration 193/1000 | Loss: 0.00001495
Iteration 194/1000 | Loss: 0.00001449
Iteration 195/1000 | Loss: 0.00001436
Iteration 196/1000 | Loss: 0.00001482
Iteration 197/1000 | Loss: 0.00001481
Iteration 198/1000 | Loss: 0.00001457
Iteration 199/1000 | Loss: 0.00001464
Iteration 200/1000 | Loss: 0.00003880
Iteration 201/1000 | Loss: 0.00003880
Iteration 202/1000 | Loss: 0.00003145
Iteration 203/1000 | Loss: 0.00003702
Iteration 204/1000 | Loss: 0.00002232
Iteration 205/1000 | Loss: 0.00003594
Iteration 206/1000 | Loss: 0.00002033
Iteration 207/1000 | Loss: 0.00001476
Iteration 208/1000 | Loss: 0.00001494
Iteration 209/1000 | Loss: 0.00003732
Iteration 210/1000 | Loss: 0.00001746
Iteration 211/1000 | Loss: 0.00003820
Iteration 212/1000 | Loss: 0.00001853
Iteration 213/1000 | Loss: 0.00003564
Iteration 214/1000 | Loss: 0.00001758
Iteration 215/1000 | Loss: 0.00003718
Iteration 216/1000 | Loss: 0.00002240
Iteration 217/1000 | Loss: 0.00003603
Iteration 218/1000 | Loss: 0.00002725
Iteration 219/1000 | Loss: 0.00001523
Iteration 220/1000 | Loss: 0.00001435
Iteration 221/1000 | Loss: 0.00001438
Iteration 222/1000 | Loss: 0.00001467
Iteration 223/1000 | Loss: 0.00003703
Iteration 224/1000 | Loss: 0.00002420
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001470
Iteration 227/1000 | Loss: 0.00001486
Iteration 228/1000 | Loss: 0.00001474
Iteration 229/1000 | Loss: 0.00001478
Iteration 230/1000 | Loss: 0.00001473
Iteration 231/1000 | Loss: 0.00001476
Iteration 232/1000 | Loss: 0.00001465
Iteration 233/1000 | Loss: 0.00001476
Iteration 234/1000 | Loss: 0.00001476
Iteration 235/1000 | Loss: 0.00003795
Iteration 236/1000 | Loss: 0.00001866
Iteration 237/1000 | Loss: 0.00003516
Iteration 238/1000 | Loss: 0.00002060
Iteration 239/1000 | Loss: 0.00002977
Iteration 240/1000 | Loss: 0.00002366
Iteration 241/1000 | Loss: 0.00002483
Iteration 242/1000 | Loss: 0.00001816
Iteration 243/1000 | Loss: 0.00001583
Iteration 244/1000 | Loss: 0.00002045
Iteration 245/1000 | Loss: 0.00002936
Iteration 246/1000 | Loss: 0.00002239
Iteration 247/1000 | Loss: 0.00003137
Iteration 248/1000 | Loss: 0.00002473
Iteration 249/1000 | Loss: 0.00003050
Iteration 250/1000 | Loss: 0.00002504
Iteration 251/1000 | Loss: 0.00002930
Iteration 252/1000 | Loss: 0.00002888
Iteration 253/1000 | Loss: 0.00002719
Iteration 254/1000 | Loss: 0.00002738
Iteration 255/1000 | Loss: 0.00002032
Iteration 256/1000 | Loss: 0.00003186
Iteration 257/1000 | Loss: 0.00002106
Iteration 258/1000 | Loss: 0.00002954
Iteration 259/1000 | Loss: 0.00001600
Iteration 260/1000 | Loss: 0.00001499
Iteration 261/1000 | Loss: 0.00001461
Iteration 262/1000 | Loss: 0.00001474
Iteration 263/1000 | Loss: 0.00001462
Iteration 264/1000 | Loss: 0.00001444
Iteration 265/1000 | Loss: 0.00001441
Iteration 266/1000 | Loss: 0.00001440
Iteration 267/1000 | Loss: 0.00001428
Iteration 268/1000 | Loss: 0.00001457
Iteration 269/1000 | Loss: 0.00001456
Iteration 270/1000 | Loss: 0.00001461
Iteration 271/1000 | Loss: 0.00001479
Iteration 272/1000 | Loss: 0.00001479
Iteration 273/1000 | Loss: 0.00001458
Iteration 274/1000 | Loss: 0.00001451
Iteration 275/1000 | Loss: 0.00001449
Iteration 276/1000 | Loss: 0.00001469
Iteration 277/1000 | Loss: 0.00001477
Iteration 278/1000 | Loss: 0.00001474
Iteration 279/1000 | Loss: 0.00001478
Iteration 280/1000 | Loss: 0.00001470
Iteration 281/1000 | Loss: 0.00001479
Iteration 282/1000 | Loss: 0.00001477
Iteration 283/1000 | Loss: 0.00001471
Iteration 284/1000 | Loss: 0.00001470
Iteration 285/1000 | Loss: 0.00001479
Iteration 286/1000 | Loss: 0.00001445
Iteration 287/1000 | Loss: 0.00001428
Iteration 288/1000 | Loss: 0.00001460
Iteration 289/1000 | Loss: 0.00001490
Iteration 290/1000 | Loss: 0.00001454
Iteration 291/1000 | Loss: 0.00001445
Iteration 292/1000 | Loss: 0.00001463
Iteration 293/1000 | Loss: 0.00001492
Iteration 294/1000 | Loss: 0.00001439
Iteration 295/1000 | Loss: 0.00001426
Iteration 296/1000 | Loss: 0.00001441
Iteration 297/1000 | Loss: 0.00001426
Iteration 298/1000 | Loss: 0.00001468
Iteration 299/1000 | Loss: 0.00001489
Iteration 300/1000 | Loss: 0.00001443
Iteration 301/1000 | Loss: 0.00001427
Iteration 302/1000 | Loss: 0.00001470
Iteration 303/1000 | Loss: 0.00001470
Iteration 304/1000 | Loss: 0.00001499
Iteration 305/1000 | Loss: 0.00001439
Iteration 306/1000 | Loss: 0.00001425
Iteration 307/1000 | Loss: 0.00001436
Iteration 308/1000 | Loss: 0.00001424
Iteration 309/1000 | Loss: 0.00001423
Iteration 310/1000 | Loss: 0.00001423
Iteration 311/1000 | Loss: 0.00001419
Iteration 312/1000 | Loss: 0.00001418
Iteration 313/1000 | Loss: 0.00001417
Iteration 314/1000 | Loss: 0.00001417
Iteration 315/1000 | Loss: 0.00001417
Iteration 316/1000 | Loss: 0.00001417
Iteration 317/1000 | Loss: 0.00001417
Iteration 318/1000 | Loss: 0.00001415
Iteration 319/1000 | Loss: 0.00001415
Iteration 320/1000 | Loss: 0.00001415
Iteration 321/1000 | Loss: 0.00001415
Iteration 322/1000 | Loss: 0.00001416
Iteration 323/1000 | Loss: 0.00001416
Iteration 324/1000 | Loss: 0.00001416
Iteration 325/1000 | Loss: 0.00001416
Iteration 326/1000 | Loss: 0.00001416
Iteration 327/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [1.4164590538712218e-05, 1.4164590538712218e-05, 1.4164590538712218e-05, 1.4164590538712218e-05, 1.4164590538712218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4164590538712218e-05

Optimization complete. Final v2v error: 3.033437728881836 mm

Highest mean error: 10.680316925048828 mm for frame 164

Lowest mean error: 2.700140953063965 mm for frame 77

Saving results

Total time: 310.7337291240692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876911
Iteration 2/25 | Loss: 0.00135480
Iteration 3/25 | Loss: 0.00112244
Iteration 4/25 | Loss: 0.00110377
Iteration 5/25 | Loss: 0.00110064
Iteration 6/25 | Loss: 0.00110001
Iteration 7/25 | Loss: 0.00110001
Iteration 8/25 | Loss: 0.00110000
Iteration 9/25 | Loss: 0.00110000
Iteration 10/25 | Loss: 0.00110000
Iteration 11/25 | Loss: 0.00110000
Iteration 12/25 | Loss: 0.00110000
Iteration 13/25 | Loss: 0.00110000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010999994119629264, 0.0010999994119629264, 0.0010999994119629264, 0.0010999994119629264, 0.0010999994119629264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010999994119629264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68258905
Iteration 2/25 | Loss: 0.00294804
Iteration 3/25 | Loss: 0.00294804
Iteration 4/25 | Loss: 0.00294804
Iteration 5/25 | Loss: 0.00294804
Iteration 6/25 | Loss: 0.00294804
Iteration 7/25 | Loss: 0.00294804
Iteration 8/25 | Loss: 0.00294804
Iteration 9/25 | Loss: 0.00294804
Iteration 10/25 | Loss: 0.00294804
Iteration 11/25 | Loss: 0.00294804
Iteration 12/25 | Loss: 0.00294804
Iteration 13/25 | Loss: 0.00294804
Iteration 14/25 | Loss: 0.00294804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002948036650195718, 0.002948036650195718, 0.002948036650195718, 0.002948036650195718, 0.002948036650195718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002948036650195718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294804
Iteration 2/1000 | Loss: 0.00005759
Iteration 3/1000 | Loss: 0.00004567
Iteration 4/1000 | Loss: 0.00004262
Iteration 5/1000 | Loss: 0.00004060
Iteration 6/1000 | Loss: 0.00003877
Iteration 7/1000 | Loss: 0.00003800
Iteration 8/1000 | Loss: 0.00003741
Iteration 9/1000 | Loss: 0.00003695
Iteration 10/1000 | Loss: 0.00003670
Iteration 11/1000 | Loss: 0.00003653
Iteration 12/1000 | Loss: 0.00003646
Iteration 13/1000 | Loss: 0.00003646
Iteration 14/1000 | Loss: 0.00003646
Iteration 15/1000 | Loss: 0.00003646
Iteration 16/1000 | Loss: 0.00003646
Iteration 17/1000 | Loss: 0.00003646
Iteration 18/1000 | Loss: 0.00003646
Iteration 19/1000 | Loss: 0.00003645
Iteration 20/1000 | Loss: 0.00003645
Iteration 21/1000 | Loss: 0.00003645
Iteration 22/1000 | Loss: 0.00003645
Iteration 23/1000 | Loss: 0.00003645
Iteration 24/1000 | Loss: 0.00003644
Iteration 25/1000 | Loss: 0.00003644
Iteration 26/1000 | Loss: 0.00003643
Iteration 27/1000 | Loss: 0.00003643
Iteration 28/1000 | Loss: 0.00003642
Iteration 29/1000 | Loss: 0.00003642
Iteration 30/1000 | Loss: 0.00003641
Iteration 31/1000 | Loss: 0.00003641
Iteration 32/1000 | Loss: 0.00003639
Iteration 33/1000 | Loss: 0.00003638
Iteration 34/1000 | Loss: 0.00003636
Iteration 35/1000 | Loss: 0.00003636
Iteration 36/1000 | Loss: 0.00003635
Iteration 37/1000 | Loss: 0.00003635
Iteration 38/1000 | Loss: 0.00003635
Iteration 39/1000 | Loss: 0.00003635
Iteration 40/1000 | Loss: 0.00003635
Iteration 41/1000 | Loss: 0.00003635
Iteration 42/1000 | Loss: 0.00003635
Iteration 43/1000 | Loss: 0.00003634
Iteration 44/1000 | Loss: 0.00003633
Iteration 45/1000 | Loss: 0.00003633
Iteration 46/1000 | Loss: 0.00003633
Iteration 47/1000 | Loss: 0.00003633
Iteration 48/1000 | Loss: 0.00003632
Iteration 49/1000 | Loss: 0.00003630
Iteration 50/1000 | Loss: 0.00003630
Iteration 51/1000 | Loss: 0.00003629
Iteration 52/1000 | Loss: 0.00003629
Iteration 53/1000 | Loss: 0.00003628
Iteration 54/1000 | Loss: 0.00003628
Iteration 55/1000 | Loss: 0.00003627
Iteration 56/1000 | Loss: 0.00003627
Iteration 57/1000 | Loss: 0.00003627
Iteration 58/1000 | Loss: 0.00003627
Iteration 59/1000 | Loss: 0.00003627
Iteration 60/1000 | Loss: 0.00003626
Iteration 61/1000 | Loss: 0.00003626
Iteration 62/1000 | Loss: 0.00003626
Iteration 63/1000 | Loss: 0.00003626
Iteration 64/1000 | Loss: 0.00003625
Iteration 65/1000 | Loss: 0.00003625
Iteration 66/1000 | Loss: 0.00003625
Iteration 67/1000 | Loss: 0.00003625
Iteration 68/1000 | Loss: 0.00003625
Iteration 69/1000 | Loss: 0.00003624
Iteration 70/1000 | Loss: 0.00003624
Iteration 71/1000 | Loss: 0.00003624
Iteration 72/1000 | Loss: 0.00003624
Iteration 73/1000 | Loss: 0.00003624
Iteration 74/1000 | Loss: 0.00003624
Iteration 75/1000 | Loss: 0.00003624
Iteration 76/1000 | Loss: 0.00003624
Iteration 77/1000 | Loss: 0.00003624
Iteration 78/1000 | Loss: 0.00003624
Iteration 79/1000 | Loss: 0.00003623
Iteration 80/1000 | Loss: 0.00003623
Iteration 81/1000 | Loss: 0.00003623
Iteration 82/1000 | Loss: 0.00003623
Iteration 83/1000 | Loss: 0.00003623
Iteration 84/1000 | Loss: 0.00003623
Iteration 85/1000 | Loss: 0.00003623
Iteration 86/1000 | Loss: 0.00003623
Iteration 87/1000 | Loss: 0.00003622
Iteration 88/1000 | Loss: 0.00003622
Iteration 89/1000 | Loss: 0.00003622
Iteration 90/1000 | Loss: 0.00003621
Iteration 91/1000 | Loss: 0.00003621
Iteration 92/1000 | Loss: 0.00003621
Iteration 93/1000 | Loss: 0.00003621
Iteration 94/1000 | Loss: 0.00003620
Iteration 95/1000 | Loss: 0.00003620
Iteration 96/1000 | Loss: 0.00003619
Iteration 97/1000 | Loss: 0.00003619
Iteration 98/1000 | Loss: 0.00003619
Iteration 99/1000 | Loss: 0.00003619
Iteration 100/1000 | Loss: 0.00003619
Iteration 101/1000 | Loss: 0.00003619
Iteration 102/1000 | Loss: 0.00003619
Iteration 103/1000 | Loss: 0.00003619
Iteration 104/1000 | Loss: 0.00003619
Iteration 105/1000 | Loss: 0.00003619
Iteration 106/1000 | Loss: 0.00003619
Iteration 107/1000 | Loss: 0.00003618
Iteration 108/1000 | Loss: 0.00003618
Iteration 109/1000 | Loss: 0.00003618
Iteration 110/1000 | Loss: 0.00003618
Iteration 111/1000 | Loss: 0.00003617
Iteration 112/1000 | Loss: 0.00003617
Iteration 113/1000 | Loss: 0.00003617
Iteration 114/1000 | Loss: 0.00003617
Iteration 115/1000 | Loss: 0.00003617
Iteration 116/1000 | Loss: 0.00003616
Iteration 117/1000 | Loss: 0.00003616
Iteration 118/1000 | Loss: 0.00003616
Iteration 119/1000 | Loss: 0.00003616
Iteration 120/1000 | Loss: 0.00003616
Iteration 121/1000 | Loss: 0.00003616
Iteration 122/1000 | Loss: 0.00003616
Iteration 123/1000 | Loss: 0.00003616
Iteration 124/1000 | Loss: 0.00003616
Iteration 125/1000 | Loss: 0.00003616
Iteration 126/1000 | Loss: 0.00003616
Iteration 127/1000 | Loss: 0.00003615
Iteration 128/1000 | Loss: 0.00003615
Iteration 129/1000 | Loss: 0.00003615
Iteration 130/1000 | Loss: 0.00003615
Iteration 131/1000 | Loss: 0.00003615
Iteration 132/1000 | Loss: 0.00003615
Iteration 133/1000 | Loss: 0.00003615
Iteration 134/1000 | Loss: 0.00003615
Iteration 135/1000 | Loss: 0.00003615
Iteration 136/1000 | Loss: 0.00003615
Iteration 137/1000 | Loss: 0.00003615
Iteration 138/1000 | Loss: 0.00003615
Iteration 139/1000 | Loss: 0.00003614
Iteration 140/1000 | Loss: 0.00003614
Iteration 141/1000 | Loss: 0.00003614
Iteration 142/1000 | Loss: 0.00003614
Iteration 143/1000 | Loss: 0.00003614
Iteration 144/1000 | Loss: 0.00003614
Iteration 145/1000 | Loss: 0.00003614
Iteration 146/1000 | Loss: 0.00003614
Iteration 147/1000 | Loss: 0.00003614
Iteration 148/1000 | Loss: 0.00003614
Iteration 149/1000 | Loss: 0.00003614
Iteration 150/1000 | Loss: 0.00003614
Iteration 151/1000 | Loss: 0.00003614
Iteration 152/1000 | Loss: 0.00003614
Iteration 153/1000 | Loss: 0.00003614
Iteration 154/1000 | Loss: 0.00003614
Iteration 155/1000 | Loss: 0.00003614
Iteration 156/1000 | Loss: 0.00003614
Iteration 157/1000 | Loss: 0.00003614
Iteration 158/1000 | Loss: 0.00003613
Iteration 159/1000 | Loss: 0.00003613
Iteration 160/1000 | Loss: 0.00003613
Iteration 161/1000 | Loss: 0.00003613
Iteration 162/1000 | Loss: 0.00003613
Iteration 163/1000 | Loss: 0.00003613
Iteration 164/1000 | Loss: 0.00003613
Iteration 165/1000 | Loss: 0.00003613
Iteration 166/1000 | Loss: 0.00003613
Iteration 167/1000 | Loss: 0.00003613
Iteration 168/1000 | Loss: 0.00003613
Iteration 169/1000 | Loss: 0.00003613
Iteration 170/1000 | Loss: 0.00003613
Iteration 171/1000 | Loss: 0.00003613
Iteration 172/1000 | Loss: 0.00003613
Iteration 173/1000 | Loss: 0.00003613
Iteration 174/1000 | Loss: 0.00003613
Iteration 175/1000 | Loss: 0.00003613
Iteration 176/1000 | Loss: 0.00003613
Iteration 177/1000 | Loss: 0.00003613
Iteration 178/1000 | Loss: 0.00003613
Iteration 179/1000 | Loss: 0.00003613
Iteration 180/1000 | Loss: 0.00003613
Iteration 181/1000 | Loss: 0.00003613
Iteration 182/1000 | Loss: 0.00003613
Iteration 183/1000 | Loss: 0.00003613
Iteration 184/1000 | Loss: 0.00003613
Iteration 185/1000 | Loss: 0.00003613
Iteration 186/1000 | Loss: 0.00003613
Iteration 187/1000 | Loss: 0.00003613
Iteration 188/1000 | Loss: 0.00003613
Iteration 189/1000 | Loss: 0.00003613
Iteration 190/1000 | Loss: 0.00003613
Iteration 191/1000 | Loss: 0.00003613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [3.613231092458591e-05, 3.613231092458591e-05, 3.613231092458591e-05, 3.613231092458591e-05, 3.613231092458591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.613231092458591e-05

Optimization complete. Final v2v error: 4.7513041496276855 mm

Highest mean error: 5.055320739746094 mm for frame 40

Lowest mean error: 4.423129558563232 mm for frame 91

Saving results

Total time: 34.91083335876465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916128
Iteration 2/25 | Loss: 0.00122178
Iteration 3/25 | Loss: 0.00105300
Iteration 4/25 | Loss: 0.00102374
Iteration 5/25 | Loss: 0.00101942
Iteration 6/25 | Loss: 0.00101858
Iteration 7/25 | Loss: 0.00101858
Iteration 8/25 | Loss: 0.00101858
Iteration 9/25 | Loss: 0.00101858
Iteration 10/25 | Loss: 0.00101858
Iteration 11/25 | Loss: 0.00101858
Iteration 12/25 | Loss: 0.00101858
Iteration 13/25 | Loss: 0.00101858
Iteration 14/25 | Loss: 0.00101858
Iteration 15/25 | Loss: 0.00101858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010185809805989265, 0.0010185809805989265, 0.0010185809805989265, 0.0010185809805989265, 0.0010185809805989265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010185809805989265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64252996
Iteration 2/25 | Loss: 0.00280314
Iteration 3/25 | Loss: 0.00280304
Iteration 4/25 | Loss: 0.00280304
Iteration 5/25 | Loss: 0.00280304
Iteration 6/25 | Loss: 0.00280303
Iteration 7/25 | Loss: 0.00280303
Iteration 8/25 | Loss: 0.00280303
Iteration 9/25 | Loss: 0.00280303
Iteration 10/25 | Loss: 0.00280303
Iteration 11/25 | Loss: 0.00280303
Iteration 12/25 | Loss: 0.00280303
Iteration 13/25 | Loss: 0.00280303
Iteration 14/25 | Loss: 0.00280303
Iteration 15/25 | Loss: 0.00280303
Iteration 16/25 | Loss: 0.00280303
Iteration 17/25 | Loss: 0.00280303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028030341491103172, 0.0028030341491103172, 0.0028030341491103172, 0.0028030341491103172, 0.0028030341491103172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028030341491103172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280303
Iteration 2/1000 | Loss: 0.00004573
Iteration 3/1000 | Loss: 0.00003360
Iteration 4/1000 | Loss: 0.00002731
Iteration 5/1000 | Loss: 0.00002598
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002356
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002231
Iteration 11/1000 | Loss: 0.00002213
Iteration 12/1000 | Loss: 0.00002204
Iteration 13/1000 | Loss: 0.00002196
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002192
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002190
Iteration 20/1000 | Loss: 0.00002190
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002189
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002186
Iteration 26/1000 | Loss: 0.00002186
Iteration 27/1000 | Loss: 0.00002186
Iteration 28/1000 | Loss: 0.00002186
Iteration 29/1000 | Loss: 0.00002186
Iteration 30/1000 | Loss: 0.00002186
Iteration 31/1000 | Loss: 0.00002186
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00002185
Iteration 36/1000 | Loss: 0.00002185
Iteration 37/1000 | Loss: 0.00002182
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002181
Iteration 40/1000 | Loss: 0.00002181
Iteration 41/1000 | Loss: 0.00002179
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002178
Iteration 44/1000 | Loss: 0.00002178
Iteration 45/1000 | Loss: 0.00002177
Iteration 46/1000 | Loss: 0.00002176
Iteration 47/1000 | Loss: 0.00002176
Iteration 48/1000 | Loss: 0.00002175
Iteration 49/1000 | Loss: 0.00002175
Iteration 50/1000 | Loss: 0.00002175
Iteration 51/1000 | Loss: 0.00002175
Iteration 52/1000 | Loss: 0.00002174
Iteration 53/1000 | Loss: 0.00002174
Iteration 54/1000 | Loss: 0.00002174
Iteration 55/1000 | Loss: 0.00002170
Iteration 56/1000 | Loss: 0.00002170
Iteration 57/1000 | Loss: 0.00002170
Iteration 58/1000 | Loss: 0.00002169
Iteration 59/1000 | Loss: 0.00002169
Iteration 60/1000 | Loss: 0.00002168
Iteration 61/1000 | Loss: 0.00002168
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00002167
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002166
Iteration 66/1000 | Loss: 0.00002166
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002165
Iteration 70/1000 | Loss: 0.00002165
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002163
Iteration 75/1000 | Loss: 0.00002163
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002163
Iteration 81/1000 | Loss: 0.00002163
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002163
Iteration 84/1000 | Loss: 0.00002163
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002163
Iteration 87/1000 | Loss: 0.00002163
Iteration 88/1000 | Loss: 0.00002163
Iteration 89/1000 | Loss: 0.00002163
Iteration 90/1000 | Loss: 0.00002163
Iteration 91/1000 | Loss: 0.00002163
Iteration 92/1000 | Loss: 0.00002163
Iteration 93/1000 | Loss: 0.00002163
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00002163
Iteration 96/1000 | Loss: 0.00002163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.163138924515806e-05, 2.163138924515806e-05, 2.163138924515806e-05, 2.163138924515806e-05, 2.163138924515806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.163138924515806e-05

Optimization complete. Final v2v error: 4.095276832580566 mm

Highest mean error: 4.530957221984863 mm for frame 53

Lowest mean error: 3.6863954067230225 mm for frame 195

Saving results

Total time: 36.520658016204834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164320
Iteration 2/25 | Loss: 0.00249718
Iteration 3/25 | Loss: 0.00278569
Iteration 4/25 | Loss: 0.00157932
Iteration 5/25 | Loss: 0.00126604
Iteration 6/25 | Loss: 0.00119001
Iteration 7/25 | Loss: 0.00112193
Iteration 8/25 | Loss: 0.00108933
Iteration 9/25 | Loss: 0.00105031
Iteration 10/25 | Loss: 0.00102556
Iteration 11/25 | Loss: 0.00100091
Iteration 12/25 | Loss: 0.00099154
Iteration 13/25 | Loss: 0.00098767
Iteration 14/25 | Loss: 0.00099149
Iteration 15/25 | Loss: 0.00098650
Iteration 16/25 | Loss: 0.00098638
Iteration 17/25 | Loss: 0.00099550
Iteration 18/25 | Loss: 0.00099155
Iteration 19/25 | Loss: 0.00098515
Iteration 20/25 | Loss: 0.00098546
Iteration 21/25 | Loss: 0.00098791
Iteration 22/25 | Loss: 0.00097950
Iteration 23/25 | Loss: 0.00097723
Iteration 24/25 | Loss: 0.00097440
Iteration 25/25 | Loss: 0.00097906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.19136095
Iteration 2/25 | Loss: 0.00299235
Iteration 3/25 | Loss: 0.00299235
Iteration 4/25 | Loss: 0.00299235
Iteration 5/25 | Loss: 0.00299235
Iteration 6/25 | Loss: 0.00299235
Iteration 7/25 | Loss: 0.00299235
Iteration 8/25 | Loss: 0.00299235
Iteration 9/25 | Loss: 0.00299235
Iteration 10/25 | Loss: 0.00299235
Iteration 11/25 | Loss: 0.00299235
Iteration 12/25 | Loss: 0.00299235
Iteration 13/25 | Loss: 0.00299235
Iteration 14/25 | Loss: 0.00299235
Iteration 15/25 | Loss: 0.00299235
Iteration 16/25 | Loss: 0.00299235
Iteration 17/25 | Loss: 0.00299235
Iteration 18/25 | Loss: 0.00299235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0029923529364168644, 0.0029923529364168644, 0.0029923529364168644, 0.0029923529364168644, 0.0029923529364168644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029923529364168644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299235
Iteration 2/1000 | Loss: 0.00011957
Iteration 3/1000 | Loss: 0.00010542
Iteration 4/1000 | Loss: 0.00002919
Iteration 5/1000 | Loss: 0.00002729
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00083173
Iteration 8/1000 | Loss: 0.00023926
Iteration 9/1000 | Loss: 0.00002464
Iteration 10/1000 | Loss: 0.00002333
Iteration 11/1000 | Loss: 0.00002246
Iteration 12/1000 | Loss: 0.00002177
Iteration 13/1000 | Loss: 0.00002137
Iteration 14/1000 | Loss: 0.00002127
Iteration 15/1000 | Loss: 0.00002120
Iteration 16/1000 | Loss: 0.00002120
Iteration 17/1000 | Loss: 0.00002116
Iteration 18/1000 | Loss: 0.00002115
Iteration 19/1000 | Loss: 0.00002115
Iteration 20/1000 | Loss: 0.00002115
Iteration 21/1000 | Loss: 0.00002115
Iteration 22/1000 | Loss: 0.00002114
Iteration 23/1000 | Loss: 0.00002113
Iteration 24/1000 | Loss: 0.00002112
Iteration 25/1000 | Loss: 0.00002112
Iteration 26/1000 | Loss: 0.00002112
Iteration 27/1000 | Loss: 0.00002111
Iteration 28/1000 | Loss: 0.00002111
Iteration 29/1000 | Loss: 0.00002110
Iteration 30/1000 | Loss: 0.00002109
Iteration 31/1000 | Loss: 0.00002105
Iteration 32/1000 | Loss: 0.00002104
Iteration 33/1000 | Loss: 0.00002104
Iteration 34/1000 | Loss: 0.00002103
Iteration 35/1000 | Loss: 0.00002102
Iteration 36/1000 | Loss: 0.00002101
Iteration 37/1000 | Loss: 0.00002100
Iteration 38/1000 | Loss: 0.00002100
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002099
Iteration 42/1000 | Loss: 0.00002099
Iteration 43/1000 | Loss: 0.00002099
Iteration 44/1000 | Loss: 0.00002098
Iteration 45/1000 | Loss: 0.00009208
Iteration 46/1000 | Loss: 0.00002105
Iteration 47/1000 | Loss: 0.00002097
Iteration 48/1000 | Loss: 0.00002093
Iteration 49/1000 | Loss: 0.00002092
Iteration 50/1000 | Loss: 0.00002092
Iteration 51/1000 | Loss: 0.00002092
Iteration 52/1000 | Loss: 0.00002092
Iteration 53/1000 | Loss: 0.00002092
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002092
Iteration 56/1000 | Loss: 0.00002092
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002091
Iteration 60/1000 | Loss: 0.00002091
Iteration 61/1000 | Loss: 0.00002091
Iteration 62/1000 | Loss: 0.00002091
Iteration 63/1000 | Loss: 0.00002091
Iteration 64/1000 | Loss: 0.00002091
Iteration 65/1000 | Loss: 0.00002091
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002091
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002091
Iteration 72/1000 | Loss: 0.00002091
Iteration 73/1000 | Loss: 0.00002091
Iteration 74/1000 | Loss: 0.00002091
Iteration 75/1000 | Loss: 0.00002091
Iteration 76/1000 | Loss: 0.00002091
Iteration 77/1000 | Loss: 0.00002091
Iteration 78/1000 | Loss: 0.00002091
Iteration 79/1000 | Loss: 0.00002091
Iteration 80/1000 | Loss: 0.00002091
Iteration 81/1000 | Loss: 0.00002091
Iteration 82/1000 | Loss: 0.00002091
Iteration 83/1000 | Loss: 0.00002091
Iteration 84/1000 | Loss: 0.00002091
Iteration 85/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.0910620150971226e-05, 2.0910620150971226e-05, 2.0910620150971226e-05, 2.0910620150971226e-05, 2.0910620150971226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0910620150971226e-05

Optimization complete. Final v2v error: 3.947514533996582 mm

Highest mean error: 4.921694755554199 mm for frame 118

Lowest mean error: 3.6136345863342285 mm for frame 64

Saving results

Total time: 69.36733365058899
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851667
Iteration 2/25 | Loss: 0.00156143
Iteration 3/25 | Loss: 0.00128203
Iteration 4/25 | Loss: 0.00121083
Iteration 5/25 | Loss: 0.00120648
Iteration 6/25 | Loss: 0.00118528
Iteration 7/25 | Loss: 0.00115174
Iteration 8/25 | Loss: 0.00114557
Iteration 9/25 | Loss: 0.00111945
Iteration 10/25 | Loss: 0.00111149
Iteration 11/25 | Loss: 0.00111048
Iteration 12/25 | Loss: 0.00111032
Iteration 13/25 | Loss: 0.00111027
Iteration 14/25 | Loss: 0.00111027
Iteration 15/25 | Loss: 0.00111027
Iteration 16/25 | Loss: 0.00111026
Iteration 17/25 | Loss: 0.00111026
Iteration 18/25 | Loss: 0.00111026
Iteration 19/25 | Loss: 0.00111026
Iteration 20/25 | Loss: 0.00111026
Iteration 21/25 | Loss: 0.00111026
Iteration 22/25 | Loss: 0.00111025
Iteration 23/25 | Loss: 0.00111025
Iteration 24/25 | Loss: 0.00111024
Iteration 25/25 | Loss: 0.00111024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60184276
Iteration 2/25 | Loss: 0.00252732
Iteration 3/25 | Loss: 0.00252728
Iteration 4/25 | Loss: 0.00252728
Iteration 5/25 | Loss: 0.00252728
Iteration 6/25 | Loss: 0.00252728
Iteration 7/25 | Loss: 0.00252728
Iteration 8/25 | Loss: 0.00252728
Iteration 9/25 | Loss: 0.00252728
Iteration 10/25 | Loss: 0.00252728
Iteration 11/25 | Loss: 0.00252728
Iteration 12/25 | Loss: 0.00252728
Iteration 13/25 | Loss: 0.00252728
Iteration 14/25 | Loss: 0.00252728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025272772181779146, 0.0025272772181779146, 0.0025272772181779146, 0.0025272772181779146, 0.0025272772181779146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025272772181779146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252728
Iteration 2/1000 | Loss: 0.00006621
Iteration 3/1000 | Loss: 0.00005143
Iteration 4/1000 | Loss: 0.00004126
Iteration 5/1000 | Loss: 0.00003808
Iteration 6/1000 | Loss: 0.00003658
Iteration 7/1000 | Loss: 0.00003550
Iteration 8/1000 | Loss: 0.00003467
Iteration 9/1000 | Loss: 0.00003403
Iteration 10/1000 | Loss: 0.00003353
Iteration 11/1000 | Loss: 0.00003320
Iteration 12/1000 | Loss: 0.00003292
Iteration 13/1000 | Loss: 0.00003268
Iteration 14/1000 | Loss: 0.00003256
Iteration 15/1000 | Loss: 0.00003239
Iteration 16/1000 | Loss: 0.00003231
Iteration 17/1000 | Loss: 0.00003226
Iteration 18/1000 | Loss: 0.00003225
Iteration 19/1000 | Loss: 0.00003222
Iteration 20/1000 | Loss: 0.00003221
Iteration 21/1000 | Loss: 0.00003221
Iteration 22/1000 | Loss: 0.00003220
Iteration 23/1000 | Loss: 0.00003219
Iteration 24/1000 | Loss: 0.00003214
Iteration 25/1000 | Loss: 0.00003213
Iteration 26/1000 | Loss: 0.00003211
Iteration 27/1000 | Loss: 0.00003211
Iteration 28/1000 | Loss: 0.00003210
Iteration 29/1000 | Loss: 0.00003210
Iteration 30/1000 | Loss: 0.00003210
Iteration 31/1000 | Loss: 0.00003210
Iteration 32/1000 | Loss: 0.00003210
Iteration 33/1000 | Loss: 0.00003209
Iteration 34/1000 | Loss: 0.00003209
Iteration 35/1000 | Loss: 0.00003208
Iteration 36/1000 | Loss: 0.00003208
Iteration 37/1000 | Loss: 0.00003208
Iteration 38/1000 | Loss: 0.00003208
Iteration 39/1000 | Loss: 0.00003207
Iteration 40/1000 | Loss: 0.00003207
Iteration 41/1000 | Loss: 0.00003207
Iteration 42/1000 | Loss: 0.00003207
Iteration 43/1000 | Loss: 0.00003206
Iteration 44/1000 | Loss: 0.00003206
Iteration 45/1000 | Loss: 0.00003205
Iteration 46/1000 | Loss: 0.00003205
Iteration 47/1000 | Loss: 0.00003205
Iteration 48/1000 | Loss: 0.00003205
Iteration 49/1000 | Loss: 0.00003205
Iteration 50/1000 | Loss: 0.00003205
Iteration 51/1000 | Loss: 0.00003203
Iteration 52/1000 | Loss: 0.00003202
Iteration 53/1000 | Loss: 0.00003202
Iteration 54/1000 | Loss: 0.00003202
Iteration 55/1000 | Loss: 0.00003201
Iteration 56/1000 | Loss: 0.00003200
Iteration 57/1000 | Loss: 0.00003200
Iteration 58/1000 | Loss: 0.00003200
Iteration 59/1000 | Loss: 0.00003199
Iteration 60/1000 | Loss: 0.00003199
Iteration 61/1000 | Loss: 0.00003198
Iteration 62/1000 | Loss: 0.00003198
Iteration 63/1000 | Loss: 0.00003198
Iteration 64/1000 | Loss: 0.00003198
Iteration 65/1000 | Loss: 0.00003197
Iteration 66/1000 | Loss: 0.00003197
Iteration 67/1000 | Loss: 0.00003196
Iteration 68/1000 | Loss: 0.00003196
Iteration 69/1000 | Loss: 0.00003196
Iteration 70/1000 | Loss: 0.00003195
Iteration 71/1000 | Loss: 0.00003195
Iteration 72/1000 | Loss: 0.00003195
Iteration 73/1000 | Loss: 0.00003195
Iteration 74/1000 | Loss: 0.00003194
Iteration 75/1000 | Loss: 0.00003194
Iteration 76/1000 | Loss: 0.00003194
Iteration 77/1000 | Loss: 0.00003193
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003193
Iteration 80/1000 | Loss: 0.00003193
Iteration 81/1000 | Loss: 0.00003192
Iteration 82/1000 | Loss: 0.00003192
Iteration 83/1000 | Loss: 0.00003192
Iteration 84/1000 | Loss: 0.00003192
Iteration 85/1000 | Loss: 0.00003192
Iteration 86/1000 | Loss: 0.00003192
Iteration 87/1000 | Loss: 0.00003192
Iteration 88/1000 | Loss: 0.00003192
Iteration 89/1000 | Loss: 0.00003192
Iteration 90/1000 | Loss: 0.00003192
Iteration 91/1000 | Loss: 0.00003192
Iteration 92/1000 | Loss: 0.00003192
Iteration 93/1000 | Loss: 0.00003192
Iteration 94/1000 | Loss: 0.00003192
Iteration 95/1000 | Loss: 0.00003192
Iteration 96/1000 | Loss: 0.00003192
Iteration 97/1000 | Loss: 0.00003192
Iteration 98/1000 | Loss: 0.00003192
Iteration 99/1000 | Loss: 0.00003192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [3.192022631992586e-05, 3.192022631992586e-05, 3.192022631992586e-05, 3.192022631992586e-05, 3.192022631992586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.192022631992586e-05

Optimization complete. Final v2v error: 4.785027503967285 mm

Highest mean error: 5.553901195526123 mm for frame 125

Lowest mean error: 4.346170425415039 mm for frame 97

Saving results

Total time: 50.00055193901062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065394
Iteration 2/25 | Loss: 0.00225259
Iteration 3/25 | Loss: 0.00156438
Iteration 4/25 | Loss: 0.00143048
Iteration 5/25 | Loss: 0.00132231
Iteration 6/25 | Loss: 0.00125124
Iteration 7/25 | Loss: 0.00123631
Iteration 8/25 | Loss: 0.00115320
Iteration 9/25 | Loss: 0.00108964
Iteration 10/25 | Loss: 0.00106686
Iteration 11/25 | Loss: 0.00106105
Iteration 12/25 | Loss: 0.00105890
Iteration 13/25 | Loss: 0.00105777
Iteration 14/25 | Loss: 0.00105724
Iteration 15/25 | Loss: 0.00105700
Iteration 16/25 | Loss: 0.00105675
Iteration 17/25 | Loss: 0.00105637
Iteration 18/25 | Loss: 0.00105612
Iteration 19/25 | Loss: 0.00105601
Iteration 20/25 | Loss: 0.00105600
Iteration 21/25 | Loss: 0.00105600
Iteration 22/25 | Loss: 0.00105600
Iteration 23/25 | Loss: 0.00105600
Iteration 24/25 | Loss: 0.00105600
Iteration 25/25 | Loss: 0.00105600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64402008
Iteration 2/25 | Loss: 0.00346102
Iteration 3/25 | Loss: 0.00346102
Iteration 4/25 | Loss: 0.00346102
Iteration 5/25 | Loss: 0.00346102
Iteration 6/25 | Loss: 0.00346102
Iteration 7/25 | Loss: 0.00346102
Iteration 8/25 | Loss: 0.00346102
Iteration 9/25 | Loss: 0.00346102
Iteration 10/25 | Loss: 0.00346102
Iteration 11/25 | Loss: 0.00346102
Iteration 12/25 | Loss: 0.00346102
Iteration 13/25 | Loss: 0.00346102
Iteration 14/25 | Loss: 0.00346102
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003461015410721302, 0.003461015410721302, 0.003461015410721302, 0.003461015410721302, 0.003461015410721302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003461015410721302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00346102
Iteration 2/1000 | Loss: 0.00042010
Iteration 3/1000 | Loss: 0.00030459
Iteration 4/1000 | Loss: 0.00007062
Iteration 5/1000 | Loss: 0.00005163
Iteration 6/1000 | Loss: 0.00004221
Iteration 7/1000 | Loss: 0.00003596
Iteration 8/1000 | Loss: 0.00003338
Iteration 9/1000 | Loss: 0.00003131
Iteration 10/1000 | Loss: 0.00003000
Iteration 11/1000 | Loss: 0.00002927
Iteration 12/1000 | Loss: 0.00002869
Iteration 13/1000 | Loss: 0.00002834
Iteration 14/1000 | Loss: 0.00002810
Iteration 15/1000 | Loss: 0.00002804
Iteration 16/1000 | Loss: 0.00002800
Iteration 17/1000 | Loss: 0.00002797
Iteration 18/1000 | Loss: 0.00002780
Iteration 19/1000 | Loss: 0.00002776
Iteration 20/1000 | Loss: 0.00002771
Iteration 21/1000 | Loss: 0.00002769
Iteration 22/1000 | Loss: 0.00002768
Iteration 23/1000 | Loss: 0.00002766
Iteration 24/1000 | Loss: 0.00002762
Iteration 25/1000 | Loss: 0.00002760
Iteration 26/1000 | Loss: 0.00002759
Iteration 27/1000 | Loss: 0.00002759
Iteration 28/1000 | Loss: 0.00002759
Iteration 29/1000 | Loss: 0.00002758
Iteration 30/1000 | Loss: 0.00002758
Iteration 31/1000 | Loss: 0.00002758
Iteration 32/1000 | Loss: 0.00002758
Iteration 33/1000 | Loss: 0.00002757
Iteration 34/1000 | Loss: 0.00002757
Iteration 35/1000 | Loss: 0.00002756
Iteration 36/1000 | Loss: 0.00002756
Iteration 37/1000 | Loss: 0.00002755
Iteration 38/1000 | Loss: 0.00002755
Iteration 39/1000 | Loss: 0.00002755
Iteration 40/1000 | Loss: 0.00002755
Iteration 41/1000 | Loss: 0.00002754
Iteration 42/1000 | Loss: 0.00002754
Iteration 43/1000 | Loss: 0.00002752
Iteration 44/1000 | Loss: 0.00002752
Iteration 45/1000 | Loss: 0.00002752
Iteration 46/1000 | Loss: 0.00002752
Iteration 47/1000 | Loss: 0.00002752
Iteration 48/1000 | Loss: 0.00002751
Iteration 49/1000 | Loss: 0.00002751
Iteration 50/1000 | Loss: 0.00002750
Iteration 51/1000 | Loss: 0.00002750
Iteration 52/1000 | Loss: 0.00002750
Iteration 53/1000 | Loss: 0.00002749
Iteration 54/1000 | Loss: 0.00002749
Iteration 55/1000 | Loss: 0.00002749
Iteration 56/1000 | Loss: 0.00002749
Iteration 57/1000 | Loss: 0.00002748
Iteration 58/1000 | Loss: 0.00002748
Iteration 59/1000 | Loss: 0.00002748
Iteration 60/1000 | Loss: 0.00002748
Iteration 61/1000 | Loss: 0.00002748
Iteration 62/1000 | Loss: 0.00002748
Iteration 63/1000 | Loss: 0.00002748
Iteration 64/1000 | Loss: 0.00002748
Iteration 65/1000 | Loss: 0.00002748
Iteration 66/1000 | Loss: 0.00002747
Iteration 67/1000 | Loss: 0.00002747
Iteration 68/1000 | Loss: 0.00002747
Iteration 69/1000 | Loss: 0.00002746
Iteration 70/1000 | Loss: 0.00002746
Iteration 71/1000 | Loss: 0.00002746
Iteration 72/1000 | Loss: 0.00002746
Iteration 73/1000 | Loss: 0.00002746
Iteration 74/1000 | Loss: 0.00002746
Iteration 75/1000 | Loss: 0.00002745
Iteration 76/1000 | Loss: 0.00002745
Iteration 77/1000 | Loss: 0.00002745
Iteration 78/1000 | Loss: 0.00002745
Iteration 79/1000 | Loss: 0.00002745
Iteration 80/1000 | Loss: 0.00002745
Iteration 81/1000 | Loss: 0.00002745
Iteration 82/1000 | Loss: 0.00002745
Iteration 83/1000 | Loss: 0.00002745
Iteration 84/1000 | Loss: 0.00002745
Iteration 85/1000 | Loss: 0.00002745
Iteration 86/1000 | Loss: 0.00002744
Iteration 87/1000 | Loss: 0.00002744
Iteration 88/1000 | Loss: 0.00002744
Iteration 89/1000 | Loss: 0.00002744
Iteration 90/1000 | Loss: 0.00002743
Iteration 91/1000 | Loss: 0.00002743
Iteration 92/1000 | Loss: 0.00002743
Iteration 93/1000 | Loss: 0.00002743
Iteration 94/1000 | Loss: 0.00002743
Iteration 95/1000 | Loss: 0.00002743
Iteration 96/1000 | Loss: 0.00002743
Iteration 97/1000 | Loss: 0.00002743
Iteration 98/1000 | Loss: 0.00002742
Iteration 99/1000 | Loss: 0.00002742
Iteration 100/1000 | Loss: 0.00002742
Iteration 101/1000 | Loss: 0.00002742
Iteration 102/1000 | Loss: 0.00002742
Iteration 103/1000 | Loss: 0.00002742
Iteration 104/1000 | Loss: 0.00002742
Iteration 105/1000 | Loss: 0.00002742
Iteration 106/1000 | Loss: 0.00002742
Iteration 107/1000 | Loss: 0.00002742
Iteration 108/1000 | Loss: 0.00002742
Iteration 109/1000 | Loss: 0.00002742
Iteration 110/1000 | Loss: 0.00002742
Iteration 111/1000 | Loss: 0.00002742
Iteration 112/1000 | Loss: 0.00002741
Iteration 113/1000 | Loss: 0.00002741
Iteration 114/1000 | Loss: 0.00002741
Iteration 115/1000 | Loss: 0.00002741
Iteration 116/1000 | Loss: 0.00002741
Iteration 117/1000 | Loss: 0.00002741
Iteration 118/1000 | Loss: 0.00002741
Iteration 119/1000 | Loss: 0.00002741
Iteration 120/1000 | Loss: 0.00002741
Iteration 121/1000 | Loss: 0.00002741
Iteration 122/1000 | Loss: 0.00002741
Iteration 123/1000 | Loss: 0.00002741
Iteration 124/1000 | Loss: 0.00002741
Iteration 125/1000 | Loss: 0.00002741
Iteration 126/1000 | Loss: 0.00002741
Iteration 127/1000 | Loss: 0.00002741
Iteration 128/1000 | Loss: 0.00002741
Iteration 129/1000 | Loss: 0.00002741
Iteration 130/1000 | Loss: 0.00002741
Iteration 131/1000 | Loss: 0.00002741
Iteration 132/1000 | Loss: 0.00002741
Iteration 133/1000 | Loss: 0.00002741
Iteration 134/1000 | Loss: 0.00002741
Iteration 135/1000 | Loss: 0.00002741
Iteration 136/1000 | Loss: 0.00002741
Iteration 137/1000 | Loss: 0.00002741
Iteration 138/1000 | Loss: 0.00002741
Iteration 139/1000 | Loss: 0.00002741
Iteration 140/1000 | Loss: 0.00002741
Iteration 141/1000 | Loss: 0.00002741
Iteration 142/1000 | Loss: 0.00002741
Iteration 143/1000 | Loss: 0.00002741
Iteration 144/1000 | Loss: 0.00002741
Iteration 145/1000 | Loss: 0.00002741
Iteration 146/1000 | Loss: 0.00002741
Iteration 147/1000 | Loss: 0.00002741
Iteration 148/1000 | Loss: 0.00002741
Iteration 149/1000 | Loss: 0.00002741
Iteration 150/1000 | Loss: 0.00002741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.7408183086663485e-05, 2.7408183086663485e-05, 2.7408183086663485e-05, 2.7408183086663485e-05, 2.7408183086663485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7408183086663485e-05

Optimization complete. Final v2v error: 3.9600515365600586 mm

Highest mean error: 12.777037620544434 mm for frame 23

Lowest mean error: 3.496927499771118 mm for frame 123

Saving results

Total time: 63.24756836891174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909513
Iteration 2/25 | Loss: 0.00142594
Iteration 3/25 | Loss: 0.00109456
Iteration 4/25 | Loss: 0.00104264
Iteration 5/25 | Loss: 0.00103641
Iteration 6/25 | Loss: 0.00103586
Iteration 7/25 | Loss: 0.00103586
Iteration 8/25 | Loss: 0.00103586
Iteration 9/25 | Loss: 0.00103586
Iteration 10/25 | Loss: 0.00103586
Iteration 11/25 | Loss: 0.00103586
Iteration 12/25 | Loss: 0.00103586
Iteration 13/25 | Loss: 0.00103586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001035856083035469, 0.001035856083035469, 0.001035856083035469, 0.001035856083035469, 0.001035856083035469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001035856083035469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68652534
Iteration 2/25 | Loss: 0.00277525
Iteration 3/25 | Loss: 0.00277524
Iteration 4/25 | Loss: 0.00277524
Iteration 5/25 | Loss: 0.00277524
Iteration 6/25 | Loss: 0.00277524
Iteration 7/25 | Loss: 0.00277524
Iteration 8/25 | Loss: 0.00277524
Iteration 9/25 | Loss: 0.00277524
Iteration 10/25 | Loss: 0.00277524
Iteration 11/25 | Loss: 0.00277524
Iteration 12/25 | Loss: 0.00277524
Iteration 13/25 | Loss: 0.00277524
Iteration 14/25 | Loss: 0.00277524
Iteration 15/25 | Loss: 0.00277524
Iteration 16/25 | Loss: 0.00277524
Iteration 17/25 | Loss: 0.00277524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002775236265733838, 0.002775236265733838, 0.002775236265733838, 0.002775236265733838, 0.002775236265733838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002775236265733838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277524
Iteration 2/1000 | Loss: 0.00004117
Iteration 3/1000 | Loss: 0.00003219
Iteration 4/1000 | Loss: 0.00002996
Iteration 5/1000 | Loss: 0.00002844
Iteration 6/1000 | Loss: 0.00002756
Iteration 7/1000 | Loss: 0.00002699
Iteration 8/1000 | Loss: 0.00002663
Iteration 9/1000 | Loss: 0.00002647
Iteration 10/1000 | Loss: 0.00002634
Iteration 11/1000 | Loss: 0.00002632
Iteration 12/1000 | Loss: 0.00002625
Iteration 13/1000 | Loss: 0.00002610
Iteration 14/1000 | Loss: 0.00002607
Iteration 15/1000 | Loss: 0.00002605
Iteration 16/1000 | Loss: 0.00002604
Iteration 17/1000 | Loss: 0.00002601
Iteration 18/1000 | Loss: 0.00002600
Iteration 19/1000 | Loss: 0.00002599
Iteration 20/1000 | Loss: 0.00002599
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00002599
Iteration 23/1000 | Loss: 0.00002599
Iteration 24/1000 | Loss: 0.00002599
Iteration 25/1000 | Loss: 0.00002599
Iteration 26/1000 | Loss: 0.00002598
Iteration 27/1000 | Loss: 0.00002598
Iteration 28/1000 | Loss: 0.00002598
Iteration 29/1000 | Loss: 0.00002598
Iteration 30/1000 | Loss: 0.00002598
Iteration 31/1000 | Loss: 0.00002598
Iteration 32/1000 | Loss: 0.00002597
Iteration 33/1000 | Loss: 0.00002597
Iteration 34/1000 | Loss: 0.00002597
Iteration 35/1000 | Loss: 0.00002597
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00002597
Iteration 38/1000 | Loss: 0.00002597
Iteration 39/1000 | Loss: 0.00002597
Iteration 40/1000 | Loss: 0.00002597
Iteration 41/1000 | Loss: 0.00002597
Iteration 42/1000 | Loss: 0.00002597
Iteration 43/1000 | Loss: 0.00002597
Iteration 44/1000 | Loss: 0.00002597
Iteration 45/1000 | Loss: 0.00002597
Iteration 46/1000 | Loss: 0.00002596
Iteration 47/1000 | Loss: 0.00002596
Iteration 48/1000 | Loss: 0.00002596
Iteration 49/1000 | Loss: 0.00002596
Iteration 50/1000 | Loss: 0.00002595
Iteration 51/1000 | Loss: 0.00002595
Iteration 52/1000 | Loss: 0.00002595
Iteration 53/1000 | Loss: 0.00002594
Iteration 54/1000 | Loss: 0.00002594
Iteration 55/1000 | Loss: 0.00002594
Iteration 56/1000 | Loss: 0.00002593
Iteration 57/1000 | Loss: 0.00002593
Iteration 58/1000 | Loss: 0.00002592
Iteration 59/1000 | Loss: 0.00002592
Iteration 60/1000 | Loss: 0.00002592
Iteration 61/1000 | Loss: 0.00002592
Iteration 62/1000 | Loss: 0.00002592
Iteration 63/1000 | Loss: 0.00002592
Iteration 64/1000 | Loss: 0.00002592
Iteration 65/1000 | Loss: 0.00002592
Iteration 66/1000 | Loss: 0.00002592
Iteration 67/1000 | Loss: 0.00002592
Iteration 68/1000 | Loss: 0.00002592
Iteration 69/1000 | Loss: 0.00002592
Iteration 70/1000 | Loss: 0.00002592
Iteration 71/1000 | Loss: 0.00002592
Iteration 72/1000 | Loss: 0.00002591
Iteration 73/1000 | Loss: 0.00002591
Iteration 74/1000 | Loss: 0.00002591
Iteration 75/1000 | Loss: 0.00002591
Iteration 76/1000 | Loss: 0.00002591
Iteration 77/1000 | Loss: 0.00002591
Iteration 78/1000 | Loss: 0.00002591
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002591
Iteration 82/1000 | Loss: 0.00002591
Iteration 83/1000 | Loss: 0.00002590
Iteration 84/1000 | Loss: 0.00002590
Iteration 85/1000 | Loss: 0.00002590
Iteration 86/1000 | Loss: 0.00002590
Iteration 87/1000 | Loss: 0.00002590
Iteration 88/1000 | Loss: 0.00002590
Iteration 89/1000 | Loss: 0.00002589
Iteration 90/1000 | Loss: 0.00002589
Iteration 91/1000 | Loss: 0.00002589
Iteration 92/1000 | Loss: 0.00002589
Iteration 93/1000 | Loss: 0.00002589
Iteration 94/1000 | Loss: 0.00002589
Iteration 95/1000 | Loss: 0.00002589
Iteration 96/1000 | Loss: 0.00002589
Iteration 97/1000 | Loss: 0.00002589
Iteration 98/1000 | Loss: 0.00002589
Iteration 99/1000 | Loss: 0.00002589
Iteration 100/1000 | Loss: 0.00002589
Iteration 101/1000 | Loss: 0.00002589
Iteration 102/1000 | Loss: 0.00002589
Iteration 103/1000 | Loss: 0.00002588
Iteration 104/1000 | Loss: 0.00002588
Iteration 105/1000 | Loss: 0.00002588
Iteration 106/1000 | Loss: 0.00002588
Iteration 107/1000 | Loss: 0.00002588
Iteration 108/1000 | Loss: 0.00002588
Iteration 109/1000 | Loss: 0.00002588
Iteration 110/1000 | Loss: 0.00002588
Iteration 111/1000 | Loss: 0.00002588
Iteration 112/1000 | Loss: 0.00002587
Iteration 113/1000 | Loss: 0.00002587
Iteration 114/1000 | Loss: 0.00002587
Iteration 115/1000 | Loss: 0.00002587
Iteration 116/1000 | Loss: 0.00002587
Iteration 117/1000 | Loss: 0.00002587
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002585
Iteration 120/1000 | Loss: 0.00002585
Iteration 121/1000 | Loss: 0.00002585
Iteration 122/1000 | Loss: 0.00002585
Iteration 123/1000 | Loss: 0.00002585
Iteration 124/1000 | Loss: 0.00002585
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002585
Iteration 128/1000 | Loss: 0.00002585
Iteration 129/1000 | Loss: 0.00002585
Iteration 130/1000 | Loss: 0.00002585
Iteration 131/1000 | Loss: 0.00002584
Iteration 132/1000 | Loss: 0.00002584
Iteration 133/1000 | Loss: 0.00002584
Iteration 134/1000 | Loss: 0.00002584
Iteration 135/1000 | Loss: 0.00002584
Iteration 136/1000 | Loss: 0.00002584
Iteration 137/1000 | Loss: 0.00002584
Iteration 138/1000 | Loss: 0.00002584
Iteration 139/1000 | Loss: 0.00002584
Iteration 140/1000 | Loss: 0.00002584
Iteration 141/1000 | Loss: 0.00002584
Iteration 142/1000 | Loss: 0.00002584
Iteration 143/1000 | Loss: 0.00002584
Iteration 144/1000 | Loss: 0.00002584
Iteration 145/1000 | Loss: 0.00002584
Iteration 146/1000 | Loss: 0.00002584
Iteration 147/1000 | Loss: 0.00002584
Iteration 148/1000 | Loss: 0.00002584
Iteration 149/1000 | Loss: 0.00002584
Iteration 150/1000 | Loss: 0.00002584
Iteration 151/1000 | Loss: 0.00002584
Iteration 152/1000 | Loss: 0.00002584
Iteration 153/1000 | Loss: 0.00002584
Iteration 154/1000 | Loss: 0.00002584
Iteration 155/1000 | Loss: 0.00002584
Iteration 156/1000 | Loss: 0.00002584
Iteration 157/1000 | Loss: 0.00002584
Iteration 158/1000 | Loss: 0.00002584
Iteration 159/1000 | Loss: 0.00002584
Iteration 160/1000 | Loss: 0.00002584
Iteration 161/1000 | Loss: 0.00002584
Iteration 162/1000 | Loss: 0.00002584
Iteration 163/1000 | Loss: 0.00002584
Iteration 164/1000 | Loss: 0.00002584
Iteration 165/1000 | Loss: 0.00002584
Iteration 166/1000 | Loss: 0.00002584
Iteration 167/1000 | Loss: 0.00002584
Iteration 168/1000 | Loss: 0.00002584
Iteration 169/1000 | Loss: 0.00002584
Iteration 170/1000 | Loss: 0.00002584
Iteration 171/1000 | Loss: 0.00002584
Iteration 172/1000 | Loss: 0.00002584
Iteration 173/1000 | Loss: 0.00002584
Iteration 174/1000 | Loss: 0.00002584
Iteration 175/1000 | Loss: 0.00002584
Iteration 176/1000 | Loss: 0.00002584
Iteration 177/1000 | Loss: 0.00002584
Iteration 178/1000 | Loss: 0.00002584
Iteration 179/1000 | Loss: 0.00002584
Iteration 180/1000 | Loss: 0.00002584
Iteration 181/1000 | Loss: 0.00002584
Iteration 182/1000 | Loss: 0.00002584
Iteration 183/1000 | Loss: 0.00002584
Iteration 184/1000 | Loss: 0.00002584
Iteration 185/1000 | Loss: 0.00002584
Iteration 186/1000 | Loss: 0.00002584
Iteration 187/1000 | Loss: 0.00002584
Iteration 188/1000 | Loss: 0.00002584
Iteration 189/1000 | Loss: 0.00002584
Iteration 190/1000 | Loss: 0.00002584
Iteration 191/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.5836397981038317e-05, 2.5836397981038317e-05, 2.5836397981038317e-05, 2.5836397981038317e-05, 2.5836397981038317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5836397981038317e-05

Optimization complete. Final v2v error: 4.225548267364502 mm

Highest mean error: 4.672750473022461 mm for frame 130

Lowest mean error: 3.740718364715576 mm for frame 61

Saving results

Total time: 32.85386252403259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122168
Iteration 2/25 | Loss: 0.00296101
Iteration 3/25 | Loss: 0.00186520
Iteration 4/25 | Loss: 0.00168271
Iteration 5/25 | Loss: 0.00150190
Iteration 6/25 | Loss: 0.00143279
Iteration 7/25 | Loss: 0.00133700
Iteration 8/25 | Loss: 0.00126702
Iteration 9/25 | Loss: 0.00124091
Iteration 10/25 | Loss: 0.00120997
Iteration 11/25 | Loss: 0.00119243
Iteration 12/25 | Loss: 0.00118773
Iteration 13/25 | Loss: 0.00118622
Iteration 14/25 | Loss: 0.00119554
Iteration 15/25 | Loss: 0.00114995
Iteration 16/25 | Loss: 0.00115591
Iteration 17/25 | Loss: 0.00112983
Iteration 18/25 | Loss: 0.00112108
Iteration 19/25 | Loss: 0.00111641
Iteration 20/25 | Loss: 0.00111427
Iteration 21/25 | Loss: 0.00111239
Iteration 22/25 | Loss: 0.00111705
Iteration 23/25 | Loss: 0.00111029
Iteration 24/25 | Loss: 0.00110867
Iteration 25/25 | Loss: 0.00110826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65479779
Iteration 2/25 | Loss: 0.00642940
Iteration 3/25 | Loss: 0.00439962
Iteration 4/25 | Loss: 0.00439961
Iteration 5/25 | Loss: 0.00439961
Iteration 6/25 | Loss: 0.00439961
Iteration 7/25 | Loss: 0.00439961
Iteration 8/25 | Loss: 0.00439961
Iteration 9/25 | Loss: 0.00439961
Iteration 10/25 | Loss: 0.00439961
Iteration 11/25 | Loss: 0.00439961
Iteration 12/25 | Loss: 0.00439961
Iteration 13/25 | Loss: 0.00439961
Iteration 14/25 | Loss: 0.00439961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004399613477289677, 0.004399613477289677, 0.004399613477289677, 0.004399613477289677, 0.004399613477289677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004399613477289677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00439961
Iteration 2/1000 | Loss: 0.00034625
Iteration 3/1000 | Loss: 0.00026590
Iteration 4/1000 | Loss: 0.00026653
Iteration 5/1000 | Loss: 0.00020672
Iteration 6/1000 | Loss: 0.00273197
Iteration 7/1000 | Loss: 0.00430129
Iteration 8/1000 | Loss: 0.00888509
Iteration 9/1000 | Loss: 0.00345401
Iteration 10/1000 | Loss: 0.00056486
Iteration 11/1000 | Loss: 0.00185392
Iteration 12/1000 | Loss: 0.00106626
Iteration 13/1000 | Loss: 0.00089952
Iteration 14/1000 | Loss: 0.00098597
Iteration 15/1000 | Loss: 0.00104148
Iteration 16/1000 | Loss: 0.00066862
Iteration 17/1000 | Loss: 0.00043124
Iteration 18/1000 | Loss: 0.00044273
Iteration 19/1000 | Loss: 0.00021309
Iteration 20/1000 | Loss: 0.00228541
Iteration 21/1000 | Loss: 0.00146665
Iteration 22/1000 | Loss: 0.00153790
Iteration 23/1000 | Loss: 0.00124097
Iteration 24/1000 | Loss: 0.00426178
Iteration 25/1000 | Loss: 0.00188557
Iteration 26/1000 | Loss: 0.00149923
Iteration 27/1000 | Loss: 0.00085699
Iteration 28/1000 | Loss: 0.00083656
Iteration 29/1000 | Loss: 0.00051869
Iteration 30/1000 | Loss: 0.00064383
Iteration 31/1000 | Loss: 0.00037668
Iteration 32/1000 | Loss: 0.00012095
Iteration 33/1000 | Loss: 0.00125065
Iteration 34/1000 | Loss: 0.00140280
Iteration 35/1000 | Loss: 0.00120243
Iteration 36/1000 | Loss: 0.00093081
Iteration 37/1000 | Loss: 0.00053121
Iteration 38/1000 | Loss: 0.00029635
Iteration 39/1000 | Loss: 0.00061800
Iteration 40/1000 | Loss: 0.00067319
Iteration 41/1000 | Loss: 0.00450132
Iteration 42/1000 | Loss: 0.00130940
Iteration 43/1000 | Loss: 0.00031525
Iteration 44/1000 | Loss: 0.00010439
Iteration 45/1000 | Loss: 0.00157836
Iteration 46/1000 | Loss: 0.00158888
Iteration 47/1000 | Loss: 0.00174718
Iteration 48/1000 | Loss: 0.00037195
Iteration 49/1000 | Loss: 0.00261134
Iteration 50/1000 | Loss: 0.00035509
Iteration 51/1000 | Loss: 0.00030003
Iteration 52/1000 | Loss: 0.00169515
Iteration 53/1000 | Loss: 0.00091972
Iteration 54/1000 | Loss: 0.00099167
Iteration 55/1000 | Loss: 0.00060268
Iteration 56/1000 | Loss: 0.00082719
Iteration 57/1000 | Loss: 0.00056220
Iteration 58/1000 | Loss: 0.00130199
Iteration 59/1000 | Loss: 0.00040236
Iteration 60/1000 | Loss: 0.00059751
Iteration 61/1000 | Loss: 0.00081460
Iteration 62/1000 | Loss: 0.00083772
Iteration 63/1000 | Loss: 0.00067981
Iteration 64/1000 | Loss: 0.00328970
Iteration 65/1000 | Loss: 0.00008283
Iteration 66/1000 | Loss: 0.00007508
Iteration 67/1000 | Loss: 0.00057541
Iteration 68/1000 | Loss: 0.00036606
Iteration 69/1000 | Loss: 0.00184642
Iteration 70/1000 | Loss: 0.00046765
Iteration 71/1000 | Loss: 0.00171759
Iteration 72/1000 | Loss: 0.00043659
Iteration 73/1000 | Loss: 0.00033866
Iteration 74/1000 | Loss: 0.00089837
Iteration 75/1000 | Loss: 0.00027880
Iteration 76/1000 | Loss: 0.00067425
Iteration 77/1000 | Loss: 0.00014038
Iteration 78/1000 | Loss: 0.00008179
Iteration 79/1000 | Loss: 0.00033583
Iteration 80/1000 | Loss: 0.00007909
Iteration 81/1000 | Loss: 0.00059930
Iteration 82/1000 | Loss: 0.00011722
Iteration 83/1000 | Loss: 0.00006221
Iteration 84/1000 | Loss: 0.00039282
Iteration 85/1000 | Loss: 0.00048216
Iteration 86/1000 | Loss: 0.00025260
Iteration 87/1000 | Loss: 0.00049416
Iteration 88/1000 | Loss: 0.00050706
Iteration 89/1000 | Loss: 0.00056872
Iteration 90/1000 | Loss: 0.00006486
Iteration 91/1000 | Loss: 0.00005919
Iteration 92/1000 | Loss: 0.00042078
Iteration 93/1000 | Loss: 0.00013474
Iteration 94/1000 | Loss: 0.00049722
Iteration 95/1000 | Loss: 0.00046581
Iteration 96/1000 | Loss: 0.00062092
Iteration 97/1000 | Loss: 0.00048015
Iteration 98/1000 | Loss: 0.00036620
Iteration 99/1000 | Loss: 0.00077826
Iteration 100/1000 | Loss: 0.00071477
Iteration 101/1000 | Loss: 0.00016336
Iteration 102/1000 | Loss: 0.00009601
Iteration 103/1000 | Loss: 0.00011103
Iteration 104/1000 | Loss: 0.00008665
Iteration 105/1000 | Loss: 0.00008907
Iteration 106/1000 | Loss: 0.00010779
Iteration 107/1000 | Loss: 0.00010131
Iteration 108/1000 | Loss: 0.00008405
Iteration 109/1000 | Loss: 0.00041119
Iteration 110/1000 | Loss: 0.00060195
Iteration 111/1000 | Loss: 0.00065975
Iteration 112/1000 | Loss: 0.00076212
Iteration 113/1000 | Loss: 0.00013155
Iteration 114/1000 | Loss: 0.00034759
Iteration 115/1000 | Loss: 0.00045721
Iteration 116/1000 | Loss: 0.00031730
Iteration 117/1000 | Loss: 0.00056703
Iteration 118/1000 | Loss: 0.00032412
Iteration 119/1000 | Loss: 0.00019097
Iteration 120/1000 | Loss: 0.00021629
Iteration 121/1000 | Loss: 0.00056802
Iteration 122/1000 | Loss: 0.00052159
Iteration 123/1000 | Loss: 0.00065121
Iteration 124/1000 | Loss: 0.00014708
Iteration 125/1000 | Loss: 0.00015315
Iteration 126/1000 | Loss: 0.00030232
Iteration 127/1000 | Loss: 0.00033921
Iteration 128/1000 | Loss: 0.00054589
Iteration 129/1000 | Loss: 0.00051559
Iteration 130/1000 | Loss: 0.00024805
Iteration 131/1000 | Loss: 0.00023195
Iteration 132/1000 | Loss: 0.00049211
Iteration 133/1000 | Loss: 0.00031745
Iteration 134/1000 | Loss: 0.00028207
Iteration 135/1000 | Loss: 0.00013805
Iteration 136/1000 | Loss: 0.00006231
Iteration 137/1000 | Loss: 0.00005704
Iteration 138/1000 | Loss: 0.00005434
Iteration 139/1000 | Loss: 0.00126537
Iteration 140/1000 | Loss: 0.00013594
Iteration 141/1000 | Loss: 0.00005274
Iteration 142/1000 | Loss: 0.00005156
Iteration 143/1000 | Loss: 0.00005066
Iteration 144/1000 | Loss: 0.00006623
Iteration 145/1000 | Loss: 0.00139681
Iteration 146/1000 | Loss: 0.00007878
Iteration 147/1000 | Loss: 0.00005334
Iteration 148/1000 | Loss: 0.00126562
Iteration 149/1000 | Loss: 0.00320378
Iteration 150/1000 | Loss: 0.00022205
Iteration 151/1000 | Loss: 0.00005197
Iteration 152/1000 | Loss: 0.00004670
Iteration 153/1000 | Loss: 0.00004493
Iteration 154/1000 | Loss: 0.00004425
Iteration 155/1000 | Loss: 0.00004378
Iteration 156/1000 | Loss: 0.00004346
Iteration 157/1000 | Loss: 0.00125558
Iteration 158/1000 | Loss: 0.00719230
Iteration 159/1000 | Loss: 0.00279517
Iteration 160/1000 | Loss: 0.00512072
Iteration 161/1000 | Loss: 0.00013385
Iteration 162/1000 | Loss: 0.00098781
Iteration 163/1000 | Loss: 0.00005458
Iteration 164/1000 | Loss: 0.00004656
Iteration 165/1000 | Loss: 0.00004337
Iteration 166/1000 | Loss: 0.00004223
Iteration 167/1000 | Loss: 0.00004154
Iteration 168/1000 | Loss: 0.00004100
Iteration 169/1000 | Loss: 0.00004065
Iteration 170/1000 | Loss: 0.00004037
Iteration 171/1000 | Loss: 0.00004016
Iteration 172/1000 | Loss: 0.00004009
Iteration 173/1000 | Loss: 0.00004008
Iteration 174/1000 | Loss: 0.00003996
Iteration 175/1000 | Loss: 0.00003990
Iteration 176/1000 | Loss: 0.00003983
Iteration 177/1000 | Loss: 0.00003983
Iteration 178/1000 | Loss: 0.00003979
Iteration 179/1000 | Loss: 0.00003978
Iteration 180/1000 | Loss: 0.00003978
Iteration 181/1000 | Loss: 0.00003977
Iteration 182/1000 | Loss: 0.00003977
Iteration 183/1000 | Loss: 0.00003976
Iteration 184/1000 | Loss: 0.00003975
Iteration 185/1000 | Loss: 0.00125021
Iteration 186/1000 | Loss: 0.00635264
Iteration 187/1000 | Loss: 0.00007525
Iteration 188/1000 | Loss: 0.00003809
Iteration 189/1000 | Loss: 0.00002859
Iteration 190/1000 | Loss: 0.00002480
Iteration 191/1000 | Loss: 0.00002246
Iteration 192/1000 | Loss: 0.00002163
Iteration 193/1000 | Loss: 0.00002116
Iteration 194/1000 | Loss: 0.00002089
Iteration 195/1000 | Loss: 0.00002069
Iteration 196/1000 | Loss: 0.00002056
Iteration 197/1000 | Loss: 0.00002043
Iteration 198/1000 | Loss: 0.00002041
Iteration 199/1000 | Loss: 0.00002036
Iteration 200/1000 | Loss: 0.00002035
Iteration 201/1000 | Loss: 0.00002035
Iteration 202/1000 | Loss: 0.00002034
Iteration 203/1000 | Loss: 0.00002034
Iteration 204/1000 | Loss: 0.00002033
Iteration 205/1000 | Loss: 0.00002031
Iteration 206/1000 | Loss: 0.00002026
Iteration 207/1000 | Loss: 0.00002023
Iteration 208/1000 | Loss: 0.00002023
Iteration 209/1000 | Loss: 0.00002022
Iteration 210/1000 | Loss: 0.00002016
Iteration 211/1000 | Loss: 0.00002014
Iteration 212/1000 | Loss: 0.00002012
Iteration 213/1000 | Loss: 0.00002011
Iteration 214/1000 | Loss: 0.00002011
Iteration 215/1000 | Loss: 0.00002010
Iteration 216/1000 | Loss: 0.00002010
Iteration 217/1000 | Loss: 0.00002009
Iteration 218/1000 | Loss: 0.00002009
Iteration 219/1000 | Loss: 0.00002009
Iteration 220/1000 | Loss: 0.00002008
Iteration 221/1000 | Loss: 0.00002007
Iteration 222/1000 | Loss: 0.00002007
Iteration 223/1000 | Loss: 0.00002006
Iteration 224/1000 | Loss: 0.00002005
Iteration 225/1000 | Loss: 0.00002005
Iteration 226/1000 | Loss: 0.00002004
Iteration 227/1000 | Loss: 0.00002003
Iteration 228/1000 | Loss: 0.00002003
Iteration 229/1000 | Loss: 0.00002002
Iteration 230/1000 | Loss: 0.00002002
Iteration 231/1000 | Loss: 0.00002002
Iteration 232/1000 | Loss: 0.00002002
Iteration 233/1000 | Loss: 0.00002001
Iteration 234/1000 | Loss: 0.00001998
Iteration 235/1000 | Loss: 0.00001998
Iteration 236/1000 | Loss: 0.00001998
Iteration 237/1000 | Loss: 0.00001997
Iteration 238/1000 | Loss: 0.00001996
Iteration 239/1000 | Loss: 0.00001996
Iteration 240/1000 | Loss: 0.00001996
Iteration 241/1000 | Loss: 0.00001995
Iteration 242/1000 | Loss: 0.00001995
Iteration 243/1000 | Loss: 0.00001995
Iteration 244/1000 | Loss: 0.00001994
Iteration 245/1000 | Loss: 0.00001994
Iteration 246/1000 | Loss: 0.00001994
Iteration 247/1000 | Loss: 0.00001994
Iteration 248/1000 | Loss: 0.00001993
Iteration 249/1000 | Loss: 0.00001993
Iteration 250/1000 | Loss: 0.00001993
Iteration 251/1000 | Loss: 0.00001993
Iteration 252/1000 | Loss: 0.00001992
Iteration 253/1000 | Loss: 0.00001992
Iteration 254/1000 | Loss: 0.00001992
Iteration 255/1000 | Loss: 0.00001992
Iteration 256/1000 | Loss: 0.00001991
Iteration 257/1000 | Loss: 0.00001991
Iteration 258/1000 | Loss: 0.00001991
Iteration 259/1000 | Loss: 0.00001990
Iteration 260/1000 | Loss: 0.00001990
Iteration 261/1000 | Loss: 0.00001990
Iteration 262/1000 | Loss: 0.00001990
Iteration 263/1000 | Loss: 0.00001990
Iteration 264/1000 | Loss: 0.00001990
Iteration 265/1000 | Loss: 0.00001989
Iteration 266/1000 | Loss: 0.00001989
Iteration 267/1000 | Loss: 0.00001988
Iteration 268/1000 | Loss: 0.00001988
Iteration 269/1000 | Loss: 0.00001988
Iteration 270/1000 | Loss: 0.00001987
Iteration 271/1000 | Loss: 0.00001987
Iteration 272/1000 | Loss: 0.00001987
Iteration 273/1000 | Loss: 0.00001987
Iteration 274/1000 | Loss: 0.00001987
Iteration 275/1000 | Loss: 0.00001987
Iteration 276/1000 | Loss: 0.00001987
Iteration 277/1000 | Loss: 0.00001986
Iteration 278/1000 | Loss: 0.00001986
Iteration 279/1000 | Loss: 0.00001986
Iteration 280/1000 | Loss: 0.00001986
Iteration 281/1000 | Loss: 0.00001986
Iteration 282/1000 | Loss: 0.00001986
Iteration 283/1000 | Loss: 0.00001986
Iteration 284/1000 | Loss: 0.00001986
Iteration 285/1000 | Loss: 0.00001986
Iteration 286/1000 | Loss: 0.00001986
Iteration 287/1000 | Loss: 0.00001986
Iteration 288/1000 | Loss: 0.00001985
Iteration 289/1000 | Loss: 0.00001985
Iteration 290/1000 | Loss: 0.00001985
Iteration 291/1000 | Loss: 0.00001985
Iteration 292/1000 | Loss: 0.00001985
Iteration 293/1000 | Loss: 0.00001985
Iteration 294/1000 | Loss: 0.00001984
Iteration 295/1000 | Loss: 0.00001984
Iteration 296/1000 | Loss: 0.00001984
Iteration 297/1000 | Loss: 0.00001984
Iteration 298/1000 | Loss: 0.00001984
Iteration 299/1000 | Loss: 0.00001984
Iteration 300/1000 | Loss: 0.00001983
Iteration 301/1000 | Loss: 0.00001983
Iteration 302/1000 | Loss: 0.00001983
Iteration 303/1000 | Loss: 0.00001983
Iteration 304/1000 | Loss: 0.00001983
Iteration 305/1000 | Loss: 0.00001983
Iteration 306/1000 | Loss: 0.00001982
Iteration 307/1000 | Loss: 0.00001982
Iteration 308/1000 | Loss: 0.00001982
Iteration 309/1000 | Loss: 0.00001982
Iteration 310/1000 | Loss: 0.00001982
Iteration 311/1000 | Loss: 0.00001982
Iteration 312/1000 | Loss: 0.00001982
Iteration 313/1000 | Loss: 0.00001982
Iteration 314/1000 | Loss: 0.00001982
Iteration 315/1000 | Loss: 0.00001982
Iteration 316/1000 | Loss: 0.00001982
Iteration 317/1000 | Loss: 0.00001982
Iteration 318/1000 | Loss: 0.00001982
Iteration 319/1000 | Loss: 0.00001982
Iteration 320/1000 | Loss: 0.00001982
Iteration 321/1000 | Loss: 0.00001982
Iteration 322/1000 | Loss: 0.00001982
Iteration 323/1000 | Loss: 0.00001981
Iteration 324/1000 | Loss: 0.00001981
Iteration 325/1000 | Loss: 0.00001981
Iteration 326/1000 | Loss: 0.00001981
Iteration 327/1000 | Loss: 0.00001981
Iteration 328/1000 | Loss: 0.00001981
Iteration 329/1000 | Loss: 0.00001981
Iteration 330/1000 | Loss: 0.00001981
Iteration 331/1000 | Loss: 0.00001981
Iteration 332/1000 | Loss: 0.00001981
Iteration 333/1000 | Loss: 0.00001981
Iteration 334/1000 | Loss: 0.00001981
Iteration 335/1000 | Loss: 0.00001981
Iteration 336/1000 | Loss: 0.00001980
Iteration 337/1000 | Loss: 0.00001980
Iteration 338/1000 | Loss: 0.00001980
Iteration 339/1000 | Loss: 0.00001980
Iteration 340/1000 | Loss: 0.00001980
Iteration 341/1000 | Loss: 0.00001980
Iteration 342/1000 | Loss: 0.00001980
Iteration 343/1000 | Loss: 0.00001980
Iteration 344/1000 | Loss: 0.00001980
Iteration 345/1000 | Loss: 0.00001980
Iteration 346/1000 | Loss: 0.00001980
Iteration 347/1000 | Loss: 0.00001980
Iteration 348/1000 | Loss: 0.00001980
Iteration 349/1000 | Loss: 0.00001980
Iteration 350/1000 | Loss: 0.00001980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [1.9802952010650188e-05, 1.9802952010650188e-05, 1.9802952010650188e-05, 1.9802952010650188e-05, 1.9802952010650188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9802952010650188e-05

Optimization complete. Final v2v error: 3.701681613922119 mm

Highest mean error: 11.301389694213867 mm for frame 226

Lowest mean error: 3.0241968631744385 mm for frame 202

Saving results

Total time: 359.9808099269867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120023
Iteration 2/25 | Loss: 0.00217935
Iteration 3/25 | Loss: 0.00151050
Iteration 4/25 | Loss: 0.00139323
Iteration 5/25 | Loss: 0.00139861
Iteration 6/25 | Loss: 0.00130319
Iteration 7/25 | Loss: 0.00125036
Iteration 8/25 | Loss: 0.00121130
Iteration 9/25 | Loss: 0.00119466
Iteration 10/25 | Loss: 0.00118965
Iteration 11/25 | Loss: 0.00118562
Iteration 12/25 | Loss: 0.00118524
Iteration 13/25 | Loss: 0.00118052
Iteration 14/25 | Loss: 0.00117993
Iteration 15/25 | Loss: 0.00117223
Iteration 16/25 | Loss: 0.00116950
Iteration 17/25 | Loss: 0.00116827
Iteration 18/25 | Loss: 0.00116655
Iteration 19/25 | Loss: 0.00116562
Iteration 20/25 | Loss: 0.00116455
Iteration 21/25 | Loss: 0.00116720
Iteration 22/25 | Loss: 0.00116512
Iteration 23/25 | Loss: 0.00116493
Iteration 24/25 | Loss: 0.00116430
Iteration 25/25 | Loss: 0.00116588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68577111
Iteration 2/25 | Loss: 0.00494379
Iteration 3/25 | Loss: 0.00494378
Iteration 4/25 | Loss: 0.00494378
Iteration 5/25 | Loss: 0.00494378
Iteration 6/25 | Loss: 0.00494378
Iteration 7/25 | Loss: 0.00494378
Iteration 8/25 | Loss: 0.00494378
Iteration 9/25 | Loss: 0.00494378
Iteration 10/25 | Loss: 0.00494378
Iteration 11/25 | Loss: 0.00494378
Iteration 12/25 | Loss: 0.00494378
Iteration 13/25 | Loss: 0.00494378
Iteration 14/25 | Loss: 0.00494378
Iteration 15/25 | Loss: 0.00494378
Iteration 16/25 | Loss: 0.00494378
Iteration 17/25 | Loss: 0.00494378
Iteration 18/25 | Loss: 0.00494378
Iteration 19/25 | Loss: 0.00494378
Iteration 20/25 | Loss: 0.00494378
Iteration 21/25 | Loss: 0.00494378
Iteration 22/25 | Loss: 0.00494378
Iteration 23/25 | Loss: 0.00494378
Iteration 24/25 | Loss: 0.00494378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004943775944411755, 0.004943775944411755, 0.004943775944411755, 0.004943775944411755, 0.004943775944411755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004943775944411755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00494378
Iteration 2/1000 | Loss: 0.00045033
Iteration 3/1000 | Loss: 0.00133460
Iteration 4/1000 | Loss: 0.00190756
Iteration 5/1000 | Loss: 0.00438670
Iteration 6/1000 | Loss: 0.00227273
Iteration 7/1000 | Loss: 0.00217419
Iteration 8/1000 | Loss: 0.00074965
Iteration 9/1000 | Loss: 0.00106840
Iteration 10/1000 | Loss: 0.00130186
Iteration 11/1000 | Loss: 0.00295402
Iteration 12/1000 | Loss: 0.00131543
Iteration 13/1000 | Loss: 0.00139859
Iteration 14/1000 | Loss: 0.00098742
Iteration 15/1000 | Loss: 0.00138725
Iteration 16/1000 | Loss: 0.00066799
Iteration 17/1000 | Loss: 0.00067182
Iteration 18/1000 | Loss: 0.00057024
Iteration 19/1000 | Loss: 0.00030919
Iteration 20/1000 | Loss: 0.00200640
Iteration 21/1000 | Loss: 0.00315831
Iteration 22/1000 | Loss: 0.00284112
Iteration 23/1000 | Loss: 0.00306148
Iteration 24/1000 | Loss: 0.00231809
Iteration 25/1000 | Loss: 0.00206461
Iteration 26/1000 | Loss: 0.00161258
Iteration 27/1000 | Loss: 0.00124144
Iteration 28/1000 | Loss: 0.00303950
Iteration 29/1000 | Loss: 0.00611638
Iteration 30/1000 | Loss: 0.00414764
Iteration 31/1000 | Loss: 0.00133588
Iteration 32/1000 | Loss: 0.00175749
Iteration 33/1000 | Loss: 0.00201845
Iteration 34/1000 | Loss: 0.00209235
Iteration 35/1000 | Loss: 0.00242836
Iteration 36/1000 | Loss: 0.00293245
Iteration 37/1000 | Loss: 0.00358710
Iteration 38/1000 | Loss: 0.00380841
Iteration 39/1000 | Loss: 0.00219497
Iteration 40/1000 | Loss: 0.00267358
Iteration 41/1000 | Loss: 0.00215393
Iteration 42/1000 | Loss: 0.00145397
Iteration 43/1000 | Loss: 0.00274581
Iteration 44/1000 | Loss: 0.00223402
Iteration 45/1000 | Loss: 0.00257662
Iteration 46/1000 | Loss: 0.00369861
Iteration 47/1000 | Loss: 0.00340417
Iteration 48/1000 | Loss: 0.00235994
Iteration 49/1000 | Loss: 0.00333490
Iteration 50/1000 | Loss: 0.00149753
Iteration 51/1000 | Loss: 0.00124013
Iteration 52/1000 | Loss: 0.00214827
Iteration 53/1000 | Loss: 0.00172848
Iteration 54/1000 | Loss: 0.00188477
Iteration 55/1000 | Loss: 0.00156482
Iteration 56/1000 | Loss: 0.00308346
Iteration 57/1000 | Loss: 0.00247649
Iteration 58/1000 | Loss: 0.00154528
Iteration 59/1000 | Loss: 0.00225073
Iteration 60/1000 | Loss: 0.00178800
Iteration 61/1000 | Loss: 0.00135654
Iteration 62/1000 | Loss: 0.00145079
Iteration 63/1000 | Loss: 0.00166048
Iteration 64/1000 | Loss: 0.00360990
Iteration 65/1000 | Loss: 0.00402029
Iteration 66/1000 | Loss: 0.00298904
Iteration 67/1000 | Loss: 0.00148495
Iteration 68/1000 | Loss: 0.00219115
Iteration 69/1000 | Loss: 0.00237940
Iteration 70/1000 | Loss: 0.00126225
Iteration 71/1000 | Loss: 0.00164599
Iteration 72/1000 | Loss: 0.00126129
Iteration 73/1000 | Loss: 0.00195821
Iteration 74/1000 | Loss: 0.00138536
Iteration 75/1000 | Loss: 0.00131593
Iteration 76/1000 | Loss: 0.00155169
Iteration 77/1000 | Loss: 0.00101657
Iteration 78/1000 | Loss: 0.00166030
Iteration 79/1000 | Loss: 0.00133436
Iteration 80/1000 | Loss: 0.00144782
Iteration 81/1000 | Loss: 0.00160301
Iteration 82/1000 | Loss: 0.00169095
Iteration 83/1000 | Loss: 0.00200698
Iteration 84/1000 | Loss: 0.00170960
Iteration 85/1000 | Loss: 0.00229844
Iteration 86/1000 | Loss: 0.00207351
Iteration 87/1000 | Loss: 0.00164655
Iteration 88/1000 | Loss: 0.00210885
Iteration 89/1000 | Loss: 0.00189541
Iteration 90/1000 | Loss: 0.00240021
Iteration 91/1000 | Loss: 0.00124851
Iteration 92/1000 | Loss: 0.00141699
Iteration 93/1000 | Loss: 0.00149371
Iteration 94/1000 | Loss: 0.00132230
Iteration 95/1000 | Loss: 0.00141901
Iteration 96/1000 | Loss: 0.00109044
Iteration 97/1000 | Loss: 0.00137820
Iteration 98/1000 | Loss: 0.00153633
Iteration 99/1000 | Loss: 0.00145147
Iteration 100/1000 | Loss: 0.00153235
Iteration 101/1000 | Loss: 0.00165434
Iteration 102/1000 | Loss: 0.00120311
Iteration 103/1000 | Loss: 0.00155565
Iteration 104/1000 | Loss: 0.00109058
Iteration 105/1000 | Loss: 0.00209513
Iteration 106/1000 | Loss: 0.00130727
Iteration 107/1000 | Loss: 0.00219912
Iteration 108/1000 | Loss: 0.00133194
Iteration 109/1000 | Loss: 0.00207696
Iteration 110/1000 | Loss: 0.00052938
Iteration 111/1000 | Loss: 0.00087469
Iteration 112/1000 | Loss: 0.00114146
Iteration 113/1000 | Loss: 0.00113447
Iteration 114/1000 | Loss: 0.00045301
Iteration 115/1000 | Loss: 0.00027784
Iteration 116/1000 | Loss: 0.00077391
Iteration 117/1000 | Loss: 0.00010737
Iteration 118/1000 | Loss: 0.00016110
Iteration 119/1000 | Loss: 0.00011892
Iteration 120/1000 | Loss: 0.00006936
Iteration 121/1000 | Loss: 0.00061225
Iteration 122/1000 | Loss: 0.00052766
Iteration 123/1000 | Loss: 0.00067042
Iteration 124/1000 | Loss: 0.00068612
Iteration 125/1000 | Loss: 0.00077884
Iteration 126/1000 | Loss: 0.00054406
Iteration 127/1000 | Loss: 0.00056391
Iteration 128/1000 | Loss: 0.00052312
Iteration 129/1000 | Loss: 0.00136316
Iteration 130/1000 | Loss: 0.00161640
Iteration 131/1000 | Loss: 0.00083932
Iteration 132/1000 | Loss: 0.00026849
Iteration 133/1000 | Loss: 0.00006816
Iteration 134/1000 | Loss: 0.00095974
Iteration 135/1000 | Loss: 0.00041438
Iteration 136/1000 | Loss: 0.00058863
Iteration 137/1000 | Loss: 0.00035310
Iteration 138/1000 | Loss: 0.00042753
Iteration 139/1000 | Loss: 0.00074513
Iteration 140/1000 | Loss: 0.00031778
Iteration 141/1000 | Loss: 0.00013311
Iteration 142/1000 | Loss: 0.00030130
Iteration 143/1000 | Loss: 0.00008648
Iteration 144/1000 | Loss: 0.00007560
Iteration 145/1000 | Loss: 0.00012382
Iteration 146/1000 | Loss: 0.00006513
Iteration 147/1000 | Loss: 0.00117521
Iteration 148/1000 | Loss: 0.00026719
Iteration 149/1000 | Loss: 0.00091400
Iteration 150/1000 | Loss: 0.00011280
Iteration 151/1000 | Loss: 0.00006428
Iteration 152/1000 | Loss: 0.00005185
Iteration 153/1000 | Loss: 0.00004573
Iteration 154/1000 | Loss: 0.00004176
Iteration 155/1000 | Loss: 0.00053499
Iteration 156/1000 | Loss: 0.00006662
Iteration 157/1000 | Loss: 0.00004272
Iteration 158/1000 | Loss: 0.00003679
Iteration 159/1000 | Loss: 0.00003383
Iteration 160/1000 | Loss: 0.00004119
Iteration 161/1000 | Loss: 0.00003737
Iteration 162/1000 | Loss: 0.00022948
Iteration 163/1000 | Loss: 0.00032395
Iteration 164/1000 | Loss: 0.00013124
Iteration 165/1000 | Loss: 0.00045028
Iteration 166/1000 | Loss: 0.00016135
Iteration 167/1000 | Loss: 0.00024429
Iteration 168/1000 | Loss: 0.00022201
Iteration 169/1000 | Loss: 0.00026068
Iteration 170/1000 | Loss: 0.00019934
Iteration 171/1000 | Loss: 0.00008186
Iteration 172/1000 | Loss: 0.00003395
Iteration 173/1000 | Loss: 0.00003214
Iteration 174/1000 | Loss: 0.00040777
Iteration 175/1000 | Loss: 0.00020397
Iteration 176/1000 | Loss: 0.00040499
Iteration 177/1000 | Loss: 0.00026136
Iteration 178/1000 | Loss: 0.00004372
Iteration 179/1000 | Loss: 0.00010604
Iteration 180/1000 | Loss: 0.00003309
Iteration 181/1000 | Loss: 0.00003432
Iteration 182/1000 | Loss: 0.00003380
Iteration 183/1000 | Loss: 0.00051749
Iteration 184/1000 | Loss: 0.00018292
Iteration 185/1000 | Loss: 0.00003251
Iteration 186/1000 | Loss: 0.00052650
Iteration 187/1000 | Loss: 0.00033973
Iteration 188/1000 | Loss: 0.00036305
Iteration 189/1000 | Loss: 0.00004249
Iteration 190/1000 | Loss: 0.00015263
Iteration 191/1000 | Loss: 0.00033283
Iteration 192/1000 | Loss: 0.00042872
Iteration 193/1000 | Loss: 0.00012377
Iteration 194/1000 | Loss: 0.00019627
Iteration 195/1000 | Loss: 0.00010999
Iteration 196/1000 | Loss: 0.00005903
Iteration 197/1000 | Loss: 0.00004361
Iteration 198/1000 | Loss: 0.00004152
Iteration 199/1000 | Loss: 0.00003515
Iteration 200/1000 | Loss: 0.00003074
Iteration 201/1000 | Loss: 0.00017686
Iteration 202/1000 | Loss: 0.00031585
Iteration 203/1000 | Loss: 0.00014521
Iteration 204/1000 | Loss: 0.00030216
Iteration 205/1000 | Loss: 0.00023214
Iteration 206/1000 | Loss: 0.00033454
Iteration 207/1000 | Loss: 0.00005727
Iteration 208/1000 | Loss: 0.00003563
Iteration 209/1000 | Loss: 0.00003141
Iteration 210/1000 | Loss: 0.00002834
Iteration 211/1000 | Loss: 0.00002701
Iteration 212/1000 | Loss: 0.00002624
Iteration 213/1000 | Loss: 0.00002577
Iteration 214/1000 | Loss: 0.00002597
Iteration 215/1000 | Loss: 0.00002550
Iteration 216/1000 | Loss: 0.00003398
Iteration 217/1000 | Loss: 0.00003029
Iteration 218/1000 | Loss: 0.00003364
Iteration 219/1000 | Loss: 0.00002872
Iteration 220/1000 | Loss: 0.00003240
Iteration 221/1000 | Loss: 0.00003148
Iteration 222/1000 | Loss: 0.00003281
Iteration 223/1000 | Loss: 0.00003032
Iteration 224/1000 | Loss: 0.00002911
Iteration 225/1000 | Loss: 0.00002855
Iteration 226/1000 | Loss: 0.00003043
Iteration 227/1000 | Loss: 0.00002602
Iteration 228/1000 | Loss: 0.00002521
Iteration 229/1000 | Loss: 0.00003316
Iteration 230/1000 | Loss: 0.00003544
Iteration 231/1000 | Loss: 0.00002871
Iteration 232/1000 | Loss: 0.00002613
Iteration 233/1000 | Loss: 0.00002455
Iteration 234/1000 | Loss: 0.00002408
Iteration 235/1000 | Loss: 0.00002379
Iteration 236/1000 | Loss: 0.00002378
Iteration 237/1000 | Loss: 0.00002377
Iteration 238/1000 | Loss: 0.00002373
Iteration 239/1000 | Loss: 0.00002371
Iteration 240/1000 | Loss: 0.00002370
Iteration 241/1000 | Loss: 0.00002370
Iteration 242/1000 | Loss: 0.00002370
Iteration 243/1000 | Loss: 0.00002369
Iteration 244/1000 | Loss: 0.00002369
Iteration 245/1000 | Loss: 0.00002369
Iteration 246/1000 | Loss: 0.00002368
Iteration 247/1000 | Loss: 0.00002366
Iteration 248/1000 | Loss: 0.00002365
Iteration 249/1000 | Loss: 0.00002396
Iteration 250/1000 | Loss: 0.00002365
Iteration 251/1000 | Loss: 0.00002382
Iteration 252/1000 | Loss: 0.00002382
Iteration 253/1000 | Loss: 0.00002381
Iteration 254/1000 | Loss: 0.00002375
Iteration 255/1000 | Loss: 0.00002373
Iteration 256/1000 | Loss: 0.00002371
Iteration 257/1000 | Loss: 0.00002371
Iteration 258/1000 | Loss: 0.00002359
Iteration 259/1000 | Loss: 0.00002357
Iteration 260/1000 | Loss: 0.00002357
Iteration 261/1000 | Loss: 0.00002357
Iteration 262/1000 | Loss: 0.00002356
Iteration 263/1000 | Loss: 0.00002355
Iteration 264/1000 | Loss: 0.00002355
Iteration 265/1000 | Loss: 0.00002354
Iteration 266/1000 | Loss: 0.00002354
Iteration 267/1000 | Loss: 0.00002353
Iteration 268/1000 | Loss: 0.00002353
Iteration 269/1000 | Loss: 0.00002353
Iteration 270/1000 | Loss: 0.00002352
Iteration 271/1000 | Loss: 0.00002352
Iteration 272/1000 | Loss: 0.00002352
Iteration 273/1000 | Loss: 0.00002352
Iteration 274/1000 | Loss: 0.00002352
Iteration 275/1000 | Loss: 0.00002352
Iteration 276/1000 | Loss: 0.00002351
Iteration 277/1000 | Loss: 0.00002351
Iteration 278/1000 | Loss: 0.00002351
Iteration 279/1000 | Loss: 0.00002351
Iteration 280/1000 | Loss: 0.00002351
Iteration 281/1000 | Loss: 0.00002351
Iteration 282/1000 | Loss: 0.00002351
Iteration 283/1000 | Loss: 0.00002351
Iteration 284/1000 | Loss: 0.00002368
Iteration 285/1000 | Loss: 0.00002368
Iteration 286/1000 | Loss: 0.00002368
Iteration 287/1000 | Loss: 0.00002368
Iteration 288/1000 | Loss: 0.00002351
Iteration 289/1000 | Loss: 0.00002351
Iteration 290/1000 | Loss: 0.00002351
Iteration 291/1000 | Loss: 0.00002351
Iteration 292/1000 | Loss: 0.00002351
Iteration 293/1000 | Loss: 0.00002351
Iteration 294/1000 | Loss: 0.00002351
Iteration 295/1000 | Loss: 0.00002351
Iteration 296/1000 | Loss: 0.00002351
Iteration 297/1000 | Loss: 0.00002351
Iteration 298/1000 | Loss: 0.00002351
Iteration 299/1000 | Loss: 0.00002351
Iteration 300/1000 | Loss: 0.00002351
Iteration 301/1000 | Loss: 0.00002351
Iteration 302/1000 | Loss: 0.00002351
Iteration 303/1000 | Loss: 0.00002351
Iteration 304/1000 | Loss: 0.00002351
Iteration 305/1000 | Loss: 0.00002351
Iteration 306/1000 | Loss: 0.00002351
Iteration 307/1000 | Loss: 0.00002351
Iteration 308/1000 | Loss: 0.00002351
Iteration 309/1000 | Loss: 0.00002351
Iteration 310/1000 | Loss: 0.00002351
Iteration 311/1000 | Loss: 0.00002351
Iteration 312/1000 | Loss: 0.00002351
Iteration 313/1000 | Loss: 0.00002351
Iteration 314/1000 | Loss: 0.00002351
Iteration 315/1000 | Loss: 0.00002351
Iteration 316/1000 | Loss: 0.00002351
Iteration 317/1000 | Loss: 0.00002351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.350907561776694e-05, 2.350907561776694e-05, 2.350907561776694e-05, 2.350907561776694e-05, 2.350907561776694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.350907561776694e-05

Optimization complete. Final v2v error: 3.709289312362671 mm

Highest mean error: 21.307939529418945 mm for frame 135

Lowest mean error: 2.9383862018585205 mm for frame 239

Saving results

Total time: 432.5980203151703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884157
Iteration 2/25 | Loss: 0.00107485
Iteration 3/25 | Loss: 0.00093056
Iteration 4/25 | Loss: 0.00091035
Iteration 5/25 | Loss: 0.00090634
Iteration 6/25 | Loss: 0.00090545
Iteration 7/25 | Loss: 0.00090545
Iteration 8/25 | Loss: 0.00090545
Iteration 9/25 | Loss: 0.00090545
Iteration 10/25 | Loss: 0.00090545
Iteration 11/25 | Loss: 0.00090545
Iteration 12/25 | Loss: 0.00090545
Iteration 13/25 | Loss: 0.00090545
Iteration 14/25 | Loss: 0.00090545
Iteration 15/25 | Loss: 0.00090545
Iteration 16/25 | Loss: 0.00090545
Iteration 17/25 | Loss: 0.00090545
Iteration 18/25 | Loss: 0.00090545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009054513648152351, 0.0009054513648152351, 0.0009054513648152351, 0.0009054513648152351, 0.0009054513648152351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009054513648152351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68191731
Iteration 2/25 | Loss: 0.00275205
Iteration 3/25 | Loss: 0.00275205
Iteration 4/25 | Loss: 0.00275205
Iteration 5/25 | Loss: 0.00275205
Iteration 6/25 | Loss: 0.00275205
Iteration 7/25 | Loss: 0.00275205
Iteration 8/25 | Loss: 0.00275205
Iteration 9/25 | Loss: 0.00275205
Iteration 10/25 | Loss: 0.00275205
Iteration 11/25 | Loss: 0.00275205
Iteration 12/25 | Loss: 0.00275205
Iteration 13/25 | Loss: 0.00275205
Iteration 14/25 | Loss: 0.00275205
Iteration 15/25 | Loss: 0.00275205
Iteration 16/25 | Loss: 0.00275205
Iteration 17/25 | Loss: 0.00275205
Iteration 18/25 | Loss: 0.00275205
Iteration 19/25 | Loss: 0.00275205
Iteration 20/25 | Loss: 0.00275205
Iteration 21/25 | Loss: 0.00275205
Iteration 22/25 | Loss: 0.00275205
Iteration 23/25 | Loss: 0.00275205
Iteration 24/25 | Loss: 0.00275205
Iteration 25/25 | Loss: 0.00275205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275205
Iteration 2/1000 | Loss: 0.00002406
Iteration 3/1000 | Loss: 0.00001703
Iteration 4/1000 | Loss: 0.00001447
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001307
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001283
Iteration 16/1000 | Loss: 0.00001282
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001279
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001275
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001274
Iteration 47/1000 | Loss: 0.00001274
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001273
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001272
Iteration 54/1000 | Loss: 0.00001272
Iteration 55/1000 | Loss: 0.00001272
Iteration 56/1000 | Loss: 0.00001272
Iteration 57/1000 | Loss: 0.00001272
Iteration 58/1000 | Loss: 0.00001272
Iteration 59/1000 | Loss: 0.00001272
Iteration 60/1000 | Loss: 0.00001271
Iteration 61/1000 | Loss: 0.00001271
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001270
Iteration 64/1000 | Loss: 0.00001270
Iteration 65/1000 | Loss: 0.00001270
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001264
Iteration 95/1000 | Loss: 0.00001263
Iteration 96/1000 | Loss: 0.00001263
Iteration 97/1000 | Loss: 0.00001263
Iteration 98/1000 | Loss: 0.00001263
Iteration 99/1000 | Loss: 0.00001263
Iteration 100/1000 | Loss: 0.00001263
Iteration 101/1000 | Loss: 0.00001263
Iteration 102/1000 | Loss: 0.00001263
Iteration 103/1000 | Loss: 0.00001262
Iteration 104/1000 | Loss: 0.00001262
Iteration 105/1000 | Loss: 0.00001262
Iteration 106/1000 | Loss: 0.00001262
Iteration 107/1000 | Loss: 0.00001262
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001261
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001260
Iteration 118/1000 | Loss: 0.00001260
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00001260
Iteration 121/1000 | Loss: 0.00001260
Iteration 122/1000 | Loss: 0.00001260
Iteration 123/1000 | Loss: 0.00001260
Iteration 124/1000 | Loss: 0.00001260
Iteration 125/1000 | Loss: 0.00001259
Iteration 126/1000 | Loss: 0.00001259
Iteration 127/1000 | Loss: 0.00001259
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001259
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Iteration 133/1000 | Loss: 0.00001259
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001258
Iteration 147/1000 | Loss: 0.00001258
Iteration 148/1000 | Loss: 0.00001258
Iteration 149/1000 | Loss: 0.00001258
Iteration 150/1000 | Loss: 0.00001258
Iteration 151/1000 | Loss: 0.00001258
Iteration 152/1000 | Loss: 0.00001258
Iteration 153/1000 | Loss: 0.00001258
Iteration 154/1000 | Loss: 0.00001258
Iteration 155/1000 | Loss: 0.00001258
Iteration 156/1000 | Loss: 0.00001258
Iteration 157/1000 | Loss: 0.00001258
Iteration 158/1000 | Loss: 0.00001258
Iteration 159/1000 | Loss: 0.00001258
Iteration 160/1000 | Loss: 0.00001258
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001258
Iteration 163/1000 | Loss: 0.00001258
Iteration 164/1000 | Loss: 0.00001258
Iteration 165/1000 | Loss: 0.00001258
Iteration 166/1000 | Loss: 0.00001258
Iteration 167/1000 | Loss: 0.00001258
Iteration 168/1000 | Loss: 0.00001258
Iteration 169/1000 | Loss: 0.00001258
Iteration 170/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.2583526768139564e-05, 1.2583526768139564e-05, 1.2583526768139564e-05, 1.2583526768139564e-05, 1.2583526768139564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2583526768139564e-05

Optimization complete. Final v2v error: 3.065751314163208 mm

Highest mean error: 3.46028733253479 mm for frame 31

Lowest mean error: 2.5500478744506836 mm for frame 12

Saving results

Total time: 30.76059603691101
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137625
Iteration 2/25 | Loss: 0.00299798
Iteration 3/25 | Loss: 0.00180455
Iteration 4/25 | Loss: 0.00183597
Iteration 5/25 | Loss: 0.00179066
Iteration 6/25 | Loss: 0.00163084
Iteration 7/25 | Loss: 0.00143809
Iteration 8/25 | Loss: 0.00142912
Iteration 9/25 | Loss: 0.00141074
Iteration 10/25 | Loss: 0.00137045
Iteration 11/25 | Loss: 0.00133636
Iteration 12/25 | Loss: 0.00131191
Iteration 13/25 | Loss: 0.00131190
Iteration 14/25 | Loss: 0.00127423
Iteration 15/25 | Loss: 0.00126024
Iteration 16/25 | Loss: 0.00130919
Iteration 17/25 | Loss: 0.00127716
Iteration 18/25 | Loss: 0.00126981
Iteration 19/25 | Loss: 0.00125329
Iteration 20/25 | Loss: 0.00123757
Iteration 21/25 | Loss: 0.00124863
Iteration 22/25 | Loss: 0.00123049
Iteration 23/25 | Loss: 0.00120747
Iteration 24/25 | Loss: 0.00122118
Iteration 25/25 | Loss: 0.00125867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78208280
Iteration 2/25 | Loss: 0.01496729
Iteration 3/25 | Loss: 0.00889427
Iteration 4/25 | Loss: 0.00889422
Iteration 5/25 | Loss: 0.00889422
Iteration 6/25 | Loss: 0.00889422
Iteration 7/25 | Loss: 0.00889422
Iteration 8/25 | Loss: 0.00889422
Iteration 9/25 | Loss: 0.00889422
Iteration 10/25 | Loss: 0.00889422
Iteration 11/25 | Loss: 0.00889421
Iteration 12/25 | Loss: 0.00889421
Iteration 13/25 | Loss: 0.00889421
Iteration 14/25 | Loss: 0.00889421
Iteration 15/25 | Loss: 0.00889421
Iteration 16/25 | Loss: 0.00889421
Iteration 17/25 | Loss: 0.00889421
Iteration 18/25 | Loss: 0.00889421
Iteration 19/25 | Loss: 0.00889421
Iteration 20/25 | Loss: 0.00889421
Iteration 21/25 | Loss: 0.00889421
Iteration 22/25 | Loss: 0.00889421
Iteration 23/25 | Loss: 0.00889421
Iteration 24/25 | Loss: 0.00889421
Iteration 25/25 | Loss: 0.00889421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00889421
Iteration 2/1000 | Loss: 0.01032295
Iteration 3/1000 | Loss: 0.00953909
Iteration 4/1000 | Loss: 0.00812388
Iteration 5/1000 | Loss: 0.00736893
Iteration 6/1000 | Loss: 0.00504622
Iteration 7/1000 | Loss: 0.01017053
Iteration 8/1000 | Loss: 0.00554799
Iteration 9/1000 | Loss: 0.00590053
Iteration 10/1000 | Loss: 0.00784160
Iteration 11/1000 | Loss: 0.00923170
Iteration 12/1000 | Loss: 0.01062098
Iteration 13/1000 | Loss: 0.00429527
Iteration 14/1000 | Loss: 0.00510037
Iteration 15/1000 | Loss: 0.00445491
Iteration 16/1000 | Loss: 0.00491498
Iteration 17/1000 | Loss: 0.00511002
Iteration 18/1000 | Loss: 0.00816195
Iteration 19/1000 | Loss: 0.00457594
Iteration 20/1000 | Loss: 0.00612020
Iteration 21/1000 | Loss: 0.00511163
Iteration 22/1000 | Loss: 0.00350172
Iteration 23/1000 | Loss: 0.00397090
Iteration 24/1000 | Loss: 0.00596594
Iteration 25/1000 | Loss: 0.00610053
Iteration 26/1000 | Loss: 0.00845914
Iteration 27/1000 | Loss: 0.00659003
Iteration 28/1000 | Loss: 0.00612557
Iteration 29/1000 | Loss: 0.00496811
Iteration 30/1000 | Loss: 0.00602143
Iteration 31/1000 | Loss: 0.00643212
Iteration 32/1000 | Loss: 0.00466933
Iteration 33/1000 | Loss: 0.00473973
Iteration 34/1000 | Loss: 0.00475272
Iteration 35/1000 | Loss: 0.01010989
Iteration 36/1000 | Loss: 0.00659076
Iteration 37/1000 | Loss: 0.00700736
Iteration 38/1000 | Loss: 0.00547284
Iteration 39/1000 | Loss: 0.00497223
Iteration 40/1000 | Loss: 0.00308640
Iteration 41/1000 | Loss: 0.00320500
Iteration 42/1000 | Loss: 0.00737362
Iteration 43/1000 | Loss: 0.00652267
Iteration 44/1000 | Loss: 0.00420368
Iteration 45/1000 | Loss: 0.00350172
Iteration 46/1000 | Loss: 0.00364228
Iteration 47/1000 | Loss: 0.00386771
Iteration 48/1000 | Loss: 0.00772289
Iteration 49/1000 | Loss: 0.00442486
Iteration 50/1000 | Loss: 0.00362271
Iteration 51/1000 | Loss: 0.00386328
Iteration 52/1000 | Loss: 0.00368259
Iteration 53/1000 | Loss: 0.00620194
Iteration 54/1000 | Loss: 0.00365032
Iteration 55/1000 | Loss: 0.00746894
Iteration 56/1000 | Loss: 0.00509995
Iteration 57/1000 | Loss: 0.00442671
Iteration 58/1000 | Loss: 0.00397151
Iteration 59/1000 | Loss: 0.00535466
Iteration 60/1000 | Loss: 0.00596373
Iteration 61/1000 | Loss: 0.00368080
Iteration 62/1000 | Loss: 0.00375799
Iteration 63/1000 | Loss: 0.00364791
Iteration 64/1000 | Loss: 0.00546846
Iteration 65/1000 | Loss: 0.00446913
Iteration 66/1000 | Loss: 0.00613958
Iteration 67/1000 | Loss: 0.00527232
Iteration 68/1000 | Loss: 0.00728815
Iteration 69/1000 | Loss: 0.00303745
Iteration 70/1000 | Loss: 0.00349920
Iteration 71/1000 | Loss: 0.00362771
Iteration 72/1000 | Loss: 0.00407541
Iteration 73/1000 | Loss: 0.00333657
Iteration 74/1000 | Loss: 0.00356283
Iteration 75/1000 | Loss: 0.00352181
Iteration 76/1000 | Loss: 0.00698203
Iteration 77/1000 | Loss: 0.00339389
Iteration 78/1000 | Loss: 0.00490883
Iteration 79/1000 | Loss: 0.00372560
Iteration 80/1000 | Loss: 0.00449880
Iteration 81/1000 | Loss: 0.00382297
Iteration 82/1000 | Loss: 0.00357982
Iteration 83/1000 | Loss: 0.00376613
Iteration 84/1000 | Loss: 0.00379650
Iteration 85/1000 | Loss: 0.00697870
Iteration 86/1000 | Loss: 0.00400603
Iteration 87/1000 | Loss: 0.00322283
Iteration 88/1000 | Loss: 0.00319143
Iteration 89/1000 | Loss: 0.00354400
Iteration 90/1000 | Loss: 0.00351224
Iteration 91/1000 | Loss: 0.00365145
Iteration 92/1000 | Loss: 0.00339478
Iteration 93/1000 | Loss: 0.00330301
Iteration 94/1000 | Loss: 0.00344503
Iteration 95/1000 | Loss: 0.00364185
Iteration 96/1000 | Loss: 0.00353764
Iteration 97/1000 | Loss: 0.00620757
Iteration 98/1000 | Loss: 0.00519272
Iteration 99/1000 | Loss: 0.00419035
Iteration 100/1000 | Loss: 0.00388715
Iteration 101/1000 | Loss: 0.00576270
Iteration 102/1000 | Loss: 0.00362968
Iteration 103/1000 | Loss: 0.00382226
Iteration 104/1000 | Loss: 0.00387064
Iteration 105/1000 | Loss: 0.00477092
Iteration 106/1000 | Loss: 0.00511869
Iteration 107/1000 | Loss: 0.00241621
Iteration 108/1000 | Loss: 0.00438351
Iteration 109/1000 | Loss: 0.00315145
Iteration 110/1000 | Loss: 0.00321963
Iteration 111/1000 | Loss: 0.00590292
Iteration 112/1000 | Loss: 0.00336194
Iteration 113/1000 | Loss: 0.00224767
Iteration 114/1000 | Loss: 0.00334601
Iteration 115/1000 | Loss: 0.00336582
Iteration 116/1000 | Loss: 0.00359672
Iteration 117/1000 | Loss: 0.00305041
Iteration 118/1000 | Loss: 0.00464418
Iteration 119/1000 | Loss: 0.00327168
Iteration 120/1000 | Loss: 0.00519556
Iteration 121/1000 | Loss: 0.00310354
Iteration 122/1000 | Loss: 0.00517139
Iteration 123/1000 | Loss: 0.00371776
Iteration 124/1000 | Loss: 0.00301448
Iteration 125/1000 | Loss: 0.00293416
Iteration 126/1000 | Loss: 0.00300388
Iteration 127/1000 | Loss: 0.00297630
Iteration 128/1000 | Loss: 0.00505861
Iteration 129/1000 | Loss: 0.00331155
Iteration 130/1000 | Loss: 0.00356164
Iteration 131/1000 | Loss: 0.00303337
Iteration 132/1000 | Loss: 0.00332563
Iteration 133/1000 | Loss: 0.00302345
Iteration 134/1000 | Loss: 0.00433536
Iteration 135/1000 | Loss: 0.00480506
Iteration 136/1000 | Loss: 0.00630918
Iteration 137/1000 | Loss: 0.00284377
Iteration 138/1000 | Loss: 0.00628725
Iteration 139/1000 | Loss: 0.00330469
Iteration 140/1000 | Loss: 0.00533667
Iteration 141/1000 | Loss: 0.00467066
Iteration 142/1000 | Loss: 0.00521237
Iteration 143/1000 | Loss: 0.00389665
Iteration 144/1000 | Loss: 0.00472716
Iteration 145/1000 | Loss: 0.00488751
Iteration 146/1000 | Loss: 0.00303423
Iteration 147/1000 | Loss: 0.00312852
Iteration 148/1000 | Loss: 0.00324474
Iteration 149/1000 | Loss: 0.00295268
Iteration 150/1000 | Loss: 0.00280504
Iteration 151/1000 | Loss: 0.00473247
Iteration 152/1000 | Loss: 0.00655670
Iteration 153/1000 | Loss: 0.00464622
Iteration 154/1000 | Loss: 0.00335908
Iteration 155/1000 | Loss: 0.00332880
Iteration 156/1000 | Loss: 0.00333008
Iteration 157/1000 | Loss: 0.00465358
Iteration 158/1000 | Loss: 0.00265432
Iteration 159/1000 | Loss: 0.00286159
Iteration 160/1000 | Loss: 0.00288149
Iteration 161/1000 | Loss: 0.00709847
Iteration 162/1000 | Loss: 0.00656816
Iteration 163/1000 | Loss: 0.00563499
Iteration 164/1000 | Loss: 0.00429396
Iteration 165/1000 | Loss: 0.01233891
Iteration 166/1000 | Loss: 0.00536196
Iteration 167/1000 | Loss: 0.00523960
Iteration 168/1000 | Loss: 0.00385208
Iteration 169/1000 | Loss: 0.00269408
Iteration 170/1000 | Loss: 0.00309143
Iteration 171/1000 | Loss: 0.00291219
Iteration 172/1000 | Loss: 0.00324325
Iteration 173/1000 | Loss: 0.00382119
Iteration 174/1000 | Loss: 0.00294067
Iteration 175/1000 | Loss: 0.00199395
Iteration 176/1000 | Loss: 0.00197651
Iteration 177/1000 | Loss: 0.00258434
Iteration 178/1000 | Loss: 0.00245063
Iteration 179/1000 | Loss: 0.00406689
Iteration 180/1000 | Loss: 0.00165403
Iteration 181/1000 | Loss: 0.00206882
Iteration 182/1000 | Loss: 0.00120678
Iteration 183/1000 | Loss: 0.00217084
Iteration 184/1000 | Loss: 0.00150285
Iteration 185/1000 | Loss: 0.00169840
Iteration 186/1000 | Loss: 0.00248307
Iteration 187/1000 | Loss: 0.00171816
Iteration 188/1000 | Loss: 0.00137247
Iteration 189/1000 | Loss: 0.00167091
Iteration 190/1000 | Loss: 0.00135147
Iteration 191/1000 | Loss: 0.00124873
Iteration 192/1000 | Loss: 0.00147872
Iteration 193/1000 | Loss: 0.00127332
Iteration 194/1000 | Loss: 0.00181578
Iteration 195/1000 | Loss: 0.00126557
Iteration 196/1000 | Loss: 0.00173479
Iteration 197/1000 | Loss: 0.00195335
Iteration 198/1000 | Loss: 0.00137554
Iteration 199/1000 | Loss: 0.00228526
Iteration 200/1000 | Loss: 0.00112188
Iteration 201/1000 | Loss: 0.00315408
Iteration 202/1000 | Loss: 0.00106811
Iteration 203/1000 | Loss: 0.00152190
Iteration 204/1000 | Loss: 0.00171349
Iteration 205/1000 | Loss: 0.00088659
Iteration 206/1000 | Loss: 0.00112165
Iteration 207/1000 | Loss: 0.00127251
Iteration 208/1000 | Loss: 0.00056216
Iteration 209/1000 | Loss: 0.00105679
Iteration 210/1000 | Loss: 0.00101218
Iteration 211/1000 | Loss: 0.00128692
Iteration 212/1000 | Loss: 0.00129145
Iteration 213/1000 | Loss: 0.00148817
Iteration 214/1000 | Loss: 0.00132658
Iteration 215/1000 | Loss: 0.00125067
Iteration 216/1000 | Loss: 0.00118972
Iteration 217/1000 | Loss: 0.00118363
Iteration 218/1000 | Loss: 0.00120487
Iteration 219/1000 | Loss: 0.00123879
Iteration 220/1000 | Loss: 0.00117698
Iteration 221/1000 | Loss: 0.00102458
Iteration 222/1000 | Loss: 0.00123380
Iteration 223/1000 | Loss: 0.00107729
Iteration 224/1000 | Loss: 0.00118708
Iteration 225/1000 | Loss: 0.00127241
Iteration 226/1000 | Loss: 0.00118605
Iteration 227/1000 | Loss: 0.00253639
Iteration 228/1000 | Loss: 0.00121666
Iteration 229/1000 | Loss: 0.00170440
Iteration 230/1000 | Loss: 0.00128517
Iteration 231/1000 | Loss: 0.00111014
Iteration 232/1000 | Loss: 0.00093664
Iteration 233/1000 | Loss: 0.00116410
Iteration 234/1000 | Loss: 0.00106517
Iteration 235/1000 | Loss: 0.00122408
Iteration 236/1000 | Loss: 0.00154612
Iteration 237/1000 | Loss: 0.00085815
Iteration 238/1000 | Loss: 0.00087567
Iteration 239/1000 | Loss: 0.00077314
Iteration 240/1000 | Loss: 0.00087123
Iteration 241/1000 | Loss: 0.00100424
Iteration 242/1000 | Loss: 0.00181952
Iteration 243/1000 | Loss: 0.00087980
Iteration 244/1000 | Loss: 0.00141734
Iteration 245/1000 | Loss: 0.00101932
Iteration 246/1000 | Loss: 0.00091886
Iteration 247/1000 | Loss: 0.00086495
Iteration 248/1000 | Loss: 0.00081425
Iteration 249/1000 | Loss: 0.00083009
Iteration 250/1000 | Loss: 0.00076692
Iteration 251/1000 | Loss: 0.00118614
Iteration 252/1000 | Loss: 0.00092755
Iteration 253/1000 | Loss: 0.00092592
Iteration 254/1000 | Loss: 0.00082118
Iteration 255/1000 | Loss: 0.00096298
Iteration 256/1000 | Loss: 0.00110921
Iteration 257/1000 | Loss: 0.00070244
Iteration 258/1000 | Loss: 0.00177727
Iteration 259/1000 | Loss: 0.00082602
Iteration 260/1000 | Loss: 0.00105213
Iteration 261/1000 | Loss: 0.00090753
Iteration 262/1000 | Loss: 0.00112898
Iteration 263/1000 | Loss: 0.00142106
Iteration 264/1000 | Loss: 0.00085929
Iteration 265/1000 | Loss: 0.00063134
Iteration 266/1000 | Loss: 0.00133935
Iteration 267/1000 | Loss: 0.00085997
Iteration 268/1000 | Loss: 0.00091760
Iteration 269/1000 | Loss: 0.00166705
Iteration 270/1000 | Loss: 0.00078390
Iteration 271/1000 | Loss: 0.00043417
Iteration 272/1000 | Loss: 0.00099464
Iteration 273/1000 | Loss: 0.00039214
Iteration 274/1000 | Loss: 0.00064753
Iteration 275/1000 | Loss: 0.00103869
Iteration 276/1000 | Loss: 0.00030729
Iteration 277/1000 | Loss: 0.00028133
Iteration 278/1000 | Loss: 0.00028557
Iteration 279/1000 | Loss: 0.00019044
Iteration 280/1000 | Loss: 0.00106532
Iteration 281/1000 | Loss: 0.00022030
Iteration 282/1000 | Loss: 0.00046965
Iteration 283/1000 | Loss: 0.00034742
Iteration 284/1000 | Loss: 0.00025767
Iteration 285/1000 | Loss: 0.00032194
Iteration 286/1000 | Loss: 0.00030461
Iteration 287/1000 | Loss: 0.00031267
Iteration 288/1000 | Loss: 0.00036389
Iteration 289/1000 | Loss: 0.00023478
Iteration 290/1000 | Loss: 0.00044521
Iteration 291/1000 | Loss: 0.00088887
Iteration 292/1000 | Loss: 0.00078871
Iteration 293/1000 | Loss: 0.00015791
Iteration 294/1000 | Loss: 0.00016016
Iteration 295/1000 | Loss: 0.00035275
Iteration 296/1000 | Loss: 0.00016959
Iteration 297/1000 | Loss: 0.00007683
Iteration 298/1000 | Loss: 0.00051724
Iteration 299/1000 | Loss: 0.00043437
Iteration 300/1000 | Loss: 0.00026486
Iteration 301/1000 | Loss: 0.00008179
Iteration 302/1000 | Loss: 0.00013028
Iteration 303/1000 | Loss: 0.00014191
Iteration 304/1000 | Loss: 0.00011332
Iteration 305/1000 | Loss: 0.00010436
Iteration 306/1000 | Loss: 0.00010538
Iteration 307/1000 | Loss: 0.00017914
Iteration 308/1000 | Loss: 0.00016991
Iteration 309/1000 | Loss: 0.00019103
Iteration 310/1000 | Loss: 0.00007797
Iteration 311/1000 | Loss: 0.00048620
Iteration 312/1000 | Loss: 0.00010384
Iteration 313/1000 | Loss: 0.00026801
Iteration 314/1000 | Loss: 0.00003815
Iteration 315/1000 | Loss: 0.00003880
Iteration 316/1000 | Loss: 0.00003776
Iteration 317/1000 | Loss: 0.00003548
Iteration 318/1000 | Loss: 0.00015463
Iteration 319/1000 | Loss: 0.00078961
Iteration 320/1000 | Loss: 0.00010274
Iteration 321/1000 | Loss: 0.00005815
Iteration 322/1000 | Loss: 0.00019221
Iteration 323/1000 | Loss: 0.00003345
Iteration 324/1000 | Loss: 0.00064329
Iteration 325/1000 | Loss: 0.00084310
Iteration 326/1000 | Loss: 0.00130930
Iteration 327/1000 | Loss: 0.00012262
Iteration 328/1000 | Loss: 0.00004573
Iteration 329/1000 | Loss: 0.00004514
Iteration 330/1000 | Loss: 0.00003431
Iteration 331/1000 | Loss: 0.00006683
Iteration 332/1000 | Loss: 0.00003222
Iteration 333/1000 | Loss: 0.00003219
Iteration 334/1000 | Loss: 0.00003045
Iteration 335/1000 | Loss: 0.00002823
Iteration 336/1000 | Loss: 0.00002784
Iteration 337/1000 | Loss: 0.00018033
Iteration 338/1000 | Loss: 0.00004061
Iteration 339/1000 | Loss: 0.00003111
Iteration 340/1000 | Loss: 0.00005380
Iteration 341/1000 | Loss: 0.00002852
Iteration 342/1000 | Loss: 0.00002734
Iteration 343/1000 | Loss: 0.00002723
Iteration 344/1000 | Loss: 0.00002717
Iteration 345/1000 | Loss: 0.00002713
Iteration 346/1000 | Loss: 0.00047252
Iteration 347/1000 | Loss: 0.00013792
Iteration 348/1000 | Loss: 0.00009777
Iteration 349/1000 | Loss: 0.00003139
Iteration 350/1000 | Loss: 0.00002702
Iteration 351/1000 | Loss: 0.00002699
Iteration 352/1000 | Loss: 0.00002698
Iteration 353/1000 | Loss: 0.00002696
Iteration 354/1000 | Loss: 0.00002701
Iteration 355/1000 | Loss: 0.00002700
Iteration 356/1000 | Loss: 0.00043244
Iteration 357/1000 | Loss: 0.00003984
Iteration 358/1000 | Loss: 0.00005331
Iteration 359/1000 | Loss: 0.00003613
Iteration 360/1000 | Loss: 0.00003300
Iteration 361/1000 | Loss: 0.00003107
Iteration 362/1000 | Loss: 0.00002695
Iteration 363/1000 | Loss: 0.00002692
Iteration 364/1000 | Loss: 0.00002691
Iteration 365/1000 | Loss: 0.00002691
Iteration 366/1000 | Loss: 0.00002686
Iteration 367/1000 | Loss: 0.00002685
Iteration 368/1000 | Loss: 0.00002921
Iteration 369/1000 | Loss: 0.00004299
Iteration 370/1000 | Loss: 0.00002973
Iteration 371/1000 | Loss: 0.00051539
Iteration 372/1000 | Loss: 0.00003056
Iteration 373/1000 | Loss: 0.00008929
Iteration 374/1000 | Loss: 0.00003360
Iteration 375/1000 | Loss: 0.00002747
Iteration 376/1000 | Loss: 0.00009142
Iteration 377/1000 | Loss: 0.00002732
Iteration 378/1000 | Loss: 0.00002698
Iteration 379/1000 | Loss: 0.00002686
Iteration 380/1000 | Loss: 0.00002686
Iteration 381/1000 | Loss: 0.00002685
Iteration 382/1000 | Loss: 0.00002683
Iteration 383/1000 | Loss: 0.00002680
Iteration 384/1000 | Loss: 0.00002679
Iteration 385/1000 | Loss: 0.00002679
Iteration 386/1000 | Loss: 0.00002678
Iteration 387/1000 | Loss: 0.00002678
Iteration 388/1000 | Loss: 0.00002678
Iteration 389/1000 | Loss: 0.00002677
Iteration 390/1000 | Loss: 0.00002677
Iteration 391/1000 | Loss: 0.00002677
Iteration 392/1000 | Loss: 0.00002671
Iteration 393/1000 | Loss: 0.00002671
Iteration 394/1000 | Loss: 0.00002670
Iteration 395/1000 | Loss: 0.00002670
Iteration 396/1000 | Loss: 0.00002670
Iteration 397/1000 | Loss: 0.00002669
Iteration 398/1000 | Loss: 0.00002669
Iteration 399/1000 | Loss: 0.00002667
Iteration 400/1000 | Loss: 0.00002666
Iteration 401/1000 | Loss: 0.00002666
Iteration 402/1000 | Loss: 0.00002666
Iteration 403/1000 | Loss: 0.00002666
Iteration 404/1000 | Loss: 0.00002666
Iteration 405/1000 | Loss: 0.00002666
Iteration 406/1000 | Loss: 0.00002666
Iteration 407/1000 | Loss: 0.00002665
Iteration 408/1000 | Loss: 0.00002665
Iteration 409/1000 | Loss: 0.00002665
Iteration 410/1000 | Loss: 0.00002665
Iteration 411/1000 | Loss: 0.00002665
Iteration 412/1000 | Loss: 0.00002665
Iteration 413/1000 | Loss: 0.00002665
Iteration 414/1000 | Loss: 0.00002665
Iteration 415/1000 | Loss: 0.00002665
Iteration 416/1000 | Loss: 0.00002665
Iteration 417/1000 | Loss: 0.00002665
Iteration 418/1000 | Loss: 0.00002665
Iteration 419/1000 | Loss: 0.00002665
Iteration 420/1000 | Loss: 0.00002664
Iteration 421/1000 | Loss: 0.00002664
Iteration 422/1000 | Loss: 0.00002664
Iteration 423/1000 | Loss: 0.00002664
Iteration 424/1000 | Loss: 0.00002664
Iteration 425/1000 | Loss: 0.00002664
Iteration 426/1000 | Loss: 0.00002664
Iteration 427/1000 | Loss: 0.00002664
Iteration 428/1000 | Loss: 0.00002664
Iteration 429/1000 | Loss: 0.00002664
Iteration 430/1000 | Loss: 0.00002664
Iteration 431/1000 | Loss: 0.00002664
Iteration 432/1000 | Loss: 0.00002664
Iteration 433/1000 | Loss: 0.00002664
Iteration 434/1000 | Loss: 0.00002664
Iteration 435/1000 | Loss: 0.00002664
Iteration 436/1000 | Loss: 0.00002664
Iteration 437/1000 | Loss: 0.00002664
Iteration 438/1000 | Loss: 0.00002664
Iteration 439/1000 | Loss: 0.00002664
Iteration 440/1000 | Loss: 0.00002664
Iteration 441/1000 | Loss: 0.00002664
Iteration 442/1000 | Loss: 0.00002664
Iteration 443/1000 | Loss: 0.00002664
Iteration 444/1000 | Loss: 0.00002664
Iteration 445/1000 | Loss: 0.00002664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 445. Stopping optimization.
Last 5 losses: [2.663604391273111e-05, 2.663604391273111e-05, 2.663604391273111e-05, 2.663604391273111e-05, 2.663604391273111e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.663604391273111e-05

Optimization complete. Final v2v error: 3.8539605140686035 mm

Highest mean error: 20.900632858276367 mm for frame 235

Lowest mean error: 2.758120536804199 mm for frame 144

Saving results

Total time: 628.9948377609253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119822
Iteration 2/25 | Loss: 0.00225202
Iteration 3/25 | Loss: 0.00200966
Iteration 4/25 | Loss: 0.00207222
Iteration 5/25 | Loss: 0.00191749
Iteration 6/25 | Loss: 0.00174728
Iteration 7/25 | Loss: 0.00145655
Iteration 8/25 | Loss: 0.00130295
Iteration 9/25 | Loss: 0.00124865
Iteration 10/25 | Loss: 0.00123051
Iteration 11/25 | Loss: 0.00120641
Iteration 12/25 | Loss: 0.00120328
Iteration 13/25 | Loss: 0.00118828
Iteration 14/25 | Loss: 0.00118044
Iteration 15/25 | Loss: 0.00117687
Iteration 16/25 | Loss: 0.00118276
Iteration 17/25 | Loss: 0.00118441
Iteration 18/25 | Loss: 0.00117288
Iteration 19/25 | Loss: 0.00116774
Iteration 20/25 | Loss: 0.00116595
Iteration 21/25 | Loss: 0.00116593
Iteration 22/25 | Loss: 0.00116559
Iteration 23/25 | Loss: 0.00116563
Iteration 24/25 | Loss: 0.00116536
Iteration 25/25 | Loss: 0.00116496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63493013
Iteration 2/25 | Loss: 0.00316789
Iteration 3/25 | Loss: 0.00316787
Iteration 4/25 | Loss: 0.00316787
Iteration 5/25 | Loss: 0.00316787
Iteration 6/25 | Loss: 0.00316787
Iteration 7/25 | Loss: 0.00316787
Iteration 8/25 | Loss: 0.00316787
Iteration 9/25 | Loss: 0.00316787
Iteration 10/25 | Loss: 0.00316787
Iteration 11/25 | Loss: 0.00316787
Iteration 12/25 | Loss: 0.00316787
Iteration 13/25 | Loss: 0.00316787
Iteration 14/25 | Loss: 0.00316787
Iteration 15/25 | Loss: 0.00316787
Iteration 16/25 | Loss: 0.00316787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0031678725499659777, 0.0031678725499659777, 0.0031678725499659777, 0.0031678725499659777, 0.0031678725499659777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031678725499659777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316787
Iteration 2/1000 | Loss: 0.00017194
Iteration 3/1000 | Loss: 0.00013844
Iteration 4/1000 | Loss: 0.00027090
Iteration 5/1000 | Loss: 0.00011726
Iteration 6/1000 | Loss: 0.00023524
Iteration 7/1000 | Loss: 0.00037506
Iteration 8/1000 | Loss: 0.00049608
Iteration 9/1000 | Loss: 0.00015881
Iteration 10/1000 | Loss: 0.00011403
Iteration 11/1000 | Loss: 0.00010561
Iteration 12/1000 | Loss: 0.00021189
Iteration 13/1000 | Loss: 0.00016140
Iteration 14/1000 | Loss: 0.00017561
Iteration 15/1000 | Loss: 0.00046601
Iteration 16/1000 | Loss: 0.00015136
Iteration 17/1000 | Loss: 0.00016217
Iteration 18/1000 | Loss: 0.00012317
Iteration 19/1000 | Loss: 0.00043716
Iteration 20/1000 | Loss: 0.00017712
Iteration 21/1000 | Loss: 0.00045980
Iteration 22/1000 | Loss: 0.00032832
Iteration 23/1000 | Loss: 0.00056945
Iteration 24/1000 | Loss: 0.00037408
Iteration 25/1000 | Loss: 0.00017757
Iteration 26/1000 | Loss: 0.00018756
Iteration 27/1000 | Loss: 0.00018453
Iteration 28/1000 | Loss: 0.00023477
Iteration 29/1000 | Loss: 0.00022673
Iteration 30/1000 | Loss: 0.00009280
Iteration 31/1000 | Loss: 0.00007744
Iteration 32/1000 | Loss: 0.00013460
Iteration 33/1000 | Loss: 0.00009868
Iteration 34/1000 | Loss: 0.00008181
Iteration 35/1000 | Loss: 0.00019031
Iteration 36/1000 | Loss: 0.00021884
Iteration 37/1000 | Loss: 0.00018588
Iteration 38/1000 | Loss: 0.00020263
Iteration 39/1000 | Loss: 0.00019821
Iteration 40/1000 | Loss: 0.00032159
Iteration 41/1000 | Loss: 0.00008365
Iteration 42/1000 | Loss: 0.00024965
Iteration 43/1000 | Loss: 0.00027997
Iteration 44/1000 | Loss: 0.00024326
Iteration 45/1000 | Loss: 0.00037464
Iteration 46/1000 | Loss: 0.00033443
Iteration 47/1000 | Loss: 0.00020928
Iteration 48/1000 | Loss: 0.00008456
Iteration 49/1000 | Loss: 0.00007794
Iteration 50/1000 | Loss: 0.00008452
Iteration 51/1000 | Loss: 0.00010964
Iteration 52/1000 | Loss: 0.00013003
Iteration 53/1000 | Loss: 0.00013937
Iteration 54/1000 | Loss: 0.00015847
Iteration 55/1000 | Loss: 0.00014974
Iteration 56/1000 | Loss: 0.00017029
Iteration 57/1000 | Loss: 0.00066044
Iteration 58/1000 | Loss: 0.00060460
Iteration 59/1000 | Loss: 0.00027234
Iteration 60/1000 | Loss: 0.00018695
Iteration 61/1000 | Loss: 0.00036691
Iteration 62/1000 | Loss: 0.00013408
Iteration 63/1000 | Loss: 0.00010597
Iteration 64/1000 | Loss: 0.00035303
Iteration 65/1000 | Loss: 0.00040001
Iteration 66/1000 | Loss: 0.00012905
Iteration 67/1000 | Loss: 0.00014671
Iteration 68/1000 | Loss: 0.00010337
Iteration 69/1000 | Loss: 0.00052204
Iteration 70/1000 | Loss: 0.00024217
Iteration 71/1000 | Loss: 0.00078565
Iteration 72/1000 | Loss: 0.00110624
Iteration 73/1000 | Loss: 0.00111513
Iteration 74/1000 | Loss: 0.00071787
Iteration 75/1000 | Loss: 0.00043615
Iteration 76/1000 | Loss: 0.00066725
Iteration 77/1000 | Loss: 0.00009806
Iteration 78/1000 | Loss: 0.00007888
Iteration 79/1000 | Loss: 0.00006172
Iteration 80/1000 | Loss: 0.00004920
Iteration 81/1000 | Loss: 0.00005062
Iteration 82/1000 | Loss: 0.00005183
Iteration 83/1000 | Loss: 0.00003642
Iteration 84/1000 | Loss: 0.00004786
Iteration 85/1000 | Loss: 0.00003635
Iteration 86/1000 | Loss: 0.00003899
Iteration 87/1000 | Loss: 0.00003582
Iteration 88/1000 | Loss: 0.00003625
Iteration 89/1000 | Loss: 0.00004766
Iteration 90/1000 | Loss: 0.00005511
Iteration 91/1000 | Loss: 0.00005044
Iteration 92/1000 | Loss: 0.00003189
Iteration 93/1000 | Loss: 0.00004857
Iteration 94/1000 | Loss: 0.00003656
Iteration 95/1000 | Loss: 0.00003679
Iteration 96/1000 | Loss: 0.00005055
Iteration 97/1000 | Loss: 0.00004659
Iteration 98/1000 | Loss: 0.00004250
Iteration 99/1000 | Loss: 0.00004189
Iteration 100/1000 | Loss: 0.00003818
Iteration 101/1000 | Loss: 0.00003697
Iteration 102/1000 | Loss: 0.00004065
Iteration 103/1000 | Loss: 0.00004603
Iteration 104/1000 | Loss: 0.00003265
Iteration 105/1000 | Loss: 0.00002937
Iteration 106/1000 | Loss: 0.00002784
Iteration 107/1000 | Loss: 0.00002719
Iteration 108/1000 | Loss: 0.00002687
Iteration 109/1000 | Loss: 0.00002661
Iteration 110/1000 | Loss: 0.00002640
Iteration 111/1000 | Loss: 0.00002619
Iteration 112/1000 | Loss: 0.00002610
Iteration 113/1000 | Loss: 0.00002608
Iteration 114/1000 | Loss: 0.00002603
Iteration 115/1000 | Loss: 0.00002603
Iteration 116/1000 | Loss: 0.00002601
Iteration 117/1000 | Loss: 0.00002601
Iteration 118/1000 | Loss: 0.00002601
Iteration 119/1000 | Loss: 0.00002600
Iteration 120/1000 | Loss: 0.00002600
Iteration 121/1000 | Loss: 0.00002600
Iteration 122/1000 | Loss: 0.00002599
Iteration 123/1000 | Loss: 0.00002599
Iteration 124/1000 | Loss: 0.00002599
Iteration 125/1000 | Loss: 0.00002599
Iteration 126/1000 | Loss: 0.00002599
Iteration 127/1000 | Loss: 0.00002598
Iteration 128/1000 | Loss: 0.00002598
Iteration 129/1000 | Loss: 0.00002598
Iteration 130/1000 | Loss: 0.00002598
Iteration 131/1000 | Loss: 0.00002598
Iteration 132/1000 | Loss: 0.00002598
Iteration 133/1000 | Loss: 0.00002598
Iteration 134/1000 | Loss: 0.00002598
Iteration 135/1000 | Loss: 0.00002598
Iteration 136/1000 | Loss: 0.00002597
Iteration 137/1000 | Loss: 0.00002596
Iteration 138/1000 | Loss: 0.00002596
Iteration 139/1000 | Loss: 0.00002595
Iteration 140/1000 | Loss: 0.00002595
Iteration 141/1000 | Loss: 0.00002595
Iteration 142/1000 | Loss: 0.00002595
Iteration 143/1000 | Loss: 0.00002595
Iteration 144/1000 | Loss: 0.00002595
Iteration 145/1000 | Loss: 0.00002595
Iteration 146/1000 | Loss: 0.00002594
Iteration 147/1000 | Loss: 0.00002594
Iteration 148/1000 | Loss: 0.00002594
Iteration 149/1000 | Loss: 0.00002594
Iteration 150/1000 | Loss: 0.00002594
Iteration 151/1000 | Loss: 0.00002594
Iteration 152/1000 | Loss: 0.00002593
Iteration 153/1000 | Loss: 0.00002593
Iteration 154/1000 | Loss: 0.00002593
Iteration 155/1000 | Loss: 0.00002593
Iteration 156/1000 | Loss: 0.00002593
Iteration 157/1000 | Loss: 0.00002593
Iteration 158/1000 | Loss: 0.00002593
Iteration 159/1000 | Loss: 0.00002593
Iteration 160/1000 | Loss: 0.00002593
Iteration 161/1000 | Loss: 0.00002593
Iteration 162/1000 | Loss: 0.00002592
Iteration 163/1000 | Loss: 0.00002592
Iteration 164/1000 | Loss: 0.00002592
Iteration 165/1000 | Loss: 0.00002592
Iteration 166/1000 | Loss: 0.00002592
Iteration 167/1000 | Loss: 0.00002592
Iteration 168/1000 | Loss: 0.00002591
Iteration 169/1000 | Loss: 0.00002591
Iteration 170/1000 | Loss: 0.00002591
Iteration 171/1000 | Loss: 0.00002591
Iteration 172/1000 | Loss: 0.00002591
Iteration 173/1000 | Loss: 0.00002591
Iteration 174/1000 | Loss: 0.00002591
Iteration 175/1000 | Loss: 0.00002591
Iteration 176/1000 | Loss: 0.00002591
Iteration 177/1000 | Loss: 0.00002590
Iteration 178/1000 | Loss: 0.00002590
Iteration 179/1000 | Loss: 0.00002590
Iteration 180/1000 | Loss: 0.00002590
Iteration 181/1000 | Loss: 0.00002590
Iteration 182/1000 | Loss: 0.00002590
Iteration 183/1000 | Loss: 0.00002590
Iteration 184/1000 | Loss: 0.00002590
Iteration 185/1000 | Loss: 0.00002590
Iteration 186/1000 | Loss: 0.00002590
Iteration 187/1000 | Loss: 0.00002590
Iteration 188/1000 | Loss: 0.00002590
Iteration 189/1000 | Loss: 0.00002590
Iteration 190/1000 | Loss: 0.00002590
Iteration 191/1000 | Loss: 0.00002590
Iteration 192/1000 | Loss: 0.00002590
Iteration 193/1000 | Loss: 0.00002590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.5904077119776048e-05, 2.5904077119776048e-05, 2.5904077119776048e-05, 2.5904077119776048e-05, 2.5904077119776048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5904077119776048e-05

Optimization complete. Final v2v error: 4.3685712814331055 mm

Highest mean error: 4.85264778137207 mm for frame 208

Lowest mean error: 4.17664909362793 mm for frame 161

Saving results

Total time: 232.7053198814392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161961
Iteration 2/25 | Loss: 0.01161961
Iteration 3/25 | Loss: 0.01161960
Iteration 4/25 | Loss: 0.01161960
Iteration 5/25 | Loss: 0.01161960
Iteration 6/25 | Loss: 0.01161960
Iteration 7/25 | Loss: 0.01161960
Iteration 8/25 | Loss: 0.01161960
Iteration 9/25 | Loss: 0.01161960
Iteration 10/25 | Loss: 0.01161960
Iteration 11/25 | Loss: 0.01161959
Iteration 12/25 | Loss: 0.01161959
Iteration 13/25 | Loss: 0.01161959
Iteration 14/25 | Loss: 0.01161959
Iteration 15/25 | Loss: 0.01161959
Iteration 16/25 | Loss: 0.01161959
Iteration 17/25 | Loss: 0.01161959
Iteration 18/25 | Loss: 0.01161959
Iteration 19/25 | Loss: 0.01161959
Iteration 20/25 | Loss: 0.01161959
Iteration 21/25 | Loss: 0.01161959
Iteration 22/25 | Loss: 0.01161959
Iteration 23/25 | Loss: 0.01161959
Iteration 24/25 | Loss: 0.01161959
Iteration 25/25 | Loss: 0.01161958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99328876
Iteration 2/25 | Loss: 0.07842164
Iteration 3/25 | Loss: 0.07424031
Iteration 4/25 | Loss: 0.07422837
Iteration 5/25 | Loss: 0.07422835
Iteration 6/25 | Loss: 0.07422835
Iteration 7/25 | Loss: 0.07422833
Iteration 8/25 | Loss: 0.07422833
Iteration 9/25 | Loss: 0.07422833
Iteration 10/25 | Loss: 0.07422833
Iteration 11/25 | Loss: 0.07422832
Iteration 12/25 | Loss: 0.07422833
Iteration 13/25 | Loss: 0.07422833
Iteration 14/25 | Loss: 0.07422832
Iteration 15/25 | Loss: 0.07422832
Iteration 16/25 | Loss: 0.07422832
Iteration 17/25 | Loss: 0.07422832
Iteration 18/25 | Loss: 0.07422832
Iteration 19/25 | Loss: 0.07422832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.07422832399606705, 0.07422832399606705, 0.07422832399606705, 0.07422832399606705, 0.07422832399606705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07422832399606705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07422832
Iteration 2/1000 | Loss: 0.00039751
Iteration 3/1000 | Loss: 0.00014105
Iteration 4/1000 | Loss: 0.00006883
Iteration 5/1000 | Loss: 0.00004324
Iteration 6/1000 | Loss: 0.00003771
Iteration 7/1000 | Loss: 0.00003304
Iteration 8/1000 | Loss: 0.00002912
Iteration 9/1000 | Loss: 0.00002598
Iteration 10/1000 | Loss: 0.00002312
Iteration 11/1000 | Loss: 0.00002141
Iteration 12/1000 | Loss: 0.00001996
Iteration 13/1000 | Loss: 0.00001872
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001623
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00003944
Iteration 20/1000 | Loss: 0.00002506
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00003926
Iteration 23/1000 | Loss: 0.00002713
Iteration 24/1000 | Loss: 0.00004139
Iteration 25/1000 | Loss: 0.00002077
Iteration 26/1000 | Loss: 0.00003631
Iteration 27/1000 | Loss: 0.00001841
Iteration 28/1000 | Loss: 0.00003823
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00003638
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001427
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001338
Iteration 38/1000 | Loss: 0.00001338
Iteration 39/1000 | Loss: 0.00001337
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001333
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001312
Iteration 79/1000 | Loss: 0.00001312
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001311
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001311
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001320
Iteration 90/1000 | Loss: 0.00001320
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001316
Iteration 109/1000 | Loss: 0.00001309
Iteration 110/1000 | Loss: 0.00001309
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001315
Iteration 116/1000 | Loss: 0.00001315
Iteration 117/1000 | Loss: 0.00001315
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00001315
Iteration 122/1000 | Loss: 0.00001315
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3154064617992844e-05, 1.3154064617992844e-05, 1.3154064617992844e-05, 1.3154064617992844e-05, 1.3154064617992844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3154064617992844e-05

Optimization complete. Final v2v error: 3.1291160583496094 mm

Highest mean error: 10.879940032958984 mm for frame 167

Lowest mean error: 2.8875222206115723 mm for frame 164

Saving results

Total time: 69.31889414787292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760537
Iteration 2/25 | Loss: 0.00135740
Iteration 3/25 | Loss: 0.00114406
Iteration 4/25 | Loss: 0.00109008
Iteration 5/25 | Loss: 0.00106474
Iteration 6/25 | Loss: 0.00105749
Iteration 7/25 | Loss: 0.00105464
Iteration 8/25 | Loss: 0.00105635
Iteration 9/25 | Loss: 0.00105433
Iteration 10/25 | Loss: 0.00105287
Iteration 11/25 | Loss: 0.00105555
Iteration 12/25 | Loss: 0.00105335
Iteration 13/25 | Loss: 0.00105170
Iteration 14/25 | Loss: 0.00105066
Iteration 15/25 | Loss: 0.00104808
Iteration 16/25 | Loss: 0.00104746
Iteration 17/25 | Loss: 0.00104726
Iteration 18/25 | Loss: 0.00104712
Iteration 19/25 | Loss: 0.00104707
Iteration 20/25 | Loss: 0.00104706
Iteration 21/25 | Loss: 0.00104706
Iteration 22/25 | Loss: 0.00104706
Iteration 23/25 | Loss: 0.00104706
Iteration 24/25 | Loss: 0.00104706
Iteration 25/25 | Loss: 0.00104706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86527288
Iteration 2/25 | Loss: 0.00353692
Iteration 3/25 | Loss: 0.00353692
Iteration 4/25 | Loss: 0.00353692
Iteration 5/25 | Loss: 0.00353692
Iteration 6/25 | Loss: 0.00353692
Iteration 7/25 | Loss: 0.00353692
Iteration 8/25 | Loss: 0.00353692
Iteration 9/25 | Loss: 0.00353692
Iteration 10/25 | Loss: 0.00353692
Iteration 11/25 | Loss: 0.00353692
Iteration 12/25 | Loss: 0.00353692
Iteration 13/25 | Loss: 0.00353692
Iteration 14/25 | Loss: 0.00353692
Iteration 15/25 | Loss: 0.00353692
Iteration 16/25 | Loss: 0.00353692
Iteration 17/25 | Loss: 0.00353692
Iteration 18/25 | Loss: 0.00353692
Iteration 19/25 | Loss: 0.00353692
Iteration 20/25 | Loss: 0.00353692
Iteration 21/25 | Loss: 0.00353692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0035369214601814747, 0.0035369214601814747, 0.0035369214601814747, 0.0035369214601814747, 0.0035369214601814747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035369214601814747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00353692
Iteration 2/1000 | Loss: 0.00007318
Iteration 3/1000 | Loss: 0.00004522
Iteration 4/1000 | Loss: 0.00003576
Iteration 5/1000 | Loss: 0.00003241
Iteration 6/1000 | Loss: 0.00003044
Iteration 7/1000 | Loss: 0.00002938
Iteration 8/1000 | Loss: 0.00002834
Iteration 9/1000 | Loss: 0.00002766
Iteration 10/1000 | Loss: 0.00002707
Iteration 11/1000 | Loss: 0.00002668
Iteration 12/1000 | Loss: 0.00002644
Iteration 13/1000 | Loss: 0.00002621
Iteration 14/1000 | Loss: 0.00002599
Iteration 15/1000 | Loss: 0.00002579
Iteration 16/1000 | Loss: 0.00002567
Iteration 17/1000 | Loss: 0.00002564
Iteration 18/1000 | Loss: 0.00002563
Iteration 19/1000 | Loss: 0.00002554
Iteration 20/1000 | Loss: 0.00002551
Iteration 21/1000 | Loss: 0.00002550
Iteration 22/1000 | Loss: 0.00002549
Iteration 23/1000 | Loss: 0.00002549
Iteration 24/1000 | Loss: 0.00002549
Iteration 25/1000 | Loss: 0.00002549
Iteration 26/1000 | Loss: 0.00002549
Iteration 27/1000 | Loss: 0.00002549
Iteration 28/1000 | Loss: 0.00002549
Iteration 29/1000 | Loss: 0.00002549
Iteration 30/1000 | Loss: 0.00002549
Iteration 31/1000 | Loss: 0.00002548
Iteration 32/1000 | Loss: 0.00002548
Iteration 33/1000 | Loss: 0.00002548
Iteration 34/1000 | Loss: 0.00002548
Iteration 35/1000 | Loss: 0.00002548
Iteration 36/1000 | Loss: 0.00002547
Iteration 37/1000 | Loss: 0.00002547
Iteration 38/1000 | Loss: 0.00002545
Iteration 39/1000 | Loss: 0.00002544
Iteration 40/1000 | Loss: 0.00002544
Iteration 41/1000 | Loss: 0.00002544
Iteration 42/1000 | Loss: 0.00002543
Iteration 43/1000 | Loss: 0.00002543
Iteration 44/1000 | Loss: 0.00002543
Iteration 45/1000 | Loss: 0.00002542
Iteration 46/1000 | Loss: 0.00002542
Iteration 47/1000 | Loss: 0.00002542
Iteration 48/1000 | Loss: 0.00002541
Iteration 49/1000 | Loss: 0.00002541
Iteration 50/1000 | Loss: 0.00002541
Iteration 51/1000 | Loss: 0.00002541
Iteration 52/1000 | Loss: 0.00002541
Iteration 53/1000 | Loss: 0.00002540
Iteration 54/1000 | Loss: 0.00002540
Iteration 55/1000 | Loss: 0.00002540
Iteration 56/1000 | Loss: 0.00002540
Iteration 57/1000 | Loss: 0.00002539
Iteration 58/1000 | Loss: 0.00002539
Iteration 59/1000 | Loss: 0.00002539
Iteration 60/1000 | Loss: 0.00002538
Iteration 61/1000 | Loss: 0.00002538
Iteration 62/1000 | Loss: 0.00002537
Iteration 63/1000 | Loss: 0.00002536
Iteration 64/1000 | Loss: 0.00002536
Iteration 65/1000 | Loss: 0.00002536
Iteration 66/1000 | Loss: 0.00002536
Iteration 67/1000 | Loss: 0.00002536
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002535
Iteration 70/1000 | Loss: 0.00002535
Iteration 71/1000 | Loss: 0.00002533
Iteration 72/1000 | Loss: 0.00002532
Iteration 73/1000 | Loss: 0.00002532
Iteration 74/1000 | Loss: 0.00002531
Iteration 75/1000 | Loss: 0.00002531
Iteration 76/1000 | Loss: 0.00002530
Iteration 77/1000 | Loss: 0.00002530
Iteration 78/1000 | Loss: 0.00002530
Iteration 79/1000 | Loss: 0.00002528
Iteration 80/1000 | Loss: 0.00002527
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002527
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002527
Iteration 87/1000 | Loss: 0.00002527
Iteration 88/1000 | Loss: 0.00002527
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002527
Iteration 91/1000 | Loss: 0.00002526
Iteration 92/1000 | Loss: 0.00002526
Iteration 93/1000 | Loss: 0.00002526
Iteration 94/1000 | Loss: 0.00002526
Iteration 95/1000 | Loss: 0.00002526
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002525
Iteration 98/1000 | Loss: 0.00002525
Iteration 99/1000 | Loss: 0.00002524
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002524
Iteration 102/1000 | Loss: 0.00002523
Iteration 103/1000 | Loss: 0.00002523
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002523
Iteration 106/1000 | Loss: 0.00002523
Iteration 107/1000 | Loss: 0.00002523
Iteration 108/1000 | Loss: 0.00002523
Iteration 109/1000 | Loss: 0.00002523
Iteration 110/1000 | Loss: 0.00002522
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002522
Iteration 117/1000 | Loss: 0.00002522
Iteration 118/1000 | Loss: 0.00002521
Iteration 119/1000 | Loss: 0.00002521
Iteration 120/1000 | Loss: 0.00002521
Iteration 121/1000 | Loss: 0.00002521
Iteration 122/1000 | Loss: 0.00002521
Iteration 123/1000 | Loss: 0.00002521
Iteration 124/1000 | Loss: 0.00002521
Iteration 125/1000 | Loss: 0.00002521
Iteration 126/1000 | Loss: 0.00002520
Iteration 127/1000 | Loss: 0.00002520
Iteration 128/1000 | Loss: 0.00002520
Iteration 129/1000 | Loss: 0.00002520
Iteration 130/1000 | Loss: 0.00002520
Iteration 131/1000 | Loss: 0.00002520
Iteration 132/1000 | Loss: 0.00002520
Iteration 133/1000 | Loss: 0.00002520
Iteration 134/1000 | Loss: 0.00002520
Iteration 135/1000 | Loss: 0.00002520
Iteration 136/1000 | Loss: 0.00002519
Iteration 137/1000 | Loss: 0.00002519
Iteration 138/1000 | Loss: 0.00002519
Iteration 139/1000 | Loss: 0.00002519
Iteration 140/1000 | Loss: 0.00002519
Iteration 141/1000 | Loss: 0.00002519
Iteration 142/1000 | Loss: 0.00002519
Iteration 143/1000 | Loss: 0.00002518
Iteration 144/1000 | Loss: 0.00002518
Iteration 145/1000 | Loss: 0.00002518
Iteration 146/1000 | Loss: 0.00002518
Iteration 147/1000 | Loss: 0.00002518
Iteration 148/1000 | Loss: 0.00002518
Iteration 149/1000 | Loss: 0.00002518
Iteration 150/1000 | Loss: 0.00002518
Iteration 151/1000 | Loss: 0.00002518
Iteration 152/1000 | Loss: 0.00002518
Iteration 153/1000 | Loss: 0.00002518
Iteration 154/1000 | Loss: 0.00002518
Iteration 155/1000 | Loss: 0.00002518
Iteration 156/1000 | Loss: 0.00002518
Iteration 157/1000 | Loss: 0.00002518
Iteration 158/1000 | Loss: 0.00002518
Iteration 159/1000 | Loss: 0.00002518
Iteration 160/1000 | Loss: 0.00002518
Iteration 161/1000 | Loss: 0.00002518
Iteration 162/1000 | Loss: 0.00002517
Iteration 163/1000 | Loss: 0.00002517
Iteration 164/1000 | Loss: 0.00002517
Iteration 165/1000 | Loss: 0.00002517
Iteration 166/1000 | Loss: 0.00002517
Iteration 167/1000 | Loss: 0.00002517
Iteration 168/1000 | Loss: 0.00002517
Iteration 169/1000 | Loss: 0.00002517
Iteration 170/1000 | Loss: 0.00002517
Iteration 171/1000 | Loss: 0.00002517
Iteration 172/1000 | Loss: 0.00002517
Iteration 173/1000 | Loss: 0.00002517
Iteration 174/1000 | Loss: 0.00002517
Iteration 175/1000 | Loss: 0.00002517
Iteration 176/1000 | Loss: 0.00002516
Iteration 177/1000 | Loss: 0.00002516
Iteration 178/1000 | Loss: 0.00002516
Iteration 179/1000 | Loss: 0.00002516
Iteration 180/1000 | Loss: 0.00002516
Iteration 181/1000 | Loss: 0.00002516
Iteration 182/1000 | Loss: 0.00002516
Iteration 183/1000 | Loss: 0.00002516
Iteration 184/1000 | Loss: 0.00002516
Iteration 185/1000 | Loss: 0.00002516
Iteration 186/1000 | Loss: 0.00002516
Iteration 187/1000 | Loss: 0.00002516
Iteration 188/1000 | Loss: 0.00002516
Iteration 189/1000 | Loss: 0.00002516
Iteration 190/1000 | Loss: 0.00002516
Iteration 191/1000 | Loss: 0.00002516
Iteration 192/1000 | Loss: 0.00002516
Iteration 193/1000 | Loss: 0.00002515
Iteration 194/1000 | Loss: 0.00002515
Iteration 195/1000 | Loss: 0.00002515
Iteration 196/1000 | Loss: 0.00002515
Iteration 197/1000 | Loss: 0.00002515
Iteration 198/1000 | Loss: 0.00002515
Iteration 199/1000 | Loss: 0.00002515
Iteration 200/1000 | Loss: 0.00002515
Iteration 201/1000 | Loss: 0.00002515
Iteration 202/1000 | Loss: 0.00002515
Iteration 203/1000 | Loss: 0.00002515
Iteration 204/1000 | Loss: 0.00002515
Iteration 205/1000 | Loss: 0.00002514
Iteration 206/1000 | Loss: 0.00002514
Iteration 207/1000 | Loss: 0.00002514
Iteration 208/1000 | Loss: 0.00002514
Iteration 209/1000 | Loss: 0.00002514
Iteration 210/1000 | Loss: 0.00002514
Iteration 211/1000 | Loss: 0.00002514
Iteration 212/1000 | Loss: 0.00002514
Iteration 213/1000 | Loss: 0.00002514
Iteration 214/1000 | Loss: 0.00002514
Iteration 215/1000 | Loss: 0.00002514
Iteration 216/1000 | Loss: 0.00002514
Iteration 217/1000 | Loss: 0.00002514
Iteration 218/1000 | Loss: 0.00002514
Iteration 219/1000 | Loss: 0.00002514
Iteration 220/1000 | Loss: 0.00002514
Iteration 221/1000 | Loss: 0.00002514
Iteration 222/1000 | Loss: 0.00002514
Iteration 223/1000 | Loss: 0.00002514
Iteration 224/1000 | Loss: 0.00002514
Iteration 225/1000 | Loss: 0.00002514
Iteration 226/1000 | Loss: 0.00002514
Iteration 227/1000 | Loss: 0.00002514
Iteration 228/1000 | Loss: 0.00002514
Iteration 229/1000 | Loss: 0.00002514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.514291372790467e-05, 2.514291372790467e-05, 2.514291372790467e-05, 2.514291372790467e-05, 2.514291372790467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.514291372790467e-05

Optimization complete. Final v2v error: 4.2992939949035645 mm

Highest mean error: 5.135452747344971 mm for frame 112

Lowest mean error: 3.7147138118743896 mm for frame 0

Saving results

Total time: 66.8036322593689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117711
Iteration 2/25 | Loss: 0.00249158
Iteration 3/25 | Loss: 0.00164604
Iteration 4/25 | Loss: 0.00145138
Iteration 5/25 | Loss: 0.00139254
Iteration 6/25 | Loss: 0.00136678
Iteration 7/25 | Loss: 0.00135057
Iteration 8/25 | Loss: 0.00132983
Iteration 9/25 | Loss: 0.00132017
Iteration 10/25 | Loss: 0.00131219
Iteration 11/25 | Loss: 0.00130424
Iteration 12/25 | Loss: 0.00129969
Iteration 13/25 | Loss: 0.00130323
Iteration 14/25 | Loss: 0.00129731
Iteration 15/25 | Loss: 0.00129313
Iteration 16/25 | Loss: 0.00128983
Iteration 17/25 | Loss: 0.00128478
Iteration 18/25 | Loss: 0.00128189
Iteration 19/25 | Loss: 0.00127948
Iteration 20/25 | Loss: 0.00127827
Iteration 21/25 | Loss: 0.00127757
Iteration 22/25 | Loss: 0.00128052
Iteration 23/25 | Loss: 0.00128130
Iteration 24/25 | Loss: 0.00127635
Iteration 25/25 | Loss: 0.00127243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68015027
Iteration 2/25 | Loss: 0.00573384
Iteration 3/25 | Loss: 0.00573383
Iteration 4/25 | Loss: 0.00573383
Iteration 5/25 | Loss: 0.00573383
Iteration 6/25 | Loss: 0.00573383
Iteration 7/25 | Loss: 0.00573383
Iteration 8/25 | Loss: 0.00573383
Iteration 9/25 | Loss: 0.00573383
Iteration 10/25 | Loss: 0.00573383
Iteration 11/25 | Loss: 0.00573383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.005733831319957972, 0.005733831319957972, 0.005733831319957972, 0.005733831319957972, 0.005733831319957972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005733831319957972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00573383
Iteration 2/1000 | Loss: 0.00048950
Iteration 3/1000 | Loss: 0.00068307
Iteration 4/1000 | Loss: 0.00122593
Iteration 5/1000 | Loss: 0.00039623
Iteration 6/1000 | Loss: 0.00044973
Iteration 7/1000 | Loss: 0.00027576
Iteration 8/1000 | Loss: 0.00021096
Iteration 9/1000 | Loss: 0.00018411
Iteration 10/1000 | Loss: 0.00017056
Iteration 11/1000 | Loss: 0.00015936
Iteration 12/1000 | Loss: 0.00015210
Iteration 13/1000 | Loss: 0.00014619
Iteration 14/1000 | Loss: 0.00022860
Iteration 15/1000 | Loss: 0.00152603
Iteration 16/1000 | Loss: 0.00729508
Iteration 17/1000 | Loss: 0.00322206
Iteration 18/1000 | Loss: 0.00200475
Iteration 19/1000 | Loss: 0.00144428
Iteration 20/1000 | Loss: 0.00220521
Iteration 21/1000 | Loss: 0.00133141
Iteration 22/1000 | Loss: 0.00089465
Iteration 23/1000 | Loss: 0.00067875
Iteration 24/1000 | Loss: 0.00146360
Iteration 25/1000 | Loss: 0.00148093
Iteration 26/1000 | Loss: 0.00083637
Iteration 27/1000 | Loss: 0.00099216
Iteration 28/1000 | Loss: 0.00012008
Iteration 29/1000 | Loss: 0.00100597
Iteration 30/1000 | Loss: 0.00093433
Iteration 31/1000 | Loss: 0.00050845
Iteration 32/1000 | Loss: 0.00061798
Iteration 33/1000 | Loss: 0.00005538
Iteration 34/1000 | Loss: 0.00040793
Iteration 35/1000 | Loss: 0.00046161
Iteration 36/1000 | Loss: 0.00059750
Iteration 37/1000 | Loss: 0.00054248
Iteration 38/1000 | Loss: 0.00098267
Iteration 39/1000 | Loss: 0.00071760
Iteration 40/1000 | Loss: 0.00052962
Iteration 41/1000 | Loss: 0.00050107
Iteration 42/1000 | Loss: 0.00053175
Iteration 43/1000 | Loss: 0.00070329
Iteration 44/1000 | Loss: 0.00042239
Iteration 45/1000 | Loss: 0.00087285
Iteration 46/1000 | Loss: 0.00054803
Iteration 47/1000 | Loss: 0.00071016
Iteration 48/1000 | Loss: 0.00052699
Iteration 49/1000 | Loss: 0.00079419
Iteration 50/1000 | Loss: 0.00079236
Iteration 51/1000 | Loss: 0.00104435
Iteration 52/1000 | Loss: 0.00048884
Iteration 53/1000 | Loss: 0.00067526
Iteration 54/1000 | Loss: 0.00091138
Iteration 55/1000 | Loss: 0.00045136
Iteration 56/1000 | Loss: 0.00003782
Iteration 57/1000 | Loss: 0.00006776
Iteration 58/1000 | Loss: 0.00002964
Iteration 59/1000 | Loss: 0.00002790
Iteration 60/1000 | Loss: 0.00002645
Iteration 61/1000 | Loss: 0.00002514
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002213
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00036186
Iteration 66/1000 | Loss: 0.00003342
Iteration 67/1000 | Loss: 0.00002494
Iteration 68/1000 | Loss: 0.00002293
Iteration 69/1000 | Loss: 0.00002084
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001767
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001764
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001761
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001760
Iteration 90/1000 | Loss: 0.00001759
Iteration 91/1000 | Loss: 0.00001759
Iteration 92/1000 | Loss: 0.00001759
Iteration 93/1000 | Loss: 0.00001758
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001758
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001747
Iteration 122/1000 | Loss: 0.00001747
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001746
Iteration 132/1000 | Loss: 0.00001746
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001745
Iteration 148/1000 | Loss: 0.00001745
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001744
Iteration 156/1000 | Loss: 0.00001744
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001744
Iteration 163/1000 | Loss: 0.00048783
Iteration 164/1000 | Loss: 0.00031770
Iteration 165/1000 | Loss: 0.00027595
Iteration 166/1000 | Loss: 0.00004931
Iteration 167/1000 | Loss: 0.00001785
Iteration 168/1000 | Loss: 0.00001680
Iteration 169/1000 | Loss: 0.00001591
Iteration 170/1000 | Loss: 0.00001542
Iteration 171/1000 | Loss: 0.00001510
Iteration 172/1000 | Loss: 0.00001498
Iteration 173/1000 | Loss: 0.00001492
Iteration 174/1000 | Loss: 0.00001492
Iteration 175/1000 | Loss: 0.00001491
Iteration 176/1000 | Loss: 0.00001490
Iteration 177/1000 | Loss: 0.00001489
Iteration 178/1000 | Loss: 0.00001489
Iteration 179/1000 | Loss: 0.00001489
Iteration 180/1000 | Loss: 0.00001488
Iteration 181/1000 | Loss: 0.00001488
Iteration 182/1000 | Loss: 0.00001488
Iteration 183/1000 | Loss: 0.00001488
Iteration 184/1000 | Loss: 0.00001488
Iteration 185/1000 | Loss: 0.00001487
Iteration 186/1000 | Loss: 0.00001487
Iteration 187/1000 | Loss: 0.00001487
Iteration 188/1000 | Loss: 0.00001487
Iteration 189/1000 | Loss: 0.00001486
Iteration 190/1000 | Loss: 0.00001486
Iteration 191/1000 | Loss: 0.00001486
Iteration 192/1000 | Loss: 0.00001486
Iteration 193/1000 | Loss: 0.00001485
Iteration 194/1000 | Loss: 0.00001485
Iteration 195/1000 | Loss: 0.00001485
Iteration 196/1000 | Loss: 0.00001485
Iteration 197/1000 | Loss: 0.00001484
Iteration 198/1000 | Loss: 0.00001484
Iteration 199/1000 | Loss: 0.00001484
Iteration 200/1000 | Loss: 0.00001484
Iteration 201/1000 | Loss: 0.00001484
Iteration 202/1000 | Loss: 0.00001483
Iteration 203/1000 | Loss: 0.00001483
Iteration 204/1000 | Loss: 0.00001483
Iteration 205/1000 | Loss: 0.00001483
Iteration 206/1000 | Loss: 0.00001483
Iteration 207/1000 | Loss: 0.00001482
Iteration 208/1000 | Loss: 0.00001482
Iteration 209/1000 | Loss: 0.00001482
Iteration 210/1000 | Loss: 0.00001482
Iteration 211/1000 | Loss: 0.00001482
Iteration 212/1000 | Loss: 0.00001482
Iteration 213/1000 | Loss: 0.00001482
Iteration 214/1000 | Loss: 0.00001482
Iteration 215/1000 | Loss: 0.00001482
Iteration 216/1000 | Loss: 0.00001482
Iteration 217/1000 | Loss: 0.00001481
Iteration 218/1000 | Loss: 0.00001481
Iteration 219/1000 | Loss: 0.00001481
Iteration 220/1000 | Loss: 0.00001481
Iteration 221/1000 | Loss: 0.00001481
Iteration 222/1000 | Loss: 0.00001480
Iteration 223/1000 | Loss: 0.00001480
Iteration 224/1000 | Loss: 0.00001480
Iteration 225/1000 | Loss: 0.00001480
Iteration 226/1000 | Loss: 0.00001480
Iteration 227/1000 | Loss: 0.00001480
Iteration 228/1000 | Loss: 0.00001480
Iteration 229/1000 | Loss: 0.00001480
Iteration 230/1000 | Loss: 0.00001480
Iteration 231/1000 | Loss: 0.00001480
Iteration 232/1000 | Loss: 0.00001480
Iteration 233/1000 | Loss: 0.00001480
Iteration 234/1000 | Loss: 0.00001480
Iteration 235/1000 | Loss: 0.00001480
Iteration 236/1000 | Loss: 0.00001480
Iteration 237/1000 | Loss: 0.00001479
Iteration 238/1000 | Loss: 0.00001479
Iteration 239/1000 | Loss: 0.00001479
Iteration 240/1000 | Loss: 0.00001479
Iteration 241/1000 | Loss: 0.00001479
Iteration 242/1000 | Loss: 0.00001479
Iteration 243/1000 | Loss: 0.00001479
Iteration 244/1000 | Loss: 0.00001479
Iteration 245/1000 | Loss: 0.00001479
Iteration 246/1000 | Loss: 0.00001479
Iteration 247/1000 | Loss: 0.00001479
Iteration 248/1000 | Loss: 0.00001479
Iteration 249/1000 | Loss: 0.00001479
Iteration 250/1000 | Loss: 0.00001479
Iteration 251/1000 | Loss: 0.00001479
Iteration 252/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.4788376574870199e-05, 1.4788376574870199e-05, 1.4788376574870199e-05, 1.4788376574870199e-05, 1.4788376574870199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4788376574870199e-05

Optimization complete. Final v2v error: 3.2595927715301514 mm

Highest mean error: 10.885449409484863 mm for frame 160

Lowest mean error: 3.0487282276153564 mm for frame 123

Saving results

Total time: 194.54738426208496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663604
Iteration 2/25 | Loss: 0.00107571
Iteration 3/25 | Loss: 0.00096561
Iteration 4/25 | Loss: 0.00094670
Iteration 5/25 | Loss: 0.00094212
Iteration 6/25 | Loss: 0.00094055
Iteration 7/25 | Loss: 0.00094045
Iteration 8/25 | Loss: 0.00094045
Iteration 9/25 | Loss: 0.00094045
Iteration 10/25 | Loss: 0.00094045
Iteration 11/25 | Loss: 0.00094045
Iteration 12/25 | Loss: 0.00094045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009404472075402737, 0.0009404472075402737, 0.0009404472075402737, 0.0009404472075402737, 0.0009404472075402737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009404472075402737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95506263
Iteration 2/25 | Loss: 0.00269430
Iteration 3/25 | Loss: 0.00269429
Iteration 4/25 | Loss: 0.00269429
Iteration 5/25 | Loss: 0.00269429
Iteration 6/25 | Loss: 0.00269429
Iteration 7/25 | Loss: 0.00269429
Iteration 8/25 | Loss: 0.00269429
Iteration 9/25 | Loss: 0.00269429
Iteration 10/25 | Loss: 0.00269429
Iteration 11/25 | Loss: 0.00269429
Iteration 12/25 | Loss: 0.00269429
Iteration 13/25 | Loss: 0.00269429
Iteration 14/25 | Loss: 0.00269429
Iteration 15/25 | Loss: 0.00269429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002694292226806283, 0.002694292226806283, 0.002694292226806283, 0.002694292226806283, 0.002694292226806283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002694292226806283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269429
Iteration 2/1000 | Loss: 0.00003197
Iteration 3/1000 | Loss: 0.00002333
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002076
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001937
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001922
Iteration 11/1000 | Loss: 0.00001919
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001916
Iteration 15/1000 | Loss: 0.00001915
Iteration 16/1000 | Loss: 0.00001913
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001912
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00001911
Iteration 21/1000 | Loss: 0.00001910
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001909
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001908
Iteration 27/1000 | Loss: 0.00001908
Iteration 28/1000 | Loss: 0.00001908
Iteration 29/1000 | Loss: 0.00001907
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001906
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001902
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001899
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001899
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001898
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001896
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001895
Iteration 77/1000 | Loss: 0.00001895
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001894
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001888
Iteration 109/1000 | Loss: 0.00001888
Iteration 110/1000 | Loss: 0.00001888
Iteration 111/1000 | Loss: 0.00001888
Iteration 112/1000 | Loss: 0.00001888
Iteration 113/1000 | Loss: 0.00001888
Iteration 114/1000 | Loss: 0.00001888
Iteration 115/1000 | Loss: 0.00001888
Iteration 116/1000 | Loss: 0.00001888
Iteration 117/1000 | Loss: 0.00001888
Iteration 118/1000 | Loss: 0.00001887
Iteration 119/1000 | Loss: 0.00001887
Iteration 120/1000 | Loss: 0.00001887
Iteration 121/1000 | Loss: 0.00001887
Iteration 122/1000 | Loss: 0.00001887
Iteration 123/1000 | Loss: 0.00001887
Iteration 124/1000 | Loss: 0.00001886
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Iteration 129/1000 | Loss: 0.00001886
Iteration 130/1000 | Loss: 0.00001886
Iteration 131/1000 | Loss: 0.00001886
Iteration 132/1000 | Loss: 0.00001886
Iteration 133/1000 | Loss: 0.00001886
Iteration 134/1000 | Loss: 0.00001886
Iteration 135/1000 | Loss: 0.00001886
Iteration 136/1000 | Loss: 0.00001886
Iteration 137/1000 | Loss: 0.00001886
Iteration 138/1000 | Loss: 0.00001885
Iteration 139/1000 | Loss: 0.00001885
Iteration 140/1000 | Loss: 0.00001885
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001885
Iteration 147/1000 | Loss: 0.00001885
Iteration 148/1000 | Loss: 0.00001885
Iteration 149/1000 | Loss: 0.00001885
Iteration 150/1000 | Loss: 0.00001885
Iteration 151/1000 | Loss: 0.00001885
Iteration 152/1000 | Loss: 0.00001885
Iteration 153/1000 | Loss: 0.00001885
Iteration 154/1000 | Loss: 0.00001885
Iteration 155/1000 | Loss: 0.00001885
Iteration 156/1000 | Loss: 0.00001885
Iteration 157/1000 | Loss: 0.00001885
Iteration 158/1000 | Loss: 0.00001885
Iteration 159/1000 | Loss: 0.00001885
Iteration 160/1000 | Loss: 0.00001885
Iteration 161/1000 | Loss: 0.00001885
Iteration 162/1000 | Loss: 0.00001885
Iteration 163/1000 | Loss: 0.00001885
Iteration 164/1000 | Loss: 0.00001885
Iteration 165/1000 | Loss: 0.00001885
Iteration 166/1000 | Loss: 0.00001885
Iteration 167/1000 | Loss: 0.00001885
Iteration 168/1000 | Loss: 0.00001885
Iteration 169/1000 | Loss: 0.00001885
Iteration 170/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.8849459593184292e-05, 1.8849459593184292e-05, 1.8849459593184292e-05, 1.8849459593184292e-05, 1.8849459593184292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8849459593184292e-05

Optimization complete. Final v2v error: 3.7428674697875977 mm

Highest mean error: 4.15839958190918 mm for frame 54

Lowest mean error: 3.27234148979187 mm for frame 79

Saving results

Total time: 34.66549754142761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874154
Iteration 2/25 | Loss: 0.00137278
Iteration 3/25 | Loss: 0.00111728
Iteration 4/25 | Loss: 0.00108042
Iteration 5/25 | Loss: 0.00106425
Iteration 6/25 | Loss: 0.00106172
Iteration 7/25 | Loss: 0.00106150
Iteration 8/25 | Loss: 0.00106150
Iteration 9/25 | Loss: 0.00106150
Iteration 10/25 | Loss: 0.00106150
Iteration 11/25 | Loss: 0.00106150
Iteration 12/25 | Loss: 0.00106150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010615024948492646, 0.0010615024948492646, 0.0010615024948492646, 0.0010615024948492646, 0.0010615024948492646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010615024948492646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51599884
Iteration 2/25 | Loss: 0.00274077
Iteration 3/25 | Loss: 0.00274072
Iteration 4/25 | Loss: 0.00274072
Iteration 5/25 | Loss: 0.00274072
Iteration 6/25 | Loss: 0.00274071
Iteration 7/25 | Loss: 0.00274071
Iteration 8/25 | Loss: 0.00274071
Iteration 9/25 | Loss: 0.00274071
Iteration 10/25 | Loss: 0.00274071
Iteration 11/25 | Loss: 0.00274071
Iteration 12/25 | Loss: 0.00274071
Iteration 13/25 | Loss: 0.00274071
Iteration 14/25 | Loss: 0.00274071
Iteration 15/25 | Loss: 0.00274071
Iteration 16/25 | Loss: 0.00274071
Iteration 17/25 | Loss: 0.00274071
Iteration 18/25 | Loss: 0.00274071
Iteration 19/25 | Loss: 0.00274071
Iteration 20/25 | Loss: 0.00274071
Iteration 21/25 | Loss: 0.00274071
Iteration 22/25 | Loss: 0.00274071
Iteration 23/25 | Loss: 0.00274071
Iteration 24/25 | Loss: 0.00274071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0027407140005379915, 0.0027407140005379915, 0.0027407140005379915, 0.0027407140005379915, 0.0027407140005379915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027407140005379915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00274071
Iteration 2/1000 | Loss: 0.00006154
Iteration 3/1000 | Loss: 0.00004202
Iteration 4/1000 | Loss: 0.00003374
Iteration 5/1000 | Loss: 0.00003096
Iteration 6/1000 | Loss: 0.00002948
Iteration 7/1000 | Loss: 0.00002837
Iteration 8/1000 | Loss: 0.00002767
Iteration 9/1000 | Loss: 0.00002715
Iteration 10/1000 | Loss: 0.00002673
Iteration 11/1000 | Loss: 0.00002652
Iteration 12/1000 | Loss: 0.00002644
Iteration 13/1000 | Loss: 0.00002639
Iteration 14/1000 | Loss: 0.00002630
Iteration 15/1000 | Loss: 0.00002622
Iteration 16/1000 | Loss: 0.00002618
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002609
Iteration 19/1000 | Loss: 0.00002606
Iteration 20/1000 | Loss: 0.00002605
Iteration 21/1000 | Loss: 0.00002604
Iteration 22/1000 | Loss: 0.00002604
Iteration 23/1000 | Loss: 0.00002602
Iteration 24/1000 | Loss: 0.00002602
Iteration 25/1000 | Loss: 0.00002601
Iteration 26/1000 | Loss: 0.00002601
Iteration 27/1000 | Loss: 0.00002601
Iteration 28/1000 | Loss: 0.00002601
Iteration 29/1000 | Loss: 0.00002600
Iteration 30/1000 | Loss: 0.00002600
Iteration 31/1000 | Loss: 0.00002599
Iteration 32/1000 | Loss: 0.00002599
Iteration 33/1000 | Loss: 0.00002599
Iteration 34/1000 | Loss: 0.00002598
Iteration 35/1000 | Loss: 0.00002598
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00002597
Iteration 38/1000 | Loss: 0.00002597
Iteration 39/1000 | Loss: 0.00002597
Iteration 40/1000 | Loss: 0.00002597
Iteration 41/1000 | Loss: 0.00002597
Iteration 42/1000 | Loss: 0.00002596
Iteration 43/1000 | Loss: 0.00002596
Iteration 44/1000 | Loss: 0.00002596
Iteration 45/1000 | Loss: 0.00002596
Iteration 46/1000 | Loss: 0.00002596
Iteration 47/1000 | Loss: 0.00002596
Iteration 48/1000 | Loss: 0.00002596
Iteration 49/1000 | Loss: 0.00002595
Iteration 50/1000 | Loss: 0.00002595
Iteration 51/1000 | Loss: 0.00002595
Iteration 52/1000 | Loss: 0.00002595
Iteration 53/1000 | Loss: 0.00002594
Iteration 54/1000 | Loss: 0.00002594
Iteration 55/1000 | Loss: 0.00002593
Iteration 56/1000 | Loss: 0.00002593
Iteration 57/1000 | Loss: 0.00002593
Iteration 58/1000 | Loss: 0.00002593
Iteration 59/1000 | Loss: 0.00002593
Iteration 60/1000 | Loss: 0.00002593
Iteration 61/1000 | Loss: 0.00002593
Iteration 62/1000 | Loss: 0.00002593
Iteration 63/1000 | Loss: 0.00002593
Iteration 64/1000 | Loss: 0.00002593
Iteration 65/1000 | Loss: 0.00002593
Iteration 66/1000 | Loss: 0.00002593
Iteration 67/1000 | Loss: 0.00002592
Iteration 68/1000 | Loss: 0.00002592
Iteration 69/1000 | Loss: 0.00002592
Iteration 70/1000 | Loss: 0.00002592
Iteration 71/1000 | Loss: 0.00002592
Iteration 72/1000 | Loss: 0.00002592
Iteration 73/1000 | Loss: 0.00002592
Iteration 74/1000 | Loss: 0.00002592
Iteration 75/1000 | Loss: 0.00002592
Iteration 76/1000 | Loss: 0.00002591
Iteration 77/1000 | Loss: 0.00002591
Iteration 78/1000 | Loss: 0.00002591
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002591
Iteration 82/1000 | Loss: 0.00002591
Iteration 83/1000 | Loss: 0.00002591
Iteration 84/1000 | Loss: 0.00002591
Iteration 85/1000 | Loss: 0.00002591
Iteration 86/1000 | Loss: 0.00002591
Iteration 87/1000 | Loss: 0.00002591
Iteration 88/1000 | Loss: 0.00002590
Iteration 89/1000 | Loss: 0.00002590
Iteration 90/1000 | Loss: 0.00002590
Iteration 91/1000 | Loss: 0.00002590
Iteration 92/1000 | Loss: 0.00002590
Iteration 93/1000 | Loss: 0.00002590
Iteration 94/1000 | Loss: 0.00002589
Iteration 95/1000 | Loss: 0.00002589
Iteration 96/1000 | Loss: 0.00002589
Iteration 97/1000 | Loss: 0.00002589
Iteration 98/1000 | Loss: 0.00002589
Iteration 99/1000 | Loss: 0.00002589
Iteration 100/1000 | Loss: 0.00002589
Iteration 101/1000 | Loss: 0.00002589
Iteration 102/1000 | Loss: 0.00002589
Iteration 103/1000 | Loss: 0.00002588
Iteration 104/1000 | Loss: 0.00002588
Iteration 105/1000 | Loss: 0.00002588
Iteration 106/1000 | Loss: 0.00002588
Iteration 107/1000 | Loss: 0.00002588
Iteration 108/1000 | Loss: 0.00002588
Iteration 109/1000 | Loss: 0.00002587
Iteration 110/1000 | Loss: 0.00002587
Iteration 111/1000 | Loss: 0.00002587
Iteration 112/1000 | Loss: 0.00002587
Iteration 113/1000 | Loss: 0.00002587
Iteration 114/1000 | Loss: 0.00002587
Iteration 115/1000 | Loss: 0.00002587
Iteration 116/1000 | Loss: 0.00002586
Iteration 117/1000 | Loss: 0.00002586
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002586
Iteration 120/1000 | Loss: 0.00002586
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002585
Iteration 123/1000 | Loss: 0.00002585
Iteration 124/1000 | Loss: 0.00002585
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002585
Iteration 128/1000 | Loss: 0.00002585
Iteration 129/1000 | Loss: 0.00002585
Iteration 130/1000 | Loss: 0.00002584
Iteration 131/1000 | Loss: 0.00002584
Iteration 132/1000 | Loss: 0.00002584
Iteration 133/1000 | Loss: 0.00002584
Iteration 134/1000 | Loss: 0.00002583
Iteration 135/1000 | Loss: 0.00002583
Iteration 136/1000 | Loss: 0.00002583
Iteration 137/1000 | Loss: 0.00002583
Iteration 138/1000 | Loss: 0.00002582
Iteration 139/1000 | Loss: 0.00002582
Iteration 140/1000 | Loss: 0.00002582
Iteration 141/1000 | Loss: 0.00002582
Iteration 142/1000 | Loss: 0.00002582
Iteration 143/1000 | Loss: 0.00002582
Iteration 144/1000 | Loss: 0.00002582
Iteration 145/1000 | Loss: 0.00002582
Iteration 146/1000 | Loss: 0.00002582
Iteration 147/1000 | Loss: 0.00002582
Iteration 148/1000 | Loss: 0.00002582
Iteration 149/1000 | Loss: 0.00002582
Iteration 150/1000 | Loss: 0.00002581
Iteration 151/1000 | Loss: 0.00002581
Iteration 152/1000 | Loss: 0.00002581
Iteration 153/1000 | Loss: 0.00002581
Iteration 154/1000 | Loss: 0.00002581
Iteration 155/1000 | Loss: 0.00002581
Iteration 156/1000 | Loss: 0.00002581
Iteration 157/1000 | Loss: 0.00002581
Iteration 158/1000 | Loss: 0.00002581
Iteration 159/1000 | Loss: 0.00002581
Iteration 160/1000 | Loss: 0.00002581
Iteration 161/1000 | Loss: 0.00002581
Iteration 162/1000 | Loss: 0.00002581
Iteration 163/1000 | Loss: 0.00002581
Iteration 164/1000 | Loss: 0.00002581
Iteration 165/1000 | Loss: 0.00002581
Iteration 166/1000 | Loss: 0.00002581
Iteration 167/1000 | Loss: 0.00002581
Iteration 168/1000 | Loss: 0.00002581
Iteration 169/1000 | Loss: 0.00002581
Iteration 170/1000 | Loss: 0.00002581
Iteration 171/1000 | Loss: 0.00002581
Iteration 172/1000 | Loss: 0.00002580
Iteration 173/1000 | Loss: 0.00002580
Iteration 174/1000 | Loss: 0.00002580
Iteration 175/1000 | Loss: 0.00002580
Iteration 176/1000 | Loss: 0.00002580
Iteration 177/1000 | Loss: 0.00002580
Iteration 178/1000 | Loss: 0.00002580
Iteration 179/1000 | Loss: 0.00002580
Iteration 180/1000 | Loss: 0.00002580
Iteration 181/1000 | Loss: 0.00002580
Iteration 182/1000 | Loss: 0.00002580
Iteration 183/1000 | Loss: 0.00002579
Iteration 184/1000 | Loss: 0.00002579
Iteration 185/1000 | Loss: 0.00002579
Iteration 186/1000 | Loss: 0.00002579
Iteration 187/1000 | Loss: 0.00002579
Iteration 188/1000 | Loss: 0.00002579
Iteration 189/1000 | Loss: 0.00002579
Iteration 190/1000 | Loss: 0.00002579
Iteration 191/1000 | Loss: 0.00002579
Iteration 192/1000 | Loss: 0.00002579
Iteration 193/1000 | Loss: 0.00002579
Iteration 194/1000 | Loss: 0.00002579
Iteration 195/1000 | Loss: 0.00002579
Iteration 196/1000 | Loss: 0.00002579
Iteration 197/1000 | Loss: 0.00002579
Iteration 198/1000 | Loss: 0.00002579
Iteration 199/1000 | Loss: 0.00002579
Iteration 200/1000 | Loss: 0.00002579
Iteration 201/1000 | Loss: 0.00002579
Iteration 202/1000 | Loss: 0.00002579
Iteration 203/1000 | Loss: 0.00002579
Iteration 204/1000 | Loss: 0.00002579
Iteration 205/1000 | Loss: 0.00002579
Iteration 206/1000 | Loss: 0.00002579
Iteration 207/1000 | Loss: 0.00002579
Iteration 208/1000 | Loss: 0.00002579
Iteration 209/1000 | Loss: 0.00002579
Iteration 210/1000 | Loss: 0.00002579
Iteration 211/1000 | Loss: 0.00002579
Iteration 212/1000 | Loss: 0.00002579
Iteration 213/1000 | Loss: 0.00002579
Iteration 214/1000 | Loss: 0.00002579
Iteration 215/1000 | Loss: 0.00002579
Iteration 216/1000 | Loss: 0.00002579
Iteration 217/1000 | Loss: 0.00002579
Iteration 218/1000 | Loss: 0.00002579
Iteration 219/1000 | Loss: 0.00002579
Iteration 220/1000 | Loss: 0.00002579
Iteration 221/1000 | Loss: 0.00002579
Iteration 222/1000 | Loss: 0.00002579
Iteration 223/1000 | Loss: 0.00002579
Iteration 224/1000 | Loss: 0.00002579
Iteration 225/1000 | Loss: 0.00002579
Iteration 226/1000 | Loss: 0.00002579
Iteration 227/1000 | Loss: 0.00002579
Iteration 228/1000 | Loss: 0.00002579
Iteration 229/1000 | Loss: 0.00002579
Iteration 230/1000 | Loss: 0.00002579
Iteration 231/1000 | Loss: 0.00002579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.5793900931603275e-05, 2.5793900931603275e-05, 2.5793900931603275e-05, 2.5793900931603275e-05, 2.5793900931603275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5793900931603275e-05

Optimization complete. Final v2v error: 4.376321315765381 mm

Highest mean error: 5.5896196365356445 mm for frame 54

Lowest mean error: 3.8634400367736816 mm for frame 85

Saving results

Total time: 41.721405029296875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00280036
Iteration 2/25 | Loss: 0.00128696
Iteration 3/25 | Loss: 0.00100855
Iteration 4/25 | Loss: 0.00094949
Iteration 5/25 | Loss: 0.00093653
Iteration 6/25 | Loss: 0.00093070
Iteration 7/25 | Loss: 0.00092907
Iteration 8/25 | Loss: 0.00092828
Iteration 9/25 | Loss: 0.00092808
Iteration 10/25 | Loss: 0.00092808
Iteration 11/25 | Loss: 0.00092808
Iteration 12/25 | Loss: 0.00092808
Iteration 13/25 | Loss: 0.00092808
Iteration 14/25 | Loss: 0.00092808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009280762751586735, 0.0009280762751586735, 0.0009280762751586735, 0.0009280762751586735, 0.0009280762751586735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009280762751586735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74024451
Iteration 2/25 | Loss: 0.00333988
Iteration 3/25 | Loss: 0.00333988
Iteration 4/25 | Loss: 0.00333988
Iteration 5/25 | Loss: 0.00333988
Iteration 6/25 | Loss: 0.00333988
Iteration 7/25 | Loss: 0.00333988
Iteration 8/25 | Loss: 0.00333988
Iteration 9/25 | Loss: 0.00333988
Iteration 10/25 | Loss: 0.00333988
Iteration 11/25 | Loss: 0.00333988
Iteration 12/25 | Loss: 0.00333988
Iteration 13/25 | Loss: 0.00333988
Iteration 14/25 | Loss: 0.00333988
Iteration 15/25 | Loss: 0.00333988
Iteration 16/25 | Loss: 0.00333988
Iteration 17/25 | Loss: 0.00333988
Iteration 18/25 | Loss: 0.00333988
Iteration 19/25 | Loss: 0.00333988
Iteration 20/25 | Loss: 0.00333988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0033398785162717104, 0.0033398785162717104, 0.0033398785162717104, 0.0033398785162717104, 0.0033398785162717104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033398785162717104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333988
Iteration 2/1000 | Loss: 0.00004065
Iteration 3/1000 | Loss: 0.00002483
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001866
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001697
Iteration 8/1000 | Loss: 0.00001651
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001580
Iteration 11/1000 | Loss: 0.00001563
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001536
Iteration 14/1000 | Loss: 0.00001536
Iteration 15/1000 | Loss: 0.00001534
Iteration 16/1000 | Loss: 0.00001530
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001517
Iteration 19/1000 | Loss: 0.00001514
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001503
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001500
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001499
Iteration 35/1000 | Loss: 0.00001499
Iteration 36/1000 | Loss: 0.00001499
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001497
Iteration 47/1000 | Loss: 0.00001496
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001496
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001492
Iteration 61/1000 | Loss: 0.00001492
Iteration 62/1000 | Loss: 0.00001492
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001491
Iteration 65/1000 | Loss: 0.00001491
Iteration 66/1000 | Loss: 0.00001491
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001490
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001489
Iteration 74/1000 | Loss: 0.00001489
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001488
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001486
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001485
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001485
Iteration 102/1000 | Loss: 0.00001485
Iteration 103/1000 | Loss: 0.00001485
Iteration 104/1000 | Loss: 0.00001485
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001484
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001484
Iteration 113/1000 | Loss: 0.00001484
Iteration 114/1000 | Loss: 0.00001484
Iteration 115/1000 | Loss: 0.00001484
Iteration 116/1000 | Loss: 0.00001484
Iteration 117/1000 | Loss: 0.00001484
Iteration 118/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.4839592950011138e-05, 1.4839592950011138e-05, 1.4839592950011138e-05, 1.4839592950011138e-05, 1.4839592950011138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4839592950011138e-05

Optimization complete. Final v2v error: 3.3559179306030273 mm

Highest mean error: 3.495832681655884 mm for frame 94

Lowest mean error: 3.1924283504486084 mm for frame 125

Saving results

Total time: 46.73081302642822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01231365
Iteration 2/25 | Loss: 0.00198622
Iteration 3/25 | Loss: 0.00131279
Iteration 4/25 | Loss: 0.00122614
Iteration 5/25 | Loss: 0.00120346
Iteration 6/25 | Loss: 0.00121066
Iteration 7/25 | Loss: 0.00120728
Iteration 8/25 | Loss: 0.00119846
Iteration 9/25 | Loss: 0.00119896
Iteration 10/25 | Loss: 0.00119957
Iteration 11/25 | Loss: 0.00119301
Iteration 12/25 | Loss: 0.00118687
Iteration 13/25 | Loss: 0.00118461
Iteration 14/25 | Loss: 0.00118786
Iteration 15/25 | Loss: 0.00118990
Iteration 16/25 | Loss: 0.00118816
Iteration 17/25 | Loss: 0.00118753
Iteration 18/25 | Loss: 0.00118917
Iteration 19/25 | Loss: 0.00119486
Iteration 20/25 | Loss: 0.00119141
Iteration 21/25 | Loss: 0.00119049
Iteration 22/25 | Loss: 0.00118924
Iteration 23/25 | Loss: 0.00118269
Iteration 24/25 | Loss: 0.00118302
Iteration 25/25 | Loss: 0.00118383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16865349
Iteration 2/25 | Loss: 0.00252349
Iteration 3/25 | Loss: 0.00252349
Iteration 4/25 | Loss: 0.00252349
Iteration 5/25 | Loss: 0.00252349
Iteration 6/25 | Loss: 0.00252349
Iteration 7/25 | Loss: 0.00252349
Iteration 8/25 | Loss: 0.00252349
Iteration 9/25 | Loss: 0.00252349
Iteration 10/25 | Loss: 0.00252349
Iteration 11/25 | Loss: 0.00252349
Iteration 12/25 | Loss: 0.00252349
Iteration 13/25 | Loss: 0.00252349
Iteration 14/25 | Loss: 0.00252349
Iteration 15/25 | Loss: 0.00252349
Iteration 16/25 | Loss: 0.00252349
Iteration 17/25 | Loss: 0.00252349
Iteration 18/25 | Loss: 0.00252349
Iteration 19/25 | Loss: 0.00252349
Iteration 20/25 | Loss: 0.00252349
Iteration 21/25 | Loss: 0.00252349
Iteration 22/25 | Loss: 0.00252349
Iteration 23/25 | Loss: 0.00252349
Iteration 24/25 | Loss: 0.00252349
Iteration 25/25 | Loss: 0.00252349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252349
Iteration 2/1000 | Loss: 0.00053221
Iteration 3/1000 | Loss: 0.00057053
Iteration 4/1000 | Loss: 0.00063882
Iteration 5/1000 | Loss: 0.00065622
Iteration 6/1000 | Loss: 0.00068464
Iteration 7/1000 | Loss: 0.00065558
Iteration 8/1000 | Loss: 0.00067410
Iteration 9/1000 | Loss: 0.00235164
Iteration 10/1000 | Loss: 0.00138181
Iteration 11/1000 | Loss: 0.00157287
Iteration 12/1000 | Loss: 0.00149331
Iteration 13/1000 | Loss: 0.00186725
Iteration 14/1000 | Loss: 0.00163844
Iteration 15/1000 | Loss: 0.00162593
Iteration 16/1000 | Loss: 0.00128809
Iteration 17/1000 | Loss: 0.00126420
Iteration 18/1000 | Loss: 0.00082387
Iteration 19/1000 | Loss: 0.00036179
Iteration 20/1000 | Loss: 0.00150103
Iteration 21/1000 | Loss: 0.00138579
Iteration 22/1000 | Loss: 0.00186284
Iteration 23/1000 | Loss: 0.00130479
Iteration 24/1000 | Loss: 0.00113774
Iteration 25/1000 | Loss: 0.00099525
Iteration 26/1000 | Loss: 0.00103316
Iteration 27/1000 | Loss: 0.00082035
Iteration 28/1000 | Loss: 0.00068497
Iteration 29/1000 | Loss: 0.00069582
Iteration 30/1000 | Loss: 0.00073326
Iteration 31/1000 | Loss: 0.00063639
Iteration 32/1000 | Loss: 0.00102065
Iteration 33/1000 | Loss: 0.00103393
Iteration 34/1000 | Loss: 0.00104588
Iteration 35/1000 | Loss: 0.00052445
Iteration 36/1000 | Loss: 0.00082055
Iteration 37/1000 | Loss: 0.00068889
Iteration 38/1000 | Loss: 0.00089968
Iteration 39/1000 | Loss: 0.00087051
Iteration 40/1000 | Loss: 0.00102382
Iteration 41/1000 | Loss: 0.00063344
Iteration 42/1000 | Loss: 0.00091728
Iteration 43/1000 | Loss: 0.00076407
Iteration 44/1000 | Loss: 0.00074225
Iteration 45/1000 | Loss: 0.00057836
Iteration 46/1000 | Loss: 0.00085360
Iteration 47/1000 | Loss: 0.00066976
Iteration 48/1000 | Loss: 0.00065975
Iteration 49/1000 | Loss: 0.00080708
Iteration 50/1000 | Loss: 0.00071373
Iteration 51/1000 | Loss: 0.00070584
Iteration 52/1000 | Loss: 0.00053958
Iteration 53/1000 | Loss: 0.00085708
Iteration 54/1000 | Loss: 0.00071462
Iteration 55/1000 | Loss: 0.00090361
Iteration 56/1000 | Loss: 0.00069085
Iteration 57/1000 | Loss: 0.00068645
Iteration 58/1000 | Loss: 0.00089661
Iteration 59/1000 | Loss: 0.00081737
Iteration 60/1000 | Loss: 0.00052502
Iteration 61/1000 | Loss: 0.00057290
Iteration 62/1000 | Loss: 0.00095337
Iteration 63/1000 | Loss: 0.00091587
Iteration 64/1000 | Loss: 0.00095453
Iteration 65/1000 | Loss: 0.00061330
Iteration 66/1000 | Loss: 0.00122825
Iteration 67/1000 | Loss: 0.00118402
Iteration 68/1000 | Loss: 0.00087304
Iteration 69/1000 | Loss: 0.00044928
Iteration 70/1000 | Loss: 0.00071634
Iteration 71/1000 | Loss: 0.00068097
Iteration 72/1000 | Loss: 0.00050732
Iteration 73/1000 | Loss: 0.00063753
Iteration 74/1000 | Loss: 0.00040637
Iteration 75/1000 | Loss: 0.00038381
Iteration 76/1000 | Loss: 0.00086750
Iteration 77/1000 | Loss: 0.00094403
Iteration 78/1000 | Loss: 0.00084556
Iteration 79/1000 | Loss: 0.00068643
Iteration 80/1000 | Loss: 0.00032519
Iteration 81/1000 | Loss: 0.00080252
Iteration 82/1000 | Loss: 0.00095082
Iteration 83/1000 | Loss: 0.00046776
Iteration 84/1000 | Loss: 0.00051065
Iteration 85/1000 | Loss: 0.00045884
Iteration 86/1000 | Loss: 0.00048634
Iteration 87/1000 | Loss: 0.00056276
Iteration 88/1000 | Loss: 0.00044112
Iteration 89/1000 | Loss: 0.00029451
Iteration 90/1000 | Loss: 0.00039133
Iteration 91/1000 | Loss: 0.00046725
Iteration 92/1000 | Loss: 0.00044455
Iteration 93/1000 | Loss: 0.00038945
Iteration 94/1000 | Loss: 0.00045445
Iteration 95/1000 | Loss: 0.00045168
Iteration 96/1000 | Loss: 0.00043776
Iteration 97/1000 | Loss: 0.00040686
Iteration 98/1000 | Loss: 0.00048465
Iteration 99/1000 | Loss: 0.00050259
Iteration 100/1000 | Loss: 0.00051486
Iteration 101/1000 | Loss: 0.00066453
Iteration 102/1000 | Loss: 0.00085137
Iteration 103/1000 | Loss: 0.00048929
Iteration 104/1000 | Loss: 0.00055831
Iteration 105/1000 | Loss: 0.00044237
Iteration 106/1000 | Loss: 0.00040507
Iteration 107/1000 | Loss: 0.00071822
Iteration 108/1000 | Loss: 0.00099524
Iteration 109/1000 | Loss: 0.00056216
Iteration 110/1000 | Loss: 0.00055021
Iteration 111/1000 | Loss: 0.00049050
Iteration 112/1000 | Loss: 0.00048999
Iteration 113/1000 | Loss: 0.00044498
Iteration 114/1000 | Loss: 0.00055981
Iteration 115/1000 | Loss: 0.00061587
Iteration 116/1000 | Loss: 0.00037587
Iteration 117/1000 | Loss: 0.00053229
Iteration 118/1000 | Loss: 0.00043725
Iteration 119/1000 | Loss: 0.00038505
Iteration 120/1000 | Loss: 0.00045645
Iteration 121/1000 | Loss: 0.00042873
Iteration 122/1000 | Loss: 0.00050792
Iteration 123/1000 | Loss: 0.00046129
Iteration 124/1000 | Loss: 0.00055933
Iteration 125/1000 | Loss: 0.00044833
Iteration 126/1000 | Loss: 0.00048739
Iteration 127/1000 | Loss: 0.00056644
Iteration 128/1000 | Loss: 0.00085503
Iteration 129/1000 | Loss: 0.00052458
Iteration 130/1000 | Loss: 0.00045260
Iteration 131/1000 | Loss: 0.00038774
Iteration 132/1000 | Loss: 0.00039260
Iteration 133/1000 | Loss: 0.00040628
Iteration 134/1000 | Loss: 0.00045886
Iteration 135/1000 | Loss: 0.00051643
Iteration 136/1000 | Loss: 0.00083645
Iteration 137/1000 | Loss: 0.00060697
Iteration 138/1000 | Loss: 0.00087463
Iteration 139/1000 | Loss: 0.00034279
Iteration 140/1000 | Loss: 0.00039598
Iteration 141/1000 | Loss: 0.00046761
Iteration 142/1000 | Loss: 0.00053273
Iteration 143/1000 | Loss: 0.00054014
Iteration 144/1000 | Loss: 0.00053269
Iteration 145/1000 | Loss: 0.00035649
Iteration 146/1000 | Loss: 0.00041435
Iteration 147/1000 | Loss: 0.00045766
Iteration 148/1000 | Loss: 0.00043810
Iteration 149/1000 | Loss: 0.00047976
Iteration 150/1000 | Loss: 0.00046822
Iteration 151/1000 | Loss: 0.00051502
Iteration 152/1000 | Loss: 0.00047066
Iteration 153/1000 | Loss: 0.00042746
Iteration 154/1000 | Loss: 0.00047159
Iteration 155/1000 | Loss: 0.00038209
Iteration 156/1000 | Loss: 0.00050415
Iteration 157/1000 | Loss: 0.00047392
Iteration 158/1000 | Loss: 0.00052290
Iteration 159/1000 | Loss: 0.00059815
Iteration 160/1000 | Loss: 0.00058842
Iteration 161/1000 | Loss: 0.00044006
Iteration 162/1000 | Loss: 0.00032922
Iteration 163/1000 | Loss: 0.00035667
Iteration 164/1000 | Loss: 0.00038555
Iteration 165/1000 | Loss: 0.00033391
Iteration 166/1000 | Loss: 0.00038655
Iteration 167/1000 | Loss: 0.00045685
Iteration 168/1000 | Loss: 0.00059821
Iteration 169/1000 | Loss: 0.00047433
Iteration 170/1000 | Loss: 0.00040750
Iteration 171/1000 | Loss: 0.00042131
Iteration 172/1000 | Loss: 0.00039847
Iteration 173/1000 | Loss: 0.00071835
Iteration 174/1000 | Loss: 0.00044924
Iteration 175/1000 | Loss: 0.00045893
Iteration 176/1000 | Loss: 0.00053291
Iteration 177/1000 | Loss: 0.00049347
Iteration 178/1000 | Loss: 0.00053161
Iteration 179/1000 | Loss: 0.00041201
Iteration 180/1000 | Loss: 0.00053497
Iteration 181/1000 | Loss: 0.00051113
Iteration 182/1000 | Loss: 0.00063583
Iteration 183/1000 | Loss: 0.00048588
Iteration 184/1000 | Loss: 0.00053545
Iteration 185/1000 | Loss: 0.00049118
Iteration 186/1000 | Loss: 0.00127794
Iteration 187/1000 | Loss: 0.00036782
Iteration 188/1000 | Loss: 0.00033418
Iteration 189/1000 | Loss: 0.00061438
Iteration 190/1000 | Loss: 0.00038585
Iteration 191/1000 | Loss: 0.00053907
Iteration 192/1000 | Loss: 0.00032981
Iteration 193/1000 | Loss: 0.00066610
Iteration 194/1000 | Loss: 0.00049394
Iteration 195/1000 | Loss: 0.00042531
Iteration 196/1000 | Loss: 0.00035815
Iteration 197/1000 | Loss: 0.00100413
Iteration 198/1000 | Loss: 0.00080699
Iteration 199/1000 | Loss: 0.00041554
Iteration 200/1000 | Loss: 0.00049215
Iteration 201/1000 | Loss: 0.00043458
Iteration 202/1000 | Loss: 0.00048812
Iteration 203/1000 | Loss: 0.00031477
Iteration 204/1000 | Loss: 0.00031420
Iteration 205/1000 | Loss: 0.00030463
Iteration 206/1000 | Loss: 0.00049686
Iteration 207/1000 | Loss: 0.00044880
Iteration 208/1000 | Loss: 0.00048905
Iteration 209/1000 | Loss: 0.00093195
Iteration 210/1000 | Loss: 0.00041235
Iteration 211/1000 | Loss: 0.00041239
Iteration 212/1000 | Loss: 0.00051780
Iteration 213/1000 | Loss: 0.00033836
Iteration 214/1000 | Loss: 0.00032106
Iteration 215/1000 | Loss: 0.00039170
Iteration 216/1000 | Loss: 0.00034852
Iteration 217/1000 | Loss: 0.00049192
Iteration 218/1000 | Loss: 0.00042569
Iteration 219/1000 | Loss: 0.00044232
Iteration 220/1000 | Loss: 0.00049400
Iteration 221/1000 | Loss: 0.00025448
Iteration 222/1000 | Loss: 0.00035642
Iteration 223/1000 | Loss: 0.00043637
Iteration 224/1000 | Loss: 0.00033142
Iteration 225/1000 | Loss: 0.00018372
Iteration 226/1000 | Loss: 0.00028258
Iteration 227/1000 | Loss: 0.00038634
Iteration 228/1000 | Loss: 0.00037533
Iteration 229/1000 | Loss: 0.00033139
Iteration 230/1000 | Loss: 0.00036138
Iteration 231/1000 | Loss: 0.00043253
Iteration 232/1000 | Loss: 0.00043121
Iteration 233/1000 | Loss: 0.00041098
Iteration 234/1000 | Loss: 0.00047048
Iteration 235/1000 | Loss: 0.00046318
Iteration 236/1000 | Loss: 0.00044062
Iteration 237/1000 | Loss: 0.00039804
Iteration 238/1000 | Loss: 0.00034200
Iteration 239/1000 | Loss: 0.00026814
Iteration 240/1000 | Loss: 0.00030905
Iteration 241/1000 | Loss: 0.00032988
Iteration 242/1000 | Loss: 0.00052397
Iteration 243/1000 | Loss: 0.00042693
Iteration 244/1000 | Loss: 0.00041072
Iteration 245/1000 | Loss: 0.00043603
Iteration 246/1000 | Loss: 0.00025913
Iteration 247/1000 | Loss: 0.00028396
Iteration 248/1000 | Loss: 0.00035858
Iteration 249/1000 | Loss: 0.00024272
Iteration 250/1000 | Loss: 0.00032491
Iteration 251/1000 | Loss: 0.00030104
Iteration 252/1000 | Loss: 0.00024158
Iteration 253/1000 | Loss: 0.00019144
Iteration 254/1000 | Loss: 0.00039241
Iteration 255/1000 | Loss: 0.00038976
Iteration 256/1000 | Loss: 0.00039744
Iteration 257/1000 | Loss: 0.00039944
Iteration 258/1000 | Loss: 0.00036127
Iteration 259/1000 | Loss: 0.00037629
Iteration 260/1000 | Loss: 0.00030450
Iteration 261/1000 | Loss: 0.00024471
Iteration 262/1000 | Loss: 0.00023047
Iteration 263/1000 | Loss: 0.00030401
Iteration 264/1000 | Loss: 0.00025496
Iteration 265/1000 | Loss: 0.00028550
Iteration 266/1000 | Loss: 0.00031449
Iteration 267/1000 | Loss: 0.00019431
Iteration 268/1000 | Loss: 0.00032281
Iteration 269/1000 | Loss: 0.00017064
Iteration 270/1000 | Loss: 0.00012515
Iteration 271/1000 | Loss: 0.00035539
Iteration 272/1000 | Loss: 0.00027618
Iteration 273/1000 | Loss: 0.00019467
Iteration 274/1000 | Loss: 0.00034433
Iteration 275/1000 | Loss: 0.00097270
Iteration 276/1000 | Loss: 0.00057111
Iteration 277/1000 | Loss: 0.00022315
Iteration 278/1000 | Loss: 0.00027823
Iteration 279/1000 | Loss: 0.00023117
Iteration 280/1000 | Loss: 0.00026274
Iteration 281/1000 | Loss: 0.00017982
Iteration 282/1000 | Loss: 0.00032944
Iteration 283/1000 | Loss: 0.00022985
Iteration 284/1000 | Loss: 0.00019170
Iteration 285/1000 | Loss: 0.00024987
Iteration 286/1000 | Loss: 0.00020047
Iteration 287/1000 | Loss: 0.00019075
Iteration 288/1000 | Loss: 0.00025054
Iteration 289/1000 | Loss: 0.00027089
Iteration 290/1000 | Loss: 0.00030626
Iteration 291/1000 | Loss: 0.00029522
Iteration 292/1000 | Loss: 0.00024542
Iteration 293/1000 | Loss: 0.00019650
Iteration 294/1000 | Loss: 0.00022439
Iteration 295/1000 | Loss: 0.00058249
Iteration 296/1000 | Loss: 0.00024547
Iteration 297/1000 | Loss: 0.00032128
Iteration 298/1000 | Loss: 0.00015730
Iteration 299/1000 | Loss: 0.00017393
Iteration 300/1000 | Loss: 0.00016160
Iteration 301/1000 | Loss: 0.00013213
Iteration 302/1000 | Loss: 0.00014706
Iteration 303/1000 | Loss: 0.00010327
Iteration 304/1000 | Loss: 0.00011597
Iteration 305/1000 | Loss: 0.00013029
Iteration 306/1000 | Loss: 0.00014551
Iteration 307/1000 | Loss: 0.00010198
Iteration 308/1000 | Loss: 0.00014976
Iteration 309/1000 | Loss: 0.00013630
Iteration 310/1000 | Loss: 0.00011883
Iteration 311/1000 | Loss: 0.00015325
Iteration 312/1000 | Loss: 0.00012029
Iteration 313/1000 | Loss: 0.00015253
Iteration 314/1000 | Loss: 0.00011576
Iteration 315/1000 | Loss: 0.00014197
Iteration 316/1000 | Loss: 0.00011090
Iteration 317/1000 | Loss: 0.00014769
Iteration 318/1000 | Loss: 0.00010554
Iteration 319/1000 | Loss: 0.00017082
Iteration 320/1000 | Loss: 0.00023230
Iteration 321/1000 | Loss: 0.00016160
Iteration 322/1000 | Loss: 0.00011459
Iteration 323/1000 | Loss: 0.00013243
Iteration 324/1000 | Loss: 0.00011625
Iteration 325/1000 | Loss: 0.00015806
Iteration 326/1000 | Loss: 0.00016045
Iteration 327/1000 | Loss: 0.00008759
Iteration 328/1000 | Loss: 0.00011199
Iteration 329/1000 | Loss: 0.00013379
Iteration 330/1000 | Loss: 0.00011265
Iteration 331/1000 | Loss: 0.00015569
Iteration 332/1000 | Loss: 0.00015641
Iteration 333/1000 | Loss: 0.00015072
Iteration 334/1000 | Loss: 0.00011491
Iteration 335/1000 | Loss: 0.00013125
Iteration 336/1000 | Loss: 0.00005977
Iteration 337/1000 | Loss: 0.00007910
Iteration 338/1000 | Loss: 0.00010094
Iteration 339/1000 | Loss: 0.00013531
Iteration 340/1000 | Loss: 0.00011128
Iteration 341/1000 | Loss: 0.00013515
Iteration 342/1000 | Loss: 0.00011711
Iteration 343/1000 | Loss: 0.00016525
Iteration 344/1000 | Loss: 0.00011724
Iteration 345/1000 | Loss: 0.00015268
Iteration 346/1000 | Loss: 0.00011844
Iteration 347/1000 | Loss: 0.00014778
Iteration 348/1000 | Loss: 0.00016078
Iteration 349/1000 | Loss: 0.00016674
Iteration 350/1000 | Loss: 0.00014777
Iteration 351/1000 | Loss: 0.00012336
Iteration 352/1000 | Loss: 0.00014705
Iteration 353/1000 | Loss: 0.00013205
Iteration 354/1000 | Loss: 0.00011089
Iteration 355/1000 | Loss: 0.00005653
Iteration 356/1000 | Loss: 0.00012362
Iteration 357/1000 | Loss: 0.00012342
Iteration 358/1000 | Loss: 0.00013747
Iteration 359/1000 | Loss: 0.00011367
Iteration 360/1000 | Loss: 0.00010931
Iteration 361/1000 | Loss: 0.00010731
Iteration 362/1000 | Loss: 0.00014043
Iteration 363/1000 | Loss: 0.00012185
Iteration 364/1000 | Loss: 0.00015415
Iteration 365/1000 | Loss: 0.00012423
Iteration 366/1000 | Loss: 0.00015771
Iteration 367/1000 | Loss: 0.00013722
Iteration 368/1000 | Loss: 0.00014443
Iteration 369/1000 | Loss: 0.00015938
Iteration 370/1000 | Loss: 0.00015378
Iteration 371/1000 | Loss: 0.00013762
Iteration 372/1000 | Loss: 0.00012688
Iteration 373/1000 | Loss: 0.00016321
Iteration 374/1000 | Loss: 0.00011878
Iteration 375/1000 | Loss: 0.00011912
Iteration 376/1000 | Loss: 0.00014558
Iteration 377/1000 | Loss: 0.00011769
Iteration 378/1000 | Loss: 0.00011687
Iteration 379/1000 | Loss: 0.00011848
Iteration 380/1000 | Loss: 0.00013863
Iteration 381/1000 | Loss: 0.00011503
Iteration 382/1000 | Loss: 0.00011821
Iteration 383/1000 | Loss: 0.00013318
Iteration 384/1000 | Loss: 0.00017928
Iteration 385/1000 | Loss: 0.00015968
Iteration 386/1000 | Loss: 0.00005932
Iteration 387/1000 | Loss: 0.00004591
Iteration 388/1000 | Loss: 0.00003991
Iteration 389/1000 | Loss: 0.00004161
Iteration 390/1000 | Loss: 0.00004490
Iteration 391/1000 | Loss: 0.00004216
Iteration 392/1000 | Loss: 0.00004894
Iteration 393/1000 | Loss: 0.00004475
Iteration 394/1000 | Loss: 0.00005172
Iteration 395/1000 | Loss: 0.00004374
Iteration 396/1000 | Loss: 0.00004848
Iteration 397/1000 | Loss: 0.00004559
Iteration 398/1000 | Loss: 0.00004687
Iteration 399/1000 | Loss: 0.00005026
Iteration 400/1000 | Loss: 0.00004312
Iteration 401/1000 | Loss: 0.00005256
Iteration 402/1000 | Loss: 0.00004265
Iteration 403/1000 | Loss: 0.00004989
Iteration 404/1000 | Loss: 0.00004273
Iteration 405/1000 | Loss: 0.00005291
Iteration 406/1000 | Loss: 0.00003896
Iteration 407/1000 | Loss: 0.00004652
Iteration 408/1000 | Loss: 0.00003769
Iteration 409/1000 | Loss: 0.00004690
Iteration 410/1000 | Loss: 0.00005849
Iteration 411/1000 | Loss: 0.00004649
Iteration 412/1000 | Loss: 0.00004957
Iteration 413/1000 | Loss: 0.00004483
Iteration 414/1000 | Loss: 0.00004669
Iteration 415/1000 | Loss: 0.00004302
Iteration 416/1000 | Loss: 0.00004185
Iteration 417/1000 | Loss: 0.00004768
Iteration 418/1000 | Loss: 0.00005945
Iteration 419/1000 | Loss: 0.00004199
Iteration 420/1000 | Loss: 0.00003694
Iteration 421/1000 | Loss: 0.00003565
Iteration 422/1000 | Loss: 0.00003512
Iteration 423/1000 | Loss: 0.00003477
Iteration 424/1000 | Loss: 0.00003467
Iteration 425/1000 | Loss: 0.00003466
Iteration 426/1000 | Loss: 0.00003465
Iteration 427/1000 | Loss: 0.00003464
Iteration 428/1000 | Loss: 0.00003463
Iteration 429/1000 | Loss: 0.00003463
Iteration 430/1000 | Loss: 0.00003463
Iteration 431/1000 | Loss: 0.00003463
Iteration 432/1000 | Loss: 0.00003463
Iteration 433/1000 | Loss: 0.00003462
Iteration 434/1000 | Loss: 0.00003462
Iteration 435/1000 | Loss: 0.00003462
Iteration 436/1000 | Loss: 0.00003462
Iteration 437/1000 | Loss: 0.00003462
Iteration 438/1000 | Loss: 0.00003461
Iteration 439/1000 | Loss: 0.00003461
Iteration 440/1000 | Loss: 0.00003461
Iteration 441/1000 | Loss: 0.00003461
Iteration 442/1000 | Loss: 0.00003461
Iteration 443/1000 | Loss: 0.00003461
Iteration 444/1000 | Loss: 0.00003461
Iteration 445/1000 | Loss: 0.00003461
Iteration 446/1000 | Loss: 0.00003461
Iteration 447/1000 | Loss: 0.00003461
Iteration 448/1000 | Loss: 0.00003461
Iteration 449/1000 | Loss: 0.00003461
Iteration 450/1000 | Loss: 0.00003461
Iteration 451/1000 | Loss: 0.00003460
Iteration 452/1000 | Loss: 0.00003460
Iteration 453/1000 | Loss: 0.00003460
Iteration 454/1000 | Loss: 0.00003460
Iteration 455/1000 | Loss: 0.00003460
Iteration 456/1000 | Loss: 0.00003460
Iteration 457/1000 | Loss: 0.00003460
Iteration 458/1000 | Loss: 0.00003460
Iteration 459/1000 | Loss: 0.00003460
Iteration 460/1000 | Loss: 0.00003460
Iteration 461/1000 | Loss: 0.00003459
Iteration 462/1000 | Loss: 0.00003459
Iteration 463/1000 | Loss: 0.00003459
Iteration 464/1000 | Loss: 0.00003459
Iteration 465/1000 | Loss: 0.00003459
Iteration 466/1000 | Loss: 0.00003459
Iteration 467/1000 | Loss: 0.00003459
Iteration 468/1000 | Loss: 0.00003459
Iteration 469/1000 | Loss: 0.00003459
Iteration 470/1000 | Loss: 0.00003458
Iteration 471/1000 | Loss: 0.00003458
Iteration 472/1000 | Loss: 0.00003458
Iteration 473/1000 | Loss: 0.00003458
Iteration 474/1000 | Loss: 0.00003458
Iteration 475/1000 | Loss: 0.00003458
Iteration 476/1000 | Loss: 0.00003458
Iteration 477/1000 | Loss: 0.00003458
Iteration 478/1000 | Loss: 0.00003458
Iteration 479/1000 | Loss: 0.00003458
Iteration 480/1000 | Loss: 0.00003458
Iteration 481/1000 | Loss: 0.00003457
Iteration 482/1000 | Loss: 0.00003457
Iteration 483/1000 | Loss: 0.00003457
Iteration 484/1000 | Loss: 0.00003457
Iteration 485/1000 | Loss: 0.00003457
Iteration 486/1000 | Loss: 0.00003457
Iteration 487/1000 | Loss: 0.00003457
Iteration 488/1000 | Loss: 0.00003457
Iteration 489/1000 | Loss: 0.00003456
Iteration 490/1000 | Loss: 0.00003456
Iteration 491/1000 | Loss: 0.00003456
Iteration 492/1000 | Loss: 0.00003456
Iteration 493/1000 | Loss: 0.00003456
Iteration 494/1000 | Loss: 0.00003456
Iteration 495/1000 | Loss: 0.00003455
Iteration 496/1000 | Loss: 0.00003455
Iteration 497/1000 | Loss: 0.00003454
Iteration 498/1000 | Loss: 0.00003454
Iteration 499/1000 | Loss: 0.00003453
Iteration 500/1000 | Loss: 0.00003453
Iteration 501/1000 | Loss: 0.00003452
Iteration 502/1000 | Loss: 0.00003452
Iteration 503/1000 | Loss: 0.00003452
Iteration 504/1000 | Loss: 0.00003452
Iteration 505/1000 | Loss: 0.00003451
Iteration 506/1000 | Loss: 0.00003451
Iteration 507/1000 | Loss: 0.00003451
Iteration 508/1000 | Loss: 0.00003450
Iteration 509/1000 | Loss: 0.00003450
Iteration 510/1000 | Loss: 0.00003450
Iteration 511/1000 | Loss: 0.00003448
Iteration 512/1000 | Loss: 0.00003446
Iteration 513/1000 | Loss: 0.00003446
Iteration 514/1000 | Loss: 0.00003445
Iteration 515/1000 | Loss: 0.00003444
Iteration 516/1000 | Loss: 0.00003444
Iteration 517/1000 | Loss: 0.00003443
Iteration 518/1000 | Loss: 0.00003443
Iteration 519/1000 | Loss: 0.00003443
Iteration 520/1000 | Loss: 0.00003442
Iteration 521/1000 | Loss: 0.00003442
Iteration 522/1000 | Loss: 0.00003442
Iteration 523/1000 | Loss: 0.00003442
Iteration 524/1000 | Loss: 0.00003442
Iteration 525/1000 | Loss: 0.00003441
Iteration 526/1000 | Loss: 0.00003440
Iteration 527/1000 | Loss: 0.00003439
Iteration 528/1000 | Loss: 0.00003439
Iteration 529/1000 | Loss: 0.00003438
Iteration 530/1000 | Loss: 0.00003438
Iteration 531/1000 | Loss: 0.00003438
Iteration 532/1000 | Loss: 0.00003438
Iteration 533/1000 | Loss: 0.00003435
Iteration 534/1000 | Loss: 0.00003434
Iteration 535/1000 | Loss: 0.00003433
Iteration 536/1000 | Loss: 0.00003433
Iteration 537/1000 | Loss: 0.00003433
Iteration 538/1000 | Loss: 0.00003433
Iteration 539/1000 | Loss: 0.00003433
Iteration 540/1000 | Loss: 0.00003432
Iteration 541/1000 | Loss: 0.00003432
Iteration 542/1000 | Loss: 0.00003432
Iteration 543/1000 | Loss: 0.00003432
Iteration 544/1000 | Loss: 0.00003432
Iteration 545/1000 | Loss: 0.00003432
Iteration 546/1000 | Loss: 0.00003432
Iteration 547/1000 | Loss: 0.00003431
Iteration 548/1000 | Loss: 0.00003431
Iteration 549/1000 | Loss: 0.00003431
Iteration 550/1000 | Loss: 0.00003431
Iteration 551/1000 | Loss: 0.00003431
Iteration 552/1000 | Loss: 0.00003431
Iteration 553/1000 | Loss: 0.00003431
Iteration 554/1000 | Loss: 0.00003430
Iteration 555/1000 | Loss: 0.00003430
Iteration 556/1000 | Loss: 0.00003429
Iteration 557/1000 | Loss: 0.00003429
Iteration 558/1000 | Loss: 0.00003428
Iteration 559/1000 | Loss: 0.00003428
Iteration 560/1000 | Loss: 0.00003428
Iteration 561/1000 | Loss: 0.00003428
Iteration 562/1000 | Loss: 0.00003428
Iteration 563/1000 | Loss: 0.00003428
Iteration 564/1000 | Loss: 0.00003428
Iteration 565/1000 | Loss: 0.00003428
Iteration 566/1000 | Loss: 0.00003428
Iteration 567/1000 | Loss: 0.00003428
Iteration 568/1000 | Loss: 0.00003427
Iteration 569/1000 | Loss: 0.00003427
Iteration 570/1000 | Loss: 0.00003427
Iteration 571/1000 | Loss: 0.00003427
Iteration 572/1000 | Loss: 0.00003427
Iteration 573/1000 | Loss: 0.00003427
Iteration 574/1000 | Loss: 0.00003427
Iteration 575/1000 | Loss: 0.00003427
Iteration 576/1000 | Loss: 0.00003427
Iteration 577/1000 | Loss: 0.00003427
Iteration 578/1000 | Loss: 0.00003427
Iteration 579/1000 | Loss: 0.00003426
Iteration 580/1000 | Loss: 0.00003426
Iteration 581/1000 | Loss: 0.00003426
Iteration 582/1000 | Loss: 0.00003426
Iteration 583/1000 | Loss: 0.00003426
Iteration 584/1000 | Loss: 0.00003426
Iteration 585/1000 | Loss: 0.00003426
Iteration 586/1000 | Loss: 0.00003426
Iteration 587/1000 | Loss: 0.00003426
Iteration 588/1000 | Loss: 0.00003426
Iteration 589/1000 | Loss: 0.00003426
Iteration 590/1000 | Loss: 0.00003425
Iteration 591/1000 | Loss: 0.00003425
Iteration 592/1000 | Loss: 0.00003425
Iteration 593/1000 | Loss: 0.00003425
Iteration 594/1000 | Loss: 0.00003425
Iteration 595/1000 | Loss: 0.00003425
Iteration 596/1000 | Loss: 0.00003425
Iteration 597/1000 | Loss: 0.00003425
Iteration 598/1000 | Loss: 0.00003425
Iteration 599/1000 | Loss: 0.00003425
Iteration 600/1000 | Loss: 0.00003425
Iteration 601/1000 | Loss: 0.00003425
Iteration 602/1000 | Loss: 0.00003424
Iteration 603/1000 | Loss: 0.00003424
Iteration 604/1000 | Loss: 0.00003424
Iteration 605/1000 | Loss: 0.00003424
Iteration 606/1000 | Loss: 0.00003424
Iteration 607/1000 | Loss: 0.00003424
Iteration 608/1000 | Loss: 0.00003424
Iteration 609/1000 | Loss: 0.00003424
Iteration 610/1000 | Loss: 0.00003424
Iteration 611/1000 | Loss: 0.00003424
Iteration 612/1000 | Loss: 0.00003424
Iteration 613/1000 | Loss: 0.00003424
Iteration 614/1000 | Loss: 0.00003424
Iteration 615/1000 | Loss: 0.00003424
Iteration 616/1000 | Loss: 0.00003423
Iteration 617/1000 | Loss: 0.00003423
Iteration 618/1000 | Loss: 0.00003423
Iteration 619/1000 | Loss: 0.00003423
Iteration 620/1000 | Loss: 0.00003423
Iteration 621/1000 | Loss: 0.00003423
Iteration 622/1000 | Loss: 0.00003423
Iteration 623/1000 | Loss: 0.00003423
Iteration 624/1000 | Loss: 0.00003423
Iteration 625/1000 | Loss: 0.00003423
Iteration 626/1000 | Loss: 0.00003423
Iteration 627/1000 | Loss: 0.00003423
Iteration 628/1000 | Loss: 0.00003423
Iteration 629/1000 | Loss: 0.00003423
Iteration 630/1000 | Loss: 0.00003423
Iteration 631/1000 | Loss: 0.00003423
Iteration 632/1000 | Loss: 0.00003423
Iteration 633/1000 | Loss: 0.00003423
Iteration 634/1000 | Loss: 0.00003423
Iteration 635/1000 | Loss: 0.00003422
Iteration 636/1000 | Loss: 0.00003422
Iteration 637/1000 | Loss: 0.00003422
Iteration 638/1000 | Loss: 0.00003422
Iteration 639/1000 | Loss: 0.00003422
Iteration 640/1000 | Loss: 0.00003422
Iteration 641/1000 | Loss: 0.00003422
Iteration 642/1000 | Loss: 0.00003422
Iteration 643/1000 | Loss: 0.00003422
Iteration 644/1000 | Loss: 0.00003422
Iteration 645/1000 | Loss: 0.00003422
Iteration 646/1000 | Loss: 0.00003422
Iteration 647/1000 | Loss: 0.00003422
Iteration 648/1000 | Loss: 0.00003422
Iteration 649/1000 | Loss: 0.00003422
Iteration 650/1000 | Loss: 0.00003422
Iteration 651/1000 | Loss: 0.00003422
Iteration 652/1000 | Loss: 0.00003422
Iteration 653/1000 | Loss: 0.00003421
Iteration 654/1000 | Loss: 0.00003421
Iteration 655/1000 | Loss: 0.00003421
Iteration 656/1000 | Loss: 0.00003421
Iteration 657/1000 | Loss: 0.00003421
Iteration 658/1000 | Loss: 0.00003421
Iteration 659/1000 | Loss: 0.00003421
Iteration 660/1000 | Loss: 0.00003421
Iteration 661/1000 | Loss: 0.00003421
Iteration 662/1000 | Loss: 0.00003421
Iteration 663/1000 | Loss: 0.00003421
Iteration 664/1000 | Loss: 0.00003421
Iteration 665/1000 | Loss: 0.00003421
Iteration 666/1000 | Loss: 0.00003421
Iteration 667/1000 | Loss: 0.00003421
Iteration 668/1000 | Loss: 0.00003421
Iteration 669/1000 | Loss: 0.00003421
Iteration 670/1000 | Loss: 0.00003421
Iteration 671/1000 | Loss: 0.00003421
Iteration 672/1000 | Loss: 0.00003421
Iteration 673/1000 | Loss: 0.00003421
Iteration 674/1000 | Loss: 0.00003421
Iteration 675/1000 | Loss: 0.00003421
Iteration 676/1000 | Loss: 0.00003421
Iteration 677/1000 | Loss: 0.00003421
Iteration 678/1000 | Loss: 0.00003421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 678. Stopping optimization.
Last 5 losses: [3.420662324060686e-05, 3.420662324060686e-05, 3.420662324060686e-05, 3.420662324060686e-05, 3.420662324060686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.420662324060686e-05

Optimization complete. Final v2v error: 4.767761707305908 mm

Highest mean error: 6.218140125274658 mm for frame 54

Lowest mean error: 4.0908918380737305 mm for frame 193

Saving results

Total time: 749.0299952030182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836845
Iteration 2/25 | Loss: 0.00160225
Iteration 3/25 | Loss: 0.00115268
Iteration 4/25 | Loss: 0.00101557
Iteration 5/25 | Loss: 0.00094966
Iteration 6/25 | Loss: 0.00095347
Iteration 7/25 | Loss: 0.00094505
Iteration 8/25 | Loss: 0.00092768
Iteration 9/25 | Loss: 0.00091796
Iteration 10/25 | Loss: 0.00091350
Iteration 11/25 | Loss: 0.00090713
Iteration 12/25 | Loss: 0.00090645
Iteration 13/25 | Loss: 0.00090544
Iteration 14/25 | Loss: 0.00090435
Iteration 15/25 | Loss: 0.00090402
Iteration 16/25 | Loss: 0.00090384
Iteration 17/25 | Loss: 0.00090377
Iteration 18/25 | Loss: 0.00090376
Iteration 19/25 | Loss: 0.00090376
Iteration 20/25 | Loss: 0.00090376
Iteration 21/25 | Loss: 0.00090376
Iteration 22/25 | Loss: 0.00090376
Iteration 23/25 | Loss: 0.00090376
Iteration 24/25 | Loss: 0.00090376
Iteration 25/25 | Loss: 0.00090376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19725013
Iteration 2/25 | Loss: 0.00273311
Iteration 3/25 | Loss: 0.00270249
Iteration 4/25 | Loss: 0.00270249
Iteration 5/25 | Loss: 0.00270248
Iteration 6/25 | Loss: 0.00270248
Iteration 7/25 | Loss: 0.00270248
Iteration 8/25 | Loss: 0.00270248
Iteration 9/25 | Loss: 0.00270248
Iteration 10/25 | Loss: 0.00270248
Iteration 11/25 | Loss: 0.00270248
Iteration 12/25 | Loss: 0.00270248
Iteration 13/25 | Loss: 0.00270248
Iteration 14/25 | Loss: 0.00270248
Iteration 15/25 | Loss: 0.00270248
Iteration 16/25 | Loss: 0.00270248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0027024829760193825, 0.0027024829760193825, 0.0027024829760193825, 0.0027024829760193825, 0.0027024829760193825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027024829760193825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270248
Iteration 2/1000 | Loss: 0.00003918
Iteration 3/1000 | Loss: 0.00002939
Iteration 4/1000 | Loss: 0.00002745
Iteration 5/1000 | Loss: 0.00002586
Iteration 6/1000 | Loss: 0.00002501
Iteration 7/1000 | Loss: 0.00002444
Iteration 8/1000 | Loss: 0.00002408
Iteration 9/1000 | Loss: 0.00002390
Iteration 10/1000 | Loss: 0.00002386
Iteration 11/1000 | Loss: 0.00002374
Iteration 12/1000 | Loss: 0.00002371
Iteration 13/1000 | Loss: 0.00002371
Iteration 14/1000 | Loss: 0.00002369
Iteration 15/1000 | Loss: 0.00002367
Iteration 16/1000 | Loss: 0.00002367
Iteration 17/1000 | Loss: 0.00002367
Iteration 18/1000 | Loss: 0.00002366
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002363
Iteration 21/1000 | Loss: 0.00002363
Iteration 22/1000 | Loss: 0.00002363
Iteration 23/1000 | Loss: 0.00002363
Iteration 24/1000 | Loss: 0.00002363
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002362
Iteration 29/1000 | Loss: 0.00002362
Iteration 30/1000 | Loss: 0.00002362
Iteration 31/1000 | Loss: 0.00002362
Iteration 32/1000 | Loss: 0.00002362
Iteration 33/1000 | Loss: 0.00002362
Iteration 34/1000 | Loss: 0.00002362
Iteration 35/1000 | Loss: 0.00002362
Iteration 36/1000 | Loss: 0.00002362
Iteration 37/1000 | Loss: 0.00002361
Iteration 38/1000 | Loss: 0.00002360
Iteration 39/1000 | Loss: 0.00002359
Iteration 40/1000 | Loss: 0.00002359
Iteration 41/1000 | Loss: 0.00002359
Iteration 42/1000 | Loss: 0.00002359
Iteration 43/1000 | Loss: 0.00002358
Iteration 44/1000 | Loss: 0.00002358
Iteration 45/1000 | Loss: 0.00002358
Iteration 46/1000 | Loss: 0.00002357
Iteration 47/1000 | Loss: 0.00002357
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002357
Iteration 50/1000 | Loss: 0.00002357
Iteration 51/1000 | Loss: 0.00002357
Iteration 52/1000 | Loss: 0.00002357
Iteration 53/1000 | Loss: 0.00002357
Iteration 54/1000 | Loss: 0.00002357
Iteration 55/1000 | Loss: 0.00002356
Iteration 56/1000 | Loss: 0.00002356
Iteration 57/1000 | Loss: 0.00002356
Iteration 58/1000 | Loss: 0.00002356
Iteration 59/1000 | Loss: 0.00002356
Iteration 60/1000 | Loss: 0.00002356
Iteration 61/1000 | Loss: 0.00002356
Iteration 62/1000 | Loss: 0.00002356
Iteration 63/1000 | Loss: 0.00002355
Iteration 64/1000 | Loss: 0.00002355
Iteration 65/1000 | Loss: 0.00002355
Iteration 66/1000 | Loss: 0.00002355
Iteration 67/1000 | Loss: 0.00002355
Iteration 68/1000 | Loss: 0.00002355
Iteration 69/1000 | Loss: 0.00002355
Iteration 70/1000 | Loss: 0.00002355
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002354
Iteration 73/1000 | Loss: 0.00002354
Iteration 74/1000 | Loss: 0.00002354
Iteration 75/1000 | Loss: 0.00002354
Iteration 76/1000 | Loss: 0.00002354
Iteration 77/1000 | Loss: 0.00002354
Iteration 78/1000 | Loss: 0.00002354
Iteration 79/1000 | Loss: 0.00002354
Iteration 80/1000 | Loss: 0.00002354
Iteration 81/1000 | Loss: 0.00002354
Iteration 82/1000 | Loss: 0.00002354
Iteration 83/1000 | Loss: 0.00002354
Iteration 84/1000 | Loss: 0.00002354
Iteration 85/1000 | Loss: 0.00002354
Iteration 86/1000 | Loss: 0.00002354
Iteration 87/1000 | Loss: 0.00002353
Iteration 88/1000 | Loss: 0.00002353
Iteration 89/1000 | Loss: 0.00002353
Iteration 90/1000 | Loss: 0.00002353
Iteration 91/1000 | Loss: 0.00002353
Iteration 92/1000 | Loss: 0.00002353
Iteration 93/1000 | Loss: 0.00002353
Iteration 94/1000 | Loss: 0.00002353
Iteration 95/1000 | Loss: 0.00002353
Iteration 96/1000 | Loss: 0.00002353
Iteration 97/1000 | Loss: 0.00002353
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002353
Iteration 100/1000 | Loss: 0.00002353
Iteration 101/1000 | Loss: 0.00002353
Iteration 102/1000 | Loss: 0.00002352
Iteration 103/1000 | Loss: 0.00002352
Iteration 104/1000 | Loss: 0.00002352
Iteration 105/1000 | Loss: 0.00002352
Iteration 106/1000 | Loss: 0.00002352
Iteration 107/1000 | Loss: 0.00002352
Iteration 108/1000 | Loss: 0.00002352
Iteration 109/1000 | Loss: 0.00002352
Iteration 110/1000 | Loss: 0.00002352
Iteration 111/1000 | Loss: 0.00002352
Iteration 112/1000 | Loss: 0.00002352
Iteration 113/1000 | Loss: 0.00002352
Iteration 114/1000 | Loss: 0.00002352
Iteration 115/1000 | Loss: 0.00002352
Iteration 116/1000 | Loss: 0.00002351
Iteration 117/1000 | Loss: 0.00002351
Iteration 118/1000 | Loss: 0.00002351
Iteration 119/1000 | Loss: 0.00002351
Iteration 120/1000 | Loss: 0.00002351
Iteration 121/1000 | Loss: 0.00002351
Iteration 122/1000 | Loss: 0.00002351
Iteration 123/1000 | Loss: 0.00002351
Iteration 124/1000 | Loss: 0.00002351
Iteration 125/1000 | Loss: 0.00002351
Iteration 126/1000 | Loss: 0.00002351
Iteration 127/1000 | Loss: 0.00002351
Iteration 128/1000 | Loss: 0.00002351
Iteration 129/1000 | Loss: 0.00002351
Iteration 130/1000 | Loss: 0.00002350
Iteration 131/1000 | Loss: 0.00002350
Iteration 132/1000 | Loss: 0.00002350
Iteration 133/1000 | Loss: 0.00002350
Iteration 134/1000 | Loss: 0.00002350
Iteration 135/1000 | Loss: 0.00002350
Iteration 136/1000 | Loss: 0.00002350
Iteration 137/1000 | Loss: 0.00002350
Iteration 138/1000 | Loss: 0.00002350
Iteration 139/1000 | Loss: 0.00002350
Iteration 140/1000 | Loss: 0.00002350
Iteration 141/1000 | Loss: 0.00002350
Iteration 142/1000 | Loss: 0.00002349
Iteration 143/1000 | Loss: 0.00002349
Iteration 144/1000 | Loss: 0.00002349
Iteration 145/1000 | Loss: 0.00002349
Iteration 146/1000 | Loss: 0.00002349
Iteration 147/1000 | Loss: 0.00002349
Iteration 148/1000 | Loss: 0.00002349
Iteration 149/1000 | Loss: 0.00002348
Iteration 150/1000 | Loss: 0.00002348
Iteration 151/1000 | Loss: 0.00002348
Iteration 152/1000 | Loss: 0.00002348
Iteration 153/1000 | Loss: 0.00002348
Iteration 154/1000 | Loss: 0.00002348
Iteration 155/1000 | Loss: 0.00002348
Iteration 156/1000 | Loss: 0.00002348
Iteration 157/1000 | Loss: 0.00002348
Iteration 158/1000 | Loss: 0.00002348
Iteration 159/1000 | Loss: 0.00002347
Iteration 160/1000 | Loss: 0.00002347
Iteration 161/1000 | Loss: 0.00002347
Iteration 162/1000 | Loss: 0.00002346
Iteration 163/1000 | Loss: 0.00002346
Iteration 164/1000 | Loss: 0.00002346
Iteration 165/1000 | Loss: 0.00002346
Iteration 166/1000 | Loss: 0.00002346
Iteration 167/1000 | Loss: 0.00002346
Iteration 168/1000 | Loss: 0.00002346
Iteration 169/1000 | Loss: 0.00002346
Iteration 170/1000 | Loss: 0.00002346
Iteration 171/1000 | Loss: 0.00002346
Iteration 172/1000 | Loss: 0.00002346
Iteration 173/1000 | Loss: 0.00002346
Iteration 174/1000 | Loss: 0.00002345
Iteration 175/1000 | Loss: 0.00002345
Iteration 176/1000 | Loss: 0.00002345
Iteration 177/1000 | Loss: 0.00002345
Iteration 178/1000 | Loss: 0.00002345
Iteration 179/1000 | Loss: 0.00002345
Iteration 180/1000 | Loss: 0.00002345
Iteration 181/1000 | Loss: 0.00002345
Iteration 182/1000 | Loss: 0.00002345
Iteration 183/1000 | Loss: 0.00002345
Iteration 184/1000 | Loss: 0.00002345
Iteration 185/1000 | Loss: 0.00002345
Iteration 186/1000 | Loss: 0.00002345
Iteration 187/1000 | Loss: 0.00002344
Iteration 188/1000 | Loss: 0.00002344
Iteration 189/1000 | Loss: 0.00002344
Iteration 190/1000 | Loss: 0.00002344
Iteration 191/1000 | Loss: 0.00002344
Iteration 192/1000 | Loss: 0.00002344
Iteration 193/1000 | Loss: 0.00002344
Iteration 194/1000 | Loss: 0.00002344
Iteration 195/1000 | Loss: 0.00002344
Iteration 196/1000 | Loss: 0.00002344
Iteration 197/1000 | Loss: 0.00002344
Iteration 198/1000 | Loss: 0.00002344
Iteration 199/1000 | Loss: 0.00002344
Iteration 200/1000 | Loss: 0.00002344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.3444250473403372e-05, 2.3444250473403372e-05, 2.3444250473403372e-05, 2.3444250473403372e-05, 2.3444250473403372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3444250473403372e-05

Optimization complete. Final v2v error: 3.437012195587158 mm

Highest mean error: 21.465288162231445 mm for frame 114

Lowest mean error: 3.001476764678955 mm for frame 9

Saving results

Total time: 54.826338052749634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949996
Iteration 2/25 | Loss: 0.00121368
Iteration 3/25 | Loss: 0.00102440
Iteration 4/25 | Loss: 0.00099419
Iteration 5/25 | Loss: 0.00098717
Iteration 6/25 | Loss: 0.00098561
Iteration 7/25 | Loss: 0.00098559
Iteration 8/25 | Loss: 0.00098559
Iteration 9/25 | Loss: 0.00098559
Iteration 10/25 | Loss: 0.00098559
Iteration 11/25 | Loss: 0.00098559
Iteration 12/25 | Loss: 0.00098559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009855860844254494, 0.0009855860844254494, 0.0009855860844254494, 0.0009855860844254494, 0.0009855860844254494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009855860844254494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15969539
Iteration 2/25 | Loss: 0.00242453
Iteration 3/25 | Loss: 0.00242452
Iteration 4/25 | Loss: 0.00242452
Iteration 5/25 | Loss: 0.00242451
Iteration 6/25 | Loss: 0.00242451
Iteration 7/25 | Loss: 0.00242451
Iteration 8/25 | Loss: 0.00242451
Iteration 9/25 | Loss: 0.00242451
Iteration 10/25 | Loss: 0.00242451
Iteration 11/25 | Loss: 0.00242451
Iteration 12/25 | Loss: 0.00242451
Iteration 13/25 | Loss: 0.00242451
Iteration 14/25 | Loss: 0.00242451
Iteration 15/25 | Loss: 0.00242451
Iteration 16/25 | Loss: 0.00242451
Iteration 17/25 | Loss: 0.00242451
Iteration 18/25 | Loss: 0.00242451
Iteration 19/25 | Loss: 0.00242451
Iteration 20/25 | Loss: 0.00242451
Iteration 21/25 | Loss: 0.00242451
Iteration 22/25 | Loss: 0.00242451
Iteration 23/25 | Loss: 0.00242451
Iteration 24/25 | Loss: 0.00242451
Iteration 25/25 | Loss: 0.00242451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242451
Iteration 2/1000 | Loss: 0.00004066
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002339
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00001965
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001821
Iteration 10/1000 | Loss: 0.00001817
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001804
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001789
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001776
Iteration 47/1000 | Loss: 0.00001776
Iteration 48/1000 | Loss: 0.00001776
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001774
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001774
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001773
Iteration 66/1000 | Loss: 0.00001773
Iteration 67/1000 | Loss: 0.00001773
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001772
Iteration 71/1000 | Loss: 0.00001772
Iteration 72/1000 | Loss: 0.00001772
Iteration 73/1000 | Loss: 0.00001772
Iteration 74/1000 | Loss: 0.00001772
Iteration 75/1000 | Loss: 0.00001772
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.771768438629806e-05, 1.771768438629806e-05, 1.771768438629806e-05, 1.771768438629806e-05, 1.771768438629806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.771768438629806e-05

Optimization complete. Final v2v error: 3.6343650817871094 mm

Highest mean error: 4.298410415649414 mm for frame 8

Lowest mean error: 3.281949281692505 mm for frame 60

Saving results

Total time: 35.391357421875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01133960
Iteration 2/25 | Loss: 0.00246078
Iteration 3/25 | Loss: 0.00164693
Iteration 4/25 | Loss: 0.00141280
Iteration 5/25 | Loss: 0.00129934
Iteration 6/25 | Loss: 0.00125044
Iteration 7/25 | Loss: 0.00121941
Iteration 8/25 | Loss: 0.00119338
Iteration 9/25 | Loss: 0.00117250
Iteration 10/25 | Loss: 0.00115687
Iteration 11/25 | Loss: 0.00116410
Iteration 12/25 | Loss: 0.00115898
Iteration 13/25 | Loss: 0.00114832
Iteration 14/25 | Loss: 0.00113902
Iteration 15/25 | Loss: 0.00113479
Iteration 16/25 | Loss: 0.00113683
Iteration 17/25 | Loss: 0.00113225
Iteration 18/25 | Loss: 0.00113364
Iteration 19/25 | Loss: 0.00112883
Iteration 20/25 | Loss: 0.00112569
Iteration 21/25 | Loss: 0.00112388
Iteration 22/25 | Loss: 0.00112293
Iteration 23/25 | Loss: 0.00112229
Iteration 24/25 | Loss: 0.00112225
Iteration 25/25 | Loss: 0.00112224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67233455
Iteration 2/25 | Loss: 0.00339382
Iteration 3/25 | Loss: 0.00339381
Iteration 4/25 | Loss: 0.00339380
Iteration 5/25 | Loss: 0.00339380
Iteration 6/25 | Loss: 0.00339380
Iteration 7/25 | Loss: 0.00339380
Iteration 8/25 | Loss: 0.00339380
Iteration 9/25 | Loss: 0.00339380
Iteration 10/25 | Loss: 0.00339380
Iteration 11/25 | Loss: 0.00339380
Iteration 12/25 | Loss: 0.00339380
Iteration 13/25 | Loss: 0.00339380
Iteration 14/25 | Loss: 0.00339380
Iteration 15/25 | Loss: 0.00339380
Iteration 16/25 | Loss: 0.00339380
Iteration 17/25 | Loss: 0.00339380
Iteration 18/25 | Loss: 0.00339380
Iteration 19/25 | Loss: 0.00339380
Iteration 20/25 | Loss: 0.00339380
Iteration 21/25 | Loss: 0.00339380
Iteration 22/25 | Loss: 0.00339380
Iteration 23/25 | Loss: 0.00339380
Iteration 24/25 | Loss: 0.00339380
Iteration 25/25 | Loss: 0.00339380

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00339380
Iteration 2/1000 | Loss: 0.00084427
Iteration 3/1000 | Loss: 0.00012741
Iteration 4/1000 | Loss: 0.00006887
Iteration 5/1000 | Loss: 0.00005111
Iteration 6/1000 | Loss: 0.00004049
Iteration 7/1000 | Loss: 0.00003489
Iteration 8/1000 | Loss: 0.00003222
Iteration 9/1000 | Loss: 0.00018129
Iteration 10/1000 | Loss: 0.00017012
Iteration 11/1000 | Loss: 0.00003410
Iteration 12/1000 | Loss: 0.00002961
Iteration 13/1000 | Loss: 0.00002698
Iteration 14/1000 | Loss: 0.00002565
Iteration 15/1000 | Loss: 0.00002493
Iteration 16/1000 | Loss: 0.00002456
Iteration 17/1000 | Loss: 0.00002436
Iteration 18/1000 | Loss: 0.00002422
Iteration 19/1000 | Loss: 0.00002410
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002408
Iteration 22/1000 | Loss: 0.00002407
Iteration 23/1000 | Loss: 0.00002407
Iteration 24/1000 | Loss: 0.00002406
Iteration 25/1000 | Loss: 0.00002405
Iteration 26/1000 | Loss: 0.00002400
Iteration 27/1000 | Loss: 0.00002394
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00002392
Iteration 30/1000 | Loss: 0.00002392
Iteration 31/1000 | Loss: 0.00002391
Iteration 32/1000 | Loss: 0.00002391
Iteration 33/1000 | Loss: 0.00002390
Iteration 34/1000 | Loss: 0.00002390
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002388
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002388
Iteration 39/1000 | Loss: 0.00002387
Iteration 40/1000 | Loss: 0.00002387
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002381
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002381
Iteration 45/1000 | Loss: 0.00002380
Iteration 46/1000 | Loss: 0.00002380
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00002379
Iteration 49/1000 | Loss: 0.00002379
Iteration 50/1000 | Loss: 0.00002378
Iteration 51/1000 | Loss: 0.00002378
Iteration 52/1000 | Loss: 0.00002378
Iteration 53/1000 | Loss: 0.00002377
Iteration 54/1000 | Loss: 0.00002377
Iteration 55/1000 | Loss: 0.00002377
Iteration 56/1000 | Loss: 0.00002376
Iteration 57/1000 | Loss: 0.00002376
Iteration 58/1000 | Loss: 0.00002376
Iteration 59/1000 | Loss: 0.00002376
Iteration 60/1000 | Loss: 0.00002376
Iteration 61/1000 | Loss: 0.00002376
Iteration 62/1000 | Loss: 0.00002376
Iteration 63/1000 | Loss: 0.00002376
Iteration 64/1000 | Loss: 0.00002376
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002375
Iteration 67/1000 | Loss: 0.00002375
Iteration 68/1000 | Loss: 0.00002375
Iteration 69/1000 | Loss: 0.00002375
Iteration 70/1000 | Loss: 0.00002375
Iteration 71/1000 | Loss: 0.00002375
Iteration 72/1000 | Loss: 0.00002375
Iteration 73/1000 | Loss: 0.00002375
Iteration 74/1000 | Loss: 0.00002375
Iteration 75/1000 | Loss: 0.00002375
Iteration 76/1000 | Loss: 0.00002375
Iteration 77/1000 | Loss: 0.00002375
Iteration 78/1000 | Loss: 0.00002375
Iteration 79/1000 | Loss: 0.00002374
Iteration 80/1000 | Loss: 0.00002374
Iteration 81/1000 | Loss: 0.00002374
Iteration 82/1000 | Loss: 0.00002374
Iteration 83/1000 | Loss: 0.00002374
Iteration 84/1000 | Loss: 0.00002374
Iteration 85/1000 | Loss: 0.00002374
Iteration 86/1000 | Loss: 0.00002374
Iteration 87/1000 | Loss: 0.00002373
Iteration 88/1000 | Loss: 0.00002373
Iteration 89/1000 | Loss: 0.00002373
Iteration 90/1000 | Loss: 0.00002373
Iteration 91/1000 | Loss: 0.00002373
Iteration 92/1000 | Loss: 0.00002373
Iteration 93/1000 | Loss: 0.00002372
Iteration 94/1000 | Loss: 0.00002372
Iteration 95/1000 | Loss: 0.00002372
Iteration 96/1000 | Loss: 0.00002372
Iteration 97/1000 | Loss: 0.00002372
Iteration 98/1000 | Loss: 0.00002372
Iteration 99/1000 | Loss: 0.00002372
Iteration 100/1000 | Loss: 0.00002372
Iteration 101/1000 | Loss: 0.00002372
Iteration 102/1000 | Loss: 0.00002372
Iteration 103/1000 | Loss: 0.00002372
Iteration 104/1000 | Loss: 0.00002372
Iteration 105/1000 | Loss: 0.00002372
Iteration 106/1000 | Loss: 0.00002372
Iteration 107/1000 | Loss: 0.00002372
Iteration 108/1000 | Loss: 0.00002372
Iteration 109/1000 | Loss: 0.00002372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.3715951101621613e-05, 2.3715951101621613e-05, 2.3715951101621613e-05, 2.3715951101621613e-05, 2.3715951101621613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3715951101621613e-05

Optimization complete. Final v2v error: 4.177491188049316 mm

Highest mean error: 4.617961406707764 mm for frame 29

Lowest mean error: 3.8832168579101562 mm for frame 229

Saving results

Total time: 86.17356133460999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067957
Iteration 2/25 | Loss: 0.00325117
Iteration 3/25 | Loss: 0.00220829
Iteration 4/25 | Loss: 0.00212615
Iteration 5/25 | Loss: 0.00196805
Iteration 6/25 | Loss: 0.00196251
Iteration 7/25 | Loss: 0.00179343
Iteration 8/25 | Loss: 0.00184608
Iteration 9/25 | Loss: 0.00196146
Iteration 10/25 | Loss: 0.00210794
Iteration 11/25 | Loss: 0.00154895
Iteration 12/25 | Loss: 0.00124002
Iteration 13/25 | Loss: 0.00118174
Iteration 14/25 | Loss: 0.00115597
Iteration 15/25 | Loss: 0.00115749
Iteration 16/25 | Loss: 0.00115900
Iteration 17/25 | Loss: 0.00114178
Iteration 18/25 | Loss: 0.00113197
Iteration 19/25 | Loss: 0.00113084
Iteration 20/25 | Loss: 0.00113190
Iteration 21/25 | Loss: 0.00113003
Iteration 22/25 | Loss: 0.00112983
Iteration 23/25 | Loss: 0.00112784
Iteration 24/25 | Loss: 0.00113066
Iteration 25/25 | Loss: 0.00113035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.31147289
Iteration 2/25 | Loss: 0.00428563
Iteration 3/25 | Loss: 0.00285127
Iteration 4/25 | Loss: 0.00285127
Iteration 5/25 | Loss: 0.00285127
Iteration 6/25 | Loss: 0.00285127
Iteration 7/25 | Loss: 0.00285127
Iteration 8/25 | Loss: 0.00285127
Iteration 9/25 | Loss: 0.00285127
Iteration 10/25 | Loss: 0.00285127
Iteration 11/25 | Loss: 0.00285126
Iteration 12/25 | Loss: 0.00285126
Iteration 13/25 | Loss: 0.00285126
Iteration 14/25 | Loss: 0.00285126
Iteration 15/25 | Loss: 0.00285126
Iteration 16/25 | Loss: 0.00285126
Iteration 17/25 | Loss: 0.00285126
Iteration 18/25 | Loss: 0.00285126
Iteration 19/25 | Loss: 0.00285126
Iteration 20/25 | Loss: 0.00285126
Iteration 21/25 | Loss: 0.00285126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0028512647841125727, 0.0028512647841125727, 0.0028512647841125727, 0.0028512647841125727, 0.0028512647841125727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028512647841125727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285126
Iteration 2/1000 | Loss: 0.00116366
Iteration 3/1000 | Loss: 0.00012104
Iteration 4/1000 | Loss: 0.00013810
Iteration 5/1000 | Loss: 0.00009424
Iteration 6/1000 | Loss: 0.00157308
Iteration 7/1000 | Loss: 0.00023357
Iteration 8/1000 | Loss: 0.00080576
Iteration 9/1000 | Loss: 0.00013263
Iteration 10/1000 | Loss: 0.00013467
Iteration 11/1000 | Loss: 0.00014261
Iteration 12/1000 | Loss: 0.00015020
Iteration 13/1000 | Loss: 0.00054394
Iteration 14/1000 | Loss: 0.00007940
Iteration 15/1000 | Loss: 0.00015865
Iteration 16/1000 | Loss: 0.00017373
Iteration 17/1000 | Loss: 0.00012900
Iteration 18/1000 | Loss: 0.00010228
Iteration 19/1000 | Loss: 0.00012928
Iteration 20/1000 | Loss: 0.00011064
Iteration 21/1000 | Loss: 0.00011876
Iteration 22/1000 | Loss: 0.00012678
Iteration 23/1000 | Loss: 0.00012548
Iteration 24/1000 | Loss: 0.00012829
Iteration 25/1000 | Loss: 0.00011279
Iteration 26/1000 | Loss: 0.00006979
Iteration 27/1000 | Loss: 0.00008225
Iteration 28/1000 | Loss: 0.00010160
Iteration 29/1000 | Loss: 0.00009931
Iteration 30/1000 | Loss: 0.00009278
Iteration 31/1000 | Loss: 0.00007519
Iteration 32/1000 | Loss: 0.00008897
Iteration 33/1000 | Loss: 0.00013442
Iteration 34/1000 | Loss: 0.00009092
Iteration 35/1000 | Loss: 0.00008902
Iteration 36/1000 | Loss: 0.00009072
Iteration 37/1000 | Loss: 0.00008552
Iteration 38/1000 | Loss: 0.00009294
Iteration 39/1000 | Loss: 0.00008761
Iteration 40/1000 | Loss: 0.00009099
Iteration 41/1000 | Loss: 0.00012032
Iteration 42/1000 | Loss: 0.00009029
Iteration 43/1000 | Loss: 0.00011073
Iteration 44/1000 | Loss: 0.00008537
Iteration 45/1000 | Loss: 0.00008390
Iteration 46/1000 | Loss: 0.00003729
Iteration 47/1000 | Loss: 0.00006694
Iteration 48/1000 | Loss: 0.00005134
Iteration 49/1000 | Loss: 0.00005743
Iteration 50/1000 | Loss: 0.00005034
Iteration 51/1000 | Loss: 0.00005910
Iteration 52/1000 | Loss: 0.00004975
Iteration 53/1000 | Loss: 0.00007174
Iteration 54/1000 | Loss: 0.00005111
Iteration 55/1000 | Loss: 0.00007170
Iteration 56/1000 | Loss: 0.00005088
Iteration 57/1000 | Loss: 0.00002996
Iteration 58/1000 | Loss: 0.00002795
Iteration 59/1000 | Loss: 0.00002714
Iteration 60/1000 | Loss: 0.00002672
Iteration 61/1000 | Loss: 0.00002646
Iteration 62/1000 | Loss: 0.00002640
Iteration 63/1000 | Loss: 0.00002638
Iteration 64/1000 | Loss: 0.00002637
Iteration 65/1000 | Loss: 0.00002632
Iteration 66/1000 | Loss: 0.00002627
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002620
Iteration 69/1000 | Loss: 0.00002616
Iteration 70/1000 | Loss: 0.00002616
Iteration 71/1000 | Loss: 0.00002616
Iteration 72/1000 | Loss: 0.00002612
Iteration 73/1000 | Loss: 0.00002611
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002609
Iteration 76/1000 | Loss: 0.00002609
Iteration 77/1000 | Loss: 0.00002609
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002609
Iteration 80/1000 | Loss: 0.00002609
Iteration 81/1000 | Loss: 0.00002608
Iteration 82/1000 | Loss: 0.00002608
Iteration 83/1000 | Loss: 0.00002608
Iteration 84/1000 | Loss: 0.00002608
Iteration 85/1000 | Loss: 0.00002608
Iteration 86/1000 | Loss: 0.00002608
Iteration 87/1000 | Loss: 0.00002608
Iteration 88/1000 | Loss: 0.00002608
Iteration 89/1000 | Loss: 0.00002607
Iteration 90/1000 | Loss: 0.00002606
Iteration 91/1000 | Loss: 0.00002606
Iteration 92/1000 | Loss: 0.00002606
Iteration 93/1000 | Loss: 0.00002605
Iteration 94/1000 | Loss: 0.00002598
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002598
Iteration 97/1000 | Loss: 0.00002596
Iteration 98/1000 | Loss: 0.00002596
Iteration 99/1000 | Loss: 0.00002596
Iteration 100/1000 | Loss: 0.00002596
Iteration 101/1000 | Loss: 0.00002596
Iteration 102/1000 | Loss: 0.00002596
Iteration 103/1000 | Loss: 0.00002596
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002595
Iteration 107/1000 | Loss: 0.00002595
Iteration 108/1000 | Loss: 0.00002595
Iteration 109/1000 | Loss: 0.00002595
Iteration 110/1000 | Loss: 0.00002595
Iteration 111/1000 | Loss: 0.00002595
Iteration 112/1000 | Loss: 0.00002595
Iteration 113/1000 | Loss: 0.00002594
Iteration 114/1000 | Loss: 0.00002594
Iteration 115/1000 | Loss: 0.00002593
Iteration 116/1000 | Loss: 0.00002593
Iteration 117/1000 | Loss: 0.00002593
Iteration 118/1000 | Loss: 0.00002593
Iteration 119/1000 | Loss: 0.00002592
Iteration 120/1000 | Loss: 0.00002592
Iteration 121/1000 | Loss: 0.00002591
Iteration 122/1000 | Loss: 0.00002591
Iteration 123/1000 | Loss: 0.00002591
Iteration 124/1000 | Loss: 0.00002591
Iteration 125/1000 | Loss: 0.00002591
Iteration 126/1000 | Loss: 0.00002591
Iteration 127/1000 | Loss: 0.00002591
Iteration 128/1000 | Loss: 0.00002591
Iteration 129/1000 | Loss: 0.00002591
Iteration 130/1000 | Loss: 0.00002590
Iteration 131/1000 | Loss: 0.00002590
Iteration 132/1000 | Loss: 0.00002590
Iteration 133/1000 | Loss: 0.00002590
Iteration 134/1000 | Loss: 0.00002590
Iteration 135/1000 | Loss: 0.00002590
Iteration 136/1000 | Loss: 0.00002590
Iteration 137/1000 | Loss: 0.00002589
Iteration 138/1000 | Loss: 0.00002589
Iteration 139/1000 | Loss: 0.00002589
Iteration 140/1000 | Loss: 0.00002589
Iteration 141/1000 | Loss: 0.00002589
Iteration 142/1000 | Loss: 0.00002588
Iteration 143/1000 | Loss: 0.00002588
Iteration 144/1000 | Loss: 0.00002588
Iteration 145/1000 | Loss: 0.00002588
Iteration 146/1000 | Loss: 0.00002587
Iteration 147/1000 | Loss: 0.00002587
Iteration 148/1000 | Loss: 0.00002587
Iteration 149/1000 | Loss: 0.00002587
Iteration 150/1000 | Loss: 0.00002587
Iteration 151/1000 | Loss: 0.00002587
Iteration 152/1000 | Loss: 0.00002587
Iteration 153/1000 | Loss: 0.00002587
Iteration 154/1000 | Loss: 0.00002586
Iteration 155/1000 | Loss: 0.00002586
Iteration 156/1000 | Loss: 0.00002586
Iteration 157/1000 | Loss: 0.00002586
Iteration 158/1000 | Loss: 0.00002585
Iteration 159/1000 | Loss: 0.00002585
Iteration 160/1000 | Loss: 0.00002585
Iteration 161/1000 | Loss: 0.00002585
Iteration 162/1000 | Loss: 0.00002584
Iteration 163/1000 | Loss: 0.00002584
Iteration 164/1000 | Loss: 0.00002584
Iteration 165/1000 | Loss: 0.00002584
Iteration 166/1000 | Loss: 0.00002584
Iteration 167/1000 | Loss: 0.00002584
Iteration 168/1000 | Loss: 0.00002584
Iteration 169/1000 | Loss: 0.00002584
Iteration 170/1000 | Loss: 0.00002584
Iteration 171/1000 | Loss: 0.00002584
Iteration 172/1000 | Loss: 0.00002584
Iteration 173/1000 | Loss: 0.00002583
Iteration 174/1000 | Loss: 0.00002583
Iteration 175/1000 | Loss: 0.00002583
Iteration 176/1000 | Loss: 0.00002583
Iteration 177/1000 | Loss: 0.00002583
Iteration 178/1000 | Loss: 0.00002583
Iteration 179/1000 | Loss: 0.00002583
Iteration 180/1000 | Loss: 0.00002583
Iteration 181/1000 | Loss: 0.00002583
Iteration 182/1000 | Loss: 0.00002583
Iteration 183/1000 | Loss: 0.00002583
Iteration 184/1000 | Loss: 0.00002583
Iteration 185/1000 | Loss: 0.00002583
Iteration 186/1000 | Loss: 0.00002583
Iteration 187/1000 | Loss: 0.00002583
Iteration 188/1000 | Loss: 0.00002583
Iteration 189/1000 | Loss: 0.00002583
Iteration 190/1000 | Loss: 0.00002583
Iteration 191/1000 | Loss: 0.00002583
Iteration 192/1000 | Loss: 0.00002583
Iteration 193/1000 | Loss: 0.00002583
Iteration 194/1000 | Loss: 0.00002583
Iteration 195/1000 | Loss: 0.00002583
Iteration 196/1000 | Loss: 0.00002583
Iteration 197/1000 | Loss: 0.00002583
Iteration 198/1000 | Loss: 0.00002583
Iteration 199/1000 | Loss: 0.00002583
Iteration 200/1000 | Loss: 0.00002583
Iteration 201/1000 | Loss: 0.00002583
Iteration 202/1000 | Loss: 0.00002583
Iteration 203/1000 | Loss: 0.00002583
Iteration 204/1000 | Loss: 0.00002583
Iteration 205/1000 | Loss: 0.00002583
Iteration 206/1000 | Loss: 0.00002583
Iteration 207/1000 | Loss: 0.00002583
Iteration 208/1000 | Loss: 0.00002583
Iteration 209/1000 | Loss: 0.00002583
Iteration 210/1000 | Loss: 0.00002583
Iteration 211/1000 | Loss: 0.00002583
Iteration 212/1000 | Loss: 0.00002583
Iteration 213/1000 | Loss: 0.00002583
Iteration 214/1000 | Loss: 0.00002583
Iteration 215/1000 | Loss: 0.00002583
Iteration 216/1000 | Loss: 0.00002583
Iteration 217/1000 | Loss: 0.00002583
Iteration 218/1000 | Loss: 0.00002583
Iteration 219/1000 | Loss: 0.00002583
Iteration 220/1000 | Loss: 0.00002583
Iteration 221/1000 | Loss: 0.00002583
Iteration 222/1000 | Loss: 0.00002583
Iteration 223/1000 | Loss: 0.00002583
Iteration 224/1000 | Loss: 0.00002583
Iteration 225/1000 | Loss: 0.00002583
Iteration 226/1000 | Loss: 0.00002583
Iteration 227/1000 | Loss: 0.00002583
Iteration 228/1000 | Loss: 0.00002583
Iteration 229/1000 | Loss: 0.00002583
Iteration 230/1000 | Loss: 0.00002583
Iteration 231/1000 | Loss: 0.00002583
Iteration 232/1000 | Loss: 0.00002583
Iteration 233/1000 | Loss: 0.00002583
Iteration 234/1000 | Loss: 0.00002583
Iteration 235/1000 | Loss: 0.00002583
Iteration 236/1000 | Loss: 0.00002583
Iteration 237/1000 | Loss: 0.00002583
Iteration 238/1000 | Loss: 0.00002583
Iteration 239/1000 | Loss: 0.00002583
Iteration 240/1000 | Loss: 0.00002583
Iteration 241/1000 | Loss: 0.00002583
Iteration 242/1000 | Loss: 0.00002583
Iteration 243/1000 | Loss: 0.00002583
Iteration 244/1000 | Loss: 0.00002583
Iteration 245/1000 | Loss: 0.00002583
Iteration 246/1000 | Loss: 0.00002583
Iteration 247/1000 | Loss: 0.00002583
Iteration 248/1000 | Loss: 0.00002583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.582753950264305e-05, 2.582753950264305e-05, 2.582753950264305e-05, 2.582753950264305e-05, 2.582753950264305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.582753950264305e-05

Optimization complete. Final v2v error: 4.331554412841797 mm

Highest mean error: 4.818388938903809 mm for frame 14

Lowest mean error: 3.9460184574127197 mm for frame 48

Saving results

Total time: 140.54619073867798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01129712
Iteration 2/25 | Loss: 0.00164715
Iteration 3/25 | Loss: 0.00122495
Iteration 4/25 | Loss: 0.00120876
Iteration 5/25 | Loss: 0.00107354
Iteration 6/25 | Loss: 0.00107381
Iteration 7/25 | Loss: 0.00107525
Iteration 8/25 | Loss: 0.00106805
Iteration 9/25 | Loss: 0.00103767
Iteration 10/25 | Loss: 0.00102685
Iteration 11/25 | Loss: 0.00101217
Iteration 12/25 | Loss: 0.00101728
Iteration 13/25 | Loss: 0.00101653
Iteration 14/25 | Loss: 0.00100822
Iteration 15/25 | Loss: 0.00100669
Iteration 16/25 | Loss: 0.00100059
Iteration 17/25 | Loss: 0.00099582
Iteration 18/25 | Loss: 0.00099914
Iteration 19/25 | Loss: 0.00099683
Iteration 20/25 | Loss: 0.00099342
Iteration 21/25 | Loss: 0.00098859
Iteration 22/25 | Loss: 0.00098904
Iteration 23/25 | Loss: 0.00098860
Iteration 24/25 | Loss: 0.00099439
Iteration 25/25 | Loss: 0.00098505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73879349
Iteration 2/25 | Loss: 0.00310220
Iteration 3/25 | Loss: 0.00310220
Iteration 4/25 | Loss: 0.00310220
Iteration 5/25 | Loss: 0.00310220
Iteration 6/25 | Loss: 0.00310220
Iteration 7/25 | Loss: 0.00310220
Iteration 8/25 | Loss: 0.00310220
Iteration 9/25 | Loss: 0.00310220
Iteration 10/25 | Loss: 0.00310220
Iteration 11/25 | Loss: 0.00310220
Iteration 12/25 | Loss: 0.00310220
Iteration 13/25 | Loss: 0.00310220
Iteration 14/25 | Loss: 0.00310220
Iteration 15/25 | Loss: 0.00310220
Iteration 16/25 | Loss: 0.00310220
Iteration 17/25 | Loss: 0.00310220
Iteration 18/25 | Loss: 0.00310220
Iteration 19/25 | Loss: 0.00310220
Iteration 20/25 | Loss: 0.00310220
Iteration 21/25 | Loss: 0.00310220
Iteration 22/25 | Loss: 0.00310220
Iteration 23/25 | Loss: 0.00310220
Iteration 24/25 | Loss: 0.00310220
Iteration 25/25 | Loss: 0.00310220

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310220
Iteration 2/1000 | Loss: 0.00009293
Iteration 3/1000 | Loss: 0.00004093
Iteration 4/1000 | Loss: 0.00028390
Iteration 5/1000 | Loss: 0.00004793
Iteration 6/1000 | Loss: 0.00004144
Iteration 7/1000 | Loss: 0.00119521
Iteration 8/1000 | Loss: 0.00132241
Iteration 9/1000 | Loss: 0.00032935
Iteration 10/1000 | Loss: 0.00004815
Iteration 11/1000 | Loss: 0.00003749
Iteration 12/1000 | Loss: 0.00003328
Iteration 13/1000 | Loss: 0.00002924
Iteration 14/1000 | Loss: 0.00002997
Iteration 15/1000 | Loss: 0.00002652
Iteration 16/1000 | Loss: 0.00002550
Iteration 17/1000 | Loss: 0.00002444
Iteration 18/1000 | Loss: 0.00002370
Iteration 19/1000 | Loss: 0.00034391
Iteration 20/1000 | Loss: 0.00039277
Iteration 21/1000 | Loss: 0.00007125
Iteration 22/1000 | Loss: 0.00013414
Iteration 23/1000 | Loss: 0.00003012
Iteration 24/1000 | Loss: 0.00002550
Iteration 25/1000 | Loss: 0.00002821
Iteration 26/1000 | Loss: 0.00002296
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002152
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002044
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002032
Iteration 43/1000 | Loss: 0.00002032
Iteration 44/1000 | Loss: 0.00002032
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002032
Iteration 47/1000 | Loss: 0.00002032
Iteration 48/1000 | Loss: 0.00002032
Iteration 49/1000 | Loss: 0.00002032
Iteration 50/1000 | Loss: 0.00002032
Iteration 51/1000 | Loss: 0.00002032
Iteration 52/1000 | Loss: 0.00002032
Iteration 53/1000 | Loss: 0.00002031
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002030
Iteration 59/1000 | Loss: 0.00002030
Iteration 60/1000 | Loss: 0.00002030
Iteration 61/1000 | Loss: 0.00002030
Iteration 62/1000 | Loss: 0.00002030
Iteration 63/1000 | Loss: 0.00002029
Iteration 64/1000 | Loss: 0.00002029
Iteration 65/1000 | Loss: 0.00002029
Iteration 66/1000 | Loss: 0.00002029
Iteration 67/1000 | Loss: 0.00002029
Iteration 68/1000 | Loss: 0.00002029
Iteration 69/1000 | Loss: 0.00002029
Iteration 70/1000 | Loss: 0.00002028
Iteration 71/1000 | Loss: 0.00002028
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002026
Iteration 77/1000 | Loss: 0.00002026
Iteration 78/1000 | Loss: 0.00002026
Iteration 79/1000 | Loss: 0.00002025
Iteration 80/1000 | Loss: 0.00002024
Iteration 81/1000 | Loss: 0.00002024
Iteration 82/1000 | Loss: 0.00002024
Iteration 83/1000 | Loss: 0.00002024
Iteration 84/1000 | Loss: 0.00002024
Iteration 85/1000 | Loss: 0.00002024
Iteration 86/1000 | Loss: 0.00002024
Iteration 87/1000 | Loss: 0.00002024
Iteration 88/1000 | Loss: 0.00002023
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002023
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002023
Iteration 95/1000 | Loss: 0.00002023
Iteration 96/1000 | Loss: 0.00002023
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002023
Iteration 99/1000 | Loss: 0.00002023
Iteration 100/1000 | Loss: 0.00002023
Iteration 101/1000 | Loss: 0.00002023
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002022
Iteration 105/1000 | Loss: 0.00002022
Iteration 106/1000 | Loss: 0.00002022
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002022
Iteration 111/1000 | Loss: 0.00002022
Iteration 112/1000 | Loss: 0.00002022
Iteration 113/1000 | Loss: 0.00002021
Iteration 114/1000 | Loss: 0.00002021
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002021
Iteration 118/1000 | Loss: 0.00002021
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002021
Iteration 121/1000 | Loss: 0.00002021
Iteration 122/1000 | Loss: 0.00002021
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002021
Iteration 125/1000 | Loss: 0.00002021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.020691317738965e-05, 2.020691317738965e-05, 2.020691317738965e-05, 2.020691317738965e-05, 2.020691317738965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.020691317738965e-05

Optimization complete. Final v2v error: 3.9104270935058594 mm

Highest mean error: 4.6938090324401855 mm for frame 65

Lowest mean error: 3.545199155807495 mm for frame 41

Saving results

Total time: 94.9777135848999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886240
Iteration 2/25 | Loss: 0.00131587
Iteration 3/25 | Loss: 0.00100770
Iteration 4/25 | Loss: 0.00097386
Iteration 5/25 | Loss: 0.00096215
Iteration 6/25 | Loss: 0.00095840
Iteration 7/25 | Loss: 0.00095751
Iteration 8/25 | Loss: 0.00095751
Iteration 9/25 | Loss: 0.00095751
Iteration 10/25 | Loss: 0.00095751
Iteration 11/25 | Loss: 0.00095751
Iteration 12/25 | Loss: 0.00095751
Iteration 13/25 | Loss: 0.00095751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009575137519277632, 0.0009575137519277632, 0.0009575137519277632, 0.0009575137519277632, 0.0009575137519277632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009575137519277632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45768917
Iteration 2/25 | Loss: 0.00267237
Iteration 3/25 | Loss: 0.00267236
Iteration 4/25 | Loss: 0.00267236
Iteration 5/25 | Loss: 0.00267236
Iteration 6/25 | Loss: 0.00267236
Iteration 7/25 | Loss: 0.00267236
Iteration 8/25 | Loss: 0.00267236
Iteration 9/25 | Loss: 0.00267236
Iteration 10/25 | Loss: 0.00267236
Iteration 11/25 | Loss: 0.00267236
Iteration 12/25 | Loss: 0.00267236
Iteration 13/25 | Loss: 0.00267236
Iteration 14/25 | Loss: 0.00267236
Iteration 15/25 | Loss: 0.00267236
Iteration 16/25 | Loss: 0.00267236
Iteration 17/25 | Loss: 0.00267236
Iteration 18/25 | Loss: 0.00267236
Iteration 19/25 | Loss: 0.00267236
Iteration 20/25 | Loss: 0.00267236
Iteration 21/25 | Loss: 0.00267236
Iteration 22/25 | Loss: 0.00267236
Iteration 23/25 | Loss: 0.00267236
Iteration 24/25 | Loss: 0.00267236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002672359813004732, 0.002672359813004732, 0.002672359813004732, 0.002672359813004732, 0.002672359813004732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002672359813004732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267236
Iteration 2/1000 | Loss: 0.00003664
Iteration 3/1000 | Loss: 0.00002773
Iteration 4/1000 | Loss: 0.00002472
Iteration 5/1000 | Loss: 0.00002327
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002154
Iteration 9/1000 | Loss: 0.00002115
Iteration 10/1000 | Loss: 0.00002091
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002064
Iteration 14/1000 | Loss: 0.00002063
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002041
Iteration 21/1000 | Loss: 0.00002038
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002035
Iteration 25/1000 | Loss: 0.00002035
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002034
Iteration 28/1000 | Loss: 0.00002031
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00002030
Iteration 32/1000 | Loss: 0.00002030
Iteration 33/1000 | Loss: 0.00002030
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00002029
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00002028
Iteration 38/1000 | Loss: 0.00002026
Iteration 39/1000 | Loss: 0.00002026
Iteration 40/1000 | Loss: 0.00002026
Iteration 41/1000 | Loss: 0.00002026
Iteration 42/1000 | Loss: 0.00002026
Iteration 43/1000 | Loss: 0.00002026
Iteration 44/1000 | Loss: 0.00002026
Iteration 45/1000 | Loss: 0.00002026
Iteration 46/1000 | Loss: 0.00002026
Iteration 47/1000 | Loss: 0.00002026
Iteration 48/1000 | Loss: 0.00002026
Iteration 49/1000 | Loss: 0.00002026
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00002024
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002023
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002021
Iteration 59/1000 | Loss: 0.00002021
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002021
Iteration 62/1000 | Loss: 0.00002021
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002020
Iteration 66/1000 | Loss: 0.00002019
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002018
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00002018
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002018
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002017
Iteration 84/1000 | Loss: 0.00002017
Iteration 85/1000 | Loss: 0.00002017
Iteration 86/1000 | Loss: 0.00002016
Iteration 87/1000 | Loss: 0.00002016
Iteration 88/1000 | Loss: 0.00002015
Iteration 89/1000 | Loss: 0.00002015
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002015
Iteration 96/1000 | Loss: 0.00002015
Iteration 97/1000 | Loss: 0.00002015
Iteration 98/1000 | Loss: 0.00002014
Iteration 99/1000 | Loss: 0.00002014
Iteration 100/1000 | Loss: 0.00002014
Iteration 101/1000 | Loss: 0.00002014
Iteration 102/1000 | Loss: 0.00002014
Iteration 103/1000 | Loss: 0.00002014
Iteration 104/1000 | Loss: 0.00002014
Iteration 105/1000 | Loss: 0.00002014
Iteration 106/1000 | Loss: 0.00002014
Iteration 107/1000 | Loss: 0.00002014
Iteration 108/1000 | Loss: 0.00002014
Iteration 109/1000 | Loss: 0.00002014
Iteration 110/1000 | Loss: 0.00002014
Iteration 111/1000 | Loss: 0.00002014
Iteration 112/1000 | Loss: 0.00002014
Iteration 113/1000 | Loss: 0.00002014
Iteration 114/1000 | Loss: 0.00002014
Iteration 115/1000 | Loss: 0.00002014
Iteration 116/1000 | Loss: 0.00002014
Iteration 117/1000 | Loss: 0.00002014
Iteration 118/1000 | Loss: 0.00002014
Iteration 119/1000 | Loss: 0.00002014
Iteration 120/1000 | Loss: 0.00002014
Iteration 121/1000 | Loss: 0.00002014
Iteration 122/1000 | Loss: 0.00002014
Iteration 123/1000 | Loss: 0.00002014
Iteration 124/1000 | Loss: 0.00002014
Iteration 125/1000 | Loss: 0.00002014
Iteration 126/1000 | Loss: 0.00002014
Iteration 127/1000 | Loss: 0.00002014
Iteration 128/1000 | Loss: 0.00002014
Iteration 129/1000 | Loss: 0.00002014
Iteration 130/1000 | Loss: 0.00002014
Iteration 131/1000 | Loss: 0.00002014
Iteration 132/1000 | Loss: 0.00002014
Iteration 133/1000 | Loss: 0.00002014
Iteration 134/1000 | Loss: 0.00002014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.0139324988122098e-05, 2.0139324988122098e-05, 2.0139324988122098e-05, 2.0139324988122098e-05, 2.0139324988122098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0139324988122098e-05

Optimization complete. Final v2v error: 3.802678108215332 mm

Highest mean error: 5.446662425994873 mm for frame 63

Lowest mean error: 3.2541544437408447 mm for frame 0

Saving results

Total time: 37.77022671699524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6785/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6785/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742816
Iteration 2/25 | Loss: 0.00165186
Iteration 3/25 | Loss: 0.00117794
Iteration 4/25 | Loss: 0.00101902
Iteration 5/25 | Loss: 0.00100190
Iteration 6/25 | Loss: 0.00098470
Iteration 7/25 | Loss: 0.00097823
Iteration 8/25 | Loss: 0.00097509
Iteration 9/25 | Loss: 0.00097676
Iteration 10/25 | Loss: 0.00097783
Iteration 11/25 | Loss: 0.00097647
Iteration 12/25 | Loss: 0.00097227
Iteration 13/25 | Loss: 0.00096994
Iteration 14/25 | Loss: 0.00097021
Iteration 15/25 | Loss: 0.00096869
Iteration 16/25 | Loss: 0.00096967
Iteration 17/25 | Loss: 0.00096990
Iteration 18/25 | Loss: 0.00096967
Iteration 19/25 | Loss: 0.00096972
Iteration 20/25 | Loss: 0.00096923
Iteration 21/25 | Loss: 0.00096781
Iteration 22/25 | Loss: 0.00096954
Iteration 23/25 | Loss: 0.00096950
Iteration 24/25 | Loss: 0.00096662
Iteration 25/25 | Loss: 0.00096754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55906725
Iteration 2/25 | Loss: 0.00288620
Iteration 3/25 | Loss: 0.00288615
Iteration 4/25 | Loss: 0.00288615
Iteration 5/25 | Loss: 0.00288615
Iteration 6/25 | Loss: 0.00288615
Iteration 7/25 | Loss: 0.00288615
Iteration 8/25 | Loss: 0.00288615
Iteration 9/25 | Loss: 0.00288615
Iteration 10/25 | Loss: 0.00288615
Iteration 11/25 | Loss: 0.00288615
Iteration 12/25 | Loss: 0.00288615
Iteration 13/25 | Loss: 0.00288615
Iteration 14/25 | Loss: 0.00288615
Iteration 15/25 | Loss: 0.00288615
Iteration 16/25 | Loss: 0.00288615
Iteration 17/25 | Loss: 0.00288615
Iteration 18/25 | Loss: 0.00288615
Iteration 19/25 | Loss: 0.00288615
Iteration 20/25 | Loss: 0.00288615
Iteration 21/25 | Loss: 0.00288615
Iteration 22/25 | Loss: 0.00288615
Iteration 23/25 | Loss: 0.00288615
Iteration 24/25 | Loss: 0.00288615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002886149799451232, 0.002886149799451232, 0.002886149799451232, 0.002886149799451232, 0.002886149799451232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002886149799451232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288615
Iteration 2/1000 | Loss: 0.00006390
Iteration 3/1000 | Loss: 0.00009314
Iteration 4/1000 | Loss: 0.00007955
Iteration 5/1000 | Loss: 0.00003772
Iteration 6/1000 | Loss: 0.00006797
Iteration 7/1000 | Loss: 0.00007876
Iteration 8/1000 | Loss: 0.00006470
Iteration 9/1000 | Loss: 0.00007225
Iteration 10/1000 | Loss: 0.00006465
Iteration 11/1000 | Loss: 0.00022444
Iteration 12/1000 | Loss: 0.00020575
Iteration 13/1000 | Loss: 0.00016900
Iteration 14/1000 | Loss: 0.00017844
Iteration 15/1000 | Loss: 0.00005804
Iteration 16/1000 | Loss: 0.00013685
Iteration 17/1000 | Loss: 0.00003323
Iteration 18/1000 | Loss: 0.00002894
Iteration 19/1000 | Loss: 0.00002549
Iteration 20/1000 | Loss: 0.00002407
Iteration 21/1000 | Loss: 0.00002311
Iteration 22/1000 | Loss: 0.00002248
Iteration 23/1000 | Loss: 0.00002200
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002109
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002076
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002061
Iteration 30/1000 | Loss: 0.00002059
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002052
Iteration 36/1000 | Loss: 0.00002052
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002050
Iteration 40/1000 | Loss: 0.00002050
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00002049
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00002049
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002048
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002047
Iteration 51/1000 | Loss: 0.00002046
Iteration 52/1000 | Loss: 0.00002046
Iteration 53/1000 | Loss: 0.00002045
Iteration 54/1000 | Loss: 0.00002045
Iteration 55/1000 | Loss: 0.00002045
Iteration 56/1000 | Loss: 0.00002045
Iteration 57/1000 | Loss: 0.00002044
Iteration 58/1000 | Loss: 0.00002044
Iteration 59/1000 | Loss: 0.00002044
Iteration 60/1000 | Loss: 0.00002043
Iteration 61/1000 | Loss: 0.00002043
Iteration 62/1000 | Loss: 0.00002042
Iteration 63/1000 | Loss: 0.00002042
Iteration 64/1000 | Loss: 0.00002042
Iteration 65/1000 | Loss: 0.00002041
Iteration 66/1000 | Loss: 0.00002041
Iteration 67/1000 | Loss: 0.00002041
Iteration 68/1000 | Loss: 0.00002041
Iteration 69/1000 | Loss: 0.00002040
Iteration 70/1000 | Loss: 0.00002040
Iteration 71/1000 | Loss: 0.00002040
Iteration 72/1000 | Loss: 0.00002040
Iteration 73/1000 | Loss: 0.00002039
Iteration 74/1000 | Loss: 0.00002039
Iteration 75/1000 | Loss: 0.00002039
Iteration 76/1000 | Loss: 0.00002039
Iteration 77/1000 | Loss: 0.00002038
Iteration 78/1000 | Loss: 0.00002038
Iteration 79/1000 | Loss: 0.00002038
Iteration 80/1000 | Loss: 0.00002037
Iteration 81/1000 | Loss: 0.00002037
Iteration 82/1000 | Loss: 0.00002037
Iteration 83/1000 | Loss: 0.00002037
Iteration 84/1000 | Loss: 0.00002037
Iteration 85/1000 | Loss: 0.00002037
Iteration 86/1000 | Loss: 0.00002037
Iteration 87/1000 | Loss: 0.00002037
Iteration 88/1000 | Loss: 0.00002037
Iteration 89/1000 | Loss: 0.00002037
Iteration 90/1000 | Loss: 0.00002037
Iteration 91/1000 | Loss: 0.00002037
Iteration 92/1000 | Loss: 0.00002036
Iteration 93/1000 | Loss: 0.00002036
Iteration 94/1000 | Loss: 0.00002036
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002036
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002036
Iteration 103/1000 | Loss: 0.00002036
Iteration 104/1000 | Loss: 0.00002036
Iteration 105/1000 | Loss: 0.00002036
Iteration 106/1000 | Loss: 0.00002036
Iteration 107/1000 | Loss: 0.00002036
Iteration 108/1000 | Loss: 0.00002036
Iteration 109/1000 | Loss: 0.00002036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.0358007532195188e-05, 2.0358007532195188e-05, 2.0358007532195188e-05, 2.0358007532195188e-05, 2.0358007532195188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0358007532195188e-05

Optimization complete. Final v2v error: 3.9426381587982178 mm

Highest mean error: 4.605008602142334 mm for frame 149

Lowest mean error: 3.3452706336975098 mm for frame 1

Saving results

Total time: 91.07047414779663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009187
Iteration 2/25 | Loss: 0.00281775
Iteration 3/25 | Loss: 0.00220733
Iteration 4/25 | Loss: 0.00205244
Iteration 5/25 | Loss: 0.00192300
Iteration 6/25 | Loss: 0.00188380
Iteration 7/25 | Loss: 0.00173213
Iteration 8/25 | Loss: 0.00162198
Iteration 9/25 | Loss: 0.00150999
Iteration 10/25 | Loss: 0.00148956
Iteration 11/25 | Loss: 0.00147258
Iteration 12/25 | Loss: 0.00145435
Iteration 13/25 | Loss: 0.00145323
Iteration 14/25 | Loss: 0.00146205
Iteration 15/25 | Loss: 0.00144736
Iteration 16/25 | Loss: 0.00143312
Iteration 17/25 | Loss: 0.00143274
Iteration 18/25 | Loss: 0.00143405
Iteration 19/25 | Loss: 0.00142962
Iteration 20/25 | Loss: 0.00143214
Iteration 21/25 | Loss: 0.00144014
Iteration 22/25 | Loss: 0.00142125
Iteration 23/25 | Loss: 0.00141952
Iteration 24/25 | Loss: 0.00141882
Iteration 25/25 | Loss: 0.00141859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56370521
Iteration 2/25 | Loss: 0.00394042
Iteration 3/25 | Loss: 0.00311190
Iteration 4/25 | Loss: 0.00311190
Iteration 5/25 | Loss: 0.00311190
Iteration 6/25 | Loss: 0.00311190
Iteration 7/25 | Loss: 0.00311190
Iteration 8/25 | Loss: 0.00311190
Iteration 9/25 | Loss: 0.00311190
Iteration 10/25 | Loss: 0.00311190
Iteration 11/25 | Loss: 0.00311190
Iteration 12/25 | Loss: 0.00311190
Iteration 13/25 | Loss: 0.00311190
Iteration 14/25 | Loss: 0.00311190
Iteration 15/25 | Loss: 0.00311190
Iteration 16/25 | Loss: 0.00311190
Iteration 17/25 | Loss: 0.00311190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0031118961051106453, 0.0031118961051106453, 0.0031118961051106453, 0.0031118961051106453, 0.0031118961051106453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031118961051106453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311190
Iteration 2/1000 | Loss: 0.00083394
Iteration 3/1000 | Loss: 0.00040194
Iteration 4/1000 | Loss: 0.00025183
Iteration 5/1000 | Loss: 0.00144469
Iteration 6/1000 | Loss: 0.00016482
Iteration 7/1000 | Loss: 0.00116556
Iteration 8/1000 | Loss: 0.00016962
Iteration 9/1000 | Loss: 0.00027740
Iteration 10/1000 | Loss: 0.00013542
Iteration 11/1000 | Loss: 0.00095581
Iteration 12/1000 | Loss: 0.00013488
Iteration 13/1000 | Loss: 0.00053196
Iteration 14/1000 | Loss: 0.00053462
Iteration 15/1000 | Loss: 0.00012292
Iteration 16/1000 | Loss: 0.00012037
Iteration 17/1000 | Loss: 0.00016690
Iteration 18/1000 | Loss: 0.00075875
Iteration 19/1000 | Loss: 0.00058054
Iteration 20/1000 | Loss: 0.00011714
Iteration 21/1000 | Loss: 0.00026981
Iteration 22/1000 | Loss: 0.00188376
Iteration 23/1000 | Loss: 0.00320616
Iteration 24/1000 | Loss: 0.00170567
Iteration 25/1000 | Loss: 0.00019451
Iteration 26/1000 | Loss: 0.00029958
Iteration 27/1000 | Loss: 0.00026607
Iteration 28/1000 | Loss: 0.00037999
Iteration 29/1000 | Loss: 0.00055660
Iteration 30/1000 | Loss: 0.00012446
Iteration 31/1000 | Loss: 0.00016822
Iteration 32/1000 | Loss: 0.00011145
Iteration 33/1000 | Loss: 0.00018919
Iteration 34/1000 | Loss: 0.00040356
Iteration 35/1000 | Loss: 0.00092377
Iteration 36/1000 | Loss: 0.00016492
Iteration 37/1000 | Loss: 0.00011181
Iteration 38/1000 | Loss: 0.00010773
Iteration 39/1000 | Loss: 0.00010675
Iteration 40/1000 | Loss: 0.00010589
Iteration 41/1000 | Loss: 0.00010500
Iteration 42/1000 | Loss: 0.00058490
Iteration 43/1000 | Loss: 0.00013081
Iteration 44/1000 | Loss: 0.00025012
Iteration 45/1000 | Loss: 0.00012774
Iteration 46/1000 | Loss: 0.00015881
Iteration 47/1000 | Loss: 0.00010380
Iteration 48/1000 | Loss: 0.00024626
Iteration 49/1000 | Loss: 0.00021359
Iteration 50/1000 | Loss: 0.00010986
Iteration 51/1000 | Loss: 0.00024183
Iteration 52/1000 | Loss: 0.00036127
Iteration 53/1000 | Loss: 0.00026825
Iteration 54/1000 | Loss: 0.00043411
Iteration 55/1000 | Loss: 0.00010353
Iteration 56/1000 | Loss: 0.00017714
Iteration 57/1000 | Loss: 0.00014161
Iteration 58/1000 | Loss: 0.00010316
Iteration 59/1000 | Loss: 0.00020373
Iteration 60/1000 | Loss: 0.00012830
Iteration 61/1000 | Loss: 0.00018139
Iteration 62/1000 | Loss: 0.00011616
Iteration 63/1000 | Loss: 0.00015615
Iteration 64/1000 | Loss: 0.00011101
Iteration 65/1000 | Loss: 0.00031710
Iteration 66/1000 | Loss: 0.00010525
Iteration 67/1000 | Loss: 0.00018466
Iteration 68/1000 | Loss: 0.00010631
Iteration 69/1000 | Loss: 0.00013886
Iteration 70/1000 | Loss: 0.00010349
Iteration 71/1000 | Loss: 0.00018057
Iteration 72/1000 | Loss: 0.00040829
Iteration 73/1000 | Loss: 0.00010674
Iteration 74/1000 | Loss: 0.00019091
Iteration 75/1000 | Loss: 0.00010583
Iteration 76/1000 | Loss: 0.00010846
Iteration 77/1000 | Loss: 0.00009896
Iteration 78/1000 | Loss: 0.00009841
Iteration 79/1000 | Loss: 0.00023586
Iteration 80/1000 | Loss: 0.00014869
Iteration 81/1000 | Loss: 0.00010722
Iteration 82/1000 | Loss: 0.00010101
Iteration 83/1000 | Loss: 0.00009906
Iteration 84/1000 | Loss: 0.00009812
Iteration 85/1000 | Loss: 0.00020224
Iteration 86/1000 | Loss: 0.00009684
Iteration 87/1000 | Loss: 0.00009635
Iteration 88/1000 | Loss: 0.00009607
Iteration 89/1000 | Loss: 0.00016440
Iteration 90/1000 | Loss: 0.00015653
Iteration 91/1000 | Loss: 0.00009595
Iteration 92/1000 | Loss: 0.00009579
Iteration 93/1000 | Loss: 0.00015787
Iteration 94/1000 | Loss: 0.00009656
Iteration 95/1000 | Loss: 0.00009577
Iteration 96/1000 | Loss: 0.00011187
Iteration 97/1000 | Loss: 0.00009617
Iteration 98/1000 | Loss: 0.00009567
Iteration 99/1000 | Loss: 0.00009754
Iteration 100/1000 | Loss: 0.00009555
Iteration 101/1000 | Loss: 0.00009551
Iteration 102/1000 | Loss: 0.00009538
Iteration 103/1000 | Loss: 0.00009532
Iteration 104/1000 | Loss: 0.00009531
Iteration 105/1000 | Loss: 0.00009529
Iteration 106/1000 | Loss: 0.00009529
Iteration 107/1000 | Loss: 0.00009529
Iteration 108/1000 | Loss: 0.00009528
Iteration 109/1000 | Loss: 0.00009528
Iteration 110/1000 | Loss: 0.00009528
Iteration 111/1000 | Loss: 0.00009528
Iteration 112/1000 | Loss: 0.00009528
Iteration 113/1000 | Loss: 0.00009528
Iteration 114/1000 | Loss: 0.00009528
Iteration 115/1000 | Loss: 0.00009528
Iteration 116/1000 | Loss: 0.00009528
Iteration 117/1000 | Loss: 0.00009527
Iteration 118/1000 | Loss: 0.00009527
Iteration 119/1000 | Loss: 0.00009527
Iteration 120/1000 | Loss: 0.00009526
Iteration 121/1000 | Loss: 0.00009526
Iteration 122/1000 | Loss: 0.00009525
Iteration 123/1000 | Loss: 0.00009525
Iteration 124/1000 | Loss: 0.00009525
Iteration 125/1000 | Loss: 0.00009525
Iteration 126/1000 | Loss: 0.00009525
Iteration 127/1000 | Loss: 0.00009525
Iteration 128/1000 | Loss: 0.00009524
Iteration 129/1000 | Loss: 0.00009524
Iteration 130/1000 | Loss: 0.00009524
Iteration 131/1000 | Loss: 0.00009524
Iteration 132/1000 | Loss: 0.00009524
Iteration 133/1000 | Loss: 0.00009524
Iteration 134/1000 | Loss: 0.00009524
Iteration 135/1000 | Loss: 0.00009524
Iteration 136/1000 | Loss: 0.00009524
Iteration 137/1000 | Loss: 0.00009524
Iteration 138/1000 | Loss: 0.00009524
Iteration 139/1000 | Loss: 0.00009523
Iteration 140/1000 | Loss: 0.00009523
Iteration 141/1000 | Loss: 0.00009523
Iteration 142/1000 | Loss: 0.00009522
Iteration 143/1000 | Loss: 0.00009522
Iteration 144/1000 | Loss: 0.00009522
Iteration 145/1000 | Loss: 0.00009522
Iteration 146/1000 | Loss: 0.00009522
Iteration 147/1000 | Loss: 0.00009522
Iteration 148/1000 | Loss: 0.00009522
Iteration 149/1000 | Loss: 0.00009521
Iteration 150/1000 | Loss: 0.00009521
Iteration 151/1000 | Loss: 0.00009521
Iteration 152/1000 | Loss: 0.00009521
Iteration 153/1000 | Loss: 0.00009521
Iteration 154/1000 | Loss: 0.00009521
Iteration 155/1000 | Loss: 0.00009521
Iteration 156/1000 | Loss: 0.00009520
Iteration 157/1000 | Loss: 0.00009520
Iteration 158/1000 | Loss: 0.00009520
Iteration 159/1000 | Loss: 0.00009520
Iteration 160/1000 | Loss: 0.00009520
Iteration 161/1000 | Loss: 0.00009520
Iteration 162/1000 | Loss: 0.00009520
Iteration 163/1000 | Loss: 0.00009520
Iteration 164/1000 | Loss: 0.00009520
Iteration 165/1000 | Loss: 0.00009519
Iteration 166/1000 | Loss: 0.00009519
Iteration 167/1000 | Loss: 0.00009519
Iteration 168/1000 | Loss: 0.00009519
Iteration 169/1000 | Loss: 0.00009519
Iteration 170/1000 | Loss: 0.00009518
Iteration 171/1000 | Loss: 0.00009517
Iteration 172/1000 | Loss: 0.00009517
Iteration 173/1000 | Loss: 0.00009517
Iteration 174/1000 | Loss: 0.00009516
Iteration 175/1000 | Loss: 0.00009514
Iteration 176/1000 | Loss: 0.00009514
Iteration 177/1000 | Loss: 0.00009514
Iteration 178/1000 | Loss: 0.00009514
Iteration 179/1000 | Loss: 0.00009514
Iteration 180/1000 | Loss: 0.00009514
Iteration 181/1000 | Loss: 0.00009514
Iteration 182/1000 | Loss: 0.00009514
Iteration 183/1000 | Loss: 0.00009514
Iteration 184/1000 | Loss: 0.00009514
Iteration 185/1000 | Loss: 0.00009513
Iteration 186/1000 | Loss: 0.00009511
Iteration 187/1000 | Loss: 0.00009511
Iteration 188/1000 | Loss: 0.00009511
Iteration 189/1000 | Loss: 0.00009511
Iteration 190/1000 | Loss: 0.00009510
Iteration 191/1000 | Loss: 0.00009510
Iteration 192/1000 | Loss: 0.00009510
Iteration 193/1000 | Loss: 0.00009510
Iteration 194/1000 | Loss: 0.00009510
Iteration 195/1000 | Loss: 0.00009510
Iteration 196/1000 | Loss: 0.00009510
Iteration 197/1000 | Loss: 0.00009509
Iteration 198/1000 | Loss: 0.00009509
Iteration 199/1000 | Loss: 0.00009509
Iteration 200/1000 | Loss: 0.00009509
Iteration 201/1000 | Loss: 0.00009509
Iteration 202/1000 | Loss: 0.00009509
Iteration 203/1000 | Loss: 0.00009509
Iteration 204/1000 | Loss: 0.00009508
Iteration 205/1000 | Loss: 0.00009508
Iteration 206/1000 | Loss: 0.00009508
Iteration 207/1000 | Loss: 0.00009507
Iteration 208/1000 | Loss: 0.00009507
Iteration 209/1000 | Loss: 0.00009507
Iteration 210/1000 | Loss: 0.00009506
Iteration 211/1000 | Loss: 0.00009506
Iteration 212/1000 | Loss: 0.00009506
Iteration 213/1000 | Loss: 0.00009506
Iteration 214/1000 | Loss: 0.00009506
Iteration 215/1000 | Loss: 0.00009506
Iteration 216/1000 | Loss: 0.00009506
Iteration 217/1000 | Loss: 0.00009505
Iteration 218/1000 | Loss: 0.00009505
Iteration 219/1000 | Loss: 0.00009505
Iteration 220/1000 | Loss: 0.00009505
Iteration 221/1000 | Loss: 0.00009505
Iteration 222/1000 | Loss: 0.00009505
Iteration 223/1000 | Loss: 0.00009505
Iteration 224/1000 | Loss: 0.00009505
Iteration 225/1000 | Loss: 0.00009504
Iteration 226/1000 | Loss: 0.00009504
Iteration 227/1000 | Loss: 0.00009504
Iteration 228/1000 | Loss: 0.00009504
Iteration 229/1000 | Loss: 0.00009504
Iteration 230/1000 | Loss: 0.00009503
Iteration 231/1000 | Loss: 0.00009503
Iteration 232/1000 | Loss: 0.00009503
Iteration 233/1000 | Loss: 0.00009503
Iteration 234/1000 | Loss: 0.00009503
Iteration 235/1000 | Loss: 0.00009503
Iteration 236/1000 | Loss: 0.00009503
Iteration 237/1000 | Loss: 0.00009503
Iteration 238/1000 | Loss: 0.00009502
Iteration 239/1000 | Loss: 0.00009502
Iteration 240/1000 | Loss: 0.00009502
Iteration 241/1000 | Loss: 0.00009502
Iteration 242/1000 | Loss: 0.00009502
Iteration 243/1000 | Loss: 0.00009502
Iteration 244/1000 | Loss: 0.00009501
Iteration 245/1000 | Loss: 0.00009501
Iteration 246/1000 | Loss: 0.00009501
Iteration 247/1000 | Loss: 0.00009501
Iteration 248/1000 | Loss: 0.00009501
Iteration 249/1000 | Loss: 0.00009501
Iteration 250/1000 | Loss: 0.00009501
Iteration 251/1000 | Loss: 0.00009501
Iteration 252/1000 | Loss: 0.00009501
Iteration 253/1000 | Loss: 0.00009501
Iteration 254/1000 | Loss: 0.00009501
Iteration 255/1000 | Loss: 0.00009501
Iteration 256/1000 | Loss: 0.00009501
Iteration 257/1000 | Loss: 0.00009501
Iteration 258/1000 | Loss: 0.00009501
Iteration 259/1000 | Loss: 0.00009501
Iteration 260/1000 | Loss: 0.00009501
Iteration 261/1000 | Loss: 0.00009501
Iteration 262/1000 | Loss: 0.00009501
Iteration 263/1000 | Loss: 0.00009501
Iteration 264/1000 | Loss: 0.00009501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [9.501186286797747e-05, 9.501186286797747e-05, 9.501186286797747e-05, 9.501186286797747e-05, 9.501186286797747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.501186286797747e-05

Optimization complete. Final v2v error: 5.217434883117676 mm

Highest mean error: 11.88737678527832 mm for frame 21

Lowest mean error: 3.482825756072998 mm for frame 5

Saving results

Total time: 191.19972562789917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008468
Iteration 2/25 | Loss: 0.00430302
Iteration 3/25 | Loss: 0.00253366
Iteration 4/25 | Loss: 0.00178939
Iteration 5/25 | Loss: 0.00174939
Iteration 6/25 | Loss: 0.00171977
Iteration 7/25 | Loss: 0.00170928
Iteration 8/25 | Loss: 0.00166948
Iteration 9/25 | Loss: 0.00166450
Iteration 10/25 | Loss: 0.00165000
Iteration 11/25 | Loss: 0.00163788
Iteration 12/25 | Loss: 0.00163403
Iteration 13/25 | Loss: 0.00163822
Iteration 14/25 | Loss: 0.00162645
Iteration 15/25 | Loss: 0.00162433
Iteration 16/25 | Loss: 0.00162388
Iteration 17/25 | Loss: 0.00162337
Iteration 18/25 | Loss: 0.00162232
Iteration 19/25 | Loss: 0.00163560
Iteration 20/25 | Loss: 0.00161635
Iteration 21/25 | Loss: 0.00160561
Iteration 22/25 | Loss: 0.00159163
Iteration 23/25 | Loss: 0.00158259
Iteration 24/25 | Loss: 0.00157974
Iteration 25/25 | Loss: 0.00157939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.07109070
Iteration 2/25 | Loss: 0.00480786
Iteration 3/25 | Loss: 0.00478622
Iteration 4/25 | Loss: 0.00478616
Iteration 5/25 | Loss: 0.00478616
Iteration 6/25 | Loss: 0.00478616
Iteration 7/25 | Loss: 0.00478616
Iteration 8/25 | Loss: 0.00478616
Iteration 9/25 | Loss: 0.00478616
Iteration 10/25 | Loss: 0.00478616
Iteration 11/25 | Loss: 0.00478616
Iteration 12/25 | Loss: 0.00478616
Iteration 13/25 | Loss: 0.00478616
Iteration 14/25 | Loss: 0.00478616
Iteration 15/25 | Loss: 0.00478616
Iteration 16/25 | Loss: 0.00478616
Iteration 17/25 | Loss: 0.00478616
Iteration 18/25 | Loss: 0.00478616
Iteration 19/25 | Loss: 0.00478616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0047861551865935326, 0.0047861551865935326, 0.0047861551865935326, 0.0047861551865935326, 0.0047861551865935326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047861551865935326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00478616
Iteration 2/1000 | Loss: 0.00162658
Iteration 3/1000 | Loss: 0.00048828
Iteration 4/1000 | Loss: 0.00106824
Iteration 5/1000 | Loss: 0.00533746
Iteration 6/1000 | Loss: 0.00702827
Iteration 7/1000 | Loss: 0.00229632
Iteration 8/1000 | Loss: 0.00194501
Iteration 9/1000 | Loss: 0.00128697
Iteration 10/1000 | Loss: 0.00020580
Iteration 11/1000 | Loss: 0.00094571
Iteration 12/1000 | Loss: 0.00385908
Iteration 13/1000 | Loss: 0.00083745
Iteration 14/1000 | Loss: 0.00062545
Iteration 15/1000 | Loss: 0.00074970
Iteration 16/1000 | Loss: 0.00037601
Iteration 17/1000 | Loss: 0.00074333
Iteration 18/1000 | Loss: 0.00010586
Iteration 19/1000 | Loss: 0.00008807
Iteration 20/1000 | Loss: 0.00007839
Iteration 21/1000 | Loss: 0.00033381
Iteration 22/1000 | Loss: 0.00049941
Iteration 23/1000 | Loss: 0.00010152
Iteration 24/1000 | Loss: 0.00099186
Iteration 25/1000 | Loss: 0.00079284
Iteration 26/1000 | Loss: 0.00094174
Iteration 27/1000 | Loss: 0.00062759
Iteration 28/1000 | Loss: 0.00030677
Iteration 29/1000 | Loss: 0.00006512
Iteration 30/1000 | Loss: 0.00020673
Iteration 31/1000 | Loss: 0.00005684
Iteration 32/1000 | Loss: 0.00078452
Iteration 33/1000 | Loss: 0.00031455
Iteration 34/1000 | Loss: 0.00043330
Iteration 35/1000 | Loss: 0.00018851
Iteration 36/1000 | Loss: 0.00005924
Iteration 37/1000 | Loss: 0.00005178
Iteration 38/1000 | Loss: 0.00004586
Iteration 39/1000 | Loss: 0.00004350
Iteration 40/1000 | Loss: 0.00004101
Iteration 41/1000 | Loss: 0.00004002
Iteration 42/1000 | Loss: 0.00020917
Iteration 43/1000 | Loss: 0.00067428
Iteration 44/1000 | Loss: 0.00019564
Iteration 45/1000 | Loss: 0.00005157
Iteration 46/1000 | Loss: 0.00004475
Iteration 47/1000 | Loss: 0.00003905
Iteration 48/1000 | Loss: 0.00025522
Iteration 49/1000 | Loss: 0.00003858
Iteration 50/1000 | Loss: 0.00003472
Iteration 51/1000 | Loss: 0.00018751
Iteration 52/1000 | Loss: 0.00018681
Iteration 53/1000 | Loss: 0.00005049
Iteration 54/1000 | Loss: 0.00003942
Iteration 55/1000 | Loss: 0.00008517
Iteration 56/1000 | Loss: 0.00006948
Iteration 57/1000 | Loss: 0.00008753
Iteration 58/1000 | Loss: 0.00008989
Iteration 59/1000 | Loss: 0.00012894
Iteration 60/1000 | Loss: 0.00003546
Iteration 61/1000 | Loss: 0.00003290
Iteration 62/1000 | Loss: 0.00003134
Iteration 63/1000 | Loss: 0.00003024
Iteration 64/1000 | Loss: 0.00002947
Iteration 65/1000 | Loss: 0.00002894
Iteration 66/1000 | Loss: 0.00002846
Iteration 67/1000 | Loss: 0.00068435
Iteration 68/1000 | Loss: 0.00050099
Iteration 69/1000 | Loss: 0.00007103
Iteration 70/1000 | Loss: 0.00003078
Iteration 71/1000 | Loss: 0.00002845
Iteration 72/1000 | Loss: 0.00002800
Iteration 73/1000 | Loss: 0.00086423
Iteration 74/1000 | Loss: 0.00004376
Iteration 75/1000 | Loss: 0.00003092
Iteration 76/1000 | Loss: 0.00002689
Iteration 77/1000 | Loss: 0.00002490
Iteration 78/1000 | Loss: 0.00002331
Iteration 79/1000 | Loss: 0.00002264
Iteration 80/1000 | Loss: 0.00002213
Iteration 81/1000 | Loss: 0.00002179
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002123
Iteration 89/1000 | Loss: 0.00002123
Iteration 90/1000 | Loss: 0.00002122
Iteration 91/1000 | Loss: 0.00002122
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002110
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002108
Iteration 97/1000 | Loss: 0.00002107
Iteration 98/1000 | Loss: 0.00002107
Iteration 99/1000 | Loss: 0.00002107
Iteration 100/1000 | Loss: 0.00002106
Iteration 101/1000 | Loss: 0.00002106
Iteration 102/1000 | Loss: 0.00002106
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00002105
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002103
Iteration 107/1000 | Loss: 0.00002103
Iteration 108/1000 | Loss: 0.00002103
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002102
Iteration 112/1000 | Loss: 0.00002102
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002100
Iteration 118/1000 | Loss: 0.00002100
Iteration 119/1000 | Loss: 0.00002100
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002100
Iteration 122/1000 | Loss: 0.00002099
Iteration 123/1000 | Loss: 0.00002099
Iteration 124/1000 | Loss: 0.00002099
Iteration 125/1000 | Loss: 0.00002099
Iteration 126/1000 | Loss: 0.00002099
Iteration 127/1000 | Loss: 0.00002099
Iteration 128/1000 | Loss: 0.00002099
Iteration 129/1000 | Loss: 0.00002099
Iteration 130/1000 | Loss: 0.00002099
Iteration 131/1000 | Loss: 0.00002098
Iteration 132/1000 | Loss: 0.00002098
Iteration 133/1000 | Loss: 0.00002098
Iteration 134/1000 | Loss: 0.00002098
Iteration 135/1000 | Loss: 0.00002097
Iteration 136/1000 | Loss: 0.00002097
Iteration 137/1000 | Loss: 0.00002097
Iteration 138/1000 | Loss: 0.00002096
Iteration 139/1000 | Loss: 0.00002096
Iteration 140/1000 | Loss: 0.00002096
Iteration 141/1000 | Loss: 0.00002095
Iteration 142/1000 | Loss: 0.00002095
Iteration 143/1000 | Loss: 0.00002095
Iteration 144/1000 | Loss: 0.00002095
Iteration 145/1000 | Loss: 0.00002095
Iteration 146/1000 | Loss: 0.00002095
Iteration 147/1000 | Loss: 0.00002095
Iteration 148/1000 | Loss: 0.00002095
Iteration 149/1000 | Loss: 0.00002095
Iteration 150/1000 | Loss: 0.00002095
Iteration 151/1000 | Loss: 0.00002094
Iteration 152/1000 | Loss: 0.00002094
Iteration 153/1000 | Loss: 0.00002094
Iteration 154/1000 | Loss: 0.00002094
Iteration 155/1000 | Loss: 0.00002094
Iteration 156/1000 | Loss: 0.00002094
Iteration 157/1000 | Loss: 0.00002094
Iteration 158/1000 | Loss: 0.00002094
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002093
Iteration 165/1000 | Loss: 0.00002093
Iteration 166/1000 | Loss: 0.00002093
Iteration 167/1000 | Loss: 0.00002093
Iteration 168/1000 | Loss: 0.00002093
Iteration 169/1000 | Loss: 0.00002092
Iteration 170/1000 | Loss: 0.00002092
Iteration 171/1000 | Loss: 0.00002092
Iteration 172/1000 | Loss: 0.00002092
Iteration 173/1000 | Loss: 0.00002092
Iteration 174/1000 | Loss: 0.00002092
Iteration 175/1000 | Loss: 0.00002092
Iteration 176/1000 | Loss: 0.00002092
Iteration 177/1000 | Loss: 0.00002091
Iteration 178/1000 | Loss: 0.00002091
Iteration 179/1000 | Loss: 0.00002091
Iteration 180/1000 | Loss: 0.00002091
Iteration 181/1000 | Loss: 0.00002091
Iteration 182/1000 | Loss: 0.00002091
Iteration 183/1000 | Loss: 0.00002090
Iteration 184/1000 | Loss: 0.00002090
Iteration 185/1000 | Loss: 0.00002090
Iteration 186/1000 | Loss: 0.00002090
Iteration 187/1000 | Loss: 0.00002089
Iteration 188/1000 | Loss: 0.00002089
Iteration 189/1000 | Loss: 0.00002089
Iteration 190/1000 | Loss: 0.00002088
Iteration 191/1000 | Loss: 0.00002088
Iteration 192/1000 | Loss: 0.00002088
Iteration 193/1000 | Loss: 0.00002088
Iteration 194/1000 | Loss: 0.00002088
Iteration 195/1000 | Loss: 0.00002088
Iteration 196/1000 | Loss: 0.00002088
Iteration 197/1000 | Loss: 0.00002088
Iteration 198/1000 | Loss: 0.00002087
Iteration 199/1000 | Loss: 0.00002087
Iteration 200/1000 | Loss: 0.00002087
Iteration 201/1000 | Loss: 0.00002087
Iteration 202/1000 | Loss: 0.00002087
Iteration 203/1000 | Loss: 0.00002087
Iteration 204/1000 | Loss: 0.00002087
Iteration 205/1000 | Loss: 0.00002087
Iteration 206/1000 | Loss: 0.00002087
Iteration 207/1000 | Loss: 0.00002087
Iteration 208/1000 | Loss: 0.00002087
Iteration 209/1000 | Loss: 0.00002087
Iteration 210/1000 | Loss: 0.00002087
Iteration 211/1000 | Loss: 0.00002086
Iteration 212/1000 | Loss: 0.00002086
Iteration 213/1000 | Loss: 0.00002086
Iteration 214/1000 | Loss: 0.00002086
Iteration 215/1000 | Loss: 0.00002086
Iteration 216/1000 | Loss: 0.00002086
Iteration 217/1000 | Loss: 0.00002086
Iteration 218/1000 | Loss: 0.00002085
Iteration 219/1000 | Loss: 0.00002085
Iteration 220/1000 | Loss: 0.00002085
Iteration 221/1000 | Loss: 0.00002085
Iteration 222/1000 | Loss: 0.00002085
Iteration 223/1000 | Loss: 0.00002085
Iteration 224/1000 | Loss: 0.00002085
Iteration 225/1000 | Loss: 0.00002085
Iteration 226/1000 | Loss: 0.00002085
Iteration 227/1000 | Loss: 0.00002085
Iteration 228/1000 | Loss: 0.00002084
Iteration 229/1000 | Loss: 0.00002084
Iteration 230/1000 | Loss: 0.00002084
Iteration 231/1000 | Loss: 0.00002084
Iteration 232/1000 | Loss: 0.00002084
Iteration 233/1000 | Loss: 0.00002084
Iteration 234/1000 | Loss: 0.00002084
Iteration 235/1000 | Loss: 0.00002084
Iteration 236/1000 | Loss: 0.00002084
Iteration 237/1000 | Loss: 0.00002084
Iteration 238/1000 | Loss: 0.00002084
Iteration 239/1000 | Loss: 0.00002084
Iteration 240/1000 | Loss: 0.00002084
Iteration 241/1000 | Loss: 0.00002084
Iteration 242/1000 | Loss: 0.00002084
Iteration 243/1000 | Loss: 0.00002084
Iteration 244/1000 | Loss: 0.00002084
Iteration 245/1000 | Loss: 0.00002084
Iteration 246/1000 | Loss: 0.00002084
Iteration 247/1000 | Loss: 0.00002084
Iteration 248/1000 | Loss: 0.00002084
Iteration 249/1000 | Loss: 0.00002084
Iteration 250/1000 | Loss: 0.00002084
Iteration 251/1000 | Loss: 0.00002084
Iteration 252/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [2.0840756405959837e-05, 2.0840756405959837e-05, 2.0840756405959837e-05, 2.0840756405959837e-05, 2.0840756405959837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0840756405959837e-05

Optimization complete. Final v2v error: 3.640439033508301 mm

Highest mean error: 5.41117000579834 mm for frame 77

Lowest mean error: 2.9369747638702393 mm for frame 140

Saving results

Total time: 175.7897253036499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004206
Iteration 2/25 | Loss: 0.00202888
Iteration 3/25 | Loss: 0.00159648
Iteration 4/25 | Loss: 0.00152852
Iteration 5/25 | Loss: 0.00152234
Iteration 6/25 | Loss: 0.00152999
Iteration 7/25 | Loss: 0.00146504
Iteration 8/25 | Loss: 0.00137987
Iteration 9/25 | Loss: 0.00143097
Iteration 10/25 | Loss: 0.00137517
Iteration 11/25 | Loss: 0.00135333
Iteration 12/25 | Loss: 0.00134659
Iteration 13/25 | Loss: 0.00133681
Iteration 14/25 | Loss: 0.00132794
Iteration 15/25 | Loss: 0.00133689
Iteration 16/25 | Loss: 0.00131977
Iteration 17/25 | Loss: 0.00132357
Iteration 18/25 | Loss: 0.00131078
Iteration 19/25 | Loss: 0.00130336
Iteration 20/25 | Loss: 0.00129421
Iteration 21/25 | Loss: 0.00131113
Iteration 22/25 | Loss: 0.00129474
Iteration 23/25 | Loss: 0.00128730
Iteration 24/25 | Loss: 0.00129329
Iteration 25/25 | Loss: 0.00128765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56234527
Iteration 2/25 | Loss: 0.00105143
Iteration 3/25 | Loss: 0.00103939
Iteration 4/25 | Loss: 0.00103939
Iteration 5/25 | Loss: 0.00103939
Iteration 6/25 | Loss: 0.00103939
Iteration 7/25 | Loss: 0.00103939
Iteration 8/25 | Loss: 0.00103939
Iteration 9/25 | Loss: 0.00103939
Iteration 10/25 | Loss: 0.00103939
Iteration 11/25 | Loss: 0.00103939
Iteration 12/25 | Loss: 0.00103939
Iteration 13/25 | Loss: 0.00103939
Iteration 14/25 | Loss: 0.00103939
Iteration 15/25 | Loss: 0.00103939
Iteration 16/25 | Loss: 0.00103939
Iteration 17/25 | Loss: 0.00103939
Iteration 18/25 | Loss: 0.00103939
Iteration 19/25 | Loss: 0.00103938
Iteration 20/25 | Loss: 0.00103938
Iteration 21/25 | Loss: 0.00103938
Iteration 22/25 | Loss: 0.00103938
Iteration 23/25 | Loss: 0.00103938
Iteration 24/25 | Loss: 0.00103938
Iteration 25/25 | Loss: 0.00103938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103938
Iteration 2/1000 | Loss: 0.00067832
Iteration 3/1000 | Loss: 0.00029162
Iteration 4/1000 | Loss: 0.00039311
Iteration 5/1000 | Loss: 0.00046763
Iteration 6/1000 | Loss: 0.00026181
Iteration 7/1000 | Loss: 0.00047226
Iteration 8/1000 | Loss: 0.00072968
Iteration 9/1000 | Loss: 0.00052191
Iteration 10/1000 | Loss: 0.00048374
Iteration 11/1000 | Loss: 0.00036558
Iteration 12/1000 | Loss: 0.00062999
Iteration 13/1000 | Loss: 0.00074211
Iteration 14/1000 | Loss: 0.00044875
Iteration 15/1000 | Loss: 0.00071468
Iteration 16/1000 | Loss: 0.00031758
Iteration 17/1000 | Loss: 0.00022651
Iteration 18/1000 | Loss: 0.00016665
Iteration 19/1000 | Loss: 0.00020400
Iteration 20/1000 | Loss: 0.00022224
Iteration 21/1000 | Loss: 0.00015482
Iteration 22/1000 | Loss: 0.00027930
Iteration 23/1000 | Loss: 0.00034449
Iteration 24/1000 | Loss: 0.00026962
Iteration 25/1000 | Loss: 0.00033915
Iteration 26/1000 | Loss: 0.00020145
Iteration 27/1000 | Loss: 0.00019788
Iteration 28/1000 | Loss: 0.00035821
Iteration 29/1000 | Loss: 0.00042107
Iteration 30/1000 | Loss: 0.00022116
Iteration 31/1000 | Loss: 0.00035092
Iteration 32/1000 | Loss: 0.00005918
Iteration 33/1000 | Loss: 0.00024282
Iteration 34/1000 | Loss: 0.00004852
Iteration 35/1000 | Loss: 0.00011528
Iteration 36/1000 | Loss: 0.00016590
Iteration 37/1000 | Loss: 0.00014891
Iteration 38/1000 | Loss: 0.00032668
Iteration 39/1000 | Loss: 0.00035567
Iteration 40/1000 | Loss: 0.00025372
Iteration 41/1000 | Loss: 0.00024473
Iteration 42/1000 | Loss: 0.00015614
Iteration 43/1000 | Loss: 0.00021420
Iteration 44/1000 | Loss: 0.00022792
Iteration 45/1000 | Loss: 0.00021367
Iteration 46/1000 | Loss: 0.00036835
Iteration 47/1000 | Loss: 0.00111670
Iteration 48/1000 | Loss: 0.00097167
Iteration 49/1000 | Loss: 0.00114035
Iteration 50/1000 | Loss: 0.00056735
Iteration 51/1000 | Loss: 0.00024643
Iteration 52/1000 | Loss: 0.00046328
Iteration 53/1000 | Loss: 0.00045945
Iteration 54/1000 | Loss: 0.00040957
Iteration 55/1000 | Loss: 0.00020796
Iteration 56/1000 | Loss: 0.00005807
Iteration 57/1000 | Loss: 0.00027469
Iteration 58/1000 | Loss: 0.00037005
Iteration 59/1000 | Loss: 0.00028675
Iteration 60/1000 | Loss: 0.00006461
Iteration 61/1000 | Loss: 0.00004721
Iteration 62/1000 | Loss: 0.00022395
Iteration 63/1000 | Loss: 0.00030485
Iteration 64/1000 | Loss: 0.00021483
Iteration 65/1000 | Loss: 0.00032318
Iteration 66/1000 | Loss: 0.00006728
Iteration 67/1000 | Loss: 0.00099306
Iteration 68/1000 | Loss: 0.00025257
Iteration 69/1000 | Loss: 0.00020843
Iteration 70/1000 | Loss: 0.00032743
Iteration 71/1000 | Loss: 0.00005366
Iteration 72/1000 | Loss: 0.00004163
Iteration 73/1000 | Loss: 0.00004956
Iteration 74/1000 | Loss: 0.00003920
Iteration 75/1000 | Loss: 0.00004862
Iteration 76/1000 | Loss: 0.00004957
Iteration 77/1000 | Loss: 0.00004330
Iteration 78/1000 | Loss: 0.00007005
Iteration 79/1000 | Loss: 0.00004548
Iteration 80/1000 | Loss: 0.00004700
Iteration 81/1000 | Loss: 0.00005640
Iteration 82/1000 | Loss: 0.00017718
Iteration 83/1000 | Loss: 0.00042284
Iteration 84/1000 | Loss: 0.00030306
Iteration 85/1000 | Loss: 0.00011020
Iteration 86/1000 | Loss: 0.00011475
Iteration 87/1000 | Loss: 0.00026782
Iteration 88/1000 | Loss: 0.00029839
Iteration 89/1000 | Loss: 0.00005840
Iteration 90/1000 | Loss: 0.00020001
Iteration 91/1000 | Loss: 0.00003303
Iteration 92/1000 | Loss: 0.00002541
Iteration 93/1000 | Loss: 0.00002295
Iteration 94/1000 | Loss: 0.00018673
Iteration 95/1000 | Loss: 0.00025761
Iteration 96/1000 | Loss: 0.00006992
Iteration 97/1000 | Loss: 0.00014515
Iteration 98/1000 | Loss: 0.00007212
Iteration 99/1000 | Loss: 0.00003105
Iteration 100/1000 | Loss: 0.00002684
Iteration 101/1000 | Loss: 0.00012956
Iteration 102/1000 | Loss: 0.00012871
Iteration 103/1000 | Loss: 0.00003250
Iteration 104/1000 | Loss: 0.00007705
Iteration 105/1000 | Loss: 0.00028311
Iteration 106/1000 | Loss: 0.00021261
Iteration 107/1000 | Loss: 0.00027498
Iteration 108/1000 | Loss: 0.00014985
Iteration 109/1000 | Loss: 0.00021502
Iteration 110/1000 | Loss: 0.00016242
Iteration 111/1000 | Loss: 0.00014843
Iteration 112/1000 | Loss: 0.00007706
Iteration 113/1000 | Loss: 0.00002425
Iteration 114/1000 | Loss: 0.00009268
Iteration 115/1000 | Loss: 0.00005796
Iteration 116/1000 | Loss: 0.00022417
Iteration 117/1000 | Loss: 0.00016620
Iteration 118/1000 | Loss: 0.00027776
Iteration 119/1000 | Loss: 0.00021130
Iteration 120/1000 | Loss: 0.00014271
Iteration 121/1000 | Loss: 0.00011509
Iteration 122/1000 | Loss: 0.00002257
Iteration 123/1000 | Loss: 0.00002113
Iteration 124/1000 | Loss: 0.00002048
Iteration 125/1000 | Loss: 0.00002015
Iteration 126/1000 | Loss: 0.00014781
Iteration 127/1000 | Loss: 0.00005290
Iteration 128/1000 | Loss: 0.00010863
Iteration 129/1000 | Loss: 0.00009273
Iteration 130/1000 | Loss: 0.00003704
Iteration 131/1000 | Loss: 0.00016026
Iteration 132/1000 | Loss: 0.00013271
Iteration 133/1000 | Loss: 0.00006850
Iteration 134/1000 | Loss: 0.00017098
Iteration 135/1000 | Loss: 0.00008217
Iteration 136/1000 | Loss: 0.00002365
Iteration 137/1000 | Loss: 0.00002182
Iteration 138/1000 | Loss: 0.00009931
Iteration 139/1000 | Loss: 0.00009876
Iteration 140/1000 | Loss: 0.00002475
Iteration 141/1000 | Loss: 0.00002248
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002063
Iteration 144/1000 | Loss: 0.00025072
Iteration 145/1000 | Loss: 0.00015524
Iteration 146/1000 | Loss: 0.00002679
Iteration 147/1000 | Loss: 0.00002158
Iteration 148/1000 | Loss: 0.00001987
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001863
Iteration 151/1000 | Loss: 0.00001816
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001749
Iteration 155/1000 | Loss: 0.00001747
Iteration 156/1000 | Loss: 0.00001742
Iteration 157/1000 | Loss: 0.00001741
Iteration 158/1000 | Loss: 0.00001738
Iteration 159/1000 | Loss: 0.00001738
Iteration 160/1000 | Loss: 0.00001737
Iteration 161/1000 | Loss: 0.00001737
Iteration 162/1000 | Loss: 0.00001736
Iteration 163/1000 | Loss: 0.00001736
Iteration 164/1000 | Loss: 0.00001735
Iteration 165/1000 | Loss: 0.00001735
Iteration 166/1000 | Loss: 0.00001734
Iteration 167/1000 | Loss: 0.00001734
Iteration 168/1000 | Loss: 0.00001734
Iteration 169/1000 | Loss: 0.00001733
Iteration 170/1000 | Loss: 0.00001733
Iteration 171/1000 | Loss: 0.00001733
Iteration 172/1000 | Loss: 0.00001732
Iteration 173/1000 | Loss: 0.00001732
Iteration 174/1000 | Loss: 0.00001732
Iteration 175/1000 | Loss: 0.00001732
Iteration 176/1000 | Loss: 0.00001731
Iteration 177/1000 | Loss: 0.00001731
Iteration 178/1000 | Loss: 0.00001731
Iteration 179/1000 | Loss: 0.00001731
Iteration 180/1000 | Loss: 0.00001731
Iteration 181/1000 | Loss: 0.00001730
Iteration 182/1000 | Loss: 0.00001730
Iteration 183/1000 | Loss: 0.00001730
Iteration 184/1000 | Loss: 0.00001729
Iteration 185/1000 | Loss: 0.00001729
Iteration 186/1000 | Loss: 0.00001729
Iteration 187/1000 | Loss: 0.00001728
Iteration 188/1000 | Loss: 0.00001728
Iteration 189/1000 | Loss: 0.00001728
Iteration 190/1000 | Loss: 0.00001728
Iteration 191/1000 | Loss: 0.00001728
Iteration 192/1000 | Loss: 0.00001727
Iteration 193/1000 | Loss: 0.00001727
Iteration 194/1000 | Loss: 0.00001727
Iteration 195/1000 | Loss: 0.00001727
Iteration 196/1000 | Loss: 0.00001727
Iteration 197/1000 | Loss: 0.00001726
Iteration 198/1000 | Loss: 0.00001726
Iteration 199/1000 | Loss: 0.00001726
Iteration 200/1000 | Loss: 0.00001726
Iteration 201/1000 | Loss: 0.00001726
Iteration 202/1000 | Loss: 0.00001726
Iteration 203/1000 | Loss: 0.00001726
Iteration 204/1000 | Loss: 0.00001726
Iteration 205/1000 | Loss: 0.00001726
Iteration 206/1000 | Loss: 0.00001726
Iteration 207/1000 | Loss: 0.00001726
Iteration 208/1000 | Loss: 0.00001726
Iteration 209/1000 | Loss: 0.00001726
Iteration 210/1000 | Loss: 0.00001726
Iteration 211/1000 | Loss: 0.00001726
Iteration 212/1000 | Loss: 0.00001726
Iteration 213/1000 | Loss: 0.00001726
Iteration 214/1000 | Loss: 0.00001726
Iteration 215/1000 | Loss: 0.00001726
Iteration 216/1000 | Loss: 0.00001726
Iteration 217/1000 | Loss: 0.00001726
Iteration 218/1000 | Loss: 0.00001726
Iteration 219/1000 | Loss: 0.00001726
Iteration 220/1000 | Loss: 0.00001726
Iteration 221/1000 | Loss: 0.00001726
Iteration 222/1000 | Loss: 0.00001726
Iteration 223/1000 | Loss: 0.00001726
Iteration 224/1000 | Loss: 0.00001726
Iteration 225/1000 | Loss: 0.00001726
Iteration 226/1000 | Loss: 0.00001726
Iteration 227/1000 | Loss: 0.00001726
Iteration 228/1000 | Loss: 0.00001726
Iteration 229/1000 | Loss: 0.00001726
Iteration 230/1000 | Loss: 0.00001726
Iteration 231/1000 | Loss: 0.00001726
Iteration 232/1000 | Loss: 0.00001726
Iteration 233/1000 | Loss: 0.00001726
Iteration 234/1000 | Loss: 0.00001726
Iteration 235/1000 | Loss: 0.00001726
Iteration 236/1000 | Loss: 0.00001726
Iteration 237/1000 | Loss: 0.00001726
Iteration 238/1000 | Loss: 0.00001726
Iteration 239/1000 | Loss: 0.00001726
Iteration 240/1000 | Loss: 0.00001726
Iteration 241/1000 | Loss: 0.00001726
Iteration 242/1000 | Loss: 0.00001726
Iteration 243/1000 | Loss: 0.00001726
Iteration 244/1000 | Loss: 0.00001726
Iteration 245/1000 | Loss: 0.00001726
Iteration 246/1000 | Loss: 0.00001726
Iteration 247/1000 | Loss: 0.00001726
Iteration 248/1000 | Loss: 0.00001726
Iteration 249/1000 | Loss: 0.00001726
Iteration 250/1000 | Loss: 0.00001726
Iteration 251/1000 | Loss: 0.00001726
Iteration 252/1000 | Loss: 0.00001726
Iteration 253/1000 | Loss: 0.00001726
Iteration 254/1000 | Loss: 0.00001726
Iteration 255/1000 | Loss: 0.00001726
Iteration 256/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.7255291822948493e-05, 1.7255291822948493e-05, 1.7255291822948493e-05, 1.7255291822948493e-05, 1.7255291822948493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7255291822948493e-05

Optimization complete. Final v2v error: 3.481121063232422 mm

Highest mean error: 4.54525089263916 mm for frame 68

Lowest mean error: 3.0632433891296387 mm for frame 25

Saving results

Total time: 263.2336311340332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843129
Iteration 2/25 | Loss: 0.00190239
Iteration 3/25 | Loss: 0.00157061
Iteration 4/25 | Loss: 0.00153145
Iteration 5/25 | Loss: 0.00152139
Iteration 6/25 | Loss: 0.00152415
Iteration 7/25 | Loss: 0.00152075
Iteration 8/25 | Loss: 0.00152430
Iteration 9/25 | Loss: 0.00151954
Iteration 10/25 | Loss: 0.00152008
Iteration 11/25 | Loss: 0.00151230
Iteration 12/25 | Loss: 0.00150382
Iteration 13/25 | Loss: 0.00149805
Iteration 14/25 | Loss: 0.00149690
Iteration 15/25 | Loss: 0.00149970
Iteration 16/25 | Loss: 0.00151126
Iteration 17/25 | Loss: 0.00149766
Iteration 18/25 | Loss: 0.00149243
Iteration 19/25 | Loss: 0.00148572
Iteration 20/25 | Loss: 0.00148260
Iteration 21/25 | Loss: 0.00148181
Iteration 22/25 | Loss: 0.00148146
Iteration 23/25 | Loss: 0.00148136
Iteration 24/25 | Loss: 0.00148135
Iteration 25/25 | Loss: 0.00148135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38379467
Iteration 2/25 | Loss: 0.00195683
Iteration 3/25 | Loss: 0.00195677
Iteration 4/25 | Loss: 0.00195676
Iteration 5/25 | Loss: 0.00195676
Iteration 6/25 | Loss: 0.00195676
Iteration 7/25 | Loss: 0.00195676
Iteration 8/25 | Loss: 0.00195676
Iteration 9/25 | Loss: 0.00195676
Iteration 10/25 | Loss: 0.00195676
Iteration 11/25 | Loss: 0.00195676
Iteration 12/25 | Loss: 0.00195676
Iteration 13/25 | Loss: 0.00195676
Iteration 14/25 | Loss: 0.00195676
Iteration 15/25 | Loss: 0.00195676
Iteration 16/25 | Loss: 0.00195676
Iteration 17/25 | Loss: 0.00195676
Iteration 18/25 | Loss: 0.00195676
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019567597191780806, 0.0019567597191780806, 0.0019567597191780806, 0.0019567597191780806, 0.0019567597191780806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019567597191780806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195676
Iteration 2/1000 | Loss: 0.00034410
Iteration 3/1000 | Loss: 0.00024203
Iteration 4/1000 | Loss: 0.00020449
Iteration 5/1000 | Loss: 0.00017693
Iteration 6/1000 | Loss: 0.00079748
Iteration 7/1000 | Loss: 0.00015710
Iteration 8/1000 | Loss: 0.00068788
Iteration 9/1000 | Loss: 0.00016269
Iteration 10/1000 | Loss: 0.00014127
Iteration 11/1000 | Loss: 0.00013364
Iteration 12/1000 | Loss: 0.00012926
Iteration 13/1000 | Loss: 0.00012593
Iteration 14/1000 | Loss: 0.00092870
Iteration 15/1000 | Loss: 0.00056539
Iteration 16/1000 | Loss: 0.00059708
Iteration 17/1000 | Loss: 0.00012585
Iteration 18/1000 | Loss: 0.00012077
Iteration 19/1000 | Loss: 0.00011798
Iteration 20/1000 | Loss: 0.00011550
Iteration 21/1000 | Loss: 0.00011363
Iteration 22/1000 | Loss: 0.00011218
Iteration 23/1000 | Loss: 0.00011111
Iteration 24/1000 | Loss: 0.00011017
Iteration 25/1000 | Loss: 0.00010920
Iteration 26/1000 | Loss: 0.00010843
Iteration 27/1000 | Loss: 0.00010774
Iteration 28/1000 | Loss: 0.00010722
Iteration 29/1000 | Loss: 0.00010680
Iteration 30/1000 | Loss: 0.00010626
Iteration 31/1000 | Loss: 0.00010617
Iteration 32/1000 | Loss: 0.00010585
Iteration 33/1000 | Loss: 0.00188043
Iteration 34/1000 | Loss: 0.00877223
Iteration 35/1000 | Loss: 0.00403060
Iteration 36/1000 | Loss: 0.00120431
Iteration 37/1000 | Loss: 0.00016364
Iteration 38/1000 | Loss: 0.00011335
Iteration 39/1000 | Loss: 0.00009696
Iteration 40/1000 | Loss: 0.00008687
Iteration 41/1000 | Loss: 0.00034075
Iteration 42/1000 | Loss: 0.00091068
Iteration 43/1000 | Loss: 0.00009873
Iteration 44/1000 | Loss: 0.00007925
Iteration 45/1000 | Loss: 0.00007212
Iteration 46/1000 | Loss: 0.00006925
Iteration 47/1000 | Loss: 0.00006643
Iteration 48/1000 | Loss: 0.00068736
Iteration 49/1000 | Loss: 0.00006970
Iteration 50/1000 | Loss: 0.00006491
Iteration 51/1000 | Loss: 0.00006285
Iteration 52/1000 | Loss: 0.00006147
Iteration 53/1000 | Loss: 0.00092166
Iteration 54/1000 | Loss: 0.00008789
Iteration 55/1000 | Loss: 0.00006929
Iteration 56/1000 | Loss: 0.00006016
Iteration 57/1000 | Loss: 0.00005822
Iteration 58/1000 | Loss: 0.00005731
Iteration 59/1000 | Loss: 0.00071376
Iteration 60/1000 | Loss: 0.00166640
Iteration 61/1000 | Loss: 0.00086193
Iteration 62/1000 | Loss: 0.00053015
Iteration 63/1000 | Loss: 0.00009924
Iteration 64/1000 | Loss: 0.00007548
Iteration 65/1000 | Loss: 0.00006174
Iteration 66/1000 | Loss: 0.00005584
Iteration 67/1000 | Loss: 0.00005398
Iteration 68/1000 | Loss: 0.00005203
Iteration 69/1000 | Loss: 0.00005051
Iteration 70/1000 | Loss: 0.00004969
Iteration 71/1000 | Loss: 0.00027375
Iteration 72/1000 | Loss: 0.00005882
Iteration 73/1000 | Loss: 0.00022802
Iteration 74/1000 | Loss: 0.00023743
Iteration 75/1000 | Loss: 0.00019142
Iteration 76/1000 | Loss: 0.00007454
Iteration 77/1000 | Loss: 0.00012199
Iteration 78/1000 | Loss: 0.00014990
Iteration 79/1000 | Loss: 0.00014306
Iteration 80/1000 | Loss: 0.00005065
Iteration 81/1000 | Loss: 0.00004906
Iteration 82/1000 | Loss: 0.00004833
Iteration 83/1000 | Loss: 0.00079239
Iteration 84/1000 | Loss: 0.00068932
Iteration 85/1000 | Loss: 0.00006618
Iteration 86/1000 | Loss: 0.00005077
Iteration 87/1000 | Loss: 0.00004836
Iteration 88/1000 | Loss: 0.00072906
Iteration 89/1000 | Loss: 0.00022304
Iteration 90/1000 | Loss: 0.00006709
Iteration 91/1000 | Loss: 0.00005789
Iteration 92/1000 | Loss: 0.00004923
Iteration 93/1000 | Loss: 0.00004706
Iteration 94/1000 | Loss: 0.00004613
Iteration 95/1000 | Loss: 0.00004549
Iteration 96/1000 | Loss: 0.00004500
Iteration 97/1000 | Loss: 0.00004465
Iteration 98/1000 | Loss: 0.00004427
Iteration 99/1000 | Loss: 0.00004401
Iteration 100/1000 | Loss: 0.00004384
Iteration 101/1000 | Loss: 0.00004381
Iteration 102/1000 | Loss: 0.00004378
Iteration 103/1000 | Loss: 0.00004378
Iteration 104/1000 | Loss: 0.00004376
Iteration 105/1000 | Loss: 0.00004374
Iteration 106/1000 | Loss: 0.00004374
Iteration 107/1000 | Loss: 0.00004373
Iteration 108/1000 | Loss: 0.00004370
Iteration 109/1000 | Loss: 0.00004369
Iteration 110/1000 | Loss: 0.00004368
Iteration 111/1000 | Loss: 0.00004367
Iteration 112/1000 | Loss: 0.00004367
Iteration 113/1000 | Loss: 0.00004366
Iteration 114/1000 | Loss: 0.00004363
Iteration 115/1000 | Loss: 0.00004363
Iteration 116/1000 | Loss: 0.00004362
Iteration 117/1000 | Loss: 0.00004361
Iteration 118/1000 | Loss: 0.00004361
Iteration 119/1000 | Loss: 0.00004359
Iteration 120/1000 | Loss: 0.00004358
Iteration 121/1000 | Loss: 0.00004358
Iteration 122/1000 | Loss: 0.00004357
Iteration 123/1000 | Loss: 0.00004357
Iteration 124/1000 | Loss: 0.00004357
Iteration 125/1000 | Loss: 0.00004357
Iteration 126/1000 | Loss: 0.00004356
Iteration 127/1000 | Loss: 0.00004356
Iteration 128/1000 | Loss: 0.00004356
Iteration 129/1000 | Loss: 0.00004356
Iteration 130/1000 | Loss: 0.00004356
Iteration 131/1000 | Loss: 0.00004356
Iteration 132/1000 | Loss: 0.00004355
Iteration 133/1000 | Loss: 0.00004355
Iteration 134/1000 | Loss: 0.00004354
Iteration 135/1000 | Loss: 0.00004353
Iteration 136/1000 | Loss: 0.00004353
Iteration 137/1000 | Loss: 0.00004353
Iteration 138/1000 | Loss: 0.00004353
Iteration 139/1000 | Loss: 0.00004353
Iteration 140/1000 | Loss: 0.00004353
Iteration 141/1000 | Loss: 0.00004352
Iteration 142/1000 | Loss: 0.00004352
Iteration 143/1000 | Loss: 0.00004352
Iteration 144/1000 | Loss: 0.00004352
Iteration 145/1000 | Loss: 0.00004352
Iteration 146/1000 | Loss: 0.00004352
Iteration 147/1000 | Loss: 0.00004352
Iteration 148/1000 | Loss: 0.00004352
Iteration 149/1000 | Loss: 0.00004352
Iteration 150/1000 | Loss: 0.00004351
Iteration 151/1000 | Loss: 0.00004350
Iteration 152/1000 | Loss: 0.00004349
Iteration 153/1000 | Loss: 0.00004349
Iteration 154/1000 | Loss: 0.00004348
Iteration 155/1000 | Loss: 0.00004347
Iteration 156/1000 | Loss: 0.00004346
Iteration 157/1000 | Loss: 0.00004346
Iteration 158/1000 | Loss: 0.00004346
Iteration 159/1000 | Loss: 0.00004346
Iteration 160/1000 | Loss: 0.00004345
Iteration 161/1000 | Loss: 0.00004345
Iteration 162/1000 | Loss: 0.00004345
Iteration 163/1000 | Loss: 0.00004345
Iteration 164/1000 | Loss: 0.00004345
Iteration 165/1000 | Loss: 0.00004344
Iteration 166/1000 | Loss: 0.00004344
Iteration 167/1000 | Loss: 0.00004344
Iteration 168/1000 | Loss: 0.00004343
Iteration 169/1000 | Loss: 0.00004342
Iteration 170/1000 | Loss: 0.00004342
Iteration 171/1000 | Loss: 0.00004342
Iteration 172/1000 | Loss: 0.00004342
Iteration 173/1000 | Loss: 0.00004341
Iteration 174/1000 | Loss: 0.00004341
Iteration 175/1000 | Loss: 0.00004341
Iteration 176/1000 | Loss: 0.00004341
Iteration 177/1000 | Loss: 0.00004341
Iteration 178/1000 | Loss: 0.00004340
Iteration 179/1000 | Loss: 0.00004340
Iteration 180/1000 | Loss: 0.00004340
Iteration 181/1000 | Loss: 0.00004339
Iteration 182/1000 | Loss: 0.00004339
Iteration 183/1000 | Loss: 0.00004339
Iteration 184/1000 | Loss: 0.00004339
Iteration 185/1000 | Loss: 0.00004339
Iteration 186/1000 | Loss: 0.00004338
Iteration 187/1000 | Loss: 0.00004338
Iteration 188/1000 | Loss: 0.00004338
Iteration 189/1000 | Loss: 0.00004338
Iteration 190/1000 | Loss: 0.00004338
Iteration 191/1000 | Loss: 0.00004338
Iteration 192/1000 | Loss: 0.00004338
Iteration 193/1000 | Loss: 0.00004338
Iteration 194/1000 | Loss: 0.00004338
Iteration 195/1000 | Loss: 0.00004338
Iteration 196/1000 | Loss: 0.00083807
Iteration 197/1000 | Loss: 0.00004758
Iteration 198/1000 | Loss: 0.00004462
Iteration 199/1000 | Loss: 0.00004312
Iteration 200/1000 | Loss: 0.00004251
Iteration 201/1000 | Loss: 0.00004185
Iteration 202/1000 | Loss: 0.00004120
Iteration 203/1000 | Loss: 0.00096405
Iteration 204/1000 | Loss: 0.00063100
Iteration 205/1000 | Loss: 0.00004118
Iteration 206/1000 | Loss: 0.00074419
Iteration 207/1000 | Loss: 0.00071041
Iteration 208/1000 | Loss: 0.00272816
Iteration 209/1000 | Loss: 0.00135282
Iteration 210/1000 | Loss: 0.00077894
Iteration 211/1000 | Loss: 0.00005571
Iteration 212/1000 | Loss: 0.00055176
Iteration 213/1000 | Loss: 0.00006215
Iteration 214/1000 | Loss: 0.00004976
Iteration 215/1000 | Loss: 0.00003992
Iteration 216/1000 | Loss: 0.00003811
Iteration 217/1000 | Loss: 0.00003631
Iteration 218/1000 | Loss: 0.00003482
Iteration 219/1000 | Loss: 0.00003395
Iteration 220/1000 | Loss: 0.00003337
Iteration 221/1000 | Loss: 0.00003280
Iteration 222/1000 | Loss: 0.00003249
Iteration 223/1000 | Loss: 0.00003226
Iteration 224/1000 | Loss: 0.00003217
Iteration 225/1000 | Loss: 0.00003216
Iteration 226/1000 | Loss: 0.00003208
Iteration 227/1000 | Loss: 0.00003207
Iteration 228/1000 | Loss: 0.00003207
Iteration 229/1000 | Loss: 0.00003207
Iteration 230/1000 | Loss: 0.00003206
Iteration 231/1000 | Loss: 0.00003206
Iteration 232/1000 | Loss: 0.00003205
Iteration 233/1000 | Loss: 0.00003202
Iteration 234/1000 | Loss: 0.00003201
Iteration 235/1000 | Loss: 0.00003201
Iteration 236/1000 | Loss: 0.00003201
Iteration 237/1000 | Loss: 0.00003200
Iteration 238/1000 | Loss: 0.00003200
Iteration 239/1000 | Loss: 0.00003194
Iteration 240/1000 | Loss: 0.00003193
Iteration 241/1000 | Loss: 0.00003192
Iteration 242/1000 | Loss: 0.00003189
Iteration 243/1000 | Loss: 0.00003188
Iteration 244/1000 | Loss: 0.00003188
Iteration 245/1000 | Loss: 0.00003188
Iteration 246/1000 | Loss: 0.00003187
Iteration 247/1000 | Loss: 0.00003187
Iteration 248/1000 | Loss: 0.00003185
Iteration 249/1000 | Loss: 0.00003185
Iteration 250/1000 | Loss: 0.00003185
Iteration 251/1000 | Loss: 0.00003184
Iteration 252/1000 | Loss: 0.00003184
Iteration 253/1000 | Loss: 0.00003184
Iteration 254/1000 | Loss: 0.00003184
Iteration 255/1000 | Loss: 0.00003184
Iteration 256/1000 | Loss: 0.00003184
Iteration 257/1000 | Loss: 0.00003184
Iteration 258/1000 | Loss: 0.00003184
Iteration 259/1000 | Loss: 0.00003183
Iteration 260/1000 | Loss: 0.00003183
Iteration 261/1000 | Loss: 0.00003183
Iteration 262/1000 | Loss: 0.00003183
Iteration 263/1000 | Loss: 0.00003183
Iteration 264/1000 | Loss: 0.00003182
Iteration 265/1000 | Loss: 0.00003181
Iteration 266/1000 | Loss: 0.00003181
Iteration 267/1000 | Loss: 0.00003180
Iteration 268/1000 | Loss: 0.00003180
Iteration 269/1000 | Loss: 0.00003180
Iteration 270/1000 | Loss: 0.00003179
Iteration 271/1000 | Loss: 0.00003179
Iteration 272/1000 | Loss: 0.00003179
Iteration 273/1000 | Loss: 0.00003179
Iteration 274/1000 | Loss: 0.00003179
Iteration 275/1000 | Loss: 0.00003178
Iteration 276/1000 | Loss: 0.00003177
Iteration 277/1000 | Loss: 0.00003177
Iteration 278/1000 | Loss: 0.00003177
Iteration 279/1000 | Loss: 0.00003177
Iteration 280/1000 | Loss: 0.00003177
Iteration 281/1000 | Loss: 0.00003176
Iteration 282/1000 | Loss: 0.00003176
Iteration 283/1000 | Loss: 0.00003176
Iteration 284/1000 | Loss: 0.00003176
Iteration 285/1000 | Loss: 0.00003175
Iteration 286/1000 | Loss: 0.00003175
Iteration 287/1000 | Loss: 0.00003175
Iteration 288/1000 | Loss: 0.00003175
Iteration 289/1000 | Loss: 0.00003175
Iteration 290/1000 | Loss: 0.00003175
Iteration 291/1000 | Loss: 0.00003175
Iteration 292/1000 | Loss: 0.00003175
Iteration 293/1000 | Loss: 0.00003175
Iteration 294/1000 | Loss: 0.00003175
Iteration 295/1000 | Loss: 0.00003175
Iteration 296/1000 | Loss: 0.00003175
Iteration 297/1000 | Loss: 0.00003175
Iteration 298/1000 | Loss: 0.00003174
Iteration 299/1000 | Loss: 0.00003174
Iteration 300/1000 | Loss: 0.00003174
Iteration 301/1000 | Loss: 0.00003174
Iteration 302/1000 | Loss: 0.00003174
Iteration 303/1000 | Loss: 0.00003174
Iteration 304/1000 | Loss: 0.00003174
Iteration 305/1000 | Loss: 0.00003174
Iteration 306/1000 | Loss: 0.00003174
Iteration 307/1000 | Loss: 0.00003174
Iteration 308/1000 | Loss: 0.00003174
Iteration 309/1000 | Loss: 0.00003174
Iteration 310/1000 | Loss: 0.00003174
Iteration 311/1000 | Loss: 0.00003173
Iteration 312/1000 | Loss: 0.00003173
Iteration 313/1000 | Loss: 0.00003173
Iteration 314/1000 | Loss: 0.00003173
Iteration 315/1000 | Loss: 0.00003173
Iteration 316/1000 | Loss: 0.00003172
Iteration 317/1000 | Loss: 0.00003172
Iteration 318/1000 | Loss: 0.00003172
Iteration 319/1000 | Loss: 0.00003171
Iteration 320/1000 | Loss: 0.00003170
Iteration 321/1000 | Loss: 0.00003170
Iteration 322/1000 | Loss: 0.00003170
Iteration 323/1000 | Loss: 0.00003170
Iteration 324/1000 | Loss: 0.00003170
Iteration 325/1000 | Loss: 0.00003170
Iteration 326/1000 | Loss: 0.00003170
Iteration 327/1000 | Loss: 0.00003170
Iteration 328/1000 | Loss: 0.00003169
Iteration 329/1000 | Loss: 0.00003169
Iteration 330/1000 | Loss: 0.00003168
Iteration 331/1000 | Loss: 0.00003168
Iteration 332/1000 | Loss: 0.00003167
Iteration 333/1000 | Loss: 0.00003167
Iteration 334/1000 | Loss: 0.00003167
Iteration 335/1000 | Loss: 0.00003166
Iteration 336/1000 | Loss: 0.00003166
Iteration 337/1000 | Loss: 0.00003166
Iteration 338/1000 | Loss: 0.00003166
Iteration 339/1000 | Loss: 0.00003165
Iteration 340/1000 | Loss: 0.00003165
Iteration 341/1000 | Loss: 0.00003165
Iteration 342/1000 | Loss: 0.00003165
Iteration 343/1000 | Loss: 0.00003164
Iteration 344/1000 | Loss: 0.00003164
Iteration 345/1000 | Loss: 0.00003164
Iteration 346/1000 | Loss: 0.00003164
Iteration 347/1000 | Loss: 0.00003163
Iteration 348/1000 | Loss: 0.00003163
Iteration 349/1000 | Loss: 0.00003163
Iteration 350/1000 | Loss: 0.00003163
Iteration 351/1000 | Loss: 0.00003163
Iteration 352/1000 | Loss: 0.00003163
Iteration 353/1000 | Loss: 0.00003163
Iteration 354/1000 | Loss: 0.00003162
Iteration 355/1000 | Loss: 0.00003162
Iteration 356/1000 | Loss: 0.00003162
Iteration 357/1000 | Loss: 0.00003162
Iteration 358/1000 | Loss: 0.00003162
Iteration 359/1000 | Loss: 0.00003162
Iteration 360/1000 | Loss: 0.00003162
Iteration 361/1000 | Loss: 0.00003162
Iteration 362/1000 | Loss: 0.00003162
Iteration 363/1000 | Loss: 0.00003162
Iteration 364/1000 | Loss: 0.00003162
Iteration 365/1000 | Loss: 0.00003162
Iteration 366/1000 | Loss: 0.00003162
Iteration 367/1000 | Loss: 0.00003162
Iteration 368/1000 | Loss: 0.00003161
Iteration 369/1000 | Loss: 0.00003161
Iteration 370/1000 | Loss: 0.00003161
Iteration 371/1000 | Loss: 0.00003161
Iteration 372/1000 | Loss: 0.00003161
Iteration 373/1000 | Loss: 0.00003161
Iteration 374/1000 | Loss: 0.00003161
Iteration 375/1000 | Loss: 0.00003160
Iteration 376/1000 | Loss: 0.00003160
Iteration 377/1000 | Loss: 0.00003160
Iteration 378/1000 | Loss: 0.00003160
Iteration 379/1000 | Loss: 0.00003160
Iteration 380/1000 | Loss: 0.00003160
Iteration 381/1000 | Loss: 0.00003160
Iteration 382/1000 | Loss: 0.00003159
Iteration 383/1000 | Loss: 0.00003159
Iteration 384/1000 | Loss: 0.00003159
Iteration 385/1000 | Loss: 0.00003159
Iteration 386/1000 | Loss: 0.00003159
Iteration 387/1000 | Loss: 0.00003159
Iteration 388/1000 | Loss: 0.00003159
Iteration 389/1000 | Loss: 0.00003159
Iteration 390/1000 | Loss: 0.00003159
Iteration 391/1000 | Loss: 0.00003158
Iteration 392/1000 | Loss: 0.00003158
Iteration 393/1000 | Loss: 0.00003158
Iteration 394/1000 | Loss: 0.00003158
Iteration 395/1000 | Loss: 0.00003158
Iteration 396/1000 | Loss: 0.00003157
Iteration 397/1000 | Loss: 0.00003157
Iteration 398/1000 | Loss: 0.00003157
Iteration 399/1000 | Loss: 0.00003157
Iteration 400/1000 | Loss: 0.00003157
Iteration 401/1000 | Loss: 0.00003156
Iteration 402/1000 | Loss: 0.00003156
Iteration 403/1000 | Loss: 0.00003156
Iteration 404/1000 | Loss: 0.00003156
Iteration 405/1000 | Loss: 0.00003156
Iteration 406/1000 | Loss: 0.00003155
Iteration 407/1000 | Loss: 0.00003155
Iteration 408/1000 | Loss: 0.00003155
Iteration 409/1000 | Loss: 0.00003155
Iteration 410/1000 | Loss: 0.00003155
Iteration 411/1000 | Loss: 0.00003155
Iteration 412/1000 | Loss: 0.00003155
Iteration 413/1000 | Loss: 0.00003155
Iteration 414/1000 | Loss: 0.00003155
Iteration 415/1000 | Loss: 0.00003155
Iteration 416/1000 | Loss: 0.00003155
Iteration 417/1000 | Loss: 0.00003155
Iteration 418/1000 | Loss: 0.00003155
Iteration 419/1000 | Loss: 0.00003155
Iteration 420/1000 | Loss: 0.00003154
Iteration 421/1000 | Loss: 0.00003154
Iteration 422/1000 | Loss: 0.00003154
Iteration 423/1000 | Loss: 0.00003154
Iteration 424/1000 | Loss: 0.00003154
Iteration 425/1000 | Loss: 0.00003154
Iteration 426/1000 | Loss: 0.00003154
Iteration 427/1000 | Loss: 0.00003154
Iteration 428/1000 | Loss: 0.00003154
Iteration 429/1000 | Loss: 0.00003154
Iteration 430/1000 | Loss: 0.00003154
Iteration 431/1000 | Loss: 0.00003154
Iteration 432/1000 | Loss: 0.00003154
Iteration 433/1000 | Loss: 0.00003153
Iteration 434/1000 | Loss: 0.00003153
Iteration 435/1000 | Loss: 0.00003153
Iteration 436/1000 | Loss: 0.00003153
Iteration 437/1000 | Loss: 0.00003153
Iteration 438/1000 | Loss: 0.00003153
Iteration 439/1000 | Loss: 0.00003153
Iteration 440/1000 | Loss: 0.00003152
Iteration 441/1000 | Loss: 0.00003152
Iteration 442/1000 | Loss: 0.00003152
Iteration 443/1000 | Loss: 0.00003152
Iteration 444/1000 | Loss: 0.00003152
Iteration 445/1000 | Loss: 0.00003152
Iteration 446/1000 | Loss: 0.00003152
Iteration 447/1000 | Loss: 0.00003152
Iteration 448/1000 | Loss: 0.00003152
Iteration 449/1000 | Loss: 0.00003152
Iteration 450/1000 | Loss: 0.00003152
Iteration 451/1000 | Loss: 0.00003152
Iteration 452/1000 | Loss: 0.00003151
Iteration 453/1000 | Loss: 0.00003151
Iteration 454/1000 | Loss: 0.00003151
Iteration 455/1000 | Loss: 0.00003151
Iteration 456/1000 | Loss: 0.00003151
Iteration 457/1000 | Loss: 0.00003151
Iteration 458/1000 | Loss: 0.00003151
Iteration 459/1000 | Loss: 0.00003151
Iteration 460/1000 | Loss: 0.00003151
Iteration 461/1000 | Loss: 0.00003150
Iteration 462/1000 | Loss: 0.00003150
Iteration 463/1000 | Loss: 0.00003150
Iteration 464/1000 | Loss: 0.00003150
Iteration 465/1000 | Loss: 0.00003150
Iteration 466/1000 | Loss: 0.00003150
Iteration 467/1000 | Loss: 0.00003150
Iteration 468/1000 | Loss: 0.00003150
Iteration 469/1000 | Loss: 0.00003149
Iteration 470/1000 | Loss: 0.00003149
Iteration 471/1000 | Loss: 0.00003149
Iteration 472/1000 | Loss: 0.00003149
Iteration 473/1000 | Loss: 0.00003149
Iteration 474/1000 | Loss: 0.00003149
Iteration 475/1000 | Loss: 0.00003149
Iteration 476/1000 | Loss: 0.00003149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 476. Stopping optimization.
Last 5 losses: [3.149388066958636e-05, 3.149388066958636e-05, 3.149388066958636e-05, 3.149388066958636e-05, 3.149388066958636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.149388066958636e-05

Optimization complete. Final v2v error: 4.577311992645264 mm

Highest mean error: 5.031010150909424 mm for frame 66

Lowest mean error: 3.426494598388672 mm for frame 5

Saving results

Total time: 241.27387261390686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590984
Iteration 2/25 | Loss: 0.00129975
Iteration 3/25 | Loss: 0.00122870
Iteration 4/25 | Loss: 0.00122057
Iteration 5/25 | Loss: 0.00121785
Iteration 6/25 | Loss: 0.00121770
Iteration 7/25 | Loss: 0.00121770
Iteration 8/25 | Loss: 0.00121770
Iteration 9/25 | Loss: 0.00121770
Iteration 10/25 | Loss: 0.00121770
Iteration 11/25 | Loss: 0.00121770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012176973978057504, 0.0012176973978057504, 0.0012176973978057504, 0.0012176973978057504, 0.0012176973978057504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012176973978057504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.45989990
Iteration 2/25 | Loss: 0.00071423
Iteration 3/25 | Loss: 0.00071416
Iteration 4/25 | Loss: 0.00071416
Iteration 5/25 | Loss: 0.00071416
Iteration 6/25 | Loss: 0.00071416
Iteration 7/25 | Loss: 0.00071416
Iteration 8/25 | Loss: 0.00071416
Iteration 9/25 | Loss: 0.00071416
Iteration 10/25 | Loss: 0.00071416
Iteration 11/25 | Loss: 0.00071416
Iteration 12/25 | Loss: 0.00071416
Iteration 13/25 | Loss: 0.00071416
Iteration 14/25 | Loss: 0.00071416
Iteration 15/25 | Loss: 0.00071416
Iteration 16/25 | Loss: 0.00071415
Iteration 17/25 | Loss: 0.00071415
Iteration 18/25 | Loss: 0.00071415
Iteration 19/25 | Loss: 0.00071415
Iteration 20/25 | Loss: 0.00071415
Iteration 21/25 | Loss: 0.00071415
Iteration 22/25 | Loss: 0.00071415
Iteration 23/25 | Loss: 0.00071415
Iteration 24/25 | Loss: 0.00071415
Iteration 25/25 | Loss: 0.00071415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071415
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00001543
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001396
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001356
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001353
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001351
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001337
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001335
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001335
Iteration 119/1000 | Loss: 0.00001335
Iteration 120/1000 | Loss: 0.00001335
Iteration 121/1000 | Loss: 0.00001335
Iteration 122/1000 | Loss: 0.00001335
Iteration 123/1000 | Loss: 0.00001335
Iteration 124/1000 | Loss: 0.00001335
Iteration 125/1000 | Loss: 0.00001335
Iteration 126/1000 | Loss: 0.00001335
Iteration 127/1000 | Loss: 0.00001335
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001335
Iteration 131/1000 | Loss: 0.00001335
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001335
Iteration 135/1000 | Loss: 0.00001335
Iteration 136/1000 | Loss: 0.00001335
Iteration 137/1000 | Loss: 0.00001335
Iteration 138/1000 | Loss: 0.00001335
Iteration 139/1000 | Loss: 0.00001335
Iteration 140/1000 | Loss: 0.00001335
Iteration 141/1000 | Loss: 0.00001335
Iteration 142/1000 | Loss: 0.00001335
Iteration 143/1000 | Loss: 0.00001335
Iteration 144/1000 | Loss: 0.00001335
Iteration 145/1000 | Loss: 0.00001335
Iteration 146/1000 | Loss: 0.00001335
Iteration 147/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.3345486877369694e-05, 1.3345486877369694e-05, 1.3345486877369694e-05, 1.3345486877369694e-05, 1.3345486877369694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3345486877369694e-05

Optimization complete. Final v2v error: 3.085974931716919 mm

Highest mean error: 3.423135995864868 mm for frame 102

Lowest mean error: 2.7728235721588135 mm for frame 163

Saving results

Total time: 35.530877351760864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948865
Iteration 2/25 | Loss: 0.00341763
Iteration 3/25 | Loss: 0.00236925
Iteration 4/25 | Loss: 0.00186044
Iteration 5/25 | Loss: 0.00176279
Iteration 6/25 | Loss: 0.00175359
Iteration 7/25 | Loss: 0.00171970
Iteration 8/25 | Loss: 0.00169056
Iteration 9/25 | Loss: 0.00164321
Iteration 10/25 | Loss: 0.00161597
Iteration 11/25 | Loss: 0.00160910
Iteration 12/25 | Loss: 0.00160115
Iteration 13/25 | Loss: 0.00159336
Iteration 14/25 | Loss: 0.00158973
Iteration 15/25 | Loss: 0.00158610
Iteration 16/25 | Loss: 0.00159786
Iteration 17/25 | Loss: 0.00160458
Iteration 18/25 | Loss: 0.00157537
Iteration 19/25 | Loss: 0.00156419
Iteration 20/25 | Loss: 0.00155660
Iteration 21/25 | Loss: 0.00155656
Iteration 22/25 | Loss: 0.00155614
Iteration 23/25 | Loss: 0.00155962
Iteration 24/25 | Loss: 0.00154763
Iteration 25/25 | Loss: 0.00154115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41500282
Iteration 2/25 | Loss: 0.00341918
Iteration 3/25 | Loss: 0.00332872
Iteration 4/25 | Loss: 0.00332872
Iteration 5/25 | Loss: 0.00332872
Iteration 6/25 | Loss: 0.00332872
Iteration 7/25 | Loss: 0.00332872
Iteration 8/25 | Loss: 0.00332872
Iteration 9/25 | Loss: 0.00332872
Iteration 10/25 | Loss: 0.00332872
Iteration 11/25 | Loss: 0.00332872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0033287208061665297, 0.0033287208061665297, 0.0033287208061665297, 0.0033287208061665297, 0.0033287208061665297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033287208061665297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332872
Iteration 2/1000 | Loss: 0.00248399
Iteration 3/1000 | Loss: 0.00109122
Iteration 4/1000 | Loss: 0.00062397
Iteration 5/1000 | Loss: 0.00079928
Iteration 6/1000 | Loss: 0.00066118
Iteration 7/1000 | Loss: 0.00044473
Iteration 8/1000 | Loss: 0.00032883
Iteration 9/1000 | Loss: 0.00040842
Iteration 10/1000 | Loss: 0.00028479
Iteration 11/1000 | Loss: 0.00013316
Iteration 12/1000 | Loss: 0.00153714
Iteration 13/1000 | Loss: 0.00288002
Iteration 14/1000 | Loss: 0.00450995
Iteration 15/1000 | Loss: 0.00107136
Iteration 16/1000 | Loss: 0.00069250
Iteration 17/1000 | Loss: 0.00059733
Iteration 18/1000 | Loss: 0.00036767
Iteration 19/1000 | Loss: 0.00070224
Iteration 20/1000 | Loss: 0.00078543
Iteration 21/1000 | Loss: 0.00033336
Iteration 22/1000 | Loss: 0.00011930
Iteration 23/1000 | Loss: 0.00010203
Iteration 24/1000 | Loss: 0.00009162
Iteration 25/1000 | Loss: 0.00028683
Iteration 26/1000 | Loss: 0.00073660
Iteration 27/1000 | Loss: 0.00077861
Iteration 28/1000 | Loss: 0.00086191
Iteration 29/1000 | Loss: 0.00023976
Iteration 30/1000 | Loss: 0.00025146
Iteration 31/1000 | Loss: 0.00080128
Iteration 32/1000 | Loss: 0.00053311
Iteration 33/1000 | Loss: 0.00105301
Iteration 34/1000 | Loss: 0.00068112
Iteration 35/1000 | Loss: 0.00031749
Iteration 36/1000 | Loss: 0.00080047
Iteration 37/1000 | Loss: 0.00062725
Iteration 38/1000 | Loss: 0.00021933
Iteration 39/1000 | Loss: 0.00046459
Iteration 40/1000 | Loss: 0.00069707
Iteration 41/1000 | Loss: 0.00058416
Iteration 42/1000 | Loss: 0.00061134
Iteration 43/1000 | Loss: 0.00020574
Iteration 44/1000 | Loss: 0.00007963
Iteration 45/1000 | Loss: 0.00049356
Iteration 46/1000 | Loss: 0.00064338
Iteration 47/1000 | Loss: 0.00103910
Iteration 48/1000 | Loss: 0.00041949
Iteration 49/1000 | Loss: 0.00009332
Iteration 50/1000 | Loss: 0.00019719
Iteration 51/1000 | Loss: 0.00015372
Iteration 52/1000 | Loss: 0.00007438
Iteration 53/1000 | Loss: 0.00019723
Iteration 54/1000 | Loss: 0.00007186
Iteration 55/1000 | Loss: 0.00007018
Iteration 56/1000 | Loss: 0.00056125
Iteration 57/1000 | Loss: 0.00112369
Iteration 58/1000 | Loss: 0.00037446
Iteration 59/1000 | Loss: 0.00056415
Iteration 60/1000 | Loss: 0.00015095
Iteration 61/1000 | Loss: 0.00013934
Iteration 62/1000 | Loss: 0.00019461
Iteration 63/1000 | Loss: 0.00065449
Iteration 64/1000 | Loss: 0.00011356
Iteration 65/1000 | Loss: 0.00007877
Iteration 66/1000 | Loss: 0.00073014
Iteration 67/1000 | Loss: 0.00063205
Iteration 68/1000 | Loss: 0.00007934
Iteration 69/1000 | Loss: 0.00006224
Iteration 70/1000 | Loss: 0.00005810
Iteration 71/1000 | Loss: 0.00005579
Iteration 72/1000 | Loss: 0.00005227
Iteration 73/1000 | Loss: 0.00030690
Iteration 74/1000 | Loss: 0.00007619
Iteration 75/1000 | Loss: 0.00010774
Iteration 76/1000 | Loss: 0.00037665
Iteration 77/1000 | Loss: 0.00026150
Iteration 78/1000 | Loss: 0.00004713
Iteration 79/1000 | Loss: 0.00004565
Iteration 80/1000 | Loss: 0.00034011
Iteration 81/1000 | Loss: 0.00004497
Iteration 82/1000 | Loss: 0.00004294
Iteration 83/1000 | Loss: 0.00033593
Iteration 84/1000 | Loss: 0.00004216
Iteration 85/1000 | Loss: 0.00004052
Iteration 86/1000 | Loss: 0.00003921
Iteration 87/1000 | Loss: 0.00003817
Iteration 88/1000 | Loss: 0.00050011
Iteration 89/1000 | Loss: 0.00036297
Iteration 90/1000 | Loss: 0.00005371
Iteration 91/1000 | Loss: 0.00003852
Iteration 92/1000 | Loss: 0.00003607
Iteration 93/1000 | Loss: 0.00003461
Iteration 94/1000 | Loss: 0.00003371
Iteration 95/1000 | Loss: 0.00003293
Iteration 96/1000 | Loss: 0.00003249
Iteration 97/1000 | Loss: 0.00003207
Iteration 98/1000 | Loss: 0.00003176
Iteration 99/1000 | Loss: 0.00003158
Iteration 100/1000 | Loss: 0.00003133
Iteration 101/1000 | Loss: 0.00064776
Iteration 102/1000 | Loss: 0.00036548
Iteration 103/1000 | Loss: 0.00003378
Iteration 104/1000 | Loss: 0.00003045
Iteration 105/1000 | Loss: 0.00002899
Iteration 106/1000 | Loss: 0.00002824
Iteration 107/1000 | Loss: 0.00002743
Iteration 108/1000 | Loss: 0.00002703
Iteration 109/1000 | Loss: 0.00002680
Iteration 110/1000 | Loss: 0.00002659
Iteration 111/1000 | Loss: 0.00002647
Iteration 112/1000 | Loss: 0.00002641
Iteration 113/1000 | Loss: 0.00002637
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002629
Iteration 116/1000 | Loss: 0.00002628
Iteration 117/1000 | Loss: 0.00002625
Iteration 118/1000 | Loss: 0.00002625
Iteration 119/1000 | Loss: 0.00002625
Iteration 120/1000 | Loss: 0.00002624
Iteration 121/1000 | Loss: 0.00002624
Iteration 122/1000 | Loss: 0.00002623
Iteration 123/1000 | Loss: 0.00002623
Iteration 124/1000 | Loss: 0.00002622
Iteration 125/1000 | Loss: 0.00002622
Iteration 126/1000 | Loss: 0.00002622
Iteration 127/1000 | Loss: 0.00002622
Iteration 128/1000 | Loss: 0.00002622
Iteration 129/1000 | Loss: 0.00031437
Iteration 130/1000 | Loss: 0.00017246
Iteration 131/1000 | Loss: 0.00018963
Iteration 132/1000 | Loss: 0.00002677
Iteration 133/1000 | Loss: 0.00002545
Iteration 134/1000 | Loss: 0.00002483
Iteration 135/1000 | Loss: 0.00002436
Iteration 136/1000 | Loss: 0.00002411
Iteration 137/1000 | Loss: 0.00002408
Iteration 138/1000 | Loss: 0.00002405
Iteration 139/1000 | Loss: 0.00002404
Iteration 140/1000 | Loss: 0.00002404
Iteration 141/1000 | Loss: 0.00002404
Iteration 142/1000 | Loss: 0.00002404
Iteration 143/1000 | Loss: 0.00002404
Iteration 144/1000 | Loss: 0.00002404
Iteration 145/1000 | Loss: 0.00002403
Iteration 146/1000 | Loss: 0.00002403
Iteration 147/1000 | Loss: 0.00002403
Iteration 148/1000 | Loss: 0.00002402
Iteration 149/1000 | Loss: 0.00002402
Iteration 150/1000 | Loss: 0.00002402
Iteration 151/1000 | Loss: 0.00002402
Iteration 152/1000 | Loss: 0.00002401
Iteration 153/1000 | Loss: 0.00002401
Iteration 154/1000 | Loss: 0.00002396
Iteration 155/1000 | Loss: 0.00002396
Iteration 156/1000 | Loss: 0.00002394
Iteration 157/1000 | Loss: 0.00002393
Iteration 158/1000 | Loss: 0.00002393
Iteration 159/1000 | Loss: 0.00002392
Iteration 160/1000 | Loss: 0.00002392
Iteration 161/1000 | Loss: 0.00002392
Iteration 162/1000 | Loss: 0.00002391
Iteration 163/1000 | Loss: 0.00002391
Iteration 164/1000 | Loss: 0.00002390
Iteration 165/1000 | Loss: 0.00002390
Iteration 166/1000 | Loss: 0.00002389
Iteration 167/1000 | Loss: 0.00002389
Iteration 168/1000 | Loss: 0.00002389
Iteration 169/1000 | Loss: 0.00002389
Iteration 170/1000 | Loss: 0.00002389
Iteration 171/1000 | Loss: 0.00002389
Iteration 172/1000 | Loss: 0.00002389
Iteration 173/1000 | Loss: 0.00002389
Iteration 174/1000 | Loss: 0.00002389
Iteration 175/1000 | Loss: 0.00002389
Iteration 176/1000 | Loss: 0.00002389
Iteration 177/1000 | Loss: 0.00002389
Iteration 178/1000 | Loss: 0.00002389
Iteration 179/1000 | Loss: 0.00002389
Iteration 180/1000 | Loss: 0.00002389
Iteration 181/1000 | Loss: 0.00002389
Iteration 182/1000 | Loss: 0.00002389
Iteration 183/1000 | Loss: 0.00002389
Iteration 184/1000 | Loss: 0.00002389
Iteration 185/1000 | Loss: 0.00002389
Iteration 186/1000 | Loss: 0.00002389
Iteration 187/1000 | Loss: 0.00002389
Iteration 188/1000 | Loss: 0.00002389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.388545726716984e-05, 2.388545726716984e-05, 2.388545726716984e-05, 2.388545726716984e-05, 2.388545726716984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.388545726716984e-05

Optimization complete. Final v2v error: 3.821378469467163 mm

Highest mean error: 7.468168258666992 mm for frame 133

Lowest mean error: 2.8849077224731445 mm for frame 193

Saving results

Total time: 233.7030394077301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796665
Iteration 2/25 | Loss: 0.00128372
Iteration 3/25 | Loss: 0.00120212
Iteration 4/25 | Loss: 0.00118886
Iteration 5/25 | Loss: 0.00118665
Iteration 6/25 | Loss: 0.00118665
Iteration 7/25 | Loss: 0.00118665
Iteration 8/25 | Loss: 0.00118665
Iteration 9/25 | Loss: 0.00118665
Iteration 10/25 | Loss: 0.00118665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011866510612890124, 0.0011866510612890124, 0.0011866510612890124, 0.0011866510612890124, 0.0011866510612890124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011866510612890124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48951304
Iteration 2/25 | Loss: 0.00060020
Iteration 3/25 | Loss: 0.00060020
Iteration 4/25 | Loss: 0.00060020
Iteration 5/25 | Loss: 0.00060020
Iteration 6/25 | Loss: 0.00060019
Iteration 7/25 | Loss: 0.00060019
Iteration 8/25 | Loss: 0.00060019
Iteration 9/25 | Loss: 0.00060019
Iteration 10/25 | Loss: 0.00060019
Iteration 11/25 | Loss: 0.00060019
Iteration 12/25 | Loss: 0.00060019
Iteration 13/25 | Loss: 0.00060019
Iteration 14/25 | Loss: 0.00060019
Iteration 15/25 | Loss: 0.00060019
Iteration 16/25 | Loss: 0.00060019
Iteration 17/25 | Loss: 0.00060019
Iteration 18/25 | Loss: 0.00060019
Iteration 19/25 | Loss: 0.00060019
Iteration 20/25 | Loss: 0.00060019
Iteration 21/25 | Loss: 0.00060019
Iteration 22/25 | Loss: 0.00060019
Iteration 23/25 | Loss: 0.00060019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006001935689710081, 0.0006001935689710081, 0.0006001935689710081, 0.0006001935689710081, 0.0006001935689710081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006001935689710081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060019
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00001760
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001485
Iteration 9/1000 | Loss: 0.00001463
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001423
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001414
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001412
Iteration 26/1000 | Loss: 0.00001412
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001411
Iteration 30/1000 | Loss: 0.00001411
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001402
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001401
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001392
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001383
Iteration 102/1000 | Loss: 0.00001383
Iteration 103/1000 | Loss: 0.00001383
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.3825138921674807e-05, 1.3825138921674807e-05, 1.3825138921674807e-05, 1.3825138921674807e-05, 1.3825138921674807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3825138921674807e-05

Optimization complete. Final v2v error: 3.1541216373443604 mm

Highest mean error: 3.43143630027771 mm for frame 62

Lowest mean error: 3.038908004760742 mm for frame 145

Saving results

Total time: 32.53035497665405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922555
Iteration 2/25 | Loss: 0.00269090
Iteration 3/25 | Loss: 0.00198838
Iteration 4/25 | Loss: 0.00175765
Iteration 5/25 | Loss: 0.00170318
Iteration 6/25 | Loss: 0.00158522
Iteration 7/25 | Loss: 0.00157097
Iteration 8/25 | Loss: 0.00151927
Iteration 9/25 | Loss: 0.00146649
Iteration 10/25 | Loss: 0.00142659
Iteration 11/25 | Loss: 0.00140030
Iteration 12/25 | Loss: 0.00141562
Iteration 13/25 | Loss: 0.00139069
Iteration 14/25 | Loss: 0.00137325
Iteration 15/25 | Loss: 0.00136318
Iteration 16/25 | Loss: 0.00136021
Iteration 17/25 | Loss: 0.00135164
Iteration 18/25 | Loss: 0.00134913
Iteration 19/25 | Loss: 0.00134786
Iteration 20/25 | Loss: 0.00135804
Iteration 21/25 | Loss: 0.00134372
Iteration 22/25 | Loss: 0.00133766
Iteration 23/25 | Loss: 0.00134157
Iteration 24/25 | Loss: 0.00133095
Iteration 25/25 | Loss: 0.00132879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44063282
Iteration 2/25 | Loss: 0.00099807
Iteration 3/25 | Loss: 0.00097272
Iteration 4/25 | Loss: 0.00097272
Iteration 5/25 | Loss: 0.00097272
Iteration 6/25 | Loss: 0.00097272
Iteration 7/25 | Loss: 0.00097272
Iteration 8/25 | Loss: 0.00097272
Iteration 9/25 | Loss: 0.00097272
Iteration 10/25 | Loss: 0.00097272
Iteration 11/25 | Loss: 0.00097272
Iteration 12/25 | Loss: 0.00097272
Iteration 13/25 | Loss: 0.00097272
Iteration 14/25 | Loss: 0.00097272
Iteration 15/25 | Loss: 0.00097272
Iteration 16/25 | Loss: 0.00097272
Iteration 17/25 | Loss: 0.00097272
Iteration 18/25 | Loss: 0.00097272
Iteration 19/25 | Loss: 0.00097272
Iteration 20/25 | Loss: 0.00097272
Iteration 21/25 | Loss: 0.00097272
Iteration 22/25 | Loss: 0.00097272
Iteration 23/25 | Loss: 0.00097272
Iteration 24/25 | Loss: 0.00097272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009727155556902289, 0.0009727155556902289, 0.0009727155556902289, 0.0009727155556902289, 0.0009727155556902289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009727155556902289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097272
Iteration 2/1000 | Loss: 0.00013913
Iteration 3/1000 | Loss: 0.00035277
Iteration 4/1000 | Loss: 0.00096991
Iteration 5/1000 | Loss: 0.00056592
Iteration 6/1000 | Loss: 0.00081522
Iteration 7/1000 | Loss: 0.00061272
Iteration 8/1000 | Loss: 0.00100443
Iteration 9/1000 | Loss: 0.00013519
Iteration 10/1000 | Loss: 0.00011722
Iteration 11/1000 | Loss: 0.00025784
Iteration 12/1000 | Loss: 0.00011687
Iteration 13/1000 | Loss: 0.00010586
Iteration 14/1000 | Loss: 0.00081942
Iteration 15/1000 | Loss: 0.00071838
Iteration 16/1000 | Loss: 0.00082991
Iteration 17/1000 | Loss: 0.00020332
Iteration 18/1000 | Loss: 0.00071089
Iteration 19/1000 | Loss: 0.00058280
Iteration 20/1000 | Loss: 0.00211039
Iteration 21/1000 | Loss: 0.00109834
Iteration 22/1000 | Loss: 0.00144133
Iteration 23/1000 | Loss: 0.00116869
Iteration 24/1000 | Loss: 0.00093142
Iteration 25/1000 | Loss: 0.00086861
Iteration 26/1000 | Loss: 0.00047208
Iteration 27/1000 | Loss: 0.00087188
Iteration 28/1000 | Loss: 0.00070479
Iteration 29/1000 | Loss: 0.00052949
Iteration 30/1000 | Loss: 0.00037987
Iteration 31/1000 | Loss: 0.00041528
Iteration 32/1000 | Loss: 0.00020385
Iteration 33/1000 | Loss: 0.00006662
Iteration 34/1000 | Loss: 0.00006057
Iteration 35/1000 | Loss: 0.00010766
Iteration 36/1000 | Loss: 0.00006071
Iteration 37/1000 | Loss: 0.00025901
Iteration 38/1000 | Loss: 0.00031991
Iteration 39/1000 | Loss: 0.00033852
Iteration 40/1000 | Loss: 0.00088159
Iteration 41/1000 | Loss: 0.00007568
Iteration 42/1000 | Loss: 0.00009538
Iteration 43/1000 | Loss: 0.00025477
Iteration 44/1000 | Loss: 0.00005764
Iteration 45/1000 | Loss: 0.00009034
Iteration 46/1000 | Loss: 0.00005365
Iteration 47/1000 | Loss: 0.00016270
Iteration 48/1000 | Loss: 0.00011290
Iteration 49/1000 | Loss: 0.00011197
Iteration 50/1000 | Loss: 0.00012918
Iteration 51/1000 | Loss: 0.00008022
Iteration 52/1000 | Loss: 0.00040066
Iteration 53/1000 | Loss: 0.00024339
Iteration 54/1000 | Loss: 0.00029103
Iteration 55/1000 | Loss: 0.00155306
Iteration 56/1000 | Loss: 0.00029544
Iteration 57/1000 | Loss: 0.00041481
Iteration 58/1000 | Loss: 0.00004851
Iteration 59/1000 | Loss: 0.00004442
Iteration 60/1000 | Loss: 0.00004265
Iteration 61/1000 | Loss: 0.00004100
Iteration 62/1000 | Loss: 0.00003972
Iteration 63/1000 | Loss: 0.00059234
Iteration 64/1000 | Loss: 0.00005195
Iteration 65/1000 | Loss: 0.00005487
Iteration 66/1000 | Loss: 0.00003826
Iteration 67/1000 | Loss: 0.00003769
Iteration 68/1000 | Loss: 0.00003717
Iteration 69/1000 | Loss: 0.00029354
Iteration 70/1000 | Loss: 0.00003701
Iteration 71/1000 | Loss: 0.00003666
Iteration 72/1000 | Loss: 0.00003651
Iteration 73/1000 | Loss: 0.00003650
Iteration 74/1000 | Loss: 0.00003650
Iteration 75/1000 | Loss: 0.00003649
Iteration 76/1000 | Loss: 0.00003649
Iteration 77/1000 | Loss: 0.00003647
Iteration 78/1000 | Loss: 0.00003646
Iteration 79/1000 | Loss: 0.00003646
Iteration 80/1000 | Loss: 0.00003646
Iteration 81/1000 | Loss: 0.00003636
Iteration 82/1000 | Loss: 0.00003636
Iteration 83/1000 | Loss: 0.00003635
Iteration 84/1000 | Loss: 0.00003635
Iteration 85/1000 | Loss: 0.00003634
Iteration 86/1000 | Loss: 0.00003634
Iteration 87/1000 | Loss: 0.00003632
Iteration 88/1000 | Loss: 0.00003631
Iteration 89/1000 | Loss: 0.00003629
Iteration 90/1000 | Loss: 0.00003629
Iteration 91/1000 | Loss: 0.00003629
Iteration 92/1000 | Loss: 0.00024744
Iteration 93/1000 | Loss: 0.00024744
Iteration 94/1000 | Loss: 0.00046959
Iteration 95/1000 | Loss: 0.00027358
Iteration 96/1000 | Loss: 0.00035736
Iteration 97/1000 | Loss: 0.00003705
Iteration 98/1000 | Loss: 0.00003645
Iteration 99/1000 | Loss: 0.00003625
Iteration 100/1000 | Loss: 0.00003622
Iteration 101/1000 | Loss: 0.00003621
Iteration 102/1000 | Loss: 0.00003621
Iteration 103/1000 | Loss: 0.00003621
Iteration 104/1000 | Loss: 0.00027630
Iteration 105/1000 | Loss: 0.00005780
Iteration 106/1000 | Loss: 0.00003626
Iteration 107/1000 | Loss: 0.00003624
Iteration 108/1000 | Loss: 0.00011379
Iteration 109/1000 | Loss: 0.00003628
Iteration 110/1000 | Loss: 0.00003623
Iteration 111/1000 | Loss: 0.00003621
Iteration 112/1000 | Loss: 0.00003619
Iteration 113/1000 | Loss: 0.00003619
Iteration 114/1000 | Loss: 0.00003618
Iteration 115/1000 | Loss: 0.00003617
Iteration 116/1000 | Loss: 0.00003617
Iteration 117/1000 | Loss: 0.00003615
Iteration 118/1000 | Loss: 0.00003615
Iteration 119/1000 | Loss: 0.00003614
Iteration 120/1000 | Loss: 0.00003614
Iteration 121/1000 | Loss: 0.00003614
Iteration 122/1000 | Loss: 0.00003613
Iteration 123/1000 | Loss: 0.00003613
Iteration 124/1000 | Loss: 0.00003613
Iteration 125/1000 | Loss: 0.00003613
Iteration 126/1000 | Loss: 0.00003613
Iteration 127/1000 | Loss: 0.00003613
Iteration 128/1000 | Loss: 0.00003613
Iteration 129/1000 | Loss: 0.00003613
Iteration 130/1000 | Loss: 0.00003613
Iteration 131/1000 | Loss: 0.00003613
Iteration 132/1000 | Loss: 0.00003613
Iteration 133/1000 | Loss: 0.00003613
Iteration 134/1000 | Loss: 0.00003613
Iteration 135/1000 | Loss: 0.00003613
Iteration 136/1000 | Loss: 0.00003613
Iteration 137/1000 | Loss: 0.00003613
Iteration 138/1000 | Loss: 0.00003613
Iteration 139/1000 | Loss: 0.00003613
Iteration 140/1000 | Loss: 0.00003613
Iteration 141/1000 | Loss: 0.00003613
Iteration 142/1000 | Loss: 0.00003613
Iteration 143/1000 | Loss: 0.00003613
Iteration 144/1000 | Loss: 0.00003613
Iteration 145/1000 | Loss: 0.00003613
Iteration 146/1000 | Loss: 0.00003613
Iteration 147/1000 | Loss: 0.00003613
Iteration 148/1000 | Loss: 0.00003613
Iteration 149/1000 | Loss: 0.00003613
Iteration 150/1000 | Loss: 0.00003613
Iteration 151/1000 | Loss: 0.00003613
Iteration 152/1000 | Loss: 0.00003613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.612774526118301e-05, 3.612774526118301e-05, 3.612774526118301e-05, 3.612774526118301e-05, 3.612774526118301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.612774526118301e-05

Optimization complete. Final v2v error: 4.191088676452637 mm

Highest mean error: 6.349100112915039 mm for frame 99

Lowest mean error: 3.2771477699279785 mm for frame 24

Saving results

Total time: 160.7372670173645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504342
Iteration 2/25 | Loss: 0.00155132
Iteration 3/25 | Loss: 0.00133377
Iteration 4/25 | Loss: 0.00131174
Iteration 5/25 | Loss: 0.00130560
Iteration 6/25 | Loss: 0.00130442
Iteration 7/25 | Loss: 0.00130442
Iteration 8/25 | Loss: 0.00130442
Iteration 9/25 | Loss: 0.00130442
Iteration 10/25 | Loss: 0.00130442
Iteration 11/25 | Loss: 0.00130442
Iteration 12/25 | Loss: 0.00130442
Iteration 13/25 | Loss: 0.00130442
Iteration 14/25 | Loss: 0.00130442
Iteration 15/25 | Loss: 0.00130442
Iteration 16/25 | Loss: 0.00130442
Iteration 17/25 | Loss: 0.00130442
Iteration 18/25 | Loss: 0.00130442
Iteration 19/25 | Loss: 0.00130442
Iteration 20/25 | Loss: 0.00130442
Iteration 21/25 | Loss: 0.00130442
Iteration 22/25 | Loss: 0.00130442
Iteration 23/25 | Loss: 0.00130442
Iteration 24/25 | Loss: 0.00130442
Iteration 25/25 | Loss: 0.00130442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46566105
Iteration 2/25 | Loss: 0.00070770
Iteration 3/25 | Loss: 0.00070769
Iteration 4/25 | Loss: 0.00070769
Iteration 5/25 | Loss: 0.00070769
Iteration 6/25 | Loss: 0.00070769
Iteration 7/25 | Loss: 0.00070769
Iteration 8/25 | Loss: 0.00070769
Iteration 9/25 | Loss: 0.00070769
Iteration 10/25 | Loss: 0.00070769
Iteration 11/25 | Loss: 0.00070769
Iteration 12/25 | Loss: 0.00070769
Iteration 13/25 | Loss: 0.00070769
Iteration 14/25 | Loss: 0.00070769
Iteration 15/25 | Loss: 0.00070769
Iteration 16/25 | Loss: 0.00070769
Iteration 17/25 | Loss: 0.00070769
Iteration 18/25 | Loss: 0.00070769
Iteration 19/25 | Loss: 0.00070769
Iteration 20/25 | Loss: 0.00070769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007076854817569256, 0.0007076854817569256, 0.0007076854817569256, 0.0007076854817569256, 0.0007076854817569256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007076854817569256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070769
Iteration 2/1000 | Loss: 0.00005780
Iteration 3/1000 | Loss: 0.00003780
Iteration 4/1000 | Loss: 0.00003430
Iteration 5/1000 | Loss: 0.00003293
Iteration 6/1000 | Loss: 0.00003177
Iteration 7/1000 | Loss: 0.00003075
Iteration 8/1000 | Loss: 0.00003014
Iteration 9/1000 | Loss: 0.00002971
Iteration 10/1000 | Loss: 0.00002933
Iteration 11/1000 | Loss: 0.00002910
Iteration 12/1000 | Loss: 0.00002889
Iteration 13/1000 | Loss: 0.00002868
Iteration 14/1000 | Loss: 0.00002856
Iteration 15/1000 | Loss: 0.00002837
Iteration 16/1000 | Loss: 0.00002828
Iteration 17/1000 | Loss: 0.00002826
Iteration 18/1000 | Loss: 0.00002826
Iteration 19/1000 | Loss: 0.00002821
Iteration 20/1000 | Loss: 0.00002813
Iteration 21/1000 | Loss: 0.00002804
Iteration 22/1000 | Loss: 0.00002801
Iteration 23/1000 | Loss: 0.00002801
Iteration 24/1000 | Loss: 0.00002800
Iteration 25/1000 | Loss: 0.00002800
Iteration 26/1000 | Loss: 0.00002800
Iteration 27/1000 | Loss: 0.00002798
Iteration 28/1000 | Loss: 0.00002798
Iteration 29/1000 | Loss: 0.00002797
Iteration 30/1000 | Loss: 0.00002796
Iteration 31/1000 | Loss: 0.00002796
Iteration 32/1000 | Loss: 0.00002795
Iteration 33/1000 | Loss: 0.00002795
Iteration 34/1000 | Loss: 0.00002794
Iteration 35/1000 | Loss: 0.00002793
Iteration 36/1000 | Loss: 0.00002793
Iteration 37/1000 | Loss: 0.00002793
Iteration 38/1000 | Loss: 0.00002793
Iteration 39/1000 | Loss: 0.00002793
Iteration 40/1000 | Loss: 0.00002793
Iteration 41/1000 | Loss: 0.00002791
Iteration 42/1000 | Loss: 0.00002791
Iteration 43/1000 | Loss: 0.00002791
Iteration 44/1000 | Loss: 0.00002791
Iteration 45/1000 | Loss: 0.00002791
Iteration 46/1000 | Loss: 0.00002791
Iteration 47/1000 | Loss: 0.00002790
Iteration 48/1000 | Loss: 0.00002790
Iteration 49/1000 | Loss: 0.00002790
Iteration 50/1000 | Loss: 0.00002790
Iteration 51/1000 | Loss: 0.00002790
Iteration 52/1000 | Loss: 0.00002790
Iteration 53/1000 | Loss: 0.00002790
Iteration 54/1000 | Loss: 0.00002790
Iteration 55/1000 | Loss: 0.00002790
Iteration 56/1000 | Loss: 0.00002790
Iteration 57/1000 | Loss: 0.00002790
Iteration 58/1000 | Loss: 0.00002790
Iteration 59/1000 | Loss: 0.00002790
Iteration 60/1000 | Loss: 0.00002790
Iteration 61/1000 | Loss: 0.00002789
Iteration 62/1000 | Loss: 0.00002789
Iteration 63/1000 | Loss: 0.00002789
Iteration 64/1000 | Loss: 0.00002788
Iteration 65/1000 | Loss: 0.00002788
Iteration 66/1000 | Loss: 0.00002788
Iteration 67/1000 | Loss: 0.00002788
Iteration 68/1000 | Loss: 0.00002788
Iteration 69/1000 | Loss: 0.00002787
Iteration 70/1000 | Loss: 0.00002786
Iteration 71/1000 | Loss: 0.00002786
Iteration 72/1000 | Loss: 0.00002786
Iteration 73/1000 | Loss: 0.00002786
Iteration 74/1000 | Loss: 0.00002786
Iteration 75/1000 | Loss: 0.00002786
Iteration 76/1000 | Loss: 0.00002786
Iteration 77/1000 | Loss: 0.00002786
Iteration 78/1000 | Loss: 0.00002786
Iteration 79/1000 | Loss: 0.00002786
Iteration 80/1000 | Loss: 0.00002785
Iteration 81/1000 | Loss: 0.00002785
Iteration 82/1000 | Loss: 0.00002784
Iteration 83/1000 | Loss: 0.00002784
Iteration 84/1000 | Loss: 0.00002784
Iteration 85/1000 | Loss: 0.00002784
Iteration 86/1000 | Loss: 0.00002784
Iteration 87/1000 | Loss: 0.00002784
Iteration 88/1000 | Loss: 0.00002784
Iteration 89/1000 | Loss: 0.00002784
Iteration 90/1000 | Loss: 0.00002783
Iteration 91/1000 | Loss: 0.00002783
Iteration 92/1000 | Loss: 0.00002783
Iteration 93/1000 | Loss: 0.00002783
Iteration 94/1000 | Loss: 0.00002783
Iteration 95/1000 | Loss: 0.00002782
Iteration 96/1000 | Loss: 0.00002782
Iteration 97/1000 | Loss: 0.00002782
Iteration 98/1000 | Loss: 0.00002782
Iteration 99/1000 | Loss: 0.00002781
Iteration 100/1000 | Loss: 0.00002781
Iteration 101/1000 | Loss: 0.00002781
Iteration 102/1000 | Loss: 0.00002781
Iteration 103/1000 | Loss: 0.00002781
Iteration 104/1000 | Loss: 0.00002780
Iteration 105/1000 | Loss: 0.00002780
Iteration 106/1000 | Loss: 0.00002780
Iteration 107/1000 | Loss: 0.00002780
Iteration 108/1000 | Loss: 0.00002780
Iteration 109/1000 | Loss: 0.00002779
Iteration 110/1000 | Loss: 0.00002779
Iteration 111/1000 | Loss: 0.00002779
Iteration 112/1000 | Loss: 0.00002779
Iteration 113/1000 | Loss: 0.00002779
Iteration 114/1000 | Loss: 0.00002778
Iteration 115/1000 | Loss: 0.00002778
Iteration 116/1000 | Loss: 0.00002778
Iteration 117/1000 | Loss: 0.00002777
Iteration 118/1000 | Loss: 0.00002776
Iteration 119/1000 | Loss: 0.00002776
Iteration 120/1000 | Loss: 0.00002776
Iteration 121/1000 | Loss: 0.00002775
Iteration 122/1000 | Loss: 0.00002775
Iteration 123/1000 | Loss: 0.00002775
Iteration 124/1000 | Loss: 0.00002774
Iteration 125/1000 | Loss: 0.00002774
Iteration 126/1000 | Loss: 0.00002773
Iteration 127/1000 | Loss: 0.00002773
Iteration 128/1000 | Loss: 0.00002773
Iteration 129/1000 | Loss: 0.00002772
Iteration 130/1000 | Loss: 0.00002772
Iteration 131/1000 | Loss: 0.00002771
Iteration 132/1000 | Loss: 0.00002770
Iteration 133/1000 | Loss: 0.00002770
Iteration 134/1000 | Loss: 0.00002769
Iteration 135/1000 | Loss: 0.00002769
Iteration 136/1000 | Loss: 0.00002769
Iteration 137/1000 | Loss: 0.00002769
Iteration 138/1000 | Loss: 0.00002769
Iteration 139/1000 | Loss: 0.00002769
Iteration 140/1000 | Loss: 0.00002769
Iteration 141/1000 | Loss: 0.00002769
Iteration 142/1000 | Loss: 0.00002768
Iteration 143/1000 | Loss: 0.00002768
Iteration 144/1000 | Loss: 0.00002768
Iteration 145/1000 | Loss: 0.00002768
Iteration 146/1000 | Loss: 0.00002768
Iteration 147/1000 | Loss: 0.00002768
Iteration 148/1000 | Loss: 0.00002767
Iteration 149/1000 | Loss: 0.00002767
Iteration 150/1000 | Loss: 0.00002767
Iteration 151/1000 | Loss: 0.00002767
Iteration 152/1000 | Loss: 0.00002767
Iteration 153/1000 | Loss: 0.00002767
Iteration 154/1000 | Loss: 0.00002767
Iteration 155/1000 | Loss: 0.00002766
Iteration 156/1000 | Loss: 0.00002766
Iteration 157/1000 | Loss: 0.00002766
Iteration 158/1000 | Loss: 0.00002765
Iteration 159/1000 | Loss: 0.00002765
Iteration 160/1000 | Loss: 0.00002765
Iteration 161/1000 | Loss: 0.00002765
Iteration 162/1000 | Loss: 0.00002765
Iteration 163/1000 | Loss: 0.00002765
Iteration 164/1000 | Loss: 0.00002765
Iteration 165/1000 | Loss: 0.00002765
Iteration 166/1000 | Loss: 0.00002765
Iteration 167/1000 | Loss: 0.00002765
Iteration 168/1000 | Loss: 0.00002764
Iteration 169/1000 | Loss: 0.00002764
Iteration 170/1000 | Loss: 0.00002764
Iteration 171/1000 | Loss: 0.00002763
Iteration 172/1000 | Loss: 0.00002763
Iteration 173/1000 | Loss: 0.00002763
Iteration 174/1000 | Loss: 0.00002763
Iteration 175/1000 | Loss: 0.00002763
Iteration 176/1000 | Loss: 0.00002763
Iteration 177/1000 | Loss: 0.00002763
Iteration 178/1000 | Loss: 0.00002763
Iteration 179/1000 | Loss: 0.00002763
Iteration 180/1000 | Loss: 0.00002762
Iteration 181/1000 | Loss: 0.00002762
Iteration 182/1000 | Loss: 0.00002762
Iteration 183/1000 | Loss: 0.00002762
Iteration 184/1000 | Loss: 0.00002762
Iteration 185/1000 | Loss: 0.00002762
Iteration 186/1000 | Loss: 0.00002762
Iteration 187/1000 | Loss: 0.00002762
Iteration 188/1000 | Loss: 0.00002762
Iteration 189/1000 | Loss: 0.00002762
Iteration 190/1000 | Loss: 0.00002761
Iteration 191/1000 | Loss: 0.00002761
Iteration 192/1000 | Loss: 0.00002761
Iteration 193/1000 | Loss: 0.00002761
Iteration 194/1000 | Loss: 0.00002761
Iteration 195/1000 | Loss: 0.00002761
Iteration 196/1000 | Loss: 0.00002760
Iteration 197/1000 | Loss: 0.00002760
Iteration 198/1000 | Loss: 0.00002760
Iteration 199/1000 | Loss: 0.00002760
Iteration 200/1000 | Loss: 0.00002760
Iteration 201/1000 | Loss: 0.00002760
Iteration 202/1000 | Loss: 0.00002760
Iteration 203/1000 | Loss: 0.00002760
Iteration 204/1000 | Loss: 0.00002759
Iteration 205/1000 | Loss: 0.00002759
Iteration 206/1000 | Loss: 0.00002759
Iteration 207/1000 | Loss: 0.00002759
Iteration 208/1000 | Loss: 0.00002759
Iteration 209/1000 | Loss: 0.00002759
Iteration 210/1000 | Loss: 0.00002759
Iteration 211/1000 | Loss: 0.00002759
Iteration 212/1000 | Loss: 0.00002759
Iteration 213/1000 | Loss: 0.00002759
Iteration 214/1000 | Loss: 0.00002759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.7594600396696478e-05, 2.7594600396696478e-05, 2.7594600396696478e-05, 2.7594600396696478e-05, 2.7594600396696478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7594600396696478e-05

Optimization complete. Final v2v error: 4.430238723754883 mm

Highest mean error: 5.0836896896362305 mm for frame 179

Lowest mean error: 3.9604926109313965 mm for frame 145

Saving results

Total time: 54.8209753036499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936405
Iteration 2/25 | Loss: 0.00260681
Iteration 3/25 | Loss: 0.00204072
Iteration 4/25 | Loss: 0.00196348
Iteration 5/25 | Loss: 0.00186422
Iteration 6/25 | Loss: 0.00165949
Iteration 7/25 | Loss: 0.00156768
Iteration 8/25 | Loss: 0.00152568
Iteration 9/25 | Loss: 0.00149494
Iteration 10/25 | Loss: 0.00147022
Iteration 11/25 | Loss: 0.00144439
Iteration 12/25 | Loss: 0.00144036
Iteration 13/25 | Loss: 0.00143351
Iteration 14/25 | Loss: 0.00142795
Iteration 15/25 | Loss: 0.00142378
Iteration 16/25 | Loss: 0.00141534
Iteration 17/25 | Loss: 0.00142218
Iteration 18/25 | Loss: 0.00143168
Iteration 19/25 | Loss: 0.00143401
Iteration 20/25 | Loss: 0.00142444
Iteration 21/25 | Loss: 0.00142193
Iteration 22/25 | Loss: 0.00141388
Iteration 23/25 | Loss: 0.00141240
Iteration 24/25 | Loss: 0.00141166
Iteration 25/25 | Loss: 0.00141136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45878601
Iteration 2/25 | Loss: 0.00090302
Iteration 3/25 | Loss: 0.00090302
Iteration 4/25 | Loss: 0.00090302
Iteration 5/25 | Loss: 0.00090302
Iteration 6/25 | Loss: 0.00090302
Iteration 7/25 | Loss: 0.00090302
Iteration 8/25 | Loss: 0.00090302
Iteration 9/25 | Loss: 0.00090302
Iteration 10/25 | Loss: 0.00090302
Iteration 11/25 | Loss: 0.00090302
Iteration 12/25 | Loss: 0.00090302
Iteration 13/25 | Loss: 0.00090302
Iteration 14/25 | Loss: 0.00090302
Iteration 15/25 | Loss: 0.00090302
Iteration 16/25 | Loss: 0.00090302
Iteration 17/25 | Loss: 0.00090302
Iteration 18/25 | Loss: 0.00090302
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009030171204358339, 0.0009030171204358339, 0.0009030171204358339, 0.0009030171204358339, 0.0009030171204358339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009030171204358339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090302
Iteration 2/1000 | Loss: 0.00744645
Iteration 3/1000 | Loss: 0.00345163
Iteration 4/1000 | Loss: 0.00282971
Iteration 5/1000 | Loss: 0.00066703
Iteration 6/1000 | Loss: 0.00029000
Iteration 7/1000 | Loss: 0.00043045
Iteration 8/1000 | Loss: 0.00029155
Iteration 9/1000 | Loss: 0.00263001
Iteration 10/1000 | Loss: 0.00017172
Iteration 11/1000 | Loss: 0.00047064
Iteration 12/1000 | Loss: 0.00018574
Iteration 13/1000 | Loss: 0.00016855
Iteration 14/1000 | Loss: 0.00025648
Iteration 15/1000 | Loss: 0.00379367
Iteration 16/1000 | Loss: 0.00629880
Iteration 17/1000 | Loss: 0.00645559
Iteration 18/1000 | Loss: 0.00063632
Iteration 19/1000 | Loss: 0.00063950
Iteration 20/1000 | Loss: 0.00303372
Iteration 21/1000 | Loss: 0.00149797
Iteration 22/1000 | Loss: 0.00035022
Iteration 23/1000 | Loss: 0.00017115
Iteration 24/1000 | Loss: 0.00046007
Iteration 25/1000 | Loss: 0.00235189
Iteration 26/1000 | Loss: 0.00068176
Iteration 27/1000 | Loss: 0.00041404
Iteration 28/1000 | Loss: 0.00024623
Iteration 29/1000 | Loss: 0.00230480
Iteration 30/1000 | Loss: 0.00084195
Iteration 31/1000 | Loss: 0.00093903
Iteration 32/1000 | Loss: 0.00102964
Iteration 33/1000 | Loss: 0.00110777
Iteration 34/1000 | Loss: 0.00014556
Iteration 35/1000 | Loss: 0.00147116
Iteration 36/1000 | Loss: 0.00017809
Iteration 37/1000 | Loss: 0.00009406
Iteration 38/1000 | Loss: 0.00009329
Iteration 39/1000 | Loss: 0.00007743
Iteration 40/1000 | Loss: 0.00018600
Iteration 41/1000 | Loss: 0.00007916
Iteration 42/1000 | Loss: 0.00007159
Iteration 43/1000 | Loss: 0.00008978
Iteration 44/1000 | Loss: 0.00006730
Iteration 45/1000 | Loss: 0.00032708
Iteration 46/1000 | Loss: 0.00005527
Iteration 47/1000 | Loss: 0.00004895
Iteration 48/1000 | Loss: 0.00004256
Iteration 49/1000 | Loss: 0.00004057
Iteration 50/1000 | Loss: 0.00003915
Iteration 51/1000 | Loss: 0.00003763
Iteration 52/1000 | Loss: 0.00003594
Iteration 53/1000 | Loss: 0.00026695
Iteration 54/1000 | Loss: 0.00048252
Iteration 55/1000 | Loss: 0.00024817
Iteration 56/1000 | Loss: 0.00029748
Iteration 57/1000 | Loss: 0.00004624
Iteration 58/1000 | Loss: 0.00004052
Iteration 59/1000 | Loss: 0.00003636
Iteration 60/1000 | Loss: 0.00014116
Iteration 61/1000 | Loss: 0.00003627
Iteration 62/1000 | Loss: 0.00010738
Iteration 63/1000 | Loss: 0.00003479
Iteration 64/1000 | Loss: 0.00003381
Iteration 65/1000 | Loss: 0.00003313
Iteration 66/1000 | Loss: 0.00026886
Iteration 67/1000 | Loss: 0.00041442
Iteration 68/1000 | Loss: 0.00005157
Iteration 69/1000 | Loss: 0.00004549
Iteration 70/1000 | Loss: 0.00004093
Iteration 71/1000 | Loss: 0.00003721
Iteration 72/1000 | Loss: 0.00003504
Iteration 73/1000 | Loss: 0.00003402
Iteration 74/1000 | Loss: 0.00017332
Iteration 75/1000 | Loss: 0.00004145
Iteration 76/1000 | Loss: 0.00003637
Iteration 77/1000 | Loss: 0.00003437
Iteration 78/1000 | Loss: 0.00003230
Iteration 79/1000 | Loss: 0.00003121
Iteration 80/1000 | Loss: 0.00003078
Iteration 81/1000 | Loss: 0.00003061
Iteration 82/1000 | Loss: 0.00003060
Iteration 83/1000 | Loss: 0.00003047
Iteration 84/1000 | Loss: 0.00003044
Iteration 85/1000 | Loss: 0.00003033
Iteration 86/1000 | Loss: 0.00003033
Iteration 87/1000 | Loss: 0.00003033
Iteration 88/1000 | Loss: 0.00003024
Iteration 89/1000 | Loss: 0.00003024
Iteration 90/1000 | Loss: 0.00003023
Iteration 91/1000 | Loss: 0.00003022
Iteration 92/1000 | Loss: 0.00003021
Iteration 93/1000 | Loss: 0.00003021
Iteration 94/1000 | Loss: 0.00003021
Iteration 95/1000 | Loss: 0.00003020
Iteration 96/1000 | Loss: 0.00003020
Iteration 97/1000 | Loss: 0.00003020
Iteration 98/1000 | Loss: 0.00003019
Iteration 99/1000 | Loss: 0.00003019
Iteration 100/1000 | Loss: 0.00003018
Iteration 101/1000 | Loss: 0.00003017
Iteration 102/1000 | Loss: 0.00003016
Iteration 103/1000 | Loss: 0.00003016
Iteration 104/1000 | Loss: 0.00003016
Iteration 105/1000 | Loss: 0.00003016
Iteration 106/1000 | Loss: 0.00003016
Iteration 107/1000 | Loss: 0.00003016
Iteration 108/1000 | Loss: 0.00003016
Iteration 109/1000 | Loss: 0.00003016
Iteration 110/1000 | Loss: 0.00003016
Iteration 111/1000 | Loss: 0.00003015
Iteration 112/1000 | Loss: 0.00003015
Iteration 113/1000 | Loss: 0.00003015
Iteration 114/1000 | Loss: 0.00003015
Iteration 115/1000 | Loss: 0.00003015
Iteration 116/1000 | Loss: 0.00003014
Iteration 117/1000 | Loss: 0.00003014
Iteration 118/1000 | Loss: 0.00003014
Iteration 119/1000 | Loss: 0.00003014
Iteration 120/1000 | Loss: 0.00003014
Iteration 121/1000 | Loss: 0.00003014
Iteration 122/1000 | Loss: 0.00003014
Iteration 123/1000 | Loss: 0.00003013
Iteration 124/1000 | Loss: 0.00003013
Iteration 125/1000 | Loss: 0.00003013
Iteration 126/1000 | Loss: 0.00003012
Iteration 127/1000 | Loss: 0.00003012
Iteration 128/1000 | Loss: 0.00003012
Iteration 129/1000 | Loss: 0.00003011
Iteration 130/1000 | Loss: 0.00003011
Iteration 131/1000 | Loss: 0.00003011
Iteration 132/1000 | Loss: 0.00003011
Iteration 133/1000 | Loss: 0.00003011
Iteration 134/1000 | Loss: 0.00003011
Iteration 135/1000 | Loss: 0.00003011
Iteration 136/1000 | Loss: 0.00003010
Iteration 137/1000 | Loss: 0.00003010
Iteration 138/1000 | Loss: 0.00003010
Iteration 139/1000 | Loss: 0.00003007
Iteration 140/1000 | Loss: 0.00003007
Iteration 141/1000 | Loss: 0.00003007
Iteration 142/1000 | Loss: 0.00003007
Iteration 143/1000 | Loss: 0.00003007
Iteration 144/1000 | Loss: 0.00003007
Iteration 145/1000 | Loss: 0.00003007
Iteration 146/1000 | Loss: 0.00003007
Iteration 147/1000 | Loss: 0.00003007
Iteration 148/1000 | Loss: 0.00003007
Iteration 149/1000 | Loss: 0.00003007
Iteration 150/1000 | Loss: 0.00003007
Iteration 151/1000 | Loss: 0.00003006
Iteration 152/1000 | Loss: 0.00003006
Iteration 153/1000 | Loss: 0.00003006
Iteration 154/1000 | Loss: 0.00003006
Iteration 155/1000 | Loss: 0.00003006
Iteration 156/1000 | Loss: 0.00003006
Iteration 157/1000 | Loss: 0.00003006
Iteration 158/1000 | Loss: 0.00003006
Iteration 159/1000 | Loss: 0.00003005
Iteration 160/1000 | Loss: 0.00003005
Iteration 161/1000 | Loss: 0.00003005
Iteration 162/1000 | Loss: 0.00003005
Iteration 163/1000 | Loss: 0.00003005
Iteration 164/1000 | Loss: 0.00003005
Iteration 165/1000 | Loss: 0.00003005
Iteration 166/1000 | Loss: 0.00003005
Iteration 167/1000 | Loss: 0.00003004
Iteration 168/1000 | Loss: 0.00003004
Iteration 169/1000 | Loss: 0.00003003
Iteration 170/1000 | Loss: 0.00003003
Iteration 171/1000 | Loss: 0.00003003
Iteration 172/1000 | Loss: 0.00003002
Iteration 173/1000 | Loss: 0.00003002
Iteration 174/1000 | Loss: 0.00003002
Iteration 175/1000 | Loss: 0.00003001
Iteration 176/1000 | Loss: 0.00003001
Iteration 177/1000 | Loss: 0.00003001
Iteration 178/1000 | Loss: 0.00003001
Iteration 179/1000 | Loss: 0.00003000
Iteration 180/1000 | Loss: 0.00003000
Iteration 181/1000 | Loss: 0.00003000
Iteration 182/1000 | Loss: 0.00003000
Iteration 183/1000 | Loss: 0.00003000
Iteration 184/1000 | Loss: 0.00003000
Iteration 185/1000 | Loss: 0.00003000
Iteration 186/1000 | Loss: 0.00003000
Iteration 187/1000 | Loss: 0.00003000
Iteration 188/1000 | Loss: 0.00003000
Iteration 189/1000 | Loss: 0.00002999
Iteration 190/1000 | Loss: 0.00002999
Iteration 191/1000 | Loss: 0.00002999
Iteration 192/1000 | Loss: 0.00002999
Iteration 193/1000 | Loss: 0.00002999
Iteration 194/1000 | Loss: 0.00002999
Iteration 195/1000 | Loss: 0.00002999
Iteration 196/1000 | Loss: 0.00002999
Iteration 197/1000 | Loss: 0.00002999
Iteration 198/1000 | Loss: 0.00002999
Iteration 199/1000 | Loss: 0.00002999
Iteration 200/1000 | Loss: 0.00002999
Iteration 201/1000 | Loss: 0.00002999
Iteration 202/1000 | Loss: 0.00002999
Iteration 203/1000 | Loss: 0.00002999
Iteration 204/1000 | Loss: 0.00002998
Iteration 205/1000 | Loss: 0.00002998
Iteration 206/1000 | Loss: 0.00002998
Iteration 207/1000 | Loss: 0.00002998
Iteration 208/1000 | Loss: 0.00002998
Iteration 209/1000 | Loss: 0.00002997
Iteration 210/1000 | Loss: 0.00002997
Iteration 211/1000 | Loss: 0.00002997
Iteration 212/1000 | Loss: 0.00002997
Iteration 213/1000 | Loss: 0.00002997
Iteration 214/1000 | Loss: 0.00002997
Iteration 215/1000 | Loss: 0.00002997
Iteration 216/1000 | Loss: 0.00002997
Iteration 217/1000 | Loss: 0.00002997
Iteration 218/1000 | Loss: 0.00002997
Iteration 219/1000 | Loss: 0.00002997
Iteration 220/1000 | Loss: 0.00002997
Iteration 221/1000 | Loss: 0.00002997
Iteration 222/1000 | Loss: 0.00002997
Iteration 223/1000 | Loss: 0.00002997
Iteration 224/1000 | Loss: 0.00002996
Iteration 225/1000 | Loss: 0.00002996
Iteration 226/1000 | Loss: 0.00002996
Iteration 227/1000 | Loss: 0.00002996
Iteration 228/1000 | Loss: 0.00002996
Iteration 229/1000 | Loss: 0.00002996
Iteration 230/1000 | Loss: 0.00002996
Iteration 231/1000 | Loss: 0.00002996
Iteration 232/1000 | Loss: 0.00002996
Iteration 233/1000 | Loss: 0.00002996
Iteration 234/1000 | Loss: 0.00002996
Iteration 235/1000 | Loss: 0.00002996
Iteration 236/1000 | Loss: 0.00002996
Iteration 237/1000 | Loss: 0.00002996
Iteration 238/1000 | Loss: 0.00002996
Iteration 239/1000 | Loss: 0.00002996
Iteration 240/1000 | Loss: 0.00002996
Iteration 241/1000 | Loss: 0.00002996
Iteration 242/1000 | Loss: 0.00002996
Iteration 243/1000 | Loss: 0.00002996
Iteration 244/1000 | Loss: 0.00002996
Iteration 245/1000 | Loss: 0.00002996
Iteration 246/1000 | Loss: 0.00002996
Iteration 247/1000 | Loss: 0.00002996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.9964730856590904e-05, 2.9964730856590904e-05, 2.9964730856590904e-05, 2.9964730856590904e-05, 2.9964730856590904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9964730856590904e-05

Optimization complete. Final v2v error: 4.400045394897461 mm

Highest mean error: 6.954074382781982 mm for frame 68

Lowest mean error: 3.7381410598754883 mm for frame 124

Saving results

Total time: 165.33929562568665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406054
Iteration 2/25 | Loss: 0.00127211
Iteration 3/25 | Loss: 0.00119417
Iteration 4/25 | Loss: 0.00118086
Iteration 5/25 | Loss: 0.00117646
Iteration 6/25 | Loss: 0.00117624
Iteration 7/25 | Loss: 0.00117624
Iteration 8/25 | Loss: 0.00117624
Iteration 9/25 | Loss: 0.00117624
Iteration 10/25 | Loss: 0.00117624
Iteration 11/25 | Loss: 0.00117624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011762380599975586, 0.0011762380599975586, 0.0011762380599975586, 0.0011762380599975586, 0.0011762380599975586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011762380599975586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46885347
Iteration 2/25 | Loss: 0.00064483
Iteration 3/25 | Loss: 0.00064483
Iteration 4/25 | Loss: 0.00064483
Iteration 5/25 | Loss: 0.00064482
Iteration 6/25 | Loss: 0.00064482
Iteration 7/25 | Loss: 0.00064482
Iteration 8/25 | Loss: 0.00064482
Iteration 9/25 | Loss: 0.00064482
Iteration 10/25 | Loss: 0.00064482
Iteration 11/25 | Loss: 0.00064482
Iteration 12/25 | Loss: 0.00064482
Iteration 13/25 | Loss: 0.00064482
Iteration 14/25 | Loss: 0.00064482
Iteration 15/25 | Loss: 0.00064482
Iteration 16/25 | Loss: 0.00064482
Iteration 17/25 | Loss: 0.00064482
Iteration 18/25 | Loss: 0.00064482
Iteration 19/25 | Loss: 0.00064482
Iteration 20/25 | Loss: 0.00064482
Iteration 21/25 | Loss: 0.00064482
Iteration 22/25 | Loss: 0.00064482
Iteration 23/25 | Loss: 0.00064482
Iteration 24/25 | Loss: 0.00064482
Iteration 25/25 | Loss: 0.00064482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064482
Iteration 2/1000 | Loss: 0.00002431
Iteration 3/1000 | Loss: 0.00001608
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001202
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001197
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001174
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001172
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001171
Iteration 69/1000 | Loss: 0.00001171
Iteration 70/1000 | Loss: 0.00001171
Iteration 71/1000 | Loss: 0.00001171
Iteration 72/1000 | Loss: 0.00001170
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001170
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001168
Iteration 80/1000 | Loss: 0.00001168
Iteration 81/1000 | Loss: 0.00001168
Iteration 82/1000 | Loss: 0.00001168
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001167
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001167
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001165
Iteration 93/1000 | Loss: 0.00001165
Iteration 94/1000 | Loss: 0.00001165
Iteration 95/1000 | Loss: 0.00001164
Iteration 96/1000 | Loss: 0.00001164
Iteration 97/1000 | Loss: 0.00001164
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001163
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001162
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001158
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.157742281066021e-05, 1.157742281066021e-05, 1.157742281066021e-05, 1.157742281066021e-05, 1.157742281066021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.157742281066021e-05

Optimization complete. Final v2v error: 2.9090819358825684 mm

Highest mean error: 3.2805042266845703 mm for frame 97

Lowest mean error: 2.7325100898742676 mm for frame 4

Saving results

Total time: 38.68571949005127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485867
Iteration 2/25 | Loss: 0.00133670
Iteration 3/25 | Loss: 0.00126601
Iteration 4/25 | Loss: 0.00125988
Iteration 5/25 | Loss: 0.00125852
Iteration 6/25 | Loss: 0.00125852
Iteration 7/25 | Loss: 0.00125852
Iteration 8/25 | Loss: 0.00125852
Iteration 9/25 | Loss: 0.00125852
Iteration 10/25 | Loss: 0.00125852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012585246004164219, 0.0012585246004164219, 0.0012585246004164219, 0.0012585246004164219, 0.0012585246004164219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012585246004164219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46239197
Iteration 2/25 | Loss: 0.00066123
Iteration 3/25 | Loss: 0.00066121
Iteration 4/25 | Loss: 0.00066121
Iteration 5/25 | Loss: 0.00066121
Iteration 6/25 | Loss: 0.00066121
Iteration 7/25 | Loss: 0.00066121
Iteration 8/25 | Loss: 0.00066120
Iteration 9/25 | Loss: 0.00066120
Iteration 10/25 | Loss: 0.00066120
Iteration 11/25 | Loss: 0.00066120
Iteration 12/25 | Loss: 0.00066120
Iteration 13/25 | Loss: 0.00066120
Iteration 14/25 | Loss: 0.00066120
Iteration 15/25 | Loss: 0.00066120
Iteration 16/25 | Loss: 0.00066120
Iteration 17/25 | Loss: 0.00066120
Iteration 18/25 | Loss: 0.00066120
Iteration 19/25 | Loss: 0.00066120
Iteration 20/25 | Loss: 0.00066120
Iteration 21/25 | Loss: 0.00066120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006612036959268153, 0.0006612036959268153, 0.0006612036959268153, 0.0006612036959268153, 0.0006612036959268153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006612036959268153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066120
Iteration 2/1000 | Loss: 0.00004047
Iteration 3/1000 | Loss: 0.00002629
Iteration 4/1000 | Loss: 0.00002382
Iteration 5/1000 | Loss: 0.00002244
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002068
Iteration 9/1000 | Loss: 0.00002048
Iteration 10/1000 | Loss: 0.00002046
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00002008
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00002007
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002000
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001991
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001970
Iteration 22/1000 | Loss: 0.00001969
Iteration 23/1000 | Loss: 0.00001969
Iteration 24/1000 | Loss: 0.00001968
Iteration 25/1000 | Loss: 0.00001968
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001967
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001965
Iteration 31/1000 | Loss: 0.00001965
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001965
Iteration 34/1000 | Loss: 0.00001965
Iteration 35/1000 | Loss: 0.00001965
Iteration 36/1000 | Loss: 0.00001964
Iteration 37/1000 | Loss: 0.00001964
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001963
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001959
Iteration 44/1000 | Loss: 0.00001959
Iteration 45/1000 | Loss: 0.00001959
Iteration 46/1000 | Loss: 0.00001958
Iteration 47/1000 | Loss: 0.00001958
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001954
Iteration 52/1000 | Loss: 0.00001953
Iteration 53/1000 | Loss: 0.00001953
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001945
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001944
Iteration 79/1000 | Loss: 0.00001944
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001944
Iteration 83/1000 | Loss: 0.00001944
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001942
Iteration 87/1000 | Loss: 0.00001942
Iteration 88/1000 | Loss: 0.00001942
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001942
Iteration 91/1000 | Loss: 0.00001941
Iteration 92/1000 | Loss: 0.00001941
Iteration 93/1000 | Loss: 0.00001940
Iteration 94/1000 | Loss: 0.00001940
Iteration 95/1000 | Loss: 0.00001939
Iteration 96/1000 | Loss: 0.00001939
Iteration 97/1000 | Loss: 0.00001939
Iteration 98/1000 | Loss: 0.00001938
Iteration 99/1000 | Loss: 0.00001938
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001936
Iteration 103/1000 | Loss: 0.00001936
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001935
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001935
Iteration 108/1000 | Loss: 0.00001935
Iteration 109/1000 | Loss: 0.00001934
Iteration 110/1000 | Loss: 0.00001934
Iteration 111/1000 | Loss: 0.00001934
Iteration 112/1000 | Loss: 0.00001933
Iteration 113/1000 | Loss: 0.00001933
Iteration 114/1000 | Loss: 0.00001933
Iteration 115/1000 | Loss: 0.00001933
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001932
Iteration 118/1000 | Loss: 0.00001932
Iteration 119/1000 | Loss: 0.00001932
Iteration 120/1000 | Loss: 0.00001932
Iteration 121/1000 | Loss: 0.00001932
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001931
Iteration 125/1000 | Loss: 0.00001930
Iteration 126/1000 | Loss: 0.00001930
Iteration 127/1000 | Loss: 0.00001930
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001926
Iteration 144/1000 | Loss: 0.00001926
Iteration 145/1000 | Loss: 0.00001926
Iteration 146/1000 | Loss: 0.00001926
Iteration 147/1000 | Loss: 0.00001925
Iteration 148/1000 | Loss: 0.00001925
Iteration 149/1000 | Loss: 0.00001925
Iteration 150/1000 | Loss: 0.00001925
Iteration 151/1000 | Loss: 0.00001925
Iteration 152/1000 | Loss: 0.00001925
Iteration 153/1000 | Loss: 0.00001925
Iteration 154/1000 | Loss: 0.00001925
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001924
Iteration 157/1000 | Loss: 0.00001924
Iteration 158/1000 | Loss: 0.00001924
Iteration 159/1000 | Loss: 0.00001924
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001924
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001923
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001922
Iteration 171/1000 | Loss: 0.00001922
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001922
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001921
Iteration 178/1000 | Loss: 0.00001921
Iteration 179/1000 | Loss: 0.00001921
Iteration 180/1000 | Loss: 0.00001921
Iteration 181/1000 | Loss: 0.00001921
Iteration 182/1000 | Loss: 0.00001921
Iteration 183/1000 | Loss: 0.00001921
Iteration 184/1000 | Loss: 0.00001921
Iteration 185/1000 | Loss: 0.00001921
Iteration 186/1000 | Loss: 0.00001921
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.9209970560041256e-05, 1.9209970560041256e-05, 1.9209970560041256e-05, 1.9209970560041256e-05, 1.9209970560041256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9209970560041256e-05

Optimization complete. Final v2v error: 3.5555906295776367 mm

Highest mean error: 4.141537189483643 mm for frame 58

Lowest mean error: 3.2817904949188232 mm for frame 11

Saving results

Total time: 44.86163306236267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082604
Iteration 2/25 | Loss: 0.00234031
Iteration 3/25 | Loss: 0.00175357
Iteration 4/25 | Loss: 0.00183536
Iteration 5/25 | Loss: 0.00182772
Iteration 6/25 | Loss: 0.00186274
Iteration 7/25 | Loss: 0.00176722
Iteration 8/25 | Loss: 0.00167611
Iteration 9/25 | Loss: 0.00177204
Iteration 10/25 | Loss: 0.00188402
Iteration 11/25 | Loss: 0.00180359
Iteration 12/25 | Loss: 0.00167328
Iteration 13/25 | Loss: 0.00154687
Iteration 14/25 | Loss: 0.00151668
Iteration 15/25 | Loss: 0.00147989
Iteration 16/25 | Loss: 0.00143490
Iteration 17/25 | Loss: 0.00141730
Iteration 18/25 | Loss: 0.00140283
Iteration 19/25 | Loss: 0.00138661
Iteration 20/25 | Loss: 0.00137184
Iteration 21/25 | Loss: 0.00136889
Iteration 22/25 | Loss: 0.00135800
Iteration 23/25 | Loss: 0.00135613
Iteration 24/25 | Loss: 0.00134361
Iteration 25/25 | Loss: 0.00133948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30847359
Iteration 2/25 | Loss: 0.00114411
Iteration 3/25 | Loss: 0.00096727
Iteration 4/25 | Loss: 0.00096726
Iteration 5/25 | Loss: 0.00096726
Iteration 6/25 | Loss: 0.00096726
Iteration 7/25 | Loss: 0.00096726
Iteration 8/25 | Loss: 0.00096726
Iteration 9/25 | Loss: 0.00096726
Iteration 10/25 | Loss: 0.00096726
Iteration 11/25 | Loss: 0.00096726
Iteration 12/25 | Loss: 0.00096726
Iteration 13/25 | Loss: 0.00096726
Iteration 14/25 | Loss: 0.00096726
Iteration 15/25 | Loss: 0.00096726
Iteration 16/25 | Loss: 0.00096726
Iteration 17/25 | Loss: 0.00096726
Iteration 18/25 | Loss: 0.00096726
Iteration 19/25 | Loss: 0.00096726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000967263535130769, 0.000967263535130769, 0.000967263535130769, 0.000967263535130769, 0.000967263535130769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000967263535130769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096726
Iteration 2/1000 | Loss: 0.00034872
Iteration 3/1000 | Loss: 0.00054746
Iteration 4/1000 | Loss: 0.00027985
Iteration 5/1000 | Loss: 0.00020549
Iteration 6/1000 | Loss: 0.00017690
Iteration 7/1000 | Loss: 0.00022745
Iteration 8/1000 | Loss: 0.00021707
Iteration 9/1000 | Loss: 0.00018419
Iteration 10/1000 | Loss: 0.00020045
Iteration 11/1000 | Loss: 0.00022205
Iteration 12/1000 | Loss: 0.00018589
Iteration 13/1000 | Loss: 0.00015704
Iteration 14/1000 | Loss: 0.00022997
Iteration 15/1000 | Loss: 0.00017061
Iteration 16/1000 | Loss: 0.00014165
Iteration 17/1000 | Loss: 0.00020423
Iteration 18/1000 | Loss: 0.00025943
Iteration 19/1000 | Loss: 0.00023696
Iteration 20/1000 | Loss: 0.00019678
Iteration 21/1000 | Loss: 0.00023563
Iteration 22/1000 | Loss: 0.00021130
Iteration 23/1000 | Loss: 0.00024540
Iteration 24/1000 | Loss: 0.00023612
Iteration 25/1000 | Loss: 0.00023151
Iteration 26/1000 | Loss: 0.00023695
Iteration 27/1000 | Loss: 0.00024548
Iteration 28/1000 | Loss: 0.00022263
Iteration 29/1000 | Loss: 0.00022818
Iteration 30/1000 | Loss: 0.00029152
Iteration 31/1000 | Loss: 0.00028280
Iteration 32/1000 | Loss: 0.00026758
Iteration 33/1000 | Loss: 0.00033032
Iteration 34/1000 | Loss: 0.00028948
Iteration 35/1000 | Loss: 0.00027589
Iteration 36/1000 | Loss: 0.00026910
Iteration 37/1000 | Loss: 0.00038798
Iteration 38/1000 | Loss: 0.00025057
Iteration 39/1000 | Loss: 0.00026139
Iteration 40/1000 | Loss: 0.00025256
Iteration 41/1000 | Loss: 0.00028364
Iteration 42/1000 | Loss: 0.00027085
Iteration 43/1000 | Loss: 0.00027119
Iteration 44/1000 | Loss: 0.00026869
Iteration 45/1000 | Loss: 0.00024666
Iteration 46/1000 | Loss: 0.00075254
Iteration 47/1000 | Loss: 0.00102746
Iteration 48/1000 | Loss: 0.00029880
Iteration 49/1000 | Loss: 0.00018496
Iteration 50/1000 | Loss: 0.00014758
Iteration 51/1000 | Loss: 0.00019244
Iteration 52/1000 | Loss: 0.00017923
Iteration 53/1000 | Loss: 0.00048524
Iteration 54/1000 | Loss: 0.00055589
Iteration 55/1000 | Loss: 0.00018183
Iteration 56/1000 | Loss: 0.00018160
Iteration 57/1000 | Loss: 0.00017580
Iteration 58/1000 | Loss: 0.00024803
Iteration 59/1000 | Loss: 0.00022255
Iteration 60/1000 | Loss: 0.00023651
Iteration 61/1000 | Loss: 0.00023162
Iteration 62/1000 | Loss: 0.00024810
Iteration 63/1000 | Loss: 0.00022707
Iteration 64/1000 | Loss: 0.00021745
Iteration 65/1000 | Loss: 0.00014476
Iteration 66/1000 | Loss: 0.00005850
Iteration 67/1000 | Loss: 0.00004698
Iteration 68/1000 | Loss: 0.00017839
Iteration 69/1000 | Loss: 0.00029112
Iteration 70/1000 | Loss: 0.00027780
Iteration 71/1000 | Loss: 0.00022449
Iteration 72/1000 | Loss: 0.00023853
Iteration 73/1000 | Loss: 0.00020245
Iteration 74/1000 | Loss: 0.00027964
Iteration 75/1000 | Loss: 0.00023389
Iteration 76/1000 | Loss: 0.00020115
Iteration 77/1000 | Loss: 0.00023765
Iteration 78/1000 | Loss: 0.00017985
Iteration 79/1000 | Loss: 0.00020582
Iteration 80/1000 | Loss: 0.00018654
Iteration 81/1000 | Loss: 0.00023445
Iteration 82/1000 | Loss: 0.00022700
Iteration 83/1000 | Loss: 0.00016141
Iteration 84/1000 | Loss: 0.00013685
Iteration 85/1000 | Loss: 0.00023303
Iteration 86/1000 | Loss: 0.00022382
Iteration 87/1000 | Loss: 0.00024187
Iteration 88/1000 | Loss: 0.00023735
Iteration 89/1000 | Loss: 0.00020704
Iteration 90/1000 | Loss: 0.00018987
Iteration 91/1000 | Loss: 0.00018770
Iteration 92/1000 | Loss: 0.00019460
Iteration 93/1000 | Loss: 0.00023032
Iteration 94/1000 | Loss: 0.00025521
Iteration 95/1000 | Loss: 0.00022679
Iteration 96/1000 | Loss: 0.00025256
Iteration 97/1000 | Loss: 0.00022107
Iteration 98/1000 | Loss: 0.00020124
Iteration 99/1000 | Loss: 0.00024875
Iteration 100/1000 | Loss: 0.00022996
Iteration 101/1000 | Loss: 0.00017922
Iteration 102/1000 | Loss: 0.00014213
Iteration 103/1000 | Loss: 0.00022051
Iteration 104/1000 | Loss: 0.00021654
Iteration 105/1000 | Loss: 0.00022393
Iteration 106/1000 | Loss: 0.00023537
Iteration 107/1000 | Loss: 0.00017959
Iteration 108/1000 | Loss: 0.00015681
Iteration 109/1000 | Loss: 0.00019687
Iteration 110/1000 | Loss: 0.00024246
Iteration 111/1000 | Loss: 0.00028026
Iteration 112/1000 | Loss: 0.00023336
Iteration 113/1000 | Loss: 0.00017294
Iteration 114/1000 | Loss: 0.00015687
Iteration 115/1000 | Loss: 0.00022796
Iteration 116/1000 | Loss: 0.00026316
Iteration 117/1000 | Loss: 0.00023038
Iteration 118/1000 | Loss: 0.00012729
Iteration 119/1000 | Loss: 0.00017293
Iteration 120/1000 | Loss: 0.00014771
Iteration 121/1000 | Loss: 0.00010850
Iteration 122/1000 | Loss: 0.00013702
Iteration 123/1000 | Loss: 0.00012454
Iteration 124/1000 | Loss: 0.00012577
Iteration 125/1000 | Loss: 0.00015152
Iteration 126/1000 | Loss: 0.00015924
Iteration 127/1000 | Loss: 0.00008284
Iteration 128/1000 | Loss: 0.00013288
Iteration 129/1000 | Loss: 0.00015712
Iteration 130/1000 | Loss: 0.00016321
Iteration 131/1000 | Loss: 0.00019011
Iteration 132/1000 | Loss: 0.00016981
Iteration 133/1000 | Loss: 0.00011965
Iteration 134/1000 | Loss: 0.00011485
Iteration 135/1000 | Loss: 0.00013635
Iteration 136/1000 | Loss: 0.00016721
Iteration 137/1000 | Loss: 0.00015305
Iteration 138/1000 | Loss: 0.00018842
Iteration 139/1000 | Loss: 0.00017485
Iteration 140/1000 | Loss: 0.00018542
Iteration 141/1000 | Loss: 0.00018698
Iteration 142/1000 | Loss: 0.00019304
Iteration 143/1000 | Loss: 0.00018671
Iteration 144/1000 | Loss: 0.00016419
Iteration 145/1000 | Loss: 0.00020197
Iteration 146/1000 | Loss: 0.00024966
Iteration 147/1000 | Loss: 0.00022178
Iteration 148/1000 | Loss: 0.00024269
Iteration 149/1000 | Loss: 0.00012929
Iteration 150/1000 | Loss: 0.00020863
Iteration 151/1000 | Loss: 0.00017610
Iteration 152/1000 | Loss: 0.00015593
Iteration 153/1000 | Loss: 0.00022226
Iteration 154/1000 | Loss: 0.00022036
Iteration 155/1000 | Loss: 0.00021092
Iteration 156/1000 | Loss: 0.00022423
Iteration 157/1000 | Loss: 0.00011689
Iteration 158/1000 | Loss: 0.00020152
Iteration 159/1000 | Loss: 0.00017566
Iteration 160/1000 | Loss: 0.00019610
Iteration 161/1000 | Loss: 0.00020488
Iteration 162/1000 | Loss: 0.00027776
Iteration 163/1000 | Loss: 0.00020665
Iteration 164/1000 | Loss: 0.00023802
Iteration 165/1000 | Loss: 0.00021242
Iteration 166/1000 | Loss: 0.00017940
Iteration 167/1000 | Loss: 0.00018335
Iteration 168/1000 | Loss: 0.00022334
Iteration 169/1000 | Loss: 0.00021644
Iteration 170/1000 | Loss: 0.00020075
Iteration 171/1000 | Loss: 0.00019291
Iteration 172/1000 | Loss: 0.00021995
Iteration 173/1000 | Loss: 0.00023735
Iteration 174/1000 | Loss: 0.00025271
Iteration 175/1000 | Loss: 0.00023945
Iteration 176/1000 | Loss: 0.00026403
Iteration 177/1000 | Loss: 0.00023834
Iteration 178/1000 | Loss: 0.00025395
Iteration 179/1000 | Loss: 0.00025156
Iteration 180/1000 | Loss: 0.00024450
Iteration 181/1000 | Loss: 0.00024013
Iteration 182/1000 | Loss: 0.00024140
Iteration 183/1000 | Loss: 0.00024185
Iteration 184/1000 | Loss: 0.00027330
Iteration 185/1000 | Loss: 0.00021007
Iteration 186/1000 | Loss: 0.00024829
Iteration 187/1000 | Loss: 0.00020465
Iteration 188/1000 | Loss: 0.00024840
Iteration 189/1000 | Loss: 0.00026280
Iteration 190/1000 | Loss: 0.00026009
Iteration 191/1000 | Loss: 0.00023669
Iteration 192/1000 | Loss: 0.00027860
Iteration 193/1000 | Loss: 0.00023884
Iteration 194/1000 | Loss: 0.00025332
Iteration 195/1000 | Loss: 0.00021018
Iteration 196/1000 | Loss: 0.00024019
Iteration 197/1000 | Loss: 0.00023003
Iteration 198/1000 | Loss: 0.00025666
Iteration 199/1000 | Loss: 0.00024070
Iteration 200/1000 | Loss: 0.00025691
Iteration 201/1000 | Loss: 0.00026741
Iteration 202/1000 | Loss: 0.00028800
Iteration 203/1000 | Loss: 0.00028340
Iteration 204/1000 | Loss: 0.00024619
Iteration 205/1000 | Loss: 0.00024997
Iteration 206/1000 | Loss: 0.00017937
Iteration 207/1000 | Loss: 0.00010993
Iteration 208/1000 | Loss: 0.00014149
Iteration 209/1000 | Loss: 0.00013954
Iteration 210/1000 | Loss: 0.00013499
Iteration 211/1000 | Loss: 0.00014887
Iteration 212/1000 | Loss: 0.00010713
Iteration 213/1000 | Loss: 0.00011267
Iteration 214/1000 | Loss: 0.00014502
Iteration 215/1000 | Loss: 0.00013727
Iteration 216/1000 | Loss: 0.00013588
Iteration 217/1000 | Loss: 0.00015291
Iteration 218/1000 | Loss: 0.00012581
Iteration 219/1000 | Loss: 0.00015315
Iteration 220/1000 | Loss: 0.00018985
Iteration 221/1000 | Loss: 0.00011965
Iteration 222/1000 | Loss: 0.00012935
Iteration 223/1000 | Loss: 0.00013391
Iteration 224/1000 | Loss: 0.00013806
Iteration 225/1000 | Loss: 0.00015491
Iteration 226/1000 | Loss: 0.00009322
Iteration 227/1000 | Loss: 0.00007775
Iteration 228/1000 | Loss: 0.00014571
Iteration 229/1000 | Loss: 0.00011779
Iteration 230/1000 | Loss: 0.00016576
Iteration 231/1000 | Loss: 0.00014948
Iteration 232/1000 | Loss: 0.00020113
Iteration 233/1000 | Loss: 0.00022236
Iteration 234/1000 | Loss: 0.00009078
Iteration 235/1000 | Loss: 0.00009227
Iteration 236/1000 | Loss: 0.00013055
Iteration 237/1000 | Loss: 0.00015425
Iteration 238/1000 | Loss: 0.00008973
Iteration 239/1000 | Loss: 0.00013281
Iteration 240/1000 | Loss: 0.00014406
Iteration 241/1000 | Loss: 0.00015687
Iteration 242/1000 | Loss: 0.00016942
Iteration 243/1000 | Loss: 0.00011653
Iteration 244/1000 | Loss: 0.00020635
Iteration 245/1000 | Loss: 0.00013878
Iteration 246/1000 | Loss: 0.00008479
Iteration 247/1000 | Loss: 0.00008680
Iteration 248/1000 | Loss: 0.00017326
Iteration 249/1000 | Loss: 0.00016263
Iteration 250/1000 | Loss: 0.00013819
Iteration 251/1000 | Loss: 0.00014780
Iteration 252/1000 | Loss: 0.00016608
Iteration 253/1000 | Loss: 0.00014853
Iteration 254/1000 | Loss: 0.00016127
Iteration 255/1000 | Loss: 0.00012472
Iteration 256/1000 | Loss: 0.00016103
Iteration 257/1000 | Loss: 0.00017390
Iteration 258/1000 | Loss: 0.00014372
Iteration 259/1000 | Loss: 0.00014719
Iteration 260/1000 | Loss: 0.00016003
Iteration 261/1000 | Loss: 0.00015252
Iteration 262/1000 | Loss: 0.00014630
Iteration 263/1000 | Loss: 0.00016914
Iteration 264/1000 | Loss: 0.00015856
Iteration 265/1000 | Loss: 0.00014004
Iteration 266/1000 | Loss: 0.00016016
Iteration 267/1000 | Loss: 0.00015520
Iteration 268/1000 | Loss: 0.00016011
Iteration 269/1000 | Loss: 0.00013697
Iteration 270/1000 | Loss: 0.00015150
Iteration 271/1000 | Loss: 0.00012898
Iteration 272/1000 | Loss: 0.00013824
Iteration 273/1000 | Loss: 0.00014635
Iteration 274/1000 | Loss: 0.00018575
Iteration 275/1000 | Loss: 0.00028823
Iteration 276/1000 | Loss: 0.00018573
Iteration 277/1000 | Loss: 0.00020848
Iteration 278/1000 | Loss: 0.00016070
Iteration 279/1000 | Loss: 0.00012389
Iteration 280/1000 | Loss: 0.00016131
Iteration 281/1000 | Loss: 0.00017816
Iteration 282/1000 | Loss: 0.00016653
Iteration 283/1000 | Loss: 0.00020850
Iteration 284/1000 | Loss: 0.00018637
Iteration 285/1000 | Loss: 0.00061709
Iteration 286/1000 | Loss: 0.00017179
Iteration 287/1000 | Loss: 0.00021051
Iteration 288/1000 | Loss: 0.00017112
Iteration 289/1000 | Loss: 0.00018568
Iteration 290/1000 | Loss: 0.00014320
Iteration 291/1000 | Loss: 0.00019694
Iteration 292/1000 | Loss: 0.00013873
Iteration 293/1000 | Loss: 0.00015734
Iteration 294/1000 | Loss: 0.00018053
Iteration 295/1000 | Loss: 0.00017300
Iteration 296/1000 | Loss: 0.00018847
Iteration 297/1000 | Loss: 0.00018126
Iteration 298/1000 | Loss: 0.00017464
Iteration 299/1000 | Loss: 0.00017312
Iteration 300/1000 | Loss: 0.00013624
Iteration 301/1000 | Loss: 0.00023358
Iteration 302/1000 | Loss: 0.00015848
Iteration 303/1000 | Loss: 0.00017099
Iteration 304/1000 | Loss: 0.00018995
Iteration 305/1000 | Loss: 0.00021400
Iteration 306/1000 | Loss: 0.00018630
Iteration 307/1000 | Loss: 0.00023730
Iteration 308/1000 | Loss: 0.00015242
Iteration 309/1000 | Loss: 0.00019877
Iteration 310/1000 | Loss: 0.00017362
Iteration 311/1000 | Loss: 0.00018366
Iteration 312/1000 | Loss: 0.00017553
Iteration 313/1000 | Loss: 0.00017986
Iteration 314/1000 | Loss: 0.00017486
Iteration 315/1000 | Loss: 0.00017663
Iteration 316/1000 | Loss: 0.00016566
Iteration 317/1000 | Loss: 0.00017766
Iteration 318/1000 | Loss: 0.00015582
Iteration 319/1000 | Loss: 0.00018218
Iteration 320/1000 | Loss: 0.00016767
Iteration 321/1000 | Loss: 0.00017969
Iteration 322/1000 | Loss: 0.00017145
Iteration 323/1000 | Loss: 0.00019840
Iteration 324/1000 | Loss: 0.00014398
Iteration 325/1000 | Loss: 0.00019446
Iteration 326/1000 | Loss: 0.00018686
Iteration 327/1000 | Loss: 0.00018640
Iteration 328/1000 | Loss: 0.00018034
Iteration 329/1000 | Loss: 0.00017510
Iteration 330/1000 | Loss: 0.00018063
Iteration 331/1000 | Loss: 0.00019623
Iteration 332/1000 | Loss: 0.00017501
Iteration 333/1000 | Loss: 0.00018810
Iteration 334/1000 | Loss: 0.00018175
Iteration 335/1000 | Loss: 0.00019353
Iteration 336/1000 | Loss: 0.00017919
Iteration 337/1000 | Loss: 0.00018773
Iteration 338/1000 | Loss: 0.00018209
Iteration 339/1000 | Loss: 0.00017522
Iteration 340/1000 | Loss: 0.00016851
Iteration 341/1000 | Loss: 0.00019078
Iteration 342/1000 | Loss: 0.00012124
Iteration 343/1000 | Loss: 0.00032041
Iteration 344/1000 | Loss: 0.00020681
Iteration 345/1000 | Loss: 0.00010919
Iteration 346/1000 | Loss: 0.00009949
Iteration 347/1000 | Loss: 0.00010159
Iteration 348/1000 | Loss: 0.00007975
Iteration 349/1000 | Loss: 0.00016067
Iteration 350/1000 | Loss: 0.00016352
Iteration 351/1000 | Loss: 0.00014827
Iteration 352/1000 | Loss: 0.00015744
Iteration 353/1000 | Loss: 0.00016860
Iteration 354/1000 | Loss: 0.00019486
Iteration 355/1000 | Loss: 0.00011454
Iteration 356/1000 | Loss: 0.00015980
Iteration 357/1000 | Loss: 0.00009113
Iteration 358/1000 | Loss: 0.00013546
Iteration 359/1000 | Loss: 0.00020812
Iteration 360/1000 | Loss: 0.00010427
Iteration 361/1000 | Loss: 0.00022100
Iteration 362/1000 | Loss: 0.00010702
Iteration 363/1000 | Loss: 0.00016552
Iteration 364/1000 | Loss: 0.00012111
Iteration 365/1000 | Loss: 0.00040934
Iteration 366/1000 | Loss: 0.00012990
Iteration 367/1000 | Loss: 0.00025614
Iteration 368/1000 | Loss: 0.00018187
Iteration 369/1000 | Loss: 0.00019361
Iteration 370/1000 | Loss: 0.00012884
Iteration 371/1000 | Loss: 0.00015692
Iteration 372/1000 | Loss: 0.00013733
Iteration 373/1000 | Loss: 0.00015472
Iteration 374/1000 | Loss: 0.00012992
Iteration 375/1000 | Loss: 0.00015697
Iteration 376/1000 | Loss: 0.00013306
Iteration 377/1000 | Loss: 0.00007207
Iteration 378/1000 | Loss: 0.00009750
Iteration 379/1000 | Loss: 0.00008950
Iteration 380/1000 | Loss: 0.00004284
Iteration 381/1000 | Loss: 0.00003709
Iteration 382/1000 | Loss: 0.00003174
Iteration 383/1000 | Loss: 0.00002719
Iteration 384/1000 | Loss: 0.00002482
Iteration 385/1000 | Loss: 0.00002300
Iteration 386/1000 | Loss: 0.00002211
Iteration 387/1000 | Loss: 0.00002167
Iteration 388/1000 | Loss: 0.00002127
Iteration 389/1000 | Loss: 0.00002099
Iteration 390/1000 | Loss: 0.00002068
Iteration 391/1000 | Loss: 0.00002039
Iteration 392/1000 | Loss: 0.00002021
Iteration 393/1000 | Loss: 0.00002011
Iteration 394/1000 | Loss: 0.00001997
Iteration 395/1000 | Loss: 0.00001977
Iteration 396/1000 | Loss: 0.00001964
Iteration 397/1000 | Loss: 0.00001961
Iteration 398/1000 | Loss: 0.00001955
Iteration 399/1000 | Loss: 0.00001955
Iteration 400/1000 | Loss: 0.00001954
Iteration 401/1000 | Loss: 0.00001952
Iteration 402/1000 | Loss: 0.00001952
Iteration 403/1000 | Loss: 0.00001948
Iteration 404/1000 | Loss: 0.00001945
Iteration 405/1000 | Loss: 0.00001944
Iteration 406/1000 | Loss: 0.00001944
Iteration 407/1000 | Loss: 0.00001944
Iteration 408/1000 | Loss: 0.00001943
Iteration 409/1000 | Loss: 0.00001943
Iteration 410/1000 | Loss: 0.00001942
Iteration 411/1000 | Loss: 0.00001938
Iteration 412/1000 | Loss: 0.00001938
Iteration 413/1000 | Loss: 0.00001933
Iteration 414/1000 | Loss: 0.00001931
Iteration 415/1000 | Loss: 0.00001931
Iteration 416/1000 | Loss: 0.00001930
Iteration 417/1000 | Loss: 0.00001930
Iteration 418/1000 | Loss: 0.00001930
Iteration 419/1000 | Loss: 0.00001929
Iteration 420/1000 | Loss: 0.00001929
Iteration 421/1000 | Loss: 0.00001928
Iteration 422/1000 | Loss: 0.00001928
Iteration 423/1000 | Loss: 0.00001928
Iteration 424/1000 | Loss: 0.00001928
Iteration 425/1000 | Loss: 0.00001928
Iteration 426/1000 | Loss: 0.00001928
Iteration 427/1000 | Loss: 0.00001928
Iteration 428/1000 | Loss: 0.00001928
Iteration 429/1000 | Loss: 0.00001928
Iteration 430/1000 | Loss: 0.00001928
Iteration 431/1000 | Loss: 0.00001927
Iteration 432/1000 | Loss: 0.00001927
Iteration 433/1000 | Loss: 0.00001927
Iteration 434/1000 | Loss: 0.00001927
Iteration 435/1000 | Loss: 0.00001927
Iteration 436/1000 | Loss: 0.00001926
Iteration 437/1000 | Loss: 0.00001926
Iteration 438/1000 | Loss: 0.00001926
Iteration 439/1000 | Loss: 0.00001926
Iteration 440/1000 | Loss: 0.00001926
Iteration 441/1000 | Loss: 0.00001926
Iteration 442/1000 | Loss: 0.00001926
Iteration 443/1000 | Loss: 0.00001926
Iteration 444/1000 | Loss: 0.00001926
Iteration 445/1000 | Loss: 0.00001926
Iteration 446/1000 | Loss: 0.00001926
Iteration 447/1000 | Loss: 0.00001926
Iteration 448/1000 | Loss: 0.00001926
Iteration 449/1000 | Loss: 0.00001926
Iteration 450/1000 | Loss: 0.00001926
Iteration 451/1000 | Loss: 0.00001926
Iteration 452/1000 | Loss: 0.00001926
Iteration 453/1000 | Loss: 0.00001926
Iteration 454/1000 | Loss: 0.00001926
Iteration 455/1000 | Loss: 0.00001926
Iteration 456/1000 | Loss: 0.00001926
Iteration 457/1000 | Loss: 0.00001926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 457. Stopping optimization.
Last 5 losses: [1.9262761270510964e-05, 1.9262761270510964e-05, 1.9262761270510964e-05, 1.9262761270510964e-05, 1.9262761270510964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9262761270510964e-05

Optimization complete. Final v2v error: 3.6990749835968018 mm

Highest mean error: 4.118233680725098 mm for frame 64

Lowest mean error: 3.5773792266845703 mm for frame 35

Saving results

Total time: 583.3511319160461
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407907
Iteration 2/25 | Loss: 0.00128186
Iteration 3/25 | Loss: 0.00120622
Iteration 4/25 | Loss: 0.00120135
Iteration 5/25 | Loss: 0.00120135
Iteration 6/25 | Loss: 0.00120135
Iteration 7/25 | Loss: 0.00120135
Iteration 8/25 | Loss: 0.00120135
Iteration 9/25 | Loss: 0.00120135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012013493105769157, 0.0012013493105769157, 0.0012013493105769157, 0.0012013493105769157, 0.0012013493105769157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012013493105769157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70848548
Iteration 2/25 | Loss: 0.00057004
Iteration 3/25 | Loss: 0.00057004
Iteration 4/25 | Loss: 0.00057004
Iteration 5/25 | Loss: 0.00057004
Iteration 6/25 | Loss: 0.00057004
Iteration 7/25 | Loss: 0.00057004
Iteration 8/25 | Loss: 0.00057004
Iteration 9/25 | Loss: 0.00057004
Iteration 10/25 | Loss: 0.00057004
Iteration 11/25 | Loss: 0.00057004
Iteration 12/25 | Loss: 0.00057004
Iteration 13/25 | Loss: 0.00057004
Iteration 14/25 | Loss: 0.00057004
Iteration 15/25 | Loss: 0.00057004
Iteration 16/25 | Loss: 0.00057004
Iteration 17/25 | Loss: 0.00057004
Iteration 18/25 | Loss: 0.00057004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005700409528799355, 0.0005700409528799355, 0.0005700409528799355, 0.0005700409528799355, 0.0005700409528799355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005700409528799355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057004
Iteration 2/1000 | Loss: 0.00002684
Iteration 3/1000 | Loss: 0.00001813
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001489
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001396
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001323
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001298
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001295
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001291
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001288
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001280
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001277
Iteration 120/1000 | Loss: 0.00001277
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001276
Iteration 124/1000 | Loss: 0.00001276
Iteration 125/1000 | Loss: 0.00001275
Iteration 126/1000 | Loss: 0.00001275
Iteration 127/1000 | Loss: 0.00001275
Iteration 128/1000 | Loss: 0.00001275
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001271
Iteration 135/1000 | Loss: 0.00001271
Iteration 136/1000 | Loss: 0.00001271
Iteration 137/1000 | Loss: 0.00001271
Iteration 138/1000 | Loss: 0.00001270
Iteration 139/1000 | Loss: 0.00001270
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001270
Iteration 142/1000 | Loss: 0.00001270
Iteration 143/1000 | Loss: 0.00001270
Iteration 144/1000 | Loss: 0.00001270
Iteration 145/1000 | Loss: 0.00001270
Iteration 146/1000 | Loss: 0.00001270
Iteration 147/1000 | Loss: 0.00001269
Iteration 148/1000 | Loss: 0.00001269
Iteration 149/1000 | Loss: 0.00001268
Iteration 150/1000 | Loss: 0.00001268
Iteration 151/1000 | Loss: 0.00001268
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001265
Iteration 161/1000 | Loss: 0.00001265
Iteration 162/1000 | Loss: 0.00001265
Iteration 163/1000 | Loss: 0.00001265
Iteration 164/1000 | Loss: 0.00001265
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.264694583369419e-05, 1.264694583369419e-05, 1.264694583369419e-05, 1.264694583369419e-05, 1.264694583369419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.264694583369419e-05

Optimization complete. Final v2v error: 2.9883241653442383 mm

Highest mean error: 3.4108834266662598 mm for frame 203

Lowest mean error: 2.754157066345215 mm for frame 166

Saving results

Total time: 44.38742733001709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983617
Iteration 2/25 | Loss: 0.00411780
Iteration 3/25 | Loss: 0.00345269
Iteration 4/25 | Loss: 0.00254681
Iteration 5/25 | Loss: 0.00232924
Iteration 6/25 | Loss: 0.00230725
Iteration 7/25 | Loss: 0.00213281
Iteration 8/25 | Loss: 0.00219053
Iteration 9/25 | Loss: 0.00199312
Iteration 10/25 | Loss: 0.00191384
Iteration 11/25 | Loss: 0.00175761
Iteration 12/25 | Loss: 0.00170965
Iteration 13/25 | Loss: 0.00168957
Iteration 14/25 | Loss: 0.00168445
Iteration 15/25 | Loss: 0.00165147
Iteration 16/25 | Loss: 0.00164384
Iteration 17/25 | Loss: 0.00164041
Iteration 18/25 | Loss: 0.00165829
Iteration 19/25 | Loss: 0.00164291
Iteration 20/25 | Loss: 0.00163057
Iteration 21/25 | Loss: 0.00161232
Iteration 22/25 | Loss: 0.00160651
Iteration 23/25 | Loss: 0.00160346
Iteration 24/25 | Loss: 0.00160156
Iteration 25/25 | Loss: 0.00160987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42642748
Iteration 2/25 | Loss: 0.00434509
Iteration 3/25 | Loss: 0.00359945
Iteration 4/25 | Loss: 0.00359931
Iteration 5/25 | Loss: 0.00359931
Iteration 6/25 | Loss: 0.00359931
Iteration 7/25 | Loss: 0.00359931
Iteration 8/25 | Loss: 0.00359931
Iteration 9/25 | Loss: 0.00359931
Iteration 10/25 | Loss: 0.00359931
Iteration 11/25 | Loss: 0.00359931
Iteration 12/25 | Loss: 0.00359931
Iteration 13/25 | Loss: 0.00359931
Iteration 14/25 | Loss: 0.00359931
Iteration 15/25 | Loss: 0.00359931
Iteration 16/25 | Loss: 0.00359931
Iteration 17/25 | Loss: 0.00359931
Iteration 18/25 | Loss: 0.00359931
Iteration 19/25 | Loss: 0.00359931
Iteration 20/25 | Loss: 0.00359931
Iteration 21/25 | Loss: 0.00359931
Iteration 22/25 | Loss: 0.00359931
Iteration 23/25 | Loss: 0.00359931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003599307266995311, 0.003599307266995311, 0.003599307266995311, 0.003599307266995311, 0.003599307266995311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003599307266995311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00359931
Iteration 2/1000 | Loss: 0.00416960
Iteration 3/1000 | Loss: 0.00084444
Iteration 4/1000 | Loss: 0.00072536
Iteration 5/1000 | Loss: 0.00049018
Iteration 6/1000 | Loss: 0.00053428
Iteration 7/1000 | Loss: 0.00028182
Iteration 8/1000 | Loss: 0.00043213
Iteration 9/1000 | Loss: 0.00174116
Iteration 10/1000 | Loss: 0.00051885
Iteration 11/1000 | Loss: 0.00042223
Iteration 12/1000 | Loss: 0.00063155
Iteration 13/1000 | Loss: 0.00025168
Iteration 14/1000 | Loss: 0.00090601
Iteration 15/1000 | Loss: 0.00035281
Iteration 16/1000 | Loss: 0.00038800
Iteration 17/1000 | Loss: 0.00026463
Iteration 18/1000 | Loss: 0.00011428
Iteration 19/1000 | Loss: 0.00053589
Iteration 20/1000 | Loss: 0.00052423
Iteration 21/1000 | Loss: 0.00130400
Iteration 22/1000 | Loss: 0.00033917
Iteration 23/1000 | Loss: 0.00023361
Iteration 24/1000 | Loss: 0.00020930
Iteration 25/1000 | Loss: 0.00032457
Iteration 26/1000 | Loss: 0.00011639
Iteration 27/1000 | Loss: 0.00025536
Iteration 28/1000 | Loss: 0.00015986
Iteration 29/1000 | Loss: 0.00008798
Iteration 30/1000 | Loss: 0.00020047
Iteration 31/1000 | Loss: 0.00010922
Iteration 32/1000 | Loss: 0.00014924
Iteration 33/1000 | Loss: 0.00030649
Iteration 34/1000 | Loss: 0.00017104
Iteration 35/1000 | Loss: 0.00037993
Iteration 36/1000 | Loss: 0.00014702
Iteration 37/1000 | Loss: 0.00037216
Iteration 38/1000 | Loss: 0.00009111
Iteration 39/1000 | Loss: 0.00019797
Iteration 40/1000 | Loss: 0.00008282
Iteration 41/1000 | Loss: 0.00044223
Iteration 42/1000 | Loss: 0.00044716
Iteration 43/1000 | Loss: 0.00009112
Iteration 44/1000 | Loss: 0.00007472
Iteration 45/1000 | Loss: 0.00007320
Iteration 46/1000 | Loss: 0.00007200
Iteration 47/1000 | Loss: 0.00033853
Iteration 48/1000 | Loss: 0.00035329
Iteration 49/1000 | Loss: 0.00012541
Iteration 50/1000 | Loss: 0.00007137
Iteration 51/1000 | Loss: 0.00007000
Iteration 52/1000 | Loss: 0.00019632
Iteration 53/1000 | Loss: 0.00020518
Iteration 54/1000 | Loss: 0.00016611
Iteration 55/1000 | Loss: 0.00006906
Iteration 56/1000 | Loss: 0.00025288
Iteration 57/1000 | Loss: 0.00057357
Iteration 58/1000 | Loss: 0.00009251
Iteration 59/1000 | Loss: 0.00012634
Iteration 60/1000 | Loss: 0.00022004
Iteration 61/1000 | Loss: 0.00026418
Iteration 62/1000 | Loss: 0.00012186
Iteration 63/1000 | Loss: 0.00016022
Iteration 64/1000 | Loss: 0.00013926
Iteration 65/1000 | Loss: 0.00024534
Iteration 66/1000 | Loss: 0.00036047
Iteration 67/1000 | Loss: 0.00036688
Iteration 68/1000 | Loss: 0.00016614
Iteration 69/1000 | Loss: 0.00017606
Iteration 70/1000 | Loss: 0.00025712
Iteration 71/1000 | Loss: 0.00018788
Iteration 72/1000 | Loss: 0.00011516
Iteration 73/1000 | Loss: 0.00019787
Iteration 74/1000 | Loss: 0.00024603
Iteration 75/1000 | Loss: 0.00024459
Iteration 76/1000 | Loss: 0.00028831
Iteration 77/1000 | Loss: 0.00026507
Iteration 78/1000 | Loss: 0.00007377
Iteration 79/1000 | Loss: 0.00021666
Iteration 80/1000 | Loss: 0.00029215
Iteration 81/1000 | Loss: 0.00010738
Iteration 82/1000 | Loss: 0.00006924
Iteration 83/1000 | Loss: 0.00006782
Iteration 84/1000 | Loss: 0.00017524
Iteration 85/1000 | Loss: 0.00006734
Iteration 86/1000 | Loss: 0.00006611
Iteration 87/1000 | Loss: 0.00012956
Iteration 88/1000 | Loss: 0.00006985
Iteration 89/1000 | Loss: 0.00024110
Iteration 90/1000 | Loss: 0.00018095
Iteration 91/1000 | Loss: 0.00024515
Iteration 92/1000 | Loss: 0.00007637
Iteration 93/1000 | Loss: 0.00012034
Iteration 94/1000 | Loss: 0.00007807
Iteration 95/1000 | Loss: 0.00008959
Iteration 96/1000 | Loss: 0.00027749
Iteration 97/1000 | Loss: 0.00006975
Iteration 98/1000 | Loss: 0.00007442
Iteration 99/1000 | Loss: 0.00007851
Iteration 100/1000 | Loss: 0.00020448
Iteration 101/1000 | Loss: 0.00006854
Iteration 102/1000 | Loss: 0.00006457
Iteration 103/1000 | Loss: 0.00006340
Iteration 104/1000 | Loss: 0.00006181
Iteration 105/1000 | Loss: 0.00011986
Iteration 106/1000 | Loss: 0.00007775
Iteration 107/1000 | Loss: 0.00006489
Iteration 108/1000 | Loss: 0.00008028
Iteration 109/1000 | Loss: 0.00006049
Iteration 110/1000 | Loss: 0.00011633
Iteration 111/1000 | Loss: 0.00020345
Iteration 112/1000 | Loss: 0.00008678
Iteration 113/1000 | Loss: 0.00014466
Iteration 114/1000 | Loss: 0.00007436
Iteration 115/1000 | Loss: 0.00007449
Iteration 116/1000 | Loss: 0.00006709
Iteration 117/1000 | Loss: 0.00005677
Iteration 118/1000 | Loss: 0.00008718
Iteration 119/1000 | Loss: 0.00012870
Iteration 120/1000 | Loss: 0.00005529
Iteration 121/1000 | Loss: 0.00015864
Iteration 122/1000 | Loss: 0.00009110
Iteration 123/1000 | Loss: 0.00011790
Iteration 124/1000 | Loss: 0.00005466
Iteration 125/1000 | Loss: 0.00008302
Iteration 126/1000 | Loss: 0.00009246
Iteration 127/1000 | Loss: 0.00009123
Iteration 128/1000 | Loss: 0.00005401
Iteration 129/1000 | Loss: 0.00010501
Iteration 130/1000 | Loss: 0.00006883
Iteration 131/1000 | Loss: 0.00021126
Iteration 132/1000 | Loss: 0.00006229
Iteration 133/1000 | Loss: 0.00006437
Iteration 134/1000 | Loss: 0.00007602
Iteration 135/1000 | Loss: 0.00005381
Iteration 136/1000 | Loss: 0.00007111
Iteration 137/1000 | Loss: 0.00005656
Iteration 138/1000 | Loss: 0.00006393
Iteration 139/1000 | Loss: 0.00005361
Iteration 140/1000 | Loss: 0.00019494
Iteration 141/1000 | Loss: 0.00005351
Iteration 142/1000 | Loss: 0.00005329
Iteration 143/1000 | Loss: 0.00005316
Iteration 144/1000 | Loss: 0.00005305
Iteration 145/1000 | Loss: 0.00005299
Iteration 146/1000 | Loss: 0.00005298
Iteration 147/1000 | Loss: 0.00005298
Iteration 148/1000 | Loss: 0.00005297
Iteration 149/1000 | Loss: 0.00005286
Iteration 150/1000 | Loss: 0.00005284
Iteration 151/1000 | Loss: 0.00005282
Iteration 152/1000 | Loss: 0.00005281
Iteration 153/1000 | Loss: 0.00005276
Iteration 154/1000 | Loss: 0.00005273
Iteration 155/1000 | Loss: 0.00005257
Iteration 156/1000 | Loss: 0.00008697
Iteration 157/1000 | Loss: 0.00032538
Iteration 158/1000 | Loss: 0.00017876
Iteration 159/1000 | Loss: 0.00006524
Iteration 160/1000 | Loss: 0.00017808
Iteration 161/1000 | Loss: 0.00011327
Iteration 162/1000 | Loss: 0.00009154
Iteration 163/1000 | Loss: 0.00005855
Iteration 164/1000 | Loss: 0.00005483
Iteration 165/1000 | Loss: 0.00005308
Iteration 166/1000 | Loss: 0.00005272
Iteration 167/1000 | Loss: 0.00023765
Iteration 168/1000 | Loss: 0.00023765
Iteration 169/1000 | Loss: 0.00054478
Iteration 170/1000 | Loss: 0.00005506
Iteration 171/1000 | Loss: 0.00010863
Iteration 172/1000 | Loss: 0.00022326
Iteration 173/1000 | Loss: 0.00005414
Iteration 174/1000 | Loss: 0.00015323
Iteration 175/1000 | Loss: 0.00022960
Iteration 176/1000 | Loss: 0.00009237
Iteration 177/1000 | Loss: 0.00020706
Iteration 178/1000 | Loss: 0.00005349
Iteration 179/1000 | Loss: 0.00008273
Iteration 180/1000 | Loss: 0.00007401
Iteration 181/1000 | Loss: 0.00018107
Iteration 182/1000 | Loss: 0.00007132
Iteration 183/1000 | Loss: 0.00015556
Iteration 184/1000 | Loss: 0.00048161
Iteration 185/1000 | Loss: 0.00032162
Iteration 186/1000 | Loss: 0.00006761
Iteration 187/1000 | Loss: 0.00010146
Iteration 188/1000 | Loss: 0.00005654
Iteration 189/1000 | Loss: 0.00009017
Iteration 190/1000 | Loss: 0.00022964
Iteration 191/1000 | Loss: 0.00005458
Iteration 192/1000 | Loss: 0.00005414
Iteration 193/1000 | Loss: 0.00005385
Iteration 194/1000 | Loss: 0.00010718
Iteration 195/1000 | Loss: 0.00007364
Iteration 196/1000 | Loss: 0.00019209
Iteration 197/1000 | Loss: 0.00018927
Iteration 198/1000 | Loss: 0.00017121
Iteration 199/1000 | Loss: 0.00006755
Iteration 200/1000 | Loss: 0.00005375
Iteration 201/1000 | Loss: 0.00005358
Iteration 202/1000 | Loss: 0.00005354
Iteration 203/1000 | Loss: 0.00005351
Iteration 204/1000 | Loss: 0.00005345
Iteration 205/1000 | Loss: 0.00005340
Iteration 206/1000 | Loss: 0.00005336
Iteration 207/1000 | Loss: 0.00005336
Iteration 208/1000 | Loss: 0.00005335
Iteration 209/1000 | Loss: 0.00005335
Iteration 210/1000 | Loss: 0.00005334
Iteration 211/1000 | Loss: 0.00005334
Iteration 212/1000 | Loss: 0.00005334
Iteration 213/1000 | Loss: 0.00005333
Iteration 214/1000 | Loss: 0.00005333
Iteration 215/1000 | Loss: 0.00005332
Iteration 216/1000 | Loss: 0.00005332
Iteration 217/1000 | Loss: 0.00005327
Iteration 218/1000 | Loss: 0.00005327
Iteration 219/1000 | Loss: 0.00005327
Iteration 220/1000 | Loss: 0.00005327
Iteration 221/1000 | Loss: 0.00005326
Iteration 222/1000 | Loss: 0.00005323
Iteration 223/1000 | Loss: 0.00005322
Iteration 224/1000 | Loss: 0.00005321
Iteration 225/1000 | Loss: 0.00005321
Iteration 226/1000 | Loss: 0.00005319
Iteration 227/1000 | Loss: 0.00005318
Iteration 228/1000 | Loss: 0.00005318
Iteration 229/1000 | Loss: 0.00005317
Iteration 230/1000 | Loss: 0.00014289
Iteration 231/1000 | Loss: 0.00005657
Iteration 232/1000 | Loss: 0.00014611
Iteration 233/1000 | Loss: 0.00005337
Iteration 234/1000 | Loss: 0.00005308
Iteration 235/1000 | Loss: 0.00005307
Iteration 236/1000 | Loss: 0.00005305
Iteration 237/1000 | Loss: 0.00005303
Iteration 238/1000 | Loss: 0.00005302
Iteration 239/1000 | Loss: 0.00005302
Iteration 240/1000 | Loss: 0.00005302
Iteration 241/1000 | Loss: 0.00005302
Iteration 242/1000 | Loss: 0.00005301
Iteration 243/1000 | Loss: 0.00005301
Iteration 244/1000 | Loss: 0.00005301
Iteration 245/1000 | Loss: 0.00005301
Iteration 246/1000 | Loss: 0.00005301
Iteration 247/1000 | Loss: 0.00005301
Iteration 248/1000 | Loss: 0.00005300
Iteration 249/1000 | Loss: 0.00005300
Iteration 250/1000 | Loss: 0.00005300
Iteration 251/1000 | Loss: 0.00005300
Iteration 252/1000 | Loss: 0.00005300
Iteration 253/1000 | Loss: 0.00005297
Iteration 254/1000 | Loss: 0.00005295
Iteration 255/1000 | Loss: 0.00005295
Iteration 256/1000 | Loss: 0.00005294
Iteration 257/1000 | Loss: 0.00005293
Iteration 258/1000 | Loss: 0.00005290
Iteration 259/1000 | Loss: 0.00005290
Iteration 260/1000 | Loss: 0.00005290
Iteration 261/1000 | Loss: 0.00005290
Iteration 262/1000 | Loss: 0.00005290
Iteration 263/1000 | Loss: 0.00005290
Iteration 264/1000 | Loss: 0.00005290
Iteration 265/1000 | Loss: 0.00005289
Iteration 266/1000 | Loss: 0.00005288
Iteration 267/1000 | Loss: 0.00005287
Iteration 268/1000 | Loss: 0.00005287
Iteration 269/1000 | Loss: 0.00015240
Iteration 270/1000 | Loss: 0.00020446
Iteration 271/1000 | Loss: 0.00006393
Iteration 272/1000 | Loss: 0.00007269
Iteration 273/1000 | Loss: 0.00011000
Iteration 274/1000 | Loss: 0.00010998
Iteration 275/1000 | Loss: 0.00050873
Iteration 276/1000 | Loss: 0.00015010
Iteration 277/1000 | Loss: 0.00007438
Iteration 278/1000 | Loss: 0.00007458
Iteration 279/1000 | Loss: 0.00007832
Iteration 280/1000 | Loss: 0.00007049
Iteration 281/1000 | Loss: 0.00005245
Iteration 282/1000 | Loss: 0.00005240
Iteration 283/1000 | Loss: 0.00010324
Iteration 284/1000 | Loss: 0.00005473
Iteration 285/1000 | Loss: 0.00005273
Iteration 286/1000 | Loss: 0.00005203
Iteration 287/1000 | Loss: 0.00012593
Iteration 288/1000 | Loss: 0.00005653
Iteration 289/1000 | Loss: 0.00007251
Iteration 290/1000 | Loss: 0.00020140
Iteration 291/1000 | Loss: 0.00036533
Iteration 292/1000 | Loss: 0.00022678
Iteration 293/1000 | Loss: 0.00005699
Iteration 294/1000 | Loss: 0.00005243
Iteration 295/1000 | Loss: 0.00007387
Iteration 296/1000 | Loss: 0.00005171
Iteration 297/1000 | Loss: 0.00008968
Iteration 298/1000 | Loss: 0.00005209
Iteration 299/1000 | Loss: 0.00007001
Iteration 300/1000 | Loss: 0.00005150
Iteration 301/1000 | Loss: 0.00028093
Iteration 302/1000 | Loss: 0.00018657
Iteration 303/1000 | Loss: 0.00005720
Iteration 304/1000 | Loss: 0.00005182
Iteration 305/1000 | Loss: 0.00006528
Iteration 306/1000 | Loss: 0.00030627
Iteration 307/1000 | Loss: 0.00026955
Iteration 308/1000 | Loss: 0.00059275
Iteration 309/1000 | Loss: 0.00016854
Iteration 310/1000 | Loss: 0.00009869
Iteration 311/1000 | Loss: 0.00005845
Iteration 312/1000 | Loss: 0.00005540
Iteration 313/1000 | Loss: 0.00005433
Iteration 314/1000 | Loss: 0.00005366
Iteration 315/1000 | Loss: 0.00012301
Iteration 316/1000 | Loss: 0.00005293
Iteration 317/1000 | Loss: 0.00005263
Iteration 318/1000 | Loss: 0.00012162
Iteration 319/1000 | Loss: 0.00012991
Iteration 320/1000 | Loss: 0.00005219
Iteration 321/1000 | Loss: 0.00005183
Iteration 322/1000 | Loss: 0.00005149
Iteration 323/1000 | Loss: 0.00015516
Iteration 324/1000 | Loss: 0.00005342
Iteration 325/1000 | Loss: 0.00018668
Iteration 326/1000 | Loss: 0.00009528
Iteration 327/1000 | Loss: 0.00005088
Iteration 328/1000 | Loss: 0.00005048
Iteration 329/1000 | Loss: 0.00013190
Iteration 330/1000 | Loss: 0.00005310
Iteration 331/1000 | Loss: 0.00017458
Iteration 332/1000 | Loss: 0.00004992
Iteration 333/1000 | Loss: 0.00004898
Iteration 334/1000 | Loss: 0.00043439
Iteration 335/1000 | Loss: 0.00039583
Iteration 336/1000 | Loss: 0.00004968
Iteration 337/1000 | Loss: 0.00066460
Iteration 338/1000 | Loss: 0.00044790
Iteration 339/1000 | Loss: 0.00030895
Iteration 340/1000 | Loss: 0.00010521
Iteration 341/1000 | Loss: 0.00007172
Iteration 342/1000 | Loss: 0.00012412
Iteration 343/1000 | Loss: 0.00046938
Iteration 344/1000 | Loss: 0.00019709
Iteration 345/1000 | Loss: 0.00047169
Iteration 346/1000 | Loss: 0.00019506
Iteration 347/1000 | Loss: 0.00008764
Iteration 348/1000 | Loss: 0.00005733
Iteration 349/1000 | Loss: 0.00043300
Iteration 350/1000 | Loss: 0.00024489
Iteration 351/1000 | Loss: 0.00004908
Iteration 352/1000 | Loss: 0.00045633
Iteration 353/1000 | Loss: 0.00019300
Iteration 354/1000 | Loss: 0.00041647
Iteration 355/1000 | Loss: 0.00029075
Iteration 356/1000 | Loss: 0.00011403
Iteration 357/1000 | Loss: 0.00008966
Iteration 358/1000 | Loss: 0.00008265
Iteration 359/1000 | Loss: 0.00005432
Iteration 360/1000 | Loss: 0.00005239
Iteration 361/1000 | Loss: 0.00008878
Iteration 362/1000 | Loss: 0.00004995
Iteration 363/1000 | Loss: 0.00009816
Iteration 364/1000 | Loss: 0.00004832
Iteration 365/1000 | Loss: 0.00040691
Iteration 366/1000 | Loss: 0.00016861
Iteration 367/1000 | Loss: 0.00021455
Iteration 368/1000 | Loss: 0.00012733
Iteration 369/1000 | Loss: 0.00012019
Iteration 370/1000 | Loss: 0.00006661
Iteration 371/1000 | Loss: 0.00005095
Iteration 372/1000 | Loss: 0.00004709
Iteration 373/1000 | Loss: 0.00023067
Iteration 374/1000 | Loss: 0.00024181
Iteration 375/1000 | Loss: 0.00024101
Iteration 376/1000 | Loss: 0.00004811
Iteration 377/1000 | Loss: 0.00026404
Iteration 378/1000 | Loss: 0.00018145
Iteration 379/1000 | Loss: 0.00005245
Iteration 380/1000 | Loss: 0.00008648
Iteration 381/1000 | Loss: 0.00004779
Iteration 382/1000 | Loss: 0.00029902
Iteration 383/1000 | Loss: 0.00022835
Iteration 384/1000 | Loss: 0.00006378
Iteration 385/1000 | Loss: 0.00005009
Iteration 386/1000 | Loss: 0.00029141
Iteration 387/1000 | Loss: 0.00081425
Iteration 388/1000 | Loss: 0.00031642
Iteration 389/1000 | Loss: 0.00005859
Iteration 390/1000 | Loss: 0.00016627
Iteration 391/1000 | Loss: 0.00005813
Iteration 392/1000 | Loss: 0.00005156
Iteration 393/1000 | Loss: 0.00032687
Iteration 394/1000 | Loss: 0.00009748
Iteration 395/1000 | Loss: 0.00005952
Iteration 396/1000 | Loss: 0.00009613
Iteration 397/1000 | Loss: 0.00024804
Iteration 398/1000 | Loss: 0.00004987
Iteration 399/1000 | Loss: 0.00006625
Iteration 400/1000 | Loss: 0.00005683
Iteration 401/1000 | Loss: 0.00014115
Iteration 402/1000 | Loss: 0.00005485
Iteration 403/1000 | Loss: 0.00044376
Iteration 404/1000 | Loss: 0.00034578
Iteration 405/1000 | Loss: 0.00047656
Iteration 406/1000 | Loss: 0.00036732
Iteration 407/1000 | Loss: 0.00082260
Iteration 408/1000 | Loss: 0.00036726
Iteration 409/1000 | Loss: 0.00005827
Iteration 410/1000 | Loss: 0.00027873
Iteration 411/1000 | Loss: 0.00047444
Iteration 412/1000 | Loss: 0.00012445
Iteration 413/1000 | Loss: 0.00004783
Iteration 414/1000 | Loss: 0.00008807
Iteration 415/1000 | Loss: 0.00013803
Iteration 416/1000 | Loss: 0.00004262
Iteration 417/1000 | Loss: 0.00024169
Iteration 418/1000 | Loss: 0.00041448
Iteration 419/1000 | Loss: 0.00018667
Iteration 420/1000 | Loss: 0.00004662
Iteration 421/1000 | Loss: 0.00023345
Iteration 422/1000 | Loss: 0.00041550
Iteration 423/1000 | Loss: 0.00005286
Iteration 424/1000 | Loss: 0.00004784
Iteration 425/1000 | Loss: 0.00004453
Iteration 426/1000 | Loss: 0.00013075
Iteration 427/1000 | Loss: 0.00007134
Iteration 428/1000 | Loss: 0.00005243
Iteration 429/1000 | Loss: 0.00004169
Iteration 430/1000 | Loss: 0.00012336
Iteration 431/1000 | Loss: 0.00004208
Iteration 432/1000 | Loss: 0.00004066
Iteration 433/1000 | Loss: 0.00031109
Iteration 434/1000 | Loss: 0.00042364
Iteration 435/1000 | Loss: 0.00016502
Iteration 436/1000 | Loss: 0.00012094
Iteration 437/1000 | Loss: 0.00006181
Iteration 438/1000 | Loss: 0.00017959
Iteration 439/1000 | Loss: 0.00004040
Iteration 440/1000 | Loss: 0.00003930
Iteration 441/1000 | Loss: 0.00013450
Iteration 442/1000 | Loss: 0.00011956
Iteration 443/1000 | Loss: 0.00020239
Iteration 444/1000 | Loss: 0.00004139
Iteration 445/1000 | Loss: 0.00010482
Iteration 446/1000 | Loss: 0.00010268
Iteration 447/1000 | Loss: 0.00006977
Iteration 448/1000 | Loss: 0.00010544
Iteration 449/1000 | Loss: 0.00010787
Iteration 450/1000 | Loss: 0.00006337
Iteration 451/1000 | Loss: 0.00006358
Iteration 452/1000 | Loss: 0.00005787
Iteration 453/1000 | Loss: 0.00004798
Iteration 454/1000 | Loss: 0.00003656
Iteration 455/1000 | Loss: 0.00036110
Iteration 456/1000 | Loss: 0.00073334
Iteration 457/1000 | Loss: 0.00067697
Iteration 458/1000 | Loss: 0.00042036
Iteration 459/1000 | Loss: 0.00038783
Iteration 460/1000 | Loss: 0.00034647
Iteration 461/1000 | Loss: 0.00011120
Iteration 462/1000 | Loss: 0.00016212
Iteration 463/1000 | Loss: 0.00005765
Iteration 464/1000 | Loss: 0.00007882
Iteration 465/1000 | Loss: 0.00006535
Iteration 466/1000 | Loss: 0.00006529
Iteration 467/1000 | Loss: 0.00005925
Iteration 468/1000 | Loss: 0.00003717
Iteration 469/1000 | Loss: 0.00028190
Iteration 470/1000 | Loss: 0.00060717
Iteration 471/1000 | Loss: 0.00131640
Iteration 472/1000 | Loss: 0.00121118
Iteration 473/1000 | Loss: 0.00064235
Iteration 474/1000 | Loss: 0.00005174
Iteration 475/1000 | Loss: 0.00028218
Iteration 476/1000 | Loss: 0.00007182
Iteration 477/1000 | Loss: 0.00016833
Iteration 478/1000 | Loss: 0.00005916
Iteration 479/1000 | Loss: 0.00017104
Iteration 480/1000 | Loss: 0.00003995
Iteration 481/1000 | Loss: 0.00012481
Iteration 482/1000 | Loss: 0.00003829
Iteration 483/1000 | Loss: 0.00003737
Iteration 484/1000 | Loss: 0.00006454
Iteration 485/1000 | Loss: 0.00008470
Iteration 486/1000 | Loss: 0.00003612
Iteration 487/1000 | Loss: 0.00003578
Iteration 488/1000 | Loss: 0.00003526
Iteration 489/1000 | Loss: 0.00022169
Iteration 490/1000 | Loss: 0.00022013
Iteration 491/1000 | Loss: 0.00014049
Iteration 492/1000 | Loss: 0.00070905
Iteration 493/1000 | Loss: 0.00020928
Iteration 494/1000 | Loss: 0.00019088
Iteration 495/1000 | Loss: 0.00004726
Iteration 496/1000 | Loss: 0.00005874
Iteration 497/1000 | Loss: 0.00004076
Iteration 498/1000 | Loss: 0.00003955
Iteration 499/1000 | Loss: 0.00012999
Iteration 500/1000 | Loss: 0.00005831
Iteration 501/1000 | Loss: 0.00003787
Iteration 502/1000 | Loss: 0.00003738
Iteration 503/1000 | Loss: 0.00003693
Iteration 504/1000 | Loss: 0.00003645
Iteration 505/1000 | Loss: 0.00003601
Iteration 506/1000 | Loss: 0.00010682
Iteration 507/1000 | Loss: 0.00020640
Iteration 508/1000 | Loss: 0.00004121
Iteration 509/1000 | Loss: 0.00009287
Iteration 510/1000 | Loss: 0.00003726
Iteration 511/1000 | Loss: 0.00008147
Iteration 512/1000 | Loss: 0.00003622
Iteration 513/1000 | Loss: 0.00003574
Iteration 514/1000 | Loss: 0.00011299
Iteration 515/1000 | Loss: 0.00050774
Iteration 516/1000 | Loss: 0.00052381
Iteration 517/1000 | Loss: 0.00026786
Iteration 518/1000 | Loss: 0.00012414
Iteration 519/1000 | Loss: 0.00009625
Iteration 520/1000 | Loss: 0.00004035
Iteration 521/1000 | Loss: 0.00010669
Iteration 522/1000 | Loss: 0.00021588
Iteration 523/1000 | Loss: 0.00003790
Iteration 524/1000 | Loss: 0.00003498
Iteration 525/1000 | Loss: 0.00006518
Iteration 526/1000 | Loss: 0.00003911
Iteration 527/1000 | Loss: 0.00004124
Iteration 528/1000 | Loss: 0.00004989
Iteration 529/1000 | Loss: 0.00003445
Iteration 530/1000 | Loss: 0.00003356
Iteration 531/1000 | Loss: 0.00022973
Iteration 532/1000 | Loss: 0.00018139
Iteration 533/1000 | Loss: 0.00021667
Iteration 534/1000 | Loss: 0.00016683
Iteration 535/1000 | Loss: 0.00023487
Iteration 536/1000 | Loss: 0.00026218
Iteration 537/1000 | Loss: 0.00041287
Iteration 538/1000 | Loss: 0.00017847
Iteration 539/1000 | Loss: 0.00021549
Iteration 540/1000 | Loss: 0.00003617
Iteration 541/1000 | Loss: 0.00004715
Iteration 542/1000 | Loss: 0.00003361
Iteration 543/1000 | Loss: 0.00025486
Iteration 544/1000 | Loss: 0.00020500
Iteration 545/1000 | Loss: 0.00005306
Iteration 546/1000 | Loss: 0.00044651
Iteration 547/1000 | Loss: 0.00026453
Iteration 548/1000 | Loss: 0.00005818
Iteration 549/1000 | Loss: 0.00021246
Iteration 550/1000 | Loss: 0.00046434
Iteration 551/1000 | Loss: 0.00006045
Iteration 552/1000 | Loss: 0.00004760
Iteration 553/1000 | Loss: 0.00003734
Iteration 554/1000 | Loss: 0.00008132
Iteration 555/1000 | Loss: 0.00003565
Iteration 556/1000 | Loss: 0.00008205
Iteration 557/1000 | Loss: 0.00013249
Iteration 558/1000 | Loss: 0.00071055
Iteration 559/1000 | Loss: 0.00017600
Iteration 560/1000 | Loss: 0.00047686
Iteration 561/1000 | Loss: 0.00149256
Iteration 562/1000 | Loss: 0.00016201
Iteration 563/1000 | Loss: 0.00101197
Iteration 564/1000 | Loss: 0.00029670
Iteration 565/1000 | Loss: 0.00224679
Iteration 566/1000 | Loss: 0.00050410
Iteration 567/1000 | Loss: 0.00013985
Iteration 568/1000 | Loss: 0.00004844
Iteration 569/1000 | Loss: 0.00006301
Iteration 570/1000 | Loss: 0.00004430
Iteration 571/1000 | Loss: 0.00115964
Iteration 572/1000 | Loss: 0.00078938
Iteration 573/1000 | Loss: 0.00118360
Iteration 574/1000 | Loss: 0.00008629
Iteration 575/1000 | Loss: 0.00005107
Iteration 576/1000 | Loss: 0.00014861
Iteration 577/1000 | Loss: 0.00004109
Iteration 578/1000 | Loss: 0.00003463
Iteration 579/1000 | Loss: 0.00003400
Iteration 580/1000 | Loss: 0.00003352
Iteration 581/1000 | Loss: 0.00003312
Iteration 582/1000 | Loss: 0.00003284
Iteration 583/1000 | Loss: 0.00003237
Iteration 584/1000 | Loss: 0.00022567
Iteration 585/1000 | Loss: 0.00003736
Iteration 586/1000 | Loss: 0.00008736
Iteration 587/1000 | Loss: 0.00003422
Iteration 588/1000 | Loss: 0.00008303
Iteration 589/1000 | Loss: 0.00003395
Iteration 590/1000 | Loss: 0.00010098
Iteration 591/1000 | Loss: 0.00003295
Iteration 592/1000 | Loss: 0.00003251
Iteration 593/1000 | Loss: 0.00011396
Iteration 594/1000 | Loss: 0.00003350
Iteration 595/1000 | Loss: 0.00025008
Iteration 596/1000 | Loss: 0.00019188
Iteration 597/1000 | Loss: 0.00027366
Iteration 598/1000 | Loss: 0.00020732
Iteration 599/1000 | Loss: 0.00022912
Iteration 600/1000 | Loss: 0.00024148
Iteration 601/1000 | Loss: 0.00015579
Iteration 602/1000 | Loss: 0.00011060
Iteration 603/1000 | Loss: 0.00003369
Iteration 604/1000 | Loss: 0.00006661
Iteration 605/1000 | Loss: 0.00003265
Iteration 606/1000 | Loss: 0.00026161
Iteration 607/1000 | Loss: 0.00012974
Iteration 608/1000 | Loss: 0.00025192
Iteration 609/1000 | Loss: 0.00015684
Iteration 610/1000 | Loss: 0.00003219
Iteration 611/1000 | Loss: 0.00026541
Iteration 612/1000 | Loss: 0.00058314
Iteration 613/1000 | Loss: 0.00031951
Iteration 614/1000 | Loss: 0.00023149
Iteration 615/1000 | Loss: 0.00026416
Iteration 616/1000 | Loss: 0.00025208
Iteration 617/1000 | Loss: 0.00011136
Iteration 618/1000 | Loss: 0.00015206
Iteration 619/1000 | Loss: 0.00030208
Iteration 620/1000 | Loss: 0.00024672
Iteration 621/1000 | Loss: 0.00019623
Iteration 622/1000 | Loss: 0.00019613
Iteration 623/1000 | Loss: 0.00004954
Iteration 624/1000 | Loss: 0.00008945
Iteration 625/1000 | Loss: 0.00003481
Iteration 626/1000 | Loss: 0.00003386
Iteration 627/1000 | Loss: 0.00003331
Iteration 628/1000 | Loss: 0.00003283
Iteration 629/1000 | Loss: 0.00021166
Iteration 630/1000 | Loss: 0.00003823
Iteration 631/1000 | Loss: 0.00003477
Iteration 632/1000 | Loss: 0.00007318
Iteration 633/1000 | Loss: 0.00003327
Iteration 634/1000 | Loss: 0.00003288
Iteration 635/1000 | Loss: 0.00023153
Iteration 636/1000 | Loss: 0.00030484
Iteration 637/1000 | Loss: 0.00016319
Iteration 638/1000 | Loss: 0.00003334
Iteration 639/1000 | Loss: 0.00011867
Iteration 640/1000 | Loss: 0.00003271
Iteration 641/1000 | Loss: 0.00022891
Iteration 642/1000 | Loss: 0.00017716
Iteration 643/1000 | Loss: 0.00023981
Iteration 644/1000 | Loss: 0.00007130
Iteration 645/1000 | Loss: 0.00023633
Iteration 646/1000 | Loss: 0.00023226
Iteration 647/1000 | Loss: 0.00028007
Iteration 648/1000 | Loss: 0.00023770
Iteration 649/1000 | Loss: 0.00019416
Iteration 650/1000 | Loss: 0.00022923
Iteration 651/1000 | Loss: 0.00056640
Iteration 652/1000 | Loss: 0.00006346
Iteration 653/1000 | Loss: 0.00019722
Iteration 654/1000 | Loss: 0.00014688
Iteration 655/1000 | Loss: 0.00031350
Iteration 656/1000 | Loss: 0.00007317
Iteration 657/1000 | Loss: 0.00015439
Iteration 658/1000 | Loss: 0.00029590
Iteration 659/1000 | Loss: 0.00020361
Iteration 660/1000 | Loss: 0.00014797
Iteration 661/1000 | Loss: 0.00018715
Iteration 662/1000 | Loss: 0.00031691
Iteration 663/1000 | Loss: 0.00011983
Iteration 664/1000 | Loss: 0.00005260
Iteration 665/1000 | Loss: 0.00008999
Iteration 666/1000 | Loss: 0.00016725
Iteration 667/1000 | Loss: 0.00004852
Iteration 668/1000 | Loss: 0.00003408
Iteration 669/1000 | Loss: 0.00028382
Iteration 670/1000 | Loss: 0.00006327
Iteration 671/1000 | Loss: 0.00015325
Iteration 672/1000 | Loss: 0.00003592
Iteration 673/1000 | Loss: 0.00009641
Iteration 674/1000 | Loss: 0.00003415
Iteration 675/1000 | Loss: 0.00003378
Iteration 676/1000 | Loss: 0.00003338
Iteration 677/1000 | Loss: 0.00003309
Iteration 678/1000 | Loss: 0.00006899
Iteration 679/1000 | Loss: 0.00003256
Iteration 680/1000 | Loss: 0.00003219
Iteration 681/1000 | Loss: 0.00010508
Iteration 682/1000 | Loss: 0.00024027
Iteration 683/1000 | Loss: 0.00005905
Iteration 684/1000 | Loss: 0.00004514
Iteration 685/1000 | Loss: 0.00004002
Iteration 686/1000 | Loss: 0.00003803
Iteration 687/1000 | Loss: 0.00003317
Iteration 688/1000 | Loss: 0.00003282
Iteration 689/1000 | Loss: 0.00026021
Iteration 690/1000 | Loss: 0.00005676
Iteration 691/1000 | Loss: 0.00008874
Iteration 692/1000 | Loss: 0.00008810
Iteration 693/1000 | Loss: 0.00003906
Iteration 694/1000 | Loss: 0.00003695
Iteration 695/1000 | Loss: 0.00003372
Iteration 696/1000 | Loss: 0.00003311
Iteration 697/1000 | Loss: 0.00007640
Iteration 698/1000 | Loss: 0.00033695
Iteration 699/1000 | Loss: 0.00018026
Iteration 700/1000 | Loss: 0.00025670
Iteration 701/1000 | Loss: 0.00003638
Iteration 702/1000 | Loss: 0.00003469
Iteration 703/1000 | Loss: 0.00003398
Iteration 704/1000 | Loss: 0.00025389
Iteration 705/1000 | Loss: 0.00023543
Iteration 706/1000 | Loss: 0.00032519
Iteration 707/1000 | Loss: 0.00006034
Iteration 708/1000 | Loss: 0.00003532
Iteration 709/1000 | Loss: 0.00007739
Iteration 710/1000 | Loss: 0.00003416
Iteration 711/1000 | Loss: 0.00003371
Iteration 712/1000 | Loss: 0.00004864
Iteration 713/1000 | Loss: 0.00014326
Iteration 714/1000 | Loss: 0.00016851
Iteration 715/1000 | Loss: 0.00004291
Iteration 716/1000 | Loss: 0.00007182
Iteration 717/1000 | Loss: 0.00006327
Iteration 718/1000 | Loss: 0.00003470
Iteration 719/1000 | Loss: 0.00005413
Iteration 720/1000 | Loss: 0.00003205
Iteration 721/1000 | Loss: 0.00039697
Iteration 722/1000 | Loss: 0.00049779
Iteration 723/1000 | Loss: 0.00070475
Iteration 724/1000 | Loss: 0.00018139
Iteration 725/1000 | Loss: 0.00007200
Iteration 726/1000 | Loss: 0.00003219
Iteration 727/1000 | Loss: 0.00012735
Iteration 728/1000 | Loss: 0.00004065
Iteration 729/1000 | Loss: 0.00046237
Iteration 730/1000 | Loss: 0.00023559
Iteration 731/1000 | Loss: 0.00062702
Iteration 732/1000 | Loss: 0.00018442
Iteration 733/1000 | Loss: 0.00064079
Iteration 734/1000 | Loss: 0.00027254
Iteration 735/1000 | Loss: 0.00059571
Iteration 736/1000 | Loss: 0.00027859
Iteration 737/1000 | Loss: 0.00056006
Iteration 738/1000 | Loss: 0.00025603
Iteration 739/1000 | Loss: 0.00054630
Iteration 740/1000 | Loss: 0.00024868
Iteration 741/1000 | Loss: 0.00053725
Iteration 742/1000 | Loss: 0.00024488
Iteration 743/1000 | Loss: 0.00061590
Iteration 744/1000 | Loss: 0.00066123
Iteration 745/1000 | Loss: 0.00062673
Iteration 746/1000 | Loss: 0.00004932
Iteration 747/1000 | Loss: 0.00019809
Iteration 748/1000 | Loss: 0.00013765
Iteration 749/1000 | Loss: 0.00005545
Iteration 750/1000 | Loss: 0.00004563
Iteration 751/1000 | Loss: 0.00004292
Iteration 752/1000 | Loss: 0.00003052
Iteration 753/1000 | Loss: 0.00004246
Iteration 754/1000 | Loss: 0.00010090
Iteration 755/1000 | Loss: 0.00005804
Iteration 756/1000 | Loss: 0.00002843
Iteration 757/1000 | Loss: 0.00005061
Iteration 758/1000 | Loss: 0.00025794
Iteration 759/1000 | Loss: 0.00018572
Iteration 760/1000 | Loss: 0.00002792
Iteration 761/1000 | Loss: 0.00004272
Iteration 762/1000 | Loss: 0.00002822
Iteration 763/1000 | Loss: 0.00003020
Iteration 764/1000 | Loss: 0.00024543
Iteration 765/1000 | Loss: 0.00007815
Iteration 766/1000 | Loss: 0.00005100
Iteration 767/1000 | Loss: 0.00002957
Iteration 768/1000 | Loss: 0.00002894
Iteration 769/1000 | Loss: 0.00008358
Iteration 770/1000 | Loss: 0.00046126
Iteration 771/1000 | Loss: 0.00004977
Iteration 772/1000 | Loss: 0.00012856
Iteration 773/1000 | Loss: 0.00015488
Iteration 774/1000 | Loss: 0.00019889
Iteration 775/1000 | Loss: 0.00002868
Iteration 776/1000 | Loss: 0.00002806
Iteration 777/1000 | Loss: 0.00007796
Iteration 778/1000 | Loss: 0.00002801
Iteration 779/1000 | Loss: 0.00002743
Iteration 780/1000 | Loss: 0.00044046
Iteration 781/1000 | Loss: 0.00029496
Iteration 782/1000 | Loss: 0.00002783
Iteration 783/1000 | Loss: 0.00046235
Iteration 784/1000 | Loss: 0.00012023
Iteration 785/1000 | Loss: 0.00003308
Iteration 786/1000 | Loss: 0.00006517
Iteration 787/1000 | Loss: 0.00003118
Iteration 788/1000 | Loss: 0.00003063
Iteration 789/1000 | Loss: 0.00003005
Iteration 790/1000 | Loss: 0.00011125
Iteration 791/1000 | Loss: 0.00002956
Iteration 792/1000 | Loss: 0.00007498
Iteration 793/1000 | Loss: 0.00002911
Iteration 794/1000 | Loss: 0.00002885
Iteration 795/1000 | Loss: 0.00002853
Iteration 796/1000 | Loss: 0.00010454
Iteration 797/1000 | Loss: 0.00003055
Iteration 798/1000 | Loss: 0.00002858
Iteration 799/1000 | Loss: 0.00007857
Iteration 800/1000 | Loss: 0.00003532
Iteration 801/1000 | Loss: 0.00008260
Iteration 802/1000 | Loss: 0.00003291
Iteration 803/1000 | Loss: 0.00020139
Iteration 804/1000 | Loss: 0.00003366
Iteration 805/1000 | Loss: 0.00007703
Iteration 806/1000 | Loss: 0.00002997
Iteration 807/1000 | Loss: 0.00006896
Iteration 808/1000 | Loss: 0.00002921
Iteration 809/1000 | Loss: 0.00002887
Iteration 810/1000 | Loss: 0.00010682
Iteration 811/1000 | Loss: 0.00059051
Iteration 812/1000 | Loss: 0.00028681
Iteration 813/1000 | Loss: 0.00007919
Iteration 814/1000 | Loss: 0.00006960
Iteration 815/1000 | Loss: 0.00002865
Iteration 816/1000 | Loss: 0.00002835
Iteration 817/1000 | Loss: 0.00002820
Iteration 818/1000 | Loss: 0.00002800
Iteration 819/1000 | Loss: 0.00011763
Iteration 820/1000 | Loss: 0.00002824
Iteration 821/1000 | Loss: 0.00002747
Iteration 822/1000 | Loss: 0.00025093
Iteration 823/1000 | Loss: 0.00018132
Iteration 824/1000 | Loss: 0.00004902
Iteration 825/1000 | Loss: 0.00003603
Iteration 826/1000 | Loss: 0.00003113
Iteration 827/1000 | Loss: 0.00003810
Iteration 828/1000 | Loss: 0.00002750
Iteration 829/1000 | Loss: 0.00005064
Iteration 830/1000 | Loss: 0.00002711
Iteration 831/1000 | Loss: 0.00026429
Iteration 832/1000 | Loss: 0.00031811
Iteration 833/1000 | Loss: 0.00032761
Iteration 834/1000 | Loss: 0.00019159
Iteration 835/1000 | Loss: 0.00016358
Iteration 836/1000 | Loss: 0.00013648
Iteration 837/1000 | Loss: 0.00003105
Iteration 838/1000 | Loss: 0.00007805
Iteration 839/1000 | Loss: 0.00004736
Iteration 840/1000 | Loss: 0.00011690
Iteration 841/1000 | Loss: 0.00014726
Iteration 842/1000 | Loss: 0.00005535
Iteration 843/1000 | Loss: 0.00002741
Iteration 844/1000 | Loss: 0.00002695
Iteration 845/1000 | Loss: 0.00015122
Iteration 846/1000 | Loss: 0.00024437
Iteration 847/1000 | Loss: 0.00003794
Iteration 848/1000 | Loss: 0.00003000
Iteration 849/1000 | Loss: 0.00010064
Iteration 850/1000 | Loss: 0.00002834
Iteration 851/1000 | Loss: 0.00002785
Iteration 852/1000 | Loss: 0.00002753
Iteration 853/1000 | Loss: 0.00002725
Iteration 854/1000 | Loss: 0.00002724
Iteration 855/1000 | Loss: 0.00002702
Iteration 856/1000 | Loss: 0.00002672
Iteration 857/1000 | Loss: 0.00002646
Iteration 858/1000 | Loss: 0.00002618
Iteration 859/1000 | Loss: 0.00024266
Iteration 860/1000 | Loss: 0.00014781
Iteration 861/1000 | Loss: 0.00007225
Iteration 862/1000 | Loss: 0.00010346
Iteration 863/1000 | Loss: 0.00002965
Iteration 864/1000 | Loss: 0.00005262
Iteration 865/1000 | Loss: 0.00002721
Iteration 866/1000 | Loss: 0.00011959
Iteration 867/1000 | Loss: 0.00002683
Iteration 868/1000 | Loss: 0.00005583
Iteration 869/1000 | Loss: 0.00006476
Iteration 870/1000 | Loss: 0.00003489
Iteration 871/1000 | Loss: 0.00004431
Iteration 872/1000 | Loss: 0.00002599
Iteration 873/1000 | Loss: 0.00023502
Iteration 874/1000 | Loss: 0.00011505
Iteration 875/1000 | Loss: 0.00004050
Iteration 876/1000 | Loss: 0.00007428
Iteration 877/1000 | Loss: 0.00002877
Iteration 878/1000 | Loss: 0.00002900
Iteration 879/1000 | Loss: 0.00002688
Iteration 880/1000 | Loss: 0.00009336
Iteration 881/1000 | Loss: 0.00002666
Iteration 882/1000 | Loss: 0.00002614
Iteration 883/1000 | Loss: 0.00002589
Iteration 884/1000 | Loss: 0.00045231
Iteration 885/1000 | Loss: 0.00021210
Iteration 886/1000 | Loss: 0.00002993
Iteration 887/1000 | Loss: 0.00003893
Iteration 888/1000 | Loss: 0.00002645
Iteration 889/1000 | Loss: 0.00004852
Iteration 890/1000 | Loss: 0.00002596
Iteration 891/1000 | Loss: 0.00022461
Iteration 892/1000 | Loss: 0.00004833
Iteration 893/1000 | Loss: 0.00002627
Iteration 894/1000 | Loss: 0.00002572
Iteration 895/1000 | Loss: 0.00045921
Iteration 896/1000 | Loss: 0.00010925
Iteration 897/1000 | Loss: 0.00002967
Iteration 898/1000 | Loss: 0.00009475
Iteration 899/1000 | Loss: 0.00106884
Iteration 900/1000 | Loss: 0.00014258
Iteration 901/1000 | Loss: 0.00050272
Iteration 902/1000 | Loss: 0.00003114
Iteration 903/1000 | Loss: 0.00002609
Iteration 904/1000 | Loss: 0.00007013
Iteration 905/1000 | Loss: 0.00002498
Iteration 906/1000 | Loss: 0.00007435
Iteration 907/1000 | Loss: 0.00002466
Iteration 908/1000 | Loss: 0.00002445
Iteration 909/1000 | Loss: 0.00002424
Iteration 910/1000 | Loss: 0.00002416
Iteration 911/1000 | Loss: 0.00002414
Iteration 912/1000 | Loss: 0.00002411
Iteration 913/1000 | Loss: 0.00002410
Iteration 914/1000 | Loss: 0.00002408
Iteration 915/1000 | Loss: 0.00002406
Iteration 916/1000 | Loss: 0.00002399
Iteration 917/1000 | Loss: 0.00002398
Iteration 918/1000 | Loss: 0.00002394
Iteration 919/1000 | Loss: 0.00002394
Iteration 920/1000 | Loss: 0.00002393
Iteration 921/1000 | Loss: 0.00002393
Iteration 922/1000 | Loss: 0.00002393
Iteration 923/1000 | Loss: 0.00002392
Iteration 924/1000 | Loss: 0.00002390
Iteration 925/1000 | Loss: 0.00002389
Iteration 926/1000 | Loss: 0.00002388
Iteration 927/1000 | Loss: 0.00002383
Iteration 928/1000 | Loss: 0.00002382
Iteration 929/1000 | Loss: 0.00002380
Iteration 930/1000 | Loss: 0.00002379
Iteration 931/1000 | Loss: 0.00002379
Iteration 932/1000 | Loss: 0.00002377
Iteration 933/1000 | Loss: 0.00002377
Iteration 934/1000 | Loss: 0.00002377
Iteration 935/1000 | Loss: 0.00002377
Iteration 936/1000 | Loss: 0.00002377
Iteration 937/1000 | Loss: 0.00002376
Iteration 938/1000 | Loss: 0.00010039
Iteration 939/1000 | Loss: 0.00003049
Iteration 940/1000 | Loss: 0.00005598
Iteration 941/1000 | Loss: 0.00002381
Iteration 942/1000 | Loss: 0.00002369
Iteration 943/1000 | Loss: 0.00002367
Iteration 944/1000 | Loss: 0.00002367
Iteration 945/1000 | Loss: 0.00002366
Iteration 946/1000 | Loss: 0.00007493
Iteration 947/1000 | Loss: 0.00003669
Iteration 948/1000 | Loss: 0.00002633
Iteration 949/1000 | Loss: 0.00004772
Iteration 950/1000 | Loss: 0.00002371
Iteration 951/1000 | Loss: 0.00002352
Iteration 952/1000 | Loss: 0.00002351
Iteration 953/1000 | Loss: 0.00002348
Iteration 954/1000 | Loss: 0.00002348
Iteration 955/1000 | Loss: 0.00002347
Iteration 956/1000 | Loss: 0.00002347
Iteration 957/1000 | Loss: 0.00002347
Iteration 958/1000 | Loss: 0.00002346
Iteration 959/1000 | Loss: 0.00019392
Iteration 960/1000 | Loss: 0.00031780
Iteration 961/1000 | Loss: 0.00002399
Iteration 962/1000 | Loss: 0.00004688
Iteration 963/1000 | Loss: 0.00002379
Iteration 964/1000 | Loss: 0.00002354
Iteration 965/1000 | Loss: 0.00005967
Iteration 966/1000 | Loss: 0.00002497
Iteration 967/1000 | Loss: 0.00002404
Iteration 968/1000 | Loss: 0.00002347
Iteration 969/1000 | Loss: 0.00002347
Iteration 970/1000 | Loss: 0.00002346
Iteration 971/1000 | Loss: 0.00019898
Iteration 972/1000 | Loss: 0.00011174
Iteration 973/1000 | Loss: 0.00022717
Iteration 974/1000 | Loss: 0.00011867
Iteration 975/1000 | Loss: 0.00003333
Iteration 976/1000 | Loss: 0.00002354
Iteration 977/1000 | Loss: 0.00018798
Iteration 978/1000 | Loss: 0.00015097
Iteration 979/1000 | Loss: 0.00003504
Iteration 980/1000 | Loss: 0.00002717
Iteration 981/1000 | Loss: 0.00018827
Iteration 982/1000 | Loss: 0.00020454
Iteration 983/1000 | Loss: 0.00002901
Iteration 984/1000 | Loss: 0.00007991
Iteration 985/1000 | Loss: 0.00002381
Iteration 986/1000 | Loss: 0.00003500
Iteration 987/1000 | Loss: 0.00002358
Iteration 988/1000 | Loss: 0.00018467
Iteration 989/1000 | Loss: 0.00012707
Iteration 990/1000 | Loss: 0.00020302
Iteration 991/1000 | Loss: 0.00011989
Iteration 992/1000 | Loss: 0.00018198
Iteration 993/1000 | Loss: 0.00019538
Iteration 994/1000 | Loss: 0.00002651
Iteration 995/1000 | Loss: 0.00019817
Iteration 996/1000 | Loss: 0.00009846
Iteration 997/1000 | Loss: 0.00002578
Iteration 998/1000 | Loss: 0.00003001
Iteration 999/1000 | Loss: 0.00002363
Iteration 1000/1000 | Loss: 0.00017751

Optimization complete. Final v2v error: 4.501091480255127 mm

Highest mean error: 79.92646789550781 mm for frame 54

Lowest mean error: 3.6504602432250977 mm for frame 14

Saving results

Total time: 1467.399176120758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003970
Iteration 2/25 | Loss: 0.00191263
Iteration 3/25 | Loss: 0.00165573
Iteration 4/25 | Loss: 0.00156201
Iteration 5/25 | Loss: 0.00156613
Iteration 6/25 | Loss: 0.00153196
Iteration 7/25 | Loss: 0.00148791
Iteration 8/25 | Loss: 0.00147613
Iteration 9/25 | Loss: 0.00143866
Iteration 10/25 | Loss: 0.00134497
Iteration 11/25 | Loss: 0.00134925
Iteration 12/25 | Loss: 0.00132486
Iteration 13/25 | Loss: 0.00130767
Iteration 14/25 | Loss: 0.00129521
Iteration 15/25 | Loss: 0.00129600
Iteration 16/25 | Loss: 0.00128975
Iteration 17/25 | Loss: 0.00127903
Iteration 18/25 | Loss: 0.00127499
Iteration 19/25 | Loss: 0.00126976
Iteration 20/25 | Loss: 0.00127092
Iteration 21/25 | Loss: 0.00127173
Iteration 22/25 | Loss: 0.00126693
Iteration 23/25 | Loss: 0.00126810
Iteration 24/25 | Loss: 0.00126925
Iteration 25/25 | Loss: 0.00126773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57117617
Iteration 2/25 | Loss: 0.00106321
Iteration 3/25 | Loss: 0.00106321
Iteration 4/25 | Loss: 0.00106321
Iteration 5/25 | Loss: 0.00106321
Iteration 6/25 | Loss: 0.00106321
Iteration 7/25 | Loss: 0.00106321
Iteration 8/25 | Loss: 0.00106321
Iteration 9/25 | Loss: 0.00106321
Iteration 10/25 | Loss: 0.00106321
Iteration 11/25 | Loss: 0.00106321
Iteration 12/25 | Loss: 0.00106321
Iteration 13/25 | Loss: 0.00106321
Iteration 14/25 | Loss: 0.00106321
Iteration 15/25 | Loss: 0.00106321
Iteration 16/25 | Loss: 0.00106321
Iteration 17/25 | Loss: 0.00106321
Iteration 18/25 | Loss: 0.00106321
Iteration 19/25 | Loss: 0.00106321
Iteration 20/25 | Loss: 0.00106321
Iteration 21/25 | Loss: 0.00106321
Iteration 22/25 | Loss: 0.00106321
Iteration 23/25 | Loss: 0.00106321
Iteration 24/25 | Loss: 0.00106321
Iteration 25/25 | Loss: 0.00106321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106321
Iteration 2/1000 | Loss: 0.00023547
Iteration 3/1000 | Loss: 0.00042068
Iteration 4/1000 | Loss: 0.00007926
Iteration 5/1000 | Loss: 0.00011439
Iteration 6/1000 | Loss: 0.00003726
Iteration 7/1000 | Loss: 0.00003268
Iteration 8/1000 | Loss: 0.00002949
Iteration 9/1000 | Loss: 0.00017545
Iteration 10/1000 | Loss: 0.00010558
Iteration 11/1000 | Loss: 0.00010501
Iteration 12/1000 | Loss: 0.00008300
Iteration 13/1000 | Loss: 0.00006514
Iteration 14/1000 | Loss: 0.00046953
Iteration 15/1000 | Loss: 0.00031084
Iteration 16/1000 | Loss: 0.00017651
Iteration 17/1000 | Loss: 0.00018438
Iteration 18/1000 | Loss: 0.00007053
Iteration 19/1000 | Loss: 0.00012816
Iteration 20/1000 | Loss: 0.00019020
Iteration 21/1000 | Loss: 0.00012098
Iteration 22/1000 | Loss: 0.00012015
Iteration 23/1000 | Loss: 0.00015153
Iteration 24/1000 | Loss: 0.00030051
Iteration 25/1000 | Loss: 0.00018566
Iteration 26/1000 | Loss: 0.00015881
Iteration 27/1000 | Loss: 0.00003312
Iteration 28/1000 | Loss: 0.00024917
Iteration 29/1000 | Loss: 0.00011453
Iteration 30/1000 | Loss: 0.00016939
Iteration 31/1000 | Loss: 0.00013843
Iteration 32/1000 | Loss: 0.00003885
Iteration 33/1000 | Loss: 0.00020588
Iteration 34/1000 | Loss: 0.00004263
Iteration 35/1000 | Loss: 0.00011166
Iteration 36/1000 | Loss: 0.00004513
Iteration 37/1000 | Loss: 0.00002419
Iteration 38/1000 | Loss: 0.00016685
Iteration 39/1000 | Loss: 0.00049936
Iteration 40/1000 | Loss: 0.00017344
Iteration 41/1000 | Loss: 0.00002724
Iteration 42/1000 | Loss: 0.00002249
Iteration 43/1000 | Loss: 0.00002054
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00029394
Iteration 47/1000 | Loss: 0.00003188
Iteration 48/1000 | Loss: 0.00002321
Iteration 49/1000 | Loss: 0.00001974
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001821
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001743
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001702
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001698
Iteration 122/1000 | Loss: 0.00001698
Iteration 123/1000 | Loss: 0.00001698
Iteration 124/1000 | Loss: 0.00001697
Iteration 125/1000 | Loss: 0.00001697
Iteration 126/1000 | Loss: 0.00001697
Iteration 127/1000 | Loss: 0.00001697
Iteration 128/1000 | Loss: 0.00001697
Iteration 129/1000 | Loss: 0.00001697
Iteration 130/1000 | Loss: 0.00001697
Iteration 131/1000 | Loss: 0.00001697
Iteration 132/1000 | Loss: 0.00001697
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001696
Iteration 138/1000 | Loss: 0.00001695
Iteration 139/1000 | Loss: 0.00001695
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001695
Iteration 143/1000 | Loss: 0.00001695
Iteration 144/1000 | Loss: 0.00001695
Iteration 145/1000 | Loss: 0.00001694
Iteration 146/1000 | Loss: 0.00001694
Iteration 147/1000 | Loss: 0.00001694
Iteration 148/1000 | Loss: 0.00001694
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001692
Iteration 153/1000 | Loss: 0.00001692
Iteration 154/1000 | Loss: 0.00001692
Iteration 155/1000 | Loss: 0.00001692
Iteration 156/1000 | Loss: 0.00001692
Iteration 157/1000 | Loss: 0.00001691
Iteration 158/1000 | Loss: 0.00001691
Iteration 159/1000 | Loss: 0.00001691
Iteration 160/1000 | Loss: 0.00001691
Iteration 161/1000 | Loss: 0.00001691
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001690
Iteration 164/1000 | Loss: 0.00001690
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001688
Iteration 169/1000 | Loss: 0.00001687
Iteration 170/1000 | Loss: 0.00001686
Iteration 171/1000 | Loss: 0.00001686
Iteration 172/1000 | Loss: 0.00001686
Iteration 173/1000 | Loss: 0.00001685
Iteration 174/1000 | Loss: 0.00001685
Iteration 175/1000 | Loss: 0.00001685
Iteration 176/1000 | Loss: 0.00001685
Iteration 177/1000 | Loss: 0.00001685
Iteration 178/1000 | Loss: 0.00001685
Iteration 179/1000 | Loss: 0.00001684
Iteration 180/1000 | Loss: 0.00001684
Iteration 181/1000 | Loss: 0.00001684
Iteration 182/1000 | Loss: 0.00001684
Iteration 183/1000 | Loss: 0.00001684
Iteration 184/1000 | Loss: 0.00001684
Iteration 185/1000 | Loss: 0.00001684
Iteration 186/1000 | Loss: 0.00001684
Iteration 187/1000 | Loss: 0.00001683
Iteration 188/1000 | Loss: 0.00001683
Iteration 189/1000 | Loss: 0.00001683
Iteration 190/1000 | Loss: 0.00001683
Iteration 191/1000 | Loss: 0.00001682
Iteration 192/1000 | Loss: 0.00001682
Iteration 193/1000 | Loss: 0.00001682
Iteration 194/1000 | Loss: 0.00001682
Iteration 195/1000 | Loss: 0.00001682
Iteration 196/1000 | Loss: 0.00001682
Iteration 197/1000 | Loss: 0.00001682
Iteration 198/1000 | Loss: 0.00001682
Iteration 199/1000 | Loss: 0.00001682
Iteration 200/1000 | Loss: 0.00001681
Iteration 201/1000 | Loss: 0.00001681
Iteration 202/1000 | Loss: 0.00001681
Iteration 203/1000 | Loss: 0.00001681
Iteration 204/1000 | Loss: 0.00001681
Iteration 205/1000 | Loss: 0.00001681
Iteration 206/1000 | Loss: 0.00001681
Iteration 207/1000 | Loss: 0.00001681
Iteration 208/1000 | Loss: 0.00001681
Iteration 209/1000 | Loss: 0.00001681
Iteration 210/1000 | Loss: 0.00001681
Iteration 211/1000 | Loss: 0.00001681
Iteration 212/1000 | Loss: 0.00001681
Iteration 213/1000 | Loss: 0.00001681
Iteration 214/1000 | Loss: 0.00001681
Iteration 215/1000 | Loss: 0.00001681
Iteration 216/1000 | Loss: 0.00001681
Iteration 217/1000 | Loss: 0.00001680
Iteration 218/1000 | Loss: 0.00001680
Iteration 219/1000 | Loss: 0.00001680
Iteration 220/1000 | Loss: 0.00001680
Iteration 221/1000 | Loss: 0.00001680
Iteration 222/1000 | Loss: 0.00001680
Iteration 223/1000 | Loss: 0.00001680
Iteration 224/1000 | Loss: 0.00001680
Iteration 225/1000 | Loss: 0.00001680
Iteration 226/1000 | Loss: 0.00001680
Iteration 227/1000 | Loss: 0.00001680
Iteration 228/1000 | Loss: 0.00001680
Iteration 229/1000 | Loss: 0.00001680
Iteration 230/1000 | Loss: 0.00001680
Iteration 231/1000 | Loss: 0.00001680
Iteration 232/1000 | Loss: 0.00001680
Iteration 233/1000 | Loss: 0.00001680
Iteration 234/1000 | Loss: 0.00001679
Iteration 235/1000 | Loss: 0.00001679
Iteration 236/1000 | Loss: 0.00001679
Iteration 237/1000 | Loss: 0.00001679
Iteration 238/1000 | Loss: 0.00001679
Iteration 239/1000 | Loss: 0.00001679
Iteration 240/1000 | Loss: 0.00001679
Iteration 241/1000 | Loss: 0.00001679
Iteration 242/1000 | Loss: 0.00001679
Iteration 243/1000 | Loss: 0.00001679
Iteration 244/1000 | Loss: 0.00001679
Iteration 245/1000 | Loss: 0.00001679
Iteration 246/1000 | Loss: 0.00001679
Iteration 247/1000 | Loss: 0.00001679
Iteration 248/1000 | Loss: 0.00001678
Iteration 249/1000 | Loss: 0.00001678
Iteration 250/1000 | Loss: 0.00001678
Iteration 251/1000 | Loss: 0.00001678
Iteration 252/1000 | Loss: 0.00001678
Iteration 253/1000 | Loss: 0.00001678
Iteration 254/1000 | Loss: 0.00001678
Iteration 255/1000 | Loss: 0.00001678
Iteration 256/1000 | Loss: 0.00001678
Iteration 257/1000 | Loss: 0.00001678
Iteration 258/1000 | Loss: 0.00001678
Iteration 259/1000 | Loss: 0.00001678
Iteration 260/1000 | Loss: 0.00001678
Iteration 261/1000 | Loss: 0.00001678
Iteration 262/1000 | Loss: 0.00001678
Iteration 263/1000 | Loss: 0.00001678
Iteration 264/1000 | Loss: 0.00001678
Iteration 265/1000 | Loss: 0.00001678
Iteration 266/1000 | Loss: 0.00001678
Iteration 267/1000 | Loss: 0.00001678
Iteration 268/1000 | Loss: 0.00001678
Iteration 269/1000 | Loss: 0.00001678
Iteration 270/1000 | Loss: 0.00001678
Iteration 271/1000 | Loss: 0.00001678
Iteration 272/1000 | Loss: 0.00001678
Iteration 273/1000 | Loss: 0.00001678
Iteration 274/1000 | Loss: 0.00001678
Iteration 275/1000 | Loss: 0.00001678
Iteration 276/1000 | Loss: 0.00001678
Iteration 277/1000 | Loss: 0.00001678
Iteration 278/1000 | Loss: 0.00001678
Iteration 279/1000 | Loss: 0.00001678
Iteration 280/1000 | Loss: 0.00001678
Iteration 281/1000 | Loss: 0.00001678
Iteration 282/1000 | Loss: 0.00001678
Iteration 283/1000 | Loss: 0.00001678
Iteration 284/1000 | Loss: 0.00001678
Iteration 285/1000 | Loss: 0.00001678
Iteration 286/1000 | Loss: 0.00001678
Iteration 287/1000 | Loss: 0.00001678
Iteration 288/1000 | Loss: 0.00001678
Iteration 289/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [1.677910768194124e-05, 1.677910768194124e-05, 1.677910768194124e-05, 1.677910768194124e-05, 1.677910768194124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.677910768194124e-05

Optimization complete. Final v2v error: 3.434438943862915 mm

Highest mean error: 4.660427093505859 mm for frame 72

Lowest mean error: 3.025186061859131 mm for frame 37

Saving results

Total time: 137.64713191986084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428803
Iteration 2/25 | Loss: 0.00134524
Iteration 3/25 | Loss: 0.00125131
Iteration 4/25 | Loss: 0.00123722
Iteration 5/25 | Loss: 0.00123240
Iteration 6/25 | Loss: 0.00123178
Iteration 7/25 | Loss: 0.00123177
Iteration 8/25 | Loss: 0.00123177
Iteration 9/25 | Loss: 0.00123177
Iteration 10/25 | Loss: 0.00123177
Iteration 11/25 | Loss: 0.00123177
Iteration 12/25 | Loss: 0.00123177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00123176584020257, 0.00123176584020257, 0.00123176584020257, 0.00123176584020257, 0.00123176584020257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00123176584020257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49625969
Iteration 2/25 | Loss: 0.00067513
Iteration 3/25 | Loss: 0.00067513
Iteration 4/25 | Loss: 0.00067513
Iteration 5/25 | Loss: 0.00067513
Iteration 6/25 | Loss: 0.00067512
Iteration 7/25 | Loss: 0.00067512
Iteration 8/25 | Loss: 0.00067512
Iteration 9/25 | Loss: 0.00067512
Iteration 10/25 | Loss: 0.00067512
Iteration 11/25 | Loss: 0.00067512
Iteration 12/25 | Loss: 0.00067512
Iteration 13/25 | Loss: 0.00067512
Iteration 14/25 | Loss: 0.00067512
Iteration 15/25 | Loss: 0.00067512
Iteration 16/25 | Loss: 0.00067512
Iteration 17/25 | Loss: 0.00067512
Iteration 18/25 | Loss: 0.00067512
Iteration 19/25 | Loss: 0.00067512
Iteration 20/25 | Loss: 0.00067512
Iteration 21/25 | Loss: 0.00067512
Iteration 22/25 | Loss: 0.00067512
Iteration 23/25 | Loss: 0.00067512
Iteration 24/25 | Loss: 0.00067512
Iteration 25/25 | Loss: 0.00067512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067512
Iteration 2/1000 | Loss: 0.00003175
Iteration 3/1000 | Loss: 0.00002540
Iteration 4/1000 | Loss: 0.00002266
Iteration 5/1000 | Loss: 0.00002166
Iteration 6/1000 | Loss: 0.00002104
Iteration 7/1000 | Loss: 0.00002063
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001997
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001959
Iteration 12/1000 | Loss: 0.00001946
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001915
Iteration 15/1000 | Loss: 0.00001909
Iteration 16/1000 | Loss: 0.00001909
Iteration 17/1000 | Loss: 0.00001908
Iteration 18/1000 | Loss: 0.00001907
Iteration 19/1000 | Loss: 0.00001907
Iteration 20/1000 | Loss: 0.00001906
Iteration 21/1000 | Loss: 0.00001906
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001905
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001899
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001897
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001897
Iteration 31/1000 | Loss: 0.00001897
Iteration 32/1000 | Loss: 0.00001897
Iteration 33/1000 | Loss: 0.00001896
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001887
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001885
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001884
Iteration 44/1000 | Loss: 0.00001884
Iteration 45/1000 | Loss: 0.00001883
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001877
Iteration 58/1000 | Loss: 0.00001877
Iteration 59/1000 | Loss: 0.00001877
Iteration 60/1000 | Loss: 0.00001876
Iteration 61/1000 | Loss: 0.00001876
Iteration 62/1000 | Loss: 0.00001876
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00001876
Iteration 65/1000 | Loss: 0.00001875
Iteration 66/1000 | Loss: 0.00001875
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001875
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001873
Iteration 82/1000 | Loss: 0.00001873
Iteration 83/1000 | Loss: 0.00001873
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001872
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001872
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00001870
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00001870
Iteration 96/1000 | Loss: 0.00001870
Iteration 97/1000 | Loss: 0.00001870
Iteration 98/1000 | Loss: 0.00001870
Iteration 99/1000 | Loss: 0.00001870
Iteration 100/1000 | Loss: 0.00001870
Iteration 101/1000 | Loss: 0.00001869
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001869
Iteration 104/1000 | Loss: 0.00001869
Iteration 105/1000 | Loss: 0.00001869
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001869
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001867
Iteration 118/1000 | Loss: 0.00001867
Iteration 119/1000 | Loss: 0.00001867
Iteration 120/1000 | Loss: 0.00001867
Iteration 121/1000 | Loss: 0.00001867
Iteration 122/1000 | Loss: 0.00001867
Iteration 123/1000 | Loss: 0.00001867
Iteration 124/1000 | Loss: 0.00001866
Iteration 125/1000 | Loss: 0.00001866
Iteration 126/1000 | Loss: 0.00001866
Iteration 127/1000 | Loss: 0.00001866
Iteration 128/1000 | Loss: 0.00001866
Iteration 129/1000 | Loss: 0.00001866
Iteration 130/1000 | Loss: 0.00001866
Iteration 131/1000 | Loss: 0.00001866
Iteration 132/1000 | Loss: 0.00001866
Iteration 133/1000 | Loss: 0.00001866
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001865
Iteration 137/1000 | Loss: 0.00001865
Iteration 138/1000 | Loss: 0.00001865
Iteration 139/1000 | Loss: 0.00001865
Iteration 140/1000 | Loss: 0.00001865
Iteration 141/1000 | Loss: 0.00001865
Iteration 142/1000 | Loss: 0.00001865
Iteration 143/1000 | Loss: 0.00001865
Iteration 144/1000 | Loss: 0.00001865
Iteration 145/1000 | Loss: 0.00001865
Iteration 146/1000 | Loss: 0.00001865
Iteration 147/1000 | Loss: 0.00001864
Iteration 148/1000 | Loss: 0.00001864
Iteration 149/1000 | Loss: 0.00001864
Iteration 150/1000 | Loss: 0.00001864
Iteration 151/1000 | Loss: 0.00001864
Iteration 152/1000 | Loss: 0.00001864
Iteration 153/1000 | Loss: 0.00001864
Iteration 154/1000 | Loss: 0.00001864
Iteration 155/1000 | Loss: 0.00001864
Iteration 156/1000 | Loss: 0.00001864
Iteration 157/1000 | Loss: 0.00001864
Iteration 158/1000 | Loss: 0.00001864
Iteration 159/1000 | Loss: 0.00001864
Iteration 160/1000 | Loss: 0.00001864
Iteration 161/1000 | Loss: 0.00001864
Iteration 162/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.8637530956766568e-05, 1.8637530956766568e-05, 1.8637530956766568e-05, 1.8637530956766568e-05, 1.8637530956766568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8637530956766568e-05

Optimization complete. Final v2v error: 3.6364550590515137 mm

Highest mean error: 4.3442206382751465 mm for frame 19

Lowest mean error: 3.433706521987915 mm for frame 8

Saving results

Total time: 38.33250117301941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035165
Iteration 2/25 | Loss: 0.01035165
Iteration 3/25 | Loss: 0.01035165
Iteration 4/25 | Loss: 0.01035165
Iteration 5/25 | Loss: 0.01035165
Iteration 6/25 | Loss: 0.01035164
Iteration 7/25 | Loss: 0.01035164
Iteration 8/25 | Loss: 0.01035164
Iteration 9/25 | Loss: 0.01035164
Iteration 10/25 | Loss: 0.01035164
Iteration 11/25 | Loss: 0.01035164
Iteration 12/25 | Loss: 0.01035164
Iteration 13/25 | Loss: 0.01035164
Iteration 14/25 | Loss: 0.01035164
Iteration 15/25 | Loss: 0.01035164
Iteration 16/25 | Loss: 0.01035163
Iteration 17/25 | Loss: 0.01035163
Iteration 18/25 | Loss: 0.01035163
Iteration 19/25 | Loss: 0.01035163
Iteration 20/25 | Loss: 0.01035163
Iteration 21/25 | Loss: 0.01035163
Iteration 22/25 | Loss: 0.01035163
Iteration 23/25 | Loss: 0.01035163
Iteration 24/25 | Loss: 0.01035162
Iteration 25/25 | Loss: 0.01035162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04340363
Iteration 2/25 | Loss: 0.18553463
Iteration 3/25 | Loss: 0.17973591
Iteration 4/25 | Loss: 0.17886229
Iteration 5/25 | Loss: 0.17885350
Iteration 6/25 | Loss: 0.17885350
Iteration 7/25 | Loss: 0.17885348
Iteration 8/25 | Loss: 0.17885348
Iteration 9/25 | Loss: 0.17885348
Iteration 10/25 | Loss: 0.17885348
Iteration 11/25 | Loss: 0.17885348
Iteration 12/25 | Loss: 0.17885347
Iteration 13/25 | Loss: 0.17885347
Iteration 14/25 | Loss: 0.17885347
Iteration 15/25 | Loss: 0.17885347
Iteration 16/25 | Loss: 0.17885347
Iteration 17/25 | Loss: 0.17885347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.17885346710681915, 0.17885346710681915, 0.17885346710681915, 0.17885346710681915, 0.17885346710681915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17885346710681915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17885347
Iteration 2/1000 | Loss: 0.00549879
Iteration 3/1000 | Loss: 0.00897546
Iteration 4/1000 | Loss: 0.00233005
Iteration 5/1000 | Loss: 0.00483946
Iteration 6/1000 | Loss: 0.00079141
Iteration 7/1000 | Loss: 0.00273554
Iteration 8/1000 | Loss: 0.00056801
Iteration 9/1000 | Loss: 0.00097717
Iteration 10/1000 | Loss: 0.00028914
Iteration 11/1000 | Loss: 0.00143152
Iteration 12/1000 | Loss: 0.00015113
Iteration 13/1000 | Loss: 0.00049209
Iteration 14/1000 | Loss: 0.00013136
Iteration 15/1000 | Loss: 0.00009551
Iteration 16/1000 | Loss: 0.00050331
Iteration 17/1000 | Loss: 0.00006516
Iteration 18/1000 | Loss: 0.00007990
Iteration 19/1000 | Loss: 0.00005219
Iteration 20/1000 | Loss: 0.00005665
Iteration 21/1000 | Loss: 0.00010590
Iteration 22/1000 | Loss: 0.00012874
Iteration 23/1000 | Loss: 0.00030328
Iteration 24/1000 | Loss: 0.00165254
Iteration 25/1000 | Loss: 0.00031370
Iteration 26/1000 | Loss: 0.00062396
Iteration 27/1000 | Loss: 0.00006540
Iteration 28/1000 | Loss: 0.00033727
Iteration 29/1000 | Loss: 0.00007473
Iteration 30/1000 | Loss: 0.00003089
Iteration 31/1000 | Loss: 0.00003008
Iteration 32/1000 | Loss: 0.00004821
Iteration 33/1000 | Loss: 0.00002904
Iteration 34/1000 | Loss: 0.00044199
Iteration 35/1000 | Loss: 0.00017901
Iteration 36/1000 | Loss: 0.00044357
Iteration 37/1000 | Loss: 0.00003241
Iteration 38/1000 | Loss: 0.00003089
Iteration 39/1000 | Loss: 0.00025173
Iteration 40/1000 | Loss: 0.00010751
Iteration 41/1000 | Loss: 0.00002954
Iteration 42/1000 | Loss: 0.00010265
Iteration 43/1000 | Loss: 0.00002800
Iteration 44/1000 | Loss: 0.00012113
Iteration 45/1000 | Loss: 0.00041668
Iteration 46/1000 | Loss: 0.00022770
Iteration 47/1000 | Loss: 0.00031667
Iteration 48/1000 | Loss: 0.00031319
Iteration 49/1000 | Loss: 0.00005157
Iteration 50/1000 | Loss: 0.00004781
Iteration 51/1000 | Loss: 0.00002776
Iteration 52/1000 | Loss: 0.00008692
Iteration 53/1000 | Loss: 0.00003389
Iteration 54/1000 | Loss: 0.00005178
Iteration 55/1000 | Loss: 0.00002629
Iteration 56/1000 | Loss: 0.00015120
Iteration 57/1000 | Loss: 0.00010303
Iteration 58/1000 | Loss: 0.00004835
Iteration 59/1000 | Loss: 0.00025075
Iteration 60/1000 | Loss: 0.00005029
Iteration 61/1000 | Loss: 0.00026299
Iteration 62/1000 | Loss: 0.00003375
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00013041
Iteration 65/1000 | Loss: 0.00009012
Iteration 66/1000 | Loss: 0.00003980
Iteration 67/1000 | Loss: 0.00002519
Iteration 68/1000 | Loss: 0.00002490
Iteration 69/1000 | Loss: 0.00002464
Iteration 70/1000 | Loss: 0.00043160
Iteration 71/1000 | Loss: 0.00076109
Iteration 72/1000 | Loss: 0.00041008
Iteration 73/1000 | Loss: 0.00010269
Iteration 74/1000 | Loss: 0.00006752
Iteration 75/1000 | Loss: 0.00003179
Iteration 76/1000 | Loss: 0.00006617
Iteration 77/1000 | Loss: 0.00002479
Iteration 78/1000 | Loss: 0.00003349
Iteration 79/1000 | Loss: 0.00002442
Iteration 80/1000 | Loss: 0.00011859
Iteration 81/1000 | Loss: 0.00004386
Iteration 82/1000 | Loss: 0.00005181
Iteration 83/1000 | Loss: 0.00005049
Iteration 84/1000 | Loss: 0.00006640
Iteration 85/1000 | Loss: 0.00002419
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002393
Iteration 88/1000 | Loss: 0.00002393
Iteration 89/1000 | Loss: 0.00002393
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002391
Iteration 93/1000 | Loss: 0.00002391
Iteration 94/1000 | Loss: 0.00002856
Iteration 95/1000 | Loss: 0.00002376
Iteration 96/1000 | Loss: 0.00002376
Iteration 97/1000 | Loss: 0.00002376
Iteration 98/1000 | Loss: 0.00002375
Iteration 99/1000 | Loss: 0.00002375
Iteration 100/1000 | Loss: 0.00002375
Iteration 101/1000 | Loss: 0.00002978
Iteration 102/1000 | Loss: 0.00002538
Iteration 103/1000 | Loss: 0.00002363
Iteration 104/1000 | Loss: 0.00002363
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002361
Iteration 107/1000 | Loss: 0.00002361
Iteration 108/1000 | Loss: 0.00002361
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002360
Iteration 111/1000 | Loss: 0.00002403
Iteration 112/1000 | Loss: 0.00002358
Iteration 113/1000 | Loss: 0.00002358
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00002358
Iteration 116/1000 | Loss: 0.00002357
Iteration 117/1000 | Loss: 0.00002357
Iteration 118/1000 | Loss: 0.00002357
Iteration 119/1000 | Loss: 0.00002357
Iteration 120/1000 | Loss: 0.00002357
Iteration 121/1000 | Loss: 0.00002357
Iteration 122/1000 | Loss: 0.00002357
Iteration 123/1000 | Loss: 0.00002357
Iteration 124/1000 | Loss: 0.00002357
Iteration 125/1000 | Loss: 0.00002357
Iteration 126/1000 | Loss: 0.00002357
Iteration 127/1000 | Loss: 0.00002357
Iteration 128/1000 | Loss: 0.00002356
Iteration 129/1000 | Loss: 0.00002355
Iteration 130/1000 | Loss: 0.00002354
Iteration 131/1000 | Loss: 0.00002354
Iteration 132/1000 | Loss: 0.00002354
Iteration 133/1000 | Loss: 0.00002353
Iteration 134/1000 | Loss: 0.00002353
Iteration 135/1000 | Loss: 0.00002352
Iteration 136/1000 | Loss: 0.00002352
Iteration 137/1000 | Loss: 0.00002351
Iteration 138/1000 | Loss: 0.00002351
Iteration 139/1000 | Loss: 0.00002350
Iteration 140/1000 | Loss: 0.00002350
Iteration 141/1000 | Loss: 0.00002350
Iteration 142/1000 | Loss: 0.00002349
Iteration 143/1000 | Loss: 0.00002349
Iteration 144/1000 | Loss: 0.00002348
Iteration 145/1000 | Loss: 0.00002347
Iteration 146/1000 | Loss: 0.00002347
Iteration 147/1000 | Loss: 0.00002346
Iteration 148/1000 | Loss: 0.00002346
Iteration 149/1000 | Loss: 0.00002403
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002403
Iteration 152/1000 | Loss: 0.00002403
Iteration 153/1000 | Loss: 0.00002400
Iteration 154/1000 | Loss: 0.00006496
Iteration 155/1000 | Loss: 0.00002487
Iteration 156/1000 | Loss: 0.00002529
Iteration 157/1000 | Loss: 0.00002338
Iteration 158/1000 | Loss: 0.00002338
Iteration 159/1000 | Loss: 0.00002337
Iteration 160/1000 | Loss: 0.00002337
Iteration 161/1000 | Loss: 0.00002337
Iteration 162/1000 | Loss: 0.00002337
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002336
Iteration 165/1000 | Loss: 0.00002336
Iteration 166/1000 | Loss: 0.00002336
Iteration 167/1000 | Loss: 0.00002336
Iteration 168/1000 | Loss: 0.00002336
Iteration 169/1000 | Loss: 0.00002336
Iteration 170/1000 | Loss: 0.00002336
Iteration 171/1000 | Loss: 0.00002336
Iteration 172/1000 | Loss: 0.00002336
Iteration 173/1000 | Loss: 0.00002336
Iteration 174/1000 | Loss: 0.00002335
Iteration 175/1000 | Loss: 0.00002335
Iteration 176/1000 | Loss: 0.00002335
Iteration 177/1000 | Loss: 0.00002335
Iteration 178/1000 | Loss: 0.00003720
Iteration 179/1000 | Loss: 0.00003720
Iteration 180/1000 | Loss: 0.00002744
Iteration 181/1000 | Loss: 0.00004328
Iteration 182/1000 | Loss: 0.00002795
Iteration 183/1000 | Loss: 0.00002335
Iteration 184/1000 | Loss: 0.00002335
Iteration 185/1000 | Loss: 0.00002334
Iteration 186/1000 | Loss: 0.00002334
Iteration 187/1000 | Loss: 0.00002334
Iteration 188/1000 | Loss: 0.00002334
Iteration 189/1000 | Loss: 0.00002334
Iteration 190/1000 | Loss: 0.00002334
Iteration 191/1000 | Loss: 0.00002334
Iteration 192/1000 | Loss: 0.00002334
Iteration 193/1000 | Loss: 0.00002334
Iteration 194/1000 | Loss: 0.00002334
Iteration 195/1000 | Loss: 0.00002334
Iteration 196/1000 | Loss: 0.00002334
Iteration 197/1000 | Loss: 0.00002334
Iteration 198/1000 | Loss: 0.00002334
Iteration 199/1000 | Loss: 0.00002334
Iteration 200/1000 | Loss: 0.00002334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.3338910978054628e-05, 2.3338910978054628e-05, 2.3338910978054628e-05, 2.3338910978054628e-05, 2.3338910978054628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3338910978054628e-05

Optimization complete. Final v2v error: 3.997692823410034 mm

Highest mean error: 5.417736053466797 mm for frame 222

Lowest mean error: 3.1356019973754883 mm for frame 37

Saving results

Total time: 167.5455722808838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928762
Iteration 2/25 | Loss: 0.00156554
Iteration 3/25 | Loss: 0.00150261
Iteration 4/25 | Loss: 0.00130417
Iteration 5/25 | Loss: 0.00128887
Iteration 6/25 | Loss: 0.00128904
Iteration 7/25 | Loss: 0.00128483
Iteration 8/25 | Loss: 0.00127719
Iteration 9/25 | Loss: 0.00127169
Iteration 10/25 | Loss: 0.00127161
Iteration 11/25 | Loss: 0.00126517
Iteration 12/25 | Loss: 0.00126148
Iteration 13/25 | Loss: 0.00126056
Iteration 14/25 | Loss: 0.00126032
Iteration 15/25 | Loss: 0.00126030
Iteration 16/25 | Loss: 0.00126029
Iteration 17/25 | Loss: 0.00126029
Iteration 18/25 | Loss: 0.00126029
Iteration 19/25 | Loss: 0.00126029
Iteration 20/25 | Loss: 0.00126029
Iteration 21/25 | Loss: 0.00126029
Iteration 22/25 | Loss: 0.00126029
Iteration 23/25 | Loss: 0.00126029
Iteration 24/25 | Loss: 0.00126029
Iteration 25/25 | Loss: 0.00126029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57147527
Iteration 2/25 | Loss: 0.00081351
Iteration 3/25 | Loss: 0.00066537
Iteration 4/25 | Loss: 0.00066537
Iteration 5/25 | Loss: 0.00066537
Iteration 6/25 | Loss: 0.00066537
Iteration 7/25 | Loss: 0.00066537
Iteration 8/25 | Loss: 0.00066537
Iteration 9/25 | Loss: 0.00066537
Iteration 10/25 | Loss: 0.00066537
Iteration 11/25 | Loss: 0.00066537
Iteration 12/25 | Loss: 0.00066537
Iteration 13/25 | Loss: 0.00066537
Iteration 14/25 | Loss: 0.00066537
Iteration 15/25 | Loss: 0.00066537
Iteration 16/25 | Loss: 0.00066537
Iteration 17/25 | Loss: 0.00066537
Iteration 18/25 | Loss: 0.00066537
Iteration 19/25 | Loss: 0.00066537
Iteration 20/25 | Loss: 0.00066537
Iteration 21/25 | Loss: 0.00066537
Iteration 22/25 | Loss: 0.00066537
Iteration 23/25 | Loss: 0.00066537
Iteration 24/25 | Loss: 0.00066537
Iteration 25/25 | Loss: 0.00066537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066537
Iteration 2/1000 | Loss: 0.00011076
Iteration 3/1000 | Loss: 0.00017020
Iteration 4/1000 | Loss: 0.00004742
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002447
Iteration 7/1000 | Loss: 0.00002374
Iteration 8/1000 | Loss: 0.00002301
Iteration 9/1000 | Loss: 0.00003951
Iteration 10/1000 | Loss: 0.00002224
Iteration 11/1000 | Loss: 0.00002207
Iteration 12/1000 | Loss: 0.00003369
Iteration 13/1000 | Loss: 0.00002460
Iteration 14/1000 | Loss: 0.00002139
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00049700
Iteration 17/1000 | Loss: 0.00002496
Iteration 18/1000 | Loss: 0.00002801
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00004330
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00003840
Iteration 23/1000 | Loss: 0.00001965
Iteration 24/1000 | Loss: 0.00001963
Iteration 25/1000 | Loss: 0.00001946
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00003326
Iteration 28/1000 | Loss: 0.00002123
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001919
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001919
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001918
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00001918
Iteration 40/1000 | Loss: 0.00001918
Iteration 41/1000 | Loss: 0.00001917
Iteration 42/1000 | Loss: 0.00001917
Iteration 43/1000 | Loss: 0.00001917
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001916
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001914
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001913
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001909
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00003251
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001893
Iteration 65/1000 | Loss: 0.00001893
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001892
Iteration 68/1000 | Loss: 0.00001892
Iteration 69/1000 | Loss: 0.00001892
Iteration 70/1000 | Loss: 0.00001892
Iteration 71/1000 | Loss: 0.00001892
Iteration 72/1000 | Loss: 0.00001892
Iteration 73/1000 | Loss: 0.00001892
Iteration 74/1000 | Loss: 0.00001892
Iteration 75/1000 | Loss: 0.00001892
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001891
Iteration 78/1000 | Loss: 0.00001891
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001891
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001890
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001889
Iteration 85/1000 | Loss: 0.00001889
Iteration 86/1000 | Loss: 0.00002626
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001940
Iteration 89/1000 | Loss: 0.00002022
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001880
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001878
Iteration 114/1000 | Loss: 0.00001878
Iteration 115/1000 | Loss: 0.00001877
Iteration 116/1000 | Loss: 0.00001877
Iteration 117/1000 | Loss: 0.00001877
Iteration 118/1000 | Loss: 0.00001877
Iteration 119/1000 | Loss: 0.00001877
Iteration 120/1000 | Loss: 0.00001877
Iteration 121/1000 | Loss: 0.00001877
Iteration 122/1000 | Loss: 0.00001877
Iteration 123/1000 | Loss: 0.00001876
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001876
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001876
Iteration 130/1000 | Loss: 0.00001876
Iteration 131/1000 | Loss: 0.00001876
Iteration 132/1000 | Loss: 0.00001876
Iteration 133/1000 | Loss: 0.00001876
Iteration 134/1000 | Loss: 0.00001876
Iteration 135/1000 | Loss: 0.00001876
Iteration 136/1000 | Loss: 0.00001876
Iteration 137/1000 | Loss: 0.00001875
Iteration 138/1000 | Loss: 0.00001875
Iteration 139/1000 | Loss: 0.00001875
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001875
Iteration 142/1000 | Loss: 0.00001875
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001874
Iteration 145/1000 | Loss: 0.00001874
Iteration 146/1000 | Loss: 0.00001874
Iteration 147/1000 | Loss: 0.00001874
Iteration 148/1000 | Loss: 0.00001874
Iteration 149/1000 | Loss: 0.00001874
Iteration 150/1000 | Loss: 0.00001874
Iteration 151/1000 | Loss: 0.00001874
Iteration 152/1000 | Loss: 0.00001874
Iteration 153/1000 | Loss: 0.00001874
Iteration 154/1000 | Loss: 0.00001874
Iteration 155/1000 | Loss: 0.00001874
Iteration 156/1000 | Loss: 0.00001873
Iteration 157/1000 | Loss: 0.00001873
Iteration 158/1000 | Loss: 0.00001873
Iteration 159/1000 | Loss: 0.00001873
Iteration 160/1000 | Loss: 0.00001873
Iteration 161/1000 | Loss: 0.00001873
Iteration 162/1000 | Loss: 0.00001873
Iteration 163/1000 | Loss: 0.00001873
Iteration 164/1000 | Loss: 0.00001873
Iteration 165/1000 | Loss: 0.00001873
Iteration 166/1000 | Loss: 0.00001873
Iteration 167/1000 | Loss: 0.00001873
Iteration 168/1000 | Loss: 0.00001873
Iteration 169/1000 | Loss: 0.00001873
Iteration 170/1000 | Loss: 0.00001873
Iteration 171/1000 | Loss: 0.00001873
Iteration 172/1000 | Loss: 0.00001872
Iteration 173/1000 | Loss: 0.00001872
Iteration 174/1000 | Loss: 0.00001872
Iteration 175/1000 | Loss: 0.00001872
Iteration 176/1000 | Loss: 0.00001872
Iteration 177/1000 | Loss: 0.00001872
Iteration 178/1000 | Loss: 0.00001872
Iteration 179/1000 | Loss: 0.00001872
Iteration 180/1000 | Loss: 0.00001872
Iteration 181/1000 | Loss: 0.00001872
Iteration 182/1000 | Loss: 0.00001872
Iteration 183/1000 | Loss: 0.00001872
Iteration 184/1000 | Loss: 0.00001872
Iteration 185/1000 | Loss: 0.00001872
Iteration 186/1000 | Loss: 0.00001872
Iteration 187/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.8720296793617308e-05, 1.8720296793617308e-05, 1.8720296793617308e-05, 1.8720296793617308e-05, 1.8720296793617308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8720296793617308e-05

Optimization complete. Final v2v error: 3.6170527935028076 mm

Highest mean error: 4.305752754211426 mm for frame 187

Lowest mean error: 3.01871657371521 mm for frame 200

Saving results

Total time: 84.23476004600525
