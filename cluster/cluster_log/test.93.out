Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=93, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5208-5263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885713
Iteration 2/25 | Loss: 0.00148925
Iteration 3/25 | Loss: 0.00096471
Iteration 4/25 | Loss: 0.00088883
Iteration 5/25 | Loss: 0.00088359
Iteration 6/25 | Loss: 0.00087275
Iteration 7/25 | Loss: 0.00086381
Iteration 8/25 | Loss: 0.00086441
Iteration 9/25 | Loss: 0.00086040
Iteration 10/25 | Loss: 0.00085799
Iteration 11/25 | Loss: 0.00085985
Iteration 12/25 | Loss: 0.00085502
Iteration 13/25 | Loss: 0.00085465
Iteration 14/25 | Loss: 0.00085455
Iteration 15/25 | Loss: 0.00085452
Iteration 16/25 | Loss: 0.00085451
Iteration 17/25 | Loss: 0.00085451
Iteration 18/25 | Loss: 0.00085451
Iteration 19/25 | Loss: 0.00085450
Iteration 20/25 | Loss: 0.00085450
Iteration 21/25 | Loss: 0.00085450
Iteration 22/25 | Loss: 0.00085450
Iteration 23/25 | Loss: 0.00085450
Iteration 24/25 | Loss: 0.00085450
Iteration 25/25 | Loss: 0.00085450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21424532
Iteration 2/25 | Loss: 0.00051290
Iteration 3/25 | Loss: 0.00051290
Iteration 4/25 | Loss: 0.00051290
Iteration 5/25 | Loss: 0.00051290
Iteration 6/25 | Loss: 0.00051290
Iteration 7/25 | Loss: 0.00051290
Iteration 8/25 | Loss: 0.00051290
Iteration 9/25 | Loss: 0.00051290
Iteration 10/25 | Loss: 0.00051290
Iteration 11/25 | Loss: 0.00051290
Iteration 12/25 | Loss: 0.00051290
Iteration 13/25 | Loss: 0.00051290
Iteration 14/25 | Loss: 0.00051290
Iteration 15/25 | Loss: 0.00051290
Iteration 16/25 | Loss: 0.00051290
Iteration 17/25 | Loss: 0.00051290
Iteration 18/25 | Loss: 0.00051290
Iteration 19/25 | Loss: 0.00051290
Iteration 20/25 | Loss: 0.00051290
Iteration 21/25 | Loss: 0.00051290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005128973862156272, 0.0005128973862156272, 0.0005128973862156272, 0.0005128973862156272, 0.0005128973862156272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005128973862156272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051290
Iteration 2/1000 | Loss: 0.00001762
Iteration 3/1000 | Loss: 0.00005099
Iteration 4/1000 | Loss: 0.00005055
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001084
Iteration 7/1000 | Loss: 0.00001054
Iteration 8/1000 | Loss: 0.00005928
Iteration 9/1000 | Loss: 0.00001020
Iteration 10/1000 | Loss: 0.00001019
Iteration 11/1000 | Loss: 0.00001018
Iteration 12/1000 | Loss: 0.00001013
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001012
Iteration 15/1000 | Loss: 0.00001012
Iteration 16/1000 | Loss: 0.00000996
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000987
Iteration 19/1000 | Loss: 0.00000986
Iteration 20/1000 | Loss: 0.00000984
Iteration 21/1000 | Loss: 0.00000983
Iteration 22/1000 | Loss: 0.00000983
Iteration 23/1000 | Loss: 0.00000982
Iteration 24/1000 | Loss: 0.00000982
Iteration 25/1000 | Loss: 0.00000982
Iteration 26/1000 | Loss: 0.00000982
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000980
Iteration 30/1000 | Loss: 0.00000980
Iteration 31/1000 | Loss: 0.00000979
Iteration 32/1000 | Loss: 0.00000979
Iteration 33/1000 | Loss: 0.00000978
Iteration 34/1000 | Loss: 0.00000978
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000978
Iteration 37/1000 | Loss: 0.00000978
Iteration 38/1000 | Loss: 0.00000978
Iteration 39/1000 | Loss: 0.00000978
Iteration 40/1000 | Loss: 0.00000978
Iteration 41/1000 | Loss: 0.00000978
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000974
Iteration 51/1000 | Loss: 0.00000974
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000973
Iteration 54/1000 | Loss: 0.00004584
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00000968
Iteration 58/1000 | Loss: 0.00000968
Iteration 59/1000 | Loss: 0.00000968
Iteration 60/1000 | Loss: 0.00000968
Iteration 61/1000 | Loss: 0.00000968
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000968
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000968
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000967
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000967
Iteration 74/1000 | Loss: 0.00000967
Iteration 75/1000 | Loss: 0.00000967
Iteration 76/1000 | Loss: 0.00000966
Iteration 77/1000 | Loss: 0.00000966
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00001902
Iteration 81/1000 | Loss: 0.00000966
Iteration 82/1000 | Loss: 0.00000966
Iteration 83/1000 | Loss: 0.00000966
Iteration 84/1000 | Loss: 0.00000966
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000965
Iteration 87/1000 | Loss: 0.00000965
Iteration 88/1000 | Loss: 0.00000965
Iteration 89/1000 | Loss: 0.00000964
Iteration 90/1000 | Loss: 0.00000964
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000964
Iteration 96/1000 | Loss: 0.00000964
Iteration 97/1000 | Loss: 0.00000964
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000963
Iteration 101/1000 | Loss: 0.00000963
Iteration 102/1000 | Loss: 0.00000963
Iteration 103/1000 | Loss: 0.00000963
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000962
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000962
Iteration 112/1000 | Loss: 0.00000962
Iteration 113/1000 | Loss: 0.00000962
Iteration 114/1000 | Loss: 0.00000962
Iteration 115/1000 | Loss: 0.00000962
Iteration 116/1000 | Loss: 0.00000962
Iteration 117/1000 | Loss: 0.00000962
Iteration 118/1000 | Loss: 0.00000962
Iteration 119/1000 | Loss: 0.00000962
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Iteration 130/1000 | Loss: 0.00000961
Iteration 131/1000 | Loss: 0.00000961
Iteration 132/1000 | Loss: 0.00000961
Iteration 133/1000 | Loss: 0.00000961
Iteration 134/1000 | Loss: 0.00000961
Iteration 135/1000 | Loss: 0.00000961
Iteration 136/1000 | Loss: 0.00000961
Iteration 137/1000 | Loss: 0.00000961
Iteration 138/1000 | Loss: 0.00000960
Iteration 139/1000 | Loss: 0.00000960
Iteration 140/1000 | Loss: 0.00000960
Iteration 141/1000 | Loss: 0.00000960
Iteration 142/1000 | Loss: 0.00004406
Iteration 143/1000 | Loss: 0.00000963
Iteration 144/1000 | Loss: 0.00000960
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000957
Iteration 151/1000 | Loss: 0.00000957
Iteration 152/1000 | Loss: 0.00000957
Iteration 153/1000 | Loss: 0.00000957
Iteration 154/1000 | Loss: 0.00000957
Iteration 155/1000 | Loss: 0.00000957
Iteration 156/1000 | Loss: 0.00000957
Iteration 157/1000 | Loss: 0.00000957
Iteration 158/1000 | Loss: 0.00000957
Iteration 159/1000 | Loss: 0.00000957
Iteration 160/1000 | Loss: 0.00000956
Iteration 161/1000 | Loss: 0.00000956
Iteration 162/1000 | Loss: 0.00000956
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000955
Iteration 168/1000 | Loss: 0.00000955
Iteration 169/1000 | Loss: 0.00000955
Iteration 170/1000 | Loss: 0.00000955
Iteration 171/1000 | Loss: 0.00000955
Iteration 172/1000 | Loss: 0.00000955
Iteration 173/1000 | Loss: 0.00000955
Iteration 174/1000 | Loss: 0.00000955
Iteration 175/1000 | Loss: 0.00000955
Iteration 176/1000 | Loss: 0.00000955
Iteration 177/1000 | Loss: 0.00000955
Iteration 178/1000 | Loss: 0.00000955
Iteration 179/1000 | Loss: 0.00000955
Iteration 180/1000 | Loss: 0.00000955
Iteration 181/1000 | Loss: 0.00000955
Iteration 182/1000 | Loss: 0.00000955
Iteration 183/1000 | Loss: 0.00000955
Iteration 184/1000 | Loss: 0.00000955
Iteration 185/1000 | Loss: 0.00000955
Iteration 186/1000 | Loss: 0.00000955
Iteration 187/1000 | Loss: 0.00000955
Iteration 188/1000 | Loss: 0.00000955
Iteration 189/1000 | Loss: 0.00000955
Iteration 190/1000 | Loss: 0.00000955
Iteration 191/1000 | Loss: 0.00000955
Iteration 192/1000 | Loss: 0.00000955
Iteration 193/1000 | Loss: 0.00000955
Iteration 194/1000 | Loss: 0.00000955
Iteration 195/1000 | Loss: 0.00000955
Iteration 196/1000 | Loss: 0.00000955
Iteration 197/1000 | Loss: 0.00000955
Iteration 198/1000 | Loss: 0.00000955
Iteration 199/1000 | Loss: 0.00000955
Iteration 200/1000 | Loss: 0.00000955
Iteration 201/1000 | Loss: 0.00000955
Iteration 202/1000 | Loss: 0.00000955
Iteration 203/1000 | Loss: 0.00000955
Iteration 204/1000 | Loss: 0.00000955
Iteration 205/1000 | Loss: 0.00000955
Iteration 206/1000 | Loss: 0.00000955
Iteration 207/1000 | Loss: 0.00000955
Iteration 208/1000 | Loss: 0.00000955
Iteration 209/1000 | Loss: 0.00000955
Iteration 210/1000 | Loss: 0.00000955
Iteration 211/1000 | Loss: 0.00000955
Iteration 212/1000 | Loss: 0.00000955
Iteration 213/1000 | Loss: 0.00000955
Iteration 214/1000 | Loss: 0.00000955
Iteration 215/1000 | Loss: 0.00000955
Iteration 216/1000 | Loss: 0.00000955
Iteration 217/1000 | Loss: 0.00000955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.54703409661306e-06, 9.54703409661306e-06, 9.54703409661306e-06, 9.54703409661306e-06, 9.54703409661306e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.54703409661306e-06

Optimization complete. Final v2v error: 2.572460651397705 mm

Highest mean error: 8.288533210754395 mm for frame 44

Lowest mean error: 2.24536395072937 mm for frame 225

Saving results

Total time: 70.89922380447388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873512
Iteration 2/25 | Loss: 0.00216224
Iteration 3/25 | Loss: 0.00170954
Iteration 4/25 | Loss: 0.00168735
Iteration 5/25 | Loss: 0.00146952
Iteration 6/25 | Loss: 0.00136959
Iteration 7/25 | Loss: 0.00133799
Iteration 8/25 | Loss: 0.00133876
Iteration 9/25 | Loss: 0.00133757
Iteration 10/25 | Loss: 0.00133768
Iteration 11/25 | Loss: 0.00131301
Iteration 12/25 | Loss: 0.00130379
Iteration 13/25 | Loss: 0.00131915
Iteration 14/25 | Loss: 0.00130819
Iteration 15/25 | Loss: 0.00134806
Iteration 16/25 | Loss: 0.00127306
Iteration 17/25 | Loss: 0.00127373
Iteration 18/25 | Loss: 0.00123318
Iteration 19/25 | Loss: 0.00122762
Iteration 20/25 | Loss: 0.00121063
Iteration 21/25 | Loss: 0.00120956
Iteration 22/25 | Loss: 0.00120931
Iteration 23/25 | Loss: 0.00120926
Iteration 24/25 | Loss: 0.00120926
Iteration 25/25 | Loss: 0.00120925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54848921
Iteration 2/25 | Loss: 0.00166017
Iteration 3/25 | Loss: 0.00165978
Iteration 4/25 | Loss: 0.00165978
Iteration 5/25 | Loss: 0.00165978
Iteration 6/25 | Loss: 0.00165978
Iteration 7/25 | Loss: 0.00165978
Iteration 8/25 | Loss: 0.00165978
Iteration 9/25 | Loss: 0.00165978
Iteration 10/25 | Loss: 0.00165978
Iteration 11/25 | Loss: 0.00165978
Iteration 12/25 | Loss: 0.00165978
Iteration 13/25 | Loss: 0.00165978
Iteration 14/25 | Loss: 0.00165978
Iteration 15/25 | Loss: 0.00165978
Iteration 16/25 | Loss: 0.00165978
Iteration 17/25 | Loss: 0.00165978
Iteration 18/25 | Loss: 0.00165978
Iteration 19/25 | Loss: 0.00165978
Iteration 20/25 | Loss: 0.00165978
Iteration 21/25 | Loss: 0.00165978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016597791109234095, 0.0016597791109234095, 0.0016597791109234095, 0.0016597791109234095, 0.0016597791109234095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016597791109234095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165978
Iteration 2/1000 | Loss: 0.00358239
Iteration 3/1000 | Loss: 0.00015845
Iteration 4/1000 | Loss: 0.00011301
Iteration 5/1000 | Loss: 0.00008204
Iteration 6/1000 | Loss: 0.00007246
Iteration 7/1000 | Loss: 0.00006603
Iteration 8/1000 | Loss: 0.00013764
Iteration 9/1000 | Loss: 0.00006363
Iteration 10/1000 | Loss: 0.00005756
Iteration 11/1000 | Loss: 0.00005596
Iteration 12/1000 | Loss: 0.00005506
Iteration 13/1000 | Loss: 0.00005413
Iteration 14/1000 | Loss: 0.00006458
Iteration 15/1000 | Loss: 0.00005704
Iteration 16/1000 | Loss: 0.00005380
Iteration 17/1000 | Loss: 0.00005265
Iteration 18/1000 | Loss: 0.00005175
Iteration 19/1000 | Loss: 0.00005118
Iteration 20/1000 | Loss: 0.00005072
Iteration 21/1000 | Loss: 0.00005044
Iteration 22/1000 | Loss: 0.00005020
Iteration 23/1000 | Loss: 0.00006454
Iteration 24/1000 | Loss: 0.00011401
Iteration 25/1000 | Loss: 0.00005532
Iteration 26/1000 | Loss: 0.00005116
Iteration 27/1000 | Loss: 0.00004983
Iteration 28/1000 | Loss: 0.00004903
Iteration 29/1000 | Loss: 0.00004859
Iteration 30/1000 | Loss: 0.00004841
Iteration 31/1000 | Loss: 0.00004828
Iteration 32/1000 | Loss: 0.00004822
Iteration 33/1000 | Loss: 0.00004820
Iteration 34/1000 | Loss: 0.00004815
Iteration 35/1000 | Loss: 0.00004815
Iteration 36/1000 | Loss: 0.00004813
Iteration 37/1000 | Loss: 0.00004812
Iteration 38/1000 | Loss: 0.00004811
Iteration 39/1000 | Loss: 0.00004811
Iteration 40/1000 | Loss: 0.00004811
Iteration 41/1000 | Loss: 0.00004809
Iteration 42/1000 | Loss: 0.00004804
Iteration 43/1000 | Loss: 0.00004804
Iteration 44/1000 | Loss: 0.00004804
Iteration 45/1000 | Loss: 0.00004804
Iteration 46/1000 | Loss: 0.00004803
Iteration 47/1000 | Loss: 0.00004803
Iteration 48/1000 | Loss: 0.00004802
Iteration 49/1000 | Loss: 0.00004802
Iteration 50/1000 | Loss: 0.00004801
Iteration 51/1000 | Loss: 0.00004801
Iteration 52/1000 | Loss: 0.00004801
Iteration 53/1000 | Loss: 0.00004800
Iteration 54/1000 | Loss: 0.00004800
Iteration 55/1000 | Loss: 0.00004799
Iteration 56/1000 | Loss: 0.00004799
Iteration 57/1000 | Loss: 0.00004799
Iteration 58/1000 | Loss: 0.00004798
Iteration 59/1000 | Loss: 0.00004798
Iteration 60/1000 | Loss: 0.00004797
Iteration 61/1000 | Loss: 0.00004796
Iteration 62/1000 | Loss: 0.00004796
Iteration 63/1000 | Loss: 0.00004796
Iteration 64/1000 | Loss: 0.00004796
Iteration 65/1000 | Loss: 0.00004795
Iteration 66/1000 | Loss: 0.00004795
Iteration 67/1000 | Loss: 0.00004795
Iteration 68/1000 | Loss: 0.00004794
Iteration 69/1000 | Loss: 0.00004794
Iteration 70/1000 | Loss: 0.00004794
Iteration 71/1000 | Loss: 0.00004794
Iteration 72/1000 | Loss: 0.00004794
Iteration 73/1000 | Loss: 0.00004794
Iteration 74/1000 | Loss: 0.00004794
Iteration 75/1000 | Loss: 0.00004794
Iteration 76/1000 | Loss: 0.00004794
Iteration 77/1000 | Loss: 0.00004793
Iteration 78/1000 | Loss: 0.00004793
Iteration 79/1000 | Loss: 0.00004793
Iteration 80/1000 | Loss: 0.00004793
Iteration 81/1000 | Loss: 0.00004793
Iteration 82/1000 | Loss: 0.00004792
Iteration 83/1000 | Loss: 0.00004792
Iteration 84/1000 | Loss: 0.00004792
Iteration 85/1000 | Loss: 0.00004792
Iteration 86/1000 | Loss: 0.00004792
Iteration 87/1000 | Loss: 0.00004792
Iteration 88/1000 | Loss: 0.00004792
Iteration 89/1000 | Loss: 0.00004792
Iteration 90/1000 | Loss: 0.00004792
Iteration 91/1000 | Loss: 0.00004791
Iteration 92/1000 | Loss: 0.00004791
Iteration 93/1000 | Loss: 0.00004791
Iteration 94/1000 | Loss: 0.00004791
Iteration 95/1000 | Loss: 0.00004791
Iteration 96/1000 | Loss: 0.00004791
Iteration 97/1000 | Loss: 0.00004791
Iteration 98/1000 | Loss: 0.00004791
Iteration 99/1000 | Loss: 0.00004791
Iteration 100/1000 | Loss: 0.00004791
Iteration 101/1000 | Loss: 0.00004791
Iteration 102/1000 | Loss: 0.00004791
Iteration 103/1000 | Loss: 0.00004791
Iteration 104/1000 | Loss: 0.00004791
Iteration 105/1000 | Loss: 0.00004791
Iteration 106/1000 | Loss: 0.00004791
Iteration 107/1000 | Loss: 0.00004790
Iteration 108/1000 | Loss: 0.00004790
Iteration 109/1000 | Loss: 0.00004790
Iteration 110/1000 | Loss: 0.00004790
Iteration 111/1000 | Loss: 0.00004790
Iteration 112/1000 | Loss: 0.00004790
Iteration 113/1000 | Loss: 0.00004790
Iteration 114/1000 | Loss: 0.00004790
Iteration 115/1000 | Loss: 0.00004790
Iteration 116/1000 | Loss: 0.00004790
Iteration 117/1000 | Loss: 0.00004789
Iteration 118/1000 | Loss: 0.00004789
Iteration 119/1000 | Loss: 0.00004789
Iteration 120/1000 | Loss: 0.00004789
Iteration 121/1000 | Loss: 0.00004789
Iteration 122/1000 | Loss: 0.00004789
Iteration 123/1000 | Loss: 0.00004789
Iteration 124/1000 | Loss: 0.00004789
Iteration 125/1000 | Loss: 0.00004789
Iteration 126/1000 | Loss: 0.00004788
Iteration 127/1000 | Loss: 0.00004788
Iteration 128/1000 | Loss: 0.00004788
Iteration 129/1000 | Loss: 0.00004788
Iteration 130/1000 | Loss: 0.00004788
Iteration 131/1000 | Loss: 0.00004788
Iteration 132/1000 | Loss: 0.00004788
Iteration 133/1000 | Loss: 0.00004788
Iteration 134/1000 | Loss: 0.00004788
Iteration 135/1000 | Loss: 0.00004788
Iteration 136/1000 | Loss: 0.00004788
Iteration 137/1000 | Loss: 0.00004788
Iteration 138/1000 | Loss: 0.00004788
Iteration 139/1000 | Loss: 0.00004788
Iteration 140/1000 | Loss: 0.00004788
Iteration 141/1000 | Loss: 0.00004788
Iteration 142/1000 | Loss: 0.00004788
Iteration 143/1000 | Loss: 0.00004787
Iteration 144/1000 | Loss: 0.00004787
Iteration 145/1000 | Loss: 0.00004787
Iteration 146/1000 | Loss: 0.00004787
Iteration 147/1000 | Loss: 0.00004787
Iteration 148/1000 | Loss: 0.00004787
Iteration 149/1000 | Loss: 0.00004787
Iteration 150/1000 | Loss: 0.00004787
Iteration 151/1000 | Loss: 0.00004787
Iteration 152/1000 | Loss: 0.00004787
Iteration 153/1000 | Loss: 0.00004787
Iteration 154/1000 | Loss: 0.00004787
Iteration 155/1000 | Loss: 0.00004787
Iteration 156/1000 | Loss: 0.00004786
Iteration 157/1000 | Loss: 0.00004786
Iteration 158/1000 | Loss: 0.00004786
Iteration 159/1000 | Loss: 0.00004786
Iteration 160/1000 | Loss: 0.00004786
Iteration 161/1000 | Loss: 0.00004786
Iteration 162/1000 | Loss: 0.00004786
Iteration 163/1000 | Loss: 0.00004786
Iteration 164/1000 | Loss: 0.00004786
Iteration 165/1000 | Loss: 0.00004786
Iteration 166/1000 | Loss: 0.00004786
Iteration 167/1000 | Loss: 0.00004786
Iteration 168/1000 | Loss: 0.00004786
Iteration 169/1000 | Loss: 0.00004786
Iteration 170/1000 | Loss: 0.00004786
Iteration 171/1000 | Loss: 0.00004786
Iteration 172/1000 | Loss: 0.00004786
Iteration 173/1000 | Loss: 0.00004786
Iteration 174/1000 | Loss: 0.00004786
Iteration 175/1000 | Loss: 0.00004786
Iteration 176/1000 | Loss: 0.00004786
Iteration 177/1000 | Loss: 0.00004786
Iteration 178/1000 | Loss: 0.00004786
Iteration 179/1000 | Loss: 0.00004786
Iteration 180/1000 | Loss: 0.00004786
Iteration 181/1000 | Loss: 0.00004786
Iteration 182/1000 | Loss: 0.00004786
Iteration 183/1000 | Loss: 0.00004786
Iteration 184/1000 | Loss: 0.00004786
Iteration 185/1000 | Loss: 0.00004786
Iteration 186/1000 | Loss: 0.00004786
Iteration 187/1000 | Loss: 0.00004786
Iteration 188/1000 | Loss: 0.00004786
Iteration 189/1000 | Loss: 0.00004786
Iteration 190/1000 | Loss: 0.00004786
Iteration 191/1000 | Loss: 0.00004786
Iteration 192/1000 | Loss: 0.00004786
Iteration 193/1000 | Loss: 0.00004786
Iteration 194/1000 | Loss: 0.00004786
Iteration 195/1000 | Loss: 0.00004786
Iteration 196/1000 | Loss: 0.00004786
Iteration 197/1000 | Loss: 0.00004786
Iteration 198/1000 | Loss: 0.00004786
Iteration 199/1000 | Loss: 0.00004786
Iteration 200/1000 | Loss: 0.00004786
Iteration 201/1000 | Loss: 0.00004786
Iteration 202/1000 | Loss: 0.00004786
Iteration 203/1000 | Loss: 0.00004786
Iteration 204/1000 | Loss: 0.00004786
Iteration 205/1000 | Loss: 0.00004786
Iteration 206/1000 | Loss: 0.00004786
Iteration 207/1000 | Loss: 0.00004786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [4.785871351487003e-05, 4.785871351487003e-05, 4.785871351487003e-05, 4.785871351487003e-05, 4.785871351487003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.785871351487003e-05

Optimization complete. Final v2v error: 5.056019306182861 mm

Highest mean error: 11.311339378356934 mm for frame 42

Lowest mean error: 3.077892303466797 mm for frame 83

Saving results

Total time: 110.39416646957397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404980
Iteration 2/25 | Loss: 0.00095286
Iteration 3/25 | Loss: 0.00084282
Iteration 4/25 | Loss: 0.00082992
Iteration 5/25 | Loss: 0.00082651
Iteration 6/25 | Loss: 0.00082543
Iteration 7/25 | Loss: 0.00082533
Iteration 8/25 | Loss: 0.00082533
Iteration 9/25 | Loss: 0.00082533
Iteration 10/25 | Loss: 0.00082533
Iteration 11/25 | Loss: 0.00082533
Iteration 12/25 | Loss: 0.00082533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000825327995698899, 0.000825327995698899, 0.000825327995698899, 0.000825327995698899, 0.000825327995698899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000825327995698899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.78340197
Iteration 2/25 | Loss: 0.00044805
Iteration 3/25 | Loss: 0.00044803
Iteration 4/25 | Loss: 0.00044803
Iteration 5/25 | Loss: 0.00044803
Iteration 6/25 | Loss: 0.00044803
Iteration 7/25 | Loss: 0.00044803
Iteration 8/25 | Loss: 0.00044803
Iteration 9/25 | Loss: 0.00044803
Iteration 10/25 | Loss: 0.00044803
Iteration 11/25 | Loss: 0.00044803
Iteration 12/25 | Loss: 0.00044803
Iteration 13/25 | Loss: 0.00044803
Iteration 14/25 | Loss: 0.00044803
Iteration 15/25 | Loss: 0.00044803
Iteration 16/25 | Loss: 0.00044803
Iteration 17/25 | Loss: 0.00044803
Iteration 18/25 | Loss: 0.00044803
Iteration 19/25 | Loss: 0.00044803
Iteration 20/25 | Loss: 0.00044803
Iteration 21/25 | Loss: 0.00044803
Iteration 22/25 | Loss: 0.00044803
Iteration 23/25 | Loss: 0.00044803
Iteration 24/25 | Loss: 0.00044803
Iteration 25/25 | Loss: 0.00044803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044803
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001389
Iteration 4/1000 | Loss: 0.00001087
Iteration 5/1000 | Loss: 0.00001022
Iteration 6/1000 | Loss: 0.00000978
Iteration 7/1000 | Loss: 0.00000956
Iteration 8/1000 | Loss: 0.00000938
Iteration 9/1000 | Loss: 0.00000929
Iteration 10/1000 | Loss: 0.00000927
Iteration 11/1000 | Loss: 0.00000920
Iteration 12/1000 | Loss: 0.00000919
Iteration 13/1000 | Loss: 0.00000913
Iteration 14/1000 | Loss: 0.00000912
Iteration 15/1000 | Loss: 0.00000909
Iteration 16/1000 | Loss: 0.00000908
Iteration 17/1000 | Loss: 0.00000903
Iteration 18/1000 | Loss: 0.00000900
Iteration 19/1000 | Loss: 0.00000899
Iteration 20/1000 | Loss: 0.00000899
Iteration 21/1000 | Loss: 0.00000899
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000898
Iteration 24/1000 | Loss: 0.00000897
Iteration 25/1000 | Loss: 0.00000897
Iteration 26/1000 | Loss: 0.00000896
Iteration 27/1000 | Loss: 0.00000896
Iteration 28/1000 | Loss: 0.00000896
Iteration 29/1000 | Loss: 0.00000896
Iteration 30/1000 | Loss: 0.00000896
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000896
Iteration 33/1000 | Loss: 0.00000895
Iteration 34/1000 | Loss: 0.00000895
Iteration 35/1000 | Loss: 0.00000895
Iteration 36/1000 | Loss: 0.00000894
Iteration 37/1000 | Loss: 0.00000894
Iteration 38/1000 | Loss: 0.00000894
Iteration 39/1000 | Loss: 0.00000894
Iteration 40/1000 | Loss: 0.00000894
Iteration 41/1000 | Loss: 0.00000893
Iteration 42/1000 | Loss: 0.00000893
Iteration 43/1000 | Loss: 0.00000893
Iteration 44/1000 | Loss: 0.00000893
Iteration 45/1000 | Loss: 0.00000892
Iteration 46/1000 | Loss: 0.00000892
Iteration 47/1000 | Loss: 0.00000892
Iteration 48/1000 | Loss: 0.00000891
Iteration 49/1000 | Loss: 0.00000891
Iteration 50/1000 | Loss: 0.00000891
Iteration 51/1000 | Loss: 0.00000891
Iteration 52/1000 | Loss: 0.00000890
Iteration 53/1000 | Loss: 0.00000890
Iteration 54/1000 | Loss: 0.00000890
Iteration 55/1000 | Loss: 0.00000890
Iteration 56/1000 | Loss: 0.00000890
Iteration 57/1000 | Loss: 0.00000890
Iteration 58/1000 | Loss: 0.00000890
Iteration 59/1000 | Loss: 0.00000889
Iteration 60/1000 | Loss: 0.00000889
Iteration 61/1000 | Loss: 0.00000889
Iteration 62/1000 | Loss: 0.00000889
Iteration 63/1000 | Loss: 0.00000889
Iteration 64/1000 | Loss: 0.00000889
Iteration 65/1000 | Loss: 0.00000889
Iteration 66/1000 | Loss: 0.00000889
Iteration 67/1000 | Loss: 0.00000888
Iteration 68/1000 | Loss: 0.00000888
Iteration 69/1000 | Loss: 0.00000887
Iteration 70/1000 | Loss: 0.00000887
Iteration 71/1000 | Loss: 0.00000887
Iteration 72/1000 | Loss: 0.00000886
Iteration 73/1000 | Loss: 0.00000886
Iteration 74/1000 | Loss: 0.00000886
Iteration 75/1000 | Loss: 0.00000886
Iteration 76/1000 | Loss: 0.00000886
Iteration 77/1000 | Loss: 0.00000886
Iteration 78/1000 | Loss: 0.00000886
Iteration 79/1000 | Loss: 0.00000886
Iteration 80/1000 | Loss: 0.00000886
Iteration 81/1000 | Loss: 0.00000886
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000886
Iteration 89/1000 | Loss: 0.00000886
Iteration 90/1000 | Loss: 0.00000886
Iteration 91/1000 | Loss: 0.00000886
Iteration 92/1000 | Loss: 0.00000886
Iteration 93/1000 | Loss: 0.00000885
Iteration 94/1000 | Loss: 0.00000885
Iteration 95/1000 | Loss: 0.00000885
Iteration 96/1000 | Loss: 0.00000885
Iteration 97/1000 | Loss: 0.00000885
Iteration 98/1000 | Loss: 0.00000885
Iteration 99/1000 | Loss: 0.00000885
Iteration 100/1000 | Loss: 0.00000885
Iteration 101/1000 | Loss: 0.00000885
Iteration 102/1000 | Loss: 0.00000885
Iteration 103/1000 | Loss: 0.00000885
Iteration 104/1000 | Loss: 0.00000885
Iteration 105/1000 | Loss: 0.00000885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [8.854316547513008e-06, 8.854316547513008e-06, 8.854316547513008e-06, 8.854316547513008e-06, 8.854316547513008e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.854316547513008e-06

Optimization complete. Final v2v error: 2.5285441875457764 mm

Highest mean error: 3.1500678062438965 mm for frame 85

Lowest mean error: 2.194538116455078 mm for frame 42

Saving results

Total time: 31.024413108825684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450493
Iteration 2/25 | Loss: 0.00093825
Iteration 3/25 | Loss: 0.00083706
Iteration 4/25 | Loss: 0.00082730
Iteration 5/25 | Loss: 0.00082498
Iteration 6/25 | Loss: 0.00082410
Iteration 7/25 | Loss: 0.00082400
Iteration 8/25 | Loss: 0.00082400
Iteration 9/25 | Loss: 0.00082400
Iteration 10/25 | Loss: 0.00082400
Iteration 11/25 | Loss: 0.00082400
Iteration 12/25 | Loss: 0.00082400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008239978342317045, 0.0008239978342317045, 0.0008239978342317045, 0.0008239978342317045, 0.0008239978342317045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008239978342317045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40284562
Iteration 2/25 | Loss: 0.00049174
Iteration 3/25 | Loss: 0.00049172
Iteration 4/25 | Loss: 0.00049172
Iteration 5/25 | Loss: 0.00049172
Iteration 6/25 | Loss: 0.00049172
Iteration 7/25 | Loss: 0.00049172
Iteration 8/25 | Loss: 0.00049172
Iteration 9/25 | Loss: 0.00049172
Iteration 10/25 | Loss: 0.00049172
Iteration 11/25 | Loss: 0.00049172
Iteration 12/25 | Loss: 0.00049172
Iteration 13/25 | Loss: 0.00049172
Iteration 14/25 | Loss: 0.00049172
Iteration 15/25 | Loss: 0.00049172
Iteration 16/25 | Loss: 0.00049172
Iteration 17/25 | Loss: 0.00049172
Iteration 18/25 | Loss: 0.00049172
Iteration 19/25 | Loss: 0.00049172
Iteration 20/25 | Loss: 0.00049172
Iteration 21/25 | Loss: 0.00049172
Iteration 22/25 | Loss: 0.00049172
Iteration 23/25 | Loss: 0.00049172
Iteration 24/25 | Loss: 0.00049172
Iteration 25/25 | Loss: 0.00049172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004917195183224976, 0.0004917195183224976, 0.0004917195183224976, 0.0004917195183224976, 0.0004917195183224976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004917195183224976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049172
Iteration 2/1000 | Loss: 0.00001865
Iteration 3/1000 | Loss: 0.00001048
Iteration 4/1000 | Loss: 0.00000893
Iteration 5/1000 | Loss: 0.00000835
Iteration 6/1000 | Loss: 0.00000801
Iteration 7/1000 | Loss: 0.00000777
Iteration 8/1000 | Loss: 0.00000760
Iteration 9/1000 | Loss: 0.00000759
Iteration 10/1000 | Loss: 0.00000758
Iteration 11/1000 | Loss: 0.00000758
Iteration 12/1000 | Loss: 0.00000757
Iteration 13/1000 | Loss: 0.00000757
Iteration 14/1000 | Loss: 0.00000757
Iteration 15/1000 | Loss: 0.00000757
Iteration 16/1000 | Loss: 0.00000757
Iteration 17/1000 | Loss: 0.00000755
Iteration 18/1000 | Loss: 0.00000755
Iteration 19/1000 | Loss: 0.00000754
Iteration 20/1000 | Loss: 0.00000753
Iteration 21/1000 | Loss: 0.00000753
Iteration 22/1000 | Loss: 0.00000753
Iteration 23/1000 | Loss: 0.00000752
Iteration 24/1000 | Loss: 0.00000752
Iteration 25/1000 | Loss: 0.00000752
Iteration 26/1000 | Loss: 0.00000752
Iteration 27/1000 | Loss: 0.00000752
Iteration 28/1000 | Loss: 0.00000751
Iteration 29/1000 | Loss: 0.00000751
Iteration 30/1000 | Loss: 0.00000751
Iteration 31/1000 | Loss: 0.00000751
Iteration 32/1000 | Loss: 0.00000750
Iteration 33/1000 | Loss: 0.00000750
Iteration 34/1000 | Loss: 0.00000750
Iteration 35/1000 | Loss: 0.00000750
Iteration 36/1000 | Loss: 0.00000750
Iteration 37/1000 | Loss: 0.00000750
Iteration 38/1000 | Loss: 0.00000749
Iteration 39/1000 | Loss: 0.00000749
Iteration 40/1000 | Loss: 0.00000749
Iteration 41/1000 | Loss: 0.00000749
Iteration 42/1000 | Loss: 0.00000747
Iteration 43/1000 | Loss: 0.00000747
Iteration 44/1000 | Loss: 0.00000747
Iteration 45/1000 | Loss: 0.00000746
Iteration 46/1000 | Loss: 0.00000746
Iteration 47/1000 | Loss: 0.00000746
Iteration 48/1000 | Loss: 0.00000746
Iteration 49/1000 | Loss: 0.00000745
Iteration 50/1000 | Loss: 0.00000745
Iteration 51/1000 | Loss: 0.00000744
Iteration 52/1000 | Loss: 0.00000744
Iteration 53/1000 | Loss: 0.00000744
Iteration 54/1000 | Loss: 0.00000743
Iteration 55/1000 | Loss: 0.00000743
Iteration 56/1000 | Loss: 0.00000742
Iteration 57/1000 | Loss: 0.00000742
Iteration 58/1000 | Loss: 0.00000742
Iteration 59/1000 | Loss: 0.00000742
Iteration 60/1000 | Loss: 0.00000742
Iteration 61/1000 | Loss: 0.00000741
Iteration 62/1000 | Loss: 0.00000741
Iteration 63/1000 | Loss: 0.00000741
Iteration 64/1000 | Loss: 0.00000741
Iteration 65/1000 | Loss: 0.00000741
Iteration 66/1000 | Loss: 0.00000741
Iteration 67/1000 | Loss: 0.00000741
Iteration 68/1000 | Loss: 0.00000741
Iteration 69/1000 | Loss: 0.00000741
Iteration 70/1000 | Loss: 0.00000741
Iteration 71/1000 | Loss: 0.00000741
Iteration 72/1000 | Loss: 0.00000740
Iteration 73/1000 | Loss: 0.00000740
Iteration 74/1000 | Loss: 0.00000740
Iteration 75/1000 | Loss: 0.00000740
Iteration 76/1000 | Loss: 0.00000740
Iteration 77/1000 | Loss: 0.00000740
Iteration 78/1000 | Loss: 0.00000740
Iteration 79/1000 | Loss: 0.00000740
Iteration 80/1000 | Loss: 0.00000739
Iteration 81/1000 | Loss: 0.00000739
Iteration 82/1000 | Loss: 0.00000738
Iteration 83/1000 | Loss: 0.00000738
Iteration 84/1000 | Loss: 0.00000738
Iteration 85/1000 | Loss: 0.00000738
Iteration 86/1000 | Loss: 0.00000738
Iteration 87/1000 | Loss: 0.00000738
Iteration 88/1000 | Loss: 0.00000738
Iteration 89/1000 | Loss: 0.00000738
Iteration 90/1000 | Loss: 0.00000738
Iteration 91/1000 | Loss: 0.00000738
Iteration 92/1000 | Loss: 0.00000738
Iteration 93/1000 | Loss: 0.00000737
Iteration 94/1000 | Loss: 0.00000737
Iteration 95/1000 | Loss: 0.00000737
Iteration 96/1000 | Loss: 0.00000737
Iteration 97/1000 | Loss: 0.00000737
Iteration 98/1000 | Loss: 0.00000736
Iteration 99/1000 | Loss: 0.00000736
Iteration 100/1000 | Loss: 0.00000736
Iteration 101/1000 | Loss: 0.00000736
Iteration 102/1000 | Loss: 0.00000735
Iteration 103/1000 | Loss: 0.00000735
Iteration 104/1000 | Loss: 0.00000735
Iteration 105/1000 | Loss: 0.00000735
Iteration 106/1000 | Loss: 0.00000735
Iteration 107/1000 | Loss: 0.00000735
Iteration 108/1000 | Loss: 0.00000735
Iteration 109/1000 | Loss: 0.00000735
Iteration 110/1000 | Loss: 0.00000734
Iteration 111/1000 | Loss: 0.00000734
Iteration 112/1000 | Loss: 0.00000733
Iteration 113/1000 | Loss: 0.00000733
Iteration 114/1000 | Loss: 0.00000733
Iteration 115/1000 | Loss: 0.00000733
Iteration 116/1000 | Loss: 0.00000733
Iteration 117/1000 | Loss: 0.00000733
Iteration 118/1000 | Loss: 0.00000733
Iteration 119/1000 | Loss: 0.00000733
Iteration 120/1000 | Loss: 0.00000732
Iteration 121/1000 | Loss: 0.00000732
Iteration 122/1000 | Loss: 0.00000732
Iteration 123/1000 | Loss: 0.00000732
Iteration 124/1000 | Loss: 0.00000732
Iteration 125/1000 | Loss: 0.00000731
Iteration 126/1000 | Loss: 0.00000731
Iteration 127/1000 | Loss: 0.00000731
Iteration 128/1000 | Loss: 0.00000730
Iteration 129/1000 | Loss: 0.00000730
Iteration 130/1000 | Loss: 0.00000730
Iteration 131/1000 | Loss: 0.00000730
Iteration 132/1000 | Loss: 0.00000730
Iteration 133/1000 | Loss: 0.00000730
Iteration 134/1000 | Loss: 0.00000729
Iteration 135/1000 | Loss: 0.00000729
Iteration 136/1000 | Loss: 0.00000729
Iteration 137/1000 | Loss: 0.00000729
Iteration 138/1000 | Loss: 0.00000729
Iteration 139/1000 | Loss: 0.00000729
Iteration 140/1000 | Loss: 0.00000729
Iteration 141/1000 | Loss: 0.00000728
Iteration 142/1000 | Loss: 0.00000728
Iteration 143/1000 | Loss: 0.00000728
Iteration 144/1000 | Loss: 0.00000728
Iteration 145/1000 | Loss: 0.00000728
Iteration 146/1000 | Loss: 0.00000728
Iteration 147/1000 | Loss: 0.00000728
Iteration 148/1000 | Loss: 0.00000727
Iteration 149/1000 | Loss: 0.00000727
Iteration 150/1000 | Loss: 0.00000727
Iteration 151/1000 | Loss: 0.00000727
Iteration 152/1000 | Loss: 0.00000727
Iteration 153/1000 | Loss: 0.00000727
Iteration 154/1000 | Loss: 0.00000727
Iteration 155/1000 | Loss: 0.00000727
Iteration 156/1000 | Loss: 0.00000726
Iteration 157/1000 | Loss: 0.00000726
Iteration 158/1000 | Loss: 0.00000726
Iteration 159/1000 | Loss: 0.00000726
Iteration 160/1000 | Loss: 0.00000726
Iteration 161/1000 | Loss: 0.00000726
Iteration 162/1000 | Loss: 0.00000726
Iteration 163/1000 | Loss: 0.00000726
Iteration 164/1000 | Loss: 0.00000726
Iteration 165/1000 | Loss: 0.00000726
Iteration 166/1000 | Loss: 0.00000726
Iteration 167/1000 | Loss: 0.00000726
Iteration 168/1000 | Loss: 0.00000726
Iteration 169/1000 | Loss: 0.00000726
Iteration 170/1000 | Loss: 0.00000726
Iteration 171/1000 | Loss: 0.00000726
Iteration 172/1000 | Loss: 0.00000725
Iteration 173/1000 | Loss: 0.00000725
Iteration 174/1000 | Loss: 0.00000725
Iteration 175/1000 | Loss: 0.00000725
Iteration 176/1000 | Loss: 0.00000725
Iteration 177/1000 | Loss: 0.00000725
Iteration 178/1000 | Loss: 0.00000725
Iteration 179/1000 | Loss: 0.00000725
Iteration 180/1000 | Loss: 0.00000725
Iteration 181/1000 | Loss: 0.00000725
Iteration 182/1000 | Loss: 0.00000725
Iteration 183/1000 | Loss: 0.00000725
Iteration 184/1000 | Loss: 0.00000725
Iteration 185/1000 | Loss: 0.00000725
Iteration 186/1000 | Loss: 0.00000725
Iteration 187/1000 | Loss: 0.00000725
Iteration 188/1000 | Loss: 0.00000725
Iteration 189/1000 | Loss: 0.00000725
Iteration 190/1000 | Loss: 0.00000725
Iteration 191/1000 | Loss: 0.00000725
Iteration 192/1000 | Loss: 0.00000725
Iteration 193/1000 | Loss: 0.00000725
Iteration 194/1000 | Loss: 0.00000724
Iteration 195/1000 | Loss: 0.00000724
Iteration 196/1000 | Loss: 0.00000724
Iteration 197/1000 | Loss: 0.00000724
Iteration 198/1000 | Loss: 0.00000724
Iteration 199/1000 | Loss: 0.00000724
Iteration 200/1000 | Loss: 0.00000724
Iteration 201/1000 | Loss: 0.00000724
Iteration 202/1000 | Loss: 0.00000724
Iteration 203/1000 | Loss: 0.00000724
Iteration 204/1000 | Loss: 0.00000724
Iteration 205/1000 | Loss: 0.00000724
Iteration 206/1000 | Loss: 0.00000724
Iteration 207/1000 | Loss: 0.00000724
Iteration 208/1000 | Loss: 0.00000724
Iteration 209/1000 | Loss: 0.00000723
Iteration 210/1000 | Loss: 0.00000723
Iteration 211/1000 | Loss: 0.00000723
Iteration 212/1000 | Loss: 0.00000723
Iteration 213/1000 | Loss: 0.00000723
Iteration 214/1000 | Loss: 0.00000723
Iteration 215/1000 | Loss: 0.00000723
Iteration 216/1000 | Loss: 0.00000723
Iteration 217/1000 | Loss: 0.00000723
Iteration 218/1000 | Loss: 0.00000723
Iteration 219/1000 | Loss: 0.00000723
Iteration 220/1000 | Loss: 0.00000723
Iteration 221/1000 | Loss: 0.00000723
Iteration 222/1000 | Loss: 0.00000723
Iteration 223/1000 | Loss: 0.00000723
Iteration 224/1000 | Loss: 0.00000723
Iteration 225/1000 | Loss: 0.00000723
Iteration 226/1000 | Loss: 0.00000723
Iteration 227/1000 | Loss: 0.00000723
Iteration 228/1000 | Loss: 0.00000723
Iteration 229/1000 | Loss: 0.00000723
Iteration 230/1000 | Loss: 0.00000723
Iteration 231/1000 | Loss: 0.00000723
Iteration 232/1000 | Loss: 0.00000723
Iteration 233/1000 | Loss: 0.00000723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [7.228059075714555e-06, 7.228059075714555e-06, 7.228059075714555e-06, 7.228059075714555e-06, 7.228059075714555e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.228059075714555e-06

Optimization complete. Final v2v error: 2.1956851482391357 mm

Highest mean error: 2.6925785541534424 mm for frame 60

Lowest mean error: 1.9823100566864014 mm for frame 96

Saving results

Total time: 35.41070914268494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139937
Iteration 2/25 | Loss: 0.00210982
Iteration 3/25 | Loss: 0.00124575
Iteration 4/25 | Loss: 0.00114796
Iteration 5/25 | Loss: 0.00111898
Iteration 6/25 | Loss: 0.00110820
Iteration 7/25 | Loss: 0.00110853
Iteration 8/25 | Loss: 0.00107878
Iteration 9/25 | Loss: 0.00106984
Iteration 10/25 | Loss: 0.00106148
Iteration 11/25 | Loss: 0.00104149
Iteration 12/25 | Loss: 0.00103270
Iteration 13/25 | Loss: 0.00103319
Iteration 14/25 | Loss: 0.00102756
Iteration 15/25 | Loss: 0.00102592
Iteration 16/25 | Loss: 0.00103614
Iteration 17/25 | Loss: 0.00103032
Iteration 18/25 | Loss: 0.00103229
Iteration 19/25 | Loss: 0.00102757
Iteration 20/25 | Loss: 0.00101720
Iteration 21/25 | Loss: 0.00101465
Iteration 22/25 | Loss: 0.00101572
Iteration 23/25 | Loss: 0.00101375
Iteration 24/25 | Loss: 0.00100835
Iteration 25/25 | Loss: 0.00100421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98120427
Iteration 2/25 | Loss: 0.00124458
Iteration 3/25 | Loss: 0.00124457
Iteration 4/25 | Loss: 0.00124457
Iteration 5/25 | Loss: 0.00124457
Iteration 6/25 | Loss: 0.00124457
Iteration 7/25 | Loss: 0.00124457
Iteration 8/25 | Loss: 0.00124457
Iteration 9/25 | Loss: 0.00109794
Iteration 10/25 | Loss: 0.00109794
Iteration 11/25 | Loss: 0.00109794
Iteration 12/25 | Loss: 0.00109794
Iteration 13/25 | Loss: 0.00109794
Iteration 14/25 | Loss: 0.00109794
Iteration 15/25 | Loss: 0.00109794
Iteration 16/25 | Loss: 0.00109794
Iteration 17/25 | Loss: 0.00109794
Iteration 18/25 | Loss: 0.00109794
Iteration 19/25 | Loss: 0.00109794
Iteration 20/25 | Loss: 0.00109794
Iteration 21/25 | Loss: 0.00109794
Iteration 22/25 | Loss: 0.00109794
Iteration 23/25 | Loss: 0.00109794
Iteration 24/25 | Loss: 0.00109794
Iteration 25/25 | Loss: 0.00109794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109794
Iteration 2/1000 | Loss: 0.00192851
Iteration 3/1000 | Loss: 0.00078068
Iteration 4/1000 | Loss: 0.00021657
Iteration 5/1000 | Loss: 0.00017228
Iteration 6/1000 | Loss: 0.00008307
Iteration 7/1000 | Loss: 0.00023721
Iteration 8/1000 | Loss: 0.00036022
Iteration 9/1000 | Loss: 0.00005609
Iteration 10/1000 | Loss: 0.00005645
Iteration 11/1000 | Loss: 0.00008730
Iteration 12/1000 | Loss: 0.00019872
Iteration 13/1000 | Loss: 0.00023751
Iteration 14/1000 | Loss: 0.00005799
Iteration 15/1000 | Loss: 0.00026219
Iteration 16/1000 | Loss: 0.00017338
Iteration 17/1000 | Loss: 0.00020701
Iteration 18/1000 | Loss: 0.00014908
Iteration 19/1000 | Loss: 0.00013391
Iteration 20/1000 | Loss: 0.00010201
Iteration 21/1000 | Loss: 0.00011464
Iteration 22/1000 | Loss: 0.00050480
Iteration 23/1000 | Loss: 0.00044954
Iteration 24/1000 | Loss: 0.00044148
Iteration 25/1000 | Loss: 0.00012297
Iteration 26/1000 | Loss: 0.00029814
Iteration 27/1000 | Loss: 0.00021916
Iteration 28/1000 | Loss: 0.00014004
Iteration 29/1000 | Loss: 0.00008135
Iteration 30/1000 | Loss: 0.00008658
Iteration 31/1000 | Loss: 0.00031557
Iteration 32/1000 | Loss: 0.00018030
Iteration 33/1000 | Loss: 0.00018817
Iteration 34/1000 | Loss: 0.00009457
Iteration 35/1000 | Loss: 0.00003868
Iteration 36/1000 | Loss: 0.00003517
Iteration 37/1000 | Loss: 0.00011103
Iteration 38/1000 | Loss: 0.00009027
Iteration 39/1000 | Loss: 0.00010153
Iteration 40/1000 | Loss: 0.00003339
Iteration 41/1000 | Loss: 0.00003120
Iteration 42/1000 | Loss: 0.00016866
Iteration 43/1000 | Loss: 0.00030990
Iteration 44/1000 | Loss: 0.00028543
Iteration 45/1000 | Loss: 0.00079927
Iteration 46/1000 | Loss: 0.00043825
Iteration 47/1000 | Loss: 0.00020481
Iteration 48/1000 | Loss: 0.00037950
Iteration 49/1000 | Loss: 0.00010753
Iteration 50/1000 | Loss: 0.00009290
Iteration 51/1000 | Loss: 0.00007593
Iteration 52/1000 | Loss: 0.00012198
Iteration 53/1000 | Loss: 0.00042160
Iteration 54/1000 | Loss: 0.00007781
Iteration 55/1000 | Loss: 0.00008018
Iteration 56/1000 | Loss: 0.00004265
Iteration 57/1000 | Loss: 0.00003202
Iteration 58/1000 | Loss: 0.00012649
Iteration 59/1000 | Loss: 0.00014207
Iteration 60/1000 | Loss: 0.00011845
Iteration 61/1000 | Loss: 0.00012444
Iteration 62/1000 | Loss: 0.00009346
Iteration 63/1000 | Loss: 0.00022588
Iteration 64/1000 | Loss: 0.00019692
Iteration 65/1000 | Loss: 0.00007626
Iteration 66/1000 | Loss: 0.00012812
Iteration 67/1000 | Loss: 0.00019188
Iteration 68/1000 | Loss: 0.00011157
Iteration 69/1000 | Loss: 0.00014070
Iteration 70/1000 | Loss: 0.00013013
Iteration 71/1000 | Loss: 0.00013185
Iteration 72/1000 | Loss: 0.00010364
Iteration 73/1000 | Loss: 0.00012060
Iteration 74/1000 | Loss: 0.00008707
Iteration 75/1000 | Loss: 0.00003116
Iteration 76/1000 | Loss: 0.00003217
Iteration 77/1000 | Loss: 0.00002724
Iteration 78/1000 | Loss: 0.00002638
Iteration 79/1000 | Loss: 0.00002580
Iteration 80/1000 | Loss: 0.00002542
Iteration 81/1000 | Loss: 0.00017290
Iteration 82/1000 | Loss: 0.00013737
Iteration 83/1000 | Loss: 0.00052687
Iteration 84/1000 | Loss: 0.00042672
Iteration 85/1000 | Loss: 0.00004351
Iteration 86/1000 | Loss: 0.00004268
Iteration 87/1000 | Loss: 0.00053757
Iteration 88/1000 | Loss: 0.00043028
Iteration 89/1000 | Loss: 0.00005775
Iteration 90/1000 | Loss: 0.00003731
Iteration 91/1000 | Loss: 0.00002571
Iteration 92/1000 | Loss: 0.00002594
Iteration 93/1000 | Loss: 0.00002378
Iteration 94/1000 | Loss: 0.00002349
Iteration 95/1000 | Loss: 0.00002308
Iteration 96/1000 | Loss: 0.00002288
Iteration 97/1000 | Loss: 0.00002265
Iteration 98/1000 | Loss: 0.00002247
Iteration 99/1000 | Loss: 0.00002230
Iteration 100/1000 | Loss: 0.00002228
Iteration 101/1000 | Loss: 0.00002223
Iteration 102/1000 | Loss: 0.00002223
Iteration 103/1000 | Loss: 0.00002223
Iteration 104/1000 | Loss: 0.00002223
Iteration 105/1000 | Loss: 0.00002223
Iteration 106/1000 | Loss: 0.00002223
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002451
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002227
Iteration 112/1000 | Loss: 0.00002191
Iteration 113/1000 | Loss: 0.00002175
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002155
Iteration 116/1000 | Loss: 0.00002153
Iteration 117/1000 | Loss: 0.00002153
Iteration 118/1000 | Loss: 0.00002153
Iteration 119/1000 | Loss: 0.00002153
Iteration 120/1000 | Loss: 0.00002153
Iteration 121/1000 | Loss: 0.00002153
Iteration 122/1000 | Loss: 0.00002153
Iteration 123/1000 | Loss: 0.00002153
Iteration 124/1000 | Loss: 0.00002152
Iteration 125/1000 | Loss: 0.00002152
Iteration 126/1000 | Loss: 0.00002152
Iteration 127/1000 | Loss: 0.00002152
Iteration 128/1000 | Loss: 0.00002152
Iteration 129/1000 | Loss: 0.00002152
Iteration 130/1000 | Loss: 0.00002152
Iteration 131/1000 | Loss: 0.00002152
Iteration 132/1000 | Loss: 0.00002151
Iteration 133/1000 | Loss: 0.00002151
Iteration 134/1000 | Loss: 0.00002151
Iteration 135/1000 | Loss: 0.00002151
Iteration 136/1000 | Loss: 0.00002151
Iteration 137/1000 | Loss: 0.00002150
Iteration 138/1000 | Loss: 0.00002150
Iteration 139/1000 | Loss: 0.00002150
Iteration 140/1000 | Loss: 0.00002150
Iteration 141/1000 | Loss: 0.00002150
Iteration 142/1000 | Loss: 0.00002150
Iteration 143/1000 | Loss: 0.00002150
Iteration 144/1000 | Loss: 0.00002150
Iteration 145/1000 | Loss: 0.00002149
Iteration 146/1000 | Loss: 0.00002149
Iteration 147/1000 | Loss: 0.00002149
Iteration 148/1000 | Loss: 0.00002149
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002148
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00002148
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002147
Iteration 155/1000 | Loss: 0.00002147
Iteration 156/1000 | Loss: 0.00002147
Iteration 157/1000 | Loss: 0.00002147
Iteration 158/1000 | Loss: 0.00002147
Iteration 159/1000 | Loss: 0.00002147
Iteration 160/1000 | Loss: 0.00002147
Iteration 161/1000 | Loss: 0.00002146
Iteration 162/1000 | Loss: 0.00002146
Iteration 163/1000 | Loss: 0.00002146
Iteration 164/1000 | Loss: 0.00002146
Iteration 165/1000 | Loss: 0.00002146
Iteration 166/1000 | Loss: 0.00002146
Iteration 167/1000 | Loss: 0.00002146
Iteration 168/1000 | Loss: 0.00002146
Iteration 169/1000 | Loss: 0.00002146
Iteration 170/1000 | Loss: 0.00002146
Iteration 171/1000 | Loss: 0.00002146
Iteration 172/1000 | Loss: 0.00002146
Iteration 173/1000 | Loss: 0.00002145
Iteration 174/1000 | Loss: 0.00002145
Iteration 175/1000 | Loss: 0.00002145
Iteration 176/1000 | Loss: 0.00002145
Iteration 177/1000 | Loss: 0.00002145
Iteration 178/1000 | Loss: 0.00002145
Iteration 179/1000 | Loss: 0.00002145
Iteration 180/1000 | Loss: 0.00002145
Iteration 181/1000 | Loss: 0.00002145
Iteration 182/1000 | Loss: 0.00002145
Iteration 183/1000 | Loss: 0.00002145
Iteration 184/1000 | Loss: 0.00002144
Iteration 185/1000 | Loss: 0.00002144
Iteration 186/1000 | Loss: 0.00002144
Iteration 187/1000 | Loss: 0.00002144
Iteration 188/1000 | Loss: 0.00002143
Iteration 189/1000 | Loss: 0.00002142
Iteration 190/1000 | Loss: 0.00002142
Iteration 191/1000 | Loss: 0.00002142
Iteration 192/1000 | Loss: 0.00002141
Iteration 193/1000 | Loss: 0.00002141
Iteration 194/1000 | Loss: 0.00002140
Iteration 195/1000 | Loss: 0.00002140
Iteration 196/1000 | Loss: 0.00002140
Iteration 197/1000 | Loss: 0.00002140
Iteration 198/1000 | Loss: 0.00002139
Iteration 199/1000 | Loss: 0.00002139
Iteration 200/1000 | Loss: 0.00002139
Iteration 201/1000 | Loss: 0.00002139
Iteration 202/1000 | Loss: 0.00002139
Iteration 203/1000 | Loss: 0.00002138
Iteration 204/1000 | Loss: 0.00002138
Iteration 205/1000 | Loss: 0.00002138
Iteration 206/1000 | Loss: 0.00002138
Iteration 207/1000 | Loss: 0.00002138
Iteration 208/1000 | Loss: 0.00002138
Iteration 209/1000 | Loss: 0.00002138
Iteration 210/1000 | Loss: 0.00002138
Iteration 211/1000 | Loss: 0.00002138
Iteration 212/1000 | Loss: 0.00002137
Iteration 213/1000 | Loss: 0.00002137
Iteration 214/1000 | Loss: 0.00002137
Iteration 215/1000 | Loss: 0.00002137
Iteration 216/1000 | Loss: 0.00002137
Iteration 217/1000 | Loss: 0.00002137
Iteration 218/1000 | Loss: 0.00002137
Iteration 219/1000 | Loss: 0.00002137
Iteration 220/1000 | Loss: 0.00002137
Iteration 221/1000 | Loss: 0.00002137
Iteration 222/1000 | Loss: 0.00002137
Iteration 223/1000 | Loss: 0.00002137
Iteration 224/1000 | Loss: 0.00002137
Iteration 225/1000 | Loss: 0.00002136
Iteration 226/1000 | Loss: 0.00002136
Iteration 227/1000 | Loss: 0.00002136
Iteration 228/1000 | Loss: 0.00002136
Iteration 229/1000 | Loss: 0.00002136
Iteration 230/1000 | Loss: 0.00002136
Iteration 231/1000 | Loss: 0.00002136
Iteration 232/1000 | Loss: 0.00002135
Iteration 233/1000 | Loss: 0.00002135
Iteration 234/1000 | Loss: 0.00002135
Iteration 235/1000 | Loss: 0.00002135
Iteration 236/1000 | Loss: 0.00002135
Iteration 237/1000 | Loss: 0.00002135
Iteration 238/1000 | Loss: 0.00002135
Iteration 239/1000 | Loss: 0.00002135
Iteration 240/1000 | Loss: 0.00002135
Iteration 241/1000 | Loss: 0.00002135
Iteration 242/1000 | Loss: 0.00002135
Iteration 243/1000 | Loss: 0.00002134
Iteration 244/1000 | Loss: 0.00002134
Iteration 245/1000 | Loss: 0.00002134
Iteration 246/1000 | Loss: 0.00002134
Iteration 247/1000 | Loss: 0.00002134
Iteration 248/1000 | Loss: 0.00002134
Iteration 249/1000 | Loss: 0.00002134
Iteration 250/1000 | Loss: 0.00002134
Iteration 251/1000 | Loss: 0.00002133
Iteration 252/1000 | Loss: 0.00002133
Iteration 253/1000 | Loss: 0.00002133
Iteration 254/1000 | Loss: 0.00002133
Iteration 255/1000 | Loss: 0.00002133
Iteration 256/1000 | Loss: 0.00002133
Iteration 257/1000 | Loss: 0.00002133
Iteration 258/1000 | Loss: 0.00002133
Iteration 259/1000 | Loss: 0.00002133
Iteration 260/1000 | Loss: 0.00002132
Iteration 261/1000 | Loss: 0.00002132
Iteration 262/1000 | Loss: 0.00002132
Iteration 263/1000 | Loss: 0.00002132
Iteration 264/1000 | Loss: 0.00002132
Iteration 265/1000 | Loss: 0.00002132
Iteration 266/1000 | Loss: 0.00002132
Iteration 267/1000 | Loss: 0.00002132
Iteration 268/1000 | Loss: 0.00002132
Iteration 269/1000 | Loss: 0.00002132
Iteration 270/1000 | Loss: 0.00002131
Iteration 271/1000 | Loss: 0.00002131
Iteration 272/1000 | Loss: 0.00002131
Iteration 273/1000 | Loss: 0.00002131
Iteration 274/1000 | Loss: 0.00002131
Iteration 275/1000 | Loss: 0.00002131
Iteration 276/1000 | Loss: 0.00002131
Iteration 277/1000 | Loss: 0.00002131
Iteration 278/1000 | Loss: 0.00002131
Iteration 279/1000 | Loss: 0.00002131
Iteration 280/1000 | Loss: 0.00002131
Iteration 281/1000 | Loss: 0.00002131
Iteration 282/1000 | Loss: 0.00002131
Iteration 283/1000 | Loss: 0.00002131
Iteration 284/1000 | Loss: 0.00002131
Iteration 285/1000 | Loss: 0.00002131
Iteration 286/1000 | Loss: 0.00002131
Iteration 287/1000 | Loss: 0.00002131
Iteration 288/1000 | Loss: 0.00002131
Iteration 289/1000 | Loss: 0.00002131
Iteration 290/1000 | Loss: 0.00002131
Iteration 291/1000 | Loss: 0.00002131
Iteration 292/1000 | Loss: 0.00002131
Iteration 293/1000 | Loss: 0.00002131
Iteration 294/1000 | Loss: 0.00002131
Iteration 295/1000 | Loss: 0.00002131
Iteration 296/1000 | Loss: 0.00002131
Iteration 297/1000 | Loss: 0.00002131
Iteration 298/1000 | Loss: 0.00002131
Iteration 299/1000 | Loss: 0.00002131
Iteration 300/1000 | Loss: 0.00002131
Iteration 301/1000 | Loss: 0.00002131
Iteration 302/1000 | Loss: 0.00002131
Iteration 303/1000 | Loss: 0.00002131
Iteration 304/1000 | Loss: 0.00002131
Iteration 305/1000 | Loss: 0.00002131
Iteration 306/1000 | Loss: 0.00002131
Iteration 307/1000 | Loss: 0.00002131
Iteration 308/1000 | Loss: 0.00002131
Iteration 309/1000 | Loss: 0.00002131
Iteration 310/1000 | Loss: 0.00002131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [2.1311232558218762e-05, 2.1311232558218762e-05, 2.1311232558218762e-05, 2.1311232558218762e-05, 2.1311232558218762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1311232558218762e-05

Optimization complete. Final v2v error: 3.7483208179473877 mm

Highest mean error: 4.531310558319092 mm for frame 36

Lowest mean error: 3.264571189880371 mm for frame 209

Saving results

Total time: 239.42047572135925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852611
Iteration 2/25 | Loss: 0.00107417
Iteration 3/25 | Loss: 0.00087663
Iteration 4/25 | Loss: 0.00080788
Iteration 5/25 | Loss: 0.00080278
Iteration 6/25 | Loss: 0.00079994
Iteration 7/25 | Loss: 0.00079923
Iteration 8/25 | Loss: 0.00079942
Iteration 9/25 | Loss: 0.00079900
Iteration 10/25 | Loss: 0.00079900
Iteration 11/25 | Loss: 0.00079900
Iteration 12/25 | Loss: 0.00079899
Iteration 13/25 | Loss: 0.00079899
Iteration 14/25 | Loss: 0.00079899
Iteration 15/25 | Loss: 0.00079899
Iteration 16/25 | Loss: 0.00079899
Iteration 17/25 | Loss: 0.00079899
Iteration 18/25 | Loss: 0.00079899
Iteration 19/25 | Loss: 0.00079899
Iteration 20/25 | Loss: 0.00079899
Iteration 21/25 | Loss: 0.00079899
Iteration 22/25 | Loss: 0.00079954
Iteration 23/25 | Loss: 0.00079898
Iteration 24/25 | Loss: 0.00079898
Iteration 25/25 | Loss: 0.00079898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.73314238
Iteration 2/25 | Loss: 0.00046472
Iteration 3/25 | Loss: 0.00046471
Iteration 4/25 | Loss: 0.00046471
Iteration 5/25 | Loss: 0.00046471
Iteration 6/25 | Loss: 0.00046471
Iteration 7/25 | Loss: 0.00046471
Iteration 8/25 | Loss: 0.00046471
Iteration 9/25 | Loss: 0.00046471
Iteration 10/25 | Loss: 0.00046471
Iteration 11/25 | Loss: 0.00046471
Iteration 12/25 | Loss: 0.00046471
Iteration 13/25 | Loss: 0.00046471
Iteration 14/25 | Loss: 0.00046471
Iteration 15/25 | Loss: 0.00046471
Iteration 16/25 | Loss: 0.00046471
Iteration 17/25 | Loss: 0.00046471
Iteration 18/25 | Loss: 0.00046471
Iteration 19/25 | Loss: 0.00046471
Iteration 20/25 | Loss: 0.00046471
Iteration 21/25 | Loss: 0.00046471
Iteration 22/25 | Loss: 0.00046471
Iteration 23/25 | Loss: 0.00046471
Iteration 24/25 | Loss: 0.00046471
Iteration 25/25 | Loss: 0.00046471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046471
Iteration 2/1000 | Loss: 0.00001362
Iteration 3/1000 | Loss: 0.00000945
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00000850
Iteration 6/1000 | Loss: 0.00000862
Iteration 7/1000 | Loss: 0.00000805
Iteration 8/1000 | Loss: 0.00000795
Iteration 9/1000 | Loss: 0.00000792
Iteration 10/1000 | Loss: 0.00000780
Iteration 11/1000 | Loss: 0.00000779
Iteration 12/1000 | Loss: 0.00000832
Iteration 13/1000 | Loss: 0.00000832
Iteration 14/1000 | Loss: 0.00000774
Iteration 15/1000 | Loss: 0.00000764
Iteration 16/1000 | Loss: 0.00000761
Iteration 17/1000 | Loss: 0.00000760
Iteration 18/1000 | Loss: 0.00000760
Iteration 19/1000 | Loss: 0.00000759
Iteration 20/1000 | Loss: 0.00000759
Iteration 21/1000 | Loss: 0.00000759
Iteration 22/1000 | Loss: 0.00000759
Iteration 23/1000 | Loss: 0.00000759
Iteration 24/1000 | Loss: 0.00000759
Iteration 25/1000 | Loss: 0.00000758
Iteration 26/1000 | Loss: 0.00000758
Iteration 27/1000 | Loss: 0.00000758
Iteration 28/1000 | Loss: 0.00000758
Iteration 29/1000 | Loss: 0.00000758
Iteration 30/1000 | Loss: 0.00000757
Iteration 31/1000 | Loss: 0.00000757
Iteration 32/1000 | Loss: 0.00000757
Iteration 33/1000 | Loss: 0.00000757
Iteration 34/1000 | Loss: 0.00000757
Iteration 35/1000 | Loss: 0.00000757
Iteration 36/1000 | Loss: 0.00000756
Iteration 37/1000 | Loss: 0.00000756
Iteration 38/1000 | Loss: 0.00000752
Iteration 39/1000 | Loss: 0.00000752
Iteration 40/1000 | Loss: 0.00000752
Iteration 41/1000 | Loss: 0.00000752
Iteration 42/1000 | Loss: 0.00000752
Iteration 43/1000 | Loss: 0.00000752
Iteration 44/1000 | Loss: 0.00000752
Iteration 45/1000 | Loss: 0.00000752
Iteration 46/1000 | Loss: 0.00000752
Iteration 47/1000 | Loss: 0.00000752
Iteration 48/1000 | Loss: 0.00000752
Iteration 49/1000 | Loss: 0.00000752
Iteration 50/1000 | Loss: 0.00000748
Iteration 51/1000 | Loss: 0.00000748
Iteration 52/1000 | Loss: 0.00000748
Iteration 53/1000 | Loss: 0.00000748
Iteration 54/1000 | Loss: 0.00000748
Iteration 55/1000 | Loss: 0.00000748
Iteration 56/1000 | Loss: 0.00000748
Iteration 57/1000 | Loss: 0.00000747
Iteration 58/1000 | Loss: 0.00000747
Iteration 59/1000 | Loss: 0.00000747
Iteration 60/1000 | Loss: 0.00000747
Iteration 61/1000 | Loss: 0.00000747
Iteration 62/1000 | Loss: 0.00000747
Iteration 63/1000 | Loss: 0.00000747
Iteration 64/1000 | Loss: 0.00000747
Iteration 65/1000 | Loss: 0.00000747
Iteration 66/1000 | Loss: 0.00000746
Iteration 67/1000 | Loss: 0.00000746
Iteration 68/1000 | Loss: 0.00000745
Iteration 69/1000 | Loss: 0.00000745
Iteration 70/1000 | Loss: 0.00000745
Iteration 71/1000 | Loss: 0.00000745
Iteration 72/1000 | Loss: 0.00000744
Iteration 73/1000 | Loss: 0.00000744
Iteration 74/1000 | Loss: 0.00000744
Iteration 75/1000 | Loss: 0.00000744
Iteration 76/1000 | Loss: 0.00000744
Iteration 77/1000 | Loss: 0.00000744
Iteration 78/1000 | Loss: 0.00000743
Iteration 79/1000 | Loss: 0.00000743
Iteration 80/1000 | Loss: 0.00000743
Iteration 81/1000 | Loss: 0.00000743
Iteration 82/1000 | Loss: 0.00000743
Iteration 83/1000 | Loss: 0.00000743
Iteration 84/1000 | Loss: 0.00000742
Iteration 85/1000 | Loss: 0.00000742
Iteration 86/1000 | Loss: 0.00000742
Iteration 87/1000 | Loss: 0.00000742
Iteration 88/1000 | Loss: 0.00000742
Iteration 89/1000 | Loss: 0.00000742
Iteration 90/1000 | Loss: 0.00000742
Iteration 91/1000 | Loss: 0.00000742
Iteration 92/1000 | Loss: 0.00000742
Iteration 93/1000 | Loss: 0.00000741
Iteration 94/1000 | Loss: 0.00000741
Iteration 95/1000 | Loss: 0.00000741
Iteration 96/1000 | Loss: 0.00000741
Iteration 97/1000 | Loss: 0.00000741
Iteration 98/1000 | Loss: 0.00000740
Iteration 99/1000 | Loss: 0.00000740
Iteration 100/1000 | Loss: 0.00000740
Iteration 101/1000 | Loss: 0.00000740
Iteration 102/1000 | Loss: 0.00000740
Iteration 103/1000 | Loss: 0.00000740
Iteration 104/1000 | Loss: 0.00000740
Iteration 105/1000 | Loss: 0.00000740
Iteration 106/1000 | Loss: 0.00000740
Iteration 107/1000 | Loss: 0.00000740
Iteration 108/1000 | Loss: 0.00000739
Iteration 109/1000 | Loss: 0.00000739
Iteration 110/1000 | Loss: 0.00000739
Iteration 111/1000 | Loss: 0.00000739
Iteration 112/1000 | Loss: 0.00000739
Iteration 113/1000 | Loss: 0.00000739
Iteration 114/1000 | Loss: 0.00000739
Iteration 115/1000 | Loss: 0.00000738
Iteration 116/1000 | Loss: 0.00000738
Iteration 117/1000 | Loss: 0.00000738
Iteration 118/1000 | Loss: 0.00000738
Iteration 119/1000 | Loss: 0.00000738
Iteration 120/1000 | Loss: 0.00000738
Iteration 121/1000 | Loss: 0.00000738
Iteration 122/1000 | Loss: 0.00000738
Iteration 123/1000 | Loss: 0.00000738
Iteration 124/1000 | Loss: 0.00000738
Iteration 125/1000 | Loss: 0.00000738
Iteration 126/1000 | Loss: 0.00000738
Iteration 127/1000 | Loss: 0.00000738
Iteration 128/1000 | Loss: 0.00000738
Iteration 129/1000 | Loss: 0.00000738
Iteration 130/1000 | Loss: 0.00000738
Iteration 131/1000 | Loss: 0.00000738
Iteration 132/1000 | Loss: 0.00000738
Iteration 133/1000 | Loss: 0.00000737
Iteration 134/1000 | Loss: 0.00000737
Iteration 135/1000 | Loss: 0.00000737
Iteration 136/1000 | Loss: 0.00000737
Iteration 137/1000 | Loss: 0.00000737
Iteration 138/1000 | Loss: 0.00000737
Iteration 139/1000 | Loss: 0.00000737
Iteration 140/1000 | Loss: 0.00000737
Iteration 141/1000 | Loss: 0.00000737
Iteration 142/1000 | Loss: 0.00000737
Iteration 143/1000 | Loss: 0.00000737
Iteration 144/1000 | Loss: 0.00000736
Iteration 145/1000 | Loss: 0.00000736
Iteration 146/1000 | Loss: 0.00000736
Iteration 147/1000 | Loss: 0.00000736
Iteration 148/1000 | Loss: 0.00000736
Iteration 149/1000 | Loss: 0.00000735
Iteration 150/1000 | Loss: 0.00000735
Iteration 151/1000 | Loss: 0.00000734
Iteration 152/1000 | Loss: 0.00000734
Iteration 153/1000 | Loss: 0.00000739
Iteration 154/1000 | Loss: 0.00000738
Iteration 155/1000 | Loss: 0.00000734
Iteration 156/1000 | Loss: 0.00000734
Iteration 157/1000 | Loss: 0.00000734
Iteration 158/1000 | Loss: 0.00000734
Iteration 159/1000 | Loss: 0.00000734
Iteration 160/1000 | Loss: 0.00000734
Iteration 161/1000 | Loss: 0.00000734
Iteration 162/1000 | Loss: 0.00000734
Iteration 163/1000 | Loss: 0.00000734
Iteration 164/1000 | Loss: 0.00000734
Iteration 165/1000 | Loss: 0.00000734
Iteration 166/1000 | Loss: 0.00000734
Iteration 167/1000 | Loss: 0.00000734
Iteration 168/1000 | Loss: 0.00000734
Iteration 169/1000 | Loss: 0.00000734
Iteration 170/1000 | Loss: 0.00000734
Iteration 171/1000 | Loss: 0.00000734
Iteration 172/1000 | Loss: 0.00000734
Iteration 173/1000 | Loss: 0.00000734
Iteration 174/1000 | Loss: 0.00000734
Iteration 175/1000 | Loss: 0.00000734
Iteration 176/1000 | Loss: 0.00000734
Iteration 177/1000 | Loss: 0.00000734
Iteration 178/1000 | Loss: 0.00000734
Iteration 179/1000 | Loss: 0.00000734
Iteration 180/1000 | Loss: 0.00000734
Iteration 181/1000 | Loss: 0.00000734
Iteration 182/1000 | Loss: 0.00000734
Iteration 183/1000 | Loss: 0.00000734
Iteration 184/1000 | Loss: 0.00000734
Iteration 185/1000 | Loss: 0.00000734
Iteration 186/1000 | Loss: 0.00000734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [7.335348072956549e-06, 7.335348072956549e-06, 7.335348072956549e-06, 7.335348072956549e-06, 7.335348072956549e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.335348072956549e-06

Optimization complete. Final v2v error: 2.3289425373077393 mm

Highest mean error: 2.5721333026885986 mm for frame 163

Lowest mean error: 2.147379159927368 mm for frame 229

Saving results

Total time: 48.799328327178955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072659
Iteration 2/25 | Loss: 0.00315947
Iteration 3/25 | Loss: 0.00182830
Iteration 4/25 | Loss: 0.00165980
Iteration 5/25 | Loss: 0.00153609
Iteration 6/25 | Loss: 0.00145363
Iteration 7/25 | Loss: 0.00146003
Iteration 8/25 | Loss: 0.00136875
Iteration 9/25 | Loss: 0.00133394
Iteration 10/25 | Loss: 0.00131942
Iteration 11/25 | Loss: 0.00130361
Iteration 12/25 | Loss: 0.00126755
Iteration 13/25 | Loss: 0.00124052
Iteration 14/25 | Loss: 0.00122517
Iteration 15/25 | Loss: 0.00119747
Iteration 16/25 | Loss: 0.00119727
Iteration 17/25 | Loss: 0.00120434
Iteration 18/25 | Loss: 0.00118489
Iteration 19/25 | Loss: 0.00118112
Iteration 20/25 | Loss: 0.00117855
Iteration 21/25 | Loss: 0.00117420
Iteration 22/25 | Loss: 0.00117488
Iteration 23/25 | Loss: 0.00117657
Iteration 24/25 | Loss: 0.00117088
Iteration 25/25 | Loss: 0.00118083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35719264
Iteration 2/25 | Loss: 0.00285109
Iteration 3/25 | Loss: 0.00285109
Iteration 4/25 | Loss: 0.00284870
Iteration 5/25 | Loss: 0.00284870
Iteration 6/25 | Loss: 0.00284870
Iteration 7/25 | Loss: 0.00284870
Iteration 8/25 | Loss: 0.00284870
Iteration 9/25 | Loss: 0.00284870
Iteration 10/25 | Loss: 0.00284870
Iteration 11/25 | Loss: 0.00284870
Iteration 12/25 | Loss: 0.00284870
Iteration 13/25 | Loss: 0.00284870
Iteration 14/25 | Loss: 0.00284870
Iteration 15/25 | Loss: 0.00284870
Iteration 16/25 | Loss: 0.00284870
Iteration 17/25 | Loss: 0.00284870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028486994560807943, 0.0028486994560807943, 0.0028486994560807943, 0.0028486994560807943, 0.0028486994560807943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028486994560807943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284870
Iteration 2/1000 | Loss: 0.00393568
Iteration 3/1000 | Loss: 0.00216365
Iteration 4/1000 | Loss: 0.00162179
Iteration 5/1000 | Loss: 0.00250423
Iteration 6/1000 | Loss: 0.00095357
Iteration 7/1000 | Loss: 0.00176523
Iteration 8/1000 | Loss: 0.00068100
Iteration 9/1000 | Loss: 0.00425912
Iteration 10/1000 | Loss: 0.00558109
Iteration 11/1000 | Loss: 0.00412997
Iteration 12/1000 | Loss: 0.00564320
Iteration 13/1000 | Loss: 0.00429360
Iteration 14/1000 | Loss: 0.00949127
Iteration 15/1000 | Loss: 0.00149875
Iteration 16/1000 | Loss: 0.00040607
Iteration 17/1000 | Loss: 0.00333028
Iteration 18/1000 | Loss: 0.00022746
Iteration 19/1000 | Loss: 0.00026238
Iteration 20/1000 | Loss: 0.00066682
Iteration 21/1000 | Loss: 0.00035078
Iteration 22/1000 | Loss: 0.00089696
Iteration 23/1000 | Loss: 0.00037016
Iteration 24/1000 | Loss: 0.00033970
Iteration 25/1000 | Loss: 0.00039501
Iteration 26/1000 | Loss: 0.00204691
Iteration 27/1000 | Loss: 0.00194646
Iteration 28/1000 | Loss: 0.00060762
Iteration 29/1000 | Loss: 0.00072874
Iteration 30/1000 | Loss: 0.00052404
Iteration 31/1000 | Loss: 0.00113540
Iteration 32/1000 | Loss: 0.00051180
Iteration 33/1000 | Loss: 0.00300079
Iteration 34/1000 | Loss: 0.00130774
Iteration 35/1000 | Loss: 0.00609642
Iteration 36/1000 | Loss: 0.00468394
Iteration 37/1000 | Loss: 0.00546017
Iteration 38/1000 | Loss: 0.00535017
Iteration 39/1000 | Loss: 0.00666195
Iteration 40/1000 | Loss: 0.00550817
Iteration 41/1000 | Loss: 0.00383931
Iteration 42/1000 | Loss: 0.00622755
Iteration 43/1000 | Loss: 0.00386641
Iteration 44/1000 | Loss: 0.00465083
Iteration 45/1000 | Loss: 0.00332328
Iteration 46/1000 | Loss: 0.00452755
Iteration 47/1000 | Loss: 0.00312864
Iteration 48/1000 | Loss: 0.00435892
Iteration 49/1000 | Loss: 0.00320937
Iteration 50/1000 | Loss: 0.00400679
Iteration 51/1000 | Loss: 0.00211051
Iteration 52/1000 | Loss: 0.00287417
Iteration 53/1000 | Loss: 0.00206686
Iteration 54/1000 | Loss: 0.00474764
Iteration 55/1000 | Loss: 0.00189702
Iteration 56/1000 | Loss: 0.00272135
Iteration 57/1000 | Loss: 0.00091701
Iteration 58/1000 | Loss: 0.00230888
Iteration 59/1000 | Loss: 0.00133487
Iteration 60/1000 | Loss: 0.00110084
Iteration 61/1000 | Loss: 0.00070290
Iteration 62/1000 | Loss: 0.00027989
Iteration 63/1000 | Loss: 0.00175639
Iteration 64/1000 | Loss: 0.00153767
Iteration 65/1000 | Loss: 0.00085324
Iteration 66/1000 | Loss: 0.00117307
Iteration 67/1000 | Loss: 0.00086683
Iteration 68/1000 | Loss: 0.00151561
Iteration 69/1000 | Loss: 0.00078863
Iteration 70/1000 | Loss: 0.00083241
Iteration 71/1000 | Loss: 0.00034430
Iteration 72/1000 | Loss: 0.00034112
Iteration 73/1000 | Loss: 0.00023973
Iteration 74/1000 | Loss: 0.00013040
Iteration 75/1000 | Loss: 0.00004865
Iteration 76/1000 | Loss: 0.00037670
Iteration 77/1000 | Loss: 0.00125415
Iteration 78/1000 | Loss: 0.00187520
Iteration 79/1000 | Loss: 0.00103723
Iteration 80/1000 | Loss: 0.00034949
Iteration 81/1000 | Loss: 0.00163053
Iteration 82/1000 | Loss: 0.00033311
Iteration 83/1000 | Loss: 0.00025958
Iteration 84/1000 | Loss: 0.00007370
Iteration 85/1000 | Loss: 0.00060240
Iteration 86/1000 | Loss: 0.00008855
Iteration 87/1000 | Loss: 0.00005416
Iteration 88/1000 | Loss: 0.00004184
Iteration 89/1000 | Loss: 0.00059897
Iteration 90/1000 | Loss: 0.00010119
Iteration 91/1000 | Loss: 0.00022852
Iteration 92/1000 | Loss: 0.00003227
Iteration 93/1000 | Loss: 0.00031844
Iteration 94/1000 | Loss: 0.00166551
Iteration 95/1000 | Loss: 0.00064164
Iteration 96/1000 | Loss: 0.00052517
Iteration 97/1000 | Loss: 0.00122913
Iteration 98/1000 | Loss: 0.00041716
Iteration 99/1000 | Loss: 0.00050286
Iteration 100/1000 | Loss: 0.00005100
Iteration 101/1000 | Loss: 0.00004034
Iteration 102/1000 | Loss: 0.00005033
Iteration 103/1000 | Loss: 0.00004501
Iteration 104/1000 | Loss: 0.00004941
Iteration 105/1000 | Loss: 0.00079381
Iteration 106/1000 | Loss: 0.00014855
Iteration 107/1000 | Loss: 0.00018813
Iteration 108/1000 | Loss: 0.00076230
Iteration 109/1000 | Loss: 0.00004678
Iteration 110/1000 | Loss: 0.00005433
Iteration 111/1000 | Loss: 0.00004602
Iteration 112/1000 | Loss: 0.00007476
Iteration 113/1000 | Loss: 0.00003210
Iteration 114/1000 | Loss: 0.00002854
Iteration 115/1000 | Loss: 0.00003624
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00003067
Iteration 118/1000 | Loss: 0.00002475
Iteration 119/1000 | Loss: 0.00002649
Iteration 120/1000 | Loss: 0.00002866
Iteration 121/1000 | Loss: 0.00002930
Iteration 122/1000 | Loss: 0.00008134
Iteration 123/1000 | Loss: 0.00004236
Iteration 124/1000 | Loss: 0.00003036
Iteration 125/1000 | Loss: 0.00004242
Iteration 126/1000 | Loss: 0.00002974
Iteration 127/1000 | Loss: 0.00003662
Iteration 128/1000 | Loss: 0.00003073
Iteration 129/1000 | Loss: 0.00017191
Iteration 130/1000 | Loss: 0.00004897
Iteration 131/1000 | Loss: 0.00003323
Iteration 132/1000 | Loss: 0.00002311
Iteration 133/1000 | Loss: 0.00002947
Iteration 134/1000 | Loss: 0.00005620
Iteration 135/1000 | Loss: 0.00009889
Iteration 136/1000 | Loss: 0.00002119
Iteration 137/1000 | Loss: 0.00001872
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00006784
Iteration 140/1000 | Loss: 0.00001767
Iteration 141/1000 | Loss: 0.00001769
Iteration 142/1000 | Loss: 0.00001645
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001606
Iteration 146/1000 | Loss: 0.00001804
Iteration 147/1000 | Loss: 0.00001593
Iteration 148/1000 | Loss: 0.00001592
Iteration 149/1000 | Loss: 0.00001582
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001560
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001556
Iteration 156/1000 | Loss: 0.00001555
Iteration 157/1000 | Loss: 0.00001555
Iteration 158/1000 | Loss: 0.00001597
Iteration 159/1000 | Loss: 0.00001539
Iteration 160/1000 | Loss: 0.00001891
Iteration 161/1000 | Loss: 0.00001532
Iteration 162/1000 | Loss: 0.00001532
Iteration 163/1000 | Loss: 0.00001531
Iteration 164/1000 | Loss: 0.00001531
Iteration 165/1000 | Loss: 0.00001531
Iteration 166/1000 | Loss: 0.00001531
Iteration 167/1000 | Loss: 0.00001531
Iteration 168/1000 | Loss: 0.00001531
Iteration 169/1000 | Loss: 0.00001530
Iteration 170/1000 | Loss: 0.00001530
Iteration 171/1000 | Loss: 0.00001530
Iteration 172/1000 | Loss: 0.00001530
Iteration 173/1000 | Loss: 0.00001530
Iteration 174/1000 | Loss: 0.00001530
Iteration 175/1000 | Loss: 0.00001530
Iteration 176/1000 | Loss: 0.00001530
Iteration 177/1000 | Loss: 0.00001530
Iteration 178/1000 | Loss: 0.00001530
Iteration 179/1000 | Loss: 0.00001530
Iteration 180/1000 | Loss: 0.00001529
Iteration 181/1000 | Loss: 0.00001529
Iteration 182/1000 | Loss: 0.00001529
Iteration 183/1000 | Loss: 0.00001527
Iteration 184/1000 | Loss: 0.00001527
Iteration 185/1000 | Loss: 0.00001527
Iteration 186/1000 | Loss: 0.00001927
Iteration 187/1000 | Loss: 0.00105229
Iteration 188/1000 | Loss: 0.00026473
Iteration 189/1000 | Loss: 0.00003398
Iteration 190/1000 | Loss: 0.00002323
Iteration 191/1000 | Loss: 0.00001356
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00007252
Iteration 194/1000 | Loss: 0.00001346
Iteration 195/1000 | Loss: 0.00002303
Iteration 196/1000 | Loss: 0.00001304
Iteration 197/1000 | Loss: 0.00001491
Iteration 198/1000 | Loss: 0.00001234
Iteration 199/1000 | Loss: 0.00001233
Iteration 200/1000 | Loss: 0.00001233
Iteration 201/1000 | Loss: 0.00001233
Iteration 202/1000 | Loss: 0.00001233
Iteration 203/1000 | Loss: 0.00001233
Iteration 204/1000 | Loss: 0.00001233
Iteration 205/1000 | Loss: 0.00001233
Iteration 206/1000 | Loss: 0.00001232
Iteration 207/1000 | Loss: 0.00001232
Iteration 208/1000 | Loss: 0.00001232
Iteration 209/1000 | Loss: 0.00001232
Iteration 210/1000 | Loss: 0.00001232
Iteration 211/1000 | Loss: 0.00001230
Iteration 212/1000 | Loss: 0.00001229
Iteration 213/1000 | Loss: 0.00001229
Iteration 214/1000 | Loss: 0.00001228
Iteration 215/1000 | Loss: 0.00001228
Iteration 216/1000 | Loss: 0.00001228
Iteration 217/1000 | Loss: 0.00001228
Iteration 218/1000 | Loss: 0.00001304
Iteration 219/1000 | Loss: 0.00001227
Iteration 220/1000 | Loss: 0.00001226
Iteration 221/1000 | Loss: 0.00001226
Iteration 222/1000 | Loss: 0.00001226
Iteration 223/1000 | Loss: 0.00001226
Iteration 224/1000 | Loss: 0.00001226
Iteration 225/1000 | Loss: 0.00001226
Iteration 226/1000 | Loss: 0.00001226
Iteration 227/1000 | Loss: 0.00001226
Iteration 228/1000 | Loss: 0.00001226
Iteration 229/1000 | Loss: 0.00001226
Iteration 230/1000 | Loss: 0.00001226
Iteration 231/1000 | Loss: 0.00001226
Iteration 232/1000 | Loss: 0.00001226
Iteration 233/1000 | Loss: 0.00001225
Iteration 234/1000 | Loss: 0.00001225
Iteration 235/1000 | Loss: 0.00001225
Iteration 236/1000 | Loss: 0.00001224
Iteration 237/1000 | Loss: 0.00001224
Iteration 238/1000 | Loss: 0.00001224
Iteration 239/1000 | Loss: 0.00001224
Iteration 240/1000 | Loss: 0.00001224
Iteration 241/1000 | Loss: 0.00001224
Iteration 242/1000 | Loss: 0.00001224
Iteration 243/1000 | Loss: 0.00001224
Iteration 244/1000 | Loss: 0.00001223
Iteration 245/1000 | Loss: 0.00001223
Iteration 246/1000 | Loss: 0.00001223
Iteration 247/1000 | Loss: 0.00001223
Iteration 248/1000 | Loss: 0.00001223
Iteration 249/1000 | Loss: 0.00001223
Iteration 250/1000 | Loss: 0.00001223
Iteration 251/1000 | Loss: 0.00001223
Iteration 252/1000 | Loss: 0.00001223
Iteration 253/1000 | Loss: 0.00001223
Iteration 254/1000 | Loss: 0.00001223
Iteration 255/1000 | Loss: 0.00001223
Iteration 256/1000 | Loss: 0.00001223
Iteration 257/1000 | Loss: 0.00001223
Iteration 258/1000 | Loss: 0.00001222
Iteration 259/1000 | Loss: 0.00001222
Iteration 260/1000 | Loss: 0.00001222
Iteration 261/1000 | Loss: 0.00001222
Iteration 262/1000 | Loss: 0.00001221
Iteration 263/1000 | Loss: 0.00001221
Iteration 264/1000 | Loss: 0.00001221
Iteration 265/1000 | Loss: 0.00001221
Iteration 266/1000 | Loss: 0.00001221
Iteration 267/1000 | Loss: 0.00001221
Iteration 268/1000 | Loss: 0.00001221
Iteration 269/1000 | Loss: 0.00001221
Iteration 270/1000 | Loss: 0.00001221
Iteration 271/1000 | Loss: 0.00001221
Iteration 272/1000 | Loss: 0.00001221
Iteration 273/1000 | Loss: 0.00001221
Iteration 274/1000 | Loss: 0.00001221
Iteration 275/1000 | Loss: 0.00001221
Iteration 276/1000 | Loss: 0.00001221
Iteration 277/1000 | Loss: 0.00001221
Iteration 278/1000 | Loss: 0.00001221
Iteration 279/1000 | Loss: 0.00001221
Iteration 280/1000 | Loss: 0.00001221
Iteration 281/1000 | Loss: 0.00001221
Iteration 282/1000 | Loss: 0.00001221
Iteration 283/1000 | Loss: 0.00001221
Iteration 284/1000 | Loss: 0.00001221
Iteration 285/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.2211470675538294e-05, 1.2211470675538294e-05, 1.2211470675538294e-05, 1.2211470675538294e-05, 1.2211470675538294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2211470675538294e-05

Optimization complete. Final v2v error: 2.7951931953430176 mm

Highest mean error: 5.380678653717041 mm for frame 64

Lowest mean error: 2.1912693977355957 mm for frame 116

Saving results

Total time: 322.6548385620117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004543
Iteration 2/25 | Loss: 0.00247443
Iteration 3/25 | Loss: 0.00147283
Iteration 4/25 | Loss: 0.00130394
Iteration 5/25 | Loss: 0.00124613
Iteration 6/25 | Loss: 0.00131383
Iteration 7/25 | Loss: 0.00110617
Iteration 8/25 | Loss: 0.00107358
Iteration 9/25 | Loss: 0.00093115
Iteration 10/25 | Loss: 0.00088854
Iteration 11/25 | Loss: 0.00085359
Iteration 12/25 | Loss: 0.00086879
Iteration 13/25 | Loss: 0.00084564
Iteration 14/25 | Loss: 0.00083650
Iteration 15/25 | Loss: 0.00083970
Iteration 16/25 | Loss: 0.00083772
Iteration 17/25 | Loss: 0.00083682
Iteration 18/25 | Loss: 0.00083845
Iteration 19/25 | Loss: 0.00083779
Iteration 20/25 | Loss: 0.00083637
Iteration 21/25 | Loss: 0.00083637
Iteration 22/25 | Loss: 0.00083637
Iteration 23/25 | Loss: 0.00083637
Iteration 24/25 | Loss: 0.00083637
Iteration 25/25 | Loss: 0.00083637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38900495
Iteration 2/25 | Loss: 0.00041772
Iteration 3/25 | Loss: 0.00034554
Iteration 4/25 | Loss: 0.00034554
Iteration 5/25 | Loss: 0.00034554
Iteration 6/25 | Loss: 0.00034554
Iteration 7/25 | Loss: 0.00034554
Iteration 8/25 | Loss: 0.00034554
Iteration 9/25 | Loss: 0.00034554
Iteration 10/25 | Loss: 0.00034554
Iteration 11/25 | Loss: 0.00034554
Iteration 12/25 | Loss: 0.00034554
Iteration 13/25 | Loss: 0.00034554
Iteration 14/25 | Loss: 0.00034554
Iteration 15/25 | Loss: 0.00034554
Iteration 16/25 | Loss: 0.00034554
Iteration 17/25 | Loss: 0.00034554
Iteration 18/25 | Loss: 0.00034554
Iteration 19/25 | Loss: 0.00034554
Iteration 20/25 | Loss: 0.00034554
Iteration 21/25 | Loss: 0.00034554
Iteration 22/25 | Loss: 0.00034554
Iteration 23/25 | Loss: 0.00034554
Iteration 24/25 | Loss: 0.00034554
Iteration 25/25 | Loss: 0.00034554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034554
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00044894
Iteration 4/1000 | Loss: 0.00162833
Iteration 5/1000 | Loss: 0.00006484
Iteration 6/1000 | Loss: 0.00045465
Iteration 7/1000 | Loss: 0.00019398
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00010251
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00002991
Iteration 12/1000 | Loss: 0.00007980
Iteration 13/1000 | Loss: 0.00019543
Iteration 14/1000 | Loss: 0.00004125
Iteration 15/1000 | Loss: 0.00002328
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00002278
Iteration 18/1000 | Loss: 0.00001238
Iteration 19/1000 | Loss: 0.00002291
Iteration 20/1000 | Loss: 0.00002651
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001082
Iteration 24/1000 | Loss: 0.00001082
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00001082
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001081
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00002379
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001070
Iteration 44/1000 | Loss: 0.00001070
Iteration 45/1000 | Loss: 0.00001070
Iteration 46/1000 | Loss: 0.00001070
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001070
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001790
Iteration 52/1000 | Loss: 0.00001790
Iteration 53/1000 | Loss: 0.00005846
Iteration 54/1000 | Loss: 0.00002569
Iteration 55/1000 | Loss: 0.00001070
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00001094
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00005615
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001061
Iteration 71/1000 | Loss: 0.00001061
Iteration 72/1000 | Loss: 0.00001061
Iteration 73/1000 | Loss: 0.00001061
Iteration 74/1000 | Loss: 0.00001061
Iteration 75/1000 | Loss: 0.00001061
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001060
Iteration 80/1000 | Loss: 0.00001060
Iteration 81/1000 | Loss: 0.00001060
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00002472
Iteration 84/1000 | Loss: 0.00002358
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001062
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001609
Iteration 89/1000 | Loss: 0.00006380
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00002449
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00003831
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00003544
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001083
Iteration 101/1000 | Loss: 0.00001063
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001057
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001056
Iteration 121/1000 | Loss: 0.00001056
Iteration 122/1000 | Loss: 0.00001056
Iteration 123/1000 | Loss: 0.00001056
Iteration 124/1000 | Loss: 0.00001056
Iteration 125/1000 | Loss: 0.00001056
Iteration 126/1000 | Loss: 0.00001056
Iteration 127/1000 | Loss: 0.00001056
Iteration 128/1000 | Loss: 0.00001055
Iteration 129/1000 | Loss: 0.00001055
Iteration 130/1000 | Loss: 0.00001055
Iteration 131/1000 | Loss: 0.00001055
Iteration 132/1000 | Loss: 0.00001055
Iteration 133/1000 | Loss: 0.00001055
Iteration 134/1000 | Loss: 0.00001055
Iteration 135/1000 | Loss: 0.00001055
Iteration 136/1000 | Loss: 0.00001055
Iteration 137/1000 | Loss: 0.00001055
Iteration 138/1000 | Loss: 0.00001055
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001054
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001054
Iteration 144/1000 | Loss: 0.00001054
Iteration 145/1000 | Loss: 0.00001054
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001054
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001054
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.0541503797867335e-05, 1.0541503797867335e-05, 1.0541503797867335e-05, 1.0541503797867335e-05, 1.0541503797867335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0541503797867335e-05

Optimization complete. Final v2v error: 2.7186503410339355 mm

Highest mean error: 3.32466983795166 mm for frame 88

Lowest mean error: 2.457566738128662 mm for frame 132

Saving results

Total time: 110.28121256828308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760803
Iteration 2/25 | Loss: 0.00135818
Iteration 3/25 | Loss: 0.00100140
Iteration 4/25 | Loss: 0.00093907
Iteration 5/25 | Loss: 0.00091964
Iteration 6/25 | Loss: 0.00091407
Iteration 7/25 | Loss: 0.00091091
Iteration 8/25 | Loss: 0.00090858
Iteration 9/25 | Loss: 0.00090589
Iteration 10/25 | Loss: 0.00091017
Iteration 11/25 | Loss: 0.00090642
Iteration 12/25 | Loss: 0.00090714
Iteration 13/25 | Loss: 0.00090516
Iteration 14/25 | Loss: 0.00090220
Iteration 15/25 | Loss: 0.00090126
Iteration 16/25 | Loss: 0.00090074
Iteration 17/25 | Loss: 0.00090062
Iteration 18/25 | Loss: 0.00090055
Iteration 19/25 | Loss: 0.00090054
Iteration 20/25 | Loss: 0.00090054
Iteration 21/25 | Loss: 0.00090054
Iteration 22/25 | Loss: 0.00090054
Iteration 23/25 | Loss: 0.00090053
Iteration 24/25 | Loss: 0.00090053
Iteration 25/25 | Loss: 0.00090053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.71413231
Iteration 2/25 | Loss: 0.00049374
Iteration 3/25 | Loss: 0.00049362
Iteration 4/25 | Loss: 0.00049362
Iteration 5/25 | Loss: 0.00049362
Iteration 6/25 | Loss: 0.00049362
Iteration 7/25 | Loss: 0.00049362
Iteration 8/25 | Loss: 0.00049362
Iteration 9/25 | Loss: 0.00049362
Iteration 10/25 | Loss: 0.00049362
Iteration 11/25 | Loss: 0.00049362
Iteration 12/25 | Loss: 0.00049362
Iteration 13/25 | Loss: 0.00049362
Iteration 14/25 | Loss: 0.00049362
Iteration 15/25 | Loss: 0.00049361
Iteration 16/25 | Loss: 0.00049361
Iteration 17/25 | Loss: 0.00049361
Iteration 18/25 | Loss: 0.00049361
Iteration 19/25 | Loss: 0.00049361
Iteration 20/25 | Loss: 0.00049361
Iteration 21/25 | Loss: 0.00049361
Iteration 22/25 | Loss: 0.00049361
Iteration 23/25 | Loss: 0.00049361
Iteration 24/25 | Loss: 0.00049361
Iteration 25/25 | Loss: 0.00049361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049361
Iteration 2/1000 | Loss: 0.00007925
Iteration 3/1000 | Loss: 0.00005920
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00001888
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001558
Iteration 11/1000 | Loss: 0.00001540
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001511
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001506
Iteration 19/1000 | Loss: 0.00001505
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001497
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001494
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00007371
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00004667
Iteration 49/1000 | Loss: 0.00002834
Iteration 50/1000 | Loss: 0.00005921
Iteration 51/1000 | Loss: 0.00002575
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.4750854461453855e-05, 1.4750854461453855e-05, 1.4750854461453855e-05, 1.4750854461453855e-05, 1.4750854461453855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4750854461453855e-05

Optimization complete. Final v2v error: 3.128401041030884 mm

Highest mean error: 5.183418273925781 mm for frame 170

Lowest mean error: 2.2773005962371826 mm for frame 94

Saving results

Total time: 70.42008852958679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103520
Iteration 2/25 | Loss: 0.00331716
Iteration 3/25 | Loss: 0.00203362
Iteration 4/25 | Loss: 0.00186479
Iteration 5/25 | Loss: 0.00140977
Iteration 6/25 | Loss: 0.00113962
Iteration 7/25 | Loss: 0.00106551
Iteration 8/25 | Loss: 0.00104883
Iteration 9/25 | Loss: 0.00092704
Iteration 10/25 | Loss: 0.00089965
Iteration 11/25 | Loss: 0.00089484
Iteration 12/25 | Loss: 0.00089113
Iteration 13/25 | Loss: 0.00088543
Iteration 14/25 | Loss: 0.00088249
Iteration 15/25 | Loss: 0.00088217
Iteration 16/25 | Loss: 0.00088205
Iteration 17/25 | Loss: 0.00088117
Iteration 18/25 | Loss: 0.00088224
Iteration 19/25 | Loss: 0.00088150
Iteration 20/25 | Loss: 0.00087847
Iteration 21/25 | Loss: 0.00087833
Iteration 22/25 | Loss: 0.00087830
Iteration 23/25 | Loss: 0.00087830
Iteration 24/25 | Loss: 0.00087830
Iteration 25/25 | Loss: 0.00087830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51815033
Iteration 2/25 | Loss: 0.00050125
Iteration 3/25 | Loss: 0.00050124
Iteration 4/25 | Loss: 0.00050124
Iteration 5/25 | Loss: 0.00050124
Iteration 6/25 | Loss: 0.00050124
Iteration 7/25 | Loss: 0.00050124
Iteration 8/25 | Loss: 0.00050124
Iteration 9/25 | Loss: 0.00050124
Iteration 10/25 | Loss: 0.00050124
Iteration 11/25 | Loss: 0.00050124
Iteration 12/25 | Loss: 0.00050124
Iteration 13/25 | Loss: 0.00050124
Iteration 14/25 | Loss: 0.00050124
Iteration 15/25 | Loss: 0.00050124
Iteration 16/25 | Loss: 0.00050124
Iteration 17/25 | Loss: 0.00050124
Iteration 18/25 | Loss: 0.00050124
Iteration 19/25 | Loss: 0.00050124
Iteration 20/25 | Loss: 0.00050124
Iteration 21/25 | Loss: 0.00050124
Iteration 22/25 | Loss: 0.00050124
Iteration 23/25 | Loss: 0.00050124
Iteration 24/25 | Loss: 0.00050124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005012369365431368, 0.0005012369365431368, 0.0005012369365431368, 0.0005012369365431368, 0.0005012369365431368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005012369365431368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050124
Iteration 2/1000 | Loss: 0.00012324
Iteration 3/1000 | Loss: 0.00009637
Iteration 4/1000 | Loss: 0.00012293
Iteration 5/1000 | Loss: 0.00010426
Iteration 6/1000 | Loss: 0.00012384
Iteration 7/1000 | Loss: 0.00005180
Iteration 8/1000 | Loss: 0.00014279
Iteration 9/1000 | Loss: 0.00006879
Iteration 10/1000 | Loss: 0.00016283
Iteration 11/1000 | Loss: 0.00019096
Iteration 12/1000 | Loss: 0.00002324
Iteration 13/1000 | Loss: 0.00004368
Iteration 14/1000 | Loss: 0.00004092
Iteration 15/1000 | Loss: 0.00002088
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00007400
Iteration 18/1000 | Loss: 0.00026022
Iteration 19/1000 | Loss: 0.00015422
Iteration 20/1000 | Loss: 0.00033770
Iteration 21/1000 | Loss: 0.00023897
Iteration 22/1000 | Loss: 0.00007882
Iteration 23/1000 | Loss: 0.00020452
Iteration 24/1000 | Loss: 0.00012536
Iteration 25/1000 | Loss: 0.00032523
Iteration 26/1000 | Loss: 0.00010986
Iteration 27/1000 | Loss: 0.00018342
Iteration 28/1000 | Loss: 0.00009961
Iteration 29/1000 | Loss: 0.00014859
Iteration 30/1000 | Loss: 0.00020346
Iteration 31/1000 | Loss: 0.00044861
Iteration 32/1000 | Loss: 0.00018294
Iteration 33/1000 | Loss: 0.00023915
Iteration 34/1000 | Loss: 0.00025827
Iteration 35/1000 | Loss: 0.00002399
Iteration 36/1000 | Loss: 0.00012913
Iteration 37/1000 | Loss: 0.00022907
Iteration 38/1000 | Loss: 0.00012194
Iteration 39/1000 | Loss: 0.00009621
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00010244
Iteration 45/1000 | Loss: 0.00019382
Iteration 46/1000 | Loss: 0.00053430
Iteration 47/1000 | Loss: 0.00009765
Iteration 48/1000 | Loss: 0.00008166
Iteration 49/1000 | Loss: 0.00002340
Iteration 50/1000 | Loss: 0.00012446
Iteration 51/1000 | Loss: 0.00002608
Iteration 52/1000 | Loss: 0.00001673
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001517
Iteration 57/1000 | Loss: 0.00001501
Iteration 58/1000 | Loss: 0.00002664
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001424
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001420
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001420
Iteration 154/1000 | Loss: 0.00001420
Iteration 155/1000 | Loss: 0.00001420
Iteration 156/1000 | Loss: 0.00001420
Iteration 157/1000 | Loss: 0.00001420
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4195380572346039e-05, 1.4195380572346039e-05, 1.4195380572346039e-05, 1.4195380572346039e-05, 1.4195380572346039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4195380572346039e-05

Optimization complete. Final v2v error: 3.089670181274414 mm

Highest mean error: 8.380156517028809 mm for frame 171

Lowest mean error: 2.4813578128814697 mm for frame 156

Saving results

Total time: 130.29722094535828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450530
Iteration 2/25 | Loss: 0.00096080
Iteration 3/25 | Loss: 0.00085478
Iteration 4/25 | Loss: 0.00084096
Iteration 5/25 | Loss: 0.00083732
Iteration 6/25 | Loss: 0.00083651
Iteration 7/25 | Loss: 0.00083651
Iteration 8/25 | Loss: 0.00083651
Iteration 9/25 | Loss: 0.00083651
Iteration 10/25 | Loss: 0.00083651
Iteration 11/25 | Loss: 0.00083651
Iteration 12/25 | Loss: 0.00083651
Iteration 13/25 | Loss: 0.00083651
Iteration 14/25 | Loss: 0.00083651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008365059038624167, 0.0008365059038624167, 0.0008365059038624167, 0.0008365059038624167, 0.0008365059038624167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008365059038624167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35339022
Iteration 2/25 | Loss: 0.00046006
Iteration 3/25 | Loss: 0.00046005
Iteration 4/25 | Loss: 0.00046005
Iteration 5/25 | Loss: 0.00046004
Iteration 6/25 | Loss: 0.00046004
Iteration 7/25 | Loss: 0.00046004
Iteration 8/25 | Loss: 0.00046004
Iteration 9/25 | Loss: 0.00046004
Iteration 10/25 | Loss: 0.00046004
Iteration 11/25 | Loss: 0.00046004
Iteration 12/25 | Loss: 0.00046004
Iteration 13/25 | Loss: 0.00046004
Iteration 14/25 | Loss: 0.00046004
Iteration 15/25 | Loss: 0.00046004
Iteration 16/25 | Loss: 0.00046004
Iteration 17/25 | Loss: 0.00046004
Iteration 18/25 | Loss: 0.00046004
Iteration 19/25 | Loss: 0.00046004
Iteration 20/25 | Loss: 0.00046004
Iteration 21/25 | Loss: 0.00046004
Iteration 22/25 | Loss: 0.00046004
Iteration 23/25 | Loss: 0.00046004
Iteration 24/25 | Loss: 0.00046004
Iteration 25/25 | Loss: 0.00046004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046004
Iteration 2/1000 | Loss: 0.00001542
Iteration 3/1000 | Loss: 0.00001075
Iteration 4/1000 | Loss: 0.00000990
Iteration 5/1000 | Loss: 0.00000947
Iteration 6/1000 | Loss: 0.00000921
Iteration 7/1000 | Loss: 0.00000912
Iteration 8/1000 | Loss: 0.00000909
Iteration 9/1000 | Loss: 0.00000907
Iteration 10/1000 | Loss: 0.00000903
Iteration 11/1000 | Loss: 0.00000896
Iteration 12/1000 | Loss: 0.00000895
Iteration 13/1000 | Loss: 0.00000895
Iteration 14/1000 | Loss: 0.00000894
Iteration 15/1000 | Loss: 0.00000892
Iteration 16/1000 | Loss: 0.00000890
Iteration 17/1000 | Loss: 0.00000884
Iteration 18/1000 | Loss: 0.00000882
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000880
Iteration 22/1000 | Loss: 0.00000879
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000876
Iteration 27/1000 | Loss: 0.00000876
Iteration 28/1000 | Loss: 0.00000876
Iteration 29/1000 | Loss: 0.00000875
Iteration 30/1000 | Loss: 0.00000875
Iteration 31/1000 | Loss: 0.00000874
Iteration 32/1000 | Loss: 0.00000873
Iteration 33/1000 | Loss: 0.00000873
Iteration 34/1000 | Loss: 0.00000869
Iteration 35/1000 | Loss: 0.00000869
Iteration 36/1000 | Loss: 0.00000869
Iteration 37/1000 | Loss: 0.00000868
Iteration 38/1000 | Loss: 0.00000867
Iteration 39/1000 | Loss: 0.00000866
Iteration 40/1000 | Loss: 0.00000866
Iteration 41/1000 | Loss: 0.00000866
Iteration 42/1000 | Loss: 0.00000866
Iteration 43/1000 | Loss: 0.00000865
Iteration 44/1000 | Loss: 0.00000865
Iteration 45/1000 | Loss: 0.00000865
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000863
Iteration 49/1000 | Loss: 0.00000863
Iteration 50/1000 | Loss: 0.00000863
Iteration 51/1000 | Loss: 0.00000862
Iteration 52/1000 | Loss: 0.00000862
Iteration 53/1000 | Loss: 0.00000862
Iteration 54/1000 | Loss: 0.00000862
Iteration 55/1000 | Loss: 0.00000862
Iteration 56/1000 | Loss: 0.00000862
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000861
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000860
Iteration 62/1000 | Loss: 0.00000860
Iteration 63/1000 | Loss: 0.00000860
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000859
Iteration 66/1000 | Loss: 0.00000859
Iteration 67/1000 | Loss: 0.00000859
Iteration 68/1000 | Loss: 0.00000859
Iteration 69/1000 | Loss: 0.00000859
Iteration 70/1000 | Loss: 0.00000858
Iteration 71/1000 | Loss: 0.00000858
Iteration 72/1000 | Loss: 0.00000858
Iteration 73/1000 | Loss: 0.00000858
Iteration 74/1000 | Loss: 0.00000857
Iteration 75/1000 | Loss: 0.00000857
Iteration 76/1000 | Loss: 0.00000857
Iteration 77/1000 | Loss: 0.00000857
Iteration 78/1000 | Loss: 0.00000857
Iteration 79/1000 | Loss: 0.00000857
Iteration 80/1000 | Loss: 0.00000857
Iteration 81/1000 | Loss: 0.00000857
Iteration 82/1000 | Loss: 0.00000857
Iteration 83/1000 | Loss: 0.00000857
Iteration 84/1000 | Loss: 0.00000856
Iteration 85/1000 | Loss: 0.00000856
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000856
Iteration 88/1000 | Loss: 0.00000856
Iteration 89/1000 | Loss: 0.00000856
Iteration 90/1000 | Loss: 0.00000856
Iteration 91/1000 | Loss: 0.00000856
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000855
Iteration 95/1000 | Loss: 0.00000855
Iteration 96/1000 | Loss: 0.00000855
Iteration 97/1000 | Loss: 0.00000855
Iteration 98/1000 | Loss: 0.00000855
Iteration 99/1000 | Loss: 0.00000854
Iteration 100/1000 | Loss: 0.00000854
Iteration 101/1000 | Loss: 0.00000854
Iteration 102/1000 | Loss: 0.00000854
Iteration 103/1000 | Loss: 0.00000854
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000854
Iteration 107/1000 | Loss: 0.00000854
Iteration 108/1000 | Loss: 0.00000854
Iteration 109/1000 | Loss: 0.00000854
Iteration 110/1000 | Loss: 0.00000854
Iteration 111/1000 | Loss: 0.00000854
Iteration 112/1000 | Loss: 0.00000854
Iteration 113/1000 | Loss: 0.00000853
Iteration 114/1000 | Loss: 0.00000853
Iteration 115/1000 | Loss: 0.00000853
Iteration 116/1000 | Loss: 0.00000853
Iteration 117/1000 | Loss: 0.00000853
Iteration 118/1000 | Loss: 0.00000853
Iteration 119/1000 | Loss: 0.00000853
Iteration 120/1000 | Loss: 0.00000853
Iteration 121/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [8.532309038855601e-06, 8.532309038855601e-06, 8.532309038855601e-06, 8.532309038855601e-06, 8.532309038855601e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.532309038855601e-06

Optimization complete. Final v2v error: 2.503699779510498 mm

Highest mean error: 2.8852128982543945 mm for frame 184

Lowest mean error: 2.2594215869903564 mm for frame 155

Saving results

Total time: 32.11161136627197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386968
Iteration 2/25 | Loss: 0.00098388
Iteration 3/25 | Loss: 0.00084533
Iteration 4/25 | Loss: 0.00082893
Iteration 5/25 | Loss: 0.00082518
Iteration 6/25 | Loss: 0.00082415
Iteration 7/25 | Loss: 0.00082400
Iteration 8/25 | Loss: 0.00082400
Iteration 9/25 | Loss: 0.00082400
Iteration 10/25 | Loss: 0.00082400
Iteration 11/25 | Loss: 0.00082400
Iteration 12/25 | Loss: 0.00082400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008239976596087217, 0.0008239976596087217, 0.0008239976596087217, 0.0008239976596087217, 0.0008239976596087217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008239976596087217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37666428
Iteration 2/25 | Loss: 0.00035031
Iteration 3/25 | Loss: 0.00035031
Iteration 4/25 | Loss: 0.00035031
Iteration 5/25 | Loss: 0.00035031
Iteration 6/25 | Loss: 0.00035031
Iteration 7/25 | Loss: 0.00035031
Iteration 8/25 | Loss: 0.00035030
Iteration 9/25 | Loss: 0.00035030
Iteration 10/25 | Loss: 0.00035030
Iteration 11/25 | Loss: 0.00035030
Iteration 12/25 | Loss: 0.00035030
Iteration 13/25 | Loss: 0.00035030
Iteration 14/25 | Loss: 0.00035030
Iteration 15/25 | Loss: 0.00035030
Iteration 16/25 | Loss: 0.00035030
Iteration 17/25 | Loss: 0.00035030
Iteration 18/25 | Loss: 0.00035030
Iteration 19/25 | Loss: 0.00035030
Iteration 20/25 | Loss: 0.00035030
Iteration 21/25 | Loss: 0.00035030
Iteration 22/25 | Loss: 0.00035030
Iteration 23/25 | Loss: 0.00035030
Iteration 24/25 | Loss: 0.00035030
Iteration 25/25 | Loss: 0.00035030

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035030
Iteration 2/1000 | Loss: 0.00002820
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001198
Iteration 6/1000 | Loss: 0.00001126
Iteration 7/1000 | Loss: 0.00001076
Iteration 8/1000 | Loss: 0.00001049
Iteration 9/1000 | Loss: 0.00001030
Iteration 10/1000 | Loss: 0.00001029
Iteration 11/1000 | Loss: 0.00001018
Iteration 12/1000 | Loss: 0.00001016
Iteration 13/1000 | Loss: 0.00001014
Iteration 14/1000 | Loss: 0.00001013
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001003
Iteration 19/1000 | Loss: 0.00001003
Iteration 20/1000 | Loss: 0.00001002
Iteration 21/1000 | Loss: 0.00001001
Iteration 22/1000 | Loss: 0.00001000
Iteration 23/1000 | Loss: 0.00001000
Iteration 24/1000 | Loss: 0.00001000
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000997
Iteration 35/1000 | Loss: 0.00000997
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000995
Iteration 39/1000 | Loss: 0.00000995
Iteration 40/1000 | Loss: 0.00000995
Iteration 41/1000 | Loss: 0.00000995
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000994
Iteration 49/1000 | Loss: 0.00000994
Iteration 50/1000 | Loss: 0.00000994
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000992
Iteration 56/1000 | Loss: 0.00000992
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000991
Iteration 64/1000 | Loss: 0.00000990
Iteration 65/1000 | Loss: 0.00000989
Iteration 66/1000 | Loss: 0.00000989
Iteration 67/1000 | Loss: 0.00000989
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000987
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000984
Iteration 86/1000 | Loss: 0.00000983
Iteration 87/1000 | Loss: 0.00000983
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000982
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000981
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000978
Iteration 115/1000 | Loss: 0.00000978
Iteration 116/1000 | Loss: 0.00000978
Iteration 117/1000 | Loss: 0.00000978
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000978
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000977
Iteration 123/1000 | Loss: 0.00000977
Iteration 124/1000 | Loss: 0.00000977
Iteration 125/1000 | Loss: 0.00000977
Iteration 126/1000 | Loss: 0.00000977
Iteration 127/1000 | Loss: 0.00000977
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000976
Iteration 132/1000 | Loss: 0.00000976
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000975
Iteration 137/1000 | Loss: 0.00000975
Iteration 138/1000 | Loss: 0.00000975
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000975
Iteration 146/1000 | Loss: 0.00000975
Iteration 147/1000 | Loss: 0.00000975
Iteration 148/1000 | Loss: 0.00000975
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000974
Iteration 152/1000 | Loss: 0.00000974
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000974
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000974
Iteration 157/1000 | Loss: 0.00000974
Iteration 158/1000 | Loss: 0.00000974
Iteration 159/1000 | Loss: 0.00000974
Iteration 160/1000 | Loss: 0.00000974
Iteration 161/1000 | Loss: 0.00000974
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Iteration 165/1000 | Loss: 0.00000973
Iteration 166/1000 | Loss: 0.00000973
Iteration 167/1000 | Loss: 0.00000973
Iteration 168/1000 | Loss: 0.00000973
Iteration 169/1000 | Loss: 0.00000973
Iteration 170/1000 | Loss: 0.00000973
Iteration 171/1000 | Loss: 0.00000973
Iteration 172/1000 | Loss: 0.00000973
Iteration 173/1000 | Loss: 0.00000973
Iteration 174/1000 | Loss: 0.00000973
Iteration 175/1000 | Loss: 0.00000973
Iteration 176/1000 | Loss: 0.00000972
Iteration 177/1000 | Loss: 0.00000972
Iteration 178/1000 | Loss: 0.00000972
Iteration 179/1000 | Loss: 0.00000972
Iteration 180/1000 | Loss: 0.00000972
Iteration 181/1000 | Loss: 0.00000972
Iteration 182/1000 | Loss: 0.00000972
Iteration 183/1000 | Loss: 0.00000972
Iteration 184/1000 | Loss: 0.00000972
Iteration 185/1000 | Loss: 0.00000972
Iteration 186/1000 | Loss: 0.00000972
Iteration 187/1000 | Loss: 0.00000972
Iteration 188/1000 | Loss: 0.00000972
Iteration 189/1000 | Loss: 0.00000972
Iteration 190/1000 | Loss: 0.00000972
Iteration 191/1000 | Loss: 0.00000972
Iteration 192/1000 | Loss: 0.00000972
Iteration 193/1000 | Loss: 0.00000972
Iteration 194/1000 | Loss: 0.00000972
Iteration 195/1000 | Loss: 0.00000972
Iteration 196/1000 | Loss: 0.00000972
Iteration 197/1000 | Loss: 0.00000972
Iteration 198/1000 | Loss: 0.00000972
Iteration 199/1000 | Loss: 0.00000972
Iteration 200/1000 | Loss: 0.00000972
Iteration 201/1000 | Loss: 0.00000972
Iteration 202/1000 | Loss: 0.00000972
Iteration 203/1000 | Loss: 0.00000972
Iteration 204/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [9.718564797367435e-06, 9.718564797367435e-06, 9.718564797367435e-06, 9.718564797367435e-06, 9.718564797367435e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.718564797367435e-06

Optimization complete. Final v2v error: 2.5863702297210693 mm

Highest mean error: 3.1540093421936035 mm for frame 66

Lowest mean error: 2.2149040699005127 mm for frame 12

Saving results

Total time: 36.43631076812744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061812
Iteration 2/25 | Loss: 0.00196664
Iteration 3/25 | Loss: 0.00172402
Iteration 4/25 | Loss: 0.00108077
Iteration 5/25 | Loss: 0.00100434
Iteration 6/25 | Loss: 0.00102656
Iteration 7/25 | Loss: 0.00101024
Iteration 8/25 | Loss: 0.00095839
Iteration 9/25 | Loss: 0.00091641
Iteration 10/25 | Loss: 0.00090991
Iteration 11/25 | Loss: 0.00089521
Iteration 12/25 | Loss: 0.00089283
Iteration 13/25 | Loss: 0.00089397
Iteration 14/25 | Loss: 0.00088329
Iteration 15/25 | Loss: 0.00087746
Iteration 16/25 | Loss: 0.00086979
Iteration 17/25 | Loss: 0.00086844
Iteration 18/25 | Loss: 0.00086483
Iteration 19/25 | Loss: 0.00086353
Iteration 20/25 | Loss: 0.00086285
Iteration 21/25 | Loss: 0.00086267
Iteration 22/25 | Loss: 0.00086385
Iteration 23/25 | Loss: 0.00085742
Iteration 24/25 | Loss: 0.00085976
Iteration 25/25 | Loss: 0.00085596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43168128
Iteration 2/25 | Loss: 0.00058139
Iteration 3/25 | Loss: 0.00051234
Iteration 4/25 | Loss: 0.00051234
Iteration 5/25 | Loss: 0.00051233
Iteration 6/25 | Loss: 0.00051233
Iteration 7/25 | Loss: 0.00051233
Iteration 8/25 | Loss: 0.00051233
Iteration 9/25 | Loss: 0.00051233
Iteration 10/25 | Loss: 0.00051233
Iteration 11/25 | Loss: 0.00051233
Iteration 12/25 | Loss: 0.00051233
Iteration 13/25 | Loss: 0.00051233
Iteration 14/25 | Loss: 0.00051233
Iteration 15/25 | Loss: 0.00051233
Iteration 16/25 | Loss: 0.00051233
Iteration 17/25 | Loss: 0.00051233
Iteration 18/25 | Loss: 0.00051233
Iteration 19/25 | Loss: 0.00051233
Iteration 20/25 | Loss: 0.00051233
Iteration 21/25 | Loss: 0.00051233
Iteration 22/25 | Loss: 0.00051233
Iteration 23/25 | Loss: 0.00051233
Iteration 24/25 | Loss: 0.00051233
Iteration 25/25 | Loss: 0.00051233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051233
Iteration 2/1000 | Loss: 0.00007161
Iteration 3/1000 | Loss: 0.00200444
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00005070
Iteration 6/1000 | Loss: 0.00001613
Iteration 7/1000 | Loss: 0.00016176
Iteration 8/1000 | Loss: 0.00070586
Iteration 9/1000 | Loss: 0.00204321
Iteration 10/1000 | Loss: 0.00015781
Iteration 11/1000 | Loss: 0.00011455
Iteration 12/1000 | Loss: 0.00027574
Iteration 13/1000 | Loss: 0.00098764
Iteration 14/1000 | Loss: 0.00041165
Iteration 15/1000 | Loss: 0.00018179
Iteration 16/1000 | Loss: 0.00040497
Iteration 17/1000 | Loss: 0.00093630
Iteration 18/1000 | Loss: 0.00004237
Iteration 19/1000 | Loss: 0.00022161
Iteration 20/1000 | Loss: 0.00001836
Iteration 21/1000 | Loss: 0.00001620
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00012956
Iteration 24/1000 | Loss: 0.00006322
Iteration 25/1000 | Loss: 0.00019277
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00046335
Iteration 30/1000 | Loss: 0.00033079
Iteration 31/1000 | Loss: 0.00029245
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00003771
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00003069
Iteration 36/1000 | Loss: 0.00014580
Iteration 37/1000 | Loss: 0.00002238
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001057
Iteration 42/1000 | Loss: 0.00001045
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001035
Iteration 45/1000 | Loss: 0.00001035
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00005101
Iteration 49/1000 | Loss: 0.00002774
Iteration 50/1000 | Loss: 0.00001618
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00003325
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001017
Iteration 59/1000 | Loss: 0.00001016
Iteration 60/1000 | Loss: 0.00001016
Iteration 61/1000 | Loss: 0.00001016
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001013
Iteration 68/1000 | Loss: 0.00001013
Iteration 69/1000 | Loss: 0.00001013
Iteration 70/1000 | Loss: 0.00001013
Iteration 71/1000 | Loss: 0.00001013
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001009
Iteration 81/1000 | Loss: 0.00005086
Iteration 82/1000 | Loss: 0.00001346
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001007
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00003164
Iteration 87/1000 | Loss: 0.00006579
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00002714
Iteration 90/1000 | Loss: 0.00001012
Iteration 91/1000 | Loss: 0.00001010
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001005
Iteration 94/1000 | Loss: 0.00001005
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001002
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001000
Iteration 102/1000 | Loss: 0.00001000
Iteration 103/1000 | Loss: 0.00001000
Iteration 104/1000 | Loss: 0.00001000
Iteration 105/1000 | Loss: 0.00001000
Iteration 106/1000 | Loss: 0.00001000
Iteration 107/1000 | Loss: 0.00001000
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00001000
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000998
Iteration 121/1000 | Loss: 0.00000998
Iteration 122/1000 | Loss: 0.00000998
Iteration 123/1000 | Loss: 0.00000998
Iteration 124/1000 | Loss: 0.00000998
Iteration 125/1000 | Loss: 0.00000998
Iteration 126/1000 | Loss: 0.00000998
Iteration 127/1000 | Loss: 0.00000998
Iteration 128/1000 | Loss: 0.00000998
Iteration 129/1000 | Loss: 0.00000998
Iteration 130/1000 | Loss: 0.00000998
Iteration 131/1000 | Loss: 0.00000998
Iteration 132/1000 | Loss: 0.00000998
Iteration 133/1000 | Loss: 0.00000998
Iteration 134/1000 | Loss: 0.00000998
Iteration 135/1000 | Loss: 0.00000998
Iteration 136/1000 | Loss: 0.00000998
Iteration 137/1000 | Loss: 0.00000998
Iteration 138/1000 | Loss: 0.00000998
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000997
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000997
Iteration 155/1000 | Loss: 0.00000997
Iteration 156/1000 | Loss: 0.00000997
Iteration 157/1000 | Loss: 0.00000997
Iteration 158/1000 | Loss: 0.00000997
Iteration 159/1000 | Loss: 0.00000997
Iteration 160/1000 | Loss: 0.00000997
Iteration 161/1000 | Loss: 0.00000997
Iteration 162/1000 | Loss: 0.00000997
Iteration 163/1000 | Loss: 0.00000997
Iteration 164/1000 | Loss: 0.00000997
Iteration 165/1000 | Loss: 0.00000997
Iteration 166/1000 | Loss: 0.00000997
Iteration 167/1000 | Loss: 0.00000997
Iteration 168/1000 | Loss: 0.00000997
Iteration 169/1000 | Loss: 0.00000997
Iteration 170/1000 | Loss: 0.00000997
Iteration 171/1000 | Loss: 0.00000997
Iteration 172/1000 | Loss: 0.00000997
Iteration 173/1000 | Loss: 0.00000997
Iteration 174/1000 | Loss: 0.00000997
Iteration 175/1000 | Loss: 0.00000997
Iteration 176/1000 | Loss: 0.00000997
Iteration 177/1000 | Loss: 0.00000997
Iteration 178/1000 | Loss: 0.00000997
Iteration 179/1000 | Loss: 0.00000997
Iteration 180/1000 | Loss: 0.00000997
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000997
Iteration 184/1000 | Loss: 0.00000997
Iteration 185/1000 | Loss: 0.00000997
Iteration 186/1000 | Loss: 0.00000997
Iteration 187/1000 | Loss: 0.00000997
Iteration 188/1000 | Loss: 0.00000997
Iteration 189/1000 | Loss: 0.00000997
Iteration 190/1000 | Loss: 0.00000997
Iteration 191/1000 | Loss: 0.00000997
Iteration 192/1000 | Loss: 0.00000997
Iteration 193/1000 | Loss: 0.00000997
Iteration 194/1000 | Loss: 0.00000997
Iteration 195/1000 | Loss: 0.00000997
Iteration 196/1000 | Loss: 0.00000997
Iteration 197/1000 | Loss: 0.00000997
Iteration 198/1000 | Loss: 0.00000997
Iteration 199/1000 | Loss: 0.00000997
Iteration 200/1000 | Loss: 0.00000997
Iteration 201/1000 | Loss: 0.00000997
Iteration 202/1000 | Loss: 0.00000997
Iteration 203/1000 | Loss: 0.00000997
Iteration 204/1000 | Loss: 0.00000997
Iteration 205/1000 | Loss: 0.00000997
Iteration 206/1000 | Loss: 0.00000997
Iteration 207/1000 | Loss: 0.00000997
Iteration 208/1000 | Loss: 0.00000997
Iteration 209/1000 | Loss: 0.00000997
Iteration 210/1000 | Loss: 0.00000997
Iteration 211/1000 | Loss: 0.00000997
Iteration 212/1000 | Loss: 0.00000997
Iteration 213/1000 | Loss: 0.00000997
Iteration 214/1000 | Loss: 0.00000997
Iteration 215/1000 | Loss: 0.00000997
Iteration 216/1000 | Loss: 0.00000997
Iteration 217/1000 | Loss: 0.00000997
Iteration 218/1000 | Loss: 0.00000997
Iteration 219/1000 | Loss: 0.00000997
Iteration 220/1000 | Loss: 0.00000997
Iteration 221/1000 | Loss: 0.00000997
Iteration 222/1000 | Loss: 0.00000997
Iteration 223/1000 | Loss: 0.00000997
Iteration 224/1000 | Loss: 0.00000997
Iteration 225/1000 | Loss: 0.00000997
Iteration 226/1000 | Loss: 0.00000997
Iteration 227/1000 | Loss: 0.00000997
Iteration 228/1000 | Loss: 0.00000997
Iteration 229/1000 | Loss: 0.00000997
Iteration 230/1000 | Loss: 0.00000997
Iteration 231/1000 | Loss: 0.00000997
Iteration 232/1000 | Loss: 0.00000997
Iteration 233/1000 | Loss: 0.00000997
Iteration 234/1000 | Loss: 0.00000997
Iteration 235/1000 | Loss: 0.00000997
Iteration 236/1000 | Loss: 0.00000997
Iteration 237/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [9.96807557385182e-06, 9.96807557385182e-06, 9.96807557385182e-06, 9.96807557385182e-06, 9.96807557385182e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.96807557385182e-06

Optimization complete. Final v2v error: 2.6770033836364746 mm

Highest mean error: 3.512117862701416 mm for frame 56

Lowest mean error: 2.2463438510894775 mm for frame 29

Saving results

Total time: 134.00601387023926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409161
Iteration 2/25 | Loss: 0.00104436
Iteration 3/25 | Loss: 0.00088715
Iteration 4/25 | Loss: 0.00086634
Iteration 5/25 | Loss: 0.00086075
Iteration 6/25 | Loss: 0.00085892
Iteration 7/25 | Loss: 0.00085844
Iteration 8/25 | Loss: 0.00085844
Iteration 9/25 | Loss: 0.00085844
Iteration 10/25 | Loss: 0.00085844
Iteration 11/25 | Loss: 0.00085844
Iteration 12/25 | Loss: 0.00085844
Iteration 13/25 | Loss: 0.00085844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008584379102103412, 0.0008584379102103412, 0.0008584379102103412, 0.0008584379102103412, 0.0008584379102103412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008584379102103412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51871026
Iteration 2/25 | Loss: 0.00049191
Iteration 3/25 | Loss: 0.00049191
Iteration 4/25 | Loss: 0.00049191
Iteration 5/25 | Loss: 0.00049191
Iteration 6/25 | Loss: 0.00049191
Iteration 7/25 | Loss: 0.00049191
Iteration 8/25 | Loss: 0.00049191
Iteration 9/25 | Loss: 0.00049191
Iteration 10/25 | Loss: 0.00049191
Iteration 11/25 | Loss: 0.00049191
Iteration 12/25 | Loss: 0.00049191
Iteration 13/25 | Loss: 0.00049191
Iteration 14/25 | Loss: 0.00049191
Iteration 15/25 | Loss: 0.00049191
Iteration 16/25 | Loss: 0.00049191
Iteration 17/25 | Loss: 0.00049191
Iteration 18/25 | Loss: 0.00049191
Iteration 19/25 | Loss: 0.00049191
Iteration 20/25 | Loss: 0.00049191
Iteration 21/25 | Loss: 0.00049191
Iteration 22/25 | Loss: 0.00049191
Iteration 23/25 | Loss: 0.00049191
Iteration 24/25 | Loss: 0.00049191
Iteration 25/25 | Loss: 0.00049191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049191
Iteration 2/1000 | Loss: 0.00003652
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001189
Iteration 6/1000 | Loss: 0.00001130
Iteration 7/1000 | Loss: 0.00001080
Iteration 8/1000 | Loss: 0.00001056
Iteration 9/1000 | Loss: 0.00001037
Iteration 10/1000 | Loss: 0.00001022
Iteration 11/1000 | Loss: 0.00001020
Iteration 12/1000 | Loss: 0.00001020
Iteration 13/1000 | Loss: 0.00001019
Iteration 14/1000 | Loss: 0.00001017
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001013
Iteration 17/1000 | Loss: 0.00001012
Iteration 18/1000 | Loss: 0.00001012
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001011
Iteration 22/1000 | Loss: 0.00001010
Iteration 23/1000 | Loss: 0.00001010
Iteration 24/1000 | Loss: 0.00001010
Iteration 25/1000 | Loss: 0.00001010
Iteration 26/1000 | Loss: 0.00001010
Iteration 27/1000 | Loss: 0.00001007
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001002
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001001
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000998
Iteration 38/1000 | Loss: 0.00000998
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000997
Iteration 41/1000 | Loss: 0.00000997
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000996
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000995
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000994
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000993
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000992
Iteration 60/1000 | Loss: 0.00000992
Iteration 61/1000 | Loss: 0.00000992
Iteration 62/1000 | Loss: 0.00000992
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000991
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000991
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000990
Iteration 78/1000 | Loss: 0.00000990
Iteration 79/1000 | Loss: 0.00000990
Iteration 80/1000 | Loss: 0.00000989
Iteration 81/1000 | Loss: 0.00000989
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000989
Iteration 84/1000 | Loss: 0.00000988
Iteration 85/1000 | Loss: 0.00000988
Iteration 86/1000 | Loss: 0.00000988
Iteration 87/1000 | Loss: 0.00000988
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000987
Iteration 90/1000 | Loss: 0.00000987
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000987
Iteration 94/1000 | Loss: 0.00000987
Iteration 95/1000 | Loss: 0.00000987
Iteration 96/1000 | Loss: 0.00000987
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000986
Iteration 101/1000 | Loss: 0.00000986
Iteration 102/1000 | Loss: 0.00000986
Iteration 103/1000 | Loss: 0.00000986
Iteration 104/1000 | Loss: 0.00000986
Iteration 105/1000 | Loss: 0.00000986
Iteration 106/1000 | Loss: 0.00000986
Iteration 107/1000 | Loss: 0.00000986
Iteration 108/1000 | Loss: 0.00000986
Iteration 109/1000 | Loss: 0.00000986
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000986
Iteration 112/1000 | Loss: 0.00000986
Iteration 113/1000 | Loss: 0.00000986
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000985
Iteration 116/1000 | Loss: 0.00000985
Iteration 117/1000 | Loss: 0.00000985
Iteration 118/1000 | Loss: 0.00000985
Iteration 119/1000 | Loss: 0.00000985
Iteration 120/1000 | Loss: 0.00000985
Iteration 121/1000 | Loss: 0.00000985
Iteration 122/1000 | Loss: 0.00000985
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000984
Iteration 126/1000 | Loss: 0.00000984
Iteration 127/1000 | Loss: 0.00000984
Iteration 128/1000 | Loss: 0.00000984
Iteration 129/1000 | Loss: 0.00000984
Iteration 130/1000 | Loss: 0.00000984
Iteration 131/1000 | Loss: 0.00000984
Iteration 132/1000 | Loss: 0.00000984
Iteration 133/1000 | Loss: 0.00000984
Iteration 134/1000 | Loss: 0.00000984
Iteration 135/1000 | Loss: 0.00000984
Iteration 136/1000 | Loss: 0.00000983
Iteration 137/1000 | Loss: 0.00000983
Iteration 138/1000 | Loss: 0.00000983
Iteration 139/1000 | Loss: 0.00000983
Iteration 140/1000 | Loss: 0.00000983
Iteration 141/1000 | Loss: 0.00000983
Iteration 142/1000 | Loss: 0.00000983
Iteration 143/1000 | Loss: 0.00000983
Iteration 144/1000 | Loss: 0.00000983
Iteration 145/1000 | Loss: 0.00000983
Iteration 146/1000 | Loss: 0.00000983
Iteration 147/1000 | Loss: 0.00000983
Iteration 148/1000 | Loss: 0.00000983
Iteration 149/1000 | Loss: 0.00000983
Iteration 150/1000 | Loss: 0.00000983
Iteration 151/1000 | Loss: 0.00000983
Iteration 152/1000 | Loss: 0.00000983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.825502274907194e-06, 9.825502274907194e-06, 9.825502274907194e-06, 9.825502274907194e-06, 9.825502274907194e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.825502274907194e-06

Optimization complete. Final v2v error: 2.6314473152160645 mm

Highest mean error: 3.8856005668640137 mm for frame 29

Lowest mean error: 2.2403342723846436 mm for frame 17

Saving results

Total time: 35.30170202255249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459886
Iteration 2/25 | Loss: 0.00105935
Iteration 3/25 | Loss: 0.00091560
Iteration 4/25 | Loss: 0.00089177
Iteration 5/25 | Loss: 0.00088641
Iteration 6/25 | Loss: 0.00088527
Iteration 7/25 | Loss: 0.00088527
Iteration 8/25 | Loss: 0.00088527
Iteration 9/25 | Loss: 0.00088527
Iteration 10/25 | Loss: 0.00088527
Iteration 11/25 | Loss: 0.00088527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008852730970829725, 0.0008852730970829725, 0.0008852730970829725, 0.0008852730970829725, 0.0008852730970829725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008852730970829725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.29629207
Iteration 2/25 | Loss: 0.00056232
Iteration 3/25 | Loss: 0.00056232
Iteration 4/25 | Loss: 0.00056232
Iteration 5/25 | Loss: 0.00056232
Iteration 6/25 | Loss: 0.00056232
Iteration 7/25 | Loss: 0.00056232
Iteration 8/25 | Loss: 0.00056232
Iteration 9/25 | Loss: 0.00056232
Iteration 10/25 | Loss: 0.00056232
Iteration 11/25 | Loss: 0.00056232
Iteration 12/25 | Loss: 0.00056232
Iteration 13/25 | Loss: 0.00056232
Iteration 14/25 | Loss: 0.00056232
Iteration 15/25 | Loss: 0.00056232
Iteration 16/25 | Loss: 0.00056232
Iteration 17/25 | Loss: 0.00056232
Iteration 18/25 | Loss: 0.00056232
Iteration 19/25 | Loss: 0.00056232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005623173201456666, 0.0005623173201456666, 0.0005623173201456666, 0.0005623173201456666, 0.0005623173201456666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005623173201456666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056232
Iteration 2/1000 | Loss: 0.00003223
Iteration 3/1000 | Loss: 0.00002057
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001370
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001359
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001358
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001356
Iteration 37/1000 | Loss: 0.00001356
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001350
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001350
Iteration 57/1000 | Loss: 0.00001350
Iteration 58/1000 | Loss: 0.00001350
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001345
Iteration 78/1000 | Loss: 0.00001345
Iteration 79/1000 | Loss: 0.00001345
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001342
Iteration 89/1000 | Loss: 0.00001342
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001341
Iteration 93/1000 | Loss: 0.00001341
Iteration 94/1000 | Loss: 0.00001341
Iteration 95/1000 | Loss: 0.00001341
Iteration 96/1000 | Loss: 0.00001340
Iteration 97/1000 | Loss: 0.00001340
Iteration 98/1000 | Loss: 0.00001340
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001340
Iteration 101/1000 | Loss: 0.00001340
Iteration 102/1000 | Loss: 0.00001339
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001339
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001339
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001338
Iteration 120/1000 | Loss: 0.00001338
Iteration 121/1000 | Loss: 0.00001338
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001337
Iteration 132/1000 | Loss: 0.00001337
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.3360986486077309e-05, 1.3360986486077309e-05, 1.3360986486077309e-05, 1.3360986486077309e-05, 1.3360986486077309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3360986486077309e-05

Optimization complete. Final v2v error: 2.9892992973327637 mm

Highest mean error: 4.235647678375244 mm for frame 89

Lowest mean error: 2.375681161880493 mm for frame 136

Saving results

Total time: 40.21823334693909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604140
Iteration 2/25 | Loss: 0.00161422
Iteration 3/25 | Loss: 0.00105953
Iteration 4/25 | Loss: 0.00102271
Iteration 5/25 | Loss: 0.00101838
Iteration 6/25 | Loss: 0.00101601
Iteration 7/25 | Loss: 0.00101601
Iteration 8/25 | Loss: 0.00101601
Iteration 9/25 | Loss: 0.00101601
Iteration 10/25 | Loss: 0.00101601
Iteration 11/25 | Loss: 0.00101601
Iteration 12/25 | Loss: 0.00101601
Iteration 13/25 | Loss: 0.00101601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010160134406760335, 0.0010160134406760335, 0.0010160134406760335, 0.0010160134406760335, 0.0010160134406760335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010160134406760335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02999151
Iteration 2/25 | Loss: 0.00061362
Iteration 3/25 | Loss: 0.00061360
Iteration 4/25 | Loss: 0.00061360
Iteration 5/25 | Loss: 0.00061360
Iteration 6/25 | Loss: 0.00061360
Iteration 7/25 | Loss: 0.00061360
Iteration 8/25 | Loss: 0.00061360
Iteration 9/25 | Loss: 0.00061360
Iteration 10/25 | Loss: 0.00061360
Iteration 11/25 | Loss: 0.00061360
Iteration 12/25 | Loss: 0.00061360
Iteration 13/25 | Loss: 0.00061360
Iteration 14/25 | Loss: 0.00061360
Iteration 15/25 | Loss: 0.00061360
Iteration 16/25 | Loss: 0.00061360
Iteration 17/25 | Loss: 0.00061360
Iteration 18/25 | Loss: 0.00061360
Iteration 19/25 | Loss: 0.00061360
Iteration 20/25 | Loss: 0.00061360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006135996081866324, 0.0006135996081866324, 0.0006135996081866324, 0.0006135996081866324, 0.0006135996081866324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006135996081866324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061360
Iteration 2/1000 | Loss: 0.00007544
Iteration 3/1000 | Loss: 0.00005158
Iteration 4/1000 | Loss: 0.00004360
Iteration 5/1000 | Loss: 0.00004098
Iteration 6/1000 | Loss: 0.00003977
Iteration 7/1000 | Loss: 0.00003902
Iteration 8/1000 | Loss: 0.00003805
Iteration 9/1000 | Loss: 0.00003720
Iteration 10/1000 | Loss: 0.00003663
Iteration 11/1000 | Loss: 0.00003625
Iteration 12/1000 | Loss: 0.00003571
Iteration 13/1000 | Loss: 0.00003528
Iteration 14/1000 | Loss: 0.00003494
Iteration 15/1000 | Loss: 0.00003463
Iteration 16/1000 | Loss: 0.00003439
Iteration 17/1000 | Loss: 0.00003415
Iteration 18/1000 | Loss: 0.00003392
Iteration 19/1000 | Loss: 0.00003375
Iteration 20/1000 | Loss: 0.00003371
Iteration 21/1000 | Loss: 0.00003367
Iteration 22/1000 | Loss: 0.00003366
Iteration 23/1000 | Loss: 0.00003365
Iteration 24/1000 | Loss: 0.00003364
Iteration 25/1000 | Loss: 0.00003360
Iteration 26/1000 | Loss: 0.00003355
Iteration 27/1000 | Loss: 0.00003354
Iteration 28/1000 | Loss: 0.00003352
Iteration 29/1000 | Loss: 0.00003352
Iteration 30/1000 | Loss: 0.00003350
Iteration 31/1000 | Loss: 0.00003350
Iteration 32/1000 | Loss: 0.00003350
Iteration 33/1000 | Loss: 0.00003349
Iteration 34/1000 | Loss: 0.00003349
Iteration 35/1000 | Loss: 0.00003349
Iteration 36/1000 | Loss: 0.00003349
Iteration 37/1000 | Loss: 0.00003349
Iteration 38/1000 | Loss: 0.00003348
Iteration 39/1000 | Loss: 0.00003348
Iteration 40/1000 | Loss: 0.00003346
Iteration 41/1000 | Loss: 0.00003346
Iteration 42/1000 | Loss: 0.00003344
Iteration 43/1000 | Loss: 0.00003343
Iteration 44/1000 | Loss: 0.00003343
Iteration 45/1000 | Loss: 0.00003343
Iteration 46/1000 | Loss: 0.00003343
Iteration 47/1000 | Loss: 0.00003343
Iteration 48/1000 | Loss: 0.00003343
Iteration 49/1000 | Loss: 0.00003343
Iteration 50/1000 | Loss: 0.00003343
Iteration 51/1000 | Loss: 0.00003343
Iteration 52/1000 | Loss: 0.00003342
Iteration 53/1000 | Loss: 0.00003342
Iteration 54/1000 | Loss: 0.00003342
Iteration 55/1000 | Loss: 0.00003342
Iteration 56/1000 | Loss: 0.00003342
Iteration 57/1000 | Loss: 0.00003341
Iteration 58/1000 | Loss: 0.00003340
Iteration 59/1000 | Loss: 0.00003340
Iteration 60/1000 | Loss: 0.00003340
Iteration 61/1000 | Loss: 0.00003340
Iteration 62/1000 | Loss: 0.00003340
Iteration 63/1000 | Loss: 0.00003340
Iteration 64/1000 | Loss: 0.00003340
Iteration 65/1000 | Loss: 0.00003340
Iteration 66/1000 | Loss: 0.00003340
Iteration 67/1000 | Loss: 0.00003340
Iteration 68/1000 | Loss: 0.00003340
Iteration 69/1000 | Loss: 0.00003339
Iteration 70/1000 | Loss: 0.00003339
Iteration 71/1000 | Loss: 0.00003339
Iteration 72/1000 | Loss: 0.00003339
Iteration 73/1000 | Loss: 0.00003339
Iteration 74/1000 | Loss: 0.00003339
Iteration 75/1000 | Loss: 0.00003339
Iteration 76/1000 | Loss: 0.00003339
Iteration 77/1000 | Loss: 0.00003339
Iteration 78/1000 | Loss: 0.00003339
Iteration 79/1000 | Loss: 0.00003338
Iteration 80/1000 | Loss: 0.00003338
Iteration 81/1000 | Loss: 0.00003338
Iteration 82/1000 | Loss: 0.00003338
Iteration 83/1000 | Loss: 0.00003338
Iteration 84/1000 | Loss: 0.00003338
Iteration 85/1000 | Loss: 0.00003337
Iteration 86/1000 | Loss: 0.00003337
Iteration 87/1000 | Loss: 0.00003337
Iteration 88/1000 | Loss: 0.00003337
Iteration 89/1000 | Loss: 0.00003337
Iteration 90/1000 | Loss: 0.00003336
Iteration 91/1000 | Loss: 0.00003336
Iteration 92/1000 | Loss: 0.00003336
Iteration 93/1000 | Loss: 0.00003336
Iteration 94/1000 | Loss: 0.00003336
Iteration 95/1000 | Loss: 0.00003336
Iteration 96/1000 | Loss: 0.00003336
Iteration 97/1000 | Loss: 0.00003336
Iteration 98/1000 | Loss: 0.00003336
Iteration 99/1000 | Loss: 0.00003336
Iteration 100/1000 | Loss: 0.00003336
Iteration 101/1000 | Loss: 0.00003336
Iteration 102/1000 | Loss: 0.00003335
Iteration 103/1000 | Loss: 0.00003335
Iteration 104/1000 | Loss: 0.00003335
Iteration 105/1000 | Loss: 0.00003334
Iteration 106/1000 | Loss: 0.00003334
Iteration 107/1000 | Loss: 0.00003334
Iteration 108/1000 | Loss: 0.00003334
Iteration 109/1000 | Loss: 0.00003333
Iteration 110/1000 | Loss: 0.00003333
Iteration 111/1000 | Loss: 0.00003333
Iteration 112/1000 | Loss: 0.00003333
Iteration 113/1000 | Loss: 0.00003333
Iteration 114/1000 | Loss: 0.00003333
Iteration 115/1000 | Loss: 0.00003333
Iteration 116/1000 | Loss: 0.00003333
Iteration 117/1000 | Loss: 0.00003332
Iteration 118/1000 | Loss: 0.00003332
Iteration 119/1000 | Loss: 0.00003332
Iteration 120/1000 | Loss: 0.00003332
Iteration 121/1000 | Loss: 0.00003332
Iteration 122/1000 | Loss: 0.00003332
Iteration 123/1000 | Loss: 0.00003332
Iteration 124/1000 | Loss: 0.00003332
Iteration 125/1000 | Loss: 0.00003332
Iteration 126/1000 | Loss: 0.00003332
Iteration 127/1000 | Loss: 0.00003332
Iteration 128/1000 | Loss: 0.00003332
Iteration 129/1000 | Loss: 0.00003332
Iteration 130/1000 | Loss: 0.00003332
Iteration 131/1000 | Loss: 0.00003332
Iteration 132/1000 | Loss: 0.00003332
Iteration 133/1000 | Loss: 0.00003332
Iteration 134/1000 | Loss: 0.00003331
Iteration 135/1000 | Loss: 0.00003331
Iteration 136/1000 | Loss: 0.00003331
Iteration 137/1000 | Loss: 0.00003331
Iteration 138/1000 | Loss: 0.00003331
Iteration 139/1000 | Loss: 0.00003331
Iteration 140/1000 | Loss: 0.00003331
Iteration 141/1000 | Loss: 0.00003331
Iteration 142/1000 | Loss: 0.00003331
Iteration 143/1000 | Loss: 0.00003331
Iteration 144/1000 | Loss: 0.00003331
Iteration 145/1000 | Loss: 0.00003331
Iteration 146/1000 | Loss: 0.00003331
Iteration 147/1000 | Loss: 0.00003331
Iteration 148/1000 | Loss: 0.00003330
Iteration 149/1000 | Loss: 0.00003330
Iteration 150/1000 | Loss: 0.00003330
Iteration 151/1000 | Loss: 0.00003330
Iteration 152/1000 | Loss: 0.00003330
Iteration 153/1000 | Loss: 0.00003330
Iteration 154/1000 | Loss: 0.00003330
Iteration 155/1000 | Loss: 0.00003330
Iteration 156/1000 | Loss: 0.00003330
Iteration 157/1000 | Loss: 0.00003330
Iteration 158/1000 | Loss: 0.00003330
Iteration 159/1000 | Loss: 0.00003330
Iteration 160/1000 | Loss: 0.00003329
Iteration 161/1000 | Loss: 0.00003329
Iteration 162/1000 | Loss: 0.00003329
Iteration 163/1000 | Loss: 0.00003329
Iteration 164/1000 | Loss: 0.00003329
Iteration 165/1000 | Loss: 0.00003329
Iteration 166/1000 | Loss: 0.00003329
Iteration 167/1000 | Loss: 0.00003329
Iteration 168/1000 | Loss: 0.00003329
Iteration 169/1000 | Loss: 0.00003329
Iteration 170/1000 | Loss: 0.00003329
Iteration 171/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.32949057337828e-05, 3.32949057337828e-05, 3.32949057337828e-05, 3.32949057337828e-05, 3.32949057337828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.32949057337828e-05

Optimization complete. Final v2v error: 4.200085163116455 mm

Highest mean error: 5.4949259757995605 mm for frame 136

Lowest mean error: 2.922538995742798 mm for frame 59

Saving results

Total time: 49.140193462371826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816109
Iteration 2/25 | Loss: 0.00194898
Iteration 3/25 | Loss: 0.00122604
Iteration 4/25 | Loss: 0.00114252
Iteration 5/25 | Loss: 0.00112467
Iteration 6/25 | Loss: 0.00111916
Iteration 7/25 | Loss: 0.00111759
Iteration 8/25 | Loss: 0.00111716
Iteration 9/25 | Loss: 0.00111708
Iteration 10/25 | Loss: 0.00111706
Iteration 11/25 | Loss: 0.00111706
Iteration 12/25 | Loss: 0.00111706
Iteration 13/25 | Loss: 0.00111706
Iteration 14/25 | Loss: 0.00111706
Iteration 15/25 | Loss: 0.00111706
Iteration 16/25 | Loss: 0.00111706
Iteration 17/25 | Loss: 0.00111706
Iteration 18/25 | Loss: 0.00111706
Iteration 19/25 | Loss: 0.00111706
Iteration 20/25 | Loss: 0.00111706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011170573998242617, 0.0011170573998242617, 0.0011170573998242617, 0.0011170573998242617, 0.0011170573998242617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011170573998242617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36303425
Iteration 2/25 | Loss: 0.00052621
Iteration 3/25 | Loss: 0.00052620
Iteration 4/25 | Loss: 0.00052620
Iteration 5/25 | Loss: 0.00052620
Iteration 6/25 | Loss: 0.00052620
Iteration 7/25 | Loss: 0.00052620
Iteration 8/25 | Loss: 0.00052620
Iteration 9/25 | Loss: 0.00052620
Iteration 10/25 | Loss: 0.00052620
Iteration 11/25 | Loss: 0.00052620
Iteration 12/25 | Loss: 0.00052620
Iteration 13/25 | Loss: 0.00052620
Iteration 14/25 | Loss: 0.00052620
Iteration 15/25 | Loss: 0.00052620
Iteration 16/25 | Loss: 0.00052620
Iteration 17/25 | Loss: 0.00052620
Iteration 18/25 | Loss: 0.00052620
Iteration 19/25 | Loss: 0.00052620
Iteration 20/25 | Loss: 0.00052620
Iteration 21/25 | Loss: 0.00052620
Iteration 22/25 | Loss: 0.00052620
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005261984770186245, 0.0005261984770186245, 0.0005261984770186245, 0.0005261984770186245, 0.0005261984770186245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005261984770186245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052620
Iteration 2/1000 | Loss: 0.00005526
Iteration 3/1000 | Loss: 0.00003223
Iteration 4/1000 | Loss: 0.00002759
Iteration 5/1000 | Loss: 0.00002537
Iteration 6/1000 | Loss: 0.00002427
Iteration 7/1000 | Loss: 0.00002375
Iteration 8/1000 | Loss: 0.00002332
Iteration 9/1000 | Loss: 0.00002308
Iteration 10/1000 | Loss: 0.00002288
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002279
Iteration 13/1000 | Loss: 0.00002279
Iteration 14/1000 | Loss: 0.00002278
Iteration 15/1000 | Loss: 0.00002271
Iteration 16/1000 | Loss: 0.00002271
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002271
Iteration 19/1000 | Loss: 0.00002271
Iteration 20/1000 | Loss: 0.00002271
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002271
Iteration 24/1000 | Loss: 0.00002271
Iteration 25/1000 | Loss: 0.00002270
Iteration 26/1000 | Loss: 0.00002270
Iteration 27/1000 | Loss: 0.00002270
Iteration 28/1000 | Loss: 0.00002270
Iteration 29/1000 | Loss: 0.00002268
Iteration 30/1000 | Loss: 0.00002268
Iteration 31/1000 | Loss: 0.00002267
Iteration 32/1000 | Loss: 0.00002267
Iteration 33/1000 | Loss: 0.00002267
Iteration 34/1000 | Loss: 0.00002265
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002259
Iteration 37/1000 | Loss: 0.00002259
Iteration 38/1000 | Loss: 0.00002259
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002255
Iteration 41/1000 | Loss: 0.00002255
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002254
Iteration 44/1000 | Loss: 0.00002254
Iteration 45/1000 | Loss: 0.00002254
Iteration 46/1000 | Loss: 0.00002253
Iteration 47/1000 | Loss: 0.00002253
Iteration 48/1000 | Loss: 0.00002253
Iteration 49/1000 | Loss: 0.00002253
Iteration 50/1000 | Loss: 0.00002252
Iteration 51/1000 | Loss: 0.00002252
Iteration 52/1000 | Loss: 0.00002252
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002252
Iteration 55/1000 | Loss: 0.00002252
Iteration 56/1000 | Loss: 0.00002252
Iteration 57/1000 | Loss: 0.00002251
Iteration 58/1000 | Loss: 0.00002249
Iteration 59/1000 | Loss: 0.00002249
Iteration 60/1000 | Loss: 0.00002249
Iteration 61/1000 | Loss: 0.00002249
Iteration 62/1000 | Loss: 0.00002249
Iteration 63/1000 | Loss: 0.00002249
Iteration 64/1000 | Loss: 0.00002249
Iteration 65/1000 | Loss: 0.00002249
Iteration 66/1000 | Loss: 0.00002249
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002248
Iteration 70/1000 | Loss: 0.00002248
Iteration 71/1000 | Loss: 0.00002246
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002246
Iteration 76/1000 | Loss: 0.00002246
Iteration 77/1000 | Loss: 0.00002246
Iteration 78/1000 | Loss: 0.00002246
Iteration 79/1000 | Loss: 0.00002246
Iteration 80/1000 | Loss: 0.00002246
Iteration 81/1000 | Loss: 0.00002245
Iteration 82/1000 | Loss: 0.00002245
Iteration 83/1000 | Loss: 0.00002245
Iteration 84/1000 | Loss: 0.00002245
Iteration 85/1000 | Loss: 0.00002245
Iteration 86/1000 | Loss: 0.00002245
Iteration 87/1000 | Loss: 0.00002245
Iteration 88/1000 | Loss: 0.00002245
Iteration 89/1000 | Loss: 0.00002245
Iteration 90/1000 | Loss: 0.00002245
Iteration 91/1000 | Loss: 0.00002245
Iteration 92/1000 | Loss: 0.00002245
Iteration 93/1000 | Loss: 0.00002245
Iteration 94/1000 | Loss: 0.00002245
Iteration 95/1000 | Loss: 0.00002245
Iteration 96/1000 | Loss: 0.00002245
Iteration 97/1000 | Loss: 0.00002245
Iteration 98/1000 | Loss: 0.00002245
Iteration 99/1000 | Loss: 0.00002245
Iteration 100/1000 | Loss: 0.00002245
Iteration 101/1000 | Loss: 0.00002245
Iteration 102/1000 | Loss: 0.00002245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.244876122858841e-05, 2.244876122858841e-05, 2.244876122858841e-05, 2.244876122858841e-05, 2.244876122858841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.244876122858841e-05

Optimization complete. Final v2v error: 4.09814977645874 mm

Highest mean error: 4.458281517028809 mm for frame 223

Lowest mean error: 3.5894837379455566 mm for frame 43

Saving results

Total time: 44.036091804504395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407303
Iteration 2/25 | Loss: 0.00090969
Iteration 3/25 | Loss: 0.00082572
Iteration 4/25 | Loss: 0.00081923
Iteration 5/25 | Loss: 0.00081779
Iteration 6/25 | Loss: 0.00081779
Iteration 7/25 | Loss: 0.00081779
Iteration 8/25 | Loss: 0.00081779
Iteration 9/25 | Loss: 0.00081779
Iteration 10/25 | Loss: 0.00081779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008177885902114213, 0.0008177885902114213, 0.0008177885902114213, 0.0008177885902114213, 0.0008177885902114213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008177885902114213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50650525
Iteration 2/25 | Loss: 0.00034780
Iteration 3/25 | Loss: 0.00034780
Iteration 4/25 | Loss: 0.00034780
Iteration 5/25 | Loss: 0.00034780
Iteration 6/25 | Loss: 0.00034780
Iteration 7/25 | Loss: 0.00034780
Iteration 8/25 | Loss: 0.00034780
Iteration 9/25 | Loss: 0.00034780
Iteration 10/25 | Loss: 0.00034780
Iteration 11/25 | Loss: 0.00034780
Iteration 12/25 | Loss: 0.00034780
Iteration 13/25 | Loss: 0.00034780
Iteration 14/25 | Loss: 0.00034780
Iteration 15/25 | Loss: 0.00034780
Iteration 16/25 | Loss: 0.00034780
Iteration 17/25 | Loss: 0.00034780
Iteration 18/25 | Loss: 0.00034780
Iteration 19/25 | Loss: 0.00034780
Iteration 20/25 | Loss: 0.00034780
Iteration 21/25 | Loss: 0.00034780
Iteration 22/25 | Loss: 0.00034780
Iteration 23/25 | Loss: 0.00034780
Iteration 24/25 | Loss: 0.00034780
Iteration 25/25 | Loss: 0.00034780

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034780
Iteration 2/1000 | Loss: 0.00001594
Iteration 3/1000 | Loss: 0.00001261
Iteration 4/1000 | Loss: 0.00001149
Iteration 5/1000 | Loss: 0.00001101
Iteration 6/1000 | Loss: 0.00001063
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001031
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00001012
Iteration 11/1000 | Loss: 0.00001006
Iteration 12/1000 | Loss: 0.00001006
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00001002
Iteration 15/1000 | Loss: 0.00001001
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000998
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000990
Iteration 22/1000 | Loss: 0.00000989
Iteration 23/1000 | Loss: 0.00000989
Iteration 24/1000 | Loss: 0.00000989
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000988
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000980
Iteration 36/1000 | Loss: 0.00000978
Iteration 37/1000 | Loss: 0.00000977
Iteration 38/1000 | Loss: 0.00000976
Iteration 39/1000 | Loss: 0.00000976
Iteration 40/1000 | Loss: 0.00000974
Iteration 41/1000 | Loss: 0.00000973
Iteration 42/1000 | Loss: 0.00000970
Iteration 43/1000 | Loss: 0.00000970
Iteration 44/1000 | Loss: 0.00000969
Iteration 45/1000 | Loss: 0.00000969
Iteration 46/1000 | Loss: 0.00000969
Iteration 47/1000 | Loss: 0.00000967
Iteration 48/1000 | Loss: 0.00000967
Iteration 49/1000 | Loss: 0.00000967
Iteration 50/1000 | Loss: 0.00000966
Iteration 51/1000 | Loss: 0.00000966
Iteration 52/1000 | Loss: 0.00000965
Iteration 53/1000 | Loss: 0.00000965
Iteration 54/1000 | Loss: 0.00000965
Iteration 55/1000 | Loss: 0.00000965
Iteration 56/1000 | Loss: 0.00000962
Iteration 57/1000 | Loss: 0.00000961
Iteration 58/1000 | Loss: 0.00000961
Iteration 59/1000 | Loss: 0.00000961
Iteration 60/1000 | Loss: 0.00000961
Iteration 61/1000 | Loss: 0.00000961
Iteration 62/1000 | Loss: 0.00000961
Iteration 63/1000 | Loss: 0.00000961
Iteration 64/1000 | Loss: 0.00000961
Iteration 65/1000 | Loss: 0.00000961
Iteration 66/1000 | Loss: 0.00000961
Iteration 67/1000 | Loss: 0.00000961
Iteration 68/1000 | Loss: 0.00000961
Iteration 69/1000 | Loss: 0.00000961
Iteration 70/1000 | Loss: 0.00000961
Iteration 71/1000 | Loss: 0.00000960
Iteration 72/1000 | Loss: 0.00000960
Iteration 73/1000 | Loss: 0.00000960
Iteration 74/1000 | Loss: 0.00000959
Iteration 75/1000 | Loss: 0.00000959
Iteration 76/1000 | Loss: 0.00000959
Iteration 77/1000 | Loss: 0.00000958
Iteration 78/1000 | Loss: 0.00000957
Iteration 79/1000 | Loss: 0.00000957
Iteration 80/1000 | Loss: 0.00000957
Iteration 81/1000 | Loss: 0.00000957
Iteration 82/1000 | Loss: 0.00000957
Iteration 83/1000 | Loss: 0.00000956
Iteration 84/1000 | Loss: 0.00000956
Iteration 85/1000 | Loss: 0.00000956
Iteration 86/1000 | Loss: 0.00000956
Iteration 87/1000 | Loss: 0.00000956
Iteration 88/1000 | Loss: 0.00000956
Iteration 89/1000 | Loss: 0.00000956
Iteration 90/1000 | Loss: 0.00000955
Iteration 91/1000 | Loss: 0.00000955
Iteration 92/1000 | Loss: 0.00000955
Iteration 93/1000 | Loss: 0.00000955
Iteration 94/1000 | Loss: 0.00000955
Iteration 95/1000 | Loss: 0.00000955
Iteration 96/1000 | Loss: 0.00000955
Iteration 97/1000 | Loss: 0.00000955
Iteration 98/1000 | Loss: 0.00000955
Iteration 99/1000 | Loss: 0.00000954
Iteration 100/1000 | Loss: 0.00000954
Iteration 101/1000 | Loss: 0.00000953
Iteration 102/1000 | Loss: 0.00000953
Iteration 103/1000 | Loss: 0.00000953
Iteration 104/1000 | Loss: 0.00000953
Iteration 105/1000 | Loss: 0.00000952
Iteration 106/1000 | Loss: 0.00000952
Iteration 107/1000 | Loss: 0.00000952
Iteration 108/1000 | Loss: 0.00000952
Iteration 109/1000 | Loss: 0.00000952
Iteration 110/1000 | Loss: 0.00000951
Iteration 111/1000 | Loss: 0.00000951
Iteration 112/1000 | Loss: 0.00000951
Iteration 113/1000 | Loss: 0.00000951
Iteration 114/1000 | Loss: 0.00000951
Iteration 115/1000 | Loss: 0.00000951
Iteration 116/1000 | Loss: 0.00000950
Iteration 117/1000 | Loss: 0.00000950
Iteration 118/1000 | Loss: 0.00000950
Iteration 119/1000 | Loss: 0.00000950
Iteration 120/1000 | Loss: 0.00000950
Iteration 121/1000 | Loss: 0.00000949
Iteration 122/1000 | Loss: 0.00000949
Iteration 123/1000 | Loss: 0.00000949
Iteration 124/1000 | Loss: 0.00000949
Iteration 125/1000 | Loss: 0.00000949
Iteration 126/1000 | Loss: 0.00000949
Iteration 127/1000 | Loss: 0.00000949
Iteration 128/1000 | Loss: 0.00000949
Iteration 129/1000 | Loss: 0.00000949
Iteration 130/1000 | Loss: 0.00000949
Iteration 131/1000 | Loss: 0.00000949
Iteration 132/1000 | Loss: 0.00000949
Iteration 133/1000 | Loss: 0.00000949
Iteration 134/1000 | Loss: 0.00000949
Iteration 135/1000 | Loss: 0.00000949
Iteration 136/1000 | Loss: 0.00000949
Iteration 137/1000 | Loss: 0.00000949
Iteration 138/1000 | Loss: 0.00000949
Iteration 139/1000 | Loss: 0.00000949
Iteration 140/1000 | Loss: 0.00000949
Iteration 141/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [9.488975592830684e-06, 9.488975592830684e-06, 9.488975592830684e-06, 9.488975592830684e-06, 9.488975592830684e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.488975592830684e-06

Optimization complete. Final v2v error: 2.6474597454071045 mm

Highest mean error: 2.8814218044281006 mm for frame 71

Lowest mean error: 2.4565486907958984 mm for frame 0

Saving results

Total time: 36.542142152786255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520471
Iteration 2/25 | Loss: 0.00119534
Iteration 3/25 | Loss: 0.00093190
Iteration 4/25 | Loss: 0.00090470
Iteration 5/25 | Loss: 0.00089989
Iteration 6/25 | Loss: 0.00089857
Iteration 7/25 | Loss: 0.00089839
Iteration 8/25 | Loss: 0.00089839
Iteration 9/25 | Loss: 0.00089839
Iteration 10/25 | Loss: 0.00089839
Iteration 11/25 | Loss: 0.00089839
Iteration 12/25 | Loss: 0.00089839
Iteration 13/25 | Loss: 0.00089839
Iteration 14/25 | Loss: 0.00089839
Iteration 15/25 | Loss: 0.00089839
Iteration 16/25 | Loss: 0.00089839
Iteration 17/25 | Loss: 0.00089839
Iteration 18/25 | Loss: 0.00089839
Iteration 19/25 | Loss: 0.00089839
Iteration 20/25 | Loss: 0.00089839
Iteration 21/25 | Loss: 0.00089839
Iteration 22/25 | Loss: 0.00089839
Iteration 23/25 | Loss: 0.00089839
Iteration 24/25 | Loss: 0.00089839
Iteration 25/25 | Loss: 0.00089839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37590075
Iteration 2/25 | Loss: 0.00035854
Iteration 3/25 | Loss: 0.00035852
Iteration 4/25 | Loss: 0.00035852
Iteration 5/25 | Loss: 0.00035852
Iteration 6/25 | Loss: 0.00035852
Iteration 7/25 | Loss: 0.00035852
Iteration 8/25 | Loss: 0.00035852
Iteration 9/25 | Loss: 0.00035852
Iteration 10/25 | Loss: 0.00035852
Iteration 11/25 | Loss: 0.00035852
Iteration 12/25 | Loss: 0.00035852
Iteration 13/25 | Loss: 0.00035852
Iteration 14/25 | Loss: 0.00035852
Iteration 15/25 | Loss: 0.00035852
Iteration 16/25 | Loss: 0.00035852
Iteration 17/25 | Loss: 0.00035852
Iteration 18/25 | Loss: 0.00035852
Iteration 19/25 | Loss: 0.00035852
Iteration 20/25 | Loss: 0.00035852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003585200756788254, 0.0003585200756788254, 0.0003585200756788254, 0.0003585200756788254, 0.0003585200756788254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003585200756788254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035852
Iteration 2/1000 | Loss: 0.00003082
Iteration 3/1000 | Loss: 0.00001978
Iteration 4/1000 | Loss: 0.00001607
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001305
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001292
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001276
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001274
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001272
Iteration 44/1000 | Loss: 0.00001271
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001271
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001270
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001270
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001269
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001263
Iteration 127/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.263204831047915e-05, 1.263204831047915e-05, 1.263204831047915e-05, 1.263204831047915e-05, 1.263204831047915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.263204831047915e-05

Optimization complete. Final v2v error: 2.966431140899658 mm

Highest mean error: 3.503241539001465 mm for frame 41

Lowest mean error: 2.4715821743011475 mm for frame 108

Saving results

Total time: 32.76572394371033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725593
Iteration 2/25 | Loss: 0.00128756
Iteration 3/25 | Loss: 0.00108356
Iteration 4/25 | Loss: 0.00104612
Iteration 5/25 | Loss: 0.00103514
Iteration 6/25 | Loss: 0.00103242
Iteration 7/25 | Loss: 0.00103155
Iteration 8/25 | Loss: 0.00103137
Iteration 9/25 | Loss: 0.00103137
Iteration 10/25 | Loss: 0.00103137
Iteration 11/25 | Loss: 0.00103137
Iteration 12/25 | Loss: 0.00103137
Iteration 13/25 | Loss: 0.00103137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010313688544556499, 0.0010313688544556499, 0.0010313688544556499, 0.0010313688544556499, 0.0010313688544556499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010313688544556499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24159777
Iteration 2/25 | Loss: 0.00054361
Iteration 3/25 | Loss: 0.00054347
Iteration 4/25 | Loss: 0.00054347
Iteration 5/25 | Loss: 0.00054347
Iteration 6/25 | Loss: 0.00054347
Iteration 7/25 | Loss: 0.00054347
Iteration 8/25 | Loss: 0.00054347
Iteration 9/25 | Loss: 0.00054347
Iteration 10/25 | Loss: 0.00054347
Iteration 11/25 | Loss: 0.00054347
Iteration 12/25 | Loss: 0.00054347
Iteration 13/25 | Loss: 0.00054347
Iteration 14/25 | Loss: 0.00054347
Iteration 15/25 | Loss: 0.00054347
Iteration 16/25 | Loss: 0.00054347
Iteration 17/25 | Loss: 0.00054347
Iteration 18/25 | Loss: 0.00054347
Iteration 19/25 | Loss: 0.00054347
Iteration 20/25 | Loss: 0.00054347
Iteration 21/25 | Loss: 0.00054347
Iteration 22/25 | Loss: 0.00054347
Iteration 23/25 | Loss: 0.00054347
Iteration 24/25 | Loss: 0.00054347
Iteration 25/25 | Loss: 0.00054347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054347
Iteration 2/1000 | Loss: 0.00007843
Iteration 3/1000 | Loss: 0.00004583
Iteration 4/1000 | Loss: 0.00003610
Iteration 5/1000 | Loss: 0.00003311
Iteration 6/1000 | Loss: 0.00003200
Iteration 7/1000 | Loss: 0.00003126
Iteration 8/1000 | Loss: 0.00003071
Iteration 9/1000 | Loss: 0.00003016
Iteration 10/1000 | Loss: 0.00002978
Iteration 11/1000 | Loss: 0.00002941
Iteration 12/1000 | Loss: 0.00002922
Iteration 13/1000 | Loss: 0.00002892
Iteration 14/1000 | Loss: 0.00002869
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00002852
Iteration 17/1000 | Loss: 0.00002850
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002848
Iteration 20/1000 | Loss: 0.00002847
Iteration 21/1000 | Loss: 0.00002847
Iteration 22/1000 | Loss: 0.00002846
Iteration 23/1000 | Loss: 0.00002846
Iteration 24/1000 | Loss: 0.00002842
Iteration 25/1000 | Loss: 0.00002841
Iteration 26/1000 | Loss: 0.00002838
Iteration 27/1000 | Loss: 0.00002836
Iteration 28/1000 | Loss: 0.00002836
Iteration 29/1000 | Loss: 0.00002835
Iteration 30/1000 | Loss: 0.00002835
Iteration 31/1000 | Loss: 0.00002834
Iteration 32/1000 | Loss: 0.00002834
Iteration 33/1000 | Loss: 0.00002834
Iteration 34/1000 | Loss: 0.00002833
Iteration 35/1000 | Loss: 0.00002833
Iteration 36/1000 | Loss: 0.00002833
Iteration 37/1000 | Loss: 0.00002832
Iteration 38/1000 | Loss: 0.00002832
Iteration 39/1000 | Loss: 0.00002832
Iteration 40/1000 | Loss: 0.00002831
Iteration 41/1000 | Loss: 0.00002831
Iteration 42/1000 | Loss: 0.00002831
Iteration 43/1000 | Loss: 0.00002831
Iteration 44/1000 | Loss: 0.00002831
Iteration 45/1000 | Loss: 0.00002830
Iteration 46/1000 | Loss: 0.00002830
Iteration 47/1000 | Loss: 0.00002830
Iteration 48/1000 | Loss: 0.00002829
Iteration 49/1000 | Loss: 0.00002829
Iteration 50/1000 | Loss: 0.00002829
Iteration 51/1000 | Loss: 0.00002829
Iteration 52/1000 | Loss: 0.00002828
Iteration 53/1000 | Loss: 0.00002828
Iteration 54/1000 | Loss: 0.00002828
Iteration 55/1000 | Loss: 0.00002827
Iteration 56/1000 | Loss: 0.00002827
Iteration 57/1000 | Loss: 0.00002827
Iteration 58/1000 | Loss: 0.00002826
Iteration 59/1000 | Loss: 0.00002826
Iteration 60/1000 | Loss: 0.00002826
Iteration 61/1000 | Loss: 0.00002825
Iteration 62/1000 | Loss: 0.00002825
Iteration 63/1000 | Loss: 0.00002825
Iteration 64/1000 | Loss: 0.00002824
Iteration 65/1000 | Loss: 0.00002824
Iteration 66/1000 | Loss: 0.00002823
Iteration 67/1000 | Loss: 0.00002823
Iteration 68/1000 | Loss: 0.00002822
Iteration 69/1000 | Loss: 0.00002822
Iteration 70/1000 | Loss: 0.00002822
Iteration 71/1000 | Loss: 0.00002822
Iteration 72/1000 | Loss: 0.00002821
Iteration 73/1000 | Loss: 0.00002821
Iteration 74/1000 | Loss: 0.00002821
Iteration 75/1000 | Loss: 0.00002821
Iteration 76/1000 | Loss: 0.00002821
Iteration 77/1000 | Loss: 0.00002821
Iteration 78/1000 | Loss: 0.00002821
Iteration 79/1000 | Loss: 0.00002820
Iteration 80/1000 | Loss: 0.00002820
Iteration 81/1000 | Loss: 0.00002820
Iteration 82/1000 | Loss: 0.00002819
Iteration 83/1000 | Loss: 0.00002819
Iteration 84/1000 | Loss: 0.00002819
Iteration 85/1000 | Loss: 0.00002819
Iteration 86/1000 | Loss: 0.00002819
Iteration 87/1000 | Loss: 0.00002819
Iteration 88/1000 | Loss: 0.00002819
Iteration 89/1000 | Loss: 0.00002819
Iteration 90/1000 | Loss: 0.00002819
Iteration 91/1000 | Loss: 0.00002818
Iteration 92/1000 | Loss: 0.00002818
Iteration 93/1000 | Loss: 0.00002818
Iteration 94/1000 | Loss: 0.00002817
Iteration 95/1000 | Loss: 0.00002817
Iteration 96/1000 | Loss: 0.00002817
Iteration 97/1000 | Loss: 0.00002817
Iteration 98/1000 | Loss: 0.00002817
Iteration 99/1000 | Loss: 0.00002816
Iteration 100/1000 | Loss: 0.00002816
Iteration 101/1000 | Loss: 0.00002816
Iteration 102/1000 | Loss: 0.00002816
Iteration 103/1000 | Loss: 0.00002815
Iteration 104/1000 | Loss: 0.00002815
Iteration 105/1000 | Loss: 0.00002815
Iteration 106/1000 | Loss: 0.00002815
Iteration 107/1000 | Loss: 0.00002814
Iteration 108/1000 | Loss: 0.00002814
Iteration 109/1000 | Loss: 0.00002814
Iteration 110/1000 | Loss: 0.00002813
Iteration 111/1000 | Loss: 0.00002813
Iteration 112/1000 | Loss: 0.00002813
Iteration 113/1000 | Loss: 0.00002813
Iteration 114/1000 | Loss: 0.00002813
Iteration 115/1000 | Loss: 0.00002813
Iteration 116/1000 | Loss: 0.00002813
Iteration 117/1000 | Loss: 0.00002813
Iteration 118/1000 | Loss: 0.00002813
Iteration 119/1000 | Loss: 0.00002813
Iteration 120/1000 | Loss: 0.00002813
Iteration 121/1000 | Loss: 0.00002813
Iteration 122/1000 | Loss: 0.00002813
Iteration 123/1000 | Loss: 0.00002813
Iteration 124/1000 | Loss: 0.00002813
Iteration 125/1000 | Loss: 0.00002813
Iteration 126/1000 | Loss: 0.00002813
Iteration 127/1000 | Loss: 0.00002813
Iteration 128/1000 | Loss: 0.00002813
Iteration 129/1000 | Loss: 0.00002812
Iteration 130/1000 | Loss: 0.00002812
Iteration 131/1000 | Loss: 0.00002812
Iteration 132/1000 | Loss: 0.00002812
Iteration 133/1000 | Loss: 0.00002812
Iteration 134/1000 | Loss: 0.00002812
Iteration 135/1000 | Loss: 0.00002812
Iteration 136/1000 | Loss: 0.00002812
Iteration 137/1000 | Loss: 0.00002812
Iteration 138/1000 | Loss: 0.00002812
Iteration 139/1000 | Loss: 0.00002812
Iteration 140/1000 | Loss: 0.00002812
Iteration 141/1000 | Loss: 0.00002812
Iteration 142/1000 | Loss: 0.00002812
Iteration 143/1000 | Loss: 0.00002812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.8124413802288473e-05, 2.8124413802288473e-05, 2.8124413802288473e-05, 2.8124413802288473e-05, 2.8124413802288473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8124413802288473e-05

Optimization complete. Final v2v error: 4.274687767028809 mm

Highest mean error: 5.188353538513184 mm for frame 108

Lowest mean error: 3.132967472076416 mm for frame 16

Saving results

Total time: 42.177907943725586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990802
Iteration 2/25 | Loss: 0.00279508
Iteration 3/25 | Loss: 0.00194644
Iteration 4/25 | Loss: 0.00153089
Iteration 5/25 | Loss: 0.00133877
Iteration 6/25 | Loss: 0.00108028
Iteration 7/25 | Loss: 0.00101709
Iteration 8/25 | Loss: 0.00102104
Iteration 9/25 | Loss: 0.00097728
Iteration 10/25 | Loss: 0.00095585
Iteration 11/25 | Loss: 0.00095048
Iteration 12/25 | Loss: 0.00094543
Iteration 13/25 | Loss: 0.00094180
Iteration 14/25 | Loss: 0.00094031
Iteration 15/25 | Loss: 0.00093993
Iteration 16/25 | Loss: 0.00093983
Iteration 17/25 | Loss: 0.00093983
Iteration 18/25 | Loss: 0.00093983
Iteration 19/25 | Loss: 0.00093983
Iteration 20/25 | Loss: 0.00093983
Iteration 21/25 | Loss: 0.00093983
Iteration 22/25 | Loss: 0.00093983
Iteration 23/25 | Loss: 0.00093982
Iteration 24/25 | Loss: 0.00093982
Iteration 25/25 | Loss: 0.00093982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39649999
Iteration 2/25 | Loss: 0.00026397
Iteration 3/25 | Loss: 0.00026396
Iteration 4/25 | Loss: 0.00026396
Iteration 5/25 | Loss: 0.00026396
Iteration 6/25 | Loss: 0.00026396
Iteration 7/25 | Loss: 0.00026396
Iteration 8/25 | Loss: 0.00026396
Iteration 9/25 | Loss: 0.00026396
Iteration 10/25 | Loss: 0.00026396
Iteration 11/25 | Loss: 0.00026396
Iteration 12/25 | Loss: 0.00026396
Iteration 13/25 | Loss: 0.00026396
Iteration 14/25 | Loss: 0.00026396
Iteration 15/25 | Loss: 0.00026396
Iteration 16/25 | Loss: 0.00026396
Iteration 17/25 | Loss: 0.00026396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00026395893655717373, 0.00026395893655717373, 0.00026395893655717373, 0.00026395893655717373, 0.00026395893655717373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026395893655717373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026396
Iteration 2/1000 | Loss: 0.00003705
Iteration 3/1000 | Loss: 0.00002736
Iteration 4/1000 | Loss: 0.00002513
Iteration 5/1000 | Loss: 0.00002330
Iteration 6/1000 | Loss: 0.00002255
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002163
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002120
Iteration 11/1000 | Loss: 0.00028213
Iteration 12/1000 | Loss: 0.00002475
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00001946
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001627
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001563
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001530
Iteration 23/1000 | Loss: 0.00001530
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001508
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001507
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001506
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001506
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001505
Iteration 73/1000 | Loss: 0.00001505
Iteration 74/1000 | Loss: 0.00001505
Iteration 75/1000 | Loss: 0.00001505
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.5048562090669293e-05, 1.5048562090669293e-05, 1.5048562090669293e-05, 1.5048562090669293e-05, 1.5048562090669293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5048562090669293e-05

Optimization complete. Final v2v error: 3.300112247467041 mm

Highest mean error: 3.6428160667419434 mm for frame 30

Lowest mean error: 3.052388906478882 mm for frame 19

Saving results

Total time: 59.175790548324585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765681
Iteration 2/25 | Loss: 0.00148133
Iteration 3/25 | Loss: 0.00111356
Iteration 4/25 | Loss: 0.00090748
Iteration 5/25 | Loss: 0.00086094
Iteration 6/25 | Loss: 0.00085149
Iteration 7/25 | Loss: 0.00084094
Iteration 8/25 | Loss: 0.00083869
Iteration 9/25 | Loss: 0.00083797
Iteration 10/25 | Loss: 0.00083730
Iteration 11/25 | Loss: 0.00083662
Iteration 12/25 | Loss: 0.00083635
Iteration 13/25 | Loss: 0.00083617
Iteration 14/25 | Loss: 0.00083609
Iteration 15/25 | Loss: 0.00083608
Iteration 16/25 | Loss: 0.00083608
Iteration 17/25 | Loss: 0.00083607
Iteration 18/25 | Loss: 0.00083607
Iteration 19/25 | Loss: 0.00083607
Iteration 20/25 | Loss: 0.00083607
Iteration 21/25 | Loss: 0.00083607
Iteration 22/25 | Loss: 0.00083607
Iteration 23/25 | Loss: 0.00083607
Iteration 24/25 | Loss: 0.00083607
Iteration 25/25 | Loss: 0.00083607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89740300
Iteration 2/25 | Loss: 0.00059969
Iteration 3/25 | Loss: 0.00059969
Iteration 4/25 | Loss: 0.00059969
Iteration 5/25 | Loss: 0.00059968
Iteration 6/25 | Loss: 0.00059968
Iteration 7/25 | Loss: 0.00059968
Iteration 8/25 | Loss: 0.00059968
Iteration 9/25 | Loss: 0.00059968
Iteration 10/25 | Loss: 0.00059968
Iteration 11/25 | Loss: 0.00059968
Iteration 12/25 | Loss: 0.00059968
Iteration 13/25 | Loss: 0.00059968
Iteration 14/25 | Loss: 0.00059968
Iteration 15/25 | Loss: 0.00059968
Iteration 16/25 | Loss: 0.00059968
Iteration 17/25 | Loss: 0.00059968
Iteration 18/25 | Loss: 0.00059968
Iteration 19/25 | Loss: 0.00059968
Iteration 20/25 | Loss: 0.00059968
Iteration 21/25 | Loss: 0.00059968
Iteration 22/25 | Loss: 0.00059968
Iteration 23/25 | Loss: 0.00059968
Iteration 24/25 | Loss: 0.00059968
Iteration 25/25 | Loss: 0.00059968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059968
Iteration 2/1000 | Loss: 0.00001951
Iteration 3/1000 | Loss: 0.00001173
Iteration 4/1000 | Loss: 0.00001079
Iteration 5/1000 | Loss: 0.00001047
Iteration 6/1000 | Loss: 0.00048484
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001012
Iteration 9/1000 | Loss: 0.00001001
Iteration 10/1000 | Loss: 0.00001001
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000977
Iteration 13/1000 | Loss: 0.00000976
Iteration 14/1000 | Loss: 0.00000976
Iteration 15/1000 | Loss: 0.00000975
Iteration 16/1000 | Loss: 0.00000971
Iteration 17/1000 | Loss: 0.00000971
Iteration 18/1000 | Loss: 0.00000970
Iteration 19/1000 | Loss: 0.00000969
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000969
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000967
Iteration 24/1000 | Loss: 0.00000967
Iteration 25/1000 | Loss: 0.00000967
Iteration 26/1000 | Loss: 0.00000966
Iteration 27/1000 | Loss: 0.00000966
Iteration 28/1000 | Loss: 0.00000965
Iteration 29/1000 | Loss: 0.00000965
Iteration 30/1000 | Loss: 0.00000964
Iteration 31/1000 | Loss: 0.00000963
Iteration 32/1000 | Loss: 0.00000963
Iteration 33/1000 | Loss: 0.00000962
Iteration 34/1000 | Loss: 0.00000962
Iteration 35/1000 | Loss: 0.00000962
Iteration 36/1000 | Loss: 0.00000961
Iteration 37/1000 | Loss: 0.00000961
Iteration 38/1000 | Loss: 0.00000959
Iteration 39/1000 | Loss: 0.00000958
Iteration 40/1000 | Loss: 0.00000955
Iteration 41/1000 | Loss: 0.00000955
Iteration 42/1000 | Loss: 0.00000954
Iteration 43/1000 | Loss: 0.00000954
Iteration 44/1000 | Loss: 0.00000952
Iteration 45/1000 | Loss: 0.00000951
Iteration 46/1000 | Loss: 0.00000950
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000950
Iteration 49/1000 | Loss: 0.00000950
Iteration 50/1000 | Loss: 0.00000949
Iteration 51/1000 | Loss: 0.00000949
Iteration 52/1000 | Loss: 0.00000949
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000948
Iteration 55/1000 | Loss: 0.00000948
Iteration 56/1000 | Loss: 0.00000948
Iteration 57/1000 | Loss: 0.00000948
Iteration 58/1000 | Loss: 0.00000947
Iteration 59/1000 | Loss: 0.00000947
Iteration 60/1000 | Loss: 0.00000946
Iteration 61/1000 | Loss: 0.00000946
Iteration 62/1000 | Loss: 0.00000946
Iteration 63/1000 | Loss: 0.00000946
Iteration 64/1000 | Loss: 0.00000946
Iteration 65/1000 | Loss: 0.00000946
Iteration 66/1000 | Loss: 0.00000946
Iteration 67/1000 | Loss: 0.00000946
Iteration 68/1000 | Loss: 0.00000946
Iteration 69/1000 | Loss: 0.00000945
Iteration 70/1000 | Loss: 0.00000945
Iteration 71/1000 | Loss: 0.00000945
Iteration 72/1000 | Loss: 0.00000945
Iteration 73/1000 | Loss: 0.00000945
Iteration 74/1000 | Loss: 0.00000945
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000945
Iteration 77/1000 | Loss: 0.00000945
Iteration 78/1000 | Loss: 0.00000945
Iteration 79/1000 | Loss: 0.00000945
Iteration 80/1000 | Loss: 0.00000945
Iteration 81/1000 | Loss: 0.00000945
Iteration 82/1000 | Loss: 0.00000944
Iteration 83/1000 | Loss: 0.00000944
Iteration 84/1000 | Loss: 0.00000944
Iteration 85/1000 | Loss: 0.00000943
Iteration 86/1000 | Loss: 0.00000943
Iteration 87/1000 | Loss: 0.00000943
Iteration 88/1000 | Loss: 0.00000943
Iteration 89/1000 | Loss: 0.00000942
Iteration 90/1000 | Loss: 0.00000942
Iteration 91/1000 | Loss: 0.00000942
Iteration 92/1000 | Loss: 0.00000942
Iteration 93/1000 | Loss: 0.00000942
Iteration 94/1000 | Loss: 0.00000942
Iteration 95/1000 | Loss: 0.00000942
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000942
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000941
Iteration 101/1000 | Loss: 0.00000941
Iteration 102/1000 | Loss: 0.00000941
Iteration 103/1000 | Loss: 0.00000941
Iteration 104/1000 | Loss: 0.00000941
Iteration 105/1000 | Loss: 0.00000941
Iteration 106/1000 | Loss: 0.00000940
Iteration 107/1000 | Loss: 0.00000940
Iteration 108/1000 | Loss: 0.00000940
Iteration 109/1000 | Loss: 0.00000940
Iteration 110/1000 | Loss: 0.00000940
Iteration 111/1000 | Loss: 0.00000940
Iteration 112/1000 | Loss: 0.00000940
Iteration 113/1000 | Loss: 0.00000939
Iteration 114/1000 | Loss: 0.00000939
Iteration 115/1000 | Loss: 0.00000939
Iteration 116/1000 | Loss: 0.00000939
Iteration 117/1000 | Loss: 0.00000939
Iteration 118/1000 | Loss: 0.00000939
Iteration 119/1000 | Loss: 0.00000939
Iteration 120/1000 | Loss: 0.00000939
Iteration 121/1000 | Loss: 0.00000939
Iteration 122/1000 | Loss: 0.00000939
Iteration 123/1000 | Loss: 0.00000939
Iteration 124/1000 | Loss: 0.00000939
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000939
Iteration 132/1000 | Loss: 0.00000938
Iteration 133/1000 | Loss: 0.00000938
Iteration 134/1000 | Loss: 0.00000938
Iteration 135/1000 | Loss: 0.00000938
Iteration 136/1000 | Loss: 0.00000938
Iteration 137/1000 | Loss: 0.00000938
Iteration 138/1000 | Loss: 0.00000938
Iteration 139/1000 | Loss: 0.00000938
Iteration 140/1000 | Loss: 0.00000938
Iteration 141/1000 | Loss: 0.00000938
Iteration 142/1000 | Loss: 0.00000938
Iteration 143/1000 | Loss: 0.00000938
Iteration 144/1000 | Loss: 0.00000938
Iteration 145/1000 | Loss: 0.00000938
Iteration 146/1000 | Loss: 0.00000938
Iteration 147/1000 | Loss: 0.00000938
Iteration 148/1000 | Loss: 0.00000938
Iteration 149/1000 | Loss: 0.00000938
Iteration 150/1000 | Loss: 0.00000938
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000937
Iteration 153/1000 | Loss: 0.00000937
Iteration 154/1000 | Loss: 0.00000937
Iteration 155/1000 | Loss: 0.00000937
Iteration 156/1000 | Loss: 0.00000937
Iteration 157/1000 | Loss: 0.00000937
Iteration 158/1000 | Loss: 0.00000937
Iteration 159/1000 | Loss: 0.00000937
Iteration 160/1000 | Loss: 0.00000937
Iteration 161/1000 | Loss: 0.00000937
Iteration 162/1000 | Loss: 0.00000937
Iteration 163/1000 | Loss: 0.00000936
Iteration 164/1000 | Loss: 0.00000936
Iteration 165/1000 | Loss: 0.00000936
Iteration 166/1000 | Loss: 0.00000936
Iteration 167/1000 | Loss: 0.00000936
Iteration 168/1000 | Loss: 0.00000936
Iteration 169/1000 | Loss: 0.00000936
Iteration 170/1000 | Loss: 0.00000936
Iteration 171/1000 | Loss: 0.00000935
Iteration 172/1000 | Loss: 0.00000935
Iteration 173/1000 | Loss: 0.00000935
Iteration 174/1000 | Loss: 0.00000935
Iteration 175/1000 | Loss: 0.00000935
Iteration 176/1000 | Loss: 0.00000935
Iteration 177/1000 | Loss: 0.00000935
Iteration 178/1000 | Loss: 0.00000935
Iteration 179/1000 | Loss: 0.00000935
Iteration 180/1000 | Loss: 0.00000935
Iteration 181/1000 | Loss: 0.00000934
Iteration 182/1000 | Loss: 0.00000934
Iteration 183/1000 | Loss: 0.00000934
Iteration 184/1000 | Loss: 0.00000934
Iteration 185/1000 | Loss: 0.00000934
Iteration 186/1000 | Loss: 0.00000934
Iteration 187/1000 | Loss: 0.00000934
Iteration 188/1000 | Loss: 0.00000934
Iteration 189/1000 | Loss: 0.00000934
Iteration 190/1000 | Loss: 0.00000934
Iteration 191/1000 | Loss: 0.00000933
Iteration 192/1000 | Loss: 0.00000933
Iteration 193/1000 | Loss: 0.00000933
Iteration 194/1000 | Loss: 0.00000933
Iteration 195/1000 | Loss: 0.00000933
Iteration 196/1000 | Loss: 0.00000933
Iteration 197/1000 | Loss: 0.00000933
Iteration 198/1000 | Loss: 0.00000933
Iteration 199/1000 | Loss: 0.00000933
Iteration 200/1000 | Loss: 0.00000933
Iteration 201/1000 | Loss: 0.00000933
Iteration 202/1000 | Loss: 0.00000933
Iteration 203/1000 | Loss: 0.00000933
Iteration 204/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [9.326913641416468e-06, 9.326913641416468e-06, 9.326913641416468e-06, 9.326913641416468e-06, 9.326913641416468e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.326913641416468e-06

Optimization complete. Final v2v error: 2.5955440998077393 mm

Highest mean error: 3.0184032917022705 mm for frame 64

Lowest mean error: 2.2314646244049072 mm for frame 7

Saving results

Total time: 55.296976804733276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00655004
Iteration 2/25 | Loss: 0.00113830
Iteration 3/25 | Loss: 0.00095173
Iteration 4/25 | Loss: 0.00093264
Iteration 5/25 | Loss: 0.00092791
Iteration 6/25 | Loss: 0.00092694
Iteration 7/25 | Loss: 0.00092694
Iteration 8/25 | Loss: 0.00092694
Iteration 9/25 | Loss: 0.00092694
Iteration 10/25 | Loss: 0.00092694
Iteration 11/25 | Loss: 0.00092694
Iteration 12/25 | Loss: 0.00092694
Iteration 13/25 | Loss: 0.00092694
Iteration 14/25 | Loss: 0.00092694
Iteration 15/25 | Loss: 0.00092694
Iteration 16/25 | Loss: 0.00092694
Iteration 17/25 | Loss: 0.00092694
Iteration 18/25 | Loss: 0.00092694
Iteration 19/25 | Loss: 0.00092694
Iteration 20/25 | Loss: 0.00092694
Iteration 21/25 | Loss: 0.00092694
Iteration 22/25 | Loss: 0.00092694
Iteration 23/25 | Loss: 0.00092694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009269375004805624, 0.0009269375004805624, 0.0009269375004805624, 0.0009269375004805624, 0.0009269375004805624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009269375004805624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11710608
Iteration 2/25 | Loss: 0.00040607
Iteration 3/25 | Loss: 0.00040604
Iteration 4/25 | Loss: 0.00040604
Iteration 5/25 | Loss: 0.00040604
Iteration 6/25 | Loss: 0.00040604
Iteration 7/25 | Loss: 0.00040604
Iteration 8/25 | Loss: 0.00040604
Iteration 9/25 | Loss: 0.00040604
Iteration 10/25 | Loss: 0.00040604
Iteration 11/25 | Loss: 0.00040604
Iteration 12/25 | Loss: 0.00040604
Iteration 13/25 | Loss: 0.00040604
Iteration 14/25 | Loss: 0.00040604
Iteration 15/25 | Loss: 0.00040604
Iteration 16/25 | Loss: 0.00040604
Iteration 17/25 | Loss: 0.00040604
Iteration 18/25 | Loss: 0.00040604
Iteration 19/25 | Loss: 0.00040604
Iteration 20/25 | Loss: 0.00040604
Iteration 21/25 | Loss: 0.00040604
Iteration 22/25 | Loss: 0.00040604
Iteration 23/25 | Loss: 0.00040604
Iteration 24/25 | Loss: 0.00040604
Iteration 25/25 | Loss: 0.00040604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040604
Iteration 2/1000 | Loss: 0.00003028
Iteration 3/1000 | Loss: 0.00002136
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001749
Iteration 7/1000 | Loss: 0.00001705
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001639
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001612
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001597
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001593
Iteration 22/1000 | Loss: 0.00001593
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001592
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001590
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001589
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001588
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001587
Iteration 43/1000 | Loss: 0.00001587
Iteration 44/1000 | Loss: 0.00001587
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001587
Iteration 48/1000 | Loss: 0.00001587
Iteration 49/1000 | Loss: 0.00001587
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001587
Iteration 52/1000 | Loss: 0.00001586
Iteration 53/1000 | Loss: 0.00001586
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001583
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001582
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001581
Iteration 76/1000 | Loss: 0.00001581
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001580
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001579
Iteration 88/1000 | Loss: 0.00001579
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001579
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001579
Iteration 116/1000 | Loss: 0.00001579
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001579
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001579
Iteration 125/1000 | Loss: 0.00001579
Iteration 126/1000 | Loss: 0.00001579
Iteration 127/1000 | Loss: 0.00001579
Iteration 128/1000 | Loss: 0.00001579
Iteration 129/1000 | Loss: 0.00001579
Iteration 130/1000 | Loss: 0.00001579
Iteration 131/1000 | Loss: 0.00001579
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.579165291332174e-05, 1.579165291332174e-05, 1.579165291332174e-05, 1.579165291332174e-05, 1.579165291332174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.579165291332174e-05

Optimization complete. Final v2v error: 3.3187546730041504 mm

Highest mean error: 3.9151785373687744 mm for frame 85

Lowest mean error: 2.817594051361084 mm for frame 9

Saving results

Total time: 37.14612126350403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872359
Iteration 2/25 | Loss: 0.00097063
Iteration 3/25 | Loss: 0.00084849
Iteration 4/25 | Loss: 0.00083482
Iteration 5/25 | Loss: 0.00083108
Iteration 6/25 | Loss: 0.00083014
Iteration 7/25 | Loss: 0.00083006
Iteration 8/25 | Loss: 0.00083006
Iteration 9/25 | Loss: 0.00083006
Iteration 10/25 | Loss: 0.00083006
Iteration 11/25 | Loss: 0.00083006
Iteration 12/25 | Loss: 0.00083006
Iteration 13/25 | Loss: 0.00083006
Iteration 14/25 | Loss: 0.00083006
Iteration 15/25 | Loss: 0.00083006
Iteration 16/25 | Loss: 0.00083006
Iteration 17/25 | Loss: 0.00083006
Iteration 18/25 | Loss: 0.00083006
Iteration 19/25 | Loss: 0.00083006
Iteration 20/25 | Loss: 0.00083006
Iteration 21/25 | Loss: 0.00083006
Iteration 22/25 | Loss: 0.00083006
Iteration 23/25 | Loss: 0.00083006
Iteration 24/25 | Loss: 0.00083006
Iteration 25/25 | Loss: 0.00083006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49301386
Iteration 2/25 | Loss: 0.00051047
Iteration 3/25 | Loss: 0.00051047
Iteration 4/25 | Loss: 0.00051047
Iteration 5/25 | Loss: 0.00051047
Iteration 6/25 | Loss: 0.00051047
Iteration 7/25 | Loss: 0.00051047
Iteration 8/25 | Loss: 0.00051047
Iteration 9/25 | Loss: 0.00051047
Iteration 10/25 | Loss: 0.00051047
Iteration 11/25 | Loss: 0.00051047
Iteration 12/25 | Loss: 0.00051047
Iteration 13/25 | Loss: 0.00051047
Iteration 14/25 | Loss: 0.00051047
Iteration 15/25 | Loss: 0.00051047
Iteration 16/25 | Loss: 0.00051047
Iteration 17/25 | Loss: 0.00051047
Iteration 18/25 | Loss: 0.00051047
Iteration 19/25 | Loss: 0.00051047
Iteration 20/25 | Loss: 0.00051047
Iteration 21/25 | Loss: 0.00051047
Iteration 22/25 | Loss: 0.00051047
Iteration 23/25 | Loss: 0.00051047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005104667507112026, 0.0005104667507112026, 0.0005104667507112026, 0.0005104667507112026, 0.0005104667507112026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005104667507112026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051047
Iteration 2/1000 | Loss: 0.00002533
Iteration 3/1000 | Loss: 0.00001492
Iteration 4/1000 | Loss: 0.00001139
Iteration 5/1000 | Loss: 0.00000988
Iteration 6/1000 | Loss: 0.00000939
Iteration 7/1000 | Loss: 0.00000892
Iteration 8/1000 | Loss: 0.00000870
Iteration 9/1000 | Loss: 0.00000847
Iteration 10/1000 | Loss: 0.00000837
Iteration 11/1000 | Loss: 0.00000833
Iteration 12/1000 | Loss: 0.00000832
Iteration 13/1000 | Loss: 0.00000832
Iteration 14/1000 | Loss: 0.00000831
Iteration 15/1000 | Loss: 0.00000831
Iteration 16/1000 | Loss: 0.00000831
Iteration 17/1000 | Loss: 0.00000830
Iteration 18/1000 | Loss: 0.00000830
Iteration 19/1000 | Loss: 0.00000830
Iteration 20/1000 | Loss: 0.00000829
Iteration 21/1000 | Loss: 0.00000829
Iteration 22/1000 | Loss: 0.00000829
Iteration 23/1000 | Loss: 0.00000829
Iteration 24/1000 | Loss: 0.00000828
Iteration 25/1000 | Loss: 0.00000828
Iteration 26/1000 | Loss: 0.00000828
Iteration 27/1000 | Loss: 0.00000827
Iteration 28/1000 | Loss: 0.00000827
Iteration 29/1000 | Loss: 0.00000827
Iteration 30/1000 | Loss: 0.00000827
Iteration 31/1000 | Loss: 0.00000827
Iteration 32/1000 | Loss: 0.00000826
Iteration 33/1000 | Loss: 0.00000826
Iteration 34/1000 | Loss: 0.00000825
Iteration 35/1000 | Loss: 0.00000824
Iteration 36/1000 | Loss: 0.00000824
Iteration 37/1000 | Loss: 0.00000824
Iteration 38/1000 | Loss: 0.00000824
Iteration 39/1000 | Loss: 0.00000823
Iteration 40/1000 | Loss: 0.00000823
Iteration 41/1000 | Loss: 0.00000823
Iteration 42/1000 | Loss: 0.00000823
Iteration 43/1000 | Loss: 0.00000822
Iteration 44/1000 | Loss: 0.00000822
Iteration 45/1000 | Loss: 0.00000822
Iteration 46/1000 | Loss: 0.00000819
Iteration 47/1000 | Loss: 0.00000819
Iteration 48/1000 | Loss: 0.00000816
Iteration 49/1000 | Loss: 0.00000815
Iteration 50/1000 | Loss: 0.00000815
Iteration 51/1000 | Loss: 0.00000814
Iteration 52/1000 | Loss: 0.00000814
Iteration 53/1000 | Loss: 0.00000814
Iteration 54/1000 | Loss: 0.00000813
Iteration 55/1000 | Loss: 0.00000813
Iteration 56/1000 | Loss: 0.00000813
Iteration 57/1000 | Loss: 0.00000813
Iteration 58/1000 | Loss: 0.00000812
Iteration 59/1000 | Loss: 0.00000812
Iteration 60/1000 | Loss: 0.00000811
Iteration 61/1000 | Loss: 0.00000811
Iteration 62/1000 | Loss: 0.00000811
Iteration 63/1000 | Loss: 0.00000810
Iteration 64/1000 | Loss: 0.00000810
Iteration 65/1000 | Loss: 0.00000810
Iteration 66/1000 | Loss: 0.00000810
Iteration 67/1000 | Loss: 0.00000810
Iteration 68/1000 | Loss: 0.00000810
Iteration 69/1000 | Loss: 0.00000810
Iteration 70/1000 | Loss: 0.00000810
Iteration 71/1000 | Loss: 0.00000809
Iteration 72/1000 | Loss: 0.00000809
Iteration 73/1000 | Loss: 0.00000809
Iteration 74/1000 | Loss: 0.00000809
Iteration 75/1000 | Loss: 0.00000809
Iteration 76/1000 | Loss: 0.00000809
Iteration 77/1000 | Loss: 0.00000809
Iteration 78/1000 | Loss: 0.00000808
Iteration 79/1000 | Loss: 0.00000808
Iteration 80/1000 | Loss: 0.00000808
Iteration 81/1000 | Loss: 0.00000808
Iteration 82/1000 | Loss: 0.00000808
Iteration 83/1000 | Loss: 0.00000808
Iteration 84/1000 | Loss: 0.00000807
Iteration 85/1000 | Loss: 0.00000807
Iteration 86/1000 | Loss: 0.00000806
Iteration 87/1000 | Loss: 0.00000806
Iteration 88/1000 | Loss: 0.00000806
Iteration 89/1000 | Loss: 0.00000806
Iteration 90/1000 | Loss: 0.00000806
Iteration 91/1000 | Loss: 0.00000805
Iteration 92/1000 | Loss: 0.00000805
Iteration 93/1000 | Loss: 0.00000805
Iteration 94/1000 | Loss: 0.00000805
Iteration 95/1000 | Loss: 0.00000805
Iteration 96/1000 | Loss: 0.00000805
Iteration 97/1000 | Loss: 0.00000805
Iteration 98/1000 | Loss: 0.00000805
Iteration 99/1000 | Loss: 0.00000805
Iteration 100/1000 | Loss: 0.00000804
Iteration 101/1000 | Loss: 0.00000804
Iteration 102/1000 | Loss: 0.00000804
Iteration 103/1000 | Loss: 0.00000804
Iteration 104/1000 | Loss: 0.00000804
Iteration 105/1000 | Loss: 0.00000804
Iteration 106/1000 | Loss: 0.00000804
Iteration 107/1000 | Loss: 0.00000804
Iteration 108/1000 | Loss: 0.00000804
Iteration 109/1000 | Loss: 0.00000804
Iteration 110/1000 | Loss: 0.00000804
Iteration 111/1000 | Loss: 0.00000804
Iteration 112/1000 | Loss: 0.00000804
Iteration 113/1000 | Loss: 0.00000804
Iteration 114/1000 | Loss: 0.00000804
Iteration 115/1000 | Loss: 0.00000804
Iteration 116/1000 | Loss: 0.00000804
Iteration 117/1000 | Loss: 0.00000804
Iteration 118/1000 | Loss: 0.00000804
Iteration 119/1000 | Loss: 0.00000804
Iteration 120/1000 | Loss: 0.00000804
Iteration 121/1000 | Loss: 0.00000804
Iteration 122/1000 | Loss: 0.00000804
Iteration 123/1000 | Loss: 0.00000804
Iteration 124/1000 | Loss: 0.00000804
Iteration 125/1000 | Loss: 0.00000804
Iteration 126/1000 | Loss: 0.00000804
Iteration 127/1000 | Loss: 0.00000804
Iteration 128/1000 | Loss: 0.00000804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [8.036537110456266e-06, 8.036537110456266e-06, 8.036537110456266e-06, 8.036537110456266e-06, 8.036537110456266e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.036537110456266e-06

Optimization complete. Final v2v error: 2.353689193725586 mm

Highest mean error: 3.1186273097991943 mm for frame 47

Lowest mean error: 2.1475887298583984 mm for frame 3

Saving results

Total time: 32.5453724861145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593607
Iteration 2/25 | Loss: 0.00094365
Iteration 3/25 | Loss: 0.00084231
Iteration 4/25 | Loss: 0.00083258
Iteration 5/25 | Loss: 0.00083012
Iteration 6/25 | Loss: 0.00082988
Iteration 7/25 | Loss: 0.00082988
Iteration 8/25 | Loss: 0.00082988
Iteration 9/25 | Loss: 0.00082988
Iteration 10/25 | Loss: 0.00082988
Iteration 11/25 | Loss: 0.00082988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008298819302581251, 0.0008298819302581251, 0.0008298819302581251, 0.0008298819302581251, 0.0008298819302581251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008298819302581251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.39410210
Iteration 2/25 | Loss: 0.00047048
Iteration 3/25 | Loss: 0.00047039
Iteration 4/25 | Loss: 0.00047039
Iteration 5/25 | Loss: 0.00047039
Iteration 6/25 | Loss: 0.00047038
Iteration 7/25 | Loss: 0.00047038
Iteration 8/25 | Loss: 0.00047038
Iteration 9/25 | Loss: 0.00047038
Iteration 10/25 | Loss: 0.00047038
Iteration 11/25 | Loss: 0.00047038
Iteration 12/25 | Loss: 0.00047038
Iteration 13/25 | Loss: 0.00047038
Iteration 14/25 | Loss: 0.00047038
Iteration 15/25 | Loss: 0.00047038
Iteration 16/25 | Loss: 0.00047038
Iteration 17/25 | Loss: 0.00047038
Iteration 18/25 | Loss: 0.00047038
Iteration 19/25 | Loss: 0.00047038
Iteration 20/25 | Loss: 0.00047038
Iteration 21/25 | Loss: 0.00047038
Iteration 22/25 | Loss: 0.00047038
Iteration 23/25 | Loss: 0.00047038
Iteration 24/25 | Loss: 0.00047038
Iteration 25/25 | Loss: 0.00047038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047038
Iteration 2/1000 | Loss: 0.00001980
Iteration 3/1000 | Loss: 0.00001077
Iteration 4/1000 | Loss: 0.00000969
Iteration 5/1000 | Loss: 0.00000923
Iteration 6/1000 | Loss: 0.00000894
Iteration 7/1000 | Loss: 0.00000870
Iteration 8/1000 | Loss: 0.00000869
Iteration 9/1000 | Loss: 0.00000862
Iteration 10/1000 | Loss: 0.00000859
Iteration 11/1000 | Loss: 0.00000845
Iteration 12/1000 | Loss: 0.00000843
Iteration 13/1000 | Loss: 0.00000842
Iteration 14/1000 | Loss: 0.00000836
Iteration 15/1000 | Loss: 0.00000836
Iteration 16/1000 | Loss: 0.00000835
Iteration 17/1000 | Loss: 0.00000834
Iteration 18/1000 | Loss: 0.00000829
Iteration 19/1000 | Loss: 0.00000827
Iteration 20/1000 | Loss: 0.00000825
Iteration 21/1000 | Loss: 0.00000825
Iteration 22/1000 | Loss: 0.00000825
Iteration 23/1000 | Loss: 0.00000825
Iteration 24/1000 | Loss: 0.00000825
Iteration 25/1000 | Loss: 0.00000825
Iteration 26/1000 | Loss: 0.00000825
Iteration 27/1000 | Loss: 0.00000825
Iteration 28/1000 | Loss: 0.00000825
Iteration 29/1000 | Loss: 0.00000825
Iteration 30/1000 | Loss: 0.00000824
Iteration 31/1000 | Loss: 0.00000824
Iteration 32/1000 | Loss: 0.00000824
Iteration 33/1000 | Loss: 0.00000824
Iteration 34/1000 | Loss: 0.00000824
Iteration 35/1000 | Loss: 0.00000824
Iteration 36/1000 | Loss: 0.00000824
Iteration 37/1000 | Loss: 0.00000824
Iteration 38/1000 | Loss: 0.00000824
Iteration 39/1000 | Loss: 0.00000824
Iteration 40/1000 | Loss: 0.00000823
Iteration 41/1000 | Loss: 0.00000822
Iteration 42/1000 | Loss: 0.00000822
Iteration 43/1000 | Loss: 0.00000821
Iteration 44/1000 | Loss: 0.00000820
Iteration 45/1000 | Loss: 0.00000819
Iteration 46/1000 | Loss: 0.00000819
Iteration 47/1000 | Loss: 0.00000819
Iteration 48/1000 | Loss: 0.00000819
Iteration 49/1000 | Loss: 0.00000818
Iteration 50/1000 | Loss: 0.00000818
Iteration 51/1000 | Loss: 0.00000817
Iteration 52/1000 | Loss: 0.00000817
Iteration 53/1000 | Loss: 0.00000817
Iteration 54/1000 | Loss: 0.00000816
Iteration 55/1000 | Loss: 0.00000816
Iteration 56/1000 | Loss: 0.00000816
Iteration 57/1000 | Loss: 0.00000816
Iteration 58/1000 | Loss: 0.00000815
Iteration 59/1000 | Loss: 0.00000815
Iteration 60/1000 | Loss: 0.00000815
Iteration 61/1000 | Loss: 0.00000815
Iteration 62/1000 | Loss: 0.00000814
Iteration 63/1000 | Loss: 0.00000814
Iteration 64/1000 | Loss: 0.00000814
Iteration 65/1000 | Loss: 0.00000814
Iteration 66/1000 | Loss: 0.00000813
Iteration 67/1000 | Loss: 0.00000813
Iteration 68/1000 | Loss: 0.00000813
Iteration 69/1000 | Loss: 0.00000813
Iteration 70/1000 | Loss: 0.00000813
Iteration 71/1000 | Loss: 0.00000813
Iteration 72/1000 | Loss: 0.00000813
Iteration 73/1000 | Loss: 0.00000813
Iteration 74/1000 | Loss: 0.00000812
Iteration 75/1000 | Loss: 0.00000812
Iteration 76/1000 | Loss: 0.00000812
Iteration 77/1000 | Loss: 0.00000812
Iteration 78/1000 | Loss: 0.00000812
Iteration 79/1000 | Loss: 0.00000812
Iteration 80/1000 | Loss: 0.00000812
Iteration 81/1000 | Loss: 0.00000812
Iteration 82/1000 | Loss: 0.00000811
Iteration 83/1000 | Loss: 0.00000811
Iteration 84/1000 | Loss: 0.00000811
Iteration 85/1000 | Loss: 0.00000811
Iteration 86/1000 | Loss: 0.00000811
Iteration 87/1000 | Loss: 0.00000811
Iteration 88/1000 | Loss: 0.00000811
Iteration 89/1000 | Loss: 0.00000810
Iteration 90/1000 | Loss: 0.00000810
Iteration 91/1000 | Loss: 0.00000810
Iteration 92/1000 | Loss: 0.00000810
Iteration 93/1000 | Loss: 0.00000810
Iteration 94/1000 | Loss: 0.00000810
Iteration 95/1000 | Loss: 0.00000810
Iteration 96/1000 | Loss: 0.00000810
Iteration 97/1000 | Loss: 0.00000810
Iteration 98/1000 | Loss: 0.00000810
Iteration 99/1000 | Loss: 0.00000809
Iteration 100/1000 | Loss: 0.00000809
Iteration 101/1000 | Loss: 0.00000809
Iteration 102/1000 | Loss: 0.00000809
Iteration 103/1000 | Loss: 0.00000809
Iteration 104/1000 | Loss: 0.00000809
Iteration 105/1000 | Loss: 0.00000809
Iteration 106/1000 | Loss: 0.00000809
Iteration 107/1000 | Loss: 0.00000809
Iteration 108/1000 | Loss: 0.00000809
Iteration 109/1000 | Loss: 0.00000809
Iteration 110/1000 | Loss: 0.00000809
Iteration 111/1000 | Loss: 0.00000809
Iteration 112/1000 | Loss: 0.00000809
Iteration 113/1000 | Loss: 0.00000809
Iteration 114/1000 | Loss: 0.00000809
Iteration 115/1000 | Loss: 0.00000809
Iteration 116/1000 | Loss: 0.00000809
Iteration 117/1000 | Loss: 0.00000809
Iteration 118/1000 | Loss: 0.00000808
Iteration 119/1000 | Loss: 0.00000808
Iteration 120/1000 | Loss: 0.00000808
Iteration 121/1000 | Loss: 0.00000808
Iteration 122/1000 | Loss: 0.00000808
Iteration 123/1000 | Loss: 0.00000808
Iteration 124/1000 | Loss: 0.00000808
Iteration 125/1000 | Loss: 0.00000808
Iteration 126/1000 | Loss: 0.00000808
Iteration 127/1000 | Loss: 0.00000808
Iteration 128/1000 | Loss: 0.00000808
Iteration 129/1000 | Loss: 0.00000808
Iteration 130/1000 | Loss: 0.00000808
Iteration 131/1000 | Loss: 0.00000808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [8.081859959929716e-06, 8.081859959929716e-06, 8.081859959929716e-06, 8.081859959929716e-06, 8.081859959929716e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.081859959929716e-06

Optimization complete. Final v2v error: 2.4213454723358154 mm

Highest mean error: 2.7385599613189697 mm for frame 73

Lowest mean error: 2.1008236408233643 mm for frame 36

Saving results

Total time: 31.9998676776886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818762
Iteration 2/25 | Loss: 0.00095214
Iteration 3/25 | Loss: 0.00081655
Iteration 4/25 | Loss: 0.00080205
Iteration 5/25 | Loss: 0.00079931
Iteration 6/25 | Loss: 0.00079901
Iteration 7/25 | Loss: 0.00079901
Iteration 8/25 | Loss: 0.00079901
Iteration 9/25 | Loss: 0.00079901
Iteration 10/25 | Loss: 0.00079901
Iteration 11/25 | Loss: 0.00079901
Iteration 12/25 | Loss: 0.00079901
Iteration 13/25 | Loss: 0.00079901
Iteration 14/25 | Loss: 0.00079901
Iteration 15/25 | Loss: 0.00079901
Iteration 16/25 | Loss: 0.00079901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007990086451172829, 0.0007990086451172829, 0.0007990086451172829, 0.0007990086451172829, 0.0007990086451172829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007990086451172829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39500070
Iteration 2/25 | Loss: 0.00042279
Iteration 3/25 | Loss: 0.00042279
Iteration 4/25 | Loss: 0.00042279
Iteration 5/25 | Loss: 0.00042279
Iteration 6/25 | Loss: 0.00042279
Iteration 7/25 | Loss: 0.00042279
Iteration 8/25 | Loss: 0.00042279
Iteration 9/25 | Loss: 0.00042279
Iteration 10/25 | Loss: 0.00042279
Iteration 11/25 | Loss: 0.00042279
Iteration 12/25 | Loss: 0.00042279
Iteration 13/25 | Loss: 0.00042279
Iteration 14/25 | Loss: 0.00042279
Iteration 15/25 | Loss: 0.00042279
Iteration 16/25 | Loss: 0.00042279
Iteration 17/25 | Loss: 0.00042279
Iteration 18/25 | Loss: 0.00042279
Iteration 19/25 | Loss: 0.00042279
Iteration 20/25 | Loss: 0.00042279
Iteration 21/25 | Loss: 0.00042279
Iteration 22/25 | Loss: 0.00042279
Iteration 23/25 | Loss: 0.00042279
Iteration 24/25 | Loss: 0.00042279
Iteration 25/25 | Loss: 0.00042279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042279
Iteration 2/1000 | Loss: 0.00001771
Iteration 3/1000 | Loss: 0.00000956
Iteration 4/1000 | Loss: 0.00000860
Iteration 5/1000 | Loss: 0.00000795
Iteration 6/1000 | Loss: 0.00000759
Iteration 7/1000 | Loss: 0.00000732
Iteration 8/1000 | Loss: 0.00000720
Iteration 9/1000 | Loss: 0.00000714
Iteration 10/1000 | Loss: 0.00000712
Iteration 11/1000 | Loss: 0.00000711
Iteration 12/1000 | Loss: 0.00000710
Iteration 13/1000 | Loss: 0.00000710
Iteration 14/1000 | Loss: 0.00000710
Iteration 15/1000 | Loss: 0.00000709
Iteration 16/1000 | Loss: 0.00000703
Iteration 17/1000 | Loss: 0.00000697
Iteration 18/1000 | Loss: 0.00000688
Iteration 19/1000 | Loss: 0.00000687
Iteration 20/1000 | Loss: 0.00000687
Iteration 21/1000 | Loss: 0.00000686
Iteration 22/1000 | Loss: 0.00000686
Iteration 23/1000 | Loss: 0.00000685
Iteration 24/1000 | Loss: 0.00000685
Iteration 25/1000 | Loss: 0.00000684
Iteration 26/1000 | Loss: 0.00000684
Iteration 27/1000 | Loss: 0.00000683
Iteration 28/1000 | Loss: 0.00000683
Iteration 29/1000 | Loss: 0.00000683
Iteration 30/1000 | Loss: 0.00000683
Iteration 31/1000 | Loss: 0.00000683
Iteration 32/1000 | Loss: 0.00000683
Iteration 33/1000 | Loss: 0.00000683
Iteration 34/1000 | Loss: 0.00000682
Iteration 35/1000 | Loss: 0.00000681
Iteration 36/1000 | Loss: 0.00000680
Iteration 37/1000 | Loss: 0.00000680
Iteration 38/1000 | Loss: 0.00000679
Iteration 39/1000 | Loss: 0.00000679
Iteration 40/1000 | Loss: 0.00000679
Iteration 41/1000 | Loss: 0.00000678
Iteration 42/1000 | Loss: 0.00000678
Iteration 43/1000 | Loss: 0.00000678
Iteration 44/1000 | Loss: 0.00000678
Iteration 45/1000 | Loss: 0.00000678
Iteration 46/1000 | Loss: 0.00000677
Iteration 47/1000 | Loss: 0.00000677
Iteration 48/1000 | Loss: 0.00000676
Iteration 49/1000 | Loss: 0.00000676
Iteration 50/1000 | Loss: 0.00000676
Iteration 51/1000 | Loss: 0.00000675
Iteration 52/1000 | Loss: 0.00000675
Iteration 53/1000 | Loss: 0.00000674
Iteration 54/1000 | Loss: 0.00000674
Iteration 55/1000 | Loss: 0.00000674
Iteration 56/1000 | Loss: 0.00000674
Iteration 57/1000 | Loss: 0.00000674
Iteration 58/1000 | Loss: 0.00000673
Iteration 59/1000 | Loss: 0.00000673
Iteration 60/1000 | Loss: 0.00000673
Iteration 61/1000 | Loss: 0.00000673
Iteration 62/1000 | Loss: 0.00000673
Iteration 63/1000 | Loss: 0.00000673
Iteration 64/1000 | Loss: 0.00000672
Iteration 65/1000 | Loss: 0.00000672
Iteration 66/1000 | Loss: 0.00000672
Iteration 67/1000 | Loss: 0.00000671
Iteration 68/1000 | Loss: 0.00000671
Iteration 69/1000 | Loss: 0.00000671
Iteration 70/1000 | Loss: 0.00000670
Iteration 71/1000 | Loss: 0.00000670
Iteration 72/1000 | Loss: 0.00000670
Iteration 73/1000 | Loss: 0.00000670
Iteration 74/1000 | Loss: 0.00000669
Iteration 75/1000 | Loss: 0.00000669
Iteration 76/1000 | Loss: 0.00000668
Iteration 77/1000 | Loss: 0.00000668
Iteration 78/1000 | Loss: 0.00000668
Iteration 79/1000 | Loss: 0.00000668
Iteration 80/1000 | Loss: 0.00000668
Iteration 81/1000 | Loss: 0.00000668
Iteration 82/1000 | Loss: 0.00000668
Iteration 83/1000 | Loss: 0.00000667
Iteration 84/1000 | Loss: 0.00000667
Iteration 85/1000 | Loss: 0.00000667
Iteration 86/1000 | Loss: 0.00000667
Iteration 87/1000 | Loss: 0.00000667
Iteration 88/1000 | Loss: 0.00000667
Iteration 89/1000 | Loss: 0.00000667
Iteration 90/1000 | Loss: 0.00000667
Iteration 91/1000 | Loss: 0.00000667
Iteration 92/1000 | Loss: 0.00000667
Iteration 93/1000 | Loss: 0.00000666
Iteration 94/1000 | Loss: 0.00000666
Iteration 95/1000 | Loss: 0.00000666
Iteration 96/1000 | Loss: 0.00000666
Iteration 97/1000 | Loss: 0.00000666
Iteration 98/1000 | Loss: 0.00000665
Iteration 99/1000 | Loss: 0.00000665
Iteration 100/1000 | Loss: 0.00000665
Iteration 101/1000 | Loss: 0.00000665
Iteration 102/1000 | Loss: 0.00000665
Iteration 103/1000 | Loss: 0.00000664
Iteration 104/1000 | Loss: 0.00000664
Iteration 105/1000 | Loss: 0.00000664
Iteration 106/1000 | Loss: 0.00000664
Iteration 107/1000 | Loss: 0.00000664
Iteration 108/1000 | Loss: 0.00000664
Iteration 109/1000 | Loss: 0.00000663
Iteration 110/1000 | Loss: 0.00000663
Iteration 111/1000 | Loss: 0.00000663
Iteration 112/1000 | Loss: 0.00000663
Iteration 113/1000 | Loss: 0.00000663
Iteration 114/1000 | Loss: 0.00000663
Iteration 115/1000 | Loss: 0.00000663
Iteration 116/1000 | Loss: 0.00000662
Iteration 117/1000 | Loss: 0.00000662
Iteration 118/1000 | Loss: 0.00000662
Iteration 119/1000 | Loss: 0.00000662
Iteration 120/1000 | Loss: 0.00000662
Iteration 121/1000 | Loss: 0.00000662
Iteration 122/1000 | Loss: 0.00000661
Iteration 123/1000 | Loss: 0.00000661
Iteration 124/1000 | Loss: 0.00000661
Iteration 125/1000 | Loss: 0.00000661
Iteration 126/1000 | Loss: 0.00000661
Iteration 127/1000 | Loss: 0.00000661
Iteration 128/1000 | Loss: 0.00000661
Iteration 129/1000 | Loss: 0.00000660
Iteration 130/1000 | Loss: 0.00000660
Iteration 131/1000 | Loss: 0.00000660
Iteration 132/1000 | Loss: 0.00000660
Iteration 133/1000 | Loss: 0.00000660
Iteration 134/1000 | Loss: 0.00000660
Iteration 135/1000 | Loss: 0.00000660
Iteration 136/1000 | Loss: 0.00000660
Iteration 137/1000 | Loss: 0.00000659
Iteration 138/1000 | Loss: 0.00000659
Iteration 139/1000 | Loss: 0.00000659
Iteration 140/1000 | Loss: 0.00000659
Iteration 141/1000 | Loss: 0.00000659
Iteration 142/1000 | Loss: 0.00000659
Iteration 143/1000 | Loss: 0.00000659
Iteration 144/1000 | Loss: 0.00000659
Iteration 145/1000 | Loss: 0.00000659
Iteration 146/1000 | Loss: 0.00000659
Iteration 147/1000 | Loss: 0.00000659
Iteration 148/1000 | Loss: 0.00000658
Iteration 149/1000 | Loss: 0.00000658
Iteration 150/1000 | Loss: 0.00000658
Iteration 151/1000 | Loss: 0.00000658
Iteration 152/1000 | Loss: 0.00000658
Iteration 153/1000 | Loss: 0.00000657
Iteration 154/1000 | Loss: 0.00000657
Iteration 155/1000 | Loss: 0.00000657
Iteration 156/1000 | Loss: 0.00000657
Iteration 157/1000 | Loss: 0.00000657
Iteration 158/1000 | Loss: 0.00000657
Iteration 159/1000 | Loss: 0.00000657
Iteration 160/1000 | Loss: 0.00000657
Iteration 161/1000 | Loss: 0.00000657
Iteration 162/1000 | Loss: 0.00000657
Iteration 163/1000 | Loss: 0.00000656
Iteration 164/1000 | Loss: 0.00000656
Iteration 165/1000 | Loss: 0.00000656
Iteration 166/1000 | Loss: 0.00000656
Iteration 167/1000 | Loss: 0.00000656
Iteration 168/1000 | Loss: 0.00000656
Iteration 169/1000 | Loss: 0.00000656
Iteration 170/1000 | Loss: 0.00000656
Iteration 171/1000 | Loss: 0.00000656
Iteration 172/1000 | Loss: 0.00000656
Iteration 173/1000 | Loss: 0.00000656
Iteration 174/1000 | Loss: 0.00000656
Iteration 175/1000 | Loss: 0.00000656
Iteration 176/1000 | Loss: 0.00000656
Iteration 177/1000 | Loss: 0.00000656
Iteration 178/1000 | Loss: 0.00000656
Iteration 179/1000 | Loss: 0.00000656
Iteration 180/1000 | Loss: 0.00000656
Iteration 181/1000 | Loss: 0.00000656
Iteration 182/1000 | Loss: 0.00000656
Iteration 183/1000 | Loss: 0.00000656
Iteration 184/1000 | Loss: 0.00000656
Iteration 185/1000 | Loss: 0.00000656
Iteration 186/1000 | Loss: 0.00000656
Iteration 187/1000 | Loss: 0.00000656
Iteration 188/1000 | Loss: 0.00000656
Iteration 189/1000 | Loss: 0.00000656
Iteration 190/1000 | Loss: 0.00000656
Iteration 191/1000 | Loss: 0.00000656
Iteration 192/1000 | Loss: 0.00000656
Iteration 193/1000 | Loss: 0.00000656
Iteration 194/1000 | Loss: 0.00000656
Iteration 195/1000 | Loss: 0.00000656
Iteration 196/1000 | Loss: 0.00000656
Iteration 197/1000 | Loss: 0.00000656
Iteration 198/1000 | Loss: 0.00000656
Iteration 199/1000 | Loss: 0.00000656
Iteration 200/1000 | Loss: 0.00000656
Iteration 201/1000 | Loss: 0.00000656
Iteration 202/1000 | Loss: 0.00000656
Iteration 203/1000 | Loss: 0.00000656
Iteration 204/1000 | Loss: 0.00000656
Iteration 205/1000 | Loss: 0.00000656
Iteration 206/1000 | Loss: 0.00000656
Iteration 207/1000 | Loss: 0.00000656
Iteration 208/1000 | Loss: 0.00000656
Iteration 209/1000 | Loss: 0.00000656
Iteration 210/1000 | Loss: 0.00000656
Iteration 211/1000 | Loss: 0.00000656
Iteration 212/1000 | Loss: 0.00000656
Iteration 213/1000 | Loss: 0.00000656
Iteration 214/1000 | Loss: 0.00000656
Iteration 215/1000 | Loss: 0.00000656
Iteration 216/1000 | Loss: 0.00000656
Iteration 217/1000 | Loss: 0.00000656
Iteration 218/1000 | Loss: 0.00000656
Iteration 219/1000 | Loss: 0.00000656
Iteration 220/1000 | Loss: 0.00000656
Iteration 221/1000 | Loss: 0.00000656
Iteration 222/1000 | Loss: 0.00000656
Iteration 223/1000 | Loss: 0.00000656
Iteration 224/1000 | Loss: 0.00000656
Iteration 225/1000 | Loss: 0.00000656
Iteration 226/1000 | Loss: 0.00000656
Iteration 227/1000 | Loss: 0.00000656
Iteration 228/1000 | Loss: 0.00000656
Iteration 229/1000 | Loss: 0.00000656
Iteration 230/1000 | Loss: 0.00000656
Iteration 231/1000 | Loss: 0.00000656
Iteration 232/1000 | Loss: 0.00000656
Iteration 233/1000 | Loss: 0.00000656
Iteration 234/1000 | Loss: 0.00000656
Iteration 235/1000 | Loss: 0.00000656
Iteration 236/1000 | Loss: 0.00000656
Iteration 237/1000 | Loss: 0.00000656
Iteration 238/1000 | Loss: 0.00000656
Iteration 239/1000 | Loss: 0.00000656
Iteration 240/1000 | Loss: 0.00000656
Iteration 241/1000 | Loss: 0.00000656
Iteration 242/1000 | Loss: 0.00000656
Iteration 243/1000 | Loss: 0.00000656
Iteration 244/1000 | Loss: 0.00000656
Iteration 245/1000 | Loss: 0.00000656
Iteration 246/1000 | Loss: 0.00000656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [6.561569989571581e-06, 6.561569989571581e-06, 6.561569989571581e-06, 6.561569989571581e-06, 6.561569989571581e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.561569989571581e-06

Optimization complete. Final v2v error: 2.1804583072662354 mm

Highest mean error: 2.3868532180786133 mm for frame 30

Lowest mean error: 2.072443962097168 mm for frame 12

Saving results

Total time: 36.011863708496094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799702
Iteration 2/25 | Loss: 0.00127074
Iteration 3/25 | Loss: 0.00095565
Iteration 4/25 | Loss: 0.00089792
Iteration 5/25 | Loss: 0.00088749
Iteration 6/25 | Loss: 0.00088492
Iteration 7/25 | Loss: 0.00088483
Iteration 8/25 | Loss: 0.00088483
Iteration 9/25 | Loss: 0.00088483
Iteration 10/25 | Loss: 0.00088483
Iteration 11/25 | Loss: 0.00088483
Iteration 12/25 | Loss: 0.00088483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000884829496499151, 0.000884829496499151, 0.000884829496499151, 0.000884829496499151, 0.000884829496499151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000884829496499151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37370312
Iteration 2/25 | Loss: 0.00038272
Iteration 3/25 | Loss: 0.00038271
Iteration 4/25 | Loss: 0.00038271
Iteration 5/25 | Loss: 0.00038271
Iteration 6/25 | Loss: 0.00038271
Iteration 7/25 | Loss: 0.00038271
Iteration 8/25 | Loss: 0.00038271
Iteration 9/25 | Loss: 0.00038271
Iteration 10/25 | Loss: 0.00038271
Iteration 11/25 | Loss: 0.00038271
Iteration 12/25 | Loss: 0.00038271
Iteration 13/25 | Loss: 0.00038271
Iteration 14/25 | Loss: 0.00038271
Iteration 15/25 | Loss: 0.00038271
Iteration 16/25 | Loss: 0.00038271
Iteration 17/25 | Loss: 0.00038271
Iteration 18/25 | Loss: 0.00038271
Iteration 19/25 | Loss: 0.00038271
Iteration 20/25 | Loss: 0.00038271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00038270975346677005, 0.00038270975346677005, 0.00038270975346677005, 0.00038270975346677005, 0.00038270975346677005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038270975346677005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038271
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001319
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001314
Iteration 21/1000 | Loss: 0.00001314
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001304
Iteration 30/1000 | Loss: 0.00001304
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.2966826943738852e-05, 1.2966826943738852e-05, 1.2966826943738852e-05, 1.2966826943738852e-05, 1.2966826943738852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2966826943738852e-05

Optimization complete. Final v2v error: 3.050764322280884 mm

Highest mean error: 3.477999210357666 mm for frame 152

Lowest mean error: 2.7288260459899902 mm for frame 56

Saving results

Total time: 34.774916887283325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362418
Iteration 2/25 | Loss: 0.00101140
Iteration 3/25 | Loss: 0.00085323
Iteration 4/25 | Loss: 0.00083480
Iteration 5/25 | Loss: 0.00082928
Iteration 6/25 | Loss: 0.00082829
Iteration 7/25 | Loss: 0.00082829
Iteration 8/25 | Loss: 0.00082829
Iteration 9/25 | Loss: 0.00082829
Iteration 10/25 | Loss: 0.00082829
Iteration 11/25 | Loss: 0.00082829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000828289077617228, 0.000828289077617228, 0.000828289077617228, 0.000828289077617228, 0.000828289077617228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000828289077617228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81007683
Iteration 2/25 | Loss: 0.00051154
Iteration 3/25 | Loss: 0.00051153
Iteration 4/25 | Loss: 0.00051153
Iteration 5/25 | Loss: 0.00051153
Iteration 6/25 | Loss: 0.00051153
Iteration 7/25 | Loss: 0.00051153
Iteration 8/25 | Loss: 0.00051153
Iteration 9/25 | Loss: 0.00051153
Iteration 10/25 | Loss: 0.00051153
Iteration 11/25 | Loss: 0.00051153
Iteration 12/25 | Loss: 0.00051153
Iteration 13/25 | Loss: 0.00051153
Iteration 14/25 | Loss: 0.00051153
Iteration 15/25 | Loss: 0.00051153
Iteration 16/25 | Loss: 0.00051153
Iteration 17/25 | Loss: 0.00051153
Iteration 18/25 | Loss: 0.00051153
Iteration 19/25 | Loss: 0.00051153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005115311942063272, 0.0005115311942063272, 0.0005115311942063272, 0.0005115311942063272, 0.0005115311942063272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005115311942063272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051153
Iteration 2/1000 | Loss: 0.00002427
Iteration 3/1000 | Loss: 0.00001267
Iteration 4/1000 | Loss: 0.00001092
Iteration 5/1000 | Loss: 0.00001003
Iteration 6/1000 | Loss: 0.00000951
Iteration 7/1000 | Loss: 0.00000917
Iteration 8/1000 | Loss: 0.00000893
Iteration 9/1000 | Loss: 0.00000886
Iteration 10/1000 | Loss: 0.00000865
Iteration 11/1000 | Loss: 0.00000846
Iteration 12/1000 | Loss: 0.00000845
Iteration 13/1000 | Loss: 0.00000842
Iteration 14/1000 | Loss: 0.00000842
Iteration 15/1000 | Loss: 0.00000841
Iteration 16/1000 | Loss: 0.00000840
Iteration 17/1000 | Loss: 0.00000839
Iteration 18/1000 | Loss: 0.00000838
Iteration 19/1000 | Loss: 0.00000836
Iteration 20/1000 | Loss: 0.00000836
Iteration 21/1000 | Loss: 0.00000835
Iteration 22/1000 | Loss: 0.00000834
Iteration 23/1000 | Loss: 0.00000831
Iteration 24/1000 | Loss: 0.00000828
Iteration 25/1000 | Loss: 0.00000826
Iteration 26/1000 | Loss: 0.00000826
Iteration 27/1000 | Loss: 0.00000826
Iteration 28/1000 | Loss: 0.00000825
Iteration 29/1000 | Loss: 0.00000824
Iteration 30/1000 | Loss: 0.00000824
Iteration 31/1000 | Loss: 0.00000823
Iteration 32/1000 | Loss: 0.00000822
Iteration 33/1000 | Loss: 0.00000822
Iteration 34/1000 | Loss: 0.00000822
Iteration 35/1000 | Loss: 0.00000821
Iteration 36/1000 | Loss: 0.00000821
Iteration 37/1000 | Loss: 0.00000820
Iteration 38/1000 | Loss: 0.00000820
Iteration 39/1000 | Loss: 0.00000820
Iteration 40/1000 | Loss: 0.00000820
Iteration 41/1000 | Loss: 0.00000819
Iteration 42/1000 | Loss: 0.00000819
Iteration 43/1000 | Loss: 0.00000819
Iteration 44/1000 | Loss: 0.00000819
Iteration 45/1000 | Loss: 0.00000819
Iteration 46/1000 | Loss: 0.00000818
Iteration 47/1000 | Loss: 0.00000818
Iteration 48/1000 | Loss: 0.00000818
Iteration 49/1000 | Loss: 0.00000818
Iteration 50/1000 | Loss: 0.00000818
Iteration 51/1000 | Loss: 0.00000818
Iteration 52/1000 | Loss: 0.00000817
Iteration 53/1000 | Loss: 0.00000817
Iteration 54/1000 | Loss: 0.00000817
Iteration 55/1000 | Loss: 0.00000817
Iteration 56/1000 | Loss: 0.00000817
Iteration 57/1000 | Loss: 0.00000817
Iteration 58/1000 | Loss: 0.00000816
Iteration 59/1000 | Loss: 0.00000816
Iteration 60/1000 | Loss: 0.00000816
Iteration 61/1000 | Loss: 0.00000816
Iteration 62/1000 | Loss: 0.00000816
Iteration 63/1000 | Loss: 0.00000816
Iteration 64/1000 | Loss: 0.00000815
Iteration 65/1000 | Loss: 0.00000815
Iteration 66/1000 | Loss: 0.00000815
Iteration 67/1000 | Loss: 0.00000815
Iteration 68/1000 | Loss: 0.00000815
Iteration 69/1000 | Loss: 0.00000815
Iteration 70/1000 | Loss: 0.00000815
Iteration 71/1000 | Loss: 0.00000815
Iteration 72/1000 | Loss: 0.00000814
Iteration 73/1000 | Loss: 0.00000814
Iteration 74/1000 | Loss: 0.00000814
Iteration 75/1000 | Loss: 0.00000814
Iteration 76/1000 | Loss: 0.00000814
Iteration 77/1000 | Loss: 0.00000814
Iteration 78/1000 | Loss: 0.00000814
Iteration 79/1000 | Loss: 0.00000814
Iteration 80/1000 | Loss: 0.00000813
Iteration 81/1000 | Loss: 0.00000813
Iteration 82/1000 | Loss: 0.00000813
Iteration 83/1000 | Loss: 0.00000813
Iteration 84/1000 | Loss: 0.00000813
Iteration 85/1000 | Loss: 0.00000812
Iteration 86/1000 | Loss: 0.00000812
Iteration 87/1000 | Loss: 0.00000812
Iteration 88/1000 | Loss: 0.00000812
Iteration 89/1000 | Loss: 0.00000812
Iteration 90/1000 | Loss: 0.00000812
Iteration 91/1000 | Loss: 0.00000812
Iteration 92/1000 | Loss: 0.00000811
Iteration 93/1000 | Loss: 0.00000811
Iteration 94/1000 | Loss: 0.00000811
Iteration 95/1000 | Loss: 0.00000811
Iteration 96/1000 | Loss: 0.00000811
Iteration 97/1000 | Loss: 0.00000811
Iteration 98/1000 | Loss: 0.00000811
Iteration 99/1000 | Loss: 0.00000811
Iteration 100/1000 | Loss: 0.00000811
Iteration 101/1000 | Loss: 0.00000810
Iteration 102/1000 | Loss: 0.00000810
Iteration 103/1000 | Loss: 0.00000810
Iteration 104/1000 | Loss: 0.00000810
Iteration 105/1000 | Loss: 0.00000810
Iteration 106/1000 | Loss: 0.00000810
Iteration 107/1000 | Loss: 0.00000810
Iteration 108/1000 | Loss: 0.00000810
Iteration 109/1000 | Loss: 0.00000810
Iteration 110/1000 | Loss: 0.00000810
Iteration 111/1000 | Loss: 0.00000810
Iteration 112/1000 | Loss: 0.00000810
Iteration 113/1000 | Loss: 0.00000809
Iteration 114/1000 | Loss: 0.00000809
Iteration 115/1000 | Loss: 0.00000809
Iteration 116/1000 | Loss: 0.00000809
Iteration 117/1000 | Loss: 0.00000809
Iteration 118/1000 | Loss: 0.00000809
Iteration 119/1000 | Loss: 0.00000809
Iteration 120/1000 | Loss: 0.00000809
Iteration 121/1000 | Loss: 0.00000809
Iteration 122/1000 | Loss: 0.00000809
Iteration 123/1000 | Loss: 0.00000809
Iteration 124/1000 | Loss: 0.00000809
Iteration 125/1000 | Loss: 0.00000808
Iteration 126/1000 | Loss: 0.00000808
Iteration 127/1000 | Loss: 0.00000808
Iteration 128/1000 | Loss: 0.00000808
Iteration 129/1000 | Loss: 0.00000808
Iteration 130/1000 | Loss: 0.00000808
Iteration 131/1000 | Loss: 0.00000808
Iteration 132/1000 | Loss: 0.00000808
Iteration 133/1000 | Loss: 0.00000808
Iteration 134/1000 | Loss: 0.00000808
Iteration 135/1000 | Loss: 0.00000808
Iteration 136/1000 | Loss: 0.00000808
Iteration 137/1000 | Loss: 0.00000808
Iteration 138/1000 | Loss: 0.00000807
Iteration 139/1000 | Loss: 0.00000807
Iteration 140/1000 | Loss: 0.00000807
Iteration 141/1000 | Loss: 0.00000807
Iteration 142/1000 | Loss: 0.00000807
Iteration 143/1000 | Loss: 0.00000807
Iteration 144/1000 | Loss: 0.00000807
Iteration 145/1000 | Loss: 0.00000807
Iteration 146/1000 | Loss: 0.00000807
Iteration 147/1000 | Loss: 0.00000807
Iteration 148/1000 | Loss: 0.00000807
Iteration 149/1000 | Loss: 0.00000807
Iteration 150/1000 | Loss: 0.00000807
Iteration 151/1000 | Loss: 0.00000807
Iteration 152/1000 | Loss: 0.00000807
Iteration 153/1000 | Loss: 0.00000807
Iteration 154/1000 | Loss: 0.00000807
Iteration 155/1000 | Loss: 0.00000807
Iteration 156/1000 | Loss: 0.00000806
Iteration 157/1000 | Loss: 0.00000806
Iteration 158/1000 | Loss: 0.00000806
Iteration 159/1000 | Loss: 0.00000806
Iteration 160/1000 | Loss: 0.00000806
Iteration 161/1000 | Loss: 0.00000806
Iteration 162/1000 | Loss: 0.00000806
Iteration 163/1000 | Loss: 0.00000806
Iteration 164/1000 | Loss: 0.00000806
Iteration 165/1000 | Loss: 0.00000806
Iteration 166/1000 | Loss: 0.00000806
Iteration 167/1000 | Loss: 0.00000806
Iteration 168/1000 | Loss: 0.00000806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [8.059795618464705e-06, 8.059795618464705e-06, 8.059795618464705e-06, 8.059795618464705e-06, 8.059795618464705e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.059795618464705e-06

Optimization complete. Final v2v error: 2.452523708343506 mm

Highest mean error: 2.926285743713379 mm for frame 75

Lowest mean error: 2.0910580158233643 mm for frame 8

Saving results

Total time: 40.621721506118774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989687
Iteration 2/25 | Loss: 0.00271694
Iteration 3/25 | Loss: 0.00139895
Iteration 4/25 | Loss: 0.00118301
Iteration 5/25 | Loss: 0.00122739
Iteration 6/25 | Loss: 0.00113500
Iteration 7/25 | Loss: 0.00098103
Iteration 8/25 | Loss: 0.00093996
Iteration 9/25 | Loss: 0.00092457
Iteration 10/25 | Loss: 0.00092160
Iteration 11/25 | Loss: 0.00091528
Iteration 12/25 | Loss: 0.00091164
Iteration 13/25 | Loss: 0.00091026
Iteration 14/25 | Loss: 0.00090908
Iteration 15/25 | Loss: 0.00090872
Iteration 16/25 | Loss: 0.00090869
Iteration 17/25 | Loss: 0.00090869
Iteration 18/25 | Loss: 0.00090869
Iteration 19/25 | Loss: 0.00090869
Iteration 20/25 | Loss: 0.00090869
Iteration 21/25 | Loss: 0.00090869
Iteration 22/25 | Loss: 0.00090868
Iteration 23/25 | Loss: 0.00090868
Iteration 24/25 | Loss: 0.00090867
Iteration 25/25 | Loss: 0.00090867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36982059
Iteration 2/25 | Loss: 0.00034049
Iteration 3/25 | Loss: 0.00030042
Iteration 4/25 | Loss: 0.00030042
Iteration 5/25 | Loss: 0.00030042
Iteration 6/25 | Loss: 0.00030042
Iteration 7/25 | Loss: 0.00030042
Iteration 8/25 | Loss: 0.00030042
Iteration 9/25 | Loss: 0.00030042
Iteration 10/25 | Loss: 0.00030042
Iteration 11/25 | Loss: 0.00030042
Iteration 12/25 | Loss: 0.00030042
Iteration 13/25 | Loss: 0.00030042
Iteration 14/25 | Loss: 0.00030042
Iteration 15/25 | Loss: 0.00030042
Iteration 16/25 | Loss: 0.00030042
Iteration 17/25 | Loss: 0.00030042
Iteration 18/25 | Loss: 0.00030042
Iteration 19/25 | Loss: 0.00030042
Iteration 20/25 | Loss: 0.00030042
Iteration 21/25 | Loss: 0.00030042
Iteration 22/25 | Loss: 0.00030042
Iteration 23/25 | Loss: 0.00030042
Iteration 24/25 | Loss: 0.00030042
Iteration 25/25 | Loss: 0.00030042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030042
Iteration 2/1000 | Loss: 0.00008114
Iteration 3/1000 | Loss: 0.00002159
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00007730
Iteration 7/1000 | Loss: 0.00006265
Iteration 8/1000 | Loss: 0.00001486
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00004026
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00008520
Iteration 14/1000 | Loss: 0.00011982
Iteration 15/1000 | Loss: 0.00003318
Iteration 16/1000 | Loss: 0.00003376
Iteration 17/1000 | Loss: 0.00001678
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00009848
Iteration 23/1000 | Loss: 0.00001798
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001234
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001226
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001225
Iteration 65/1000 | Loss: 0.00001225
Iteration 66/1000 | Loss: 0.00001225
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001224
Iteration 70/1000 | Loss: 0.00001224
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001223
Iteration 73/1000 | Loss: 0.00001223
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001222
Iteration 76/1000 | Loss: 0.00001222
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00007383
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00002371
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001212
Iteration 113/1000 | Loss: 0.00012173
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00014292
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001138
Iteration 121/1000 | Loss: 0.00001138
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001134
Iteration 135/1000 | Loss: 0.00001134
Iteration 136/1000 | Loss: 0.00001134
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001131
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001126
Iteration 147/1000 | Loss: 0.00001125
Iteration 148/1000 | Loss: 0.00001125
Iteration 149/1000 | Loss: 0.00001125
Iteration 150/1000 | Loss: 0.00001125
Iteration 151/1000 | Loss: 0.00001125
Iteration 152/1000 | Loss: 0.00001125
Iteration 153/1000 | Loss: 0.00001125
Iteration 154/1000 | Loss: 0.00001125
Iteration 155/1000 | Loss: 0.00001124
Iteration 156/1000 | Loss: 0.00001123
Iteration 157/1000 | Loss: 0.00001123
Iteration 158/1000 | Loss: 0.00001123
Iteration 159/1000 | Loss: 0.00001122
Iteration 160/1000 | Loss: 0.00001122
Iteration 161/1000 | Loss: 0.00001122
Iteration 162/1000 | Loss: 0.00001121
Iteration 163/1000 | Loss: 0.00001121
Iteration 164/1000 | Loss: 0.00001121
Iteration 165/1000 | Loss: 0.00001121
Iteration 166/1000 | Loss: 0.00001121
Iteration 167/1000 | Loss: 0.00001121
Iteration 168/1000 | Loss: 0.00001121
Iteration 169/1000 | Loss: 0.00001121
Iteration 170/1000 | Loss: 0.00001120
Iteration 171/1000 | Loss: 0.00001120
Iteration 172/1000 | Loss: 0.00001119
Iteration 173/1000 | Loss: 0.00001119
Iteration 174/1000 | Loss: 0.00001119
Iteration 175/1000 | Loss: 0.00001119
Iteration 176/1000 | Loss: 0.00001118
Iteration 177/1000 | Loss: 0.00001118
Iteration 178/1000 | Loss: 0.00001118
Iteration 179/1000 | Loss: 0.00001118
Iteration 180/1000 | Loss: 0.00001118
Iteration 181/1000 | Loss: 0.00001118
Iteration 182/1000 | Loss: 0.00001118
Iteration 183/1000 | Loss: 0.00001118
Iteration 184/1000 | Loss: 0.00001118
Iteration 185/1000 | Loss: 0.00001118
Iteration 186/1000 | Loss: 0.00001118
Iteration 187/1000 | Loss: 0.00001118
Iteration 188/1000 | Loss: 0.00001118
Iteration 189/1000 | Loss: 0.00001118
Iteration 190/1000 | Loss: 0.00001118
Iteration 191/1000 | Loss: 0.00001118
Iteration 192/1000 | Loss: 0.00001118
Iteration 193/1000 | Loss: 0.00001118
Iteration 194/1000 | Loss: 0.00001118
Iteration 195/1000 | Loss: 0.00001118
Iteration 196/1000 | Loss: 0.00001118
Iteration 197/1000 | Loss: 0.00001118
Iteration 198/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.1175263352924958e-05, 1.1175263352924958e-05, 1.1175263352924958e-05, 1.1175263352924958e-05, 1.1175263352924958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1175263352924958e-05

Optimization complete. Final v2v error: 2.760658025741577 mm

Highest mean error: 5.221566200256348 mm for frame 83

Lowest mean error: 2.287604570388794 mm for frame 228

Saving results

Total time: 116.18686389923096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801255
Iteration 2/25 | Loss: 0.00175579
Iteration 3/25 | Loss: 0.00123224
Iteration 4/25 | Loss: 0.00121349
Iteration 5/25 | Loss: 0.00120929
Iteration 6/25 | Loss: 0.00120793
Iteration 7/25 | Loss: 0.00120772
Iteration 8/25 | Loss: 0.00120772
Iteration 9/25 | Loss: 0.00120772
Iteration 10/25 | Loss: 0.00120772
Iteration 11/25 | Loss: 0.00120772
Iteration 12/25 | Loss: 0.00120772
Iteration 13/25 | Loss: 0.00120772
Iteration 14/25 | Loss: 0.00120772
Iteration 15/25 | Loss: 0.00120772
Iteration 16/25 | Loss: 0.00120772
Iteration 17/25 | Loss: 0.00120772
Iteration 18/25 | Loss: 0.00120772
Iteration 19/25 | Loss: 0.00120772
Iteration 20/25 | Loss: 0.00120772
Iteration 21/25 | Loss: 0.00120772
Iteration 22/25 | Loss: 0.00120772
Iteration 23/25 | Loss: 0.00120772
Iteration 24/25 | Loss: 0.00120772
Iteration 25/25 | Loss: 0.00120772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62565756
Iteration 2/25 | Loss: 0.00065999
Iteration 3/25 | Loss: 0.00065999
Iteration 4/25 | Loss: 0.00065999
Iteration 5/25 | Loss: 0.00065999
Iteration 6/25 | Loss: 0.00065999
Iteration 7/25 | Loss: 0.00065999
Iteration 8/25 | Loss: 0.00065999
Iteration 9/25 | Loss: 0.00065999
Iteration 10/25 | Loss: 0.00065999
Iteration 11/25 | Loss: 0.00065999
Iteration 12/25 | Loss: 0.00065999
Iteration 13/25 | Loss: 0.00065999
Iteration 14/25 | Loss: 0.00065999
Iteration 15/25 | Loss: 0.00065999
Iteration 16/25 | Loss: 0.00065999
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006599858752451837, 0.0006599858752451837, 0.0006599858752451837, 0.0006599858752451837, 0.0006599858752451837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006599858752451837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065999
Iteration 2/1000 | Loss: 0.00010117
Iteration 3/1000 | Loss: 0.00006175
Iteration 4/1000 | Loss: 0.00005391
Iteration 5/1000 | Loss: 0.00005043
Iteration 6/1000 | Loss: 0.00004898
Iteration 7/1000 | Loss: 0.00004803
Iteration 8/1000 | Loss: 0.00004702
Iteration 9/1000 | Loss: 0.00004596
Iteration 10/1000 | Loss: 0.00004504
Iteration 11/1000 | Loss: 0.00004424
Iteration 12/1000 | Loss: 0.00004360
Iteration 13/1000 | Loss: 0.00004312
Iteration 14/1000 | Loss: 0.00004248
Iteration 15/1000 | Loss: 0.00004202
Iteration 16/1000 | Loss: 0.00004172
Iteration 17/1000 | Loss: 0.00004149
Iteration 18/1000 | Loss: 0.00004121
Iteration 19/1000 | Loss: 0.00004107
Iteration 20/1000 | Loss: 0.00004099
Iteration 21/1000 | Loss: 0.00004085
Iteration 22/1000 | Loss: 0.00004081
Iteration 23/1000 | Loss: 0.00004070
Iteration 24/1000 | Loss: 0.00004059
Iteration 25/1000 | Loss: 0.00004054
Iteration 26/1000 | Loss: 0.00004049
Iteration 27/1000 | Loss: 0.00004049
Iteration 28/1000 | Loss: 0.00004049
Iteration 29/1000 | Loss: 0.00004049
Iteration 30/1000 | Loss: 0.00004047
Iteration 31/1000 | Loss: 0.00004047
Iteration 32/1000 | Loss: 0.00004046
Iteration 33/1000 | Loss: 0.00004046
Iteration 34/1000 | Loss: 0.00004046
Iteration 35/1000 | Loss: 0.00004046
Iteration 36/1000 | Loss: 0.00004045
Iteration 37/1000 | Loss: 0.00004045
Iteration 38/1000 | Loss: 0.00004045
Iteration 39/1000 | Loss: 0.00004045
Iteration 40/1000 | Loss: 0.00004045
Iteration 41/1000 | Loss: 0.00004045
Iteration 42/1000 | Loss: 0.00004045
Iteration 43/1000 | Loss: 0.00004045
Iteration 44/1000 | Loss: 0.00004045
Iteration 45/1000 | Loss: 0.00004045
Iteration 46/1000 | Loss: 0.00004045
Iteration 47/1000 | Loss: 0.00004044
Iteration 48/1000 | Loss: 0.00004044
Iteration 49/1000 | Loss: 0.00004044
Iteration 50/1000 | Loss: 0.00004044
Iteration 51/1000 | Loss: 0.00004043
Iteration 52/1000 | Loss: 0.00004042
Iteration 53/1000 | Loss: 0.00004042
Iteration 54/1000 | Loss: 0.00004042
Iteration 55/1000 | Loss: 0.00004041
Iteration 56/1000 | Loss: 0.00004041
Iteration 57/1000 | Loss: 0.00004040
Iteration 58/1000 | Loss: 0.00004040
Iteration 59/1000 | Loss: 0.00004040
Iteration 60/1000 | Loss: 0.00004040
Iteration 61/1000 | Loss: 0.00004040
Iteration 62/1000 | Loss: 0.00004040
Iteration 63/1000 | Loss: 0.00004039
Iteration 64/1000 | Loss: 0.00004039
Iteration 65/1000 | Loss: 0.00004039
Iteration 66/1000 | Loss: 0.00004039
Iteration 67/1000 | Loss: 0.00004038
Iteration 68/1000 | Loss: 0.00004037
Iteration 69/1000 | Loss: 0.00004036
Iteration 70/1000 | Loss: 0.00004036
Iteration 71/1000 | Loss: 0.00004036
Iteration 72/1000 | Loss: 0.00004035
Iteration 73/1000 | Loss: 0.00004034
Iteration 74/1000 | Loss: 0.00004034
Iteration 75/1000 | Loss: 0.00004034
Iteration 76/1000 | Loss: 0.00004034
Iteration 77/1000 | Loss: 0.00004032
Iteration 78/1000 | Loss: 0.00004032
Iteration 79/1000 | Loss: 0.00004031
Iteration 80/1000 | Loss: 0.00004031
Iteration 81/1000 | Loss: 0.00004031
Iteration 82/1000 | Loss: 0.00004030
Iteration 83/1000 | Loss: 0.00004030
Iteration 84/1000 | Loss: 0.00004030
Iteration 85/1000 | Loss: 0.00004029
Iteration 86/1000 | Loss: 0.00004029
Iteration 87/1000 | Loss: 0.00004028
Iteration 88/1000 | Loss: 0.00004027
Iteration 89/1000 | Loss: 0.00004027
Iteration 90/1000 | Loss: 0.00004027
Iteration 91/1000 | Loss: 0.00004027
Iteration 92/1000 | Loss: 0.00004027
Iteration 93/1000 | Loss: 0.00004026
Iteration 94/1000 | Loss: 0.00004026
Iteration 95/1000 | Loss: 0.00004026
Iteration 96/1000 | Loss: 0.00004026
Iteration 97/1000 | Loss: 0.00004025
Iteration 98/1000 | Loss: 0.00004025
Iteration 99/1000 | Loss: 0.00004025
Iteration 100/1000 | Loss: 0.00004025
Iteration 101/1000 | Loss: 0.00004025
Iteration 102/1000 | Loss: 0.00004025
Iteration 103/1000 | Loss: 0.00004024
Iteration 104/1000 | Loss: 0.00004024
Iteration 105/1000 | Loss: 0.00004024
Iteration 106/1000 | Loss: 0.00004024
Iteration 107/1000 | Loss: 0.00004024
Iteration 108/1000 | Loss: 0.00004024
Iteration 109/1000 | Loss: 0.00004024
Iteration 110/1000 | Loss: 0.00004023
Iteration 111/1000 | Loss: 0.00004023
Iteration 112/1000 | Loss: 0.00004023
Iteration 113/1000 | Loss: 0.00004023
Iteration 114/1000 | Loss: 0.00004023
Iteration 115/1000 | Loss: 0.00004023
Iteration 116/1000 | Loss: 0.00004023
Iteration 117/1000 | Loss: 0.00004023
Iteration 118/1000 | Loss: 0.00004023
Iteration 119/1000 | Loss: 0.00004023
Iteration 120/1000 | Loss: 0.00004023
Iteration 121/1000 | Loss: 0.00004023
Iteration 122/1000 | Loss: 0.00004023
Iteration 123/1000 | Loss: 0.00004023
Iteration 124/1000 | Loss: 0.00004023
Iteration 125/1000 | Loss: 0.00004023
Iteration 126/1000 | Loss: 0.00004023
Iteration 127/1000 | Loss: 0.00004023
Iteration 128/1000 | Loss: 0.00004023
Iteration 129/1000 | Loss: 0.00004023
Iteration 130/1000 | Loss: 0.00004023
Iteration 131/1000 | Loss: 0.00004023
Iteration 132/1000 | Loss: 0.00004023
Iteration 133/1000 | Loss: 0.00004023
Iteration 134/1000 | Loss: 0.00004023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [4.0227114368462935e-05, 4.0227114368462935e-05, 4.0227114368462935e-05, 4.0227114368462935e-05, 4.0227114368462935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0227114368462935e-05

Optimization complete. Final v2v error: 5.050670623779297 mm

Highest mean error: 5.6688456535339355 mm for frame 62

Lowest mean error: 3.7603142261505127 mm for frame 11

Saving results

Total time: 53.26583671569824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818104
Iteration 2/25 | Loss: 0.00121769
Iteration 3/25 | Loss: 0.00096380
Iteration 4/25 | Loss: 0.00091943
Iteration 5/25 | Loss: 0.00090983
Iteration 6/25 | Loss: 0.00090714
Iteration 7/25 | Loss: 0.00090675
Iteration 8/25 | Loss: 0.00090675
Iteration 9/25 | Loss: 0.00090675
Iteration 10/25 | Loss: 0.00090675
Iteration 11/25 | Loss: 0.00090675
Iteration 12/25 | Loss: 0.00090675
Iteration 13/25 | Loss: 0.00090675
Iteration 14/25 | Loss: 0.00090675
Iteration 15/25 | Loss: 0.00090675
Iteration 16/25 | Loss: 0.00090675
Iteration 17/25 | Loss: 0.00090675
Iteration 18/25 | Loss: 0.00090675
Iteration 19/25 | Loss: 0.00090675
Iteration 20/25 | Loss: 0.00090675
Iteration 21/25 | Loss: 0.00090675
Iteration 22/25 | Loss: 0.00090675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009067468345165253, 0.0009067468345165253, 0.0009067468345165253, 0.0009067468345165253, 0.0009067468345165253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009067468345165253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18199909
Iteration 2/25 | Loss: 0.00036642
Iteration 3/25 | Loss: 0.00036639
Iteration 4/25 | Loss: 0.00036639
Iteration 5/25 | Loss: 0.00036639
Iteration 6/25 | Loss: 0.00036639
Iteration 7/25 | Loss: 0.00036639
Iteration 8/25 | Loss: 0.00036639
Iteration 9/25 | Loss: 0.00036639
Iteration 10/25 | Loss: 0.00036639
Iteration 11/25 | Loss: 0.00036639
Iteration 12/25 | Loss: 0.00036639
Iteration 13/25 | Loss: 0.00036639
Iteration 14/25 | Loss: 0.00036639
Iteration 15/25 | Loss: 0.00036639
Iteration 16/25 | Loss: 0.00036639
Iteration 17/25 | Loss: 0.00036639
Iteration 18/25 | Loss: 0.00036639
Iteration 19/25 | Loss: 0.00036639
Iteration 20/25 | Loss: 0.00036639
Iteration 21/25 | Loss: 0.00036639
Iteration 22/25 | Loss: 0.00036639
Iteration 23/25 | Loss: 0.00036639
Iteration 24/25 | Loss: 0.00036639
Iteration 25/25 | Loss: 0.00036639
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003663903917185962, 0.0003663903917185962, 0.0003663903917185962, 0.0003663903917185962, 0.0003663903917185962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003663903917185962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036639
Iteration 2/1000 | Loss: 0.00004389
Iteration 3/1000 | Loss: 0.00003007
Iteration 4/1000 | Loss: 0.00002309
Iteration 5/1000 | Loss: 0.00002174
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001920
Iteration 9/1000 | Loss: 0.00001870
Iteration 10/1000 | Loss: 0.00001831
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001704
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001703
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001701
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001700
Iteration 34/1000 | Loss: 0.00001700
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001699
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001695
Iteration 60/1000 | Loss: 0.00001695
Iteration 61/1000 | Loss: 0.00001695
Iteration 62/1000 | Loss: 0.00001695
Iteration 63/1000 | Loss: 0.00001694
Iteration 64/1000 | Loss: 0.00001694
Iteration 65/1000 | Loss: 0.00001694
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001693
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001693
Iteration 76/1000 | Loss: 0.00001693
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001692
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001692
Iteration 82/1000 | Loss: 0.00001692
Iteration 83/1000 | Loss: 0.00001692
Iteration 84/1000 | Loss: 0.00001692
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001692
Iteration 87/1000 | Loss: 0.00001692
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001691
Iteration 103/1000 | Loss: 0.00001691
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6896003216970712e-05, 1.6896003216970712e-05, 1.6896003216970712e-05, 1.6896003216970712e-05, 1.6896003216970712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6896003216970712e-05

Optimization complete. Final v2v error: 3.3924479484558105 mm

Highest mean error: 4.609312057495117 mm for frame 116

Lowest mean error: 2.407823085784912 mm for frame 215

Saving results

Total time: 46.22840929031372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953540
Iteration 2/25 | Loss: 0.00136743
Iteration 3/25 | Loss: 0.00113923
Iteration 4/25 | Loss: 0.00095455
Iteration 5/25 | Loss: 0.00092330
Iteration 6/25 | Loss: 0.00090665
Iteration 7/25 | Loss: 0.00089751
Iteration 8/25 | Loss: 0.00089818
Iteration 9/25 | Loss: 0.00089413
Iteration 10/25 | Loss: 0.00089228
Iteration 11/25 | Loss: 0.00089063
Iteration 12/25 | Loss: 0.00088959
Iteration 13/25 | Loss: 0.00088744
Iteration 14/25 | Loss: 0.00088666
Iteration 15/25 | Loss: 0.00088543
Iteration 16/25 | Loss: 0.00088491
Iteration 17/25 | Loss: 0.00088487
Iteration 18/25 | Loss: 0.00088487
Iteration 19/25 | Loss: 0.00088487
Iteration 20/25 | Loss: 0.00088486
Iteration 21/25 | Loss: 0.00088486
Iteration 22/25 | Loss: 0.00088486
Iteration 23/25 | Loss: 0.00088485
Iteration 24/25 | Loss: 0.00088485
Iteration 25/25 | Loss: 0.00088485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51053417
Iteration 2/25 | Loss: 0.00051910
Iteration 3/25 | Loss: 0.00046708
Iteration 4/25 | Loss: 0.00046707
Iteration 5/25 | Loss: 0.00046707
Iteration 6/25 | Loss: 0.00046707
Iteration 7/25 | Loss: 0.00046707
Iteration 8/25 | Loss: 0.00046707
Iteration 9/25 | Loss: 0.00046707
Iteration 10/25 | Loss: 0.00046707
Iteration 11/25 | Loss: 0.00046707
Iteration 12/25 | Loss: 0.00046707
Iteration 13/25 | Loss: 0.00046707
Iteration 14/25 | Loss: 0.00046707
Iteration 15/25 | Loss: 0.00046707
Iteration 16/25 | Loss: 0.00046707
Iteration 17/25 | Loss: 0.00046707
Iteration 18/25 | Loss: 0.00046707
Iteration 19/25 | Loss: 0.00046707
Iteration 20/25 | Loss: 0.00046707
Iteration 21/25 | Loss: 0.00046707
Iteration 22/25 | Loss: 0.00046707
Iteration 23/25 | Loss: 0.00046707
Iteration 24/25 | Loss: 0.00046707
Iteration 25/25 | Loss: 0.00046707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046707
Iteration 2/1000 | Loss: 0.00007333
Iteration 3/1000 | Loss: 0.00010645
Iteration 4/1000 | Loss: 0.00026554
Iteration 5/1000 | Loss: 0.00001995
Iteration 6/1000 | Loss: 0.00009889
Iteration 7/1000 | Loss: 0.00005964
Iteration 8/1000 | Loss: 0.00001946
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00010460
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00006585
Iteration 14/1000 | Loss: 0.00009796
Iteration 15/1000 | Loss: 0.00004243
Iteration 16/1000 | Loss: 0.00001523
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00046375
Iteration 20/1000 | Loss: 0.00002652
Iteration 21/1000 | Loss: 0.00010488
Iteration 22/1000 | Loss: 0.00001558
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00002721
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00001286
Iteration 44/1000 | Loss: 0.00001285
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00002272
Iteration 48/1000 | Loss: 0.00001789
Iteration 49/1000 | Loss: 0.00001283
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001276
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001266
Iteration 74/1000 | Loss: 0.00001266
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001265
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001263
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001259
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001258
Iteration 93/1000 | Loss: 0.00001258
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001258
Iteration 96/1000 | Loss: 0.00001258
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001257
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001257
Iteration 107/1000 | Loss: 0.00001257
Iteration 108/1000 | Loss: 0.00001257
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001252
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001252
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001252
Iteration 149/1000 | Loss: 0.00001252
Iteration 150/1000 | Loss: 0.00001252
Iteration 151/1000 | Loss: 0.00001252
Iteration 152/1000 | Loss: 0.00001252
Iteration 153/1000 | Loss: 0.00001252
Iteration 154/1000 | Loss: 0.00001252
Iteration 155/1000 | Loss: 0.00001252
Iteration 156/1000 | Loss: 0.00001252
Iteration 157/1000 | Loss: 0.00001251
Iteration 158/1000 | Loss: 0.00001251
Iteration 159/1000 | Loss: 0.00001251
Iteration 160/1000 | Loss: 0.00001251
Iteration 161/1000 | Loss: 0.00001251
Iteration 162/1000 | Loss: 0.00001251
Iteration 163/1000 | Loss: 0.00001251
Iteration 164/1000 | Loss: 0.00001251
Iteration 165/1000 | Loss: 0.00001251
Iteration 166/1000 | Loss: 0.00001251
Iteration 167/1000 | Loss: 0.00001251
Iteration 168/1000 | Loss: 0.00001251
Iteration 169/1000 | Loss: 0.00001251
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001251
Iteration 174/1000 | Loss: 0.00001251
Iteration 175/1000 | Loss: 0.00001251
Iteration 176/1000 | Loss: 0.00001251
Iteration 177/1000 | Loss: 0.00001251
Iteration 178/1000 | Loss: 0.00001250
Iteration 179/1000 | Loss: 0.00001250
Iteration 180/1000 | Loss: 0.00001250
Iteration 181/1000 | Loss: 0.00001250
Iteration 182/1000 | Loss: 0.00001250
Iteration 183/1000 | Loss: 0.00001250
Iteration 184/1000 | Loss: 0.00001250
Iteration 185/1000 | Loss: 0.00001250
Iteration 186/1000 | Loss: 0.00001250
Iteration 187/1000 | Loss: 0.00001250
Iteration 188/1000 | Loss: 0.00001250
Iteration 189/1000 | Loss: 0.00001250
Iteration 190/1000 | Loss: 0.00001250
Iteration 191/1000 | Loss: 0.00001250
Iteration 192/1000 | Loss: 0.00001250
Iteration 193/1000 | Loss: 0.00001250
Iteration 194/1000 | Loss: 0.00001250
Iteration 195/1000 | Loss: 0.00001250
Iteration 196/1000 | Loss: 0.00001250
Iteration 197/1000 | Loss: 0.00001250
Iteration 198/1000 | Loss: 0.00001250
Iteration 199/1000 | Loss: 0.00001249
Iteration 200/1000 | Loss: 0.00001249
Iteration 201/1000 | Loss: 0.00001249
Iteration 202/1000 | Loss: 0.00001249
Iteration 203/1000 | Loss: 0.00001249
Iteration 204/1000 | Loss: 0.00001249
Iteration 205/1000 | Loss: 0.00001249
Iteration 206/1000 | Loss: 0.00001249
Iteration 207/1000 | Loss: 0.00001249
Iteration 208/1000 | Loss: 0.00001249
Iteration 209/1000 | Loss: 0.00001249
Iteration 210/1000 | Loss: 0.00001249
Iteration 211/1000 | Loss: 0.00001249
Iteration 212/1000 | Loss: 0.00001249
Iteration 213/1000 | Loss: 0.00001249
Iteration 214/1000 | Loss: 0.00001249
Iteration 215/1000 | Loss: 0.00001249
Iteration 216/1000 | Loss: 0.00001249
Iteration 217/1000 | Loss: 0.00001249
Iteration 218/1000 | Loss: 0.00001249
Iteration 219/1000 | Loss: 0.00001249
Iteration 220/1000 | Loss: 0.00001249
Iteration 221/1000 | Loss: 0.00001249
Iteration 222/1000 | Loss: 0.00001249
Iteration 223/1000 | Loss: 0.00001249
Iteration 224/1000 | Loss: 0.00001249
Iteration 225/1000 | Loss: 0.00001249
Iteration 226/1000 | Loss: 0.00001249
Iteration 227/1000 | Loss: 0.00001249
Iteration 228/1000 | Loss: 0.00001249
Iteration 229/1000 | Loss: 0.00001249
Iteration 230/1000 | Loss: 0.00001249
Iteration 231/1000 | Loss: 0.00001249
Iteration 232/1000 | Loss: 0.00001249
Iteration 233/1000 | Loss: 0.00001249
Iteration 234/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.2490490007621702e-05, 1.2490490007621702e-05, 1.2490490007621702e-05, 1.2490490007621702e-05, 1.2490490007621702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2490490007621702e-05

Optimization complete. Final v2v error: 2.9747135639190674 mm

Highest mean error: 3.7340173721313477 mm for frame 178

Lowest mean error: 2.413851737976074 mm for frame 200

Saving results

Total time: 89.80084204673767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044579
Iteration 2/25 | Loss: 0.00164776
Iteration 3/25 | Loss: 0.00135237
Iteration 4/25 | Loss: 0.00123217
Iteration 5/25 | Loss: 0.00117377
Iteration 6/25 | Loss: 0.00127500
Iteration 7/25 | Loss: 0.00098665
Iteration 8/25 | Loss: 0.00093589
Iteration 9/25 | Loss: 0.00091958
Iteration 10/25 | Loss: 0.00090719
Iteration 11/25 | Loss: 0.00090583
Iteration 12/25 | Loss: 0.00089687
Iteration 13/25 | Loss: 0.00088302
Iteration 14/25 | Loss: 0.00087071
Iteration 15/25 | Loss: 0.00087376
Iteration 16/25 | Loss: 0.00087003
Iteration 17/25 | Loss: 0.00086735
Iteration 18/25 | Loss: 0.00086220
Iteration 19/25 | Loss: 0.00086020
Iteration 20/25 | Loss: 0.00085944
Iteration 21/25 | Loss: 0.00085890
Iteration 22/25 | Loss: 0.00085994
Iteration 23/25 | Loss: 0.00085756
Iteration 24/25 | Loss: 0.00085450
Iteration 25/25 | Loss: 0.00085344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44582319
Iteration 2/25 | Loss: 0.00047460
Iteration 3/25 | Loss: 0.00047460
Iteration 4/25 | Loss: 0.00047460
Iteration 5/25 | Loss: 0.00047460
Iteration 6/25 | Loss: 0.00047460
Iteration 7/25 | Loss: 0.00047460
Iteration 8/25 | Loss: 0.00047460
Iteration 9/25 | Loss: 0.00047460
Iteration 10/25 | Loss: 0.00047460
Iteration 11/25 | Loss: 0.00047460
Iteration 12/25 | Loss: 0.00047460
Iteration 13/25 | Loss: 0.00047460
Iteration 14/25 | Loss: 0.00047460
Iteration 15/25 | Loss: 0.00047460
Iteration 16/25 | Loss: 0.00047460
Iteration 17/25 | Loss: 0.00047460
Iteration 18/25 | Loss: 0.00047460
Iteration 19/25 | Loss: 0.00047460
Iteration 20/25 | Loss: 0.00047460
Iteration 21/25 | Loss: 0.00047460
Iteration 22/25 | Loss: 0.00047460
Iteration 23/25 | Loss: 0.00047460
Iteration 24/25 | Loss: 0.00047460
Iteration 25/25 | Loss: 0.00047460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047460
Iteration 2/1000 | Loss: 0.00057278
Iteration 3/1000 | Loss: 0.00002896
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001032
Iteration 8/1000 | Loss: 0.00001007
Iteration 9/1000 | Loss: 0.00017160
Iteration 10/1000 | Loss: 0.00179774
Iteration 11/1000 | Loss: 0.00015223
Iteration 12/1000 | Loss: 0.00018654
Iteration 13/1000 | Loss: 0.00017260
Iteration 14/1000 | Loss: 0.00009351
Iteration 15/1000 | Loss: 0.00002450
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00000982
Iteration 19/1000 | Loss: 0.00000972
Iteration 20/1000 | Loss: 0.00000971
Iteration 21/1000 | Loss: 0.00000964
Iteration 22/1000 | Loss: 0.00000963
Iteration 23/1000 | Loss: 0.00000962
Iteration 24/1000 | Loss: 0.00000962
Iteration 25/1000 | Loss: 0.00000961
Iteration 26/1000 | Loss: 0.00000960
Iteration 27/1000 | Loss: 0.00000960
Iteration 28/1000 | Loss: 0.00000959
Iteration 29/1000 | Loss: 0.00000958
Iteration 30/1000 | Loss: 0.00000958
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000957
Iteration 33/1000 | Loss: 0.00000957
Iteration 34/1000 | Loss: 0.00000956
Iteration 35/1000 | Loss: 0.00000956
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000955
Iteration 38/1000 | Loss: 0.00000955
Iteration 39/1000 | Loss: 0.00000954
Iteration 40/1000 | Loss: 0.00000954
Iteration 41/1000 | Loss: 0.00000954
Iteration 42/1000 | Loss: 0.00000954
Iteration 43/1000 | Loss: 0.00000954
Iteration 44/1000 | Loss: 0.00000954
Iteration 45/1000 | Loss: 0.00000954
Iteration 46/1000 | Loss: 0.00000954
Iteration 47/1000 | Loss: 0.00000954
Iteration 48/1000 | Loss: 0.00000953
Iteration 49/1000 | Loss: 0.00000952
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00000952
Iteration 52/1000 | Loss: 0.00000951
Iteration 53/1000 | Loss: 0.00000951
Iteration 54/1000 | Loss: 0.00000951
Iteration 55/1000 | Loss: 0.00000950
Iteration 56/1000 | Loss: 0.00000950
Iteration 57/1000 | Loss: 0.00000950
Iteration 58/1000 | Loss: 0.00000950
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000949
Iteration 71/1000 | Loss: 0.00000949
Iteration 72/1000 | Loss: 0.00000949
Iteration 73/1000 | Loss: 0.00000949
Iteration 74/1000 | Loss: 0.00000949
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000947
Iteration 81/1000 | Loss: 0.00000947
Iteration 82/1000 | Loss: 0.00000947
Iteration 83/1000 | Loss: 0.00000947
Iteration 84/1000 | Loss: 0.00000947
Iteration 85/1000 | Loss: 0.00000946
Iteration 86/1000 | Loss: 0.00000946
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000945
Iteration 89/1000 | Loss: 0.00000945
Iteration 90/1000 | Loss: 0.00000945
Iteration 91/1000 | Loss: 0.00000945
Iteration 92/1000 | Loss: 0.00000945
Iteration 93/1000 | Loss: 0.00000945
Iteration 94/1000 | Loss: 0.00000945
Iteration 95/1000 | Loss: 0.00000944
Iteration 96/1000 | Loss: 0.00000944
Iteration 97/1000 | Loss: 0.00000944
Iteration 98/1000 | Loss: 0.00000944
Iteration 99/1000 | Loss: 0.00000944
Iteration 100/1000 | Loss: 0.00000944
Iteration 101/1000 | Loss: 0.00000944
Iteration 102/1000 | Loss: 0.00000943
Iteration 103/1000 | Loss: 0.00000943
Iteration 104/1000 | Loss: 0.00000943
Iteration 105/1000 | Loss: 0.00000943
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000942
Iteration 109/1000 | Loss: 0.00000942
Iteration 110/1000 | Loss: 0.00000942
Iteration 111/1000 | Loss: 0.00000942
Iteration 112/1000 | Loss: 0.00000942
Iteration 113/1000 | Loss: 0.00000942
Iteration 114/1000 | Loss: 0.00000942
Iteration 115/1000 | Loss: 0.00000942
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000942
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000939
Iteration 123/1000 | Loss: 0.00000938
Iteration 124/1000 | Loss: 0.00000938
Iteration 125/1000 | Loss: 0.00000938
Iteration 126/1000 | Loss: 0.00000938
Iteration 127/1000 | Loss: 0.00000938
Iteration 128/1000 | Loss: 0.00000937
Iteration 129/1000 | Loss: 0.00000936
Iteration 130/1000 | Loss: 0.00000930
Iteration 131/1000 | Loss: 0.00000930
Iteration 132/1000 | Loss: 0.00000929
Iteration 133/1000 | Loss: 0.00000929
Iteration 134/1000 | Loss: 0.00000929
Iteration 135/1000 | Loss: 0.00000929
Iteration 136/1000 | Loss: 0.00000929
Iteration 137/1000 | Loss: 0.00000928
Iteration 138/1000 | Loss: 0.00000928
Iteration 139/1000 | Loss: 0.00000928
Iteration 140/1000 | Loss: 0.00000928
Iteration 141/1000 | Loss: 0.00000928
Iteration 142/1000 | Loss: 0.00000928
Iteration 143/1000 | Loss: 0.00000928
Iteration 144/1000 | Loss: 0.00000927
Iteration 145/1000 | Loss: 0.00000927
Iteration 146/1000 | Loss: 0.00000926
Iteration 147/1000 | Loss: 0.00000926
Iteration 148/1000 | Loss: 0.00000926
Iteration 149/1000 | Loss: 0.00000926
Iteration 150/1000 | Loss: 0.00000926
Iteration 151/1000 | Loss: 0.00000926
Iteration 152/1000 | Loss: 0.00000926
Iteration 153/1000 | Loss: 0.00000926
Iteration 154/1000 | Loss: 0.00000926
Iteration 155/1000 | Loss: 0.00000926
Iteration 156/1000 | Loss: 0.00000926
Iteration 157/1000 | Loss: 0.00000926
Iteration 158/1000 | Loss: 0.00000926
Iteration 159/1000 | Loss: 0.00000926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [9.26442771742586e-06, 9.26442771742586e-06, 9.26442771742586e-06, 9.26442771742586e-06, 9.26442771742586e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.26442771742586e-06

Optimization complete. Final v2v error: 2.5740888118743896 mm

Highest mean error: 3.1385176181793213 mm for frame 49

Lowest mean error: 2.2394042015075684 mm for frame 20

Saving results

Total time: 82.80878067016602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488395
Iteration 2/25 | Loss: 0.00120619
Iteration 3/25 | Loss: 0.00095930
Iteration 4/25 | Loss: 0.00094855
Iteration 5/25 | Loss: 0.00094667
Iteration 6/25 | Loss: 0.00094636
Iteration 7/25 | Loss: 0.00094636
Iteration 8/25 | Loss: 0.00094636
Iteration 9/25 | Loss: 0.00094636
Iteration 10/25 | Loss: 0.00094636
Iteration 11/25 | Loss: 0.00094636
Iteration 12/25 | Loss: 0.00094636
Iteration 13/25 | Loss: 0.00094636
Iteration 14/25 | Loss: 0.00094636
Iteration 15/25 | Loss: 0.00094636
Iteration 16/25 | Loss: 0.00094636
Iteration 17/25 | Loss: 0.00094636
Iteration 18/25 | Loss: 0.00094636
Iteration 19/25 | Loss: 0.00094636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009463561000302434, 0.0009463561000302434, 0.0009463561000302434, 0.0009463561000302434, 0.0009463561000302434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009463561000302434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39308274
Iteration 2/25 | Loss: 0.00049777
Iteration 3/25 | Loss: 0.00049777
Iteration 4/25 | Loss: 0.00049777
Iteration 5/25 | Loss: 0.00049777
Iteration 6/25 | Loss: 0.00049777
Iteration 7/25 | Loss: 0.00049777
Iteration 8/25 | Loss: 0.00049777
Iteration 9/25 | Loss: 0.00049777
Iteration 10/25 | Loss: 0.00049777
Iteration 11/25 | Loss: 0.00049777
Iteration 12/25 | Loss: 0.00049777
Iteration 13/25 | Loss: 0.00049777
Iteration 14/25 | Loss: 0.00049777
Iteration 15/25 | Loss: 0.00049777
Iteration 16/25 | Loss: 0.00049777
Iteration 17/25 | Loss: 0.00049777
Iteration 18/25 | Loss: 0.00049777
Iteration 19/25 | Loss: 0.00049777
Iteration 20/25 | Loss: 0.00049777
Iteration 21/25 | Loss: 0.00049777
Iteration 22/25 | Loss: 0.00049777
Iteration 23/25 | Loss: 0.00049777
Iteration 24/25 | Loss: 0.00049777
Iteration 25/25 | Loss: 0.00049777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049777
Iteration 2/1000 | Loss: 0.00003275
Iteration 3/1000 | Loss: 0.00002218
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00002010
Iteration 6/1000 | Loss: 0.00001949
Iteration 7/1000 | Loss: 0.00001918
Iteration 8/1000 | Loss: 0.00001901
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001881
Iteration 13/1000 | Loss: 0.00001881
Iteration 14/1000 | Loss: 0.00001880
Iteration 15/1000 | Loss: 0.00001880
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001868
Iteration 19/1000 | Loss: 0.00001868
Iteration 20/1000 | Loss: 0.00001868
Iteration 21/1000 | Loss: 0.00001866
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001866
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001866
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001866
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001865
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001864
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001859
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001857
Iteration 63/1000 | Loss: 0.00001857
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001853
Iteration 71/1000 | Loss: 0.00001853
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001851
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001846
Iteration 83/1000 | Loss: 0.00001846
Iteration 84/1000 | Loss: 0.00001846
Iteration 85/1000 | Loss: 0.00001846
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001846
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001846
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.8464068489265628e-05, 1.8464068489265628e-05, 1.8464068489265628e-05, 1.8464068489265628e-05, 1.8464068489265628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8464068489265628e-05

Optimization complete. Final v2v error: 3.373175621032715 mm

Highest mean error: 3.965299606323242 mm for frame 157

Lowest mean error: 2.994968891143799 mm for frame 5

Saving results

Total time: 34.25098395347595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777149
Iteration 2/25 | Loss: 0.00150494
Iteration 3/25 | Loss: 0.00100373
Iteration 4/25 | Loss: 0.00095805
Iteration 5/25 | Loss: 0.00095222
Iteration 6/25 | Loss: 0.00094992
Iteration 7/25 | Loss: 0.00094921
Iteration 8/25 | Loss: 0.00094921
Iteration 9/25 | Loss: 0.00094921
Iteration 10/25 | Loss: 0.00094921
Iteration 11/25 | Loss: 0.00094921
Iteration 12/25 | Loss: 0.00094921
Iteration 13/25 | Loss: 0.00094921
Iteration 14/25 | Loss: 0.00094921
Iteration 15/25 | Loss: 0.00094921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009492094395682216, 0.0009492094395682216, 0.0009492094395682216, 0.0009492094395682216, 0.0009492094395682216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009492094395682216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26732135
Iteration 2/25 | Loss: 0.00037235
Iteration 3/25 | Loss: 0.00037235
Iteration 4/25 | Loss: 0.00037235
Iteration 5/25 | Loss: 0.00037235
Iteration 6/25 | Loss: 0.00037235
Iteration 7/25 | Loss: 0.00037235
Iteration 8/25 | Loss: 0.00037235
Iteration 9/25 | Loss: 0.00037235
Iteration 10/25 | Loss: 0.00037235
Iteration 11/25 | Loss: 0.00037235
Iteration 12/25 | Loss: 0.00037235
Iteration 13/25 | Loss: 0.00037235
Iteration 14/25 | Loss: 0.00037235
Iteration 15/25 | Loss: 0.00037235
Iteration 16/25 | Loss: 0.00037235
Iteration 17/25 | Loss: 0.00037235
Iteration 18/25 | Loss: 0.00037235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003723502450156957, 0.0003723502450156957, 0.0003723502450156957, 0.0003723502450156957, 0.0003723502450156957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003723502450156957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037235
Iteration 2/1000 | Loss: 0.00004800
Iteration 3/1000 | Loss: 0.00002656
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002067
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001922
Iteration 8/1000 | Loss: 0.00001856
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001765
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001693
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001689
Iteration 20/1000 | Loss: 0.00001685
Iteration 21/1000 | Loss: 0.00001684
Iteration 22/1000 | Loss: 0.00001683
Iteration 23/1000 | Loss: 0.00001683
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001682
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001679
Iteration 28/1000 | Loss: 0.00001679
Iteration 29/1000 | Loss: 0.00001679
Iteration 30/1000 | Loss: 0.00001679
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001679
Iteration 33/1000 | Loss: 0.00001678
Iteration 34/1000 | Loss: 0.00001678
Iteration 35/1000 | Loss: 0.00001678
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001677
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001675
Iteration 47/1000 | Loss: 0.00001675
Iteration 48/1000 | Loss: 0.00001675
Iteration 49/1000 | Loss: 0.00001675
Iteration 50/1000 | Loss: 0.00001675
Iteration 51/1000 | Loss: 0.00001675
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001674
Iteration 54/1000 | Loss: 0.00001674
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001673
Iteration 58/1000 | Loss: 0.00001673
Iteration 59/1000 | Loss: 0.00001673
Iteration 60/1000 | Loss: 0.00001673
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001672
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001671
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001670
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001669
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001668
Iteration 93/1000 | Loss: 0.00001668
Iteration 94/1000 | Loss: 0.00001668
Iteration 95/1000 | Loss: 0.00001668
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001667
Iteration 106/1000 | Loss: 0.00001667
Iteration 107/1000 | Loss: 0.00001667
Iteration 108/1000 | Loss: 0.00001666
Iteration 109/1000 | Loss: 0.00001666
Iteration 110/1000 | Loss: 0.00001666
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001665
Iteration 115/1000 | Loss: 0.00001665
Iteration 116/1000 | Loss: 0.00001665
Iteration 117/1000 | Loss: 0.00001665
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001665
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001662
Iteration 134/1000 | Loss: 0.00001662
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001662
Iteration 137/1000 | Loss: 0.00001662
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001661
Iteration 140/1000 | Loss: 0.00001661
Iteration 141/1000 | Loss: 0.00001661
Iteration 142/1000 | Loss: 0.00001661
Iteration 143/1000 | Loss: 0.00001661
Iteration 144/1000 | Loss: 0.00001661
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001661
Iteration 147/1000 | Loss: 0.00001661
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001661
Iteration 155/1000 | Loss: 0.00001661
Iteration 156/1000 | Loss: 0.00001661
Iteration 157/1000 | Loss: 0.00001661
Iteration 158/1000 | Loss: 0.00001661
Iteration 159/1000 | Loss: 0.00001661
Iteration 160/1000 | Loss: 0.00001661
Iteration 161/1000 | Loss: 0.00001661
Iteration 162/1000 | Loss: 0.00001661
Iteration 163/1000 | Loss: 0.00001661
Iteration 164/1000 | Loss: 0.00001661
Iteration 165/1000 | Loss: 0.00001661
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.6611684259260073e-05, 1.6611684259260073e-05, 1.6611684259260073e-05, 1.6611684259260073e-05, 1.6611684259260073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6611684259260073e-05

Optimization complete. Final v2v error: 3.396557331085205 mm

Highest mean error: 4.2984843254089355 mm for frame 161

Lowest mean error: 2.879513740539551 mm for frame 181

Saving results

Total time: 39.965901374816895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394510
Iteration 2/25 | Loss: 0.00104129
Iteration 3/25 | Loss: 0.00086834
Iteration 4/25 | Loss: 0.00084739
Iteration 5/25 | Loss: 0.00084128
Iteration 6/25 | Loss: 0.00083903
Iteration 7/25 | Loss: 0.00083844
Iteration 8/25 | Loss: 0.00083844
Iteration 9/25 | Loss: 0.00083844
Iteration 10/25 | Loss: 0.00083844
Iteration 11/25 | Loss: 0.00083844
Iteration 12/25 | Loss: 0.00083844
Iteration 13/25 | Loss: 0.00083844
Iteration 14/25 | Loss: 0.00083844
Iteration 15/25 | Loss: 0.00083844
Iteration 16/25 | Loss: 0.00083844
Iteration 17/25 | Loss: 0.00083844
Iteration 18/25 | Loss: 0.00083844
Iteration 19/25 | Loss: 0.00083844
Iteration 20/25 | Loss: 0.00083844
Iteration 21/25 | Loss: 0.00083844
Iteration 22/25 | Loss: 0.00083844
Iteration 23/25 | Loss: 0.00083844
Iteration 24/25 | Loss: 0.00083844
Iteration 25/25 | Loss: 0.00083844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37897956
Iteration 2/25 | Loss: 0.00055253
Iteration 3/25 | Loss: 0.00055251
Iteration 4/25 | Loss: 0.00055251
Iteration 5/25 | Loss: 0.00055250
Iteration 6/25 | Loss: 0.00055250
Iteration 7/25 | Loss: 0.00055250
Iteration 8/25 | Loss: 0.00055250
Iteration 9/25 | Loss: 0.00055250
Iteration 10/25 | Loss: 0.00055250
Iteration 11/25 | Loss: 0.00055250
Iteration 12/25 | Loss: 0.00055250
Iteration 13/25 | Loss: 0.00055250
Iteration 14/25 | Loss: 0.00055250
Iteration 15/25 | Loss: 0.00055250
Iteration 16/25 | Loss: 0.00055250
Iteration 17/25 | Loss: 0.00055250
Iteration 18/25 | Loss: 0.00055250
Iteration 19/25 | Loss: 0.00055250
Iteration 20/25 | Loss: 0.00055250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005525029264390469, 0.0005525029264390469, 0.0005525029264390469, 0.0005525029264390469, 0.0005525029264390469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005525029264390469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055250
Iteration 2/1000 | Loss: 0.00003215
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001471
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001305
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001178
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001162
Iteration 12/1000 | Loss: 0.00001161
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001159
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001132
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001127
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001127
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001125
Iteration 74/1000 | Loss: 0.00001125
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001123
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001123
Iteration 85/1000 | Loss: 0.00001123
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001122
Iteration 88/1000 | Loss: 0.00001122
Iteration 89/1000 | Loss: 0.00001122
Iteration 90/1000 | Loss: 0.00001122
Iteration 91/1000 | Loss: 0.00001122
Iteration 92/1000 | Loss: 0.00001121
Iteration 93/1000 | Loss: 0.00001121
Iteration 94/1000 | Loss: 0.00001121
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001120
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001114
Iteration 132/1000 | Loss: 0.00001114
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001114
Iteration 138/1000 | Loss: 0.00001114
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001113
Iteration 141/1000 | Loss: 0.00001113
Iteration 142/1000 | Loss: 0.00001113
Iteration 143/1000 | Loss: 0.00001113
Iteration 144/1000 | Loss: 0.00001113
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001113
Iteration 148/1000 | Loss: 0.00001113
Iteration 149/1000 | Loss: 0.00001113
Iteration 150/1000 | Loss: 0.00001113
Iteration 151/1000 | Loss: 0.00001112
Iteration 152/1000 | Loss: 0.00001112
Iteration 153/1000 | Loss: 0.00001112
Iteration 154/1000 | Loss: 0.00001112
Iteration 155/1000 | Loss: 0.00001111
Iteration 156/1000 | Loss: 0.00001111
Iteration 157/1000 | Loss: 0.00001111
Iteration 158/1000 | Loss: 0.00001111
Iteration 159/1000 | Loss: 0.00001111
Iteration 160/1000 | Loss: 0.00001110
Iteration 161/1000 | Loss: 0.00001110
Iteration 162/1000 | Loss: 0.00001110
Iteration 163/1000 | Loss: 0.00001110
Iteration 164/1000 | Loss: 0.00001109
Iteration 165/1000 | Loss: 0.00001109
Iteration 166/1000 | Loss: 0.00001109
Iteration 167/1000 | Loss: 0.00001108
Iteration 168/1000 | Loss: 0.00001108
Iteration 169/1000 | Loss: 0.00001108
Iteration 170/1000 | Loss: 0.00001108
Iteration 171/1000 | Loss: 0.00001108
Iteration 172/1000 | Loss: 0.00001108
Iteration 173/1000 | Loss: 0.00001108
Iteration 174/1000 | Loss: 0.00001107
Iteration 175/1000 | Loss: 0.00001107
Iteration 176/1000 | Loss: 0.00001107
Iteration 177/1000 | Loss: 0.00001107
Iteration 178/1000 | Loss: 0.00001107
Iteration 179/1000 | Loss: 0.00001107
Iteration 180/1000 | Loss: 0.00001107
Iteration 181/1000 | Loss: 0.00001107
Iteration 182/1000 | Loss: 0.00001107
Iteration 183/1000 | Loss: 0.00001107
Iteration 184/1000 | Loss: 0.00001107
Iteration 185/1000 | Loss: 0.00001107
Iteration 186/1000 | Loss: 0.00001107
Iteration 187/1000 | Loss: 0.00001107
Iteration 188/1000 | Loss: 0.00001107
Iteration 189/1000 | Loss: 0.00001107
Iteration 190/1000 | Loss: 0.00001106
Iteration 191/1000 | Loss: 0.00001106
Iteration 192/1000 | Loss: 0.00001106
Iteration 193/1000 | Loss: 0.00001106
Iteration 194/1000 | Loss: 0.00001106
Iteration 195/1000 | Loss: 0.00001106
Iteration 196/1000 | Loss: 0.00001106
Iteration 197/1000 | Loss: 0.00001106
Iteration 198/1000 | Loss: 0.00001106
Iteration 199/1000 | Loss: 0.00001106
Iteration 200/1000 | Loss: 0.00001105
Iteration 201/1000 | Loss: 0.00001105
Iteration 202/1000 | Loss: 0.00001105
Iteration 203/1000 | Loss: 0.00001105
Iteration 204/1000 | Loss: 0.00001105
Iteration 205/1000 | Loss: 0.00001105
Iteration 206/1000 | Loss: 0.00001105
Iteration 207/1000 | Loss: 0.00001105
Iteration 208/1000 | Loss: 0.00001105
Iteration 209/1000 | Loss: 0.00001105
Iteration 210/1000 | Loss: 0.00001105
Iteration 211/1000 | Loss: 0.00001105
Iteration 212/1000 | Loss: 0.00001105
Iteration 213/1000 | Loss: 0.00001105
Iteration 214/1000 | Loss: 0.00001105
Iteration 215/1000 | Loss: 0.00001105
Iteration 216/1000 | Loss: 0.00001105
Iteration 217/1000 | Loss: 0.00001105
Iteration 218/1000 | Loss: 0.00001105
Iteration 219/1000 | Loss: 0.00001105
Iteration 220/1000 | Loss: 0.00001105
Iteration 221/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.1049924069084227e-05, 1.1049924069084227e-05, 1.1049924069084227e-05, 1.1049924069084227e-05, 1.1049924069084227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1049924069084227e-05

Optimization complete. Final v2v error: 2.5796597003936768 mm

Highest mean error: 4.785863399505615 mm for frame 74

Lowest mean error: 1.9766262769699097 mm for frame 111

Saving results

Total time: 41.070088624954224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941425
Iteration 2/25 | Loss: 0.00123134
Iteration 3/25 | Loss: 0.00111317
Iteration 4/25 | Loss: 0.00089456
Iteration 5/25 | Loss: 0.00088659
Iteration 6/25 | Loss: 0.00088354
Iteration 7/25 | Loss: 0.00088214
Iteration 8/25 | Loss: 0.00088639
Iteration 9/25 | Loss: 0.00088533
Iteration 10/25 | Loss: 0.00088309
Iteration 11/25 | Loss: 0.00088234
Iteration 12/25 | Loss: 0.00088212
Iteration 13/25 | Loss: 0.00088358
Iteration 14/25 | Loss: 0.00088302
Iteration 15/25 | Loss: 0.00088316
Iteration 16/25 | Loss: 0.00088214
Iteration 17/25 | Loss: 0.00088226
Iteration 18/25 | Loss: 0.00088206
Iteration 19/25 | Loss: 0.00088228
Iteration 20/25 | Loss: 0.00088196
Iteration 21/25 | Loss: 0.00088222
Iteration 22/25 | Loss: 0.00088194
Iteration 23/25 | Loss: 0.00088206
Iteration 24/25 | Loss: 0.00088215
Iteration 25/25 | Loss: 0.00088190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51443577
Iteration 2/25 | Loss: 0.00049343
Iteration 3/25 | Loss: 0.00042680
Iteration 4/25 | Loss: 0.00042680
Iteration 5/25 | Loss: 0.00042680
Iteration 6/25 | Loss: 0.00042680
Iteration 7/25 | Loss: 0.00042680
Iteration 8/25 | Loss: 0.00042680
Iteration 9/25 | Loss: 0.00042680
Iteration 10/25 | Loss: 0.00042680
Iteration 11/25 | Loss: 0.00042680
Iteration 12/25 | Loss: 0.00042680
Iteration 13/25 | Loss: 0.00042680
Iteration 14/25 | Loss: 0.00042680
Iteration 15/25 | Loss: 0.00042680
Iteration 16/25 | Loss: 0.00042680
Iteration 17/25 | Loss: 0.00042680
Iteration 18/25 | Loss: 0.00042680
Iteration 19/25 | Loss: 0.00042680
Iteration 20/25 | Loss: 0.00042680
Iteration 21/25 | Loss: 0.00042680
Iteration 22/25 | Loss: 0.00042680
Iteration 23/25 | Loss: 0.00042680
Iteration 24/25 | Loss: 0.00042680
Iteration 25/25 | Loss: 0.00042680

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042680
Iteration 2/1000 | Loss: 0.00003232
Iteration 3/1000 | Loss: 0.00018003
Iteration 4/1000 | Loss: 0.00004694
Iteration 5/1000 | Loss: 0.00004042
Iteration 6/1000 | Loss: 0.00004474
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00003687
Iteration 9/1000 | Loss: 0.00003816
Iteration 10/1000 | Loss: 0.00003178
Iteration 11/1000 | Loss: 0.00003569
Iteration 12/1000 | Loss: 0.00003343
Iteration 13/1000 | Loss: 0.00002774
Iteration 14/1000 | Loss: 0.00003663
Iteration 15/1000 | Loss: 0.00002875
Iteration 16/1000 | Loss: 0.00003769
Iteration 17/1000 | Loss: 0.00003310
Iteration 18/1000 | Loss: 0.00003573
Iteration 19/1000 | Loss: 0.00003578
Iteration 20/1000 | Loss: 0.00003351
Iteration 21/1000 | Loss: 0.00002475
Iteration 22/1000 | Loss: 0.00002478
Iteration 23/1000 | Loss: 0.00001735
Iteration 24/1000 | Loss: 0.00003398
Iteration 25/1000 | Loss: 0.00005968
Iteration 26/1000 | Loss: 0.00003867
Iteration 27/1000 | Loss: 0.00004203
Iteration 28/1000 | Loss: 0.00003311
Iteration 29/1000 | Loss: 0.00004025
Iteration 30/1000 | Loss: 0.00004380
Iteration 31/1000 | Loss: 0.00003855
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00004775
Iteration 34/1000 | Loss: 0.00005180
Iteration 35/1000 | Loss: 0.00003767
Iteration 36/1000 | Loss: 0.00003996
Iteration 37/1000 | Loss: 0.00003673
Iteration 38/1000 | Loss: 0.00003476
Iteration 39/1000 | Loss: 0.00003331
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00003026
Iteration 42/1000 | Loss: 0.00001913
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001323
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001293
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001288
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001282
Iteration 108/1000 | Loss: 0.00001282
Iteration 109/1000 | Loss: 0.00001282
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001281
Iteration 112/1000 | Loss: 0.00001281
Iteration 113/1000 | Loss: 0.00001281
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001280
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00002153
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001272
Iteration 154/1000 | Loss: 0.00001272
Iteration 155/1000 | Loss: 0.00001272
Iteration 156/1000 | Loss: 0.00001272
Iteration 157/1000 | Loss: 0.00001272
Iteration 158/1000 | Loss: 0.00001272
Iteration 159/1000 | Loss: 0.00001272
Iteration 160/1000 | Loss: 0.00001272
Iteration 161/1000 | Loss: 0.00001272
Iteration 162/1000 | Loss: 0.00001272
Iteration 163/1000 | Loss: 0.00001272
Iteration 164/1000 | Loss: 0.00001272
Iteration 165/1000 | Loss: 0.00001272
Iteration 166/1000 | Loss: 0.00001271
Iteration 167/1000 | Loss: 0.00001270
Iteration 168/1000 | Loss: 0.00001270
Iteration 169/1000 | Loss: 0.00001270
Iteration 170/1000 | Loss: 0.00001270
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00002019
Iteration 175/1000 | Loss: 0.00001434
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001269
Iteration 178/1000 | Loss: 0.00001269
Iteration 179/1000 | Loss: 0.00001269
Iteration 180/1000 | Loss: 0.00001269
Iteration 181/1000 | Loss: 0.00001269
Iteration 182/1000 | Loss: 0.00001269
Iteration 183/1000 | Loss: 0.00001269
Iteration 184/1000 | Loss: 0.00001269
Iteration 185/1000 | Loss: 0.00001269
Iteration 186/1000 | Loss: 0.00001592
Iteration 187/1000 | Loss: 0.00001397
Iteration 188/1000 | Loss: 0.00001485
Iteration 189/1000 | Loss: 0.00001568
Iteration 190/1000 | Loss: 0.00001268
Iteration 191/1000 | Loss: 0.00001268
Iteration 192/1000 | Loss: 0.00001268
Iteration 193/1000 | Loss: 0.00001268
Iteration 194/1000 | Loss: 0.00001268
Iteration 195/1000 | Loss: 0.00001268
Iteration 196/1000 | Loss: 0.00001268
Iteration 197/1000 | Loss: 0.00001268
Iteration 198/1000 | Loss: 0.00001268
Iteration 199/1000 | Loss: 0.00001268
Iteration 200/1000 | Loss: 0.00001268
Iteration 201/1000 | Loss: 0.00001268
Iteration 202/1000 | Loss: 0.00001268
Iteration 203/1000 | Loss: 0.00001268
Iteration 204/1000 | Loss: 0.00001268
Iteration 205/1000 | Loss: 0.00001268
Iteration 206/1000 | Loss: 0.00001268
Iteration 207/1000 | Loss: 0.00001268
Iteration 208/1000 | Loss: 0.00001268
Iteration 209/1000 | Loss: 0.00001268
Iteration 210/1000 | Loss: 0.00001268
Iteration 211/1000 | Loss: 0.00001268
Iteration 212/1000 | Loss: 0.00001268
Iteration 213/1000 | Loss: 0.00001268
Iteration 214/1000 | Loss: 0.00001268
Iteration 215/1000 | Loss: 0.00001268
Iteration 216/1000 | Loss: 0.00001268
Iteration 217/1000 | Loss: 0.00001268
Iteration 218/1000 | Loss: 0.00001268
Iteration 219/1000 | Loss: 0.00001268
Iteration 220/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.268022333533736e-05, 1.268022333533736e-05, 1.268022333533736e-05, 1.268022333533736e-05, 1.268022333533736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.268022333533736e-05

Optimization complete. Final v2v error: 2.9978373050689697 mm

Highest mean error: 3.6893150806427 mm for frame 39

Lowest mean error: 2.443023920059204 mm for frame 121

Saving results

Total time: 132.50035095214844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856290
Iteration 2/25 | Loss: 0.00094753
Iteration 3/25 | Loss: 0.00081205
Iteration 4/25 | Loss: 0.00079947
Iteration 5/25 | Loss: 0.00079657
Iteration 6/25 | Loss: 0.00079617
Iteration 7/25 | Loss: 0.00079617
Iteration 8/25 | Loss: 0.00079617
Iteration 9/25 | Loss: 0.00079617
Iteration 10/25 | Loss: 0.00079617
Iteration 11/25 | Loss: 0.00079617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007961688097566366, 0.0007961688097566366, 0.0007961688097566366, 0.0007961688097566366, 0.0007961688097566366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007961688097566366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38450789
Iteration 2/25 | Loss: 0.00035437
Iteration 3/25 | Loss: 0.00035434
Iteration 4/25 | Loss: 0.00035434
Iteration 5/25 | Loss: 0.00035434
Iteration 6/25 | Loss: 0.00035434
Iteration 7/25 | Loss: 0.00035434
Iteration 8/25 | Loss: 0.00035434
Iteration 9/25 | Loss: 0.00035434
Iteration 10/25 | Loss: 0.00035434
Iteration 11/25 | Loss: 0.00035434
Iteration 12/25 | Loss: 0.00035434
Iteration 13/25 | Loss: 0.00035434
Iteration 14/25 | Loss: 0.00035434
Iteration 15/25 | Loss: 0.00035434
Iteration 16/25 | Loss: 0.00035434
Iteration 17/25 | Loss: 0.00035434
Iteration 18/25 | Loss: 0.00035434
Iteration 19/25 | Loss: 0.00035434
Iteration 20/25 | Loss: 0.00035434
Iteration 21/25 | Loss: 0.00035434
Iteration 22/25 | Loss: 0.00035434
Iteration 23/25 | Loss: 0.00035434
Iteration 24/25 | Loss: 0.00035434
Iteration 25/25 | Loss: 0.00035434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035434
Iteration 2/1000 | Loss: 0.00001393
Iteration 3/1000 | Loss: 0.00000870
Iteration 4/1000 | Loss: 0.00000763
Iteration 5/1000 | Loss: 0.00000705
Iteration 6/1000 | Loss: 0.00000666
Iteration 7/1000 | Loss: 0.00000663
Iteration 8/1000 | Loss: 0.00000635
Iteration 9/1000 | Loss: 0.00000630
Iteration 10/1000 | Loss: 0.00000628
Iteration 11/1000 | Loss: 0.00000627
Iteration 12/1000 | Loss: 0.00000627
Iteration 13/1000 | Loss: 0.00000626
Iteration 14/1000 | Loss: 0.00000626
Iteration 15/1000 | Loss: 0.00000626
Iteration 16/1000 | Loss: 0.00000626
Iteration 17/1000 | Loss: 0.00000624
Iteration 18/1000 | Loss: 0.00000624
Iteration 19/1000 | Loss: 0.00000623
Iteration 20/1000 | Loss: 0.00000623
Iteration 21/1000 | Loss: 0.00000623
Iteration 22/1000 | Loss: 0.00000622
Iteration 23/1000 | Loss: 0.00000622
Iteration 24/1000 | Loss: 0.00000620
Iteration 25/1000 | Loss: 0.00000619
Iteration 26/1000 | Loss: 0.00000619
Iteration 27/1000 | Loss: 0.00000618
Iteration 28/1000 | Loss: 0.00000613
Iteration 29/1000 | Loss: 0.00000611
Iteration 30/1000 | Loss: 0.00000610
Iteration 31/1000 | Loss: 0.00000609
Iteration 32/1000 | Loss: 0.00000608
Iteration 33/1000 | Loss: 0.00000608
Iteration 34/1000 | Loss: 0.00000608
Iteration 35/1000 | Loss: 0.00000607
Iteration 36/1000 | Loss: 0.00000607
Iteration 37/1000 | Loss: 0.00000606
Iteration 38/1000 | Loss: 0.00000606
Iteration 39/1000 | Loss: 0.00000606
Iteration 40/1000 | Loss: 0.00000605
Iteration 41/1000 | Loss: 0.00000605
Iteration 42/1000 | Loss: 0.00000605
Iteration 43/1000 | Loss: 0.00000605
Iteration 44/1000 | Loss: 0.00000604
Iteration 45/1000 | Loss: 0.00000604
Iteration 46/1000 | Loss: 0.00000603
Iteration 47/1000 | Loss: 0.00000603
Iteration 48/1000 | Loss: 0.00000603
Iteration 49/1000 | Loss: 0.00000603
Iteration 50/1000 | Loss: 0.00000603
Iteration 51/1000 | Loss: 0.00000603
Iteration 52/1000 | Loss: 0.00000602
Iteration 53/1000 | Loss: 0.00000602
Iteration 54/1000 | Loss: 0.00000602
Iteration 55/1000 | Loss: 0.00000601
Iteration 56/1000 | Loss: 0.00000601
Iteration 57/1000 | Loss: 0.00000600
Iteration 58/1000 | Loss: 0.00000600
Iteration 59/1000 | Loss: 0.00000600
Iteration 60/1000 | Loss: 0.00000600
Iteration 61/1000 | Loss: 0.00000599
Iteration 62/1000 | Loss: 0.00000599
Iteration 63/1000 | Loss: 0.00000599
Iteration 64/1000 | Loss: 0.00000599
Iteration 65/1000 | Loss: 0.00000599
Iteration 66/1000 | Loss: 0.00000599
Iteration 67/1000 | Loss: 0.00000599
Iteration 68/1000 | Loss: 0.00000598
Iteration 69/1000 | Loss: 0.00000598
Iteration 70/1000 | Loss: 0.00000598
Iteration 71/1000 | Loss: 0.00000597
Iteration 72/1000 | Loss: 0.00000597
Iteration 73/1000 | Loss: 0.00000597
Iteration 74/1000 | Loss: 0.00000597
Iteration 75/1000 | Loss: 0.00000597
Iteration 76/1000 | Loss: 0.00000597
Iteration 77/1000 | Loss: 0.00000597
Iteration 78/1000 | Loss: 0.00000597
Iteration 79/1000 | Loss: 0.00000597
Iteration 80/1000 | Loss: 0.00000597
Iteration 81/1000 | Loss: 0.00000596
Iteration 82/1000 | Loss: 0.00000596
Iteration 83/1000 | Loss: 0.00000596
Iteration 84/1000 | Loss: 0.00000595
Iteration 85/1000 | Loss: 0.00000595
Iteration 86/1000 | Loss: 0.00000595
Iteration 87/1000 | Loss: 0.00000595
Iteration 88/1000 | Loss: 0.00000595
Iteration 89/1000 | Loss: 0.00000595
Iteration 90/1000 | Loss: 0.00000594
Iteration 91/1000 | Loss: 0.00000594
Iteration 92/1000 | Loss: 0.00000594
Iteration 93/1000 | Loss: 0.00000594
Iteration 94/1000 | Loss: 0.00000594
Iteration 95/1000 | Loss: 0.00000594
Iteration 96/1000 | Loss: 0.00000593
Iteration 97/1000 | Loss: 0.00000593
Iteration 98/1000 | Loss: 0.00000593
Iteration 99/1000 | Loss: 0.00000593
Iteration 100/1000 | Loss: 0.00000593
Iteration 101/1000 | Loss: 0.00000593
Iteration 102/1000 | Loss: 0.00000593
Iteration 103/1000 | Loss: 0.00000593
Iteration 104/1000 | Loss: 0.00000592
Iteration 105/1000 | Loss: 0.00000592
Iteration 106/1000 | Loss: 0.00000592
Iteration 107/1000 | Loss: 0.00000592
Iteration 108/1000 | Loss: 0.00000591
Iteration 109/1000 | Loss: 0.00000591
Iteration 110/1000 | Loss: 0.00000591
Iteration 111/1000 | Loss: 0.00000591
Iteration 112/1000 | Loss: 0.00000591
Iteration 113/1000 | Loss: 0.00000591
Iteration 114/1000 | Loss: 0.00000591
Iteration 115/1000 | Loss: 0.00000591
Iteration 116/1000 | Loss: 0.00000591
Iteration 117/1000 | Loss: 0.00000591
Iteration 118/1000 | Loss: 0.00000591
Iteration 119/1000 | Loss: 0.00000591
Iteration 120/1000 | Loss: 0.00000591
Iteration 121/1000 | Loss: 0.00000591
Iteration 122/1000 | Loss: 0.00000591
Iteration 123/1000 | Loss: 0.00000591
Iteration 124/1000 | Loss: 0.00000591
Iteration 125/1000 | Loss: 0.00000591
Iteration 126/1000 | Loss: 0.00000591
Iteration 127/1000 | Loss: 0.00000590
Iteration 128/1000 | Loss: 0.00000590
Iteration 129/1000 | Loss: 0.00000590
Iteration 130/1000 | Loss: 0.00000589
Iteration 131/1000 | Loss: 0.00000589
Iteration 132/1000 | Loss: 0.00000589
Iteration 133/1000 | Loss: 0.00000589
Iteration 134/1000 | Loss: 0.00000589
Iteration 135/1000 | Loss: 0.00000589
Iteration 136/1000 | Loss: 0.00000589
Iteration 137/1000 | Loss: 0.00000589
Iteration 138/1000 | Loss: 0.00000589
Iteration 139/1000 | Loss: 0.00000589
Iteration 140/1000 | Loss: 0.00000589
Iteration 141/1000 | Loss: 0.00000589
Iteration 142/1000 | Loss: 0.00000589
Iteration 143/1000 | Loss: 0.00000589
Iteration 144/1000 | Loss: 0.00000589
Iteration 145/1000 | Loss: 0.00000589
Iteration 146/1000 | Loss: 0.00000589
Iteration 147/1000 | Loss: 0.00000589
Iteration 148/1000 | Loss: 0.00000589
Iteration 149/1000 | Loss: 0.00000589
Iteration 150/1000 | Loss: 0.00000589
Iteration 151/1000 | Loss: 0.00000589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [5.8887094382953364e-06, 5.8887094382953364e-06, 5.8887094382953364e-06, 5.8887094382953364e-06, 5.8887094382953364e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.8887094382953364e-06

Optimization complete. Final v2v error: 2.094325065612793 mm

Highest mean error: 2.255828380584717 mm for frame 36

Lowest mean error: 1.9573005437850952 mm for frame 147

Saving results

Total time: 30.251859664916992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791763
Iteration 2/25 | Loss: 0.00112777
Iteration 3/25 | Loss: 0.00088829
Iteration 4/25 | Loss: 0.00086721
Iteration 5/25 | Loss: 0.00086512
Iteration 6/25 | Loss: 0.00086512
Iteration 7/25 | Loss: 0.00086512
Iteration 8/25 | Loss: 0.00086512
Iteration 9/25 | Loss: 0.00086512
Iteration 10/25 | Loss: 0.00086512
Iteration 11/25 | Loss: 0.00086512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008651154348626733, 0.0008651154348626733, 0.0008651154348626733, 0.0008651154348626733, 0.0008651154348626733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008651154348626733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36430836
Iteration 2/25 | Loss: 0.00032445
Iteration 3/25 | Loss: 0.00032443
Iteration 4/25 | Loss: 0.00032443
Iteration 5/25 | Loss: 0.00032443
Iteration 6/25 | Loss: 0.00032443
Iteration 7/25 | Loss: 0.00032443
Iteration 8/25 | Loss: 0.00032443
Iteration 9/25 | Loss: 0.00032443
Iteration 10/25 | Loss: 0.00032443
Iteration 11/25 | Loss: 0.00032443
Iteration 12/25 | Loss: 0.00032443
Iteration 13/25 | Loss: 0.00032443
Iteration 14/25 | Loss: 0.00032443
Iteration 15/25 | Loss: 0.00032443
Iteration 16/25 | Loss: 0.00032443
Iteration 17/25 | Loss: 0.00032443
Iteration 18/25 | Loss: 0.00032443
Iteration 19/25 | Loss: 0.00032443
Iteration 20/25 | Loss: 0.00032443
Iteration 21/25 | Loss: 0.00032443
Iteration 22/25 | Loss: 0.00032443
Iteration 23/25 | Loss: 0.00032443
Iteration 24/25 | Loss: 0.00032443
Iteration 25/25 | Loss: 0.00032443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032443
Iteration 2/1000 | Loss: 0.00002497
Iteration 3/1000 | Loss: 0.00001689
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001357
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001210
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001133
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001120
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001116
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00001115
Iteration 30/1000 | Loss: 0.00001115
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001105
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001100
Iteration 63/1000 | Loss: 0.00001100
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001099
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001098
Iteration 70/1000 | Loss: 0.00001098
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001094
Iteration 83/1000 | Loss: 0.00001094
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001094
Iteration 87/1000 | Loss: 0.00001094
Iteration 88/1000 | Loss: 0.00001094
Iteration 89/1000 | Loss: 0.00001094
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001094
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001094
Iteration 106/1000 | Loss: 0.00001094
Iteration 107/1000 | Loss: 0.00001094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.0944597306661308e-05, 1.0944597306661308e-05, 1.0944597306661308e-05, 1.0944597306661308e-05, 1.0944597306661308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0944597306661308e-05

Optimization complete. Final v2v error: 2.7682366371154785 mm

Highest mean error: 3.3055834770202637 mm for frame 234

Lowest mean error: 2.169687509536743 mm for frame 79

Saving results

Total time: 36.697452545166016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019859
Iteration 2/25 | Loss: 0.00113848
Iteration 3/25 | Loss: 0.00094817
Iteration 4/25 | Loss: 0.00089715
Iteration 5/25 | Loss: 0.00089089
Iteration 6/25 | Loss: 0.00088932
Iteration 7/25 | Loss: 0.00088904
Iteration 8/25 | Loss: 0.00088904
Iteration 9/25 | Loss: 0.00088904
Iteration 10/25 | Loss: 0.00088904
Iteration 11/25 | Loss: 0.00088904
Iteration 12/25 | Loss: 0.00088904
Iteration 13/25 | Loss: 0.00088904
Iteration 14/25 | Loss: 0.00088904
Iteration 15/25 | Loss: 0.00088904
Iteration 16/25 | Loss: 0.00088904
Iteration 17/25 | Loss: 0.00088904
Iteration 18/25 | Loss: 0.00088904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008890354074537754, 0.0008890354074537754, 0.0008890354074537754, 0.0008890354074537754, 0.0008890354074537754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008890354074537754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.11019421
Iteration 2/25 | Loss: 0.00052058
Iteration 3/25 | Loss: 0.00052058
Iteration 4/25 | Loss: 0.00052057
Iteration 5/25 | Loss: 0.00052057
Iteration 6/25 | Loss: 0.00052057
Iteration 7/25 | Loss: 0.00052057
Iteration 8/25 | Loss: 0.00052057
Iteration 9/25 | Loss: 0.00052057
Iteration 10/25 | Loss: 0.00052057
Iteration 11/25 | Loss: 0.00052057
Iteration 12/25 | Loss: 0.00052057
Iteration 13/25 | Loss: 0.00052057
Iteration 14/25 | Loss: 0.00052057
Iteration 15/25 | Loss: 0.00052057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005205725319683552, 0.0005205725319683552, 0.0005205725319683552, 0.0005205725319683552, 0.0005205725319683552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005205725319683552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052057
Iteration 2/1000 | Loss: 0.00002094
Iteration 3/1000 | Loss: 0.00001244
Iteration 4/1000 | Loss: 0.00004659
Iteration 5/1000 | Loss: 0.00001101
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001046
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00001025
Iteration 10/1000 | Loss: 0.00001023
Iteration 11/1000 | Loss: 0.00001015
Iteration 12/1000 | Loss: 0.00001013
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001011
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001011
Iteration 22/1000 | Loss: 0.00001011
Iteration 23/1000 | Loss: 0.00001009
Iteration 24/1000 | Loss: 0.00001009
Iteration 25/1000 | Loss: 0.00001008
Iteration 26/1000 | Loss: 0.00001007
Iteration 27/1000 | Loss: 0.00001006
Iteration 28/1000 | Loss: 0.00001006
Iteration 29/1000 | Loss: 0.00001004
Iteration 30/1000 | Loss: 0.00001003
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001003
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001002
Iteration 35/1000 | Loss: 0.00001002
Iteration 36/1000 | Loss: 0.00001001
Iteration 37/1000 | Loss: 0.00001001
Iteration 38/1000 | Loss: 0.00001001
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00001000
Iteration 41/1000 | Loss: 0.00001000
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000999
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000999
Iteration 47/1000 | Loss: 0.00000999
Iteration 48/1000 | Loss: 0.00000999
Iteration 49/1000 | Loss: 0.00000999
Iteration 50/1000 | Loss: 0.00000999
Iteration 51/1000 | Loss: 0.00000999
Iteration 52/1000 | Loss: 0.00000999
Iteration 53/1000 | Loss: 0.00000998
Iteration 54/1000 | Loss: 0.00000998
Iteration 55/1000 | Loss: 0.00000998
Iteration 56/1000 | Loss: 0.00000998
Iteration 57/1000 | Loss: 0.00000997
Iteration 58/1000 | Loss: 0.00000997
Iteration 59/1000 | Loss: 0.00000997
Iteration 60/1000 | Loss: 0.00000997
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000997
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000997
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000996
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000996
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000996
Iteration 75/1000 | Loss: 0.00000996
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000993
Iteration 82/1000 | Loss: 0.00000993
Iteration 83/1000 | Loss: 0.00000992
Iteration 84/1000 | Loss: 0.00000992
Iteration 85/1000 | Loss: 0.00000992
Iteration 86/1000 | Loss: 0.00000992
Iteration 87/1000 | Loss: 0.00000992
Iteration 88/1000 | Loss: 0.00000992
Iteration 89/1000 | Loss: 0.00000992
Iteration 90/1000 | Loss: 0.00000992
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000992
Iteration 93/1000 | Loss: 0.00000992
Iteration 94/1000 | Loss: 0.00000992
Iteration 95/1000 | Loss: 0.00000992
Iteration 96/1000 | Loss: 0.00000991
Iteration 97/1000 | Loss: 0.00000991
Iteration 98/1000 | Loss: 0.00000991
Iteration 99/1000 | Loss: 0.00000991
Iteration 100/1000 | Loss: 0.00000991
Iteration 101/1000 | Loss: 0.00000991
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Iteration 104/1000 | Loss: 0.00000991
Iteration 105/1000 | Loss: 0.00000991
Iteration 106/1000 | Loss: 0.00000991
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000991
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000990
Iteration 116/1000 | Loss: 0.00000990
Iteration 117/1000 | Loss: 0.00000990
Iteration 118/1000 | Loss: 0.00000990
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000990
Iteration 123/1000 | Loss: 0.00000990
Iteration 124/1000 | Loss: 0.00000990
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [9.902589226840064e-06, 9.902589226840064e-06, 9.902589226840064e-06, 9.902589226840064e-06, 9.902589226840064e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.902589226840064e-06

Optimization complete. Final v2v error: 2.6051785945892334 mm

Highest mean error: 3.081267833709717 mm for frame 172

Lowest mean error: 2.274240732192993 mm for frame 138

Saving results

Total time: 32.01081418991089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428360
Iteration 2/25 | Loss: 0.00089436
Iteration 3/25 | Loss: 0.00081506
Iteration 4/25 | Loss: 0.00080299
Iteration 5/25 | Loss: 0.00079896
Iteration 6/25 | Loss: 0.00079795
Iteration 7/25 | Loss: 0.00079795
Iteration 8/25 | Loss: 0.00079795
Iteration 9/25 | Loss: 0.00079795
Iteration 10/25 | Loss: 0.00079795
Iteration 11/25 | Loss: 0.00079795
Iteration 12/25 | Loss: 0.00079795
Iteration 13/25 | Loss: 0.00079795
Iteration 14/25 | Loss: 0.00079795
Iteration 15/25 | Loss: 0.00079795
Iteration 16/25 | Loss: 0.00079795
Iteration 17/25 | Loss: 0.00079795
Iteration 18/25 | Loss: 0.00079795
Iteration 19/25 | Loss: 0.00079795
Iteration 20/25 | Loss: 0.00079795
Iteration 21/25 | Loss: 0.00079795
Iteration 22/25 | Loss: 0.00079795
Iteration 23/25 | Loss: 0.00079795
Iteration 24/25 | Loss: 0.00079795
Iteration 25/25 | Loss: 0.00079795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65291023
Iteration 2/25 | Loss: 0.00046229
Iteration 3/25 | Loss: 0.00046228
Iteration 4/25 | Loss: 0.00046228
Iteration 5/25 | Loss: 0.00046228
Iteration 6/25 | Loss: 0.00046228
Iteration 7/25 | Loss: 0.00046228
Iteration 8/25 | Loss: 0.00046228
Iteration 9/25 | Loss: 0.00046228
Iteration 10/25 | Loss: 0.00046228
Iteration 11/25 | Loss: 0.00046228
Iteration 12/25 | Loss: 0.00046228
Iteration 13/25 | Loss: 0.00046228
Iteration 14/25 | Loss: 0.00046228
Iteration 15/25 | Loss: 0.00046228
Iteration 16/25 | Loss: 0.00046228
Iteration 17/25 | Loss: 0.00046228
Iteration 18/25 | Loss: 0.00046228
Iteration 19/25 | Loss: 0.00046228
Iteration 20/25 | Loss: 0.00046228
Iteration 21/25 | Loss: 0.00046228
Iteration 22/25 | Loss: 0.00046228
Iteration 23/25 | Loss: 0.00046228
Iteration 24/25 | Loss: 0.00046228
Iteration 25/25 | Loss: 0.00046228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046228
Iteration 2/1000 | Loss: 0.00001268
Iteration 3/1000 | Loss: 0.00000974
Iteration 4/1000 | Loss: 0.00000904
Iteration 5/1000 | Loss: 0.00000863
Iteration 6/1000 | Loss: 0.00000849
Iteration 7/1000 | Loss: 0.00000828
Iteration 8/1000 | Loss: 0.00000825
Iteration 9/1000 | Loss: 0.00000824
Iteration 10/1000 | Loss: 0.00000823
Iteration 11/1000 | Loss: 0.00000816
Iteration 12/1000 | Loss: 0.00000802
Iteration 13/1000 | Loss: 0.00000800
Iteration 14/1000 | Loss: 0.00000795
Iteration 15/1000 | Loss: 0.00000795
Iteration 16/1000 | Loss: 0.00000795
Iteration 17/1000 | Loss: 0.00000795
Iteration 18/1000 | Loss: 0.00000795
Iteration 19/1000 | Loss: 0.00000795
Iteration 20/1000 | Loss: 0.00000795
Iteration 21/1000 | Loss: 0.00000794
Iteration 22/1000 | Loss: 0.00000794
Iteration 23/1000 | Loss: 0.00000793
Iteration 24/1000 | Loss: 0.00000793
Iteration 25/1000 | Loss: 0.00000792
Iteration 26/1000 | Loss: 0.00000792
Iteration 27/1000 | Loss: 0.00000791
Iteration 28/1000 | Loss: 0.00000791
Iteration 29/1000 | Loss: 0.00000791
Iteration 30/1000 | Loss: 0.00000790
Iteration 31/1000 | Loss: 0.00000790
Iteration 32/1000 | Loss: 0.00000789
Iteration 33/1000 | Loss: 0.00000789
Iteration 34/1000 | Loss: 0.00000788
Iteration 35/1000 | Loss: 0.00000786
Iteration 36/1000 | Loss: 0.00000785
Iteration 37/1000 | Loss: 0.00000784
Iteration 38/1000 | Loss: 0.00000783
Iteration 39/1000 | Loss: 0.00000783
Iteration 40/1000 | Loss: 0.00000782
Iteration 41/1000 | Loss: 0.00000781
Iteration 42/1000 | Loss: 0.00000781
Iteration 43/1000 | Loss: 0.00000779
Iteration 44/1000 | Loss: 0.00000779
Iteration 45/1000 | Loss: 0.00000778
Iteration 46/1000 | Loss: 0.00000778
Iteration 47/1000 | Loss: 0.00000778
Iteration 48/1000 | Loss: 0.00000777
Iteration 49/1000 | Loss: 0.00000776
Iteration 50/1000 | Loss: 0.00000776
Iteration 51/1000 | Loss: 0.00000776
Iteration 52/1000 | Loss: 0.00000773
Iteration 53/1000 | Loss: 0.00000773
Iteration 54/1000 | Loss: 0.00000773
Iteration 55/1000 | Loss: 0.00000773
Iteration 56/1000 | Loss: 0.00000773
Iteration 57/1000 | Loss: 0.00000773
Iteration 58/1000 | Loss: 0.00000773
Iteration 59/1000 | Loss: 0.00000773
Iteration 60/1000 | Loss: 0.00000773
Iteration 61/1000 | Loss: 0.00000773
Iteration 62/1000 | Loss: 0.00000772
Iteration 63/1000 | Loss: 0.00000772
Iteration 64/1000 | Loss: 0.00000772
Iteration 65/1000 | Loss: 0.00000771
Iteration 66/1000 | Loss: 0.00000771
Iteration 67/1000 | Loss: 0.00000771
Iteration 68/1000 | Loss: 0.00000770
Iteration 69/1000 | Loss: 0.00000770
Iteration 70/1000 | Loss: 0.00000770
Iteration 71/1000 | Loss: 0.00000770
Iteration 72/1000 | Loss: 0.00000770
Iteration 73/1000 | Loss: 0.00000770
Iteration 74/1000 | Loss: 0.00000769
Iteration 75/1000 | Loss: 0.00000769
Iteration 76/1000 | Loss: 0.00000769
Iteration 77/1000 | Loss: 0.00000769
Iteration 78/1000 | Loss: 0.00000768
Iteration 79/1000 | Loss: 0.00000768
Iteration 80/1000 | Loss: 0.00000768
Iteration 81/1000 | Loss: 0.00000768
Iteration 82/1000 | Loss: 0.00000768
Iteration 83/1000 | Loss: 0.00000767
Iteration 84/1000 | Loss: 0.00000767
Iteration 85/1000 | Loss: 0.00000767
Iteration 86/1000 | Loss: 0.00000766
Iteration 87/1000 | Loss: 0.00000765
Iteration 88/1000 | Loss: 0.00000765
Iteration 89/1000 | Loss: 0.00000765
Iteration 90/1000 | Loss: 0.00000765
Iteration 91/1000 | Loss: 0.00000765
Iteration 92/1000 | Loss: 0.00000765
Iteration 93/1000 | Loss: 0.00000765
Iteration 94/1000 | Loss: 0.00000765
Iteration 95/1000 | Loss: 0.00000764
Iteration 96/1000 | Loss: 0.00000764
Iteration 97/1000 | Loss: 0.00000764
Iteration 98/1000 | Loss: 0.00000764
Iteration 99/1000 | Loss: 0.00000764
Iteration 100/1000 | Loss: 0.00000764
Iteration 101/1000 | Loss: 0.00000764
Iteration 102/1000 | Loss: 0.00000763
Iteration 103/1000 | Loss: 0.00000762
Iteration 104/1000 | Loss: 0.00000762
Iteration 105/1000 | Loss: 0.00000761
Iteration 106/1000 | Loss: 0.00000761
Iteration 107/1000 | Loss: 0.00000761
Iteration 108/1000 | Loss: 0.00000761
Iteration 109/1000 | Loss: 0.00000761
Iteration 110/1000 | Loss: 0.00000761
Iteration 111/1000 | Loss: 0.00000761
Iteration 112/1000 | Loss: 0.00000760
Iteration 113/1000 | Loss: 0.00000760
Iteration 114/1000 | Loss: 0.00000760
Iteration 115/1000 | Loss: 0.00000760
Iteration 116/1000 | Loss: 0.00000759
Iteration 117/1000 | Loss: 0.00000759
Iteration 118/1000 | Loss: 0.00000759
Iteration 119/1000 | Loss: 0.00000759
Iteration 120/1000 | Loss: 0.00000759
Iteration 121/1000 | Loss: 0.00000758
Iteration 122/1000 | Loss: 0.00000758
Iteration 123/1000 | Loss: 0.00000758
Iteration 124/1000 | Loss: 0.00000757
Iteration 125/1000 | Loss: 0.00000757
Iteration 126/1000 | Loss: 0.00000757
Iteration 127/1000 | Loss: 0.00000757
Iteration 128/1000 | Loss: 0.00000757
Iteration 129/1000 | Loss: 0.00000756
Iteration 130/1000 | Loss: 0.00000756
Iteration 131/1000 | Loss: 0.00000756
Iteration 132/1000 | Loss: 0.00000756
Iteration 133/1000 | Loss: 0.00000756
Iteration 134/1000 | Loss: 0.00000756
Iteration 135/1000 | Loss: 0.00000756
Iteration 136/1000 | Loss: 0.00000755
Iteration 137/1000 | Loss: 0.00000755
Iteration 138/1000 | Loss: 0.00000755
Iteration 139/1000 | Loss: 0.00000755
Iteration 140/1000 | Loss: 0.00000755
Iteration 141/1000 | Loss: 0.00000754
Iteration 142/1000 | Loss: 0.00000754
Iteration 143/1000 | Loss: 0.00000754
Iteration 144/1000 | Loss: 0.00000754
Iteration 145/1000 | Loss: 0.00000754
Iteration 146/1000 | Loss: 0.00000754
Iteration 147/1000 | Loss: 0.00000754
Iteration 148/1000 | Loss: 0.00000753
Iteration 149/1000 | Loss: 0.00000753
Iteration 150/1000 | Loss: 0.00000753
Iteration 151/1000 | Loss: 0.00000753
Iteration 152/1000 | Loss: 0.00000753
Iteration 153/1000 | Loss: 0.00000753
Iteration 154/1000 | Loss: 0.00000753
Iteration 155/1000 | Loss: 0.00000753
Iteration 156/1000 | Loss: 0.00000753
Iteration 157/1000 | Loss: 0.00000753
Iteration 158/1000 | Loss: 0.00000753
Iteration 159/1000 | Loss: 0.00000753
Iteration 160/1000 | Loss: 0.00000753
Iteration 161/1000 | Loss: 0.00000753
Iteration 162/1000 | Loss: 0.00000753
Iteration 163/1000 | Loss: 0.00000753
Iteration 164/1000 | Loss: 0.00000753
Iteration 165/1000 | Loss: 0.00000753
Iteration 166/1000 | Loss: 0.00000753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [7.528998139605392e-06, 7.528998139605392e-06, 7.528998139605392e-06, 7.528998139605392e-06, 7.528998139605392e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.528998139605392e-06

Optimization complete. Final v2v error: 2.3759162425994873 mm

Highest mean error: 2.795053720474243 mm for frame 45

Lowest mean error: 2.146280527114868 mm for frame 25

Saving results

Total time: 38.33337950706482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374690
Iteration 2/25 | Loss: 0.00096142
Iteration 3/25 | Loss: 0.00087460
Iteration 4/25 | Loss: 0.00085906
Iteration 5/25 | Loss: 0.00085254
Iteration 6/25 | Loss: 0.00085072
Iteration 7/25 | Loss: 0.00085070
Iteration 8/25 | Loss: 0.00085070
Iteration 9/25 | Loss: 0.00085070
Iteration 10/25 | Loss: 0.00085070
Iteration 11/25 | Loss: 0.00085070
Iteration 12/25 | Loss: 0.00085070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008507000748068094, 0.0008507000748068094, 0.0008507000748068094, 0.0008507000748068094, 0.0008507000748068094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008507000748068094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.44480705
Iteration 2/25 | Loss: 0.00057586
Iteration 3/25 | Loss: 0.00057584
Iteration 4/25 | Loss: 0.00057584
Iteration 5/25 | Loss: 0.00057584
Iteration 6/25 | Loss: 0.00057584
Iteration 7/25 | Loss: 0.00057584
Iteration 8/25 | Loss: 0.00057584
Iteration 9/25 | Loss: 0.00057584
Iteration 10/25 | Loss: 0.00057584
Iteration 11/25 | Loss: 0.00057584
Iteration 12/25 | Loss: 0.00057584
Iteration 13/25 | Loss: 0.00057584
Iteration 14/25 | Loss: 0.00057584
Iteration 15/25 | Loss: 0.00057584
Iteration 16/25 | Loss: 0.00057584
Iteration 17/25 | Loss: 0.00057584
Iteration 18/25 | Loss: 0.00057584
Iteration 19/25 | Loss: 0.00057584
Iteration 20/25 | Loss: 0.00057584
Iteration 21/25 | Loss: 0.00057584
Iteration 22/25 | Loss: 0.00057584
Iteration 23/25 | Loss: 0.00057584
Iteration 24/25 | Loss: 0.00057584
Iteration 25/25 | Loss: 0.00057584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057584
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001290
Iteration 4/1000 | Loss: 0.00001178
Iteration 5/1000 | Loss: 0.00001147
Iteration 6/1000 | Loss: 0.00001123
Iteration 7/1000 | Loss: 0.00001113
Iteration 8/1000 | Loss: 0.00001109
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001081
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001053
Iteration 15/1000 | Loss: 0.00001053
Iteration 16/1000 | Loss: 0.00001053
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001049
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001048
Iteration 25/1000 | Loss: 0.00001048
Iteration 26/1000 | Loss: 0.00001048
Iteration 27/1000 | Loss: 0.00001047
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001040
Iteration 53/1000 | Loss: 0.00001040
Iteration 54/1000 | Loss: 0.00001040
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001039
Iteration 58/1000 | Loss: 0.00001039
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001036
Iteration 69/1000 | Loss: 0.00001036
Iteration 70/1000 | Loss: 0.00001036
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001035
Iteration 75/1000 | Loss: 0.00001035
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001035
Iteration 78/1000 | Loss: 0.00001035
Iteration 79/1000 | Loss: 0.00001035
Iteration 80/1000 | Loss: 0.00001035
Iteration 81/1000 | Loss: 0.00001035
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.0353702236898243e-05, 1.0353702236898243e-05, 1.0353702236898243e-05, 1.0353702236898243e-05, 1.0353702236898243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0353702236898243e-05

Optimization complete. Final v2v error: 2.721541166305542 mm

Highest mean error: 3.247048854827881 mm for frame 77

Lowest mean error: 2.348830223083496 mm for frame 54

Saving results

Total time: 28.44317317008972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794277
Iteration 2/25 | Loss: 0.00170739
Iteration 3/25 | Loss: 0.00107722
Iteration 4/25 | Loss: 0.00096791
Iteration 5/25 | Loss: 0.00091938
Iteration 6/25 | Loss: 0.00089598
Iteration 7/25 | Loss: 0.00089624
Iteration 8/25 | Loss: 0.00085956
Iteration 9/25 | Loss: 0.00084316
Iteration 10/25 | Loss: 0.00084128
Iteration 11/25 | Loss: 0.00084304
Iteration 12/25 | Loss: 0.00083372
Iteration 13/25 | Loss: 0.00083060
Iteration 14/25 | Loss: 0.00083456
Iteration 15/25 | Loss: 0.00083199
Iteration 16/25 | Loss: 0.00082848
Iteration 17/25 | Loss: 0.00082840
Iteration 18/25 | Loss: 0.00082751
Iteration 19/25 | Loss: 0.00082878
Iteration 20/25 | Loss: 0.00082569
Iteration 21/25 | Loss: 0.00082329
Iteration 22/25 | Loss: 0.00082737
Iteration 23/25 | Loss: 0.00082280
Iteration 24/25 | Loss: 0.00082260
Iteration 25/25 | Loss: 0.00082243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06057644
Iteration 2/25 | Loss: 0.00053270
Iteration 3/25 | Loss: 0.00053269
Iteration 4/25 | Loss: 0.00053269
Iteration 5/25 | Loss: 0.00053269
Iteration 6/25 | Loss: 0.00053269
Iteration 7/25 | Loss: 0.00053269
Iteration 8/25 | Loss: 0.00053269
Iteration 9/25 | Loss: 0.00053269
Iteration 10/25 | Loss: 0.00053269
Iteration 11/25 | Loss: 0.00053269
Iteration 12/25 | Loss: 0.00053269
Iteration 13/25 | Loss: 0.00053269
Iteration 14/25 | Loss: 0.00053269
Iteration 15/25 | Loss: 0.00053269
Iteration 16/25 | Loss: 0.00053269
Iteration 17/25 | Loss: 0.00053269
Iteration 18/25 | Loss: 0.00053269
Iteration 19/25 | Loss: 0.00053269
Iteration 20/25 | Loss: 0.00053269
Iteration 21/25 | Loss: 0.00053269
Iteration 22/25 | Loss: 0.00053269
Iteration 23/25 | Loss: 0.00053269
Iteration 24/25 | Loss: 0.00053269
Iteration 25/25 | Loss: 0.00053269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053269
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00005963
Iteration 4/1000 | Loss: 0.00016711
Iteration 5/1000 | Loss: 0.00015960
Iteration 6/1000 | Loss: 0.00010700
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00001696
Iteration 9/1000 | Loss: 0.00001773
Iteration 10/1000 | Loss: 0.00001831
Iteration 11/1000 | Loss: 0.00001147
Iteration 12/1000 | Loss: 0.00001056
Iteration 13/1000 | Loss: 0.00001018
Iteration 14/1000 | Loss: 0.00000987
Iteration 15/1000 | Loss: 0.00000963
Iteration 16/1000 | Loss: 0.00000939
Iteration 17/1000 | Loss: 0.00000924
Iteration 18/1000 | Loss: 0.00000919
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000915
Iteration 22/1000 | Loss: 0.00000915
Iteration 23/1000 | Loss: 0.00000915
Iteration 24/1000 | Loss: 0.00000914
Iteration 25/1000 | Loss: 0.00000914
Iteration 26/1000 | Loss: 0.00000913
Iteration 27/1000 | Loss: 0.00000913
Iteration 28/1000 | Loss: 0.00000913
Iteration 29/1000 | Loss: 0.00000912
Iteration 30/1000 | Loss: 0.00000911
Iteration 31/1000 | Loss: 0.00000911
Iteration 32/1000 | Loss: 0.00000911
Iteration 33/1000 | Loss: 0.00000911
Iteration 34/1000 | Loss: 0.00000910
Iteration 35/1000 | Loss: 0.00000910
Iteration 36/1000 | Loss: 0.00000909
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000906
Iteration 40/1000 | Loss: 0.00000906
Iteration 41/1000 | Loss: 0.00000906
Iteration 42/1000 | Loss: 0.00000906
Iteration 43/1000 | Loss: 0.00000906
Iteration 44/1000 | Loss: 0.00000905
Iteration 45/1000 | Loss: 0.00000905
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000904
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000903
Iteration 50/1000 | Loss: 0.00000902
Iteration 51/1000 | Loss: 0.00000902
Iteration 52/1000 | Loss: 0.00000901
Iteration 53/1000 | Loss: 0.00000901
Iteration 54/1000 | Loss: 0.00000901
Iteration 55/1000 | Loss: 0.00000901
Iteration 56/1000 | Loss: 0.00000900
Iteration 57/1000 | Loss: 0.00000900
Iteration 58/1000 | Loss: 0.00000899
Iteration 59/1000 | Loss: 0.00000899
Iteration 60/1000 | Loss: 0.00000898
Iteration 61/1000 | Loss: 0.00000898
Iteration 62/1000 | Loss: 0.00000898
Iteration 63/1000 | Loss: 0.00000898
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000897
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000895
Iteration 70/1000 | Loss: 0.00000895
Iteration 71/1000 | Loss: 0.00000895
Iteration 72/1000 | Loss: 0.00000895
Iteration 73/1000 | Loss: 0.00000895
Iteration 74/1000 | Loss: 0.00000894
Iteration 75/1000 | Loss: 0.00000894
Iteration 76/1000 | Loss: 0.00000894
Iteration 77/1000 | Loss: 0.00000893
Iteration 78/1000 | Loss: 0.00000893
Iteration 79/1000 | Loss: 0.00000893
Iteration 80/1000 | Loss: 0.00000893
Iteration 81/1000 | Loss: 0.00000893
Iteration 82/1000 | Loss: 0.00000893
Iteration 83/1000 | Loss: 0.00000893
Iteration 84/1000 | Loss: 0.00000893
Iteration 85/1000 | Loss: 0.00000893
Iteration 86/1000 | Loss: 0.00000893
Iteration 87/1000 | Loss: 0.00000893
Iteration 88/1000 | Loss: 0.00000893
Iteration 89/1000 | Loss: 0.00000893
Iteration 90/1000 | Loss: 0.00000892
Iteration 91/1000 | Loss: 0.00000892
Iteration 92/1000 | Loss: 0.00000892
Iteration 93/1000 | Loss: 0.00000892
Iteration 94/1000 | Loss: 0.00000892
Iteration 95/1000 | Loss: 0.00000892
Iteration 96/1000 | Loss: 0.00000892
Iteration 97/1000 | Loss: 0.00000892
Iteration 98/1000 | Loss: 0.00000892
Iteration 99/1000 | Loss: 0.00000892
Iteration 100/1000 | Loss: 0.00000892
Iteration 101/1000 | Loss: 0.00000892
Iteration 102/1000 | Loss: 0.00000892
Iteration 103/1000 | Loss: 0.00000892
Iteration 104/1000 | Loss: 0.00000892
Iteration 105/1000 | Loss: 0.00000892
Iteration 106/1000 | Loss: 0.00000892
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000891
Iteration 109/1000 | Loss: 0.00000891
Iteration 110/1000 | Loss: 0.00000891
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000891
Iteration 113/1000 | Loss: 0.00000891
Iteration 114/1000 | Loss: 0.00000891
Iteration 115/1000 | Loss: 0.00000891
Iteration 116/1000 | Loss: 0.00000891
Iteration 117/1000 | Loss: 0.00000891
Iteration 118/1000 | Loss: 0.00000891
Iteration 119/1000 | Loss: 0.00000891
Iteration 120/1000 | Loss: 0.00000891
Iteration 121/1000 | Loss: 0.00000891
Iteration 122/1000 | Loss: 0.00000891
Iteration 123/1000 | Loss: 0.00000891
Iteration 124/1000 | Loss: 0.00000891
Iteration 125/1000 | Loss: 0.00000891
Iteration 126/1000 | Loss: 0.00000891
Iteration 127/1000 | Loss: 0.00000891
Iteration 128/1000 | Loss: 0.00000891
Iteration 129/1000 | Loss: 0.00000890
Iteration 130/1000 | Loss: 0.00000890
Iteration 131/1000 | Loss: 0.00000890
Iteration 132/1000 | Loss: 0.00000890
Iteration 133/1000 | Loss: 0.00000890
Iteration 134/1000 | Loss: 0.00000890
Iteration 135/1000 | Loss: 0.00000890
Iteration 136/1000 | Loss: 0.00000890
Iteration 137/1000 | Loss: 0.00000890
Iteration 138/1000 | Loss: 0.00000890
Iteration 139/1000 | Loss: 0.00000890
Iteration 140/1000 | Loss: 0.00000890
Iteration 141/1000 | Loss: 0.00000890
Iteration 142/1000 | Loss: 0.00000890
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000890
Iteration 147/1000 | Loss: 0.00000890
Iteration 148/1000 | Loss: 0.00000890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [8.899382919480558e-06, 8.899382919480558e-06, 8.899382919480558e-06, 8.899382919480558e-06, 8.899382919480558e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.899382919480558e-06

Optimization complete. Final v2v error: 2.545030355453491 mm

Highest mean error: 3.365480422973633 mm for frame 74

Lowest mean error: 2.2636303901672363 mm for frame 167

Saving results

Total time: 82.32349610328674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378533
Iteration 2/25 | Loss: 0.00101501
Iteration 3/25 | Loss: 0.00089675
Iteration 4/25 | Loss: 0.00088544
Iteration 5/25 | Loss: 0.00088209
Iteration 6/25 | Loss: 0.00088147
Iteration 7/25 | Loss: 0.00088147
Iteration 8/25 | Loss: 0.00088147
Iteration 9/25 | Loss: 0.00088147
Iteration 10/25 | Loss: 0.00088147
Iteration 11/25 | Loss: 0.00088147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008814707980491221, 0.0008814707980491221, 0.0008814707980491221, 0.0008814707980491221, 0.0008814707980491221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008814707980491221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87846047
Iteration 2/25 | Loss: 0.00060410
Iteration 3/25 | Loss: 0.00060410
Iteration 4/25 | Loss: 0.00060409
Iteration 5/25 | Loss: 0.00060409
Iteration 6/25 | Loss: 0.00060409
Iteration 7/25 | Loss: 0.00060409
Iteration 8/25 | Loss: 0.00060409
Iteration 9/25 | Loss: 0.00060409
Iteration 10/25 | Loss: 0.00060409
Iteration 11/25 | Loss: 0.00060409
Iteration 12/25 | Loss: 0.00060409
Iteration 13/25 | Loss: 0.00060409
Iteration 14/25 | Loss: 0.00060409
Iteration 15/25 | Loss: 0.00060409
Iteration 16/25 | Loss: 0.00060409
Iteration 17/25 | Loss: 0.00060409
Iteration 18/25 | Loss: 0.00060409
Iteration 19/25 | Loss: 0.00060409
Iteration 20/25 | Loss: 0.00060409
Iteration 21/25 | Loss: 0.00060409
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006040927255526185, 0.0006040927255526185, 0.0006040927255526185, 0.0006040927255526185, 0.0006040927255526185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006040927255526185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060409
Iteration 2/1000 | Loss: 0.00002852
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001472
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001416
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001368
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001328
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001327
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001324
Iteration 50/1000 | Loss: 0.00001324
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001321
Iteration 57/1000 | Loss: 0.00001321
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001320
Iteration 61/1000 | Loss: 0.00001320
Iteration 62/1000 | Loss: 0.00001320
Iteration 63/1000 | Loss: 0.00001320
Iteration 64/1000 | Loss: 0.00001320
Iteration 65/1000 | Loss: 0.00001320
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001320
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001319
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001312
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001311
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001309
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001309
Iteration 93/1000 | Loss: 0.00001309
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001309
Iteration 98/1000 | Loss: 0.00001309
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001308
Iteration 101/1000 | Loss: 0.00001308
Iteration 102/1000 | Loss: 0.00001308
Iteration 103/1000 | Loss: 0.00001308
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001307
Iteration 107/1000 | Loss: 0.00001307
Iteration 108/1000 | Loss: 0.00001307
Iteration 109/1000 | Loss: 0.00001307
Iteration 110/1000 | Loss: 0.00001307
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001304
Iteration 145/1000 | Loss: 0.00001304
Iteration 146/1000 | Loss: 0.00001304
Iteration 147/1000 | Loss: 0.00001304
Iteration 148/1000 | Loss: 0.00001304
Iteration 149/1000 | Loss: 0.00001304
Iteration 150/1000 | Loss: 0.00001304
Iteration 151/1000 | Loss: 0.00001304
Iteration 152/1000 | Loss: 0.00001304
Iteration 153/1000 | Loss: 0.00001304
Iteration 154/1000 | Loss: 0.00001304
Iteration 155/1000 | Loss: 0.00001304
Iteration 156/1000 | Loss: 0.00001304
Iteration 157/1000 | Loss: 0.00001304
Iteration 158/1000 | Loss: 0.00001304
Iteration 159/1000 | Loss: 0.00001304
Iteration 160/1000 | Loss: 0.00001304
Iteration 161/1000 | Loss: 0.00001304
Iteration 162/1000 | Loss: 0.00001304
Iteration 163/1000 | Loss: 0.00001304
Iteration 164/1000 | Loss: 0.00001304
Iteration 165/1000 | Loss: 0.00001304
Iteration 166/1000 | Loss: 0.00001304
Iteration 167/1000 | Loss: 0.00001304
Iteration 168/1000 | Loss: 0.00001304
Iteration 169/1000 | Loss: 0.00001304
Iteration 170/1000 | Loss: 0.00001304
Iteration 171/1000 | Loss: 0.00001304
Iteration 172/1000 | Loss: 0.00001304
Iteration 173/1000 | Loss: 0.00001304
Iteration 174/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.3042735190538224e-05, 1.3042735190538224e-05, 1.3042735190538224e-05, 1.3042735190538224e-05, 1.3042735190538224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3042735190538224e-05

Optimization complete. Final v2v error: 3.008256673812866 mm

Highest mean error: 3.0973892211914062 mm for frame 2

Lowest mean error: 2.967240333557129 mm for frame 190

Saving results

Total time: 41.362544298172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367219
Iteration 2/25 | Loss: 0.00105710
Iteration 3/25 | Loss: 0.00087186
Iteration 4/25 | Loss: 0.00084388
Iteration 5/25 | Loss: 0.00083960
Iteration 6/25 | Loss: 0.00083859
Iteration 7/25 | Loss: 0.00083859
Iteration 8/25 | Loss: 0.00083859
Iteration 9/25 | Loss: 0.00083859
Iteration 10/25 | Loss: 0.00083859
Iteration 11/25 | Loss: 0.00083859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008385883993469179, 0.0008385883993469179, 0.0008385883993469179, 0.0008385883993469179, 0.0008385883993469179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008385883993469179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37179720
Iteration 2/25 | Loss: 0.00038413
Iteration 3/25 | Loss: 0.00038412
Iteration 4/25 | Loss: 0.00038412
Iteration 5/25 | Loss: 0.00038412
Iteration 6/25 | Loss: 0.00038412
Iteration 7/25 | Loss: 0.00038412
Iteration 8/25 | Loss: 0.00038412
Iteration 9/25 | Loss: 0.00038412
Iteration 10/25 | Loss: 0.00038412
Iteration 11/25 | Loss: 0.00038412
Iteration 12/25 | Loss: 0.00038412
Iteration 13/25 | Loss: 0.00038412
Iteration 14/25 | Loss: 0.00038412
Iteration 15/25 | Loss: 0.00038412
Iteration 16/25 | Loss: 0.00038412
Iteration 17/25 | Loss: 0.00038412
Iteration 18/25 | Loss: 0.00038412
Iteration 19/25 | Loss: 0.00038412
Iteration 20/25 | Loss: 0.00038412
Iteration 21/25 | Loss: 0.00038412
Iteration 22/25 | Loss: 0.00038412
Iteration 23/25 | Loss: 0.00038412
Iteration 24/25 | Loss: 0.00038412
Iteration 25/25 | Loss: 0.00038412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038412
Iteration 2/1000 | Loss: 0.00002461
Iteration 3/1000 | Loss: 0.00001356
Iteration 4/1000 | Loss: 0.00001197
Iteration 5/1000 | Loss: 0.00001127
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001046
Iteration 9/1000 | Loss: 0.00001042
Iteration 10/1000 | Loss: 0.00001041
Iteration 11/1000 | Loss: 0.00001040
Iteration 12/1000 | Loss: 0.00001038
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001031
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001017
Iteration 17/1000 | Loss: 0.00001016
Iteration 18/1000 | Loss: 0.00001015
Iteration 19/1000 | Loss: 0.00001014
Iteration 20/1000 | Loss: 0.00001012
Iteration 21/1000 | Loss: 0.00001012
Iteration 22/1000 | Loss: 0.00001012
Iteration 23/1000 | Loss: 0.00001011
Iteration 24/1000 | Loss: 0.00001010
Iteration 25/1000 | Loss: 0.00001010
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001009
Iteration 28/1000 | Loss: 0.00001007
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001004
Iteration 31/1000 | Loss: 0.00001002
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000993
Iteration 42/1000 | Loss: 0.00000993
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000992
Iteration 45/1000 | Loss: 0.00000992
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000989
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000985
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000983
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000983
Iteration 64/1000 | Loss: 0.00000983
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000983
Iteration 68/1000 | Loss: 0.00000983
Iteration 69/1000 | Loss: 0.00000983
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000982
Iteration 73/1000 | Loss: 0.00000982
Iteration 74/1000 | Loss: 0.00000982
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000981
Iteration 78/1000 | Loss: 0.00000981
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000981
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000980
Iteration 85/1000 | Loss: 0.00000980
Iteration 86/1000 | Loss: 0.00000980
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000977
Iteration 101/1000 | Loss: 0.00000977
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000976
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Iteration 122/1000 | Loss: 0.00000975
Iteration 123/1000 | Loss: 0.00000975
Iteration 124/1000 | Loss: 0.00000975
Iteration 125/1000 | Loss: 0.00000975
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000975
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000974
Iteration 134/1000 | Loss: 0.00000974
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000974
Iteration 141/1000 | Loss: 0.00000974
Iteration 142/1000 | Loss: 0.00000974
Iteration 143/1000 | Loss: 0.00000974
Iteration 144/1000 | Loss: 0.00000974
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000974
Iteration 147/1000 | Loss: 0.00000973
Iteration 148/1000 | Loss: 0.00000973
Iteration 149/1000 | Loss: 0.00000973
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Iteration 165/1000 | Loss: 0.00000973
Iteration 166/1000 | Loss: 0.00000973
Iteration 167/1000 | Loss: 0.00000973
Iteration 168/1000 | Loss: 0.00000973
Iteration 169/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [9.73467558651464e-06, 9.73467558651464e-06, 9.73467558651464e-06, 9.73467558651464e-06, 9.73467558651464e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.73467558651464e-06

Optimization complete. Final v2v error: 2.5868217945098877 mm

Highest mean error: 2.8424232006073 mm for frame 0

Lowest mean error: 2.434579610824585 mm for frame 150

Saving results

Total time: 34.191516160964966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00630313
Iteration 2/25 | Loss: 0.00090402
Iteration 3/25 | Loss: 0.00082336
Iteration 4/25 | Loss: 0.00081006
Iteration 5/25 | Loss: 0.00080597
Iteration 6/25 | Loss: 0.00080473
Iteration 7/25 | Loss: 0.00080473
Iteration 8/25 | Loss: 0.00080473
Iteration 9/25 | Loss: 0.00080473
Iteration 10/25 | Loss: 0.00080473
Iteration 11/25 | Loss: 0.00080473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000804731622338295, 0.000804731622338295, 0.000804731622338295, 0.000804731622338295, 0.000804731622338295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000804731622338295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97514296
Iteration 2/25 | Loss: 0.00043975
Iteration 3/25 | Loss: 0.00043975
Iteration 4/25 | Loss: 0.00043975
Iteration 5/25 | Loss: 0.00043975
Iteration 6/25 | Loss: 0.00043975
Iteration 7/25 | Loss: 0.00043974
Iteration 8/25 | Loss: 0.00043974
Iteration 9/25 | Loss: 0.00043974
Iteration 10/25 | Loss: 0.00043974
Iteration 11/25 | Loss: 0.00043974
Iteration 12/25 | Loss: 0.00043974
Iteration 13/25 | Loss: 0.00043974
Iteration 14/25 | Loss: 0.00043974
Iteration 15/25 | Loss: 0.00043974
Iteration 16/25 | Loss: 0.00043974
Iteration 17/25 | Loss: 0.00043974
Iteration 18/25 | Loss: 0.00043974
Iteration 19/25 | Loss: 0.00043974
Iteration 20/25 | Loss: 0.00043974
Iteration 21/25 | Loss: 0.00043974
Iteration 22/25 | Loss: 0.00043974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00043974409345537424, 0.00043974409345537424, 0.00043974409345537424, 0.00043974409345537424, 0.00043974409345537424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043974409345537424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043974
Iteration 2/1000 | Loss: 0.00001816
Iteration 3/1000 | Loss: 0.00001248
Iteration 4/1000 | Loss: 0.00001125
Iteration 5/1000 | Loss: 0.00001053
Iteration 6/1000 | Loss: 0.00001003
Iteration 7/1000 | Loss: 0.00000961
Iteration 8/1000 | Loss: 0.00000937
Iteration 9/1000 | Loss: 0.00000932
Iteration 10/1000 | Loss: 0.00000920
Iteration 11/1000 | Loss: 0.00000917
Iteration 12/1000 | Loss: 0.00000908
Iteration 13/1000 | Loss: 0.00000905
Iteration 14/1000 | Loss: 0.00000904
Iteration 15/1000 | Loss: 0.00000903
Iteration 16/1000 | Loss: 0.00000900
Iteration 17/1000 | Loss: 0.00000895
Iteration 18/1000 | Loss: 0.00000893
Iteration 19/1000 | Loss: 0.00000892
Iteration 20/1000 | Loss: 0.00000892
Iteration 21/1000 | Loss: 0.00000892
Iteration 22/1000 | Loss: 0.00000891
Iteration 23/1000 | Loss: 0.00000890
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000887
Iteration 26/1000 | Loss: 0.00000887
Iteration 27/1000 | Loss: 0.00000886
Iteration 28/1000 | Loss: 0.00000886
Iteration 29/1000 | Loss: 0.00000885
Iteration 30/1000 | Loss: 0.00000885
Iteration 31/1000 | Loss: 0.00000885
Iteration 32/1000 | Loss: 0.00000885
Iteration 33/1000 | Loss: 0.00000884
Iteration 34/1000 | Loss: 0.00000884
Iteration 35/1000 | Loss: 0.00000884
Iteration 36/1000 | Loss: 0.00000883
Iteration 37/1000 | Loss: 0.00000883
Iteration 38/1000 | Loss: 0.00000883
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000882
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000881
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000880
Iteration 47/1000 | Loss: 0.00000880
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000878
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000877
Iteration 56/1000 | Loss: 0.00000877
Iteration 57/1000 | Loss: 0.00000877
Iteration 58/1000 | Loss: 0.00000876
Iteration 59/1000 | Loss: 0.00000876
Iteration 60/1000 | Loss: 0.00000875
Iteration 61/1000 | Loss: 0.00000875
Iteration 62/1000 | Loss: 0.00000874
Iteration 63/1000 | Loss: 0.00000874
Iteration 64/1000 | Loss: 0.00000873
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000872
Iteration 67/1000 | Loss: 0.00000872
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000872
Iteration 70/1000 | Loss: 0.00000872
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000868
Iteration 77/1000 | Loss: 0.00000868
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000867
Iteration 82/1000 | Loss: 0.00000866
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000864
Iteration 94/1000 | Loss: 0.00000864
Iteration 95/1000 | Loss: 0.00000864
Iteration 96/1000 | Loss: 0.00000864
Iteration 97/1000 | Loss: 0.00000864
Iteration 98/1000 | Loss: 0.00000864
Iteration 99/1000 | Loss: 0.00000863
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000863
Iteration 102/1000 | Loss: 0.00000863
Iteration 103/1000 | Loss: 0.00000863
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000862
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000862
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000862
Iteration 110/1000 | Loss: 0.00000862
Iteration 111/1000 | Loss: 0.00000862
Iteration 112/1000 | Loss: 0.00000862
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000861
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000860
Iteration 118/1000 | Loss: 0.00000860
Iteration 119/1000 | Loss: 0.00000860
Iteration 120/1000 | Loss: 0.00000860
Iteration 121/1000 | Loss: 0.00000859
Iteration 122/1000 | Loss: 0.00000859
Iteration 123/1000 | Loss: 0.00000859
Iteration 124/1000 | Loss: 0.00000859
Iteration 125/1000 | Loss: 0.00000859
Iteration 126/1000 | Loss: 0.00000859
Iteration 127/1000 | Loss: 0.00000859
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [8.591689947934356e-06, 8.591689947934356e-06, 8.591689947934356e-06, 8.591689947934356e-06, 8.591689947934356e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.591689947934356e-06

Optimization complete. Final v2v error: 2.5208539962768555 mm

Highest mean error: 2.9170279502868652 mm for frame 107

Lowest mean error: 2.220113515853882 mm for frame 147

Saving results

Total time: 34.38088274002075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538229
Iteration 2/25 | Loss: 0.00111642
Iteration 3/25 | Loss: 0.00094356
Iteration 4/25 | Loss: 0.00092087
Iteration 5/25 | Loss: 0.00091554
Iteration 6/25 | Loss: 0.00091395
Iteration 7/25 | Loss: 0.00091353
Iteration 8/25 | Loss: 0.00091339
Iteration 9/25 | Loss: 0.00091339
Iteration 10/25 | Loss: 0.00091339
Iteration 11/25 | Loss: 0.00091339
Iteration 12/25 | Loss: 0.00091339
Iteration 13/25 | Loss: 0.00091339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009133886778727174, 0.0009133886778727174, 0.0009133886778727174, 0.0009133886778727174, 0.0009133886778727174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009133886778727174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42686880
Iteration 2/25 | Loss: 0.00054760
Iteration 3/25 | Loss: 0.00054759
Iteration 4/25 | Loss: 0.00054759
Iteration 5/25 | Loss: 0.00054759
Iteration 6/25 | Loss: 0.00054759
Iteration 7/25 | Loss: 0.00054759
Iteration 8/25 | Loss: 0.00054759
Iteration 9/25 | Loss: 0.00054759
Iteration 10/25 | Loss: 0.00054759
Iteration 11/25 | Loss: 0.00054759
Iteration 12/25 | Loss: 0.00054759
Iteration 13/25 | Loss: 0.00054759
Iteration 14/25 | Loss: 0.00054759
Iteration 15/25 | Loss: 0.00054759
Iteration 16/25 | Loss: 0.00054759
Iteration 17/25 | Loss: 0.00054759
Iteration 18/25 | Loss: 0.00054759
Iteration 19/25 | Loss: 0.00054759
Iteration 20/25 | Loss: 0.00054759
Iteration 21/25 | Loss: 0.00054759
Iteration 22/25 | Loss: 0.00054759
Iteration 23/25 | Loss: 0.00054759
Iteration 24/25 | Loss: 0.00054759
Iteration 25/25 | Loss: 0.00054759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054759
Iteration 2/1000 | Loss: 0.00003351
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00002005
Iteration 6/1000 | Loss: 0.00001925
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001758
Iteration 16/1000 | Loss: 0.00001758
Iteration 17/1000 | Loss: 0.00001754
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001749
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001748
Iteration 31/1000 | Loss: 0.00001748
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001747
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001746
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001746
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001746
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001746
Iteration 49/1000 | Loss: 0.00001745
Iteration 50/1000 | Loss: 0.00001745
Iteration 51/1000 | Loss: 0.00001745
Iteration 52/1000 | Loss: 0.00001745
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001744
Iteration 55/1000 | Loss: 0.00001744
Iteration 56/1000 | Loss: 0.00001744
Iteration 57/1000 | Loss: 0.00001744
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001744
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001742
Iteration 68/1000 | Loss: 0.00001742
Iteration 69/1000 | Loss: 0.00001742
Iteration 70/1000 | Loss: 0.00001742
Iteration 71/1000 | Loss: 0.00001741
Iteration 72/1000 | Loss: 0.00001741
Iteration 73/1000 | Loss: 0.00001741
Iteration 74/1000 | Loss: 0.00001741
Iteration 75/1000 | Loss: 0.00001740
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00001740
Iteration 78/1000 | Loss: 0.00001740
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001739
Iteration 84/1000 | Loss: 0.00001739
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001738
Iteration 87/1000 | Loss: 0.00001738
Iteration 88/1000 | Loss: 0.00001738
Iteration 89/1000 | Loss: 0.00001737
Iteration 90/1000 | Loss: 0.00001737
Iteration 91/1000 | Loss: 0.00001737
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001737
Iteration 94/1000 | Loss: 0.00001737
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001735
Iteration 106/1000 | Loss: 0.00001735
Iteration 107/1000 | Loss: 0.00001735
Iteration 108/1000 | Loss: 0.00001735
Iteration 109/1000 | Loss: 0.00001735
Iteration 110/1000 | Loss: 0.00001734
Iteration 111/1000 | Loss: 0.00001734
Iteration 112/1000 | Loss: 0.00001734
Iteration 113/1000 | Loss: 0.00001734
Iteration 114/1000 | Loss: 0.00001734
Iteration 115/1000 | Loss: 0.00001734
Iteration 116/1000 | Loss: 0.00001734
Iteration 117/1000 | Loss: 0.00001734
Iteration 118/1000 | Loss: 0.00001734
Iteration 119/1000 | Loss: 0.00001734
Iteration 120/1000 | Loss: 0.00001734
Iteration 121/1000 | Loss: 0.00001734
Iteration 122/1000 | Loss: 0.00001734
Iteration 123/1000 | Loss: 0.00001734
Iteration 124/1000 | Loss: 0.00001734
Iteration 125/1000 | Loss: 0.00001734
Iteration 126/1000 | Loss: 0.00001734
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.7336995369987562e-05, 1.7336995369987562e-05, 1.7336995369987562e-05, 1.7336995369987562e-05, 1.7336995369987562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7336995369987562e-05

Optimization complete. Final v2v error: 3.3571345806121826 mm

Highest mean error: 4.214056491851807 mm for frame 32

Lowest mean error: 2.4740147590637207 mm for frame 212

Saving results

Total time: 40.1074161529541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110361
Iteration 2/25 | Loss: 0.01110361
Iteration 3/25 | Loss: 0.01110361
Iteration 4/25 | Loss: 0.01110361
Iteration 5/25 | Loss: 0.01110361
Iteration 6/25 | Loss: 0.01110361
Iteration 7/25 | Loss: 0.01110361
Iteration 8/25 | Loss: 0.01110361
Iteration 9/25 | Loss: 0.01110360
Iteration 10/25 | Loss: 0.01110360
Iteration 11/25 | Loss: 0.01110360
Iteration 12/25 | Loss: 0.01110360
Iteration 13/25 | Loss: 0.01110360
Iteration 14/25 | Loss: 0.01110360
Iteration 15/25 | Loss: 0.01110360
Iteration 16/25 | Loss: 0.01110360
Iteration 17/25 | Loss: 0.01110360
Iteration 18/25 | Loss: 0.01110360
Iteration 19/25 | Loss: 0.01110359
Iteration 20/25 | Loss: 0.01110359
Iteration 21/25 | Loss: 0.01110359
Iteration 22/25 | Loss: 0.01110359
Iteration 23/25 | Loss: 0.01110359
Iteration 24/25 | Loss: 0.01110359
Iteration 25/25 | Loss: 0.01110359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74197841
Iteration 2/25 | Loss: 0.15164505
Iteration 3/25 | Loss: 0.15156397
Iteration 4/25 | Loss: 0.15156393
Iteration 5/25 | Loss: 0.15156391
Iteration 6/25 | Loss: 0.15156391
Iteration 7/25 | Loss: 0.15156391
Iteration 8/25 | Loss: 0.15156391
Iteration 9/25 | Loss: 0.15156391
Iteration 10/25 | Loss: 0.15156391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.15156391263008118, 0.15156391263008118, 0.15156391263008118, 0.15156391263008118, 0.15156391263008118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15156391263008118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15156390
Iteration 2/1000 | Loss: 0.00392414
Iteration 3/1000 | Loss: 0.00064731
Iteration 4/1000 | Loss: 0.00038695
Iteration 5/1000 | Loss: 0.00029188
Iteration 6/1000 | Loss: 0.00013253
Iteration 7/1000 | Loss: 0.00034587
Iteration 8/1000 | Loss: 0.00004952
Iteration 9/1000 | Loss: 0.00005592
Iteration 10/1000 | Loss: 0.00008864
Iteration 11/1000 | Loss: 0.00002866
Iteration 12/1000 | Loss: 0.00009669
Iteration 13/1000 | Loss: 0.00004645
Iteration 14/1000 | Loss: 0.00002971
Iteration 15/1000 | Loss: 0.00002188
Iteration 16/1000 | Loss: 0.00004800
Iteration 17/1000 | Loss: 0.00003326
Iteration 18/1000 | Loss: 0.00002238
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00007430
Iteration 21/1000 | Loss: 0.00008654
Iteration 22/1000 | Loss: 0.00005005
Iteration 23/1000 | Loss: 0.00002566
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00010525
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001754
Iteration 28/1000 | Loss: 0.00005878
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00014845
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00012560
Iteration 36/1000 | Loss: 0.00006655
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00006538
Iteration 43/1000 | Loss: 0.00001467
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001414
Iteration 55/1000 | Loss: 0.00001411
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001376
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001373
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001371
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001369
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001368
Iteration 133/1000 | Loss: 0.00001368
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001369
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001368
Iteration 143/1000 | Loss: 0.00001368
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001368
Iteration 148/1000 | Loss: 0.00001367
Iteration 149/1000 | Loss: 0.00001367
Iteration 150/1000 | Loss: 0.00001367
Iteration 151/1000 | Loss: 0.00001366
Iteration 152/1000 | Loss: 0.00001366
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001371
Iteration 158/1000 | Loss: 0.00001370
Iteration 159/1000 | Loss: 0.00001370
Iteration 160/1000 | Loss: 0.00001369
Iteration 161/1000 | Loss: 0.00001369
Iteration 162/1000 | Loss: 0.00001369
Iteration 163/1000 | Loss: 0.00001369
Iteration 164/1000 | Loss: 0.00001369
Iteration 165/1000 | Loss: 0.00005037
Iteration 166/1000 | Loss: 0.00007840
Iteration 167/1000 | Loss: 0.00002393
Iteration 168/1000 | Loss: 0.00002997
Iteration 169/1000 | Loss: 0.00001597
Iteration 170/1000 | Loss: 0.00001371
Iteration 171/1000 | Loss: 0.00001368
Iteration 172/1000 | Loss: 0.00001368
Iteration 173/1000 | Loss: 0.00001368
Iteration 174/1000 | Loss: 0.00003878
Iteration 175/1000 | Loss: 0.00001428
Iteration 176/1000 | Loss: 0.00002007
Iteration 177/1000 | Loss: 0.00004618
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001363
Iteration 180/1000 | Loss: 0.00001361
Iteration 181/1000 | Loss: 0.00001361
Iteration 182/1000 | Loss: 0.00001361
Iteration 183/1000 | Loss: 0.00001360
Iteration 184/1000 | Loss: 0.00001360
Iteration 185/1000 | Loss: 0.00001360
Iteration 186/1000 | Loss: 0.00001360
Iteration 187/1000 | Loss: 0.00001360
Iteration 188/1000 | Loss: 0.00001360
Iteration 189/1000 | Loss: 0.00001360
Iteration 190/1000 | Loss: 0.00001371
Iteration 191/1000 | Loss: 0.00001360
Iteration 192/1000 | Loss: 0.00001359
Iteration 193/1000 | Loss: 0.00001359
Iteration 194/1000 | Loss: 0.00001359
Iteration 195/1000 | Loss: 0.00001359
Iteration 196/1000 | Loss: 0.00001359
Iteration 197/1000 | Loss: 0.00001359
Iteration 198/1000 | Loss: 0.00001359
Iteration 199/1000 | Loss: 0.00001359
Iteration 200/1000 | Loss: 0.00001359
Iteration 201/1000 | Loss: 0.00001359
Iteration 202/1000 | Loss: 0.00001370
Iteration 203/1000 | Loss: 0.00001370
Iteration 204/1000 | Loss: 0.00001370
Iteration 205/1000 | Loss: 0.00001370
Iteration 206/1000 | Loss: 0.00001370
Iteration 207/1000 | Loss: 0.00001369
Iteration 208/1000 | Loss: 0.00001368
Iteration 209/1000 | Loss: 0.00001368
Iteration 210/1000 | Loss: 0.00001368
Iteration 211/1000 | Loss: 0.00001367
Iteration 212/1000 | Loss: 0.00001367
Iteration 213/1000 | Loss: 0.00001366
Iteration 214/1000 | Loss: 0.00001366
Iteration 215/1000 | Loss: 0.00001377
Iteration 216/1000 | Loss: 0.00002411
Iteration 217/1000 | Loss: 0.00004426
Iteration 218/1000 | Loss: 0.00005219
Iteration 219/1000 | Loss: 0.00001365
Iteration 220/1000 | Loss: 0.00001770
Iteration 221/1000 | Loss: 0.00001775
Iteration 222/1000 | Loss: 0.00019825
Iteration 223/1000 | Loss: 0.00002138
Iteration 224/1000 | Loss: 0.00001699
Iteration 225/1000 | Loss: 0.00003065
Iteration 226/1000 | Loss: 0.00010700
Iteration 227/1000 | Loss: 0.00003105
Iteration 228/1000 | Loss: 0.00002447
Iteration 229/1000 | Loss: 0.00002395
Iteration 230/1000 | Loss: 0.00001490
Iteration 231/1000 | Loss: 0.00001644
Iteration 232/1000 | Loss: 0.00001453
Iteration 233/1000 | Loss: 0.00001941
Iteration 234/1000 | Loss: 0.00005455
Iteration 235/1000 | Loss: 0.00001402
Iteration 236/1000 | Loss: 0.00001360
Iteration 237/1000 | Loss: 0.00001360
Iteration 238/1000 | Loss: 0.00001714
Iteration 239/1000 | Loss: 0.00001358
Iteration 240/1000 | Loss: 0.00001358
Iteration 241/1000 | Loss: 0.00001358
Iteration 242/1000 | Loss: 0.00001358
Iteration 243/1000 | Loss: 0.00001357
Iteration 244/1000 | Loss: 0.00001357
Iteration 245/1000 | Loss: 0.00001357
Iteration 246/1000 | Loss: 0.00001357
Iteration 247/1000 | Loss: 0.00001357
Iteration 248/1000 | Loss: 0.00001357
Iteration 249/1000 | Loss: 0.00001357
Iteration 250/1000 | Loss: 0.00001357
Iteration 251/1000 | Loss: 0.00001356
Iteration 252/1000 | Loss: 0.00001355
Iteration 253/1000 | Loss: 0.00001355
Iteration 254/1000 | Loss: 0.00001355
Iteration 255/1000 | Loss: 0.00001355
Iteration 256/1000 | Loss: 0.00001355
Iteration 257/1000 | Loss: 0.00001355
Iteration 258/1000 | Loss: 0.00001355
Iteration 259/1000 | Loss: 0.00001355
Iteration 260/1000 | Loss: 0.00001355
Iteration 261/1000 | Loss: 0.00001355
Iteration 262/1000 | Loss: 0.00001355
Iteration 263/1000 | Loss: 0.00001355
Iteration 264/1000 | Loss: 0.00001355
Iteration 265/1000 | Loss: 0.00001355
Iteration 266/1000 | Loss: 0.00001355
Iteration 267/1000 | Loss: 0.00001355
Iteration 268/1000 | Loss: 0.00001354
Iteration 269/1000 | Loss: 0.00001354
Iteration 270/1000 | Loss: 0.00001354
Iteration 271/1000 | Loss: 0.00001354
Iteration 272/1000 | Loss: 0.00001354
Iteration 273/1000 | Loss: 0.00001354
Iteration 274/1000 | Loss: 0.00001354
Iteration 275/1000 | Loss: 0.00001354
Iteration 276/1000 | Loss: 0.00001353
Iteration 277/1000 | Loss: 0.00001353
Iteration 278/1000 | Loss: 0.00001353
Iteration 279/1000 | Loss: 0.00001353
Iteration 280/1000 | Loss: 0.00001353
Iteration 281/1000 | Loss: 0.00001353
Iteration 282/1000 | Loss: 0.00001353
Iteration 283/1000 | Loss: 0.00001353
Iteration 284/1000 | Loss: 0.00001353
Iteration 285/1000 | Loss: 0.00001353
Iteration 286/1000 | Loss: 0.00001353
Iteration 287/1000 | Loss: 0.00001353
Iteration 288/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.3530580872611608e-05, 1.3530580872611608e-05, 1.3530580872611608e-05, 1.3530580872611608e-05, 1.3530580872611608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3530580872611608e-05

Optimization complete. Final v2v error: 3.0473194122314453 mm

Highest mean error: 8.765900611877441 mm for frame 121

Lowest mean error: 2.6574325561523438 mm for frame 50

Saving results

Total time: 145.01180291175842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077423
Iteration 2/25 | Loss: 0.01077423
Iteration 3/25 | Loss: 0.01077423
Iteration 4/25 | Loss: 0.01077423
Iteration 5/25 | Loss: 0.01077423
Iteration 6/25 | Loss: 0.01077423
Iteration 7/25 | Loss: 0.01077423
Iteration 8/25 | Loss: 0.01077422
Iteration 9/25 | Loss: 0.01077422
Iteration 10/25 | Loss: 0.01077422
Iteration 11/25 | Loss: 0.01077422
Iteration 12/25 | Loss: 0.01077421
Iteration 13/25 | Loss: 0.01077421
Iteration 14/25 | Loss: 0.01077421
Iteration 15/25 | Loss: 0.01077421
Iteration 16/25 | Loss: 0.01077421
Iteration 17/25 | Loss: 0.01077420
Iteration 18/25 | Loss: 0.01077420
Iteration 19/25 | Loss: 0.01077420
Iteration 20/25 | Loss: 0.01077420
Iteration 21/25 | Loss: 0.01077420
Iteration 22/25 | Loss: 0.01077419
Iteration 23/25 | Loss: 0.01077419
Iteration 24/25 | Loss: 0.01077419
Iteration 25/25 | Loss: 0.01077419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68355119
Iteration 2/25 | Loss: 0.07242961
Iteration 3/25 | Loss: 0.06274493
Iteration 4/25 | Loss: 0.06244377
Iteration 5/25 | Loss: 0.06241334
Iteration 6/25 | Loss: 0.06241334
Iteration 7/25 | Loss: 0.06241334
Iteration 8/25 | Loss: 0.06241334
Iteration 9/25 | Loss: 0.06241334
Iteration 10/25 | Loss: 0.06241334
Iteration 11/25 | Loss: 0.06241334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06241333857178688, 0.06241333857178688, 0.06241333857178688, 0.06241333857178688, 0.06241333857178688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06241333857178688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06241334
Iteration 2/1000 | Loss: 0.01261364
Iteration 3/1000 | Loss: 0.01134747
Iteration 4/1000 | Loss: 0.01272360
Iteration 5/1000 | Loss: 0.00420593
Iteration 6/1000 | Loss: 0.00312663
Iteration 7/1000 | Loss: 0.00631877
Iteration 8/1000 | Loss: 0.00379886
Iteration 9/1000 | Loss: 0.00397240
Iteration 10/1000 | Loss: 0.00162905
Iteration 11/1000 | Loss: 0.00062035
Iteration 12/1000 | Loss: 0.00027352
Iteration 13/1000 | Loss: 0.00035356
Iteration 14/1000 | Loss: 0.00023448
Iteration 15/1000 | Loss: 0.00093856
Iteration 16/1000 | Loss: 0.00031834
Iteration 17/1000 | Loss: 0.00034122
Iteration 18/1000 | Loss: 0.00013952
Iteration 19/1000 | Loss: 0.00015495
Iteration 20/1000 | Loss: 0.00014536
Iteration 21/1000 | Loss: 0.00045119
Iteration 22/1000 | Loss: 0.00046046
Iteration 23/1000 | Loss: 0.00035682
Iteration 24/1000 | Loss: 0.00018624
Iteration 25/1000 | Loss: 0.00036138
Iteration 26/1000 | Loss: 0.00023528
Iteration 27/1000 | Loss: 0.00085131
Iteration 28/1000 | Loss: 0.00030173
Iteration 29/1000 | Loss: 0.00018107
Iteration 30/1000 | Loss: 0.00019773
Iteration 31/1000 | Loss: 0.00035612
Iteration 32/1000 | Loss: 0.00015567
Iteration 33/1000 | Loss: 0.00019822
Iteration 34/1000 | Loss: 0.00015254
Iteration 35/1000 | Loss: 0.00008263
Iteration 36/1000 | Loss: 0.00008335
Iteration 37/1000 | Loss: 0.00054938
Iteration 38/1000 | Loss: 0.00062311
Iteration 39/1000 | Loss: 0.00048878
Iteration 40/1000 | Loss: 0.00013273
Iteration 41/1000 | Loss: 0.00046939
Iteration 42/1000 | Loss: 0.00040676
Iteration 43/1000 | Loss: 0.00007634
Iteration 44/1000 | Loss: 0.00006933
Iteration 45/1000 | Loss: 0.00011700
Iteration 46/1000 | Loss: 0.00015459
Iteration 47/1000 | Loss: 0.00006290
Iteration 48/1000 | Loss: 0.00026144
Iteration 49/1000 | Loss: 0.00032474
Iteration 50/1000 | Loss: 0.00013908
Iteration 51/1000 | Loss: 0.00009330
Iteration 52/1000 | Loss: 0.00018080
Iteration 53/1000 | Loss: 0.00010762
Iteration 54/1000 | Loss: 0.00008426
Iteration 55/1000 | Loss: 0.00036098
Iteration 56/1000 | Loss: 0.00018506
Iteration 57/1000 | Loss: 0.00005790
Iteration 58/1000 | Loss: 0.00008226
Iteration 59/1000 | Loss: 0.00009230
Iteration 60/1000 | Loss: 0.00011838
Iteration 61/1000 | Loss: 0.00007240
Iteration 62/1000 | Loss: 0.00007882
Iteration 63/1000 | Loss: 0.00005746
Iteration 64/1000 | Loss: 0.00026903
Iteration 65/1000 | Loss: 0.00023892
Iteration 66/1000 | Loss: 0.00007195
Iteration 67/1000 | Loss: 0.00014637
Iteration 68/1000 | Loss: 0.00036153
Iteration 69/1000 | Loss: 0.00016780
Iteration 70/1000 | Loss: 0.00007260
Iteration 71/1000 | Loss: 0.00015144
Iteration 72/1000 | Loss: 0.00005847
Iteration 73/1000 | Loss: 0.00005237
Iteration 74/1000 | Loss: 0.00007299
Iteration 75/1000 | Loss: 0.00038074
Iteration 76/1000 | Loss: 0.00014494
Iteration 77/1000 | Loss: 0.00023802
Iteration 78/1000 | Loss: 0.00018168
Iteration 79/1000 | Loss: 0.00018461
Iteration 80/1000 | Loss: 0.00007233
Iteration 81/1000 | Loss: 0.00006522
Iteration 82/1000 | Loss: 0.00051919
Iteration 83/1000 | Loss: 0.00017129
Iteration 84/1000 | Loss: 0.00009766
Iteration 85/1000 | Loss: 0.00031796
Iteration 86/1000 | Loss: 0.00005971
Iteration 87/1000 | Loss: 0.00010049
Iteration 88/1000 | Loss: 0.00005070
Iteration 89/1000 | Loss: 0.00004913
Iteration 90/1000 | Loss: 0.00006369
Iteration 91/1000 | Loss: 0.00004845
Iteration 92/1000 | Loss: 0.00006077
Iteration 93/1000 | Loss: 0.00007183
Iteration 94/1000 | Loss: 0.00005093
Iteration 95/1000 | Loss: 0.00022353
Iteration 96/1000 | Loss: 0.00015270
Iteration 97/1000 | Loss: 0.00018836
Iteration 98/1000 | Loss: 0.00013143
Iteration 99/1000 | Loss: 0.00021906
Iteration 100/1000 | Loss: 0.00014412
Iteration 101/1000 | Loss: 0.00020001
Iteration 102/1000 | Loss: 0.00011788
Iteration 103/1000 | Loss: 0.00018550
Iteration 104/1000 | Loss: 0.00011710
Iteration 105/1000 | Loss: 0.00006342
Iteration 106/1000 | Loss: 0.00005143
Iteration 107/1000 | Loss: 0.00005516
Iteration 108/1000 | Loss: 0.00006611
Iteration 109/1000 | Loss: 0.00005063
Iteration 110/1000 | Loss: 0.00007635
Iteration 111/1000 | Loss: 0.00017860
Iteration 112/1000 | Loss: 0.00006455
Iteration 113/1000 | Loss: 0.00009556
Iteration 114/1000 | Loss: 0.00007356
Iteration 115/1000 | Loss: 0.00009530
Iteration 116/1000 | Loss: 0.00009055
Iteration 117/1000 | Loss: 0.00004971
Iteration 118/1000 | Loss: 0.00005411
Iteration 119/1000 | Loss: 0.00005833
Iteration 120/1000 | Loss: 0.00005206
Iteration 121/1000 | Loss: 0.00006259
Iteration 122/1000 | Loss: 0.00004873
Iteration 123/1000 | Loss: 0.00009805
Iteration 124/1000 | Loss: 0.00004732
Iteration 125/1000 | Loss: 0.00004575
Iteration 126/1000 | Loss: 0.00005805
Iteration 127/1000 | Loss: 0.00004573
Iteration 128/1000 | Loss: 0.00005139
Iteration 129/1000 | Loss: 0.00004739
Iteration 130/1000 | Loss: 0.00004475
Iteration 131/1000 | Loss: 0.00004454
Iteration 132/1000 | Loss: 0.00004452
Iteration 133/1000 | Loss: 0.00006587
Iteration 134/1000 | Loss: 0.00004442
Iteration 135/1000 | Loss: 0.00004695
Iteration 136/1000 | Loss: 0.00005537
Iteration 137/1000 | Loss: 0.00004492
Iteration 138/1000 | Loss: 0.00009322
Iteration 139/1000 | Loss: 0.00025632
Iteration 140/1000 | Loss: 0.00019392
Iteration 141/1000 | Loss: 0.00007258
Iteration 142/1000 | Loss: 0.00018023
Iteration 143/1000 | Loss: 0.00004693
Iteration 144/1000 | Loss: 0.00005753
Iteration 145/1000 | Loss: 0.00004469
Iteration 146/1000 | Loss: 0.00007896
Iteration 147/1000 | Loss: 0.00004468
Iteration 148/1000 | Loss: 0.00004452
Iteration 149/1000 | Loss: 0.00009388
Iteration 150/1000 | Loss: 0.00008183
Iteration 151/1000 | Loss: 0.00026802
Iteration 152/1000 | Loss: 0.00006249
Iteration 153/1000 | Loss: 0.00004853
Iteration 154/1000 | Loss: 0.00005813
Iteration 155/1000 | Loss: 0.00011199
Iteration 156/1000 | Loss: 0.00014384
Iteration 157/1000 | Loss: 0.00015505
Iteration 158/1000 | Loss: 0.00007020
Iteration 159/1000 | Loss: 0.00007294
Iteration 160/1000 | Loss: 0.00007086
Iteration 161/1000 | Loss: 0.00004484
Iteration 162/1000 | Loss: 0.00004456
Iteration 163/1000 | Loss: 0.00009107
Iteration 164/1000 | Loss: 0.00004449
Iteration 165/1000 | Loss: 0.00005024
Iteration 166/1000 | Loss: 0.00004436
Iteration 167/1000 | Loss: 0.00005280
Iteration 168/1000 | Loss: 0.00004488
Iteration 169/1000 | Loss: 0.00004659
Iteration 170/1000 | Loss: 0.00004405
Iteration 171/1000 | Loss: 0.00004573
Iteration 172/1000 | Loss: 0.00004402
Iteration 173/1000 | Loss: 0.00004402
Iteration 174/1000 | Loss: 0.00004402
Iteration 175/1000 | Loss: 0.00004402
Iteration 176/1000 | Loss: 0.00004402
Iteration 177/1000 | Loss: 0.00004402
Iteration 178/1000 | Loss: 0.00004402
Iteration 179/1000 | Loss: 0.00004402
Iteration 180/1000 | Loss: 0.00004402
Iteration 181/1000 | Loss: 0.00004401
Iteration 182/1000 | Loss: 0.00004400
Iteration 183/1000 | Loss: 0.00004400
Iteration 184/1000 | Loss: 0.00004400
Iteration 185/1000 | Loss: 0.00004400
Iteration 186/1000 | Loss: 0.00004399
Iteration 187/1000 | Loss: 0.00004399
Iteration 188/1000 | Loss: 0.00004399
Iteration 189/1000 | Loss: 0.00004399
Iteration 190/1000 | Loss: 0.00004399
Iteration 191/1000 | Loss: 0.00004399
Iteration 192/1000 | Loss: 0.00004398
Iteration 193/1000 | Loss: 0.00004398
Iteration 194/1000 | Loss: 0.00004398
Iteration 195/1000 | Loss: 0.00004398
Iteration 196/1000 | Loss: 0.00004398
Iteration 197/1000 | Loss: 0.00004398
Iteration 198/1000 | Loss: 0.00004398
Iteration 199/1000 | Loss: 0.00004398
Iteration 200/1000 | Loss: 0.00004398
Iteration 201/1000 | Loss: 0.00004398
Iteration 202/1000 | Loss: 0.00004397
Iteration 203/1000 | Loss: 0.00004397
Iteration 204/1000 | Loss: 0.00004397
Iteration 205/1000 | Loss: 0.00004397
Iteration 206/1000 | Loss: 0.00004397
Iteration 207/1000 | Loss: 0.00004396
Iteration 208/1000 | Loss: 0.00004871
Iteration 209/1000 | Loss: 0.00004961
Iteration 210/1000 | Loss: 0.00004399
Iteration 211/1000 | Loss: 0.00004396
Iteration 212/1000 | Loss: 0.00004396
Iteration 213/1000 | Loss: 0.00004396
Iteration 214/1000 | Loss: 0.00004396
Iteration 215/1000 | Loss: 0.00004396
Iteration 216/1000 | Loss: 0.00004396
Iteration 217/1000 | Loss: 0.00004396
Iteration 218/1000 | Loss: 0.00004396
Iteration 219/1000 | Loss: 0.00004396
Iteration 220/1000 | Loss: 0.00004396
Iteration 221/1000 | Loss: 0.00004396
Iteration 222/1000 | Loss: 0.00004395
Iteration 223/1000 | Loss: 0.00004395
Iteration 224/1000 | Loss: 0.00004395
Iteration 225/1000 | Loss: 0.00004395
Iteration 226/1000 | Loss: 0.00004395
Iteration 227/1000 | Loss: 0.00004395
Iteration 228/1000 | Loss: 0.00004395
Iteration 229/1000 | Loss: 0.00004394
Iteration 230/1000 | Loss: 0.00004394
Iteration 231/1000 | Loss: 0.00004394
Iteration 232/1000 | Loss: 0.00004393
Iteration 233/1000 | Loss: 0.00004393
Iteration 234/1000 | Loss: 0.00004393
Iteration 235/1000 | Loss: 0.00004393
Iteration 236/1000 | Loss: 0.00004392
Iteration 237/1000 | Loss: 0.00004392
Iteration 238/1000 | Loss: 0.00004392
Iteration 239/1000 | Loss: 0.00004392
Iteration 240/1000 | Loss: 0.00004392
Iteration 241/1000 | Loss: 0.00004392
Iteration 242/1000 | Loss: 0.00004392
Iteration 243/1000 | Loss: 0.00004846
Iteration 244/1000 | Loss: 0.00004430
Iteration 245/1000 | Loss: 0.00005393
Iteration 246/1000 | Loss: 0.00004551
Iteration 247/1000 | Loss: 0.00005431
Iteration 248/1000 | Loss: 0.00004470
Iteration 249/1000 | Loss: 0.00004391
Iteration 250/1000 | Loss: 0.00004391
Iteration 251/1000 | Loss: 0.00004391
Iteration 252/1000 | Loss: 0.00004391
Iteration 253/1000 | Loss: 0.00004391
Iteration 254/1000 | Loss: 0.00004391
Iteration 255/1000 | Loss: 0.00004391
Iteration 256/1000 | Loss: 0.00004391
Iteration 257/1000 | Loss: 0.00004391
Iteration 258/1000 | Loss: 0.00004391
Iteration 259/1000 | Loss: 0.00004390
Iteration 260/1000 | Loss: 0.00004390
Iteration 261/1000 | Loss: 0.00004390
Iteration 262/1000 | Loss: 0.00004390
Iteration 263/1000 | Loss: 0.00004390
Iteration 264/1000 | Loss: 0.00004899
Iteration 265/1000 | Loss: 0.00004906
Iteration 266/1000 | Loss: 0.00004390
Iteration 267/1000 | Loss: 0.00004390
Iteration 268/1000 | Loss: 0.00004390
Iteration 269/1000 | Loss: 0.00004389
Iteration 270/1000 | Loss: 0.00004389
Iteration 271/1000 | Loss: 0.00004389
Iteration 272/1000 | Loss: 0.00004389
Iteration 273/1000 | Loss: 0.00004389
Iteration 274/1000 | Loss: 0.00004389
Iteration 275/1000 | Loss: 0.00004389
Iteration 276/1000 | Loss: 0.00004389
Iteration 277/1000 | Loss: 0.00004388
Iteration 278/1000 | Loss: 0.00004388
Iteration 279/1000 | Loss: 0.00004388
Iteration 280/1000 | Loss: 0.00004388
Iteration 281/1000 | Loss: 0.00004388
Iteration 282/1000 | Loss: 0.00004388
Iteration 283/1000 | Loss: 0.00004388
Iteration 284/1000 | Loss: 0.00004388
Iteration 285/1000 | Loss: 0.00004388
Iteration 286/1000 | Loss: 0.00004388
Iteration 287/1000 | Loss: 0.00004388
Iteration 288/1000 | Loss: 0.00004388
Iteration 289/1000 | Loss: 0.00004388
Iteration 290/1000 | Loss: 0.00004388
Iteration 291/1000 | Loss: 0.00004388
Iteration 292/1000 | Loss: 0.00004388
Iteration 293/1000 | Loss: 0.00004388
Iteration 294/1000 | Loss: 0.00004388
Iteration 295/1000 | Loss: 0.00004388
Iteration 296/1000 | Loss: 0.00004388
Iteration 297/1000 | Loss: 0.00004388
Iteration 298/1000 | Loss: 0.00004388
Iteration 299/1000 | Loss: 0.00004388
Iteration 300/1000 | Loss: 0.00004388
Iteration 301/1000 | Loss: 0.00004387
Iteration 302/1000 | Loss: 0.00004387
Iteration 303/1000 | Loss: 0.00004387
Iteration 304/1000 | Loss: 0.00004387
Iteration 305/1000 | Loss: 0.00004387
Iteration 306/1000 | Loss: 0.00004387
Iteration 307/1000 | Loss: 0.00004387
Iteration 308/1000 | Loss: 0.00004387
Iteration 309/1000 | Loss: 0.00004387
Iteration 310/1000 | Loss: 0.00004387
Iteration 311/1000 | Loss: 0.00004387
Iteration 312/1000 | Loss: 0.00004387
Iteration 313/1000 | Loss: 0.00004387
Iteration 314/1000 | Loss: 0.00004387
Iteration 315/1000 | Loss: 0.00004387
Iteration 316/1000 | Loss: 0.00004387
Iteration 317/1000 | Loss: 0.00004387
Iteration 318/1000 | Loss: 0.00004387
Iteration 319/1000 | Loss: 0.00004387
Iteration 320/1000 | Loss: 0.00004387
Iteration 321/1000 | Loss: 0.00004387
Iteration 322/1000 | Loss: 0.00004387
Iteration 323/1000 | Loss: 0.00004387
Iteration 324/1000 | Loss: 0.00004387
Iteration 325/1000 | Loss: 0.00004387
Iteration 326/1000 | Loss: 0.00004387
Iteration 327/1000 | Loss: 0.00004387
Iteration 328/1000 | Loss: 0.00004387
Iteration 329/1000 | Loss: 0.00004387
Iteration 330/1000 | Loss: 0.00004387
Iteration 331/1000 | Loss: 0.00004387
Iteration 332/1000 | Loss: 0.00004387
Iteration 333/1000 | Loss: 0.00004387
Iteration 334/1000 | Loss: 0.00004387
Iteration 335/1000 | Loss: 0.00004387
Iteration 336/1000 | Loss: 0.00004387
Iteration 337/1000 | Loss: 0.00004387
Iteration 338/1000 | Loss: 0.00004387
Iteration 339/1000 | Loss: 0.00004387
Iteration 340/1000 | Loss: 0.00004387
Iteration 341/1000 | Loss: 0.00004387
Iteration 342/1000 | Loss: 0.00004387
Iteration 343/1000 | Loss: 0.00004387
Iteration 344/1000 | Loss: 0.00004387
Iteration 345/1000 | Loss: 0.00004387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [4.387496301205829e-05, 4.387496301205829e-05, 4.387496301205829e-05, 4.387496301205829e-05, 4.387496301205829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.387496301205829e-05

Optimization complete. Final v2v error: 3.658684015274048 mm

Highest mean error: 20.20524787902832 mm for frame 194

Lowest mean error: 2.370854616165161 mm for frame 207

Saving results

Total time: 303.5061469078064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870039
Iteration 2/25 | Loss: 0.00166642
Iteration 3/25 | Loss: 0.00123911
Iteration 4/25 | Loss: 0.00115272
Iteration 5/25 | Loss: 0.00113977
Iteration 6/25 | Loss: 0.00113663
Iteration 7/25 | Loss: 0.00113532
Iteration 8/25 | Loss: 0.00113266
Iteration 9/25 | Loss: 0.00113085
Iteration 10/25 | Loss: 0.00113045
Iteration 11/25 | Loss: 0.00113024
Iteration 12/25 | Loss: 0.00113006
Iteration 13/25 | Loss: 0.00113177
Iteration 14/25 | Loss: 0.00112680
Iteration 15/25 | Loss: 0.00112635
Iteration 16/25 | Loss: 0.00112634
Iteration 17/25 | Loss: 0.00112634
Iteration 18/25 | Loss: 0.00112634
Iteration 19/25 | Loss: 0.00112634
Iteration 20/25 | Loss: 0.00112634
Iteration 21/25 | Loss: 0.00112634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001126340706832707, 0.001126340706832707, 0.001126340706832707, 0.001126340706832707, 0.001126340706832707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001126340706832707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31390953
Iteration 2/25 | Loss: 0.00083167
Iteration 3/25 | Loss: 0.00072658
Iteration 4/25 | Loss: 0.00072658
Iteration 5/25 | Loss: 0.00072658
Iteration 6/25 | Loss: 0.00072658
Iteration 7/25 | Loss: 0.00072658
Iteration 8/25 | Loss: 0.00072658
Iteration 9/25 | Loss: 0.00072658
Iteration 10/25 | Loss: 0.00072658
Iteration 11/25 | Loss: 0.00072658
Iteration 12/25 | Loss: 0.00072658
Iteration 13/25 | Loss: 0.00072658
Iteration 14/25 | Loss: 0.00072658
Iteration 15/25 | Loss: 0.00072658
Iteration 16/25 | Loss: 0.00072658
Iteration 17/25 | Loss: 0.00072658
Iteration 18/25 | Loss: 0.00072658
Iteration 19/25 | Loss: 0.00072658
Iteration 20/25 | Loss: 0.00072658
Iteration 21/25 | Loss: 0.00072658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007265756721608341, 0.0007265756721608341, 0.0007265756721608341, 0.0007265756721608341, 0.0007265756721608341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007265756721608341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072658
Iteration 2/1000 | Loss: 0.00024976
Iteration 3/1000 | Loss: 0.00035539
Iteration 4/1000 | Loss: 0.00097060
Iteration 5/1000 | Loss: 0.00011670
Iteration 6/1000 | Loss: 0.00012183
Iteration 7/1000 | Loss: 0.00005186
Iteration 8/1000 | Loss: 0.00005275
Iteration 9/1000 | Loss: 0.00007915
Iteration 10/1000 | Loss: 0.00007819
Iteration 11/1000 | Loss: 0.00003529
Iteration 12/1000 | Loss: 0.00009286
Iteration 13/1000 | Loss: 0.00004015
Iteration 14/1000 | Loss: 0.00004182
Iteration 15/1000 | Loss: 0.00003374
Iteration 16/1000 | Loss: 0.00003331
Iteration 17/1000 | Loss: 0.00018307
Iteration 18/1000 | Loss: 0.00003329
Iteration 19/1000 | Loss: 0.00026393
Iteration 20/1000 | Loss: 0.00020850
Iteration 21/1000 | Loss: 0.00003355
Iteration 22/1000 | Loss: 0.00029445
Iteration 23/1000 | Loss: 0.00003603
Iteration 24/1000 | Loss: 0.00003319
Iteration 25/1000 | Loss: 0.00003196
Iteration 26/1000 | Loss: 0.00010775
Iteration 27/1000 | Loss: 0.00006861
Iteration 28/1000 | Loss: 0.00003084
Iteration 29/1000 | Loss: 0.00007704
Iteration 30/1000 | Loss: 0.00005177
Iteration 31/1000 | Loss: 0.00006999
Iteration 32/1000 | Loss: 0.00003019
Iteration 33/1000 | Loss: 0.00003016
Iteration 34/1000 | Loss: 0.00003012
Iteration 35/1000 | Loss: 0.00003007
Iteration 36/1000 | Loss: 0.00003003
Iteration 37/1000 | Loss: 0.00003002
Iteration 38/1000 | Loss: 0.00003000
Iteration 39/1000 | Loss: 0.00002999
Iteration 40/1000 | Loss: 0.00002998
Iteration 41/1000 | Loss: 0.00002997
Iteration 42/1000 | Loss: 0.00002997
Iteration 43/1000 | Loss: 0.00002997
Iteration 44/1000 | Loss: 0.00002993
Iteration 45/1000 | Loss: 0.00002992
Iteration 46/1000 | Loss: 0.00002991
Iteration 47/1000 | Loss: 0.00002990
Iteration 48/1000 | Loss: 0.00002990
Iteration 49/1000 | Loss: 0.00002990
Iteration 50/1000 | Loss: 0.00002989
Iteration 51/1000 | Loss: 0.00002989
Iteration 52/1000 | Loss: 0.00002989
Iteration 53/1000 | Loss: 0.00002988
Iteration 54/1000 | Loss: 0.00007604
Iteration 55/1000 | Loss: 0.00003017
Iteration 56/1000 | Loss: 0.00002985
Iteration 57/1000 | Loss: 0.00006632
Iteration 58/1000 | Loss: 0.00004066
Iteration 59/1000 | Loss: 0.00002994
Iteration 60/1000 | Loss: 0.00002994
Iteration 61/1000 | Loss: 0.00004878
Iteration 62/1000 | Loss: 0.00002990
Iteration 63/1000 | Loss: 0.00002977
Iteration 64/1000 | Loss: 0.00002977
Iteration 65/1000 | Loss: 0.00002977
Iteration 66/1000 | Loss: 0.00002976
Iteration 67/1000 | Loss: 0.00002976
Iteration 68/1000 | Loss: 0.00002976
Iteration 69/1000 | Loss: 0.00002976
Iteration 70/1000 | Loss: 0.00002976
Iteration 71/1000 | Loss: 0.00002975
Iteration 72/1000 | Loss: 0.00002975
Iteration 73/1000 | Loss: 0.00002974
Iteration 74/1000 | Loss: 0.00002974
Iteration 75/1000 | Loss: 0.00002973
Iteration 76/1000 | Loss: 0.00002973
Iteration 77/1000 | Loss: 0.00002972
Iteration 78/1000 | Loss: 0.00002971
Iteration 79/1000 | Loss: 0.00002971
Iteration 80/1000 | Loss: 0.00002968
Iteration 81/1000 | Loss: 0.00002968
Iteration 82/1000 | Loss: 0.00002966
Iteration 83/1000 | Loss: 0.00002966
Iteration 84/1000 | Loss: 0.00002965
Iteration 85/1000 | Loss: 0.00002965
Iteration 86/1000 | Loss: 0.00002965
Iteration 87/1000 | Loss: 0.00002965
Iteration 88/1000 | Loss: 0.00002963
Iteration 89/1000 | Loss: 0.00002963
Iteration 90/1000 | Loss: 0.00002963
Iteration 91/1000 | Loss: 0.00002963
Iteration 92/1000 | Loss: 0.00002963
Iteration 93/1000 | Loss: 0.00002963
Iteration 94/1000 | Loss: 0.00002963
Iteration 95/1000 | Loss: 0.00002963
Iteration 96/1000 | Loss: 0.00002963
Iteration 97/1000 | Loss: 0.00002963
Iteration 98/1000 | Loss: 0.00002963
Iteration 99/1000 | Loss: 0.00002962
Iteration 100/1000 | Loss: 0.00008831
Iteration 101/1000 | Loss: 0.00003551
Iteration 102/1000 | Loss: 0.00003184
Iteration 103/1000 | Loss: 0.00002970
Iteration 104/1000 | Loss: 0.00006193
Iteration 105/1000 | Loss: 0.00003493
Iteration 106/1000 | Loss: 0.00004310
Iteration 107/1000 | Loss: 0.00003118
Iteration 108/1000 | Loss: 0.00003021
Iteration 109/1000 | Loss: 0.00002974
Iteration 110/1000 | Loss: 0.00002956
Iteration 111/1000 | Loss: 0.00002950
Iteration 112/1000 | Loss: 0.00002950
Iteration 113/1000 | Loss: 0.00002949
Iteration 114/1000 | Loss: 0.00002949
Iteration 115/1000 | Loss: 0.00002949
Iteration 116/1000 | Loss: 0.00002949
Iteration 117/1000 | Loss: 0.00002949
Iteration 118/1000 | Loss: 0.00002948
Iteration 119/1000 | Loss: 0.00002948
Iteration 120/1000 | Loss: 0.00002947
Iteration 121/1000 | Loss: 0.00002947
Iteration 122/1000 | Loss: 0.00002947
Iteration 123/1000 | Loss: 0.00002947
Iteration 124/1000 | Loss: 0.00002947
Iteration 125/1000 | Loss: 0.00002946
Iteration 126/1000 | Loss: 0.00002946
Iteration 127/1000 | Loss: 0.00002946
Iteration 128/1000 | Loss: 0.00002946
Iteration 129/1000 | Loss: 0.00002946
Iteration 130/1000 | Loss: 0.00002946
Iteration 131/1000 | Loss: 0.00002946
Iteration 132/1000 | Loss: 0.00002946
Iteration 133/1000 | Loss: 0.00002946
Iteration 134/1000 | Loss: 0.00002946
Iteration 135/1000 | Loss: 0.00002946
Iteration 136/1000 | Loss: 0.00002946
Iteration 137/1000 | Loss: 0.00002946
Iteration 138/1000 | Loss: 0.00002946
Iteration 139/1000 | Loss: 0.00002946
Iteration 140/1000 | Loss: 0.00002946
Iteration 141/1000 | Loss: 0.00002946
Iteration 142/1000 | Loss: 0.00002946
Iteration 143/1000 | Loss: 0.00002946
Iteration 144/1000 | Loss: 0.00002946
Iteration 145/1000 | Loss: 0.00002946
Iteration 146/1000 | Loss: 0.00002946
Iteration 147/1000 | Loss: 0.00002946
Iteration 148/1000 | Loss: 0.00002946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.9460692530847155e-05, 2.9460692530847155e-05, 2.9460692530847155e-05, 2.9460692530847155e-05, 2.9460692530847155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9460692530847155e-05

Optimization complete. Final v2v error: 4.33500862121582 mm

Highest mean error: 5.331821918487549 mm for frame 223

Lowest mean error: 3.825988531112671 mm for frame 0

Saving results

Total time: 120.71742463111877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075265
Iteration 2/25 | Loss: 0.00199850
Iteration 3/25 | Loss: 0.00140305
Iteration 4/25 | Loss: 0.00123169
Iteration 5/25 | Loss: 0.00107004
Iteration 6/25 | Loss: 0.00114082
Iteration 7/25 | Loss: 0.00110091
Iteration 8/25 | Loss: 0.00114912
Iteration 9/25 | Loss: 0.00092478
Iteration 10/25 | Loss: 0.00092775
Iteration 11/25 | Loss: 0.00093221
Iteration 12/25 | Loss: 0.00092057
Iteration 13/25 | Loss: 0.00091875
Iteration 14/25 | Loss: 0.00090455
Iteration 15/25 | Loss: 0.00090295
Iteration 16/25 | Loss: 0.00090250
Iteration 17/25 | Loss: 0.00090236
Iteration 18/25 | Loss: 0.00090226
Iteration 19/25 | Loss: 0.00090194
Iteration 20/25 | Loss: 0.00090171
Iteration 21/25 | Loss: 0.00090162
Iteration 22/25 | Loss: 0.00090162
Iteration 23/25 | Loss: 0.00090162
Iteration 24/25 | Loss: 0.00090161
Iteration 25/25 | Loss: 0.00090161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36846769
Iteration 2/25 | Loss: 0.00129114
Iteration 3/25 | Loss: 0.00054213
Iteration 4/25 | Loss: 0.00054212
Iteration 5/25 | Loss: 0.00054212
Iteration 6/25 | Loss: 0.00054212
Iteration 7/25 | Loss: 0.00054212
Iteration 8/25 | Loss: 0.00054212
Iteration 9/25 | Loss: 0.00054211
Iteration 10/25 | Loss: 0.00054211
Iteration 11/25 | Loss: 0.00054211
Iteration 12/25 | Loss: 0.00054211
Iteration 13/25 | Loss: 0.00054211
Iteration 14/25 | Loss: 0.00054211
Iteration 15/25 | Loss: 0.00054211
Iteration 16/25 | Loss: 0.00054211
Iteration 17/25 | Loss: 0.00054211
Iteration 18/25 | Loss: 0.00054211
Iteration 19/25 | Loss: 0.00054211
Iteration 20/25 | Loss: 0.00054211
Iteration 21/25 | Loss: 0.00054211
Iteration 22/25 | Loss: 0.00054211
Iteration 23/25 | Loss: 0.00054211
Iteration 24/25 | Loss: 0.00054211
Iteration 25/25 | Loss: 0.00054211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054211
Iteration 2/1000 | Loss: 0.00002951
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00001744
Iteration 8/1000 | Loss: 0.00001722
Iteration 9/1000 | Loss: 0.00001705
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001674
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001664
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001658
Iteration 38/1000 | Loss: 0.00001658
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001657
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001656
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001654
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001653
Iteration 71/1000 | Loss: 0.00001653
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001650
Iteration 82/1000 | Loss: 0.00001650
Iteration 83/1000 | Loss: 0.00001650
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001648
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001648
Iteration 100/1000 | Loss: 0.00001648
Iteration 101/1000 | Loss: 0.00001648
Iteration 102/1000 | Loss: 0.00001648
Iteration 103/1000 | Loss: 0.00001648
Iteration 104/1000 | Loss: 0.00001648
Iteration 105/1000 | Loss: 0.00001647
Iteration 106/1000 | Loss: 0.00001647
Iteration 107/1000 | Loss: 0.00001647
Iteration 108/1000 | Loss: 0.00001647
Iteration 109/1000 | Loss: 0.00001647
Iteration 110/1000 | Loss: 0.00001647
Iteration 111/1000 | Loss: 0.00001646
Iteration 112/1000 | Loss: 0.00001646
Iteration 113/1000 | Loss: 0.00001646
Iteration 114/1000 | Loss: 0.00001646
Iteration 115/1000 | Loss: 0.00001646
Iteration 116/1000 | Loss: 0.00001646
Iteration 117/1000 | Loss: 0.00001646
Iteration 118/1000 | Loss: 0.00001646
Iteration 119/1000 | Loss: 0.00001646
Iteration 120/1000 | Loss: 0.00001646
Iteration 121/1000 | Loss: 0.00001646
Iteration 122/1000 | Loss: 0.00001645
Iteration 123/1000 | Loss: 0.00001645
Iteration 124/1000 | Loss: 0.00001645
Iteration 125/1000 | Loss: 0.00001644
Iteration 126/1000 | Loss: 0.00001644
Iteration 127/1000 | Loss: 0.00001644
Iteration 128/1000 | Loss: 0.00001644
Iteration 129/1000 | Loss: 0.00001644
Iteration 130/1000 | Loss: 0.00001644
Iteration 131/1000 | Loss: 0.00001644
Iteration 132/1000 | Loss: 0.00001644
Iteration 133/1000 | Loss: 0.00001644
Iteration 134/1000 | Loss: 0.00001644
Iteration 135/1000 | Loss: 0.00001643
Iteration 136/1000 | Loss: 0.00001643
Iteration 137/1000 | Loss: 0.00001643
Iteration 138/1000 | Loss: 0.00001643
Iteration 139/1000 | Loss: 0.00001643
Iteration 140/1000 | Loss: 0.00001643
Iteration 141/1000 | Loss: 0.00001643
Iteration 142/1000 | Loss: 0.00001643
Iteration 143/1000 | Loss: 0.00001643
Iteration 144/1000 | Loss: 0.00001643
Iteration 145/1000 | Loss: 0.00001643
Iteration 146/1000 | Loss: 0.00001642
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001642
Iteration 149/1000 | Loss: 0.00001642
Iteration 150/1000 | Loss: 0.00001642
Iteration 151/1000 | Loss: 0.00001642
Iteration 152/1000 | Loss: 0.00001642
Iteration 153/1000 | Loss: 0.00001642
Iteration 154/1000 | Loss: 0.00001642
Iteration 155/1000 | Loss: 0.00001642
Iteration 156/1000 | Loss: 0.00001642
Iteration 157/1000 | Loss: 0.00001642
Iteration 158/1000 | Loss: 0.00001642
Iteration 159/1000 | Loss: 0.00001642
Iteration 160/1000 | Loss: 0.00001642
Iteration 161/1000 | Loss: 0.00001642
Iteration 162/1000 | Loss: 0.00001642
Iteration 163/1000 | Loss: 0.00001642
Iteration 164/1000 | Loss: 0.00001642
Iteration 165/1000 | Loss: 0.00001642
Iteration 166/1000 | Loss: 0.00001642
Iteration 167/1000 | Loss: 0.00001642
Iteration 168/1000 | Loss: 0.00001642
Iteration 169/1000 | Loss: 0.00001642
Iteration 170/1000 | Loss: 0.00001641
Iteration 171/1000 | Loss: 0.00001641
Iteration 172/1000 | Loss: 0.00001641
Iteration 173/1000 | Loss: 0.00001641
Iteration 174/1000 | Loss: 0.00001641
Iteration 175/1000 | Loss: 0.00001641
Iteration 176/1000 | Loss: 0.00001641
Iteration 177/1000 | Loss: 0.00001641
Iteration 178/1000 | Loss: 0.00001641
Iteration 179/1000 | Loss: 0.00001641
Iteration 180/1000 | Loss: 0.00001641
Iteration 181/1000 | Loss: 0.00001641
Iteration 182/1000 | Loss: 0.00001641
Iteration 183/1000 | Loss: 0.00001641
Iteration 184/1000 | Loss: 0.00001641
Iteration 185/1000 | Loss: 0.00001641
Iteration 186/1000 | Loss: 0.00001641
Iteration 187/1000 | Loss: 0.00001641
Iteration 188/1000 | Loss: 0.00001641
Iteration 189/1000 | Loss: 0.00001641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.641329981794115e-05, 1.641329981794115e-05, 1.641329981794115e-05, 1.641329981794115e-05, 1.641329981794115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.641329981794115e-05

Optimization complete. Final v2v error: 3.37068247795105 mm

Highest mean error: 3.646653890609741 mm for frame 239

Lowest mean error: 3.2323944568634033 mm for frame 171

Saving results

Total time: 74.62059473991394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078892
Iteration 2/25 | Loss: 0.00213855
Iteration 3/25 | Loss: 0.00127828
Iteration 4/25 | Loss: 0.00120048
Iteration 5/25 | Loss: 0.00120104
Iteration 6/25 | Loss: 0.00118860
Iteration 7/25 | Loss: 0.00114608
Iteration 8/25 | Loss: 0.00111568
Iteration 9/25 | Loss: 0.00109342
Iteration 10/25 | Loss: 0.00107169
Iteration 11/25 | Loss: 0.00105027
Iteration 12/25 | Loss: 0.00105339
Iteration 13/25 | Loss: 0.00103602
Iteration 14/25 | Loss: 0.00103347
Iteration 15/25 | Loss: 0.00102720
Iteration 16/25 | Loss: 0.00102737
Iteration 17/25 | Loss: 0.00102576
Iteration 18/25 | Loss: 0.00102992
Iteration 19/25 | Loss: 0.00102649
Iteration 20/25 | Loss: 0.00102808
Iteration 21/25 | Loss: 0.00102151
Iteration 22/25 | Loss: 0.00102618
Iteration 23/25 | Loss: 0.00102863
Iteration 24/25 | Loss: 0.00102585
Iteration 25/25 | Loss: 0.00102535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.44129515
Iteration 2/25 | Loss: 0.00068600
Iteration 3/25 | Loss: 0.00055785
Iteration 4/25 | Loss: 0.00055785
Iteration 5/25 | Loss: 0.00055785
Iteration 6/25 | Loss: 0.00055785
Iteration 7/25 | Loss: 0.00055785
Iteration 8/25 | Loss: 0.00055785
Iteration 9/25 | Loss: 0.00055785
Iteration 10/25 | Loss: 0.00055785
Iteration 11/25 | Loss: 0.00055785
Iteration 12/25 | Loss: 0.00055785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005578513373620808, 0.0005578513373620808, 0.0005578513373620808, 0.0005578513373620808, 0.0005578513373620808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005578513373620808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055785
Iteration 2/1000 | Loss: 0.00021857
Iteration 3/1000 | Loss: 0.00008931
Iteration 4/1000 | Loss: 0.00006750
Iteration 5/1000 | Loss: 0.00004982
Iteration 6/1000 | Loss: 0.00004623
Iteration 7/1000 | Loss: 0.00004233
Iteration 8/1000 | Loss: 0.00003766
Iteration 9/1000 | Loss: 0.00004275
Iteration 10/1000 | Loss: 0.00004074
Iteration 11/1000 | Loss: 0.00003980
Iteration 12/1000 | Loss: 0.00004038
Iteration 13/1000 | Loss: 0.00018932
Iteration 14/1000 | Loss: 0.00005546
Iteration 15/1000 | Loss: 0.00003086
Iteration 16/1000 | Loss: 0.00003659
Iteration 17/1000 | Loss: 0.00003881
Iteration 18/1000 | Loss: 0.00003470
Iteration 19/1000 | Loss: 0.00003005
Iteration 20/1000 | Loss: 0.00003509
Iteration 21/1000 | Loss: 0.00003535
Iteration 22/1000 | Loss: 0.00003451
Iteration 23/1000 | Loss: 0.00008067
Iteration 24/1000 | Loss: 0.00033030
Iteration 25/1000 | Loss: 0.00032495
Iteration 26/1000 | Loss: 0.00064314
Iteration 27/1000 | Loss: 0.00035736
Iteration 28/1000 | Loss: 0.00006402
Iteration 29/1000 | Loss: 0.00002587
Iteration 30/1000 | Loss: 0.00002260
Iteration 31/1000 | Loss: 0.00002103
Iteration 32/1000 | Loss: 0.00002010
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001852
Iteration 39/1000 | Loss: 0.00001851
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001838
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001834
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001833
Iteration 74/1000 | Loss: 0.00001833
Iteration 75/1000 | Loss: 0.00001833
Iteration 76/1000 | Loss: 0.00001833
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00001832
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001828
Iteration 94/1000 | Loss: 0.00001828
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001825
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001825
Iteration 105/1000 | Loss: 0.00001824
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001824
Iteration 109/1000 | Loss: 0.00001823
Iteration 110/1000 | Loss: 0.00001823
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001823
Iteration 113/1000 | Loss: 0.00001823
Iteration 114/1000 | Loss: 0.00001823
Iteration 115/1000 | Loss: 0.00001823
Iteration 116/1000 | Loss: 0.00001823
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001822
Iteration 121/1000 | Loss: 0.00001822
Iteration 122/1000 | Loss: 0.00001822
Iteration 123/1000 | Loss: 0.00001822
Iteration 124/1000 | Loss: 0.00001822
Iteration 125/1000 | Loss: 0.00001822
Iteration 126/1000 | Loss: 0.00001822
Iteration 127/1000 | Loss: 0.00001822
Iteration 128/1000 | Loss: 0.00001822
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001821
Iteration 134/1000 | Loss: 0.00001821
Iteration 135/1000 | Loss: 0.00001821
Iteration 136/1000 | Loss: 0.00001821
Iteration 137/1000 | Loss: 0.00001821
Iteration 138/1000 | Loss: 0.00001821
Iteration 139/1000 | Loss: 0.00001821
Iteration 140/1000 | Loss: 0.00001821
Iteration 141/1000 | Loss: 0.00001821
Iteration 142/1000 | Loss: 0.00001821
Iteration 143/1000 | Loss: 0.00001821
Iteration 144/1000 | Loss: 0.00001821
Iteration 145/1000 | Loss: 0.00001821
Iteration 146/1000 | Loss: 0.00001821
Iteration 147/1000 | Loss: 0.00001821
Iteration 148/1000 | Loss: 0.00001821
Iteration 149/1000 | Loss: 0.00001821
Iteration 150/1000 | Loss: 0.00001821
Iteration 151/1000 | Loss: 0.00001821
Iteration 152/1000 | Loss: 0.00001821
Iteration 153/1000 | Loss: 0.00001821
Iteration 154/1000 | Loss: 0.00001821
Iteration 155/1000 | Loss: 0.00001821
Iteration 156/1000 | Loss: 0.00001821
Iteration 157/1000 | Loss: 0.00001821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.8212194845546037e-05, 1.8212194845546037e-05, 1.8212194845546037e-05, 1.8212194845546037e-05, 1.8212194845546037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8212194845546037e-05

Optimization complete. Final v2v error: 3.4453015327453613 mm

Highest mean error: 4.110395908355713 mm for frame 1

Lowest mean error: 3.018681287765503 mm for frame 188

Saving results

Total time: 111.44684934616089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082160
Iteration 2/25 | Loss: 0.01082160
Iteration 3/25 | Loss: 0.00538480
Iteration 4/25 | Loss: 0.00361204
Iteration 5/25 | Loss: 0.00266063
Iteration 6/25 | Loss: 0.00239509
Iteration 7/25 | Loss: 0.00220741
Iteration 8/25 | Loss: 0.00206027
Iteration 9/25 | Loss: 0.00200889
Iteration 10/25 | Loss: 0.00185218
Iteration 11/25 | Loss: 0.00177475
Iteration 12/25 | Loss: 0.00162302
Iteration 13/25 | Loss: 0.00157830
Iteration 14/25 | Loss: 0.00155398
Iteration 15/25 | Loss: 0.00142127
Iteration 16/25 | Loss: 0.00132469
Iteration 17/25 | Loss: 0.00128948
Iteration 18/25 | Loss: 0.00128849
Iteration 19/25 | Loss: 0.00127967
Iteration 20/25 | Loss: 0.00126850
Iteration 21/25 | Loss: 0.00126442
Iteration 22/25 | Loss: 0.00127578
Iteration 23/25 | Loss: 0.00125400
Iteration 24/25 | Loss: 0.00125214
Iteration 25/25 | Loss: 0.00124839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68476790
Iteration 2/25 | Loss: 0.00160390
Iteration 3/25 | Loss: 0.00160390
Iteration 4/25 | Loss: 0.00160390
Iteration 5/25 | Loss: 0.00160390
Iteration 6/25 | Loss: 0.00160390
Iteration 7/25 | Loss: 0.00160390
Iteration 8/25 | Loss: 0.00160390
Iteration 9/25 | Loss: 0.00160390
Iteration 10/25 | Loss: 0.00160390
Iteration 11/25 | Loss: 0.00160390
Iteration 12/25 | Loss: 0.00160390
Iteration 13/25 | Loss: 0.00160390
Iteration 14/25 | Loss: 0.00160389
Iteration 15/25 | Loss: 0.00160389
Iteration 16/25 | Loss: 0.00160389
Iteration 17/25 | Loss: 0.00160389
Iteration 18/25 | Loss: 0.00160389
Iteration 19/25 | Loss: 0.00160389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001603894867002964, 0.001603894867002964, 0.001603894867002964, 0.001603894867002964, 0.001603894867002964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001603894867002964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160389
Iteration 2/1000 | Loss: 0.00142757
Iteration 3/1000 | Loss: 0.00115679
Iteration 4/1000 | Loss: 0.00102856
Iteration 5/1000 | Loss: 0.00053371
Iteration 6/1000 | Loss: 0.00031555
Iteration 7/1000 | Loss: 0.00017135
Iteration 8/1000 | Loss: 0.00013660
Iteration 9/1000 | Loss: 0.00012885
Iteration 10/1000 | Loss: 0.00014225
Iteration 11/1000 | Loss: 0.00008898
Iteration 12/1000 | Loss: 0.00008741
Iteration 13/1000 | Loss: 0.00009436
Iteration 14/1000 | Loss: 0.00009135
Iteration 15/1000 | Loss: 0.00010057
Iteration 16/1000 | Loss: 0.00009437
Iteration 17/1000 | Loss: 0.00084471
Iteration 18/1000 | Loss: 0.00013517
Iteration 19/1000 | Loss: 0.00028808
Iteration 20/1000 | Loss: 0.00030649
Iteration 21/1000 | Loss: 0.00006917
Iteration 22/1000 | Loss: 0.00007082
Iteration 23/1000 | Loss: 0.00006467
Iteration 24/1000 | Loss: 0.00006456
Iteration 25/1000 | Loss: 0.00003963
Iteration 26/1000 | Loss: 0.00003534
Iteration 27/1000 | Loss: 0.00003364
Iteration 28/1000 | Loss: 0.00003214
Iteration 29/1000 | Loss: 0.00025114
Iteration 30/1000 | Loss: 0.00004176
Iteration 31/1000 | Loss: 0.00003498
Iteration 32/1000 | Loss: 0.00003285
Iteration 33/1000 | Loss: 0.00003157
Iteration 34/1000 | Loss: 0.00003079
Iteration 35/1000 | Loss: 0.00002993
Iteration 36/1000 | Loss: 0.00024656
Iteration 37/1000 | Loss: 0.00012284
Iteration 38/1000 | Loss: 0.00062675
Iteration 39/1000 | Loss: 0.00006756
Iteration 40/1000 | Loss: 0.00006678
Iteration 41/1000 | Loss: 0.00005192
Iteration 42/1000 | Loss: 0.00003233
Iteration 43/1000 | Loss: 0.00002925
Iteration 44/1000 | Loss: 0.00002754
Iteration 45/1000 | Loss: 0.00002663
Iteration 46/1000 | Loss: 0.00002598
Iteration 47/1000 | Loss: 0.00002560
Iteration 48/1000 | Loss: 0.00002527
Iteration 49/1000 | Loss: 0.00002755
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002487
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00002463
Iteration 54/1000 | Loss: 0.00002452
Iteration 55/1000 | Loss: 0.00002440
Iteration 56/1000 | Loss: 0.00002423
Iteration 57/1000 | Loss: 0.00002418
Iteration 58/1000 | Loss: 0.00002401
Iteration 59/1000 | Loss: 0.00002401
Iteration 60/1000 | Loss: 0.00002394
Iteration 61/1000 | Loss: 0.00002394
Iteration 62/1000 | Loss: 0.00002391
Iteration 63/1000 | Loss: 0.00002391
Iteration 64/1000 | Loss: 0.00002389
Iteration 65/1000 | Loss: 0.00002388
Iteration 66/1000 | Loss: 0.00002387
Iteration 67/1000 | Loss: 0.00002387
Iteration 68/1000 | Loss: 0.00002387
Iteration 69/1000 | Loss: 0.00002386
Iteration 70/1000 | Loss: 0.00002386
Iteration 71/1000 | Loss: 0.00002386
Iteration 72/1000 | Loss: 0.00002386
Iteration 73/1000 | Loss: 0.00002386
Iteration 74/1000 | Loss: 0.00002386
Iteration 75/1000 | Loss: 0.00002386
Iteration 76/1000 | Loss: 0.00002386
Iteration 77/1000 | Loss: 0.00002386
Iteration 78/1000 | Loss: 0.00002386
Iteration 79/1000 | Loss: 0.00002386
Iteration 80/1000 | Loss: 0.00002386
Iteration 81/1000 | Loss: 0.00002386
Iteration 82/1000 | Loss: 0.00002386
Iteration 83/1000 | Loss: 0.00002386
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002385
Iteration 86/1000 | Loss: 0.00002385
Iteration 87/1000 | Loss: 0.00002385
Iteration 88/1000 | Loss: 0.00002385
Iteration 89/1000 | Loss: 0.00002384
Iteration 90/1000 | Loss: 0.00002384
Iteration 91/1000 | Loss: 0.00002384
Iteration 92/1000 | Loss: 0.00002384
Iteration 93/1000 | Loss: 0.00002384
Iteration 94/1000 | Loss: 0.00002384
Iteration 95/1000 | Loss: 0.00002384
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002384
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002384
Iteration 103/1000 | Loss: 0.00002383
Iteration 104/1000 | Loss: 0.00002383
Iteration 105/1000 | Loss: 0.00002383
Iteration 106/1000 | Loss: 0.00002383
Iteration 107/1000 | Loss: 0.00002383
Iteration 108/1000 | Loss: 0.00002383
Iteration 109/1000 | Loss: 0.00002383
Iteration 110/1000 | Loss: 0.00002383
Iteration 111/1000 | Loss: 0.00002383
Iteration 112/1000 | Loss: 0.00002383
Iteration 113/1000 | Loss: 0.00002383
Iteration 114/1000 | Loss: 0.00002383
Iteration 115/1000 | Loss: 0.00002382
Iteration 116/1000 | Loss: 0.00002382
Iteration 117/1000 | Loss: 0.00002382
Iteration 118/1000 | Loss: 0.00002382
Iteration 119/1000 | Loss: 0.00002381
Iteration 120/1000 | Loss: 0.00002381
Iteration 121/1000 | Loss: 0.00002381
Iteration 122/1000 | Loss: 0.00002381
Iteration 123/1000 | Loss: 0.00002381
Iteration 124/1000 | Loss: 0.00002381
Iteration 125/1000 | Loss: 0.00002381
Iteration 126/1000 | Loss: 0.00002381
Iteration 127/1000 | Loss: 0.00002380
Iteration 128/1000 | Loss: 0.00002380
Iteration 129/1000 | Loss: 0.00002380
Iteration 130/1000 | Loss: 0.00002380
Iteration 131/1000 | Loss: 0.00002380
Iteration 132/1000 | Loss: 0.00002380
Iteration 133/1000 | Loss: 0.00002380
Iteration 134/1000 | Loss: 0.00002380
Iteration 135/1000 | Loss: 0.00002380
Iteration 136/1000 | Loss: 0.00002379
Iteration 137/1000 | Loss: 0.00002379
Iteration 138/1000 | Loss: 0.00002379
Iteration 139/1000 | Loss: 0.00002379
Iteration 140/1000 | Loss: 0.00002379
Iteration 141/1000 | Loss: 0.00002379
Iteration 142/1000 | Loss: 0.00002379
Iteration 143/1000 | Loss: 0.00002379
Iteration 144/1000 | Loss: 0.00002379
Iteration 145/1000 | Loss: 0.00002379
Iteration 146/1000 | Loss: 0.00002378
Iteration 147/1000 | Loss: 0.00002378
Iteration 148/1000 | Loss: 0.00002378
Iteration 149/1000 | Loss: 0.00002378
Iteration 150/1000 | Loss: 0.00002378
Iteration 151/1000 | Loss: 0.00002378
Iteration 152/1000 | Loss: 0.00002378
Iteration 153/1000 | Loss: 0.00002378
Iteration 154/1000 | Loss: 0.00002378
Iteration 155/1000 | Loss: 0.00002378
Iteration 156/1000 | Loss: 0.00002378
Iteration 157/1000 | Loss: 0.00002378
Iteration 158/1000 | Loss: 0.00002377
Iteration 159/1000 | Loss: 0.00002377
Iteration 160/1000 | Loss: 0.00002377
Iteration 161/1000 | Loss: 0.00002377
Iteration 162/1000 | Loss: 0.00002377
Iteration 163/1000 | Loss: 0.00002377
Iteration 164/1000 | Loss: 0.00002376
Iteration 165/1000 | Loss: 0.00002376
Iteration 166/1000 | Loss: 0.00002376
Iteration 167/1000 | Loss: 0.00002376
Iteration 168/1000 | Loss: 0.00002376
Iteration 169/1000 | Loss: 0.00002375
Iteration 170/1000 | Loss: 0.00002375
Iteration 171/1000 | Loss: 0.00002375
Iteration 172/1000 | Loss: 0.00002375
Iteration 173/1000 | Loss: 0.00002375
Iteration 174/1000 | Loss: 0.00002375
Iteration 175/1000 | Loss: 0.00002375
Iteration 176/1000 | Loss: 0.00002375
Iteration 177/1000 | Loss: 0.00002375
Iteration 178/1000 | Loss: 0.00002375
Iteration 179/1000 | Loss: 0.00002374
Iteration 180/1000 | Loss: 0.00002374
Iteration 181/1000 | Loss: 0.00002374
Iteration 182/1000 | Loss: 0.00002374
Iteration 183/1000 | Loss: 0.00002374
Iteration 184/1000 | Loss: 0.00002374
Iteration 185/1000 | Loss: 0.00002374
Iteration 186/1000 | Loss: 0.00002374
Iteration 187/1000 | Loss: 0.00002374
Iteration 188/1000 | Loss: 0.00002374
Iteration 189/1000 | Loss: 0.00002374
Iteration 190/1000 | Loss: 0.00002374
Iteration 191/1000 | Loss: 0.00002374
Iteration 192/1000 | Loss: 0.00002374
Iteration 193/1000 | Loss: 0.00002374
Iteration 194/1000 | Loss: 0.00002374
Iteration 195/1000 | Loss: 0.00002374
Iteration 196/1000 | Loss: 0.00002374
Iteration 197/1000 | Loss: 0.00002374
Iteration 198/1000 | Loss: 0.00002374
Iteration 199/1000 | Loss: 0.00002374
Iteration 200/1000 | Loss: 0.00002374
Iteration 201/1000 | Loss: 0.00002374
Iteration 202/1000 | Loss: 0.00002374
Iteration 203/1000 | Loss: 0.00002374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.374238465563394e-05, 2.374238465563394e-05, 2.374238465563394e-05, 2.374238465563394e-05, 2.374238465563394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.374238465563394e-05

Optimization complete. Final v2v error: 3.7719502449035645 mm

Highest mean error: 10.845405578613281 mm for frame 16

Lowest mean error: 3.458750009536743 mm for frame 122

Saving results

Total time: 151.07366561889648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367008
Iteration 2/25 | Loss: 0.00088877
Iteration 3/25 | Loss: 0.00079744
Iteration 4/25 | Loss: 0.00078890
Iteration 5/25 | Loss: 0.00078631
Iteration 6/25 | Loss: 0.00078595
Iteration 7/25 | Loss: 0.00078595
Iteration 8/25 | Loss: 0.00078595
Iteration 9/25 | Loss: 0.00078595
Iteration 10/25 | Loss: 0.00078595
Iteration 11/25 | Loss: 0.00078595
Iteration 12/25 | Loss: 0.00078595
Iteration 13/25 | Loss: 0.00078595
Iteration 14/25 | Loss: 0.00078595
Iteration 15/25 | Loss: 0.00078595
Iteration 16/25 | Loss: 0.00078595
Iteration 17/25 | Loss: 0.00078595
Iteration 18/25 | Loss: 0.00078595
Iteration 19/25 | Loss: 0.00078595
Iteration 20/25 | Loss: 0.00078595
Iteration 21/25 | Loss: 0.00078595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007859452744014561, 0.0007859452744014561, 0.0007859452744014561, 0.0007859452744014561, 0.0007859452744014561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007859452744014561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82522535
Iteration 2/25 | Loss: 0.00045922
Iteration 3/25 | Loss: 0.00045921
Iteration 4/25 | Loss: 0.00045921
Iteration 5/25 | Loss: 0.00045921
Iteration 6/25 | Loss: 0.00045921
Iteration 7/25 | Loss: 0.00045921
Iteration 8/25 | Loss: 0.00045921
Iteration 9/25 | Loss: 0.00045921
Iteration 10/25 | Loss: 0.00045921
Iteration 11/25 | Loss: 0.00045921
Iteration 12/25 | Loss: 0.00045921
Iteration 13/25 | Loss: 0.00045921
Iteration 14/25 | Loss: 0.00045921
Iteration 15/25 | Loss: 0.00045921
Iteration 16/25 | Loss: 0.00045921
Iteration 17/25 | Loss: 0.00045921
Iteration 18/25 | Loss: 0.00045921
Iteration 19/25 | Loss: 0.00045921
Iteration 20/25 | Loss: 0.00045921
Iteration 21/25 | Loss: 0.00045921
Iteration 22/25 | Loss: 0.00045921
Iteration 23/25 | Loss: 0.00045921
Iteration 24/25 | Loss: 0.00045921
Iteration 25/25 | Loss: 0.00045921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045921
Iteration 2/1000 | Loss: 0.00001205
Iteration 3/1000 | Loss: 0.00000847
Iteration 4/1000 | Loss: 0.00000769
Iteration 5/1000 | Loss: 0.00000730
Iteration 6/1000 | Loss: 0.00000703
Iteration 7/1000 | Loss: 0.00000684
Iteration 8/1000 | Loss: 0.00000684
Iteration 9/1000 | Loss: 0.00000684
Iteration 10/1000 | Loss: 0.00000683
Iteration 11/1000 | Loss: 0.00000683
Iteration 12/1000 | Loss: 0.00000683
Iteration 13/1000 | Loss: 0.00000681
Iteration 14/1000 | Loss: 0.00000681
Iteration 15/1000 | Loss: 0.00000680
Iteration 16/1000 | Loss: 0.00000679
Iteration 17/1000 | Loss: 0.00000678
Iteration 18/1000 | Loss: 0.00000673
Iteration 19/1000 | Loss: 0.00000673
Iteration 20/1000 | Loss: 0.00000673
Iteration 21/1000 | Loss: 0.00000673
Iteration 22/1000 | Loss: 0.00000666
Iteration 23/1000 | Loss: 0.00000665
Iteration 24/1000 | Loss: 0.00000664
Iteration 25/1000 | Loss: 0.00000663
Iteration 26/1000 | Loss: 0.00000663
Iteration 27/1000 | Loss: 0.00000662
Iteration 28/1000 | Loss: 0.00000659
Iteration 29/1000 | Loss: 0.00000658
Iteration 30/1000 | Loss: 0.00000658
Iteration 31/1000 | Loss: 0.00000658
Iteration 32/1000 | Loss: 0.00000658
Iteration 33/1000 | Loss: 0.00000657
Iteration 34/1000 | Loss: 0.00000656
Iteration 35/1000 | Loss: 0.00000655
Iteration 36/1000 | Loss: 0.00000655
Iteration 37/1000 | Loss: 0.00000655
Iteration 38/1000 | Loss: 0.00000655
Iteration 39/1000 | Loss: 0.00000655
Iteration 40/1000 | Loss: 0.00000654
Iteration 41/1000 | Loss: 0.00000654
Iteration 42/1000 | Loss: 0.00000654
Iteration 43/1000 | Loss: 0.00000654
Iteration 44/1000 | Loss: 0.00000654
Iteration 45/1000 | Loss: 0.00000654
Iteration 46/1000 | Loss: 0.00000653
Iteration 47/1000 | Loss: 0.00000653
Iteration 48/1000 | Loss: 0.00000652
Iteration 49/1000 | Loss: 0.00000652
Iteration 50/1000 | Loss: 0.00000650
Iteration 51/1000 | Loss: 0.00000650
Iteration 52/1000 | Loss: 0.00000649
Iteration 53/1000 | Loss: 0.00000649
Iteration 54/1000 | Loss: 0.00000649
Iteration 55/1000 | Loss: 0.00000648
Iteration 56/1000 | Loss: 0.00000647
Iteration 57/1000 | Loss: 0.00000647
Iteration 58/1000 | Loss: 0.00000647
Iteration 59/1000 | Loss: 0.00000647
Iteration 60/1000 | Loss: 0.00000647
Iteration 61/1000 | Loss: 0.00000647
Iteration 62/1000 | Loss: 0.00000646
Iteration 63/1000 | Loss: 0.00000646
Iteration 64/1000 | Loss: 0.00000646
Iteration 65/1000 | Loss: 0.00000646
Iteration 66/1000 | Loss: 0.00000646
Iteration 67/1000 | Loss: 0.00000646
Iteration 68/1000 | Loss: 0.00000645
Iteration 69/1000 | Loss: 0.00000645
Iteration 70/1000 | Loss: 0.00000645
Iteration 71/1000 | Loss: 0.00000644
Iteration 72/1000 | Loss: 0.00000644
Iteration 73/1000 | Loss: 0.00000644
Iteration 74/1000 | Loss: 0.00000644
Iteration 75/1000 | Loss: 0.00000643
Iteration 76/1000 | Loss: 0.00000643
Iteration 77/1000 | Loss: 0.00000643
Iteration 78/1000 | Loss: 0.00000643
Iteration 79/1000 | Loss: 0.00000643
Iteration 80/1000 | Loss: 0.00000643
Iteration 81/1000 | Loss: 0.00000643
Iteration 82/1000 | Loss: 0.00000642
Iteration 83/1000 | Loss: 0.00000642
Iteration 84/1000 | Loss: 0.00000642
Iteration 85/1000 | Loss: 0.00000641
Iteration 86/1000 | Loss: 0.00000641
Iteration 87/1000 | Loss: 0.00000641
Iteration 88/1000 | Loss: 0.00000641
Iteration 89/1000 | Loss: 0.00000641
Iteration 90/1000 | Loss: 0.00000641
Iteration 91/1000 | Loss: 0.00000641
Iteration 92/1000 | Loss: 0.00000641
Iteration 93/1000 | Loss: 0.00000641
Iteration 94/1000 | Loss: 0.00000641
Iteration 95/1000 | Loss: 0.00000641
Iteration 96/1000 | Loss: 0.00000641
Iteration 97/1000 | Loss: 0.00000640
Iteration 98/1000 | Loss: 0.00000640
Iteration 99/1000 | Loss: 0.00000640
Iteration 100/1000 | Loss: 0.00000640
Iteration 101/1000 | Loss: 0.00000640
Iteration 102/1000 | Loss: 0.00000639
Iteration 103/1000 | Loss: 0.00000639
Iteration 104/1000 | Loss: 0.00000639
Iteration 105/1000 | Loss: 0.00000639
Iteration 106/1000 | Loss: 0.00000639
Iteration 107/1000 | Loss: 0.00000639
Iteration 108/1000 | Loss: 0.00000638
Iteration 109/1000 | Loss: 0.00000638
Iteration 110/1000 | Loss: 0.00000637
Iteration 111/1000 | Loss: 0.00000637
Iteration 112/1000 | Loss: 0.00000637
Iteration 113/1000 | Loss: 0.00000637
Iteration 114/1000 | Loss: 0.00000637
Iteration 115/1000 | Loss: 0.00000637
Iteration 116/1000 | Loss: 0.00000637
Iteration 117/1000 | Loss: 0.00000637
Iteration 118/1000 | Loss: 0.00000637
Iteration 119/1000 | Loss: 0.00000637
Iteration 120/1000 | Loss: 0.00000637
Iteration 121/1000 | Loss: 0.00000637
Iteration 122/1000 | Loss: 0.00000637
Iteration 123/1000 | Loss: 0.00000636
Iteration 124/1000 | Loss: 0.00000636
Iteration 125/1000 | Loss: 0.00000636
Iteration 126/1000 | Loss: 0.00000636
Iteration 127/1000 | Loss: 0.00000636
Iteration 128/1000 | Loss: 0.00000636
Iteration 129/1000 | Loss: 0.00000636
Iteration 130/1000 | Loss: 0.00000636
Iteration 131/1000 | Loss: 0.00000636
Iteration 132/1000 | Loss: 0.00000636
Iteration 133/1000 | Loss: 0.00000636
Iteration 134/1000 | Loss: 0.00000636
Iteration 135/1000 | Loss: 0.00000635
Iteration 136/1000 | Loss: 0.00000635
Iteration 137/1000 | Loss: 0.00000635
Iteration 138/1000 | Loss: 0.00000634
Iteration 139/1000 | Loss: 0.00000634
Iteration 140/1000 | Loss: 0.00000634
Iteration 141/1000 | Loss: 0.00000633
Iteration 142/1000 | Loss: 0.00000633
Iteration 143/1000 | Loss: 0.00000633
Iteration 144/1000 | Loss: 0.00000633
Iteration 145/1000 | Loss: 0.00000633
Iteration 146/1000 | Loss: 0.00000633
Iteration 147/1000 | Loss: 0.00000633
Iteration 148/1000 | Loss: 0.00000633
Iteration 149/1000 | Loss: 0.00000633
Iteration 150/1000 | Loss: 0.00000632
Iteration 151/1000 | Loss: 0.00000632
Iteration 152/1000 | Loss: 0.00000632
Iteration 153/1000 | Loss: 0.00000632
Iteration 154/1000 | Loss: 0.00000632
Iteration 155/1000 | Loss: 0.00000632
Iteration 156/1000 | Loss: 0.00000632
Iteration 157/1000 | Loss: 0.00000632
Iteration 158/1000 | Loss: 0.00000632
Iteration 159/1000 | Loss: 0.00000632
Iteration 160/1000 | Loss: 0.00000632
Iteration 161/1000 | Loss: 0.00000632
Iteration 162/1000 | Loss: 0.00000632
Iteration 163/1000 | Loss: 0.00000632
Iteration 164/1000 | Loss: 0.00000632
Iteration 165/1000 | Loss: 0.00000632
Iteration 166/1000 | Loss: 0.00000632
Iteration 167/1000 | Loss: 0.00000632
Iteration 168/1000 | Loss: 0.00000632
Iteration 169/1000 | Loss: 0.00000632
Iteration 170/1000 | Loss: 0.00000632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [6.315700829873094e-06, 6.315700829873094e-06, 6.315700829873094e-06, 6.315700829873094e-06, 6.315700829873094e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.315700829873094e-06

Optimization complete. Final v2v error: 2.161766290664673 mm

Highest mean error: 2.360429525375366 mm for frame 130

Lowest mean error: 2.0568621158599854 mm for frame 61

Saving results

Total time: 32.13933181762695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903873
Iteration 2/25 | Loss: 0.00313568
Iteration 3/25 | Loss: 0.00206131
Iteration 4/25 | Loss: 0.00159612
Iteration 5/25 | Loss: 0.00156061
Iteration 6/25 | Loss: 0.00143861
Iteration 7/25 | Loss: 0.00140984
Iteration 8/25 | Loss: 0.00140903
Iteration 9/25 | Loss: 0.00136827
Iteration 10/25 | Loss: 0.00137924
Iteration 11/25 | Loss: 0.00132274
Iteration 12/25 | Loss: 0.00129194
Iteration 13/25 | Loss: 0.00129885
Iteration 14/25 | Loss: 0.00129448
Iteration 15/25 | Loss: 0.00128477
Iteration 16/25 | Loss: 0.00128962
Iteration 17/25 | Loss: 0.00129035
Iteration 18/25 | Loss: 0.00127915
Iteration 19/25 | Loss: 0.00128507
Iteration 20/25 | Loss: 0.00128294
Iteration 21/25 | Loss: 0.00128011
Iteration 22/25 | Loss: 0.00128095
Iteration 23/25 | Loss: 0.00128095
Iteration 24/25 | Loss: 0.00128109
Iteration 25/25 | Loss: 0.00128247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.35402966
Iteration 2/25 | Loss: 0.00332138
Iteration 3/25 | Loss: 0.00332138
Iteration 4/25 | Loss: 0.00332138
Iteration 5/25 | Loss: 0.00332138
Iteration 6/25 | Loss: 0.00332138
Iteration 7/25 | Loss: 0.00332137
Iteration 8/25 | Loss: 0.00332137
Iteration 9/25 | Loss: 0.00332137
Iteration 10/25 | Loss: 0.00332137
Iteration 11/25 | Loss: 0.00332137
Iteration 12/25 | Loss: 0.00332137
Iteration 13/25 | Loss: 0.00332137
Iteration 14/25 | Loss: 0.00332137
Iteration 15/25 | Loss: 0.00332137
Iteration 16/25 | Loss: 0.00332137
Iteration 17/25 | Loss: 0.00332137
Iteration 18/25 | Loss: 0.00332137
Iteration 19/25 | Loss: 0.00332137
Iteration 20/25 | Loss: 0.00332137
Iteration 21/25 | Loss: 0.00332137
Iteration 22/25 | Loss: 0.00332137
Iteration 23/25 | Loss: 0.00332137
Iteration 24/25 | Loss: 0.00332137
Iteration 25/25 | Loss: 0.00332137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332137
Iteration 2/1000 | Loss: 0.00096711
Iteration 3/1000 | Loss: 0.00295678
Iteration 4/1000 | Loss: 0.00397673
Iteration 5/1000 | Loss: 0.00038850
Iteration 6/1000 | Loss: 0.00695866
Iteration 7/1000 | Loss: 0.00121378
Iteration 8/1000 | Loss: 0.00438636
Iteration 9/1000 | Loss: 0.00644935
Iteration 10/1000 | Loss: 0.00131554
Iteration 11/1000 | Loss: 0.00224171
Iteration 12/1000 | Loss: 0.00046718
Iteration 13/1000 | Loss: 0.00034683
Iteration 14/1000 | Loss: 0.00024154
Iteration 15/1000 | Loss: 0.00131881
Iteration 16/1000 | Loss: 0.00025805
Iteration 17/1000 | Loss: 0.00035332
Iteration 18/1000 | Loss: 0.00240170
Iteration 19/1000 | Loss: 0.00061672
Iteration 20/1000 | Loss: 0.00088425
Iteration 21/1000 | Loss: 0.00093688
Iteration 22/1000 | Loss: 0.00098669
Iteration 23/1000 | Loss: 0.00040544
Iteration 24/1000 | Loss: 0.00366302
Iteration 25/1000 | Loss: 0.00086071
Iteration 26/1000 | Loss: 0.00060893
Iteration 27/1000 | Loss: 0.00046728
Iteration 28/1000 | Loss: 0.00023073
Iteration 29/1000 | Loss: 0.00108713
Iteration 30/1000 | Loss: 0.00015086
Iteration 31/1000 | Loss: 0.00012347
Iteration 32/1000 | Loss: 0.00015234
Iteration 33/1000 | Loss: 0.00011948
Iteration 34/1000 | Loss: 0.00010857
Iteration 35/1000 | Loss: 0.00044925
Iteration 36/1000 | Loss: 0.00134477
Iteration 37/1000 | Loss: 0.00118528
Iteration 38/1000 | Loss: 0.00162024
Iteration 39/1000 | Loss: 0.00013949
Iteration 40/1000 | Loss: 0.00055683
Iteration 41/1000 | Loss: 0.00010559
Iteration 42/1000 | Loss: 0.00009462
Iteration 43/1000 | Loss: 0.00011305
Iteration 44/1000 | Loss: 0.00010202
Iteration 45/1000 | Loss: 0.00011074
Iteration 46/1000 | Loss: 0.00011912
Iteration 47/1000 | Loss: 0.00010485
Iteration 48/1000 | Loss: 0.00009721
Iteration 49/1000 | Loss: 0.00015030
Iteration 50/1000 | Loss: 0.00009785
Iteration 51/1000 | Loss: 0.00008679
Iteration 52/1000 | Loss: 0.00010329
Iteration 53/1000 | Loss: 0.00009160
Iteration 54/1000 | Loss: 0.00009779
Iteration 55/1000 | Loss: 0.00009078
Iteration 56/1000 | Loss: 0.00008430
Iteration 57/1000 | Loss: 0.00009767
Iteration 58/1000 | Loss: 0.00009107
Iteration 59/1000 | Loss: 0.00009275
Iteration 60/1000 | Loss: 0.00009436
Iteration 61/1000 | Loss: 0.00008789
Iteration 62/1000 | Loss: 0.00009304
Iteration 63/1000 | Loss: 0.00012576
Iteration 64/1000 | Loss: 0.00010314
Iteration 65/1000 | Loss: 0.00009404
Iteration 66/1000 | Loss: 0.00010003
Iteration 67/1000 | Loss: 0.00010190
Iteration 68/1000 | Loss: 0.00008556
Iteration 69/1000 | Loss: 0.00006835
Iteration 70/1000 | Loss: 0.00007985
Iteration 71/1000 | Loss: 0.00008329
Iteration 72/1000 | Loss: 0.00121708
Iteration 73/1000 | Loss: 0.00062080
Iteration 74/1000 | Loss: 0.00035194
Iteration 75/1000 | Loss: 0.00117713
Iteration 76/1000 | Loss: 0.00009397
Iteration 77/1000 | Loss: 0.00007522
Iteration 78/1000 | Loss: 0.00006299
Iteration 79/1000 | Loss: 0.00005728
Iteration 80/1000 | Loss: 0.00010404
Iteration 81/1000 | Loss: 0.00005454
Iteration 82/1000 | Loss: 0.00005175
Iteration 83/1000 | Loss: 0.00004890
Iteration 84/1000 | Loss: 0.00004704
Iteration 85/1000 | Loss: 0.00004497
Iteration 86/1000 | Loss: 0.00004340
Iteration 87/1000 | Loss: 0.00098594
Iteration 88/1000 | Loss: 0.00006057
Iteration 89/1000 | Loss: 0.00004561
Iteration 90/1000 | Loss: 0.00110335
Iteration 91/1000 | Loss: 0.00006123
Iteration 92/1000 | Loss: 0.00004345
Iteration 93/1000 | Loss: 0.00003741
Iteration 94/1000 | Loss: 0.00003534
Iteration 95/1000 | Loss: 0.00003434
Iteration 96/1000 | Loss: 0.00003374
Iteration 97/1000 | Loss: 0.00003326
Iteration 98/1000 | Loss: 0.00003290
Iteration 99/1000 | Loss: 0.00003248
Iteration 100/1000 | Loss: 0.00003224
Iteration 101/1000 | Loss: 0.00003203
Iteration 102/1000 | Loss: 0.00003187
Iteration 103/1000 | Loss: 0.00003180
Iteration 104/1000 | Loss: 0.00003165
Iteration 105/1000 | Loss: 0.00003155
Iteration 106/1000 | Loss: 0.00003155
Iteration 107/1000 | Loss: 0.00074966
Iteration 108/1000 | Loss: 0.00003289
Iteration 109/1000 | Loss: 0.00003034
Iteration 110/1000 | Loss: 0.00002908
Iteration 111/1000 | Loss: 0.00002858
Iteration 112/1000 | Loss: 0.00002810
Iteration 113/1000 | Loss: 0.00002783
Iteration 114/1000 | Loss: 0.00002779
Iteration 115/1000 | Loss: 0.00002779
Iteration 116/1000 | Loss: 0.00002773
Iteration 117/1000 | Loss: 0.00002770
Iteration 118/1000 | Loss: 0.00002770
Iteration 119/1000 | Loss: 0.00002768
Iteration 120/1000 | Loss: 0.00002767
Iteration 121/1000 | Loss: 0.00002758
Iteration 122/1000 | Loss: 0.00002758
Iteration 123/1000 | Loss: 0.00002758
Iteration 124/1000 | Loss: 0.00002757
Iteration 125/1000 | Loss: 0.00002756
Iteration 126/1000 | Loss: 0.00002756
Iteration 127/1000 | Loss: 0.00002755
Iteration 128/1000 | Loss: 0.00002754
Iteration 129/1000 | Loss: 0.00002754
Iteration 130/1000 | Loss: 0.00002753
Iteration 131/1000 | Loss: 0.00002753
Iteration 132/1000 | Loss: 0.00002753
Iteration 133/1000 | Loss: 0.00002753
Iteration 134/1000 | Loss: 0.00002751
Iteration 135/1000 | Loss: 0.00002747
Iteration 136/1000 | Loss: 0.00002747
Iteration 137/1000 | Loss: 0.00002747
Iteration 138/1000 | Loss: 0.00002747
Iteration 139/1000 | Loss: 0.00002747
Iteration 140/1000 | Loss: 0.00002747
Iteration 141/1000 | Loss: 0.00002747
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002747
Iteration 144/1000 | Loss: 0.00002746
Iteration 145/1000 | Loss: 0.00002746
Iteration 146/1000 | Loss: 0.00002746
Iteration 147/1000 | Loss: 0.00002746
Iteration 148/1000 | Loss: 0.00002746
Iteration 149/1000 | Loss: 0.00002746
Iteration 150/1000 | Loss: 0.00002746
Iteration 151/1000 | Loss: 0.00002746
Iteration 152/1000 | Loss: 0.00002746
Iteration 153/1000 | Loss: 0.00002746
Iteration 154/1000 | Loss: 0.00002745
Iteration 155/1000 | Loss: 0.00002745
Iteration 156/1000 | Loss: 0.00002745
Iteration 157/1000 | Loss: 0.00002745
Iteration 158/1000 | Loss: 0.00002745
Iteration 159/1000 | Loss: 0.00002745
Iteration 160/1000 | Loss: 0.00002745
Iteration 161/1000 | Loss: 0.00002745
Iteration 162/1000 | Loss: 0.00002745
Iteration 163/1000 | Loss: 0.00002745
Iteration 164/1000 | Loss: 0.00002745
Iteration 165/1000 | Loss: 0.00002745
Iteration 166/1000 | Loss: 0.00002745
Iteration 167/1000 | Loss: 0.00002745
Iteration 168/1000 | Loss: 0.00002745
Iteration 169/1000 | Loss: 0.00002744
Iteration 170/1000 | Loss: 0.00002744
Iteration 171/1000 | Loss: 0.00002744
Iteration 172/1000 | Loss: 0.00002744
Iteration 173/1000 | Loss: 0.00002744
Iteration 174/1000 | Loss: 0.00002744
Iteration 175/1000 | Loss: 0.00002744
Iteration 176/1000 | Loss: 0.00002744
Iteration 177/1000 | Loss: 0.00002744
Iteration 178/1000 | Loss: 0.00002744
Iteration 179/1000 | Loss: 0.00002744
Iteration 180/1000 | Loss: 0.00002744
Iteration 181/1000 | Loss: 0.00002744
Iteration 182/1000 | Loss: 0.00002743
Iteration 183/1000 | Loss: 0.00002743
Iteration 184/1000 | Loss: 0.00002743
Iteration 185/1000 | Loss: 0.00002743
Iteration 186/1000 | Loss: 0.00002743
Iteration 187/1000 | Loss: 0.00002743
Iteration 188/1000 | Loss: 0.00002743
Iteration 189/1000 | Loss: 0.00002743
Iteration 190/1000 | Loss: 0.00002742
Iteration 191/1000 | Loss: 0.00002742
Iteration 192/1000 | Loss: 0.00002742
Iteration 193/1000 | Loss: 0.00002742
Iteration 194/1000 | Loss: 0.00002742
Iteration 195/1000 | Loss: 0.00002742
Iteration 196/1000 | Loss: 0.00002742
Iteration 197/1000 | Loss: 0.00002742
Iteration 198/1000 | Loss: 0.00002742
Iteration 199/1000 | Loss: 0.00002742
Iteration 200/1000 | Loss: 0.00002742
Iteration 201/1000 | Loss: 0.00002742
Iteration 202/1000 | Loss: 0.00002742
Iteration 203/1000 | Loss: 0.00002741
Iteration 204/1000 | Loss: 0.00002741
Iteration 205/1000 | Loss: 0.00002741
Iteration 206/1000 | Loss: 0.00002741
Iteration 207/1000 | Loss: 0.00002741
Iteration 208/1000 | Loss: 0.00002741
Iteration 209/1000 | Loss: 0.00002740
Iteration 210/1000 | Loss: 0.00002740
Iteration 211/1000 | Loss: 0.00002740
Iteration 212/1000 | Loss: 0.00002740
Iteration 213/1000 | Loss: 0.00002740
Iteration 214/1000 | Loss: 0.00002740
Iteration 215/1000 | Loss: 0.00002740
Iteration 216/1000 | Loss: 0.00002740
Iteration 217/1000 | Loss: 0.00002739
Iteration 218/1000 | Loss: 0.00002739
Iteration 219/1000 | Loss: 0.00002739
Iteration 220/1000 | Loss: 0.00002739
Iteration 221/1000 | Loss: 0.00002739
Iteration 222/1000 | Loss: 0.00002739
Iteration 223/1000 | Loss: 0.00002739
Iteration 224/1000 | Loss: 0.00002739
Iteration 225/1000 | Loss: 0.00002739
Iteration 226/1000 | Loss: 0.00002739
Iteration 227/1000 | Loss: 0.00002739
Iteration 228/1000 | Loss: 0.00002739
Iteration 229/1000 | Loss: 0.00002739
Iteration 230/1000 | Loss: 0.00002739
Iteration 231/1000 | Loss: 0.00002739
Iteration 232/1000 | Loss: 0.00002739
Iteration 233/1000 | Loss: 0.00002739
Iteration 234/1000 | Loss: 0.00002739
Iteration 235/1000 | Loss: 0.00002739
Iteration 236/1000 | Loss: 0.00002739
Iteration 237/1000 | Loss: 0.00002739
Iteration 238/1000 | Loss: 0.00002739
Iteration 239/1000 | Loss: 0.00002739
Iteration 240/1000 | Loss: 0.00002739
Iteration 241/1000 | Loss: 0.00002739
Iteration 242/1000 | Loss: 0.00002739
Iteration 243/1000 | Loss: 0.00002739
Iteration 244/1000 | Loss: 0.00002739
Iteration 245/1000 | Loss: 0.00002739
Iteration 246/1000 | Loss: 0.00002739
Iteration 247/1000 | Loss: 0.00002739
Iteration 248/1000 | Loss: 0.00002739
Iteration 249/1000 | Loss: 0.00002739
Iteration 250/1000 | Loss: 0.00002739
Iteration 251/1000 | Loss: 0.00002739
Iteration 252/1000 | Loss: 0.00002739
Iteration 253/1000 | Loss: 0.00002739
Iteration 254/1000 | Loss: 0.00002739
Iteration 255/1000 | Loss: 0.00002739
Iteration 256/1000 | Loss: 0.00002739
Iteration 257/1000 | Loss: 0.00002739
Iteration 258/1000 | Loss: 0.00002739
Iteration 259/1000 | Loss: 0.00002739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [2.7388365197111852e-05, 2.7388365197111852e-05, 2.7388365197111852e-05, 2.7388365197111852e-05, 2.7388365197111852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7388365197111852e-05

Optimization complete. Final v2v error: 3.8410871028900146 mm

Highest mean error: 13.230582237243652 mm for frame 32

Lowest mean error: 2.7207934856414795 mm for frame 2

Saving results

Total time: 211.07175707817078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509207
Iteration 2/25 | Loss: 0.00110768
Iteration 3/25 | Loss: 0.00095196
Iteration 4/25 | Loss: 0.00092752
Iteration 5/25 | Loss: 0.00091739
Iteration 6/25 | Loss: 0.00091533
Iteration 7/25 | Loss: 0.00091533
Iteration 8/25 | Loss: 0.00091533
Iteration 9/25 | Loss: 0.00091533
Iteration 10/25 | Loss: 0.00091533
Iteration 11/25 | Loss: 0.00091533
Iteration 12/25 | Loss: 0.00091533
Iteration 13/25 | Loss: 0.00091533
Iteration 14/25 | Loss: 0.00091533
Iteration 15/25 | Loss: 0.00091533
Iteration 16/25 | Loss: 0.00091533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009153317078016698, 0.0009153317078016698, 0.0009153317078016698, 0.0009153317078016698, 0.0009153317078016698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009153317078016698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78498709
Iteration 2/25 | Loss: 0.00045675
Iteration 3/25 | Loss: 0.00045675
Iteration 4/25 | Loss: 0.00045675
Iteration 5/25 | Loss: 0.00045675
Iteration 6/25 | Loss: 0.00045675
Iteration 7/25 | Loss: 0.00045675
Iteration 8/25 | Loss: 0.00045675
Iteration 9/25 | Loss: 0.00045675
Iteration 10/25 | Loss: 0.00045675
Iteration 11/25 | Loss: 0.00045675
Iteration 12/25 | Loss: 0.00045675
Iteration 13/25 | Loss: 0.00045675
Iteration 14/25 | Loss: 0.00045675
Iteration 15/25 | Loss: 0.00045675
Iteration 16/25 | Loss: 0.00045675
Iteration 17/25 | Loss: 0.00045675
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004567463474813849, 0.0004567463474813849, 0.0004567463474813849, 0.0004567463474813849, 0.0004567463474813849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004567463474813849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045675
Iteration 2/1000 | Loss: 0.00003936
Iteration 3/1000 | Loss: 0.00002732
Iteration 4/1000 | Loss: 0.00002542
Iteration 5/1000 | Loss: 0.00002424
Iteration 6/1000 | Loss: 0.00002368
Iteration 7/1000 | Loss: 0.00002316
Iteration 8/1000 | Loss: 0.00002260
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002186
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002116
Iteration 14/1000 | Loss: 0.00002101
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002093
Iteration 17/1000 | Loss: 0.00002087
Iteration 18/1000 | Loss: 0.00002074
Iteration 19/1000 | Loss: 0.00002071
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002051
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002049
Iteration 28/1000 | Loss: 0.00002048
Iteration 29/1000 | Loss: 0.00002045
Iteration 30/1000 | Loss: 0.00002036
Iteration 31/1000 | Loss: 0.00002030
Iteration 32/1000 | Loss: 0.00002027
Iteration 33/1000 | Loss: 0.00002027
Iteration 34/1000 | Loss: 0.00002026
Iteration 35/1000 | Loss: 0.00002026
Iteration 36/1000 | Loss: 0.00002025
Iteration 37/1000 | Loss: 0.00002025
Iteration 38/1000 | Loss: 0.00002024
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002023
Iteration 47/1000 | Loss: 0.00002021
Iteration 48/1000 | Loss: 0.00002021
Iteration 49/1000 | Loss: 0.00002021
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002019
Iteration 58/1000 | Loss: 0.00002015
Iteration 59/1000 | Loss: 0.00002012
Iteration 60/1000 | Loss: 0.00002011
Iteration 61/1000 | Loss: 0.00002011
Iteration 62/1000 | Loss: 0.00002010
Iteration 63/1000 | Loss: 0.00002009
Iteration 64/1000 | Loss: 0.00002009
Iteration 65/1000 | Loss: 0.00002009
Iteration 66/1000 | Loss: 0.00002009
Iteration 67/1000 | Loss: 0.00002008
Iteration 68/1000 | Loss: 0.00002008
Iteration 69/1000 | Loss: 0.00002008
Iteration 70/1000 | Loss: 0.00002008
Iteration 71/1000 | Loss: 0.00002008
Iteration 72/1000 | Loss: 0.00002008
Iteration 73/1000 | Loss: 0.00002007
Iteration 74/1000 | Loss: 0.00002007
Iteration 75/1000 | Loss: 0.00002007
Iteration 76/1000 | Loss: 0.00002004
Iteration 77/1000 | Loss: 0.00002004
Iteration 78/1000 | Loss: 0.00002003
Iteration 79/1000 | Loss: 0.00002003
Iteration 80/1000 | Loss: 0.00002002
Iteration 81/1000 | Loss: 0.00002002
Iteration 82/1000 | Loss: 0.00002002
Iteration 83/1000 | Loss: 0.00002002
Iteration 84/1000 | Loss: 0.00002001
Iteration 85/1000 | Loss: 0.00002001
Iteration 86/1000 | Loss: 0.00002001
Iteration 87/1000 | Loss: 0.00002000
Iteration 88/1000 | Loss: 0.00002000
Iteration 89/1000 | Loss: 0.00002000
Iteration 90/1000 | Loss: 0.00002000
Iteration 91/1000 | Loss: 0.00002000
Iteration 92/1000 | Loss: 0.00002000
Iteration 93/1000 | Loss: 0.00002000
Iteration 94/1000 | Loss: 0.00001999
Iteration 95/1000 | Loss: 0.00001999
Iteration 96/1000 | Loss: 0.00001999
Iteration 97/1000 | Loss: 0.00001999
Iteration 98/1000 | Loss: 0.00001999
Iteration 99/1000 | Loss: 0.00001999
Iteration 100/1000 | Loss: 0.00001998
Iteration 101/1000 | Loss: 0.00001998
Iteration 102/1000 | Loss: 0.00001998
Iteration 103/1000 | Loss: 0.00001998
Iteration 104/1000 | Loss: 0.00001998
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001998
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001997
Iteration 112/1000 | Loss: 0.00001997
Iteration 113/1000 | Loss: 0.00001997
Iteration 114/1000 | Loss: 0.00001997
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00001997
Iteration 117/1000 | Loss: 0.00001997
Iteration 118/1000 | Loss: 0.00001997
Iteration 119/1000 | Loss: 0.00001996
Iteration 120/1000 | Loss: 0.00001996
Iteration 121/1000 | Loss: 0.00001996
Iteration 122/1000 | Loss: 0.00001996
Iteration 123/1000 | Loss: 0.00001996
Iteration 124/1000 | Loss: 0.00001995
Iteration 125/1000 | Loss: 0.00001995
Iteration 126/1000 | Loss: 0.00001995
Iteration 127/1000 | Loss: 0.00001995
Iteration 128/1000 | Loss: 0.00001994
Iteration 129/1000 | Loss: 0.00001994
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001994
Iteration 133/1000 | Loss: 0.00001994
Iteration 134/1000 | Loss: 0.00001994
Iteration 135/1000 | Loss: 0.00001993
Iteration 136/1000 | Loss: 0.00001993
Iteration 137/1000 | Loss: 0.00001993
Iteration 138/1000 | Loss: 0.00001992
Iteration 139/1000 | Loss: 0.00001992
Iteration 140/1000 | Loss: 0.00001991
Iteration 141/1000 | Loss: 0.00001991
Iteration 142/1000 | Loss: 0.00001991
Iteration 143/1000 | Loss: 0.00001990
Iteration 144/1000 | Loss: 0.00001990
Iteration 145/1000 | Loss: 0.00001990
Iteration 146/1000 | Loss: 0.00001990
Iteration 147/1000 | Loss: 0.00001990
Iteration 148/1000 | Loss: 0.00001989
Iteration 149/1000 | Loss: 0.00001989
Iteration 150/1000 | Loss: 0.00001989
Iteration 151/1000 | Loss: 0.00001989
Iteration 152/1000 | Loss: 0.00001989
Iteration 153/1000 | Loss: 0.00001989
Iteration 154/1000 | Loss: 0.00001989
Iteration 155/1000 | Loss: 0.00001989
Iteration 156/1000 | Loss: 0.00001989
Iteration 157/1000 | Loss: 0.00001989
Iteration 158/1000 | Loss: 0.00001989
Iteration 159/1000 | Loss: 0.00001989
Iteration 160/1000 | Loss: 0.00001989
Iteration 161/1000 | Loss: 0.00001989
Iteration 162/1000 | Loss: 0.00001988
Iteration 163/1000 | Loss: 0.00001988
Iteration 164/1000 | Loss: 0.00001988
Iteration 165/1000 | Loss: 0.00001988
Iteration 166/1000 | Loss: 0.00001988
Iteration 167/1000 | Loss: 0.00001988
Iteration 168/1000 | Loss: 0.00001988
Iteration 169/1000 | Loss: 0.00001988
Iteration 170/1000 | Loss: 0.00001988
Iteration 171/1000 | Loss: 0.00001987
Iteration 172/1000 | Loss: 0.00001987
Iteration 173/1000 | Loss: 0.00001987
Iteration 174/1000 | Loss: 0.00001987
Iteration 175/1000 | Loss: 0.00001987
Iteration 176/1000 | Loss: 0.00001987
Iteration 177/1000 | Loss: 0.00001987
Iteration 178/1000 | Loss: 0.00001987
Iteration 179/1000 | Loss: 0.00001987
Iteration 180/1000 | Loss: 0.00001987
Iteration 181/1000 | Loss: 0.00001987
Iteration 182/1000 | Loss: 0.00001987
Iteration 183/1000 | Loss: 0.00001987
Iteration 184/1000 | Loss: 0.00001987
Iteration 185/1000 | Loss: 0.00001987
Iteration 186/1000 | Loss: 0.00001987
Iteration 187/1000 | Loss: 0.00001987
Iteration 188/1000 | Loss: 0.00001987
Iteration 189/1000 | Loss: 0.00001987
Iteration 190/1000 | Loss: 0.00001986
Iteration 191/1000 | Loss: 0.00001986
Iteration 192/1000 | Loss: 0.00001986
Iteration 193/1000 | Loss: 0.00001986
Iteration 194/1000 | Loss: 0.00001986
Iteration 195/1000 | Loss: 0.00001986
Iteration 196/1000 | Loss: 0.00001986
Iteration 197/1000 | Loss: 0.00001986
Iteration 198/1000 | Loss: 0.00001986
Iteration 199/1000 | Loss: 0.00001986
Iteration 200/1000 | Loss: 0.00001986
Iteration 201/1000 | Loss: 0.00001985
Iteration 202/1000 | Loss: 0.00001985
Iteration 203/1000 | Loss: 0.00001985
Iteration 204/1000 | Loss: 0.00001985
Iteration 205/1000 | Loss: 0.00001985
Iteration 206/1000 | Loss: 0.00001985
Iteration 207/1000 | Loss: 0.00001985
Iteration 208/1000 | Loss: 0.00001985
Iteration 209/1000 | Loss: 0.00001985
Iteration 210/1000 | Loss: 0.00001985
Iteration 211/1000 | Loss: 0.00001985
Iteration 212/1000 | Loss: 0.00001985
Iteration 213/1000 | Loss: 0.00001985
Iteration 214/1000 | Loss: 0.00001984
Iteration 215/1000 | Loss: 0.00001984
Iteration 216/1000 | Loss: 0.00001984
Iteration 217/1000 | Loss: 0.00001984
Iteration 218/1000 | Loss: 0.00001984
Iteration 219/1000 | Loss: 0.00001984
Iteration 220/1000 | Loss: 0.00001984
Iteration 221/1000 | Loss: 0.00001984
Iteration 222/1000 | Loss: 0.00001984
Iteration 223/1000 | Loss: 0.00001984
Iteration 224/1000 | Loss: 0.00001984
Iteration 225/1000 | Loss: 0.00001983
Iteration 226/1000 | Loss: 0.00001983
Iteration 227/1000 | Loss: 0.00001983
Iteration 228/1000 | Loss: 0.00001983
Iteration 229/1000 | Loss: 0.00001983
Iteration 230/1000 | Loss: 0.00001983
Iteration 231/1000 | Loss: 0.00001983
Iteration 232/1000 | Loss: 0.00001983
Iteration 233/1000 | Loss: 0.00001983
Iteration 234/1000 | Loss: 0.00001983
Iteration 235/1000 | Loss: 0.00001983
Iteration 236/1000 | Loss: 0.00001983
Iteration 237/1000 | Loss: 0.00001983
Iteration 238/1000 | Loss: 0.00001983
Iteration 239/1000 | Loss: 0.00001983
Iteration 240/1000 | Loss: 0.00001983
Iteration 241/1000 | Loss: 0.00001983
Iteration 242/1000 | Loss: 0.00001983
Iteration 243/1000 | Loss: 0.00001983
Iteration 244/1000 | Loss: 0.00001983
Iteration 245/1000 | Loss: 0.00001983
Iteration 246/1000 | Loss: 0.00001983
Iteration 247/1000 | Loss: 0.00001983
Iteration 248/1000 | Loss: 0.00001983
Iteration 249/1000 | Loss: 0.00001983
Iteration 250/1000 | Loss: 0.00001983
Iteration 251/1000 | Loss: 0.00001983
Iteration 252/1000 | Loss: 0.00001983
Iteration 253/1000 | Loss: 0.00001983
Iteration 254/1000 | Loss: 0.00001983
Iteration 255/1000 | Loss: 0.00001983
Iteration 256/1000 | Loss: 0.00001983
Iteration 257/1000 | Loss: 0.00001983
Iteration 258/1000 | Loss: 0.00001983
Iteration 259/1000 | Loss: 0.00001983
Iteration 260/1000 | Loss: 0.00001983
Iteration 261/1000 | Loss: 0.00001983
Iteration 262/1000 | Loss: 0.00001983
Iteration 263/1000 | Loss: 0.00001983
Iteration 264/1000 | Loss: 0.00001983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.982955654966645e-05, 1.982955654966645e-05, 1.982955654966645e-05, 1.982955654966645e-05, 1.982955654966645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.982955654966645e-05

Optimization complete. Final v2v error: 3.7429981231689453 mm

Highest mean error: 3.941054344177246 mm for frame 4

Lowest mean error: 3.567085027694702 mm for frame 49

Saving results

Total time: 63.897873878479004
