Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=288, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16128-16183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384690
Iteration 2/25 | Loss: 0.00110826
Iteration 3/25 | Loss: 0.00099273
Iteration 4/25 | Loss: 0.00097564
Iteration 5/25 | Loss: 0.00097061
Iteration 6/25 | Loss: 0.00096990
Iteration 7/25 | Loss: 0.00096990
Iteration 8/25 | Loss: 0.00096990
Iteration 9/25 | Loss: 0.00096990
Iteration 10/25 | Loss: 0.00096990
Iteration 11/25 | Loss: 0.00096990
Iteration 12/25 | Loss: 0.00096990
Iteration 13/25 | Loss: 0.00096990
Iteration 14/25 | Loss: 0.00096990
Iteration 15/25 | Loss: 0.00096990
Iteration 16/25 | Loss: 0.00096990
Iteration 17/25 | Loss: 0.00096990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009698966750875115, 0.0009698966750875115, 0.0009698966750875115, 0.0009698966750875115, 0.0009698966750875115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009698966750875115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35792327
Iteration 2/25 | Loss: 0.00092980
Iteration 3/25 | Loss: 0.00092980
Iteration 4/25 | Loss: 0.00092980
Iteration 5/25 | Loss: 0.00092980
Iteration 6/25 | Loss: 0.00092980
Iteration 7/25 | Loss: 0.00092980
Iteration 8/25 | Loss: 0.00092980
Iteration 9/25 | Loss: 0.00092980
Iteration 10/25 | Loss: 0.00092980
Iteration 11/25 | Loss: 0.00092980
Iteration 12/25 | Loss: 0.00092980
Iteration 13/25 | Loss: 0.00092980
Iteration 14/25 | Loss: 0.00092980
Iteration 15/25 | Loss: 0.00092980
Iteration 16/25 | Loss: 0.00092980
Iteration 17/25 | Loss: 0.00092980
Iteration 18/25 | Loss: 0.00092980
Iteration 19/25 | Loss: 0.00092980
Iteration 20/25 | Loss: 0.00092980
Iteration 21/25 | Loss: 0.00092980
Iteration 22/25 | Loss: 0.00092980
Iteration 23/25 | Loss: 0.00092980
Iteration 24/25 | Loss: 0.00092980
Iteration 25/25 | Loss: 0.00092980

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092980
Iteration 2/1000 | Loss: 0.00003668
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00001900
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001488
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001341
Iteration 61/1000 | Loss: 0.00001341
Iteration 62/1000 | Loss: 0.00001341
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001336
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001336
Iteration 76/1000 | Loss: 0.00001336
Iteration 77/1000 | Loss: 0.00001336
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001329
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001326
Iteration 142/1000 | Loss: 0.00001326
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001324
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001323
Iteration 161/1000 | Loss: 0.00001323
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001323
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001323
Iteration 174/1000 | Loss: 0.00001323
Iteration 175/1000 | Loss: 0.00001323
Iteration 176/1000 | Loss: 0.00001323
Iteration 177/1000 | Loss: 0.00001323
Iteration 178/1000 | Loss: 0.00001323
Iteration 179/1000 | Loss: 0.00001323
Iteration 180/1000 | Loss: 0.00001323
Iteration 181/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.3229012438387144e-05, 1.3229012438387144e-05, 1.3229012438387144e-05, 1.3229012438387144e-05, 1.3229012438387144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3229012438387144e-05

Optimization complete. Final v2v error: 2.9765307903289795 mm

Highest mean error: 3.860426187515259 mm for frame 95

Lowest mean error: 2.426266670227051 mm for frame 73

Saving results

Total time: 45.39353322982788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428717
Iteration 2/25 | Loss: 0.00114355
Iteration 3/25 | Loss: 0.00103050
Iteration 4/25 | Loss: 0.00101474
Iteration 5/25 | Loss: 0.00101035
Iteration 6/25 | Loss: 0.00100931
Iteration 7/25 | Loss: 0.00100931
Iteration 8/25 | Loss: 0.00100931
Iteration 9/25 | Loss: 0.00100931
Iteration 10/25 | Loss: 0.00100931
Iteration 11/25 | Loss: 0.00100931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010093097807839513, 0.0010093097807839513, 0.0010093097807839513, 0.0010093097807839513, 0.0010093097807839513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010093097807839513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33692014
Iteration 2/25 | Loss: 0.00078082
Iteration 3/25 | Loss: 0.00078082
Iteration 4/25 | Loss: 0.00078082
Iteration 5/25 | Loss: 0.00078082
Iteration 6/25 | Loss: 0.00078082
Iteration 7/25 | Loss: 0.00078082
Iteration 8/25 | Loss: 0.00078082
Iteration 9/25 | Loss: 0.00078082
Iteration 10/25 | Loss: 0.00078082
Iteration 11/25 | Loss: 0.00078082
Iteration 12/25 | Loss: 0.00078082
Iteration 13/25 | Loss: 0.00078082
Iteration 14/25 | Loss: 0.00078082
Iteration 15/25 | Loss: 0.00078082
Iteration 16/25 | Loss: 0.00078082
Iteration 17/25 | Loss: 0.00078082
Iteration 18/25 | Loss: 0.00078082
Iteration 19/25 | Loss: 0.00078082
Iteration 20/25 | Loss: 0.00078082
Iteration 21/25 | Loss: 0.00078082
Iteration 22/25 | Loss: 0.00078082
Iteration 23/25 | Loss: 0.00078082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007808167138136923, 0.0007808167138136923, 0.0007808167138136923, 0.0007808167138136923, 0.0007808167138136923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007808167138136923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078082
Iteration 2/1000 | Loss: 0.00003379
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00001822
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001564
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001450
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001429
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001425
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001419
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001417
Iteration 30/1000 | Loss: 0.00001417
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001402
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001401
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001396
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001396
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001384
Iteration 136/1000 | Loss: 0.00001384
Iteration 137/1000 | Loss: 0.00001384
Iteration 138/1000 | Loss: 0.00001384
Iteration 139/1000 | Loss: 0.00001384
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001383
Iteration 142/1000 | Loss: 0.00001383
Iteration 143/1000 | Loss: 0.00001383
Iteration 144/1000 | Loss: 0.00001383
Iteration 145/1000 | Loss: 0.00001383
Iteration 146/1000 | Loss: 0.00001383
Iteration 147/1000 | Loss: 0.00001383
Iteration 148/1000 | Loss: 0.00001383
Iteration 149/1000 | Loss: 0.00001383
Iteration 150/1000 | Loss: 0.00001382
Iteration 151/1000 | Loss: 0.00001382
Iteration 152/1000 | Loss: 0.00001382
Iteration 153/1000 | Loss: 0.00001382
Iteration 154/1000 | Loss: 0.00001382
Iteration 155/1000 | Loss: 0.00001382
Iteration 156/1000 | Loss: 0.00001382
Iteration 157/1000 | Loss: 0.00001382
Iteration 158/1000 | Loss: 0.00001382
Iteration 159/1000 | Loss: 0.00001382
Iteration 160/1000 | Loss: 0.00001382
Iteration 161/1000 | Loss: 0.00001382
Iteration 162/1000 | Loss: 0.00001381
Iteration 163/1000 | Loss: 0.00001381
Iteration 164/1000 | Loss: 0.00001381
Iteration 165/1000 | Loss: 0.00001381
Iteration 166/1000 | Loss: 0.00001380
Iteration 167/1000 | Loss: 0.00001380
Iteration 168/1000 | Loss: 0.00001380
Iteration 169/1000 | Loss: 0.00001380
Iteration 170/1000 | Loss: 0.00001380
Iteration 171/1000 | Loss: 0.00001380
Iteration 172/1000 | Loss: 0.00001379
Iteration 173/1000 | Loss: 0.00001379
Iteration 174/1000 | Loss: 0.00001379
Iteration 175/1000 | Loss: 0.00001379
Iteration 176/1000 | Loss: 0.00001379
Iteration 177/1000 | Loss: 0.00001379
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001379
Iteration 180/1000 | Loss: 0.00001379
Iteration 181/1000 | Loss: 0.00001379
Iteration 182/1000 | Loss: 0.00001379
Iteration 183/1000 | Loss: 0.00001379
Iteration 184/1000 | Loss: 0.00001379
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001378
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001378
Iteration 192/1000 | Loss: 0.00001378
Iteration 193/1000 | Loss: 0.00001378
Iteration 194/1000 | Loss: 0.00001378
Iteration 195/1000 | Loss: 0.00001378
Iteration 196/1000 | Loss: 0.00001378
Iteration 197/1000 | Loss: 0.00001378
Iteration 198/1000 | Loss: 0.00001378
Iteration 199/1000 | Loss: 0.00001378
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001377
Iteration 202/1000 | Loss: 0.00001377
Iteration 203/1000 | Loss: 0.00001377
Iteration 204/1000 | Loss: 0.00001377
Iteration 205/1000 | Loss: 0.00001377
Iteration 206/1000 | Loss: 0.00001377
Iteration 207/1000 | Loss: 0.00001377
Iteration 208/1000 | Loss: 0.00001377
Iteration 209/1000 | Loss: 0.00001377
Iteration 210/1000 | Loss: 0.00001377
Iteration 211/1000 | Loss: 0.00001377
Iteration 212/1000 | Loss: 0.00001376
Iteration 213/1000 | Loss: 0.00001376
Iteration 214/1000 | Loss: 0.00001376
Iteration 215/1000 | Loss: 0.00001376
Iteration 216/1000 | Loss: 0.00001376
Iteration 217/1000 | Loss: 0.00001375
Iteration 218/1000 | Loss: 0.00001375
Iteration 219/1000 | Loss: 0.00001375
Iteration 220/1000 | Loss: 0.00001375
Iteration 221/1000 | Loss: 0.00001375
Iteration 222/1000 | Loss: 0.00001375
Iteration 223/1000 | Loss: 0.00001375
Iteration 224/1000 | Loss: 0.00001374
Iteration 225/1000 | Loss: 0.00001374
Iteration 226/1000 | Loss: 0.00001374
Iteration 227/1000 | Loss: 0.00001374
Iteration 228/1000 | Loss: 0.00001374
Iteration 229/1000 | Loss: 0.00001374
Iteration 230/1000 | Loss: 0.00001374
Iteration 231/1000 | Loss: 0.00001374
Iteration 232/1000 | Loss: 0.00001374
Iteration 233/1000 | Loss: 0.00001374
Iteration 234/1000 | Loss: 0.00001373
Iteration 235/1000 | Loss: 0.00001373
Iteration 236/1000 | Loss: 0.00001373
Iteration 237/1000 | Loss: 0.00001373
Iteration 238/1000 | Loss: 0.00001373
Iteration 239/1000 | Loss: 0.00001373
Iteration 240/1000 | Loss: 0.00001373
Iteration 241/1000 | Loss: 0.00001373
Iteration 242/1000 | Loss: 0.00001372
Iteration 243/1000 | Loss: 0.00001372
Iteration 244/1000 | Loss: 0.00001372
Iteration 245/1000 | Loss: 0.00001372
Iteration 246/1000 | Loss: 0.00001372
Iteration 247/1000 | Loss: 0.00001372
Iteration 248/1000 | Loss: 0.00001372
Iteration 249/1000 | Loss: 0.00001372
Iteration 250/1000 | Loss: 0.00001372
Iteration 251/1000 | Loss: 0.00001372
Iteration 252/1000 | Loss: 0.00001372
Iteration 253/1000 | Loss: 0.00001372
Iteration 254/1000 | Loss: 0.00001372
Iteration 255/1000 | Loss: 0.00001372
Iteration 256/1000 | Loss: 0.00001372
Iteration 257/1000 | Loss: 0.00001371
Iteration 258/1000 | Loss: 0.00001371
Iteration 259/1000 | Loss: 0.00001371
Iteration 260/1000 | Loss: 0.00001371
Iteration 261/1000 | Loss: 0.00001371
Iteration 262/1000 | Loss: 0.00001371
Iteration 263/1000 | Loss: 0.00001371
Iteration 264/1000 | Loss: 0.00001371
Iteration 265/1000 | Loss: 0.00001371
Iteration 266/1000 | Loss: 0.00001371
Iteration 267/1000 | Loss: 0.00001371
Iteration 268/1000 | Loss: 0.00001371
Iteration 269/1000 | Loss: 0.00001371
Iteration 270/1000 | Loss: 0.00001371
Iteration 271/1000 | Loss: 0.00001370
Iteration 272/1000 | Loss: 0.00001370
Iteration 273/1000 | Loss: 0.00001370
Iteration 274/1000 | Loss: 0.00001370
Iteration 275/1000 | Loss: 0.00001370
Iteration 276/1000 | Loss: 0.00001370
Iteration 277/1000 | Loss: 0.00001370
Iteration 278/1000 | Loss: 0.00001370
Iteration 279/1000 | Loss: 0.00001370
Iteration 280/1000 | Loss: 0.00001370
Iteration 281/1000 | Loss: 0.00001370
Iteration 282/1000 | Loss: 0.00001370
Iteration 283/1000 | Loss: 0.00001370
Iteration 284/1000 | Loss: 0.00001370
Iteration 285/1000 | Loss: 0.00001370
Iteration 286/1000 | Loss: 0.00001370
Iteration 287/1000 | Loss: 0.00001370
Iteration 288/1000 | Loss: 0.00001370
Iteration 289/1000 | Loss: 0.00001370
Iteration 290/1000 | Loss: 0.00001370
Iteration 291/1000 | Loss: 0.00001370
Iteration 292/1000 | Loss: 0.00001370
Iteration 293/1000 | Loss: 0.00001370
Iteration 294/1000 | Loss: 0.00001370
Iteration 295/1000 | Loss: 0.00001369
Iteration 296/1000 | Loss: 0.00001369
Iteration 297/1000 | Loss: 0.00001369
Iteration 298/1000 | Loss: 0.00001369
Iteration 299/1000 | Loss: 0.00001369
Iteration 300/1000 | Loss: 0.00001369
Iteration 301/1000 | Loss: 0.00001369
Iteration 302/1000 | Loss: 0.00001369
Iteration 303/1000 | Loss: 0.00001369
Iteration 304/1000 | Loss: 0.00001369
Iteration 305/1000 | Loss: 0.00001369
Iteration 306/1000 | Loss: 0.00001369
Iteration 307/1000 | Loss: 0.00001369
Iteration 308/1000 | Loss: 0.00001369
Iteration 309/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.3693243090528995e-05, 1.3693243090528995e-05, 1.3693243090528995e-05, 1.3693243090528995e-05, 1.3693243090528995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3693243090528995e-05

Optimization complete. Final v2v error: 3.10229754447937 mm

Highest mean error: 3.4806535243988037 mm for frame 70

Lowest mean error: 2.790231943130493 mm for frame 23

Saving results

Total time: 46.4755220413208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915227
Iteration 2/25 | Loss: 0.00119730
Iteration 3/25 | Loss: 0.00109715
Iteration 4/25 | Loss: 0.00108789
Iteration 5/25 | Loss: 0.00108562
Iteration 6/25 | Loss: 0.00108515
Iteration 7/25 | Loss: 0.00108515
Iteration 8/25 | Loss: 0.00108515
Iteration 9/25 | Loss: 0.00108514
Iteration 10/25 | Loss: 0.00108514
Iteration 11/25 | Loss: 0.00108514
Iteration 12/25 | Loss: 0.00108514
Iteration 13/25 | Loss: 0.00108514
Iteration 14/25 | Loss: 0.00108514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010851433034986258, 0.0010851433034986258, 0.0010851433034986258, 0.0010851433034986258, 0.0010851433034986258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010851433034986258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23283768
Iteration 2/25 | Loss: 0.00085428
Iteration 3/25 | Loss: 0.00085426
Iteration 4/25 | Loss: 0.00085426
Iteration 5/25 | Loss: 0.00085426
Iteration 6/25 | Loss: 0.00085426
Iteration 7/25 | Loss: 0.00085426
Iteration 8/25 | Loss: 0.00085426
Iteration 9/25 | Loss: 0.00085426
Iteration 10/25 | Loss: 0.00085426
Iteration 11/25 | Loss: 0.00085426
Iteration 12/25 | Loss: 0.00085426
Iteration 13/25 | Loss: 0.00085426
Iteration 14/25 | Loss: 0.00085426
Iteration 15/25 | Loss: 0.00085426
Iteration 16/25 | Loss: 0.00085426
Iteration 17/25 | Loss: 0.00085426
Iteration 18/25 | Loss: 0.00085426
Iteration 19/25 | Loss: 0.00085426
Iteration 20/25 | Loss: 0.00085426
Iteration 21/25 | Loss: 0.00085426
Iteration 22/25 | Loss: 0.00085426
Iteration 23/25 | Loss: 0.00085426
Iteration 24/25 | Loss: 0.00085426
Iteration 25/25 | Loss: 0.00085426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085426
Iteration 2/1000 | Loss: 0.00004366
Iteration 3/1000 | Loss: 0.00002339
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001615
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001462
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001415
Iteration 13/1000 | Loss: 0.00001406
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001385
Iteration 21/1000 | Loss: 0.00001383
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001383
Iteration 25/1000 | Loss: 0.00001382
Iteration 26/1000 | Loss: 0.00001382
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001366
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001362
Iteration 49/1000 | Loss: 0.00001362
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001362
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001362
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001360
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001358
Iteration 69/1000 | Loss: 0.00001358
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001357
Iteration 73/1000 | Loss: 0.00001357
Iteration 74/1000 | Loss: 0.00001357
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001356
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001355
Iteration 84/1000 | Loss: 0.00001355
Iteration 85/1000 | Loss: 0.00001355
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001355
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001354
Iteration 93/1000 | Loss: 0.00001354
Iteration 94/1000 | Loss: 0.00001354
Iteration 95/1000 | Loss: 0.00001353
Iteration 96/1000 | Loss: 0.00001353
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001353
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001351
Iteration 125/1000 | Loss: 0.00001351
Iteration 126/1000 | Loss: 0.00001351
Iteration 127/1000 | Loss: 0.00001351
Iteration 128/1000 | Loss: 0.00001351
Iteration 129/1000 | Loss: 0.00001351
Iteration 130/1000 | Loss: 0.00001351
Iteration 131/1000 | Loss: 0.00001351
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001350
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001349
Iteration 140/1000 | Loss: 0.00001349
Iteration 141/1000 | Loss: 0.00001349
Iteration 142/1000 | Loss: 0.00001349
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001346
Iteration 149/1000 | Loss: 0.00001346
Iteration 150/1000 | Loss: 0.00001346
Iteration 151/1000 | Loss: 0.00001345
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001344
Iteration 155/1000 | Loss: 0.00001344
Iteration 156/1000 | Loss: 0.00001344
Iteration 157/1000 | Loss: 0.00001344
Iteration 158/1000 | Loss: 0.00001344
Iteration 159/1000 | Loss: 0.00001344
Iteration 160/1000 | Loss: 0.00001344
Iteration 161/1000 | Loss: 0.00001344
Iteration 162/1000 | Loss: 0.00001344
Iteration 163/1000 | Loss: 0.00001343
Iteration 164/1000 | Loss: 0.00001343
Iteration 165/1000 | Loss: 0.00001343
Iteration 166/1000 | Loss: 0.00001343
Iteration 167/1000 | Loss: 0.00001342
Iteration 168/1000 | Loss: 0.00001342
Iteration 169/1000 | Loss: 0.00001342
Iteration 170/1000 | Loss: 0.00001342
Iteration 171/1000 | Loss: 0.00001342
Iteration 172/1000 | Loss: 0.00001342
Iteration 173/1000 | Loss: 0.00001341
Iteration 174/1000 | Loss: 0.00001341
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001340
Iteration 178/1000 | Loss: 0.00001340
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001339
Iteration 181/1000 | Loss: 0.00001339
Iteration 182/1000 | Loss: 0.00001339
Iteration 183/1000 | Loss: 0.00001339
Iteration 184/1000 | Loss: 0.00001339
Iteration 185/1000 | Loss: 0.00001339
Iteration 186/1000 | Loss: 0.00001339
Iteration 187/1000 | Loss: 0.00001339
Iteration 188/1000 | Loss: 0.00001339
Iteration 189/1000 | Loss: 0.00001339
Iteration 190/1000 | Loss: 0.00001339
Iteration 191/1000 | Loss: 0.00001339
Iteration 192/1000 | Loss: 0.00001338
Iteration 193/1000 | Loss: 0.00001338
Iteration 194/1000 | Loss: 0.00001338
Iteration 195/1000 | Loss: 0.00001338
Iteration 196/1000 | Loss: 0.00001338
Iteration 197/1000 | Loss: 0.00001338
Iteration 198/1000 | Loss: 0.00001338
Iteration 199/1000 | Loss: 0.00001338
Iteration 200/1000 | Loss: 0.00001338
Iteration 201/1000 | Loss: 0.00001338
Iteration 202/1000 | Loss: 0.00001338
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001337
Iteration 205/1000 | Loss: 0.00001337
Iteration 206/1000 | Loss: 0.00001337
Iteration 207/1000 | Loss: 0.00001337
Iteration 208/1000 | Loss: 0.00001337
Iteration 209/1000 | Loss: 0.00001337
Iteration 210/1000 | Loss: 0.00001337
Iteration 211/1000 | Loss: 0.00001336
Iteration 212/1000 | Loss: 0.00001336
Iteration 213/1000 | Loss: 0.00001336
Iteration 214/1000 | Loss: 0.00001336
Iteration 215/1000 | Loss: 0.00001336
Iteration 216/1000 | Loss: 0.00001336
Iteration 217/1000 | Loss: 0.00001336
Iteration 218/1000 | Loss: 0.00001336
Iteration 219/1000 | Loss: 0.00001336
Iteration 220/1000 | Loss: 0.00001336
Iteration 221/1000 | Loss: 0.00001336
Iteration 222/1000 | Loss: 0.00001336
Iteration 223/1000 | Loss: 0.00001336
Iteration 224/1000 | Loss: 0.00001336
Iteration 225/1000 | Loss: 0.00001335
Iteration 226/1000 | Loss: 0.00001335
Iteration 227/1000 | Loss: 0.00001335
Iteration 228/1000 | Loss: 0.00001335
Iteration 229/1000 | Loss: 0.00001335
Iteration 230/1000 | Loss: 0.00001335
Iteration 231/1000 | Loss: 0.00001334
Iteration 232/1000 | Loss: 0.00001334
Iteration 233/1000 | Loss: 0.00001334
Iteration 234/1000 | Loss: 0.00001334
Iteration 235/1000 | Loss: 0.00001334
Iteration 236/1000 | Loss: 0.00001334
Iteration 237/1000 | Loss: 0.00001333
Iteration 238/1000 | Loss: 0.00001333
Iteration 239/1000 | Loss: 0.00001333
Iteration 240/1000 | Loss: 0.00001333
Iteration 241/1000 | Loss: 0.00001333
Iteration 242/1000 | Loss: 0.00001333
Iteration 243/1000 | Loss: 0.00001333
Iteration 244/1000 | Loss: 0.00001333
Iteration 245/1000 | Loss: 0.00001333
Iteration 246/1000 | Loss: 0.00001333
Iteration 247/1000 | Loss: 0.00001333
Iteration 248/1000 | Loss: 0.00001333
Iteration 249/1000 | Loss: 0.00001333
Iteration 250/1000 | Loss: 0.00001333
Iteration 251/1000 | Loss: 0.00001333
Iteration 252/1000 | Loss: 0.00001332
Iteration 253/1000 | Loss: 0.00001332
Iteration 254/1000 | Loss: 0.00001332
Iteration 255/1000 | Loss: 0.00001332
Iteration 256/1000 | Loss: 0.00001332
Iteration 257/1000 | Loss: 0.00001332
Iteration 258/1000 | Loss: 0.00001332
Iteration 259/1000 | Loss: 0.00001332
Iteration 260/1000 | Loss: 0.00001332
Iteration 261/1000 | Loss: 0.00001331
Iteration 262/1000 | Loss: 0.00001331
Iteration 263/1000 | Loss: 0.00001331
Iteration 264/1000 | Loss: 0.00001331
Iteration 265/1000 | Loss: 0.00001331
Iteration 266/1000 | Loss: 0.00001331
Iteration 267/1000 | Loss: 0.00001331
Iteration 268/1000 | Loss: 0.00001331
Iteration 269/1000 | Loss: 0.00001331
Iteration 270/1000 | Loss: 0.00001331
Iteration 271/1000 | Loss: 0.00001331
Iteration 272/1000 | Loss: 0.00001331
Iteration 273/1000 | Loss: 0.00001331
Iteration 274/1000 | Loss: 0.00001331
Iteration 275/1000 | Loss: 0.00001330
Iteration 276/1000 | Loss: 0.00001330
Iteration 277/1000 | Loss: 0.00001330
Iteration 278/1000 | Loss: 0.00001330
Iteration 279/1000 | Loss: 0.00001330
Iteration 280/1000 | Loss: 0.00001330
Iteration 281/1000 | Loss: 0.00001330
Iteration 282/1000 | Loss: 0.00001330
Iteration 283/1000 | Loss: 0.00001329
Iteration 284/1000 | Loss: 0.00001329
Iteration 285/1000 | Loss: 0.00001329
Iteration 286/1000 | Loss: 0.00001329
Iteration 287/1000 | Loss: 0.00001329
Iteration 288/1000 | Loss: 0.00001329
Iteration 289/1000 | Loss: 0.00001329
Iteration 290/1000 | Loss: 0.00001329
Iteration 291/1000 | Loss: 0.00001329
Iteration 292/1000 | Loss: 0.00001329
Iteration 293/1000 | Loss: 0.00001329
Iteration 294/1000 | Loss: 0.00001329
Iteration 295/1000 | Loss: 0.00001329
Iteration 296/1000 | Loss: 0.00001328
Iteration 297/1000 | Loss: 0.00001328
Iteration 298/1000 | Loss: 0.00001328
Iteration 299/1000 | Loss: 0.00001328
Iteration 300/1000 | Loss: 0.00001328
Iteration 301/1000 | Loss: 0.00001328
Iteration 302/1000 | Loss: 0.00001328
Iteration 303/1000 | Loss: 0.00001328
Iteration 304/1000 | Loss: 0.00001328
Iteration 305/1000 | Loss: 0.00001328
Iteration 306/1000 | Loss: 0.00001328
Iteration 307/1000 | Loss: 0.00001327
Iteration 308/1000 | Loss: 0.00001327
Iteration 309/1000 | Loss: 0.00001327
Iteration 310/1000 | Loss: 0.00001327
Iteration 311/1000 | Loss: 0.00001327
Iteration 312/1000 | Loss: 0.00001327
Iteration 313/1000 | Loss: 0.00001327
Iteration 314/1000 | Loss: 0.00001327
Iteration 315/1000 | Loss: 0.00001327
Iteration 316/1000 | Loss: 0.00001327
Iteration 317/1000 | Loss: 0.00001327
Iteration 318/1000 | Loss: 0.00001327
Iteration 319/1000 | Loss: 0.00001327
Iteration 320/1000 | Loss: 0.00001327
Iteration 321/1000 | Loss: 0.00001327
Iteration 322/1000 | Loss: 0.00001326
Iteration 323/1000 | Loss: 0.00001326
Iteration 324/1000 | Loss: 0.00001326
Iteration 325/1000 | Loss: 0.00001326
Iteration 326/1000 | Loss: 0.00001326
Iteration 327/1000 | Loss: 0.00001326
Iteration 328/1000 | Loss: 0.00001326
Iteration 329/1000 | Loss: 0.00001326
Iteration 330/1000 | Loss: 0.00001326
Iteration 331/1000 | Loss: 0.00001326
Iteration 332/1000 | Loss: 0.00001326
Iteration 333/1000 | Loss: 0.00001326
Iteration 334/1000 | Loss: 0.00001326
Iteration 335/1000 | Loss: 0.00001326
Iteration 336/1000 | Loss: 0.00001326
Iteration 337/1000 | Loss: 0.00001326
Iteration 338/1000 | Loss: 0.00001326
Iteration 339/1000 | Loss: 0.00001326
Iteration 340/1000 | Loss: 0.00001326
Iteration 341/1000 | Loss: 0.00001326
Iteration 342/1000 | Loss: 0.00001326
Iteration 343/1000 | Loss: 0.00001326
Iteration 344/1000 | Loss: 0.00001326
Iteration 345/1000 | Loss: 0.00001326
Iteration 346/1000 | Loss: 0.00001326
Iteration 347/1000 | Loss: 0.00001326
Iteration 348/1000 | Loss: 0.00001326
Iteration 349/1000 | Loss: 0.00001326
Iteration 350/1000 | Loss: 0.00001326
Iteration 351/1000 | Loss: 0.00001326
Iteration 352/1000 | Loss: 0.00001326
Iteration 353/1000 | Loss: 0.00001326
Iteration 354/1000 | Loss: 0.00001326
Iteration 355/1000 | Loss: 0.00001326
Iteration 356/1000 | Loss: 0.00001326
Iteration 357/1000 | Loss: 0.00001326
Iteration 358/1000 | Loss: 0.00001326
Iteration 359/1000 | Loss: 0.00001326
Iteration 360/1000 | Loss: 0.00001326
Iteration 361/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 361. Stopping optimization.
Last 5 losses: [1.3255604244477581e-05, 1.3255604244477581e-05, 1.3255604244477581e-05, 1.3255604244477581e-05, 1.3255604244477581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3255604244477581e-05

Optimization complete. Final v2v error: 3.049001932144165 mm

Highest mean error: 3.176825523376465 mm for frame 4

Lowest mean error: 2.876579999923706 mm for frame 117

Saving results

Total time: 47.6788375377655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798994
Iteration 2/25 | Loss: 0.00129966
Iteration 3/25 | Loss: 0.00118241
Iteration 4/25 | Loss: 0.00105284
Iteration 5/25 | Loss: 0.00105483
Iteration 6/25 | Loss: 0.00101579
Iteration 7/25 | Loss: 0.00100935
Iteration 8/25 | Loss: 0.00100865
Iteration 9/25 | Loss: 0.00100850
Iteration 10/25 | Loss: 0.00100840
Iteration 11/25 | Loss: 0.00100839
Iteration 12/25 | Loss: 0.00100837
Iteration 13/25 | Loss: 0.00100836
Iteration 14/25 | Loss: 0.00100836
Iteration 15/25 | Loss: 0.00100836
Iteration 16/25 | Loss: 0.00100836
Iteration 17/25 | Loss: 0.00100836
Iteration 18/25 | Loss: 0.00100836
Iteration 19/25 | Loss: 0.00100835
Iteration 20/25 | Loss: 0.00100835
Iteration 21/25 | Loss: 0.00100835
Iteration 22/25 | Loss: 0.00100835
Iteration 23/25 | Loss: 0.00100835
Iteration 24/25 | Loss: 0.00100835
Iteration 25/25 | Loss: 0.00100835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97471213
Iteration 2/25 | Loss: 0.00078318
Iteration 3/25 | Loss: 0.00078318
Iteration 4/25 | Loss: 0.00078318
Iteration 5/25 | Loss: 0.00078317
Iteration 6/25 | Loss: 0.00078317
Iteration 7/25 | Loss: 0.00078317
Iteration 8/25 | Loss: 0.00078317
Iteration 9/25 | Loss: 0.00078317
Iteration 10/25 | Loss: 0.00078317
Iteration 11/25 | Loss: 0.00078317
Iteration 12/25 | Loss: 0.00078317
Iteration 13/25 | Loss: 0.00078317
Iteration 14/25 | Loss: 0.00078317
Iteration 15/25 | Loss: 0.00078317
Iteration 16/25 | Loss: 0.00078317
Iteration 17/25 | Loss: 0.00078317
Iteration 18/25 | Loss: 0.00078317
Iteration 19/25 | Loss: 0.00078317
Iteration 20/25 | Loss: 0.00078317
Iteration 21/25 | Loss: 0.00078317
Iteration 22/25 | Loss: 0.00078317
Iteration 23/25 | Loss: 0.00078317
Iteration 24/25 | Loss: 0.00078317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007831722614355385, 0.0007831722614355385, 0.0007831722614355385, 0.0007831722614355385, 0.0007831722614355385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007831722614355385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078317
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00001975
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001574
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001468
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001370
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001370
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001369
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001368
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001362
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001348
Iteration 101/1000 | Loss: 0.00001348
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001343
Iteration 122/1000 | Loss: 0.00001343
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001339
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001338
Iteration 145/1000 | Loss: 0.00001337
Iteration 146/1000 | Loss: 0.00001337
Iteration 147/1000 | Loss: 0.00001337
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001335
Iteration 154/1000 | Loss: 0.00001335
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001335
Iteration 157/1000 | Loss: 0.00001335
Iteration 158/1000 | Loss: 0.00001334
Iteration 159/1000 | Loss: 0.00001334
Iteration 160/1000 | Loss: 0.00001334
Iteration 161/1000 | Loss: 0.00001334
Iteration 162/1000 | Loss: 0.00001334
Iteration 163/1000 | Loss: 0.00001334
Iteration 164/1000 | Loss: 0.00001333
Iteration 165/1000 | Loss: 0.00001333
Iteration 166/1000 | Loss: 0.00001333
Iteration 167/1000 | Loss: 0.00001333
Iteration 168/1000 | Loss: 0.00001333
Iteration 169/1000 | Loss: 0.00001332
Iteration 170/1000 | Loss: 0.00001332
Iteration 171/1000 | Loss: 0.00001332
Iteration 172/1000 | Loss: 0.00001332
Iteration 173/1000 | Loss: 0.00001332
Iteration 174/1000 | Loss: 0.00001331
Iteration 175/1000 | Loss: 0.00001331
Iteration 176/1000 | Loss: 0.00001331
Iteration 177/1000 | Loss: 0.00001331
Iteration 178/1000 | Loss: 0.00001331
Iteration 179/1000 | Loss: 0.00001331
Iteration 180/1000 | Loss: 0.00001331
Iteration 181/1000 | Loss: 0.00001330
Iteration 182/1000 | Loss: 0.00001330
Iteration 183/1000 | Loss: 0.00001330
Iteration 184/1000 | Loss: 0.00001330
Iteration 185/1000 | Loss: 0.00001330
Iteration 186/1000 | Loss: 0.00001330
Iteration 187/1000 | Loss: 0.00001329
Iteration 188/1000 | Loss: 0.00001329
Iteration 189/1000 | Loss: 0.00001329
Iteration 190/1000 | Loss: 0.00001329
Iteration 191/1000 | Loss: 0.00001329
Iteration 192/1000 | Loss: 0.00001329
Iteration 193/1000 | Loss: 0.00001329
Iteration 194/1000 | Loss: 0.00001329
Iteration 195/1000 | Loss: 0.00001329
Iteration 196/1000 | Loss: 0.00001329
Iteration 197/1000 | Loss: 0.00001329
Iteration 198/1000 | Loss: 0.00001328
Iteration 199/1000 | Loss: 0.00001328
Iteration 200/1000 | Loss: 0.00001328
Iteration 201/1000 | Loss: 0.00001328
Iteration 202/1000 | Loss: 0.00001328
Iteration 203/1000 | Loss: 0.00001328
Iteration 204/1000 | Loss: 0.00001328
Iteration 205/1000 | Loss: 0.00001328
Iteration 206/1000 | Loss: 0.00001328
Iteration 207/1000 | Loss: 0.00001328
Iteration 208/1000 | Loss: 0.00001328
Iteration 209/1000 | Loss: 0.00001328
Iteration 210/1000 | Loss: 0.00001328
Iteration 211/1000 | Loss: 0.00001328
Iteration 212/1000 | Loss: 0.00001328
Iteration 213/1000 | Loss: 0.00001328
Iteration 214/1000 | Loss: 0.00001328
Iteration 215/1000 | Loss: 0.00001328
Iteration 216/1000 | Loss: 0.00001328
Iteration 217/1000 | Loss: 0.00001328
Iteration 218/1000 | Loss: 0.00001328
Iteration 219/1000 | Loss: 0.00001328
Iteration 220/1000 | Loss: 0.00001328
Iteration 221/1000 | Loss: 0.00001328
Iteration 222/1000 | Loss: 0.00001328
Iteration 223/1000 | Loss: 0.00001328
Iteration 224/1000 | Loss: 0.00001328
Iteration 225/1000 | Loss: 0.00001328
Iteration 226/1000 | Loss: 0.00001328
Iteration 227/1000 | Loss: 0.00001328
Iteration 228/1000 | Loss: 0.00001328
Iteration 229/1000 | Loss: 0.00001328
Iteration 230/1000 | Loss: 0.00001328
Iteration 231/1000 | Loss: 0.00001328
Iteration 232/1000 | Loss: 0.00001328
Iteration 233/1000 | Loss: 0.00001328
Iteration 234/1000 | Loss: 0.00001328
Iteration 235/1000 | Loss: 0.00001328
Iteration 236/1000 | Loss: 0.00001328
Iteration 237/1000 | Loss: 0.00001328
Iteration 238/1000 | Loss: 0.00001328
Iteration 239/1000 | Loss: 0.00001328
Iteration 240/1000 | Loss: 0.00001328
Iteration 241/1000 | Loss: 0.00001328
Iteration 242/1000 | Loss: 0.00001328
Iteration 243/1000 | Loss: 0.00001328
Iteration 244/1000 | Loss: 0.00001328
Iteration 245/1000 | Loss: 0.00001328
Iteration 246/1000 | Loss: 0.00001328
Iteration 247/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.3275567653181497e-05, 1.3275567653181497e-05, 1.3275567653181497e-05, 1.3275567653181497e-05, 1.3275567653181497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3275567653181497e-05

Optimization complete. Final v2v error: 3.0548689365386963 mm

Highest mean error: 3.6407930850982666 mm for frame 66

Lowest mean error: 2.5976719856262207 mm for frame 0

Saving results

Total time: 51.03609275817871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005663
Iteration 2/25 | Loss: 0.00178448
Iteration 3/25 | Loss: 0.00134695
Iteration 4/25 | Loss: 0.00124284
Iteration 5/25 | Loss: 0.00126332
Iteration 6/25 | Loss: 0.00124501
Iteration 7/25 | Loss: 0.00119059
Iteration 8/25 | Loss: 0.00115816
Iteration 9/25 | Loss: 0.00114994
Iteration 10/25 | Loss: 0.00114518
Iteration 11/25 | Loss: 0.00114412
Iteration 12/25 | Loss: 0.00114038
Iteration 13/25 | Loss: 0.00113620
Iteration 14/25 | Loss: 0.00113482
Iteration 15/25 | Loss: 0.00113437
Iteration 16/25 | Loss: 0.00113372
Iteration 17/25 | Loss: 0.00113278
Iteration 18/25 | Loss: 0.00113682
Iteration 19/25 | Loss: 0.00113552
Iteration 20/25 | Loss: 0.00113533
Iteration 21/25 | Loss: 0.00113388
Iteration 22/25 | Loss: 0.00113257
Iteration 23/25 | Loss: 0.00113466
Iteration 24/25 | Loss: 0.00113352
Iteration 25/25 | Loss: 0.00113085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92004085
Iteration 2/25 | Loss: 0.00090350
Iteration 3/25 | Loss: 0.00090349
Iteration 4/25 | Loss: 0.00090349
Iteration 5/25 | Loss: 0.00090349
Iteration 6/25 | Loss: 0.00090349
Iteration 7/25 | Loss: 0.00090349
Iteration 8/25 | Loss: 0.00090349
Iteration 9/25 | Loss: 0.00090349
Iteration 10/25 | Loss: 0.00090349
Iteration 11/25 | Loss: 0.00090349
Iteration 12/25 | Loss: 0.00090349
Iteration 13/25 | Loss: 0.00090349
Iteration 14/25 | Loss: 0.00090349
Iteration 15/25 | Loss: 0.00090349
Iteration 16/25 | Loss: 0.00090349
Iteration 17/25 | Loss: 0.00090349
Iteration 18/25 | Loss: 0.00090349
Iteration 19/25 | Loss: 0.00090349
Iteration 20/25 | Loss: 0.00090349
Iteration 21/25 | Loss: 0.00090349
Iteration 22/25 | Loss: 0.00090349
Iteration 23/25 | Loss: 0.00090349
Iteration 24/25 | Loss: 0.00090349
Iteration 25/25 | Loss: 0.00090349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090349
Iteration 2/1000 | Loss: 0.00006305
Iteration 3/1000 | Loss: 0.00008514
Iteration 4/1000 | Loss: 0.00014132
Iteration 5/1000 | Loss: 0.00025044
Iteration 6/1000 | Loss: 0.00003484
Iteration 7/1000 | Loss: 0.00002974
Iteration 8/1000 | Loss: 0.00002678
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002452
Iteration 11/1000 | Loss: 0.00014844
Iteration 12/1000 | Loss: 0.00003004
Iteration 13/1000 | Loss: 0.00002683
Iteration 14/1000 | Loss: 0.00002530
Iteration 15/1000 | Loss: 0.00002433
Iteration 16/1000 | Loss: 0.00010892
Iteration 17/1000 | Loss: 0.00005522
Iteration 18/1000 | Loss: 0.00002916
Iteration 19/1000 | Loss: 0.00006176
Iteration 20/1000 | Loss: 0.00002697
Iteration 21/1000 | Loss: 0.00002529
Iteration 22/1000 | Loss: 0.00002451
Iteration 23/1000 | Loss: 0.00002390
Iteration 24/1000 | Loss: 0.00002276
Iteration 25/1000 | Loss: 0.00002196
Iteration 26/1000 | Loss: 0.00002153
Iteration 27/1000 | Loss: 0.00002130
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002111
Iteration 30/1000 | Loss: 0.00002093
Iteration 31/1000 | Loss: 0.00002404
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002068
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002056
Iteration 40/1000 | Loss: 0.00002056
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002055
Iteration 43/1000 | Loss: 0.00002055
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002054
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00002054
Iteration 48/1000 | Loss: 0.00002053
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002051
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002047
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002046
Iteration 60/1000 | Loss: 0.00002046
Iteration 61/1000 | Loss: 0.00002045
Iteration 62/1000 | Loss: 0.00002045
Iteration 63/1000 | Loss: 0.00002044
Iteration 64/1000 | Loss: 0.00002044
Iteration 65/1000 | Loss: 0.00002044
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002043
Iteration 69/1000 | Loss: 0.00002043
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002042
Iteration 72/1000 | Loss: 0.00002042
Iteration 73/1000 | Loss: 0.00002042
Iteration 74/1000 | Loss: 0.00002041
Iteration 75/1000 | Loss: 0.00002041
Iteration 76/1000 | Loss: 0.00002041
Iteration 77/1000 | Loss: 0.00002040
Iteration 78/1000 | Loss: 0.00002040
Iteration 79/1000 | Loss: 0.00002040
Iteration 80/1000 | Loss: 0.00002040
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00002039
Iteration 83/1000 | Loss: 0.00002039
Iteration 84/1000 | Loss: 0.00002039
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002039
Iteration 87/1000 | Loss: 0.00002038
Iteration 88/1000 | Loss: 0.00002038
Iteration 89/1000 | Loss: 0.00002038
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002037
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002037
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002035
Iteration 99/1000 | Loss: 0.00002035
Iteration 100/1000 | Loss: 0.00002035
Iteration 101/1000 | Loss: 0.00002035
Iteration 102/1000 | Loss: 0.00002034
Iteration 103/1000 | Loss: 0.00002034
Iteration 104/1000 | Loss: 0.00002034
Iteration 105/1000 | Loss: 0.00002034
Iteration 106/1000 | Loss: 0.00002034
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002033
Iteration 114/1000 | Loss: 0.00002033
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002031
Iteration 118/1000 | Loss: 0.00002031
Iteration 119/1000 | Loss: 0.00002031
Iteration 120/1000 | Loss: 0.00002031
Iteration 121/1000 | Loss: 0.00002031
Iteration 122/1000 | Loss: 0.00002031
Iteration 123/1000 | Loss: 0.00002031
Iteration 124/1000 | Loss: 0.00002031
Iteration 125/1000 | Loss: 0.00002030
Iteration 126/1000 | Loss: 0.00002030
Iteration 127/1000 | Loss: 0.00002030
Iteration 128/1000 | Loss: 0.00002029
Iteration 129/1000 | Loss: 0.00002029
Iteration 130/1000 | Loss: 0.00002029
Iteration 131/1000 | Loss: 0.00002029
Iteration 132/1000 | Loss: 0.00002029
Iteration 133/1000 | Loss: 0.00002029
Iteration 134/1000 | Loss: 0.00002029
Iteration 135/1000 | Loss: 0.00002029
Iteration 136/1000 | Loss: 0.00002029
Iteration 137/1000 | Loss: 0.00002029
Iteration 138/1000 | Loss: 0.00002028
Iteration 139/1000 | Loss: 0.00002028
Iteration 140/1000 | Loss: 0.00002028
Iteration 141/1000 | Loss: 0.00002028
Iteration 142/1000 | Loss: 0.00002028
Iteration 143/1000 | Loss: 0.00002028
Iteration 144/1000 | Loss: 0.00002027
Iteration 145/1000 | Loss: 0.00002027
Iteration 146/1000 | Loss: 0.00002026
Iteration 147/1000 | Loss: 0.00002026
Iteration 148/1000 | Loss: 0.00002026
Iteration 149/1000 | Loss: 0.00002026
Iteration 150/1000 | Loss: 0.00002026
Iteration 151/1000 | Loss: 0.00002026
Iteration 152/1000 | Loss: 0.00002026
Iteration 153/1000 | Loss: 0.00002025
Iteration 154/1000 | Loss: 0.00002025
Iteration 155/1000 | Loss: 0.00002025
Iteration 156/1000 | Loss: 0.00002024
Iteration 157/1000 | Loss: 0.00002024
Iteration 158/1000 | Loss: 0.00002024
Iteration 159/1000 | Loss: 0.00002024
Iteration 160/1000 | Loss: 0.00002024
Iteration 161/1000 | Loss: 0.00002024
Iteration 162/1000 | Loss: 0.00002024
Iteration 163/1000 | Loss: 0.00002024
Iteration 164/1000 | Loss: 0.00002024
Iteration 165/1000 | Loss: 0.00002024
Iteration 166/1000 | Loss: 0.00002024
Iteration 167/1000 | Loss: 0.00002023
Iteration 168/1000 | Loss: 0.00002023
Iteration 169/1000 | Loss: 0.00002023
Iteration 170/1000 | Loss: 0.00002023
Iteration 171/1000 | Loss: 0.00002023
Iteration 172/1000 | Loss: 0.00002023
Iteration 173/1000 | Loss: 0.00002023
Iteration 174/1000 | Loss: 0.00002023
Iteration 175/1000 | Loss: 0.00002022
Iteration 176/1000 | Loss: 0.00002022
Iteration 177/1000 | Loss: 0.00002022
Iteration 178/1000 | Loss: 0.00002022
Iteration 179/1000 | Loss: 0.00002022
Iteration 180/1000 | Loss: 0.00002022
Iteration 181/1000 | Loss: 0.00002022
Iteration 182/1000 | Loss: 0.00002022
Iteration 183/1000 | Loss: 0.00002022
Iteration 184/1000 | Loss: 0.00002022
Iteration 185/1000 | Loss: 0.00002022
Iteration 186/1000 | Loss: 0.00002021
Iteration 187/1000 | Loss: 0.00002021
Iteration 188/1000 | Loss: 0.00002021
Iteration 189/1000 | Loss: 0.00002021
Iteration 190/1000 | Loss: 0.00002021
Iteration 191/1000 | Loss: 0.00002021
Iteration 192/1000 | Loss: 0.00002021
Iteration 193/1000 | Loss: 0.00002021
Iteration 194/1000 | Loss: 0.00002021
Iteration 195/1000 | Loss: 0.00002021
Iteration 196/1000 | Loss: 0.00002021
Iteration 197/1000 | Loss: 0.00002021
Iteration 198/1000 | Loss: 0.00002021
Iteration 199/1000 | Loss: 0.00002020
Iteration 200/1000 | Loss: 0.00002020
Iteration 201/1000 | Loss: 0.00002020
Iteration 202/1000 | Loss: 0.00002020
Iteration 203/1000 | Loss: 0.00002020
Iteration 204/1000 | Loss: 0.00002020
Iteration 205/1000 | Loss: 0.00002019
Iteration 206/1000 | Loss: 0.00002019
Iteration 207/1000 | Loss: 0.00002019
Iteration 208/1000 | Loss: 0.00002019
Iteration 209/1000 | Loss: 0.00002019
Iteration 210/1000 | Loss: 0.00002019
Iteration 211/1000 | Loss: 0.00002019
Iteration 212/1000 | Loss: 0.00002019
Iteration 213/1000 | Loss: 0.00002019
Iteration 214/1000 | Loss: 0.00002019
Iteration 215/1000 | Loss: 0.00002019
Iteration 216/1000 | Loss: 0.00002019
Iteration 217/1000 | Loss: 0.00002019
Iteration 218/1000 | Loss: 0.00002019
Iteration 219/1000 | Loss: 0.00002019
Iteration 220/1000 | Loss: 0.00002019
Iteration 221/1000 | Loss: 0.00002019
Iteration 222/1000 | Loss: 0.00002019
Iteration 223/1000 | Loss: 0.00002019
Iteration 224/1000 | Loss: 0.00002019
Iteration 225/1000 | Loss: 0.00002019
Iteration 226/1000 | Loss: 0.00002019
Iteration 227/1000 | Loss: 0.00002019
Iteration 228/1000 | Loss: 0.00002019
Iteration 229/1000 | Loss: 0.00002018
Iteration 230/1000 | Loss: 0.00002018
Iteration 231/1000 | Loss: 0.00002018
Iteration 232/1000 | Loss: 0.00002018
Iteration 233/1000 | Loss: 0.00002018
Iteration 234/1000 | Loss: 0.00002018
Iteration 235/1000 | Loss: 0.00002018
Iteration 236/1000 | Loss: 0.00002018
Iteration 237/1000 | Loss: 0.00002018
Iteration 238/1000 | Loss: 0.00002018
Iteration 239/1000 | Loss: 0.00002018
Iteration 240/1000 | Loss: 0.00002018
Iteration 241/1000 | Loss: 0.00002018
Iteration 242/1000 | Loss: 0.00002018
Iteration 243/1000 | Loss: 0.00002018
Iteration 244/1000 | Loss: 0.00002018
Iteration 245/1000 | Loss: 0.00002018
Iteration 246/1000 | Loss: 0.00002018
Iteration 247/1000 | Loss: 0.00002018
Iteration 248/1000 | Loss: 0.00002018
Iteration 249/1000 | Loss: 0.00002018
Iteration 250/1000 | Loss: 0.00002018
Iteration 251/1000 | Loss: 0.00002018
Iteration 252/1000 | Loss: 0.00002018
Iteration 253/1000 | Loss: 0.00002018
Iteration 254/1000 | Loss: 0.00002018
Iteration 255/1000 | Loss: 0.00002018
Iteration 256/1000 | Loss: 0.00002017
Iteration 257/1000 | Loss: 0.00002017
Iteration 258/1000 | Loss: 0.00002017
Iteration 259/1000 | Loss: 0.00002017
Iteration 260/1000 | Loss: 0.00002017
Iteration 261/1000 | Loss: 0.00002017
Iteration 262/1000 | Loss: 0.00002017
Iteration 263/1000 | Loss: 0.00002017
Iteration 264/1000 | Loss: 0.00002017
Iteration 265/1000 | Loss: 0.00002017
Iteration 266/1000 | Loss: 0.00002017
Iteration 267/1000 | Loss: 0.00002017
Iteration 268/1000 | Loss: 0.00002017
Iteration 269/1000 | Loss: 0.00002017
Iteration 270/1000 | Loss: 0.00002017
Iteration 271/1000 | Loss: 0.00002017
Iteration 272/1000 | Loss: 0.00002017
Iteration 273/1000 | Loss: 0.00002017
Iteration 274/1000 | Loss: 0.00002017
Iteration 275/1000 | Loss: 0.00002017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.0168768969597295e-05, 2.0168768969597295e-05, 2.0168768969597295e-05, 2.0168768969597295e-05, 2.0168768969597295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0168768969597295e-05

Optimization complete. Final v2v error: 3.5538265705108643 mm

Highest mean error: 5.359727382659912 mm for frame 115

Lowest mean error: 3.1153204441070557 mm for frame 182

Saving results

Total time: 109.49310159683228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647500
Iteration 2/25 | Loss: 0.00136123
Iteration 3/25 | Loss: 0.00110526
Iteration 4/25 | Loss: 0.00106374
Iteration 5/25 | Loss: 0.00105378
Iteration 6/25 | Loss: 0.00104450
Iteration 7/25 | Loss: 0.00104043
Iteration 8/25 | Loss: 0.00104662
Iteration 9/25 | Loss: 0.00104077
Iteration 10/25 | Loss: 0.00103906
Iteration 11/25 | Loss: 0.00103759
Iteration 12/25 | Loss: 0.00103733
Iteration 13/25 | Loss: 0.00103598
Iteration 14/25 | Loss: 0.00103241
Iteration 15/25 | Loss: 0.00103206
Iteration 16/25 | Loss: 0.00103205
Iteration 17/25 | Loss: 0.00103205
Iteration 18/25 | Loss: 0.00103204
Iteration 19/25 | Loss: 0.00103204
Iteration 20/25 | Loss: 0.00103204
Iteration 21/25 | Loss: 0.00103204
Iteration 22/25 | Loss: 0.00103204
Iteration 23/25 | Loss: 0.00103204
Iteration 24/25 | Loss: 0.00103204
Iteration 25/25 | Loss: 0.00103204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03303170
Iteration 2/25 | Loss: 0.00086536
Iteration 3/25 | Loss: 0.00086505
Iteration 4/25 | Loss: 0.00086505
Iteration 5/25 | Loss: 0.00086505
Iteration 6/25 | Loss: 0.00086505
Iteration 7/25 | Loss: 0.00086505
Iteration 8/25 | Loss: 0.00086505
Iteration 9/25 | Loss: 0.00086505
Iteration 10/25 | Loss: 0.00086505
Iteration 11/25 | Loss: 0.00086505
Iteration 12/25 | Loss: 0.00086505
Iteration 13/25 | Loss: 0.00086505
Iteration 14/25 | Loss: 0.00086505
Iteration 15/25 | Loss: 0.00086505
Iteration 16/25 | Loss: 0.00086505
Iteration 17/25 | Loss: 0.00086505
Iteration 18/25 | Loss: 0.00086505
Iteration 19/25 | Loss: 0.00086505
Iteration 20/25 | Loss: 0.00086505
Iteration 21/25 | Loss: 0.00086505
Iteration 22/25 | Loss: 0.00086505
Iteration 23/25 | Loss: 0.00086505
Iteration 24/25 | Loss: 0.00086505
Iteration 25/25 | Loss: 0.00086505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086505
Iteration 2/1000 | Loss: 0.00005976
Iteration 3/1000 | Loss: 0.00003770
Iteration 4/1000 | Loss: 0.00002836
Iteration 5/1000 | Loss: 0.00002550
Iteration 6/1000 | Loss: 0.00002409
Iteration 7/1000 | Loss: 0.00002292
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00002148
Iteration 10/1000 | Loss: 0.00002102
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002037
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00001992
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001945
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001939
Iteration 27/1000 | Loss: 0.00001937
Iteration 28/1000 | Loss: 0.00001936
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001929
Iteration 32/1000 | Loss: 0.00001929
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001924
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001920
Iteration 52/1000 | Loss: 0.00001920
Iteration 53/1000 | Loss: 0.00001919
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001914
Iteration 63/1000 | Loss: 0.00001914
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001911
Iteration 75/1000 | Loss: 0.00001911
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001905
Iteration 91/1000 | Loss: 0.00001905
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001902
Iteration 99/1000 | Loss: 0.00001902
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001901
Iteration 102/1000 | Loss: 0.00001901
Iteration 103/1000 | Loss: 0.00001901
Iteration 104/1000 | Loss: 0.00001901
Iteration 105/1000 | Loss: 0.00001901
Iteration 106/1000 | Loss: 0.00001901
Iteration 107/1000 | Loss: 0.00001901
Iteration 108/1000 | Loss: 0.00001901
Iteration 109/1000 | Loss: 0.00001901
Iteration 110/1000 | Loss: 0.00001900
Iteration 111/1000 | Loss: 0.00001900
Iteration 112/1000 | Loss: 0.00001900
Iteration 113/1000 | Loss: 0.00001900
Iteration 114/1000 | Loss: 0.00001900
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001900
Iteration 118/1000 | Loss: 0.00001900
Iteration 119/1000 | Loss: 0.00001900
Iteration 120/1000 | Loss: 0.00001900
Iteration 121/1000 | Loss: 0.00001900
Iteration 122/1000 | Loss: 0.00001900
Iteration 123/1000 | Loss: 0.00001900
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001899
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00001898
Iteration 130/1000 | Loss: 0.00001898
Iteration 131/1000 | Loss: 0.00001898
Iteration 132/1000 | Loss: 0.00001898
Iteration 133/1000 | Loss: 0.00001897
Iteration 134/1000 | Loss: 0.00001897
Iteration 135/1000 | Loss: 0.00001897
Iteration 136/1000 | Loss: 0.00001897
Iteration 137/1000 | Loss: 0.00001897
Iteration 138/1000 | Loss: 0.00001897
Iteration 139/1000 | Loss: 0.00001897
Iteration 140/1000 | Loss: 0.00001897
Iteration 141/1000 | Loss: 0.00001897
Iteration 142/1000 | Loss: 0.00001897
Iteration 143/1000 | Loss: 0.00001897
Iteration 144/1000 | Loss: 0.00001897
Iteration 145/1000 | Loss: 0.00001897
Iteration 146/1000 | Loss: 0.00001897
Iteration 147/1000 | Loss: 0.00001897
Iteration 148/1000 | Loss: 0.00001897
Iteration 149/1000 | Loss: 0.00001897
Iteration 150/1000 | Loss: 0.00001897
Iteration 151/1000 | Loss: 0.00001897
Iteration 152/1000 | Loss: 0.00001897
Iteration 153/1000 | Loss: 0.00001897
Iteration 154/1000 | Loss: 0.00001897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.8967080904985778e-05, 1.8967080904985778e-05, 1.8967080904985778e-05, 1.8967080904985778e-05, 1.8967080904985778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8967080904985778e-05

Optimization complete. Final v2v error: 3.4413094520568848 mm

Highest mean error: 5.898802757263184 mm for frame 120

Lowest mean error: 2.537120819091797 mm for frame 165

Saving results

Total time: 71.26220941543579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443895
Iteration 2/25 | Loss: 0.00111315
Iteration 3/25 | Loss: 0.00103049
Iteration 4/25 | Loss: 0.00101086
Iteration 5/25 | Loss: 0.00100562
Iteration 6/25 | Loss: 0.00100461
Iteration 7/25 | Loss: 0.00100456
Iteration 8/25 | Loss: 0.00100456
Iteration 9/25 | Loss: 0.00100456
Iteration 10/25 | Loss: 0.00100456
Iteration 11/25 | Loss: 0.00100456
Iteration 12/25 | Loss: 0.00100456
Iteration 13/25 | Loss: 0.00100456
Iteration 14/25 | Loss: 0.00100456
Iteration 15/25 | Loss: 0.00100456
Iteration 16/25 | Loss: 0.00100456
Iteration 17/25 | Loss: 0.00100456
Iteration 18/25 | Loss: 0.00100456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001004555611871183, 0.001004555611871183, 0.001004555611871183, 0.001004555611871183, 0.001004555611871183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001004555611871183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38653982
Iteration 2/25 | Loss: 0.00075649
Iteration 3/25 | Loss: 0.00075649
Iteration 4/25 | Loss: 0.00075649
Iteration 5/25 | Loss: 0.00075649
Iteration 6/25 | Loss: 0.00075649
Iteration 7/25 | Loss: 0.00075649
Iteration 8/25 | Loss: 0.00075649
Iteration 9/25 | Loss: 0.00075649
Iteration 10/25 | Loss: 0.00075649
Iteration 11/25 | Loss: 0.00075649
Iteration 12/25 | Loss: 0.00075649
Iteration 13/25 | Loss: 0.00075649
Iteration 14/25 | Loss: 0.00075649
Iteration 15/25 | Loss: 0.00075649
Iteration 16/25 | Loss: 0.00075649
Iteration 17/25 | Loss: 0.00075649
Iteration 18/25 | Loss: 0.00075649
Iteration 19/25 | Loss: 0.00075649
Iteration 20/25 | Loss: 0.00075649
Iteration 21/25 | Loss: 0.00075649
Iteration 22/25 | Loss: 0.00075649
Iteration 23/25 | Loss: 0.00075649
Iteration 24/25 | Loss: 0.00075649
Iteration 25/25 | Loss: 0.00075649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075649
Iteration 2/1000 | Loss: 0.00002342
Iteration 3/1000 | Loss: 0.00001664
Iteration 4/1000 | Loss: 0.00001448
Iteration 5/1000 | Loss: 0.00001393
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001315
Iteration 8/1000 | Loss: 0.00001277
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001251
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001202
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001198
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001197
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001192
Iteration 49/1000 | Loss: 0.00001192
Iteration 50/1000 | Loss: 0.00001192
Iteration 51/1000 | Loss: 0.00001191
Iteration 52/1000 | Loss: 0.00001191
Iteration 53/1000 | Loss: 0.00001191
Iteration 54/1000 | Loss: 0.00001191
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001190
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Iteration 143/1000 | Loss: 0.00001168
Iteration 144/1000 | Loss: 0.00001168
Iteration 145/1000 | Loss: 0.00001168
Iteration 146/1000 | Loss: 0.00001168
Iteration 147/1000 | Loss: 0.00001168
Iteration 148/1000 | Loss: 0.00001168
Iteration 149/1000 | Loss: 0.00001168
Iteration 150/1000 | Loss: 0.00001168
Iteration 151/1000 | Loss: 0.00001168
Iteration 152/1000 | Loss: 0.00001168
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001168
Iteration 156/1000 | Loss: 0.00001168
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001168
Iteration 159/1000 | Loss: 0.00001168
Iteration 160/1000 | Loss: 0.00001168
Iteration 161/1000 | Loss: 0.00001168
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001168
Iteration 164/1000 | Loss: 0.00001168
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001168
Iteration 173/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.1682940566970501e-05, 1.1682940566970501e-05, 1.1682940566970501e-05, 1.1682940566970501e-05, 1.1682940566970501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1682940566970501e-05

Optimization complete. Final v2v error: 2.8984858989715576 mm

Highest mean error: 3.2051119804382324 mm for frame 133

Lowest mean error: 2.7574081420898438 mm for frame 16

Saving results

Total time: 36.9580454826355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695505
Iteration 2/25 | Loss: 0.00150157
Iteration 3/25 | Loss: 0.00122613
Iteration 4/25 | Loss: 0.00118667
Iteration 5/25 | Loss: 0.00117248
Iteration 6/25 | Loss: 0.00116997
Iteration 7/25 | Loss: 0.00116997
Iteration 8/25 | Loss: 0.00116996
Iteration 9/25 | Loss: 0.00116997
Iteration 10/25 | Loss: 0.00116997
Iteration 11/25 | Loss: 0.00116997
Iteration 12/25 | Loss: 0.00116997
Iteration 13/25 | Loss: 0.00116996
Iteration 14/25 | Loss: 0.00116997
Iteration 15/25 | Loss: 0.00116997
Iteration 16/25 | Loss: 0.00116997
Iteration 17/25 | Loss: 0.00116996
Iteration 18/25 | Loss: 0.00116997
Iteration 19/25 | Loss: 0.00116997
Iteration 20/25 | Loss: 0.00116997
Iteration 21/25 | Loss: 0.00116997
Iteration 22/25 | Loss: 0.00116997
Iteration 23/25 | Loss: 0.00116997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011699650203809142, 0.0011699650203809142, 0.0011699650203809142, 0.0011699650203809142, 0.0011699650203809142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011699650203809142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59687090
Iteration 2/25 | Loss: 0.00105022
Iteration 3/25 | Loss: 0.00105021
Iteration 4/25 | Loss: 0.00105020
Iteration 5/25 | Loss: 0.00105020
Iteration 6/25 | Loss: 0.00105020
Iteration 7/25 | Loss: 0.00105020
Iteration 8/25 | Loss: 0.00105020
Iteration 9/25 | Loss: 0.00105020
Iteration 10/25 | Loss: 0.00105020
Iteration 11/25 | Loss: 0.00105020
Iteration 12/25 | Loss: 0.00105020
Iteration 13/25 | Loss: 0.00105020
Iteration 14/25 | Loss: 0.00105020
Iteration 15/25 | Loss: 0.00105020
Iteration 16/25 | Loss: 0.00105020
Iteration 17/25 | Loss: 0.00105020
Iteration 18/25 | Loss: 0.00105020
Iteration 19/25 | Loss: 0.00105020
Iteration 20/25 | Loss: 0.00105020
Iteration 21/25 | Loss: 0.00105020
Iteration 22/25 | Loss: 0.00105020
Iteration 23/25 | Loss: 0.00105020
Iteration 24/25 | Loss: 0.00105020
Iteration 25/25 | Loss: 0.00105020

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105020
Iteration 2/1000 | Loss: 0.00008634
Iteration 3/1000 | Loss: 0.00004626
Iteration 4/1000 | Loss: 0.00003707
Iteration 5/1000 | Loss: 0.00003440
Iteration 6/1000 | Loss: 0.00003280
Iteration 7/1000 | Loss: 0.00003192
Iteration 8/1000 | Loss: 0.00003120
Iteration 9/1000 | Loss: 0.00003059
Iteration 10/1000 | Loss: 0.00003016
Iteration 11/1000 | Loss: 0.00002985
Iteration 12/1000 | Loss: 0.00002956
Iteration 13/1000 | Loss: 0.00002934
Iteration 14/1000 | Loss: 0.00002911
Iteration 15/1000 | Loss: 0.00002899
Iteration 16/1000 | Loss: 0.00002893
Iteration 17/1000 | Loss: 0.00002892
Iteration 18/1000 | Loss: 0.00002878
Iteration 19/1000 | Loss: 0.00002878
Iteration 20/1000 | Loss: 0.00002875
Iteration 21/1000 | Loss: 0.00002872
Iteration 22/1000 | Loss: 0.00002871
Iteration 23/1000 | Loss: 0.00002866
Iteration 24/1000 | Loss: 0.00002863
Iteration 25/1000 | Loss: 0.00002863
Iteration 26/1000 | Loss: 0.00002863
Iteration 27/1000 | Loss: 0.00002860
Iteration 28/1000 | Loss: 0.00002860
Iteration 29/1000 | Loss: 0.00002856
Iteration 30/1000 | Loss: 0.00002856
Iteration 31/1000 | Loss: 0.00002856
Iteration 32/1000 | Loss: 0.00002856
Iteration 33/1000 | Loss: 0.00002856
Iteration 34/1000 | Loss: 0.00002856
Iteration 35/1000 | Loss: 0.00002856
Iteration 36/1000 | Loss: 0.00002855
Iteration 37/1000 | Loss: 0.00002855
Iteration 38/1000 | Loss: 0.00002855
Iteration 39/1000 | Loss: 0.00002854
Iteration 40/1000 | Loss: 0.00002853
Iteration 41/1000 | Loss: 0.00002849
Iteration 42/1000 | Loss: 0.00002848
Iteration 43/1000 | Loss: 0.00002847
Iteration 44/1000 | Loss: 0.00002846
Iteration 45/1000 | Loss: 0.00002846
Iteration 46/1000 | Loss: 0.00002845
Iteration 47/1000 | Loss: 0.00002845
Iteration 48/1000 | Loss: 0.00002845
Iteration 49/1000 | Loss: 0.00002845
Iteration 50/1000 | Loss: 0.00002845
Iteration 51/1000 | Loss: 0.00002845
Iteration 52/1000 | Loss: 0.00002845
Iteration 53/1000 | Loss: 0.00002845
Iteration 54/1000 | Loss: 0.00002844
Iteration 55/1000 | Loss: 0.00002844
Iteration 56/1000 | Loss: 0.00002843
Iteration 57/1000 | Loss: 0.00002843
Iteration 58/1000 | Loss: 0.00002842
Iteration 59/1000 | Loss: 0.00002842
Iteration 60/1000 | Loss: 0.00002842
Iteration 61/1000 | Loss: 0.00002841
Iteration 62/1000 | Loss: 0.00002841
Iteration 63/1000 | Loss: 0.00002841
Iteration 64/1000 | Loss: 0.00002841
Iteration 65/1000 | Loss: 0.00002841
Iteration 66/1000 | Loss: 0.00002841
Iteration 67/1000 | Loss: 0.00002841
Iteration 68/1000 | Loss: 0.00002841
Iteration 69/1000 | Loss: 0.00002841
Iteration 70/1000 | Loss: 0.00002841
Iteration 71/1000 | Loss: 0.00002840
Iteration 72/1000 | Loss: 0.00002840
Iteration 73/1000 | Loss: 0.00002840
Iteration 74/1000 | Loss: 0.00002839
Iteration 75/1000 | Loss: 0.00002839
Iteration 76/1000 | Loss: 0.00002839
Iteration 77/1000 | Loss: 0.00002839
Iteration 78/1000 | Loss: 0.00002839
Iteration 79/1000 | Loss: 0.00002838
Iteration 80/1000 | Loss: 0.00002838
Iteration 81/1000 | Loss: 0.00002838
Iteration 82/1000 | Loss: 0.00002838
Iteration 83/1000 | Loss: 0.00002838
Iteration 84/1000 | Loss: 0.00002837
Iteration 85/1000 | Loss: 0.00002837
Iteration 86/1000 | Loss: 0.00002837
Iteration 87/1000 | Loss: 0.00002836
Iteration 88/1000 | Loss: 0.00002836
Iteration 89/1000 | Loss: 0.00002836
Iteration 90/1000 | Loss: 0.00002836
Iteration 91/1000 | Loss: 0.00002836
Iteration 92/1000 | Loss: 0.00002835
Iteration 93/1000 | Loss: 0.00002835
Iteration 94/1000 | Loss: 0.00002835
Iteration 95/1000 | Loss: 0.00002835
Iteration 96/1000 | Loss: 0.00002835
Iteration 97/1000 | Loss: 0.00002835
Iteration 98/1000 | Loss: 0.00002835
Iteration 99/1000 | Loss: 0.00002835
Iteration 100/1000 | Loss: 0.00002835
Iteration 101/1000 | Loss: 0.00002834
Iteration 102/1000 | Loss: 0.00002834
Iteration 103/1000 | Loss: 0.00002834
Iteration 104/1000 | Loss: 0.00002833
Iteration 105/1000 | Loss: 0.00002833
Iteration 106/1000 | Loss: 0.00002833
Iteration 107/1000 | Loss: 0.00002833
Iteration 108/1000 | Loss: 0.00002832
Iteration 109/1000 | Loss: 0.00002832
Iteration 110/1000 | Loss: 0.00002832
Iteration 111/1000 | Loss: 0.00002832
Iteration 112/1000 | Loss: 0.00002832
Iteration 113/1000 | Loss: 0.00002832
Iteration 114/1000 | Loss: 0.00002832
Iteration 115/1000 | Loss: 0.00002832
Iteration 116/1000 | Loss: 0.00002832
Iteration 117/1000 | Loss: 0.00002831
Iteration 118/1000 | Loss: 0.00002831
Iteration 119/1000 | Loss: 0.00002831
Iteration 120/1000 | Loss: 0.00002831
Iteration 121/1000 | Loss: 0.00002831
Iteration 122/1000 | Loss: 0.00002831
Iteration 123/1000 | Loss: 0.00002831
Iteration 124/1000 | Loss: 0.00002830
Iteration 125/1000 | Loss: 0.00002830
Iteration 126/1000 | Loss: 0.00002830
Iteration 127/1000 | Loss: 0.00002830
Iteration 128/1000 | Loss: 0.00002829
Iteration 129/1000 | Loss: 0.00002829
Iteration 130/1000 | Loss: 0.00002829
Iteration 131/1000 | Loss: 0.00002829
Iteration 132/1000 | Loss: 0.00002829
Iteration 133/1000 | Loss: 0.00002829
Iteration 134/1000 | Loss: 0.00002829
Iteration 135/1000 | Loss: 0.00002829
Iteration 136/1000 | Loss: 0.00002829
Iteration 137/1000 | Loss: 0.00002829
Iteration 138/1000 | Loss: 0.00002828
Iteration 139/1000 | Loss: 0.00002828
Iteration 140/1000 | Loss: 0.00002828
Iteration 141/1000 | Loss: 0.00002828
Iteration 142/1000 | Loss: 0.00002828
Iteration 143/1000 | Loss: 0.00002828
Iteration 144/1000 | Loss: 0.00002827
Iteration 145/1000 | Loss: 0.00002827
Iteration 146/1000 | Loss: 0.00002827
Iteration 147/1000 | Loss: 0.00002826
Iteration 148/1000 | Loss: 0.00002826
Iteration 149/1000 | Loss: 0.00002826
Iteration 150/1000 | Loss: 0.00002826
Iteration 151/1000 | Loss: 0.00002826
Iteration 152/1000 | Loss: 0.00002826
Iteration 153/1000 | Loss: 0.00002826
Iteration 154/1000 | Loss: 0.00002825
Iteration 155/1000 | Loss: 0.00002825
Iteration 156/1000 | Loss: 0.00002825
Iteration 157/1000 | Loss: 0.00002825
Iteration 158/1000 | Loss: 0.00002824
Iteration 159/1000 | Loss: 0.00002824
Iteration 160/1000 | Loss: 0.00002824
Iteration 161/1000 | Loss: 0.00002824
Iteration 162/1000 | Loss: 0.00002824
Iteration 163/1000 | Loss: 0.00002824
Iteration 164/1000 | Loss: 0.00002824
Iteration 165/1000 | Loss: 0.00002824
Iteration 166/1000 | Loss: 0.00002824
Iteration 167/1000 | Loss: 0.00002824
Iteration 168/1000 | Loss: 0.00002824
Iteration 169/1000 | Loss: 0.00002824
Iteration 170/1000 | Loss: 0.00002823
Iteration 171/1000 | Loss: 0.00002823
Iteration 172/1000 | Loss: 0.00002823
Iteration 173/1000 | Loss: 0.00002823
Iteration 174/1000 | Loss: 0.00002823
Iteration 175/1000 | Loss: 0.00002823
Iteration 176/1000 | Loss: 0.00002822
Iteration 177/1000 | Loss: 0.00002822
Iteration 178/1000 | Loss: 0.00002822
Iteration 179/1000 | Loss: 0.00002822
Iteration 180/1000 | Loss: 0.00002821
Iteration 181/1000 | Loss: 0.00002821
Iteration 182/1000 | Loss: 0.00002821
Iteration 183/1000 | Loss: 0.00002821
Iteration 184/1000 | Loss: 0.00002821
Iteration 185/1000 | Loss: 0.00002821
Iteration 186/1000 | Loss: 0.00002821
Iteration 187/1000 | Loss: 0.00002821
Iteration 188/1000 | Loss: 0.00002820
Iteration 189/1000 | Loss: 0.00002820
Iteration 190/1000 | Loss: 0.00002820
Iteration 191/1000 | Loss: 0.00002820
Iteration 192/1000 | Loss: 0.00002820
Iteration 193/1000 | Loss: 0.00002819
Iteration 194/1000 | Loss: 0.00002819
Iteration 195/1000 | Loss: 0.00002819
Iteration 196/1000 | Loss: 0.00002819
Iteration 197/1000 | Loss: 0.00002819
Iteration 198/1000 | Loss: 0.00002819
Iteration 199/1000 | Loss: 0.00002819
Iteration 200/1000 | Loss: 0.00002819
Iteration 201/1000 | Loss: 0.00002818
Iteration 202/1000 | Loss: 0.00002818
Iteration 203/1000 | Loss: 0.00002818
Iteration 204/1000 | Loss: 0.00002818
Iteration 205/1000 | Loss: 0.00002818
Iteration 206/1000 | Loss: 0.00002818
Iteration 207/1000 | Loss: 0.00002817
Iteration 208/1000 | Loss: 0.00002817
Iteration 209/1000 | Loss: 0.00002817
Iteration 210/1000 | Loss: 0.00002817
Iteration 211/1000 | Loss: 0.00002817
Iteration 212/1000 | Loss: 0.00002817
Iteration 213/1000 | Loss: 0.00002816
Iteration 214/1000 | Loss: 0.00002816
Iteration 215/1000 | Loss: 0.00002816
Iteration 216/1000 | Loss: 0.00002816
Iteration 217/1000 | Loss: 0.00002816
Iteration 218/1000 | Loss: 0.00002816
Iteration 219/1000 | Loss: 0.00002816
Iteration 220/1000 | Loss: 0.00002816
Iteration 221/1000 | Loss: 0.00002815
Iteration 222/1000 | Loss: 0.00002815
Iteration 223/1000 | Loss: 0.00002815
Iteration 224/1000 | Loss: 0.00002815
Iteration 225/1000 | Loss: 0.00002815
Iteration 226/1000 | Loss: 0.00002815
Iteration 227/1000 | Loss: 0.00002815
Iteration 228/1000 | Loss: 0.00002814
Iteration 229/1000 | Loss: 0.00002814
Iteration 230/1000 | Loss: 0.00002814
Iteration 231/1000 | Loss: 0.00002814
Iteration 232/1000 | Loss: 0.00002814
Iteration 233/1000 | Loss: 0.00002813
Iteration 234/1000 | Loss: 0.00002813
Iteration 235/1000 | Loss: 0.00002813
Iteration 236/1000 | Loss: 0.00002813
Iteration 237/1000 | Loss: 0.00002813
Iteration 238/1000 | Loss: 0.00002813
Iteration 239/1000 | Loss: 0.00002813
Iteration 240/1000 | Loss: 0.00002812
Iteration 241/1000 | Loss: 0.00002812
Iteration 242/1000 | Loss: 0.00002812
Iteration 243/1000 | Loss: 0.00002812
Iteration 244/1000 | Loss: 0.00002811
Iteration 245/1000 | Loss: 0.00002811
Iteration 246/1000 | Loss: 0.00002811
Iteration 247/1000 | Loss: 0.00002811
Iteration 248/1000 | Loss: 0.00002811
Iteration 249/1000 | Loss: 0.00002811
Iteration 250/1000 | Loss: 0.00002811
Iteration 251/1000 | Loss: 0.00002810
Iteration 252/1000 | Loss: 0.00002810
Iteration 253/1000 | Loss: 0.00002810
Iteration 254/1000 | Loss: 0.00002810
Iteration 255/1000 | Loss: 0.00002810
Iteration 256/1000 | Loss: 0.00002809
Iteration 257/1000 | Loss: 0.00002809
Iteration 258/1000 | Loss: 0.00002809
Iteration 259/1000 | Loss: 0.00002809
Iteration 260/1000 | Loss: 0.00002809
Iteration 261/1000 | Loss: 0.00002809
Iteration 262/1000 | Loss: 0.00002809
Iteration 263/1000 | Loss: 0.00002809
Iteration 264/1000 | Loss: 0.00002809
Iteration 265/1000 | Loss: 0.00002809
Iteration 266/1000 | Loss: 0.00002809
Iteration 267/1000 | Loss: 0.00002808
Iteration 268/1000 | Loss: 0.00002808
Iteration 269/1000 | Loss: 0.00002808
Iteration 270/1000 | Loss: 0.00002808
Iteration 271/1000 | Loss: 0.00002808
Iteration 272/1000 | Loss: 0.00002808
Iteration 273/1000 | Loss: 0.00002808
Iteration 274/1000 | Loss: 0.00002808
Iteration 275/1000 | Loss: 0.00002808
Iteration 276/1000 | Loss: 0.00002808
Iteration 277/1000 | Loss: 0.00002808
Iteration 278/1000 | Loss: 0.00002808
Iteration 279/1000 | Loss: 0.00002807
Iteration 280/1000 | Loss: 0.00002807
Iteration 281/1000 | Loss: 0.00002807
Iteration 282/1000 | Loss: 0.00002807
Iteration 283/1000 | Loss: 0.00002807
Iteration 284/1000 | Loss: 0.00002807
Iteration 285/1000 | Loss: 0.00002807
Iteration 286/1000 | Loss: 0.00002807
Iteration 287/1000 | Loss: 0.00002807
Iteration 288/1000 | Loss: 0.00002807
Iteration 289/1000 | Loss: 0.00002807
Iteration 290/1000 | Loss: 0.00002807
Iteration 291/1000 | Loss: 0.00002807
Iteration 292/1000 | Loss: 0.00002807
Iteration 293/1000 | Loss: 0.00002807
Iteration 294/1000 | Loss: 0.00002807
Iteration 295/1000 | Loss: 0.00002807
Iteration 296/1000 | Loss: 0.00002807
Iteration 297/1000 | Loss: 0.00002807
Iteration 298/1000 | Loss: 0.00002807
Iteration 299/1000 | Loss: 0.00002807
Iteration 300/1000 | Loss: 0.00002807
Iteration 301/1000 | Loss: 0.00002807
Iteration 302/1000 | Loss: 0.00002807
Iteration 303/1000 | Loss: 0.00002807
Iteration 304/1000 | Loss: 0.00002807
Iteration 305/1000 | Loss: 0.00002807
Iteration 306/1000 | Loss: 0.00002807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [2.8067381208529696e-05, 2.8067381208529696e-05, 2.8067381208529696e-05, 2.8067381208529696e-05, 2.8067381208529696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8067381208529696e-05

Optimization complete. Final v2v error: 4.232465744018555 mm

Highest mean error: 6.0497145652771 mm for frame 197

Lowest mean error: 2.6311655044555664 mm for frame 233

Saving results

Total time: 61.952272176742554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745051
Iteration 2/25 | Loss: 0.00162167
Iteration 3/25 | Loss: 0.00117659
Iteration 4/25 | Loss: 0.00112868
Iteration 5/25 | Loss: 0.00112662
Iteration 6/25 | Loss: 0.00113011
Iteration 7/25 | Loss: 0.00111968
Iteration 8/25 | Loss: 0.00109394
Iteration 9/25 | Loss: 0.00109293
Iteration 10/25 | Loss: 0.00109244
Iteration 11/25 | Loss: 0.00109207
Iteration 12/25 | Loss: 0.00109188
Iteration 13/25 | Loss: 0.00109179
Iteration 14/25 | Loss: 0.00109168
Iteration 15/25 | Loss: 0.00109144
Iteration 16/25 | Loss: 0.00109060
Iteration 17/25 | Loss: 0.00108761
Iteration 18/25 | Loss: 0.00108732
Iteration 19/25 | Loss: 0.00108726
Iteration 20/25 | Loss: 0.00108725
Iteration 21/25 | Loss: 0.00108725
Iteration 22/25 | Loss: 0.00108725
Iteration 23/25 | Loss: 0.00108725
Iteration 24/25 | Loss: 0.00108725
Iteration 25/25 | Loss: 0.00108725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.72416592
Iteration 2/25 | Loss: 0.00078491
Iteration 3/25 | Loss: 0.00078485
Iteration 4/25 | Loss: 0.00078484
Iteration 5/25 | Loss: 0.00078484
Iteration 6/25 | Loss: 0.00078484
Iteration 7/25 | Loss: 0.00078484
Iteration 8/25 | Loss: 0.00078484
Iteration 9/25 | Loss: 0.00078484
Iteration 10/25 | Loss: 0.00078484
Iteration 11/25 | Loss: 0.00078484
Iteration 12/25 | Loss: 0.00078484
Iteration 13/25 | Loss: 0.00078484
Iteration 14/25 | Loss: 0.00078484
Iteration 15/25 | Loss: 0.00078484
Iteration 16/25 | Loss: 0.00078484
Iteration 17/25 | Loss: 0.00078484
Iteration 18/25 | Loss: 0.00078484
Iteration 19/25 | Loss: 0.00078484
Iteration 20/25 | Loss: 0.00078484
Iteration 21/25 | Loss: 0.00078484
Iteration 22/25 | Loss: 0.00078484
Iteration 23/25 | Loss: 0.00078484
Iteration 24/25 | Loss: 0.00078484
Iteration 25/25 | Loss: 0.00078484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078484
Iteration 2/1000 | Loss: 0.00007741
Iteration 3/1000 | Loss: 0.00004417
Iteration 4/1000 | Loss: 0.00003517
Iteration 5/1000 | Loss: 0.00003140
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00015667
Iteration 8/1000 | Loss: 0.00029154
Iteration 9/1000 | Loss: 0.00002744
Iteration 10/1000 | Loss: 0.00002576
Iteration 11/1000 | Loss: 0.00002463
Iteration 12/1000 | Loss: 0.00002359
Iteration 13/1000 | Loss: 0.00002304
Iteration 14/1000 | Loss: 0.00002269
Iteration 15/1000 | Loss: 0.00002240
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00002182
Iteration 20/1000 | Loss: 0.00002178
Iteration 21/1000 | Loss: 0.00002178
Iteration 22/1000 | Loss: 0.00002175
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002166
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002166
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00002164
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002163
Iteration 45/1000 | Loss: 0.00002163
Iteration 46/1000 | Loss: 0.00002163
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002162
Iteration 49/1000 | Loss: 0.00002162
Iteration 50/1000 | Loss: 0.00002162
Iteration 51/1000 | Loss: 0.00002162
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002161
Iteration 54/1000 | Loss: 0.00002161
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002158
Iteration 82/1000 | Loss: 0.00002158
Iteration 83/1000 | Loss: 0.00002157
Iteration 84/1000 | Loss: 0.00002157
Iteration 85/1000 | Loss: 0.00002157
Iteration 86/1000 | Loss: 0.00002156
Iteration 87/1000 | Loss: 0.00002156
Iteration 88/1000 | Loss: 0.00002155
Iteration 89/1000 | Loss: 0.00002155
Iteration 90/1000 | Loss: 0.00002154
Iteration 91/1000 | Loss: 0.00002154
Iteration 92/1000 | Loss: 0.00002154
Iteration 93/1000 | Loss: 0.00002153
Iteration 94/1000 | Loss: 0.00002153
Iteration 95/1000 | Loss: 0.00002153
Iteration 96/1000 | Loss: 0.00002152
Iteration 97/1000 | Loss: 0.00002152
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002146
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002140
Iteration 110/1000 | Loss: 0.00002140
Iteration 111/1000 | Loss: 0.00002140
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002138
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002137
Iteration 124/1000 | Loss: 0.00002137
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002136
Iteration 128/1000 | Loss: 0.00002136
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002136
Iteration 131/1000 | Loss: 0.00002135
Iteration 132/1000 | Loss: 0.00002135
Iteration 133/1000 | Loss: 0.00002135
Iteration 134/1000 | Loss: 0.00002134
Iteration 135/1000 | Loss: 0.00002134
Iteration 136/1000 | Loss: 0.00002134
Iteration 137/1000 | Loss: 0.00002134
Iteration 138/1000 | Loss: 0.00002134
Iteration 139/1000 | Loss: 0.00002134
Iteration 140/1000 | Loss: 0.00002133
Iteration 141/1000 | Loss: 0.00002133
Iteration 142/1000 | Loss: 0.00002133
Iteration 143/1000 | Loss: 0.00002133
Iteration 144/1000 | Loss: 0.00002133
Iteration 145/1000 | Loss: 0.00002133
Iteration 146/1000 | Loss: 0.00002133
Iteration 147/1000 | Loss: 0.00002133
Iteration 148/1000 | Loss: 0.00002133
Iteration 149/1000 | Loss: 0.00002133
Iteration 150/1000 | Loss: 0.00002132
Iteration 151/1000 | Loss: 0.00002132
Iteration 152/1000 | Loss: 0.00002132
Iteration 153/1000 | Loss: 0.00002132
Iteration 154/1000 | Loss: 0.00002132
Iteration 155/1000 | Loss: 0.00002131
Iteration 156/1000 | Loss: 0.00002131
Iteration 157/1000 | Loss: 0.00002131
Iteration 158/1000 | Loss: 0.00002131
Iteration 159/1000 | Loss: 0.00002131
Iteration 160/1000 | Loss: 0.00002130
Iteration 161/1000 | Loss: 0.00002130
Iteration 162/1000 | Loss: 0.00002130
Iteration 163/1000 | Loss: 0.00002130
Iteration 164/1000 | Loss: 0.00002129
Iteration 165/1000 | Loss: 0.00002129
Iteration 166/1000 | Loss: 0.00002129
Iteration 167/1000 | Loss: 0.00002129
Iteration 168/1000 | Loss: 0.00002128
Iteration 169/1000 | Loss: 0.00002128
Iteration 170/1000 | Loss: 0.00002128
Iteration 171/1000 | Loss: 0.00002128
Iteration 172/1000 | Loss: 0.00002128
Iteration 173/1000 | Loss: 0.00002128
Iteration 174/1000 | Loss: 0.00002128
Iteration 175/1000 | Loss: 0.00002128
Iteration 176/1000 | Loss: 0.00002128
Iteration 177/1000 | Loss: 0.00002128
Iteration 178/1000 | Loss: 0.00002128
Iteration 179/1000 | Loss: 0.00002128
Iteration 180/1000 | Loss: 0.00002128
Iteration 181/1000 | Loss: 0.00002127
Iteration 182/1000 | Loss: 0.00002127
Iteration 183/1000 | Loss: 0.00002127
Iteration 184/1000 | Loss: 0.00002127
Iteration 185/1000 | Loss: 0.00002127
Iteration 186/1000 | Loss: 0.00002127
Iteration 187/1000 | Loss: 0.00002127
Iteration 188/1000 | Loss: 0.00002127
Iteration 189/1000 | Loss: 0.00002127
Iteration 190/1000 | Loss: 0.00002127
Iteration 191/1000 | Loss: 0.00002127
Iteration 192/1000 | Loss: 0.00002127
Iteration 193/1000 | Loss: 0.00002127
Iteration 194/1000 | Loss: 0.00002127
Iteration 195/1000 | Loss: 0.00002127
Iteration 196/1000 | Loss: 0.00002127
Iteration 197/1000 | Loss: 0.00002126
Iteration 198/1000 | Loss: 0.00002126
Iteration 199/1000 | Loss: 0.00002126
Iteration 200/1000 | Loss: 0.00014723
Iteration 201/1000 | Loss: 0.00002235
Iteration 202/1000 | Loss: 0.00002142
Iteration 203/1000 | Loss: 0.00002123
Iteration 204/1000 | Loss: 0.00002113
Iteration 205/1000 | Loss: 0.00002113
Iteration 206/1000 | Loss: 0.00002107
Iteration 207/1000 | Loss: 0.00002097
Iteration 208/1000 | Loss: 0.00002092
Iteration 209/1000 | Loss: 0.00002091
Iteration 210/1000 | Loss: 0.00002090
Iteration 211/1000 | Loss: 0.00002090
Iteration 212/1000 | Loss: 0.00002089
Iteration 213/1000 | Loss: 0.00002088
Iteration 214/1000 | Loss: 0.00002087
Iteration 215/1000 | Loss: 0.00002087
Iteration 216/1000 | Loss: 0.00002087
Iteration 217/1000 | Loss: 0.00002087
Iteration 218/1000 | Loss: 0.00002087
Iteration 219/1000 | Loss: 0.00002087
Iteration 220/1000 | Loss: 0.00002087
Iteration 221/1000 | Loss: 0.00002086
Iteration 222/1000 | Loss: 0.00002086
Iteration 223/1000 | Loss: 0.00002085
Iteration 224/1000 | Loss: 0.00002085
Iteration 225/1000 | Loss: 0.00002083
Iteration 226/1000 | Loss: 0.00002083
Iteration 227/1000 | Loss: 0.00002083
Iteration 228/1000 | Loss: 0.00002082
Iteration 229/1000 | Loss: 0.00002082
Iteration 230/1000 | Loss: 0.00002082
Iteration 231/1000 | Loss: 0.00002082
Iteration 232/1000 | Loss: 0.00002082
Iteration 233/1000 | Loss: 0.00002082
Iteration 234/1000 | Loss: 0.00002082
Iteration 235/1000 | Loss: 0.00002082
Iteration 236/1000 | Loss: 0.00002081
Iteration 237/1000 | Loss: 0.00002081
Iteration 238/1000 | Loss: 0.00002081
Iteration 239/1000 | Loss: 0.00002081
Iteration 240/1000 | Loss: 0.00002081
Iteration 241/1000 | Loss: 0.00002081
Iteration 242/1000 | Loss: 0.00002081
Iteration 243/1000 | Loss: 0.00002081
Iteration 244/1000 | Loss: 0.00002081
Iteration 245/1000 | Loss: 0.00002081
Iteration 246/1000 | Loss: 0.00002081
Iteration 247/1000 | Loss: 0.00002081
Iteration 248/1000 | Loss: 0.00002081
Iteration 249/1000 | Loss: 0.00002081
Iteration 250/1000 | Loss: 0.00002081
Iteration 251/1000 | Loss: 0.00002081
Iteration 252/1000 | Loss: 0.00002081
Iteration 253/1000 | Loss: 0.00002081
Iteration 254/1000 | Loss: 0.00002081
Iteration 255/1000 | Loss: 0.00002081
Iteration 256/1000 | Loss: 0.00002081
Iteration 257/1000 | Loss: 0.00002081
Iteration 258/1000 | Loss: 0.00002081
Iteration 259/1000 | Loss: 0.00002081
Iteration 260/1000 | Loss: 0.00002081
Iteration 261/1000 | Loss: 0.00002081
Iteration 262/1000 | Loss: 0.00002081
Iteration 263/1000 | Loss: 0.00002081
Iteration 264/1000 | Loss: 0.00002081
Iteration 265/1000 | Loss: 0.00002081
Iteration 266/1000 | Loss: 0.00002081
Iteration 267/1000 | Loss: 0.00002081
Iteration 268/1000 | Loss: 0.00002081
Iteration 269/1000 | Loss: 0.00002081
Iteration 270/1000 | Loss: 0.00002081
Iteration 271/1000 | Loss: 0.00002081
Iteration 272/1000 | Loss: 0.00002081
Iteration 273/1000 | Loss: 0.00002081
Iteration 274/1000 | Loss: 0.00002081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [2.0812613001908176e-05, 2.0812613001908176e-05, 2.0812613001908176e-05, 2.0812613001908176e-05, 2.0812613001908176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0812613001908176e-05

Optimization complete. Final v2v error: 3.618034839630127 mm

Highest mean error: 12.31519603729248 mm for frame 211

Lowest mean error: 2.807720422744751 mm for frame 230

Saving results

Total time: 96.62907099723816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00266985
Iteration 2/25 | Loss: 0.00120522
Iteration 3/25 | Loss: 0.00104686
Iteration 4/25 | Loss: 0.00100938
Iteration 5/25 | Loss: 0.00100403
Iteration 6/25 | Loss: 0.00100277
Iteration 7/25 | Loss: 0.00100221
Iteration 8/25 | Loss: 0.00100210
Iteration 9/25 | Loss: 0.00100210
Iteration 10/25 | Loss: 0.00100210
Iteration 11/25 | Loss: 0.00100210
Iteration 12/25 | Loss: 0.00100210
Iteration 13/25 | Loss: 0.00100210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001002100994810462, 0.001002100994810462, 0.001002100994810462, 0.001002100994810462, 0.001002100994810462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001002100994810462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36259401
Iteration 2/25 | Loss: 0.00090566
Iteration 3/25 | Loss: 0.00090566
Iteration 4/25 | Loss: 0.00090566
Iteration 5/25 | Loss: 0.00090566
Iteration 6/25 | Loss: 0.00090566
Iteration 7/25 | Loss: 0.00090566
Iteration 8/25 | Loss: 0.00090566
Iteration 9/25 | Loss: 0.00090566
Iteration 10/25 | Loss: 0.00090566
Iteration 11/25 | Loss: 0.00090566
Iteration 12/25 | Loss: 0.00090566
Iteration 13/25 | Loss: 0.00090566
Iteration 14/25 | Loss: 0.00090566
Iteration 15/25 | Loss: 0.00090566
Iteration 16/25 | Loss: 0.00090566
Iteration 17/25 | Loss: 0.00090566
Iteration 18/25 | Loss: 0.00090566
Iteration 19/25 | Loss: 0.00090566
Iteration 20/25 | Loss: 0.00090566
Iteration 21/25 | Loss: 0.00090566
Iteration 22/25 | Loss: 0.00090566
Iteration 23/25 | Loss: 0.00090566
Iteration 24/25 | Loss: 0.00090566
Iteration 25/25 | Loss: 0.00090566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090566
Iteration 2/1000 | Loss: 0.00003923
Iteration 3/1000 | Loss: 0.00002598
Iteration 4/1000 | Loss: 0.00001809
Iteration 5/1000 | Loss: 0.00001683
Iteration 6/1000 | Loss: 0.00001609
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001517
Iteration 9/1000 | Loss: 0.00001473
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001369
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001359
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001351
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001346
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001342
Iteration 96/1000 | Loss: 0.00001342
Iteration 97/1000 | Loss: 0.00001342
Iteration 98/1000 | Loss: 0.00001342
Iteration 99/1000 | Loss: 0.00001341
Iteration 100/1000 | Loss: 0.00001341
Iteration 101/1000 | Loss: 0.00001341
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001341
Iteration 104/1000 | Loss: 0.00001341
Iteration 105/1000 | Loss: 0.00001341
Iteration 106/1000 | Loss: 0.00001340
Iteration 107/1000 | Loss: 0.00001340
Iteration 108/1000 | Loss: 0.00001340
Iteration 109/1000 | Loss: 0.00001340
Iteration 110/1000 | Loss: 0.00001340
Iteration 111/1000 | Loss: 0.00001340
Iteration 112/1000 | Loss: 0.00001340
Iteration 113/1000 | Loss: 0.00001340
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001339
Iteration 116/1000 | Loss: 0.00001339
Iteration 117/1000 | Loss: 0.00001339
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001338
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001338
Iteration 127/1000 | Loss: 0.00001338
Iteration 128/1000 | Loss: 0.00001338
Iteration 129/1000 | Loss: 0.00001338
Iteration 130/1000 | Loss: 0.00001338
Iteration 131/1000 | Loss: 0.00001338
Iteration 132/1000 | Loss: 0.00001338
Iteration 133/1000 | Loss: 0.00001338
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00001337
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001337
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001336
Iteration 154/1000 | Loss: 0.00001336
Iteration 155/1000 | Loss: 0.00001336
Iteration 156/1000 | Loss: 0.00001336
Iteration 157/1000 | Loss: 0.00001336
Iteration 158/1000 | Loss: 0.00001336
Iteration 159/1000 | Loss: 0.00001336
Iteration 160/1000 | Loss: 0.00001336
Iteration 161/1000 | Loss: 0.00001336
Iteration 162/1000 | Loss: 0.00001336
Iteration 163/1000 | Loss: 0.00001336
Iteration 164/1000 | Loss: 0.00001336
Iteration 165/1000 | Loss: 0.00001336
Iteration 166/1000 | Loss: 0.00001336
Iteration 167/1000 | Loss: 0.00001336
Iteration 168/1000 | Loss: 0.00001336
Iteration 169/1000 | Loss: 0.00001336
Iteration 170/1000 | Loss: 0.00001336
Iteration 171/1000 | Loss: 0.00001336
Iteration 172/1000 | Loss: 0.00001336
Iteration 173/1000 | Loss: 0.00001336
Iteration 174/1000 | Loss: 0.00001336
Iteration 175/1000 | Loss: 0.00001336
Iteration 176/1000 | Loss: 0.00001336
Iteration 177/1000 | Loss: 0.00001336
Iteration 178/1000 | Loss: 0.00001336
Iteration 179/1000 | Loss: 0.00001336
Iteration 180/1000 | Loss: 0.00001336
Iteration 181/1000 | Loss: 0.00001336
Iteration 182/1000 | Loss: 0.00001336
Iteration 183/1000 | Loss: 0.00001336
Iteration 184/1000 | Loss: 0.00001336
Iteration 185/1000 | Loss: 0.00001336
Iteration 186/1000 | Loss: 0.00001336
Iteration 187/1000 | Loss: 0.00001336
Iteration 188/1000 | Loss: 0.00001336
Iteration 189/1000 | Loss: 0.00001336
Iteration 190/1000 | Loss: 0.00001336
Iteration 191/1000 | Loss: 0.00001336
Iteration 192/1000 | Loss: 0.00001336
Iteration 193/1000 | Loss: 0.00001336
Iteration 194/1000 | Loss: 0.00001336
Iteration 195/1000 | Loss: 0.00001336
Iteration 196/1000 | Loss: 0.00001336
Iteration 197/1000 | Loss: 0.00001336
Iteration 198/1000 | Loss: 0.00001336
Iteration 199/1000 | Loss: 0.00001336
Iteration 200/1000 | Loss: 0.00001336
Iteration 201/1000 | Loss: 0.00001336
Iteration 202/1000 | Loss: 0.00001336
Iteration 203/1000 | Loss: 0.00001336
Iteration 204/1000 | Loss: 0.00001336
Iteration 205/1000 | Loss: 0.00001336
Iteration 206/1000 | Loss: 0.00001336
Iteration 207/1000 | Loss: 0.00001336
Iteration 208/1000 | Loss: 0.00001336
Iteration 209/1000 | Loss: 0.00001336
Iteration 210/1000 | Loss: 0.00001336
Iteration 211/1000 | Loss: 0.00001336
Iteration 212/1000 | Loss: 0.00001336
Iteration 213/1000 | Loss: 0.00001336
Iteration 214/1000 | Loss: 0.00001336
Iteration 215/1000 | Loss: 0.00001336
Iteration 216/1000 | Loss: 0.00001336
Iteration 217/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.336176137556322e-05, 1.336176137556322e-05, 1.336176137556322e-05, 1.336176137556322e-05, 1.336176137556322e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.336176137556322e-05

Optimization complete. Final v2v error: 3.0519278049468994 mm

Highest mean error: 3.5095584392547607 mm for frame 87

Lowest mean error: 2.8017923831939697 mm for frame 0

Saving results

Total time: 40.79754853248596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052928
Iteration 2/25 | Loss: 0.00196720
Iteration 3/25 | Loss: 0.00151027
Iteration 4/25 | Loss: 0.00144347
Iteration 5/25 | Loss: 0.00142218
Iteration 6/25 | Loss: 0.00135671
Iteration 7/25 | Loss: 0.00127645
Iteration 8/25 | Loss: 0.00125747
Iteration 9/25 | Loss: 0.00123000
Iteration 10/25 | Loss: 0.00119833
Iteration 11/25 | Loss: 0.00117405
Iteration 12/25 | Loss: 0.00117509
Iteration 13/25 | Loss: 0.00114078
Iteration 14/25 | Loss: 0.00113125
Iteration 15/25 | Loss: 0.00113062
Iteration 16/25 | Loss: 0.00112825
Iteration 17/25 | Loss: 0.00113092
Iteration 18/25 | Loss: 0.00112881
Iteration 19/25 | Loss: 0.00113217
Iteration 20/25 | Loss: 0.00115021
Iteration 21/25 | Loss: 0.00112061
Iteration 22/25 | Loss: 0.00112039
Iteration 23/25 | Loss: 0.00111655
Iteration 24/25 | Loss: 0.00111572
Iteration 25/25 | Loss: 0.00111470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40221167
Iteration 2/25 | Loss: 0.00152262
Iteration 3/25 | Loss: 0.00147382
Iteration 4/25 | Loss: 0.00147382
Iteration 5/25 | Loss: 0.00147382
Iteration 6/25 | Loss: 0.00147382
Iteration 7/25 | Loss: 0.00147382
Iteration 8/25 | Loss: 0.00147382
Iteration 9/25 | Loss: 0.00147382
Iteration 10/25 | Loss: 0.00147382
Iteration 11/25 | Loss: 0.00147382
Iteration 12/25 | Loss: 0.00147382
Iteration 13/25 | Loss: 0.00147382
Iteration 14/25 | Loss: 0.00147382
Iteration 15/25 | Loss: 0.00147382
Iteration 16/25 | Loss: 0.00147382
Iteration 17/25 | Loss: 0.00147382
Iteration 18/25 | Loss: 0.00147382
Iteration 19/25 | Loss: 0.00147382
Iteration 20/25 | Loss: 0.00147382
Iteration 21/25 | Loss: 0.00147382
Iteration 22/25 | Loss: 0.00147382
Iteration 23/25 | Loss: 0.00147382
Iteration 24/25 | Loss: 0.00147382
Iteration 25/25 | Loss: 0.00147382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147382
Iteration 2/1000 | Loss: 0.00692937
Iteration 3/1000 | Loss: 0.00587645
Iteration 4/1000 | Loss: 0.00724384
Iteration 5/1000 | Loss: 0.00561415
Iteration 6/1000 | Loss: 0.00561315
Iteration 7/1000 | Loss: 0.00201518
Iteration 8/1000 | Loss: 0.00168249
Iteration 9/1000 | Loss: 0.00205749
Iteration 10/1000 | Loss: 0.00327841
Iteration 11/1000 | Loss: 0.00368922
Iteration 12/1000 | Loss: 0.00306759
Iteration 13/1000 | Loss: 0.00296416
Iteration 14/1000 | Loss: 0.00401743
Iteration 15/1000 | Loss: 0.00300531
Iteration 16/1000 | Loss: 0.00195176
Iteration 17/1000 | Loss: 0.00212748
Iteration 18/1000 | Loss: 0.00257373
Iteration 19/1000 | Loss: 0.00219064
Iteration 20/1000 | Loss: 0.00078722
Iteration 21/1000 | Loss: 0.00208048
Iteration 22/1000 | Loss: 0.00185645
Iteration 23/1000 | Loss: 0.00143360
Iteration 24/1000 | Loss: 0.00221139
Iteration 25/1000 | Loss: 0.00253440
Iteration 26/1000 | Loss: 0.00154713
Iteration 27/1000 | Loss: 0.00158231
Iteration 28/1000 | Loss: 0.00264965
Iteration 29/1000 | Loss: 0.00194669
Iteration 30/1000 | Loss: 0.00184007
Iteration 31/1000 | Loss: 0.00190092
Iteration 32/1000 | Loss: 0.00316937
Iteration 33/1000 | Loss: 0.00091941
Iteration 34/1000 | Loss: 0.00078686
Iteration 35/1000 | Loss: 0.00143204
Iteration 36/1000 | Loss: 0.00017740
Iteration 37/1000 | Loss: 0.00146810
Iteration 38/1000 | Loss: 0.00028262
Iteration 39/1000 | Loss: 0.00183220
Iteration 40/1000 | Loss: 0.00137085
Iteration 41/1000 | Loss: 0.00080044
Iteration 42/1000 | Loss: 0.00126487
Iteration 43/1000 | Loss: 0.00148221
Iteration 44/1000 | Loss: 0.00120671
Iteration 45/1000 | Loss: 0.00010526
Iteration 46/1000 | Loss: 0.00217334
Iteration 47/1000 | Loss: 0.00139154
Iteration 48/1000 | Loss: 0.00178971
Iteration 49/1000 | Loss: 0.00012598
Iteration 50/1000 | Loss: 0.00009226
Iteration 51/1000 | Loss: 0.00175506
Iteration 52/1000 | Loss: 0.00022836
Iteration 53/1000 | Loss: 0.00057748
Iteration 54/1000 | Loss: 0.00069930
Iteration 55/1000 | Loss: 0.00039754
Iteration 56/1000 | Loss: 0.00051628
Iteration 57/1000 | Loss: 0.00030311
Iteration 58/1000 | Loss: 0.00083507
Iteration 59/1000 | Loss: 0.00020321
Iteration 60/1000 | Loss: 0.00008178
Iteration 61/1000 | Loss: 0.00006653
Iteration 62/1000 | Loss: 0.00126237
Iteration 63/1000 | Loss: 0.00007349
Iteration 64/1000 | Loss: 0.00022818
Iteration 65/1000 | Loss: 0.00015980
Iteration 66/1000 | Loss: 0.00005661
Iteration 67/1000 | Loss: 0.00005423
Iteration 68/1000 | Loss: 0.00736626
Iteration 69/1000 | Loss: 0.00725704
Iteration 70/1000 | Loss: 0.00075397
Iteration 71/1000 | Loss: 0.00108065
Iteration 72/1000 | Loss: 0.00073449
Iteration 73/1000 | Loss: 0.00051083
Iteration 74/1000 | Loss: 0.00026929
Iteration 75/1000 | Loss: 0.00005884
Iteration 76/1000 | Loss: 0.00020446
Iteration 77/1000 | Loss: 0.00020040
Iteration 78/1000 | Loss: 0.00019039
Iteration 79/1000 | Loss: 0.00394493
Iteration 80/1000 | Loss: 0.00288937
Iteration 81/1000 | Loss: 0.00012886
Iteration 82/1000 | Loss: 0.00011215
Iteration 83/1000 | Loss: 0.00027668
Iteration 84/1000 | Loss: 0.00027726
Iteration 85/1000 | Loss: 0.00216929
Iteration 86/1000 | Loss: 0.00178308
Iteration 87/1000 | Loss: 0.00014222
Iteration 88/1000 | Loss: 0.00007366
Iteration 89/1000 | Loss: 0.00004484
Iteration 90/1000 | Loss: 0.00058068
Iteration 91/1000 | Loss: 0.00031282
Iteration 92/1000 | Loss: 0.00047739
Iteration 93/1000 | Loss: 0.00388150
Iteration 94/1000 | Loss: 0.00382133
Iteration 95/1000 | Loss: 0.00270345
Iteration 96/1000 | Loss: 0.00319441
Iteration 97/1000 | Loss: 0.00375129
Iteration 98/1000 | Loss: 0.00349393
Iteration 99/1000 | Loss: 0.00404935
Iteration 100/1000 | Loss: 0.00481126
Iteration 101/1000 | Loss: 0.00327433
Iteration 102/1000 | Loss: 0.00150740
Iteration 103/1000 | Loss: 0.00227169
Iteration 104/1000 | Loss: 0.00017342
Iteration 105/1000 | Loss: 0.00077217
Iteration 106/1000 | Loss: 0.00009180
Iteration 107/1000 | Loss: 0.00007610
Iteration 108/1000 | Loss: 0.00228898
Iteration 109/1000 | Loss: 0.00031826
Iteration 110/1000 | Loss: 0.00013184
Iteration 111/1000 | Loss: 0.00028127
Iteration 112/1000 | Loss: 0.00010371
Iteration 113/1000 | Loss: 0.00144931
Iteration 114/1000 | Loss: 0.00109873
Iteration 115/1000 | Loss: 0.00120848
Iteration 116/1000 | Loss: 0.00097669
Iteration 117/1000 | Loss: 0.00017255
Iteration 118/1000 | Loss: 0.00040980
Iteration 119/1000 | Loss: 0.00269526
Iteration 120/1000 | Loss: 0.00143199
Iteration 121/1000 | Loss: 0.00302185
Iteration 122/1000 | Loss: 0.00119936
Iteration 123/1000 | Loss: 0.00148204
Iteration 124/1000 | Loss: 0.00168115
Iteration 125/1000 | Loss: 0.00141372
Iteration 126/1000 | Loss: 0.00177016
Iteration 127/1000 | Loss: 0.00156923
Iteration 128/1000 | Loss: 0.00082639
Iteration 129/1000 | Loss: 0.00034352
Iteration 130/1000 | Loss: 0.00008948
Iteration 131/1000 | Loss: 0.00006595
Iteration 132/1000 | Loss: 0.00021454
Iteration 133/1000 | Loss: 0.00004327
Iteration 134/1000 | Loss: 0.00016824
Iteration 135/1000 | Loss: 0.00041121
Iteration 136/1000 | Loss: 0.00038317
Iteration 137/1000 | Loss: 0.00010483
Iteration 138/1000 | Loss: 0.00006713
Iteration 139/1000 | Loss: 0.00020897
Iteration 140/1000 | Loss: 0.00039921
Iteration 141/1000 | Loss: 0.00022413
Iteration 142/1000 | Loss: 0.00011710
Iteration 143/1000 | Loss: 0.00018766
Iteration 144/1000 | Loss: 0.00021473
Iteration 145/1000 | Loss: 0.00003017
Iteration 146/1000 | Loss: 0.00002624
Iteration 147/1000 | Loss: 0.00003985
Iteration 148/1000 | Loss: 0.00002424
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00046029
Iteration 151/1000 | Loss: 0.00034317
Iteration 152/1000 | Loss: 0.00019406
Iteration 153/1000 | Loss: 0.00009037
Iteration 154/1000 | Loss: 0.00003433
Iteration 155/1000 | Loss: 0.00002976
Iteration 156/1000 | Loss: 0.00002657
Iteration 157/1000 | Loss: 0.00002349
Iteration 158/1000 | Loss: 0.00002127
Iteration 159/1000 | Loss: 0.00003448
Iteration 160/1000 | Loss: 0.00003431
Iteration 161/1000 | Loss: 0.00003108
Iteration 162/1000 | Loss: 0.00001883
Iteration 163/1000 | Loss: 0.00002559
Iteration 164/1000 | Loss: 0.00001869
Iteration 165/1000 | Loss: 0.00002538
Iteration 166/1000 | Loss: 0.00004824
Iteration 167/1000 | Loss: 0.00001632
Iteration 168/1000 | Loss: 0.00004031
Iteration 169/1000 | Loss: 0.00001495
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001386
Iteration 172/1000 | Loss: 0.00001355
Iteration 173/1000 | Loss: 0.00001332
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001310
Iteration 177/1000 | Loss: 0.00001298
Iteration 178/1000 | Loss: 0.00001291
Iteration 179/1000 | Loss: 0.00001286
Iteration 180/1000 | Loss: 0.00001275
Iteration 181/1000 | Loss: 0.00001267
Iteration 182/1000 | Loss: 0.00001266
Iteration 183/1000 | Loss: 0.00001265
Iteration 184/1000 | Loss: 0.00001265
Iteration 185/1000 | Loss: 0.00002608
Iteration 186/1000 | Loss: 0.00001264
Iteration 187/1000 | Loss: 0.00001262
Iteration 188/1000 | Loss: 0.00001261
Iteration 189/1000 | Loss: 0.00001261
Iteration 190/1000 | Loss: 0.00001260
Iteration 191/1000 | Loss: 0.00001258
Iteration 192/1000 | Loss: 0.00001253
Iteration 193/1000 | Loss: 0.00001253
Iteration 194/1000 | Loss: 0.00001252
Iteration 195/1000 | Loss: 0.00001252
Iteration 196/1000 | Loss: 0.00001251
Iteration 197/1000 | Loss: 0.00001251
Iteration 198/1000 | Loss: 0.00001250
Iteration 199/1000 | Loss: 0.00001250
Iteration 200/1000 | Loss: 0.00001247
Iteration 201/1000 | Loss: 0.00001243
Iteration 202/1000 | Loss: 0.00001242
Iteration 203/1000 | Loss: 0.00001242
Iteration 204/1000 | Loss: 0.00001242
Iteration 205/1000 | Loss: 0.00001242
Iteration 206/1000 | Loss: 0.00001242
Iteration 207/1000 | Loss: 0.00001241
Iteration 208/1000 | Loss: 0.00001241
Iteration 209/1000 | Loss: 0.00001241
Iteration 210/1000 | Loss: 0.00001241
Iteration 211/1000 | Loss: 0.00001240
Iteration 212/1000 | Loss: 0.00001240
Iteration 213/1000 | Loss: 0.00001239
Iteration 214/1000 | Loss: 0.00001239
Iteration 215/1000 | Loss: 0.00001238
Iteration 216/1000 | Loss: 0.00001238
Iteration 217/1000 | Loss: 0.00001238
Iteration 218/1000 | Loss: 0.00001237
Iteration 219/1000 | Loss: 0.00001234
Iteration 220/1000 | Loss: 0.00001234
Iteration 221/1000 | Loss: 0.00001234
Iteration 222/1000 | Loss: 0.00001233
Iteration 223/1000 | Loss: 0.00001233
Iteration 224/1000 | Loss: 0.00001233
Iteration 225/1000 | Loss: 0.00001232
Iteration 226/1000 | Loss: 0.00001232
Iteration 227/1000 | Loss: 0.00001232
Iteration 228/1000 | Loss: 0.00001232
Iteration 229/1000 | Loss: 0.00001231
Iteration 230/1000 | Loss: 0.00001231
Iteration 231/1000 | Loss: 0.00001230
Iteration 232/1000 | Loss: 0.00001230
Iteration 233/1000 | Loss: 0.00001229
Iteration 234/1000 | Loss: 0.00001229
Iteration 235/1000 | Loss: 0.00001229
Iteration 236/1000 | Loss: 0.00001228
Iteration 237/1000 | Loss: 0.00001228
Iteration 238/1000 | Loss: 0.00001228
Iteration 239/1000 | Loss: 0.00001228
Iteration 240/1000 | Loss: 0.00001228
Iteration 241/1000 | Loss: 0.00001227
Iteration 242/1000 | Loss: 0.00001227
Iteration 243/1000 | Loss: 0.00001227
Iteration 244/1000 | Loss: 0.00001227
Iteration 245/1000 | Loss: 0.00001227
Iteration 246/1000 | Loss: 0.00001227
Iteration 247/1000 | Loss: 0.00001226
Iteration 248/1000 | Loss: 0.00001226
Iteration 249/1000 | Loss: 0.00001226
Iteration 250/1000 | Loss: 0.00001226
Iteration 251/1000 | Loss: 0.00001226
Iteration 252/1000 | Loss: 0.00001226
Iteration 253/1000 | Loss: 0.00001226
Iteration 254/1000 | Loss: 0.00001226
Iteration 255/1000 | Loss: 0.00001225
Iteration 256/1000 | Loss: 0.00001225
Iteration 257/1000 | Loss: 0.00001225
Iteration 258/1000 | Loss: 0.00001225
Iteration 259/1000 | Loss: 0.00001225
Iteration 260/1000 | Loss: 0.00001225
Iteration 261/1000 | Loss: 0.00001225
Iteration 262/1000 | Loss: 0.00001225
Iteration 263/1000 | Loss: 0.00001225
Iteration 264/1000 | Loss: 0.00001225
Iteration 265/1000 | Loss: 0.00001225
Iteration 266/1000 | Loss: 0.00001224
Iteration 267/1000 | Loss: 0.00001224
Iteration 268/1000 | Loss: 0.00001224
Iteration 269/1000 | Loss: 0.00001224
Iteration 270/1000 | Loss: 0.00001224
Iteration 271/1000 | Loss: 0.00001224
Iteration 272/1000 | Loss: 0.00001224
Iteration 273/1000 | Loss: 0.00001224
Iteration 274/1000 | Loss: 0.00001224
Iteration 275/1000 | Loss: 0.00001224
Iteration 276/1000 | Loss: 0.00001224
Iteration 277/1000 | Loss: 0.00001224
Iteration 278/1000 | Loss: 0.00001224
Iteration 279/1000 | Loss: 0.00001224
Iteration 280/1000 | Loss: 0.00001224
Iteration 281/1000 | Loss: 0.00001223
Iteration 282/1000 | Loss: 0.00001223
Iteration 283/1000 | Loss: 0.00001223
Iteration 284/1000 | Loss: 0.00001223
Iteration 285/1000 | Loss: 0.00001223
Iteration 286/1000 | Loss: 0.00001223
Iteration 287/1000 | Loss: 0.00001223
Iteration 288/1000 | Loss: 0.00001223
Iteration 289/1000 | Loss: 0.00001223
Iteration 290/1000 | Loss: 0.00001223
Iteration 291/1000 | Loss: 0.00001223
Iteration 292/1000 | Loss: 0.00001223
Iteration 293/1000 | Loss: 0.00001223
Iteration 294/1000 | Loss: 0.00001223
Iteration 295/1000 | Loss: 0.00001223
Iteration 296/1000 | Loss: 0.00001223
Iteration 297/1000 | Loss: 0.00001223
Iteration 298/1000 | Loss: 0.00001222
Iteration 299/1000 | Loss: 0.00001222
Iteration 300/1000 | Loss: 0.00001222
Iteration 301/1000 | Loss: 0.00001222
Iteration 302/1000 | Loss: 0.00001222
Iteration 303/1000 | Loss: 0.00001222
Iteration 304/1000 | Loss: 0.00001222
Iteration 305/1000 | Loss: 0.00001222
Iteration 306/1000 | Loss: 0.00001222
Iteration 307/1000 | Loss: 0.00001222
Iteration 308/1000 | Loss: 0.00001222
Iteration 309/1000 | Loss: 0.00001222
Iteration 310/1000 | Loss: 0.00001222
Iteration 311/1000 | Loss: 0.00001222
Iteration 312/1000 | Loss: 0.00001222
Iteration 313/1000 | Loss: 0.00001222
Iteration 314/1000 | Loss: 0.00001222
Iteration 315/1000 | Loss: 0.00001222
Iteration 316/1000 | Loss: 0.00001222
Iteration 317/1000 | Loss: 0.00001222
Iteration 318/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [1.222093851538375e-05, 1.222093851538375e-05, 1.222093851538375e-05, 1.222093851538375e-05, 1.222093851538375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.222093851538375e-05

Optimization complete. Final v2v error: 2.896270275115967 mm

Highest mean error: 5.144724369049072 mm for frame 68

Lowest mean error: 2.4418346881866455 mm for frame 127

Saving results

Total time: 301.2969250679016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931442
Iteration 2/25 | Loss: 0.00189631
Iteration 3/25 | Loss: 0.00137105
Iteration 4/25 | Loss: 0.00129420
Iteration 5/25 | Loss: 0.00127739
Iteration 6/25 | Loss: 0.00128287
Iteration 7/25 | Loss: 0.00126851
Iteration 8/25 | Loss: 0.00124978
Iteration 9/25 | Loss: 0.00124165
Iteration 10/25 | Loss: 0.00122720
Iteration 11/25 | Loss: 0.00123050
Iteration 12/25 | Loss: 0.00122198
Iteration 13/25 | Loss: 0.00121698
Iteration 14/25 | Loss: 0.00122149
Iteration 15/25 | Loss: 0.00121868
Iteration 16/25 | Loss: 0.00122035
Iteration 17/25 | Loss: 0.00121745
Iteration 18/25 | Loss: 0.00121661
Iteration 19/25 | Loss: 0.00121429
Iteration 20/25 | Loss: 0.00121414
Iteration 21/25 | Loss: 0.00121428
Iteration 22/25 | Loss: 0.00121540
Iteration 23/25 | Loss: 0.00121050
Iteration 24/25 | Loss: 0.00120968
Iteration 25/25 | Loss: 0.00121373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.35269403
Iteration 2/25 | Loss: 0.00265479
Iteration 3/25 | Loss: 0.00251681
Iteration 4/25 | Loss: 0.00251681
Iteration 5/25 | Loss: 0.00251680
Iteration 6/25 | Loss: 0.00251680
Iteration 7/25 | Loss: 0.00251680
Iteration 8/25 | Loss: 0.00251680
Iteration 9/25 | Loss: 0.00251680
Iteration 10/25 | Loss: 0.00251680
Iteration 11/25 | Loss: 0.00251680
Iteration 12/25 | Loss: 0.00251680
Iteration 13/25 | Loss: 0.00251680
Iteration 14/25 | Loss: 0.00251680
Iteration 15/25 | Loss: 0.00251680
Iteration 16/25 | Loss: 0.00251680
Iteration 17/25 | Loss: 0.00251680
Iteration 18/25 | Loss: 0.00251680
Iteration 19/25 | Loss: 0.00251680
Iteration 20/25 | Loss: 0.00251680
Iteration 21/25 | Loss: 0.00251680
Iteration 22/25 | Loss: 0.00251680
Iteration 23/25 | Loss: 0.00251680
Iteration 24/25 | Loss: 0.00251680
Iteration 25/25 | Loss: 0.00251680

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251680
Iteration 2/1000 | Loss: 0.00044585
Iteration 3/1000 | Loss: 0.00090260
Iteration 4/1000 | Loss: 0.00345537
Iteration 5/1000 | Loss: 0.00182852
Iteration 6/1000 | Loss: 0.00224896
Iteration 7/1000 | Loss: 0.00093068
Iteration 8/1000 | Loss: 0.00050132
Iteration 9/1000 | Loss: 0.00022415
Iteration 10/1000 | Loss: 0.00022773
Iteration 11/1000 | Loss: 0.00010052
Iteration 12/1000 | Loss: 0.00085985
Iteration 13/1000 | Loss: 0.00025269
Iteration 14/1000 | Loss: 0.00037293
Iteration 15/1000 | Loss: 0.00008055
Iteration 16/1000 | Loss: 0.00005343
Iteration 17/1000 | Loss: 0.00006309
Iteration 18/1000 | Loss: 0.00005475
Iteration 19/1000 | Loss: 0.00177077
Iteration 20/1000 | Loss: 0.00018876
Iteration 21/1000 | Loss: 0.00005331
Iteration 22/1000 | Loss: 0.00005471
Iteration 23/1000 | Loss: 0.00005358
Iteration 24/1000 | Loss: 0.00005618
Iteration 25/1000 | Loss: 0.00005614
Iteration 26/1000 | Loss: 0.00040424
Iteration 27/1000 | Loss: 0.00006929
Iteration 28/1000 | Loss: 0.00004959
Iteration 29/1000 | Loss: 0.00003239
Iteration 30/1000 | Loss: 0.00004034
Iteration 31/1000 | Loss: 0.00004297
Iteration 32/1000 | Loss: 0.00004916
Iteration 33/1000 | Loss: 0.00004589
Iteration 34/1000 | Loss: 0.00004814
Iteration 35/1000 | Loss: 0.00004312
Iteration 36/1000 | Loss: 0.00004613
Iteration 37/1000 | Loss: 0.00004427
Iteration 38/1000 | Loss: 0.00004374
Iteration 39/1000 | Loss: 0.00004250
Iteration 40/1000 | Loss: 0.00004428
Iteration 41/1000 | Loss: 0.00081368
Iteration 42/1000 | Loss: 0.00046790
Iteration 43/1000 | Loss: 0.00004730
Iteration 44/1000 | Loss: 0.00004399
Iteration 45/1000 | Loss: 0.00100127
Iteration 46/1000 | Loss: 0.00006989
Iteration 47/1000 | Loss: 0.00004060
Iteration 48/1000 | Loss: 0.00004528
Iteration 49/1000 | Loss: 0.00003969
Iteration 50/1000 | Loss: 0.00004283
Iteration 51/1000 | Loss: 0.00003846
Iteration 52/1000 | Loss: 0.00004214
Iteration 53/1000 | Loss: 0.00003854
Iteration 54/1000 | Loss: 0.00172206
Iteration 55/1000 | Loss: 0.00022367
Iteration 56/1000 | Loss: 0.00003850
Iteration 57/1000 | Loss: 0.00002910
Iteration 58/1000 | Loss: 0.00004544
Iteration 59/1000 | Loss: 0.00098140
Iteration 60/1000 | Loss: 0.00009426
Iteration 61/1000 | Loss: 0.00003940
Iteration 62/1000 | Loss: 0.00002939
Iteration 63/1000 | Loss: 0.00002856
Iteration 64/1000 | Loss: 0.00002981
Iteration 65/1000 | Loss: 0.00003830
Iteration 66/1000 | Loss: 0.00003893
Iteration 67/1000 | Loss: 0.00002518
Iteration 68/1000 | Loss: 0.00003825
Iteration 69/1000 | Loss: 0.00004385
Iteration 70/1000 | Loss: 0.00003794
Iteration 71/1000 | Loss: 0.00003140
Iteration 72/1000 | Loss: 0.00093909
Iteration 73/1000 | Loss: 0.00008033
Iteration 74/1000 | Loss: 0.00004209
Iteration 75/1000 | Loss: 0.00003577
Iteration 76/1000 | Loss: 0.00005236
Iteration 77/1000 | Loss: 0.00003151
Iteration 78/1000 | Loss: 0.00002690
Iteration 79/1000 | Loss: 0.00002386
Iteration 80/1000 | Loss: 0.00002305
Iteration 81/1000 | Loss: 0.00002243
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002161
Iteration 84/1000 | Loss: 0.00082129
Iteration 85/1000 | Loss: 0.00003051
Iteration 86/1000 | Loss: 0.00002141
Iteration 87/1000 | Loss: 0.00080042
Iteration 88/1000 | Loss: 0.00021760
Iteration 89/1000 | Loss: 0.00146169
Iteration 90/1000 | Loss: 0.00028982
Iteration 91/1000 | Loss: 0.00056228
Iteration 92/1000 | Loss: 0.00002765
Iteration 93/1000 | Loss: 0.00075508
Iteration 94/1000 | Loss: 0.00013400
Iteration 95/1000 | Loss: 0.00001974
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00066692
Iteration 99/1000 | Loss: 0.00003076
Iteration 100/1000 | Loss: 0.00001882
Iteration 101/1000 | Loss: 0.00040172
Iteration 102/1000 | Loss: 0.00002493
Iteration 103/1000 | Loss: 0.00001822
Iteration 104/1000 | Loss: 0.00001632
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001437
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001311
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001307
Iteration 118/1000 | Loss: 0.00001307
Iteration 119/1000 | Loss: 0.00001307
Iteration 120/1000 | Loss: 0.00001307
Iteration 121/1000 | Loss: 0.00001307
Iteration 122/1000 | Loss: 0.00001307
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001299
Iteration 136/1000 | Loss: 0.00001299
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001299
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001296
Iteration 144/1000 | Loss: 0.00001296
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001296
Iteration 147/1000 | Loss: 0.00001295
Iteration 148/1000 | Loss: 0.00001295
Iteration 149/1000 | Loss: 0.00001295
Iteration 150/1000 | Loss: 0.00001295
Iteration 151/1000 | Loss: 0.00001295
Iteration 152/1000 | Loss: 0.00001295
Iteration 153/1000 | Loss: 0.00001295
Iteration 154/1000 | Loss: 0.00001294
Iteration 155/1000 | Loss: 0.00001294
Iteration 156/1000 | Loss: 0.00001294
Iteration 157/1000 | Loss: 0.00001294
Iteration 158/1000 | Loss: 0.00001294
Iteration 159/1000 | Loss: 0.00001294
Iteration 160/1000 | Loss: 0.00001294
Iteration 161/1000 | Loss: 0.00001294
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001293
Iteration 164/1000 | Loss: 0.00001293
Iteration 165/1000 | Loss: 0.00001293
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001292
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001292
Iteration 176/1000 | Loss: 0.00001292
Iteration 177/1000 | Loss: 0.00001292
Iteration 178/1000 | Loss: 0.00001292
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001292
Iteration 181/1000 | Loss: 0.00001292
Iteration 182/1000 | Loss: 0.00001291
Iteration 183/1000 | Loss: 0.00001291
Iteration 184/1000 | Loss: 0.00001291
Iteration 185/1000 | Loss: 0.00001291
Iteration 186/1000 | Loss: 0.00001290
Iteration 187/1000 | Loss: 0.00001290
Iteration 188/1000 | Loss: 0.00001290
Iteration 189/1000 | Loss: 0.00001290
Iteration 190/1000 | Loss: 0.00001290
Iteration 191/1000 | Loss: 0.00001290
Iteration 192/1000 | Loss: 0.00001290
Iteration 193/1000 | Loss: 0.00001290
Iteration 194/1000 | Loss: 0.00001290
Iteration 195/1000 | Loss: 0.00001290
Iteration 196/1000 | Loss: 0.00001290
Iteration 197/1000 | Loss: 0.00001290
Iteration 198/1000 | Loss: 0.00001290
Iteration 199/1000 | Loss: 0.00001290
Iteration 200/1000 | Loss: 0.00001289
Iteration 201/1000 | Loss: 0.00001289
Iteration 202/1000 | Loss: 0.00001289
Iteration 203/1000 | Loss: 0.00001289
Iteration 204/1000 | Loss: 0.00001288
Iteration 205/1000 | Loss: 0.00001288
Iteration 206/1000 | Loss: 0.00001288
Iteration 207/1000 | Loss: 0.00001288
Iteration 208/1000 | Loss: 0.00001288
Iteration 209/1000 | Loss: 0.00001288
Iteration 210/1000 | Loss: 0.00001287
Iteration 211/1000 | Loss: 0.00001287
Iteration 212/1000 | Loss: 0.00001287
Iteration 213/1000 | Loss: 0.00001287
Iteration 214/1000 | Loss: 0.00001287
Iteration 215/1000 | Loss: 0.00001287
Iteration 216/1000 | Loss: 0.00001287
Iteration 217/1000 | Loss: 0.00001287
Iteration 218/1000 | Loss: 0.00001287
Iteration 219/1000 | Loss: 0.00001286
Iteration 220/1000 | Loss: 0.00001286
Iteration 221/1000 | Loss: 0.00001286
Iteration 222/1000 | Loss: 0.00001286
Iteration 223/1000 | Loss: 0.00001286
Iteration 224/1000 | Loss: 0.00001286
Iteration 225/1000 | Loss: 0.00001286
Iteration 226/1000 | Loss: 0.00001286
Iteration 227/1000 | Loss: 0.00001286
Iteration 228/1000 | Loss: 0.00001286
Iteration 229/1000 | Loss: 0.00001285
Iteration 230/1000 | Loss: 0.00001285
Iteration 231/1000 | Loss: 0.00001285
Iteration 232/1000 | Loss: 0.00001285
Iteration 233/1000 | Loss: 0.00001285
Iteration 234/1000 | Loss: 0.00001285
Iteration 235/1000 | Loss: 0.00001285
Iteration 236/1000 | Loss: 0.00001285
Iteration 237/1000 | Loss: 0.00001285
Iteration 238/1000 | Loss: 0.00001284
Iteration 239/1000 | Loss: 0.00001284
Iteration 240/1000 | Loss: 0.00001284
Iteration 241/1000 | Loss: 0.00001284
Iteration 242/1000 | Loss: 0.00001284
Iteration 243/1000 | Loss: 0.00001284
Iteration 244/1000 | Loss: 0.00001284
Iteration 245/1000 | Loss: 0.00001284
Iteration 246/1000 | Loss: 0.00001284
Iteration 247/1000 | Loss: 0.00001284
Iteration 248/1000 | Loss: 0.00001284
Iteration 249/1000 | Loss: 0.00001284
Iteration 250/1000 | Loss: 0.00001284
Iteration 251/1000 | Loss: 0.00001284
Iteration 252/1000 | Loss: 0.00001284
Iteration 253/1000 | Loss: 0.00001284
Iteration 254/1000 | Loss: 0.00001284
Iteration 255/1000 | Loss: 0.00001284
Iteration 256/1000 | Loss: 0.00001284
Iteration 257/1000 | Loss: 0.00001284
Iteration 258/1000 | Loss: 0.00001284
Iteration 259/1000 | Loss: 0.00001284
Iteration 260/1000 | Loss: 0.00001284
Iteration 261/1000 | Loss: 0.00001284
Iteration 262/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.2842274372815154e-05, 1.2842274372815154e-05, 1.2842274372815154e-05, 1.2842274372815154e-05, 1.2842274372815154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2842274372815154e-05

Optimization complete. Final v2v error: 2.9453284740448 mm

Highest mean error: 4.810997009277344 mm for frame 63

Lowest mean error: 2.4163193702697754 mm for frame 106

Saving results

Total time: 209.76043057441711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844599
Iteration 2/25 | Loss: 0.00136545
Iteration 3/25 | Loss: 0.00118097
Iteration 4/25 | Loss: 0.00116321
Iteration 5/25 | Loss: 0.00116082
Iteration 6/25 | Loss: 0.00116080
Iteration 7/25 | Loss: 0.00116080
Iteration 8/25 | Loss: 0.00116080
Iteration 9/25 | Loss: 0.00116080
Iteration 10/25 | Loss: 0.00116080
Iteration 11/25 | Loss: 0.00116080
Iteration 12/25 | Loss: 0.00116080
Iteration 13/25 | Loss: 0.00116080
Iteration 14/25 | Loss: 0.00116080
Iteration 15/25 | Loss: 0.00116080
Iteration 16/25 | Loss: 0.00116080
Iteration 17/25 | Loss: 0.00116080
Iteration 18/25 | Loss: 0.00116080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011607969645410776, 0.0011607969645410776, 0.0011607969645410776, 0.0011607969645410776, 0.0011607969645410776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011607969645410776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98949605
Iteration 2/25 | Loss: 0.00042455
Iteration 3/25 | Loss: 0.00042454
Iteration 4/25 | Loss: 0.00042454
Iteration 5/25 | Loss: 0.00042454
Iteration 6/25 | Loss: 0.00042454
Iteration 7/25 | Loss: 0.00042454
Iteration 8/25 | Loss: 0.00042454
Iteration 9/25 | Loss: 0.00042454
Iteration 10/25 | Loss: 0.00042454
Iteration 11/25 | Loss: 0.00042454
Iteration 12/25 | Loss: 0.00042454
Iteration 13/25 | Loss: 0.00042454
Iteration 14/25 | Loss: 0.00042454
Iteration 15/25 | Loss: 0.00042454
Iteration 16/25 | Loss: 0.00042454
Iteration 17/25 | Loss: 0.00042454
Iteration 18/25 | Loss: 0.00042454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004245377203915268, 0.0004245377203915268, 0.0004245377203915268, 0.0004245377203915268, 0.0004245377203915268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004245377203915268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042454
Iteration 2/1000 | Loss: 0.00004179
Iteration 3/1000 | Loss: 0.00003025
Iteration 4/1000 | Loss: 0.00002596
Iteration 5/1000 | Loss: 0.00002458
Iteration 6/1000 | Loss: 0.00002372
Iteration 7/1000 | Loss: 0.00002306
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002228
Iteration 10/1000 | Loss: 0.00002199
Iteration 11/1000 | Loss: 0.00002182
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002149
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002141
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002131
Iteration 19/1000 | Loss: 0.00002131
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002131
Iteration 22/1000 | Loss: 0.00002131
Iteration 23/1000 | Loss: 0.00002130
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00002124
Iteration 27/1000 | Loss: 0.00002123
Iteration 28/1000 | Loss: 0.00002123
Iteration 29/1000 | Loss: 0.00002121
Iteration 30/1000 | Loss: 0.00002121
Iteration 31/1000 | Loss: 0.00002121
Iteration 32/1000 | Loss: 0.00002121
Iteration 33/1000 | Loss: 0.00002121
Iteration 34/1000 | Loss: 0.00002119
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00002115
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002114
Iteration 42/1000 | Loss: 0.00002114
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002112
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002111
Iteration 52/1000 | Loss: 0.00002111
Iteration 53/1000 | Loss: 0.00002111
Iteration 54/1000 | Loss: 0.00002111
Iteration 55/1000 | Loss: 0.00002111
Iteration 56/1000 | Loss: 0.00002111
Iteration 57/1000 | Loss: 0.00002111
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002110
Iteration 62/1000 | Loss: 0.00002109
Iteration 63/1000 | Loss: 0.00002108
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002104
Iteration 68/1000 | Loss: 0.00002102
Iteration 69/1000 | Loss: 0.00002102
Iteration 70/1000 | Loss: 0.00002102
Iteration 71/1000 | Loss: 0.00002102
Iteration 72/1000 | Loss: 0.00002101
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002101
Iteration 75/1000 | Loss: 0.00002101
Iteration 76/1000 | Loss: 0.00002101
Iteration 77/1000 | Loss: 0.00002101
Iteration 78/1000 | Loss: 0.00002101
Iteration 79/1000 | Loss: 0.00002101
Iteration 80/1000 | Loss: 0.00002101
Iteration 81/1000 | Loss: 0.00002101
Iteration 82/1000 | Loss: 0.00002101
Iteration 83/1000 | Loss: 0.00002101
Iteration 84/1000 | Loss: 0.00002101
Iteration 85/1000 | Loss: 0.00002101
Iteration 86/1000 | Loss: 0.00002101
Iteration 87/1000 | Loss: 0.00002100
Iteration 88/1000 | Loss: 0.00002100
Iteration 89/1000 | Loss: 0.00002100
Iteration 90/1000 | Loss: 0.00002100
Iteration 91/1000 | Loss: 0.00002100
Iteration 92/1000 | Loss: 0.00002100
Iteration 93/1000 | Loss: 0.00002100
Iteration 94/1000 | Loss: 0.00002100
Iteration 95/1000 | Loss: 0.00002100
Iteration 96/1000 | Loss: 0.00002100
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002100
Iteration 101/1000 | Loss: 0.00002099
Iteration 102/1000 | Loss: 0.00002099
Iteration 103/1000 | Loss: 0.00002099
Iteration 104/1000 | Loss: 0.00002099
Iteration 105/1000 | Loss: 0.00002099
Iteration 106/1000 | Loss: 0.00002099
Iteration 107/1000 | Loss: 0.00002099
Iteration 108/1000 | Loss: 0.00002099
Iteration 109/1000 | Loss: 0.00002099
Iteration 110/1000 | Loss: 0.00002099
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002098
Iteration 113/1000 | Loss: 0.00002098
Iteration 114/1000 | Loss: 0.00002098
Iteration 115/1000 | Loss: 0.00002098
Iteration 116/1000 | Loss: 0.00002098
Iteration 117/1000 | Loss: 0.00002098
Iteration 118/1000 | Loss: 0.00002098
Iteration 119/1000 | Loss: 0.00002098
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002098
Iteration 122/1000 | Loss: 0.00002098
Iteration 123/1000 | Loss: 0.00002098
Iteration 124/1000 | Loss: 0.00002098
Iteration 125/1000 | Loss: 0.00002098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.097795186273288e-05, 2.097795186273288e-05, 2.097795186273288e-05, 2.097795186273288e-05, 2.097795186273288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.097795186273288e-05

Optimization complete. Final v2v error: 3.8666157722473145 mm

Highest mean error: 3.9248545169830322 mm for frame 126

Lowest mean error: 3.814572811126709 mm for frame 149

Saving results

Total time: 35.32221293449402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741933
Iteration 2/25 | Loss: 0.00136201
Iteration 3/25 | Loss: 0.00107243
Iteration 4/25 | Loss: 0.00104090
Iteration 5/25 | Loss: 0.00103500
Iteration 6/25 | Loss: 0.00103422
Iteration 7/25 | Loss: 0.00103422
Iteration 8/25 | Loss: 0.00103422
Iteration 9/25 | Loss: 0.00103422
Iteration 10/25 | Loss: 0.00103422
Iteration 11/25 | Loss: 0.00103422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010342223104089499, 0.0010342223104089499, 0.0010342223104089499, 0.0010342223104089499, 0.0010342223104089499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010342223104089499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35674500
Iteration 2/25 | Loss: 0.00044539
Iteration 3/25 | Loss: 0.00044539
Iteration 4/25 | Loss: 0.00044539
Iteration 5/25 | Loss: 0.00044539
Iteration 6/25 | Loss: 0.00044539
Iteration 7/25 | Loss: 0.00044539
Iteration 8/25 | Loss: 0.00044538
Iteration 9/25 | Loss: 0.00044538
Iteration 10/25 | Loss: 0.00044538
Iteration 11/25 | Loss: 0.00044538
Iteration 12/25 | Loss: 0.00044538
Iteration 13/25 | Loss: 0.00044538
Iteration 14/25 | Loss: 0.00044538
Iteration 15/25 | Loss: 0.00044538
Iteration 16/25 | Loss: 0.00044538
Iteration 17/25 | Loss: 0.00044538
Iteration 18/25 | Loss: 0.00044538
Iteration 19/25 | Loss: 0.00044538
Iteration 20/25 | Loss: 0.00044538
Iteration 21/25 | Loss: 0.00044538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00044538441579788923, 0.00044538441579788923, 0.00044538441579788923, 0.00044538441579788923, 0.00044538441579788923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044538441579788923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044538
Iteration 2/1000 | Loss: 0.00002991
Iteration 3/1000 | Loss: 0.00002060
Iteration 4/1000 | Loss: 0.00001854
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001633
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001473
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001441
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001392
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001391
Iteration 39/1000 | Loss: 0.00001391
Iteration 40/1000 | Loss: 0.00001391
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001390
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001387
Iteration 66/1000 | Loss: 0.00001387
Iteration 67/1000 | Loss: 0.00001387
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001383
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001377
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001376
Iteration 117/1000 | Loss: 0.00001376
Iteration 118/1000 | Loss: 0.00001376
Iteration 119/1000 | Loss: 0.00001376
Iteration 120/1000 | Loss: 0.00001376
Iteration 121/1000 | Loss: 0.00001376
Iteration 122/1000 | Loss: 0.00001376
Iteration 123/1000 | Loss: 0.00001376
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001374
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001372
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001372
Iteration 134/1000 | Loss: 0.00001372
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001371
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001371
Iteration 145/1000 | Loss: 0.00001371
Iteration 146/1000 | Loss: 0.00001371
Iteration 147/1000 | Loss: 0.00001371
Iteration 148/1000 | Loss: 0.00001371
Iteration 149/1000 | Loss: 0.00001371
Iteration 150/1000 | Loss: 0.00001371
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001370
Iteration 153/1000 | Loss: 0.00001370
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001370
Iteration 156/1000 | Loss: 0.00001370
Iteration 157/1000 | Loss: 0.00001370
Iteration 158/1000 | Loss: 0.00001370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.3702580872632097e-05, 1.3702580872632097e-05, 1.3702580872632097e-05, 1.3702580872632097e-05, 1.3702580872632097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3702580872632097e-05

Optimization complete. Final v2v error: 3.1730289459228516 mm

Highest mean error: 3.4297256469726562 mm for frame 71

Lowest mean error: 2.8522472381591797 mm for frame 35

Saving results

Total time: 41.57499551773071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778436
Iteration 2/25 | Loss: 0.00171301
Iteration 3/25 | Loss: 0.00131183
Iteration 4/25 | Loss: 0.00119543
Iteration 5/25 | Loss: 0.00112828
Iteration 6/25 | Loss: 0.00110335
Iteration 7/25 | Loss: 0.00109405
Iteration 8/25 | Loss: 0.00108778
Iteration 9/25 | Loss: 0.00108791
Iteration 10/25 | Loss: 0.00108566
Iteration 11/25 | Loss: 0.00108612
Iteration 12/25 | Loss: 0.00108569
Iteration 13/25 | Loss: 0.00108422
Iteration 14/25 | Loss: 0.00108435
Iteration 15/25 | Loss: 0.00108366
Iteration 16/25 | Loss: 0.00108233
Iteration 17/25 | Loss: 0.00107889
Iteration 18/25 | Loss: 0.00107809
Iteration 19/25 | Loss: 0.00107796
Iteration 20/25 | Loss: 0.00107795
Iteration 21/25 | Loss: 0.00107795
Iteration 22/25 | Loss: 0.00107795
Iteration 23/25 | Loss: 0.00107795
Iteration 24/25 | Loss: 0.00107795
Iteration 25/25 | Loss: 0.00107794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35903311
Iteration 2/25 | Loss: 0.00051011
Iteration 3/25 | Loss: 0.00051009
Iteration 4/25 | Loss: 0.00051009
Iteration 5/25 | Loss: 0.00051009
Iteration 6/25 | Loss: 0.00051009
Iteration 7/25 | Loss: 0.00051009
Iteration 8/25 | Loss: 0.00051009
Iteration 9/25 | Loss: 0.00051009
Iteration 10/25 | Loss: 0.00051009
Iteration 11/25 | Loss: 0.00051008
Iteration 12/25 | Loss: 0.00051008
Iteration 13/25 | Loss: 0.00051008
Iteration 14/25 | Loss: 0.00051008
Iteration 15/25 | Loss: 0.00051008
Iteration 16/25 | Loss: 0.00051008
Iteration 17/25 | Loss: 0.00051008
Iteration 18/25 | Loss: 0.00051008
Iteration 19/25 | Loss: 0.00051008
Iteration 20/25 | Loss: 0.00051008
Iteration 21/25 | Loss: 0.00051008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005100847920402884, 0.0005100847920402884, 0.0005100847920402884, 0.0005100847920402884, 0.0005100847920402884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005100847920402884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051008
Iteration 2/1000 | Loss: 0.00003289
Iteration 3/1000 | Loss: 0.00002176
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001801
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001608
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001586
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001582
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001582
Iteration 22/1000 | Loss: 0.00001582
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001568
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001567
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001545
Iteration 152/1000 | Loss: 0.00001544
Iteration 153/1000 | Loss: 0.00001544
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001544
Iteration 168/1000 | Loss: 0.00001544
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001544
Iteration 171/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.543574580864515e-05, 1.543574580864515e-05, 1.543574580864515e-05, 1.543574580864515e-05, 1.543574580864515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.543574580864515e-05

Optimization complete. Final v2v error: 3.3450448513031006 mm

Highest mean error: 3.7421514987945557 mm for frame 67

Lowest mean error: 3.019214391708374 mm for frame 8

Saving results

Total time: 69.78526186943054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824645
Iteration 2/25 | Loss: 0.00106315
Iteration 3/25 | Loss: 0.00099249
Iteration 4/25 | Loss: 0.00097438
Iteration 5/25 | Loss: 0.00096849
Iteration 6/25 | Loss: 0.00096747
Iteration 7/25 | Loss: 0.00096747
Iteration 8/25 | Loss: 0.00096747
Iteration 9/25 | Loss: 0.00096747
Iteration 10/25 | Loss: 0.00096747
Iteration 11/25 | Loss: 0.00096747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000967474770732224, 0.000967474770732224, 0.000967474770732224, 0.000967474770732224, 0.000967474770732224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000967474770732224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40430331
Iteration 2/25 | Loss: 0.00068687
Iteration 3/25 | Loss: 0.00068687
Iteration 4/25 | Loss: 0.00068686
Iteration 5/25 | Loss: 0.00068686
Iteration 6/25 | Loss: 0.00068686
Iteration 7/25 | Loss: 0.00068686
Iteration 8/25 | Loss: 0.00068686
Iteration 9/25 | Loss: 0.00068686
Iteration 10/25 | Loss: 0.00068686
Iteration 11/25 | Loss: 0.00068686
Iteration 12/25 | Loss: 0.00068686
Iteration 13/25 | Loss: 0.00068686
Iteration 14/25 | Loss: 0.00068686
Iteration 15/25 | Loss: 0.00068686
Iteration 16/25 | Loss: 0.00068686
Iteration 17/25 | Loss: 0.00068686
Iteration 18/25 | Loss: 0.00068686
Iteration 19/25 | Loss: 0.00068686
Iteration 20/25 | Loss: 0.00068686
Iteration 21/25 | Loss: 0.00068686
Iteration 22/25 | Loss: 0.00068686
Iteration 23/25 | Loss: 0.00068686
Iteration 24/25 | Loss: 0.00068686
Iteration 25/25 | Loss: 0.00068686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068686
Iteration 2/1000 | Loss: 0.00002537
Iteration 3/1000 | Loss: 0.00001575
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001119
Iteration 8/1000 | Loss: 0.00001091
Iteration 9/1000 | Loss: 0.00001072
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001040
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001039
Iteration 18/1000 | Loss: 0.00001039
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001036
Iteration 21/1000 | Loss: 0.00001035
Iteration 22/1000 | Loss: 0.00001034
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001029
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001023
Iteration 29/1000 | Loss: 0.00001022
Iteration 30/1000 | Loss: 0.00001022
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001022
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001022
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001022
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001021
Iteration 39/1000 | Loss: 0.00001021
Iteration 40/1000 | Loss: 0.00001020
Iteration 41/1000 | Loss: 0.00001020
Iteration 42/1000 | Loss: 0.00001019
Iteration 43/1000 | Loss: 0.00001019
Iteration 44/1000 | Loss: 0.00001018
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001015
Iteration 50/1000 | Loss: 0.00001015
Iteration 51/1000 | Loss: 0.00001015
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001015
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001013
Iteration 60/1000 | Loss: 0.00001013
Iteration 61/1000 | Loss: 0.00001012
Iteration 62/1000 | Loss: 0.00001012
Iteration 63/1000 | Loss: 0.00001012
Iteration 64/1000 | Loss: 0.00001012
Iteration 65/1000 | Loss: 0.00001011
Iteration 66/1000 | Loss: 0.00001011
Iteration 67/1000 | Loss: 0.00001011
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001008
Iteration 72/1000 | Loss: 0.00001008
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001007
Iteration 75/1000 | Loss: 0.00001007
Iteration 76/1000 | Loss: 0.00001007
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001006
Iteration 79/1000 | Loss: 0.00001005
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001004
Iteration 82/1000 | Loss: 0.00001004
Iteration 83/1000 | Loss: 0.00001003
Iteration 84/1000 | Loss: 0.00001003
Iteration 85/1000 | Loss: 0.00001003
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001003
Iteration 88/1000 | Loss: 0.00001003
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001002
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001002
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001000
Iteration 100/1000 | Loss: 0.00001000
Iteration 101/1000 | Loss: 0.00001000
Iteration 102/1000 | Loss: 0.00001000
Iteration 103/1000 | Loss: 0.00001000
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000999
Iteration 106/1000 | Loss: 0.00000999
Iteration 107/1000 | Loss: 0.00000999
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000998
Iteration 113/1000 | Loss: 0.00000998
Iteration 114/1000 | Loss: 0.00000998
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000994
Iteration 147/1000 | Loss: 0.00000994
Iteration 148/1000 | Loss: 0.00000993
Iteration 149/1000 | Loss: 0.00000993
Iteration 150/1000 | Loss: 0.00000993
Iteration 151/1000 | Loss: 0.00000993
Iteration 152/1000 | Loss: 0.00000992
Iteration 153/1000 | Loss: 0.00000992
Iteration 154/1000 | Loss: 0.00000992
Iteration 155/1000 | Loss: 0.00000992
Iteration 156/1000 | Loss: 0.00000992
Iteration 157/1000 | Loss: 0.00000992
Iteration 158/1000 | Loss: 0.00000991
Iteration 159/1000 | Loss: 0.00000991
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000990
Iteration 168/1000 | Loss: 0.00000990
Iteration 169/1000 | Loss: 0.00000990
Iteration 170/1000 | Loss: 0.00000989
Iteration 171/1000 | Loss: 0.00000989
Iteration 172/1000 | Loss: 0.00000989
Iteration 173/1000 | Loss: 0.00000989
Iteration 174/1000 | Loss: 0.00000989
Iteration 175/1000 | Loss: 0.00000989
Iteration 176/1000 | Loss: 0.00000989
Iteration 177/1000 | Loss: 0.00000989
Iteration 178/1000 | Loss: 0.00000989
Iteration 179/1000 | Loss: 0.00000989
Iteration 180/1000 | Loss: 0.00000988
Iteration 181/1000 | Loss: 0.00000988
Iteration 182/1000 | Loss: 0.00000988
Iteration 183/1000 | Loss: 0.00000988
Iteration 184/1000 | Loss: 0.00000988
Iteration 185/1000 | Loss: 0.00000987
Iteration 186/1000 | Loss: 0.00000987
Iteration 187/1000 | Loss: 0.00000987
Iteration 188/1000 | Loss: 0.00000987
Iteration 189/1000 | Loss: 0.00000986
Iteration 190/1000 | Loss: 0.00000986
Iteration 191/1000 | Loss: 0.00000986
Iteration 192/1000 | Loss: 0.00000986
Iteration 193/1000 | Loss: 0.00000986
Iteration 194/1000 | Loss: 0.00000986
Iteration 195/1000 | Loss: 0.00000986
Iteration 196/1000 | Loss: 0.00000986
Iteration 197/1000 | Loss: 0.00000986
Iteration 198/1000 | Loss: 0.00000986
Iteration 199/1000 | Loss: 0.00000986
Iteration 200/1000 | Loss: 0.00000986
Iteration 201/1000 | Loss: 0.00000986
Iteration 202/1000 | Loss: 0.00000986
Iteration 203/1000 | Loss: 0.00000986
Iteration 204/1000 | Loss: 0.00000986
Iteration 205/1000 | Loss: 0.00000985
Iteration 206/1000 | Loss: 0.00000985
Iteration 207/1000 | Loss: 0.00000985
Iteration 208/1000 | Loss: 0.00000985
Iteration 209/1000 | Loss: 0.00000985
Iteration 210/1000 | Loss: 0.00000985
Iteration 211/1000 | Loss: 0.00000985
Iteration 212/1000 | Loss: 0.00000985
Iteration 213/1000 | Loss: 0.00000985
Iteration 214/1000 | Loss: 0.00000985
Iteration 215/1000 | Loss: 0.00000985
Iteration 216/1000 | Loss: 0.00000985
Iteration 217/1000 | Loss: 0.00000985
Iteration 218/1000 | Loss: 0.00000985
Iteration 219/1000 | Loss: 0.00000985
Iteration 220/1000 | Loss: 0.00000985
Iteration 221/1000 | Loss: 0.00000985
Iteration 222/1000 | Loss: 0.00000985
Iteration 223/1000 | Loss: 0.00000985
Iteration 224/1000 | Loss: 0.00000985
Iteration 225/1000 | Loss: 0.00000985
Iteration 226/1000 | Loss: 0.00000985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [9.846386092249304e-06, 9.846386092249304e-06, 9.846386092249304e-06, 9.846386092249304e-06, 9.846386092249304e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.846386092249304e-06

Optimization complete. Final v2v error: 2.7188491821289062 mm

Highest mean error: 2.7782394886016846 mm for frame 64

Lowest mean error: 2.66496205329895 mm for frame 95

Saving results

Total time: 37.21627879142761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917710
Iteration 2/25 | Loss: 0.00120987
Iteration 3/25 | Loss: 0.00106787
Iteration 4/25 | Loss: 0.00104466
Iteration 5/25 | Loss: 0.00103619
Iteration 6/25 | Loss: 0.00103373
Iteration 7/25 | Loss: 0.00103353
Iteration 8/25 | Loss: 0.00103353
Iteration 9/25 | Loss: 0.00103353
Iteration 10/25 | Loss: 0.00103353
Iteration 11/25 | Loss: 0.00103353
Iteration 12/25 | Loss: 0.00103346
Iteration 13/25 | Loss: 0.00103346
Iteration 14/25 | Loss: 0.00103346
Iteration 15/25 | Loss: 0.00103346
Iteration 16/25 | Loss: 0.00103346
Iteration 17/25 | Loss: 0.00103346
Iteration 18/25 | Loss: 0.00103346
Iteration 19/25 | Loss: 0.00103346
Iteration 20/25 | Loss: 0.00103346
Iteration 21/25 | Loss: 0.00103346
Iteration 22/25 | Loss: 0.00103346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010334630496799946, 0.0010334630496799946, 0.0010334630496799946, 0.0010334630496799946, 0.0010334630496799946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010334630496799946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38020122
Iteration 2/25 | Loss: 0.00074972
Iteration 3/25 | Loss: 0.00074970
Iteration 4/25 | Loss: 0.00074970
Iteration 5/25 | Loss: 0.00074970
Iteration 6/25 | Loss: 0.00074970
Iteration 7/25 | Loss: 0.00074970
Iteration 8/25 | Loss: 0.00074970
Iteration 9/25 | Loss: 0.00074970
Iteration 10/25 | Loss: 0.00074970
Iteration 11/25 | Loss: 0.00074970
Iteration 12/25 | Loss: 0.00074970
Iteration 13/25 | Loss: 0.00074970
Iteration 14/25 | Loss: 0.00074970
Iteration 15/25 | Loss: 0.00074970
Iteration 16/25 | Loss: 0.00074970
Iteration 17/25 | Loss: 0.00074970
Iteration 18/25 | Loss: 0.00074970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007496987818740308, 0.0007496987818740308, 0.0007496987818740308, 0.0007496987818740308, 0.0007496987818740308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007496987818740308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074970
Iteration 2/1000 | Loss: 0.00004685
Iteration 3/1000 | Loss: 0.00002890
Iteration 4/1000 | Loss: 0.00002316
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002083
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001906
Iteration 10/1000 | Loss: 0.00001878
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001845
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001825
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001822
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001819
Iteration 24/1000 | Loss: 0.00001818
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001818
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001814
Iteration 29/1000 | Loss: 0.00001814
Iteration 30/1000 | Loss: 0.00001814
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001810
Iteration 39/1000 | Loss: 0.00001809
Iteration 40/1000 | Loss: 0.00001809
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001803
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001803
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001803
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001801
Iteration 90/1000 | Loss: 0.00001801
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001801
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001801
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001801
Iteration 99/1000 | Loss: 0.00001801
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001800
Iteration 102/1000 | Loss: 0.00001800
Iteration 103/1000 | Loss: 0.00001800
Iteration 104/1000 | Loss: 0.00001800
Iteration 105/1000 | Loss: 0.00001800
Iteration 106/1000 | Loss: 0.00001800
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001798
Iteration 110/1000 | Loss: 0.00001798
Iteration 111/1000 | Loss: 0.00001798
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001797
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001797
Iteration 119/1000 | Loss: 0.00001797
Iteration 120/1000 | Loss: 0.00001797
Iteration 121/1000 | Loss: 0.00001797
Iteration 122/1000 | Loss: 0.00001797
Iteration 123/1000 | Loss: 0.00001797
Iteration 124/1000 | Loss: 0.00001797
Iteration 125/1000 | Loss: 0.00001796
Iteration 126/1000 | Loss: 0.00001796
Iteration 127/1000 | Loss: 0.00001796
Iteration 128/1000 | Loss: 0.00001796
Iteration 129/1000 | Loss: 0.00001796
Iteration 130/1000 | Loss: 0.00001795
Iteration 131/1000 | Loss: 0.00001795
Iteration 132/1000 | Loss: 0.00001795
Iteration 133/1000 | Loss: 0.00001795
Iteration 134/1000 | Loss: 0.00001795
Iteration 135/1000 | Loss: 0.00001795
Iteration 136/1000 | Loss: 0.00001794
Iteration 137/1000 | Loss: 0.00001794
Iteration 138/1000 | Loss: 0.00001794
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001794
Iteration 142/1000 | Loss: 0.00001794
Iteration 143/1000 | Loss: 0.00001794
Iteration 144/1000 | Loss: 0.00001794
Iteration 145/1000 | Loss: 0.00001794
Iteration 146/1000 | Loss: 0.00001793
Iteration 147/1000 | Loss: 0.00001792
Iteration 148/1000 | Loss: 0.00001792
Iteration 149/1000 | Loss: 0.00001792
Iteration 150/1000 | Loss: 0.00001792
Iteration 151/1000 | Loss: 0.00001792
Iteration 152/1000 | Loss: 0.00001792
Iteration 153/1000 | Loss: 0.00001792
Iteration 154/1000 | Loss: 0.00001792
Iteration 155/1000 | Loss: 0.00001792
Iteration 156/1000 | Loss: 0.00001792
Iteration 157/1000 | Loss: 0.00001792
Iteration 158/1000 | Loss: 0.00001791
Iteration 159/1000 | Loss: 0.00001791
Iteration 160/1000 | Loss: 0.00001791
Iteration 161/1000 | Loss: 0.00001791
Iteration 162/1000 | Loss: 0.00001791
Iteration 163/1000 | Loss: 0.00001791
Iteration 164/1000 | Loss: 0.00001791
Iteration 165/1000 | Loss: 0.00001791
Iteration 166/1000 | Loss: 0.00001791
Iteration 167/1000 | Loss: 0.00001791
Iteration 168/1000 | Loss: 0.00001791
Iteration 169/1000 | Loss: 0.00001790
Iteration 170/1000 | Loss: 0.00001790
Iteration 171/1000 | Loss: 0.00001790
Iteration 172/1000 | Loss: 0.00001790
Iteration 173/1000 | Loss: 0.00001790
Iteration 174/1000 | Loss: 0.00001789
Iteration 175/1000 | Loss: 0.00001789
Iteration 176/1000 | Loss: 0.00001789
Iteration 177/1000 | Loss: 0.00001789
Iteration 178/1000 | Loss: 0.00001789
Iteration 179/1000 | Loss: 0.00001789
Iteration 180/1000 | Loss: 0.00001789
Iteration 181/1000 | Loss: 0.00001789
Iteration 182/1000 | Loss: 0.00001789
Iteration 183/1000 | Loss: 0.00001789
Iteration 184/1000 | Loss: 0.00001789
Iteration 185/1000 | Loss: 0.00001789
Iteration 186/1000 | Loss: 0.00001789
Iteration 187/1000 | Loss: 0.00001789
Iteration 188/1000 | Loss: 0.00001789
Iteration 189/1000 | Loss: 0.00001788
Iteration 190/1000 | Loss: 0.00001788
Iteration 191/1000 | Loss: 0.00001788
Iteration 192/1000 | Loss: 0.00001788
Iteration 193/1000 | Loss: 0.00001788
Iteration 194/1000 | Loss: 0.00001788
Iteration 195/1000 | Loss: 0.00001788
Iteration 196/1000 | Loss: 0.00001788
Iteration 197/1000 | Loss: 0.00001788
Iteration 198/1000 | Loss: 0.00001788
Iteration 199/1000 | Loss: 0.00001788
Iteration 200/1000 | Loss: 0.00001788
Iteration 201/1000 | Loss: 0.00001788
Iteration 202/1000 | Loss: 0.00001788
Iteration 203/1000 | Loss: 0.00001788
Iteration 204/1000 | Loss: 0.00001788
Iteration 205/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.7877231584861875e-05, 1.7877231584861875e-05, 1.7877231584861875e-05, 1.7877231584861875e-05, 1.7877231584861875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7877231584861875e-05

Optimization complete. Final v2v error: 3.4930429458618164 mm

Highest mean error: 5.549577713012695 mm for frame 70

Lowest mean error: 2.9860446453094482 mm for frame 2

Saving results

Total time: 41.54557657241821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440600
Iteration 2/25 | Loss: 0.00111011
Iteration 3/25 | Loss: 0.00101965
Iteration 4/25 | Loss: 0.00100753
Iteration 5/25 | Loss: 0.00100332
Iteration 6/25 | Loss: 0.00100301
Iteration 7/25 | Loss: 0.00100301
Iteration 8/25 | Loss: 0.00100301
Iteration 9/25 | Loss: 0.00100301
Iteration 10/25 | Loss: 0.00100301
Iteration 11/25 | Loss: 0.00100301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010030126431956887, 0.0010030126431956887, 0.0010030126431956887, 0.0010030126431956887, 0.0010030126431956887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010030126431956887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39591587
Iteration 2/25 | Loss: 0.00081117
Iteration 3/25 | Loss: 0.00081116
Iteration 4/25 | Loss: 0.00081116
Iteration 5/25 | Loss: 0.00081116
Iteration 6/25 | Loss: 0.00081116
Iteration 7/25 | Loss: 0.00081116
Iteration 8/25 | Loss: 0.00081116
Iteration 9/25 | Loss: 0.00081116
Iteration 10/25 | Loss: 0.00081116
Iteration 11/25 | Loss: 0.00081116
Iteration 12/25 | Loss: 0.00081116
Iteration 13/25 | Loss: 0.00081116
Iteration 14/25 | Loss: 0.00081116
Iteration 15/25 | Loss: 0.00081116
Iteration 16/25 | Loss: 0.00081116
Iteration 17/25 | Loss: 0.00081116
Iteration 18/25 | Loss: 0.00081116
Iteration 19/25 | Loss: 0.00081116
Iteration 20/25 | Loss: 0.00081116
Iteration 21/25 | Loss: 0.00081116
Iteration 22/25 | Loss: 0.00081116
Iteration 23/25 | Loss: 0.00081116
Iteration 24/25 | Loss: 0.00081116
Iteration 25/25 | Loss: 0.00081116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081116
Iteration 2/1000 | Loss: 0.00002296
Iteration 3/1000 | Loss: 0.00001760
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001546
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001422
Iteration 11/1000 | Loss: 0.00001412
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001398
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001384
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001381
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001381
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001381
Iteration 36/1000 | Loss: 0.00001381
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [1.3812412362312898e-05, 1.3812412362312898e-05, 1.3812412362312898e-05, 1.3812412362312898e-05, 1.3812412362312898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3812412362312898e-05

Optimization complete. Final v2v error: 3.1067726612091064 mm

Highest mean error: 3.6862525939941406 mm for frame 130

Lowest mean error: 2.8109869956970215 mm for frame 26

Saving results

Total time: 29.04586696624756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814794
Iteration 2/25 | Loss: 0.00126226
Iteration 3/25 | Loss: 0.00102440
Iteration 4/25 | Loss: 0.00099664
Iteration 5/25 | Loss: 0.00099104
Iteration 6/25 | Loss: 0.00098996
Iteration 7/25 | Loss: 0.00098969
Iteration 8/25 | Loss: 0.00098958
Iteration 9/25 | Loss: 0.00098958
Iteration 10/25 | Loss: 0.00098958
Iteration 11/25 | Loss: 0.00098958
Iteration 12/25 | Loss: 0.00098958
Iteration 13/25 | Loss: 0.00098958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000989582622423768, 0.000989582622423768, 0.000989582622423768, 0.000989582622423768, 0.000989582622423768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000989582622423768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38260663
Iteration 2/25 | Loss: 0.00071682
Iteration 3/25 | Loss: 0.00071682
Iteration 4/25 | Loss: 0.00071682
Iteration 5/25 | Loss: 0.00071682
Iteration 6/25 | Loss: 0.00071681
Iteration 7/25 | Loss: 0.00071681
Iteration 8/25 | Loss: 0.00071681
Iteration 9/25 | Loss: 0.00071681
Iteration 10/25 | Loss: 0.00071681
Iteration 11/25 | Loss: 0.00071681
Iteration 12/25 | Loss: 0.00071681
Iteration 13/25 | Loss: 0.00071681
Iteration 14/25 | Loss: 0.00071681
Iteration 15/25 | Loss: 0.00071681
Iteration 16/25 | Loss: 0.00071681
Iteration 17/25 | Loss: 0.00071681
Iteration 18/25 | Loss: 0.00071681
Iteration 19/25 | Loss: 0.00071681
Iteration 20/25 | Loss: 0.00071681
Iteration 21/25 | Loss: 0.00071681
Iteration 22/25 | Loss: 0.00071681
Iteration 23/25 | Loss: 0.00071681
Iteration 24/25 | Loss: 0.00071681
Iteration 25/25 | Loss: 0.00071681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071681
Iteration 2/1000 | Loss: 0.00001957
Iteration 3/1000 | Loss: 0.00001254
Iteration 4/1000 | Loss: 0.00001109
Iteration 5/1000 | Loss: 0.00001030
Iteration 6/1000 | Loss: 0.00000974
Iteration 7/1000 | Loss: 0.00000938
Iteration 8/1000 | Loss: 0.00000910
Iteration 9/1000 | Loss: 0.00000890
Iteration 10/1000 | Loss: 0.00000882
Iteration 11/1000 | Loss: 0.00000881
Iteration 12/1000 | Loss: 0.00000876
Iteration 13/1000 | Loss: 0.00000874
Iteration 14/1000 | Loss: 0.00000872
Iteration 15/1000 | Loss: 0.00000868
Iteration 16/1000 | Loss: 0.00000868
Iteration 17/1000 | Loss: 0.00000867
Iteration 18/1000 | Loss: 0.00000866
Iteration 19/1000 | Loss: 0.00000862
Iteration 20/1000 | Loss: 0.00000862
Iteration 21/1000 | Loss: 0.00000857
Iteration 22/1000 | Loss: 0.00000857
Iteration 23/1000 | Loss: 0.00000857
Iteration 24/1000 | Loss: 0.00000857
Iteration 25/1000 | Loss: 0.00000857
Iteration 26/1000 | Loss: 0.00000857
Iteration 27/1000 | Loss: 0.00000857
Iteration 28/1000 | Loss: 0.00000857
Iteration 29/1000 | Loss: 0.00000857
Iteration 30/1000 | Loss: 0.00000857
Iteration 31/1000 | Loss: 0.00000856
Iteration 32/1000 | Loss: 0.00000856
Iteration 33/1000 | Loss: 0.00000855
Iteration 34/1000 | Loss: 0.00000855
Iteration 35/1000 | Loss: 0.00000855
Iteration 36/1000 | Loss: 0.00000854
Iteration 37/1000 | Loss: 0.00000854
Iteration 38/1000 | Loss: 0.00000853
Iteration 39/1000 | Loss: 0.00000852
Iteration 40/1000 | Loss: 0.00000851
Iteration 41/1000 | Loss: 0.00000851
Iteration 42/1000 | Loss: 0.00000851
Iteration 43/1000 | Loss: 0.00000851
Iteration 44/1000 | Loss: 0.00000850
Iteration 45/1000 | Loss: 0.00000850
Iteration 46/1000 | Loss: 0.00000850
Iteration 47/1000 | Loss: 0.00000850
Iteration 48/1000 | Loss: 0.00000849
Iteration 49/1000 | Loss: 0.00000849
Iteration 50/1000 | Loss: 0.00000849
Iteration 51/1000 | Loss: 0.00000848
Iteration 52/1000 | Loss: 0.00000848
Iteration 53/1000 | Loss: 0.00000848
Iteration 54/1000 | Loss: 0.00000848
Iteration 55/1000 | Loss: 0.00000847
Iteration 56/1000 | Loss: 0.00000847
Iteration 57/1000 | Loss: 0.00000847
Iteration 58/1000 | Loss: 0.00000847
Iteration 59/1000 | Loss: 0.00000847
Iteration 60/1000 | Loss: 0.00000847
Iteration 61/1000 | Loss: 0.00000847
Iteration 62/1000 | Loss: 0.00000847
Iteration 63/1000 | Loss: 0.00000846
Iteration 64/1000 | Loss: 0.00000846
Iteration 65/1000 | Loss: 0.00000846
Iteration 66/1000 | Loss: 0.00000846
Iteration 67/1000 | Loss: 0.00000846
Iteration 68/1000 | Loss: 0.00000846
Iteration 69/1000 | Loss: 0.00000846
Iteration 70/1000 | Loss: 0.00000846
Iteration 71/1000 | Loss: 0.00000846
Iteration 72/1000 | Loss: 0.00000846
Iteration 73/1000 | Loss: 0.00000846
Iteration 74/1000 | Loss: 0.00000846
Iteration 75/1000 | Loss: 0.00000845
Iteration 76/1000 | Loss: 0.00000845
Iteration 77/1000 | Loss: 0.00000845
Iteration 78/1000 | Loss: 0.00000845
Iteration 79/1000 | Loss: 0.00000845
Iteration 80/1000 | Loss: 0.00000845
Iteration 81/1000 | Loss: 0.00000845
Iteration 82/1000 | Loss: 0.00000845
Iteration 83/1000 | Loss: 0.00000845
Iteration 84/1000 | Loss: 0.00000844
Iteration 85/1000 | Loss: 0.00000844
Iteration 86/1000 | Loss: 0.00000844
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000844
Iteration 93/1000 | Loss: 0.00000844
Iteration 94/1000 | Loss: 0.00000844
Iteration 95/1000 | Loss: 0.00000844
Iteration 96/1000 | Loss: 0.00000843
Iteration 97/1000 | Loss: 0.00000843
Iteration 98/1000 | Loss: 0.00000843
Iteration 99/1000 | Loss: 0.00000843
Iteration 100/1000 | Loss: 0.00000843
Iteration 101/1000 | Loss: 0.00000843
Iteration 102/1000 | Loss: 0.00000843
Iteration 103/1000 | Loss: 0.00000843
Iteration 104/1000 | Loss: 0.00000843
Iteration 105/1000 | Loss: 0.00000843
Iteration 106/1000 | Loss: 0.00000843
Iteration 107/1000 | Loss: 0.00000843
Iteration 108/1000 | Loss: 0.00000843
Iteration 109/1000 | Loss: 0.00000843
Iteration 110/1000 | Loss: 0.00000843
Iteration 111/1000 | Loss: 0.00000843
Iteration 112/1000 | Loss: 0.00000843
Iteration 113/1000 | Loss: 0.00000843
Iteration 114/1000 | Loss: 0.00000843
Iteration 115/1000 | Loss: 0.00000843
Iteration 116/1000 | Loss: 0.00000843
Iteration 117/1000 | Loss: 0.00000843
Iteration 118/1000 | Loss: 0.00000843
Iteration 119/1000 | Loss: 0.00000842
Iteration 120/1000 | Loss: 0.00000842
Iteration 121/1000 | Loss: 0.00000842
Iteration 122/1000 | Loss: 0.00000842
Iteration 123/1000 | Loss: 0.00000842
Iteration 124/1000 | Loss: 0.00000842
Iteration 125/1000 | Loss: 0.00000842
Iteration 126/1000 | Loss: 0.00000842
Iteration 127/1000 | Loss: 0.00000842
Iteration 128/1000 | Loss: 0.00000842
Iteration 129/1000 | Loss: 0.00000842
Iteration 130/1000 | Loss: 0.00000842
Iteration 131/1000 | Loss: 0.00000842
Iteration 132/1000 | Loss: 0.00000842
Iteration 133/1000 | Loss: 0.00000842
Iteration 134/1000 | Loss: 0.00000841
Iteration 135/1000 | Loss: 0.00000841
Iteration 136/1000 | Loss: 0.00000841
Iteration 137/1000 | Loss: 0.00000841
Iteration 138/1000 | Loss: 0.00000841
Iteration 139/1000 | Loss: 0.00000841
Iteration 140/1000 | Loss: 0.00000841
Iteration 141/1000 | Loss: 0.00000841
Iteration 142/1000 | Loss: 0.00000841
Iteration 143/1000 | Loss: 0.00000841
Iteration 144/1000 | Loss: 0.00000841
Iteration 145/1000 | Loss: 0.00000841
Iteration 146/1000 | Loss: 0.00000840
Iteration 147/1000 | Loss: 0.00000840
Iteration 148/1000 | Loss: 0.00000840
Iteration 149/1000 | Loss: 0.00000840
Iteration 150/1000 | Loss: 0.00000840
Iteration 151/1000 | Loss: 0.00000840
Iteration 152/1000 | Loss: 0.00000840
Iteration 153/1000 | Loss: 0.00000840
Iteration 154/1000 | Loss: 0.00000840
Iteration 155/1000 | Loss: 0.00000840
Iteration 156/1000 | Loss: 0.00000839
Iteration 157/1000 | Loss: 0.00000839
Iteration 158/1000 | Loss: 0.00000839
Iteration 159/1000 | Loss: 0.00000839
Iteration 160/1000 | Loss: 0.00000839
Iteration 161/1000 | Loss: 0.00000839
Iteration 162/1000 | Loss: 0.00000839
Iteration 163/1000 | Loss: 0.00000839
Iteration 164/1000 | Loss: 0.00000839
Iteration 165/1000 | Loss: 0.00000839
Iteration 166/1000 | Loss: 0.00000839
Iteration 167/1000 | Loss: 0.00000839
Iteration 168/1000 | Loss: 0.00000839
Iteration 169/1000 | Loss: 0.00000839
Iteration 170/1000 | Loss: 0.00000839
Iteration 171/1000 | Loss: 0.00000839
Iteration 172/1000 | Loss: 0.00000838
Iteration 173/1000 | Loss: 0.00000838
Iteration 174/1000 | Loss: 0.00000838
Iteration 175/1000 | Loss: 0.00000838
Iteration 176/1000 | Loss: 0.00000838
Iteration 177/1000 | Loss: 0.00000838
Iteration 178/1000 | Loss: 0.00000838
Iteration 179/1000 | Loss: 0.00000838
Iteration 180/1000 | Loss: 0.00000838
Iteration 181/1000 | Loss: 0.00000838
Iteration 182/1000 | Loss: 0.00000838
Iteration 183/1000 | Loss: 0.00000838
Iteration 184/1000 | Loss: 0.00000838
Iteration 185/1000 | Loss: 0.00000838
Iteration 186/1000 | Loss: 0.00000838
Iteration 187/1000 | Loss: 0.00000838
Iteration 188/1000 | Loss: 0.00000838
Iteration 189/1000 | Loss: 0.00000838
Iteration 190/1000 | Loss: 0.00000837
Iteration 191/1000 | Loss: 0.00000837
Iteration 192/1000 | Loss: 0.00000837
Iteration 193/1000 | Loss: 0.00000837
Iteration 194/1000 | Loss: 0.00000837
Iteration 195/1000 | Loss: 0.00000837
Iteration 196/1000 | Loss: 0.00000837
Iteration 197/1000 | Loss: 0.00000837
Iteration 198/1000 | Loss: 0.00000837
Iteration 199/1000 | Loss: 0.00000837
Iteration 200/1000 | Loss: 0.00000837
Iteration 201/1000 | Loss: 0.00000837
Iteration 202/1000 | Loss: 0.00000837
Iteration 203/1000 | Loss: 0.00000837
Iteration 204/1000 | Loss: 0.00000837
Iteration 205/1000 | Loss: 0.00000837
Iteration 206/1000 | Loss: 0.00000837
Iteration 207/1000 | Loss: 0.00000836
Iteration 208/1000 | Loss: 0.00000836
Iteration 209/1000 | Loss: 0.00000836
Iteration 210/1000 | Loss: 0.00000836
Iteration 211/1000 | Loss: 0.00000836
Iteration 212/1000 | Loss: 0.00000836
Iteration 213/1000 | Loss: 0.00000836
Iteration 214/1000 | Loss: 0.00000836
Iteration 215/1000 | Loss: 0.00000836
Iteration 216/1000 | Loss: 0.00000836
Iteration 217/1000 | Loss: 0.00000836
Iteration 218/1000 | Loss: 0.00000836
Iteration 219/1000 | Loss: 0.00000835
Iteration 220/1000 | Loss: 0.00000835
Iteration 221/1000 | Loss: 0.00000835
Iteration 222/1000 | Loss: 0.00000835
Iteration 223/1000 | Loss: 0.00000835
Iteration 224/1000 | Loss: 0.00000835
Iteration 225/1000 | Loss: 0.00000835
Iteration 226/1000 | Loss: 0.00000834
Iteration 227/1000 | Loss: 0.00000834
Iteration 228/1000 | Loss: 0.00000834
Iteration 229/1000 | Loss: 0.00000834
Iteration 230/1000 | Loss: 0.00000834
Iteration 231/1000 | Loss: 0.00000834
Iteration 232/1000 | Loss: 0.00000834
Iteration 233/1000 | Loss: 0.00000834
Iteration 234/1000 | Loss: 0.00000834
Iteration 235/1000 | Loss: 0.00000834
Iteration 236/1000 | Loss: 0.00000834
Iteration 237/1000 | Loss: 0.00000834
Iteration 238/1000 | Loss: 0.00000834
Iteration 239/1000 | Loss: 0.00000834
Iteration 240/1000 | Loss: 0.00000834
Iteration 241/1000 | Loss: 0.00000834
Iteration 242/1000 | Loss: 0.00000834
Iteration 243/1000 | Loss: 0.00000834
Iteration 244/1000 | Loss: 0.00000834
Iteration 245/1000 | Loss: 0.00000834
Iteration 246/1000 | Loss: 0.00000834
Iteration 247/1000 | Loss: 0.00000834
Iteration 248/1000 | Loss: 0.00000834
Iteration 249/1000 | Loss: 0.00000834
Iteration 250/1000 | Loss: 0.00000834
Iteration 251/1000 | Loss: 0.00000834
Iteration 252/1000 | Loss: 0.00000834
Iteration 253/1000 | Loss: 0.00000834
Iteration 254/1000 | Loss: 0.00000834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [8.340166459674947e-06, 8.340166459674947e-06, 8.340166459674947e-06, 8.340166459674947e-06, 8.340166459674947e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.340166459674947e-06

Optimization complete. Final v2v error: 2.4911105632781982 mm

Highest mean error: 2.7911109924316406 mm for frame 87

Lowest mean error: 2.397193431854248 mm for frame 23

Saving results

Total time: 41.52558207511902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036120
Iteration 2/25 | Loss: 0.00251374
Iteration 3/25 | Loss: 0.00166258
Iteration 4/25 | Loss: 0.00138532
Iteration 5/25 | Loss: 0.00127874
Iteration 6/25 | Loss: 0.00135079
Iteration 7/25 | Loss: 0.00120720
Iteration 8/25 | Loss: 0.00114688
Iteration 9/25 | Loss: 0.00110954
Iteration 10/25 | Loss: 0.00108991
Iteration 11/25 | Loss: 0.00108244
Iteration 12/25 | Loss: 0.00108180
Iteration 13/25 | Loss: 0.00107974
Iteration 14/25 | Loss: 0.00108086
Iteration 15/25 | Loss: 0.00108063
Iteration 16/25 | Loss: 0.00107748
Iteration 17/25 | Loss: 0.00107396
Iteration 18/25 | Loss: 0.00107872
Iteration 19/25 | Loss: 0.00107662
Iteration 20/25 | Loss: 0.00107572
Iteration 21/25 | Loss: 0.00107910
Iteration 22/25 | Loss: 0.00107497
Iteration 23/25 | Loss: 0.00107447
Iteration 24/25 | Loss: 0.00107493
Iteration 25/25 | Loss: 0.00107253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41271961
Iteration 2/25 | Loss: 0.00166744
Iteration 3/25 | Loss: 0.00133909
Iteration 4/25 | Loss: 0.00133020
Iteration 5/25 | Loss: 0.00130702
Iteration 6/25 | Loss: 0.00130702
Iteration 7/25 | Loss: 0.00130702
Iteration 8/25 | Loss: 0.00130702
Iteration 9/25 | Loss: 0.00130702
Iteration 10/25 | Loss: 0.00130702
Iteration 11/25 | Loss: 0.00130702
Iteration 12/25 | Loss: 0.00130701
Iteration 13/25 | Loss: 0.00130701
Iteration 14/25 | Loss: 0.00130701
Iteration 15/25 | Loss: 0.00130701
Iteration 16/25 | Loss: 0.00130701
Iteration 17/25 | Loss: 0.00130701
Iteration 18/25 | Loss: 0.00130701
Iteration 19/25 | Loss: 0.00130701
Iteration 20/25 | Loss: 0.00130701
Iteration 21/25 | Loss: 0.00130701
Iteration 22/25 | Loss: 0.00130701
Iteration 23/25 | Loss: 0.00130701
Iteration 24/25 | Loss: 0.00130701
Iteration 25/25 | Loss: 0.00130701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130701
Iteration 2/1000 | Loss: 0.00085532
Iteration 3/1000 | Loss: 0.00046460
Iteration 4/1000 | Loss: 0.00019366
Iteration 5/1000 | Loss: 0.00044899
Iteration 6/1000 | Loss: 0.00008424
Iteration 7/1000 | Loss: 0.00008615
Iteration 8/1000 | Loss: 0.00015963
Iteration 9/1000 | Loss: 0.00013274
Iteration 10/1000 | Loss: 0.00014025
Iteration 11/1000 | Loss: 0.00011899
Iteration 12/1000 | Loss: 0.00024504
Iteration 13/1000 | Loss: 0.00014101
Iteration 14/1000 | Loss: 0.00026915
Iteration 15/1000 | Loss: 0.00036611
Iteration 16/1000 | Loss: 0.00016082
Iteration 17/1000 | Loss: 0.00026989
Iteration 18/1000 | Loss: 0.00006216
Iteration 19/1000 | Loss: 0.00009209
Iteration 20/1000 | Loss: 0.00005783
Iteration 21/1000 | Loss: 0.00033628
Iteration 22/1000 | Loss: 0.00025423
Iteration 23/1000 | Loss: 0.00036480
Iteration 24/1000 | Loss: 0.00011934
Iteration 25/1000 | Loss: 0.00010544
Iteration 26/1000 | Loss: 0.00012620
Iteration 27/1000 | Loss: 0.00011570
Iteration 28/1000 | Loss: 0.00025768
Iteration 29/1000 | Loss: 0.00031351
Iteration 30/1000 | Loss: 0.00014536
Iteration 31/1000 | Loss: 0.00006001
Iteration 32/1000 | Loss: 0.00020821
Iteration 33/1000 | Loss: 0.00007200
Iteration 34/1000 | Loss: 0.00018956
Iteration 35/1000 | Loss: 0.00015871
Iteration 36/1000 | Loss: 0.00014542
Iteration 37/1000 | Loss: 0.00012070
Iteration 38/1000 | Loss: 0.00016158
Iteration 39/1000 | Loss: 0.00014371
Iteration 40/1000 | Loss: 0.00008620
Iteration 41/1000 | Loss: 0.00019745
Iteration 42/1000 | Loss: 0.00014939
Iteration 43/1000 | Loss: 0.00016269
Iteration 44/1000 | Loss: 0.00023188
Iteration 45/1000 | Loss: 0.00017198
Iteration 46/1000 | Loss: 0.00012866
Iteration 47/1000 | Loss: 0.00004262
Iteration 48/1000 | Loss: 0.00012379
Iteration 49/1000 | Loss: 0.00004766
Iteration 50/1000 | Loss: 0.00013065
Iteration 51/1000 | Loss: 0.00014570
Iteration 52/1000 | Loss: 0.00009682
Iteration 53/1000 | Loss: 0.00014317
Iteration 54/1000 | Loss: 0.00006989
Iteration 55/1000 | Loss: 0.00010188
Iteration 56/1000 | Loss: 0.00011612
Iteration 57/1000 | Loss: 0.00006551
Iteration 58/1000 | Loss: 0.00004489
Iteration 59/1000 | Loss: 0.00006941
Iteration 60/1000 | Loss: 0.00009721
Iteration 61/1000 | Loss: 0.00010115
Iteration 62/1000 | Loss: 0.00014131
Iteration 63/1000 | Loss: 0.00014670
Iteration 64/1000 | Loss: 0.00014488
Iteration 65/1000 | Loss: 0.00013131
Iteration 66/1000 | Loss: 0.00004397
Iteration 67/1000 | Loss: 0.00004914
Iteration 68/1000 | Loss: 0.00004729
Iteration 69/1000 | Loss: 0.00010721
Iteration 70/1000 | Loss: 0.00009447
Iteration 71/1000 | Loss: 0.00033774
Iteration 72/1000 | Loss: 0.00032060
Iteration 73/1000 | Loss: 0.00010093
Iteration 74/1000 | Loss: 0.00013513
Iteration 75/1000 | Loss: 0.00014112
Iteration 76/1000 | Loss: 0.00017133
Iteration 77/1000 | Loss: 0.00032973
Iteration 78/1000 | Loss: 0.00054559
Iteration 79/1000 | Loss: 0.00013126
Iteration 80/1000 | Loss: 0.00005608
Iteration 81/1000 | Loss: 0.00004546
Iteration 82/1000 | Loss: 0.00004441
Iteration 83/1000 | Loss: 0.00004651
Iteration 84/1000 | Loss: 0.00005669
Iteration 85/1000 | Loss: 0.00009859
Iteration 86/1000 | Loss: 0.00004619
Iteration 87/1000 | Loss: 0.00005730
Iteration 88/1000 | Loss: 0.00003677
Iteration 89/1000 | Loss: 0.00006351
Iteration 90/1000 | Loss: 0.00006913
Iteration 91/1000 | Loss: 0.00004163
Iteration 92/1000 | Loss: 0.00004361
Iteration 93/1000 | Loss: 0.00003640
Iteration 94/1000 | Loss: 0.00013832
Iteration 95/1000 | Loss: 0.00066259
Iteration 96/1000 | Loss: 0.00015578
Iteration 97/1000 | Loss: 0.00018170
Iteration 98/1000 | Loss: 0.00013703
Iteration 99/1000 | Loss: 0.00005116
Iteration 100/1000 | Loss: 0.00005526
Iteration 101/1000 | Loss: 0.00004703
Iteration 102/1000 | Loss: 0.00029522
Iteration 103/1000 | Loss: 0.00004951
Iteration 104/1000 | Loss: 0.00003425
Iteration 105/1000 | Loss: 0.00014288
Iteration 106/1000 | Loss: 0.00122734
Iteration 107/1000 | Loss: 0.00032531
Iteration 108/1000 | Loss: 0.00012461
Iteration 109/1000 | Loss: 0.00074472
Iteration 110/1000 | Loss: 0.00003358
Iteration 111/1000 | Loss: 0.00003123
Iteration 112/1000 | Loss: 0.00008454
Iteration 113/1000 | Loss: 0.00005509
Iteration 114/1000 | Loss: 0.00011038
Iteration 115/1000 | Loss: 0.00003774
Iteration 116/1000 | Loss: 0.00008443
Iteration 117/1000 | Loss: 0.00005943
Iteration 118/1000 | Loss: 0.00003776
Iteration 119/1000 | Loss: 0.00006638
Iteration 120/1000 | Loss: 0.00058641
Iteration 121/1000 | Loss: 0.00003276
Iteration 122/1000 | Loss: 0.00002945
Iteration 123/1000 | Loss: 0.00002900
Iteration 124/1000 | Loss: 0.00002881
Iteration 125/1000 | Loss: 0.00002860
Iteration 126/1000 | Loss: 0.00024018
Iteration 127/1000 | Loss: 0.00024018
Iteration 128/1000 | Loss: 0.00087748
Iteration 129/1000 | Loss: 0.00073106
Iteration 130/1000 | Loss: 0.00011461
Iteration 131/1000 | Loss: 0.00036448
Iteration 132/1000 | Loss: 0.00006838
Iteration 133/1000 | Loss: 0.00004237
Iteration 134/1000 | Loss: 0.00012845
Iteration 135/1000 | Loss: 0.00012091
Iteration 136/1000 | Loss: 0.00003203
Iteration 137/1000 | Loss: 0.00002817
Iteration 138/1000 | Loss: 0.00002559
Iteration 139/1000 | Loss: 0.00003039
Iteration 140/1000 | Loss: 0.00009538
Iteration 141/1000 | Loss: 0.00002193
Iteration 142/1000 | Loss: 0.00007554
Iteration 143/1000 | Loss: 0.00008721
Iteration 144/1000 | Loss: 0.00001954
Iteration 145/1000 | Loss: 0.00010747
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001825
Iteration 148/1000 | Loss: 0.00001796
Iteration 149/1000 | Loss: 0.00006377
Iteration 150/1000 | Loss: 0.00001761
Iteration 151/1000 | Loss: 0.00001759
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001758
Iteration 154/1000 | Loss: 0.00009890
Iteration 155/1000 | Loss: 0.00005908
Iteration 156/1000 | Loss: 0.00021016
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001743
Iteration 159/1000 | Loss: 0.00001742
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001740
Iteration 163/1000 | Loss: 0.00001740
Iteration 164/1000 | Loss: 0.00001739
Iteration 165/1000 | Loss: 0.00001739
Iteration 166/1000 | Loss: 0.00001738
Iteration 167/1000 | Loss: 0.00001738
Iteration 168/1000 | Loss: 0.00001736
Iteration 169/1000 | Loss: 0.00001736
Iteration 170/1000 | Loss: 0.00001735
Iteration 171/1000 | Loss: 0.00001735
Iteration 172/1000 | Loss: 0.00001734
Iteration 173/1000 | Loss: 0.00001734
Iteration 174/1000 | Loss: 0.00001734
Iteration 175/1000 | Loss: 0.00001734
Iteration 176/1000 | Loss: 0.00001733
Iteration 177/1000 | Loss: 0.00001733
Iteration 178/1000 | Loss: 0.00001733
Iteration 179/1000 | Loss: 0.00001732
Iteration 180/1000 | Loss: 0.00001731
Iteration 181/1000 | Loss: 0.00001731
Iteration 182/1000 | Loss: 0.00001731
Iteration 183/1000 | Loss: 0.00001730
Iteration 184/1000 | Loss: 0.00001730
Iteration 185/1000 | Loss: 0.00001730
Iteration 186/1000 | Loss: 0.00001730
Iteration 187/1000 | Loss: 0.00001729
Iteration 188/1000 | Loss: 0.00001729
Iteration 189/1000 | Loss: 0.00005334
Iteration 190/1000 | Loss: 0.00133958
Iteration 191/1000 | Loss: 0.00002480
Iteration 192/1000 | Loss: 0.00001749
Iteration 193/1000 | Loss: 0.00002387
Iteration 194/1000 | Loss: 0.00005423
Iteration 195/1000 | Loss: 0.00001726
Iteration 196/1000 | Loss: 0.00001722
Iteration 197/1000 | Loss: 0.00001722
Iteration 198/1000 | Loss: 0.00001721
Iteration 199/1000 | Loss: 0.00001721
Iteration 200/1000 | Loss: 0.00001721
Iteration 201/1000 | Loss: 0.00001721
Iteration 202/1000 | Loss: 0.00001720
Iteration 203/1000 | Loss: 0.00001720
Iteration 204/1000 | Loss: 0.00001720
Iteration 205/1000 | Loss: 0.00001719
Iteration 206/1000 | Loss: 0.00001719
Iteration 207/1000 | Loss: 0.00001719
Iteration 208/1000 | Loss: 0.00001719
Iteration 209/1000 | Loss: 0.00001719
Iteration 210/1000 | Loss: 0.00001719
Iteration 211/1000 | Loss: 0.00001719
Iteration 212/1000 | Loss: 0.00001718
Iteration 213/1000 | Loss: 0.00001718
Iteration 214/1000 | Loss: 0.00001718
Iteration 215/1000 | Loss: 0.00001718
Iteration 216/1000 | Loss: 0.00001718
Iteration 217/1000 | Loss: 0.00001717
Iteration 218/1000 | Loss: 0.00001717
Iteration 219/1000 | Loss: 0.00001717
Iteration 220/1000 | Loss: 0.00001717
Iteration 221/1000 | Loss: 0.00001717
Iteration 222/1000 | Loss: 0.00001716
Iteration 223/1000 | Loss: 0.00001716
Iteration 224/1000 | Loss: 0.00001715
Iteration 225/1000 | Loss: 0.00001715
Iteration 226/1000 | Loss: 0.00001715
Iteration 227/1000 | Loss: 0.00001715
Iteration 228/1000 | Loss: 0.00001715
Iteration 229/1000 | Loss: 0.00001714
Iteration 230/1000 | Loss: 0.00001714
Iteration 231/1000 | Loss: 0.00001714
Iteration 232/1000 | Loss: 0.00001713
Iteration 233/1000 | Loss: 0.00001713
Iteration 234/1000 | Loss: 0.00001713
Iteration 235/1000 | Loss: 0.00001713
Iteration 236/1000 | Loss: 0.00001713
Iteration 237/1000 | Loss: 0.00001713
Iteration 238/1000 | Loss: 0.00001713
Iteration 239/1000 | Loss: 0.00001713
Iteration 240/1000 | Loss: 0.00001713
Iteration 241/1000 | Loss: 0.00001713
Iteration 242/1000 | Loss: 0.00001713
Iteration 243/1000 | Loss: 0.00001713
Iteration 244/1000 | Loss: 0.00001713
Iteration 245/1000 | Loss: 0.00001712
Iteration 246/1000 | Loss: 0.00001712
Iteration 247/1000 | Loss: 0.00001712
Iteration 248/1000 | Loss: 0.00001712
Iteration 249/1000 | Loss: 0.00001712
Iteration 250/1000 | Loss: 0.00001712
Iteration 251/1000 | Loss: 0.00001712
Iteration 252/1000 | Loss: 0.00001712
Iteration 253/1000 | Loss: 0.00001712
Iteration 254/1000 | Loss: 0.00001712
Iteration 255/1000 | Loss: 0.00001712
Iteration 256/1000 | Loss: 0.00001712
Iteration 257/1000 | Loss: 0.00001712
Iteration 258/1000 | Loss: 0.00001712
Iteration 259/1000 | Loss: 0.00001712
Iteration 260/1000 | Loss: 0.00001712
Iteration 261/1000 | Loss: 0.00001711
Iteration 262/1000 | Loss: 0.00001711
Iteration 263/1000 | Loss: 0.00001711
Iteration 264/1000 | Loss: 0.00001711
Iteration 265/1000 | Loss: 0.00001711
Iteration 266/1000 | Loss: 0.00001711
Iteration 267/1000 | Loss: 0.00001711
Iteration 268/1000 | Loss: 0.00001711
Iteration 269/1000 | Loss: 0.00001711
Iteration 270/1000 | Loss: 0.00001711
Iteration 271/1000 | Loss: 0.00001711
Iteration 272/1000 | Loss: 0.00001711
Iteration 273/1000 | Loss: 0.00001711
Iteration 274/1000 | Loss: 0.00001711
Iteration 275/1000 | Loss: 0.00001711
Iteration 276/1000 | Loss: 0.00001711
Iteration 277/1000 | Loss: 0.00001711
Iteration 278/1000 | Loss: 0.00001711
Iteration 279/1000 | Loss: 0.00001711
Iteration 280/1000 | Loss: 0.00001711
Iteration 281/1000 | Loss: 0.00001711
Iteration 282/1000 | Loss: 0.00001710
Iteration 283/1000 | Loss: 0.00001710
Iteration 284/1000 | Loss: 0.00001710
Iteration 285/1000 | Loss: 0.00001710
Iteration 286/1000 | Loss: 0.00001710
Iteration 287/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.710489777906332e-05, 1.710489777906332e-05, 1.710489777906332e-05, 1.710489777906332e-05, 1.710489777906332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.710489777906332e-05

Optimization complete. Final v2v error: 2.912820816040039 mm

Highest mean error: 10.575543403625488 mm for frame 74

Lowest mean error: 2.3912696838378906 mm for frame 221

Saving results

Total time: 312.8078603744507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375565
Iteration 2/25 | Loss: 0.00114122
Iteration 3/25 | Loss: 0.00099316
Iteration 4/25 | Loss: 0.00097916
Iteration 5/25 | Loss: 0.00097573
Iteration 6/25 | Loss: 0.00097478
Iteration 7/25 | Loss: 0.00097478
Iteration 8/25 | Loss: 0.00097478
Iteration 9/25 | Loss: 0.00097478
Iteration 10/25 | Loss: 0.00097478
Iteration 11/25 | Loss: 0.00097478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009747760486789048, 0.0009747760486789048, 0.0009747760486789048, 0.0009747760486789048, 0.0009747760486789048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009747760486789048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51132524
Iteration 2/25 | Loss: 0.00066801
Iteration 3/25 | Loss: 0.00066800
Iteration 4/25 | Loss: 0.00066800
Iteration 5/25 | Loss: 0.00066800
Iteration 6/25 | Loss: 0.00066800
Iteration 7/25 | Loss: 0.00066800
Iteration 8/25 | Loss: 0.00066800
Iteration 9/25 | Loss: 0.00066800
Iteration 10/25 | Loss: 0.00066800
Iteration 11/25 | Loss: 0.00066800
Iteration 12/25 | Loss: 0.00066800
Iteration 13/25 | Loss: 0.00066800
Iteration 14/25 | Loss: 0.00066800
Iteration 15/25 | Loss: 0.00066800
Iteration 16/25 | Loss: 0.00066800
Iteration 17/25 | Loss: 0.00066800
Iteration 18/25 | Loss: 0.00066800
Iteration 19/25 | Loss: 0.00066800
Iteration 20/25 | Loss: 0.00066800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006680017686448991, 0.0006680017686448991, 0.0006680017686448991, 0.0006680017686448991, 0.0006680017686448991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006680017686448991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066800
Iteration 2/1000 | Loss: 0.00002435
Iteration 3/1000 | Loss: 0.00001238
Iteration 4/1000 | Loss: 0.00001068
Iteration 5/1000 | Loss: 0.00000980
Iteration 6/1000 | Loss: 0.00000916
Iteration 7/1000 | Loss: 0.00000876
Iteration 8/1000 | Loss: 0.00000857
Iteration 9/1000 | Loss: 0.00000834
Iteration 10/1000 | Loss: 0.00000809
Iteration 11/1000 | Loss: 0.00000808
Iteration 12/1000 | Loss: 0.00000805
Iteration 13/1000 | Loss: 0.00000802
Iteration 14/1000 | Loss: 0.00000799
Iteration 15/1000 | Loss: 0.00000798
Iteration 16/1000 | Loss: 0.00000797
Iteration 17/1000 | Loss: 0.00000797
Iteration 18/1000 | Loss: 0.00000796
Iteration 19/1000 | Loss: 0.00000795
Iteration 20/1000 | Loss: 0.00000793
Iteration 21/1000 | Loss: 0.00000792
Iteration 22/1000 | Loss: 0.00000791
Iteration 23/1000 | Loss: 0.00000790
Iteration 24/1000 | Loss: 0.00000789
Iteration 25/1000 | Loss: 0.00000788
Iteration 26/1000 | Loss: 0.00000781
Iteration 27/1000 | Loss: 0.00000780
Iteration 28/1000 | Loss: 0.00000779
Iteration 29/1000 | Loss: 0.00000779
Iteration 30/1000 | Loss: 0.00000779
Iteration 31/1000 | Loss: 0.00000779
Iteration 32/1000 | Loss: 0.00000778
Iteration 33/1000 | Loss: 0.00000778
Iteration 34/1000 | Loss: 0.00000778
Iteration 35/1000 | Loss: 0.00000777
Iteration 36/1000 | Loss: 0.00000777
Iteration 37/1000 | Loss: 0.00000775
Iteration 38/1000 | Loss: 0.00000775
Iteration 39/1000 | Loss: 0.00000775
Iteration 40/1000 | Loss: 0.00000775
Iteration 41/1000 | Loss: 0.00000774
Iteration 42/1000 | Loss: 0.00000774
Iteration 43/1000 | Loss: 0.00000774
Iteration 44/1000 | Loss: 0.00000774
Iteration 45/1000 | Loss: 0.00000774
Iteration 46/1000 | Loss: 0.00000774
Iteration 47/1000 | Loss: 0.00000773
Iteration 48/1000 | Loss: 0.00000773
Iteration 49/1000 | Loss: 0.00000773
Iteration 50/1000 | Loss: 0.00000772
Iteration 51/1000 | Loss: 0.00000772
Iteration 52/1000 | Loss: 0.00000772
Iteration 53/1000 | Loss: 0.00000772
Iteration 54/1000 | Loss: 0.00000771
Iteration 55/1000 | Loss: 0.00000771
Iteration 56/1000 | Loss: 0.00000771
Iteration 57/1000 | Loss: 0.00000771
Iteration 58/1000 | Loss: 0.00000771
Iteration 59/1000 | Loss: 0.00000771
Iteration 60/1000 | Loss: 0.00000771
Iteration 61/1000 | Loss: 0.00000771
Iteration 62/1000 | Loss: 0.00000771
Iteration 63/1000 | Loss: 0.00000771
Iteration 64/1000 | Loss: 0.00000771
Iteration 65/1000 | Loss: 0.00000771
Iteration 66/1000 | Loss: 0.00000770
Iteration 67/1000 | Loss: 0.00000770
Iteration 68/1000 | Loss: 0.00000770
Iteration 69/1000 | Loss: 0.00000770
Iteration 70/1000 | Loss: 0.00000770
Iteration 71/1000 | Loss: 0.00000770
Iteration 72/1000 | Loss: 0.00000769
Iteration 73/1000 | Loss: 0.00000769
Iteration 74/1000 | Loss: 0.00000769
Iteration 75/1000 | Loss: 0.00000769
Iteration 76/1000 | Loss: 0.00000769
Iteration 77/1000 | Loss: 0.00000769
Iteration 78/1000 | Loss: 0.00000768
Iteration 79/1000 | Loss: 0.00000768
Iteration 80/1000 | Loss: 0.00000768
Iteration 81/1000 | Loss: 0.00000768
Iteration 82/1000 | Loss: 0.00000768
Iteration 83/1000 | Loss: 0.00000767
Iteration 84/1000 | Loss: 0.00000767
Iteration 85/1000 | Loss: 0.00000767
Iteration 86/1000 | Loss: 0.00000767
Iteration 87/1000 | Loss: 0.00000767
Iteration 88/1000 | Loss: 0.00000767
Iteration 89/1000 | Loss: 0.00000767
Iteration 90/1000 | Loss: 0.00000766
Iteration 91/1000 | Loss: 0.00000766
Iteration 92/1000 | Loss: 0.00000766
Iteration 93/1000 | Loss: 0.00000765
Iteration 94/1000 | Loss: 0.00000765
Iteration 95/1000 | Loss: 0.00000765
Iteration 96/1000 | Loss: 0.00000764
Iteration 97/1000 | Loss: 0.00000764
Iteration 98/1000 | Loss: 0.00000763
Iteration 99/1000 | Loss: 0.00000763
Iteration 100/1000 | Loss: 0.00000763
Iteration 101/1000 | Loss: 0.00000762
Iteration 102/1000 | Loss: 0.00000762
Iteration 103/1000 | Loss: 0.00000762
Iteration 104/1000 | Loss: 0.00000762
Iteration 105/1000 | Loss: 0.00000762
Iteration 106/1000 | Loss: 0.00000762
Iteration 107/1000 | Loss: 0.00000762
Iteration 108/1000 | Loss: 0.00000762
Iteration 109/1000 | Loss: 0.00000762
Iteration 110/1000 | Loss: 0.00000762
Iteration 111/1000 | Loss: 0.00000762
Iteration 112/1000 | Loss: 0.00000762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [7.617937626491766e-06, 7.617937626491766e-06, 7.617937626491766e-06, 7.617937626491766e-06, 7.617937626491766e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.617937626491766e-06

Optimization complete. Final v2v error: 2.3982346057891846 mm

Highest mean error: 2.8061788082122803 mm for frame 145

Lowest mean error: 2.1854259967803955 mm for frame 64

Saving results

Total time: 36.33660101890564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059670
Iteration 2/25 | Loss: 0.01059670
Iteration 3/25 | Loss: 0.01059670
Iteration 4/25 | Loss: 0.01059670
Iteration 5/25 | Loss: 0.01059670
Iteration 6/25 | Loss: 0.01059669
Iteration 7/25 | Loss: 0.01059669
Iteration 8/25 | Loss: 0.01059669
Iteration 9/25 | Loss: 0.00204975
Iteration 10/25 | Loss: 0.00153283
Iteration 11/25 | Loss: 0.00139119
Iteration 12/25 | Loss: 0.00138000
Iteration 13/25 | Loss: 0.00140404
Iteration 14/25 | Loss: 0.00127934
Iteration 15/25 | Loss: 0.00121220
Iteration 16/25 | Loss: 0.00115794
Iteration 17/25 | Loss: 0.00115282
Iteration 18/25 | Loss: 0.00113830
Iteration 19/25 | Loss: 0.00112451
Iteration 20/25 | Loss: 0.00112123
Iteration 21/25 | Loss: 0.00113134
Iteration 22/25 | Loss: 0.00112968
Iteration 23/25 | Loss: 0.00112136
Iteration 24/25 | Loss: 0.00111505
Iteration 25/25 | Loss: 0.00111201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37544370
Iteration 2/25 | Loss: 0.00139862
Iteration 3/25 | Loss: 0.00127499
Iteration 4/25 | Loss: 0.00127499
Iteration 5/25 | Loss: 0.00127499
Iteration 6/25 | Loss: 0.00127499
Iteration 7/25 | Loss: 0.00127499
Iteration 8/25 | Loss: 0.00127498
Iteration 9/25 | Loss: 0.00127498
Iteration 10/25 | Loss: 0.00127498
Iteration 11/25 | Loss: 0.00127498
Iteration 12/25 | Loss: 0.00127498
Iteration 13/25 | Loss: 0.00127498
Iteration 14/25 | Loss: 0.00127498
Iteration 15/25 | Loss: 0.00127498
Iteration 16/25 | Loss: 0.00127498
Iteration 17/25 | Loss: 0.00127498
Iteration 18/25 | Loss: 0.00127498
Iteration 19/25 | Loss: 0.00127498
Iteration 20/25 | Loss: 0.00127498
Iteration 21/25 | Loss: 0.00127498
Iteration 22/25 | Loss: 0.00127498
Iteration 23/25 | Loss: 0.00127498
Iteration 24/25 | Loss: 0.00127498
Iteration 25/25 | Loss: 0.00127498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001274983398616314, 0.001274983398616314, 0.001274983398616314, 0.001274983398616314, 0.001274983398616314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274983398616314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127498
Iteration 2/1000 | Loss: 0.00048638
Iteration 3/1000 | Loss: 0.00094956
Iteration 4/1000 | Loss: 0.00052333
Iteration 5/1000 | Loss: 0.00065441
Iteration 6/1000 | Loss: 0.00076702
Iteration 7/1000 | Loss: 0.00097166
Iteration 8/1000 | Loss: 0.00092133
Iteration 9/1000 | Loss: 0.00054391
Iteration 10/1000 | Loss: 0.00040824
Iteration 11/1000 | Loss: 0.00050494
Iteration 12/1000 | Loss: 0.00043017
Iteration 13/1000 | Loss: 0.00037823
Iteration 14/1000 | Loss: 0.00043425
Iteration 15/1000 | Loss: 0.00035528
Iteration 16/1000 | Loss: 0.00054388
Iteration 17/1000 | Loss: 0.00071116
Iteration 18/1000 | Loss: 0.00030213
Iteration 19/1000 | Loss: 0.00028755
Iteration 20/1000 | Loss: 0.00047039
Iteration 21/1000 | Loss: 0.00031505
Iteration 22/1000 | Loss: 0.00056918
Iteration 23/1000 | Loss: 0.00057744
Iteration 24/1000 | Loss: 0.00079605
Iteration 25/1000 | Loss: 0.00055972
Iteration 26/1000 | Loss: 0.00065193
Iteration 27/1000 | Loss: 0.00059798
Iteration 28/1000 | Loss: 0.00053509
Iteration 29/1000 | Loss: 0.00051299
Iteration 30/1000 | Loss: 0.00049299
Iteration 31/1000 | Loss: 0.00040787
Iteration 32/1000 | Loss: 0.00043404
Iteration 33/1000 | Loss: 0.00043873
Iteration 34/1000 | Loss: 0.00038724
Iteration 35/1000 | Loss: 0.00043599
Iteration 36/1000 | Loss: 0.00047886
Iteration 37/1000 | Loss: 0.00046479
Iteration 38/1000 | Loss: 0.00065040
Iteration 39/1000 | Loss: 0.00049824
Iteration 40/1000 | Loss: 0.00045939
Iteration 41/1000 | Loss: 0.00036579
Iteration 42/1000 | Loss: 0.00046824
Iteration 43/1000 | Loss: 0.00042352
Iteration 44/1000 | Loss: 0.00038737
Iteration 45/1000 | Loss: 0.00036974
Iteration 46/1000 | Loss: 0.00047073
Iteration 47/1000 | Loss: 0.00045889
Iteration 48/1000 | Loss: 0.00032738
Iteration 49/1000 | Loss: 0.00044512
Iteration 50/1000 | Loss: 0.00044587
Iteration 51/1000 | Loss: 0.00059208
Iteration 52/1000 | Loss: 0.00060182
Iteration 53/1000 | Loss: 0.00050454
Iteration 54/1000 | Loss: 0.00048912
Iteration 55/1000 | Loss: 0.00035356
Iteration 56/1000 | Loss: 0.00032221
Iteration 57/1000 | Loss: 0.00025575
Iteration 58/1000 | Loss: 0.00033415
Iteration 59/1000 | Loss: 0.00034564
Iteration 60/1000 | Loss: 0.00041599
Iteration 61/1000 | Loss: 0.00036529
Iteration 62/1000 | Loss: 0.00027459
Iteration 63/1000 | Loss: 0.00083322
Iteration 64/1000 | Loss: 0.00081144
Iteration 65/1000 | Loss: 0.00089006
Iteration 66/1000 | Loss: 0.00039789
Iteration 67/1000 | Loss: 0.00037658
Iteration 68/1000 | Loss: 0.00031761
Iteration 69/1000 | Loss: 0.00035225
Iteration 70/1000 | Loss: 0.00068789
Iteration 71/1000 | Loss: 0.00024964
Iteration 72/1000 | Loss: 0.00062605
Iteration 73/1000 | Loss: 0.00054483
Iteration 74/1000 | Loss: 0.00064874
Iteration 75/1000 | Loss: 0.00024580
Iteration 76/1000 | Loss: 0.00021293
Iteration 77/1000 | Loss: 0.00018867
Iteration 78/1000 | Loss: 0.00024388
Iteration 79/1000 | Loss: 0.00021202
Iteration 80/1000 | Loss: 0.00029314
Iteration 81/1000 | Loss: 0.00026084
Iteration 82/1000 | Loss: 0.00039314
Iteration 83/1000 | Loss: 0.00034878
Iteration 84/1000 | Loss: 0.00036263
Iteration 85/1000 | Loss: 0.00039038
Iteration 86/1000 | Loss: 0.00043028
Iteration 87/1000 | Loss: 0.00030171
Iteration 88/1000 | Loss: 0.00033650
Iteration 89/1000 | Loss: 0.00032452
Iteration 90/1000 | Loss: 0.00019131
Iteration 91/1000 | Loss: 0.00021242
Iteration 92/1000 | Loss: 0.00016613
Iteration 93/1000 | Loss: 0.00018583
Iteration 94/1000 | Loss: 0.00039233
Iteration 95/1000 | Loss: 0.00010231
Iteration 96/1000 | Loss: 0.00073215
Iteration 97/1000 | Loss: 0.00090146
Iteration 98/1000 | Loss: 0.00017610
Iteration 99/1000 | Loss: 0.00016920
Iteration 100/1000 | Loss: 0.00021860
Iteration 101/1000 | Loss: 0.00030829
Iteration 102/1000 | Loss: 0.00022778
Iteration 103/1000 | Loss: 0.00020071
Iteration 104/1000 | Loss: 0.00018591
Iteration 105/1000 | Loss: 0.00028079
Iteration 106/1000 | Loss: 0.00018787
Iteration 107/1000 | Loss: 0.00017928
Iteration 108/1000 | Loss: 0.00024861
Iteration 109/1000 | Loss: 0.00031472
Iteration 110/1000 | Loss: 0.00028477
Iteration 111/1000 | Loss: 0.00140965
Iteration 112/1000 | Loss: 0.00109473
Iteration 113/1000 | Loss: 0.00024584
Iteration 114/1000 | Loss: 0.00015153
Iteration 115/1000 | Loss: 0.00030515
Iteration 116/1000 | Loss: 0.00023234
Iteration 117/1000 | Loss: 0.00024537
Iteration 118/1000 | Loss: 0.00067752
Iteration 119/1000 | Loss: 0.00006008
Iteration 120/1000 | Loss: 0.00009788
Iteration 121/1000 | Loss: 0.00015267
Iteration 122/1000 | Loss: 0.00033342
Iteration 123/1000 | Loss: 0.00030814
Iteration 124/1000 | Loss: 0.00032017
Iteration 125/1000 | Loss: 0.00070178
Iteration 126/1000 | Loss: 0.00029100
Iteration 127/1000 | Loss: 0.00026123
Iteration 128/1000 | Loss: 0.00061452
Iteration 129/1000 | Loss: 0.00040911
Iteration 130/1000 | Loss: 0.00093587
Iteration 131/1000 | Loss: 0.00038892
Iteration 132/1000 | Loss: 0.00031629
Iteration 133/1000 | Loss: 0.00013396
Iteration 134/1000 | Loss: 0.00016529
Iteration 135/1000 | Loss: 0.00024076
Iteration 136/1000 | Loss: 0.00026505
Iteration 137/1000 | Loss: 0.00038491
Iteration 138/1000 | Loss: 0.00019045
Iteration 139/1000 | Loss: 0.00012504
Iteration 140/1000 | Loss: 0.00009294
Iteration 141/1000 | Loss: 0.00004608
Iteration 142/1000 | Loss: 0.00003645
Iteration 143/1000 | Loss: 0.00004456
Iteration 144/1000 | Loss: 0.00014562
Iteration 145/1000 | Loss: 0.00011704
Iteration 146/1000 | Loss: 0.00009006
Iteration 147/1000 | Loss: 0.00012173
Iteration 148/1000 | Loss: 0.00006803
Iteration 149/1000 | Loss: 0.00007050
Iteration 150/1000 | Loss: 0.00012340
Iteration 151/1000 | Loss: 0.00012586
Iteration 152/1000 | Loss: 0.00014501
Iteration 153/1000 | Loss: 0.00017817
Iteration 154/1000 | Loss: 0.00016181
Iteration 155/1000 | Loss: 0.00012092
Iteration 156/1000 | Loss: 0.00014148
Iteration 157/1000 | Loss: 0.00014363
Iteration 158/1000 | Loss: 0.00014294
Iteration 159/1000 | Loss: 0.00014176
Iteration 160/1000 | Loss: 0.00014282
Iteration 161/1000 | Loss: 0.00013859
Iteration 162/1000 | Loss: 0.00018546
Iteration 163/1000 | Loss: 0.00004638
Iteration 164/1000 | Loss: 0.00010712
Iteration 165/1000 | Loss: 0.00012197
Iteration 166/1000 | Loss: 0.00014806
Iteration 167/1000 | Loss: 0.00014328
Iteration 168/1000 | Loss: 0.00012912
Iteration 169/1000 | Loss: 0.00015773
Iteration 170/1000 | Loss: 0.00024448
Iteration 171/1000 | Loss: 0.00019840
Iteration 172/1000 | Loss: 0.00012473
Iteration 173/1000 | Loss: 0.00007723
Iteration 174/1000 | Loss: 0.00012622
Iteration 175/1000 | Loss: 0.00021213
Iteration 176/1000 | Loss: 0.00014743
Iteration 177/1000 | Loss: 0.00021863
Iteration 178/1000 | Loss: 0.00020948
Iteration 179/1000 | Loss: 0.00004225
Iteration 180/1000 | Loss: 0.00008587
Iteration 181/1000 | Loss: 0.00026287
Iteration 182/1000 | Loss: 0.00024783
Iteration 183/1000 | Loss: 0.00025938
Iteration 184/1000 | Loss: 0.00018141
Iteration 185/1000 | Loss: 0.00018342
Iteration 186/1000 | Loss: 0.00025825
Iteration 187/1000 | Loss: 0.00005279
Iteration 188/1000 | Loss: 0.00005027
Iteration 189/1000 | Loss: 0.00014057
Iteration 190/1000 | Loss: 0.00019218
Iteration 191/1000 | Loss: 0.00018853
Iteration 192/1000 | Loss: 0.00022458
Iteration 193/1000 | Loss: 0.00013554
Iteration 194/1000 | Loss: 0.00016978
Iteration 195/1000 | Loss: 0.00029962
Iteration 196/1000 | Loss: 0.00024221
Iteration 197/1000 | Loss: 0.00004962
Iteration 198/1000 | Loss: 0.00011404
Iteration 199/1000 | Loss: 0.00012731
Iteration 200/1000 | Loss: 0.00025479
Iteration 201/1000 | Loss: 0.00004122
Iteration 202/1000 | Loss: 0.00002886
Iteration 203/1000 | Loss: 0.00008612
Iteration 204/1000 | Loss: 0.00002871
Iteration 205/1000 | Loss: 0.00008342
Iteration 206/1000 | Loss: 0.00003196
Iteration 207/1000 | Loss: 0.00011441
Iteration 208/1000 | Loss: 0.00003086
Iteration 209/1000 | Loss: 0.00003759
Iteration 210/1000 | Loss: 0.00057304
Iteration 211/1000 | Loss: 0.00014809
Iteration 212/1000 | Loss: 0.00025068
Iteration 213/1000 | Loss: 0.00022248
Iteration 214/1000 | Loss: 0.00016271
Iteration 215/1000 | Loss: 0.00011523
Iteration 216/1000 | Loss: 0.00005175
Iteration 217/1000 | Loss: 0.00012819
Iteration 218/1000 | Loss: 0.00025017
Iteration 219/1000 | Loss: 0.00034189
Iteration 220/1000 | Loss: 0.00015285
Iteration 221/1000 | Loss: 0.00018398
Iteration 222/1000 | Loss: 0.00002825
Iteration 223/1000 | Loss: 0.00019601
Iteration 224/1000 | Loss: 0.00003581
Iteration 225/1000 | Loss: 0.00003894
Iteration 226/1000 | Loss: 0.00030587
Iteration 227/1000 | Loss: 0.00022125
Iteration 228/1000 | Loss: 0.00020964
Iteration 229/1000 | Loss: 0.00016840
Iteration 230/1000 | Loss: 0.00012303
Iteration 231/1000 | Loss: 0.00010978
Iteration 232/1000 | Loss: 0.00009675
Iteration 233/1000 | Loss: 0.00015811
Iteration 234/1000 | Loss: 0.00008727
Iteration 235/1000 | Loss: 0.00010960
Iteration 236/1000 | Loss: 0.00008445
Iteration 237/1000 | Loss: 0.00015561
Iteration 238/1000 | Loss: 0.00017565
Iteration 239/1000 | Loss: 0.00008191
Iteration 240/1000 | Loss: 0.00002667
Iteration 241/1000 | Loss: 0.00025472
Iteration 242/1000 | Loss: 0.00026492
Iteration 243/1000 | Loss: 0.00022221
Iteration 244/1000 | Loss: 0.00023557
Iteration 245/1000 | Loss: 0.00005692
Iteration 246/1000 | Loss: 0.00011845
Iteration 247/1000 | Loss: 0.00017401
Iteration 248/1000 | Loss: 0.00009319
Iteration 249/1000 | Loss: 0.00011387
Iteration 250/1000 | Loss: 0.00010629
Iteration 251/1000 | Loss: 0.00011686
Iteration 252/1000 | Loss: 0.00018860
Iteration 253/1000 | Loss: 0.00011614
Iteration 254/1000 | Loss: 0.00016140
Iteration 255/1000 | Loss: 0.00015782
Iteration 256/1000 | Loss: 0.00010381
Iteration 257/1000 | Loss: 0.00005137
Iteration 258/1000 | Loss: 0.00012639
Iteration 259/1000 | Loss: 0.00017149
Iteration 260/1000 | Loss: 0.00003040
Iteration 261/1000 | Loss: 0.00002292
Iteration 262/1000 | Loss: 0.00006402
Iteration 263/1000 | Loss: 0.00002200
Iteration 264/1000 | Loss: 0.00002069
Iteration 265/1000 | Loss: 0.00011573
Iteration 266/1000 | Loss: 0.00004077
Iteration 267/1000 | Loss: 0.00011183
Iteration 268/1000 | Loss: 0.00011422
Iteration 269/1000 | Loss: 0.00010935
Iteration 270/1000 | Loss: 0.00011729
Iteration 271/1000 | Loss: 0.00009951
Iteration 272/1000 | Loss: 0.00010666
Iteration 273/1000 | Loss: 0.00004327
Iteration 274/1000 | Loss: 0.00008368
Iteration 275/1000 | Loss: 0.00005635
Iteration 276/1000 | Loss: 0.00001854
Iteration 277/1000 | Loss: 0.00003523
Iteration 278/1000 | Loss: 0.00016122
Iteration 279/1000 | Loss: 0.00005824
Iteration 280/1000 | Loss: 0.00011968
Iteration 281/1000 | Loss: 0.00005855
Iteration 282/1000 | Loss: 0.00002165
Iteration 283/1000 | Loss: 0.00013576
Iteration 284/1000 | Loss: 0.00003843
Iteration 285/1000 | Loss: 0.00008038
Iteration 286/1000 | Loss: 0.00011333
Iteration 287/1000 | Loss: 0.00005482
Iteration 288/1000 | Loss: 0.00002339
Iteration 289/1000 | Loss: 0.00001935
Iteration 290/1000 | Loss: 0.00019783
Iteration 291/1000 | Loss: 0.00014067
Iteration 292/1000 | Loss: 0.00020332
Iteration 293/1000 | Loss: 0.00017925
Iteration 294/1000 | Loss: 0.00002602
Iteration 295/1000 | Loss: 0.00001966
Iteration 296/1000 | Loss: 0.00001818
Iteration 297/1000 | Loss: 0.00001740
Iteration 298/1000 | Loss: 0.00001690
Iteration 299/1000 | Loss: 0.00001664
Iteration 300/1000 | Loss: 0.00001640
Iteration 301/1000 | Loss: 0.00001619
Iteration 302/1000 | Loss: 0.00001611
Iteration 303/1000 | Loss: 0.00001611
Iteration 304/1000 | Loss: 0.00009436
Iteration 305/1000 | Loss: 0.00011407
Iteration 306/1000 | Loss: 0.00017108
Iteration 307/1000 | Loss: 0.00017156
Iteration 308/1000 | Loss: 0.00008957
Iteration 309/1000 | Loss: 0.00001892
Iteration 310/1000 | Loss: 0.00001767
Iteration 311/1000 | Loss: 0.00008819
Iteration 312/1000 | Loss: 0.00007362
Iteration 313/1000 | Loss: 0.00010105
Iteration 314/1000 | Loss: 0.00008696
Iteration 315/1000 | Loss: 0.00017767
Iteration 316/1000 | Loss: 0.00010466
Iteration 317/1000 | Loss: 0.00014469
Iteration 318/1000 | Loss: 0.00009963
Iteration 319/1000 | Loss: 0.00002434
Iteration 320/1000 | Loss: 0.00001855
Iteration 321/1000 | Loss: 0.00001694
Iteration 322/1000 | Loss: 0.00001588
Iteration 323/1000 | Loss: 0.00001576
Iteration 324/1000 | Loss: 0.00001575
Iteration 325/1000 | Loss: 0.00001562
Iteration 326/1000 | Loss: 0.00001556
Iteration 327/1000 | Loss: 0.00001553
Iteration 328/1000 | Loss: 0.00001552
Iteration 329/1000 | Loss: 0.00001552
Iteration 330/1000 | Loss: 0.00001551
Iteration 331/1000 | Loss: 0.00001549
Iteration 332/1000 | Loss: 0.00001548
Iteration 333/1000 | Loss: 0.00001548
Iteration 334/1000 | Loss: 0.00001548
Iteration 335/1000 | Loss: 0.00001547
Iteration 336/1000 | Loss: 0.00001547
Iteration 337/1000 | Loss: 0.00001547
Iteration 338/1000 | Loss: 0.00001547
Iteration 339/1000 | Loss: 0.00001546
Iteration 340/1000 | Loss: 0.00001546
Iteration 341/1000 | Loss: 0.00001546
Iteration 342/1000 | Loss: 0.00001546
Iteration 343/1000 | Loss: 0.00001545
Iteration 344/1000 | Loss: 0.00001545
Iteration 345/1000 | Loss: 0.00001545
Iteration 346/1000 | Loss: 0.00001544
Iteration 347/1000 | Loss: 0.00001544
Iteration 348/1000 | Loss: 0.00001544
Iteration 349/1000 | Loss: 0.00001544
Iteration 350/1000 | Loss: 0.00001544
Iteration 351/1000 | Loss: 0.00001544
Iteration 352/1000 | Loss: 0.00001543
Iteration 353/1000 | Loss: 0.00001543
Iteration 354/1000 | Loss: 0.00001543
Iteration 355/1000 | Loss: 0.00001542
Iteration 356/1000 | Loss: 0.00001542
Iteration 357/1000 | Loss: 0.00001541
Iteration 358/1000 | Loss: 0.00001541
Iteration 359/1000 | Loss: 0.00001540
Iteration 360/1000 | Loss: 0.00001540
Iteration 361/1000 | Loss: 0.00001539
Iteration 362/1000 | Loss: 0.00001538
Iteration 363/1000 | Loss: 0.00001538
Iteration 364/1000 | Loss: 0.00001538
Iteration 365/1000 | Loss: 0.00001538
Iteration 366/1000 | Loss: 0.00001538
Iteration 367/1000 | Loss: 0.00001538
Iteration 368/1000 | Loss: 0.00001537
Iteration 369/1000 | Loss: 0.00001537
Iteration 370/1000 | Loss: 0.00001537
Iteration 371/1000 | Loss: 0.00001536
Iteration 372/1000 | Loss: 0.00001536
Iteration 373/1000 | Loss: 0.00001536
Iteration 374/1000 | Loss: 0.00001535
Iteration 375/1000 | Loss: 0.00001535
Iteration 376/1000 | Loss: 0.00001535
Iteration 377/1000 | Loss: 0.00001535
Iteration 378/1000 | Loss: 0.00001534
Iteration 379/1000 | Loss: 0.00001534
Iteration 380/1000 | Loss: 0.00001534
Iteration 381/1000 | Loss: 0.00001534
Iteration 382/1000 | Loss: 0.00001534
Iteration 383/1000 | Loss: 0.00001534
Iteration 384/1000 | Loss: 0.00001534
Iteration 385/1000 | Loss: 0.00001534
Iteration 386/1000 | Loss: 0.00001534
Iteration 387/1000 | Loss: 0.00001534
Iteration 388/1000 | Loss: 0.00001534
Iteration 389/1000 | Loss: 0.00001534
Iteration 390/1000 | Loss: 0.00001534
Iteration 391/1000 | Loss: 0.00001533
Iteration 392/1000 | Loss: 0.00001533
Iteration 393/1000 | Loss: 0.00001533
Iteration 394/1000 | Loss: 0.00001533
Iteration 395/1000 | Loss: 0.00001533
Iteration 396/1000 | Loss: 0.00001533
Iteration 397/1000 | Loss: 0.00001533
Iteration 398/1000 | Loss: 0.00001533
Iteration 399/1000 | Loss: 0.00001533
Iteration 400/1000 | Loss: 0.00001533
Iteration 401/1000 | Loss: 0.00001533
Iteration 402/1000 | Loss: 0.00001533
Iteration 403/1000 | Loss: 0.00001533
Iteration 404/1000 | Loss: 0.00001533
Iteration 405/1000 | Loss: 0.00001533
Iteration 406/1000 | Loss: 0.00001533
Iteration 407/1000 | Loss: 0.00001533
Iteration 408/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 408. Stopping optimization.
Last 5 losses: [1.5327546861954033e-05, 1.5327546861954033e-05, 1.5327546861954033e-05, 1.5327546861954033e-05, 1.5327546861954033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5327546861954033e-05

Optimization complete. Final v2v error: 3.188185930252075 mm

Highest mean error: 4.988004684448242 mm for frame 59

Lowest mean error: 2.719177007675171 mm for frame 133

Saving results

Total time: 552.885267496109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002827
Iteration 2/25 | Loss: 0.00256339
Iteration 3/25 | Loss: 0.00182220
Iteration 4/25 | Loss: 0.00168138
Iteration 5/25 | Loss: 0.00151209
Iteration 6/25 | Loss: 0.00125445
Iteration 7/25 | Loss: 0.00116031
Iteration 8/25 | Loss: 0.00111753
Iteration 9/25 | Loss: 0.00109914
Iteration 10/25 | Loss: 0.00109153
Iteration 11/25 | Loss: 0.00109031
Iteration 12/25 | Loss: 0.00109004
Iteration 13/25 | Loss: 0.00108992
Iteration 14/25 | Loss: 0.00108990
Iteration 15/25 | Loss: 0.00108989
Iteration 16/25 | Loss: 0.00108989
Iteration 17/25 | Loss: 0.00108989
Iteration 18/25 | Loss: 0.00108989
Iteration 19/25 | Loss: 0.00108989
Iteration 20/25 | Loss: 0.00108989
Iteration 21/25 | Loss: 0.00108989
Iteration 22/25 | Loss: 0.00108989
Iteration 23/25 | Loss: 0.00108989
Iteration 24/25 | Loss: 0.00108989
Iteration 25/25 | Loss: 0.00108989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27381015
Iteration 2/25 | Loss: 0.00041486
Iteration 3/25 | Loss: 0.00041484
Iteration 4/25 | Loss: 0.00041484
Iteration 5/25 | Loss: 0.00041484
Iteration 6/25 | Loss: 0.00041484
Iteration 7/25 | Loss: 0.00041484
Iteration 8/25 | Loss: 0.00041484
Iteration 9/25 | Loss: 0.00041484
Iteration 10/25 | Loss: 0.00041484
Iteration 11/25 | Loss: 0.00041484
Iteration 12/25 | Loss: 0.00041484
Iteration 13/25 | Loss: 0.00041484
Iteration 14/25 | Loss: 0.00041484
Iteration 15/25 | Loss: 0.00041484
Iteration 16/25 | Loss: 0.00041484
Iteration 17/25 | Loss: 0.00041484
Iteration 18/25 | Loss: 0.00041484
Iteration 19/25 | Loss: 0.00041484
Iteration 20/25 | Loss: 0.00041484
Iteration 21/25 | Loss: 0.00041484
Iteration 22/25 | Loss: 0.00041484
Iteration 23/25 | Loss: 0.00041484
Iteration 24/25 | Loss: 0.00041484
Iteration 25/25 | Loss: 0.00041484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041484
Iteration 2/1000 | Loss: 0.00004856
Iteration 3/1000 | Loss: 0.00002693
Iteration 4/1000 | Loss: 0.00002069
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001768
Iteration 7/1000 | Loss: 0.00001672
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001601
Iteration 10/1000 | Loss: 0.00001572
Iteration 11/1000 | Loss: 0.00001552
Iteration 12/1000 | Loss: 0.00001551
Iteration 13/1000 | Loss: 0.00001545
Iteration 14/1000 | Loss: 0.00001544
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001523
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001522
Iteration 32/1000 | Loss: 0.00001522
Iteration 33/1000 | Loss: 0.00001521
Iteration 34/1000 | Loss: 0.00001521
Iteration 35/1000 | Loss: 0.00001520
Iteration 36/1000 | Loss: 0.00001520
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001519
Iteration 39/1000 | Loss: 0.00001519
Iteration 40/1000 | Loss: 0.00001519
Iteration 41/1000 | Loss: 0.00001519
Iteration 42/1000 | Loss: 0.00001518
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001507
Iteration 67/1000 | Loss: 0.00001507
Iteration 68/1000 | Loss: 0.00001506
Iteration 69/1000 | Loss: 0.00001506
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001503
Iteration 80/1000 | Loss: 0.00001503
Iteration 81/1000 | Loss: 0.00001503
Iteration 82/1000 | Loss: 0.00001503
Iteration 83/1000 | Loss: 0.00001503
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001500
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001499
Iteration 98/1000 | Loss: 0.00001499
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001497
Iteration 105/1000 | Loss: 0.00001497
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001496
Iteration 111/1000 | Loss: 0.00001496
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001495
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001495
Iteration 121/1000 | Loss: 0.00001495
Iteration 122/1000 | Loss: 0.00001495
Iteration 123/1000 | Loss: 0.00001495
Iteration 124/1000 | Loss: 0.00001495
Iteration 125/1000 | Loss: 0.00001495
Iteration 126/1000 | Loss: 0.00001495
Iteration 127/1000 | Loss: 0.00001495
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001494
Iteration 130/1000 | Loss: 0.00001494
Iteration 131/1000 | Loss: 0.00001494
Iteration 132/1000 | Loss: 0.00001494
Iteration 133/1000 | Loss: 0.00001494
Iteration 134/1000 | Loss: 0.00001494
Iteration 135/1000 | Loss: 0.00001494
Iteration 136/1000 | Loss: 0.00001494
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00019466
Iteration 140/1000 | Loss: 0.00002353
Iteration 141/1000 | Loss: 0.00002046
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001682
Iteration 145/1000 | Loss: 0.00001641
Iteration 146/1000 | Loss: 0.00001625
Iteration 147/1000 | Loss: 0.00001596
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001557
Iteration 150/1000 | Loss: 0.00001555
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001545
Iteration 155/1000 | Loss: 0.00001545
Iteration 156/1000 | Loss: 0.00001545
Iteration 157/1000 | Loss: 0.00001545
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001543
Iteration 164/1000 | Loss: 0.00001543
Iteration 165/1000 | Loss: 0.00001543
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001542
Iteration 168/1000 | Loss: 0.00001542
Iteration 169/1000 | Loss: 0.00001542
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001539
Iteration 176/1000 | Loss: 0.00001539
Iteration 177/1000 | Loss: 0.00001539
Iteration 178/1000 | Loss: 0.00001539
Iteration 179/1000 | Loss: 0.00001539
Iteration 180/1000 | Loss: 0.00001539
Iteration 181/1000 | Loss: 0.00001538
Iteration 182/1000 | Loss: 0.00001538
Iteration 183/1000 | Loss: 0.00001538
Iteration 184/1000 | Loss: 0.00001538
Iteration 185/1000 | Loss: 0.00001538
Iteration 186/1000 | Loss: 0.00001537
Iteration 187/1000 | Loss: 0.00001537
Iteration 188/1000 | Loss: 0.00001537
Iteration 189/1000 | Loss: 0.00001537
Iteration 190/1000 | Loss: 0.00001537
Iteration 191/1000 | Loss: 0.00001537
Iteration 192/1000 | Loss: 0.00001537
Iteration 193/1000 | Loss: 0.00001537
Iteration 194/1000 | Loss: 0.00001537
Iteration 195/1000 | Loss: 0.00001537
Iteration 196/1000 | Loss: 0.00001537
Iteration 197/1000 | Loss: 0.00001537
Iteration 198/1000 | Loss: 0.00001537
Iteration 199/1000 | Loss: 0.00001537
Iteration 200/1000 | Loss: 0.00001537
Iteration 201/1000 | Loss: 0.00001537
Iteration 202/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.536550189484842e-05, 1.536550189484842e-05, 1.536550189484842e-05, 1.536550189484842e-05, 1.536550189484842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536550189484842e-05

Optimization complete. Final v2v error: 3.214635133743286 mm

Highest mean error: 6.710196495056152 mm for frame 1

Lowest mean error: 2.850562572479248 mm for frame 110

Saving results

Total time: 66.93045711517334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390226
Iteration 2/25 | Loss: 0.00113715
Iteration 3/25 | Loss: 0.00103925
Iteration 4/25 | Loss: 0.00101679
Iteration 5/25 | Loss: 0.00100997
Iteration 6/25 | Loss: 0.00100749
Iteration 7/25 | Loss: 0.00100655
Iteration 8/25 | Loss: 0.00100652
Iteration 9/25 | Loss: 0.00100652
Iteration 10/25 | Loss: 0.00100652
Iteration 11/25 | Loss: 0.00100652
Iteration 12/25 | Loss: 0.00100652
Iteration 13/25 | Loss: 0.00100652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010065222159028053, 0.0010065222159028053, 0.0010065222159028053, 0.0010065222159028053, 0.0010065222159028053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010065222159028053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38522065
Iteration 2/25 | Loss: 0.00118501
Iteration 3/25 | Loss: 0.00118500
Iteration 4/25 | Loss: 0.00118500
Iteration 5/25 | Loss: 0.00118500
Iteration 6/25 | Loss: 0.00118500
Iteration 7/25 | Loss: 0.00118500
Iteration 8/25 | Loss: 0.00118500
Iteration 9/25 | Loss: 0.00118500
Iteration 10/25 | Loss: 0.00118500
Iteration 11/25 | Loss: 0.00118500
Iteration 12/25 | Loss: 0.00118500
Iteration 13/25 | Loss: 0.00118500
Iteration 14/25 | Loss: 0.00118500
Iteration 15/25 | Loss: 0.00118500
Iteration 16/25 | Loss: 0.00118500
Iteration 17/25 | Loss: 0.00118500
Iteration 18/25 | Loss: 0.00118500
Iteration 19/25 | Loss: 0.00118500
Iteration 20/25 | Loss: 0.00118500
Iteration 21/25 | Loss: 0.00118500
Iteration 22/25 | Loss: 0.00118500
Iteration 23/25 | Loss: 0.00118500
Iteration 24/25 | Loss: 0.00118500
Iteration 25/25 | Loss: 0.00118500

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118500
Iteration 2/1000 | Loss: 0.00004257
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00001709
Iteration 5/1000 | Loss: 0.00001551
Iteration 6/1000 | Loss: 0.00001479
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001305
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001256
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001251
Iteration 75/1000 | Loss: 0.00001251
Iteration 76/1000 | Loss: 0.00001251
Iteration 77/1000 | Loss: 0.00001250
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001248
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001247
Iteration 83/1000 | Loss: 0.00001247
Iteration 84/1000 | Loss: 0.00001247
Iteration 85/1000 | Loss: 0.00001246
Iteration 86/1000 | Loss: 0.00001246
Iteration 87/1000 | Loss: 0.00001246
Iteration 88/1000 | Loss: 0.00001246
Iteration 89/1000 | Loss: 0.00001246
Iteration 90/1000 | Loss: 0.00001246
Iteration 91/1000 | Loss: 0.00001246
Iteration 92/1000 | Loss: 0.00001246
Iteration 93/1000 | Loss: 0.00001246
Iteration 94/1000 | Loss: 0.00001245
Iteration 95/1000 | Loss: 0.00001245
Iteration 96/1000 | Loss: 0.00001245
Iteration 97/1000 | Loss: 0.00001245
Iteration 98/1000 | Loss: 0.00001245
Iteration 99/1000 | Loss: 0.00001245
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001244
Iteration 105/1000 | Loss: 0.00001244
Iteration 106/1000 | Loss: 0.00001244
Iteration 107/1000 | Loss: 0.00001244
Iteration 108/1000 | Loss: 0.00001244
Iteration 109/1000 | Loss: 0.00001244
Iteration 110/1000 | Loss: 0.00001243
Iteration 111/1000 | Loss: 0.00001243
Iteration 112/1000 | Loss: 0.00001243
Iteration 113/1000 | Loss: 0.00001243
Iteration 114/1000 | Loss: 0.00001243
Iteration 115/1000 | Loss: 0.00001243
Iteration 116/1000 | Loss: 0.00001243
Iteration 117/1000 | Loss: 0.00001243
Iteration 118/1000 | Loss: 0.00001243
Iteration 119/1000 | Loss: 0.00001243
Iteration 120/1000 | Loss: 0.00001243
Iteration 121/1000 | Loss: 0.00001243
Iteration 122/1000 | Loss: 0.00001243
Iteration 123/1000 | Loss: 0.00001243
Iteration 124/1000 | Loss: 0.00001243
Iteration 125/1000 | Loss: 0.00001243
Iteration 126/1000 | Loss: 0.00001242
Iteration 127/1000 | Loss: 0.00001242
Iteration 128/1000 | Loss: 0.00001242
Iteration 129/1000 | Loss: 0.00001242
Iteration 130/1000 | Loss: 0.00001242
Iteration 131/1000 | Loss: 0.00001242
Iteration 132/1000 | Loss: 0.00001242
Iteration 133/1000 | Loss: 0.00001242
Iteration 134/1000 | Loss: 0.00001242
Iteration 135/1000 | Loss: 0.00001242
Iteration 136/1000 | Loss: 0.00001242
Iteration 137/1000 | Loss: 0.00001242
Iteration 138/1000 | Loss: 0.00001242
Iteration 139/1000 | Loss: 0.00001242
Iteration 140/1000 | Loss: 0.00001242
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001241
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001241
Iteration 147/1000 | Loss: 0.00001241
Iteration 148/1000 | Loss: 0.00001241
Iteration 149/1000 | Loss: 0.00001241
Iteration 150/1000 | Loss: 0.00001241
Iteration 151/1000 | Loss: 0.00001241
Iteration 152/1000 | Loss: 0.00001241
Iteration 153/1000 | Loss: 0.00001241
Iteration 154/1000 | Loss: 0.00001240
Iteration 155/1000 | Loss: 0.00001240
Iteration 156/1000 | Loss: 0.00001240
Iteration 157/1000 | Loss: 0.00001240
Iteration 158/1000 | Loss: 0.00001240
Iteration 159/1000 | Loss: 0.00001240
Iteration 160/1000 | Loss: 0.00001240
Iteration 161/1000 | Loss: 0.00001240
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001240
Iteration 165/1000 | Loss: 0.00001240
Iteration 166/1000 | Loss: 0.00001240
Iteration 167/1000 | Loss: 0.00001240
Iteration 168/1000 | Loss: 0.00001239
Iteration 169/1000 | Loss: 0.00001239
Iteration 170/1000 | Loss: 0.00001239
Iteration 171/1000 | Loss: 0.00001239
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001239
Iteration 180/1000 | Loss: 0.00001239
Iteration 181/1000 | Loss: 0.00001238
Iteration 182/1000 | Loss: 0.00001238
Iteration 183/1000 | Loss: 0.00001238
Iteration 184/1000 | Loss: 0.00001238
Iteration 185/1000 | Loss: 0.00001238
Iteration 186/1000 | Loss: 0.00001238
Iteration 187/1000 | Loss: 0.00001238
Iteration 188/1000 | Loss: 0.00001238
Iteration 189/1000 | Loss: 0.00001238
Iteration 190/1000 | Loss: 0.00001238
Iteration 191/1000 | Loss: 0.00001238
Iteration 192/1000 | Loss: 0.00001238
Iteration 193/1000 | Loss: 0.00001238
Iteration 194/1000 | Loss: 0.00001238
Iteration 195/1000 | Loss: 0.00001238
Iteration 196/1000 | Loss: 0.00001238
Iteration 197/1000 | Loss: 0.00001238
Iteration 198/1000 | Loss: 0.00001238
Iteration 199/1000 | Loss: 0.00001238
Iteration 200/1000 | Loss: 0.00001238
Iteration 201/1000 | Loss: 0.00001238
Iteration 202/1000 | Loss: 0.00001238
Iteration 203/1000 | Loss: 0.00001238
Iteration 204/1000 | Loss: 0.00001238
Iteration 205/1000 | Loss: 0.00001238
Iteration 206/1000 | Loss: 0.00001238
Iteration 207/1000 | Loss: 0.00001238
Iteration 208/1000 | Loss: 0.00001238
Iteration 209/1000 | Loss: 0.00001238
Iteration 210/1000 | Loss: 0.00001238
Iteration 211/1000 | Loss: 0.00001238
Iteration 212/1000 | Loss: 0.00001238
Iteration 213/1000 | Loss: 0.00001238
Iteration 214/1000 | Loss: 0.00001238
Iteration 215/1000 | Loss: 0.00001238
Iteration 216/1000 | Loss: 0.00001238
Iteration 217/1000 | Loss: 0.00001238
Iteration 218/1000 | Loss: 0.00001238
Iteration 219/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.238320783158997e-05, 1.238320783158997e-05, 1.238320783158997e-05, 1.238320783158997e-05, 1.238320783158997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.238320783158997e-05

Optimization complete. Final v2v error: 2.8699514865875244 mm

Highest mean error: 3.3175711631774902 mm for frame 2

Lowest mean error: 2.575448751449585 mm for frame 40

Saving results

Total time: 40.13962769508362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622917
Iteration 2/25 | Loss: 0.00139935
Iteration 3/25 | Loss: 0.00112810
Iteration 4/25 | Loss: 0.00108008
Iteration 5/25 | Loss: 0.00106450
Iteration 6/25 | Loss: 0.00106755
Iteration 7/25 | Loss: 0.00105577
Iteration 8/25 | Loss: 0.00105525
Iteration 9/25 | Loss: 0.00104186
Iteration 10/25 | Loss: 0.00103055
Iteration 11/25 | Loss: 0.00103015
Iteration 12/25 | Loss: 0.00102594
Iteration 13/25 | Loss: 0.00102231
Iteration 14/25 | Loss: 0.00102199
Iteration 15/25 | Loss: 0.00102155
Iteration 16/25 | Loss: 0.00102674
Iteration 17/25 | Loss: 0.00102399
Iteration 18/25 | Loss: 0.00102027
Iteration 19/25 | Loss: 0.00101994
Iteration 20/25 | Loss: 0.00101923
Iteration 21/25 | Loss: 0.00101995
Iteration 22/25 | Loss: 0.00101972
Iteration 23/25 | Loss: 0.00101916
Iteration 24/25 | Loss: 0.00101931
Iteration 25/25 | Loss: 0.00101930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74820876
Iteration 2/25 | Loss: 0.00098072
Iteration 3/25 | Loss: 0.00096572
Iteration 4/25 | Loss: 0.00096571
Iteration 5/25 | Loss: 0.00096571
Iteration 6/25 | Loss: 0.00096571
Iteration 7/25 | Loss: 0.00096571
Iteration 8/25 | Loss: 0.00096571
Iteration 9/25 | Loss: 0.00096571
Iteration 10/25 | Loss: 0.00096571
Iteration 11/25 | Loss: 0.00096571
Iteration 12/25 | Loss: 0.00096571
Iteration 13/25 | Loss: 0.00096571
Iteration 14/25 | Loss: 0.00096571
Iteration 15/25 | Loss: 0.00096571
Iteration 16/25 | Loss: 0.00096571
Iteration 17/25 | Loss: 0.00096571
Iteration 18/25 | Loss: 0.00096571
Iteration 19/25 | Loss: 0.00096571
Iteration 20/25 | Loss: 0.00096571
Iteration 21/25 | Loss: 0.00096571
Iteration 22/25 | Loss: 0.00096571
Iteration 23/25 | Loss: 0.00096571
Iteration 24/25 | Loss: 0.00096571
Iteration 25/25 | Loss: 0.00096571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096571
Iteration 2/1000 | Loss: 0.00006055
Iteration 3/1000 | Loss: 0.00003723
Iteration 4/1000 | Loss: 0.00003230
Iteration 5/1000 | Loss: 0.00008538
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002669
Iteration 8/1000 | Loss: 0.00002716
Iteration 9/1000 | Loss: 0.00003195
Iteration 10/1000 | Loss: 0.00002713
Iteration 11/1000 | Loss: 0.00003112
Iteration 12/1000 | Loss: 0.00005164
Iteration 13/1000 | Loss: 0.00003446
Iteration 14/1000 | Loss: 0.00004401
Iteration 15/1000 | Loss: 0.00003025
Iteration 16/1000 | Loss: 0.00002643
Iteration 17/1000 | Loss: 0.00003382
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002575
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00002600
Iteration 22/1000 | Loss: 0.00003221
Iteration 23/1000 | Loss: 0.00003270
Iteration 24/1000 | Loss: 0.00002223
Iteration 25/1000 | Loss: 0.00001900
Iteration 26/1000 | Loss: 0.00003275
Iteration 27/1000 | Loss: 0.00002525
Iteration 28/1000 | Loss: 0.00002477
Iteration 29/1000 | Loss: 0.00021403
Iteration 30/1000 | Loss: 0.00044530
Iteration 31/1000 | Loss: 0.00032322
Iteration 32/1000 | Loss: 0.00034494
Iteration 33/1000 | Loss: 0.00029959
Iteration 34/1000 | Loss: 0.00005127
Iteration 35/1000 | Loss: 0.00002706
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00003078
Iteration 38/1000 | Loss: 0.00002441
Iteration 39/1000 | Loss: 0.00003105
Iteration 40/1000 | Loss: 0.00002503
Iteration 41/1000 | Loss: 0.00009433
Iteration 42/1000 | Loss: 0.00003245
Iteration 43/1000 | Loss: 0.00002637
Iteration 44/1000 | Loss: 0.00002917
Iteration 45/1000 | Loss: 0.00002382
Iteration 46/1000 | Loss: 0.00002508
Iteration 47/1000 | Loss: 0.00002732
Iteration 48/1000 | Loss: 0.00002914
Iteration 49/1000 | Loss: 0.00004360
Iteration 50/1000 | Loss: 0.00002892
Iteration 51/1000 | Loss: 0.00002500
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002491
Iteration 54/1000 | Loss: 0.00002320
Iteration 55/1000 | Loss: 0.00004879
Iteration 56/1000 | Loss: 0.00002763
Iteration 57/1000 | Loss: 0.00003510
Iteration 58/1000 | Loss: 0.00002524
Iteration 59/1000 | Loss: 0.00002217
Iteration 60/1000 | Loss: 0.00002846
Iteration 61/1000 | Loss: 0.00002325
Iteration 62/1000 | Loss: 0.00002067
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001550
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001544
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001537
Iteration 105/1000 | Loss: 0.00001536
Iteration 106/1000 | Loss: 0.00001536
Iteration 107/1000 | Loss: 0.00001536
Iteration 108/1000 | Loss: 0.00001536
Iteration 109/1000 | Loss: 0.00001536
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001535
Iteration 112/1000 | Loss: 0.00001534
Iteration 113/1000 | Loss: 0.00001534
Iteration 114/1000 | Loss: 0.00001534
Iteration 115/1000 | Loss: 0.00001534
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001530
Iteration 122/1000 | Loss: 0.00001530
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001527
Iteration 127/1000 | Loss: 0.00001527
Iteration 128/1000 | Loss: 0.00001526
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001524
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Iteration 141/1000 | Loss: 0.00001522
Iteration 142/1000 | Loss: 0.00001522
Iteration 143/1000 | Loss: 0.00001522
Iteration 144/1000 | Loss: 0.00001522
Iteration 145/1000 | Loss: 0.00001521
Iteration 146/1000 | Loss: 0.00001521
Iteration 147/1000 | Loss: 0.00001521
Iteration 148/1000 | Loss: 0.00001521
Iteration 149/1000 | Loss: 0.00001520
Iteration 150/1000 | Loss: 0.00001520
Iteration 151/1000 | Loss: 0.00001520
Iteration 152/1000 | Loss: 0.00001519
Iteration 153/1000 | Loss: 0.00001519
Iteration 154/1000 | Loss: 0.00001519
Iteration 155/1000 | Loss: 0.00001518
Iteration 156/1000 | Loss: 0.00001518
Iteration 157/1000 | Loss: 0.00001518
Iteration 158/1000 | Loss: 0.00001518
Iteration 159/1000 | Loss: 0.00001518
Iteration 160/1000 | Loss: 0.00001518
Iteration 161/1000 | Loss: 0.00001518
Iteration 162/1000 | Loss: 0.00001518
Iteration 163/1000 | Loss: 0.00001518
Iteration 164/1000 | Loss: 0.00001518
Iteration 165/1000 | Loss: 0.00001517
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001517
Iteration 168/1000 | Loss: 0.00001517
Iteration 169/1000 | Loss: 0.00001517
Iteration 170/1000 | Loss: 0.00001517
Iteration 171/1000 | Loss: 0.00001517
Iteration 172/1000 | Loss: 0.00001517
Iteration 173/1000 | Loss: 0.00001517
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001516
Iteration 176/1000 | Loss: 0.00001516
Iteration 177/1000 | Loss: 0.00001516
Iteration 178/1000 | Loss: 0.00001516
Iteration 179/1000 | Loss: 0.00001516
Iteration 180/1000 | Loss: 0.00001516
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00001515
Iteration 183/1000 | Loss: 0.00001515
Iteration 184/1000 | Loss: 0.00001515
Iteration 185/1000 | Loss: 0.00001515
Iteration 186/1000 | Loss: 0.00001515
Iteration 187/1000 | Loss: 0.00001514
Iteration 188/1000 | Loss: 0.00001514
Iteration 189/1000 | Loss: 0.00001514
Iteration 190/1000 | Loss: 0.00001514
Iteration 191/1000 | Loss: 0.00001514
Iteration 192/1000 | Loss: 0.00001514
Iteration 193/1000 | Loss: 0.00001514
Iteration 194/1000 | Loss: 0.00001514
Iteration 195/1000 | Loss: 0.00001514
Iteration 196/1000 | Loss: 0.00001513
Iteration 197/1000 | Loss: 0.00001513
Iteration 198/1000 | Loss: 0.00001513
Iteration 199/1000 | Loss: 0.00001513
Iteration 200/1000 | Loss: 0.00001513
Iteration 201/1000 | Loss: 0.00001513
Iteration 202/1000 | Loss: 0.00001513
Iteration 203/1000 | Loss: 0.00001513
Iteration 204/1000 | Loss: 0.00001513
Iteration 205/1000 | Loss: 0.00001513
Iteration 206/1000 | Loss: 0.00001513
Iteration 207/1000 | Loss: 0.00001513
Iteration 208/1000 | Loss: 0.00001513
Iteration 209/1000 | Loss: 0.00001512
Iteration 210/1000 | Loss: 0.00001512
Iteration 211/1000 | Loss: 0.00001512
Iteration 212/1000 | Loss: 0.00001512
Iteration 213/1000 | Loss: 0.00001512
Iteration 214/1000 | Loss: 0.00001512
Iteration 215/1000 | Loss: 0.00001512
Iteration 216/1000 | Loss: 0.00001512
Iteration 217/1000 | Loss: 0.00001512
Iteration 218/1000 | Loss: 0.00001512
Iteration 219/1000 | Loss: 0.00001512
Iteration 220/1000 | Loss: 0.00001512
Iteration 221/1000 | Loss: 0.00001512
Iteration 222/1000 | Loss: 0.00001512
Iteration 223/1000 | Loss: 0.00001512
Iteration 224/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.5120983334782068e-05, 1.5120983334782068e-05, 1.5120983334782068e-05, 1.5120983334782068e-05, 1.5120983334782068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5120983334782068e-05

Optimization complete. Final v2v error: 3.2309939861297607 mm

Highest mean error: 4.51859712600708 mm for frame 64

Lowest mean error: 2.3885364532470703 mm for frame 181

Saving results

Total time: 170.94708776474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024653
Iteration 2/25 | Loss: 0.00289773
Iteration 3/25 | Loss: 0.00169236
Iteration 4/25 | Loss: 0.00147199
Iteration 5/25 | Loss: 0.00139956
Iteration 6/25 | Loss: 0.00134305
Iteration 7/25 | Loss: 0.00130431
Iteration 8/25 | Loss: 0.00118280
Iteration 9/25 | Loss: 0.00109195
Iteration 10/25 | Loss: 0.00106355
Iteration 11/25 | Loss: 0.00104520
Iteration 12/25 | Loss: 0.00102349
Iteration 13/25 | Loss: 0.00101963
Iteration 14/25 | Loss: 0.00101180
Iteration 15/25 | Loss: 0.00101412
Iteration 16/25 | Loss: 0.00101390
Iteration 17/25 | Loss: 0.00101087
Iteration 18/25 | Loss: 0.00100999
Iteration 19/25 | Loss: 0.00100997
Iteration 20/25 | Loss: 0.00100990
Iteration 21/25 | Loss: 0.00100990
Iteration 22/25 | Loss: 0.00100989
Iteration 23/25 | Loss: 0.00100989
Iteration 24/25 | Loss: 0.00100989
Iteration 25/25 | Loss: 0.00100989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36791122
Iteration 2/25 | Loss: 0.00073998
Iteration 3/25 | Loss: 0.00068884
Iteration 4/25 | Loss: 0.00068884
Iteration 5/25 | Loss: 0.00068883
Iteration 6/25 | Loss: 0.00068883
Iteration 7/25 | Loss: 0.00068883
Iteration 8/25 | Loss: 0.00068883
Iteration 9/25 | Loss: 0.00068883
Iteration 10/25 | Loss: 0.00068883
Iteration 11/25 | Loss: 0.00068883
Iteration 12/25 | Loss: 0.00068883
Iteration 13/25 | Loss: 0.00068883
Iteration 14/25 | Loss: 0.00068883
Iteration 15/25 | Loss: 0.00068883
Iteration 16/25 | Loss: 0.00068883
Iteration 17/25 | Loss: 0.00068883
Iteration 18/25 | Loss: 0.00068883
Iteration 19/25 | Loss: 0.00068883
Iteration 20/25 | Loss: 0.00068883
Iteration 21/25 | Loss: 0.00068883
Iteration 22/25 | Loss: 0.00068883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006888317293487489, 0.0006888317293487489, 0.0006888317293487489, 0.0006888317293487489, 0.0006888317293487489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006888317293487489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068883
Iteration 2/1000 | Loss: 0.00006559
Iteration 3/1000 | Loss: 0.00031740
Iteration 4/1000 | Loss: 0.00027410
Iteration 5/1000 | Loss: 0.00002796
Iteration 6/1000 | Loss: 0.00002749
Iteration 7/1000 | Loss: 0.00003241
Iteration 8/1000 | Loss: 0.00006972
Iteration 9/1000 | Loss: 0.00026356
Iteration 10/1000 | Loss: 0.00011471
Iteration 11/1000 | Loss: 0.00027309
Iteration 12/1000 | Loss: 0.00005128
Iteration 13/1000 | Loss: 0.00002205
Iteration 14/1000 | Loss: 0.00004592
Iteration 15/1000 | Loss: 0.00005841
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00003899
Iteration 18/1000 | Loss: 0.00001533
Iteration 19/1000 | Loss: 0.00005669
Iteration 20/1000 | Loss: 0.00003610
Iteration 21/1000 | Loss: 0.00001582
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00001909
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001541
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00002515
Iteration 33/1000 | Loss: 0.00008854
Iteration 34/1000 | Loss: 0.00002229
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001310
Iteration 43/1000 | Loss: 0.00001310
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001310
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001310
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.3098195267957635e-05, 1.3098195267957635e-05, 1.3098195267957635e-05, 1.3098195267957635e-05, 1.3098195267957635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3098195267957635e-05

Optimization complete. Final v2v error: 3.0525670051574707 mm

Highest mean error: 6.16217565536499 mm for frame 80

Lowest mean error: 2.785789728164673 mm for frame 53

Saving results

Total time: 93.07626390457153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789018
Iteration 2/25 | Loss: 0.00173725
Iteration 3/25 | Loss: 0.00126721
Iteration 4/25 | Loss: 0.00117492
Iteration 5/25 | Loss: 0.00116571
Iteration 6/25 | Loss: 0.00116572
Iteration 7/25 | Loss: 0.00116568
Iteration 8/25 | Loss: 0.00115803
Iteration 9/25 | Loss: 0.00115372
Iteration 10/25 | Loss: 0.00114151
Iteration 11/25 | Loss: 0.00113242
Iteration 12/25 | Loss: 0.00112856
Iteration 13/25 | Loss: 0.00112741
Iteration 14/25 | Loss: 0.00112701
Iteration 15/25 | Loss: 0.00112687
Iteration 16/25 | Loss: 0.00112687
Iteration 17/25 | Loss: 0.00112687
Iteration 18/25 | Loss: 0.00112687
Iteration 19/25 | Loss: 0.00112687
Iteration 20/25 | Loss: 0.00112687
Iteration 21/25 | Loss: 0.00112687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00112686597276479, 0.00112686597276479, 0.00112686597276479, 0.00112686597276479, 0.00112686597276479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00112686597276479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80368090
Iteration 2/25 | Loss: 0.00073969
Iteration 3/25 | Loss: 0.00073965
Iteration 4/25 | Loss: 0.00073965
Iteration 5/25 | Loss: 0.00073965
Iteration 6/25 | Loss: 0.00073965
Iteration 7/25 | Loss: 0.00073965
Iteration 8/25 | Loss: 0.00073965
Iteration 9/25 | Loss: 0.00073965
Iteration 10/25 | Loss: 0.00073965
Iteration 11/25 | Loss: 0.00073965
Iteration 12/25 | Loss: 0.00073965
Iteration 13/25 | Loss: 0.00073965
Iteration 14/25 | Loss: 0.00073965
Iteration 15/25 | Loss: 0.00073965
Iteration 16/25 | Loss: 0.00073965
Iteration 17/25 | Loss: 0.00073965
Iteration 18/25 | Loss: 0.00073965
Iteration 19/25 | Loss: 0.00073965
Iteration 20/25 | Loss: 0.00073965
Iteration 21/25 | Loss: 0.00073965
Iteration 22/25 | Loss: 0.00073965
Iteration 23/25 | Loss: 0.00073965
Iteration 24/25 | Loss: 0.00073965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007396508590318263, 0.0007396508590318263, 0.0007396508590318263, 0.0007396508590318263, 0.0007396508590318263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007396508590318263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073965
Iteration 2/1000 | Loss: 0.00003463
Iteration 3/1000 | Loss: 0.00002706
Iteration 4/1000 | Loss: 0.00002510
Iteration 5/1000 | Loss: 0.00002376
Iteration 6/1000 | Loss: 0.00006029
Iteration 7/1000 | Loss: 0.00002296
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00025537
Iteration 11/1000 | Loss: 0.00002314
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00002078
Iteration 14/1000 | Loss: 0.00002024
Iteration 15/1000 | Loss: 0.00004823
Iteration 16/1000 | Loss: 0.00001993
Iteration 17/1000 | Loss: 0.00003589
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001973
Iteration 20/1000 | Loss: 0.00001970
Iteration 21/1000 | Loss: 0.00004561
Iteration 22/1000 | Loss: 0.00001960
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001936
Iteration 34/1000 | Loss: 0.00001935
Iteration 35/1000 | Loss: 0.00001934
Iteration 36/1000 | Loss: 0.00001934
Iteration 37/1000 | Loss: 0.00001933
Iteration 38/1000 | Loss: 0.00001932
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00004809
Iteration 43/1000 | Loss: 0.00001949
Iteration 44/1000 | Loss: 0.00001917
Iteration 45/1000 | Loss: 0.00001916
Iteration 46/1000 | Loss: 0.00001916
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001913
Iteration 55/1000 | Loss: 0.00001913
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001907
Iteration 70/1000 | Loss: 0.00001907
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001906
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001905
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001905
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001905
Iteration 87/1000 | Loss: 0.00001905
Iteration 88/1000 | Loss: 0.00001905
Iteration 89/1000 | Loss: 0.00001905
Iteration 90/1000 | Loss: 0.00001904
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001904
Iteration 99/1000 | Loss: 0.00001904
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001903
Iteration 118/1000 | Loss: 0.00001903
Iteration 119/1000 | Loss: 0.00001903
Iteration 120/1000 | Loss: 0.00001903
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001903
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001903
Iteration 131/1000 | Loss: 0.00001903
Iteration 132/1000 | Loss: 0.00001903
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.9025488654733635e-05, 1.9025488654733635e-05, 1.9025488654733635e-05, 1.9025488654733635e-05, 1.9025488654733635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9025488654733635e-05

Optimization complete. Final v2v error: 3.581343650817871 mm

Highest mean error: 4.069364547729492 mm for frame 23

Lowest mean error: 2.9899163246154785 mm for frame 59

Saving results

Total time: 74.21192169189453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622704
Iteration 2/25 | Loss: 0.00146469
Iteration 3/25 | Loss: 0.00111810
Iteration 4/25 | Loss: 0.00108174
Iteration 5/25 | Loss: 0.00107771
Iteration 6/25 | Loss: 0.00107682
Iteration 7/25 | Loss: 0.00107682
Iteration 8/25 | Loss: 0.00107682
Iteration 9/25 | Loss: 0.00107682
Iteration 10/25 | Loss: 0.00107682
Iteration 11/25 | Loss: 0.00107682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010768234496936202, 0.0010768234496936202, 0.0010768234496936202, 0.0010768234496936202, 0.0010768234496936202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010768234496936202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18715537
Iteration 2/25 | Loss: 0.00045557
Iteration 3/25 | Loss: 0.00045557
Iteration 4/25 | Loss: 0.00045557
Iteration 5/25 | Loss: 0.00045557
Iteration 6/25 | Loss: 0.00045557
Iteration 7/25 | Loss: 0.00045557
Iteration 8/25 | Loss: 0.00045557
Iteration 9/25 | Loss: 0.00045557
Iteration 10/25 | Loss: 0.00045557
Iteration 11/25 | Loss: 0.00045557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00045557107659988105, 0.00045557107659988105, 0.00045557107659988105, 0.00045557107659988105, 0.00045557107659988105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045557107659988105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045557
Iteration 2/1000 | Loss: 0.00004164
Iteration 3/1000 | Loss: 0.00002315
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001500
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001459
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001414
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001401
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001400
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001395
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001392
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001391
Iteration 52/1000 | Loss: 0.00001391
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001383
Iteration 63/1000 | Loss: 0.00001381
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001380
Iteration 67/1000 | Loss: 0.00001380
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001371
Iteration 114/1000 | Loss: 0.00001371
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001369
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001368
Iteration 148/1000 | Loss: 0.00001368
Iteration 149/1000 | Loss: 0.00001368
Iteration 150/1000 | Loss: 0.00001368
Iteration 151/1000 | Loss: 0.00001368
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001368
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001368
Iteration 167/1000 | Loss: 0.00001368
Iteration 168/1000 | Loss: 0.00001368
Iteration 169/1000 | Loss: 0.00001368
Iteration 170/1000 | Loss: 0.00001368
Iteration 171/1000 | Loss: 0.00001368
Iteration 172/1000 | Loss: 0.00001368
Iteration 173/1000 | Loss: 0.00001368
Iteration 174/1000 | Loss: 0.00001368
Iteration 175/1000 | Loss: 0.00001368
Iteration 176/1000 | Loss: 0.00001368
Iteration 177/1000 | Loss: 0.00001368
Iteration 178/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3675085028808098e-05, 1.3675085028808098e-05, 1.3675085028808098e-05, 1.3675085028808098e-05, 1.3675085028808098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3675085028808098e-05

Optimization complete. Final v2v error: 3.1164357662200928 mm

Highest mean error: 3.8356475830078125 mm for frame 54

Lowest mean error: 2.906813859939575 mm for frame 13

Saving results

Total time: 37.37136697769165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684415
Iteration 2/25 | Loss: 0.00150333
Iteration 3/25 | Loss: 0.00117104
Iteration 4/25 | Loss: 0.00114251
Iteration 5/25 | Loss: 0.00113772
Iteration 6/25 | Loss: 0.00111616
Iteration 7/25 | Loss: 0.00111221
Iteration 8/25 | Loss: 0.00109299
Iteration 9/25 | Loss: 0.00108434
Iteration 10/25 | Loss: 0.00107275
Iteration 11/25 | Loss: 0.00106887
Iteration 12/25 | Loss: 0.00106160
Iteration 13/25 | Loss: 0.00105705
Iteration 14/25 | Loss: 0.00105519
Iteration 15/25 | Loss: 0.00105475
Iteration 16/25 | Loss: 0.00105440
Iteration 17/25 | Loss: 0.00105418
Iteration 18/25 | Loss: 0.00105413
Iteration 19/25 | Loss: 0.00105415
Iteration 20/25 | Loss: 0.00105430
Iteration 21/25 | Loss: 0.00105418
Iteration 22/25 | Loss: 0.00105372
Iteration 23/25 | Loss: 0.00105470
Iteration 24/25 | Loss: 0.00105335
Iteration 25/25 | Loss: 0.00105400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28613424
Iteration 2/25 | Loss: 0.00078151
Iteration 3/25 | Loss: 0.00078146
Iteration 4/25 | Loss: 0.00078146
Iteration 5/25 | Loss: 0.00078146
Iteration 6/25 | Loss: 0.00078146
Iteration 7/25 | Loss: 0.00078146
Iteration 8/25 | Loss: 0.00078146
Iteration 9/25 | Loss: 0.00078146
Iteration 10/25 | Loss: 0.00078146
Iteration 11/25 | Loss: 0.00078146
Iteration 12/25 | Loss: 0.00078146
Iteration 13/25 | Loss: 0.00078146
Iteration 14/25 | Loss: 0.00078146
Iteration 15/25 | Loss: 0.00078146
Iteration 16/25 | Loss: 0.00078146
Iteration 17/25 | Loss: 0.00078146
Iteration 18/25 | Loss: 0.00078146
Iteration 19/25 | Loss: 0.00078146
Iteration 20/25 | Loss: 0.00078146
Iteration 21/25 | Loss: 0.00078146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007814605487510562, 0.0007814605487510562, 0.0007814605487510562, 0.0007814605487510562, 0.0007814605487510562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007814605487510562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078146
Iteration 2/1000 | Loss: 0.00005561
Iteration 3/1000 | Loss: 0.00003901
Iteration 4/1000 | Loss: 0.00003462
Iteration 5/1000 | Loss: 0.00003304
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00003218
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002765
Iteration 10/1000 | Loss: 0.00002636
Iteration 11/1000 | Loss: 0.00002717
Iteration 12/1000 | Loss: 0.00002263
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00001889
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001801
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001717
Iteration 22/1000 | Loss: 0.00001716
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001703
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001700
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001696
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001693
Iteration 53/1000 | Loss: 0.00001693
Iteration 54/1000 | Loss: 0.00001693
Iteration 55/1000 | Loss: 0.00001693
Iteration 56/1000 | Loss: 0.00001693
Iteration 57/1000 | Loss: 0.00001693
Iteration 58/1000 | Loss: 0.00001693
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001691
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001690
Iteration 70/1000 | Loss: 0.00001690
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001686
Iteration 80/1000 | Loss: 0.00001686
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001685
Iteration 83/1000 | Loss: 0.00001685
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Iteration 94/1000 | Loss: 0.00001684
Iteration 95/1000 | Loss: 0.00001684
Iteration 96/1000 | Loss: 0.00001683
Iteration 97/1000 | Loss: 0.00001683
Iteration 98/1000 | Loss: 0.00001683
Iteration 99/1000 | Loss: 0.00001683
Iteration 100/1000 | Loss: 0.00001683
Iteration 101/1000 | Loss: 0.00001683
Iteration 102/1000 | Loss: 0.00001683
Iteration 103/1000 | Loss: 0.00001683
Iteration 104/1000 | Loss: 0.00001683
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001683
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001683
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001683
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001683
Iteration 124/1000 | Loss: 0.00001683
Iteration 125/1000 | Loss: 0.00001683
Iteration 126/1000 | Loss: 0.00001683
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.6831871107569896e-05, 1.6831871107569896e-05, 1.6831871107569896e-05, 1.6831871107569896e-05, 1.6831871107569896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6831871107569896e-05

Optimization complete. Final v2v error: 3.2906179428100586 mm

Highest mean error: 11.313142776489258 mm for frame 16

Lowest mean error: 2.5732386112213135 mm for frame 190

Saving results

Total time: 90.3090546131134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790541
Iteration 2/25 | Loss: 0.00129463
Iteration 3/25 | Loss: 0.00111243
Iteration 4/25 | Loss: 0.00107939
Iteration 5/25 | Loss: 0.00107035
Iteration 6/25 | Loss: 0.00106272
Iteration 7/25 | Loss: 0.00105639
Iteration 8/25 | Loss: 0.00104526
Iteration 9/25 | Loss: 0.00104466
Iteration 10/25 | Loss: 0.00103719
Iteration 11/25 | Loss: 0.00103336
Iteration 12/25 | Loss: 0.00103555
Iteration 13/25 | Loss: 0.00103152
Iteration 14/25 | Loss: 0.00102938
Iteration 15/25 | Loss: 0.00102897
Iteration 16/25 | Loss: 0.00102883
Iteration 17/25 | Loss: 0.00102883
Iteration 18/25 | Loss: 0.00102882
Iteration 19/25 | Loss: 0.00102882
Iteration 20/25 | Loss: 0.00102882
Iteration 21/25 | Loss: 0.00102882
Iteration 22/25 | Loss: 0.00102882
Iteration 23/25 | Loss: 0.00102882
Iteration 24/25 | Loss: 0.00102882
Iteration 25/25 | Loss: 0.00102882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40528667
Iteration 2/25 | Loss: 0.00077204
Iteration 3/25 | Loss: 0.00077203
Iteration 4/25 | Loss: 0.00077203
Iteration 5/25 | Loss: 0.00077202
Iteration 6/25 | Loss: 0.00077202
Iteration 7/25 | Loss: 0.00077202
Iteration 8/25 | Loss: 0.00077202
Iteration 9/25 | Loss: 0.00077202
Iteration 10/25 | Loss: 0.00077202
Iteration 11/25 | Loss: 0.00077202
Iteration 12/25 | Loss: 0.00077202
Iteration 13/25 | Loss: 0.00077202
Iteration 14/25 | Loss: 0.00077202
Iteration 15/25 | Loss: 0.00077202
Iteration 16/25 | Loss: 0.00077202
Iteration 17/25 | Loss: 0.00077202
Iteration 18/25 | Loss: 0.00077202
Iteration 19/25 | Loss: 0.00077202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007720226421952248, 0.0007720226421952248, 0.0007720226421952248, 0.0007720226421952248, 0.0007720226421952248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007720226421952248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077202
Iteration 2/1000 | Loss: 0.00003518
Iteration 3/1000 | Loss: 0.00028247
Iteration 4/1000 | Loss: 0.00022744
Iteration 5/1000 | Loss: 0.00015482
Iteration 6/1000 | Loss: 0.00015049
Iteration 7/1000 | Loss: 0.00016526
Iteration 8/1000 | Loss: 0.00021113
Iteration 9/1000 | Loss: 0.00020475
Iteration 10/1000 | Loss: 0.00051054
Iteration 11/1000 | Loss: 0.00033047
Iteration 12/1000 | Loss: 0.00021367
Iteration 13/1000 | Loss: 0.00017071
Iteration 14/1000 | Loss: 0.00021892
Iteration 15/1000 | Loss: 0.00019158
Iteration 16/1000 | Loss: 0.00026103
Iteration 17/1000 | Loss: 0.00024749
Iteration 18/1000 | Loss: 0.00026204
Iteration 19/1000 | Loss: 0.00039675
Iteration 20/1000 | Loss: 0.00047642
Iteration 21/1000 | Loss: 0.00024105
Iteration 22/1000 | Loss: 0.00013986
Iteration 23/1000 | Loss: 0.00010580
Iteration 24/1000 | Loss: 0.00013814
Iteration 25/1000 | Loss: 0.00010717
Iteration 26/1000 | Loss: 0.00012293
Iteration 27/1000 | Loss: 0.00010659
Iteration 28/1000 | Loss: 0.00016604
Iteration 29/1000 | Loss: 0.00004678
Iteration 30/1000 | Loss: 0.00003102
Iteration 31/1000 | Loss: 0.00051745
Iteration 32/1000 | Loss: 0.00046899
Iteration 33/1000 | Loss: 0.00053790
Iteration 34/1000 | Loss: 0.00026224
Iteration 35/1000 | Loss: 0.00014546
Iteration 36/1000 | Loss: 0.00035356
Iteration 37/1000 | Loss: 0.00019809
Iteration 38/1000 | Loss: 0.00042804
Iteration 39/1000 | Loss: 0.00028083
Iteration 40/1000 | Loss: 0.00029114
Iteration 41/1000 | Loss: 0.00011402
Iteration 42/1000 | Loss: 0.00015304
Iteration 43/1000 | Loss: 0.00004980
Iteration 44/1000 | Loss: 0.00003071
Iteration 45/1000 | Loss: 0.00003530
Iteration 46/1000 | Loss: 0.00003436
Iteration 47/1000 | Loss: 0.00003133
Iteration 48/1000 | Loss: 0.00002957
Iteration 49/1000 | Loss: 0.00022969
Iteration 50/1000 | Loss: 0.00008503
Iteration 51/1000 | Loss: 0.00003154
Iteration 52/1000 | Loss: 0.00003110
Iteration 53/1000 | Loss: 0.00003131
Iteration 54/1000 | Loss: 0.00003027
Iteration 55/1000 | Loss: 0.00003007
Iteration 56/1000 | Loss: 0.00008941
Iteration 57/1000 | Loss: 0.00024284
Iteration 58/1000 | Loss: 0.00020156
Iteration 59/1000 | Loss: 0.00053856
Iteration 60/1000 | Loss: 0.00039611
Iteration 61/1000 | Loss: 0.00007878
Iteration 62/1000 | Loss: 0.00003107
Iteration 63/1000 | Loss: 0.00045828
Iteration 64/1000 | Loss: 0.00036051
Iteration 65/1000 | Loss: 0.00020344
Iteration 66/1000 | Loss: 0.00011380
Iteration 67/1000 | Loss: 0.00010946
Iteration 68/1000 | Loss: 0.00022012
Iteration 69/1000 | Loss: 0.00016850
Iteration 70/1000 | Loss: 0.00021181
Iteration 71/1000 | Loss: 0.00017166
Iteration 72/1000 | Loss: 0.00007083
Iteration 73/1000 | Loss: 0.00002865
Iteration 74/1000 | Loss: 0.00015656
Iteration 75/1000 | Loss: 0.00011586
Iteration 76/1000 | Loss: 0.00002506
Iteration 77/1000 | Loss: 0.00002309
Iteration 78/1000 | Loss: 0.00002246
Iteration 79/1000 | Loss: 0.00002184
Iteration 80/1000 | Loss: 0.00002127
Iteration 81/1000 | Loss: 0.00002358
Iteration 82/1000 | Loss: 0.00016966
Iteration 83/1000 | Loss: 0.00011739
Iteration 84/1000 | Loss: 0.00011273
Iteration 85/1000 | Loss: 0.00003398
Iteration 86/1000 | Loss: 0.00013627
Iteration 87/1000 | Loss: 0.00013935
Iteration 88/1000 | Loss: 0.00012142
Iteration 89/1000 | Loss: 0.00013700
Iteration 90/1000 | Loss: 0.00008096
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00015071
Iteration 93/1000 | Loss: 0.00006021
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001676
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00016274
Iteration 100/1000 | Loss: 0.00017323
Iteration 101/1000 | Loss: 0.00002481
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001808
Iteration 107/1000 | Loss: 0.00001807
Iteration 108/1000 | Loss: 0.00020037
Iteration 109/1000 | Loss: 0.00014121
Iteration 110/1000 | Loss: 0.00001866
Iteration 111/1000 | Loss: 0.00019115
Iteration 112/1000 | Loss: 0.00028444
Iteration 113/1000 | Loss: 0.00022715
Iteration 114/1000 | Loss: 0.00009060
Iteration 115/1000 | Loss: 0.00001977
Iteration 116/1000 | Loss: 0.00001804
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001734
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001644
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001587
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001572
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001510
Iteration 130/1000 | Loss: 0.00001510
Iteration 131/1000 | Loss: 0.00001509
Iteration 132/1000 | Loss: 0.00001508
Iteration 133/1000 | Loss: 0.00001508
Iteration 134/1000 | Loss: 0.00001508
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001507
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001507
Iteration 143/1000 | Loss: 0.00001507
Iteration 144/1000 | Loss: 0.00001506
Iteration 145/1000 | Loss: 0.00001506
Iteration 146/1000 | Loss: 0.00001506
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001505
Iteration 149/1000 | Loss: 0.00001505
Iteration 150/1000 | Loss: 0.00001505
Iteration 151/1000 | Loss: 0.00001505
Iteration 152/1000 | Loss: 0.00001504
Iteration 153/1000 | Loss: 0.00001504
Iteration 154/1000 | Loss: 0.00001504
Iteration 155/1000 | Loss: 0.00001503
Iteration 156/1000 | Loss: 0.00001503
Iteration 157/1000 | Loss: 0.00001503
Iteration 158/1000 | Loss: 0.00001503
Iteration 159/1000 | Loss: 0.00001502
Iteration 160/1000 | Loss: 0.00001502
Iteration 161/1000 | Loss: 0.00001502
Iteration 162/1000 | Loss: 0.00001502
Iteration 163/1000 | Loss: 0.00001502
Iteration 164/1000 | Loss: 0.00001502
Iteration 165/1000 | Loss: 0.00001502
Iteration 166/1000 | Loss: 0.00001502
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Iteration 170/1000 | Loss: 0.00001501
Iteration 171/1000 | Loss: 0.00001501
Iteration 172/1000 | Loss: 0.00001501
Iteration 173/1000 | Loss: 0.00001501
Iteration 174/1000 | Loss: 0.00001501
Iteration 175/1000 | Loss: 0.00001500
Iteration 176/1000 | Loss: 0.00001500
Iteration 177/1000 | Loss: 0.00001500
Iteration 178/1000 | Loss: 0.00001500
Iteration 179/1000 | Loss: 0.00001500
Iteration 180/1000 | Loss: 0.00001500
Iteration 181/1000 | Loss: 0.00001500
Iteration 182/1000 | Loss: 0.00001500
Iteration 183/1000 | Loss: 0.00001500
Iteration 184/1000 | Loss: 0.00001500
Iteration 185/1000 | Loss: 0.00001500
Iteration 186/1000 | Loss: 0.00001500
Iteration 187/1000 | Loss: 0.00001500
Iteration 188/1000 | Loss: 0.00001499
Iteration 189/1000 | Loss: 0.00001499
Iteration 190/1000 | Loss: 0.00001499
Iteration 191/1000 | Loss: 0.00001499
Iteration 192/1000 | Loss: 0.00001499
Iteration 193/1000 | Loss: 0.00001499
Iteration 194/1000 | Loss: 0.00001499
Iteration 195/1000 | Loss: 0.00001499
Iteration 196/1000 | Loss: 0.00001499
Iteration 197/1000 | Loss: 0.00001498
Iteration 198/1000 | Loss: 0.00001498
Iteration 199/1000 | Loss: 0.00001498
Iteration 200/1000 | Loss: 0.00001498
Iteration 201/1000 | Loss: 0.00001498
Iteration 202/1000 | Loss: 0.00001498
Iteration 203/1000 | Loss: 0.00001498
Iteration 204/1000 | Loss: 0.00001498
Iteration 205/1000 | Loss: 0.00001497
Iteration 206/1000 | Loss: 0.00001497
Iteration 207/1000 | Loss: 0.00001497
Iteration 208/1000 | Loss: 0.00001497
Iteration 209/1000 | Loss: 0.00001496
Iteration 210/1000 | Loss: 0.00001496
Iteration 211/1000 | Loss: 0.00001496
Iteration 212/1000 | Loss: 0.00001495
Iteration 213/1000 | Loss: 0.00001495
Iteration 214/1000 | Loss: 0.00001495
Iteration 215/1000 | Loss: 0.00001495
Iteration 216/1000 | Loss: 0.00001495
Iteration 217/1000 | Loss: 0.00001495
Iteration 218/1000 | Loss: 0.00001494
Iteration 219/1000 | Loss: 0.00001494
Iteration 220/1000 | Loss: 0.00001494
Iteration 221/1000 | Loss: 0.00001494
Iteration 222/1000 | Loss: 0.00001494
Iteration 223/1000 | Loss: 0.00001494
Iteration 224/1000 | Loss: 0.00001494
Iteration 225/1000 | Loss: 0.00001494
Iteration 226/1000 | Loss: 0.00001494
Iteration 227/1000 | Loss: 0.00001494
Iteration 228/1000 | Loss: 0.00001493
Iteration 229/1000 | Loss: 0.00001493
Iteration 230/1000 | Loss: 0.00001493
Iteration 231/1000 | Loss: 0.00001493
Iteration 232/1000 | Loss: 0.00001493
Iteration 233/1000 | Loss: 0.00001493
Iteration 234/1000 | Loss: 0.00001493
Iteration 235/1000 | Loss: 0.00001493
Iteration 236/1000 | Loss: 0.00001493
Iteration 237/1000 | Loss: 0.00001493
Iteration 238/1000 | Loss: 0.00001493
Iteration 239/1000 | Loss: 0.00001493
Iteration 240/1000 | Loss: 0.00001493
Iteration 241/1000 | Loss: 0.00001493
Iteration 242/1000 | Loss: 0.00001493
Iteration 243/1000 | Loss: 0.00001493
Iteration 244/1000 | Loss: 0.00001493
Iteration 245/1000 | Loss: 0.00001493
Iteration 246/1000 | Loss: 0.00001493
Iteration 247/1000 | Loss: 0.00001493
Iteration 248/1000 | Loss: 0.00001493
Iteration 249/1000 | Loss: 0.00001493
Iteration 250/1000 | Loss: 0.00001493
Iteration 251/1000 | Loss: 0.00001493
Iteration 252/1000 | Loss: 0.00001493
Iteration 253/1000 | Loss: 0.00001493
Iteration 254/1000 | Loss: 0.00001493
Iteration 255/1000 | Loss: 0.00001493
Iteration 256/1000 | Loss: 0.00001493
Iteration 257/1000 | Loss: 0.00001493
Iteration 258/1000 | Loss: 0.00001493
Iteration 259/1000 | Loss: 0.00001493
Iteration 260/1000 | Loss: 0.00001493
Iteration 261/1000 | Loss: 0.00001493
Iteration 262/1000 | Loss: 0.00001493
Iteration 263/1000 | Loss: 0.00001493
Iteration 264/1000 | Loss: 0.00001493
Iteration 265/1000 | Loss: 0.00001493
Iteration 266/1000 | Loss: 0.00001493
Iteration 267/1000 | Loss: 0.00001493
Iteration 268/1000 | Loss: 0.00001493
Iteration 269/1000 | Loss: 0.00001493
Iteration 270/1000 | Loss: 0.00001493
Iteration 271/1000 | Loss: 0.00001493
Iteration 272/1000 | Loss: 0.00001493
Iteration 273/1000 | Loss: 0.00001493
Iteration 274/1000 | Loss: 0.00001493
Iteration 275/1000 | Loss: 0.00001493
Iteration 276/1000 | Loss: 0.00001493
Iteration 277/1000 | Loss: 0.00001493
Iteration 278/1000 | Loss: 0.00001493
Iteration 279/1000 | Loss: 0.00001493
Iteration 280/1000 | Loss: 0.00001493
Iteration 281/1000 | Loss: 0.00001493
Iteration 282/1000 | Loss: 0.00001493
Iteration 283/1000 | Loss: 0.00001493
Iteration 284/1000 | Loss: 0.00001493
Iteration 285/1000 | Loss: 0.00001493
Iteration 286/1000 | Loss: 0.00001493
Iteration 287/1000 | Loss: 0.00001493
Iteration 288/1000 | Loss: 0.00001493
Iteration 289/1000 | Loss: 0.00001493
Iteration 290/1000 | Loss: 0.00001493
Iteration 291/1000 | Loss: 0.00001493
Iteration 292/1000 | Loss: 0.00001493
Iteration 293/1000 | Loss: 0.00001493
Iteration 294/1000 | Loss: 0.00001493
Iteration 295/1000 | Loss: 0.00001493
Iteration 296/1000 | Loss: 0.00001493
Iteration 297/1000 | Loss: 0.00001493
Iteration 298/1000 | Loss: 0.00001493
Iteration 299/1000 | Loss: 0.00001493
Iteration 300/1000 | Loss: 0.00001493
Iteration 301/1000 | Loss: 0.00001493
Iteration 302/1000 | Loss: 0.00001493
Iteration 303/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [1.4929420103726443e-05, 1.4929420103726443e-05, 1.4929420103726443e-05, 1.4929420103726443e-05, 1.4929420103726443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4929420103726443e-05

Optimization complete. Final v2v error: 3.0371623039245605 mm

Highest mean error: 4.7147603034973145 mm for frame 220

Lowest mean error: 2.3336849212646484 mm for frame 111

Saving results

Total time: 235.57861351966858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794741
Iteration 2/25 | Loss: 0.00124258
Iteration 3/25 | Loss: 0.00105969
Iteration 4/25 | Loss: 0.00103741
Iteration 5/25 | Loss: 0.00102901
Iteration 6/25 | Loss: 0.00102742
Iteration 7/25 | Loss: 0.00102562
Iteration 8/25 | Loss: 0.00102326
Iteration 9/25 | Loss: 0.00102195
Iteration 10/25 | Loss: 0.00102131
Iteration 11/25 | Loss: 0.00102100
Iteration 12/25 | Loss: 0.00102091
Iteration 13/25 | Loss: 0.00102090
Iteration 14/25 | Loss: 0.00102090
Iteration 15/25 | Loss: 0.00102090
Iteration 16/25 | Loss: 0.00102090
Iteration 17/25 | Loss: 0.00102089
Iteration 18/25 | Loss: 0.00102089
Iteration 19/25 | Loss: 0.00102089
Iteration 20/25 | Loss: 0.00102089
Iteration 21/25 | Loss: 0.00102089
Iteration 22/25 | Loss: 0.00102089
Iteration 23/25 | Loss: 0.00102089
Iteration 24/25 | Loss: 0.00102089
Iteration 25/25 | Loss: 0.00102089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61831331
Iteration 2/25 | Loss: 0.00065378
Iteration 3/25 | Loss: 0.00065378
Iteration 4/25 | Loss: 0.00065378
Iteration 5/25 | Loss: 0.00065378
Iteration 6/25 | Loss: 0.00065378
Iteration 7/25 | Loss: 0.00065377
Iteration 8/25 | Loss: 0.00065377
Iteration 9/25 | Loss: 0.00065377
Iteration 10/25 | Loss: 0.00065377
Iteration 11/25 | Loss: 0.00065377
Iteration 12/25 | Loss: 0.00065377
Iteration 13/25 | Loss: 0.00065377
Iteration 14/25 | Loss: 0.00065377
Iteration 15/25 | Loss: 0.00065377
Iteration 16/25 | Loss: 0.00065377
Iteration 17/25 | Loss: 0.00065377
Iteration 18/25 | Loss: 0.00065377
Iteration 19/25 | Loss: 0.00065377
Iteration 20/25 | Loss: 0.00065377
Iteration 21/25 | Loss: 0.00065377
Iteration 22/25 | Loss: 0.00065377
Iteration 23/25 | Loss: 0.00065377
Iteration 24/25 | Loss: 0.00065377
Iteration 25/25 | Loss: 0.00065377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065377
Iteration 2/1000 | Loss: 0.00002943
Iteration 3/1000 | Loss: 0.00001798
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001376
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001317
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001283
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001277
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001261
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001260
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00001254
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001251
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001251
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001250
Iteration 88/1000 | Loss: 0.00001250
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001249
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001248
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001247
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001246
Iteration 110/1000 | Loss: 0.00001246
Iteration 111/1000 | Loss: 0.00001246
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.245706243935274e-05, 1.245706243935274e-05, 1.245706243935274e-05, 1.245706243935274e-05, 1.245706243935274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.245706243935274e-05

Optimization complete. Final v2v error: 2.9827539920806885 mm

Highest mean error: 3.6115024089813232 mm for frame 172

Lowest mean error: 2.6317667961120605 mm for frame 52

Saving results

Total time: 48.64101576805115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057825
Iteration 2/25 | Loss: 0.00181840
Iteration 3/25 | Loss: 0.00137978
Iteration 4/25 | Loss: 0.00127004
Iteration 5/25 | Loss: 0.00115415
Iteration 6/25 | Loss: 0.00114713
Iteration 7/25 | Loss: 0.00113171
Iteration 8/25 | Loss: 0.00109765
Iteration 9/25 | Loss: 0.00109163
Iteration 10/25 | Loss: 0.00108745
Iteration 11/25 | Loss: 0.00108032
Iteration 12/25 | Loss: 0.00107906
Iteration 13/25 | Loss: 0.00107869
Iteration 14/25 | Loss: 0.00107836
Iteration 15/25 | Loss: 0.00107779
Iteration 16/25 | Loss: 0.00107751
Iteration 17/25 | Loss: 0.00107741
Iteration 18/25 | Loss: 0.00107740
Iteration 19/25 | Loss: 0.00107740
Iteration 20/25 | Loss: 0.00107740
Iteration 21/25 | Loss: 0.00107740
Iteration 22/25 | Loss: 0.00107739
Iteration 23/25 | Loss: 0.00107739
Iteration 24/25 | Loss: 0.00107739
Iteration 25/25 | Loss: 0.00107739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46744084
Iteration 2/25 | Loss: 0.00124656
Iteration 3/25 | Loss: 0.00091162
Iteration 4/25 | Loss: 0.00091162
Iteration 5/25 | Loss: 0.00091162
Iteration 6/25 | Loss: 0.00091162
Iteration 7/25 | Loss: 0.00091161
Iteration 8/25 | Loss: 0.00091161
Iteration 9/25 | Loss: 0.00091161
Iteration 10/25 | Loss: 0.00091161
Iteration 11/25 | Loss: 0.00091161
Iteration 12/25 | Loss: 0.00091161
Iteration 13/25 | Loss: 0.00091161
Iteration 14/25 | Loss: 0.00091161
Iteration 15/25 | Loss: 0.00091161
Iteration 16/25 | Loss: 0.00091161
Iteration 17/25 | Loss: 0.00091161
Iteration 18/25 | Loss: 0.00091161
Iteration 19/25 | Loss: 0.00091161
Iteration 20/25 | Loss: 0.00091161
Iteration 21/25 | Loss: 0.00091161
Iteration 22/25 | Loss: 0.00091161
Iteration 23/25 | Loss: 0.00091161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009116136352531612, 0.0009116136352531612, 0.0009116136352531612, 0.0009116136352531612, 0.0009116136352531612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009116136352531612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091161
Iteration 2/1000 | Loss: 0.00047816
Iteration 3/1000 | Loss: 0.00003732
Iteration 4/1000 | Loss: 0.00003110
Iteration 5/1000 | Loss: 0.00013765
Iteration 6/1000 | Loss: 0.00066763
Iteration 7/1000 | Loss: 0.00163604
Iteration 8/1000 | Loss: 0.00017588
Iteration 9/1000 | Loss: 0.00003085
Iteration 10/1000 | Loss: 0.00002851
Iteration 11/1000 | Loss: 0.00002769
Iteration 12/1000 | Loss: 0.00035885
Iteration 13/1000 | Loss: 0.00066340
Iteration 14/1000 | Loss: 0.00003832
Iteration 15/1000 | Loss: 0.00009179
Iteration 16/1000 | Loss: 0.00002711
Iteration 17/1000 | Loss: 0.00008496
Iteration 18/1000 | Loss: 0.00002669
Iteration 19/1000 | Loss: 0.00011879
Iteration 20/1000 | Loss: 0.00018274
Iteration 21/1000 | Loss: 0.00002696
Iteration 22/1000 | Loss: 0.00002624
Iteration 23/1000 | Loss: 0.00020482
Iteration 24/1000 | Loss: 0.00008823
Iteration 25/1000 | Loss: 0.00002625
Iteration 26/1000 | Loss: 0.00002579
Iteration 27/1000 | Loss: 0.00002558
Iteration 28/1000 | Loss: 0.00002553
Iteration 29/1000 | Loss: 0.00002551
Iteration 30/1000 | Loss: 0.00002551
Iteration 31/1000 | Loss: 0.00002550
Iteration 32/1000 | Loss: 0.00002549
Iteration 33/1000 | Loss: 0.00121732
Iteration 34/1000 | Loss: 0.00028790
Iteration 35/1000 | Loss: 0.00027601
Iteration 36/1000 | Loss: 0.00017659
Iteration 37/1000 | Loss: 0.00011924
Iteration 38/1000 | Loss: 0.00002451
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002234
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002095
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00014481
Iteration 45/1000 | Loss: 0.00002175
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00007103
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002020
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00002014
Iteration 52/1000 | Loss: 0.00002013
Iteration 53/1000 | Loss: 0.00002013
Iteration 54/1000 | Loss: 0.00002012
Iteration 55/1000 | Loss: 0.00002011
Iteration 56/1000 | Loss: 0.00002010
Iteration 57/1000 | Loss: 0.00002008
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002007
Iteration 60/1000 | Loss: 0.00002007
Iteration 61/1000 | Loss: 0.00002007
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002003
Iteration 64/1000 | Loss: 0.00002003
Iteration 65/1000 | Loss: 0.00001999
Iteration 66/1000 | Loss: 0.00015150
Iteration 67/1000 | Loss: 0.00002280
Iteration 68/1000 | Loss: 0.00011154
Iteration 69/1000 | Loss: 0.00002003
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001987
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001986
Iteration 85/1000 | Loss: 0.00001986
Iteration 86/1000 | Loss: 0.00001986
Iteration 87/1000 | Loss: 0.00001986
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001986
Iteration 91/1000 | Loss: 0.00001986
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001985
Iteration 94/1000 | Loss: 0.00001985
Iteration 95/1000 | Loss: 0.00001985
Iteration 96/1000 | Loss: 0.00001985
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001982
Iteration 106/1000 | Loss: 0.00001982
Iteration 107/1000 | Loss: 0.00001981
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001981
Iteration 110/1000 | Loss: 0.00001981
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001981
Iteration 119/1000 | Loss: 0.00001981
Iteration 120/1000 | Loss: 0.00001981
Iteration 121/1000 | Loss: 0.00001981
Iteration 122/1000 | Loss: 0.00001981
Iteration 123/1000 | Loss: 0.00001981
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001980
Iteration 130/1000 | Loss: 0.00001980
Iteration 131/1000 | Loss: 0.00001980
Iteration 132/1000 | Loss: 0.00001980
Iteration 133/1000 | Loss: 0.00001980
Iteration 134/1000 | Loss: 0.00001980
Iteration 135/1000 | Loss: 0.00001980
Iteration 136/1000 | Loss: 0.00001980
Iteration 137/1000 | Loss: 0.00001980
Iteration 138/1000 | Loss: 0.00001980
Iteration 139/1000 | Loss: 0.00001980
Iteration 140/1000 | Loss: 0.00001980
Iteration 141/1000 | Loss: 0.00001980
Iteration 142/1000 | Loss: 0.00001980
Iteration 143/1000 | Loss: 0.00001980
Iteration 144/1000 | Loss: 0.00001979
Iteration 145/1000 | Loss: 0.00001979
Iteration 146/1000 | Loss: 0.00001979
Iteration 147/1000 | Loss: 0.00001979
Iteration 148/1000 | Loss: 0.00001979
Iteration 149/1000 | Loss: 0.00001979
Iteration 150/1000 | Loss: 0.00001979
Iteration 151/1000 | Loss: 0.00001979
Iteration 152/1000 | Loss: 0.00001979
Iteration 153/1000 | Loss: 0.00001979
Iteration 154/1000 | Loss: 0.00001979
Iteration 155/1000 | Loss: 0.00001979
Iteration 156/1000 | Loss: 0.00001979
Iteration 157/1000 | Loss: 0.00001979
Iteration 158/1000 | Loss: 0.00001979
Iteration 159/1000 | Loss: 0.00001979
Iteration 160/1000 | Loss: 0.00001979
Iteration 161/1000 | Loss: 0.00001979
Iteration 162/1000 | Loss: 0.00001979
Iteration 163/1000 | Loss: 0.00001979
Iteration 164/1000 | Loss: 0.00001979
Iteration 165/1000 | Loss: 0.00001979
Iteration 166/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.979107219085563e-05, 1.979107219085563e-05, 1.979107219085563e-05, 1.979107219085563e-05, 1.979107219085563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.979107219085563e-05

Optimization complete. Final v2v error: 3.0061824321746826 mm

Highest mean error: 20.650909423828125 mm for frame 111

Lowest mean error: 2.5339815616607666 mm for frame 13

Saving results

Total time: 102.99893760681152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824736
Iteration 2/25 | Loss: 0.00135893
Iteration 3/25 | Loss: 0.00116768
Iteration 4/25 | Loss: 0.00113425
Iteration 5/25 | Loss: 0.00112783
Iteration 6/25 | Loss: 0.00112610
Iteration 7/25 | Loss: 0.00112608
Iteration 8/25 | Loss: 0.00112608
Iteration 9/25 | Loss: 0.00112608
Iteration 10/25 | Loss: 0.00112606
Iteration 11/25 | Loss: 0.00112606
Iteration 12/25 | Loss: 0.00112606
Iteration 13/25 | Loss: 0.00112606
Iteration 14/25 | Loss: 0.00112606
Iteration 15/25 | Loss: 0.00112606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011260564206168056, 0.0011260564206168056, 0.0011260564206168056, 0.0011260564206168056, 0.0011260564206168056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011260564206168056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62811810
Iteration 2/25 | Loss: 0.00094444
Iteration 3/25 | Loss: 0.00094439
Iteration 4/25 | Loss: 0.00094439
Iteration 5/25 | Loss: 0.00094439
Iteration 6/25 | Loss: 0.00094439
Iteration 7/25 | Loss: 0.00094439
Iteration 8/25 | Loss: 0.00094439
Iteration 9/25 | Loss: 0.00094439
Iteration 10/25 | Loss: 0.00094439
Iteration 11/25 | Loss: 0.00094439
Iteration 12/25 | Loss: 0.00094439
Iteration 13/25 | Loss: 0.00094439
Iteration 14/25 | Loss: 0.00094439
Iteration 15/25 | Loss: 0.00094439
Iteration 16/25 | Loss: 0.00094439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00094438815722242, 0.00094438815722242, 0.00094438815722242, 0.00094438815722242, 0.00094438815722242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00094438815722242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094439
Iteration 2/1000 | Loss: 0.00007632
Iteration 3/1000 | Loss: 0.00004910
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003273
Iteration 6/1000 | Loss: 0.00003108
Iteration 7/1000 | Loss: 0.00002963
Iteration 8/1000 | Loss: 0.00002861
Iteration 9/1000 | Loss: 0.00002777
Iteration 10/1000 | Loss: 0.00002721
Iteration 11/1000 | Loss: 0.00002679
Iteration 12/1000 | Loss: 0.00002647
Iteration 13/1000 | Loss: 0.00002619
Iteration 14/1000 | Loss: 0.00002596
Iteration 15/1000 | Loss: 0.00002594
Iteration 16/1000 | Loss: 0.00002577
Iteration 17/1000 | Loss: 0.00002561
Iteration 18/1000 | Loss: 0.00002554
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00002544
Iteration 21/1000 | Loss: 0.00002543
Iteration 22/1000 | Loss: 0.00002542
Iteration 23/1000 | Loss: 0.00002541
Iteration 24/1000 | Loss: 0.00002536
Iteration 25/1000 | Loss: 0.00002535
Iteration 26/1000 | Loss: 0.00002534
Iteration 27/1000 | Loss: 0.00002534
Iteration 28/1000 | Loss: 0.00002528
Iteration 29/1000 | Loss: 0.00002526
Iteration 30/1000 | Loss: 0.00002525
Iteration 31/1000 | Loss: 0.00002524
Iteration 32/1000 | Loss: 0.00002523
Iteration 33/1000 | Loss: 0.00002523
Iteration 34/1000 | Loss: 0.00002523
Iteration 35/1000 | Loss: 0.00002523
Iteration 36/1000 | Loss: 0.00002523
Iteration 37/1000 | Loss: 0.00002522
Iteration 38/1000 | Loss: 0.00002522
Iteration 39/1000 | Loss: 0.00002522
Iteration 40/1000 | Loss: 0.00002522
Iteration 41/1000 | Loss: 0.00002522
Iteration 42/1000 | Loss: 0.00002521
Iteration 43/1000 | Loss: 0.00002520
Iteration 44/1000 | Loss: 0.00002519
Iteration 45/1000 | Loss: 0.00002518
Iteration 46/1000 | Loss: 0.00002517
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00002517
Iteration 49/1000 | Loss: 0.00002517
Iteration 50/1000 | Loss: 0.00002517
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002517
Iteration 54/1000 | Loss: 0.00002517
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00002516
Iteration 57/1000 | Loss: 0.00002515
Iteration 58/1000 | Loss: 0.00002515
Iteration 59/1000 | Loss: 0.00002515
Iteration 60/1000 | Loss: 0.00002514
Iteration 61/1000 | Loss: 0.00002514
Iteration 62/1000 | Loss: 0.00002514
Iteration 63/1000 | Loss: 0.00002513
Iteration 64/1000 | Loss: 0.00002513
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002512
Iteration 68/1000 | Loss: 0.00002512
Iteration 69/1000 | Loss: 0.00002512
Iteration 70/1000 | Loss: 0.00002512
Iteration 71/1000 | Loss: 0.00002512
Iteration 72/1000 | Loss: 0.00002511
Iteration 73/1000 | Loss: 0.00002511
Iteration 74/1000 | Loss: 0.00002511
Iteration 75/1000 | Loss: 0.00002511
Iteration 76/1000 | Loss: 0.00002511
Iteration 77/1000 | Loss: 0.00002510
Iteration 78/1000 | Loss: 0.00002510
Iteration 79/1000 | Loss: 0.00002510
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002510
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002510
Iteration 85/1000 | Loss: 0.00002510
Iteration 86/1000 | Loss: 0.00002510
Iteration 87/1000 | Loss: 0.00002509
Iteration 88/1000 | Loss: 0.00002509
Iteration 89/1000 | Loss: 0.00002509
Iteration 90/1000 | Loss: 0.00002509
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002509
Iteration 94/1000 | Loss: 0.00002509
Iteration 95/1000 | Loss: 0.00002509
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002508
Iteration 99/1000 | Loss: 0.00002508
Iteration 100/1000 | Loss: 0.00002508
Iteration 101/1000 | Loss: 0.00002508
Iteration 102/1000 | Loss: 0.00002508
Iteration 103/1000 | Loss: 0.00002508
Iteration 104/1000 | Loss: 0.00002508
Iteration 105/1000 | Loss: 0.00002508
Iteration 106/1000 | Loss: 0.00002508
Iteration 107/1000 | Loss: 0.00002508
Iteration 108/1000 | Loss: 0.00002508
Iteration 109/1000 | Loss: 0.00002508
Iteration 110/1000 | Loss: 0.00002507
Iteration 111/1000 | Loss: 0.00002507
Iteration 112/1000 | Loss: 0.00002507
Iteration 113/1000 | Loss: 0.00002507
Iteration 114/1000 | Loss: 0.00002507
Iteration 115/1000 | Loss: 0.00002507
Iteration 116/1000 | Loss: 0.00002507
Iteration 117/1000 | Loss: 0.00002507
Iteration 118/1000 | Loss: 0.00002507
Iteration 119/1000 | Loss: 0.00002507
Iteration 120/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.507474710000679e-05, 2.507474710000679e-05, 2.507474710000679e-05, 2.507474710000679e-05, 2.507474710000679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.507474710000679e-05

Optimization complete. Final v2v error: 4.047430992126465 mm

Highest mean error: 5.530603408813477 mm for frame 163

Lowest mean error: 3.1187381744384766 mm for frame 78

Saving results

Total time: 41.37893843650818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968855
Iteration 2/25 | Loss: 0.00229712
Iteration 3/25 | Loss: 0.00131366
Iteration 4/25 | Loss: 0.00115441
Iteration 5/25 | Loss: 0.00113246
Iteration 6/25 | Loss: 0.00109458
Iteration 7/25 | Loss: 0.00108544
Iteration 8/25 | Loss: 0.00107002
Iteration 9/25 | Loss: 0.00107091
Iteration 10/25 | Loss: 0.00106316
Iteration 11/25 | Loss: 0.00106163
Iteration 12/25 | Loss: 0.00106353
Iteration 13/25 | Loss: 0.00106302
Iteration 14/25 | Loss: 0.00105698
Iteration 15/25 | Loss: 0.00105706
Iteration 16/25 | Loss: 0.00105530
Iteration 17/25 | Loss: 0.00105442
Iteration 18/25 | Loss: 0.00105433
Iteration 19/25 | Loss: 0.00105433
Iteration 20/25 | Loss: 0.00105433
Iteration 21/25 | Loss: 0.00105432
Iteration 22/25 | Loss: 0.00105432
Iteration 23/25 | Loss: 0.00105432
Iteration 24/25 | Loss: 0.00105432
Iteration 25/25 | Loss: 0.00105432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87456083
Iteration 2/25 | Loss: 0.00090775
Iteration 3/25 | Loss: 0.00088774
Iteration 4/25 | Loss: 0.00088774
Iteration 5/25 | Loss: 0.00088774
Iteration 6/25 | Loss: 0.00088774
Iteration 7/25 | Loss: 0.00088774
Iteration 8/25 | Loss: 0.00088774
Iteration 9/25 | Loss: 0.00088774
Iteration 10/25 | Loss: 0.00088774
Iteration 11/25 | Loss: 0.00088774
Iteration 12/25 | Loss: 0.00088774
Iteration 13/25 | Loss: 0.00088774
Iteration 14/25 | Loss: 0.00088774
Iteration 15/25 | Loss: 0.00088774
Iteration 16/25 | Loss: 0.00088774
Iteration 17/25 | Loss: 0.00088774
Iteration 18/25 | Loss: 0.00088774
Iteration 19/25 | Loss: 0.00088774
Iteration 20/25 | Loss: 0.00088774
Iteration 21/25 | Loss: 0.00088774
Iteration 22/25 | Loss: 0.00088774
Iteration 23/25 | Loss: 0.00088774
Iteration 24/25 | Loss: 0.00088774
Iteration 25/25 | Loss: 0.00088774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088774
Iteration 2/1000 | Loss: 0.00005805
Iteration 3/1000 | Loss: 0.00002322
Iteration 4/1000 | Loss: 0.00017883
Iteration 5/1000 | Loss: 0.00005722
Iteration 6/1000 | Loss: 0.00031847
Iteration 7/1000 | Loss: 0.00016702
Iteration 8/1000 | Loss: 0.00010854
Iteration 9/1000 | Loss: 0.00001840
Iteration 10/1000 | Loss: 0.00006502
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00004273
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00005509
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001553
Iteration 22/1000 | Loss: 0.00001549
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00001541
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001532
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001531
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001525
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001522
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001521
Iteration 58/1000 | Loss: 0.00001521
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001520
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001516
Iteration 92/1000 | Loss: 0.00001516
Iteration 93/1000 | Loss: 0.00001516
Iteration 94/1000 | Loss: 0.00001516
Iteration 95/1000 | Loss: 0.00001516
Iteration 96/1000 | Loss: 0.00001516
Iteration 97/1000 | Loss: 0.00001515
Iteration 98/1000 | Loss: 0.00001515
Iteration 99/1000 | Loss: 0.00001515
Iteration 100/1000 | Loss: 0.00001515
Iteration 101/1000 | Loss: 0.00001515
Iteration 102/1000 | Loss: 0.00001515
Iteration 103/1000 | Loss: 0.00001515
Iteration 104/1000 | Loss: 0.00001515
Iteration 105/1000 | Loss: 0.00001514
Iteration 106/1000 | Loss: 0.00001514
Iteration 107/1000 | Loss: 0.00001514
Iteration 108/1000 | Loss: 0.00001514
Iteration 109/1000 | Loss: 0.00001514
Iteration 110/1000 | Loss: 0.00001514
Iteration 111/1000 | Loss: 0.00001514
Iteration 112/1000 | Loss: 0.00001514
Iteration 113/1000 | Loss: 0.00001514
Iteration 114/1000 | Loss: 0.00001514
Iteration 115/1000 | Loss: 0.00001514
Iteration 116/1000 | Loss: 0.00001514
Iteration 117/1000 | Loss: 0.00001514
Iteration 118/1000 | Loss: 0.00001514
Iteration 119/1000 | Loss: 0.00001513
Iteration 120/1000 | Loss: 0.00001513
Iteration 121/1000 | Loss: 0.00001513
Iteration 122/1000 | Loss: 0.00001513
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001513
Iteration 126/1000 | Loss: 0.00001513
Iteration 127/1000 | Loss: 0.00001513
Iteration 128/1000 | Loss: 0.00001513
Iteration 129/1000 | Loss: 0.00001513
Iteration 130/1000 | Loss: 0.00003425
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001513
Iteration 133/1000 | Loss: 0.00001511
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001511
Iteration 145/1000 | Loss: 0.00001511
Iteration 146/1000 | Loss: 0.00001511
Iteration 147/1000 | Loss: 0.00001511
Iteration 148/1000 | Loss: 0.00001511
Iteration 149/1000 | Loss: 0.00001511
Iteration 150/1000 | Loss: 0.00001511
Iteration 151/1000 | Loss: 0.00001511
Iteration 152/1000 | Loss: 0.00001511
Iteration 153/1000 | Loss: 0.00001511
Iteration 154/1000 | Loss: 0.00001511
Iteration 155/1000 | Loss: 0.00001511
Iteration 156/1000 | Loss: 0.00001511
Iteration 157/1000 | Loss: 0.00001511
Iteration 158/1000 | Loss: 0.00001511
Iteration 159/1000 | Loss: 0.00001511
Iteration 160/1000 | Loss: 0.00001511
Iteration 161/1000 | Loss: 0.00001511
Iteration 162/1000 | Loss: 0.00001511
Iteration 163/1000 | Loss: 0.00001511
Iteration 164/1000 | Loss: 0.00001511
Iteration 165/1000 | Loss: 0.00001511
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001511
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001511
Iteration 177/1000 | Loss: 0.00001511
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001511
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001511
Iteration 182/1000 | Loss: 0.00001511
Iteration 183/1000 | Loss: 0.00001511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.5105139027582482e-05, 1.5105139027582482e-05, 1.5105139027582482e-05, 1.5105139027582482e-05, 1.5105139027582482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5105139027582482e-05

Optimization complete. Final v2v error: 3.3568711280822754 mm

Highest mean error: 3.9009032249450684 mm for frame 73

Lowest mean error: 2.9532463550567627 mm for frame 105

Saving results

Total time: 77.9403007030487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062759
Iteration 2/25 | Loss: 0.00277780
Iteration 3/25 | Loss: 0.00178580
Iteration 4/25 | Loss: 0.00159665
Iteration 5/25 | Loss: 0.00162322
Iteration 6/25 | Loss: 0.00152721
Iteration 7/25 | Loss: 0.00148665
Iteration 8/25 | Loss: 0.00147706
Iteration 9/25 | Loss: 0.00146172
Iteration 10/25 | Loss: 0.00145536
Iteration 11/25 | Loss: 0.00144596
Iteration 12/25 | Loss: 0.00144254
Iteration 13/25 | Loss: 0.00144077
Iteration 14/25 | Loss: 0.00143756
Iteration 15/25 | Loss: 0.00143589
Iteration 16/25 | Loss: 0.00143519
Iteration 17/25 | Loss: 0.00143771
Iteration 18/25 | Loss: 0.00143686
Iteration 19/25 | Loss: 0.00143596
Iteration 20/25 | Loss: 0.00143202
Iteration 21/25 | Loss: 0.00143155
Iteration 22/25 | Loss: 0.00143141
Iteration 23/25 | Loss: 0.00143133
Iteration 24/25 | Loss: 0.00143123
Iteration 25/25 | Loss: 0.00143110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17784905
Iteration 2/25 | Loss: 0.00401696
Iteration 3/25 | Loss: 0.00401696
Iteration 4/25 | Loss: 0.00401696
Iteration 5/25 | Loss: 0.00401696
Iteration 6/25 | Loss: 0.00401696
Iteration 7/25 | Loss: 0.00401696
Iteration 8/25 | Loss: 0.00401696
Iteration 9/25 | Loss: 0.00401696
Iteration 10/25 | Loss: 0.00401696
Iteration 11/25 | Loss: 0.00401696
Iteration 12/25 | Loss: 0.00401696
Iteration 13/25 | Loss: 0.00401696
Iteration 14/25 | Loss: 0.00401696
Iteration 15/25 | Loss: 0.00401696
Iteration 16/25 | Loss: 0.00401696
Iteration 17/25 | Loss: 0.00401696
Iteration 18/25 | Loss: 0.00401696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004016959108412266, 0.004016959108412266, 0.004016959108412266, 0.004016959108412266, 0.004016959108412266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004016959108412266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00401696
Iteration 2/1000 | Loss: 0.00334798
Iteration 3/1000 | Loss: 0.00345637
Iteration 4/1000 | Loss: 0.00469436
Iteration 5/1000 | Loss: 0.00148121
Iteration 6/1000 | Loss: 0.00117289
Iteration 7/1000 | Loss: 0.00140177
Iteration 8/1000 | Loss: 0.00157141
Iteration 9/1000 | Loss: 0.00179543
Iteration 10/1000 | Loss: 0.00054316
Iteration 11/1000 | Loss: 0.00166182
Iteration 12/1000 | Loss: 0.00071581
Iteration 13/1000 | Loss: 0.00088032
Iteration 14/1000 | Loss: 0.00066220
Iteration 15/1000 | Loss: 0.00053344
Iteration 16/1000 | Loss: 0.00070586
Iteration 17/1000 | Loss: 0.00049815
Iteration 18/1000 | Loss: 0.00193422
Iteration 19/1000 | Loss: 0.00014079
Iteration 20/1000 | Loss: 0.00056258
Iteration 21/1000 | Loss: 0.00022901
Iteration 22/1000 | Loss: 0.00035312
Iteration 23/1000 | Loss: 0.00118757
Iteration 24/1000 | Loss: 0.00099988
Iteration 25/1000 | Loss: 0.00067395
Iteration 26/1000 | Loss: 0.00065314
Iteration 27/1000 | Loss: 0.00107364
Iteration 28/1000 | Loss: 0.00141591
Iteration 29/1000 | Loss: 0.00185826
Iteration 30/1000 | Loss: 0.00197577
Iteration 31/1000 | Loss: 0.00133284
Iteration 32/1000 | Loss: 0.00196296
Iteration 33/1000 | Loss: 0.00148977
Iteration 34/1000 | Loss: 0.00131837
Iteration 35/1000 | Loss: 0.00130140
Iteration 36/1000 | Loss: 0.00133325
Iteration 37/1000 | Loss: 0.00119981
Iteration 38/1000 | Loss: 0.00095769
Iteration 39/1000 | Loss: 0.00093217
Iteration 40/1000 | Loss: 0.00096399
Iteration 41/1000 | Loss: 0.00166321
Iteration 42/1000 | Loss: 0.00130248
Iteration 43/1000 | Loss: 0.00106247
Iteration 44/1000 | Loss: 0.00171099
Iteration 45/1000 | Loss: 0.00092461
Iteration 46/1000 | Loss: 0.00063849
Iteration 47/1000 | Loss: 0.00070772
Iteration 48/1000 | Loss: 0.00049119
Iteration 49/1000 | Loss: 0.00039569
Iteration 50/1000 | Loss: 0.00080411
Iteration 51/1000 | Loss: 0.00212695
Iteration 52/1000 | Loss: 0.00086832
Iteration 53/1000 | Loss: 0.00255390
Iteration 54/1000 | Loss: 0.00204817
Iteration 55/1000 | Loss: 0.00194683
Iteration 56/1000 | Loss: 0.00112869
Iteration 57/1000 | Loss: 0.00099723
Iteration 58/1000 | Loss: 0.00076810
Iteration 59/1000 | Loss: 0.00034658
Iteration 60/1000 | Loss: 0.00028188
Iteration 61/1000 | Loss: 0.00068493
Iteration 62/1000 | Loss: 0.00061366
Iteration 63/1000 | Loss: 0.00035524
Iteration 64/1000 | Loss: 0.00045977
Iteration 65/1000 | Loss: 0.00037607
Iteration 66/1000 | Loss: 0.00067101
Iteration 67/1000 | Loss: 0.00073826
Iteration 68/1000 | Loss: 0.00055374
Iteration 69/1000 | Loss: 0.00061249
Iteration 70/1000 | Loss: 0.00032420
Iteration 71/1000 | Loss: 0.00072089
Iteration 72/1000 | Loss: 0.00060926
Iteration 73/1000 | Loss: 0.00045322
Iteration 74/1000 | Loss: 0.00037455
Iteration 75/1000 | Loss: 0.00010869
Iteration 76/1000 | Loss: 0.00039412
Iteration 77/1000 | Loss: 0.00038227
Iteration 78/1000 | Loss: 0.00031179
Iteration 79/1000 | Loss: 0.00030651
Iteration 80/1000 | Loss: 0.00018976
Iteration 81/1000 | Loss: 0.00022011
Iteration 82/1000 | Loss: 0.00024528
Iteration 83/1000 | Loss: 0.00033717
Iteration 84/1000 | Loss: 0.00062237
Iteration 85/1000 | Loss: 0.00079424
Iteration 86/1000 | Loss: 0.00043019
Iteration 87/1000 | Loss: 0.00046343
Iteration 88/1000 | Loss: 0.00041424
Iteration 89/1000 | Loss: 0.00043152
Iteration 90/1000 | Loss: 0.00047971
Iteration 91/1000 | Loss: 0.00036278
Iteration 92/1000 | Loss: 0.00010112
Iteration 93/1000 | Loss: 0.00007380
Iteration 94/1000 | Loss: 0.00052848
Iteration 95/1000 | Loss: 0.00008977
Iteration 96/1000 | Loss: 0.00016190
Iteration 97/1000 | Loss: 0.00006506
Iteration 98/1000 | Loss: 0.00005868
Iteration 99/1000 | Loss: 0.00047787
Iteration 100/1000 | Loss: 0.00018721
Iteration 101/1000 | Loss: 0.00044319
Iteration 102/1000 | Loss: 0.00043139
Iteration 103/1000 | Loss: 0.00044073
Iteration 104/1000 | Loss: 0.00006385
Iteration 105/1000 | Loss: 0.00052925
Iteration 106/1000 | Loss: 0.00045656
Iteration 107/1000 | Loss: 0.00048422
Iteration 108/1000 | Loss: 0.00045672
Iteration 109/1000 | Loss: 0.00052406
Iteration 110/1000 | Loss: 0.00058377
Iteration 111/1000 | Loss: 0.00103349
Iteration 112/1000 | Loss: 0.00082533
Iteration 113/1000 | Loss: 0.00018663
Iteration 114/1000 | Loss: 0.00005622
Iteration 115/1000 | Loss: 0.00081881
Iteration 116/1000 | Loss: 0.00057822
Iteration 117/1000 | Loss: 0.00044541
Iteration 118/1000 | Loss: 0.00043103
Iteration 119/1000 | Loss: 0.00005604
Iteration 120/1000 | Loss: 0.00004925
Iteration 121/1000 | Loss: 0.00004608
Iteration 122/1000 | Loss: 0.00004396
Iteration 123/1000 | Loss: 0.00004270
Iteration 124/1000 | Loss: 0.00004133
Iteration 125/1000 | Loss: 0.00005031
Iteration 126/1000 | Loss: 0.00004201
Iteration 127/1000 | Loss: 0.00003897
Iteration 128/1000 | Loss: 0.00003772
Iteration 129/1000 | Loss: 0.00003700
Iteration 130/1000 | Loss: 0.00003648
Iteration 131/1000 | Loss: 0.00003612
Iteration 132/1000 | Loss: 0.00003585
Iteration 133/1000 | Loss: 0.00003545
Iteration 134/1000 | Loss: 0.00003513
Iteration 135/1000 | Loss: 0.00003489
Iteration 136/1000 | Loss: 0.00003472
Iteration 137/1000 | Loss: 0.00003468
Iteration 138/1000 | Loss: 0.00003466
Iteration 139/1000 | Loss: 0.00003465
Iteration 140/1000 | Loss: 0.00003465
Iteration 141/1000 | Loss: 0.00003464
Iteration 142/1000 | Loss: 0.00003464
Iteration 143/1000 | Loss: 0.00003459
Iteration 144/1000 | Loss: 0.00003457
Iteration 145/1000 | Loss: 0.00003456
Iteration 146/1000 | Loss: 0.00003454
Iteration 147/1000 | Loss: 0.00003454
Iteration 148/1000 | Loss: 0.00003454
Iteration 149/1000 | Loss: 0.00003453
Iteration 150/1000 | Loss: 0.00003453
Iteration 151/1000 | Loss: 0.00003453
Iteration 152/1000 | Loss: 0.00003453
Iteration 153/1000 | Loss: 0.00003453
Iteration 154/1000 | Loss: 0.00003452
Iteration 155/1000 | Loss: 0.00003449
Iteration 156/1000 | Loss: 0.00003440
Iteration 157/1000 | Loss: 0.00003440
Iteration 158/1000 | Loss: 0.00003440
Iteration 159/1000 | Loss: 0.00003440
Iteration 160/1000 | Loss: 0.00003439
Iteration 161/1000 | Loss: 0.00003439
Iteration 162/1000 | Loss: 0.00003438
Iteration 163/1000 | Loss: 0.00003438
Iteration 164/1000 | Loss: 0.00003437
Iteration 165/1000 | Loss: 0.00003437
Iteration 166/1000 | Loss: 0.00003437
Iteration 167/1000 | Loss: 0.00003437
Iteration 168/1000 | Loss: 0.00003437
Iteration 169/1000 | Loss: 0.00003436
Iteration 170/1000 | Loss: 0.00003436
Iteration 171/1000 | Loss: 0.00003436
Iteration 172/1000 | Loss: 0.00003436
Iteration 173/1000 | Loss: 0.00003436
Iteration 174/1000 | Loss: 0.00003436
Iteration 175/1000 | Loss: 0.00003436
Iteration 176/1000 | Loss: 0.00003436
Iteration 177/1000 | Loss: 0.00003436
Iteration 178/1000 | Loss: 0.00003436
Iteration 179/1000 | Loss: 0.00003436
Iteration 180/1000 | Loss: 0.00003436
Iteration 181/1000 | Loss: 0.00003435
Iteration 182/1000 | Loss: 0.00003435
Iteration 183/1000 | Loss: 0.00003435
Iteration 184/1000 | Loss: 0.00003435
Iteration 185/1000 | Loss: 0.00003435
Iteration 186/1000 | Loss: 0.00003434
Iteration 187/1000 | Loss: 0.00003434
Iteration 188/1000 | Loss: 0.00003434
Iteration 189/1000 | Loss: 0.00003433
Iteration 190/1000 | Loss: 0.00003433
Iteration 191/1000 | Loss: 0.00003433
Iteration 192/1000 | Loss: 0.00003433
Iteration 193/1000 | Loss: 0.00003432
Iteration 194/1000 | Loss: 0.00003432
Iteration 195/1000 | Loss: 0.00003432
Iteration 196/1000 | Loss: 0.00003432
Iteration 197/1000 | Loss: 0.00003431
Iteration 198/1000 | Loss: 0.00003431
Iteration 199/1000 | Loss: 0.00003431
Iteration 200/1000 | Loss: 0.00003430
Iteration 201/1000 | Loss: 0.00003430
Iteration 202/1000 | Loss: 0.00003429
Iteration 203/1000 | Loss: 0.00003429
Iteration 204/1000 | Loss: 0.00003429
Iteration 205/1000 | Loss: 0.00003429
Iteration 206/1000 | Loss: 0.00003429
Iteration 207/1000 | Loss: 0.00003428
Iteration 208/1000 | Loss: 0.00003428
Iteration 209/1000 | Loss: 0.00003428
Iteration 210/1000 | Loss: 0.00003428
Iteration 211/1000 | Loss: 0.00003428
Iteration 212/1000 | Loss: 0.00003427
Iteration 213/1000 | Loss: 0.00003427
Iteration 214/1000 | Loss: 0.00003427
Iteration 215/1000 | Loss: 0.00003427
Iteration 216/1000 | Loss: 0.00003427
Iteration 217/1000 | Loss: 0.00003427
Iteration 218/1000 | Loss: 0.00003427
Iteration 219/1000 | Loss: 0.00003427
Iteration 220/1000 | Loss: 0.00003427
Iteration 221/1000 | Loss: 0.00003426
Iteration 222/1000 | Loss: 0.00003426
Iteration 223/1000 | Loss: 0.00003426
Iteration 224/1000 | Loss: 0.00003426
Iteration 225/1000 | Loss: 0.00003426
Iteration 226/1000 | Loss: 0.00003426
Iteration 227/1000 | Loss: 0.00003425
Iteration 228/1000 | Loss: 0.00003425
Iteration 229/1000 | Loss: 0.00003425
Iteration 230/1000 | Loss: 0.00003425
Iteration 231/1000 | Loss: 0.00003424
Iteration 232/1000 | Loss: 0.00003424
Iteration 233/1000 | Loss: 0.00003424
Iteration 234/1000 | Loss: 0.00003423
Iteration 235/1000 | Loss: 0.00003423
Iteration 236/1000 | Loss: 0.00003423
Iteration 237/1000 | Loss: 0.00003423
Iteration 238/1000 | Loss: 0.00003423
Iteration 239/1000 | Loss: 0.00003423
Iteration 240/1000 | Loss: 0.00003423
Iteration 241/1000 | Loss: 0.00003423
Iteration 242/1000 | Loss: 0.00003422
Iteration 243/1000 | Loss: 0.00003422
Iteration 244/1000 | Loss: 0.00003422
Iteration 245/1000 | Loss: 0.00003422
Iteration 246/1000 | Loss: 0.00003422
Iteration 247/1000 | Loss: 0.00003422
Iteration 248/1000 | Loss: 0.00003422
Iteration 249/1000 | Loss: 0.00003422
Iteration 250/1000 | Loss: 0.00003422
Iteration 251/1000 | Loss: 0.00003422
Iteration 252/1000 | Loss: 0.00003422
Iteration 253/1000 | Loss: 0.00003421
Iteration 254/1000 | Loss: 0.00003421
Iteration 255/1000 | Loss: 0.00003421
Iteration 256/1000 | Loss: 0.00003421
Iteration 257/1000 | Loss: 0.00003421
Iteration 258/1000 | Loss: 0.00003420
Iteration 259/1000 | Loss: 0.00003420
Iteration 260/1000 | Loss: 0.00003420
Iteration 261/1000 | Loss: 0.00003420
Iteration 262/1000 | Loss: 0.00003420
Iteration 263/1000 | Loss: 0.00003420
Iteration 264/1000 | Loss: 0.00003420
Iteration 265/1000 | Loss: 0.00003419
Iteration 266/1000 | Loss: 0.00003419
Iteration 267/1000 | Loss: 0.00003419
Iteration 268/1000 | Loss: 0.00003419
Iteration 269/1000 | Loss: 0.00003419
Iteration 270/1000 | Loss: 0.00003418
Iteration 271/1000 | Loss: 0.00003418
Iteration 272/1000 | Loss: 0.00003418
Iteration 273/1000 | Loss: 0.00003418
Iteration 274/1000 | Loss: 0.00003418
Iteration 275/1000 | Loss: 0.00003418
Iteration 276/1000 | Loss: 0.00003417
Iteration 277/1000 | Loss: 0.00003417
Iteration 278/1000 | Loss: 0.00003417
Iteration 279/1000 | Loss: 0.00003417
Iteration 280/1000 | Loss: 0.00003417
Iteration 281/1000 | Loss: 0.00003417
Iteration 282/1000 | Loss: 0.00003417
Iteration 283/1000 | Loss: 0.00003417
Iteration 284/1000 | Loss: 0.00003417
Iteration 285/1000 | Loss: 0.00003417
Iteration 286/1000 | Loss: 0.00003416
Iteration 287/1000 | Loss: 0.00003416
Iteration 288/1000 | Loss: 0.00003416
Iteration 289/1000 | Loss: 0.00003416
Iteration 290/1000 | Loss: 0.00003416
Iteration 291/1000 | Loss: 0.00003416
Iteration 292/1000 | Loss: 0.00003416
Iteration 293/1000 | Loss: 0.00003416
Iteration 294/1000 | Loss: 0.00003416
Iteration 295/1000 | Loss: 0.00003416
Iteration 296/1000 | Loss: 0.00003416
Iteration 297/1000 | Loss: 0.00003416
Iteration 298/1000 | Loss: 0.00003416
Iteration 299/1000 | Loss: 0.00003416
Iteration 300/1000 | Loss: 0.00003416
Iteration 301/1000 | Loss: 0.00003416
Iteration 302/1000 | Loss: 0.00003416
Iteration 303/1000 | Loss: 0.00003416
Iteration 304/1000 | Loss: 0.00003416
Iteration 305/1000 | Loss: 0.00003416
Iteration 306/1000 | Loss: 0.00003416
Iteration 307/1000 | Loss: 0.00003416
Iteration 308/1000 | Loss: 0.00003416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [3.416382605792023e-05, 3.416382605792023e-05, 3.416382605792023e-05, 3.416382605792023e-05, 3.416382605792023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.416382605792023e-05

Optimization complete. Final v2v error: 3.7682483196258545 mm

Highest mean error: 11.934484481811523 mm for frame 64

Lowest mean error: 2.68967866897583 mm for frame 139

Saving results

Total time: 239.68945407867432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424182
Iteration 2/25 | Loss: 0.00113968
Iteration 3/25 | Loss: 0.00101906
Iteration 4/25 | Loss: 0.00100367
Iteration 5/25 | Loss: 0.00099919
Iteration 6/25 | Loss: 0.00099806
Iteration 7/25 | Loss: 0.00099806
Iteration 8/25 | Loss: 0.00099806
Iteration 9/25 | Loss: 0.00099806
Iteration 10/25 | Loss: 0.00099806
Iteration 11/25 | Loss: 0.00099806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009980571921914816, 0.0009980571921914816, 0.0009980571921914816, 0.0009980571921914816, 0.0009980571921914816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009980571921914816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35790575
Iteration 2/25 | Loss: 0.00091449
Iteration 3/25 | Loss: 0.00091449
Iteration 4/25 | Loss: 0.00091449
Iteration 5/25 | Loss: 0.00091449
Iteration 6/25 | Loss: 0.00091449
Iteration 7/25 | Loss: 0.00091449
Iteration 8/25 | Loss: 0.00091449
Iteration 9/25 | Loss: 0.00091449
Iteration 10/25 | Loss: 0.00091449
Iteration 11/25 | Loss: 0.00091449
Iteration 12/25 | Loss: 0.00091449
Iteration 13/25 | Loss: 0.00091449
Iteration 14/25 | Loss: 0.00091449
Iteration 15/25 | Loss: 0.00091449
Iteration 16/25 | Loss: 0.00091449
Iteration 17/25 | Loss: 0.00091449
Iteration 18/25 | Loss: 0.00091449
Iteration 19/25 | Loss: 0.00091449
Iteration 20/25 | Loss: 0.00091449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009144862997345626, 0.0009144862997345626, 0.0009144862997345626, 0.0009144862997345626, 0.0009144862997345626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009144862997345626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091449
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001598
Iteration 4/1000 | Loss: 0.00001422
Iteration 5/1000 | Loss: 0.00001314
Iteration 6/1000 | Loss: 0.00001251
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001148
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001119
Iteration 12/1000 | Loss: 0.00001113
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001099
Iteration 15/1000 | Loss: 0.00001099
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001096
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001092
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001091
Iteration 27/1000 | Loss: 0.00001091
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001091
Iteration 31/1000 | Loss: 0.00001091
Iteration 32/1000 | Loss: 0.00001091
Iteration 33/1000 | Loss: 0.00001091
Iteration 34/1000 | Loss: 0.00001090
Iteration 35/1000 | Loss: 0.00001090
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001088
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001086
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001085
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001081
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001081
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001078
Iteration 104/1000 | Loss: 0.00001078
Iteration 105/1000 | Loss: 0.00001078
Iteration 106/1000 | Loss: 0.00001078
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001074
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001073
Iteration 140/1000 | Loss: 0.00001073
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001072
Iteration 150/1000 | Loss: 0.00001072
Iteration 151/1000 | Loss: 0.00001072
Iteration 152/1000 | Loss: 0.00001072
Iteration 153/1000 | Loss: 0.00001071
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001071
Iteration 156/1000 | Loss: 0.00001071
Iteration 157/1000 | Loss: 0.00001071
Iteration 158/1000 | Loss: 0.00001071
Iteration 159/1000 | Loss: 0.00001071
Iteration 160/1000 | Loss: 0.00001070
Iteration 161/1000 | Loss: 0.00001070
Iteration 162/1000 | Loss: 0.00001070
Iteration 163/1000 | Loss: 0.00001070
Iteration 164/1000 | Loss: 0.00001070
Iteration 165/1000 | Loss: 0.00001070
Iteration 166/1000 | Loss: 0.00001070
Iteration 167/1000 | Loss: 0.00001070
Iteration 168/1000 | Loss: 0.00001070
Iteration 169/1000 | Loss: 0.00001070
Iteration 170/1000 | Loss: 0.00001070
Iteration 171/1000 | Loss: 0.00001070
Iteration 172/1000 | Loss: 0.00001070
Iteration 173/1000 | Loss: 0.00001070
Iteration 174/1000 | Loss: 0.00001069
Iteration 175/1000 | Loss: 0.00001069
Iteration 176/1000 | Loss: 0.00001069
Iteration 177/1000 | Loss: 0.00001069
Iteration 178/1000 | Loss: 0.00001069
Iteration 179/1000 | Loss: 0.00001069
Iteration 180/1000 | Loss: 0.00001069
Iteration 181/1000 | Loss: 0.00001069
Iteration 182/1000 | Loss: 0.00001069
Iteration 183/1000 | Loss: 0.00001069
Iteration 184/1000 | Loss: 0.00001069
Iteration 185/1000 | Loss: 0.00001069
Iteration 186/1000 | Loss: 0.00001069
Iteration 187/1000 | Loss: 0.00001069
Iteration 188/1000 | Loss: 0.00001069
Iteration 189/1000 | Loss: 0.00001069
Iteration 190/1000 | Loss: 0.00001069
Iteration 191/1000 | Loss: 0.00001069
Iteration 192/1000 | Loss: 0.00001069
Iteration 193/1000 | Loss: 0.00001069
Iteration 194/1000 | Loss: 0.00001069
Iteration 195/1000 | Loss: 0.00001069
Iteration 196/1000 | Loss: 0.00001069
Iteration 197/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.0685818779165857e-05, 1.0685818779165857e-05, 1.0685818779165857e-05, 1.0685818779165857e-05, 1.0685818779165857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0685818779165857e-05

Optimization complete. Final v2v error: 2.7464489936828613 mm

Highest mean error: 3.2444469928741455 mm for frame 7

Lowest mean error: 2.3599436283111572 mm for frame 180

Saving results

Total time: 44.12426137924194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837648
Iteration 2/25 | Loss: 0.00107513
Iteration 3/25 | Loss: 0.00099534
Iteration 4/25 | Loss: 0.00098152
Iteration 5/25 | Loss: 0.00097660
Iteration 6/25 | Loss: 0.00097586
Iteration 7/25 | Loss: 0.00097586
Iteration 8/25 | Loss: 0.00097586
Iteration 9/25 | Loss: 0.00097586
Iteration 10/25 | Loss: 0.00097586
Iteration 11/25 | Loss: 0.00097586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009758597589097917, 0.0009758597589097917, 0.0009758597589097917, 0.0009758597589097917, 0.0009758597589097917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009758597589097917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.99941921
Iteration 2/25 | Loss: 0.00069439
Iteration 3/25 | Loss: 0.00069436
Iteration 4/25 | Loss: 0.00069436
Iteration 5/25 | Loss: 0.00069436
Iteration 6/25 | Loss: 0.00069436
Iteration 7/25 | Loss: 0.00069436
Iteration 8/25 | Loss: 0.00069436
Iteration 9/25 | Loss: 0.00069436
Iteration 10/25 | Loss: 0.00069436
Iteration 11/25 | Loss: 0.00069436
Iteration 12/25 | Loss: 0.00069436
Iteration 13/25 | Loss: 0.00069436
Iteration 14/25 | Loss: 0.00069436
Iteration 15/25 | Loss: 0.00069436
Iteration 16/25 | Loss: 0.00069436
Iteration 17/25 | Loss: 0.00069436
Iteration 18/25 | Loss: 0.00069436
Iteration 19/25 | Loss: 0.00069436
Iteration 20/25 | Loss: 0.00069436
Iteration 21/25 | Loss: 0.00069436
Iteration 22/25 | Loss: 0.00069436
Iteration 23/25 | Loss: 0.00069436
Iteration 24/25 | Loss: 0.00069436
Iteration 25/25 | Loss: 0.00069436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069436
Iteration 2/1000 | Loss: 0.00001777
Iteration 3/1000 | Loss: 0.00001309
Iteration 4/1000 | Loss: 0.00001216
Iteration 5/1000 | Loss: 0.00001170
Iteration 6/1000 | Loss: 0.00001128
Iteration 7/1000 | Loss: 0.00001098
Iteration 8/1000 | Loss: 0.00001097
Iteration 9/1000 | Loss: 0.00001072
Iteration 10/1000 | Loss: 0.00001054
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001038
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001037
Iteration 16/1000 | Loss: 0.00001037
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001026
Iteration 24/1000 | Loss: 0.00001026
Iteration 25/1000 | Loss: 0.00001020
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001020
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001020
Iteration 31/1000 | Loss: 0.00001020
Iteration 32/1000 | Loss: 0.00001020
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001020
Iteration 37/1000 | Loss: 0.00001020
Iteration 38/1000 | Loss: 0.00001020
Iteration 39/1000 | Loss: 0.00001020
Iteration 40/1000 | Loss: 0.00001019
Iteration 41/1000 | Loss: 0.00001017
Iteration 42/1000 | Loss: 0.00001017
Iteration 43/1000 | Loss: 0.00001017
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001015
Iteration 50/1000 | Loss: 0.00001015
Iteration 51/1000 | Loss: 0.00001015
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001015
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001014
Iteration 60/1000 | Loss: 0.00001013
Iteration 61/1000 | Loss: 0.00001013
Iteration 62/1000 | Loss: 0.00001012
Iteration 63/1000 | Loss: 0.00001012
Iteration 64/1000 | Loss: 0.00001011
Iteration 65/1000 | Loss: 0.00001011
Iteration 66/1000 | Loss: 0.00001010
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001008
Iteration 74/1000 | Loss: 0.00001008
Iteration 75/1000 | Loss: 0.00001008
Iteration 76/1000 | Loss: 0.00001008
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001007
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001007
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Iteration 87/1000 | Loss: 0.00001004
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001004
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001002
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001000
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000999
Iteration 106/1000 | Loss: 0.00000999
Iteration 107/1000 | Loss: 0.00000999
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000994
Iteration 124/1000 | Loss: 0.00000994
Iteration 125/1000 | Loss: 0.00000994
Iteration 126/1000 | Loss: 0.00000994
Iteration 127/1000 | Loss: 0.00000994
Iteration 128/1000 | Loss: 0.00000994
Iteration 129/1000 | Loss: 0.00000994
Iteration 130/1000 | Loss: 0.00000994
Iteration 131/1000 | Loss: 0.00000993
Iteration 132/1000 | Loss: 0.00000993
Iteration 133/1000 | Loss: 0.00000993
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000992
Iteration 137/1000 | Loss: 0.00000992
Iteration 138/1000 | Loss: 0.00000992
Iteration 139/1000 | Loss: 0.00000992
Iteration 140/1000 | Loss: 0.00000991
Iteration 141/1000 | Loss: 0.00000991
Iteration 142/1000 | Loss: 0.00000991
Iteration 143/1000 | Loss: 0.00000991
Iteration 144/1000 | Loss: 0.00000990
Iteration 145/1000 | Loss: 0.00000990
Iteration 146/1000 | Loss: 0.00000989
Iteration 147/1000 | Loss: 0.00000989
Iteration 148/1000 | Loss: 0.00000988
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Iteration 154/1000 | Loss: 0.00000987
Iteration 155/1000 | Loss: 0.00000987
Iteration 156/1000 | Loss: 0.00000987
Iteration 157/1000 | Loss: 0.00000987
Iteration 158/1000 | Loss: 0.00000986
Iteration 159/1000 | Loss: 0.00000986
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000985
Iteration 165/1000 | Loss: 0.00000985
Iteration 166/1000 | Loss: 0.00000985
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000984
Iteration 169/1000 | Loss: 0.00000984
Iteration 170/1000 | Loss: 0.00000984
Iteration 171/1000 | Loss: 0.00000983
Iteration 172/1000 | Loss: 0.00000983
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000983
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000983
Iteration 182/1000 | Loss: 0.00000983
Iteration 183/1000 | Loss: 0.00000983
Iteration 184/1000 | Loss: 0.00000983
Iteration 185/1000 | Loss: 0.00000983
Iteration 186/1000 | Loss: 0.00000983
Iteration 187/1000 | Loss: 0.00000982
Iteration 188/1000 | Loss: 0.00000982
Iteration 189/1000 | Loss: 0.00000982
Iteration 190/1000 | Loss: 0.00000982
Iteration 191/1000 | Loss: 0.00000982
Iteration 192/1000 | Loss: 0.00000982
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000982
Iteration 195/1000 | Loss: 0.00000982
Iteration 196/1000 | Loss: 0.00000982
Iteration 197/1000 | Loss: 0.00000982
Iteration 198/1000 | Loss: 0.00000982
Iteration 199/1000 | Loss: 0.00000982
Iteration 200/1000 | Loss: 0.00000982
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [9.81750781647861e-06, 9.81750781647861e-06, 9.81750781647861e-06, 9.81750781647861e-06, 9.81750781647861e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.81750781647861e-06

Optimization complete. Final v2v error: 2.7062361240386963 mm

Highest mean error: 3.2341318130493164 mm for frame 25

Lowest mean error: 2.482599973678589 mm for frame 182

Saving results

Total time: 39.637789726257324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348508
Iteration 2/25 | Loss: 0.00115448
Iteration 3/25 | Loss: 0.00101397
Iteration 4/25 | Loss: 0.00099538
Iteration 5/25 | Loss: 0.00099127
Iteration 6/25 | Loss: 0.00099030
Iteration 7/25 | Loss: 0.00099030
Iteration 8/25 | Loss: 0.00099030
Iteration 9/25 | Loss: 0.00099030
Iteration 10/25 | Loss: 0.00099030
Iteration 11/25 | Loss: 0.00099030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009903012542054057, 0.0009903012542054057, 0.0009903012542054057, 0.0009903012542054057, 0.0009903012542054057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009903012542054057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40500224
Iteration 2/25 | Loss: 0.00084276
Iteration 3/25 | Loss: 0.00084276
Iteration 4/25 | Loss: 0.00084276
Iteration 5/25 | Loss: 0.00084276
Iteration 6/25 | Loss: 0.00084276
Iteration 7/25 | Loss: 0.00084276
Iteration 8/25 | Loss: 0.00084276
Iteration 9/25 | Loss: 0.00084276
Iteration 10/25 | Loss: 0.00084276
Iteration 11/25 | Loss: 0.00084276
Iteration 12/25 | Loss: 0.00084276
Iteration 13/25 | Loss: 0.00084276
Iteration 14/25 | Loss: 0.00084276
Iteration 15/25 | Loss: 0.00084276
Iteration 16/25 | Loss: 0.00084276
Iteration 17/25 | Loss: 0.00084276
Iteration 18/25 | Loss: 0.00084276
Iteration 19/25 | Loss: 0.00084276
Iteration 20/25 | Loss: 0.00084276
Iteration 21/25 | Loss: 0.00084276
Iteration 22/25 | Loss: 0.00084276
Iteration 23/25 | Loss: 0.00084276
Iteration 24/25 | Loss: 0.00084276
Iteration 25/25 | Loss: 0.00084276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084276
Iteration 2/1000 | Loss: 0.00003231
Iteration 3/1000 | Loss: 0.00001614
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001148
Iteration 7/1000 | Loss: 0.00001117
Iteration 8/1000 | Loss: 0.00001083
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001054
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001035
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001030
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001014
Iteration 26/1000 | Loss: 0.00001014
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001010
Iteration 33/1000 | Loss: 0.00001009
Iteration 34/1000 | Loss: 0.00001009
Iteration 35/1000 | Loss: 0.00001007
Iteration 36/1000 | Loss: 0.00001007
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001006
Iteration 43/1000 | Loss: 0.00001006
Iteration 44/1000 | Loss: 0.00001006
Iteration 45/1000 | Loss: 0.00001006
Iteration 46/1000 | Loss: 0.00001006
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001005
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00001000
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00000999
Iteration 80/1000 | Loss: 0.00000999
Iteration 81/1000 | Loss: 0.00000999
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000999
Iteration 84/1000 | Loss: 0.00000999
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000998
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000998
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000994
Iteration 139/1000 | Loss: 0.00000994
Iteration 140/1000 | Loss: 0.00000994
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000994
Iteration 143/1000 | Loss: 0.00000994
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000994
Iteration 147/1000 | Loss: 0.00000994
Iteration 148/1000 | Loss: 0.00000994
Iteration 149/1000 | Loss: 0.00000994
Iteration 150/1000 | Loss: 0.00000994
Iteration 151/1000 | Loss: 0.00000994
Iteration 152/1000 | Loss: 0.00000994
Iteration 153/1000 | Loss: 0.00000994
Iteration 154/1000 | Loss: 0.00000994
Iteration 155/1000 | Loss: 0.00000994
Iteration 156/1000 | Loss: 0.00000994
Iteration 157/1000 | Loss: 0.00000994
Iteration 158/1000 | Loss: 0.00000994
Iteration 159/1000 | Loss: 0.00000994
Iteration 160/1000 | Loss: 0.00000994
Iteration 161/1000 | Loss: 0.00000994
Iteration 162/1000 | Loss: 0.00000994
Iteration 163/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [9.94177480606595e-06, 9.94177480606595e-06, 9.94177480606595e-06, 9.94177480606595e-06, 9.94177480606595e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.94177480606595e-06

Optimization complete. Final v2v error: 2.6890101432800293 mm

Highest mean error: 3.025508165359497 mm for frame 14

Lowest mean error: 2.5008771419525146 mm for frame 111

Saving results

Total time: 34.53919339179993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383261
Iteration 2/25 | Loss: 0.00112624
Iteration 3/25 | Loss: 0.00101869
Iteration 4/25 | Loss: 0.00100847
Iteration 5/25 | Loss: 0.00100534
Iteration 6/25 | Loss: 0.00100441
Iteration 7/25 | Loss: 0.00100441
Iteration 8/25 | Loss: 0.00100441
Iteration 9/25 | Loss: 0.00100441
Iteration 10/25 | Loss: 0.00100441
Iteration 11/25 | Loss: 0.00100441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010044117225334048, 0.0010044117225334048, 0.0010044117225334048, 0.0010044117225334048, 0.0010044117225334048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010044117225334048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39328349
Iteration 2/25 | Loss: 0.00084461
Iteration 3/25 | Loss: 0.00084461
Iteration 4/25 | Loss: 0.00084461
Iteration 5/25 | Loss: 0.00084461
Iteration 6/25 | Loss: 0.00084461
Iteration 7/25 | Loss: 0.00084460
Iteration 8/25 | Loss: 0.00084460
Iteration 9/25 | Loss: 0.00084460
Iteration 10/25 | Loss: 0.00084460
Iteration 11/25 | Loss: 0.00084460
Iteration 12/25 | Loss: 0.00084460
Iteration 13/25 | Loss: 0.00084460
Iteration 14/25 | Loss: 0.00084460
Iteration 15/25 | Loss: 0.00084460
Iteration 16/25 | Loss: 0.00084460
Iteration 17/25 | Loss: 0.00084460
Iteration 18/25 | Loss: 0.00084460
Iteration 19/25 | Loss: 0.00084460
Iteration 20/25 | Loss: 0.00084460
Iteration 21/25 | Loss: 0.00084460
Iteration 22/25 | Loss: 0.00084460
Iteration 23/25 | Loss: 0.00084460
Iteration 24/25 | Loss: 0.00084460
Iteration 25/25 | Loss: 0.00084460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084460
Iteration 2/1000 | Loss: 0.00002202
Iteration 3/1000 | Loss: 0.00001444
Iteration 4/1000 | Loss: 0.00001203
Iteration 5/1000 | Loss: 0.00001131
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001047
Iteration 8/1000 | Loss: 0.00001018
Iteration 9/1000 | Loss: 0.00001016
Iteration 10/1000 | Loss: 0.00001008
Iteration 11/1000 | Loss: 0.00000994
Iteration 12/1000 | Loss: 0.00000988
Iteration 13/1000 | Loss: 0.00000988
Iteration 14/1000 | Loss: 0.00000987
Iteration 15/1000 | Loss: 0.00000986
Iteration 16/1000 | Loss: 0.00000982
Iteration 17/1000 | Loss: 0.00000982
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000976
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000970
Iteration 23/1000 | Loss: 0.00000969
Iteration 24/1000 | Loss: 0.00000967
Iteration 25/1000 | Loss: 0.00000967
Iteration 26/1000 | Loss: 0.00000967
Iteration 27/1000 | Loss: 0.00000966
Iteration 28/1000 | Loss: 0.00000966
Iteration 29/1000 | Loss: 0.00000966
Iteration 30/1000 | Loss: 0.00000966
Iteration 31/1000 | Loss: 0.00000965
Iteration 32/1000 | Loss: 0.00000963
Iteration 33/1000 | Loss: 0.00000963
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000962
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000961
Iteration 42/1000 | Loss: 0.00000961
Iteration 43/1000 | Loss: 0.00000961
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000960
Iteration 46/1000 | Loss: 0.00000960
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000957
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000957
Iteration 58/1000 | Loss: 0.00000957
Iteration 59/1000 | Loss: 0.00000957
Iteration 60/1000 | Loss: 0.00000957
Iteration 61/1000 | Loss: 0.00000957
Iteration 62/1000 | Loss: 0.00000957
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000956
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000956
Iteration 67/1000 | Loss: 0.00000956
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000956
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000954
Iteration 86/1000 | Loss: 0.00000954
Iteration 87/1000 | Loss: 0.00000954
Iteration 88/1000 | Loss: 0.00000954
Iteration 89/1000 | Loss: 0.00000954
Iteration 90/1000 | Loss: 0.00000954
Iteration 91/1000 | Loss: 0.00000954
Iteration 92/1000 | Loss: 0.00000953
Iteration 93/1000 | Loss: 0.00000953
Iteration 94/1000 | Loss: 0.00000953
Iteration 95/1000 | Loss: 0.00000953
Iteration 96/1000 | Loss: 0.00000953
Iteration 97/1000 | Loss: 0.00000953
Iteration 98/1000 | Loss: 0.00000953
Iteration 99/1000 | Loss: 0.00000952
Iteration 100/1000 | Loss: 0.00000952
Iteration 101/1000 | Loss: 0.00000952
Iteration 102/1000 | Loss: 0.00000952
Iteration 103/1000 | Loss: 0.00000952
Iteration 104/1000 | Loss: 0.00000952
Iteration 105/1000 | Loss: 0.00000952
Iteration 106/1000 | Loss: 0.00000952
Iteration 107/1000 | Loss: 0.00000952
Iteration 108/1000 | Loss: 0.00000952
Iteration 109/1000 | Loss: 0.00000951
Iteration 110/1000 | Loss: 0.00000951
Iteration 111/1000 | Loss: 0.00000951
Iteration 112/1000 | Loss: 0.00000951
Iteration 113/1000 | Loss: 0.00000951
Iteration 114/1000 | Loss: 0.00000951
Iteration 115/1000 | Loss: 0.00000951
Iteration 116/1000 | Loss: 0.00000951
Iteration 117/1000 | Loss: 0.00000951
Iteration 118/1000 | Loss: 0.00000951
Iteration 119/1000 | Loss: 0.00000951
Iteration 120/1000 | Loss: 0.00000950
Iteration 121/1000 | Loss: 0.00000950
Iteration 122/1000 | Loss: 0.00000950
Iteration 123/1000 | Loss: 0.00000950
Iteration 124/1000 | Loss: 0.00000950
Iteration 125/1000 | Loss: 0.00000950
Iteration 126/1000 | Loss: 0.00000950
Iteration 127/1000 | Loss: 0.00000950
Iteration 128/1000 | Loss: 0.00000950
Iteration 129/1000 | Loss: 0.00000950
Iteration 130/1000 | Loss: 0.00000950
Iteration 131/1000 | Loss: 0.00000950
Iteration 132/1000 | Loss: 0.00000950
Iteration 133/1000 | Loss: 0.00000950
Iteration 134/1000 | Loss: 0.00000950
Iteration 135/1000 | Loss: 0.00000950
Iteration 136/1000 | Loss: 0.00000950
Iteration 137/1000 | Loss: 0.00000950
Iteration 138/1000 | Loss: 0.00000950
Iteration 139/1000 | Loss: 0.00000949
Iteration 140/1000 | Loss: 0.00000949
Iteration 141/1000 | Loss: 0.00000949
Iteration 142/1000 | Loss: 0.00000949
Iteration 143/1000 | Loss: 0.00000949
Iteration 144/1000 | Loss: 0.00000949
Iteration 145/1000 | Loss: 0.00000949
Iteration 146/1000 | Loss: 0.00000949
Iteration 147/1000 | Loss: 0.00000949
Iteration 148/1000 | Loss: 0.00000949
Iteration 149/1000 | Loss: 0.00000949
Iteration 150/1000 | Loss: 0.00000948
Iteration 151/1000 | Loss: 0.00000948
Iteration 152/1000 | Loss: 0.00000948
Iteration 153/1000 | Loss: 0.00000948
Iteration 154/1000 | Loss: 0.00000948
Iteration 155/1000 | Loss: 0.00000948
Iteration 156/1000 | Loss: 0.00000948
Iteration 157/1000 | Loss: 0.00000948
Iteration 158/1000 | Loss: 0.00000948
Iteration 159/1000 | Loss: 0.00000948
Iteration 160/1000 | Loss: 0.00000948
Iteration 161/1000 | Loss: 0.00000948
Iteration 162/1000 | Loss: 0.00000948
Iteration 163/1000 | Loss: 0.00000948
Iteration 164/1000 | Loss: 0.00000948
Iteration 165/1000 | Loss: 0.00000948
Iteration 166/1000 | Loss: 0.00000948
Iteration 167/1000 | Loss: 0.00000948
Iteration 168/1000 | Loss: 0.00000948
Iteration 169/1000 | Loss: 0.00000947
Iteration 170/1000 | Loss: 0.00000947
Iteration 171/1000 | Loss: 0.00000947
Iteration 172/1000 | Loss: 0.00000947
Iteration 173/1000 | Loss: 0.00000947
Iteration 174/1000 | Loss: 0.00000947
Iteration 175/1000 | Loss: 0.00000947
Iteration 176/1000 | Loss: 0.00000947
Iteration 177/1000 | Loss: 0.00000947
Iteration 178/1000 | Loss: 0.00000947
Iteration 179/1000 | Loss: 0.00000947
Iteration 180/1000 | Loss: 0.00000947
Iteration 181/1000 | Loss: 0.00000947
Iteration 182/1000 | Loss: 0.00000947
Iteration 183/1000 | Loss: 0.00000947
Iteration 184/1000 | Loss: 0.00000947
Iteration 185/1000 | Loss: 0.00000946
Iteration 186/1000 | Loss: 0.00000946
Iteration 187/1000 | Loss: 0.00000946
Iteration 188/1000 | Loss: 0.00000946
Iteration 189/1000 | Loss: 0.00000946
Iteration 190/1000 | Loss: 0.00000946
Iteration 191/1000 | Loss: 0.00000946
Iteration 192/1000 | Loss: 0.00000946
Iteration 193/1000 | Loss: 0.00000946
Iteration 194/1000 | Loss: 0.00000946
Iteration 195/1000 | Loss: 0.00000946
Iteration 196/1000 | Loss: 0.00000946
Iteration 197/1000 | Loss: 0.00000946
Iteration 198/1000 | Loss: 0.00000946
Iteration 199/1000 | Loss: 0.00000946
Iteration 200/1000 | Loss: 0.00000946
Iteration 201/1000 | Loss: 0.00000946
Iteration 202/1000 | Loss: 0.00000946
Iteration 203/1000 | Loss: 0.00000946
Iteration 204/1000 | Loss: 0.00000946
Iteration 205/1000 | Loss: 0.00000945
Iteration 206/1000 | Loss: 0.00000945
Iteration 207/1000 | Loss: 0.00000945
Iteration 208/1000 | Loss: 0.00000945
Iteration 209/1000 | Loss: 0.00000945
Iteration 210/1000 | Loss: 0.00000945
Iteration 211/1000 | Loss: 0.00000945
Iteration 212/1000 | Loss: 0.00000945
Iteration 213/1000 | Loss: 0.00000945
Iteration 214/1000 | Loss: 0.00000945
Iteration 215/1000 | Loss: 0.00000945
Iteration 216/1000 | Loss: 0.00000945
Iteration 217/1000 | Loss: 0.00000945
Iteration 218/1000 | Loss: 0.00000945
Iteration 219/1000 | Loss: 0.00000945
Iteration 220/1000 | Loss: 0.00000944
Iteration 221/1000 | Loss: 0.00000944
Iteration 222/1000 | Loss: 0.00000944
Iteration 223/1000 | Loss: 0.00000944
Iteration 224/1000 | Loss: 0.00000944
Iteration 225/1000 | Loss: 0.00000944
Iteration 226/1000 | Loss: 0.00000944
Iteration 227/1000 | Loss: 0.00000944
Iteration 228/1000 | Loss: 0.00000944
Iteration 229/1000 | Loss: 0.00000944
Iteration 230/1000 | Loss: 0.00000944
Iteration 231/1000 | Loss: 0.00000944
Iteration 232/1000 | Loss: 0.00000944
Iteration 233/1000 | Loss: 0.00000944
Iteration 234/1000 | Loss: 0.00000944
Iteration 235/1000 | Loss: 0.00000944
Iteration 236/1000 | Loss: 0.00000943
Iteration 237/1000 | Loss: 0.00000943
Iteration 238/1000 | Loss: 0.00000943
Iteration 239/1000 | Loss: 0.00000943
Iteration 240/1000 | Loss: 0.00000943
Iteration 241/1000 | Loss: 0.00000943
Iteration 242/1000 | Loss: 0.00000943
Iteration 243/1000 | Loss: 0.00000943
Iteration 244/1000 | Loss: 0.00000943
Iteration 245/1000 | Loss: 0.00000943
Iteration 246/1000 | Loss: 0.00000943
Iteration 247/1000 | Loss: 0.00000943
Iteration 248/1000 | Loss: 0.00000943
Iteration 249/1000 | Loss: 0.00000943
Iteration 250/1000 | Loss: 0.00000943
Iteration 251/1000 | Loss: 0.00000943
Iteration 252/1000 | Loss: 0.00000943
Iteration 253/1000 | Loss: 0.00000942
Iteration 254/1000 | Loss: 0.00000942
Iteration 255/1000 | Loss: 0.00000942
Iteration 256/1000 | Loss: 0.00000942
Iteration 257/1000 | Loss: 0.00000942
Iteration 258/1000 | Loss: 0.00000942
Iteration 259/1000 | Loss: 0.00000942
Iteration 260/1000 | Loss: 0.00000942
Iteration 261/1000 | Loss: 0.00000942
Iteration 262/1000 | Loss: 0.00000942
Iteration 263/1000 | Loss: 0.00000942
Iteration 264/1000 | Loss: 0.00000942
Iteration 265/1000 | Loss: 0.00000942
Iteration 266/1000 | Loss: 0.00000942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [9.421416507393587e-06, 9.421416507393587e-06, 9.421416507393587e-06, 9.421416507393587e-06, 9.421416507393587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.421416507393587e-06

Optimization complete. Final v2v error: 2.5798964500427246 mm

Highest mean error: 4.197791576385498 mm for frame 39

Lowest mean error: 2.16257381439209 mm for frame 75

Saving results

Total time: 39.046655893325806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541462
Iteration 2/25 | Loss: 0.00118938
Iteration 3/25 | Loss: 0.00105099
Iteration 4/25 | Loss: 0.00105551
Iteration 5/25 | Loss: 0.00100547
Iteration 6/25 | Loss: 0.00099545
Iteration 7/25 | Loss: 0.00099298
Iteration 8/25 | Loss: 0.00099261
Iteration 9/25 | Loss: 0.00099244
Iteration 10/25 | Loss: 0.00099232
Iteration 11/25 | Loss: 0.00099231
Iteration 12/25 | Loss: 0.00099231
Iteration 13/25 | Loss: 0.00099231
Iteration 14/25 | Loss: 0.00099230
Iteration 15/25 | Loss: 0.00099230
Iteration 16/25 | Loss: 0.00099230
Iteration 17/25 | Loss: 0.00099230
Iteration 18/25 | Loss: 0.00099230
Iteration 19/25 | Loss: 0.00099230
Iteration 20/25 | Loss: 0.00099230
Iteration 21/25 | Loss: 0.00099230
Iteration 22/25 | Loss: 0.00099230
Iteration 23/25 | Loss: 0.00099230
Iteration 24/25 | Loss: 0.00099230
Iteration 25/25 | Loss: 0.00099229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77036905
Iteration 2/25 | Loss: 0.00072902
Iteration 3/25 | Loss: 0.00072900
Iteration 4/25 | Loss: 0.00072900
Iteration 5/25 | Loss: 0.00072900
Iteration 6/25 | Loss: 0.00072899
Iteration 7/25 | Loss: 0.00072899
Iteration 8/25 | Loss: 0.00072899
Iteration 9/25 | Loss: 0.00072899
Iteration 10/25 | Loss: 0.00072899
Iteration 11/25 | Loss: 0.00072899
Iteration 12/25 | Loss: 0.00072899
Iteration 13/25 | Loss: 0.00072899
Iteration 14/25 | Loss: 0.00072899
Iteration 15/25 | Loss: 0.00072899
Iteration 16/25 | Loss: 0.00072899
Iteration 17/25 | Loss: 0.00072899
Iteration 18/25 | Loss: 0.00072899
Iteration 19/25 | Loss: 0.00072899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007289927452802658, 0.0007289927452802658, 0.0007289927452802658, 0.0007289927452802658, 0.0007289927452802658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007289927452802658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072899
Iteration 2/1000 | Loss: 0.00003101
Iteration 3/1000 | Loss: 0.00002123
Iteration 4/1000 | Loss: 0.00001892
Iteration 5/1000 | Loss: 0.00001778
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001579
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001479
Iteration 18/1000 | Loss: 0.00001478
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001473
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001466
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001462
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001458
Iteration 45/1000 | Loss: 0.00001458
Iteration 46/1000 | Loss: 0.00001458
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001454
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001452
Iteration 58/1000 | Loss: 0.00001452
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001446
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.4435117918765172e-05, 1.4435117918765172e-05, 1.4435117918765172e-05, 1.4435117918765172e-05, 1.4435117918765172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4435117918765172e-05

Optimization complete. Final v2v error: 3.110748767852783 mm

Highest mean error: 4.432625770568848 mm for frame 107

Lowest mean error: 2.531980514526367 mm for frame 127

Saving results

Total time: 48.994662046432495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925229
Iteration 2/25 | Loss: 0.00134351
Iteration 3/25 | Loss: 0.00122544
Iteration 4/25 | Loss: 0.00112698
Iteration 5/25 | Loss: 0.00111622
Iteration 6/25 | Loss: 0.00109323
Iteration 7/25 | Loss: 0.00108424
Iteration 8/25 | Loss: 0.00106706
Iteration 9/25 | Loss: 0.00106843
Iteration 10/25 | Loss: 0.00106470
Iteration 11/25 | Loss: 0.00106269
Iteration 12/25 | Loss: 0.00106194
Iteration 13/25 | Loss: 0.00106173
Iteration 14/25 | Loss: 0.00106152
Iteration 15/25 | Loss: 0.00106105
Iteration 16/25 | Loss: 0.00106074
Iteration 17/25 | Loss: 0.00106549
Iteration 18/25 | Loss: 0.00106392
Iteration 19/25 | Loss: 0.00106006
Iteration 20/25 | Loss: 0.00105808
Iteration 21/25 | Loss: 0.00105662
Iteration 22/25 | Loss: 0.00105641
Iteration 23/25 | Loss: 0.00105635
Iteration 24/25 | Loss: 0.00105634
Iteration 25/25 | Loss: 0.00105634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31500518
Iteration 2/25 | Loss: 0.00067099
Iteration 3/25 | Loss: 0.00067098
Iteration 4/25 | Loss: 0.00067098
Iteration 5/25 | Loss: 0.00067098
Iteration 6/25 | Loss: 0.00067098
Iteration 7/25 | Loss: 0.00067098
Iteration 8/25 | Loss: 0.00067098
Iteration 9/25 | Loss: 0.00067098
Iteration 10/25 | Loss: 0.00067098
Iteration 11/25 | Loss: 0.00067098
Iteration 12/25 | Loss: 0.00067098
Iteration 13/25 | Loss: 0.00067098
Iteration 14/25 | Loss: 0.00067098
Iteration 15/25 | Loss: 0.00067098
Iteration 16/25 | Loss: 0.00067098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006709786248393357, 0.0006709786248393357, 0.0006709786248393357, 0.0006709786248393357, 0.0006709786248393357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006709786248393357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067098
Iteration 2/1000 | Loss: 0.00006608
Iteration 3/1000 | Loss: 0.00004123
Iteration 4/1000 | Loss: 0.00003308
Iteration 5/1000 | Loss: 0.00108955
Iteration 6/1000 | Loss: 0.00069371
Iteration 7/1000 | Loss: 0.00051144
Iteration 8/1000 | Loss: 0.00094420
Iteration 9/1000 | Loss: 0.00047284
Iteration 10/1000 | Loss: 0.00028713
Iteration 11/1000 | Loss: 0.00062687
Iteration 12/1000 | Loss: 0.00032590
Iteration 13/1000 | Loss: 0.00095129
Iteration 14/1000 | Loss: 0.00035653
Iteration 15/1000 | Loss: 0.00018853
Iteration 16/1000 | Loss: 0.00002730
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00008540
Iteration 19/1000 | Loss: 0.00002758
Iteration 20/1000 | Loss: 0.00005570
Iteration 21/1000 | Loss: 0.00007740
Iteration 22/1000 | Loss: 0.00002463
Iteration 23/1000 | Loss: 0.00003878
Iteration 24/1000 | Loss: 0.00005049
Iteration 25/1000 | Loss: 0.00002379
Iteration 26/1000 | Loss: 0.00002333
Iteration 27/1000 | Loss: 0.00002295
Iteration 28/1000 | Loss: 0.00029644
Iteration 29/1000 | Loss: 0.00023179
Iteration 30/1000 | Loss: 0.00053379
Iteration 31/1000 | Loss: 0.00036710
Iteration 32/1000 | Loss: 0.00008030
Iteration 33/1000 | Loss: 0.00015708
Iteration 34/1000 | Loss: 0.00043808
Iteration 35/1000 | Loss: 0.00052228
Iteration 36/1000 | Loss: 0.00058697
Iteration 37/1000 | Loss: 0.00005730
Iteration 38/1000 | Loss: 0.00003550
Iteration 39/1000 | Loss: 0.00002374
Iteration 40/1000 | Loss: 0.00002132
Iteration 41/1000 | Loss: 0.00002025
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001912
Iteration 46/1000 | Loss: 0.00001902
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001899
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001897
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001886
Iteration 55/1000 | Loss: 0.00001882
Iteration 56/1000 | Loss: 0.00001882
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001871
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001870
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00001868
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001867
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001866
Iteration 95/1000 | Loss: 0.00001866
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001866
Iteration 98/1000 | Loss: 0.00001866
Iteration 99/1000 | Loss: 0.00001866
Iteration 100/1000 | Loss: 0.00001866
Iteration 101/1000 | Loss: 0.00001866
Iteration 102/1000 | Loss: 0.00001866
Iteration 103/1000 | Loss: 0.00001866
Iteration 104/1000 | Loss: 0.00001865
Iteration 105/1000 | Loss: 0.00001865
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001865
Iteration 108/1000 | Loss: 0.00001865
Iteration 109/1000 | Loss: 0.00001865
Iteration 110/1000 | Loss: 0.00001864
Iteration 111/1000 | Loss: 0.00001864
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001864
Iteration 114/1000 | Loss: 0.00001864
Iteration 115/1000 | Loss: 0.00001864
Iteration 116/1000 | Loss: 0.00001864
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001863
Iteration 120/1000 | Loss: 0.00001863
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001863
Iteration 126/1000 | Loss: 0.00001863
Iteration 127/1000 | Loss: 0.00001862
Iteration 128/1000 | Loss: 0.00001862
Iteration 129/1000 | Loss: 0.00001862
Iteration 130/1000 | Loss: 0.00001862
Iteration 131/1000 | Loss: 0.00001862
Iteration 132/1000 | Loss: 0.00001862
Iteration 133/1000 | Loss: 0.00001862
Iteration 134/1000 | Loss: 0.00001861
Iteration 135/1000 | Loss: 0.00001861
Iteration 136/1000 | Loss: 0.00001861
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001860
Iteration 139/1000 | Loss: 0.00001860
Iteration 140/1000 | Loss: 0.00001860
Iteration 141/1000 | Loss: 0.00001860
Iteration 142/1000 | Loss: 0.00001860
Iteration 143/1000 | Loss: 0.00001860
Iteration 144/1000 | Loss: 0.00001860
Iteration 145/1000 | Loss: 0.00001860
Iteration 146/1000 | Loss: 0.00001860
Iteration 147/1000 | Loss: 0.00001860
Iteration 148/1000 | Loss: 0.00001860
Iteration 149/1000 | Loss: 0.00001860
Iteration 150/1000 | Loss: 0.00001860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.8599874238134362e-05, 1.8599874238134362e-05, 1.8599874238134362e-05, 1.8599874238134362e-05, 1.8599874238134362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8599874238134362e-05

Optimization complete. Final v2v error: 3.6164042949676514 mm

Highest mean error: 5.150455474853516 mm for frame 137

Lowest mean error: 3.0521066188812256 mm for frame 33

Saving results

Total time: 109.93072009086609
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786051
Iteration 2/25 | Loss: 0.00134256
Iteration 3/25 | Loss: 0.00115692
Iteration 4/25 | Loss: 0.00107524
Iteration 5/25 | Loss: 0.00106278
Iteration 6/25 | Loss: 0.00105502
Iteration 7/25 | Loss: 0.00105381
Iteration 8/25 | Loss: 0.00105292
Iteration 9/25 | Loss: 0.00105244
Iteration 10/25 | Loss: 0.00105193
Iteration 11/25 | Loss: 0.00105158
Iteration 12/25 | Loss: 0.00105304
Iteration 13/25 | Loss: 0.00105223
Iteration 14/25 | Loss: 0.00105076
Iteration 15/25 | Loss: 0.00105016
Iteration 16/25 | Loss: 0.00104995
Iteration 17/25 | Loss: 0.00104992
Iteration 18/25 | Loss: 0.00104992
Iteration 19/25 | Loss: 0.00104991
Iteration 20/25 | Loss: 0.00104991
Iteration 21/25 | Loss: 0.00104991
Iteration 22/25 | Loss: 0.00104991
Iteration 23/25 | Loss: 0.00104991
Iteration 24/25 | Loss: 0.00104991
Iteration 25/25 | Loss: 0.00104991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16754413
Iteration 2/25 | Loss: 0.00073012
Iteration 3/25 | Loss: 0.00073012
Iteration 4/25 | Loss: 0.00073012
Iteration 5/25 | Loss: 0.00073012
Iteration 6/25 | Loss: 0.00073012
Iteration 7/25 | Loss: 0.00073012
Iteration 8/25 | Loss: 0.00073012
Iteration 9/25 | Loss: 0.00073012
Iteration 10/25 | Loss: 0.00073012
Iteration 11/25 | Loss: 0.00073012
Iteration 12/25 | Loss: 0.00073012
Iteration 13/25 | Loss: 0.00073012
Iteration 14/25 | Loss: 0.00073012
Iteration 15/25 | Loss: 0.00073012
Iteration 16/25 | Loss: 0.00073012
Iteration 17/25 | Loss: 0.00073012
Iteration 18/25 | Loss: 0.00073012
Iteration 19/25 | Loss: 0.00073012
Iteration 20/25 | Loss: 0.00073012
Iteration 21/25 | Loss: 0.00073012
Iteration 22/25 | Loss: 0.00073012
Iteration 23/25 | Loss: 0.00073012
Iteration 24/25 | Loss: 0.00073012
Iteration 25/25 | Loss: 0.00073012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073012
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00001978
Iteration 4/1000 | Loss: 0.00007518
Iteration 5/1000 | Loss: 0.00001727
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001600
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001519
Iteration 11/1000 | Loss: 0.00001518
Iteration 12/1000 | Loss: 0.00001509
Iteration 13/1000 | Loss: 0.00001503
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001495
Iteration 22/1000 | Loss: 0.00001495
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001492
Iteration 25/1000 | Loss: 0.00021060
Iteration 26/1000 | Loss: 0.00001680
Iteration 27/1000 | Loss: 0.00001532
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001376
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001357
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001355
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001354
Iteration 46/1000 | Loss: 0.00001354
Iteration 47/1000 | Loss: 0.00001354
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001353
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001352
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001351
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001351
Iteration 61/1000 | Loss: 0.00001351
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001342
Iteration 86/1000 | Loss: 0.00001341
Iteration 87/1000 | Loss: 0.00001341
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001341
Iteration 90/1000 | Loss: 0.00001340
Iteration 91/1000 | Loss: 0.00001340
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001339
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001339
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001337
Iteration 104/1000 | Loss: 0.00001337
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001328
Iteration 143/1000 | Loss: 0.00001328
Iteration 144/1000 | Loss: 0.00001328
Iteration 145/1000 | Loss: 0.00001328
Iteration 146/1000 | Loss: 0.00001328
Iteration 147/1000 | Loss: 0.00001328
Iteration 148/1000 | Loss: 0.00001328
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001327
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001326
Iteration 179/1000 | Loss: 0.00001326
Iteration 180/1000 | Loss: 0.00001326
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001326
Iteration 183/1000 | Loss: 0.00001326
Iteration 184/1000 | Loss: 0.00001326
Iteration 185/1000 | Loss: 0.00001326
Iteration 186/1000 | Loss: 0.00001326
Iteration 187/1000 | Loss: 0.00001326
Iteration 188/1000 | Loss: 0.00001326
Iteration 189/1000 | Loss: 0.00001326
Iteration 190/1000 | Loss: 0.00001326
Iteration 191/1000 | Loss: 0.00001326
Iteration 192/1000 | Loss: 0.00001326
Iteration 193/1000 | Loss: 0.00001326
Iteration 194/1000 | Loss: 0.00001326
Iteration 195/1000 | Loss: 0.00001326
Iteration 196/1000 | Loss: 0.00001326
Iteration 197/1000 | Loss: 0.00001326
Iteration 198/1000 | Loss: 0.00001325
Iteration 199/1000 | Loss: 0.00001325
Iteration 200/1000 | Loss: 0.00001325
Iteration 201/1000 | Loss: 0.00001325
Iteration 202/1000 | Loss: 0.00001325
Iteration 203/1000 | Loss: 0.00001325
Iteration 204/1000 | Loss: 0.00001325
Iteration 205/1000 | Loss: 0.00001325
Iteration 206/1000 | Loss: 0.00001325
Iteration 207/1000 | Loss: 0.00001325
Iteration 208/1000 | Loss: 0.00001325
Iteration 209/1000 | Loss: 0.00001325
Iteration 210/1000 | Loss: 0.00001325
Iteration 211/1000 | Loss: 0.00001325
Iteration 212/1000 | Loss: 0.00001325
Iteration 213/1000 | Loss: 0.00001325
Iteration 214/1000 | Loss: 0.00001325
Iteration 215/1000 | Loss: 0.00001325
Iteration 216/1000 | Loss: 0.00001325
Iteration 217/1000 | Loss: 0.00001325
Iteration 218/1000 | Loss: 0.00001325
Iteration 219/1000 | Loss: 0.00001325
Iteration 220/1000 | Loss: 0.00001325
Iteration 221/1000 | Loss: 0.00001325
Iteration 222/1000 | Loss: 0.00001325
Iteration 223/1000 | Loss: 0.00001325
Iteration 224/1000 | Loss: 0.00001325
Iteration 225/1000 | Loss: 0.00001325
Iteration 226/1000 | Loss: 0.00001325
Iteration 227/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.324512868450256e-05, 1.324512868450256e-05, 1.324512868450256e-05, 1.324512868450256e-05, 1.324512868450256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.324512868450256e-05

Optimization complete. Final v2v error: 3.028729200363159 mm

Highest mean error: 8.749703407287598 mm for frame 0

Lowest mean error: 2.629635810852051 mm for frame 75

Saving results

Total time: 75.90782642364502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798038
Iteration 2/25 | Loss: 0.00123647
Iteration 3/25 | Loss: 0.00099985
Iteration 4/25 | Loss: 0.00098345
Iteration 5/25 | Loss: 0.00097815
Iteration 6/25 | Loss: 0.00097636
Iteration 7/25 | Loss: 0.00097636
Iteration 8/25 | Loss: 0.00097636
Iteration 9/25 | Loss: 0.00097636
Iteration 10/25 | Loss: 0.00097636
Iteration 11/25 | Loss: 0.00097636
Iteration 12/25 | Loss: 0.00097636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009763581911101937, 0.0009763581911101937, 0.0009763581911101937, 0.0009763581911101937, 0.0009763581911101937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009763581911101937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20990324
Iteration 2/25 | Loss: 0.00079310
Iteration 3/25 | Loss: 0.00079309
Iteration 4/25 | Loss: 0.00079309
Iteration 5/25 | Loss: 0.00079309
Iteration 6/25 | Loss: 0.00079309
Iteration 7/25 | Loss: 0.00079309
Iteration 8/25 | Loss: 0.00079309
Iteration 9/25 | Loss: 0.00079309
Iteration 10/25 | Loss: 0.00079309
Iteration 11/25 | Loss: 0.00079309
Iteration 12/25 | Loss: 0.00079309
Iteration 13/25 | Loss: 0.00079309
Iteration 14/25 | Loss: 0.00079309
Iteration 15/25 | Loss: 0.00079309
Iteration 16/25 | Loss: 0.00079309
Iteration 17/25 | Loss: 0.00079309
Iteration 18/25 | Loss: 0.00079309
Iteration 19/25 | Loss: 0.00079309
Iteration 20/25 | Loss: 0.00079309
Iteration 21/25 | Loss: 0.00079309
Iteration 22/25 | Loss: 0.00079309
Iteration 23/25 | Loss: 0.00079309
Iteration 24/25 | Loss: 0.00079309
Iteration 25/25 | Loss: 0.00079309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079309
Iteration 2/1000 | Loss: 0.00004217
Iteration 3/1000 | Loss: 0.00002515
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00001541
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001299
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001229
Iteration 13/1000 | Loss: 0.00001210
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001189
Iteration 19/1000 | Loss: 0.00001184
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001175
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001165
Iteration 32/1000 | Loss: 0.00001165
Iteration 33/1000 | Loss: 0.00001165
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001161
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001157
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001156
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001150
Iteration 55/1000 | Loss: 0.00001150
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001149
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001148
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001146
Iteration 72/1000 | Loss: 0.00001146
Iteration 73/1000 | Loss: 0.00001146
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001141
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001138
Iteration 113/1000 | Loss: 0.00001138
Iteration 114/1000 | Loss: 0.00001138
Iteration 115/1000 | Loss: 0.00001138
Iteration 116/1000 | Loss: 0.00001138
Iteration 117/1000 | Loss: 0.00001138
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001137
Iteration 121/1000 | Loss: 0.00001137
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001134
Iteration 135/1000 | Loss: 0.00001134
Iteration 136/1000 | Loss: 0.00001134
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001134
Iteration 140/1000 | Loss: 0.00001134
Iteration 141/1000 | Loss: 0.00001134
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001133
Iteration 146/1000 | Loss: 0.00001133
Iteration 147/1000 | Loss: 0.00001133
Iteration 148/1000 | Loss: 0.00001133
Iteration 149/1000 | Loss: 0.00001133
Iteration 150/1000 | Loss: 0.00001133
Iteration 151/1000 | Loss: 0.00001133
Iteration 152/1000 | Loss: 0.00001133
Iteration 153/1000 | Loss: 0.00001133
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001133
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001133
Iteration 158/1000 | Loss: 0.00001133
Iteration 159/1000 | Loss: 0.00001133
Iteration 160/1000 | Loss: 0.00001133
Iteration 161/1000 | Loss: 0.00001133
Iteration 162/1000 | Loss: 0.00001133
Iteration 163/1000 | Loss: 0.00001132
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00001132
Iteration 166/1000 | Loss: 0.00001132
Iteration 167/1000 | Loss: 0.00001132
Iteration 168/1000 | Loss: 0.00001132
Iteration 169/1000 | Loss: 0.00001132
Iteration 170/1000 | Loss: 0.00001132
Iteration 171/1000 | Loss: 0.00001132
Iteration 172/1000 | Loss: 0.00001132
Iteration 173/1000 | Loss: 0.00001132
Iteration 174/1000 | Loss: 0.00001131
Iteration 175/1000 | Loss: 0.00001131
Iteration 176/1000 | Loss: 0.00001131
Iteration 177/1000 | Loss: 0.00001131
Iteration 178/1000 | Loss: 0.00001131
Iteration 179/1000 | Loss: 0.00001131
Iteration 180/1000 | Loss: 0.00001131
Iteration 181/1000 | Loss: 0.00001131
Iteration 182/1000 | Loss: 0.00001131
Iteration 183/1000 | Loss: 0.00001130
Iteration 184/1000 | Loss: 0.00001130
Iteration 185/1000 | Loss: 0.00001130
Iteration 186/1000 | Loss: 0.00001130
Iteration 187/1000 | Loss: 0.00001130
Iteration 188/1000 | Loss: 0.00001130
Iteration 189/1000 | Loss: 0.00001130
Iteration 190/1000 | Loss: 0.00001130
Iteration 191/1000 | Loss: 0.00001130
Iteration 192/1000 | Loss: 0.00001130
Iteration 193/1000 | Loss: 0.00001130
Iteration 194/1000 | Loss: 0.00001130
Iteration 195/1000 | Loss: 0.00001130
Iteration 196/1000 | Loss: 0.00001130
Iteration 197/1000 | Loss: 0.00001130
Iteration 198/1000 | Loss: 0.00001130
Iteration 199/1000 | Loss: 0.00001129
Iteration 200/1000 | Loss: 0.00001129
Iteration 201/1000 | Loss: 0.00001129
Iteration 202/1000 | Loss: 0.00001129
Iteration 203/1000 | Loss: 0.00001129
Iteration 204/1000 | Loss: 0.00001129
Iteration 205/1000 | Loss: 0.00001129
Iteration 206/1000 | Loss: 0.00001129
Iteration 207/1000 | Loss: 0.00001129
Iteration 208/1000 | Loss: 0.00001129
Iteration 209/1000 | Loss: 0.00001129
Iteration 210/1000 | Loss: 0.00001129
Iteration 211/1000 | Loss: 0.00001129
Iteration 212/1000 | Loss: 0.00001128
Iteration 213/1000 | Loss: 0.00001128
Iteration 214/1000 | Loss: 0.00001128
Iteration 215/1000 | Loss: 0.00001128
Iteration 216/1000 | Loss: 0.00001128
Iteration 217/1000 | Loss: 0.00001128
Iteration 218/1000 | Loss: 0.00001128
Iteration 219/1000 | Loss: 0.00001128
Iteration 220/1000 | Loss: 0.00001128
Iteration 221/1000 | Loss: 0.00001128
Iteration 222/1000 | Loss: 0.00001128
Iteration 223/1000 | Loss: 0.00001128
Iteration 224/1000 | Loss: 0.00001128
Iteration 225/1000 | Loss: 0.00001128
Iteration 226/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1280695616733283e-05, 1.1280695616733283e-05, 1.1280695616733283e-05, 1.1280695616733283e-05, 1.1280695616733283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1280695616733283e-05

Optimization complete. Final v2v error: 2.72479510307312 mm

Highest mean error: 4.19455099105835 mm for frame 76

Lowest mean error: 2.2030158042907715 mm for frame 109

Saving results

Total time: 44.249807834625244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791602
Iteration 2/25 | Loss: 0.00174001
Iteration 3/25 | Loss: 0.00116643
Iteration 4/25 | Loss: 0.00109637
Iteration 5/25 | Loss: 0.00109180
Iteration 6/25 | Loss: 0.00109058
Iteration 7/25 | Loss: 0.00109052
Iteration 8/25 | Loss: 0.00109052
Iteration 9/25 | Loss: 0.00109052
Iteration 10/25 | Loss: 0.00109052
Iteration 11/25 | Loss: 0.00109052
Iteration 12/25 | Loss: 0.00109052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010905190138146281, 0.0010905190138146281, 0.0010905190138146281, 0.0010905190138146281, 0.0010905190138146281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010905190138146281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38600028
Iteration 2/25 | Loss: 0.00060176
Iteration 3/25 | Loss: 0.00060175
Iteration 4/25 | Loss: 0.00060174
Iteration 5/25 | Loss: 0.00060174
Iteration 6/25 | Loss: 0.00060174
Iteration 7/25 | Loss: 0.00060174
Iteration 8/25 | Loss: 0.00060174
Iteration 9/25 | Loss: 0.00060174
Iteration 10/25 | Loss: 0.00060174
Iteration 11/25 | Loss: 0.00060174
Iteration 12/25 | Loss: 0.00060174
Iteration 13/25 | Loss: 0.00060174
Iteration 14/25 | Loss: 0.00060174
Iteration 15/25 | Loss: 0.00060174
Iteration 16/25 | Loss: 0.00060174
Iteration 17/25 | Loss: 0.00060174
Iteration 18/25 | Loss: 0.00060174
Iteration 19/25 | Loss: 0.00060174
Iteration 20/25 | Loss: 0.00060174
Iteration 21/25 | Loss: 0.00060174
Iteration 22/25 | Loss: 0.00060174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006017418927513063, 0.0006017418927513063, 0.0006017418927513063, 0.0006017418927513063, 0.0006017418927513063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006017418927513063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060174
Iteration 2/1000 | Loss: 0.00004049
Iteration 3/1000 | Loss: 0.00002594
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001712
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001665
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001649
Iteration 16/1000 | Loss: 0.00001648
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001642
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001635
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001632
Iteration 54/1000 | Loss: 0.00001632
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001631
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001631
Iteration 63/1000 | Loss: 0.00001630
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001630
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001629
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00001627
Iteration 74/1000 | Loss: 0.00001627
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001625
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001621
Iteration 124/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6212825357797556e-05, 1.6212825357797556e-05, 1.6212825357797556e-05, 1.6212825357797556e-05, 1.6212825357797556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6212825357797556e-05

Optimization complete. Final v2v error: 3.3847904205322266 mm

Highest mean error: 3.89284086227417 mm for frame 135

Lowest mean error: 3.106687307357788 mm for frame 109

Saving results

Total time: 31.697986841201782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451156
Iteration 2/25 | Loss: 0.00123977
Iteration 3/25 | Loss: 0.00104830
Iteration 4/25 | Loss: 0.00102582
Iteration 5/25 | Loss: 0.00101939
Iteration 6/25 | Loss: 0.00101722
Iteration 7/25 | Loss: 0.00101694
Iteration 8/25 | Loss: 0.00101694
Iteration 9/25 | Loss: 0.00101694
Iteration 10/25 | Loss: 0.00101694
Iteration 11/25 | Loss: 0.00101694
Iteration 12/25 | Loss: 0.00101694
Iteration 13/25 | Loss: 0.00101694
Iteration 14/25 | Loss: 0.00101694
Iteration 15/25 | Loss: 0.00101694
Iteration 16/25 | Loss: 0.00101694
Iteration 17/25 | Loss: 0.00101694
Iteration 18/25 | Loss: 0.00101694
Iteration 19/25 | Loss: 0.00101694
Iteration 20/25 | Loss: 0.00101694
Iteration 21/25 | Loss: 0.00101694
Iteration 22/25 | Loss: 0.00101694
Iteration 23/25 | Loss: 0.00101694
Iteration 24/25 | Loss: 0.00101694
Iteration 25/25 | Loss: 0.00101694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010169403394684196, 0.0010169403394684196, 0.0010169403394684196, 0.0010169403394684196, 0.0010169403394684196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010169403394684196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38578141
Iteration 2/25 | Loss: 0.00093098
Iteration 3/25 | Loss: 0.00093098
Iteration 4/25 | Loss: 0.00093098
Iteration 5/25 | Loss: 0.00093098
Iteration 6/25 | Loss: 0.00093098
Iteration 7/25 | Loss: 0.00093098
Iteration 8/25 | Loss: 0.00093098
Iteration 9/25 | Loss: 0.00093098
Iteration 10/25 | Loss: 0.00093098
Iteration 11/25 | Loss: 0.00093098
Iteration 12/25 | Loss: 0.00093098
Iteration 13/25 | Loss: 0.00093098
Iteration 14/25 | Loss: 0.00093098
Iteration 15/25 | Loss: 0.00093098
Iteration 16/25 | Loss: 0.00093098
Iteration 17/25 | Loss: 0.00093098
Iteration 18/25 | Loss: 0.00093098
Iteration 19/25 | Loss: 0.00093098
Iteration 20/25 | Loss: 0.00093098
Iteration 21/25 | Loss: 0.00093098
Iteration 22/25 | Loss: 0.00093098
Iteration 23/25 | Loss: 0.00093098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009309786255471408, 0.0009309786255471408, 0.0009309786255471408, 0.0009309786255471408, 0.0009309786255471408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009309786255471408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093098
Iteration 2/1000 | Loss: 0.00003066
Iteration 3/1000 | Loss: 0.00002020
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001765
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001617
Iteration 9/1000 | Loss: 0.00001592
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001574
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001560
Iteration 16/1000 | Loss: 0.00001556
Iteration 17/1000 | Loss: 0.00001556
Iteration 18/1000 | Loss: 0.00001555
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001551
Iteration 29/1000 | Loss: 0.00001550
Iteration 30/1000 | Loss: 0.00001550
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001546
Iteration 36/1000 | Loss: 0.00001546
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001545
Iteration 39/1000 | Loss: 0.00001542
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001540
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001538
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001537
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001535
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001534
Iteration 66/1000 | Loss: 0.00001534
Iteration 67/1000 | Loss: 0.00001534
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001526
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001526
Iteration 94/1000 | Loss: 0.00001526
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001525
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001524
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001524
Iteration 107/1000 | Loss: 0.00001524
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001523
Iteration 110/1000 | Loss: 0.00001523
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001522
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001520
Iteration 126/1000 | Loss: 0.00001520
Iteration 127/1000 | Loss: 0.00001520
Iteration 128/1000 | Loss: 0.00001520
Iteration 129/1000 | Loss: 0.00001520
Iteration 130/1000 | Loss: 0.00001520
Iteration 131/1000 | Loss: 0.00001520
Iteration 132/1000 | Loss: 0.00001520
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001518
Iteration 138/1000 | Loss: 0.00001518
Iteration 139/1000 | Loss: 0.00001518
Iteration 140/1000 | Loss: 0.00001518
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001517
Iteration 143/1000 | Loss: 0.00001517
Iteration 144/1000 | Loss: 0.00001517
Iteration 145/1000 | Loss: 0.00001517
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001516
Iteration 148/1000 | Loss: 0.00001516
Iteration 149/1000 | Loss: 0.00001516
Iteration 150/1000 | Loss: 0.00001516
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001515
Iteration 154/1000 | Loss: 0.00001515
Iteration 155/1000 | Loss: 0.00001515
Iteration 156/1000 | Loss: 0.00001515
Iteration 157/1000 | Loss: 0.00001515
Iteration 158/1000 | Loss: 0.00001515
Iteration 159/1000 | Loss: 0.00001515
Iteration 160/1000 | Loss: 0.00001515
Iteration 161/1000 | Loss: 0.00001515
Iteration 162/1000 | Loss: 0.00001515
Iteration 163/1000 | Loss: 0.00001515
Iteration 164/1000 | Loss: 0.00001514
Iteration 165/1000 | Loss: 0.00001514
Iteration 166/1000 | Loss: 0.00001514
Iteration 167/1000 | Loss: 0.00001514
Iteration 168/1000 | Loss: 0.00001514
Iteration 169/1000 | Loss: 0.00001513
Iteration 170/1000 | Loss: 0.00001513
Iteration 171/1000 | Loss: 0.00001513
Iteration 172/1000 | Loss: 0.00001513
Iteration 173/1000 | Loss: 0.00001513
Iteration 174/1000 | Loss: 0.00001513
Iteration 175/1000 | Loss: 0.00001513
Iteration 176/1000 | Loss: 0.00001513
Iteration 177/1000 | Loss: 0.00001513
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001512
Iteration 180/1000 | Loss: 0.00001512
Iteration 181/1000 | Loss: 0.00001512
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001512
Iteration 184/1000 | Loss: 0.00001512
Iteration 185/1000 | Loss: 0.00001512
Iteration 186/1000 | Loss: 0.00001512
Iteration 187/1000 | Loss: 0.00001512
Iteration 188/1000 | Loss: 0.00001511
Iteration 189/1000 | Loss: 0.00001511
Iteration 190/1000 | Loss: 0.00001511
Iteration 191/1000 | Loss: 0.00001511
Iteration 192/1000 | Loss: 0.00001511
Iteration 193/1000 | Loss: 0.00001511
Iteration 194/1000 | Loss: 0.00001511
Iteration 195/1000 | Loss: 0.00001511
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001510
Iteration 201/1000 | Loss: 0.00001510
Iteration 202/1000 | Loss: 0.00001510
Iteration 203/1000 | Loss: 0.00001510
Iteration 204/1000 | Loss: 0.00001510
Iteration 205/1000 | Loss: 0.00001510
Iteration 206/1000 | Loss: 0.00001510
Iteration 207/1000 | Loss: 0.00001510
Iteration 208/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.5101421013241634e-05, 1.5101421013241634e-05, 1.5101421013241634e-05, 1.5101421013241634e-05, 1.5101421013241634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5101421013241634e-05

Optimization complete. Final v2v error: 3.1809067726135254 mm

Highest mean error: 4.078356742858887 mm for frame 116

Lowest mean error: 2.378887176513672 mm for frame 201

Saving results

Total time: 46.31196737289429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380810
Iteration 2/25 | Loss: 0.00114503
Iteration 3/25 | Loss: 0.00101984
Iteration 4/25 | Loss: 0.00099937
Iteration 5/25 | Loss: 0.00099264
Iteration 6/25 | Loss: 0.00099083
Iteration 7/25 | Loss: 0.00099051
Iteration 8/25 | Loss: 0.00099051
Iteration 9/25 | Loss: 0.00099051
Iteration 10/25 | Loss: 0.00099051
Iteration 11/25 | Loss: 0.00099051
Iteration 12/25 | Loss: 0.00099051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009905078914016485, 0.0009905078914016485, 0.0009905078914016485, 0.0009905078914016485, 0.0009905078914016485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009905078914016485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75655508
Iteration 2/25 | Loss: 0.00091362
Iteration 3/25 | Loss: 0.00091356
Iteration 4/25 | Loss: 0.00091356
Iteration 5/25 | Loss: 0.00091356
Iteration 6/25 | Loss: 0.00091356
Iteration 7/25 | Loss: 0.00091356
Iteration 8/25 | Loss: 0.00091356
Iteration 9/25 | Loss: 0.00091356
Iteration 10/25 | Loss: 0.00091356
Iteration 11/25 | Loss: 0.00091356
Iteration 12/25 | Loss: 0.00091356
Iteration 13/25 | Loss: 0.00091356
Iteration 14/25 | Loss: 0.00091356
Iteration 15/25 | Loss: 0.00091356
Iteration 16/25 | Loss: 0.00091356
Iteration 17/25 | Loss: 0.00091356
Iteration 18/25 | Loss: 0.00091356
Iteration 19/25 | Loss: 0.00091356
Iteration 20/25 | Loss: 0.00091356
Iteration 21/25 | Loss: 0.00091356
Iteration 22/25 | Loss: 0.00091356
Iteration 23/25 | Loss: 0.00091356
Iteration 24/25 | Loss: 0.00091356
Iteration 25/25 | Loss: 0.00091356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091356
Iteration 2/1000 | Loss: 0.00004758
Iteration 3/1000 | Loss: 0.00002718
Iteration 4/1000 | Loss: 0.00001710
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001226
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001163
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001157
Iteration 31/1000 | Loss: 0.00001156
Iteration 32/1000 | Loss: 0.00001155
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001152
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001148
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001147
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001144
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001144
Iteration 54/1000 | Loss: 0.00001144
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001144
Iteration 57/1000 | Loss: 0.00001143
Iteration 58/1000 | Loss: 0.00001143
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001134
Iteration 84/1000 | Loss: 0.00001134
Iteration 85/1000 | Loss: 0.00001133
Iteration 86/1000 | Loss: 0.00001133
Iteration 87/1000 | Loss: 0.00001133
Iteration 88/1000 | Loss: 0.00001133
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001131
Iteration 94/1000 | Loss: 0.00001130
Iteration 95/1000 | Loss: 0.00001130
Iteration 96/1000 | Loss: 0.00001130
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001124
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001123
Iteration 113/1000 | Loss: 0.00001123
Iteration 114/1000 | Loss: 0.00001122
Iteration 115/1000 | Loss: 0.00001122
Iteration 116/1000 | Loss: 0.00001122
Iteration 117/1000 | Loss: 0.00001122
Iteration 118/1000 | Loss: 0.00001122
Iteration 119/1000 | Loss: 0.00001122
Iteration 120/1000 | Loss: 0.00001122
Iteration 121/1000 | Loss: 0.00001122
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001121
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001121
Iteration 128/1000 | Loss: 0.00001121
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001119
Iteration 137/1000 | Loss: 0.00001119
Iteration 138/1000 | Loss: 0.00001119
Iteration 139/1000 | Loss: 0.00001119
Iteration 140/1000 | Loss: 0.00001119
Iteration 141/1000 | Loss: 0.00001119
Iteration 142/1000 | Loss: 0.00001119
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001118
Iteration 147/1000 | Loss: 0.00001118
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001118
Iteration 152/1000 | Loss: 0.00001118
Iteration 153/1000 | Loss: 0.00001118
Iteration 154/1000 | Loss: 0.00001118
Iteration 155/1000 | Loss: 0.00001118
Iteration 156/1000 | Loss: 0.00001118
Iteration 157/1000 | Loss: 0.00001118
Iteration 158/1000 | Loss: 0.00001118
Iteration 159/1000 | Loss: 0.00001118
Iteration 160/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.1183147762494627e-05, 1.1183147762494627e-05, 1.1183147762494627e-05, 1.1183147762494627e-05, 1.1183147762494627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1183147762494627e-05

Optimization complete. Final v2v error: 2.7718088626861572 mm

Highest mean error: 3.5089828968048096 mm for frame 10

Lowest mean error: 2.2427713871002197 mm for frame 138

Saving results

Total time: 37.45792102813721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085888
Iteration 2/25 | Loss: 0.01085888
Iteration 3/25 | Loss: 0.01085888
Iteration 4/25 | Loss: 0.01085887
Iteration 5/25 | Loss: 0.00363659
Iteration 6/25 | Loss: 0.00225047
Iteration 7/25 | Loss: 0.00190503
Iteration 8/25 | Loss: 0.00212290
Iteration 9/25 | Loss: 0.00203043
Iteration 10/25 | Loss: 0.00227486
Iteration 11/25 | Loss: 0.00169830
Iteration 12/25 | Loss: 0.00155968
Iteration 13/25 | Loss: 0.00148984
Iteration 14/25 | Loss: 0.00145488
Iteration 15/25 | Loss: 0.00144705
Iteration 16/25 | Loss: 0.00145357
Iteration 17/25 | Loss: 0.00143522
Iteration 18/25 | Loss: 0.00142782
Iteration 19/25 | Loss: 0.00142348
Iteration 20/25 | Loss: 0.00142021
Iteration 21/25 | Loss: 0.00141338
Iteration 22/25 | Loss: 0.00141585
Iteration 23/25 | Loss: 0.00141403
Iteration 24/25 | Loss: 0.00141583
Iteration 25/25 | Loss: 0.00141746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63660920
Iteration 2/25 | Loss: 0.00140288
Iteration 3/25 | Loss: 0.00140287
Iteration 4/25 | Loss: 0.00127406
Iteration 5/25 | Loss: 0.00127406
Iteration 6/25 | Loss: 0.00127406
Iteration 7/25 | Loss: 0.00127406
Iteration 8/25 | Loss: 0.00127406
Iteration 9/25 | Loss: 0.00127406
Iteration 10/25 | Loss: 0.00127406
Iteration 11/25 | Loss: 0.00127406
Iteration 12/25 | Loss: 0.00127406
Iteration 13/25 | Loss: 0.00127406
Iteration 14/25 | Loss: 0.00127406
Iteration 15/25 | Loss: 0.00127406
Iteration 16/25 | Loss: 0.00127406
Iteration 17/25 | Loss: 0.00127406
Iteration 18/25 | Loss: 0.00127406
Iteration 19/25 | Loss: 0.00127406
Iteration 20/25 | Loss: 0.00127406
Iteration 21/25 | Loss: 0.00127406
Iteration 22/25 | Loss: 0.00127406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012740561505779624, 0.0012740561505779624, 0.0012740561505779624, 0.0012740561505779624, 0.0012740561505779624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012740561505779624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127406
Iteration 2/1000 | Loss: 0.00035777
Iteration 3/1000 | Loss: 0.00040865
Iteration 4/1000 | Loss: 0.00041152
Iteration 5/1000 | Loss: 0.00035643
Iteration 6/1000 | Loss: 0.00029086
Iteration 7/1000 | Loss: 0.00103148
Iteration 8/1000 | Loss: 0.00059280
Iteration 9/1000 | Loss: 0.00041057
Iteration 10/1000 | Loss: 0.00051152
Iteration 11/1000 | Loss: 0.00049322
Iteration 12/1000 | Loss: 0.00051516
Iteration 13/1000 | Loss: 0.00051973
Iteration 14/1000 | Loss: 0.00047043
Iteration 15/1000 | Loss: 0.00056115
Iteration 16/1000 | Loss: 0.00023208
Iteration 17/1000 | Loss: 0.00026352
Iteration 18/1000 | Loss: 0.00029317
Iteration 19/1000 | Loss: 0.00026120
Iteration 20/1000 | Loss: 0.00063370
Iteration 21/1000 | Loss: 0.00032962
Iteration 22/1000 | Loss: 0.00089898
Iteration 23/1000 | Loss: 0.00052397
Iteration 24/1000 | Loss: 0.00052343
Iteration 25/1000 | Loss: 0.00051546
Iteration 26/1000 | Loss: 0.00058818
Iteration 27/1000 | Loss: 0.00298784
Iteration 28/1000 | Loss: 0.00035116
Iteration 29/1000 | Loss: 0.00021121
Iteration 30/1000 | Loss: 0.00021018
Iteration 31/1000 | Loss: 0.00022597
Iteration 32/1000 | Loss: 0.00019602
Iteration 33/1000 | Loss: 0.00022677
Iteration 34/1000 | Loss: 0.00021585
Iteration 35/1000 | Loss: 0.00016115
Iteration 36/1000 | Loss: 0.00019456
Iteration 37/1000 | Loss: 0.00025615
Iteration 38/1000 | Loss: 0.00029624
Iteration 39/1000 | Loss: 0.00024795
Iteration 40/1000 | Loss: 0.00022856
Iteration 41/1000 | Loss: 0.00022679
Iteration 42/1000 | Loss: 0.00027024
Iteration 43/1000 | Loss: 0.00045131
Iteration 44/1000 | Loss: 0.00064107
Iteration 45/1000 | Loss: 0.00020666
Iteration 46/1000 | Loss: 0.00015141
Iteration 47/1000 | Loss: 0.00027311
Iteration 48/1000 | Loss: 0.00016280
Iteration 49/1000 | Loss: 0.00018147
Iteration 50/1000 | Loss: 0.00016249
Iteration 51/1000 | Loss: 0.00034431
Iteration 52/1000 | Loss: 0.00027672
Iteration 53/1000 | Loss: 0.00029140
Iteration 54/1000 | Loss: 0.00030122
Iteration 55/1000 | Loss: 0.00035912
Iteration 56/1000 | Loss: 0.00027581
Iteration 57/1000 | Loss: 0.00057015
Iteration 58/1000 | Loss: 0.00023264
Iteration 59/1000 | Loss: 0.00014663
Iteration 60/1000 | Loss: 0.00031768
Iteration 61/1000 | Loss: 0.00017133
Iteration 62/1000 | Loss: 0.00024669
Iteration 63/1000 | Loss: 0.00013303
Iteration 64/1000 | Loss: 0.00032264
Iteration 65/1000 | Loss: 0.00009945
Iteration 66/1000 | Loss: 0.00016815
Iteration 67/1000 | Loss: 0.00011538
Iteration 68/1000 | Loss: 0.00036551
Iteration 69/1000 | Loss: 0.00018836
Iteration 70/1000 | Loss: 0.00018928
Iteration 71/1000 | Loss: 0.00022057
Iteration 72/1000 | Loss: 0.00017614
Iteration 73/1000 | Loss: 0.00011157
Iteration 74/1000 | Loss: 0.00019505
Iteration 75/1000 | Loss: 0.00027058
Iteration 76/1000 | Loss: 0.00033416
Iteration 77/1000 | Loss: 0.00046027
Iteration 78/1000 | Loss: 0.00034478
Iteration 79/1000 | Loss: 0.00016880
Iteration 80/1000 | Loss: 0.00010012
Iteration 81/1000 | Loss: 0.00023058
Iteration 82/1000 | Loss: 0.00020425
Iteration 83/1000 | Loss: 0.00008504
Iteration 84/1000 | Loss: 0.00009269
Iteration 85/1000 | Loss: 0.00008840
Iteration 86/1000 | Loss: 0.00008057
Iteration 87/1000 | Loss: 0.00009555
Iteration 88/1000 | Loss: 0.00009152
Iteration 89/1000 | Loss: 0.00008723
Iteration 90/1000 | Loss: 0.00008962
Iteration 91/1000 | Loss: 0.00017872
Iteration 92/1000 | Loss: 0.00009710
Iteration 93/1000 | Loss: 0.00009377
Iteration 94/1000 | Loss: 0.00007716
Iteration 95/1000 | Loss: 0.00009027
Iteration 96/1000 | Loss: 0.00010634
Iteration 97/1000 | Loss: 0.00009738
Iteration 98/1000 | Loss: 0.00008719
Iteration 99/1000 | Loss: 0.00008537
Iteration 100/1000 | Loss: 0.00008712
Iteration 101/1000 | Loss: 0.00007811
Iteration 102/1000 | Loss: 0.00006916
Iteration 103/1000 | Loss: 0.00009470
Iteration 104/1000 | Loss: 0.00009222
Iteration 105/1000 | Loss: 0.00009257
Iteration 106/1000 | Loss: 0.00009194
Iteration 107/1000 | Loss: 0.00009376
Iteration 108/1000 | Loss: 0.00009308
Iteration 109/1000 | Loss: 0.00009482
Iteration 110/1000 | Loss: 0.00009476
Iteration 111/1000 | Loss: 0.00009573
Iteration 112/1000 | Loss: 0.00009696
Iteration 113/1000 | Loss: 0.00009496
Iteration 114/1000 | Loss: 0.00009099
Iteration 115/1000 | Loss: 0.00008444
Iteration 116/1000 | Loss: 0.00008442
Iteration 117/1000 | Loss: 0.00008433
Iteration 118/1000 | Loss: 0.00031971
Iteration 119/1000 | Loss: 0.00010067
Iteration 120/1000 | Loss: 0.00008508
Iteration 121/1000 | Loss: 0.00008312
Iteration 122/1000 | Loss: 0.00008999
Iteration 123/1000 | Loss: 0.00009082
Iteration 124/1000 | Loss: 0.00009409
Iteration 125/1000 | Loss: 0.00007769
Iteration 126/1000 | Loss: 0.00008535
Iteration 127/1000 | Loss: 0.00009537
Iteration 128/1000 | Loss: 0.00009538
Iteration 129/1000 | Loss: 0.00009528
Iteration 130/1000 | Loss: 0.00009508
Iteration 131/1000 | Loss: 0.00009140
Iteration 132/1000 | Loss: 0.00009495
Iteration 133/1000 | Loss: 0.00009341
Iteration 134/1000 | Loss: 0.00009320
Iteration 135/1000 | Loss: 0.00009299
Iteration 136/1000 | Loss: 0.00009252
Iteration 137/1000 | Loss: 0.00008695
Iteration 138/1000 | Loss: 0.00009108
Iteration 139/1000 | Loss: 0.00009500
Iteration 140/1000 | Loss: 0.00007796
Iteration 141/1000 | Loss: 0.00009753
Iteration 142/1000 | Loss: 0.00009457
Iteration 143/1000 | Loss: 0.00009417
Iteration 144/1000 | Loss: 0.00008426
Iteration 145/1000 | Loss: 0.00020773
Iteration 146/1000 | Loss: 0.00006558
Iteration 147/1000 | Loss: 0.00006951
Iteration 148/1000 | Loss: 0.00007832
Iteration 149/1000 | Loss: 0.00008274
Iteration 150/1000 | Loss: 0.00007233
Iteration 151/1000 | Loss: 0.00007845
Iteration 152/1000 | Loss: 0.00007873
Iteration 153/1000 | Loss: 0.00007845
Iteration 154/1000 | Loss: 0.00007864
Iteration 155/1000 | Loss: 0.00008155
Iteration 156/1000 | Loss: 0.00007849
Iteration 157/1000 | Loss: 0.00008263
Iteration 158/1000 | Loss: 0.00008091
Iteration 159/1000 | Loss: 0.00008417
Iteration 160/1000 | Loss: 0.00008209
Iteration 161/1000 | Loss: 0.00008617
Iteration 162/1000 | Loss: 0.00008331
Iteration 163/1000 | Loss: 0.00008308
Iteration 164/1000 | Loss: 0.00006662
Iteration 165/1000 | Loss: 0.00007714
Iteration 166/1000 | Loss: 0.00008252
Iteration 167/1000 | Loss: 0.00007692
Iteration 168/1000 | Loss: 0.00008223
Iteration 169/1000 | Loss: 0.00008051
Iteration 170/1000 | Loss: 0.00008646
Iteration 171/1000 | Loss: 0.00007796
Iteration 172/1000 | Loss: 0.00007892
Iteration 173/1000 | Loss: 0.00007766
Iteration 174/1000 | Loss: 0.00008420
Iteration 175/1000 | Loss: 0.00008470
Iteration 176/1000 | Loss: 0.00008353
Iteration 177/1000 | Loss: 0.00008440
Iteration 178/1000 | Loss: 0.00007967
Iteration 179/1000 | Loss: 0.00007845
Iteration 180/1000 | Loss: 0.00008515
Iteration 181/1000 | Loss: 0.00008712
Iteration 182/1000 | Loss: 0.00007381
Iteration 183/1000 | Loss: 0.00007716
Iteration 184/1000 | Loss: 0.00007975
Iteration 185/1000 | Loss: 0.00007873
Iteration 186/1000 | Loss: 0.00007892
Iteration 187/1000 | Loss: 0.00007861
Iteration 188/1000 | Loss: 0.00008693
Iteration 189/1000 | Loss: 0.00007897
Iteration 190/1000 | Loss: 0.00008566
Iteration 191/1000 | Loss: 0.00008969
Iteration 192/1000 | Loss: 0.00007813
Iteration 193/1000 | Loss: 0.00008039
Iteration 194/1000 | Loss: 0.00008563
Iteration 195/1000 | Loss: 0.00007509
Iteration 196/1000 | Loss: 0.00007760
Iteration 197/1000 | Loss: 0.00008926
Iteration 198/1000 | Loss: 0.00008439
Iteration 199/1000 | Loss: 0.00008150
Iteration 200/1000 | Loss: 0.00008378
Iteration 201/1000 | Loss: 0.00007764
Iteration 202/1000 | Loss: 0.00007500
Iteration 203/1000 | Loss: 0.00007062
Iteration 204/1000 | Loss: 0.00006854
Iteration 205/1000 | Loss: 0.00007327
Iteration 206/1000 | Loss: 0.00007993
Iteration 207/1000 | Loss: 0.00008129
Iteration 208/1000 | Loss: 0.00008316
Iteration 209/1000 | Loss: 0.00007394
Iteration 210/1000 | Loss: 0.00008019
Iteration 211/1000 | Loss: 0.00007440
Iteration 212/1000 | Loss: 0.00006621
Iteration 213/1000 | Loss: 0.00007636
Iteration 214/1000 | Loss: 0.00006533
Iteration 215/1000 | Loss: 0.00006336
Iteration 216/1000 | Loss: 0.00006235
Iteration 217/1000 | Loss: 0.00006164
Iteration 218/1000 | Loss: 0.00006056
Iteration 219/1000 | Loss: 0.00006005
Iteration 220/1000 | Loss: 0.00005988
Iteration 221/1000 | Loss: 0.00005967
Iteration 222/1000 | Loss: 0.00005953
Iteration 223/1000 | Loss: 0.00005935
Iteration 224/1000 | Loss: 0.00005909
Iteration 225/1000 | Loss: 0.00005902
Iteration 226/1000 | Loss: 0.00005879
Iteration 227/1000 | Loss: 0.00005863
Iteration 228/1000 | Loss: 0.00005851
Iteration 229/1000 | Loss: 0.00005849
Iteration 230/1000 | Loss: 0.00005841
Iteration 231/1000 | Loss: 0.00005841
Iteration 232/1000 | Loss: 0.00005841
Iteration 233/1000 | Loss: 0.00005841
Iteration 234/1000 | Loss: 0.00005841
Iteration 235/1000 | Loss: 0.00005841
Iteration 236/1000 | Loss: 0.00005841
Iteration 237/1000 | Loss: 0.00005841
Iteration 238/1000 | Loss: 0.00005841
Iteration 239/1000 | Loss: 0.00005841
Iteration 240/1000 | Loss: 0.00005840
Iteration 241/1000 | Loss: 0.00005840
Iteration 242/1000 | Loss: 0.00005840
Iteration 243/1000 | Loss: 0.00005840
Iteration 244/1000 | Loss: 0.00005840
Iteration 245/1000 | Loss: 0.00005840
Iteration 246/1000 | Loss: 0.00005839
Iteration 247/1000 | Loss: 0.00005839
Iteration 248/1000 | Loss: 0.00005839
Iteration 249/1000 | Loss: 0.00005838
Iteration 250/1000 | Loss: 0.00005837
Iteration 251/1000 | Loss: 0.00005837
Iteration 252/1000 | Loss: 0.00005837
Iteration 253/1000 | Loss: 0.00005837
Iteration 254/1000 | Loss: 0.00005837
Iteration 255/1000 | Loss: 0.00005837
Iteration 256/1000 | Loss: 0.00005836
Iteration 257/1000 | Loss: 0.00005836
Iteration 258/1000 | Loss: 0.00005836
Iteration 259/1000 | Loss: 0.00005836
Iteration 260/1000 | Loss: 0.00005836
Iteration 261/1000 | Loss: 0.00005836
Iteration 262/1000 | Loss: 0.00005836
Iteration 263/1000 | Loss: 0.00005836
Iteration 264/1000 | Loss: 0.00005836
Iteration 265/1000 | Loss: 0.00005836
Iteration 266/1000 | Loss: 0.00005835
Iteration 267/1000 | Loss: 0.00005834
Iteration 268/1000 | Loss: 0.00005834
Iteration 269/1000 | Loss: 0.00005834
Iteration 270/1000 | Loss: 0.00005833
Iteration 271/1000 | Loss: 0.00005833
Iteration 272/1000 | Loss: 0.00005833
Iteration 273/1000 | Loss: 0.00005833
Iteration 274/1000 | Loss: 0.00005833
Iteration 275/1000 | Loss: 0.00005833
Iteration 276/1000 | Loss: 0.00005833
Iteration 277/1000 | Loss: 0.00005833
Iteration 278/1000 | Loss: 0.00005833
Iteration 279/1000 | Loss: 0.00005832
Iteration 280/1000 | Loss: 0.00005831
Iteration 281/1000 | Loss: 0.00005831
Iteration 282/1000 | Loss: 0.00005831
Iteration 283/1000 | Loss: 0.00005831
Iteration 284/1000 | Loss: 0.00005831
Iteration 285/1000 | Loss: 0.00005830
Iteration 286/1000 | Loss: 0.00005830
Iteration 287/1000 | Loss: 0.00005830
Iteration 288/1000 | Loss: 0.00005830
Iteration 289/1000 | Loss: 0.00005830
Iteration 290/1000 | Loss: 0.00005830
Iteration 291/1000 | Loss: 0.00005830
Iteration 292/1000 | Loss: 0.00005830
Iteration 293/1000 | Loss: 0.00005829
Iteration 294/1000 | Loss: 0.00005829
Iteration 295/1000 | Loss: 0.00005829
Iteration 296/1000 | Loss: 0.00005829
Iteration 297/1000 | Loss: 0.00005829
Iteration 298/1000 | Loss: 0.00005829
Iteration 299/1000 | Loss: 0.00005829
Iteration 300/1000 | Loss: 0.00005829
Iteration 301/1000 | Loss: 0.00005829
Iteration 302/1000 | Loss: 0.00005829
Iteration 303/1000 | Loss: 0.00005829
Iteration 304/1000 | Loss: 0.00005829
Iteration 305/1000 | Loss: 0.00005829
Iteration 306/1000 | Loss: 0.00005828
Iteration 307/1000 | Loss: 0.00005828
Iteration 308/1000 | Loss: 0.00005828
Iteration 309/1000 | Loss: 0.00005828
Iteration 310/1000 | Loss: 0.00005828
Iteration 311/1000 | Loss: 0.00005828
Iteration 312/1000 | Loss: 0.00005828
Iteration 313/1000 | Loss: 0.00005828
Iteration 314/1000 | Loss: 0.00005828
Iteration 315/1000 | Loss: 0.00005828
Iteration 316/1000 | Loss: 0.00005828
Iteration 317/1000 | Loss: 0.00005827
Iteration 318/1000 | Loss: 0.00005827
Iteration 319/1000 | Loss: 0.00005827
Iteration 320/1000 | Loss: 0.00005827
Iteration 321/1000 | Loss: 0.00005827
Iteration 322/1000 | Loss: 0.00005827
Iteration 323/1000 | Loss: 0.00005827
Iteration 324/1000 | Loss: 0.00005827
Iteration 325/1000 | Loss: 0.00005827
Iteration 326/1000 | Loss: 0.00005827
Iteration 327/1000 | Loss: 0.00005827
Iteration 328/1000 | Loss: 0.00005826
Iteration 329/1000 | Loss: 0.00005826
Iteration 330/1000 | Loss: 0.00005826
Iteration 331/1000 | Loss: 0.00005826
Iteration 332/1000 | Loss: 0.00005826
Iteration 333/1000 | Loss: 0.00005825
Iteration 334/1000 | Loss: 0.00005825
Iteration 335/1000 | Loss: 0.00005825
Iteration 336/1000 | Loss: 0.00005825
Iteration 337/1000 | Loss: 0.00005824
Iteration 338/1000 | Loss: 0.00005824
Iteration 339/1000 | Loss: 0.00005824
Iteration 340/1000 | Loss: 0.00005824
Iteration 341/1000 | Loss: 0.00005824
Iteration 342/1000 | Loss: 0.00005824
Iteration 343/1000 | Loss: 0.00005824
Iteration 344/1000 | Loss: 0.00005824
Iteration 345/1000 | Loss: 0.00005823
Iteration 346/1000 | Loss: 0.00005823
Iteration 347/1000 | Loss: 0.00005823
Iteration 348/1000 | Loss: 0.00005823
Iteration 349/1000 | Loss: 0.00005823
Iteration 350/1000 | Loss: 0.00005823
Iteration 351/1000 | Loss: 0.00005823
Iteration 352/1000 | Loss: 0.00005823
Iteration 353/1000 | Loss: 0.00005823
Iteration 354/1000 | Loss: 0.00005823
Iteration 355/1000 | Loss: 0.00005823
Iteration 356/1000 | Loss: 0.00005823
Iteration 357/1000 | Loss: 0.00005823
Iteration 358/1000 | Loss: 0.00005823
Iteration 359/1000 | Loss: 0.00005823
Iteration 360/1000 | Loss: 0.00005823
Iteration 361/1000 | Loss: 0.00005823
Iteration 362/1000 | Loss: 0.00005823
Iteration 363/1000 | Loss: 0.00005823
Iteration 364/1000 | Loss: 0.00005823
Iteration 365/1000 | Loss: 0.00005823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [5.823003448313102e-05, 5.823003448313102e-05, 5.823003448313102e-05, 5.823003448313102e-05, 5.823003448313102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.823003448313102e-05

Optimization complete. Final v2v error: 5.7362871170043945 mm

Highest mean error: 7.190606594085693 mm for frame 188

Lowest mean error: 3.7239744663238525 mm for frame 58

Saving results

Total time: 417.9926767349243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905253
Iteration 2/25 | Loss: 0.00116046
Iteration 3/25 | Loss: 0.00103974
Iteration 4/25 | Loss: 0.00101691
Iteration 5/25 | Loss: 0.00100885
Iteration 6/25 | Loss: 0.00100652
Iteration 7/25 | Loss: 0.00100618
Iteration 8/25 | Loss: 0.00100618
Iteration 9/25 | Loss: 0.00100618
Iteration 10/25 | Loss: 0.00100618
Iteration 11/25 | Loss: 0.00100618
Iteration 12/25 | Loss: 0.00100618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010061762295663357, 0.0010061762295663357, 0.0010061762295663357, 0.0010061762295663357, 0.0010061762295663357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010061762295663357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30468893
Iteration 2/25 | Loss: 0.00114233
Iteration 3/25 | Loss: 0.00114231
Iteration 4/25 | Loss: 0.00114231
Iteration 5/25 | Loss: 0.00114231
Iteration 6/25 | Loss: 0.00114231
Iteration 7/25 | Loss: 0.00114231
Iteration 8/25 | Loss: 0.00114231
Iteration 9/25 | Loss: 0.00114231
Iteration 10/25 | Loss: 0.00114231
Iteration 11/25 | Loss: 0.00114231
Iteration 12/25 | Loss: 0.00114231
Iteration 13/25 | Loss: 0.00114231
Iteration 14/25 | Loss: 0.00114231
Iteration 15/25 | Loss: 0.00114231
Iteration 16/25 | Loss: 0.00114231
Iteration 17/25 | Loss: 0.00114231
Iteration 18/25 | Loss: 0.00114231
Iteration 19/25 | Loss: 0.00114231
Iteration 20/25 | Loss: 0.00114231
Iteration 21/25 | Loss: 0.00114231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001142307766713202, 0.001142307766713202, 0.001142307766713202, 0.001142307766713202, 0.001142307766713202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001142307766713202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114231
Iteration 2/1000 | Loss: 0.00004601
Iteration 3/1000 | Loss: 0.00003234
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002311
Iteration 6/1000 | Loss: 0.00002180
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001952
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001902
Iteration 12/1000 | Loss: 0.00001892
Iteration 13/1000 | Loss: 0.00001887
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001862
Iteration 18/1000 | Loss: 0.00001861
Iteration 19/1000 | Loss: 0.00001860
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001853
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001850
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001843
Iteration 51/1000 | Loss: 0.00001842
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001840
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001839
Iteration 64/1000 | Loss: 0.00001838
Iteration 65/1000 | Loss: 0.00001838
Iteration 66/1000 | Loss: 0.00001838
Iteration 67/1000 | Loss: 0.00001838
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001838
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001837
Iteration 72/1000 | Loss: 0.00001837
Iteration 73/1000 | Loss: 0.00001837
Iteration 74/1000 | Loss: 0.00001837
Iteration 75/1000 | Loss: 0.00001837
Iteration 76/1000 | Loss: 0.00001836
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001835
Iteration 79/1000 | Loss: 0.00001835
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001833
Iteration 83/1000 | Loss: 0.00001832
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001827
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001827
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001825
Iteration 117/1000 | Loss: 0.00001825
Iteration 118/1000 | Loss: 0.00001825
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001825
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00001824
Iteration 124/1000 | Loss: 0.00001824
Iteration 125/1000 | Loss: 0.00001824
Iteration 126/1000 | Loss: 0.00001824
Iteration 127/1000 | Loss: 0.00001824
Iteration 128/1000 | Loss: 0.00001824
Iteration 129/1000 | Loss: 0.00001824
Iteration 130/1000 | Loss: 0.00001824
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001823
Iteration 143/1000 | Loss: 0.00001823
Iteration 144/1000 | Loss: 0.00001823
Iteration 145/1000 | Loss: 0.00001823
Iteration 146/1000 | Loss: 0.00001823
Iteration 147/1000 | Loss: 0.00001823
Iteration 148/1000 | Loss: 0.00001823
Iteration 149/1000 | Loss: 0.00001823
Iteration 150/1000 | Loss: 0.00001823
Iteration 151/1000 | Loss: 0.00001823
Iteration 152/1000 | Loss: 0.00001823
Iteration 153/1000 | Loss: 0.00001823
Iteration 154/1000 | Loss: 0.00001823
Iteration 155/1000 | Loss: 0.00001823
Iteration 156/1000 | Loss: 0.00001823
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001823
Iteration 160/1000 | Loss: 0.00001823
Iteration 161/1000 | Loss: 0.00001823
Iteration 162/1000 | Loss: 0.00001823
Iteration 163/1000 | Loss: 0.00001823
Iteration 164/1000 | Loss: 0.00001823
Iteration 165/1000 | Loss: 0.00001823
Iteration 166/1000 | Loss: 0.00001823
Iteration 167/1000 | Loss: 0.00001823
Iteration 168/1000 | Loss: 0.00001823
Iteration 169/1000 | Loss: 0.00001823
Iteration 170/1000 | Loss: 0.00001823
Iteration 171/1000 | Loss: 0.00001823
Iteration 172/1000 | Loss: 0.00001823
Iteration 173/1000 | Loss: 0.00001823
Iteration 174/1000 | Loss: 0.00001823
Iteration 175/1000 | Loss: 0.00001823
Iteration 176/1000 | Loss: 0.00001823
Iteration 177/1000 | Loss: 0.00001823
Iteration 178/1000 | Loss: 0.00001823
Iteration 179/1000 | Loss: 0.00001823
Iteration 180/1000 | Loss: 0.00001823
Iteration 181/1000 | Loss: 0.00001823
Iteration 182/1000 | Loss: 0.00001823
Iteration 183/1000 | Loss: 0.00001823
Iteration 184/1000 | Loss: 0.00001823
Iteration 185/1000 | Loss: 0.00001823
Iteration 186/1000 | Loss: 0.00001823
Iteration 187/1000 | Loss: 0.00001823
Iteration 188/1000 | Loss: 0.00001823
Iteration 189/1000 | Loss: 0.00001823
Iteration 190/1000 | Loss: 0.00001823
Iteration 191/1000 | Loss: 0.00001823
Iteration 192/1000 | Loss: 0.00001823
Iteration 193/1000 | Loss: 0.00001823
Iteration 194/1000 | Loss: 0.00001823
Iteration 195/1000 | Loss: 0.00001823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.8225809981231578e-05, 1.8225809981231578e-05, 1.8225809981231578e-05, 1.8225809981231578e-05, 1.8225809981231578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8225809981231578e-05

Optimization complete. Final v2v error: 3.612388849258423 mm

Highest mean error: 5.484451770782471 mm for frame 69

Lowest mean error: 2.987103223800659 mm for frame 133

Saving results

Total time: 39.51419925689697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696122
Iteration 2/25 | Loss: 0.00111231
Iteration 3/25 | Loss: 0.00099165
Iteration 4/25 | Loss: 0.00096964
Iteration 5/25 | Loss: 0.00096287
Iteration 6/25 | Loss: 0.00096116
Iteration 7/25 | Loss: 0.00096116
Iteration 8/25 | Loss: 0.00096116
Iteration 9/25 | Loss: 0.00096116
Iteration 10/25 | Loss: 0.00096116
Iteration 11/25 | Loss: 0.00096116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009611558634787798, 0.0009611558634787798, 0.0009611558634787798, 0.0009611558634787798, 0.0009611558634787798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009611558634787798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.90262079
Iteration 2/25 | Loss: 0.00149280
Iteration 3/25 | Loss: 0.00149279
Iteration 4/25 | Loss: 0.00149279
Iteration 5/25 | Loss: 0.00149279
Iteration 6/25 | Loss: 0.00149279
Iteration 7/25 | Loss: 0.00149279
Iteration 8/25 | Loss: 0.00149279
Iteration 9/25 | Loss: 0.00149279
Iteration 10/25 | Loss: 0.00149279
Iteration 11/25 | Loss: 0.00149279
Iteration 12/25 | Loss: 0.00149279
Iteration 13/25 | Loss: 0.00149279
Iteration 14/25 | Loss: 0.00149279
Iteration 15/25 | Loss: 0.00149279
Iteration 16/25 | Loss: 0.00149279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014927921583876014, 0.0014927921583876014, 0.0014927921583876014, 0.0014927921583876014, 0.0014927921583876014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014927921583876014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149279
Iteration 2/1000 | Loss: 0.00003373
Iteration 3/1000 | Loss: 0.00002103
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001218
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001215
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001206
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001174
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.1690676728903782e-05, 1.1690676728903782e-05, 1.1690676728903782e-05, 1.1690676728903782e-05, 1.1690676728903782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1690676728903782e-05

Optimization complete. Final v2v error: 3.0144670009613037 mm

Highest mean error: 3.3654205799102783 mm for frame 68

Lowest mean error: 2.7368104457855225 mm for frame 176

Saving results

Total time: 42.77712893486023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443164
Iteration 2/25 | Loss: 0.00106977
Iteration 3/25 | Loss: 0.00097752
Iteration 4/25 | Loss: 0.00096611
Iteration 5/25 | Loss: 0.00096327
Iteration 6/25 | Loss: 0.00096239
Iteration 7/25 | Loss: 0.00096239
Iteration 8/25 | Loss: 0.00096239
Iteration 9/25 | Loss: 0.00096239
Iteration 10/25 | Loss: 0.00096239
Iteration 11/25 | Loss: 0.00096239
Iteration 12/25 | Loss: 0.00096239
Iteration 13/25 | Loss: 0.00096239
Iteration 14/25 | Loss: 0.00096239
Iteration 15/25 | Loss: 0.00096239
Iteration 16/25 | Loss: 0.00096239
Iteration 17/25 | Loss: 0.00096239
Iteration 18/25 | Loss: 0.00096239
Iteration 19/25 | Loss: 0.00096239
Iteration 20/25 | Loss: 0.00096239
Iteration 21/25 | Loss: 0.00096239
Iteration 22/25 | Loss: 0.00096239
Iteration 23/25 | Loss: 0.00096239
Iteration 24/25 | Loss: 0.00096239
Iteration 25/25 | Loss: 0.00096239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22906685
Iteration 2/25 | Loss: 0.00106197
Iteration 3/25 | Loss: 0.00106197
Iteration 4/25 | Loss: 0.00106197
Iteration 5/25 | Loss: 0.00106197
Iteration 6/25 | Loss: 0.00106197
Iteration 7/25 | Loss: 0.00106197
Iteration 8/25 | Loss: 0.00106197
Iteration 9/25 | Loss: 0.00106197
Iteration 10/25 | Loss: 0.00106197
Iteration 11/25 | Loss: 0.00106197
Iteration 12/25 | Loss: 0.00106197
Iteration 13/25 | Loss: 0.00106197
Iteration 14/25 | Loss: 0.00106197
Iteration 15/25 | Loss: 0.00106197
Iteration 16/25 | Loss: 0.00106197
Iteration 17/25 | Loss: 0.00106197
Iteration 18/25 | Loss: 0.00106197
Iteration 19/25 | Loss: 0.00106197
Iteration 20/25 | Loss: 0.00106197
Iteration 21/25 | Loss: 0.00106197
Iteration 22/25 | Loss: 0.00106197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010619692038744688, 0.0010619692038744688, 0.0010619692038744688, 0.0010619692038744688, 0.0010619692038744688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010619692038744688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106197
Iteration 2/1000 | Loss: 0.00003080
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001561
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001390
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001295
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001238
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001233
Iteration 20/1000 | Loss: 0.00001233
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001228
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001205
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001185
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001183
Iteration 121/1000 | Loss: 0.00001183
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001181
Iteration 126/1000 | Loss: 0.00001181
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001180
Iteration 132/1000 | Loss: 0.00001180
Iteration 133/1000 | Loss: 0.00001180
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001174
Iteration 141/1000 | Loss: 0.00001174
Iteration 142/1000 | Loss: 0.00001174
Iteration 143/1000 | Loss: 0.00001173
Iteration 144/1000 | Loss: 0.00001173
Iteration 145/1000 | Loss: 0.00001173
Iteration 146/1000 | Loss: 0.00001172
Iteration 147/1000 | Loss: 0.00001172
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001170
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001168
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001168
Iteration 164/1000 | Loss: 0.00001168
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001167
Iteration 167/1000 | Loss: 0.00001167
Iteration 168/1000 | Loss: 0.00001167
Iteration 169/1000 | Loss: 0.00001167
Iteration 170/1000 | Loss: 0.00001167
Iteration 171/1000 | Loss: 0.00001167
Iteration 172/1000 | Loss: 0.00001167
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001165
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001165
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001165
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001165
Iteration 194/1000 | Loss: 0.00001165
Iteration 195/1000 | Loss: 0.00001165
Iteration 196/1000 | Loss: 0.00001165
Iteration 197/1000 | Loss: 0.00001165
Iteration 198/1000 | Loss: 0.00001165
Iteration 199/1000 | Loss: 0.00001164
Iteration 200/1000 | Loss: 0.00001164
Iteration 201/1000 | Loss: 0.00001164
Iteration 202/1000 | Loss: 0.00001164
Iteration 203/1000 | Loss: 0.00001164
Iteration 204/1000 | Loss: 0.00001164
Iteration 205/1000 | Loss: 0.00001164
Iteration 206/1000 | Loss: 0.00001164
Iteration 207/1000 | Loss: 0.00001164
Iteration 208/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.164391142083332e-05, 1.164391142083332e-05, 1.164391142083332e-05, 1.164391142083332e-05, 1.164391142083332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.164391142083332e-05

Optimization complete. Final v2v error: 3.0017080307006836 mm

Highest mean error: 4.848814964294434 mm for frame 221

Lowest mean error: 2.4876034259796143 mm for frame 22

Saving results

Total time: 50.32882022857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603565
Iteration 2/25 | Loss: 0.00161081
Iteration 3/25 | Loss: 0.00114671
Iteration 4/25 | Loss: 0.00112288
Iteration 5/25 | Loss: 0.00111560
Iteration 6/25 | Loss: 0.00111336
Iteration 7/25 | Loss: 0.00111332
Iteration 8/25 | Loss: 0.00111332
Iteration 9/25 | Loss: 0.00111332
Iteration 10/25 | Loss: 0.00111332
Iteration 11/25 | Loss: 0.00111332
Iteration 12/25 | Loss: 0.00111332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011133161606267095, 0.0011133161606267095, 0.0011133161606267095, 0.0011133161606267095, 0.0011133161606267095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011133161606267095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87577111
Iteration 2/25 | Loss: 0.00116381
Iteration 3/25 | Loss: 0.00116380
Iteration 4/25 | Loss: 0.00116380
Iteration 5/25 | Loss: 0.00116380
Iteration 6/25 | Loss: 0.00116380
Iteration 7/25 | Loss: 0.00116380
Iteration 8/25 | Loss: 0.00116380
Iteration 9/25 | Loss: 0.00116380
Iteration 10/25 | Loss: 0.00116380
Iteration 11/25 | Loss: 0.00116380
Iteration 12/25 | Loss: 0.00116380
Iteration 13/25 | Loss: 0.00116379
Iteration 14/25 | Loss: 0.00116379
Iteration 15/25 | Loss: 0.00116379
Iteration 16/25 | Loss: 0.00116379
Iteration 17/25 | Loss: 0.00116379
Iteration 18/25 | Loss: 0.00116379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011637948919087648, 0.0011637948919087648, 0.0011637948919087648, 0.0011637948919087648, 0.0011637948919087648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011637948919087648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116379
Iteration 2/1000 | Loss: 0.00005762
Iteration 3/1000 | Loss: 0.00004128
Iteration 4/1000 | Loss: 0.00003464
Iteration 5/1000 | Loss: 0.00003292
Iteration 6/1000 | Loss: 0.00003207
Iteration 7/1000 | Loss: 0.00003152
Iteration 8/1000 | Loss: 0.00003103
Iteration 9/1000 | Loss: 0.00003067
Iteration 10/1000 | Loss: 0.00003050
Iteration 11/1000 | Loss: 0.00003024
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00002966
Iteration 14/1000 | Loss: 0.00002941
Iteration 15/1000 | Loss: 0.00002920
Iteration 16/1000 | Loss: 0.00002900
Iteration 17/1000 | Loss: 0.00002900
Iteration 18/1000 | Loss: 0.00002880
Iteration 19/1000 | Loss: 0.00002874
Iteration 20/1000 | Loss: 0.00002859
Iteration 21/1000 | Loss: 0.00002853
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002844
Iteration 24/1000 | Loss: 0.00002844
Iteration 25/1000 | Loss: 0.00002843
Iteration 26/1000 | Loss: 0.00002842
Iteration 27/1000 | Loss: 0.00002842
Iteration 28/1000 | Loss: 0.00002842
Iteration 29/1000 | Loss: 0.00002842
Iteration 30/1000 | Loss: 0.00002842
Iteration 31/1000 | Loss: 0.00002839
Iteration 32/1000 | Loss: 0.00002839
Iteration 33/1000 | Loss: 0.00002838
Iteration 34/1000 | Loss: 0.00002835
Iteration 35/1000 | Loss: 0.00002835
Iteration 36/1000 | Loss: 0.00002835
Iteration 37/1000 | Loss: 0.00002834
Iteration 38/1000 | Loss: 0.00002834
Iteration 39/1000 | Loss: 0.00002833
Iteration 40/1000 | Loss: 0.00002833
Iteration 41/1000 | Loss: 0.00002833
Iteration 42/1000 | Loss: 0.00002833
Iteration 43/1000 | Loss: 0.00002833
Iteration 44/1000 | Loss: 0.00002833
Iteration 45/1000 | Loss: 0.00002832
Iteration 46/1000 | Loss: 0.00002832
Iteration 47/1000 | Loss: 0.00002832
Iteration 48/1000 | Loss: 0.00002832
Iteration 49/1000 | Loss: 0.00002832
Iteration 50/1000 | Loss: 0.00002832
Iteration 51/1000 | Loss: 0.00002832
Iteration 52/1000 | Loss: 0.00002831
Iteration 53/1000 | Loss: 0.00002831
Iteration 54/1000 | Loss: 0.00002831
Iteration 55/1000 | Loss: 0.00002831
Iteration 56/1000 | Loss: 0.00002831
Iteration 57/1000 | Loss: 0.00002831
Iteration 58/1000 | Loss: 0.00002831
Iteration 59/1000 | Loss: 0.00002831
Iteration 60/1000 | Loss: 0.00002831
Iteration 61/1000 | Loss: 0.00002830
Iteration 62/1000 | Loss: 0.00002830
Iteration 63/1000 | Loss: 0.00002830
Iteration 64/1000 | Loss: 0.00002830
Iteration 65/1000 | Loss: 0.00002830
Iteration 66/1000 | Loss: 0.00002829
Iteration 67/1000 | Loss: 0.00002829
Iteration 68/1000 | Loss: 0.00002829
Iteration 69/1000 | Loss: 0.00002829
Iteration 70/1000 | Loss: 0.00002829
Iteration 71/1000 | Loss: 0.00002829
Iteration 72/1000 | Loss: 0.00002829
Iteration 73/1000 | Loss: 0.00002828
Iteration 74/1000 | Loss: 0.00002828
Iteration 75/1000 | Loss: 0.00002827
Iteration 76/1000 | Loss: 0.00002827
Iteration 77/1000 | Loss: 0.00002827
Iteration 78/1000 | Loss: 0.00002827
Iteration 79/1000 | Loss: 0.00002826
Iteration 80/1000 | Loss: 0.00002826
Iteration 81/1000 | Loss: 0.00002826
Iteration 82/1000 | Loss: 0.00002826
Iteration 83/1000 | Loss: 0.00002826
Iteration 84/1000 | Loss: 0.00002826
Iteration 85/1000 | Loss: 0.00002826
Iteration 86/1000 | Loss: 0.00002826
Iteration 87/1000 | Loss: 0.00002825
Iteration 88/1000 | Loss: 0.00002825
Iteration 89/1000 | Loss: 0.00002825
Iteration 90/1000 | Loss: 0.00002825
Iteration 91/1000 | Loss: 0.00002825
Iteration 92/1000 | Loss: 0.00002825
Iteration 93/1000 | Loss: 0.00002825
Iteration 94/1000 | Loss: 0.00002825
Iteration 95/1000 | Loss: 0.00002824
Iteration 96/1000 | Loss: 0.00002824
Iteration 97/1000 | Loss: 0.00002824
Iteration 98/1000 | Loss: 0.00002824
Iteration 99/1000 | Loss: 0.00002824
Iteration 100/1000 | Loss: 0.00002824
Iteration 101/1000 | Loss: 0.00002824
Iteration 102/1000 | Loss: 0.00002824
Iteration 103/1000 | Loss: 0.00002824
Iteration 104/1000 | Loss: 0.00002824
Iteration 105/1000 | Loss: 0.00002824
Iteration 106/1000 | Loss: 0.00002823
Iteration 107/1000 | Loss: 0.00002823
Iteration 108/1000 | Loss: 0.00002823
Iteration 109/1000 | Loss: 0.00002823
Iteration 110/1000 | Loss: 0.00002823
Iteration 111/1000 | Loss: 0.00002823
Iteration 112/1000 | Loss: 0.00002823
Iteration 113/1000 | Loss: 0.00002823
Iteration 114/1000 | Loss: 0.00002823
Iteration 115/1000 | Loss: 0.00002823
Iteration 116/1000 | Loss: 0.00002823
Iteration 117/1000 | Loss: 0.00002823
Iteration 118/1000 | Loss: 0.00002823
Iteration 119/1000 | Loss: 0.00002823
Iteration 120/1000 | Loss: 0.00002823
Iteration 121/1000 | Loss: 0.00002823
Iteration 122/1000 | Loss: 0.00002822
Iteration 123/1000 | Loss: 0.00002822
Iteration 124/1000 | Loss: 0.00002822
Iteration 125/1000 | Loss: 0.00002822
Iteration 126/1000 | Loss: 0.00002822
Iteration 127/1000 | Loss: 0.00002822
Iteration 128/1000 | Loss: 0.00002822
Iteration 129/1000 | Loss: 0.00002822
Iteration 130/1000 | Loss: 0.00002822
Iteration 131/1000 | Loss: 0.00002822
Iteration 132/1000 | Loss: 0.00002822
Iteration 133/1000 | Loss: 0.00002822
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002822
Iteration 136/1000 | Loss: 0.00002822
Iteration 137/1000 | Loss: 0.00002822
Iteration 138/1000 | Loss: 0.00002822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.821985435730312e-05, 2.821985435730312e-05, 2.821985435730312e-05, 2.821985435730312e-05, 2.821985435730312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.821985435730312e-05

Optimization complete. Final v2v error: 4.035799980163574 mm

Highest mean error: 5.000775337219238 mm for frame 95

Lowest mean error: 2.937135934829712 mm for frame 1

Saving results

Total time: 42.775797605514526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00174908
Iteration 2/25 | Loss: 0.00099550
Iteration 3/25 | Loss: 0.00093411
Iteration 4/25 | Loss: 0.00092087
Iteration 5/25 | Loss: 0.00091477
Iteration 6/25 | Loss: 0.00091307
Iteration 7/25 | Loss: 0.00091307
Iteration 8/25 | Loss: 0.00091307
Iteration 9/25 | Loss: 0.00091307
Iteration 10/25 | Loss: 0.00091307
Iteration 11/25 | Loss: 0.00091307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009130737744271755, 0.0009130737744271755, 0.0009130737744271755, 0.0009130737744271755, 0.0009130737744271755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009130737744271755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26821709
Iteration 2/25 | Loss: 0.00193434
Iteration 3/25 | Loss: 0.00193434
Iteration 4/25 | Loss: 0.00193434
Iteration 5/25 | Loss: 0.00193434
Iteration 6/25 | Loss: 0.00193434
Iteration 7/25 | Loss: 0.00193434
Iteration 8/25 | Loss: 0.00193434
Iteration 9/25 | Loss: 0.00193434
Iteration 10/25 | Loss: 0.00193434
Iteration 11/25 | Loss: 0.00193434
Iteration 12/25 | Loss: 0.00193434
Iteration 13/25 | Loss: 0.00193434
Iteration 14/25 | Loss: 0.00193434
Iteration 15/25 | Loss: 0.00193434
Iteration 16/25 | Loss: 0.00193434
Iteration 17/25 | Loss: 0.00193434
Iteration 18/25 | Loss: 0.00193434
Iteration 19/25 | Loss: 0.00193434
Iteration 20/25 | Loss: 0.00193434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019343357998877764, 0.0019343357998877764, 0.0019343357998877764, 0.0019343357998877764, 0.0019343357998877764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019343357998877764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193434
Iteration 2/1000 | Loss: 0.00003331
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001289
Iteration 6/1000 | Loss: 0.00001126
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001034
Iteration 9/1000 | Loss: 0.00001003
Iteration 10/1000 | Loss: 0.00000990
Iteration 11/1000 | Loss: 0.00000976
Iteration 12/1000 | Loss: 0.00000962
Iteration 13/1000 | Loss: 0.00000951
Iteration 14/1000 | Loss: 0.00000947
Iteration 15/1000 | Loss: 0.00000947
Iteration 16/1000 | Loss: 0.00000946
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000930
Iteration 19/1000 | Loss: 0.00000915
Iteration 20/1000 | Loss: 0.00000912
Iteration 21/1000 | Loss: 0.00000912
Iteration 22/1000 | Loss: 0.00000911
Iteration 23/1000 | Loss: 0.00000911
Iteration 24/1000 | Loss: 0.00000911
Iteration 25/1000 | Loss: 0.00000911
Iteration 26/1000 | Loss: 0.00000911
Iteration 27/1000 | Loss: 0.00000911
Iteration 28/1000 | Loss: 0.00000911
Iteration 29/1000 | Loss: 0.00000910
Iteration 30/1000 | Loss: 0.00000908
Iteration 31/1000 | Loss: 0.00000908
Iteration 32/1000 | Loss: 0.00000908
Iteration 33/1000 | Loss: 0.00000908
Iteration 34/1000 | Loss: 0.00000908
Iteration 35/1000 | Loss: 0.00000907
Iteration 36/1000 | Loss: 0.00000907
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000907
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000907
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000906
Iteration 44/1000 | Loss: 0.00000906
Iteration 45/1000 | Loss: 0.00000906
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000905
Iteration 48/1000 | Loss: 0.00000905
Iteration 49/1000 | Loss: 0.00000905
Iteration 50/1000 | Loss: 0.00000904
Iteration 51/1000 | Loss: 0.00000904
Iteration 52/1000 | Loss: 0.00000904
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000903
Iteration 56/1000 | Loss: 0.00000903
Iteration 57/1000 | Loss: 0.00000903
Iteration 58/1000 | Loss: 0.00000903
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000903
Iteration 61/1000 | Loss: 0.00000902
Iteration 62/1000 | Loss: 0.00000902
Iteration 63/1000 | Loss: 0.00000901
Iteration 64/1000 | Loss: 0.00000901
Iteration 65/1000 | Loss: 0.00000901
Iteration 66/1000 | Loss: 0.00000901
Iteration 67/1000 | Loss: 0.00000900
Iteration 68/1000 | Loss: 0.00000900
Iteration 69/1000 | Loss: 0.00000899
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000899
Iteration 73/1000 | Loss: 0.00000899
Iteration 74/1000 | Loss: 0.00000899
Iteration 75/1000 | Loss: 0.00000898
Iteration 76/1000 | Loss: 0.00000898
Iteration 77/1000 | Loss: 0.00000898
Iteration 78/1000 | Loss: 0.00000897
Iteration 79/1000 | Loss: 0.00000897
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000897
Iteration 82/1000 | Loss: 0.00000897
Iteration 83/1000 | Loss: 0.00000897
Iteration 84/1000 | Loss: 0.00000896
Iteration 85/1000 | Loss: 0.00000896
Iteration 86/1000 | Loss: 0.00000896
Iteration 87/1000 | Loss: 0.00000896
Iteration 88/1000 | Loss: 0.00000896
Iteration 89/1000 | Loss: 0.00000895
Iteration 90/1000 | Loss: 0.00000895
Iteration 91/1000 | Loss: 0.00000895
Iteration 92/1000 | Loss: 0.00000895
Iteration 93/1000 | Loss: 0.00000895
Iteration 94/1000 | Loss: 0.00000895
Iteration 95/1000 | Loss: 0.00000895
Iteration 96/1000 | Loss: 0.00000895
Iteration 97/1000 | Loss: 0.00000895
Iteration 98/1000 | Loss: 0.00000895
Iteration 99/1000 | Loss: 0.00000895
Iteration 100/1000 | Loss: 0.00000895
Iteration 101/1000 | Loss: 0.00000895
Iteration 102/1000 | Loss: 0.00000895
Iteration 103/1000 | Loss: 0.00000895
Iteration 104/1000 | Loss: 0.00000895
Iteration 105/1000 | Loss: 0.00000895
Iteration 106/1000 | Loss: 0.00000895
Iteration 107/1000 | Loss: 0.00000895
Iteration 108/1000 | Loss: 0.00000895
Iteration 109/1000 | Loss: 0.00000895
Iteration 110/1000 | Loss: 0.00000895
Iteration 111/1000 | Loss: 0.00000895
Iteration 112/1000 | Loss: 0.00000895
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [8.951516974775586e-06, 8.951516974775586e-06, 8.951516974775586e-06, 8.951516974775586e-06, 8.951516974775586e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.951516974775586e-06

Optimization complete. Final v2v error: 2.60624098777771 mm

Highest mean error: 2.892258882522583 mm for frame 40

Lowest mean error: 2.336129665374756 mm for frame 7

Saving results

Total time: 38.15199112892151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043940
Iteration 2/25 | Loss: 0.00185343
Iteration 3/25 | Loss: 0.00162867
Iteration 4/25 | Loss: 0.00130661
Iteration 5/25 | Loss: 0.00125740
Iteration 6/25 | Loss: 0.00103134
Iteration 7/25 | Loss: 0.00102659
Iteration 8/25 | Loss: 0.00103468
Iteration 9/25 | Loss: 0.00102982
Iteration 10/25 | Loss: 0.00102200
Iteration 11/25 | Loss: 0.00101855
Iteration 12/25 | Loss: 0.00101743
Iteration 13/25 | Loss: 0.00102238
Iteration 14/25 | Loss: 0.00102338
Iteration 15/25 | Loss: 0.00101484
Iteration 16/25 | Loss: 0.00101823
Iteration 17/25 | Loss: 0.00101334
Iteration 18/25 | Loss: 0.00101661
Iteration 19/25 | Loss: 0.00100764
Iteration 20/25 | Loss: 0.00100279
Iteration 21/25 | Loss: 0.00100796
Iteration 22/25 | Loss: 0.00100433
Iteration 23/25 | Loss: 0.00100240
Iteration 24/25 | Loss: 0.00100102
Iteration 25/25 | Loss: 0.00099901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24359620
Iteration 2/25 | Loss: 0.00128801
Iteration 3/25 | Loss: 0.00128801
Iteration 4/25 | Loss: 0.00128801
Iteration 5/25 | Loss: 0.00128801
Iteration 6/25 | Loss: 0.00128801
Iteration 7/25 | Loss: 0.00128801
Iteration 8/25 | Loss: 0.00128801
Iteration 9/25 | Loss: 0.00128801
Iteration 10/25 | Loss: 0.00128801
Iteration 11/25 | Loss: 0.00128801
Iteration 12/25 | Loss: 0.00128801
Iteration 13/25 | Loss: 0.00128801
Iteration 14/25 | Loss: 0.00128801
Iteration 15/25 | Loss: 0.00128801
Iteration 16/25 | Loss: 0.00128801
Iteration 17/25 | Loss: 0.00128801
Iteration 18/25 | Loss: 0.00128801
Iteration 19/25 | Loss: 0.00128801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012880052672699094, 0.0012880052672699094, 0.0012880052672699094, 0.0012880052672699094, 0.0012880052672699094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012880052672699094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128801
Iteration 2/1000 | Loss: 0.00024054
Iteration 3/1000 | Loss: 0.00013959
Iteration 4/1000 | Loss: 0.00020017
Iteration 5/1000 | Loss: 0.00010482
Iteration 6/1000 | Loss: 0.00001999
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00010049
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001393
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001232
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001206
Iteration 33/1000 | Loss: 0.00001206
Iteration 34/1000 | Loss: 0.00001206
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001205
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00008889
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001187
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1858072866743896e-05, 1.1858072866743896e-05, 1.1858072866743896e-05, 1.1858072866743896e-05, 1.1858072866743896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1858072866743896e-05

Optimization complete. Final v2v error: 2.969759702682495 mm

Highest mean error: 3.4695515632629395 mm for frame 58

Lowest mean error: 2.60978102684021 mm for frame 52

Saving results

Total time: 77.32112979888916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959506
Iteration 2/25 | Loss: 0.00157049
Iteration 3/25 | Loss: 0.00118800
Iteration 4/25 | Loss: 0.00116475
Iteration 5/25 | Loss: 0.00115812
Iteration 6/25 | Loss: 0.00115740
Iteration 7/25 | Loss: 0.00115740
Iteration 8/25 | Loss: 0.00115740
Iteration 9/25 | Loss: 0.00115740
Iteration 10/25 | Loss: 0.00115740
Iteration 11/25 | Loss: 0.00115740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011574029922485352, 0.0011574029922485352, 0.0011574029922485352, 0.0011574029922485352, 0.0011574029922485352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011574029922485352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79506987
Iteration 2/25 | Loss: 0.00121569
Iteration 3/25 | Loss: 0.00121569
Iteration 4/25 | Loss: 0.00121569
Iteration 5/25 | Loss: 0.00121569
Iteration 6/25 | Loss: 0.00121569
Iteration 7/25 | Loss: 0.00121569
Iteration 8/25 | Loss: 0.00121569
Iteration 9/25 | Loss: 0.00121569
Iteration 10/25 | Loss: 0.00121569
Iteration 11/25 | Loss: 0.00121569
Iteration 12/25 | Loss: 0.00121569
Iteration 13/25 | Loss: 0.00121569
Iteration 14/25 | Loss: 0.00121569
Iteration 15/25 | Loss: 0.00121569
Iteration 16/25 | Loss: 0.00121569
Iteration 17/25 | Loss: 0.00121569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012156885350123048, 0.0012156885350123048, 0.0012156885350123048, 0.0012156885350123048, 0.0012156885350123048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012156885350123048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121569
Iteration 2/1000 | Loss: 0.00006342
Iteration 3/1000 | Loss: 0.00004443
Iteration 4/1000 | Loss: 0.00003747
Iteration 5/1000 | Loss: 0.00003489
Iteration 6/1000 | Loss: 0.00003387
Iteration 7/1000 | Loss: 0.00003309
Iteration 8/1000 | Loss: 0.00003238
Iteration 9/1000 | Loss: 0.00003196
Iteration 10/1000 | Loss: 0.00003167
Iteration 11/1000 | Loss: 0.00003142
Iteration 12/1000 | Loss: 0.00003118
Iteration 13/1000 | Loss: 0.00003095
Iteration 14/1000 | Loss: 0.00003071
Iteration 15/1000 | Loss: 0.00003049
Iteration 16/1000 | Loss: 0.00003027
Iteration 17/1000 | Loss: 0.00003011
Iteration 18/1000 | Loss: 0.00003005
Iteration 19/1000 | Loss: 0.00003005
Iteration 20/1000 | Loss: 0.00002999
Iteration 21/1000 | Loss: 0.00002987
Iteration 22/1000 | Loss: 0.00002986
Iteration 23/1000 | Loss: 0.00002982
Iteration 24/1000 | Loss: 0.00002978
Iteration 25/1000 | Loss: 0.00002974
Iteration 26/1000 | Loss: 0.00002973
Iteration 27/1000 | Loss: 0.00002972
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002968
Iteration 30/1000 | Loss: 0.00002968
Iteration 31/1000 | Loss: 0.00002964
Iteration 32/1000 | Loss: 0.00002964
Iteration 33/1000 | Loss: 0.00002964
Iteration 34/1000 | Loss: 0.00002964
Iteration 35/1000 | Loss: 0.00002964
Iteration 36/1000 | Loss: 0.00002964
Iteration 37/1000 | Loss: 0.00002963
Iteration 38/1000 | Loss: 0.00002963
Iteration 39/1000 | Loss: 0.00002963
Iteration 40/1000 | Loss: 0.00002963
Iteration 41/1000 | Loss: 0.00002963
Iteration 42/1000 | Loss: 0.00002963
Iteration 43/1000 | Loss: 0.00002963
Iteration 44/1000 | Loss: 0.00002963
Iteration 45/1000 | Loss: 0.00002963
Iteration 46/1000 | Loss: 0.00002962
Iteration 47/1000 | Loss: 0.00002961
Iteration 48/1000 | Loss: 0.00002961
Iteration 49/1000 | Loss: 0.00002960
Iteration 50/1000 | Loss: 0.00002960
Iteration 51/1000 | Loss: 0.00002960
Iteration 52/1000 | Loss: 0.00002960
Iteration 53/1000 | Loss: 0.00002960
Iteration 54/1000 | Loss: 0.00002960
Iteration 55/1000 | Loss: 0.00002960
Iteration 56/1000 | Loss: 0.00002960
Iteration 57/1000 | Loss: 0.00002960
Iteration 58/1000 | Loss: 0.00002960
Iteration 59/1000 | Loss: 0.00002960
Iteration 60/1000 | Loss: 0.00002960
Iteration 61/1000 | Loss: 0.00002960
Iteration 62/1000 | Loss: 0.00002960
Iteration 63/1000 | Loss: 0.00002959
Iteration 64/1000 | Loss: 0.00002958
Iteration 65/1000 | Loss: 0.00002958
Iteration 66/1000 | Loss: 0.00002957
Iteration 67/1000 | Loss: 0.00002957
Iteration 68/1000 | Loss: 0.00002957
Iteration 69/1000 | Loss: 0.00002957
Iteration 70/1000 | Loss: 0.00002957
Iteration 71/1000 | Loss: 0.00002957
Iteration 72/1000 | Loss: 0.00002957
Iteration 73/1000 | Loss: 0.00002956
Iteration 74/1000 | Loss: 0.00002956
Iteration 75/1000 | Loss: 0.00002956
Iteration 76/1000 | Loss: 0.00002956
Iteration 77/1000 | Loss: 0.00002956
Iteration 78/1000 | Loss: 0.00002956
Iteration 79/1000 | Loss: 0.00002956
Iteration 80/1000 | Loss: 0.00002956
Iteration 81/1000 | Loss: 0.00002956
Iteration 82/1000 | Loss: 0.00002955
Iteration 83/1000 | Loss: 0.00002955
Iteration 84/1000 | Loss: 0.00002955
Iteration 85/1000 | Loss: 0.00002955
Iteration 86/1000 | Loss: 0.00002955
Iteration 87/1000 | Loss: 0.00002955
Iteration 88/1000 | Loss: 0.00002954
Iteration 89/1000 | Loss: 0.00002954
Iteration 90/1000 | Loss: 0.00002954
Iteration 91/1000 | Loss: 0.00002954
Iteration 92/1000 | Loss: 0.00002953
Iteration 93/1000 | Loss: 0.00002953
Iteration 94/1000 | Loss: 0.00002953
Iteration 95/1000 | Loss: 0.00002953
Iteration 96/1000 | Loss: 0.00002953
Iteration 97/1000 | Loss: 0.00002953
Iteration 98/1000 | Loss: 0.00002953
Iteration 99/1000 | Loss: 0.00002953
Iteration 100/1000 | Loss: 0.00002953
Iteration 101/1000 | Loss: 0.00002952
Iteration 102/1000 | Loss: 0.00002952
Iteration 103/1000 | Loss: 0.00002952
Iteration 104/1000 | Loss: 0.00002952
Iteration 105/1000 | Loss: 0.00002952
Iteration 106/1000 | Loss: 0.00002952
Iteration 107/1000 | Loss: 0.00002952
Iteration 108/1000 | Loss: 0.00002951
Iteration 109/1000 | Loss: 0.00002951
Iteration 110/1000 | Loss: 0.00002951
Iteration 111/1000 | Loss: 0.00002951
Iteration 112/1000 | Loss: 0.00002951
Iteration 113/1000 | Loss: 0.00002951
Iteration 114/1000 | Loss: 0.00002951
Iteration 115/1000 | Loss: 0.00002951
Iteration 116/1000 | Loss: 0.00002950
Iteration 117/1000 | Loss: 0.00002950
Iteration 118/1000 | Loss: 0.00002950
Iteration 119/1000 | Loss: 0.00002950
Iteration 120/1000 | Loss: 0.00002950
Iteration 121/1000 | Loss: 0.00002949
Iteration 122/1000 | Loss: 0.00002949
Iteration 123/1000 | Loss: 0.00002949
Iteration 124/1000 | Loss: 0.00002949
Iteration 125/1000 | Loss: 0.00002949
Iteration 126/1000 | Loss: 0.00002949
Iteration 127/1000 | Loss: 0.00002949
Iteration 128/1000 | Loss: 0.00002949
Iteration 129/1000 | Loss: 0.00002949
Iteration 130/1000 | Loss: 0.00002948
Iteration 131/1000 | Loss: 0.00002948
Iteration 132/1000 | Loss: 0.00002948
Iteration 133/1000 | Loss: 0.00002948
Iteration 134/1000 | Loss: 0.00002948
Iteration 135/1000 | Loss: 0.00002948
Iteration 136/1000 | Loss: 0.00002948
Iteration 137/1000 | Loss: 0.00002948
Iteration 138/1000 | Loss: 0.00002947
Iteration 139/1000 | Loss: 0.00002947
Iteration 140/1000 | Loss: 0.00002947
Iteration 141/1000 | Loss: 0.00002947
Iteration 142/1000 | Loss: 0.00002947
Iteration 143/1000 | Loss: 0.00002947
Iteration 144/1000 | Loss: 0.00002946
Iteration 145/1000 | Loss: 0.00002946
Iteration 146/1000 | Loss: 0.00002946
Iteration 147/1000 | Loss: 0.00002946
Iteration 148/1000 | Loss: 0.00002946
Iteration 149/1000 | Loss: 0.00002946
Iteration 150/1000 | Loss: 0.00002946
Iteration 151/1000 | Loss: 0.00002945
Iteration 152/1000 | Loss: 0.00002945
Iteration 153/1000 | Loss: 0.00002945
Iteration 154/1000 | Loss: 0.00002945
Iteration 155/1000 | Loss: 0.00002945
Iteration 156/1000 | Loss: 0.00002944
Iteration 157/1000 | Loss: 0.00002944
Iteration 158/1000 | Loss: 0.00002944
Iteration 159/1000 | Loss: 0.00002944
Iteration 160/1000 | Loss: 0.00002944
Iteration 161/1000 | Loss: 0.00002944
Iteration 162/1000 | Loss: 0.00002944
Iteration 163/1000 | Loss: 0.00002944
Iteration 164/1000 | Loss: 0.00002944
Iteration 165/1000 | Loss: 0.00002944
Iteration 166/1000 | Loss: 0.00002944
Iteration 167/1000 | Loss: 0.00002944
Iteration 168/1000 | Loss: 0.00002944
Iteration 169/1000 | Loss: 0.00002944
Iteration 170/1000 | Loss: 0.00002944
Iteration 171/1000 | Loss: 0.00002944
Iteration 172/1000 | Loss: 0.00002944
Iteration 173/1000 | Loss: 0.00002944
Iteration 174/1000 | Loss: 0.00002944
Iteration 175/1000 | Loss: 0.00002944
Iteration 176/1000 | Loss: 0.00002944
Iteration 177/1000 | Loss: 0.00002944
Iteration 178/1000 | Loss: 0.00002944
Iteration 179/1000 | Loss: 0.00002944
Iteration 180/1000 | Loss: 0.00002944
Iteration 181/1000 | Loss: 0.00002944
Iteration 182/1000 | Loss: 0.00002944
Iteration 183/1000 | Loss: 0.00002944
Iteration 184/1000 | Loss: 0.00002944
Iteration 185/1000 | Loss: 0.00002944
Iteration 186/1000 | Loss: 0.00002944
Iteration 187/1000 | Loss: 0.00002944
Iteration 188/1000 | Loss: 0.00002944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.9437833291012794e-05, 2.9437833291012794e-05, 2.9437833291012794e-05, 2.9437833291012794e-05, 2.9437833291012794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9437833291012794e-05

Optimization complete. Final v2v error: 4.4138264656066895 mm

Highest mean error: 5.668727874755859 mm for frame 93

Lowest mean error: 3.2634592056274414 mm for frame 54

Saving results

Total time: 54.38431930541992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976668
Iteration 2/25 | Loss: 0.00234530
Iteration 3/25 | Loss: 0.00163705
Iteration 4/25 | Loss: 0.00147183
Iteration 5/25 | Loss: 0.00142529
Iteration 6/25 | Loss: 0.00138837
Iteration 7/25 | Loss: 0.00130293
Iteration 8/25 | Loss: 0.00124500
Iteration 9/25 | Loss: 0.00119718
Iteration 10/25 | Loss: 0.00119429
Iteration 11/25 | Loss: 0.00119184
Iteration 12/25 | Loss: 0.00117284
Iteration 13/25 | Loss: 0.00116451
Iteration 14/25 | Loss: 0.00115122
Iteration 15/25 | Loss: 0.00114300
Iteration 16/25 | Loss: 0.00113189
Iteration 17/25 | Loss: 0.00113259
Iteration 18/25 | Loss: 0.00114071
Iteration 19/25 | Loss: 0.00112965
Iteration 20/25 | Loss: 0.00113062
Iteration 21/25 | Loss: 0.00112879
Iteration 22/25 | Loss: 0.00112181
Iteration 23/25 | Loss: 0.00112436
Iteration 24/25 | Loss: 0.00112931
Iteration 25/25 | Loss: 0.00112690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29569769
Iteration 2/25 | Loss: 0.00233303
Iteration 3/25 | Loss: 0.00212384
Iteration 4/25 | Loss: 0.00212384
Iteration 5/25 | Loss: 0.00212384
Iteration 6/25 | Loss: 0.00212384
Iteration 7/25 | Loss: 0.00212384
Iteration 8/25 | Loss: 0.00212384
Iteration 9/25 | Loss: 0.00212384
Iteration 10/25 | Loss: 0.00212384
Iteration 11/25 | Loss: 0.00212384
Iteration 12/25 | Loss: 0.00212384
Iteration 13/25 | Loss: 0.00212384
Iteration 14/25 | Loss: 0.00212384
Iteration 15/25 | Loss: 0.00212384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021238396875560284, 0.0021238396875560284, 0.0021238396875560284, 0.0021238396875560284, 0.0021238396875560284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021238396875560284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212384
Iteration 2/1000 | Loss: 0.00052781
Iteration 3/1000 | Loss: 0.00046575
Iteration 4/1000 | Loss: 0.00076833
Iteration 5/1000 | Loss: 0.00030015
Iteration 6/1000 | Loss: 0.00016005
Iteration 7/1000 | Loss: 0.00029989
Iteration 8/1000 | Loss: 0.00010373
Iteration 9/1000 | Loss: 0.00010307
Iteration 10/1000 | Loss: 0.00009736
Iteration 11/1000 | Loss: 0.00022299
Iteration 12/1000 | Loss: 0.00011151
Iteration 13/1000 | Loss: 0.00011759
Iteration 14/1000 | Loss: 0.00022236
Iteration 15/1000 | Loss: 0.00008320
Iteration 16/1000 | Loss: 0.00018559
Iteration 17/1000 | Loss: 0.00014103
Iteration 18/1000 | Loss: 0.00010528
Iteration 19/1000 | Loss: 0.00094962
Iteration 20/1000 | Loss: 0.00059682
Iteration 21/1000 | Loss: 0.00119229
Iteration 22/1000 | Loss: 0.00075984
Iteration 23/1000 | Loss: 0.00045599
Iteration 24/1000 | Loss: 0.00027005
Iteration 25/1000 | Loss: 0.00022277
Iteration 26/1000 | Loss: 0.00033069
Iteration 27/1000 | Loss: 0.00020541
Iteration 28/1000 | Loss: 0.00029231
Iteration 29/1000 | Loss: 0.00018477
Iteration 30/1000 | Loss: 0.00028342
Iteration 31/1000 | Loss: 0.00019746
Iteration 32/1000 | Loss: 0.00027728
Iteration 33/1000 | Loss: 0.00046814
Iteration 34/1000 | Loss: 0.00021337
Iteration 35/1000 | Loss: 0.00024125
Iteration 36/1000 | Loss: 0.00014971
Iteration 37/1000 | Loss: 0.00010089
Iteration 38/1000 | Loss: 0.00029750
Iteration 39/1000 | Loss: 0.00040879
Iteration 40/1000 | Loss: 0.00029748
Iteration 41/1000 | Loss: 0.00028445
Iteration 42/1000 | Loss: 0.00051166
Iteration 43/1000 | Loss: 0.00058981
Iteration 44/1000 | Loss: 0.00013249
Iteration 45/1000 | Loss: 0.00009934
Iteration 46/1000 | Loss: 0.00011790
Iteration 47/1000 | Loss: 0.00007934
Iteration 48/1000 | Loss: 0.00009483
Iteration 49/1000 | Loss: 0.00009348
Iteration 50/1000 | Loss: 0.00015907
Iteration 51/1000 | Loss: 0.00008091
Iteration 52/1000 | Loss: 0.00008156
Iteration 53/1000 | Loss: 0.00033765
Iteration 54/1000 | Loss: 0.00057048
Iteration 55/1000 | Loss: 0.00047720
Iteration 56/1000 | Loss: 0.00061796
Iteration 57/1000 | Loss: 0.00043004
Iteration 58/1000 | Loss: 0.00052449
Iteration 59/1000 | Loss: 0.00050481
Iteration 60/1000 | Loss: 0.00011787
Iteration 61/1000 | Loss: 0.00010821
Iteration 62/1000 | Loss: 0.00009445
Iteration 63/1000 | Loss: 0.00010591
Iteration 64/1000 | Loss: 0.00009010
Iteration 65/1000 | Loss: 0.00008423
Iteration 66/1000 | Loss: 0.00010870
Iteration 67/1000 | Loss: 0.00012432
Iteration 68/1000 | Loss: 0.00010248
Iteration 69/1000 | Loss: 0.00013763
Iteration 70/1000 | Loss: 0.00010714
Iteration 71/1000 | Loss: 0.00009169
Iteration 72/1000 | Loss: 0.00009732
Iteration 73/1000 | Loss: 0.00007667
Iteration 74/1000 | Loss: 0.00008661
Iteration 75/1000 | Loss: 0.00007110
Iteration 76/1000 | Loss: 0.00043351
Iteration 77/1000 | Loss: 0.00018928
Iteration 78/1000 | Loss: 0.00009981
Iteration 79/1000 | Loss: 0.00009020
Iteration 80/1000 | Loss: 0.00049585
Iteration 81/1000 | Loss: 0.00139224
Iteration 82/1000 | Loss: 0.00063620
Iteration 83/1000 | Loss: 0.00214446
Iteration 84/1000 | Loss: 0.00011827
Iteration 85/1000 | Loss: 0.00012348
Iteration 86/1000 | Loss: 0.00016661
Iteration 87/1000 | Loss: 0.00020259
Iteration 88/1000 | Loss: 0.00005676
Iteration 89/1000 | Loss: 0.00005256
Iteration 90/1000 | Loss: 0.00004988
Iteration 91/1000 | Loss: 0.00004821
Iteration 92/1000 | Loss: 0.00004722
Iteration 93/1000 | Loss: 0.00055931
Iteration 94/1000 | Loss: 0.00004750
Iteration 95/1000 | Loss: 0.00004545
Iteration 96/1000 | Loss: 0.00004399
Iteration 97/1000 | Loss: 0.00030124
Iteration 98/1000 | Loss: 0.00004405
Iteration 99/1000 | Loss: 0.00044224
Iteration 100/1000 | Loss: 0.00004454
Iteration 101/1000 | Loss: 0.00004164
Iteration 102/1000 | Loss: 0.00018866
Iteration 103/1000 | Loss: 0.00022653
Iteration 104/1000 | Loss: 0.00023051
Iteration 105/1000 | Loss: 0.00024706
Iteration 106/1000 | Loss: 0.00032937
Iteration 107/1000 | Loss: 0.00022952
Iteration 108/1000 | Loss: 0.00017536
Iteration 109/1000 | Loss: 0.00017065
Iteration 110/1000 | Loss: 0.00004255
Iteration 111/1000 | Loss: 0.00004017
Iteration 112/1000 | Loss: 0.00042831
Iteration 113/1000 | Loss: 0.00009428
Iteration 114/1000 | Loss: 0.00011263
Iteration 115/1000 | Loss: 0.00004255
Iteration 116/1000 | Loss: 0.00003860
Iteration 117/1000 | Loss: 0.00003617
Iteration 118/1000 | Loss: 0.00003544
Iteration 119/1000 | Loss: 0.00003493
Iteration 120/1000 | Loss: 0.00003435
Iteration 121/1000 | Loss: 0.00014005
Iteration 122/1000 | Loss: 0.00033341
Iteration 123/1000 | Loss: 0.00003585
Iteration 124/1000 | Loss: 0.00003361
Iteration 125/1000 | Loss: 0.00003251
Iteration 126/1000 | Loss: 0.00021418
Iteration 127/1000 | Loss: 0.00004810
Iteration 128/1000 | Loss: 0.00003382
Iteration 129/1000 | Loss: 0.00041806
Iteration 130/1000 | Loss: 0.00004240
Iteration 131/1000 | Loss: 0.00004521
Iteration 132/1000 | Loss: 0.00040996
Iteration 133/1000 | Loss: 0.00048901
Iteration 134/1000 | Loss: 0.00151814
Iteration 135/1000 | Loss: 0.00008719
Iteration 136/1000 | Loss: 0.00004537
Iteration 137/1000 | Loss: 0.00003313
Iteration 138/1000 | Loss: 0.00012955
Iteration 139/1000 | Loss: 0.00004180
Iteration 140/1000 | Loss: 0.00004062
Iteration 141/1000 | Loss: 0.00002603
Iteration 142/1000 | Loss: 0.00002474
Iteration 143/1000 | Loss: 0.00002404
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002305
Iteration 146/1000 | Loss: 0.00002282
Iteration 147/1000 | Loss: 0.00002264
Iteration 148/1000 | Loss: 0.00002246
Iteration 149/1000 | Loss: 0.00002242
Iteration 150/1000 | Loss: 0.00002225
Iteration 151/1000 | Loss: 0.00020913
Iteration 152/1000 | Loss: 0.00002609
Iteration 153/1000 | Loss: 0.00002298
Iteration 154/1000 | Loss: 0.00002223
Iteration 155/1000 | Loss: 0.00002208
Iteration 156/1000 | Loss: 0.00002205
Iteration 157/1000 | Loss: 0.00002205
Iteration 158/1000 | Loss: 0.00002205
Iteration 159/1000 | Loss: 0.00002205
Iteration 160/1000 | Loss: 0.00002205
Iteration 161/1000 | Loss: 0.00002205
Iteration 162/1000 | Loss: 0.00002205
Iteration 163/1000 | Loss: 0.00002205
Iteration 164/1000 | Loss: 0.00002205
Iteration 165/1000 | Loss: 0.00002205
Iteration 166/1000 | Loss: 0.00002205
Iteration 167/1000 | Loss: 0.00002204
Iteration 168/1000 | Loss: 0.00002204
Iteration 169/1000 | Loss: 0.00002204
Iteration 170/1000 | Loss: 0.00002204
Iteration 171/1000 | Loss: 0.00002204
Iteration 172/1000 | Loss: 0.00002204
Iteration 173/1000 | Loss: 0.00002204
Iteration 174/1000 | Loss: 0.00002204
Iteration 175/1000 | Loss: 0.00002203
Iteration 176/1000 | Loss: 0.00002203
Iteration 177/1000 | Loss: 0.00002203
Iteration 178/1000 | Loss: 0.00002202
Iteration 179/1000 | Loss: 0.00002202
Iteration 180/1000 | Loss: 0.00002202
Iteration 181/1000 | Loss: 0.00002201
Iteration 182/1000 | Loss: 0.00002201
Iteration 183/1000 | Loss: 0.00002201
Iteration 184/1000 | Loss: 0.00002197
Iteration 185/1000 | Loss: 0.00002197
Iteration 186/1000 | Loss: 0.00002196
Iteration 187/1000 | Loss: 0.00002196
Iteration 188/1000 | Loss: 0.00002195
Iteration 189/1000 | Loss: 0.00002194
Iteration 190/1000 | Loss: 0.00002194
Iteration 191/1000 | Loss: 0.00002193
Iteration 192/1000 | Loss: 0.00002193
Iteration 193/1000 | Loss: 0.00002192
Iteration 194/1000 | Loss: 0.00002192
Iteration 195/1000 | Loss: 0.00002192
Iteration 196/1000 | Loss: 0.00002191
Iteration 197/1000 | Loss: 0.00002191
Iteration 198/1000 | Loss: 0.00002190
Iteration 199/1000 | Loss: 0.00021519
Iteration 200/1000 | Loss: 0.00004488
Iteration 201/1000 | Loss: 0.00002212
Iteration 202/1000 | Loss: 0.00004689
Iteration 203/1000 | Loss: 0.00002198
Iteration 204/1000 | Loss: 0.00002184
Iteration 205/1000 | Loss: 0.00002182
Iteration 206/1000 | Loss: 0.00002177
Iteration 207/1000 | Loss: 0.00002177
Iteration 208/1000 | Loss: 0.00002177
Iteration 209/1000 | Loss: 0.00002177
Iteration 210/1000 | Loss: 0.00002176
Iteration 211/1000 | Loss: 0.00002176
Iteration 212/1000 | Loss: 0.00002176
Iteration 213/1000 | Loss: 0.00002176
Iteration 214/1000 | Loss: 0.00002176
Iteration 215/1000 | Loss: 0.00002176
Iteration 216/1000 | Loss: 0.00002176
Iteration 217/1000 | Loss: 0.00002175
Iteration 218/1000 | Loss: 0.00002175
Iteration 219/1000 | Loss: 0.00002175
Iteration 220/1000 | Loss: 0.00002175
Iteration 221/1000 | Loss: 0.00002175
Iteration 222/1000 | Loss: 0.00002175
Iteration 223/1000 | Loss: 0.00002175
Iteration 224/1000 | Loss: 0.00002175
Iteration 225/1000 | Loss: 0.00002174
Iteration 226/1000 | Loss: 0.00002174
Iteration 227/1000 | Loss: 0.00002174
Iteration 228/1000 | Loss: 0.00002174
Iteration 229/1000 | Loss: 0.00002174
Iteration 230/1000 | Loss: 0.00002174
Iteration 231/1000 | Loss: 0.00002173
Iteration 232/1000 | Loss: 0.00002173
Iteration 233/1000 | Loss: 0.00002173
Iteration 234/1000 | Loss: 0.00002173
Iteration 235/1000 | Loss: 0.00002172
Iteration 236/1000 | Loss: 0.00002172
Iteration 237/1000 | Loss: 0.00002172
Iteration 238/1000 | Loss: 0.00002172
Iteration 239/1000 | Loss: 0.00002172
Iteration 240/1000 | Loss: 0.00002172
Iteration 241/1000 | Loss: 0.00002171
Iteration 242/1000 | Loss: 0.00002171
Iteration 243/1000 | Loss: 0.00002171
Iteration 244/1000 | Loss: 0.00002171
Iteration 245/1000 | Loss: 0.00002171
Iteration 246/1000 | Loss: 0.00002171
Iteration 247/1000 | Loss: 0.00002170
Iteration 248/1000 | Loss: 0.00002170
Iteration 249/1000 | Loss: 0.00002170
Iteration 250/1000 | Loss: 0.00002170
Iteration 251/1000 | Loss: 0.00002169
Iteration 252/1000 | Loss: 0.00002169
Iteration 253/1000 | Loss: 0.00002169
Iteration 254/1000 | Loss: 0.00002169
Iteration 255/1000 | Loss: 0.00002169
Iteration 256/1000 | Loss: 0.00002168
Iteration 257/1000 | Loss: 0.00002168
Iteration 258/1000 | Loss: 0.00002168
Iteration 259/1000 | Loss: 0.00002168
Iteration 260/1000 | Loss: 0.00002168
Iteration 261/1000 | Loss: 0.00002168
Iteration 262/1000 | Loss: 0.00002167
Iteration 263/1000 | Loss: 0.00002167
Iteration 264/1000 | Loss: 0.00002167
Iteration 265/1000 | Loss: 0.00002167
Iteration 266/1000 | Loss: 0.00002167
Iteration 267/1000 | Loss: 0.00002167
Iteration 268/1000 | Loss: 0.00002167
Iteration 269/1000 | Loss: 0.00002167
Iteration 270/1000 | Loss: 0.00002167
Iteration 271/1000 | Loss: 0.00002167
Iteration 272/1000 | Loss: 0.00002167
Iteration 273/1000 | Loss: 0.00002167
Iteration 274/1000 | Loss: 0.00002167
Iteration 275/1000 | Loss: 0.00002167
Iteration 276/1000 | Loss: 0.00002167
Iteration 277/1000 | Loss: 0.00002167
Iteration 278/1000 | Loss: 0.00002167
Iteration 279/1000 | Loss: 0.00002167
Iteration 280/1000 | Loss: 0.00002167
Iteration 281/1000 | Loss: 0.00002167
Iteration 282/1000 | Loss: 0.00002167
Iteration 283/1000 | Loss: 0.00002167
Iteration 284/1000 | Loss: 0.00002167
Iteration 285/1000 | Loss: 0.00002167
Iteration 286/1000 | Loss: 0.00002167
Iteration 287/1000 | Loss: 0.00002167
Iteration 288/1000 | Loss: 0.00002167
Iteration 289/1000 | Loss: 0.00002167
Iteration 290/1000 | Loss: 0.00002167
Iteration 291/1000 | Loss: 0.00002167
Iteration 292/1000 | Loss: 0.00002167
Iteration 293/1000 | Loss: 0.00002167
Iteration 294/1000 | Loss: 0.00002167
Iteration 295/1000 | Loss: 0.00002167
Iteration 296/1000 | Loss: 0.00002167
Iteration 297/1000 | Loss: 0.00002167
Iteration 298/1000 | Loss: 0.00002167
Iteration 299/1000 | Loss: 0.00002167
Iteration 300/1000 | Loss: 0.00002167
Iteration 301/1000 | Loss: 0.00002167
Iteration 302/1000 | Loss: 0.00002167
Iteration 303/1000 | Loss: 0.00002167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [2.1669337002094835e-05, 2.1669337002094835e-05, 2.1669337002094835e-05, 2.1669337002094835e-05, 2.1669337002094835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1669337002094835e-05

Optimization complete. Final v2v error: 3.889735221862793 mm

Highest mean error: 4.725459098815918 mm for frame 102

Lowest mean error: 2.8162832260131836 mm for frame 1

Saving results

Total time: 276.08705258369446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2681/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2681/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880057
Iteration 2/25 | Loss: 0.00128759
Iteration 3/25 | Loss: 0.00109374
Iteration 4/25 | Loss: 0.00107564
Iteration 5/25 | Loss: 0.00107347
Iteration 6/25 | Loss: 0.00107347
Iteration 7/25 | Loss: 0.00107347
Iteration 8/25 | Loss: 0.00107347
Iteration 9/25 | Loss: 0.00107347
Iteration 10/25 | Loss: 0.00107347
Iteration 11/25 | Loss: 0.00107347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001073466963134706, 0.001073466963134706, 0.001073466963134706, 0.001073466963134706, 0.001073466963134706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001073466963134706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28693533
Iteration 2/25 | Loss: 0.00101104
Iteration 3/25 | Loss: 0.00101103
Iteration 4/25 | Loss: 0.00101103
Iteration 5/25 | Loss: 0.00101103
Iteration 6/25 | Loss: 0.00101103
Iteration 7/25 | Loss: 0.00101103
Iteration 8/25 | Loss: 0.00101103
Iteration 9/25 | Loss: 0.00101103
Iteration 10/25 | Loss: 0.00101103
Iteration 11/25 | Loss: 0.00101103
Iteration 12/25 | Loss: 0.00101103
Iteration 13/25 | Loss: 0.00101103
Iteration 14/25 | Loss: 0.00101103
Iteration 15/25 | Loss: 0.00101103
Iteration 16/25 | Loss: 0.00101103
Iteration 17/25 | Loss: 0.00101103
Iteration 18/25 | Loss: 0.00101103
Iteration 19/25 | Loss: 0.00101103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001011030632071197, 0.001011030632071197, 0.001011030632071197, 0.001011030632071197, 0.001011030632071197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001011030632071197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101103
Iteration 2/1000 | Loss: 0.00002920
Iteration 3/1000 | Loss: 0.00002031
Iteration 4/1000 | Loss: 0.00001786
Iteration 5/1000 | Loss: 0.00001724
Iteration 6/1000 | Loss: 0.00001670
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001627
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001564
Iteration 11/1000 | Loss: 0.00001561
Iteration 12/1000 | Loss: 0.00001554
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001548
Iteration 15/1000 | Loss: 0.00001547
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001530
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001522
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001510
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001492
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001490
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001490
Iteration 41/1000 | Loss: 0.00001490
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001489
Iteration 45/1000 | Loss: 0.00001489
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001487
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001485
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001484
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001483
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00001481
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001480
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001479
Iteration 92/1000 | Loss: 0.00001479
Iteration 93/1000 | Loss: 0.00001479
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001478
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001476
Iteration 118/1000 | Loss: 0.00001476
Iteration 119/1000 | Loss: 0.00001476
Iteration 120/1000 | Loss: 0.00001476
Iteration 121/1000 | Loss: 0.00001476
Iteration 122/1000 | Loss: 0.00001476
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001473
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001472
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001472
Iteration 155/1000 | Loss: 0.00001472
Iteration 156/1000 | Loss: 0.00001472
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001472
Iteration 161/1000 | Loss: 0.00001472
Iteration 162/1000 | Loss: 0.00001472
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Iteration 165/1000 | Loss: 0.00001472
Iteration 166/1000 | Loss: 0.00001472
Iteration 167/1000 | Loss: 0.00001472
Iteration 168/1000 | Loss: 0.00001472
Iteration 169/1000 | Loss: 0.00001472
Iteration 170/1000 | Loss: 0.00001472
Iteration 171/1000 | Loss: 0.00001472
Iteration 172/1000 | Loss: 0.00001472
Iteration 173/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.4724580069014337e-05, 1.4724580069014337e-05, 1.4724580069014337e-05, 1.4724580069014337e-05, 1.4724580069014337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4724580069014337e-05

Optimization complete. Final v2v error: 3.3034517765045166 mm

Highest mean error: 3.952376127243042 mm for frame 130

Lowest mean error: 2.919565200805664 mm for frame 97

Saving results

Total time: 43.85638213157654
